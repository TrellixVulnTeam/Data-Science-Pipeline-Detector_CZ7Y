{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import libraries and set plot style\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explorative Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read train dataset\ndf_train = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2021/train.csv')\ndf_test = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2021/test.csv')\nsubmission = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2021/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get columns and rows\nprint(\"There are\", df_train.shape[0], \"rows and\", df_train.shape[1], \"columns.\")\n\n# Print all columns\nprint(df_train.columns)\n\n# Divide into X and Y (features and target)\nx_train = df_train.drop(columns=['target', 'id'])\ny_train = df_train['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 25 feature columns with 10 categorical and 14 continuous variables. We get rid of the 'id' column since it is a unique identifier of each row.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Target\nLet's begin with plotting the distribution of our target variable. As it is one-dimensional, it is an easy way to have a first grasp of our data. \n\nWe see that it looks like a normal distribution centered around 7.5 with a standard deviation of ~ 2."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.hist(bins=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features\nWe have to types of features (categorical and continuous). Let's first focus on the continuous ones.\n\n### Continuous features\n\nInsights:\n - All values are roughly in the interval [0,1] but some of them lie beyond the boundaries.\n - cont4 seems to have a very sharp peak around 0.275\n - All other features behave really smooth except for con1, which has some sharp peaks.\n - There seems to be some correlated features (cont0-cont5, cont5-cont8, ...)\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get continous columns\ncont_cols = [col for col in x_train.columns if 'cont' in col]\n\n# Subset desired columns\nx_cont = x_train[cont_cols]\n\n# Plot distributions\nfig, ax = plt.subplots(figsize=(15,15))\nsns.histplot(x_cont, stat='density', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation matrix\nfig, ax = plt.subplots(figsize=(15,15))\ncolormap = sns.color_palette(\"Greens\") \ncorrelation_mat = x_cont.corr()\nsns.heatmap(correlation_mat, annot = False, ax=ax, cmap=colormap)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical features\n\n- Number of categories is very different. There are three binary features and one feature with 15 possible categories.\n- Each category occurs with a certain frequency but this does not provide us with any useful information so we omit the plots.\n- Cr√°mer's V correlation matrix shows that there are no big correlations between categorical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get desired columns\ncat_cols = [col for col in x_train.columns if 'cat' in col]\n\n# Subset\nx_cat = x_train[cat_cols]\n\n# Print all different categories\n# Since there are many, I opted for printing the counts instead of plotting them.\nfor col in cat_cols:\n    print(col, x_cat[col].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cramer's V\nfrom scipy import stats\n\ndef get_cramer_correlation(X, y):\n    confusion_matrix = pd.crosstab(X, y)\n    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2 / n\n    r, k = confusion_matrix.shape\n    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n    rcorr = r - ((r - 1) ** 2) / (n - 1)\n    kcorr = k - ((k - 1) ** 2) / (n - 1)\n    return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))\n\n\ndef get_cramer_correlation_matrix(X):\n    # Get columns\n    X_columns = X.columns\n    n_cols = len(X_columns)\n\n    # Initialize to zeros\n    coefficients = np.zeros(shape=(n_cols, n_cols))\n    for i in range(n_cols):\n        for j in range(n_cols):\n            coefficients[i, j] = get_cramer_correlation(X[X_columns[i]], X[X_columns[j]])\n\n    return coefficients\n\n\ndef plot_cramer_correlation_matrix(X):\n    # Get correlation matrix from features\n    corr_matrix = get_cramer_correlation_matrix(X)\n    \n    # Plot\n    cols = X.columns\n    fig, ax = plt.subplots(1, 1)\n    img = ax.imshow(corr_matrix, alpha=0.8, cmap='OrRd')\n    ax.set_xticks(range(len(cols)))\n    ax.set_yticks(range(len(cols)))\n    ax.set_xticklabels(cols, rotation=90)\n    ax.set_yticklabels(cols)\n    fig.colorbar(img)\n\n    return\n\nplot_cramer_correlation_matrix(x_cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline model\n\nWith categorical encodings, let's create a first conversion of the categorical features into numerical. Then, we drop the categorical features and feed a LightGBM with the remaining features."},{"metadata":{"trusted":true},"cell_type":"code","source":"from category_encoders import MEstimateEncoder\n\ndef mee_encode(train_df, test_df, column):\n    mee = MEstimateEncoder()\n    new_feature = \"{}_mee\".format(column)\n    mee.fit(train_df[column], train_df[\"target\"])\n    train_df[new_feature] = mee.transform(train_df[column])\n    test_df[new_feature] = mee.transform(test_df[column])\n    return new_feature\n\nmee_features = []\nfor col in cat_cols:\n    mee_features.append(mee_encode(df_train, df_test, col))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split features from target\ndf_train_baseline = df_train.drop(columns=cat_cols)\ndf_test_baseline = df_test.drop(columns=cat_cols)\n\nX_train_baseline = df_train_baseline.drop(columns=['target', 'id'])\ny_train_baseline = df_train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train our model with some basic parameters\nimport xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train_baseline, label=y_train_baseline)\nparam = {'max_depth': 2, 'eta': 1, 'objective': 'reg:squarederror'}\nparam['eval_metric'] = 'rmse'\n\nnum_round = 1000\nevallist = [(dtrain, 'train')]\nbst = xgb.train(param, dtrain, num_round, evallist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predictions\ndf_test_x = df_test_baseline.drop(columns=['id'])\ndtest = xgb.DMatrix(df_test_x)\npredictions = bst.predict(dtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save submission\nmy_submission = pd.DataFrame(df_test.copy()['id'])\nmy_submission = my_submission.assign(target=predictions)\nmy_submission.to_csv('/kaggle/working/submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}