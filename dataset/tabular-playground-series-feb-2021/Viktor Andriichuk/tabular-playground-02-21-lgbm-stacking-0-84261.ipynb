{"cells":[{"metadata":{},"cell_type":"markdown","source":"I used LightGBM as a first attempt to submit.\nNot much EDA or Feature engineering, I only did encode the categoricals.\nI think there are so many things for improvment, as I am just new to Kaggle.\nHope you guys like it, and please upvote it!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-feb-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-feb-2021/test.csv')\nsample_sub = pd.read_csv('../input/tabular-playground-series-feb-2021/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"delete_columns = ['id']\ntrain.drop(delete_columns, axis=1, inplace=True)\ntest.drop(delete_columns, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nplt.figure(figsize=(10,5))\nsns.histplot(train['target'], color='slategray', stat='frequency');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['target'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train[train['target'] < 4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop = train[train['target'] < 4].index\ntrain.drop(to_drop, inplace=True)\ntrain = train.reset_index()\ntrain.info()\n\nplt.figure(figsize=(10,5))\nsns.histplot(train['target'], color='slategray', stat='frequency');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['target'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's look better"},{"metadata":{"trusted":true},"cell_type":"code","source":"delete_columns = ['index']\ntrain.drop(delete_columns, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas_profiling import ProfileReport\n\nprofile = ProfileReport(train, title='Pandas Profiling Report')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"profile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop non-infarmative features: cat0, cat4, cat6, cat7\nnon_infarmative_features = ['cat0', 'cat4', 'cat6', 'cat7']\ntrain.drop(non_infarmative_features, axis=1, inplace=True)\ntest.drop(non_infarmative_features, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in train.columns:\n    if train[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values)+list(test[c].values))\n        train[c] = lbl.transform(train[c].values)\n        test[c] = lbl.transform(test[c].values)\n        \ndisplay(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VAL_SIZE          = 0.33   # 33%\nN_FOLDS           = 5\n\n# RANDOM_SEED\nRANDOM_SEED       = 42","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LightGBM\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train['target']\nX_train = train.drop('target', axis = 1)\nX_test = test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape, X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = ['cat1', 'cat2', 'cat3', 'cat5', 'cat8', 'cat9']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds = []\nmodels = []\noof_train = np.zeros(len(X_train))\ncv = KFold(n_splits=5, shuffle=True, random_state=0)\n\nparams = {\n    'metric': 'rmse', \n    'random_state': 42,\n    'n_estimators': 20000,\n    'reg_alpha': 4.739177609135503,\n    'reg_lambda': 0.023314865020550128,\n    'colsample_bytree': 0.2885575936307841,\n    'subsample': 0.9541423485889546,\n    'learning_rate': 0.02211501295658431,\n    'max_depth': 18,\n    'num_leaves': 305,\n    'min_child_samples': 118,\n    'min_data_per_groups': 89\n}\n\ny_preds = 0\n\nfor fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, y_train)):\n\n    X_tr = X_train.loc[train_index, :]\n    X_val = X_train.loc[valid_index, :]\n    y_tr = y_train[train_index]\n    y_val = y_train[valid_index]\n    \n    lgb_train = lgb.Dataset(X_tr, y_tr, categorical_feature=categorical_features)\n    lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train, categorical_feature=categorical_features)\n    \n    model = lgb.train(params, lgb_train,\n                      valid_sets=[lgb_train, lgb_eval],\n                      verbose_eval = 200,\n                      num_boost_round = 20000,\n                      early_stopping_rounds=1000)\n    \n    oof_train[valid_index] = model.predict(X_val, num_iteration=model.best_iteration)\n    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n    y_preds += y_pred/5\n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(oof_train).to_csv('oof_train_kfold.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds_lgb = pd.DataFrame(y_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VERSION = 8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_sub['target'] = y_preds_lgb\n# sample_sub.to_csv(f'submission_LightGBM_{VERSION}.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import Pool, CatBoostRegressor, cv\nfrom catboost import CatBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CATBOOST\nITERATIONS        = 3000\nLR                = 0.01\nMAX_DEPTH         = 10\nN_ESTIMATORS      = 3000\nL2_LEAF_REG       = 3.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['target'], axis=1,)\ny = train.target.values\nX_sub = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=VAL_SIZE, shuffle=True, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features_ids = np.where(X_train.dtypes == object)[0].tolist()\ncat_features_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ctb = CatBoostRegressor(learning_rate=LR,\n                          max_depth=MAX_DEPTH,\n                          n_estimators=N_ESTIMATORS,\n                          l2_leaf_reg=L2_LEAF_REG,\n                          random_seed=RANDOM_SEED,\n                          eval_metric='RMSE',\n                          custom_metric=['R2', 'MAE']\n                         )\nctb.fit(X_train, y_train,\n          eval_set=(X_test, y_test),\n          verbose_eval=100,\n          cat_features=categorical_features,\n          use_best_model=True,\n          plot=True\n         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ctb.save_model('catboost_single_model_baseline.ctb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\nidx = np.argsort(ctb.feature_importances_)\n\nplt.figure(figsize=(17,8))\n\nsns.barplot(x=ctb.feature_importances_[idx], y=np.array(ctb.feature_names_)[idx])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_submission_ctb = ctb.predict(X_sub)\npredict_submission_ctb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VERSION = 9","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_sub['target'] = predict_submission_ctb\n# sample_sub.to_csv(f'submission_CatBoost_v{VERSION}.csv', index=False)\n# sample_sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['target'], axis=1,)\ny = train.target.values\nX_sub = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=VAL_SIZE, shuffle=True, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_params = {\n    'colsample_bytree':0.4603,\n    'gamma':0.0468,\n    'learning_rate':0.05,\n    'max_depth':3,\n    'min_child_weight':1.7817,\n    'n_estimators':2200,\n    'reg_alpha':0.4640,\n    'reg_lambda':0.8571,\n    'subsample':0.5213,\n    'nthread':-1,\n    'eval_metric':'rmse',\n    'seed':RANDOM_SEED,\n#     'tree_method':'gpu_hist',\n#     'gpu_id':0\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBRegressor(**xgb_params)\nxgb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_xgb = xgb.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\ndef RMSE(y, y_pred):\n    \n    MSE = metrics.mean_squared_error(y, y_pred)\n    print(f'MSE = {MSE:.4f}')\n\n    RMSE = np.sqrt(MSE)\n    print(f'RMSE = {RMSE:.4f}')\n\n    print('-------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = RMSE(y_test, y_pred_xgb)\nrmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_submission_xgb = xgb.predict(X_sub)\npredict_submission_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VERSION = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_sub = pd.read_csv('../input/tabular-playground-series-feb-2021/sample_submission.csv')\n# sample_sub['target'] = predict_submission_xgb\n# sample_sub.to_csv(f'submission_XGBoost_v{VERSION}.csv', index=False)\n# sample_sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Сравнение features between CatBoost and XGBoost"},{"metadata":{},"cell_type":"markdown","source":"https://www.youtube.com/watch?v=5Al4pXdJmAY&ab_channel=DataStartConference"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_results = sorted(zip(map(lambda x: round(x, 4), xgb.feature_importances_), X), reverse=True)\nctb_results = sorted(zip(map(lambda x: round(x, 4), ctb.feature_importances_), X), reverse=True)\n\npd.DataFrame(data = [[(m, v) for v, m in xgb_results],\n                     [(m, v) for v, m in ctb_results]],\n             index=['xgb','ctb']).T.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots(figsize=(8,8))\n\nax1.set_xlabel('Порядок параметра')\nax1.set_ylabel('Веса Forest&Tree')\n\n# ось 1\nax1.plot([v for v,m in xgb_results][:100], 'b')\n\nax2 = ax1.twinx() #добавление оси\n\nax2.set_ylabel('Веса CatBoost')\nax2.plot([v for v,m in ctb_results][:100], 'y')\nfig.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res_dict = dict()\n\nfor i in range(20):\n        for mod in ([m for v,m in xgb_results],\n                    [m for v,m in ctb_results]):\n            if mod[i] in res_dict:\n                res_dict[mod[i]] += 1\n            else:\n                res_dict[mod[i]] = 1\n\nresulted = sorted(zip(map(lambda x: x, res_dict.values()), res_dict.keys()), reverse=True)\nresulted[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for_use_vars = [k for v,k in resulted]\nprint('В выборке: ', len(for_use_vars))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zeros = []\nfor k in [k for v,k in resulted]:\n    for model in (xgb_results, ctb_results):\n        if k in [m for v,m in model[:100]]:\n            if [v for v,m in model][list([m for v,m in model][:100]).index(k)] == 0:\n                zeros.append(k)\n\nprint('В выборке: ', len(zeros))\n\n\nmust_use = set(for_use_vars) - set(zeros)\nprint('Уже более подходящее число: ', len(must_use))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\n\nsns.heatmap(X[X.columns[[list(X.columns).index(m) for m in must_use]]].corr(), annot = True, cmap = 'Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacking"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['target'], axis=1,)\ny = train.target.values\nX_sub = test.copy()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=VAL_SIZE, shuffle=True, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape, X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\ndef RMSE(y, y_pred):\n    \n    MSE = metrics.mean_squared_error(y, y_pred)\n    print(f'MSE = {MSE:.4f}')\n\n    RMSE = np.sqrt(MSE)\n    print(f'RMSE = {RMSE:.4f}')\n\n    print('-------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RidgeCV\nfrom sklearn.svm import LinearSVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom catboost import Pool, CatBoostRegressor, cv\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression\nimport lightgbm as lgb\n\nxgb_params = {\n    'colsample_bytree':0.4603,\n    'gamma':0.0468,\n    'learning_rate':0.05,\n    'max_depth':3,\n    'min_child_weight':1.7817,\n    'n_estimators':2200,\n    'reg_alpha':0.4640,\n    'reg_lambda':0.8571,\n    'subsample':0.5213,\n    'nthread':-1,\n    'eval_metric':'rmse',\n    'seed':RANDOM_SEED,\n#     'tree_method':'gpu_hist',\n#     'gpu_id':0\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBRegressor(**xgb_params)\nrfr = RandomForestRegressor(n_estimators=100, max_depth=6, n_jobs=-1)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\nlr = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle(y, y_pred):\n    return np.sqrt(metrics.mean_squared_error(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_averaged_models = StackingAveragedModels(base_models = (xgb, rfr),\n                                                 meta_model = lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_averaged_models.fit(X_train.values, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_train_pred = stacked_averaged_models.predict(X_train.values)\nprint(rmsle(y_train, stacked_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_pred = stacked_averaged_models.predict(X_sub.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VERSION = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub = pd.read_csv('../input/tabular-playground-series-feb-2021/sample_submission.csv')\ndf_sub['target'] = stacked_pred\ndf_sub.to_csv(f'submission_stacking_v{VERSION}.csv', index=False)\ndf_sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.fit(X_train, y_train)\nxgb_train_pred = xgb.predict(X_train)\nxgb_pred = np.expm1(xgb.predict(X_test))\nprint(rmsle(y_train, xgb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_pred_stacked = xgb.predict(X_sub)\nxgb_pred_stacked","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VERSION = 7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub = pd.read_csv('../input/tabular-playground-series-feb-2021/sample_submission.csv')\ndf_sub['target'] = xgb_pred_stacked\ndf_sub.to_csv(f'submission_stacking_v{VERSION}.csv', index=False)\ndf_sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Averages"},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble = stacked_pred*0.2 + predict_submission_xgb*0.2 + predict_submission_ctb*0.3 + y_preds_lgb.values*0.3\n\ndf_sub = pd.read_csv('../input/tabular-playground-series-feb-2021/sample_submission.csv')\ndf_sub['target'] = ensemble\ndf_sub.to_csv(f'submission_average.csv', index=False)\ndf_sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(predict_submission_ctb))\nprint(len(predict_submission_xgb))\nprint(len(y_preds_lgb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_predict_xgb = pd.read_csv('./submission_XGBoost_v3.csv')\nsub_predict_catboost = pd.read_csv('./submission_CatBoost_v3.csv')\nsub_predict_lgbm = pd.read_csv('./submission_LightGBM_8.csv')\n\ndf_sub = pd.read_csv('../input/tabular-playground-series-feb-2021/sample_submission.csv')\nblend_sub_predict = (sub_predict_xgb['target'] + sub_predict_catboost['target'] + sub_predict_lgbm['target']) / 3\ndf_sub['target'] = blend_sub_predict\ndf_sub.to_csv(f'submission_average.csv', index=False)\ndf_sub.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}