{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook is a copy of [Tabular Playground Series Jan 2021 - Models](https://www.kaggle.com/gunesevitan/tabular-playground-series-jan-2021-models) by [@gunesevitan](https://www.kaggle.com/gunesevitan) adapted for [February Tabular Playground Series](https://www.kaggle.com/c/tabular-playground-series-feb-2021). Will update with Neural Networks Section in the future"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-01-31T19:05:26.286805Z","iopub.status.busy":"2021-01-31T19:05:26.286035Z","iopub.status.idle":"2021-01-31T19:05:34.992127Z","shell.execute_reply":"2021-01-31T19:05:34.991502Z"},"papermill":{"duration":8.746038,"end_time":"2021-01-31T19:05:34.992295","exception":false,"start_time":"2021-01-31T19:05:26.246257","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport random\nimport os\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Installing XGBoost 1.3+ which provides gpu speedup"},{"metadata":{},"cell_type":"markdown","source":"# Setting up Libraries"},{"metadata":{},"cell_type":"markdown","source":"***Installing XGBoost 1.3 version***"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install --upgrade xgboost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Re-compile LGBM with GPU support***\n\nhttps://www.kaggle.com/dromosys/gpu-accelerated-lightgbm-full/notebook"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!rm -r /opt/conda/lib/python3.6/site-packages/lightgbm\n!git clone --recursive https://github.com/Microsoft/LightGBM","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!apt-get install -y -qq libboost-all-dev","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..\nmake -j$(nproc)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!cd LightGBM/python-package/;python3 setup.py install --precompile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.024352,"end_time":"2021-01-31T19:05:35.042282","exception":false,"start_time":"2021-01-31T19:05:35.01793","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Tabular Playground Series - Jan 2021"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-01-31T19:05:26.286805Z","iopub.status.busy":"2021-01-31T19:05:26.286035Z","iopub.status.idle":"2021-01-31T19:05:34.992127Z","shell.execute_reply":"2021-01-31T19:05:34.991502Z"},"papermill":{"duration":8.746038,"end_time":"2021-01-31T19:05:34.992295","exception":false,"start_time":"2021-01-31T19:05:26.246257","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', 50)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import probplot, kurtosis, skew, gmean\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.ensemble import RandomTreesEmbedding\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2021-01-31T19:05:35.108Z","iopub.status.busy":"2021-01-31T19:05:35.107278Z","iopub.status.idle":"2021-01-31T19:05:38.608752Z","shell.execute_reply":"2021-01-31T19:05:38.607913Z"},"papermill":{"duration":3.541792,"end_time":"2021-01-31T19:05:38.60889","exception":false,"start_time":"2021-01-31T19:05:35.067098","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/tabular-playground-series-feb-2021/train.csv')\ndf_test = pd.read_csv('../input/tabular-playground-series-feb-2021/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2021-01-31T19:05:35.108Z","iopub.status.busy":"2021-01-31T19:05:35.107278Z","iopub.status.idle":"2021-01-31T19:05:38.608752Z","shell.execute_reply":"2021-01-31T19:05:38.607913Z"},"papermill":{"duration":3.541792,"end_time":"2021-01-31T19:05:38.60889","exception":false,"start_time":"2021-01-31T19:05:35.067098","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"continuous_features = [feature for feature in df_train.columns if feature.startswith('cont')]\ncategorical_features = [feature for feature in df_train.columns if feature.startswith('cat')]\ntarget = 'target'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2021-01-31T19:05:35.108Z","iopub.status.busy":"2021-01-31T19:05:35.107278Z","iopub.status.idle":"2021-01-31T19:05:38.608752Z","shell.execute_reply":"2021-01-31T19:05:38.607913Z"},"papermill":{"duration":3.541792,"end_time":"2021-01-31T19:05:38.60889","exception":false,"start_time":"2021-01-31T19:05:35.067098","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"print(f'Training Set Shape = {df_train.shape}')\nprint(f'Training Set Memory Usage = {df_train.memory_usage().sum() / 1024 ** 2:.2f} MB')\nprint(f'Test Set Shape = {df_test.shape}')\nprint(f'Test Set Memory Usage = {df_test.memory_usage().sum() / 1024 ** 2:.2f} MB')","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.02508,"end_time":"2021-01-31T19:05:38.659662","exception":false,"start_time":"2021-01-31T19:05:38.634582","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## 1. Target\n\n`target` is the name of target feature. It follows an extremely left tailed bimodal distribution. Target mean and median are very close to each other because there are very few outliers which can be seen from the probability plot. Those two outliers are 0 and 3.7, and they should be dealt with.\n\nBimodal distribution can be break into two components with gaussian mixture model, but it is not possible to predict components of test set."},{"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-01-31T19:05:38.7371Z","iopub.status.busy":"2021-01-31T19:05:38.735921Z","iopub.status.idle":"2021-01-31T19:05:42.222898Z","shell.execute_reply":"2021-01-31T19:05:42.223456Z"},"papermill":{"duration":3.538571,"end_time":"2021-01-31T19:05:42.223597","exception":false,"start_time":"2021-01-31T19:05:38.685026","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def plot_target(target):\n    \n    print(f'Target feature {target} Statistical Analysis\\n{\"-\" * 42}')\n        \n    print(f'Mean: {df_train[target].mean():.4}  -  Median: {df_train[target].median():.4}  -  Std: {df_train[target].std():.4}')\n    print(f'Min: {df_train[target].min():.4}  -  25%: {df_train[target].quantile(0.25):.4}  -  50%: {df_train[target].quantile(0.5):.4}  -  75%: {df_train[target].quantile(0.75):.4}  -  Max: {df_train[target].max():.4}')\n    print(f'Skew: {df_train[target].skew():.4}  -  Kurtosis: {df_train[target].kurtosis():.4}')\n    missing_values_count = df_train[df_train[target].isnull()].shape[0]\n    training_samples_count = df_train.shape[0]\n    print(f'Missing Values: {missing_values_count}/{training_samples_count} ({missing_values_count * 100 / training_samples_count:.4}%)')\n\n    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(24, 12), dpi=100)\n\n    sns.distplot(df_train[target], label=target, ax=axes[0][0])\n    axes[0][0].axvline(df_train[target].mean(), label='Target Mean', color='r', linewidth=2, linestyle='--')\n    axes[0][0].axvline(df_train[target].median(), label='Target Median', color='b', linewidth=2, linestyle='--')\n    probplot(df_train[target], plot=axes[0][1])\n    \n    gmm = GaussianMixture(n_components=2, random_state=42)\n    gmm.fit(df_train[target].values.reshape(-1, 1))\n    df_train[f'{target}_class'] = gmm.predict(df_train[target].values.reshape(-1, 1))\n    \n    sns.distplot(df_train[target], label=target, ax=axes[1][0])\n    sns.distplot(df_train[df_train[f'{target}_class'] == 0][target], label='Component 1', ax=axes[1][1])\n    sns.distplot(df_train[df_train[f'{target}_class'] == 1][target], label='Component 2', ax=axes[1][1])\n    \n    axes[0][0].legend(prop={'size': 15})\n    axes[1][1].legend(prop={'size': 15})\n    \n    for i in range(2):\n        for j in range(2):\n            axes[i][j].tick_params(axis='x', labelsize=12)\n            axes[i][j].tick_params(axis='y', labelsize=12)\n            axes[i][j].set_xlabel('')\n            axes[i][j].set_ylabel('')\n    axes[0][0].set_title(f'{target} Distribution in Training Set', fontsize=15, pad=12)\n    axes[0][1].set_title(f'{target} Probability Plot', fontsize=15, pad=12)\n    axes[1][0].set_title(f'{target} Distribution Before GMM', fontsize=15, pad=12)\n    axes[1][1].set_title(f'{target} Distribution After GMM', fontsize=15, pad=12)\n    plt.show()\n    \n\nplot_target(target)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.028486,"end_time":"2021-01-31T19:05:42.281263","exception":false,"start_time":"2021-01-31T19:05:42.252777","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## 2. Features\n\n### 2.1 Continous Features\nThere are 14 continuous features that are named from `cont1` to `cont14`. All the continous features have multimodal distributions which means they have multiple peaks. Number of peaks changes from feature to feature. None of the features have any missing values and their distributions are very similar in training and test sets."},{"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-01-31T19:05:42.359908Z","iopub.status.busy":"2021-01-31T19:05:42.359141Z","iopub.status.idle":"2021-01-31T19:06:11.589552Z","shell.execute_reply":"2021-01-31T19:06:11.590101Z"},"papermill":{"duration":29.280373,"end_time":"2021-01-31T19:06:11.590256","exception":false,"start_time":"2021-01-31T19:05:42.309883","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def plot_continuous(continuous_feature):\n            \n    print(f'Continuous feature {continuous_feature} Statistical Analysis\\n{\"-\" * 42}')\n\n    print(f'Training Mean: {float(df_train[continuous_feature].mean()):.4}  - Training Median: {float(df_train[continuous_feature].median()):.4} - Training Std: {float(df_train[continuous_feature].std()):.4}')\n    print(f'Test Mean: {float(df_test[continuous_feature].mean()):.4}  - Test Median: {float(df_test[continuous_feature].median()):.4} - Test Std: {float(df_test[continuous_feature].std()):.4}')\n    print(f'Training Min: {float(df_train[continuous_feature].min()):.4}  - Training Max: {float(df_train[continuous_feature].max()):.4}')\n    print(f'Test Min: {float(df_test[continuous_feature].min()):.4}  - Training Max: {float(df_test[continuous_feature].max()):.4}')\n    print(f'Training Skew: {float(df_train[continuous_feature].skew()):.4}  - Training Kurtosis: {float(df_train[continuous_feature].kurtosis()):.4}')\n    print(f'Test Skew: {float(df_test[continuous_feature].skew()):.4}  - Test Kurtosis: {float(df_test[continuous_feature].kurtosis()):.4}')\n    training_missing_values_count = df_train[df_train[continuous_feature].isnull()].shape[0]\n    test_missing_values_count = df_test[df_test[continuous_feature].isnull()].shape[0]\n    training_samples_count = df_train.shape[0]\n    test_samples_count = df_test.shape[0]\n    print(f'Training Missing Values: {training_missing_values_count}/{training_samples_count} ({training_missing_values_count * 100 / training_samples_count:.4}%)')\n    print(f'Test Missing Values: {test_missing_values_count}/{test_samples_count} ({test_missing_values_count * 100 / test_samples_count:.4}%)')\n\n    fig, axes = plt.subplots(ncols=2, figsize=(24, 6), dpi=100, constrained_layout=True)\n    title_size = 18\n    label_size = 18\n\n    # Continuous Feature Training and Test Set Distribution\n    sns.distplot(df_train[continuous_feature], label='Training', ax=axes[0])\n    sns.distplot(df_test[continuous_feature], label='Test', ax=axes[0])\n    axes[0].set_xlabel('')\n    axes[0].tick_params(axis='x', labelsize=label_size)\n    axes[0].tick_params(axis='y', labelsize=label_size)\n    axes[0].legend()\n    axes[0].set_title(f'{continuous_feature} Distribution in Training and Test Set', size=title_size, pad=title_size)\n    \n    # Continuous Feature vs target\n    sns.scatterplot(df_train[continuous_feature], df_train[target], ax=axes[1])\n    axes[1].set_title(f'{continuous_feature} vs {target}', size=title_size, pad=title_size)\n    axes[1].set_xlabel('')\n    axes[1].set_ylabel('')\n    axes[1].tick_params(axis='x', labelsize=label_size)\n    axes[1].tick_params(axis='y', labelsize=label_size)\n    \n    plt.show()\n    \n    \nfor continuous_feature in sorted(continuous_features, key=lambda x: int(x.split('cont')[-1])):\n    plot_continuous(continuous_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Categorical Features\nThere are 10 continuous features that are named from `cat1` to `cat10`. `cat0`, `cat1` & `cat2` are binary in nature, rest all have multiple categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = df_train[categorical_features+continuous_features].append(df_test[categorical_features+continuous_features])\nall_data[\"set\"] = [\"Train\"]*len(df_train)+[\"Test\"]*len(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_categorical(categorical_feature):\n    print(f'Categorical feature {categorical_feature} Statistical Analysis\\n{\"-\" * 42}')\n    training_missing_values_count = df_train[df_train[categorical_feature].isnull()].shape[0]\n    test_missing_values_count = df_test[df_test[categorical_feature].isnull()].shape[0]\n    training_samples_count = df_train.shape[0]\n    test_samples_count = df_test.shape[0]\n    print(f'Training Missing Values: {training_missing_values_count}/{training_samples_count} ({training_missing_values_count * 100 / training_samples_count:.4}%)')\n    print(f'Test Missing Values: {test_missing_values_count}/{test_samples_count} ({test_missing_values_count * 100 / test_samples_count:.4}%)')\n\n    title_size = 18\n    label_size = 18\n    \n    # Continuous Feature Training and Test Set Distribution\n    sns.countplot(all_data[categorical_feature], hue=all_data['set'])\n    plt.xlabel(categorical_feature)\n    plt.ylabel(\"count\")\n    plt.legend()\n    plt.title(f'{categorical_feature}', size=title_size, pad=title_size)\n    plt.show()\n    \nfor categorical_feature in sorted(categorical_features, key=lambda x: int(x.split('cat')[-1])):\n    plot_categorical(categorical_feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.188926,"end_time":"2021-01-31T19:06:11.964298","exception":false,"start_time":"2021-01-31T19:06:11.775372","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## 3. Preprocessing\n\n`Preprocessor` class incorporates the preprocessing steps such as cross-validation folds creation, outlier removal, standardization and feature engineering.\n\n* Created 5 random split folds for cross-validation\n* Continuous features are standardized for linear and neural network models, but it is not necessary for tree-based models\n* `argmax` and `argmin` features are created for continuous features\n* Continuous features are discretized with distribution components extracted by a gaussian mixture model "},{"metadata":{"execution":{"iopub.execute_input":"2021-01-31T19:06:12.382722Z","iopub.status.busy":"2021-01-31T19:06:12.381975Z","iopub.status.idle":"2021-01-31T19:06:12.384921Z","shell.execute_reply":"2021-01-31T19:06:12.384344Z"},"papermill":{"duration":0.231821,"end_time":"2021-01-31T19:06:12.385032","exception":false,"start_time":"2021-01-31T19:06:12.153211","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class Preprocessor:\n    \n    def __init__(self, train, test, n_splits, shuffle, random_state, scaler, discretize_features, create_features, encoder):\n        self.train = train.copy(deep=True)        \n        self.test = test.copy(deep=True)   \n        \n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n        self.scaler = scaler() if scaler else None\n        self.discretize_features = discretize_features\n        self.create_features = create_features\n        self.encoder = encoder\n        \n    def create_folds(self):    \n        kf = KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n        for fold, (_, val_idx) in enumerate(kf.split(self.train), 1):\n            self.train.loc[val_idx, 'fold'] = fold\n        self.train['fold'] = self.train['fold'].astype(np.uint8)\n\n    def scale(self):\n        df_all = pd.concat([self.train[continuous_features], self.test[continuous_features]], ignore_index=True, axis=0)\n        self.scaler.fit(df_all[continuous_features])\n        self.train.loc[:, continuous_features] = self.scaler.transform(self.train.loc[:, continuous_features].values)\n        self.test.loc[:, continuous_features] = self.scaler.transform(self.test.loc[:, continuous_features].values)\n        \n        print(f'Scaled {len(continuous_features)} features with {self.scaler.__class__.__name__}')\n        del df_all\n        \n    def create_idx_features(self):\n        for df in [self.train, self.test]:\n            df['cont_argmin'] = np.argmin(df[continuous_features].values, axis=1)\n            df['cont_argmax'] = np.argmax(df[continuous_features].values, axis=1)\n            \n        idx_features = ['cont_argmin', 'cont_argmax']\n        print(f'Created {len(idx_features)} idx features with argmin and argmax')\n    \n    def encode(self):\n        for e in categorical_features:\n            le = LabelEncoder()\n            self.train[e] = le.fit_transform(self.train[e])\n            self.test[e] = le.transform(self.test[e])\n    \n    def create_gmm_features(self):\n        n_component_mapping = {\n            0: 4,\n            1: 8,\n            2: 5,\n            3: 4,\n            4: 3,\n            5: 5,\n            6: 3,\n            7: 4,\n            8: 5,\n            9: 4,\n            10: 5,\n            11: 2,\n            12: 3,\n            13: 7\n        }\n        \n        for i in range(14):\n            gmm = GaussianMixture(n_components=n_component_mapping[i], random_state=self.random_state)            \n            gmm.fit(pd.concat([self.train[f'cont{i}'], self.test[f'cont{i}']], axis=0).values.reshape(-1, 1))\n            \n            self.train[f'cont{i}_class'] = gmm.predict(self.train[f'cont{i}'].values.reshape(-1, 1))\n            self.test[f'cont{i}_class'] = gmm.predict(self.test[f'cont{i}'].values.reshape(-1, 1))\n            \n        gmm_features = [f'cont{i}_class' for i in range(1, 15)]\n        print(f'Created {len(gmm_features)} gmm features with GaussianMixture')\n                \n    def transform(self):\n        self.create_folds()\n        if self.create_features:\n            self.create_idx_features()\n        if self.discretize_features:\n            self.create_gmm_features()\n        if self.scaler:\n            self.scale()\n        if self.encoder:\n            self.encode()\n\n        return self.train.copy(deep=True), self.test.copy(deep=True)\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.18944,"end_time":"2021-01-31T19:06:12.763005","exception":false,"start_time":"2021-01-31T19:06:12.573565","status":"completed"},"tags":[]},"cell_type":"markdown","source":"This is the example usage of preprocessing pipeline. It returns processed training and test sets ready for model training, but those dataframes created in this cell are only used for storing model predictions. Models are trained with their custom datasets that designed specifically for them."},{"metadata":{"execution":{"iopub.execute_input":"2021-01-31T19:06:13.149392Z","iopub.status.busy":"2021-01-31T19:06:13.148717Z","iopub.status.idle":"2021-01-31T19:06:13.32279Z","shell.execute_reply":"2021-01-31T19:06:13.3222Z"},"papermill":{"duration":0.369097,"end_time":"2021-01-31T19:06:13.322907","exception":false,"start_time":"2021-01-31T19:06:12.95381","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"cross_validation_seed = 0\npreprocessor = Preprocessor(train=df_train, test=df_test,\n                            n_splits=5, shuffle=True, random_state=cross_validation_seed,\n                            scaler=None,\n                            create_features=True, discretize_features=True, encoder=True)\ndf_train_processed, df_test_processed = preprocessor.transform()\n\nprint(f'\\nPreprocessed Training Set Shape = {df_train_processed.shape}')\nprint(f'Preprocessed Training Set Memory Usage = {df_train_processed.memory_usage().sum() / 1024 ** 2:.2f} MB')\nprint(f'Preprocessed Test Set Shape = {df_test_processed.shape}')\nprint(f'Preprocessed Test Set Memory Usage = {df_test_processed.memory_usage().sum() / 1024 ** 2:.2f} MB\\n')","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.186792,"end_time":"2021-01-31T19:06:13.698621","exception":false,"start_time":"2021-01-31T19:06:13.511829","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## 4. Tree-based Models\n\n4 tree-based models are used in the ensemble and they are LightGBM, CatBoost, XGBoost, and Random Forest. All of them are trained with more than 3 seeds for diversity. Only raw continuous features are used in tree-based models without any transformation. Even though the training logs are not present in this version of the notebook, those are the parameters used for getting the scores displayed below each cell."},{"metadata":{"execution":{"iopub.execute_input":"2021-01-31T19:06:14.130859Z","iopub.status.busy":"2021-01-31T19:06:14.118281Z","iopub.status.idle":"2021-01-31T19:06:14.163142Z","shell.execute_reply":"2021-01-31T19:06:14.163684Z"},"papermill":{"duration":0.275947,"end_time":"2021-01-31T19:06:14.16387","exception":false,"start_time":"2021-01-31T19:06:13.887923","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class TreeModels:\n    \n    def __init__(self, predictors, target, model, model_parameters, boosting_rounds, early_stopping_rounds, seeds):\n        \n        self.predictors = predictors\n        self.target = target\n               \n        self.model = model\n        self.model_parameters = model_parameters\n        self.boosting_rounds = boosting_rounds\n        self.early_stopping_rounds = early_stopping_rounds\n        self.seeds = seeds\n                \n    def _train_and_predict_lgb(self, X_train, y_train, X_test):\n        \n        seed_avg_oof_predictions = np.zeros(X_train.shape[0])\n        seed_avg_test_predictions = np.zeros(X_test.shape[0])        \n        seed_avg_importance = pd.DataFrame(data=np.zeros(len(self.predictors)), index=self.predictors, columns=['Importance'])\n        \n        for seed in self.seeds:\n            print(f'{\"-\" * 30}\\nRunning LightGBM model with seed: {seed}\\n{\"-\" * 30}\\n')\n            self.model_parameters['seed'] = seed\n            self.model_parameters['feature_fraction_seed'] = seed\n            self.model_parameters['bagging_seed'] = seed\n            self.model_parameters['drop_seed'] = seed\n            self.model_parameters['data_random_seed'] = seed\n                \n            for fold in sorted(X_train['fold'].unique()):\n\n                trn_idx, val_idx = X_train.loc[X_train['fold'] != fold].index, X_train.loc[X_train['fold'] == fold].index\n                trn = lgb.Dataset(X_train.loc[trn_idx, self.predictors], label=y_train.loc[trn_idx])\n                val = lgb.Dataset(X_train.loc[val_idx, self.predictors], label=y_train.loc[val_idx])\n\n                model = lgb.train(params=self.model_parameters,\n                                  train_set=trn,\n                                  valid_sets=[trn, val],\n                                  num_boost_round=self.boosting_rounds,\n                                  early_stopping_rounds=self.early_stopping_rounds,\n                                  verbose_eval=500)            \n\n                val_predictions = model.predict(X_train.loc[val_idx, self.predictors])\n                seed_avg_oof_predictions[val_idx] += (val_predictions / len(self.seeds))\n                test_predictions = model.predict(X_test[self.predictors])\n                seed_avg_test_predictions += (test_predictions / X_train['fold'].nunique() / len(self.seeds))\n                seed_avg_importance['Importance'] += (model.feature_importance(importance_type='gain') / X_train['fold'].nunique() / len(self.seeds))\n\n                fold_score = mean_squared_error(y_train.loc[val_idx], val_predictions, squared=False)\n                print(f'\\nLGB Fold {int(fold)} - X_trn: {X_train.loc[trn_idx, self.predictors].shape} X_val: {X_train.loc[val_idx, self.predictors].shape} - Score: {fold_score:.6} - Seed: {seed}\\n')\n            \n        df_train_processed['LGBPredictions'] = seed_avg_oof_predictions\n        df_test_processed['LGBPredictions'] = seed_avg_test_predictions\n        oof_score = mean_squared_error(y_train, df_train_processed['LGBPredictions'], squared=False)\n        print(f'{\"-\" * 30}\\nLGB OOF RMSE: {oof_score:.6} ({len(self.seeds)} Seed Average)\\n{\"-\" * 30}')\n                \n        self._plot_importance(seed_avg_importance)\n        self._plot_predictions(df_train_processed[target], df_train_processed['LGBPredictions'], df_test_processed['LGBPredictions'])\n        \n    def _train_and_predict_cb(self, X_train, y_train, X_test):\n        \n        seed_avg_oof_predictions = np.zeros(X_train.shape[0])\n        seed_avg_test_predictions = np.zeros(X_test.shape[0])        \n        seed_avg_importance = pd.DataFrame(data=np.zeros(len(self.predictors)), index=self.predictors, columns=['Importance'])\n            \n        for seed in self.seeds:\n            print(f'{\"-\" * 30}\\nRunning CatBoost model with seed: {seed}\\n{\"-\" * 30}\\n')\n            self.model_parameters['random_seed'] = seed\n            \n            for fold in sorted(X_train['fold'].unique()):\n\n                trn_idx, val_idx = X_train.loc[X_train['fold'] != fold].index, X_train.loc[X_train['fold'] == fold].index\n                trn = cb.Pool(X_train.loc[trn_idx, self.predictors], label=y_train.loc[trn_idx])\n                val = cb.Pool(X_train.loc[val_idx, self.predictors], label=y_train.loc[val_idx])\n\n                model = cb.CatBoostRegressor(**self.model_parameters)\n                model.fit(X=trn, eval_set=val)\n\n                val_predictions = model.predict(val)\n                seed_avg_oof_predictions[val_idx] += (val_predictions / len(self.seeds))\n                test_predictions = model.predict(cb.Pool(X_test[self.predictors]))\n                seed_avg_test_predictions += (test_predictions / X_train['fold'].nunique() / len(self.seeds))\n                seed_avg_importance['Importance'] += (model.get_feature_importance() / X_train['fold'].nunique() / len(self.seeds))\n\n                fold_score = mean_squared_error(df_train_processed.loc[val_idx, self.target], val_predictions, squared=False)\n                print(f'\\nCB Fold {int(fold)} - X_trn: {X_train.loc[trn_idx, self.predictors].shape} X_val: {X_train.loc[val_idx, self.predictors].shape} - Score: {fold_score:.6} - Seed: {seed}\\n')\n            \n        df_train_processed['CBPredictions'] = seed_avg_oof_predictions\n        df_test_processed['CBPredictions'] = seed_avg_test_predictions\n        oof_score = mean_squared_error(y_train, df_train_processed['CBPredictions'], squared=False)\n        print(f'{\"-\" * 30}\\nCB OOF RMSE: {oof_score:.6} ({len(self.seeds)} Seed Average)\\n{\"-\" * 30}')\n                \n        self._plot_importance(seed_avg_importance)\n        self._plot_predictions(df_train_processed[target], df_train_processed['CBPredictions'], df_test_processed['CBPredictions'])\n        \n    def _train_and_predict_xgb(self, X_train, y_train, X_test):\n        \n        seed_avg_oof_predictions = np.zeros(X_train.shape[0])\n        seed_avg_test_predictions = np.zeros(X_test.shape[0])\n        seed_avg_importance = pd.DataFrame(data=np.zeros(len(self.predictors)), index=self.predictors, columns=['Importance'])\n        \n        for seed in self.seeds:\n            print(f'{\"-\" * 30}\\nRunning XGBoost model with seed: {seed}\\n{\"-\" * 30}\\n')\n            self.model_parameters['seed'] = seed\n        \n            for fold in sorted(X_train['fold'].unique()):\n\n                trn_idx, val_idx = X_train.loc[X_train['fold'] != fold].index, X_train.loc[X_train['fold'] == fold].index\n                trn = xgb.DMatrix(X_train.loc[trn_idx, self.predictors], label=y_train.loc[trn_idx])\n                val = xgb.DMatrix(X_train.loc[val_idx, self.predictors], label=y_train.loc[val_idx])\n\n                model = xgb.train(params=self.model_parameters,\n                                  dtrain=trn,\n                                  evals=[(trn, 'train'), (val, 'val')],\n                                  num_boost_round=self.boosting_rounds, \n                                  early_stopping_rounds=self.early_stopping_rounds,\n                                  verbose_eval=500) \n\n                val_predictions = model.predict(xgb.DMatrix(X_train.loc[val_idx, self.predictors]))\n                seed_avg_oof_predictions[val_idx] += (val_predictions / len(self.seeds))\n                test_predictions = model.predict(xgb.DMatrix(X_test[self.predictors]))\n                seed_avg_test_predictions += (test_predictions / X_train['fold'].nunique() / len(self.seeds))\n                seed_avg_importance['Importance'] += (np.array(list(model.get_score(importance_type='gain').values())) / X_train['fold'].nunique() / len(self.seeds))\n\n                fold_score = mean_squared_error(df_train_processed.loc[val_idx, self.target], val_predictions, squared=False)\n                print(f'\\nXGB Fold {int(fold)} - X_trn: {X_train.loc[trn_idx, self.predictors].shape} X_val: {X_train.loc[val_idx, self.predictors].shape} - Score: {fold_score:.6} - Seed: {seed}\\n')\n            \n        df_train_processed['XGBPredictions'] = seed_avg_oof_predictions\n        df_test_processed['XGBPredictions'] = seed_avg_test_predictions\n        oof_score = mean_squared_error(y_train, df_train_processed['XGBPredictions'], squared=False)\n        print(f'{\"-\" * 30}\\nXGB OOF RMSE: {oof_score:.6} ({len(self.seeds)} Seed Average) \\n{\"-\" * 30}')\n                \n        self._plot_importance(seed_avg_importance)\n        self._plot_predictions(df_train_processed[target], df_train_processed['XGBPredictions'], df_test_processed['XGBPredictions'])\n        \n    def _train_and_predict_rf(self, X_train, y_train, X_test):\n        \n        seed_avg_oof_predictions = np.zeros(X_train.shape[0])\n        seed_avg_test_predictions = np.zeros(X_test.shape[0])\n        \n        for seed in self.seeds:\n            print(f'{\"-\" * 30}\\nRunning RandomForest model with seed: {seed}\\n{\"-\" * 30}\\n')\n            self.model_parameters['random_state'] = seed\n                \n            for fold in sorted(X_train['fold'].unique()):\n\n                trn_idx, val_idx = X_train.loc[X_train['fold'] != fold].index, X_train.loc[X_train['fold'] == fold].index\n                X_trn, y_trn = X_train.loc[trn_idx, self.predictors].astype(np.float32), y_train.loc[trn_idx].astype(np.float32)\n                X_val, y_val = X_train.loc[val_idx, self.predictors].astype(np.float32), y_train.loc[val_idx].astype(np.float32)\n\n                import cuml\n                model = cuml.ensemble.RandomForestRegressor(**self.model_parameters)\n                model.fit(X_trn, y_trn)\n\n                val_predictions = model.predict(X_val)\n                seed_avg_oof_predictions[val_idx] += (val_predictions / len(self.seeds))\n                test_predictions = model.predict(X_test[self.predictors])\n                seed_avg_test_predictions += (test_predictions / X_train['fold'].nunique() / len(self.seeds))\n\n                fold_score = mean_squared_error(df_train_processed.loc[val_idx, self.target], val_predictions, squared=False)\n                print(f'RF Fold {int(fold)} - X_trn: {X_train.loc[trn_idx, self.predictors].shape} X_val: {X_train.loc[val_idx, self.predictors].shape} - Score: {fold_score:.6}')\n\n        df_train_processed['RFPredictions'] = seed_avg_oof_predictions\n        df_test_processed['RFPredictions'] = seed_avg_test_predictions\n        oof_score = mean_squared_error(y_train, df_train_processed['RFPredictions'], squared=False)\n        print(f'{\"-\" * 30}\\nRF OOF RMSE: {oof_score:.6} ({len(self.seeds)} Seed Average) \\n{\"-\" * 30}')\n        \n        self._plot_predictions(df_train_processed[target], df_train_processed['RFPredictions'], df_test_processed['RFPredictions'])\n        \n    def _plot_importance(self, df_importance):\n        \n        df_importance.sort_values(by='Importance', inplace=True, ascending=False)\n        \n        plt.figure(figsize=(25, 6))       \n        sns.barplot(x='Importance', y=df_importance.index, data=df_importance, palette='Blues_d')\n        plt.xlabel('')\n        plt.tick_params(axis='x', labelsize=20)\n        plt.tick_params(axis='y', labelsize=20)\n        plt.title(f'{self.model} Feature Importance (Gain)', size=20, pad=20)\n        plt.show()\n        \n    def _plot_predictions(self, train_labels, train_predictions, test_predictions):\n        \n        fig, axes = plt.subplots(ncols=2, figsize=(25, 6))                                            \n        sns.scatterplot(train_labels, train_predictions, ax=axes[0])\n        sns.distplot(train_predictions, label='Train Predictions', ax=axes[1])\n        sns.distplot(test_predictions, label='Test Predictions', ax=axes[1])\n\n        axes[0].set_xlabel(f'Train Labels', size=18)\n        axes[0].set_ylabel(f'Train Predictions', size=18)\n        axes[1].set_xlabel('')\n        axes[1].legend(prop={'size': 18})\n        for i in range(2):\n            axes[i].tick_params(axis='x', labelsize=15)\n            axes[i].tick_params(axis='y', labelsize=15)\n        axes[0].set_title(f'Train Labels vs Train Predictions', size=20, pad=20)\n        axes[1].set_title(f'Predictions Distributions', size=20, pad=20)\n            \n        plt.show() \n        \n    def run(self, X_train, y_train, X_test):\n        \n        if self.model == 'LGB':\n            self._train_and_predict_lgb(X_train, y_train, X_test)\n        elif self.model == 'CB':\n            self._train_and_predict_cb(X_train, y_train, X_test)\n        elif self.model == 'XGB':\n            self._train_and_predict_xgb(X_train, y_train, X_test)\n        elif self.model == 'RF':\n            self._train_and_predict_rf(X_train, y_train, X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.227504,"end_time":"2021-01-31T19:06:14.580821","exception":false,"start_time":"2021-01-31T19:06:14.353317","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### 4.1 LightGBM"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-31T19:06:14.995598Z","iopub.status.busy":"2021-01-31T19:06:14.994822Z","iopub.status.idle":"2021-01-31T19:06:16.873972Z","shell.execute_reply":"2021-01-31T19:06:16.873341Z"},"papermill":{"duration":2.102638,"end_time":"2021-01-31T19:06:16.874098","exception":false,"start_time":"2021-01-31T19:06:14.77146","status":"completed"},"tags":[],"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model = 'LGB'\nlgb_preprocessor = Preprocessor(train=df_train, test=df_test,\n                            n_splits=5, shuffle=True, random_state=cross_validation_seed, scaler=None,\n                            create_features=True, discretize_features=True, encoder=True)\ndf_train_lgb, df_test_lgb = lgb_preprocessor.transform()\n\nprint(f'\\n{model} Training Set Shape = {df_train_lgb.shape}')\nprint(f'{model} Training Set Memory Usage = {df_train_lgb.memory_usage().sum() / 1024 ** 2:.2f} MB')\nprint(f'{model} Test Set Shape = {df_test_lgb.shape}')\nprint(f'{model} Test Set Memory Usage = {df_test_lgb.memory_usage().sum() / 1024 ** 2:.2f} MB\\n')\n\nX_train_lgb = df_train_lgb.copy(deep=True)\ny_train_lgb = df_train_lgb[target].copy(deep=True)\nX_test_lgb = df_test_lgb.copy(deep=True)\n\nlgb_parameters = {\n'predictors': continuous_features,\n'target': target,\n'model': model,\n'model_parameters': {\n    # https://www.kaggle.com/hamzaghanmi/lgbm-hyperparameter-tuning-using-optuna\n    'reg_alpha': 6.147694913504962,\n    'reg_lambda': 0.002457826062076097,\n    'colsample_bytree': 0.3,\n    'subsample': 0.8,\n    'learning_rate': 0.008,\n    'max_depth': 20,\n    'num_leaves': 111,\n    'min_child_samples': 285,\n    'random_state': 48,\n    'n_estimators': 20000,\n    'metric': 'rmse',\n    'cat_smooth': 39,\n    'objective': 'regression',\n    'seed': None,\n    'feature_fraction_seed': None,\n    'bagging_seed': None,\n    'drop_seed': None,\n    'data_random_seed': None,\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'rmse',\n    'n_jobs': -1,\n    # gpu specific\n    'device': 'gpu',\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0\n},\n'boosting_rounds': 20000,\n'early_stopping_rounds': 200,\n'seeds': [541992, 721991, 1337]\n}\n\nlgb_model = TreeModels(**lgb_parameters)\nlgb_model.run(X_train_lgb, y_train_lgb, X_test_lgb)\n\ndel df_train_lgb, df_test_lgb, X_train_lgb, y_train_lgb, X_test_lgb\ndel lgb_preprocessor, lgb_parameters, lgb_model\n\nprint('Saving LightGBM OOF and Test predictions to current working directory.')\ndf_train_processed[['id', 'LGBPredictions']].to_csv('lgb_oof_predictions.csv', index=False)\ndf_test_processed[['id', 'LGBPredictions']].to_csv('lgb_test_predictions.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.28658,"end_time":"2021-01-31T19:06:17.356876","exception":false,"start_time":"2021-01-31T19:06:17.070296","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### 4.2 CatBoost"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-31T19:06:17.749273Z","iopub.status.busy":"2021-01-31T19:06:17.7482Z","iopub.status.idle":"2021-01-31T19:06:19.56493Z","shell.execute_reply":"2021-01-31T19:06:19.565473Z"},"papermill":{"duration":2.014715,"end_time":"2021-01-31T19:06:19.565624","exception":false,"start_time":"2021-01-31T19:06:17.550909","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"model = 'CB'\ncb_preprocessor = Preprocessor(train=df_train, test=df_test,\n                               n_splits=5, shuffle=True, random_state=cross_validation_seed, scaler=None,\n                               create_features=False, discretize_features=False, encoder=True)\ndf_train_cb, df_test_cb = cb_preprocessor.transform()\n\nprint(f'\\n{model} Training Set Shape = {df_train_cb.shape}')\nprint(f'{model} Training Set Memory Usage = {df_train_cb.memory_usage().sum() / 1024 ** 2:.2f} MB')\nprint(f'{model} Test Set Shape = {df_test_cb.shape}')\nprint(f'{model} Test Set Memory Usage = {df_test_cb.memory_usage().sum() / 1024 ** 2:.2f} MB\\n')\n\nX_train_cb = df_train_cb[continuous_features + ['fold']].copy(deep=True)\ny_train_cb = df_train_cb[target].copy(deep=True)\nX_test_cb = df_test_cb[continuous_features].copy(deep=True)\n\n# https://www.kaggle.com/ttahara/tps-feb-2021-3gbdts-ensemble-baseline\ncb_parameters = {\n    'predictors': continuous_features,\n    'target': target,\n    'model': model,\n    'model_parameters': {\n        'num_boost_round': 20000,\n        'task_type': \"GPU\",\n        'learning_rate': 0.05,\n        'depth': 4,\n        'subsample': 0.6,\n        'use_best_model': True,\n        'eval_metric': 'RMSE',\n        'loss_function': 'RMSE',   \n        'random_seed': None,\n        'verbose': 0\n    },\n    'boosting_rounds': None,\n    'early_stopping_rounds': None,\n    'seeds': [541992, 721991, 1337, 42, 0]\n}\n\ncb_model = TreeModels(**cb_parameters)\ncb_model.run(X_train_cb, y_train_cb, X_test_cb)\n\ndel df_train_cb, df_test_cb, X_train_cb, y_train_cb, X_test_cb\ndel cb_preprocessor, cb_parameters, cb_model\n\nprint('Saving CatBoost OOF and Test predictions to current working directory.')\ndf_train_processed[['id', 'CBPredictions']].to_csv('cb_oof_predictions.csv', index=False)\ndf_test_processed[['id', 'CBPredictions']].to_csv('cb_test_predictions.csv', index=False)\nTreeModels._plot_predictions(None, df_train_processed[target], df_train_processed['CBPredictions'], df_test_processed['CBPredictions'])\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.201003,"end_time":"2021-01-31T19:06:19.968775","exception":false,"start_time":"2021-01-31T19:06:19.767772","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### 4.3 XGBoost"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-31T19:06:20.396685Z","iopub.status.busy":"2021-01-31T19:06:20.381651Z","iopub.status.idle":"2021-01-31T19:06:22.192143Z","shell.execute_reply":"2021-01-31T19:06:22.191618Z"},"papermill":{"duration":2.022264,"end_time":"2021-01-31T19:06:22.19226","exception":false,"start_time":"2021-01-31T19:06:20.169996","status":"completed"},"tags":[],"trusted":false,"_kg_hide-output":true},"cell_type":"code","source":"model = 'XGB'\nxgb_preprocessor = Preprocessor(train=df_train, test=df_test,\n                                n_splits=5, shuffle=True, random_state=cross_validation_seed, scaler=None,\n                                create_features=False, discretize_features=False, encoder=True)\ndf_train_xgb, df_test_xgb = xgb_preprocessor.transform()\n\nprint(f'\\n{model} Training Set Shape = {df_train_xgb.shape}')\nprint(f'{model} Training Set Memory Usage = {df_train_xgb.memory_usage().sum() / 1024 ** 2:.2f} MB')\nprint(f'{model} Test Set Shape = {df_test_xgb.shape}')\nprint(f'{model} Test Set Memory Usage = {df_test_xgb.memory_usage().sum() / 1024 ** 2:.2f} MB\\n')\n\nX_train_xgb = df_train_xgb[continuous_features + ['fold']].copy(deep=True)\ny_train_xgb = df_train_xgb[target].copy(deep=True)\nX_test_xgb = df_test_xgb[continuous_features].copy(deep=True)\n\nxgb_parameters = {\n    'predictors': continuous_features,\n    'target': target,\n    'model': model,\n    # https://www.kaggle.com/tunguz/tps-02-21-feature-importance-with-xgboost-and-shap\n    'model_parameters': {\n        'learning_rate': 0.01,\n        'colsample_bytree': 0.4, \n        'sumbsample': 0.6,\n        'max_depth': 6,\n        'min_child_weight': 100,\n        'alpha': 6,\n        'objective': 'reg:squarederror',\n        'seed': None,\n        'boosting_type': 'gbtree',\n        'tree_method': 'gpu_hist',\n        'gpu_id': 0,\n        'silent': True,\n        'verbose': 1,\n        'n_jobs': -1,\n    },\n    'boosting_rounds': 25000,\n    'early_stopping_rounds': 200,\n    'seeds': [541992, 721991, 1337]\n}\n\nxgb_model = TreeModels(**xgb_parameters)\nxgb_model.run(X_train_xgb, y_train_xgb, X_test_xgb)\n\ndel df_train_xgb, df_test_xgb, X_train_xgb, y_train_xgb, X_test_xgb\ndel xgb_preprocessor, xgb_parameters, xgb_model\n\nprint('Saving XGBoost OOF and Test predictions to current working directory.')\ndf_train_processed[['id', 'XGBPredictions']].to_csv('xgb_oof_predictions.csv', index=False)\ndf_test_processed[['id', 'XGBPredictions']].to_csv('xgb_test_predictions.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.212754,"end_time":"2021-01-31T19:06:22.617938","exception":false,"start_time":"2021-01-31T19:06:22.405184","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### 4.4 Random Forest"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-31T19:06:23.06039Z","iopub.status.busy":"2021-01-31T19:06:23.059666Z","iopub.status.idle":"2021-01-31T19:06:24.914837Z","shell.execute_reply":"2021-01-31T19:06:24.915351Z"},"papermill":{"duration":2.087326,"end_time":"2021-01-31T19:06:24.915509","exception":false,"start_time":"2021-01-31T19:06:22.828183","status":"completed"},"tags":[],"trusted":false,"_kg_hide-output":true},"cell_type":"code","source":"model = 'RF'\nrf_preprocessor = Preprocessor(train=df_train, test=df_test,\n                               n_splits=5, shuffle=True, random_state=cross_validation_seed, scaler=None,\n                               create_features=False, discretize_features=False, encoder=True)\ndf_train_rf, df_test_rf = rf_preprocessor.transform()\n\nprint(f'\\n{model} Training Set Shape = {df_train_rf.shape}')\nprint(f'{model} Training Set Memory Usage = {df_train_rf.memory_usage().sum() / 1024 ** 2:.2f} MB')\nprint(f'{model} Test Set Shape = {df_test_rf.shape}')\nprint(f'{model} Test Set Memory Usage = {df_test_rf.memory_usage().sum() / 1024 ** 2:.2f} MB\\n')\n\nX_train_rf = df_train_rf[continuous_features + ['fold']].copy(deep=True)\ny_train_rf = df_train_rf[target].copy(deep=True)\nX_test_rf = df_test_rf[continuous_features].copy(deep=True)\n\n# this is still from the previous competition\nrf_parameters = {\n    'predictors': continuous_features,\n    'target': target,\n    'model': model,\n    'model_parameters': {\n        'n_estimators': 400,\n        'split_algo': 0,\n        'split_criterion': 2,             \n        'bootstrap': True,\n        'bootstrap_features': False,\n        'max_depth': 13,\n        'max_leaves': -1,\n        'max_features': 0.5,\n        'n_bins': 2 ** 6,\n        'random_state': None,\n        'verbose': True,\n    },\n    'boosting_rounds': None,\n    'early_stopping_rounds': None,\n    'seeds': [541992, 721991, 1337, 42, 0]\n}\n\nrf_model = TreeModels(**rf_parameters)\nrf_model.run(X_train_rf, y_train_rf, X_test_rf)\n\ndel df_train_rf, df_test_rf, X_train_rf, y_train_rf, X_test_rf\ndel rf_preprocessor, rf_parameters, rf_model\n\nprint('Saving RandomForest OOF and Test predictions to current working directory.')\ndf_train_processed[['id', 'RFPredictions']].to_csv('rf_oof_predictions.csv', index=False)\ndf_test_processed[['id', 'RFPredictions']].to_csv('rf_test_predictions.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.238374,"end_time":"2021-01-31T19:06:38.723949","exception":false,"start_time":"2021-01-31T19:06:38.485575","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## 5. Submission"},{"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-01-31T19:06:39.213415Z","iopub.status.busy":"2021-01-31T19:06:39.212415Z","iopub.status.idle":"2021-01-31T19:06:39.979185Z","shell.execute_reply":"2021-01-31T19:06:39.979776Z"},"papermill":{"duration":1.013025,"end_time":"2021-01-31T19:06:39.979934","exception":false,"start_time":"2021-01-31T19:06:38.966909","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"prediction_columns = [col for col in df_train_processed.columns if col.endswith('Predictions')]\n\nfig = plt.figure(figsize=(12, 12), dpi=100)\nsns.heatmap(df_train_processed[prediction_columns + [target]].corr(),\n            annot=True,\n            square=True,\n            cmap='coolwarm',\n            annot_kws={'size': 15},\n            fmt='.4f')\n\nplt.tick_params(axis='x', labelsize=18, rotation=90)\nplt.tick_params(axis='y', labelsize=18, rotation=0)\nplt.title('Prediction Correlations', size=20, pad=20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-31T19:06:40.507366Z","iopub.status.busy":"2021-01-31T19:06:40.505829Z","iopub.status.idle":"2021-01-31T19:06:40.798693Z","shell.execute_reply":"2021-01-31T19:06:40.798007Z"},"papermill":{"duration":0.568815,"end_time":"2021-01-31T19:06:40.798811","exception":false,"start_time":"2021-01-31T19:06:40.229996","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"class SubmissionPipeline:\n    \n    def __init__(self, train, test, blend, prediction_columns, add_public_best):\n        \n        self.train = train\n        self.test = test\n        self.blend = blend\n        self.prediction_columns = prediction_columns\n        self.add_public_best = add_public_best\n        \n    def weighted_average(self):\n        self.train['FinalPredictions'] = (0.25 * self.train['LGBPredictions']) +\\\n                                         (0.25 * self.train['CBPredictions']) +\\\n                                         (0.25 * self.train['XGBPredictions']) +\\\n                                         (0.25 * self.train['RFPredictions'])\n        \n        self.test['FinalPredictions'] = (0.25 * self.test['LGBPredictions']) +\\\n                                        (0.25 * self.test['CBPredictions']) +\\\n                                        (0.25 * self.test['XGBPredictions']) +\\\n                                        (0.25 * self.test['RFPredictions'])\n        \n    def geometric_average(self):\n        self.train['FinalPredictions'] = gmean(self.train[self.prediction_columns], axis=1)\n        self.test['FinalPredictions'] = gmean(self.test[self.prediction_columns], axis=1)\n        \n    def transform(self):        \n        if self.blend == 'weighted_average':\n            self.weighted_average()\n        elif self.blend == 'geometric_average':\n            self.geometric_average()\n            \n        for prediction_column in prediction_columns:\n            oof_score = mean_squared_error(self.train[target], df_train_processed[prediction_column], squared=False)\n            print(f'{prediction_column.split(\"Predictions\")[0]} OOF RMSE: {oof_score:.6}')\n        final_oof_score = mean_squared_error(self.train[target], df_train_processed['FinalPredictions'], squared=False)\n        print(f'{\"-\" * 30}\\nFinal OOF RMSE: {final_oof_score:.6}\\n{\"-\" * 30}')\n                \n        return self.train[['id'] + self.prediction_columns + ['FinalPredictions']].copy(deep=True), self.test[['id'] + self.prediction_columns + ['FinalPredictions']].copy(deep=True)\n            \n\n\nsubmission_pipeline = SubmissionPipeline(train=df_train_processed, test=df_test_processed,\n                                         blend='weighted_average', prediction_columns=prediction_columns, add_public_best=True)   \ndf_train_submission, df_test_submission = submission_pipeline.transform()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-31T19:06:41.359265Z","iopub.status.busy":"2021-01-31T19:06:41.358546Z","iopub.status.idle":"2021-01-31T19:06:42.45827Z","shell.execute_reply":"2021-01-31T19:06:42.457715Z"},"papermill":{"duration":1.410166,"end_time":"2021-01-31T19:06:42.458391","exception":false,"start_time":"2021-01-31T19:06:41.048225","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"df_test_processed['target'] = df_test_submission['FinalPredictions']\ndf_test_processed[['id', 'target']].to_csv('submission.csv', index=False)\ndf_test_processed[['id', 'target']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf LightGBM","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}