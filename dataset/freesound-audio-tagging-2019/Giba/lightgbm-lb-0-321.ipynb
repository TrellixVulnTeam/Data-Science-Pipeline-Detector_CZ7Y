{"cells":[{"metadata":{},"cell_type":"markdown","source":"> This experiment uses Lightgbm to fit statistical features extracted from the raw files.\n>\n> As you may know, Deep Learning/CNN is the right way to deal with this kind of challenge.\n>\n> But the challenge in this Kernel is to use pure statistics and GBDT to build good solutions.\n>\n> As extracting takes a lot of time, I preprocessed train and test locally and just uploaded files.\n>\n> Giba\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport librosa\nimport matplotlib.pyplot as plt\nimport gc\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom sklearn.metrics import label_ranking_average_precision_score\nfrom sklearn.metrics import roc_auc_score\n\nfrom joblib import Parallel, delayed\nimport lightgbm as lgb\nfrom scipy import stats\n\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef calculate_overall_lwlrap_sklearn(truth, scores):\n    \"\"\"Calculate the overall lwlrap using sklearn.metrics.lrap.\"\"\"\n    # sklearn doesn't correctly apply weighting to samples with no labels, so just skip them.\n    sample_weight = np.sum(truth > 0, axis=1)\n    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n    overall_lwlrap = label_ranking_average_precision_score(\n        truth[nonzero_weight_sample_indices, :] > 0, \n        scores[nonzero_weight_sample_indices, :], \n        sample_weight=sample_weight[nonzero_weight_sample_indices])\n    return overall_lwlrap\n\ntqdm.pandas()","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/freesound-audio-tagging-2019/sample_submission.csv')\n\nlabel_columns = list( test.columns[1:] )\nlabel_mapping = dict((label, index) for index, label in enumerate(label_columns))\n\nprint(test.shape)","execution_count":3,"outputs":[{"output_type":"stream","text":"(4970, 2) (1120, 81)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Load preprocessed train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"X     = np.load( '../input/freesoundpreproc1/LGB-train-1.npy' )\nXtest = np.load( '../input/freesoundpreproc1/LGB-test-1.npy' )\nY     = np.load( '../input/freesoundpreproc1/LGB-target.npy' )\n\nX.shape, Xtest.shape, Y.shape","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"((17866, 149), (1120, 149), (17866, 80))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_fold = 10\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=69)\n\nparams = {'num_leaves': 15,\n         'min_data_in_leaf': 200, \n         'objective':'binary',\n         \"metric\": 'auc',\n         'max_depth': -1,\n         'learning_rate': 0.05,\n         \"boosting\": \"gbdt\",\n         \"bagging_fraction\": 0.85,\n         \"bagging_freq\": 1,\n         \"feature_fraction\": 0.20,\n         \"bagging_seed\": 42,\n         \"verbosity\": -1,\n         \"nthread\": -1,\n         \"random_state\": 69}\n\nPREDTRAIN = np.zeros( (X.shape[0],80) )\nPREDTEST  = np.zeros( (Xtest.shape[0],80) )\nfor f in range(len(label_columns)):\n    y = Y[:,f] #target label\n    oof      = np.zeros( X.shape[0] )\n    oof_test = np.zeros( Xtest.shape[0] )\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X,y)):\n        model = lgb.LGBMClassifier(**params, n_estimators = 20000)\n        model.fit(X[trn_idx,:], \n                  y[trn_idx], \n                  eval_set=[(X[val_idx,:], y[val_idx])], \n                  eval_metric='auc',\n                  verbose=0, \n                  early_stopping_rounds=25)\n        oof[val_idx] = model.predict_proba(X[val_idx,:], num_iteration=model.best_iteration_)[:,1]\n        oof_test    += model.predict_proba(Xtest       , num_iteration=model.best_iteration_)[:,1]/n_fold\n\n    PREDTRAIN[:,f] = oof    \n    PREDTEST [:,f] = oof_test\n    \n    print( f, str(roc_auc_score( y, oof ))[:6], label_columns[f] )\n    \nprint( 'Competition Metric Lwlrap cv:', calculate_overall_lwlrap_sklearn( Y, PREDTRAIN ) )","execution_count":9,"outputs":[{"output_type":"stream","text":"0 0.9078 Accelerating_and_revving_and_vroom\n1 0.9265 Accordion\n2 0.9402 Acoustic_guitar\n3 0.9051 Applause\n4 0.9118 Bark\n5 0.9431 Bass_drum\n6 0.9441 Bass_guitar\n7 0.8304 Bathtub_(filling_or_washing)\n8 0.8434 Bicycle_bell\n9 0.8043 Burping_and_eructation\n10 0.9236 Bus\n11 0.7229 Buzz\n12 0.8949 Car_passing_by\n13 0.9415 Cheering\n14 0.8745 Chewing_and_mastication\n15 0.9283 Child_speech_and_kid_speaking\n16 0.7691 Chink_and_clink\n17 0.8936 Chirp_and_tweet\n18 0.8934 Church_bell\n19 0.8770 Clapping\n20 0.8535 Computer_keyboard\n21 0.8632 Crackle\n22 0.8853 Cricket\n23 0.9357 Crowd\n24 0.7947 Cupboard_open_or_close\n25 0.8580 Cutlery_and_silverware\n26 0.8498 Dishes_and_pots_and_pans\n27 0.8794 Drawer_open_or_close\n28 0.8867 Drip\n29 0.8681 Electric_guitar\n30 0.8514 Fart\n31 0.8679 Female_singing\n32 0.8861 Female_speech_and_woman_speaking\n33 0.8518 Fill_(with_liquid)\n34 0.8982 Finger_snapping\n35 0.9083 Frying_(food)\n36 0.7916 Gasp\n37 0.8559 Glockenspiel\n38 0.9073 Gong\n39 0.9626 Gurgling\n40 0.8837 Harmonica\n41 0.9132 Hi-hat\n42 0.8723 Hiss\n43 0.8353 Keys_jangling\n44 0.8264 Knock\n45 0.8818 Male_singing\n46 0.8898 Male_speech_and_man_speaking\n47 0.8594 Marimba_and_xylophone\n48 0.8742 Mechanical_fan\n49 0.8352 Meow\n50 0.8537 Microwave_oven\n51 0.9185 Motorcycle\n52 0.8513 Printer\n53 0.8939 Purr\n54 0.9228 Race_car_and_auto_racing\n55 0.8666 Raindrop\n56 0.8399 Run\n57 0.8766 Scissors\n58 0.9149 Screaming\n59 0.8321 Shatter\n60 0.8158 Sigh\n61 0.8599 Sink_(filling_or_washing)\n62 0.8625 Skateboard\n63 0.8713 Slam\n64 0.8881 Sneeze\n65 0.8479 Squeak\n66 0.9434 Stream\n67 0.8718 Strum\n68 0.8702 Tap\n69 0.8975 Tick-tock\n70 0.8418 Toilet_flush\n71 0.9317 Traffic_noise_and_roadway_noise\n72 0.8835 Trickle_and_dribble\n73 0.8779 Walk_and_footsteps\n74 0.8675 Water_tap_and_faucet\n75 0.9337 Waves_and_surf\n76 0.8974 Whispering\n77 0.8594 Writing\n78 0.9379 Yell\n79 0.7757 Zipper_(clothing)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[label_columns] = PREDTEST\ntest.to_csv('submission.csv', index=False)\ntest.head()","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"          fname        ...          Zipper_(clothing)\n0  000ccb97.wav        ...                   0.021447\n1  0012633b.wav        ...                   0.070977\n2  001ed5f1.wav        ...                   0.045184\n3  00294be0.wav        ...                   0.099416\n4  003fde7a.wav        ...                   0.031863\n\n[5 rows x 81 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fname</th>\n      <th>Accelerating_and_revving_and_vroom</th>\n      <th>Accordion</th>\n      <th>Acoustic_guitar</th>\n      <th>Applause</th>\n      <th>Bark</th>\n      <th>Bass_drum</th>\n      <th>Bass_guitar</th>\n      <th>Bathtub_(filling_or_washing)</th>\n      <th>Bicycle_bell</th>\n      <th>Burping_and_eructation</th>\n      <th>Bus</th>\n      <th>Buzz</th>\n      <th>Car_passing_by</th>\n      <th>Cheering</th>\n      <th>Chewing_and_mastication</th>\n      <th>Child_speech_and_kid_speaking</th>\n      <th>Chink_and_clink</th>\n      <th>Chirp_and_tweet</th>\n      <th>Church_bell</th>\n      <th>Clapping</th>\n      <th>Computer_keyboard</th>\n      <th>Crackle</th>\n      <th>Cricket</th>\n      <th>Crowd</th>\n      <th>Cupboard_open_or_close</th>\n      <th>Cutlery_and_silverware</th>\n      <th>Dishes_and_pots_and_pans</th>\n      <th>Drawer_open_or_close</th>\n      <th>Drip</th>\n      <th>Electric_guitar</th>\n      <th>Fart</th>\n      <th>Female_singing</th>\n      <th>Female_speech_and_woman_speaking</th>\n      <th>Fill_(with_liquid)</th>\n      <th>Finger_snapping</th>\n      <th>Frying_(food)</th>\n      <th>Gasp</th>\n      <th>Glockenspiel</th>\n      <th>Gong</th>\n      <th>...</th>\n      <th>Harmonica</th>\n      <th>Hi-hat</th>\n      <th>Hiss</th>\n      <th>Keys_jangling</th>\n      <th>Knock</th>\n      <th>Male_singing</th>\n      <th>Male_speech_and_man_speaking</th>\n      <th>Marimba_and_xylophone</th>\n      <th>Mechanical_fan</th>\n      <th>Meow</th>\n      <th>Microwave_oven</th>\n      <th>Motorcycle</th>\n      <th>Printer</th>\n      <th>Purr</th>\n      <th>Race_car_and_auto_racing</th>\n      <th>Raindrop</th>\n      <th>Run</th>\n      <th>Scissors</th>\n      <th>Screaming</th>\n      <th>Shatter</th>\n      <th>Sigh</th>\n      <th>Sink_(filling_or_washing)</th>\n      <th>Skateboard</th>\n      <th>Slam</th>\n      <th>Sneeze</th>\n      <th>Squeak</th>\n      <th>Stream</th>\n      <th>Strum</th>\n      <th>Tap</th>\n      <th>Tick-tock</th>\n      <th>Toilet_flush</th>\n      <th>Traffic_noise_and_roadway_noise</th>\n      <th>Trickle_and_dribble</th>\n      <th>Walk_and_footsteps</th>\n      <th>Water_tap_and_faucet</th>\n      <th>Waves_and_surf</th>\n      <th>Whispering</th>\n      <th>Writing</th>\n      <th>Yell</th>\n      <th>Zipper_(clothing)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000ccb97.wav</td>\n      <td>0.001701</td>\n      <td>0.003465</td>\n      <td>0.005784</td>\n      <td>0.002173</td>\n      <td>0.012073</td>\n      <td>0.006242</td>\n      <td>0.002575</td>\n      <td>0.014750</td>\n      <td>0.047343</td>\n      <td>0.018991</td>\n      <td>0.005114</td>\n      <td>0.010724</td>\n      <td>0.005353</td>\n      <td>0.002466</td>\n      <td>0.011606</td>\n      <td>0.007054</td>\n      <td>0.037430</td>\n      <td>0.123227</td>\n      <td>0.005719</td>\n      <td>0.031604</td>\n      <td>0.012731</td>\n      <td>0.010826</td>\n      <td>0.022032</td>\n      <td>0.002971</td>\n      <td>0.026874</td>\n      <td>0.065533</td>\n      <td>0.085551</td>\n      <td>0.004328</td>\n      <td>0.009503</td>\n      <td>0.008450</td>\n      <td>0.010980</td>\n      <td>0.004067</td>\n      <td>0.003292</td>\n      <td>0.010778</td>\n      <td>0.102108</td>\n      <td>0.003154</td>\n      <td>0.036711</td>\n      <td>0.006091</td>\n      <td>0.007821</td>\n      <td>...</td>\n      <td>0.006402</td>\n      <td>0.186183</td>\n      <td>0.050988</td>\n      <td>0.171281</td>\n      <td>0.010754</td>\n      <td>0.002038</td>\n      <td>0.004670</td>\n      <td>0.032471</td>\n      <td>0.010836</td>\n      <td>0.011894</td>\n      <td>0.013356</td>\n      <td>0.000827</td>\n      <td>0.013836</td>\n      <td>0.006198</td>\n      <td>0.002601</td>\n      <td>0.019029</td>\n      <td>0.007605</td>\n      <td>0.011166</td>\n      <td>0.033381</td>\n      <td>0.113313</td>\n      <td>0.007103</td>\n      <td>0.008203</td>\n      <td>0.014486</td>\n      <td>0.006221</td>\n      <td>0.008932</td>\n      <td>0.044437</td>\n      <td>0.007579</td>\n      <td>0.008924</td>\n      <td>0.029098</td>\n      <td>0.006852</td>\n      <td>0.007989</td>\n      <td>0.001585</td>\n      <td>0.015085</td>\n      <td>0.008546</td>\n      <td>0.011830</td>\n      <td>0.002600</td>\n      <td>0.014909</td>\n      <td>0.014685</td>\n      <td>0.018834</td>\n      <td>0.021447</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0012633b.wav</td>\n      <td>0.043288</td>\n      <td>0.004765</td>\n      <td>0.006900</td>\n      <td>0.005487</td>\n      <td>0.015202</td>\n      <td>0.004686</td>\n      <td>0.006452</td>\n      <td>0.022611</td>\n      <td>0.010462</td>\n      <td>0.022390</td>\n      <td>0.013816</td>\n      <td>0.015363</td>\n      <td>0.041319</td>\n      <td>0.002985</td>\n      <td>0.040197</td>\n      <td>0.007133</td>\n      <td>0.008474</td>\n      <td>0.015052</td>\n      <td>0.028771</td>\n      <td>0.009913</td>\n      <td>0.070193</td>\n      <td>0.044052</td>\n      <td>0.031364</td>\n      <td>0.004928</td>\n      <td>0.021136</td>\n      <td>0.011766</td>\n      <td>0.010441</td>\n      <td>0.019389</td>\n      <td>0.013524</td>\n      <td>0.007891</td>\n      <td>0.010772</td>\n      <td>0.016518</td>\n      <td>0.010283</td>\n      <td>0.013182</td>\n      <td>0.008777</td>\n      <td>0.018699</td>\n      <td>0.016299</td>\n      <td>0.007019</td>\n      <td>0.007234</td>\n      <td>...</td>\n      <td>0.008714</td>\n      <td>0.006993</td>\n      <td>0.014951</td>\n      <td>0.014322</td>\n      <td>0.012754</td>\n      <td>0.006738</td>\n      <td>0.007020</td>\n      <td>0.008221</td>\n      <td>0.021438</td>\n      <td>0.029574</td>\n      <td>0.020073</td>\n      <td>0.046411</td>\n      <td>0.024924</td>\n      <td>0.073246</td>\n      <td>0.011561</td>\n      <td>0.008836</td>\n      <td>0.044392</td>\n      <td>0.016318</td>\n      <td>0.007874</td>\n      <td>0.007907</td>\n      <td>0.018060</td>\n      <td>0.036442</td>\n      <td>0.013169</td>\n      <td>0.010956</td>\n      <td>0.008820</td>\n      <td>0.040564</td>\n      <td>0.019688</td>\n      <td>0.012835</td>\n      <td>0.006325</td>\n      <td>0.021711</td>\n      <td>0.027569</td>\n      <td>0.028849</td>\n      <td>0.010360</td>\n      <td>0.088290</td>\n      <td>0.036637</td>\n      <td>0.052185</td>\n      <td>0.047691</td>\n      <td>0.087686</td>\n      <td>0.009724</td>\n      <td>0.070977</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001ed5f1.wav</td>\n      <td>0.007671</td>\n      <td>0.002849</td>\n      <td>0.006193</td>\n      <td>0.005636</td>\n      <td>0.035427</td>\n      <td>0.003122</td>\n      <td>0.002564</td>\n      <td>0.030687</td>\n      <td>0.009929</td>\n      <td>0.015878</td>\n      <td>0.005172</td>\n      <td>0.013116</td>\n      <td>0.025639</td>\n      <td>0.002360</td>\n      <td>0.032685</td>\n      <td>0.013589</td>\n      <td>0.020729</td>\n      <td>0.010208</td>\n      <td>0.006298</td>\n      <td>0.085823</td>\n      <td>0.070527</td>\n      <td>0.013212</td>\n      <td>0.010005</td>\n      <td>0.004673</td>\n      <td>0.034943</td>\n      <td>0.044784</td>\n      <td>0.044952</td>\n      <td>0.149141</td>\n      <td>0.014950</td>\n      <td>0.006144</td>\n      <td>0.018467</td>\n      <td>0.005525</td>\n      <td>0.018998</td>\n      <td>0.025845</td>\n      <td>0.035531</td>\n      <td>0.006336</td>\n      <td>0.026798</td>\n      <td>0.004962</td>\n      <td>0.004621</td>\n      <td>...</td>\n      <td>0.009383</td>\n      <td>0.006396</td>\n      <td>0.014552</td>\n      <td>0.021231</td>\n      <td>0.060768</td>\n      <td>0.003600</td>\n      <td>0.006575</td>\n      <td>0.007810</td>\n      <td>0.010977</td>\n      <td>0.027690</td>\n      <td>0.052321</td>\n      <td>0.008458</td>\n      <td>0.013906</td>\n      <td>0.012310</td>\n      <td>0.003723</td>\n      <td>0.011183</td>\n      <td>0.025731</td>\n      <td>0.031852</td>\n      <td>0.015984</td>\n      <td>0.079118</td>\n      <td>0.030774</td>\n      <td>0.019499</td>\n      <td>0.052701</td>\n      <td>0.074969</td>\n      <td>0.179519</td>\n      <td>0.042464</td>\n      <td>0.008699</td>\n      <td>0.009911</td>\n      <td>0.014626</td>\n      <td>0.036715</td>\n      <td>0.013758</td>\n      <td>0.006070</td>\n      <td>0.015382</td>\n      <td>0.025617</td>\n      <td>0.015145</td>\n      <td>0.002915</td>\n      <td>0.020527</td>\n      <td>0.042038</td>\n      <td>0.031089</td>\n      <td>0.045184</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00294be0.wav</td>\n      <td>0.010220</td>\n      <td>0.003050</td>\n      <td>0.006721</td>\n      <td>0.003786</td>\n      <td>0.012860</td>\n      <td>0.002891</td>\n      <td>0.002695</td>\n      <td>0.030088</td>\n      <td>0.010429</td>\n      <td>0.013716</td>\n      <td>0.007062</td>\n      <td>0.019810</td>\n      <td>0.038327</td>\n      <td>0.001689</td>\n      <td>0.088642</td>\n      <td>0.013676</td>\n      <td>0.009903</td>\n      <td>0.019943</td>\n      <td>0.011506</td>\n      <td>0.010390</td>\n      <td>0.090408</td>\n      <td>0.023384</td>\n      <td>0.068680</td>\n      <td>0.003337</td>\n      <td>0.023706</td>\n      <td>0.011499</td>\n      <td>0.010050</td>\n      <td>0.035739</td>\n      <td>0.023063</td>\n      <td>0.006036</td>\n      <td>0.009680</td>\n      <td>0.008147</td>\n      <td>0.004111</td>\n      <td>0.016224</td>\n      <td>0.011778</td>\n      <td>0.010391</td>\n      <td>0.018617</td>\n      <td>0.005704</td>\n      <td>0.009979</td>\n      <td>...</td>\n      <td>0.007332</td>\n      <td>0.005147</td>\n      <td>0.017645</td>\n      <td>0.016208</td>\n      <td>0.016605</td>\n      <td>0.003061</td>\n      <td>0.006693</td>\n      <td>0.006005</td>\n      <td>0.016220</td>\n      <td>0.038075</td>\n      <td>0.017743</td>\n      <td>0.013133</td>\n      <td>0.023955</td>\n      <td>0.187232</td>\n      <td>0.002944</td>\n      <td>0.011556</td>\n      <td>0.038248</td>\n      <td>0.021309</td>\n      <td>0.007643</td>\n      <td>0.006742</td>\n      <td>0.021274</td>\n      <td>0.017979</td>\n      <td>0.017679</td>\n      <td>0.009952</td>\n      <td>0.010671</td>\n      <td>0.016195</td>\n      <td>0.016862</td>\n      <td>0.008817</td>\n      <td>0.010971</td>\n      <td>0.143709</td>\n      <td>0.017017</td>\n      <td>0.002932</td>\n      <td>0.011373</td>\n      <td>0.086058</td>\n      <td>0.026009</td>\n      <td>0.020457</td>\n      <td>0.098102</td>\n      <td>0.132359</td>\n      <td>0.009708</td>\n      <td>0.099416</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>003fde7a.wav</td>\n      <td>0.003534</td>\n      <td>0.005038</td>\n      <td>0.012481</td>\n      <td>0.004631</td>\n      <td>0.012968</td>\n      <td>0.003730</td>\n      <td>0.003668</td>\n      <td>0.037293</td>\n      <td>0.614078</td>\n      <td>0.015593</td>\n      <td>0.005991</td>\n      <td>0.014871</td>\n      <td>0.011136</td>\n      <td>0.003979</td>\n      <td>0.010602</td>\n      <td>0.006838</td>\n      <td>0.313897</td>\n      <td>0.098104</td>\n      <td>0.108955</td>\n      <td>0.011213</td>\n      <td>0.028588</td>\n      <td>0.012039</td>\n      <td>0.024335</td>\n      <td>0.004009</td>\n      <td>0.016403</td>\n      <td>0.035801</td>\n      <td>0.024263</td>\n      <td>0.007551</td>\n      <td>0.006813</td>\n      <td>0.011213</td>\n      <td>0.016197</td>\n      <td>0.016119</td>\n      <td>0.002914</td>\n      <td>0.014758</td>\n      <td>0.018710</td>\n      <td>0.007284</td>\n      <td>0.018883</td>\n      <td>0.086364</td>\n      <td>0.049771</td>\n      <td>...</td>\n      <td>0.073677</td>\n      <td>0.015447</td>\n      <td>0.072272</td>\n      <td>0.141474</td>\n      <td>0.012210</td>\n      <td>0.004299</td>\n      <td>0.002554</td>\n      <td>0.086359</td>\n      <td>0.012210</td>\n      <td>0.023487</td>\n      <td>0.016602</td>\n      <td>0.000991</td>\n      <td>0.018055</td>\n      <td>0.009611</td>\n      <td>0.004544</td>\n      <td>0.012035</td>\n      <td>0.012272</td>\n      <td>0.103961</td>\n      <td>0.044982</td>\n      <td>0.077004</td>\n      <td>0.008640</td>\n      <td>0.035273</td>\n      <td>0.008729</td>\n      <td>0.005741</td>\n      <td>0.022810</td>\n      <td>0.038267</td>\n      <td>0.012573</td>\n      <td>0.043493</td>\n      <td>0.012674</td>\n      <td>0.035545</td>\n      <td>0.049425</td>\n      <td>0.004789</td>\n      <td>0.009789</td>\n      <td>0.009015</td>\n      <td>0.060554</td>\n      <td>0.002568</td>\n      <td>0.016323</td>\n      <td>0.018379</td>\n      <td>0.022514</td>\n      <td>0.031863</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}