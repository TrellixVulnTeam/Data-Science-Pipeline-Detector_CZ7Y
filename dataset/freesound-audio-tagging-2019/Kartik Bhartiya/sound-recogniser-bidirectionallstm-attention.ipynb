{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom scipy.io import wavfile\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\nnp.random.seed(101)\nimport IPython.display as ipd\nimport os\nfrom scipy import signal\nprint(os.listdir(\"../input\"))\nfrom tqdm import tqdm, tqdm_notebook; tqdm.pandas() # Progress bar\nfrom sklearn.metrics import label_ranking_average_precision_score\nfrom sklearn.model_selection import train_test_split\n\n# Machine Learning\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.layers import (Dense, Bidirectional, CuDNNLSTM,\n                          Dropout, LeakyReLU, Convolution2D, \n                          Conv2D, Conv1D)\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\n\nimport warnings; warnings.filterwarnings(\"ignore\")\n#print(os.listdir(\"../input/train_curated\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_cur = pd.read_csv(\"../input/train_curated.csv\")\ntrain_nos = pd.read_csv(\"../input/train_noisy.csv\")\ntest = pd.read_csv(\"../input/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cur.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_nos.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_audio_cur = \"../input/train_curated/\"\nipd.Audio(train_audio_cur+\"31a0f9cc.wav\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cur.loc[train_cur['fname']=='31a0f9cc.wav']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sampling_rate, data = wavfile.read(train_audio_cur+\"31a0f9cc.wav\")\nprint(\"Sampling Rate: {}\".format(sampling_rate))\nprint(\"Data of the audio wave: {}\".format(data))\nprint(\"Duration of Audio file: {}\".format(len(data)/sampling_rate))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_raw_wave(data):\n    plt.figure(figsize=(16,9))\n    plt.title(\"Raw Representation of Audio Wave\")\n    plt.ylabel(\"Amplitude\")\n    plt.plot(data)\nplot_raw_wave(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/davids1992/audio-representation-what-it-s-all-about\ndef log_specgram(audio, sample_rate, window_size=100,\n                 step_size=10, eps=1e-10):\n    nperseg = int(round(window_size * sample_rate / 1e3))\n    noverlap = int(round(step_size * sample_rate / 1e3))\n    freqs, times, spec = signal.spectrogram(audio,\n                                    fs=sample_rate,\n                                    window='hann',\n                                    nperseg=nperseg,\n                                    noverlap=noverlap,\n                                    detrend=False)\n    return freqs, times, np.log(spec.T.astype(np.float32) + eps)\n\ndef plot_log_specgram(audio, sample_rate, window_size=20, step_size=10, eps=1e-10):\n    \n    fig = plt.figure(figsize=(16,9))\n    freqs, times, spectrogram = log_specgram(audio, sample_rate)\n    plt.imshow(spectrogram.T, aspect='auto', origin='lower', \n               extent=[times.min(), times.max(), freqs.min(), freqs.max()])\n    plt.yticks(freqs[::16])\n    plt.xticks(times[::16])\n    plt.title('Spectrogram')\n    plt.ylabel('Freqs in Hz')\n    plt.xlabel('Seconds')\n    plt.show()\nplot_log_specgram(data, sampling_rate, window_size=1000,step_size=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import librosa\nimport librosa.display","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/davids1992/audio-representation-what-it-s-all-about\nS = librosa.feature.melspectrogram(data.astype(float), sr=sampling_rate, n_mels=128)\n\n# Convert to log scale (dB). We'll use the peak power (max) as reference.\nlog_S = librosa.power_to_db(S, ref=np.max)\n\nplt.figure(figsize=(16, 9))\nlibrosa.display.specshow(log_S, sr=sampling_rate, x_axis='time', y_axis='mel')\nplt.title('Mel power spectrogram ')\nplt.colorbar(format='%+02.0f dB')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=13)\n\n# Let's pad on the first and second deltas while we're at it\ndelta2_mfcc = librosa.feature.delta(mfcc, order=2)\n\nplt.figure(figsize=(12, 4))\nlibrosa.display.specshow(delta2_mfcc)\nplt.ylabel('MFCC coeffs')\nplt.xlabel('Time')\nplt.title('MFCC')\nplt.colorbar()\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_overall_lwlrap_sklearn(truth, scores):\n    \"\"\"Calculate the overall lwlrap using sklearn.metrics.lrap.\"\"\"\n    # sklearn doesn't correctly apply weighting to samples with no labels, so just skip them.\n    sample_weight = np.sum(truth > 0, axis=1)\n    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n    overall_lwlrap = label_ranking_average_precision_score(\n        truth[nonzero_weight_sample_indices, :] > 0, \n        scores[nonzero_weight_sample_indices, :], \n        sample_weight=sample_weight[nonzero_weight_sample_indices])\n    return overall_lwlrap\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_and_label(rows_label):\n    labeled_rows = []\n    for row in rows_label:\n        rows_label = row.split(\",\")\n        arr = np.zeros((80))\n        for label in rows_label:\n            idx = label_mapping[label]\n            arr[idx] = 1\n        labeled_rows.append(arr)\n    return labeled_rows ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_columns = test.columns[1:]\nlabel_mapping = dict((label,index) for index, label in enumerate(label_columns))\nprint(\"Total Number of Classes: {}\".format(len(label_columns)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cur_labels = split_and_label(train_cur['labels'])\ntrain_nos_labels = split_and_label(train_nos['labels'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in label_columns:\n    train_cur[col] = 0\n    train_nos[col] = 0\ntrain_cur[label_columns] = train_cur_labels\ntrain_nos[label_columns] = train_nos_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cur.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_nos.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_shape = (890,128)\nn_classes = 80\nn_epochs = 500\nhop_length = 347\nfmin = 20\nfmax = sampling_rate // 2\nn_mels = 128\nn_fft = n_mels*20\nopt = Adam(0.003, beta_1=0.75, beta_2=0.85, amsgrad=True)\nsampling_rate = 44100\nduration = 7\nsamples = sampling_rate*duration\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_audio(pathname, trim_long_data):\n    y, sr = librosa.load(pathname, sr=sampling_rate)\n    # trim silence\n    if 0 < len(y): # workaround: 0 length causes error\n        y, _ = librosa.effects.trim(y) # trim, top_db=default(60)\n    # make it unified length to conf.samples\n    if len(y) > samples: # long enough\n        if trim_long_data:\n            y = y[0:0+samples]\n    else: # pad blank\n        padding = samples - len(y)    # add padding at both ends\n        offset = padding // 2\n        y = np.pad(y, (offset, samples - len(y) - offset), 'constant')\n    return y\n\ndef audio_to_melspectrogram(audio):\n    spectrogram = librosa.feature.melspectrogram(audio, \n                                                 sr=sampling_rate,\n                                                 n_mels=n_mels,\n                                                 hop_length=hop_length,\n                                                 n_fft=n_fft,\n                                                 fmin=fmin,\n                                                 fmax=fmax)\n    spectrogram = librosa.power_to_db(spectrogram)\n    spectrogram = spectrogram.astype(np.float32)\n    return spectrogram\n\ndef read_as_melspectrogram(pathname, trim_long_data, debug_display=False):\n    x = read_audio(pathname, trim_long_data)\n    mels = audio_to_melspectrogram(x)\n    if debug_display:\n        IPython.display.display(IPython.display.Audio(x, rate=sampling_rate))\n        show_melspectrogram(mels)\n    return mels\n\ndef convert_wav_to_image(df, source):\n    X = []\n    for i, row in tqdm_notebook(df.iterrows()):\n        try:\n            x = read_as_melspectrogram(f'{source[0]}/{str(row.fname)}', trim_long_data=True)\n        except:\n            x = read_as_melspectrogram(f'{source[1]}/{str(row.fname)}', trim_long_data=True)\n\n        #x_color = mono_to_color(x)\n        X.append(x.transpose())\n        #df.loc[i, 'length'] = x.shape[1]\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([train_cur[:4000],train_nos[:2000]])\ntrain_curated_path = '../input/train_curated/'\ntrain_noisy_path = '../input/train_noisy/'\ntest_path = '../input/test/'\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#X = np.array(convert_wav_to_image(train_cur,source=[train_curated_path]))\n#X = np.array(convert_wav_to_image(train_nos, source=[train_noisy_path]))\nX = np.array(convert_wav_to_image(train, source=[train_curated_path, train_noisy_path]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = train[label_columns].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n        if self.bias:\n            eij += self.b\n        eij = K.tanh(eij)\n        a = K.exp(eij)\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Bidirectional(CuDNNLSTM(128,return_sequences=True),input_shape=input_shape))\n#model.add(Bidirectional(CuDNNLSTM(128,return_sequences=True),input_shape=input_shape))\n#model.add(Bidirectional(CuDNNLSTM(128,return_sequences=True),input_shape=input_shape))\nmodel.add(Attention(input_shape[0]))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(n_classes,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['acc'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es = EarlyStopping(monitor='val_acc', mode='max', verbose=2, patience=10)\nx_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.1, random_state=101)\nhistory = model.fit(np.array(x_train),\n          y_train,\n          batch_size=512,\n          epochs=500,\n          validation_data=(np.array(x_val), y_val),\n          callbacks = [es]\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = model.predict(np.array(x_train))\ny_val_pred = model.predict(np.array(x_val))\ntrain_lwlrap = calculate_overall_lwlrap_sklearn(y_train, y_train_pred)\nval_lwlrap = calculate_overall_lwlrap_sklearn(y_val, y_val_pred)\n\n# Check training and validation LWLRAP score\nprint('Training LWLRAP : {}'.format(round(train_lwlrap,4)))\nprint('Validation LWLRAP : {}'.format(round(val_lwlrap,4)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = np.array(convert_wav_to_image(test, [test_path]))\npredictions = model.predict(np.array(X_test))\ntest[label_columns] = predictions\ntest.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}