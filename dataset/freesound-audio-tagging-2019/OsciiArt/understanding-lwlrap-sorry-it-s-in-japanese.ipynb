{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# ライブラリ読み込み\nimport numpy as np\nimport sklearn.metrics","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"# LwLRAP計算関数\n# from official code https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8#scrollTo=cRCaCIb9oguU\ndef _one_sample_positive_class_precisions(scores, truth):\n    \"\"\"Calculate precisions for each true class for a single sample.\n\n    Args:\n      scores: np.array of (num_classes,) giving the individual classifier scores.\n      truth: np.array of (num_classes,) bools indicating which classes are true.\n\n    Returns:\n      pos_class_indices: np.array of indices of the true classes for this sample.\n      pos_class_precisions: np.array of precisions corresponding to each of those\n        classes.\n    \"\"\"\n    num_classes = scores.shape[0]\n    pos_class_indices = np.flatnonzero(truth > 0)\n    # Only calculate precisions if there are some true classes.\n    if not len(pos_class_indices):\n        return pos_class_indices, np.zeros(0)\n    # Retrieval list of classes for this sample.\n    retrieved_classes = np.argsort(scores)[::-1]\n    # class_rankings[top_scoring_class_index] == 0 etc.\n    class_rankings = np.zeros(num_classes, dtype=np.int)\n    class_rankings[retrieved_classes] = range(num_classes)\n    # Which of these is a true label?\n    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n    retrieved_class_true[class_rankings[pos_class_indices]] = True\n    # Num hits for every truncated retrieval list.\n    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n    # Precision of retrieval list truncated at each hit, in order of pos_labels.\n    precision_at_hits = (\n            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n            (1 + class_rankings[pos_class_indices].astype(np.float)))\n    return pos_class_indices, precision_at_hits\n\n\ndef calculate_per_class_lwlrap(truth, scores):\n    \"\"\"Calculate label-weighted label-ranking average precision.\n\n    Arguments:\n      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n        of presence of that class in that sample.\n      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n        test's real-valued score for each class for each sample.\n\n    Returns:\n      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n        class.\n      weight_per_class: np.array of (num_classes,) giving the prior of each\n        class within the truth labels.  Then the overall unbalanced lwlrap is\n        simply np.sum(per_class_lwlrap * weight_per_class)\n    \"\"\"\n    assert truth.shape == scores.shape\n    num_samples, num_classes = scores.shape\n    # Space to store a distinct precision value for each class on each sample.\n    # Only the classes that are true for each sample will be filled in.\n    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n    for sample_num in range(num_samples):\n        pos_class_indices, precision_at_hits = (\n            _one_sample_positive_class_precisions(scores[sample_num, :],\n                                                  truth[sample_num, :]))\n        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n            precision_at_hits)\n    labels_per_class = np.sum(truth > 0, axis=0)\n    weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n    # Form average of each column, i.e. all the precisions assigned to labels in\n    # a particular class.\n    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n                        np.maximum(1, labels_per_class))\n    # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n    #                = np.sum(precisions_for_samples_by_classes) / np.sum(precisions_for_samples_by_classes > 0)\n    #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n    #                = np.sum(per_class_lwlrap * weight_per_class)\n    return per_class_lwlrap, weight_per_class\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1 sample ごとの計算  \n### 正解ラベルが1つの場合。  \nクラスがA,B,Cの3種類とする。  \n正解ラベル = A, 予測 = (A: 0.7, B: 0.1, 0.2)の場合を例として考える。  \nまず予測をランク化 (値の大きい順に数字を振る) する。  \n-> 予測 = (A: 1, B: 3, C:2)  \nScore = 1～正解ラベルのランクまでの正解数/正解ラベルのランク と計算される。  \nこの場合、  \n正解ラベルのランク = 1  \n1～正解ラベルのランクまでの正解数 = ランク1～1までの正解数 = 1  \nなので、  \nScore = 1/1 = 1.0 となる。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 実際に計算してみる。\ny_true = np.array([1, 0, 0,])\ny_score = np.array([0.7, 0.1, 0.2])\npos_class_indices, precision_at_hits = _one_sample_positive_class_precisions(y_score, y_true)\nprint(\"正解ラベル\", pos_class_indices)\nprint(\"スコア\", precision_at_hits)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"別例  \n正解ラベル = A, 予測 = (A: 0.1, B: 0.7, 0.2)の場合。  \nランク化予測 = (A: 3, B: 1, C:2)  \n正解ラベルのランク = 3  \n1～正解ラベルのランクまでの正解数 = ランク1～3までの正解数 = 1  \nなので、  \nScore = 1/3 = 0.33 となる。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 実際に計算してみる。\ny_true = np.array([1, 0, 0,])\ny_score = np.array([0.1, 0.7, 0.2])\npos_class_indices, precision_at_hits = _one_sample_positive_class_precisions(y_score, y_true)\nprint(\"正解ラベル\", pos_class_indices)\nprint(\"スコア\", precision_at_hits)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 正解ラベルが複数の場合。  \nクラスがA,B,Cの3種類とする。  \n正解ラベル = A,C 予測 = (A: 0.7, B: 0.1, 0.2)の場合を例として考える。  \nScoreは正解ラベルごとに計算される。  \nまず正解ラベルAに対するスコアを計算すると、  \nランク化予測 = (A: 1, B: 3, C:2)  \n正解ラベルのランク = 1  \n1～正解ラベルのランクまでの正解数 = ランク1～3までの正解数 = 1  \nなので、  \nScore = 1/1 = 1.0 となる。\n\n次に正解ラベルCに対するスコアを計算すると、  \nランク化予測 = (A: 1, B: 3, C:2)  \n正解ラベルのランク = 2  \n1～正解ラベルのランクまでの正解数 = ランク1～2までの正解数 = 2  \nなので、  \nScore = 2/2 = 1.0 となる。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 実際に計算してみる。\ny_true = np.array([1, 0, 1,])\ny_score = np.array([0.7, 0.1, 0.2])\npos_class_indices, precision_at_hits = _one_sample_positive_class_precisions(y_score, y_true)\nprint(\"正解ラベル\", pos_class_indices)\nprint(\"スコア\", precision_at_hits)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"別例  \n\n正解ラベル = A,C 予測 = (A: 0.1, B: 0.7, 0.2)の場合を例として考える。  \nScoreは正解ラベルごとに計算される。  \nまず正解ラベルAに対するスコアを計算すると、  \nランク化予測 = (A: 3, B: 1, C:2)  \n正解ラベルのランク = 3  \n1～正解ラベルのランクまでの正解数 = ランク1～3までの正解数 = 2  \nなので、  \nScore = 2/3 = 0.67 となる。  \n\n次に正解ラベルCに対するスコアを計算すると、  \nランク化予測 = (A: 1, B: 3, C:2)  \n正解ラベルのランク = 2  \n1～正解ラベルのランクまでの正解数 = ランク1～2までの正解数 = 1  \nなので、  \nScore = 1/2 = 0.5 となる。  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# 実際に計算してみる。\ny_true = np.array([1, 0, 1,])\ny_score = np.array([0.1, 0.7, 0.2])\npos_class_indices, precision_at_hits = _one_sample_positive_class_precisions(y_score, y_true)\nprint(\"正解ラベル\", pos_class_indices)\nprint(\"スコア\", precision_at_hits)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 全サンプルに対する計算\nサンプル 1:  正解ラベル = A,C 予測 = (A: 0.1, B: 0.7, 0.2)  \nサンプル 2:  正解ラベル = B,C 予測 = (A: 0.1, B: 0.7, 0.2)  \nの場合を考える。  \nまずクラスごとのスコアを計算する。  \nScore = あるクラスに対するscoreの合計/あるクラスの正解ラベルの数  \nこの場合、  \nサンプル1のscore= A: 0.6667, C: 0.5  \nサンプル2のscore = B: 1.0, C: 1.0  \nなので、  \nクラスAのscore = 0.6667/1 = 0.6667  \nクラスBのscore = 1.0/1 = 1.0  \nクラスCのscore = (0.5 + 1.0) / 2 = 0.75  \nとなる。  \n全クラスに対するスコアを計算する場合、各クラスのスコアを平均するとクラスごとの正解ラベル数の偏りを考慮しないために、  \n出現頻度の高いクラスは一つのラベルの最終スコアへの影響が小さくなり、  \n出現頻度の低いクラスは1つのラベルの最終スコアへの影響が大きくなってしまう。  \nそこで各クラスの出現数を重みとして重みつき平均を取る。  \n正解ラベル出現数 = (A: 1. B: 1, C:2)  \n重み = 正解ラベル出現数 / 正解ラベル合計数 = (A: 1. B: 2, C:1) / 4 = (A: 0.25, B: 0.25. C: 0.5)  \nスコア = (各クラスのスコア * 重み)の合計 = A: 0.6667 * 0.25 + B: 1.0 * 0.25 + C: 0.75 * 0.5 = 0.7917  \nこれは結局、各ラベルに対するスコアの平均に等しい。  \n各ラベルに対するスコアの平均 = (0.6667 + 0.5 + 1.0 + 1.0 ) / 4 = 0.7917  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# 実際に計算してみる。\ny_true = np.array([[1, 0, 1,], [0, 1, 1]])\ny_score = np.array([[0.1, 0.7, 0.2], [0.1, 0.7, 0.2]])\n_, precision_at_hits1 = _one_sample_positive_class_precisions(y_score[0], y_true[0])\nprint(\"sample 1 のスコア\", precision_at_hits1)\n_, precision_at_hits2 = _one_sample_positive_class_precisions(y_score[1], y_true[1])\nprint(\"sample 2 のスコア\", precision_at_hits2)\nscore, weight = calculate_per_class_lwlrap(y_true, y_score)\nprint(\"各クラスのスコア\", score)\nprint(\"各クラスの重み\", weight)\nLwLRAP = (score*weight).sum()\nprint(\"LwLRAP\", LwLRAP)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}