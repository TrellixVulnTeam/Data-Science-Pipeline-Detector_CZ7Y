{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"class conf:\n    sampling_rate = 44100\n    duration = 2 # sec\n    hop_length = 347*duration # to make time steps 128\n    fmin = 20\n    fmax = sampling_rate // 2\n    n_mels = 128\n    n_fft = n_mels * 20\n    padmode = 'constant'\n    samples = sampling_rate * duration","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# rate = 44100\n\n\n# import librosa\n\n# print(data[0].shape)\n# plt.imshow(data[0], interpolation='nearest')\n# plt.show()\n\n# t = data[2930].copy()\n# t = np.compress([False, True], t, axis=2)\n# print(t.shape)\n# t = np.squeeze(t, axis=2)\n# print(t.shape)\n# plt.imshow(t, interpolation='nearest')\n# plt.show()\n\n# t = t.astype(float)\n# print(type(t[0][0]))\n\n# spectrogram = librosa.db_to_power(t)\n# backconvert = librosa.feature.inverse.mel_to_audio(spectrogram,\n#     sr=conf.sampling_rate,\n#     n_fft=conf.n_fft,\n#     hop_length=conf.hop_length,\n#     pad_mode=conf.padmode,\n#     fmin=conf.fmin,\n#     fmax=conf.fmax)\n# print('converted back')\n# IPython.display.display(IPython.display.Audio(backconvert, rate=rate))\n\n# res = librosa.feature.inverse.mel_to_audio(t)\n\n# # c = np.zeros((826 , 954))\n# # t = np.resize(t,(954, 954))\n# # t = np.concatenate((t, c), axis=0)\n# # t.shape = (954, 954)\n\n# # t = t[0:128,0:128]\n# t = np.transpose(t)\n\n# print(t.shape)\n# plt.imshow(t, interpolation='nearest')\n# plt.show()\n# mel_inverted_spectrogram = mel_to_spectrogram(t, mel_inversion_filter,\n#                                                 spec_thresh=spec_thresh,\n#                                                 shorten_factor=shorten_factor)\n# print(mel_inverted_spectrogram.shape)\n# fig, ax = plt.subplots(nrows=1,ncols=1, figsize=(20,4))\n# cax = ax.matshow(np.float32(mel_inverted_spectrogram), cmap=plt.cm.afmhot, origin='lower', aspect='auto',interpolation='nearest')\n# fig.colorbar(cax)\n# plt.title('Inverted mel Spectrogram')\n\n# inverted_mel_audio = invert_pretty_spectrogram(np.transpose(t), fft_size = fft_size,\n#                                             step_size = step_size, log = True, n_iter = 10)\n# IPython.display.Audio(data=res, rate=rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.io import wavfile\nfrom zipfile import ZipFile \nimport librosa\nimport fnmatch\nimport os\nfrom matplotlib import pyplot as plt\n\n# iPython specific stuff\n%matplotlib inline\nimport IPython.display\nfrom ipywidgets import interact, interactive, fixed\n\n# Packages we're using\nimport numpy as np\nimport copy\nimport scipy.ndimage\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unzip train curated"},{"metadata":{"trusted":true},"cell_type":"code","source":"\narr = os.listdir('/kaggle/working/')\nif len(arr) < 5:\n    with ZipFile('/kaggle/input/freesound-audio-tagging-2019/train_curated.zip', 'r') as zip:\n        zip.extractall()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create soundDict and list of sounds with guitar in label"},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv\nsoundsDict = {}\nwith open('/kaggle/input/freesound-audio-tagging-2019/train_curated.csv', mode='r') as infile:\n    reader = csv.reader(infile)\n    for rows in reader:\n        if rows[1] in soundsDict:\n            soundsDict[rows[1]].append(rows[0])\n        else:\n            soundsDict[rows[1]] = [rows[0]]\n\nrock = []\nfor i in soundsDict.keys():\n    if 'guitar' in i:\n        rock.append(i)\n        print(i, len(soundsDict[i]))\n\n# print('bass guitar: ', soundsDict['Bass_guitar'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 35688e71.wav\nfrom skimage.transform import rescale, resize, downscale_local_mean\nAcoustic = []\nElectric = []\n# 'Acoustic_guitar,Strum', \nfor label in ['Bass_guitar']:\n     for wav in soundsDict[label]:\n            \n#           Loading wav file to display and play\n#             rate, playable = wavfile.read('/kaggle/working/'+wav)\n#             print('label: ', label, ' wav ', wav)\n#             IPython.display.display(IPython.display.Audio(playable, rate=rate))\n            \n#           Librosa load audio floating point data\n            audio, sr = librosa.load('/kaggle/working/'+wav)\n            print(np.shape(audio))\n        \n#             D = np.abs(librosa.stft(audio))**2 sr=sr, S=D)\n            spectrogram = librosa.feature.melspectrogram(y=audio, \n                sr=conf.sampling_rate,\n                n_mels=conf.n_mels,\n                hop_length=conf.hop_length,\n                n_fft=conf.n_fft,\n                fmin=conf.fmin,\n                fmax=conf.fmax)\n    \n#           Crops the spectrogram to be around 2 seconds in length and square in size\n#             print(np.shape(spectrogram))\n            spectrogram = spectrogram[ :, 0:128]\n            if np.shape(spectrogram) == (128,128):\n                spectrogram = librosa.power_to_db(spectrogram)\n#                 spectrogram = np.absolute(spectrogram)\n                plt.title(label)\n                plt.imshow(spectrogram, interpolation='nearest')\n                plt.show()\n#               downscale spec to 32*32\n                spectrogram = rescale(spectrogram, 0.25, anti_aliasing=False)\n                plt.imshow(spectrogram, interpolation='nearest')\n                plt.show()\n            \n#                 image_rescaled = rescale(spectrogram, 4, anti_aliasing=False)\n#                 plt.imshow(image_rescaled, interpolation='nearest')\n#                 plt.show()\n                \n                if label == 'Acoustic_guitar,Strum':\n#                     print(spectrogram[0][0])\n                    Acoustic.append(spectrogram)\n                else:\n                    Electric.append(spectrogram)\n\n#                 spectrogram = librosa.db_to_power(spectrogram)\n            else:\n                print('trash')\n            \n            \n#             backconvert = librosa.feature.inverse.mel_to_audio(spectrogram,\n#                 sr=conf.sampling_rate,\n#                 n_fft=conf.n_fft,\n#                 hop_length=conf.hop_length,\n#                 pad_mode=conf.padmode,\n#                 fmin=conf.fmin,\n#                 fmax=conf.fmax)\n#             print('converted back')\n#             IPython.display.display(IPython.display.Audio(backconvert, rate=rate))\n                                                                          \n        \n            \n#     for c, i in enumerate(os.listdir(\"/kaggle/working\")):\n#         if fnmatch.fnmatch(i, '*.wav'):\n#             rate, real = wavfile.read('/kaggle/working/'+i)\n#             print(i)\n#             IPython.display.display(IPython.display.Audio(real, rate=rate))\n#             audio, sr = librosa.load('/kaggle/working/'+i)\n#         #     D = np.abs(librosa.stft(audio))**2 sr=sr, S=D)\n#             spectrogram = librosa.feature.melspectrogram(y=audio, \n#                 sr=conf.sampling_rate,\n#                 n_mels=conf.n_mels,\n#                 hop_length=conf.hop_length,\n#                 n_fft=conf.n_fft,\n#                 fmin=conf.fmin,\n#                 fmax=conf.fmax)\n\n#             print(np.shape(spectrogram))\n#             spectrogram = librosa.power_to_db(spectrogram)\n#             spectrogram = spectrogram.astype(np.float32)\n\n#             plt.imshow(spectrogram, interpolation='nearest')\n#             plt.show()\n\n#             spectrogram = librosa.db_to_power(spectrogram)\n\n#             plt.imshow(spectrogram, interpolation='nearest')\n#             plt.show()\n\n#             backconvert = librosa.feature.inverse.mel_to_audio(spectrogram,\n#                 sr=conf.sampling_rate,\n#                 n_fft=conf.n_fft,\n#                 hop_length=conf.hop_length,\n#                 pad_mode=conf.padmode,\n#                 fmin=conf.fmin,\n#                 fmax=conf.fmax)\n#             print('converted back')\n#             IPython.display.display(IPython.display.Audio(backconvert, rate=rate))\n\n#             if c == 2:\n#                 break\n\n\n# rate, real = wavfile.read('/kaggle/input/freesound-audio-tagging-2019/train_noisy/35688e71.wav')\n# IPython.display.Audio(data=real, rate=rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Acoustic=np.asarray(Acoustic)\nElectric=np.asarray(Electric)\nAcoustic = np.expand_dims(Acoustic, axis=3)\nAcoustic = np.expand_dims(Acoustic, axis=3)\nprint(np.shape(Acoustic))\n\n# print(np.shape(Electric))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generator**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras import layers\n\n# latent_dim = 10\n# height = 128\n# width = 128\n# channels = 1\n\n# generator_input = keras.Input(shape=(latent_dim,))\n\n# # First, transform the input into a 16x16 128-channels feature map\n# x = layers.Dense(40 * 64 * 64)(generator_input)\n# x = layers.LeakyReLU()(x)\n# x = layers.Reshape((64, 64, 40))(x)\n\n# # Then, add a convolution layer\n# x = layers.Conv2D(4096, 5, padding='same')(x)\n# x = layers.LeakyReLU()(x)\n\n# # Upsample to 32x32\n# x = layers.Conv2DTranspose(4096, 4, strides=2, padding='same')(x)\n# x = layers.LeakyReLU()(x)\n\n# # Few more conv layers\n# x = layers.Conv2D(4096, 5, padding='same')(x)\n# x = layers.LeakyReLU()(x)\n# x = layers.Conv2D(4096, 5, padding='same')(x)\n# x = layers.LeakyReLU()(x)\n\nlatent_dim = 32\nheight = 32\nwidth = 32\nchannels = 1\n\ngenerator_input = keras.Input(shape=(latent_dim,))\n\n# First, transform the input into a 16x16 128-channels feature map\nx = layers.Dense(128 * 16 * 16)(generator_input)\nx = layers.LeakyReLU()(x)\nx = layers.Reshape((16, 16, 128))(x)\n\n# Then, add a convolution layer\nx = layers.Conv2D(256, 5, padding='same')(x)\nx = layers.LeakyReLU()(x)\n\n# Upsample to 32x32\nx = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\nx = layers.LeakyReLU()(x)\n\n# Few more conv layers\nx = layers.Conv2D(256, 5, padding='same')(x)\nx = layers.LeakyReLU()(x)\nx = layers.Conv2D(256, 5, padding='same')(x)\nx = layers.LeakyReLU()(x)\n\n# Produce a 32x32 1-channel feature map\nx = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\ngenerator = keras.models.Model(generator_input, x)\ngenerator.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Discriminator**"},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator_input = layers.Input(shape=(height, width, channels))\nx = layers.Conv2D(128, 3)(discriminator_input)\nx = layers.LeakyReLU()(x)\nx = layers.Conv2D(128, 4, strides=2)(x)\nx = layers.LeakyReLU()(x)\nx = layers.Conv2D(128, 4, strides=2)(x)\nx = layers.LeakyReLU()(x)\nx = layers.Conv2D(128, 4, strides=2)(x)\nx = layers.LeakyReLU()(x)\nx = layers.Flatten()(x)\n# discriminator_input = layers.Input(shape=(height, width, channels))\n# x = layers.Conv2D(4096, 1)(discriminator_input)\n# x = layers.LeakyReLU()(x)\n# x = layers.Conv2D(4096, 4, strides=2)(x)\n# x = layers.LeakyReLU()(x)\n# x = layers.Conv2D(4096, 4, strides=2)(x)\n# x = layers.LeakyReLU()(x)\n# x = layers.Conv2D(4096, 4, strides=2)(x)\n# x = layers.LeakyReLU()(x)\n# x = layers.Flatten()(x)\n\n# One dropout layer - important trick!\nx = layers.Dropout(0.8)(x)\n\n# Classification layer\nx = layers.Dense(1, activation='sigmoid')(x)\n\ndiscriminator = keras.models.Model(discriminator_input, x)\ndiscriminator.summary()\n\n# To stabilize training, we use learning rate decay\n# and gradient clipping (by value) in the optimizer.\ndiscriminator_optimizer = keras.optimizers.RMSprop(lr=0.0001, clipvalue=1.0, decay=1e-8)\ndiscriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set discriminator weights to non-trainable\n# (will only apply to the `gan` model)\ndiscriminator.trainable = False\n\ngan_input = keras.Input(shape=(latent_dim,))\ngan_output = discriminator(generator(gan_input))\ngan = keras.models.Model(gan_input, gan_output)\n\ngan_optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\ngan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing import image\n\n# Load CIFAR10 data\n# (x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\n\n# Select frog images (class 6) x_train[y_train.flatten() == 6]\nx_train = Electric\nprint(np.shape(x_train))\n# Normalize data\nx_train =x_train.reshape(\n    (x_train.shape[0],) + (height, width, channels)).astype('float32') / 255.\nprint(np.shape(x_train))\n\niterations = 2000\nbatch_size = 20\nsave_dir = '/kaggle/working/'\n\n# Start training loop\nstart = 0\nfor step in range(iterations):\n    # Sample random points in the latent space\n    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n\n    # Decode them to fake images\n    generated_images = generator.predict(random_latent_vectors)\n\n    # Combine them with real images\n    stop = start + batch_size\n    real_images = x_train[start: stop]\n    combined_images = np.concatenate([generated_images, real_images])\n\n    # Assemble labels discriminating real from fake images\n    labels = np.concatenate([np.ones((batch_size, 1)),\n                             np.zeros((batch_size, 1))])\n    # Add random noise to the labels - important trick!\n    labels += 0.05 * np.random.random(labels.shape)\n\n    # Train the discriminator\n    d_loss = discriminator.train_on_batch(combined_images, labels)\n\n    # sample random points in the latent space\n    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n\n    # Assemble labels that say \"all real images\"\n    misleading_targets = np.zeros((batch_size, 1))\n\n    # Train the generator (via the gan model,\n    # where the discriminator weights are frozen)\n    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n    \n    start += batch_size\n    if start > len(x_train) - batch_size:\n      start = 0\n\n    # Occasionally save / plot\n    if step % 100 == 0:\n        # Save model weights\n        gan.save_weights('gan.h5')\n\n        # Print metrics\n        print('discriminator loss at step %s: %s' % (step, d_loss))\n        print('adversarial loss at step %s: %s' % (step, a_loss))\n\n        # Save one generated image\n        img = image.array_to_img(generated_images[0] * 255., scale=False)\n        img.save(os.path.join(save_dir, 'generated_frog' + str(step) + '.png'))\n\n        # Save one real image, for comparison\n        img = image.array_to_img(real_images[0] * 255., scale=False)\n        img.save(os.path.join(save_dir, 'real_frog' + str(step) + '.png'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Sample random points in the latent space\nrandom_latent_vectors = np.random.normal(size=(10, latent_dim))\n\n# Decode them to fake images\ngenerated_images = generator.predict(random_latent_vectors)\n\nfor i in range(generated_images.shape[0]):\n\n    test = np.squeeze(generated_images[i])\n    plt.imshow(test, interpolation='nearest')\n    plt.show()\n    \n    image_rescaled = rescale(test, 4, anti_aliasing=False)\n    print(np.shape(image_rescaled))\n    plt.imshow(image_rescaled, interpolation='nearest')\n    plt.show()\n    \n    backconvert = librosa.feature.inverse.mel_to_audio(image_rescaled,\n        sr=conf.sampling_rate,\n        n_fft=conf.n_fft,\n        hop_length=conf.hop_length,\n        pad_mode=conf.padmode,\n        fmin=conf.fmin,\n        fmax=conf.fmax)\n    print('converted back')\n    IPython.display.display(IPython.display.Audio(backconvert, rate=conf.sampling_rate))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.system('rm -rf /kaggle/working')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}