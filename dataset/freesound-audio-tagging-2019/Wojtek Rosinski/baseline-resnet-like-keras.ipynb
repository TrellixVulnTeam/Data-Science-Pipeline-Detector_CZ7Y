{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import copy\nimport gc\nimport glob\nimport os\nimport time\n\nimport cv2\nimport IPython\nimport IPython.display\nimport joblib\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy\nfrom joblib import Parallel, delayed\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom tqdm import tqdm\n\n%matplotlib inline","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_columns = 128\npd.options.display.max_rows = 128\nplt.rcParams['figure.figsize'] = (15, 8)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class EasyDict(dict):\n    def __init__(self, d=None, **kwargs):\n        if d is None:\n            d = {}\n        if kwargs:\n            d.update(**kwargs)\n        for k, v in d.items():\n            setattr(self, k, v)\n        # Class attributes\n        for k in self.__class__.__dict__.keys():\n            if not (k.startswith('__') and k.endswith('__')) and not k in ('update', 'pop'):\n                setattr(self, k, getattr(self, k))\n\n    def __setattr__(self, name, value):\n        if isinstance(value, (list, tuple)):\n            value = [self.__class__(x)\n                     if isinstance(x, dict) else x for x in value]\n        elif isinstance(value, dict) and not isinstance(value, self.__class__):\n            value = self.__class__(value)\n        super(EasyDict, self).__setattr__(name, value)\n        super(EasyDict, self).__setitem__(name, value)\n\n    __setitem__ = __setattr__\n\n    def update(self, e=None, **f):\n        d = e or dict()\n        d.update(f)\n        for k in d:\n            setattr(self, k, d[k])\n\n    def pop(self, k, d=None):\n        delattr(self, k)\n        return super(EasyDict, self).pop(k, d)","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Configuration and global parameters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train_curated.csv')\nsample_submission = pd.read_csv('../input/sample_submission.csv')\nprint('train: {}'.format(train_df.shape))\nprint('test: {}'.format(sample_submission.shape))\n\nROOT = '../input/'\ntest_root = os.path.join(ROOT, 'test/')\ntrain_root = os.path.join(ROOT, 'train_curated/')\n\n\nCONFIG = EasyDict()\nCONFIG.hop_length = 347 # to make time steps 128\nCONFIG.fmin = 20\nCONFIG.fmax = 44100 / 2\nCONFIG.n_fft = 480\n\nN_SAMPLES = 48\nSAMPLE_DIM = 256\n\nTRAINING_CONFIG = {\n    'sample_dim': (N_SAMPLES, SAMPLE_DIM),\n    'padding_mode': cv2.BORDER_REFLECT,\n}\n\nprint(CONFIG)\nprint(TRAINING_CONFIG)\n\ntrain_df.head()","execution_count":4,"outputs":[{"output_type":"stream","text":"train: (4970, 2)\ntest: (1120, 81)\n{'hop_length': 347, 'fmin': 20, 'fmax': 22050.0, 'n_fft': 480}\n{'sample_dim': (48, 256), 'padding_mode': 2}\n","name":"stdout"},{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"          fname           labels\n0  0006ae4e.wav             Bark\n1  0019ef41.wav         Raindrop\n2  001ec0ad.wav  Finger_snapping\n3  0026c7cb.wav              Run\n4  0026f116.wav  Finger_snapping","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fname</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0006ae4e.wav</td>\n      <td>Bark</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0019ef41.wav</td>\n      <td>Raindrop</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001ec0ad.wav</td>\n      <td>Finger_snapping</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0026c7cb.wav</td>\n      <td>Run</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0026f116.wav</td>\n      <td>Finger_snapping</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Data Processing class:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing functions inspired by:\n# https://github.com/xiaozhouwang/tensorflow_speech_recognition_solution/blob/master/data.py\nclass DataProcessor(object):\n    \n    def __init__(self, debug=False):\n        self.debug = debug\n        \n        # Placeholders for global statistics\n        self.mel_mean = None\n        self.mel_std = None\n        self.mel_max = None\n        self.mfcc_max = None\n        \n    def createMel(self, filename, params, normalize=False):\n        \"\"\"\n        Create Mel Spectrogram sample out of raw wavfile\n        \"\"\"\n        y, sr = librosa.load(filename, sr=None)\n        mel = librosa.feature.melspectrogram(y, sr, n_mels=N_SAMPLES, **params)\n        mel = librosa.power_to_db(mel)\n        if normalize:\n            if self.mel_mean is not None and self.mel_std is not None:\n                mel = (mel - self.mel_mean) / self.mel_std\n            else:\n                sample_mean = np.mean(mel)\n                sample_std = np.std(mel)\n                mel = (mel - sample_mean) / sample_std\n            if self.mel_max is not None:\n                mel = mel / self.mel_max\n            else:\n                mel = mel / np.max(np.abs(mel))\n        return mel\n    \n    def createMfcc(self, filename, params, normalize=False):\n        \"\"\"\n        Create MFCC sample out of raw wavfile\n        \"\"\"\n        y, sr = librosa.load(filename, sr=None)\n        nonzero_idx = [y > 0]\n        y[nonzero_idx] = np.log(y[nonzero_idx])\n        mfcc = librosa.feature.mfcc(y, sr, n_mfcc=N_SAMPLES, **params)\n        if normalize:\n            if self.mfcc_max is not None:\n                mfcc = mfcc / self.mfcc_max\n            else:\n                mfcc = mfcc / np.max(np.abs(mfcc))\n        return mfcc\n    \n    def createLogspec(self, filename, params,\n                      normalize=False,\n                      window_size=20,\n                      step_size=10, eps=1e-10):\n        \"\"\"\n        Create log spectrogram,\n        based on \n        https://www.kaggle.com/voglinio/keras-2d-model-5-fold-log-specgram-curated-only\n        \"\"\"\n        \n        y, sr = librosa.load(filename, sr=None)\n        nperseg = int(round(window_size * sr / 1e3))\n        noverlap = int(round(step_size * sr / 1e3))\n        freqs, times, spec = scipy.signal.spectrogram(\n            y,\n            fs=sr,\n            window='hann',\n            nperseg=nperseg,\n            noverlap=noverlap,\n            detrend=False)\n        spec = np.log(spec.astype(np.float32) + eps)\n        return spec\n        \n    \n    def prepareSample(self, root, row, \n                      preprocFunc, \n                      preprocParams, trainingParams, \n                      test_mode=False, normalize=False, \n                      proc_mode='split'):\n        \"\"\"\n        Prepare sample for model training.\n        Function takes row of DataFrame, extracts filename and labels and processes them.\n        \n        If proc_mode is 'split':\n        Outputs sets of arrays of constant shape padded to TRAINING_CONFIG shape\n        with selected padding mode, also specified in TRAINING_CONFIG.\n        This approach prevents loss of information caused by trimming the audio sample,\n        instead it splits it into equally-sized parts and pads them.\n        To account for creation of multiple samples, number of labels are multiplied to a number\n        equal to number of created samples.\n        \n        If proc_mode is 'resize':\n        Resizes the original processed sample to (SAMPLE_DIM, N_SAMPLES) shape.\n        \"\"\"\n        \n        assert proc_mode in ['split', 'resize'], 'proc_must be one of split or resize'\n        \n        filename = os.path.join(root, row['fname'])\n        if not test_mode:\n            labels = row['labels']\n            \n        sample = preprocFunc(filename, preprocParams, normalize=normalize)\n        # print(sample.min(), sample.max())\n        \n        if proc_mode == 'split':\n            sample_split = np.array_split(\n                sample, np.ceil(sample.shape[1] / SAMPLE_DIM), axis=1)\n            samples_pad = []\n            for i in sample_split:\n                padding_dim = SAMPLE_DIM - i.shape[1]\n                sample_pad = cv2.copyMakeBorder(i, 0, 0, 0, padding_dim, trainingParams['padding_mode'])\n                samples_pad.append(sample_pad)\n            samples_pad = np.asarray(samples_pad)\n            if not test_mode:\n                labels = [labels] * len(samples_pad)\n                labels = np.asarray(labels)\n                return samples_pad, labels\n            return samples_pad\n        elif proc_mode == 'resize':\n            sample_pad = cv2.resize(sample, (SAMPLE_DIM, N_SAMPLES), interpolation=cv2.INTER_NEAREST)\n            sample_pad = np.expand_dims(sample_pad, axis=0)\n            if not test_mode:\n                labels = np.asarray(labels)\n                return sample_pad, labels\n            return sample_pad\n        elif proc_mode == 'raw':\n            if not test_model:\n                return sample, labels\n            return sample\n\n\nprocessor = DataProcessor()","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PREFIX = 'proc'\ntrain_filename = 'train_curated_{}.joblib'.format(DATA_PREFIX)\ntest_filename = 'test_{}.joblib'.format(DATA_PREFIX)\n\n\n# Train processing/loading:\nif os.path.isfile(train_filename):\n    print('load processed train:')\n    train_dict = joblib.load(train_filename)\n    X_train = train_dict['X']\n    y_train = train_dict['y']\n    print(X_train.shape, y_train.shape)\nelse:\n    print('process train...')\n    output = Parallel(n_jobs=-3, verbose=1)(\n        delayed(processor.prepareSample)(\n            train_root, \n            train_df.iloc[f, :],\n            processor.createLogspec,\n            CONFIG,\n            TRAINING_CONFIG,\n            test_mode=False,\n            proc_mode='resize',\n        ) for f in range(train_df.shape[0]))  # change to number of sample in train data for full processing\n    X_train = np.array([x[0] for x in output])\n    y_train = np.array([x[1] for x in output])\n    y_train = pd.Series(y_train).str.get_dummies(sep=',')\n    print(X_train.shape, y_train.shape)\n    # Save output for quicker experiments\n    train_dict = {\n        'X': X_train,\n        'y': y_train,\n    }\n    joblib.dump(train_dict, train_filename)\n    \n\n# Test processing/loading:\nif os.path.isfile(test_filename):\n    print('load processed test:')\n    test_dict = joblib.load(test_filename)\n    X_test = test_dict['X']\n    print(X_test.shape)\nelse:\n    print('process test...')\n    X_test = Parallel(n_jobs=-3, verbose=1)(\n        delayed(processor.prepareSample)(\n            test_root, \n            sample_submission.iloc[f, :],\n            processor.createLogspec,\n            CONFIG,\n            TRAINING_CONFIG,\n            test_mode=True,\n            proc_mode='resize',\n        ) for f in range(sample_submission.shape[0]))  # change to number of sample in test data for full processing\n    X_test = np.array(X_test)\n    print(X_test.shape)\n    test_dict = {\n        'X': X_test,\n    }\n    joblib.dump(test_dict, test_filename)\n    \n    \n# Switch channel axis from 2nd to last\nX_train = np.moveaxis(X_train, 1, -1)\nX_test = np.moveaxis(X_test, 1, -1)\nprint(X_train.shape, y_train.shape)","execution_count":6,"outputs":[{"output_type":"stream","text":"process train...\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=-3)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=-3)]: Done 4970 out of 4970 | elapsed:  3.0min finished\n","name":"stderr"},{"output_type":"stream","text":"(4970, 1, 48, 256) (4970, 80)\nprocess test...\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=-3)]: Using backend SequentialBackend with 1 concurrent workers.\n","name":"stderr"},{"output_type":"stream","text":"(1120, 1, 48, 256)\n(4970, 48, 256, 1) (4970, 80)\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=-3)]: Done 1120 out of 1120 | elapsed:   53.3s finished\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of multilabel labels\nprint('Multilabel class distribution:')\nprint(y_train.sum(axis=1).value_counts())\n\n# Most of the samples belong to only one class.\n# There are some (around 15%), which belong to two classes.\n# Occurrence of samples belonging to more than two classes at once\n# is quite rare, around 1.5%.","execution_count":7,"outputs":[{"output_type":"stream","text":"Multilabel class distribution:\n1    4269\n2     627\n3      69\n4       4\n6       1\ndtype: int64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nfrom pprint import pprint\n\ndf_lab = (y_train.loc[y_train.sum(axis=1) > 1] > 0)\nmultilabel_combs = []\nfor i in range(df_lab.shape[0]):\n    row_label = df_lab.iloc[i, :][df_lab.iloc[i, :] > 0].index.tolist()\n    multilabel_combs.append(row_label)\n\nmultilabel_comb_counter = Counter(list(map(lambda x: ' + '.join(x), multilabel_combs)))\npprint(multilabel_comb_counter.most_common(20))\n# 20 most common combinations of labels","execution_count":8,"outputs":[{"output_type":"stream","text":"[('Acoustic_guitar + Strum', 69),\n ('Drip + Raindrop', 52),\n ('Sink_(filling_or_washing) + Water_tap_and_faucet', 46),\n ('Applause + Cheering + Crowd', 36),\n ('Cutlery_and_silverware + Dishes_and_pots_and_pans', 29),\n ('Chink_and_clink + Dishes_and_pots_and_pans', 27),\n ('Accelerating_and_revving_and_vroom + Race_car_and_auto_racing', 26),\n ('Female_speech_and_woman_speaking + Yell', 22),\n ('Slam + Squeak', 21),\n ('Screaming + Yell', 20),\n ('Applause + Cheering', 15),\n ('Meow + Purr', 13),\n ('Accelerating_and_revving_and_vroom + Motorcycle', 12),\n ('Squeak + Walk_and_footsteps', 12),\n ('Car_passing_by + Traffic_noise_and_roadway_noise', 10),\n ('Applause + Crowd', 10),\n ('Bass_guitar + Electric_guitar', 10),\n ('Gurgling + Toilet_flush', 10),\n ('Female_singing + Male_singing', 9),\n ('Cheering + Crowd', 8)]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport sklearn.metrics\n\n\n# Based on https://www.kaggle.com/voglinio/keras-2d-model-5-fold-log-specgram-curated-only\n# Core calculation of label precisions for one test sample.\ndef _one_sample_positive_class_precisions(scores, truth):\n    \"\"\"Calculate precisions for each true class for a single sample.\n\n    Args:\n      scores: np.array of (num_classes,) giving the individual classifier scores.\n      truth: np.array of (num_classes,) bools indicating which classes are true.\n\n    Returns:\n      pos_class_indices: np.array of indices of the true classes for this sample.\n      pos_class_precisions: np.array of precisions corresponding to each of those\n        classes.\n    \"\"\"\n    num_classes = scores.shape[0]\n    pos_class_indices = np.flatnonzero(truth > 0)\n    # Only calculate precisions if there are some true classes.\n    if not len(pos_class_indices):\n        return pos_class_indices, np.zeros(0)\n    # Retrieval list of classes for this sample.\n    retrieved_classes = np.argsort(scores)[::-1]\n    # class_rankings[top_scoring_class_index] == 0 etc.\n    class_rankings = np.zeros(num_classes, dtype=np.int)\n    class_rankings[retrieved_classes] = range(num_classes)\n    # Which of these is a true label?\n    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n    retrieved_class_true[class_rankings[pos_class_indices]] = True\n    # Num hits for every truncated retrieval list.\n    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n    # Precision of retrieval list truncated at each hit, in order of pos_labels.\n    precision_at_hits = (\n        retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n        (1 + class_rankings[pos_class_indices].astype(np.float)))\n    return pos_class_indices, precision_at_hits\n\n# All-in-one calculation of per-class lwlrap.\n\n\ndef calculate_per_class_lwlrap(truth, scores):\n    \"\"\"Calculate label-weighted label-ranking average precision.\n\n    Arguments:\n      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n        of presence of that class in that sample.\n      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n        test's real-valued score for each class for each sample.\n\n    Returns:\n      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n        class.\n      weight_per_class: np.array of (num_classes,) giving the prior of each\n        class within the truth labels.  Then the overall unbalanced lwlrap is\n        simply np.sum(per_class_lwlrap * weight_per_class)\n    \"\"\"\n    assert truth.shape == scores.shape\n    num_samples, num_classes = scores.shape\n    # Space to store a distinct precision value for each class on each sample.\n    # Only the classes that are true for each sample will be filled in.\n    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n    for sample_num in range(num_samples):\n        pos_class_indices, precision_at_hits = (\n            _one_sample_positive_class_precisions(scores[sample_num, :],\n                                                  truth[sample_num, :]))\n        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n            precision_at_hits)\n    labels_per_class = np.sum(truth > 0, axis=0)\n    weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n    # Form average of each column, i.e. all the precisions assigned to labels in\n    # a particular class.\n    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n                        np.maximum(1, labels_per_class))\n    # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n    #                = np.sum(precisions_for_samples_by_classes) / np.sum(precisions_for_samples_by_classes > 0)\n    #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n    #                = np.sum(per_class_lwlrap * weight_per_class)\n    return per_class_lwlrap, weight_per_class\n\n# Calculate the overall lwlrap using sklearn.metrics function.\n\n\ndef calculate_overall_lwlrap_sklearn(truth, scores):\n    \"\"\"Calculate the overall lwlrap using sklearn.metrics.lrap.\"\"\"\n    # sklearn doesn't correctly apply weighting to samples with no labels, so just skip them.\n    sample_weight = np.sum(truth > 0, axis=1)\n    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n    overall_lwlrap = sklearn.metrics.label_ranking_average_precision_score(\n        truth[nonzero_weight_sample_indices, :] > 0,\n        scores[nonzero_weight_sample_indices, :],\n        sample_weight=sample_weight[nonzero_weight_sample_indices])\n    return overall_lwlrap\n\n\n# Accumulator object version.\n\nclass lwlrap_accumulator(object):\n    \"\"\"Accumulate batches of test samples into per-class and overall lwlrap.\"\"\"\n\n    def __init__(self):\n        self.num_classes = 0\n        self.total_num_samples = 0\n\n    def accumulate_samples(self, batch_truth, batch_scores):\n        \"\"\"Cumulate a new batch of samples into the metric.\n\n        Args:\n          truth: np.array of (num_samples, num_classes) giving boolean\n            ground-truth of presence of that class in that sample for this batch.\n          scores: np.array of (num_samples, num_classes) giving the\n            classifier-under-test's real-valued score for each class for each\n            sample.\n        \"\"\"\n        assert batch_scores.shape == batch_truth.shape\n        num_samples, num_classes = batch_truth.shape\n        if not self.num_classes:\n            self.num_classes = num_classes\n            self._per_class_cumulative_precision = np.zeros(self.num_classes)\n            self._per_class_cumulative_count = np.zeros(self.num_classes,\n                                                        dtype=np.int)\n        assert num_classes == self.num_classes\n        for truth, scores in zip(batch_truth, batch_scores):\n            pos_class_indices, precision_at_hits = (\n                _one_sample_positive_class_precisions(scores, truth))\n            self._per_class_cumulative_precision[pos_class_indices] += (\n                precision_at_hits)\n            self._per_class_cumulative_count[pos_class_indices] += 1\n        self.total_num_samples += num_samples\n\n    def per_class_lwlrap(self):\n        \"\"\"Return a vector of the per-class lwlraps for the accumulated samples.\"\"\"\n        return (self._per_class_cumulative_precision /\n                np.maximum(1, self._per_class_cumulative_count))\n\n    def per_class_weight(self):\n        \"\"\"Return a normalized weight vector for the contributions of each class.\"\"\"\n        return (self._per_class_cumulative_count /\n                float(np.sum(self._per_class_cumulative_count)))\n\n    def overall_lwlrap(self):\n        \"\"\"Return the scalar overall lwlrap for cumulated samples.\"\"\"\n        return np.sum(self.per_class_lwlrap() * self.per_class_weight())","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import tensorflow.keras as keras\n# from tensorflow.keras import layers\n# from tensorflow.keras.callbacks import *\n# from tensorflow.keras.layers import *\n# from tensorflow.keras.optimizers import *\n# from tensorflow.keras.losses import *\n# from tensorflow.keras.models import Model\n\nfrom keras import layers\nfrom keras.callbacks import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras.losses import *\nfrom keras.models import Model\n\n\n# Based on https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet_common.py\ndef block1(x, filters, kernel_size=3, stride=1,\n           conv_shortcut=True, name=None):\n    \"\"\"A residual block.\n    # Arguments\n        x: input tensor.\n        filters: integer, filters of the bottleneck layer.\n        kernel_size: default 3, kernel size of the bottleneck layer.\n        stride: default 1, stride of the first layer.\n        conv_shortcut: default True, use convolution shortcut if True,\n            otherwise identity shortcut.\n        name: string, block label.\n    # Returns\n        Output tensor for the residual block.\n    \"\"\"\n    bn_axis = 3\n\n    if conv_shortcut is True:\n        shortcut = layers.Conv2D(4 * filters, 1, strides=stride,\n                                 name=name + '_0_conv')(x)\n        shortcut = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n                                             name=name + '_0_bn')(shortcut)\n    else:\n        shortcut = x\n\n    x = layers.Conv2D(filters, 1, strides=stride, name=name + '_1_conv')(x)\n    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n                                  name=name + '_1_bn')(x)\n    x = layers.PReLU(name=name + '_1_prelu')(x)\n\n    x = layers.Conv2D(filters, kernel_size, padding='SAME',\n                      name=name + '_2_conv')(x)\n    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n                                  name=name + '_2_bn')(x)\n    x = layers.PReLU(name=name + '_2_prelu')(x)\n\n    x = layers.Conv2D(4 * filters, 1, name=name + '_3_conv')(x)\n    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n                                  name=name + '_3_bn')(x)\n\n    x = layers.Add(name=name + '_add')([shortcut, x])\n    x = layers.PReLU(name=name + '_out')(x)\n    \n    return x\n\n\ndef stack1(x, filters, blocks, stride1=2, name=None):\n    \"\"\"A set of stacked residual blocks.\n    # Arguments\n        x: input tensor.\n        filters: integer, filters of the bottleneck layer in a block.\n        blocks: integer, blocks in the stacked blocks.\n        stride1: default 2, stride of the first layer in the first block.\n        name: string, stack label.\n    # Returns\n        Output tensor for the stacked blocks.\n    \"\"\"\n    x = block1(x, filters, stride=stride1, name=name + '_block1')\n    for i in range(2, blocks + 1):\n        x = block1(x, filters, conv_shortcut=False, name=name + '_block' + str(i))\n    return x\n\n\ndef ResNetlike(input_shape, num_classes):\n    \n    use_bias = False\n    num_blocks = 1\n    strides = (2, 4)\n    \n    input_layer = layers.Input(input_shape)\n    # x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)), name='conv1_pad')(input_layer)\n    x = layers.Conv2D(64, (3, 7), strides=2, use_bias=use_bias, name='conv1_conv')(input_layer)\n    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n                                      name='conv1_bn')(x)\n    x = layers.PReLU(name='conv1_prelu')(x)\n    \n    x = stack1(x, 64, num_blocks, stride1=1, name='conv2')\n    x = stack1(x, 64, num_blocks, stride1=strides, name='conv3')\n    x = stack1(x, 64, num_blocks, stride1=strides, name='conv4')\n    \n    x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n    x = layers.Dense(num_classes, activation='softmax', name='probs')(x)\n    \n    model = Model(input_layer, x, name='resnetlike')\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":10,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n\n\n# Inspired by https://www.kaggle.com/yekenot/pooled-gru-fasttext\nclass LwlRapEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = calculate_overall_lwlrap_sklearn(self.y_val, y_pred)\n            print(\"\\n LWLRAP - epoch: {} - score: {:.4f} \\n\".format(epoch +1, score))\n        return\n\n\n# Based on https://www.kaggle.com/voglinio/keras-2d-model-5-fold-log-specgram-curated-only\ndef create_unique_labels(all_labels):\n    label_dict = {}\n    all_labels_set = []\n    first_labels_set = []\n    for labs in all_labels:\n        lab = labs.split(',')\n        for l in lab:\n            if l in label_dict:\n                label_dict[l] = label_dict[l]  + 1\n            else:\n                label_dict[l]= 0\n\n        all_labels_set.append(set(lab))\n        first_labels_set.append(lab[0])\n    classes = list(label_dict.keys())\n    \n    return label_dict, classes, all_labels_set, first_labels_set\n\n\nlabel_dict, classes, all_labels_set, first_labels_set = create_unique_labels(train_df.labels)\nbinarize = MultiLabelBinarizer(classes=classes)\ny_cat = binarize.fit_transform(all_labels_set)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RUN_NAME = 'ResNetLike_mel'\nBATCH_SIZE = 32\nNUM_EPOCHS = 35\nNFOLDS = 5\nSEED = 1337\nLOAD_MODEL = False\nTRAIN_MODEL = True\nDEBUG = False\nSAVE_SUBMISSION = True\n\n\n# y_cat for categorical_crossentropy\n# y_train.values for binary_crossentropy\nlabels_set = y_cat   \nkfold = KFold(n_splits=NFOLDS, random_state=SEED)\nnum_classes = 80\nbn_axis = 3\n\noof_train = np.zeros((X_train.shape[0], num_classes))\noof_test = np.zeros((X_test.shape[0], num_classes, NFOLDS))\nvalid_scores = []\n\n\n# KFold training\nfor fold_idx, (train_idx, valid_idx) in enumerate(kfold.split(X_train)):\n    X_tr, y_tr = X_train[train_idx], labels_set[train_idx]\n    X_val, y_val = X_train[valid_idx], labels_set[valid_idx]\n    print(X_tr.shape, y_tr.shape)\n    print(X_val.shape, y_val.shape)\n    \n    checkpoint_name = '{}_{}.h5'.format(RUN_NAME, fold_idx)\n    lwl_callback = LwlRapEvaluation(validation_data=(X_val, y_val))\n    \n    callbacks_list = [\n        ModelCheckpoint(checkpoint_name, save_best_only=True, save_weights_only=True),\n        ReduceLROnPlateau(patience=5, factor=0.2),\n        EarlyStopping(patience=8, monitor='val_loss', restore_best_weights=True),\n        lwl_callback,\n    ]\n    \n    model = ResNetlike(X_train.shape[1:], y_train.shape[-1])\n    # model = BaselineModel(X_train.shape[1:])\n    # model.summary()\n    if LOAD_MODEL:\n        print('loading: {}'.format(checkpoint_name))\n        model.load_weights(checkpoint_name)\n    if TRAIN_MODEL:\n        print('training model...')\n        model.fit(\n            X_tr, y_tr, \n            validation_data=(X_val, y_val),\n            batch_size=BATCH_SIZE,\n            epochs=NUM_EPOCHS,\n            verbose=1,\n            callbacks=callbacks_list)\n    print('loading best weights from current fold')\n    model.load_weights(checkpoint_name)\n    \n    val_pred = model.predict(X_val, batch_size=BATCH_SIZE)\n    oof_train[valid_idx, :] = val_pred\n    oof_test[:, :, fold_idx] = model.predict(X_test, batch_size=BATCH_SIZE)\n    \n    val_lwlrap = calculate_overall_lwlrap_sklearn(y_val, val_pred)\n    valid_scores.append(val_lwlrap)\n    print(\"lwlrap fold: {:.4f}\".format(val_lwlrap))\n    # break\n    \n    \noof_lwl = calculate_overall_lwlrap_sklearn(labels_set, oof_train)\nprint('OOF LWLRAP: {:.4f}'.format(oof_lwl))","execution_count":13,"outputs":[{"output_type":"stream","text":"(3976, 48, 256, 1) (3976, 80)\n(994, 48, 256, 1) (994, 80)\ntraining model...\nTrain on 3976 samples, validate on 994 samples\nEpoch 1/35\n3976/3976 [==============================] - 11s 3ms/step - loss: 4.5081 - acc: 0.0978 - val_loss: 18.0130 - val_acc: 0.0221\n\n LWLRAP - epoch: 1 - score: 0.0782 \n\nEpoch 2/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 3.6574 - acc: 0.2165 - val_loss: 13.2957 - val_acc: 0.0191\n\n LWLRAP - epoch: 2 - score: 0.0975 \n\nEpoch 3/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 3.2246 - acc: 0.2918 - val_loss: 16.9711 - val_acc: 0.0191\n\n LWLRAP - epoch: 3 - score: 0.0830 \n\nEpoch 4/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.9238 - acc: 0.3549 - val_loss: 15.0006 - val_acc: 0.0161\n\n LWLRAP - epoch: 4 - score: 0.0762 \n\nEpoch 5/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.6551 - acc: 0.4007 - val_loss: 14.5814 - val_acc: 0.0201\n\n LWLRAP - epoch: 5 - score: 0.0893 \n\nEpoch 6/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.3557 - acc: 0.4605 - val_loss: 12.7641 - val_acc: 0.0201\n\n LWLRAP - epoch: 6 - score: 0.0921 \n\nEpoch 7/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.1144 - acc: 0.5053 - val_loss: 12.1922 - val_acc: 0.0302\n\n LWLRAP - epoch: 7 - score: 0.1022 \n\nEpoch 8/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.8421 - acc: 0.5757 - val_loss: 14.1850 - val_acc: 0.0332\n\n LWLRAP - epoch: 8 - score: 0.0910 \n\nEpoch 9/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.5682 - acc: 0.6466 - val_loss: 14.2001 - val_acc: 0.0211\n\n LWLRAP - epoch: 9 - score: 0.0865 \n\nEpoch 10/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.3243 - acc: 0.7100 - val_loss: 9.8503 - val_acc: 0.0724\n\n LWLRAP - epoch: 10 - score: 0.1629 \n\nEpoch 11/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.0845 - acc: 0.7651 - val_loss: 14.5469 - val_acc: 0.0292\n\n LWLRAP - epoch: 11 - score: 0.1000 \n\nEpoch 12/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.9425 - acc: 0.8036 - val_loss: 10.9716 - val_acc: 0.0322\n\n LWLRAP - epoch: 12 - score: 0.1097 \n\nEpoch 13/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.8246 - acc: 0.8418 - val_loss: 15.0683 - val_acc: 0.0231\n\n LWLRAP - epoch: 13 - score: 0.1027 \n\nEpoch 14/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.6624 - acc: 0.8785 - val_loss: 16.1527 - val_acc: 0.0252\n\n LWLRAP - epoch: 14 - score: 0.1055 \n\nEpoch 15/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.6359 - acc: 0.8836 - val_loss: 9.9333 - val_acc: 0.0644\n\n LWLRAP - epoch: 15 - score: 0.1513 \n\nEpoch 16/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.4843 - acc: 0.9173 - val_loss: 9.1433 - val_acc: 0.1288\n\n LWLRAP - epoch: 16 - score: 0.2522 \n\nEpoch 17/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3849 - acc: 0.9248 - val_loss: 6.3827 - val_acc: 0.1942\n\n LWLRAP - epoch: 17 - score: 0.3422 \n\nEpoch 18/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3595 - acc: 0.9278 - val_loss: 8.7835 - val_acc: 0.1579\n\n LWLRAP - epoch: 18 - score: 0.2742 \n\nEpoch 19/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3464 - acc: 0.9291 - val_loss: 11.0564 - val_acc: 0.1137\n\n LWLRAP - epoch: 19 - score: 0.2394 \n\nEpoch 20/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3380 - acc: 0.9258 - val_loss: 5.5691 - val_acc: 0.2143\n\n LWLRAP - epoch: 20 - score: 0.3642 \n\nEpoch 21/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3341 - acc: 0.9243 - val_loss: 12.9984 - val_acc: 0.0755\n\n LWLRAP - epoch: 21 - score: 0.1697 \n\nEpoch 22/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3308 - acc: 0.9276 - val_loss: 9.8545 - val_acc: 0.1127\n\n LWLRAP - epoch: 22 - score: 0.2188 \n\nEpoch 23/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3268 - acc: 0.9281 - val_loss: 9.2912 - val_acc: 0.1338\n\n LWLRAP - epoch: 23 - score: 0.2553 \n\nEpoch 24/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3219 - acc: 0.9261 - val_loss: 10.8660 - val_acc: 0.1107\n\n LWLRAP - epoch: 24 - score: 0.2226 \n\nEpoch 25/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3213 - acc: 0.9283 - val_loss: 6.1751 - val_acc: 0.2133\n\n LWLRAP - epoch: 25 - score: 0.3431 \n\nEpoch 26/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3234 - acc: 0.9218 - val_loss: 8.5180 - val_acc: 0.1680\n\n LWLRAP - epoch: 26 - score: 0.2867 \n\nEpoch 27/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.2966 - acc: 0.9374 - val_loss: 4.9604 - val_acc: 0.2716\n\n LWLRAP - epoch: 27 - score: 0.4300 \n\nEpoch 28/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.2950 - acc: 0.9266 - val_loss: 6.4939 - val_acc: 0.2254\n\n LWLRAP - epoch: 28 - score: 0.3811 \n\nEpoch 29/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.2907 - acc: 0.9263 - val_loss: 9.9135 - val_acc: 0.1811\n\n LWLRAP - epoch: 29 - score: 0.3072 \n\nEpoch 30/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.2917 - acc: 0.9306 - val_loss: 8.6049 - val_acc: 0.1821\n\n LWLRAP - epoch: 30 - score: 0.3000 \n\nEpoch 31/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.2881 - acc: 0.9258 - val_loss: 6.2872 - val_acc: 0.2384\n\n LWLRAP - epoch: 31 - score: 0.3815 \n\nEpoch 32/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.2859 - acc: 0.9298 - val_loss: 8.0057 - val_acc: 0.1942\n\n LWLRAP - epoch: 32 - score: 0.3313 \n\nEpoch 33/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.2847 - acc: 0.9281 - val_loss: 3.0566 - val_acc: 0.4135\n\n LWLRAP - epoch: 33 - score: 0.5672 \n\nEpoch 34/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.2858 - acc: 0.9288 - val_loss: 3.0095 - val_acc: 0.4205\n\n LWLRAP - epoch: 34 - score: 0.5723 \n\nEpoch 35/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.2837 - acc: 0.9341 - val_loss: 3.0776 - val_acc: 0.4095\n\n LWLRAP - epoch: 35 - score: 0.5637 \n\nloading best weights from current fold\nlwlrap fold: 0.5723\n(3976, 48, 256, 1) (3976, 80)\n(994, 48, 256, 1) (994, 80)\ntraining model...\nTrain on 3976 samples, validate on 994 samples\nEpoch 1/35\n3976/3976 [==============================] - 12s 3ms/step - loss: 4.4912 - acc: 0.1099 - val_loss: 17.7345 - val_acc: 0.0131\n\n LWLRAP - epoch: 1 - score: 0.0655 \n\nEpoch 2/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 3.6820 - acc: 0.2145 - val_loss: 12.9391 - val_acc: 0.0161\n\n LWLRAP - epoch: 2 - score: 0.0869 \n\nEpoch 3/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 3.2310 - acc: 0.2938 - val_loss: 17.5431 - val_acc: 0.0131\n\n LWLRAP - epoch: 3 - score: 0.0675 \n\nEpoch 4/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.9186 - acc: 0.3461 - val_loss: 14.1680 - val_acc: 0.0171\n\n LWLRAP - epoch: 4 - score: 0.0905 \n\nEpoch 5/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.6381 - acc: 0.4115 - val_loss: 17.2281 - val_acc: 0.0111\n\n LWLRAP - epoch: 5 - score: 0.0647 \n\nEpoch 6/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.3844 - acc: 0.4590 - val_loss: 13.7789 - val_acc: 0.0161\n\n LWLRAP - epoch: 6 - score: 0.0764 \n\nEpoch 7/35\n3976/3976 [==============================] - 8s 2ms/step - loss: 2.1333 - acc: 0.5108 - val_loss: 11.3177 - val_acc: 0.0131\n\n LWLRAP - epoch: 7 - score: 0.0813 \n\nEpoch 8/35\n3976/3976 [==============================] - 8s 2ms/step - loss: 1.8822 - acc: 0.5589 - val_loss: 12.6311 - val_acc: 0.0191\n\n LWLRAP - epoch: 8 - score: 0.0889 \n\nEpoch 9/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.6305 - acc: 0.6222 - val_loss: 10.7489 - val_acc: 0.0292\n\n LWLRAP - epoch: 9 - score: 0.1000 \n\nEpoch 10/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.3915 - acc: 0.6854 - val_loss: 10.0830 - val_acc: 0.0362\n\n LWLRAP - epoch: 10 - score: 0.1095 \n\nEpoch 11/35\n","name":"stdout"},{"output_type":"stream","text":"3976/3976 [==============================] - 7s 2ms/step - loss: 1.1634 - acc: 0.7399 - val_loss: 14.8250 - val_acc: 0.0302\n\n LWLRAP - epoch: 11 - score: 0.0901 \n\nEpoch 12/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.9420 - acc: 0.8068 - val_loss: 14.0706 - val_acc: 0.0191\n\n LWLRAP - epoch: 12 - score: 0.0959 \n\nEpoch 13/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.7937 - acc: 0.8519 - val_loss: 10.9775 - val_acc: 0.0352\n\n LWLRAP - epoch: 13 - score: 0.1158 \n\nEpoch 14/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.7512 - acc: 0.8546 - val_loss: 14.6807 - val_acc: 0.0221\n\n LWLRAP - epoch: 14 - score: 0.0974 \n\nEpoch 15/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.6811 - acc: 0.8732 - val_loss: 11.3777 - val_acc: 0.0473\n\n LWLRAP - epoch: 15 - score: 0.1346 \n\nEpoch 16/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.4891 - acc: 0.9135 - val_loss: 8.4840 - val_acc: 0.1388\n\n LWLRAP - epoch: 16 - score: 0.2621 \n\nEpoch 17/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.4109 - acc: 0.9258 - val_loss: 8.4312 - val_acc: 0.1479\n\n LWLRAP - epoch: 17 - score: 0.2658 \n\nEpoch 18/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3851 - acc: 0.9245 - val_loss: 11.5984 - val_acc: 0.0734\n\n LWLRAP - epoch: 18 - score: 0.1959 \n\nEpoch 19/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3705 - acc: 0.9147 - val_loss: 8.0491 - val_acc: 0.1388\n\n LWLRAP - epoch: 19 - score: 0.2678 \n\nEpoch 20/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3624 - acc: 0.9253 - val_loss: 13.0432 - val_acc: 0.0493\n\n LWLRAP - epoch: 20 - score: 0.1563 \n\nEpoch 21/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3562 - acc: 0.9245 - val_loss: 3.9541 - val_acc: 0.3028\n\n LWLRAP - epoch: 21 - score: 0.4595 \n\nEpoch 22/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3511 - acc: 0.9258 - val_loss: 9.5933 - val_acc: 0.1408\n\n LWLRAP - epoch: 22 - score: 0.2611 \n\nEpoch 23/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3423 - acc: 0.9208 - val_loss: 8.3903 - val_acc: 0.1378\n\n LWLRAP - epoch: 23 - score: 0.2446 \n\nEpoch 24/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3449 - acc: 0.9223 - val_loss: 9.3713 - val_acc: 0.1006\n\n LWLRAP - epoch: 24 - score: 0.2228 \n\nEpoch 25/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3446 - acc: 0.9271 - val_loss: 8.1476 - val_acc: 0.1479\n\n LWLRAP - epoch: 25 - score: 0.2670 \n\nEpoch 26/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3361 - acc: 0.9230 - val_loss: 11.6523 - val_acc: 0.0775\n\n LWLRAP - epoch: 26 - score: 0.1991 \n\nEpoch 27/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3281 - acc: 0.9245 - val_loss: 8.7643 - val_acc: 0.1600\n\n LWLRAP - epoch: 27 - score: 0.2784 \n\nEpoch 28/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3142 - acc: 0.9258 - val_loss: 4.1688 - val_acc: 0.3169\n\n LWLRAP - epoch: 28 - score: 0.4690 \n\nEpoch 29/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3089 - acc: 0.9271 - val_loss: 4.0470 - val_acc: 0.3290\n\n LWLRAP - epoch: 29 - score: 0.4595 \n\nloading best weights from current fold\nlwlrap fold: 0.4595\n(3976, 48, 256, 1) (3976, 80)\n(994, 48, 256, 1) (994, 80)\ntraining model...\nTrain on 3976 samples, validate on 994 samples\nEpoch 1/35\n3976/3976 [==============================] - 13s 3ms/step - loss: 4.4886 - acc: 0.0971 - val_loss: 18.4201 - val_acc: 0.0121\n\n LWLRAP - epoch: 1 - score: 0.0695 \n\nEpoch 2/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 3.6733 - acc: 0.2118 - val_loss: 8.7769 - val_acc: 0.0352\n\n LWLRAP - epoch: 2 - score: 0.1048 \n\nEpoch 3/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 3.2693 - acc: 0.2892 - val_loss: 13.1905 - val_acc: 0.0201\n\n LWLRAP - epoch: 3 - score: 0.0970 \n\nEpoch 4/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.9345 - acc: 0.3375 - val_loss: 17.9374 - val_acc: 0.0151\n\n LWLRAP - epoch: 4 - score: 0.0714 \n\nEpoch 5/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.6794 - acc: 0.3871 - val_loss: 15.5190 - val_acc: 0.0191\n\n LWLRAP - epoch: 5 - score: 0.0845 \n\nEpoch 6/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.4061 - acc: 0.4369 - val_loss: 11.5776 - val_acc: 0.0272\n\n LWLRAP - epoch: 6 - score: 0.0989 \n\nEpoch 7/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.1636 - acc: 0.4902 - val_loss: 18.2851 - val_acc: 0.0131\n\n LWLRAP - epoch: 7 - score: 0.0735 \n\nEpoch 8/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.6872 - acc: 0.6313 - val_loss: 10.0217 - val_acc: 0.0986\n\n LWLRAP - epoch: 8 - score: 0.2101 \n\nEpoch 9/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.5060 - acc: 0.6735 - val_loss: 7.7934 - val_acc: 0.1268\n\n LWLRAP - epoch: 9 - score: 0.2317 \n\nEpoch 10/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.3822 - acc: 0.7150 - val_loss: 5.0467 - val_acc: 0.1579\n\n LWLRAP - epoch: 10 - score: 0.3131 \n\nEpoch 11/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.2767 - acc: 0.7389 - val_loss: 5.1305 - val_acc: 0.1670\n\n LWLRAP - epoch: 11 - score: 0.3049 \n\nEpoch 12/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.1759 - acc: 0.7696 - val_loss: 13.2783 - val_acc: 0.0543\n\n LWLRAP - epoch: 12 - score: 0.1475 \n\nEpoch 13/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.0980 - acc: 0.7958 - val_loss: 7.8284 - val_acc: 0.1258\n\n LWLRAP - epoch: 13 - score: 0.2392 \n\nEpoch 14/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.9923 - acc: 0.8247 - val_loss: 10.1160 - val_acc: 0.1036\n\n LWLRAP - epoch: 14 - score: 0.2279 \n\nEpoch 15/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.9130 - acc: 0.8290 - val_loss: 9.3173 - val_acc: 0.1036\n\n LWLRAP - epoch: 15 - score: 0.1981 \n\nEpoch 16/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.8078 - acc: 0.8717 - val_loss: 4.4585 - val_acc: 0.2626\n\n LWLRAP - epoch: 16 - score: 0.4197 \n\nEpoch 17/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.7785 - acc: 0.8788 - val_loss: 6.3783 - val_acc: 0.1761\n\n LWLRAP - epoch: 17 - score: 0.3283 \n\nEpoch 18/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.7615 - acc: 0.8765 - val_loss: 3.0787 - val_acc: 0.3773\n\n LWLRAP - epoch: 18 - score: 0.5394 \n\nEpoch 19/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.7414 - acc: 0.8836 - val_loss: 2.9717 - val_acc: 0.3984\n\n LWLRAP - epoch: 19 - score: 0.5555 \n\nEpoch 20/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.7273 - acc: 0.8876 - val_loss: 4.6422 - val_acc: 0.2495\n\n LWLRAP - epoch: 20 - score: 0.3996 \n\nEpoch 21/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.7014 - acc: 0.8898 - val_loss: 4.6625 - val_acc: 0.2565\n\n LWLRAP - epoch: 21 - score: 0.4133 \n\nEpoch 22/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.7035 - acc: 0.8959 - val_loss: 2.9910 - val_acc: 0.3833\n\n LWLRAP - epoch: 22 - score: 0.5499 \n\nEpoch 23/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.6939 - acc: 0.8911 - val_loss: 12.1379 - val_acc: 0.1006\n\n LWLRAP - epoch: 23 - score: 0.1898 \n\nEpoch 24/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.6746 - acc: 0.8944 - val_loss: 3.0281 - val_acc: 0.3773\n\n LWLRAP - epoch: 24 - score: 0.5434 \n\nEpoch 25/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.6545 - acc: 0.9062 - val_loss: 2.8843 - val_acc: 0.4205\n\n LWLRAP - epoch: 25 - score: 0.5763 \n\nEpoch 26/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.6537 - acc: 0.9007 - val_loss: 2.9557 - val_acc: 0.4004\n\n LWLRAP - epoch: 26 - score: 0.5674 \n\nEpoch 27/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.6536 - acc: 0.9024 - val_loss: 2.8553 - val_acc: 0.4175\n\n LWLRAP - epoch: 27 - score: 0.5791 \n\nEpoch 28/35\n","name":"stdout"},{"output_type":"stream","text":"3976/3976 [==============================] - 7s 2ms/step - loss: 0.6497 - acc: 0.9017 - val_loss: 2.9386 - val_acc: 0.4074\n\n LWLRAP - epoch: 28 - score: 0.5672 \n\nEpoch 29/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.6381 - acc: 0.8999 - val_loss: 2.9227 - val_acc: 0.4145\n\n LWLRAP - epoch: 29 - score: 0.5707 \n\nEpoch 30/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.6345 - acc: 0.9014 - val_loss: 2.9752 - val_acc: 0.3954\n\n LWLRAP - epoch: 30 - score: 0.5547 \n\nEpoch 31/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.6271 - acc: 0.9067 - val_loss: 3.2608 - val_acc: 0.3622\n\n LWLRAP - epoch: 31 - score: 0.5286 \n\nEpoch 32/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.6273 - acc: 0.9082 - val_loss: 2.9809 - val_acc: 0.3954\n\n LWLRAP - epoch: 32 - score: 0.5627 \n\nEpoch 33/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.6196 - acc: 0.9074 - val_loss: 2.8463 - val_acc: 0.4235\n\n LWLRAP - epoch: 33 - score: 0.5816 \n\nEpoch 34/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.6333 - acc: 0.9024 - val_loss: 2.8327 - val_acc: 0.4165\n\n LWLRAP - epoch: 34 - score: 0.5810 \n\nEpoch 35/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.6203 - acc: 0.9074 - val_loss: 2.8381 - val_acc: 0.4135\n\n LWLRAP - epoch: 35 - score: 0.5786 \n\nloading best weights from current fold\nlwlrap fold: 0.5810\n(3976, 48, 256, 1) (3976, 80)\n(994, 48, 256, 1) (994, 80)\ntraining model...\nTrain on 3976 samples, validate on 994 samples\nEpoch 1/35\n3976/3976 [==============================] - 13s 3ms/step - loss: 4.5014 - acc: 0.1004 - val_loss: 17.7738 - val_acc: 0.0201\n\n LWLRAP - epoch: 1 - score: 0.0710 \n\nEpoch 2/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 3.6592 - acc: 0.2213 - val_loss: 12.3252 - val_acc: 0.0080\n\n LWLRAP - epoch: 2 - score: 0.0812 \n\nEpoch 3/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 3.2380 - acc: 0.2935 - val_loss: 16.8519 - val_acc: 0.0181\n\n LWLRAP - epoch: 3 - score: 0.0799 \n\nEpoch 4/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.9057 - acc: 0.3488 - val_loss: 11.1960 - val_acc: 0.0241\n\n LWLRAP - epoch: 4 - score: 0.1056 \n\nEpoch 5/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.6619 - acc: 0.3999 - val_loss: 18.2489 - val_acc: 0.0161\n\n LWLRAP - epoch: 5 - score: 0.0677 \n\nEpoch 6/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.3880 - acc: 0.4633 - val_loss: 16.0797 - val_acc: 0.0262\n\n LWLRAP - epoch: 6 - score: 0.0895 \n\nEpoch 7/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.1311 - acc: 0.5010 - val_loss: 14.8452 - val_acc: 0.0161\n\n LWLRAP - epoch: 7 - score: 0.0934 \n\nEpoch 8/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.8815 - acc: 0.5739 - val_loss: 18.2731 - val_acc: 0.0070\n\n LWLRAP - epoch: 8 - score: 0.0742 \n\nEpoch 9/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.6605 - acc: 0.6119 - val_loss: 13.5265 - val_acc: 0.0322\n\n LWLRAP - epoch: 9 - score: 0.1089 \n\nEpoch 10/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.1695 - acc: 0.7641 - val_loss: 10.5381 - val_acc: 0.0563\n\n LWLRAP - epoch: 10 - score: 0.1637 \n\nEpoch 11/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.0128 - acc: 0.8038 - val_loss: 10.4230 - val_acc: 0.0845\n\n LWLRAP - epoch: 11 - score: 0.2033 \n\nEpoch 12/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.9150 - acc: 0.8360 - val_loss: 7.1067 - val_acc: 0.1449\n\n LWLRAP - epoch: 12 - score: 0.2796 \n\nEpoch 13/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.8391 - acc: 0.8566 - val_loss: 9.5867 - val_acc: 0.0815\n\n LWLRAP - epoch: 13 - score: 0.2170 \n\nEpoch 14/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.7831 - acc: 0.8619 - val_loss: 7.9145 - val_acc: 0.1388\n\n LWLRAP - epoch: 14 - score: 0.2717 \n\nEpoch 15/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.7224 - acc: 0.8773 - val_loss: 5.5979 - val_acc: 0.1801\n\n LWLRAP - epoch: 15 - score: 0.3464 \n\nEpoch 16/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.6655 - acc: 0.8913 - val_loss: 6.1202 - val_acc: 0.1600\n\n LWLRAP - epoch: 16 - score: 0.3113 \n\nEpoch 17/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.6270 - acc: 0.8913 - val_loss: 8.3577 - val_acc: 0.1036\n\n LWLRAP - epoch: 17 - score: 0.2305 \n\nEpoch 18/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.5771 - acc: 0.9049 - val_loss: 6.9501 - val_acc: 0.1288\n\n LWLRAP - epoch: 18 - score: 0.2821 \n\nEpoch 19/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.5414 - acc: 0.9100 - val_loss: 7.8700 - val_acc: 0.1328\n\n LWLRAP - epoch: 19 - score: 0.2768 \n\nEpoch 20/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.5019 - acc: 0.9100 - val_loss: 5.8246 - val_acc: 0.1640\n\n LWLRAP - epoch: 20 - score: 0.2997 \n\nEpoch 21/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.4651 - acc: 0.9190 - val_loss: 5.6788 - val_acc: 0.1942\n\n LWLRAP - epoch: 21 - score: 0.3388 \n\nEpoch 22/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.4507 - acc: 0.9266 - val_loss: 4.3285 - val_acc: 0.2495\n\n LWLRAP - epoch: 22 - score: 0.4350 \n\nEpoch 23/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.4411 - acc: 0.9220 - val_loss: 3.7830 - val_acc: 0.3008\n\n LWLRAP - epoch: 23 - score: 0.4836 \n\nEpoch 24/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.4336 - acc: 0.9173 - val_loss: 3.6060 - val_acc: 0.3008\n\n LWLRAP - epoch: 24 - score: 0.4874 \n\nEpoch 25/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.4267 - acc: 0.9200 - val_loss: 2.9122 - val_acc: 0.3773\n\n LWLRAP - epoch: 25 - score: 0.5599 \n\nEpoch 26/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.4178 - acc: 0.9251 - val_loss: 6.7097 - val_acc: 0.2133\n\n LWLRAP - epoch: 26 - score: 0.3531 \n\nEpoch 27/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.4114 - acc: 0.9243 - val_loss: 2.9512 - val_acc: 0.3602\n\n LWLRAP - epoch: 27 - score: 0.5395 \n\nEpoch 28/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.4127 - acc: 0.9318 - val_loss: 3.7340 - val_acc: 0.3300\n\n LWLRAP - epoch: 28 - score: 0.5023 \n\nEpoch 29/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3990 - acc: 0.9213 - val_loss: 6.5019 - val_acc: 0.1982\n\n LWLRAP - epoch: 29 - score: 0.3555 \n\nEpoch 30/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.4036 - acc: 0.9253 - val_loss: 3.7654 - val_acc: 0.2988\n\n LWLRAP - epoch: 30 - score: 0.4634 \n\nEpoch 31/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3858 - acc: 0.9238 - val_loss: 2.8586 - val_acc: 0.3934\n\n LWLRAP - epoch: 31 - score: 0.5699 \n\nEpoch 32/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3860 - acc: 0.9258 - val_loss: 2.7038 - val_acc: 0.4125\n\n LWLRAP - epoch: 32 - score: 0.5843 \n\nEpoch 33/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3909 - acc: 0.9208 - val_loss: 2.6885 - val_acc: 0.4225\n\n LWLRAP - epoch: 33 - score: 0.5933 \n\nEpoch 34/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3885 - acc: 0.9251 - val_loss: 2.8296 - val_acc: 0.3934\n\n LWLRAP - epoch: 34 - score: 0.5737 \n\nEpoch 35/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3831 - acc: 0.9303 - val_loss: 2.7887 - val_acc: 0.3984\n\n LWLRAP - epoch: 35 - score: 0.5719 \n\nloading best weights from current fold\nlwlrap fold: 0.5933\n(3976, 48, 256, 1) (3976, 80)\n(994, 48, 256, 1) (994, 80)\ntraining model...\nTrain on 3976 samples, validate on 994 samples\nEpoch 1/35\n3976/3976 [==============================] - 14s 4ms/step - loss: 4.4866 - acc: 0.0973 - val_loss: 18.4801 - val_acc: 0.0111\n\n LWLRAP - epoch: 1 - score: 0.0685 \n\nEpoch 2/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 3.6365 - acc: 0.2130 - val_loss: 17.1820 - val_acc: 0.0151\n\n LWLRAP - epoch: 2 - score: 0.0778 \n\nEpoch 3/35\n","name":"stdout"},{"output_type":"stream","text":"3976/3976 [==============================] - 7s 2ms/step - loss: 3.2071 - acc: 0.2995 - val_loss: 17.6085 - val_acc: 0.0201\n\n LWLRAP - epoch: 3 - score: 0.0829 \n\nEpoch 4/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.8813 - acc: 0.3481 - val_loss: 15.1711 - val_acc: 0.0402\n\n LWLRAP - epoch: 4 - score: 0.0959 \n\nEpoch 5/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.6332 - acc: 0.4054 - val_loss: 13.9732 - val_acc: 0.0392\n\n LWLRAP - epoch: 5 - score: 0.1048 \n\nEpoch 6/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.3725 - acc: 0.4532 - val_loss: 11.9056 - val_acc: 0.0252\n\n LWLRAP - epoch: 6 - score: 0.0970 \n\nEpoch 7/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 2.1659 - acc: 0.4927 - val_loss: 16.2132 - val_acc: 0.0201\n\n LWLRAP - epoch: 7 - score: 0.0913 \n\nEpoch 8/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.9057 - acc: 0.5606 - val_loss: 14.1725 - val_acc: 0.0221\n\n LWLRAP - epoch: 8 - score: 0.1060 \n\nEpoch 9/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.6500 - acc: 0.6124 - val_loss: 13.9326 - val_acc: 0.0362\n\n LWLRAP - epoch: 9 - score: 0.1010 \n\nEpoch 10/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.4104 - acc: 0.6733 - val_loss: 11.2530 - val_acc: 0.0322\n\n LWLRAP - epoch: 10 - score: 0.1160 \n\nEpoch 11/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 1.1804 - acc: 0.7437 - val_loss: 14.4463 - val_acc: 0.0453\n\n LWLRAP - epoch: 11 - score: 0.1092 \n\nEpoch 12/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.9808 - acc: 0.7885 - val_loss: 12.5131 - val_acc: 0.0312\n\n LWLRAP - epoch: 12 - score: 0.1297 \n\nEpoch 13/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.8137 - acc: 0.8325 - val_loss: 14.1633 - val_acc: 0.0433\n\n LWLRAP - epoch: 13 - score: 0.1182 \n\nEpoch 14/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.7384 - acc: 0.8554 - val_loss: 15.5256 - val_acc: 0.0292\n\n LWLRAP - epoch: 14 - score: 0.1044 \n\nEpoch 15/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.6307 - acc: 0.8815 - val_loss: 14.2363 - val_acc: 0.0272\n\n LWLRAP - epoch: 15 - score: 0.1078 \n\nEpoch 16/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.4836 - acc: 0.9127 - val_loss: 12.8985 - val_acc: 0.0915\n\n LWLRAP - epoch: 16 - score: 0.1795 \n\nEpoch 17/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.4027 - acc: 0.9173 - val_loss: 8.7482 - val_acc: 0.1408\n\n LWLRAP - epoch: 17 - score: 0.2729 \n\nEpoch 18/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3750 - acc: 0.9178 - val_loss: 9.4290 - val_acc: 0.1429\n\n LWLRAP - epoch: 18 - score: 0.2547 \n\nEpoch 19/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3590 - acc: 0.9268 - val_loss: 11.5904 - val_acc: 0.1177\n\n LWLRAP - epoch: 19 - score: 0.2270 \n\nEpoch 20/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3386 - acc: 0.9271 - val_loss: 4.8226 - val_acc: 0.2596\n\n LWLRAP - epoch: 20 - score: 0.4321 \n\nEpoch 21/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3398 - acc: 0.9225 - val_loss: 8.6329 - val_acc: 0.1650\n\n LWLRAP - epoch: 21 - score: 0.3088 \n\nEpoch 22/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3415 - acc: 0.9258 - val_loss: 10.0746 - val_acc: 0.0966\n\n LWLRAP - epoch: 22 - score: 0.2494 \n\nEpoch 23/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3345 - acc: 0.9245 - val_loss: 10.9956 - val_acc: 0.0946\n\n LWLRAP - epoch: 23 - score: 0.2333 \n\nEpoch 24/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3339 - acc: 0.9240 - val_loss: 14.1146 - val_acc: 0.0573\n\n LWLRAP - epoch: 24 - score: 0.1499 \n\nEpoch 25/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3272 - acc: 0.9251 - val_loss: 12.7259 - val_acc: 0.0895\n\n LWLRAP - epoch: 25 - score: 0.1853 \n\nEpoch 26/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3128 - acc: 0.9238 - val_loss: 3.5480 - val_acc: 0.3592\n\n LWLRAP - epoch: 26 - score: 0.5307 \n\nEpoch 27/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.3033 - acc: 0.9238 - val_loss: 4.8667 - val_acc: 0.2817\n\n LWLRAP - epoch: 27 - score: 0.4501 \n\nEpoch 28/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.2977 - acc: 0.9291 - val_loss: 4.4277 - val_acc: 0.2757\n\n LWLRAP - epoch: 28 - score: 0.4388 \n\nEpoch 29/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.2968 - acc: 0.9225 - val_loss: 3.3773 - val_acc: 0.3853\n\n LWLRAP - epoch: 29 - score: 0.5406 \n\nEpoch 30/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.2933 - acc: 0.9200 - val_loss: 6.3341 - val_acc: 0.1932\n\n LWLRAP - epoch: 30 - score: 0.3505 \n\nEpoch 31/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.2938 - acc: 0.9266 - val_loss: 3.7254 - val_acc: 0.3471\n\n LWLRAP - epoch: 31 - score: 0.5058 \n\nEpoch 32/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.2931 - acc: 0.9278 - val_loss: 7.0667 - val_acc: 0.1932\n\n LWLRAP - epoch: 32 - score: 0.3592 \n\nEpoch 33/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.2924 - acc: 0.9193 - val_loss: 7.9965 - val_acc: 0.1861\n\n LWLRAP - epoch: 33 - score: 0.3344 \n\nEpoch 34/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.2902 - acc: 0.9291 - val_loss: 4.4428 - val_acc: 0.2897\n\n LWLRAP - epoch: 34 - score: 0.4543 \n\nEpoch 35/35\n3976/3976 [==============================] - 7s 2ms/step - loss: 0.2862 - acc: 0.9276 - val_loss: 2.9665 - val_acc: 0.4215\n\n LWLRAP - epoch: 35 - score: 0.5824 \n\nloading best weights from current fold\nlwlrap fold: 0.5824\nOOF LWLRAP: 0.5583\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_test_mean = oof_test.mean(axis=-1)\nprint(oof_test_mean.shape)\n\nsort_idx = np.argsort(classes).astype(int)\noof_test_mean_sorted = oof_test_mean[:, sort_idx]\nsample_submission.iloc[:, 1:] =  oof_test_mean_sorted\nif SAVE_SUBMISSION:\n    # sample_submission.to_csv('{}_lwl_{:.4f}.csv'.format(RUN_NAME, oof_lwl, index=False))\n    sample_submission.to_csv('submission.csv', index=False)\n    print(sample_submission.shape)\n    \nsample_submission.head()","execution_count":14,"outputs":[{"output_type":"stream","text":"(1120, 80)\n(1120, 81)\n","name":"stdout"},{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"          fname  Accelerating_and_revving_and_vroom     Accordion  \\\n0  000ccb97.wav                        7.630934e-06  1.126024e-06   \n1  0012633b.wav                        7.945251e-03  6.793572e-07   \n2  001ed5f1.wav                        4.914838e-04  5.959930e-07   \n3  00294be0.wav                        3.017973e-05  4.028038e-07   \n4  003fde7a.wav                        4.506943e-11  8.921707e-08   \n\n   Acoustic_guitar      Applause          Bark     Bass_drum   Bass_guitar  \\\n0     9.557936e-06  7.191704e-06  5.745985e-06  2.253374e-03  6.174082e-06   \n1     2.767486e-07  4.266541e-05  3.476241e-04  5.217420e-06  1.051208e-06   \n2     4.253546e-06  2.102950e-05  1.089857e-04  5.557509e-02  3.716875e-05   \n3     6.008819e-06  1.118851e-05  2.601729e-03  3.143482e-06  7.871578e-06   \n4     1.036718e-06  7.856484e-08  1.572457e-07  1.505096e-08  9.948780e-09   \n\n   Bathtub_(filling_or_washing)  Bicycle_bell  Burping_and_eructation  \\\n0                  2.355557e-02  3.019212e-03                0.001116   \n1                  1.277254e-02  2.783864e-07                0.000156   \n2                  2.423663e-04  1.361906e-05                0.000168   \n3                  2.782175e-02  2.041693e-06                0.000217   \n4                  2.414148e-09  8.056401e-01                0.000003   \n\n            Bus          Buzz  Car_passing_by      Cheering  \\\n0  1.127185e-04  4.156595e-04    1.125294e-07  6.142814e-06   \n1  9.149866e-05  8.942229e-03    1.185120e-04  4.317892e-05   \n2  1.586857e-03  4.781486e-06    2.390796e-05  2.770816e-05   \n3  2.748477e-04  3.740818e-03    1.197445e-06  6.105997e-06   \n4  3.039046e-07  7.447269e-10    1.652458e-12  5.385623e-08   \n\n   Chewing_and_mastication  Child_speech_and_kid_speaking  Chink_and_clink  \\\n0             7.523261e-03                   7.807558e-06     2.217286e-03   \n1             2.516754e-02                   1.578456e-04     3.880692e-07   \n2             6.731230e-03                   1.194379e-05     1.097298e-04   \n3             2.260438e-02                   3.871577e-02     2.511732e-05   \n4             1.137780e-07                   1.309705e-09     1.604554e-01   \n\n   Chirp_and_tweet  Church_bell  Clapping  Computer_keyboard       Crackle  \\\n0         0.019559     0.000002  0.000231           0.000821  2.206258e-03   \n1         0.000085     0.000016  0.000015           0.000534  6.352894e-02   \n2         0.000047     0.000064  0.001740           0.007760  1.945635e-04   \n3         0.000671     0.000168  0.000002           0.002720  8.103669e-05   \n4         0.000052     0.000009  0.000016           0.000004  5.334810e-10   \n\n    Cricket         Crowd  Cupboard_open_or_close  Cutlery_and_silverware  \\\n0  0.116611  7.487527e-06            5.106073e-05                0.054577   \n1  0.000111  2.674633e-04            3.477763e-04                0.000014   \n2  0.000021  8.320537e-05            7.620898e-02                0.001282   \n3  0.001542  2.616761e-05            3.992716e-05                0.000067   \n4  0.000006  4.348518e-08            2.881525e-08                0.002008   \n\n   Dishes_and_pots_and_pans  Drawer_open_or_close      Drip  Electric_guitar  \\\n0                  0.003496          1.380159e-04  0.007563     8.464166e-06   \n1                  0.000004          8.569744e-02  0.000048     5.711990e-07   \n2                  0.000364          3.849829e-02  0.000050     4.099506e-04   \n3                  0.000026          4.051256e-03  0.000066     2.201624e-06   \n4                  0.027753          8.389362e-11  0.000029     6.607043e-07   \n\n           Fart  Female_singing  Female_speech_and_woman_speaking  \\\n0  3.790464e-04    1.796575e-05                      7.161554e-04   \n1  6.306472e-04    1.985967e-05                      7.372833e-04   \n2  1.384116e-02    1.452739e-07                      5.066368e-04   \n3  3.012429e-04    5.623194e-04                      1.727634e-02   \n4  5.820608e-09    4.084433e-08                      4.364073e-08   \n\n   Fill_(with_liquid)  Finger_snapping  Frying_(food)          Gasp  \\\n0        1.204408e-03     2.411856e-02       0.010561  1.141314e-03   \n1        1.520966e-03     3.733742e-08       0.000255  8.873779e-05   \n2        3.991843e-05     1.983964e-04       0.000032  8.880346e-06   \n3        5.731898e-04     1.193709e-07       0.000065  2.994187e-05   \n4        2.580490e-07     2.657285e-06       0.000001  4.677611e-08   \n\n   Glockenspiel          Gong      Gurgling     Harmonica        Hi-hat  \\\n0  3.995070e-06  6.569998e-04  8.718882e-04  8.443955e-05  8.851263e-03   \n1  1.981254e-12  1.304141e-07  3.926818e-03  2.210444e-06  1.873633e-06   \n2  1.217020e-06  5.853775e-06  7.599831e-05  8.467414e-07  1.173972e-03   \n3  3.593231e-09  2.430043e-06  2.005642e-02  2.965128e-04  3.607208e-06   \n4  8.467334e-04  1.364170e-06  7.464735e-08  5.795021e-04  8.360082e-07   \n\n           Hiss  Keys_jangling         Knock  Male_singing  \\\n0  3.842605e-02       0.292134  1.648564e-05  3.101176e-06   \n1  3.355232e-03       0.000029  5.773754e-05  2.419432e-04   \n2  4.086373e-04       0.000173  9.300863e-02  2.091942e-06   \n3  3.375200e-04       0.000116  4.762887e-04  5.201666e-05   \n4  4.462873e-07       0.000136  1.325708e-08  1.742664e-11   \n\n   Male_speech_and_man_speaking  Marimba_and_xylophone  Mechanical_fan  \\\n0                  7.125412e-04           5.951346e-06    3.640461e-05   \n1                  8.899620e-04           2.745302e-09    2.020297e-04   \n2                  2.537540e-03           1.043665e-03    8.105561e-05   \n3                  5.360056e-04           2.270470e-06    3.666047e-06   \n4                  7.846682e-09           1.296280e-03    5.261956e-07   \n\n           Meow  Microwave_oven    Motorcycle   Printer          Purr  \\\n0  4.123669e-04        0.000210  9.366042e-07  0.000145  1.422188e-04   \n1  1.672870e-03        0.000725  2.838649e-02  0.002354  1.803961e-01   \n2  4.742294e-05        0.026263  1.132772e-03  0.006985  5.558643e-05   \n3  5.781172e-03        0.000033  2.963027e-04  0.002549  4.489451e-01   \n4  7.912064e-09        0.000184  2.701101e-11  0.000014  1.005599e-11   \n\n   Race_car_and_auto_racing  Raindrop           Run  Scissors     Screaming  \\\n0              5.197266e-08  0.011268  9.356048e-06  0.128158  1.711372e-04   \n1              1.049741e-04  0.000003  3.934470e-02  0.001237  3.008439e-05   \n2              9.856125e-05  0.000033  1.259541e-02  0.004615  5.979117e-05   \n3              3.897196e-06  0.000048  6.293690e-03  0.000108  1.262180e-05   \n4              1.347549e-11  0.000034  2.243251e-11  0.000003  1.544679e-07   \n\n    Shatter          Sigh  Sink_(filling_or_washing)    Skateboard  \\\n0  0.140231  3.475408e-04                   0.007002  1.862208e-06   \n1  0.000019  2.220316e-04                   0.000832  1.536778e-03   \n2  0.000834  3.193068e-06                   0.000080  7.580077e-04   \n3  0.000009  2.911150e-05                   0.001492  2.008946e-04   \n4  0.000191  3.297388e-09                   0.000004  3.136905e-10   \n\n           Slam        Sneeze    Squeak        Stream         Strum       Tap  \\\n0  4.675094e-04  1.831639e-03  0.002119  3.051140e-04  3.147200e-06  0.000089   \n1  1.044683e-03  1.713921e-04  0.005715  1.108031e-04  9.399754e-08  0.000007   \n2  3.379928e-01  2.925850e-03  0.133867  3.261873e-06  4.635730e-06  0.014612   \n3  7.094769e-05  9.866642e-05  0.000492  3.537733e-05  3.252812e-06  0.000263   \n4  2.702228e-07  3.415866e-07  0.000007  2.012157e-10  3.186759e-07  0.000035   \n\n   Tick-tock  Toilet_flush  Traffic_noise_and_roadway_noise  \\\n0   0.001445  2.000678e-04                     1.137943e-06   \n1   0.000114  1.511131e-02                     1.009377e-04   \n2   0.049321  3.780541e-05                     6.191605e-05   \n3   0.002591  7.771356e-04                     3.500645e-06   \n4   0.000681  4.628647e-10                     1.206940e-08   \n\n   Trickle_and_dribble  Walk_and_footsteps  Water_tap_and_faucet  \\\n0         1.875239e-03        3.074506e-05          4.461727e-03   \n1         1.844445e-05        1.155871e-01          1.066032e-03   \n2         2.500981e-06        9.177884e-02          6.771300e-06   \n3         5.882365e-04        1.856796e-02          8.137919e-04   \n4         7.399385e-07        1.406988e-10          4.495905e-07   \n\n   Waves_and_surf    Whispering       Writing          Yell  Zipper_(clothing)  \n0    1.493053e-05  3.263234e-03  6.970076e-02  1.609087e-04       4.525526e-04  \n1    2.837723e-02  1.594179e-04  9.546124e-03  2.999285e-04       3.472851e-01  \n2    2.638801e-04  4.096762e-04  9.777345e-04  3.149970e-04       8.839027e-03  \n3    3.540992e-05  9.361802e-04  3.208230e-01  2.883691e-03       3.899243e-02  \n4    8.433536e-14  8.252657e-08  1.030768e-09  8.219400e-09       6.913077e-12  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fname</th>\n      <th>Accelerating_and_revving_and_vroom</th>\n      <th>Accordion</th>\n      <th>Acoustic_guitar</th>\n      <th>Applause</th>\n      <th>Bark</th>\n      <th>Bass_drum</th>\n      <th>Bass_guitar</th>\n      <th>Bathtub_(filling_or_washing)</th>\n      <th>Bicycle_bell</th>\n      <th>Burping_and_eructation</th>\n      <th>Bus</th>\n      <th>Buzz</th>\n      <th>Car_passing_by</th>\n      <th>Cheering</th>\n      <th>Chewing_and_mastication</th>\n      <th>Child_speech_and_kid_speaking</th>\n      <th>Chink_and_clink</th>\n      <th>Chirp_and_tweet</th>\n      <th>Church_bell</th>\n      <th>Clapping</th>\n      <th>Computer_keyboard</th>\n      <th>Crackle</th>\n      <th>Cricket</th>\n      <th>Crowd</th>\n      <th>Cupboard_open_or_close</th>\n      <th>Cutlery_and_silverware</th>\n      <th>Dishes_and_pots_and_pans</th>\n      <th>Drawer_open_or_close</th>\n      <th>Drip</th>\n      <th>Electric_guitar</th>\n      <th>Fart</th>\n      <th>Female_singing</th>\n      <th>Female_speech_and_woman_speaking</th>\n      <th>Fill_(with_liquid)</th>\n      <th>Finger_snapping</th>\n      <th>Frying_(food)</th>\n      <th>Gasp</th>\n      <th>Glockenspiel</th>\n      <th>Gong</th>\n      <th>Gurgling</th>\n      <th>Harmonica</th>\n      <th>Hi-hat</th>\n      <th>Hiss</th>\n      <th>Keys_jangling</th>\n      <th>Knock</th>\n      <th>Male_singing</th>\n      <th>Male_speech_and_man_speaking</th>\n      <th>Marimba_and_xylophone</th>\n      <th>Mechanical_fan</th>\n      <th>Meow</th>\n      <th>Microwave_oven</th>\n      <th>Motorcycle</th>\n      <th>Printer</th>\n      <th>Purr</th>\n      <th>Race_car_and_auto_racing</th>\n      <th>Raindrop</th>\n      <th>Run</th>\n      <th>Scissors</th>\n      <th>Screaming</th>\n      <th>Shatter</th>\n      <th>Sigh</th>\n      <th>Sink_(filling_or_washing)</th>\n      <th>Skateboard</th>\n      <th>Slam</th>\n      <th>Sneeze</th>\n      <th>Squeak</th>\n      <th>Stream</th>\n      <th>Strum</th>\n      <th>Tap</th>\n      <th>Tick-tock</th>\n      <th>Toilet_flush</th>\n      <th>Traffic_noise_and_roadway_noise</th>\n      <th>Trickle_and_dribble</th>\n      <th>Walk_and_footsteps</th>\n      <th>Water_tap_and_faucet</th>\n      <th>Waves_and_surf</th>\n      <th>Whispering</th>\n      <th>Writing</th>\n      <th>Yell</th>\n      <th>Zipper_(clothing)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000ccb97.wav</td>\n      <td>7.630934e-06</td>\n      <td>1.126024e-06</td>\n      <td>9.557936e-06</td>\n      <td>7.191704e-06</td>\n      <td>5.745985e-06</td>\n      <td>2.253374e-03</td>\n      <td>6.174082e-06</td>\n      <td>2.355557e-02</td>\n      <td>3.019212e-03</td>\n      <td>0.001116</td>\n      <td>1.127185e-04</td>\n      <td>4.156595e-04</td>\n      <td>1.125294e-07</td>\n      <td>6.142814e-06</td>\n      <td>7.523261e-03</td>\n      <td>7.807558e-06</td>\n      <td>2.217286e-03</td>\n      <td>0.019559</td>\n      <td>0.000002</td>\n      <td>0.000231</td>\n      <td>0.000821</td>\n      <td>2.206258e-03</td>\n      <td>0.116611</td>\n      <td>7.487527e-06</td>\n      <td>5.106073e-05</td>\n      <td>0.054577</td>\n      <td>0.003496</td>\n      <td>1.380159e-04</td>\n      <td>0.007563</td>\n      <td>8.464166e-06</td>\n      <td>3.790464e-04</td>\n      <td>1.796575e-05</td>\n      <td>7.161554e-04</td>\n      <td>1.204408e-03</td>\n      <td>2.411856e-02</td>\n      <td>0.010561</td>\n      <td>1.141314e-03</td>\n      <td>3.995070e-06</td>\n      <td>6.569998e-04</td>\n      <td>8.718882e-04</td>\n      <td>8.443955e-05</td>\n      <td>8.851263e-03</td>\n      <td>3.842605e-02</td>\n      <td>0.292134</td>\n      <td>1.648564e-05</td>\n      <td>3.101176e-06</td>\n      <td>7.125412e-04</td>\n      <td>5.951346e-06</td>\n      <td>3.640461e-05</td>\n      <td>4.123669e-04</td>\n      <td>0.000210</td>\n      <td>9.366042e-07</td>\n      <td>0.000145</td>\n      <td>1.422188e-04</td>\n      <td>5.197266e-08</td>\n      <td>0.011268</td>\n      <td>9.356048e-06</td>\n      <td>0.128158</td>\n      <td>1.711372e-04</td>\n      <td>0.140231</td>\n      <td>3.475408e-04</td>\n      <td>0.007002</td>\n      <td>1.862208e-06</td>\n      <td>4.675094e-04</td>\n      <td>1.831639e-03</td>\n      <td>0.002119</td>\n      <td>3.051140e-04</td>\n      <td>3.147200e-06</td>\n      <td>0.000089</td>\n      <td>0.001445</td>\n      <td>2.000678e-04</td>\n      <td>1.137943e-06</td>\n      <td>1.875239e-03</td>\n      <td>3.074506e-05</td>\n      <td>4.461727e-03</td>\n      <td>1.493053e-05</td>\n      <td>3.263234e-03</td>\n      <td>6.970076e-02</td>\n      <td>1.609087e-04</td>\n      <td>4.525526e-04</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0012633b.wav</td>\n      <td>7.945251e-03</td>\n      <td>6.793572e-07</td>\n      <td>2.767486e-07</td>\n      <td>4.266541e-05</td>\n      <td>3.476241e-04</td>\n      <td>5.217420e-06</td>\n      <td>1.051208e-06</td>\n      <td>1.277254e-02</td>\n      <td>2.783864e-07</td>\n      <td>0.000156</td>\n      <td>9.149866e-05</td>\n      <td>8.942229e-03</td>\n      <td>1.185120e-04</td>\n      <td>4.317892e-05</td>\n      <td>2.516754e-02</td>\n      <td>1.578456e-04</td>\n      <td>3.880692e-07</td>\n      <td>0.000085</td>\n      <td>0.000016</td>\n      <td>0.000015</td>\n      <td>0.000534</td>\n      <td>6.352894e-02</td>\n      <td>0.000111</td>\n      <td>2.674633e-04</td>\n      <td>3.477763e-04</td>\n      <td>0.000014</td>\n      <td>0.000004</td>\n      <td>8.569744e-02</td>\n      <td>0.000048</td>\n      <td>5.711990e-07</td>\n      <td>6.306472e-04</td>\n      <td>1.985967e-05</td>\n      <td>7.372833e-04</td>\n      <td>1.520966e-03</td>\n      <td>3.733742e-08</td>\n      <td>0.000255</td>\n      <td>8.873779e-05</td>\n      <td>1.981254e-12</td>\n      <td>1.304141e-07</td>\n      <td>3.926818e-03</td>\n      <td>2.210444e-06</td>\n      <td>1.873633e-06</td>\n      <td>3.355232e-03</td>\n      <td>0.000029</td>\n      <td>5.773754e-05</td>\n      <td>2.419432e-04</td>\n      <td>8.899620e-04</td>\n      <td>2.745302e-09</td>\n      <td>2.020297e-04</td>\n      <td>1.672870e-03</td>\n      <td>0.000725</td>\n      <td>2.838649e-02</td>\n      <td>0.002354</td>\n      <td>1.803961e-01</td>\n      <td>1.049741e-04</td>\n      <td>0.000003</td>\n      <td>3.934470e-02</td>\n      <td>0.001237</td>\n      <td>3.008439e-05</td>\n      <td>0.000019</td>\n      <td>2.220316e-04</td>\n      <td>0.000832</td>\n      <td>1.536778e-03</td>\n      <td>1.044683e-03</td>\n      <td>1.713921e-04</td>\n      <td>0.005715</td>\n      <td>1.108031e-04</td>\n      <td>9.399754e-08</td>\n      <td>0.000007</td>\n      <td>0.000114</td>\n      <td>1.511131e-02</td>\n      <td>1.009377e-04</td>\n      <td>1.844445e-05</td>\n      <td>1.155871e-01</td>\n      <td>1.066032e-03</td>\n      <td>2.837723e-02</td>\n      <td>1.594179e-04</td>\n      <td>9.546124e-03</td>\n      <td>2.999285e-04</td>\n      <td>3.472851e-01</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001ed5f1.wav</td>\n      <td>4.914838e-04</td>\n      <td>5.959930e-07</td>\n      <td>4.253546e-06</td>\n      <td>2.102950e-05</td>\n      <td>1.089857e-04</td>\n      <td>5.557509e-02</td>\n      <td>3.716875e-05</td>\n      <td>2.423663e-04</td>\n      <td>1.361906e-05</td>\n      <td>0.000168</td>\n      <td>1.586857e-03</td>\n      <td>4.781486e-06</td>\n      <td>2.390796e-05</td>\n      <td>2.770816e-05</td>\n      <td>6.731230e-03</td>\n      <td>1.194379e-05</td>\n      <td>1.097298e-04</td>\n      <td>0.000047</td>\n      <td>0.000064</td>\n      <td>0.001740</td>\n      <td>0.007760</td>\n      <td>1.945635e-04</td>\n      <td>0.000021</td>\n      <td>8.320537e-05</td>\n      <td>7.620898e-02</td>\n      <td>0.001282</td>\n      <td>0.000364</td>\n      <td>3.849829e-02</td>\n      <td>0.000050</td>\n      <td>4.099506e-04</td>\n      <td>1.384116e-02</td>\n      <td>1.452739e-07</td>\n      <td>5.066368e-04</td>\n      <td>3.991843e-05</td>\n      <td>1.983964e-04</td>\n      <td>0.000032</td>\n      <td>8.880346e-06</td>\n      <td>1.217020e-06</td>\n      <td>5.853775e-06</td>\n      <td>7.599831e-05</td>\n      <td>8.467414e-07</td>\n      <td>1.173972e-03</td>\n      <td>4.086373e-04</td>\n      <td>0.000173</td>\n      <td>9.300863e-02</td>\n      <td>2.091942e-06</td>\n      <td>2.537540e-03</td>\n      <td>1.043665e-03</td>\n      <td>8.105561e-05</td>\n      <td>4.742294e-05</td>\n      <td>0.026263</td>\n      <td>1.132772e-03</td>\n      <td>0.006985</td>\n      <td>5.558643e-05</td>\n      <td>9.856125e-05</td>\n      <td>0.000033</td>\n      <td>1.259541e-02</td>\n      <td>0.004615</td>\n      <td>5.979117e-05</td>\n      <td>0.000834</td>\n      <td>3.193068e-06</td>\n      <td>0.000080</td>\n      <td>7.580077e-04</td>\n      <td>3.379928e-01</td>\n      <td>2.925850e-03</td>\n      <td>0.133867</td>\n      <td>3.261873e-06</td>\n      <td>4.635730e-06</td>\n      <td>0.014612</td>\n      <td>0.049321</td>\n      <td>3.780541e-05</td>\n      <td>6.191605e-05</td>\n      <td>2.500981e-06</td>\n      <td>9.177884e-02</td>\n      <td>6.771300e-06</td>\n      <td>2.638801e-04</td>\n      <td>4.096762e-04</td>\n      <td>9.777345e-04</td>\n      <td>3.149970e-04</td>\n      <td>8.839027e-03</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00294be0.wav</td>\n      <td>3.017973e-05</td>\n      <td>4.028038e-07</td>\n      <td>6.008819e-06</td>\n      <td>1.118851e-05</td>\n      <td>2.601729e-03</td>\n      <td>3.143482e-06</td>\n      <td>7.871578e-06</td>\n      <td>2.782175e-02</td>\n      <td>2.041693e-06</td>\n      <td>0.000217</td>\n      <td>2.748477e-04</td>\n      <td>3.740818e-03</td>\n      <td>1.197445e-06</td>\n      <td>6.105997e-06</td>\n      <td>2.260438e-02</td>\n      <td>3.871577e-02</td>\n      <td>2.511732e-05</td>\n      <td>0.000671</td>\n      <td>0.000168</td>\n      <td>0.000002</td>\n      <td>0.002720</td>\n      <td>8.103669e-05</td>\n      <td>0.001542</td>\n      <td>2.616761e-05</td>\n      <td>3.992716e-05</td>\n      <td>0.000067</td>\n      <td>0.000026</td>\n      <td>4.051256e-03</td>\n      <td>0.000066</td>\n      <td>2.201624e-06</td>\n      <td>3.012429e-04</td>\n      <td>5.623194e-04</td>\n      <td>1.727634e-02</td>\n      <td>5.731898e-04</td>\n      <td>1.193709e-07</td>\n      <td>0.000065</td>\n      <td>2.994187e-05</td>\n      <td>3.593231e-09</td>\n      <td>2.430043e-06</td>\n      <td>2.005642e-02</td>\n      <td>2.965128e-04</td>\n      <td>3.607208e-06</td>\n      <td>3.375200e-04</td>\n      <td>0.000116</td>\n      <td>4.762887e-04</td>\n      <td>5.201666e-05</td>\n      <td>5.360056e-04</td>\n      <td>2.270470e-06</td>\n      <td>3.666047e-06</td>\n      <td>5.781172e-03</td>\n      <td>0.000033</td>\n      <td>2.963027e-04</td>\n      <td>0.002549</td>\n      <td>4.489451e-01</td>\n      <td>3.897196e-06</td>\n      <td>0.000048</td>\n      <td>6.293690e-03</td>\n      <td>0.000108</td>\n      <td>1.262180e-05</td>\n      <td>0.000009</td>\n      <td>2.911150e-05</td>\n      <td>0.001492</td>\n      <td>2.008946e-04</td>\n      <td>7.094769e-05</td>\n      <td>9.866642e-05</td>\n      <td>0.000492</td>\n      <td>3.537733e-05</td>\n      <td>3.252812e-06</td>\n      <td>0.000263</td>\n      <td>0.002591</td>\n      <td>7.771356e-04</td>\n      <td>3.500645e-06</td>\n      <td>5.882365e-04</td>\n      <td>1.856796e-02</td>\n      <td>8.137919e-04</td>\n      <td>3.540992e-05</td>\n      <td>9.361802e-04</td>\n      <td>3.208230e-01</td>\n      <td>2.883691e-03</td>\n      <td>3.899243e-02</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>003fde7a.wav</td>\n      <td>4.506943e-11</td>\n      <td>8.921707e-08</td>\n      <td>1.036718e-06</td>\n      <td>7.856484e-08</td>\n      <td>1.572457e-07</td>\n      <td>1.505096e-08</td>\n      <td>9.948780e-09</td>\n      <td>2.414148e-09</td>\n      <td>8.056401e-01</td>\n      <td>0.000003</td>\n      <td>3.039046e-07</td>\n      <td>7.447269e-10</td>\n      <td>1.652458e-12</td>\n      <td>5.385623e-08</td>\n      <td>1.137780e-07</td>\n      <td>1.309705e-09</td>\n      <td>1.604554e-01</td>\n      <td>0.000052</td>\n      <td>0.000009</td>\n      <td>0.000016</td>\n      <td>0.000004</td>\n      <td>5.334810e-10</td>\n      <td>0.000006</td>\n      <td>4.348518e-08</td>\n      <td>2.881525e-08</td>\n      <td>0.002008</td>\n      <td>0.027753</td>\n      <td>8.389362e-11</td>\n      <td>0.000029</td>\n      <td>6.607043e-07</td>\n      <td>5.820608e-09</td>\n      <td>4.084433e-08</td>\n      <td>4.364073e-08</td>\n      <td>2.580490e-07</td>\n      <td>2.657285e-06</td>\n      <td>0.000001</td>\n      <td>4.677611e-08</td>\n      <td>8.467334e-04</td>\n      <td>1.364170e-06</td>\n      <td>7.464735e-08</td>\n      <td>5.795021e-04</td>\n      <td>8.360082e-07</td>\n      <td>4.462873e-07</td>\n      <td>0.000136</td>\n      <td>1.325708e-08</td>\n      <td>1.742664e-11</td>\n      <td>7.846682e-09</td>\n      <td>1.296280e-03</td>\n      <td>5.261956e-07</td>\n      <td>7.912064e-09</td>\n      <td>0.000184</td>\n      <td>2.701101e-11</td>\n      <td>0.000014</td>\n      <td>1.005599e-11</td>\n      <td>1.347549e-11</td>\n      <td>0.000034</td>\n      <td>2.243251e-11</td>\n      <td>0.000003</td>\n      <td>1.544679e-07</td>\n      <td>0.000191</td>\n      <td>3.297388e-09</td>\n      <td>0.000004</td>\n      <td>3.136905e-10</td>\n      <td>2.702228e-07</td>\n      <td>3.415866e-07</td>\n      <td>0.000007</td>\n      <td>2.012157e-10</td>\n      <td>3.186759e-07</td>\n      <td>0.000035</td>\n      <td>0.000681</td>\n      <td>4.628647e-10</td>\n      <td>1.206940e-08</td>\n      <td>7.399385e-07</td>\n      <td>1.406988e-10</td>\n      <td>4.495905e-07</td>\n      <td>8.433536e-14</td>\n      <td>8.252657e-08</td>\n      <td>1.030768e-09</td>\n      <td>8.219400e-09</td>\n      <td>6.913077e-12</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}