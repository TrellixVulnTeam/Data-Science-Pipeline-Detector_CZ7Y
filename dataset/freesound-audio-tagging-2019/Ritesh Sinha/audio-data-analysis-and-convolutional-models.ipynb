{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Motivation","metadata":{}},{"cell_type":"markdown","source":"Treating Audio Data as images can have their own benefits, as image data can be used in Deep Learning and popular and effective methods like Convolutional Neural Networks can be applied to the audio data which is represented as an image. This kernel is basically a Data Processing Step which shows how to convert and Audio Data to image by extracting features. \n\nOnce you have an image, you can apply various image based models and Convolutional Networks for various AI related tasks. One such use case is here is of classification of audio data.\n","metadata":{}},{"cell_type":"markdown","source":"# References and Credits:\n\nThe audio data analysis is motivated by the following resources\nhttps://www.kaggle.com/daisukelab/cnn-2d-basic-solution-powered-by-fast-ai\nhttp://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/\n\nPython library **Librosa** is used for audio feature extraction.\nData used for demonstration is from the freesound audio tagging competition. https://www.kaggle.com/c/freesound-audio-tagging-2019 \n\n### Spectrogram\nA spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. A spectrogram is a visual representation of the spectrum of frequencies in a sound or other signal as they vary with time or some other variable.\nhttps://en.wikipedia.org/wiki/Spectrogram\n\n### Convolutional Models\nConvolutional models are built with reference to deeeplearning.ai specialization on Coursera, taught by Prof. Andrew Ng. Thanks!","metadata":{}},{"cell_type":"markdown","source":"# Section:1 Audio Data Extraction","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path\nimport os\nimport IPython\nimport IPython.display\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nfrom PIL import Image               # to load images\nfrom IPython.display import display\nimport time\nprint(os.listdir(\"../input\"))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Special thanks to https://github.com/makinacorpus/easydict/blob/master/easydict/__init__.py\nclass EasyDict(dict):\n    def __init__(self, d=None, **kwargs):\n        if d is None:\n            d = {}\n        if kwargs:\n            d.update(**kwargs)\n        for k, v in d.items():\n            setattr(self, k, v)\n        # Class attributes\n        for k in self.__class__.__dict__.keys():\n            if not (k.startswith('__') and k.endswith('__')) and not k in ('update', 'pop'):\n                setattr(self, k, getattr(self, k))\n\n    def __setattr__(self, name, value):\n        if isinstance(value, (list, tuple)):\n            value = [self.__class__(x)\n                     if isinstance(x, dict) else x for x in value]\n        elif isinstance(value, dict) and not isinstance(value, self.__class__):\n            value = self.__class__(value)\n        super(EasyDict, self).__setattr__(name, value)\n        super(EasyDict, self).__setitem__(name, value)\n\n    __setitem__ = __setattr__\n\n    def update(self, e=None, **f):\n        d = e or dict()\n        d.update(f)\n        for k in d:\n            setattr(self, k, d[k])\n\n    def pop(self, k, d=None):\n        delattr(self, k)\n        return super(EasyDict, self).pop(k, d)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Following are some of the parameters which are defined for audio data handling. \n\nSampling Rate is perhaps the most important one which talks about how many times in a second, the data is collected (Hertz.) Usually this value is 44100 for audio data.","metadata":{}},{"cell_type":"code","source":"conf = EasyDict()\nconf.sampling_rate = 44100\nconf.duration = 2\nconf.hop_length = 347 # to make time steps 128\nconf.fmin = 20\nconf.fmax = conf.sampling_rate // 2\nconf.n_mels = 128\nconf.n_fft = conf.n_mels * 20\nconf.samples = conf.sampling_rate * conf.duration","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions to analyze Audio Data\n\nSome of the functions developed for the task are\n\n* read_audio - Returns the Y values of audio file.\n* audio_to_melspectrogram - Returning a mel spectrogram from raw audio.\n* mono_to_color","metadata":{}},{"cell_type":"code","source":"import librosa\nimport librosa.display\ndef read_audio(conf, pathname, trim_long_data):\n    y, sr = librosa.load(pathname, sr=conf.sampling_rate)\n    # trim silence\n    if 0 < len(y): # workaround: 0 length causes error\n        y, _ = librosa.effects.trim(y) # trim, top_db=default(60)\n    # make it unified length to conf.samples\n    if len(y) > conf.samples: # long enough\n        if trim_long_data:\n            y = y[0:0+conf.samples]\n    else: # pad blank\n        padding = conf.samples - len(y)    # add padding at both ends\n        offset = padding // 2\n        y = np.pad(y, (offset, conf.samples - len(y) - offset), 'constant')\n    return y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading train_curated data files\n\nAs you have seen above, the list of curated files are available in the data file called train_curated.csv. Lets examine the contents of the file. It tells about the name of the file (fname) and labels attached to it. (Bark, Raindrop, etc.) The objective of this competition is to build a model which can predict the labels on unknown data.","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"}},{"cell_type":"code","source":"start_time_data_processing = time.time()\n\nDATA = Path('../input')\nCSV_TRN_CURATED = DATA/'train_curated.csv'\nCSV_TRN_NOISY = DATA/'train_noisy.csv'\nCSV_SUBMISSION = DATA/'sample_submission.csv'\nTRN_CURATED = DATA/'train_curated'\nTRN_NOISY = DATA/'train_noisy'\nTEST = DATA/'test'\n\nWORK = Path('work')\nIMG_TRN_CURATED = WORK/'image/trn_curated'\nIMG_TRN_NOISY = WORK/'image/train_noisy'\nIMG_TEST = WORK/'image/test'\n\ndf_train_curated = pd.read_csv(CSV_TRN_CURATED)\nprint(df_train_curated.head(10))\n# Collecting various data frames for further processing.\ndf_bark = df_train_curated.loc[df_train_curated['labels'] == 'Bark'][1:5]\ndf_run = df_train_curated.loc[df_train_curated['labels'] == 'Run'][1:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Following function gives the Y(amplitude) values of the wave. We can examine that when we take data for only two seconds, the length is 88200 which is 2 times sampling rate. Otherwise the entire length of soundclip is returned.","metadata":{}},{"cell_type":"code","source":"buzz_1_file =  DATA/'train_curated'/'02f54ef1.wav'\ny_2_secs = read_audio(conf, buzz_1_file, trim_long_data = True)\ny_full = read_audio(conf, buzz_1_file, trim_long_data = False)\nprint(len(y_full))\nprint(len(y_2_secs))\nprint(y_full.shape[0]/44100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Looking at shape of Data for a 'Bark' audio file.","metadata":{}},{"cell_type":"code","source":"bark_file = DATA/'train_curated'/'0006ae4e.wav'\ny_bark = read_audio(conf, buzz_1_file, trim_long_data = False)\nprint(y_bark.shape[0]/ 44100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Extraction using MFCC \n\n\n**Mel Frequency Cepstral Coefficient (MFCC) **\n\nThe first step in speech analytics is to extract features i.e. identify the components of the audio signal that are good for identifying the linguistic content and discarding all the other stuff which carries information like background noise, emotion etc.\n\nhttp://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/\n\n**What is the Mel scale? **\n\nThe Mel scale relates perceived frequency, or pitch, of a pure tone to its actual measured frequency. Humans are much better at discerning small changes in pitch at low frequencies than they are at high frequencies. Incorporating this scale makes our features match more closely what humans hear.\n\nThe librosa library provides functions for getting the melspectrogram from the audio data.","metadata":{}},{"cell_type":"code","source":"def audio_to_melspectrogram(conf, audio):\n    spectrogram = librosa.feature.melspectrogram(audio, \n                                                 sr=conf.sampling_rate,\n                                                 n_mels=conf.n_mels,\n                                                 hop_length=conf.hop_length,\n                                                 n_fft=conf.n_fft,\n                                                 fmin=conf.fmin,\n                                                 fmax=conf.fmax)\n    spectrogram = librosa.power_to_db(spectrogram)\n    spectrogram = spectrogram.astype(np.float32)\n    return spectrogram\n\ndef show_melspectrogram(conf, mels, title='Log-frequency power spectrogram'):\n    librosa.display.specshow(mels, x_axis='time', y_axis='mel', \n                             sr=conf.sampling_rate, hop_length=conf.hop_length,\n                            fmin=conf.fmin, fmax=conf.fmax)\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(title)\n    plt.show()\n\ndef read_as_melspectrogram(conf, pathname, trim_long_data, debug_display=False):\n    x = read_audio(conf, pathname, trim_long_data)\n    mels = audio_to_melspectrogram(conf, x)\n    if debug_display:\n        IPython.display.display(IPython.display.Audio(x, rate=conf.sampling_rate))\n        show_melspectrogram(conf, mels)\n    return mels\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Looking at data after reading mel features\n\nThe following block examines the various features of the audio clip. In this case, the first row which contains the barking clip. We are looking to derive 128 mel features or n_mels as supplied in the conf dict. The resulting data frame shape is defined as (n_mels, ). You can verify the first dimension is 128 in this case.","metadata":{}},{"cell_type":"code","source":"TRN_CURATED = DATA/'train_curated'\nx = read_audio(conf, TRN_CURATED/'0006ae4e.wav', trim_long_data=False)\nprint(x.shape)\nmels = audio_to_melspectrogram(conf, x)\nprint(mels.dtype)\nprint(mels.shape)\nprint(mels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following chunk of code draws the spectrogram image as provided by the librosa library. The corresponding audio files are also embedded here. We have used the function *librosa.display.specshow* to display the spectrogram. By looking at the functions, you wull notice that a number of functions from librosa library is used to extract the mel features and display them in image form.\n\nLets look at how **bark** and **buzz** categories look like.","metadata":{}},{"cell_type":"code","source":"bark = read_as_melspectrogram(conf, TRN_CURATED/'0006ae4e.wav', trim_long_data=False, debug_display=True)\nbuzz = read_as_melspectrogram(conf, TRN_CURATED/'02f54ef1.wav', trim_long_data=False, debug_display=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Printing shape of the two files. The second dimension vary, this is probably has to do with the length of the audio clip. First dimension always is the number of mels.","metadata":{}},{"cell_type":"code","source":"print(bark.shape)\nprint(buzz.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Converting numerical values to image\n\nThe array of numbers are converted into a image file. By simply stacking the X value in R,G and B dimensions,we can get the image. Notice that standardization of data is done here and later it is multiplied by 255.","metadata":{}},{"cell_type":"code","source":"def mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n    # Stack X as [X,X,X]\n    X = np.stack([X, X, X], axis=-1)\n\n    # Standardize\n    mean = mean or X.mean()\n    std = std or X.std()\n    Xstd = (X - mean) / (std + eps)\n    _min, _max = Xstd.min(), Xstd.max()\n    norm_max = norm_max or _max\n    norm_min = norm_min or _min\n    if (_max - _min) > eps:\n        # Scale to [0, 255]\n        V = Xstd\n        V[V < norm_min] = norm_min\n        V[V > norm_max] = norm_max\n        V = 255 * (V - norm_min) / (norm_max - norm_min)\n        V = V.astype(np.uint8)\n    else:\n        # Just zero\n        V = np.zeros_like(Xstd, dtype=np.uint8)\n    return V\n\ndef convert_wav_to_image(df, source, img_dest):\n    X = []\n    for i, row in tqdm_notebook(df.iterrows()):\n        x = read_as_melspectrogram(conf, source/str(row.fname), trim_long_data=False)\n        x_color = mono_to_color(x)\n        X.append(x_color)\n    return df, X\n\ndef convert_wav_to_cropped_image(df, source, img_dest):\n    X = []\n    for i, row in tqdm_notebook(df.iterrows()):\n        x = read_as_melspectrogram(conf, source/str(row.fname), trim_long_data=False)\n        x_color = mono_to_color(x)\n        # - - - - - - - - - - #\n        x_color = Image.fromarray(x_color)\n        time_dim, base_dim = x_color.size\n        crop_x = random.randint(0, time_dim - base_dim)\n        x_cropped = x_color.crop([crop_x, 0, crop_x+base_dim, base_dim]) \n        # - - - - - - - - - - #\n        X.append(x_cropped)\n    return df, X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Examining the shape of array before and after converting to image","metadata":{}},{"cell_type":"code","source":"print(bark.shape)\nbark_image = mono_to_color(bark)\nprint(bark_image.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cropping the image\n\nWe have used PIL library to manipulate and show the image. As in image data, we require dimensions of the image to be fixed, we are taking the number of mels to fix the other dimension. In this way, a cropped image can be produced as below. Feel free to run the following cell a few times to see that every time, the cropped image is a different one as it is pciked from the original image.","metadata":{}},{"cell_type":"code","source":"import random\nx = Image.fromarray(bark_image)\ndisplay(x)\ntime_dim, base_dim = x.size\ncrop_x = random.randint(0, time_dim - base_dim)\nx_cropped = x.crop([crop_x, 0, crop_x+base_dim, base_dim]) \ndisplay(x_cropped)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Putting it altogether with the help of **display_cropped_image** function. We will be looking at the various types of image of activities, e.g. Bark and Run respectively as below. You can observe the similarities/ dissimilaritites yourself.","metadata":{}},{"cell_type":"code","source":"def get_cropped_image(conf, path_of_image, display_image =True):\n    mel_spec_gram = read_as_melspectrogram(conf, path_of_image, trim_long_data=False, debug_display=False)\n    img_array = mono_to_color(mel_spec_gram)\n    img = Image.fromarray(img_array)\n    time_dim, base_dim = img.size\n    cropped = random.randint(0, time_dim - base_dim)\n    cropped_image = img.crop([cropped, 0, cropped+base_dim, base_dim]) \n    if display:\n        display(cropped_image)\n    return(cropped_image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Looking at various Bark files:","metadata":{}},{"cell_type":"code","source":"x1 = get_cropped_image(conf, TRN_CURATED/df_bark.iloc[0,0])\nx2 = get_cropped_image(conf, TRN_CURATED/df_bark.iloc[1,0])\nx3 = get_cropped_image(conf, TRN_CURATED/df_bark.iloc[2,0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Looking at various Run files:","metadata":{"trusted":true}},{"cell_type":"code","source":"get_cropped_image(conf, TRN_CURATED/df_run.iloc[0,0])\nget_cropped_image(conf, TRN_CURATED/df_run.iloc[1,0])\nget_cropped_image(conf, TRN_CURATED/df_run.iloc[2,0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting the Data Set for Training","metadata":{}},{"cell_type":"code","source":"df_test_bark, X_train_bark = convert_wav_to_cropped_image(df_bark, source=TRN_CURATED, img_dest=IMG_TRN_CURATED)\ndf_test_run, X_train_run = convert_wav_to_cropped_image(df_run, source=TRN_CURATED, img_dest=IMG_TRN_CURATED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Doing a quick dimension check","metadata":{}},{"cell_type":"code","source":"# Just a quick dimension check here.\ntest_np_bark = np.vstack(X_train_bark)\ntest_np_run = np.vstack(X_train_run)\nprint(test_np_bark.shape)\nprint(test_np_run.shape)\nend_time_data_processing = time.time()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section: 2 Building a convolutional model\n\nGetting a small dataset for building a toy model with 100 samples.","metadata":{}},{"cell_type":"code","source":"df_train_curated = df_train_curated.sample(100) # taking a sample data set of 100 examples.\ndf_train_curated, X_train_curated = convert_wav_to_cropped_image(df_train_curated, source=TRN_CURATED, img_dest=IMG_TRN_CURATED)\nnp_train_curated = np.vstack(X_train_curated)\nX_train_curated = np_train_curated.reshape(-1, 128, 128, 3) # Reshaping the training dataset.\nprint(X_train_curated.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train_curated\ndf_train = df_train_curated","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can also extract the **Noisy** Dataset and append to the training examples.","metadata":{}},{"cell_type":"code","source":"# Getting the labels required for submission, there are eighty of them.\ndf_test = pd.read_csv('../input/sample_submission.csv')\nlabel_columns = list( df_test.columns[1:] )\nlabel_mapping = dict((label, index) for index, label in enumerate(label_columns))\n#label_mapping\ndef split_and_label(rows_labels):\n    row_labels_list = []\n    for row in rows_labels:\n        row_labels = row.split(',')\n        labels_array = np.zeros((80))\n        \n        for label in row_labels:\n            index = label_mapping[label]\n            labels_array[index] = 1\n        \n        row_labels_list.append(labels_array)\n    \n    return row_labels_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_curated_labels = split_and_label(df_train['labels'])\nfor f in label_columns:\n    df_train[f] = 0.0 # This adds all the labels as column names.\n\ndf_train[label_columns] = train_curated_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train inputs and Response are available now for further processing.","metadata":{}},{"cell_type":"code","source":"Y_train = np.vstack(train_curated_labels)\nprint(Y_train.shape)\nprint(X_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting test Data","metadata":{}},{"cell_type":"code","source":"df_test, X_test = convert_wav_to_cropped_image(df_test, source=TEST, img_dest=IMG_TEST)\nnp_test_small = np.vstack(X_test)\nprint(np_test_small.shape)\nX_test = np_test_small.reshape(-1, 128, 128, 3)\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"number of training examples = \" + str(X_train.shape[0]))\nprint (\"number of test examples = \" + str(X_test.shape[0]))\nprint (\"X_train shape: \" + str(X_train.shape))\nprint (\"Y_train shape: \" + str(Y_train.shape))\nprint (\"X_test shape: \" + str(X_test.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing Keras and Other libraries for Building Convolutional Models","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom keras import layers\nfrom keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.applications.imagenet_utils import preprocess_input\nimport pydot\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\nfrom keras.initializers import glorot_uniform\n#from kt_utils import *\n\nimport keras.backend as K\nK.set_image_data_format('channels_last')\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\n\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building the convolutional model with 4 convolutional layers","metadata":{}},{"cell_type":"code","source":"def Audio2DConvModel(input_shape, classes):\n    \"\"\"\n    Implementation of the Basic Model.\n    Arguments:\n    input_shape -- shape of the images of the dataset\n    Returns:\n    model -- a Model() instance in Keras\n    \"\"\"\n    X_input = Input(input_shape)\n    X = ZeroPadding2D((3, 3))(X_input)\n    X = Conv2D(64, (16, 16), strides = (1, 1), name = 'conv0')(X)\n    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n    X = Activation('relu')(X)\n    #X = MaxPooling2D((2, 2), name='max_pool')(X)\n    X = MaxPooling2D(pool_size=(2, 2), name = 'maxpool_0')(X)\n    X = Dropout(rate=0.1)(X)\n# -------------------------------------------------------------------------------------\n    X = ZeroPadding2D((3, 3))(X)\n    X = Conv2D(32, (8, 8), strides = (1, 1), name = 'conv1')(X)\n    X = BatchNormalization(axis = 3, name = 'bn1')(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D(pool_size=(2, 2), name = 'maxpool_1')(X)\n    # ------------------------------------------------------------------------------------\n    X = ZeroPadding2D((3, 3))(X)\n    X = Conv2D(16, (4, 4), strides = (1, 1), name = 'conv2')(X)\n    X = BatchNormalization(axis = 3, name = 'bn2')(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((2, 2), name='maxpool_2')(X)\n    #------------------------------------------------------------\n    X = ZeroPadding2D((3, 3))(X)\n    X = Conv2D(16, (2, 2), strides = (1, 1), name = 'conv3')(X)\n    X = BatchNormalization(axis = 3, name = 'bn32')(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((2, 2), name='maxpool_3')(X)\n    #X = AveragePooling2D(pool_size=(2, 2), name = 'avg_pool_1')(X)\n    # -------------------------------------------------------------------------------------    \n    X = Flatten()(X)\n    X = Dense(classes, activation='softmax', name='fc_softmax' + str(classes), kernel_initializer=glorot_uniform(seed=0))(X)\n    model = Model(inputs = X_input, outputs = X, name='Audio2DConvModel')\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building Convolutional Model and looking at summary","metadata":{}},{"cell_type":"code","source":"basic_conv_model = Audio2DConvModel(X_train.shape[1:], 80)\nbasic_conv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nbasic_conv_model.fit(X_train/255, Y_train, epochs=1, batch_size=64)\nbasic_conv_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating predictions","metadata":{}},{"cell_type":"code","source":"y_hat = basic_conv_model.predict(X_test/255)\ndf_test[label_columns] = y_hat\nprint(df_test.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's it. Thanks for reading. Currently I have used only 100 observations of curated data. This can be enhanced with noisy dataset. Also deep layers can be embedded and regularization can be done as well. Stay tuned!","metadata":{}}]}