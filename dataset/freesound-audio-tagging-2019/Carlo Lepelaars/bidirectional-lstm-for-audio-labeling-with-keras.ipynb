{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Birectional LSTM model for audio labeling with Keras"},{"metadata":{},"cell_type":"markdown","source":"![](https://camo.githubusercontent.com/71098172b2e99002427c06aa95eb7fdc7252519f/68747470733a2f2f626c6f672e66726565736f756e642e6f72672f77702d636f6e74656e742f75706c6f6164732f323031372f31322f757064617465645f6c6f676f2e706e67)"},{"metadata":{},"cell_type":"markdown","source":"In this Kaggle kernel we will use the curated data from the \"Freesound Audio Tagging 2019\" competition to predict the labels of .wav files. "},{"metadata":{},"cell_type":"markdown","source":"## Table of contents"},{"metadata":{},"cell_type":"markdown","source":"- [Data Description](#1)\n- [Dependencies](#2)\n- [Evaluation Metric](#3)\n- [Helper Functions and Preprocessing](#4)\n- [Modeling](#5)\n- [Visualization and Evaluation](#6)\n- [Predictions and Submission](#7)\n- [Final Checks](#8)"},{"metadata":{},"cell_type":"markdown","source":"## Data Description <a id=\"1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"From [Kaggle's data page](https://www.kaggle.com/c/freesound-audio-tagging-2019/data) for the competition:\n\nThe curated subset is a small set of manually-labeled data from FSD.\n\nNumber of clips/class: 75 except in a few cases (where there are less)\n\nTotal number of clips: 4970\n\nAvge number of labels/clip: 1.2\n\nTotal duration: 10.5 hours\n\nThe duration of the audio clips ranges from 0.3 to 30s due to the diversity of the sound categories and the preferences of Freesound users when recording/uploading sounds. It can happen that a few of these audio clips present additional acoustic material beyond the provided ground truth label(s).\n\n**Test Set:**\n\nThe test set is used for system evaluation and consists of manually-labeled data from FSD. Since most of the train data come from YFCC, some acoustic domain mismatch between the train and test set can be expected. All the acoustic material present in the test set is labeled, except human error, considering the vocabulary of 80 classes used in the competition.\n\n**Columns:**\n\n*fname*: the audio file name, eg, 0006ae4e.wav\n*labels*: the audio classification label(s) (ground truth). Note that the number of labels per clip can be one, eg, Bark or more, eg, \"Walk_and_footsteps,Slam\"."},{"metadata":{},"cell_type":"markdown","source":"## Dependencies <a id=\"2\"></a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Dependencies\nimport numpy as np \nimport pandas as pd\nimport os\nimport librosa\nimport matplotlib.pyplot as plt\nimport gc\nimport time\nfrom tqdm import tqdm, tqdm_notebook; tqdm.pandas() # Progress bar\nfrom sklearn.metrics import label_ranking_average_precision_score\nfrom sklearn.model_selection import train_test_split\n\n# Machine Learning\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.layers import (Dense, Bidirectional, CuDNNLSTM, ELU,\n                          Dropout, LeakyReLU, Conv1D, BatchNormalization)\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\n\n# Path specifications\nKAGGLE_DIR = '../input/'\ntrain_curated_path = KAGGLE_DIR + 'train_curated/'\ntest_path = KAGGLE_DIR + 'test/'\n\n# Set seed for reproducability\nseed = 1234\nnp.random.seed(seed)\ntf.set_random_seed(seed)\n\n# File sizes and specifications\nprint('\\n# Files and file sizes')\nfor file in os.listdir(KAGGLE_DIR):\n    print('{}| {} MB'.format(file.ljust(30), \n                             str(round(os.path.getsize(KAGGLE_DIR + file) / 1000000, 2))))\n\n# For keeping time. GPU limit for this competition is set to 60 min.\nt_start = time.time()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation metric <a id=\"3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"From the [competition evaluation page](https://www.kaggle.com/c/freesound-audio-tagging-2019/overview/evaluation):\n\nThe task consists of predicting the audio labels (tags) for every test clip. Some test clips bear one label while others bear several labels. The predictions are to be done at the clip level, i.e., no start/end timestamps for the sound events are required.\n\nThe primary competition metric will be label-weighted [label-ranking average precision](https://scikit-learn.org/stable/modules/model_evaluation.html#label-ranking-average-precision) (lwlrap, pronounced \"Lol wrap\"). This measures the average precision of retrieving a ranked list of relevant labels for each test clip (i.e., the system ranks all the available labels, then the precisions of the ranked lists down to each true label are averaged). This is a generalization of the mean reciprocal rank measure (used in last yearâ€™s edition of the competition) for the case where there can be multiple true labels per test item. The novel \"label-weighted\" part means that the overall score is the average over all the labels in the test set, where each label receives equal weight (by contrast, plain lrap gives each test item equal weight, thereby discounting the contribution of individual labels when they appear on the same item as multiple other labels).\n\nThe formula for label-ranking average precision (LRAP) is as follows:\n\n$LRAP(y, \\hat{f}) = \\frac{1}{n_{\\text{samples}}}\n  \\sum_{i=0}^{n_{\\text{samples}} - 1} \\frac{1}{||y_i||_0}\n  \\sum_{j:y_{ij} = 1} \\frac{|\\mathcal{L}_{ij}|}{\\text{rank}_{ij}}$"},{"metadata":{},"cell_type":"markdown","source":"Happily, the evaluation metric is provided by Kaggle and can be found in this [Google Colab file](https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8#scrollTo=52LPXQNPppex)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_overall_lwlrap_sklearn(truth, scores):\n    \"\"\"Calculate the overall lwlrap using sklearn.metrics.lrap.\"\"\"\n    # sklearn doesn't correctly apply weighting to samples with no labels, so just skip them.\n    sample_weight = np.sum(truth > 0, axis=1)\n    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n    overall_lwlrap = label_ranking_average_precision_score(\n        truth[nonzero_weight_sample_indices, :] > 0, \n        scores[nonzero_weight_sample_indices, :], \n        sample_weight=sample_weight[nonzero_weight_sample_indices])\n    return overall_lwlrap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helper Functions and Preprocessing <a id=\"4\"></a>"},{"metadata":{},"cell_type":"markdown","source":"I got the inspiration for most of the preprocessing steps and the attention layer from [this Kaggle kernel](https://www.kaggle.com/chewzy/gru-w-attention-baseline-model-curated)."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_classes = 80\n\ndef split_and_label(rows_labels, n_classes):\n    '''\n    Retrieves a list of all the relevant classes. This is necessary due to \n    the multi-labeling of the initial csv file.\n    '''\n    row_labels_list = []\n    for row in rows_labels:\n        row_labels = row.split(',')\n        labels_array = np.zeros((n_classes))\n        for label in row_labels:\n            index = label_mapping[label]\n            labels_array[index] = 1\n        row_labels_list.append(labels_array)\n    return row_labels_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load in data\ndf = pd.read_csv(KAGGLE_DIR + 'train_curated.csv')\ntest_df = pd.read_csv(KAGGLE_DIR + 'sample_submission.csv')\n\n# Retrieve labels\nlabel_columns = test_df.columns[1:]\nlabel_mapping = dict((label, index) for index, label in enumerate(label_columns))\nfor col in label_columns:\n    df[col] = 0  \ndf[label_columns] = split_and_label(df['labels'], n_classes)\ndf['num_labels'] = df[label_columns].sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Check dataframes\nprint('Training dataframe:')\ndisplay(df.head(3))\nprint('Testing dataframe:')\ntest_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing parameters\nsr = 44100 # Sampling rate\nduration = 5\nhop_length = 347 # to make time steps 128\nfmin = 20\nfmax = sr // 2\nn_mels = 128\nn_fft = n_mels * 20\nsamples = sr * duration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_audio(path):\n    '''\n    Reads in the audio file and returns\n    an array that we can turn into a melspectogram\n    '''\n    y, _ = librosa.core.load(path, sr=44100)\n    # trim silence\n    if 0 < len(y): # workaround: 0 length causes error\n        y, _ = librosa.effects.trim(y)\n    if len(y) > samples: # long enough\n        y = y[0:0+samples]\n    else: # pad blank\n        padding = samples - len(y)\n        offset = padding // 2\n        y = np.pad(y, (offset, samples - len(y) - offset), 'constant')\n    return y\n\ndef audio_to_melspectrogram(audio):\n    '''\n    Convert to melspectrogram after audio is read in\n    '''\n    spectrogram = librosa.feature.melspectrogram(audio, \n                                                 sr=sr,\n                                                 n_mels=n_mels,\n                                                 hop_length=hop_length,\n                                                 n_fft=n_fft,\n                                                 fmin=fmin,\n                                                 fmax=fmax)\n    return librosa.power_to_db(spectrogram).astype(np.float32)\n\ndef read_as_melspectrogram(path):\n    '''\n    Convert audio into a melspectrogram \n    so we can use machine learning\n    '''\n    mels = audio_to_melspectrogram(read_audio(path))\n    return mels\n\ndef convert_wav_to_image(df, path):\n    X = []\n    for _,row in tqdm_notebook(df.iterrows()):\n        x = read_as_melspectrogram('{}/{}'.format(path[0],\n                                                  str(row['fname'])))\n        X.append(x.transpose())\n    return X\n\ndef normalize(img):\n    '''\n    Normalizes an array \n    (subtract mean and divide by standard deviation)\n    '''\n    eps = 0.001\n    if np.std(img) != 0:\n        img = (img - np.mean(img)) / np.std(img)\n    else:\n        img = (img - np.mean(img)) / eps\n    return img\n\ndef normalize_dataset(X):\n    '''\n    Normalizes list of arrays\n    (subtract mean and divide by standard deviation)\n    '''\n    normalized_dataset = []\n    for img in X:\n        normalized = normalize(img)\n        normalized_dataset.append(normalized)\n    return normalized_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Preprocess dataset and create validation sets\nX = np.array(convert_wav_to_image(df, [train_curated_path]))\nX = normalize_dataset(X)\nY = df[label_columns].values\nx_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.1, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Visualize an melspectogram example\nplt.figure(figsize=(15,10))\nplt.title('Visualization of audio file', weight='bold')\nplt.imshow(X[0]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling <a id=\"5\"></a>"},{"metadata":{},"cell_type":"markdown","source":"My main inspiration for this architecture has been [this paper](https://arxiv.org/pdf/1602.05875v3.pdf)."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n        if self.bias:\n            eij += self.b\n        eij = K.tanh(eij)\n        a = K.exp(eij)\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Neural network model\ninput_shape = (636,128)\noptimizer = Adam(0.005, beta_1=0.1, beta_2=0.001, amsgrad=True)\nn_classes = 80\n\nmodel = Sequential()\nmodel.add(Bidirectional(CuDNNLSTM(256, return_sequences=True), input_shape=input_shape))\nmodel.add(Attention(636))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(400))\nmodel.add(ELU())\nmodel.add(Dropout(0.2)) \nmodel.add(Dense(n_classes, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer,\n              metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"# Train model\nes = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=10)\nhist = model.fit(np.array(x_train),\n          y_train,\n          batch_size=1024,\n          epochs=500,\n          validation_data=(np.array(x_val), y_val),\n          callbacks = [es])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization and Evaluation <a id=\"6\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Simple visualizations to keep track of the loss and accuracy over the epochs."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Visualize loss\nloss = hist.history['loss']\nval_loss = hist.history['val_loss']\nstopped_epoch = es.stopped_epoch\nepochs = range(stopped_epoch+1)\n\nplt.figure(figsize=(15,5))\nplt.plot(epochs, loss)\nplt.plot(epochs, val_loss)\nplt.title('Loss over epochs', weight='bold', fontsize=22)\nplt.xlabel('Epochs', fontsize=16)\nplt.ylabel('Loss', fontsize=16)\nplt.legend(['Training loss', 'Validation loss'], fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Visualize Accuracy\nacc = hist.history['acc']\nval_acc = hist.history['val_acc']\nepochs = range(stopped_epoch+1)\n\nplt.figure(figsize=(15,5))\nplt.plot(epochs, acc)\nplt.plot(epochs, val_acc)\nplt.title('Accuracy over epochs', weight='bold', fontsize=22)\nplt.xlabel('Epochs', fontsize=16)\nplt.ylabel('Accuracy', fontsize=16)\nplt.legend(['Training accuracy', 'Validation accuracy'], fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Training accuracy LWLRAP score:**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Make predictions for training set and validation set\ny_train_pred = model.predict(np.array(x_train))\ny_val_pred = model.predict(np.array(x_val))\ntrain_lwlrap = calculate_overall_lwlrap_sklearn(y_train, y_train_pred)\nval_lwlrap = calculate_overall_lwlrap_sklearn(y_val, y_val_pred)\n\n# Check training and validation LWLRAP score\nprint('Training LWLRAP : {}'.format(round(train_lwlrap,4)))\nprint('Validation LWLRAP : {}'.format(round(val_lwlrap,4)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions and submission <a id=\"7\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Preprocess the test set, make predictions and store them as a csv file for our submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare test set\nX_test = np.array(convert_wav_to_image(test_df, [test_path]))\nX_test = normalize_dataset(X_test)\n# Make predictions\npredictions = model.predict(np.array(X_test))\n# Save predictions in a csv file\ntest_df[label_columns] = predictions\ntest_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final checks <a id=\"8\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Lastly, we check if the submission format is correct and if we are under the one hour limit of GPU time."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Check submission format\ndisplay(test_df.head())\n\n# Check if we are under one hour of GPU time\nt_finish = time.time()\ntotal_time = round((t_finish-t_start)/3600, 4)\nprint('Kernel runtime = {} hours ({} minutes)'.format(total_time, \n                                                      int(total_time*60)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**If you like this Kaggle kernel, feel free to give an upvote and leave a comment! I will try to implement your suggestions in this kernel!**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}