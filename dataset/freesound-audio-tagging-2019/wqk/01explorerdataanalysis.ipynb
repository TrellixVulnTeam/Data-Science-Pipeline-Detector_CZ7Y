{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Intro\nThe data set for audio tagging 2019 is designed with label noise. This notebook is to explore the basic information, the audio features of three subsets.\n\nThanks to below kagglers, some functions are re-orgnized here. Wish this notebook could be helpful.\n\nref:\n* https://www.kaggle.com/maxwell110/explore-multi-labeled-data\n* https://www.kaggle.com/dude431/beginner-s-visualization-and-removing-uniformative"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport IPython.display as ipd\nimport librosa","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Basic information\nBasice information about the data set, such as head of data, numbers, label distributions and so on."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\nimport os\nprint(os.listdir(\"../input\"))\n# !ls ../input/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train_curated.csv\")\ntrain['is_curated'] = True\ntrain_noisy = pd.read_csv(\"../input/train_noisy.csv\")\ntrain_noisy['is_curated'] = False\ntrain = pd.concat([train, train_noisy], axis=0)\ndel train_noisy\n# print(train.head())\n# print(train.tail())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['n_label'] = train.labels.str.split(',').apply(lambda x: len(x))\nprint(train.head(10))\nprint(train.tail(6))\nprint(\"Number of train examples: \", train.shape[0])\nprint(\"In curated subset: \", train[train.is_curated == True].shape[0])\nprint(\"In noisy subset: \", train[train.is_curated == False].shape[0])\n# print(\"Number of classes:\", len(set(train.labels)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/sample_submission.csv\")\n# print(test.head())\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of test examples: \", test.shape[0],\n      \"\\nNumber of classes: \", len(set(test.columns[1:])))\nprint(set(test.columns[1:]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Audio duration\n\nIt's introduced that durations of samples in curated subset are from 0.3s to 30s, while those in noisy subset are from 1s to 15s, with the vast majority lasting 15s."},{"metadata":{"trusted":true},"cell_type":"code","source":"import wave\nSAMPLE_RATE = 44100\n\ntrain_1 = train[train.is_curated == True].sort_values('labels').reset_index()\ntrain_1['nframes'] = train_1['fname'].apply(lambda f: \n    wave.open('../input/train_curated/' + f).getnframes()/SAMPLE_RATE)\ntrain_1.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_2 = train[train.is_curated == False].sort_values('labels').reset_index()\ntrain_2['nframes'] = train_2['fname'].apply(lambda f: \n    wave.open('../input/train_noisy/' + f).getnframes()/SAMPLE_RATE)\ntrain_2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_new = test\ntest_new['nframes'] = test_new['fname'].apply(lambda f: \n    wave.open('../input/test/' + f).getnframes()/SAMPLE_RATE)\ntest_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(3, 1, figsize=(16,5))\ntrain_1.nframes.hist(bins=100, grid=True, rwidth=0.5, color='blue', ax=axes[0])\ntrain_2.nframes.hist(bins=100, grid=True, rwidth=0.5, color='black', ax=axes[1])\ntest_new.nframes.hist(bins=100, grid=True, rwidth=0.5, color='red', ax=axes[2])\nplt.suptitle('Duration Distribution in curated, noisy and test set', ha='center', fontsize='large');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Majority of the audio files are short than 10s, when we crop audio as cnn train data, 4-8 seconds should be OK.\n\nThere are an abnormal length in the train histogram."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1.query(\"nframes > 30\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ipd.Audio( '../input/train_curated/' + '77b925c2.wav')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_1\ndel train_2\ndel test_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train set labels distributions:\nSamples in curated subset have 1, 2, 3, 4, 6 labels, while samples in noisy subset have 1, 2, 3, 4, 5, 6, 7 labels. Most samples have a single label."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.query('is_curated == True').n_label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.query('is_curated == False').n_label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_gp = train[train.n_label == 1].groupby(\n    ['labels', 'is_curated']).agg({'fname':'count'}).reset_index()\ncat_gpp = cat_gp.pivot(index='labels', columns='is_curated', values='fname').reset_index().set_index('labels')\n\nplot = cat_gpp.plot(\n    kind='barh',\n    title=\"Number of samples per category\",\n    stacked=True,\n    color=['deeppink', 'darkslateblue'],\n    figsize=(15,20))\nplot.set_xlabel(\"Number of Samples\", fontsize=20)\nplot.set_ylabel(\"Label\", fontsize=20);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Wordcloud for labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nwordcloud = WordCloud(max_font_size=50, width=600, height=300).generate(' '.join(train.labels))\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.title(\"Wordcloud for Labels\", fontsize=35)\nplt.axis(\"off\")\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Audio features\nTry to listen samples of specific label, check waveform and spectrogram together."},{"metadata":{"trusted":true},"cell_type":"code","source":"import librosa, librosa.display\nimport matplotlib.pyplot as plt\nimport os\nimport IPython\nSAMPLE_RATE = 44100\n\ndef load_and_show(path, fname):\n    plt.figure(figsize=(10,3))\n    wav, sr = librosa.core.load(os.path.join(path, fname))\n#     melspec = librosa.feature.melspectrogram(\n#         librosa.resample(wav, sr, SAMPLE_RATE),\n#         sr=SAMPLE_RATE/2, n_fft = 1024,\n#         hop_length=512, n_mels= 128\n#     )\n#     logmel = librosa.core.power_to_db(melspec)\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(wav)), ref=np.max)\n    # https://www.kaggle.com/c/freesound-audio-tagging-2019/discussion/91827#latest-529419\n#     D = librosa.pcen(np.abs(librosa.stft(wav)))\n    # CQT = librosa.amplitude_to_db(np.abs(librosa.cqt(y, sr=sr)), ref=np.max)\n    plt.subplot(1,2,1)\n    librosa.display.waveplot(wav, sr=SAMPLE_RATE)\n    plt.subplot(1,2,2)\n    librosa.display.specshow(D, sr=SAMPLE_RATE, x_axis='time', y_axis='linear')\n    plt.title(os.path.join(path, fname))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sampling an audio in train_curated\n# choose a label randomly\nsamp = train[(train.n_label == 1) & (train.is_curated == True)].sample(1)\nspecific_label = samp.labels.values[0]\n\n# for the specific label, choose several samples\nsamples = train[(train.n_label == 1) \n               & (train.is_curated == True) \n               & (train.labels == specific_label)].sample(5)\n\n# Listen and check the wave, spectrogram of the samples.\nprint('Label:', specific_label)\nfor fname in list(samples.fname):\n#     print('../input/train_curated/{}'.format(fname))\n    load_and_show('../input/train_curated', fname)\n    IPython.display.display(ipd.Audio('../input/train_curated/{}'.format(fname)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sampling an audio in train_noisy\n# for the specific label, choose several samples\nsamples = train[(train.n_label == 1) \n               & (train.is_curated == False) \n               & (train.labels == specific_label)].sample(5)\n\n# Listen and check the wave, spectrogram of the samples.\nprint('Label:', specific_label)\nfor fname in list(samples.fname):\n    print('../input/train_noisy/{}'.format(fname))\n    load_and_show('../input/train_noisy', fname)\n    IPython.display.display(ipd.Audio('../input/train_noisy/{}'.format(fname)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can try some different labels, it seems a lot of noisy data have wrong labels.\n\nTo let the noisy data make positive effect, designed loss function or semi-supervised learning should be used."},{"metadata":{},"cell_type":"markdown","source":"### Multi labels\n\nAbout 1/5 of samples in curated and noisy subset are with multi labels, so let's check the samples with multi labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_gp = train[(train.n_label > 1) & (train.is_curated == True)].groupby('labels').agg({'fname':'count'})\ncat_gp.columns = ['counts']\n\nplot = cat_gp.sort_values(ascending=True, by='counts').plot(\n    kind='barh',\n    title=\"Number of Audio Samples per Category\",\n    color='deeppink',\n    figsize=(15,30))\nplot.set_xlabel(\"Number of Samples\", fontsize=20)\nplot.set_ylabel(\"Label\", fontsize=20);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are only about 10 kinds of multi labels whose samples are more than 10."},{"metadata":{},"cell_type":"markdown","source":"#### 2 label conditions"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_set = set(train.loc[(train.n_label == 2) & (train.is_curated == True), 'labels']) & set(\n    train.loc[(train.n_label == 2) & (train.is_curated == False), 'labels'])\n\nlabel_samp = np.random.choice(list(label_set), 1)[0]\nsamp = train[(train.labels == label_samp) & (train.is_curated == True)].sample(1)\nprint(label_samp)\nIPython.display.display(ipd.Audio('../input/train_curated/{}'.format(samp.fname.values[0])))\nload_and_show('../input/train_curated', samp.fname.values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sampling an audio in train_noisy\nsamp_n = train[(train.labels == label_samp) & (train.is_curated == False)].sample(1)\nprint(samp_n.labels.values[0])\nIPython.display.display(ipd.Audio('../input/train_noisy/{}'.format(samp_n.fname.values[0])))\nload_and_show('../input/train_noisy', samp_n.fname.values[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3 labels conditions"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_set = set(train.loc[(train.n_label == 3) & (train.is_curated == True), 'labels']) & set(\n    train.loc[(train.n_label == 3) & (train.is_curated == False), 'labels'])\n\nlabel_samp = np.random.choice(list(label_set), 1)[0]\nsamp = train[(train.labels == label_samp) & (train.is_curated == True)].sample(1)\nprint('File name:', '../input/train_curated/{}'.format(samp.fname.values[0]), '\\nLabel:', label_samp)\nIPython.display.display(ipd.Audio('../input/train_curated/{}'.format(samp.fname.values[0])))\nload_and_show('../input/train_curated', samp.fname.values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sampling an audio in train_noisy\nsamp_n = train[(train.labels == label_samp) & (train.is_curated == False)].sample(1)\nprint('File name:', '../input/train_noisy/{}'.format(samp_n.fname.values[0]), '\\nLabel:', samp_n.labels.values[0])\nIPython.display.display(ipd.Audio('../input/train_noisy/{}'.format(samp_n.fname.values[0])))\nload_and_show('../input/train_noisy', samp_n.fname.values[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4 and more labels\n\nThere are no common record with same multi labels in curated and noisy. Will check audios in different subsets."},{"metadata":{"trusted":true},"cell_type":"code","source":"samp = train[(train.n_label == 4) & (train.is_curated == True)].sample(1)\nprint('File name:', '../input/train_curated/{}'.format(samp.fname.values[0]), '\\nLabel:', samp.labels.values[0])\nIPython.display.display(ipd.Audio('../input/train_curated/{}'.format(samp.fname.values[0])))\nload_and_show('../input/train_curated', samp.fname.values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samp = train[(train.n_label == 4) & (train.is_curated == False)].sample(1)\nprint('File name:', '../input/train_noisy/{}'.format(samp.fname.values[0]), '\\nLabel:', samp.labels.values[0])\nIPython.display.display(ipd.Audio('../input/train_noisy/{}'.format(samp.fname.values[0])))\nload_and_show('../input/train_noisy', samp.fname.values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samp = train[(train.n_label == 5) & (train.is_curated == False)].sample(1)\nprint('File name:', '../input/train_noisy/{}'.format(samp.fname.values[0]), '\\nLabel:', samp.labels.values[0])\nIPython.display.display(ipd.Audio('../input/train_noisy/{}'.format(samp.fname.values[0])))\nload_and_show('../input/train_noisy', samp.fname.values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samp = train[(train.n_label == 6) & (train.is_curated == False)].sample(1)\nprint('File name:', '../input/train_noisy/{}'.format(samp.fname.values[0]), '\\nLabel:', samp.labels.values[0])\nIPython.display.display(ipd.Audio('../input/train_noisy/{}'.format(samp.fname.values[0])))\nload_and_show('../input/train_noisy', samp.fname.values[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Summary\n\n* Samples with multi labels sound really difficult even for people to label all events correctly. Spectrograms of those samples maybe confused with all events happen the same time.\n\neg. \"File name: ../input/train_noisy/5fde4352.wav Label: Fill_(with_liquid),Water_tap_and_faucet,Hiss,Toilet_flush,Sink_(filling_or_washing)\"\n\n* **A lot of noisy data seem to have wrong labels**. In the previous competition, using the robust loss function to suppress the effect of mislabeled data was one of the important points to get high score. We must care of that again."},{"metadata":{},"cell_type":"markdown","source":"### Co-occurrence\n\nHow to get labels for the trainning process.\n\nref:\n* https://www.kaggle.com/maxwell110/explore-multi-labeled-data"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}