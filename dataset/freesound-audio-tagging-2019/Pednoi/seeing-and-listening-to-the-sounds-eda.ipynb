{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hi, everyone. This is a simple kernel that samples sounds from both the curated and noisy datasets and displays them as audible audios and their corresponding mel-scaled power spectrograms.\n\nI make this kernel because it might be useful to inspect data characteristics, for example, it is noticeable that files in the noisy dataset are often cacophony and contain not only labeled sounds but also other sounds as well."},{"metadata":{},"cell_type":"markdown","source":"You can fork this kernel and change these following parameters to examine the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"SAMPLES = 3 # the number of displayed samples\nSEED = 2019 # the seed that is used to generate samples\nLABEL = 'Bark' # the label to examine (all labels are listed in the next cell output)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import os\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nimport librosa\nimport IPython.display as ipd\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/sample_submission.csv')\nlabels = test_df.columns[1:].tolist()\nprint(labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The functions about audios and spectrograms are from [this great kernel](https://www.kaggle.com/daisukelab/creating-fat2019-preprocessed-data) by [daisukelab](https://www.kaggle.com/daisukelab)."},{"metadata":{"trusted":true},"cell_type":"code","source":"class conf:\n    sampling_rate = 44100\n    duration = 2 # sec\n    hop_length = 347 * duration # sampling_rate * duration must be reduced to 128 which is the image width of duration\n    fmin = 20\n    fmax = sampling_rate // 2\n    n_mels = 128 # so the height of the image is 128\n    n_fft = n_mels * 20\n    padmode = 'constant'\n    samples = sampling_rate * duration\n    \ndef read_audio(conf, pathname, trim_long_data):\n    y, sr = librosa.load(pathname, sr=conf.sampling_rate)\n    # trim silence\n    if 0 < len(y): # workaround: 0 length causes error\n        y, _ = librosa.effects.trim(y) # trim, top_db=default(60)\n    # make it unified length to conf.samples\n    if len(y) > conf.samples: # long enough\n        if trim_long_data:\n            y = y[0:0+conf.samples]\n    else: # pad blank\n        padding = conf.samples - len(y)    # add padding at both ends\n        offset = padding // 2\n        y = np.pad(y, (offset, conf.samples - len(y) - offset), conf.padmode)\n    return y\n\ndef audio_to_melspectrogram(conf, audio, scale=1):\n    spectrogram = librosa.feature.melspectrogram(audio, \n                                                 sr=conf.sampling_rate,\n                                                 n_mels=conf.n_mels,\n                                                 hop_length=conf.hop_length,\n                                                 n_fft=int(conf.n_fft*scale),\n                                                 fmin=conf.fmin,\n                                                 fmax=conf.fmax)\n    spectrogram = librosa.power_to_db(spectrogram)\n    spectrogram = spectrogram.astype(np.float32)\n    return spectrogram","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"curated_df = pd.read_csv('../input/train_curated.csv')\nnoisy_df = pd.read_csv('../input/train_noisy.csv')\n\ncurated_df['label_list'] = curated_df['labels'].str.split(',')\nnoisy_df['label_list'] = noisy_df['labels'].str.split(',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_samples(df, source_folder):\n    samples = df[df['label_list'].apply(lambda x: LABEL in x)].sample(SAMPLES, random_state=SEED)\n    for i, row in samples.iterrows():\n        print(f'{i} {row.fname} {row.labels}')\n        audio = read_audio(conf, source_folder/row.fname, trim_long_data=False)\n        ipd.display(ipd.Audio(audio, rate=conf.sampling_rate))\n        spectrogram = audio_to_melspectrogram(conf, audio)\n        plt.figure(figsize=(15, 3))\n        plt.imshow(spectrogram)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_samples(curated_df, Path('../input/train_curated'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_samples(noisy_df, Path('../input/train_noisy'))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}