{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nfrom scipy import signal\nimport numpy as np  \nimport pandas as pd \nimport numpy as np \nimport pandas as pd \nfrom scipy.io import wavfile\nimport os\nimport glob\nimport pickle\nfrom sklearn.model_selection import train_test_split \nimport librosa as lbr\nimport IPython.display as ipd\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.pyplot as plt\nimport os\nimport librosa.display\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"INPUT_FOLDER = \"../input/\"\ntrain_files = glob.glob(\"../input/train_curated/*.wav\")\ntest_files=glob.glob(\"../input/test/*.wav\")\nprint(os.listdir(INPUT_FOLDER))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST = INPUT_FOLDER + \"sample_submission.csv\"\ntest = pd.read_csv(TEST)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_curated = pd.read_csv(\"../input/train_curated.csv\")\ntrain_curated['is_curated'] = True\ntrain_noisy = pd.read_csv('../input/train_noisy.csv')\ntrain_noisy['is_curated'] = False\ntrain = pd.concat([train_curated, train_noisy], axis=0)\ndel train_noisy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of train examples=\", train.shape[0], \"  Number of classes=\", len(set(train.labels)))\nprint(\"Number of test examples=\", test.shape[0], \"  Number of classes=\", len(set(test.columns[1:])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that the number of classes in training curated is much larger than in testing file"},{"metadata":{"trusted":true},"cell_type":"code","source":"#get only the lables that are in the testing file\n#train is for one lable per class data, train_curated is the multilabel dataset\n# train = train[train.labels.isin(test.columns[1:])]\n# print(len(train))\n# category_group = train.groupby(['labels']).count()['fname']\n# category_group.columns = ['counts']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Minimum samples per category = ', min(train.labels.value_counts()))\nprint('Maximum samples per category = ', max(train.labels.value_counts()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['n_label'] = train.labels.str.split(',').apply(lambda x: len(x))\nprint('curated\\n',train.query('is_curated == True').n_label.value_counts())\nprint('noisy\\n',train.query('is_curated == False').n_label.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#chacking the multilables\n#[label.split(',') for i, label in enumerate(train['labels']) if len(label.split(',')) >=2] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get target names from test \ntarget_names = test.columns[1:]\ntarget_names.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_targets = len(target_names)\n\nsrc_dict = {target_names[i]:i for i in range(num_targets)}\nsrc_dict_inv = {i:target_names[i] for i in range(num_targets)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_hot(labels, src_dict):\n    ar = np.zeros([len(labels), len(src_dict)])\n    invalid=['77b925c2.wav','f76181c4.wav', '6a1f682a.wav', 'c7db12aa.wav', '7752cc8a.wav','1d44b0bd.wav']\n    for i, label in enumerate(labels): \n        if label not in invalid:\n            label_list = label.split(',')\n            for la in label_list:\n                ar[i, src_dict[la]] = 1\n    return ar","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Listen to the audio"},{"metadata":{},"cell_type":"markdown","source":"Track from train curated"},{"metadata":{"trusted":true},"cell_type":"code","source":"import IPython.display as ipd  # To play sound in the notebook\ntrack=train[train.is_curated==True].fname.sample(1).values[0]\npath = '../input/train_curated/{}'.format(track)   \nlabel=train[train.fname==track].labels.values[0]\nprint(label)\nipd.Audio(path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Track from noisy"},{"metadata":{"trusted":true},"cell_type":"code","source":"track_n=train[(train.is_curated==False)&(train.labels ==label)].sample(1).fname.values[0]\npath_n = '../input/train_noisy/{}'.format(track_n)   \nprint(train[train.fname==track_n].labels.values[0])\nprint(train[train.fname==track_n].fname.values[0])\nipd.Audio(path_n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Spectrogram Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"audio, sample_rate=lbr.load(path,sr=44100)\nn_fft = int(0.03 * sample_rate) #25ms window length\nhop_length =  n_fft//2\nN_MELS = 128 #frequency bins\n#X = lbr.stft(audio[0], n_fft=n_fft, hop_length=hop_length)\nS=lbr.feature.melspectrogram(audio,n_fft=n_fft, hop_length=hop_length,n_mels=N_MELS )\nS = lbr.amplitude_to_db(abs(S))\n#S=np.log(X)\nplt.figure(figsize=(15, 5))\nlbr.display.specshow(S, sr=44100, hop_length=hop_length, x_axis='time',cmap='magma')\nplt.colorbar(format='%+2.0f dB')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"audio, sample_rate=lbr.load(path_n,sr=44100)\nn_fft = int(0.03 * sample_rate) #25ms window length\nhop_length =  n_fft//2\nN_MELS = 128 #frequency bins\n#X = lbr.stft(audio[0], n_fft=n_fft, hop_length=hop_length)\nS=lbr.feature.melspectrogram(audio,n_fft=n_fft, hop_length=hop_length,n_mels=N_MELS )\nS = lbr.amplitude_to_db(abs(S))\n#S=np.log(X)\nplt.figure(figsize=(15, 5))\nlbr.display.specshow(S, sr=44100, hop_length=hop_length, x_axis='time',cmap='magma')\nplt.colorbar(format='%+2.0f dB')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"track = audio[0:int(1 * sample_rate)] #5 secs of audio\nplt.plot(track)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building a model using MFCC"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nimport tensorflow as tf\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nfrom keras.layers import Input, Dense, Dropout, Activation, \\\n         Convolution1D, MaxPooling1D, BatchNormalization, Flatten,GlobalAveragePooling1D,Convolution2D,MaxPooling2D\nimport scipy\nfrom keras import losses\nfrom keras import backend as K\nfrom keras.activations import relu, softmax\nfrom keras.callbacks import (EarlyStopping, LearningRateScheduler,\n                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\nfrom keras.utils import Sequence\nimport shutil","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Config(object):\n    def __init__(self,\n                 sampling_rate=44100, audio_duration=2, #audio duration: specify length of the track in sec\n                 n_classes=target_names,\n                 use_mfcc=True, n_folds=1, learning_rate=0.0001, \n                 max_epochs=30, n_mfcc=64):\n        self.sampling_rate = sampling_rate\n        self.audio_duration = audio_duration\n        self.n_classes = n_classes\n        self.use_mfcc = use_mfcc\n        self.n_mfcc = n_mfcc\n        self.n_folds = n_folds\n        self.learning_rate = learning_rate\n        self.max_epochs = max_epochs\n        self.win_len= int(0.02 * sample_rate) #ms window length\n\n        self.audio_length = self.sampling_rate * self.audio_duration\n        if self.use_mfcc:\n           # self.dim = (self.n_mfcc, 1 + int(np.floor(self.audio_length/self.win_len)*2))\n            self.dim = (self.n_mfcc,401)\n        else:\n            self.dim = (self.audio_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lwrap implementation for keras\ndef tf_one_sample_positive_class_precisions(y_true, y_pred) :\n    num_samples,num_classes = y_pred.shape\n    \n    # find true labels\n    pos_class_indices = tf.where(y_true > 0) \n    \n    # put rank on each element\n    retrieved_classes = tf.nn.top_k(y_pred, k=num_classes).indices\n    sample_range = tf.zeros(shape=tf.shape(tf.transpose(y_pred)), dtype=tf.int32)\n    sample_range = tf.add(sample_range, tf.range(tf.shape(y_pred)[0], delta=1))\n    sample_range = tf.transpose(sample_range)\n    sample_range = tf.reshape(sample_range, (-1,num_classes*tf.shape(y_pred)[0]))\n    retrieved_classes = tf.reshape(retrieved_classes, (-1,num_classes*tf.shape(y_pred)[0]))\n    retrieved_class_map = tf.concat((sample_range, retrieved_classes), axis=0)\n    retrieved_class_map = tf.transpose(retrieved_class_map)\n    retrieved_class_map = tf.reshape(retrieved_class_map, (tf.shape(y_pred)[0], num_classes, 2))\n    \n    class_range = tf.zeros(shape=tf.shape(y_pred), dtype=tf.int32)\n    class_range = tf.add(class_range, tf.range(num_classes, delta=1))\n    \n    class_rankings = tf.scatter_nd(retrieved_class_map,\n                                          class_range,\n                                          tf.shape(y_pred))\n    \n    #pick_up ranks\n    num_correct_until_correct = tf.gather_nd(class_rankings, pos_class_indices)\n\n    # add one for division for \"presicion_at_hits\"\n    num_correct_until_correct_one = tf.add(num_correct_until_correct, 1) \n    num_correct_until_correct_one = tf.cast(num_correct_until_correct_one, tf.float32)\n    \n    # generate tensor [num_sample, predict_rank], \n    # top-N predicted elements have flag, N is the number of positive for each sample.\n    sample_label = pos_class_indices[:, 0]   \n    sample_label = tf.reshape(sample_label, (-1, 1))\n    sample_label = tf.cast(sample_label, tf.int32)\n    \n    num_correct_until_correct = tf.reshape(num_correct_until_correct, (-1, 1))\n    retrieved_class_true_position = tf.concat((sample_label, \n                                               num_correct_until_correct), axis=1)\n    retrieved_pos = tf.ones(shape=tf.shape(retrieved_class_true_position)[0], dtype=tf.int32)\n    retrieved_class_true = tf.scatter_nd(retrieved_class_true_position, \n                                         retrieved_pos, \n                                         tf.shape(y_pred))\n    # cumulate predict_rank\n    retrieved_cumulative_hits = tf.cumsum(retrieved_class_true, axis=1)\n\n    # find positive position\n    pos_ret_indices = tf.where(retrieved_class_true > 0)\n\n    # find cumulative hits\n    correct_rank = tf.gather_nd(retrieved_cumulative_hits, pos_ret_indices)  \n    correct_rank = tf.cast(correct_rank, tf.float32)\n\n    # compute presicion\n    precision_at_hits = tf.truediv(correct_rank, num_correct_until_correct_one)\n    return pos_class_indices, precision_at_hits\n\ndef tf_lwlrap(y_true, y_pred):\n    num_samples,num_classes = y_pred.shape\n    \n    pos_class_indices, precision_at_hits = (tf_one_sample_positive_class_precisions(y_true, y_pred))\n    pos_flgs = tf.cast(y_true > 0, tf.int32)\n    labels_per_class = tf.reduce_sum(pos_flgs, axis=0)\n    weight_per_class = tf.truediv(tf.cast(labels_per_class, tf.float32),\n                                  tf.cast(tf.reduce_sum(labels_per_class), tf.float32))\n    sum_precisions_by_classes = tf.zeros(shape=(num_classes), dtype=tf.float32)  \n    class_label = pos_class_indices[:,1]\n    sum_precisions_by_classes = tf.unsorted_segment_sum(precision_at_hits,\n                                                        class_label,\n                                                       num_classes)\n    labels_per_class = tf.cast(labels_per_class, tf.float32)\n    labels_per_class = tf.add(labels_per_class, 1e-7)\n    per_class_lwlrap = tf.truediv(sum_precisions_by_classes,\n                                  tf.cast(labels_per_class, tf.float32))\n    out = tf.cast(tf.tensordot(per_class_lwlrap, weight_per_class, axes=1), dtype=tf.float32)\n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def audio_norm(data):\n#     max_data = np.max(data)\n#     min_data = np.min(data)\n#     data = (data-min_data)/(max_data-min_data+1e-6)\n#     return data - 0.5\n    data = ( data - np.mean(data) ) / np.std(data)\n    data /= np.max(data)\n    return data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(config):\n    \n    nclass = len(config.n_classes)\n    input_length = config.audio_length\n    input_shape = (64,401,1)\n    print(input_shape)\n    rate=0.2\n    model_input = Input(input_shape, name='input')\n    layer = model_input\n    layer = Convolution2D(32, (3,3) ,activation=tf.nn.leaky_relu,name='convolution_1' ,padding='same',strides=(2,2))(layer)\n   # layer = BatchNormalization(momentum=0.9)(layer) #momentum=0.9\n    layer=MaxPooling2D(pool_size=(2, 2), strides=(2,2),padding='same')(layer)\n    layer = Dropout(rate)(layer)\n    \n    layer = Convolution2D(64, (3,3) ,activation=tf.nn.leaky_relu,name='convolution_2' , padding='same',strides=(2,2))(layer)\n   # layer = BatchNormalization(momentum=0.9)(layer) #momentum=0.9\n    layer=MaxPooling2D(pool_size=(2, 2), strides=(2,2),padding='same')(layer)\n    layer = Dropout(rate)(layer)\n    \n    layer = Convolution2D(64, (3,3) ,activation=tf.nn.leaky_relu,name='convolution_3' , padding='same')(layer)\n   # layer = BatchNormalization(momentum=0.9)(layer) #momentum=0.9\n    layer=MaxPooling2D(pool_size=(2, 2), strides=(2,2),padding='same')(layer)\n    layer = Dropout(rate)(layer)\n    \n    layer = Convolution2D(128, (3,3) ,activation=tf.nn.leaky_relu,name='convolution_4' , padding='same')(layer)\n  #  layer = BatchNormalization(momentum=0.9)(layer) #momentum=0.9\n    layer=MaxPooling2D(pool_size=(2, 2), strides=(2,2),padding='same')(layer)\n    layer = Dropout(rate)(layer)\n    \n    layer= Flatten()(layer)\n    layer = Dense(256)(layer)\n    layer = Dropout(rate)(layer)\n    layer = Dense(nclass)(layer)\n    \n    output = Activation('softmax', name='Final_output')(layer)\n    model = Model(model_input, output)\n   # opt = Adam(lr=config.learning_rate)\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[tf_lwlrap])\n    return model\n\n  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_rate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# trim silent part\ndef trim_silent(data):\n    data_tr = librosa.effects.trim(data)[0]\n    return data_tr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_data(df, config, data_dir,downsample=False):\n    input_length = config.audio_length\n    WINDOW_SIZE = int(0.03 * config.sampling_rate)\n    hop_length =  n_fft//4\n    N_MELS = 64#frequency bins\n\n    MEL_KWARGS = {\n        'n_fft': WINDOW_SIZE,\n        'hop_length': hop_length,\n        'n_mels': N_MELS \n    }\n    X = np.empty(shape=(df.shape[0], config.dim[0], config.dim[1],1))\n    \n    invalid=['77b925c2.wav','f76181c4.wav', '6a1f682a.wav', 'c7db12aa.wav', '7752cc8a.wav','1d44b0bd.wav']\n    for i, fname in enumerate(df.index):\n            \n            if fname not in invalid:\n                file_path = data_dir + fname\n                data, _ = librosa.core.load(file_path, sr=44100, res_type=\"kaiser_fast\")\n                if len(audio)/44100>=0.5:\n                    trim_silent(data)\n                   # print('data_shape: ',data.shape)\n                    # Random offset / Padding\n                    if len(data) > input_length:\n                        max_offset = len(data) - input_length\n                        offset = np.random.randint(max_offset)\n                        data = data[offset:(input_length+offset)]\n                    else:\n                        if input_length > len(data):\n                            max_offset = input_length - len(data)\n                            offset = np.random.randint(max_offset)\n                        else:\n                            offset = 0\n                        #pad with zeros\n                        data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n                    #print('before spec: ',data.shape)\n                   # data = librosa.feature.mfcc(data, sr=config.sampling_rate, n_mfcc=config.n_mfcc,**MEL_KWARGS).T\n                    if downsample==True:\n                        data = librosa.feature.melspectrogram(lbr.resample(data, 44100, 44100/2),**MEL_KWARGS)\n                    else:\n                        data = librosa.feature.melspectrogram(data,**MEL_KWARGS)\n                    data = lbr.core.power_to_db(data)\n                    #print('after padding')\n                    #print(data.shape)\n                    X[i,] = data[:,:,np.newaxis]\n            else:\n                    print(fname)\n    return X\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.set_index(\"fname\", inplace=True)\ntest.set_index('fname',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"downsample=False\nif downsample==True:\n    config = Config(sampling_rate=22050, audio_duration=3, n_folds=1, learning_rate=0.001, use_mfcc=True, n_mfcc=64,max_epochs=40)\nelse:\n    config = Config(sampling_rate=44100, audio_duration=3, n_folds=1, learning_rate=0.001, use_mfcc=True, n_mfcc=64,max_epochs=40)\nX_train=prepare_data(train[train.is_curated==True],config,'../input/train_curated/',downsample)\nX_test=prepare_data(test,config,'../input/test/',downsample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = one_hot(train_curated['labels'], src_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_noisy=one_hot(train[train.is_curated==False].labels, src_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = audio_norm(X_train)\nX_test = audio_norm(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_noisy.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PREDICTION_FOLDER = \"predictions_1d_conv\"\nif not os.path.exists(PREDICTION_FOLDER):\n    os.mkdir(PREDICTION_FOLDER)\nif os.path.exists('logs/' + PREDICTION_FOLDER):\n    shutil.rmtree('logs/' + PREDICTION_FOLDER)\n    \nK.clear_session()\ncheckpoint = ModelCheckpoint('best.h5', monitor='val_tf_lwlrap', verbose=1, save_best_only=True)\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\ntb = TensorBoard(log_dir='./logs/' + PREDICTION_FOLDER + '/fold', write_graph=True)\ncallbacks_list = [checkpoint, tb,early]\nmodel = build_model(config)\nhistory = model.fit(X_train,y_train,validation_data=(X_val,y_val),callbacks=callbacks_list,batch_size=128, epochs=40,shuffle=True)\n#model.load_weights('best.h5')\n#history = model.fit(X,y,callbacks=callbacks_list,batch_size=256, epochs=40,shuffle=True)\n\n# # Save train predictions\n# predictions = model.predict(X_train, batch_size=64, verbose=1)\n# np.save(PREDICTION_FOLDER + \"/train_predictions.npy\", predictions)\n\n# # Save test predictions\n# predictions = model.predict(X_test, batch_size=64, verbose=1)\n# np.save(PREDICTION_FOLDER + \"/test_predictions.npy\", predictions)\n\n# # Make a submission file\n# top_3 = np.array(config.n_classes)[np.argsort(-predictions, axis=1)[:, :3]]\n# predicted_labels = [' '.join(list(x)) for x in top_3]\n# test['label'] = predicted_labels\n# test[['label']].to_csv(PREDICTION_FOLDER + \"/predictions.csv\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fname in train[train.is_curated==False].index\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_val[20:21],y_val[20:21],batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare noisy data\nX_noisy=prepare_data(train[train.is_curated==False],config,'../input/train_noisy/',downsample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=[]\nfor i in range(y_noisy.shape[0]):\n    r=model.evaluate(X_noisy[i:i+1],y_noisy[i:i+1],batch_size=1)\n    results.append(r[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=np.array(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(min(results))\nprint(max(results))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexes=[]\nfor i,j in enumerate(results):\n    if(j>=0.1):\n        indexes.append(i)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d=np.vstack((X_train,X_noisy[indexes]))\ny2=np.vstack((y_train,y_noisy[indexes]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PREDICTION_FOLDER = \"predictions_1d_conv\"\nif not os.path.exists(PREDICTION_FOLDER):\n    os.mkdir(PREDICTION_FOLDER)\nif os.path.exists('logs/' + PREDICTION_FOLDER):\n    shutil.rmtree('logs/' + PREDICTION_FOLDER)\n    \nmodel.load_weights('best.h5')\ncheckpoint = ModelCheckpoint('best.h5', monitor='val_loss', verbose=1, save_best_only=True)\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\ntb = TensorBoard(log_dir='./logs/' + PREDICTION_FOLDER + '/fold', write_graph=True)\ncallbacks_list = [checkpoint, tb,early]\nmodel = build_model(config)\nhistory = model.fit(d,y2,validation_data=(X_val,y_val),callbacks=callbacks_list,batch_size=128, epochs=50,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission= model.predict(X_test, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Output all random to see a baseline\nsample_sub = pd.read_csv('../input/sample_submission.csv')\nsample_sub.iloc[:,1:] = submission\nsample_sub.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}