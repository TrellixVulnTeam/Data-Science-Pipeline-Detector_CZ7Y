{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as bk\nimport tensorflow.keras.layers as ly\nimport tensorflow.keras.models as ml\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import train_test_split\nimport xgboost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# creating features for the train-set and test-set# \nIt is going to take a while to process data, so grab a coffee and get relaxed as Kaggle always says:) better to do this part of process on CPU, save the results somewhere, and then do the training on GPU. Also make your mind about the features you want to use. I came up with these but you might wanted to add/remove."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_df=pd.read_csv('/kaggle/input/predict-volcanic-eruptions-ingv-oe/train.csv')\nn_f=12\ntotal_data=np.empty((train_df.shape[0],n_f*10))\ntime_=np.empty((train_df.shape[0],1))\nfor i_,seg_ in enumerate(train_df['segment_id']):\n    the_df=pd.read_csv(f'/kaggle/input/predict-volcanic-eruptions-ingv-oe/train/{seg_}.csv').fillna(0)\n    total_data[i_,:]=np.concatenate((the_df.abs().mean().to_numpy(),\n                                    the_df.std().to_numpy(),\n                                    the_df.mean().to_numpy(),\n                                    the_df.var().to_numpy(),\n                                    the_df.min().to_numpy(),\n                                    the_df.max().to_numpy(),\n                                    the_df.median().to_numpy(),\n                                    the_df.quantile([0.1,0.25,0.5,0.75,0.9]).to_numpy().reshape(1,-1)[0]))\n    time_[i_,0]=train_df.loc[i_,'time_to_eruption']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsample_submission_df=pd.read_csv('../input/predict-volcanic-eruptions-ingv-oe/sample_submission.csv')\nn_f=12\ntotal_data_test_=np.empty((sample_submission_df.shape[0],n_f*10))\nfor i_,seg_ in enumerate(sample_submission_df['segment_id']):\n    the_df=pd.read_csv(f'/kaggle/input/predict-volcanic-eruptions-ingv-oe/test/{seg_}.csv').fillna(0)\n    total_data_test_[i_,:]=np.concatenate((the_df.abs().mean().to_numpy(),\n                                    the_df.std().to_numpy(),\n                                    the_df.mean().to_numpy(),\n                                    the_df.var().to_numpy(),\n                                    the_df.min().to_numpy(),\n                                    the_df.max().to_numpy(),\n                                    the_df.median().to_numpy(),\n                                    the_df.quantile([0.1,0.25,0.5,0.75,0.9]).to_numpy().reshape(1,-1)[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del the_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# simple NN model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_my_model():\n    model = ml.Sequential()\n    model.add(ly.Input(total_data.shape[1]))\n    model.add(ly.BatchNormalization())\n    model.add(tfa.layers.WeightNormalization(ly.Dense(1000,activation='relu')))\n    model.add(ly.BatchNormalization())\n    model.add(ly.Dropout(0.7))\n    model.add(tfa.layers.WeightNormalization(ly.Dense(1,activation='relu')))\n\n\n    model.compile(optimizer=tfa.optimizers.AdamW(lr = 1, weight_decay = 1e-5, clipvalue = 900),loss='mean_absolute_error')\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cb_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, min_lr=1e-7, patience=2, verbose=1, mode='min')\ncb_early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", restore_best_weights=True, patience= 5, verbose = 1)\nmodel=create_my_model()\nX_train1, X_val, y_train1, y_val = train_test_split(total_data, time_, test_size=0.1, random_state=3)\nmodel.fit(X_train1,y_train1,batch_size=8,epochs=600,verbose=1,validation_data=(X_val,y_val),callbacks=[cb_lr,cb_early])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_df1=pd.read_csv('../input/predict-volcanic-eruptions-ingv-oe/sample_submission.csv')\nsample_submission_df['time_to_eruption']=model.predict(total_data_test_)\nsample_submission_df.to_csv('nn_res.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost method"},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove `tree_method` if you dont have gpu as processor\nmodel1 = xgboost.XGBRegressor(n_estimators=100000,tree_method='gpu_hist',max_depth=8,learning_rate=0.05,alpha=0.1,SUBSAMPLE=0.6)\nX_train1, X_val, y_train1, y_val = train_test_split(total_data, time_, test_size=0.1, random_state=3)\neval_set = [(X_val, y_val)]\nmodel1.fit(X_train1, y_train1,early_stopping_rounds=5,eval_metric='mae', eval_set=eval_set, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_df1=pd.read_csv('../input/predict-volcanic-eruptions-ingv-oe/sample_submission.csv')\nsample_submission_df1['time_to_eruption']=model1.predict(total_data_test_)[:,None]\nsample_submission_df1.to_csv('xgb_res.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}