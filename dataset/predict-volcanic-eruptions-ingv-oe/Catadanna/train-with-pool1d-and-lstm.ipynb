{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this competition we have a huge amount of data for one sample: more than 60000 x 10 elements. \n\nI tried to reduce this number of features by using Pool1D."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.callbacks as C","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"ROOT_COMPETITION = '/kaggle/input/predict-volcanic-eruptions-ingv-oe/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_original = pd.read_csv(ROOT_COMPETITION+'train.csv')\ntrain = train_original.loc[0:2000] # training on a part only in order to avoid OOM \ndev = train_original.loc[3000:3500]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataLoader():\n    def __init__(self):\n        self.TRAIN_TEST_SPLIT=0.1\n    \n    def get_sample_all(sample_id, where):\n        sample_path = ROOT_COMPETITION+where+'/'+str(sample_id)+'.csv'\n        df = pd.read_csv(sample_path).fillna(0)\n        np_data = df.values\n        np_data = np_data[:60000,:]\n\n        return np_data\n    \nclass EngineNN():\n    def __init__(self):\n        self.INPUT_SHAPE=10\n        self.OUTPUT_SHAPE=1\n        self.SPLITS=5\n        self.STRIDES=1000\n        self.SEED=42\n        self.EPOCHS = 100\n        self.BATCH_SIZE=128\n        self.DROPOUT = 0.2\n        self.HIDDEN_SIZE=100\n        self.LOSS = 'mean_absolute_error'\n        self.METRICS = ['mean_absolute_error']\n        self.OUTPUT_ACTIVATION = \"sigmoid\"\n        self.HIDDEN_ACTIVATION = \"relu\"\n        \n    def lstm_layer_BD_3D(self, hidden_size=self.HIDDEN_SIZE):\n        return L.Bidirectional(\n                                L.LSTM(hidden_size,\n                                dropout=self.DROPOUT,\n                                return_sequences=True,\n                                kernel_initializer='orthogonal'))\n    \n    def lstm_layer_BD_2D(self):\n        return L.Bidirectional(\n                                L.LSTM(self.HIDDEN_SIZE,\n                                dropout=self.DROPOUT,\n                                return_sequences=False,\n                                kernel_initializer='orthogonal'))\n    \n    def make_model_lstm_pooling(self, inshape=L11):\n        z = L.Input(shape=(60000, 10))\n        x = L.MaxPool1D(pool_size=self.STRIDES, strides=self.STRIDES)(z)\n        #x = L.AveragePooling1D(pool_size=self.STRIDES, strides=self.STRIDES)(z)\n        \n        x = self.lstm_layer_BD_3D()(x)\n        x = self.lstm_layer_BD_3D()(x)\n        \n        x = self.lstm_layer_BD_2D()(x)\n        x = L.Dense(self.HIDDEN_SIZE, activation='relu')(x)\n        x = L.Dense(self.OUTPUT_SHAPE, activation='sigmoid')(x)\n        \n        model = tf.keras.Model(z, x)\n        model.compile(optimizer='adam', loss=self.LOSS, metrics=self.METRICS)\n        return model\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dd = DataLoader()\n\nnp_train = train['segment_id'].apply(lambda x: dd.get_sample_all(x, 'train')).values\nnp_train = np.stack(np_train, axis=0)\nnp_train = np.nan_to_num(np_train)\n\nX_max = np.amax(np_train)\nX_min = np.amin(np_train)\n\nX_train = (np_train - X_min) / float(X_max - X_min)\n\nnp_dev = dev['segment_id'].apply(lambda x: dd.get_sample_all(x, 'train')).values\nnp_dev = np.stack(np_dev, axis=0)\nnp_dev = np.nan_to_num(np_dev)\n\nX_dev = (np_dev - X_min) / float(X_max - X_min)\n\nY_min = 6250\nY_max = 49046087\n\nY_train = train['time_to_eruption']\nY_train =  (Y_train - Y_min) / (Y_max - Y_min)\n\nY_dev = dev['time_to_eruption']\nY_dev =  (Y_dev - Y_min) / (Y_max - Y_min)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = eng.make_model_lstm_pooling()  \nf = 'best_model_pool.h5'   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = C.ModelCheckpoint(\n                filepath=f,\n                save_best_only=True,  \n                monitor='val_loss',\n                mode='min')\n\ncallback_lr = C.ReduceLROnPlateau()\n\nhistory = model.fit(\n            X_train, Y_train,\n            validation_data=(X_dev, Y_dev),\n            epochs=eng.EPOCHS,\n            batch_size=eng.BATCH_SIZE,\n            callbacks=[checkpoint, callback_lr]\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mae_val_loss = history.history['val_mean_absolute_error']\nmae_min = min(mae_val_loss)\nprint(\"MAE Min\", mae_min)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}