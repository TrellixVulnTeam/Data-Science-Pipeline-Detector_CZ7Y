{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport lightgbm as lgbm\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nfile_list=[]\nfile_list_train=[]\nfile_list_test=[]\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        file_list.append(os.path.join(dirname, filename))\n\nPATH = '/kaggle/input/predict-volcanic-eruptions-ingv-oe/'\n\nfor dirname, _, filenames in os.walk('/kaggle/input/predict-volcanic-eruptions-ingv-oe/train'):\n    for filename in filenames:\n        file_list_train.append(os.path.join(dirname, filename))\n\nfor dirname, _, filenames in os.walk('/kaggle/input/predict-volcanic-eruptions-ingv-oe/test'):\n    for filename in filenames:\n        file_list_test.append(os.path.join(dirname, filename))\n        \n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:24:07.98198Z","iopub.execute_input":"2021-12-21T22:24:07.982929Z","iopub.status.idle":"2021-12-21T22:24:09.672285Z","shell.execute_reply.started":"2021-12-21T22:24:07.982879Z","shell.execute_reply":"2021-12-21T22:24:09.671538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2.Data Understanding\n#Переглядаємо файл sample_submission\nprint(file_list[0])\nprint(pd.read_csv(file_list[0]))","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:24:09.674732Z","iopub.execute_input":"2021-12-21T22:24:09.67496Z","iopub.status.idle":"2021-12-21T22:24:09.687141Z","shell.execute_reply.started":"2021-12-21T22:24:09.67493Z","shell.execute_reply":"2021-12-21T22:24:09.68606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Переглядаємо файл 800654756 в папці train\nprint(file_list_train[0])\nprint(pd.read_csv(file_list_train[0]))","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:24:09.688425Z","iopub.execute_input":"2021-12-21T22:24:09.689161Z","iopub.status.idle":"2021-12-21T22:24:09.816367Z","shell.execute_reply.started":"2021-12-21T22:24:09.689111Z","shell.execute_reply":"2021-12-21T22:24:09.815302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Переглядаємо файл train.csv\nprint(file_list[1])\nprint(pd.read_csv(file_list[1]))","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:24:09.818554Z","iopub.execute_input":"2021-12-21T22:24:09.818795Z","iopub.status.idle":"2021-12-21T22:24:09.837269Z","shell.execute_reply.started":"2021-12-21T22:24:09.818757Z","shell.execute_reply":"2021-12-21T22:24:09.83629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Aналіз статистики по відсутнім показам датчиків\n#file_list_test\nprint(len(file_list_test)) # довжина массиву file_list_test\nkeys = list(pd.read_csv(file_list_test[0]).keys()) # Ключи(сенсори) массиву file_list_test\nprint(keys)\nnanCount = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nfor index in range(len(file_list_test)):\n    if(index % 200 == 0): \n        print(index)\n    df = pd.read_csv(file_list_test[index]) #записуємо кожний cегмент з масиву в Dataframe\n    for key in df.keys():\n        if df[key].isna().sum() == 60001: # кількість відсутніх значень (nan)\n#             print(file_list_test[index])\n#             print(keys.index(key))\n            nanCount[keys.index(key)] += 1 # підрахунок nan значень для кожного сенсору в сегменті даних\n#             print(key, 'is NAN!')\nprint(nanCount)\ndata={'sensors': keys, 'count': nanCount}\nmyFrame = pd.DataFrame(data)\nprint(myFrame)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:24:09.838736Z","iopub.execute_input":"2021-12-21T22:24:09.839503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Гістограма статистики по відсутнім показам датчиків\nmyFrame.plot(figsize =(20, 20), x=\"sensors\", y=\"count\", kind=\"bar\",  rot=5, fontsize=14 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(file_list_test[0])\nprint(pd.read_csv(file_list_test[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(file_list_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(file_list_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files_train = [file.split('/')[-1].split('.')[-2] for file in file_list_train]\nfiles_test = [file.split('/')[-1].split('.')[-2] for file in file_list_test]\nprint(files_train[0:10])\nprint(files_test[0:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Кількість перетинів індексів файлів\ntest_set = set(files_test)\ntrain_set = set(files_train)\ninter = test_set.intersection(train_set) # Метод повертає набір, що містить схожість між двома або більше наборами\nprint(inter)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Читаємо дані тренування у DataFrame\ntrain = pd.read_csv(PATH+'train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Зображуємо на діаграмі розподілення часу. \nsns.distplot(train['time_to_eruption'], hist=True, kde=True, bins=100, color='blue', hist_kws={'edgecolor':'black'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cтатистика ознаки часу\ntrain['time_to_eruption'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Візуалізуємо покази датчиків файлу train/800654756.csv.\ndf_segment_id = pd.read_csv(PATH+'/train/800654756.csv')\n\ndf_segment_id.plot(figsize=(20, 20), subplots=True, layout=(10, 1), rot = 0,\n                  lw=1, title='segment_id #800654756')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train.sort_values('time_to_eruption', axis=0, ascending=True).iloc[[0, -1], :]) #Сортує серію у зростаючому або спадному порядку за будь-яким критерієм\n\nsegment_id_min = 601524801\nsegment_id_max = 1923243961\n\ndf_segment_id_min = pd.read_csv(PATH+'/train/'+str(segment_id_min)+'.csv')\ndf_segment_id_max = pd.read_csv(PATH+'/train/'+str(segment_id_max)+'.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Візуалізуємо покази датчиків файлу train/601524801.csv.\ndf_segment_id_min.plot(figsize=(20,20), subplots=True, layout=(10,1), rot=0, lw=1, title='segment_id #601524801 (min)')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Візуалізуємо покази датчиків файлу train/1923243961.csv.\ndf_segment_id_max.plot(figsize=(20,20), subplots=True, layout=(10,1), rot=0, lw=1, title='segment_id #1923243961 (max)')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Data Preparation\n# Функція обчислень медіани, дисперсії, стандартного відхилення, зсуву, мінімального та максимального значень, квантилів тощо \ndef build_features(signal, ts, sensor_id):\n    X = pd.DataFrame()\n    f = np.fft.fft(signal) # Обчислення одновимірного дискретного перетворення Фур'є.\n    f_real = np.real(f) # Повертає дійсну частину складного аргументу.\n    X.loc[ts, f'{sensor_id}_sum'] = signal.sum()\n    X.loc[ts, f'{sensor_id}_mean'] = signal.mean()\n    X.loc[ts, f'{sensor_id}_std'] = signal.std()\n    X.loc[ts, f'{sensor_id}_var'] = signal.var()\n    X.loc[ts, f'{sensor_id}_max'] = signal.max()\n    X.loc[ts, f'{sensor_id}_min'] = signal.min()\n    X.loc[ts, f'{sensor_id}_skew'] = signal.skew()\n    X.loc[ts, f'{sensor_id}_mad'] = signal.mad()\n    X.loc[ts, f'{sensor_id}_kurtosis'] = signal.kurtosis()\n    X.loc[ts, f'{sensor_id}_quantile99'] = np.quantile(signal, 0.99)\n    X.loc[ts, f'{sensor_id}_quantile95'] = np.quantile(signal, 0.95)\n    X.loc[ts, f'{sensor_id}_quantile85'] = np.quantile(signal, 0.85)\n    X.loc[ts, f'{sensor_id}_quantile75'] = np.quantile(signal, 0.75)\n    X.loc[ts, f'{sensor_id}_quantile55'] = np.quantile(signal, 0.55)\n    X.loc[ts, f'{sensor_id}_quantile45'] = np.quantile(signal, 0.45)\n    X.loc[ts, f'{sensor_id}_quantile25'] = np.quantile(signal, 0.25)\n    X.loc[ts, f'{sensor_id}_quantile15'] = np.quantile(signal, 0.15)\n    X.loc[ts, f'{sensor_id}_quantile05'] = np.quantile(signal, 0.05)\n    X.loc[ts, f'{sensor_id}_quantile01'] = np.quantile(signal, 0.01)\n    X.loc[ts, f'{sensor_id}_fft_real_mean'] = f_real.mean()\n    X.loc[ts, f'{sensor_id}_fft_real_std'] = f_real.std()\n    X.loc[ts, f'{sensor_id}_fft_real_max'] = f_real.max()\n    X.loc[ts, f'{sensor_id}_fft_real_min'] = f_real.min()\n    \n    return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = list()\nseg=0\nprint(enumerate(train.segment_id))\n\nfor seg, segment_id in enumerate(train.segment_id): # цикл сегментів\n    signals = pd.read_csv(PATH+'/train/'+str(segment_id)+'.csv')\n    train_row=[]\n#     print(segment_id)\n    if seg % 200 == 0:\n        print('Processing segment_id={}'.format(seg))\n    \n    for sensor in range(0, 10): # цикл для розрахунку обчислень медіани, дисперсії, стандартного відхилення, зсуву, мінімального та максимального значень, квантилів тощо \n        sensor_id = f'sensor_{sensor+1}'\n        train_row.append(build_features(signals[sensor_id].fillna(0), segment_id, sensor_id))\n    \n    train_row = pd.concat(train_row, axis=1) #Об'єднання об'єкту pandas вздовж стовбців.\n    train_set.append(train_row) # додавання результату розрахунків до списку\n    seg+=1\n    \ntrain_set = pd.concat(train_set)\nprint(train_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = train_set.reset_index() # Скидаємо індекс\ntrain_set = train_set.rename(columns={'index': 'segment_id'})\nprint(train_set)\ntrain_set = pd.merge(train_set, train, on='segment_id') # Слиття Dataframe за столбцем segment_id\nprint(train_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_set.head(3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Дані для тесту \ntest_files = []\nfor dirname, _, filenames in os.walk(PATH+'/test/'):\n    for filename in filenames:\n        test_files.append(filename[:-4])\n\ntest = pd.DataFrame(test_files, columns=['segment_id'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set = list()\nseg=0\n\nfor seg, segment_id in enumerate(test.segment_id):  # цикл сегментів\n    signals = pd.read_csv(PATH+'/test/'+str(segment_id)+'.csv')\n    test_row=[]\n    \n    if seg%200 == 0:\n        print('Processing segment_id={}'.format(seg))\n        \n    for sensor in range(0, 10):  # цикл для розрахунку обчислень медіани, дисперсії, стандартного відхилення, зсуву, мінімального та максимального значень, квантилів тощо \n        sensor_id = f'sensor_{sensor+1}'\n        test_row.append(build_features(signals[sensor_id].fillna(0), segment_id, sensor_id))\n    \n    test_row = pd.concat(test_row, axis=1) #Об'єднання об'єкту pandas вздовж стовбців.\n    test_set.append(test_row) # додавання результату розрахунків до списку\n    seg+=1\n    \ntest_set = pd.concat(test_set)\nprint(test_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set=test_set.reset_index() # Скидаємо індекс\ntest_set=test_set.rename(columns={'index': 'segment_id'})\nprint(test_set)\ntest_set=pd.merge(test_set, test, on=\"segment_id\") # Слиття Dataframe за столбцем segment_id\nprint(test_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_set.head(3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_set.drop(['segment_id', 'time_to_eruption'], axis=1) # Видалення столбців з тренувального набору\ny = train_set['time_to_eruption']\n\n#Розділення даних тренування на набори для безпосередньо тренування та валідації \nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Вхідні дані тренування\nprint(X_train.head(3))\nprint('np.shape(X_train) = ', np.shape(X_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Вихідні дані тренування\nprint(y_train.head(3))\nprint('np.shape(y_train) = ', np.shape(y_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Modeling\n# модель регресії методом випадкових лісів\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(max_depth=20, random_state=0) # модель випадкового лісу с максимальною глибиною 20\nmodel.fit(X_train, y_train) # навчаємо модель","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluation\n#Передбачення побудованої моделі\ny_pred = model.predict(X_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error# Середньоквадратична помилка регресії.\nmse = mean_squared_error(y_valid, y_pred)\nprint(mse)\nfig=plt.figure()\nmulreg = fig.add_subplot(1,1,1)\nmulreg.scatter(y_valid, y_pred, color='r')\nmulreg.set_title('Nonlinear Regression ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(test_set.drop(columns=['segment_id'], axis=1)) # Передбачення для тестового набору даних\n\nsubmission = pd.DataFrame()\nsubmission['segment_id'] = test_set['segment_id']\nsubmission['time_to_eruption'] = predictions\nsubmission.to_csv('submission.csv', header=True, index=False) # Створення вихідного файлу submission.csv","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}