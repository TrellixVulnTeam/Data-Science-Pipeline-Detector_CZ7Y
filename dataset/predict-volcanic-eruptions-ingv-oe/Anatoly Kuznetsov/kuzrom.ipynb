{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import HuberRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nimport lightgbm as lgbm \nfrom lightgbm import LGBMRegressor\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n#import warnings\n#warnings.filterwarnings(\"ignore\")\n        \nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom scipy.ndimage import maximum_filter1d\nfrom scipy.ndimage import minimum_filter1d\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nStSc = StandardScaler()\nMMS = MinMaxScaler()\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"V_PATH = '../input/predict-volcanic-eruptions-ingv-oe/'\nTRAIN_PATH = V_PATH + 'train/'\nTEST_PATH = V_PATH + 'test/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SENSOR_COLS = ['sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6',\n       'sensor_7', 'sensor_8', 'sensor_9', 'sensor_10']\n\n\nSENSOR_RMEANS = [x+'_rmin' for x in SENSOR_COLS] \nSENSOR_RSTDS = [x+'_rstd' for x in SENSOR_COLS] \nSENSOR_RMINS = [x+'_rmin' for x in SENSOR_COLS] \nSENSOR_RMAXES = [x+'_rmax' for x in SENSOR_COLS]\nSENSOR_RSKEWS = [x+'_rskew' for x in SENSOR_COLS]\nSENSOR_RSUMS = [x+'_rsum' for x in SENSOR_COLS]\nSENSOR_RVARS = [x+'_rvar' for x in SENSOR_COLS]\n#SENSOR_RMADS = [x+'_rmad' for x in SENSOR_COLS]\n#SENSOR_RKURTOSISES = [x+'_rkurtosis' for x in SENSOR_COLS]\n\n\n\n#SENSOR_RGRADMEAN = [x+'_grad_rmean' for x in SENSOR_COLS]\n#SENSOR_RGRADSTD = [x+'_grad_rstd' for x in SENSOR_COLS]\n\nSENSOR_RSTATS = [SENSOR_RMEANS, SENSOR_RSTDS, SENSOR_RMINS, SENSOR_RMAXES, SENSOR_RSKEWS, SENSOR_RSUMS, SENSOR_RVARS]\n\nROLL_DESCR = ['rmean', 'rstd', 'rmin', 'rmax', 'rskew', 'rsum', 'rvar']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/predict-volcanic-eruptions-ingv-oe/train.csv')\nprint(train.shape)\nprint(train.columns)\n\ntrain.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_small=train[:200]\ntrain_small","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_files = []\nfor dirname, _, filenames in os.walk(V_PATH+'/test/'):\n    for filename in filenames:\n        test_files.append(filename[:-4]) # without .csv extension\n        \ntest = pd.DataFrame(test_files, columns=[\"segment_id\"])\ntest.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_small=test[:200]\ntest_small","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/predict-volcanic-eruptions-ingv-oe/sample_submission.csv')\nsample_submission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['time_to_eruption'], \n             hist=True, \n             kde=False, \n             bins=100, \n             color = 'blue', \n             hist_kws={'edgecolor':'black'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#pandasDataFrame.rolling(window=window_size).apply(mad) \n#pandas.rolling_kurt(pandasDataFrame, window=window_size)\n\ndef get_rolling(df, cols, window=50):\n    for col in cols:\n        \n#pd.DataFrame.rolling(window=50).apply(mad) \n#pd.rolling_kurt(pd.DataFrame, window=50)        \n        \n        df[col+'_rmin'] = minimum_filter1d(df[col].values, size=window)\n        df[col+'_rmax'] = maximum_filter1d(df[col].values, size=window)\n        \n        df[col+'_rmin'] = df[col+'_rmin'].fillna(method='bfill').fillna(method='ffill')\n        df[col+'_rmax'] = df[col+'_rmax'].fillna(method='bfill').fillna(method='ffill')\n        \n        df[col+'_rmean'] = df[col].rolling(window=window, center=True).mean().fillna(method='bfill').fillna(method='ffill')\n        df[col+'_rstd'] = df[col].rolling(window=window, center=True).std().fillna(method='bfill').fillna(method='ffill')\n        \n        df[col+'_rsum'] = df[col].rolling(window=window, center=True).sum().fillna(method='bfill').fillna(method='ffill')\n        df[col+'_rskew'] = df[col].rolling(window=window, center=True).skew().fillna(method='bfill').fillna(method='ffill')\n        \n        df[col+'_rvar'] = df[col].rolling(window=window, center=True).var().fillna(method='bfill').fillna(method='ffill') #отклонение\n        #df[col+'_rmad'] = df[col].rolling(window=window, center=True).apply(mad).fillna(method='bfill').fillna(method='ffill') #среднее абсолютное отклонение\n        \n        #df[col+'_rkurtosis'] = df[col].rolling_kurt(window=window, center=True).fillna(method='bfill').fillna(method='ffill') \n        \n        \n    return df\n\ndef get_stats(df, sensor_cols=SENSOR_COLS, rolling_cols=SENSOR_RSTATS):\n    #we create the min max etc of original sensor columns\n    df['max'] = df[sensor_cols].max(axis=1)\n    df['min'] = df[sensor_cols].min(axis=1)\n    df['std'] = df[sensor_cols].std(axis=1)\n    df['sum'] = df[sensor_cols].sum(axis=1)\n    df['skew'] = df[sensor_cols].skew(axis=1)\n    df['var'] = df[sensor_cols].var(axis=1)\n    df['mad'] = df[sensor_cols].mad(axis=1)\n    #df['kurtosis'] = df[sensor_cols].kurtosis(axis=1)\n    #df['mean'] = df[sensor_cols].mean(axis=1)\n    \n    #and with absolute values\n    #df['max_abs'] = np.abs(df[sensor_cols]).max(axis=1)\n    #df['min_abs'] = np.abs(df[sensor_cols]).min(axis=1)\n    #df['std_abs'] = np.abs(df[sensor_cols]).std(axis=1)\n    #df['sum_abs'] = np.abs(df[sensor_cols]).sum(axis=1)\n    #df['skew_abs'] = np.abs(df[sensor_cols]).skew(axis=1)\n    #df['var_abs'] = np.abs(df[sensor_cols]).var(axis=1)\n    #df['mad_abs'] = np.abs(df[sensor_cols]).mad(axis=1)\n    #df['kurtosis_abs'] = np.abs(df[sensor_cols]).kurtosis(axis=1)\n    \n    #we take mins and maxes of groups of rolling columns\n    for count,rc in enumerate(rolling_cols): #this takes a SINGLE mean, max across each GROUP of rolling\n        #columns - e.g. the max of all rolling mins\n        df[ROLL_DESCR[count]+'_max'] = df[rolling_cols[count]].max(axis=1)\n        df[ROLL_DESCR[count]+'_mean'] = df[rolling_cols[count]].mean(axis=1)\n        df[ROLL_DESCR[count]+'_min'] = df[rolling_cols[count]].min(axis=1)\n        df[ROLL_DESCR[count]+'_std'] = df[rolling_cols[count]].std(axis=1)\n        df[ROLL_DESCR[count]+'_sum'] = df[rolling_cols[count]].sum(axis=1) \n        df[ROLL_DESCR[count]+'_skew'] = df[rolling_cols[count]].skew(axis=1)    \n        df[ROLL_DESCR[count]+'_var'] = df[rolling_cols[count]].var(axis=1)    \n        #df[ROLL_DESCR[count]+'_mad'] = df[rolling_cols[count]].mad(axis=1)    \n        #df[ROLL_DESCR[count]+'_kurtosis'] = df[rolling_cols[count]].kurtosis(axis=1)    \n   \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#credit - stack overflow\n\ndef percentile(n):\n    def percentile_(x):\n        return np.percentile(x, n)\n    percentile_.__name__ = 'percentile_%s' % n\n    return percentile_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets drop the rolling mean - does not seem that useful\n\ndef get_all_stats(df, cols, rolling_cols, window=50):\n    \n    df = get_rolling(df, cols, window=window)\n    df = get_stats(df, sensor_cols=cols, rolling_cols=rolling_cols)\n    df = df.groupby(['segment'])[[x for x in df.columns if x != 'segment']].agg(['mean','max','skew','std', 'min', 'sum', 'var',\n                                    percentile(0.01), percentile(0.1),percentile(0.25), percentile(0.5), \n                                        percentile(0.75), percentile(0.9), percentile(0.99)])\n    df.columns=[a+b for a,b in df.columns]\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_index = train_small.index\nloaded_dfs = pd.DataFrame()\ncount = 0\nfor count,S in enumerate(sample_index):\n    \n    s_ID = train_small.loc[S, 'segment_id']\n    #q = train.loc[S, 'time_to_eruption']\n    temp_df = pd.read_csv(TRAIN_PATH+str(s_ID)+'.csv')\n    temp_df = temp_df.fillna(value=0)\n    \n    temp_df['segment'] = s_ID\n    #temp_df['time_to_eruption']=q\n    temp_df = get_all_stats(temp_df, SENSOR_COLS, SENSOR_RSTATS, window=50)  \n    if count%50 == 0: \n        print('Processing segment_id={}'.format(count)) \n    loaded_dfs = pd.concat([loaded_dfs, temp_df], axis=0)\n    count +=1\n    \nloaded_dfs = loaded_dfs.reset_index(drop=True) \nloaded_dfs = loaded_dfs.rename(columns={'index':'segment_id'})\n#loaded_dfs = pd.merge (loaded_dfs, train_small, on = 'segment_id' )\nloaded_dfs.head(10)\nloaded_dfs.to_csv('volcano_train_small_fts.csv', index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_index = test_small.index \ntest_dfs = pd.DataFrame()\ncount = 0\nfor count,S in enumerate(sample_index):\n    \n    s_ID = test_small.loc[S, 'segment_id']\n    temp_df = pd.read_csv(TEST_PATH+str(s_ID)+'.csv')\n    temp_df = temp_df.fillna(value=0)\n    \n    temp_df['segment'] = s_ID\n    temp_df = get_all_stats(temp_df, SENSOR_COLS, SENSOR_RSTATS, window=50)  \n    if count%50 == 0: \n        print('Processing segment_id={}'.format(count)) \n    test_dfs = pd.concat([test_dfs, temp_df], axis=0)\n    count +=1\n    \ntest_dfs = test_dfs.reset_index(drop=True) \ntest_dfs = test_dfs.rename(columns={'index':'segment_id'})\ntest_dfs = pd.merge (test_dfs, test_small, on = 'segment_id' )\ntest_dfs.head(10)\ntest_dfs.to_csv('volcano_test_small_fts.csv', index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = loaded_dfs.drop(['segment_id', 'time_to_eruption'], axis=1)\ny = loaded_dfs['time_to_eruption']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      test_size=0.2, \n                                                      random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Default parameters\nparams = {\n    'boosting_type': 'gbdt', \n    'num_leaves': 31,\n    'max_depth': -1,\n    'learning_rate': 0.1,\n    'n_estimators': 100, \n    'subsample_for_bin': 200, # 200000 is default \n    #'objective': 'binary'\n    'min_split_gain': 0.5,    # 0.0 is default \n    'min_child_weight': 1e-3, \n    'min_child_samples': 20,\n    'subsample': 1,\n    'colsample_bytree': 1.0,\n    'min_data_in_leaf': 20,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'random_state': 42\n    #'device': 'cpu', # you can use GPU to achieve faster learning\n}\n        \n# Initiate classifier to use\nmodel_lgbm_regr = LGBMRegressor(boosting_type = params['boosting_type'], \n                                num_leaves = params['num_leaves'],\n                                max_depth = params['max_depth'],\n                                learning_rate = params['learning_rate'],\n                                n_estimators = params['n_estimators'],\n                                subsample_for_bin = params['subsample_for_bin'],\n                                #objective = params['objective'],\n                                min_split_gain = params['min_split_gain'], \n                                min_child_weight = params['min_child_weight'], \n                                min_child_samples = params['min_child_samples'],\n                                subsample = params['subsample'],\n                                colsample_bytree = params['colsample_bytree'],\n                                min_data_in_leaf = params['min_data_in_leaf'],\n                                feature_fraction = params['feature_fraction'],\n                                bagging_fraction = params['bagging_fraction'],\n                                random_state = params['random_state'],\n                                #n_jobs = 5, \n                                silent = True\n                               )\n\n# To view the default model parameters:\nmodel_lgbm_regr.get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lgbm_regr.fit(X_train, y_train, \neval_set= [(X_train, y_train), (X_valid, y_valid)], eval_metric=\"mae\", verbose=200, early_stopping_rounds=50)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model_lgbm_regr.predict(test_dfs.drop(columns=['segment_id']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['segment_id'] = test_dfs[\"segment_id\"]\nsubmission['time_to_eruption'] = predictions\nsubmission.to_csv('submission.csv', header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head(6)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}