{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# WELCOME ------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom scipy.stats import skew","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/predict-volcanic-eruptions-ingv-oe/train.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test = pd.read_csv('/kaggle/input/predict-volcanic-eruptions-ingv-oe/sample_submission.csv')\nTest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_null(data):\n    data = data.fillna(0)\n    return data\n    \n    \ndef basic_stats(data):\n    \n    for col in data.columns:\n        data[f'{col}_mean'] = data[col].mean()\n        data[f'{col}_kurtosis'] = data[col].kurtosis()\n        data[f'{col}_sum'] = data[col].sum()\n        data[f'{col}_max'] = data[col].max()\n        data[f'{col}_min'] = data[col].min()\n        data[f'{col}_std'] = data[col].std()\n        data[f'{col}_skew'] = data[col].skew()\n        data[f'{col}_median'] = data[col].median()\n        data[f'{col}_mean'] = data[col].mean()\n        data[f'{col}_var'] = data[col].var()\n        data[f'{col}_99_quantile'] = data[col].quantile(0.99)\n        data[f'{col}_90_quantile'] = data[col].quantile(0.90)\n        data[f'{col}_70_quantile'] = data[col].quantile(0.70)\n        data[f'{col}_30_quantile'] = data[col].quantile(0.30)\n        data[f'{col}_10_quantile'] = data[col].quantile(0.10)\n        data[f'{col}_50_quantile'] = data[col].quantile(0.5)\n        data[f'{col}_5_quantile'] = data[col].quantile(0.05)\n        data[f'{col}_1_quantile'] = data[col].quantile(0.01)\n    data = data.iloc[0:1,10:]\n    return data\n\n\n\ndef outlier_treatment(data):\n    for col in data.columns:\n        ninety_quantile = data[col].quantile(0.99)\n        print(ninety_quantile)\n        \ndef train_merge (data,train):\n    data['segment_id'] = data['segment_id'].astype('float64')\n    final_data = data.merge(train, on='segment_id')\n    return final_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\ndata = pd.DataFrame()\nfor i in (train['segment_id']):\n    path = f'/kaggle/input/predict-volcanic-eruptions-ingv-oe/train/{i}.csv'\n    df = pd.read_csv(path)\n    df = fill_null(df)\n    df = basic_stats(df)\n    df['segment_id'] = i\n    \n    data = pd.concat([data,df],axis=0,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preparing test data \ntest_data = pd.DataFrame()\na = 0\nfor i in (Test['segment_id']):\n    path = f'/kaggle/input/predict-volcanic-eruptions-ingv-oe/test/{i}.csv'\n    df = pd.read_csv(path)\n    df = fill_null(df)\n    df = basic_stats(df)\n    df['segment_id'] = i\n    a+=1\n#     print(a)\n    test_data = pd.concat([test_data,df],axis=0,ignore_index=True)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merging data with train and adding time to explode column\n\ntrain_data = train_merge(data,train)\n\n\n# to remove outliers\n\ntrain_data['time_to_eruption'].plot(kind='box')\n\n# seems no outliers we are read to go ahead","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scaling\nfrom sklearn.preprocessing import StandardScaler\ndef scaling(X_train, X_test, trial):\n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform(X_test)\n    test_data = sc.transform(trial)\n    return X_train, X_test, test_data\n\ndef lin_model(X_train, X_test, Y_train):\n    lin = LinearRegression()\n    lin.fit(X_train, Y_train)\n    y_test_predict = lin.predict(X_test)\n    return y_test_predict\n\ndef decision_model(X_train, X_test, Y_train,trial):\n    model = DecisionTreeRegressor(criterion='mae',\n                                 max_features='auto',\n                                 min_samples_split = 2,\n                                 min_samples_leaf = 5)    \n    model.fit(X_train, Y_train)\n    y_test_predict = model.predict(X_test)\n    final_predict = model.predict(trial)\n    return y_test_predict,final_predict\n\n\ndef random_model(X_train, X_test, Y_train,trial):\n    params = {'n_estimators': 400,\n         'min_samples_split': 2,\n         'min_samples_leaf': 1,\n         'max_features': 'sqrt',\n         'max_depth': None,\n         'bootstrap': False}\n    model = RandomForestRegressor(**params)\n    model.fit(X_train, Y_train)\n    y_test_predict = model.predict(X_test)\n    final_predict = model.predict(trial)\n    return y_test_predict,final_predict\n    \n    \ndef lgbm_model(X_train, X_test,Y_test, Y_train,trial):\n    model = lgbm.LGBMRegressor(bagging_fraction=1.0, bagging_freq=3, boosting_type='gbdt',\n              class_weight=None, colsample_bytree=1.0, device='gpu',\n              feature_fraction=0.7, gpu_id=0, importance_type='split',\n              learning_rate=0.01, max_depth=-1, min_child_samples=5,\n              min_child_weight=0.001, min_split_gain=0.3, n_estimators=2500,\n              n_jobs=-1, num_leaves=150, objective=None, random_state=1176,\n              reg_alpha=1, reg_lambda=0.5, silent=True, subsample=1.0,\n              subsample_for_bin=200000, subsample_freq=0,\n              tree_method='gpu_hist')\n     \n    model.fit(X_train, Y_train,\n              eval_set=[(X_test, Y_test)], eval_metric='mae',\n              verbose=1000, early_stopping_rounds=400)\n        \n    y_test_predict = model.predict(X_test, num_iteration=model.best_iteration_)\n    final_predict = model.predict(trial, num_iteration=model.best_iteration_)\n    return y_test_predict,final_predict\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\ndef hypermeter_tunning(X_train, Y_train):\n    # Number of trees in random forest\n    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n    # Number of features to consider at every split\n    max_features = ['auto', 'sqrt']\n    # Maximum number of levels in tree\n    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n    max_depth.append(None)\n    # Minimum number of samples required to split a node\n    min_samples_split = [2, 5, 10]\n    # Minimum number of samples required at each leaf node\n    min_samples_leaf = [1, 2, 4]\n    # Method of selecting samples for training each tree\n    bootstrap = [True, False]\n    # Create the random grid\n    random_grid = {'n_estimators': n_estimators,\n                   'max_features': max_features,\n                   'max_depth': max_depth,\n                   'min_samples_split': min_samples_split,\n                   'min_samples_leaf': min_samples_leaf,\n                   'bootstrap': bootstrap}\n    \n\n    rf = RandomForestRegressor()\n    \n    # Random search of parameters, using 3 fold cross validation, \n    # search across 100 different combinations, and use all available cores\n    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n    # Fit the random search model\n    rf_random.fit(X_train, Y_train)\n    return rf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preparing data for modelling\nX = train_data.drop(columns=['segment_id','time_to_eruption'])\nY = train_data['time_to_eruption']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trial = test_data.copy()\ntrial = trial.drop(columns=['segment_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold \nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgbm\nscore = []\nfinal = []\nkfold = KFold(n_splits=5, random_state=24, shuffle=True)\n\nfor train, test in kfold.split(X):\n    X_train, X_test = X.iloc[train], X.iloc[test]\n    Y_train, Y_test = Y.iloc[train], Y.iloc[test]\n    X_train, X_test, t = scaling(X_train, X_test,trial)\n    #------- Hypermeter Tuning ----------You can try to find paramter for radom forest it takes 3 hrs\n#     parameters = hypermeter_tunning(X_train, Y_train)\n#     break\n    #====== MODEL =======================\n    y_test_predict,final_predict = random_model(X_train, X_test, Y_train,trial)\n    #====================================\n    mae = mean_absolute_error(Y_test, y_test_predict)\n    print(mae)\n    score.append(mae)\n    final.append(final_predict)\n        \n        \naverage_score = np.mean(score)\nprint('The average mae is ', average_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding our prediction in submission aaray \nimport numpy as np\nprediction = np.mean(final,0)\nTest['time_to_eruption'] = prediction\nTest.to_csv('rf_mean.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}