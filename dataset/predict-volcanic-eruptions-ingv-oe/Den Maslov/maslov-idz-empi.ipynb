{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport lightgbm as lgbm\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom tqdm import tqdm\n\nfile_list = []\nfile_list_train = []\nfile_list_test = []\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        file_list.append(os.path.join(dirname, filename))\n        \nPATH = '/kaggle/input/predict-volcanic-eruptions-ingv-oe/'\n\nfor dirname, _, filenames in os.walk('/kaggle/input/predict-volcanic-eruptions-ingv-oe/train'):\n    for filename in filenames:\n        file_list_train.append(os.path.join(dirname, filename))\n        \nfor dirname, _, filenames in os.walk('/kaggle/input/predict-volcanic-eruptions-ingv-oe/test'):\n    for filename in filenames:\n        file_list_test.append(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(file_list[2])\n\n\nprint(pd.read_csv(file_list[0]))\nprint(pd.read_csv(file_list[2]).isna().sum())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(file_list_train[0])\nprint(pd.read_csv(file_list_train[0]))\nprint(len(file_list_train))\nprint(len(file_list_test))\n\nfiles_test = [file.split('/')[-1].split('.')[-2] for file in file_list_test]\nfiles_train = [file.split('/')[-1].split('.')[-2] for file in file_list_train]\n\nprint(files_train[0:10])\nprint(files_test[0:10])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set = set(files_test)\ntrain_set = set(files_train)\ninter = test_set.intersection(train_set)\n\nprint(inter)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(PATH+'train.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(train['time_to_eruption'],\n            hist=True,\n            kde=True,\n            bins=100,\n            color='red',\n            hist_kws={'edgecolor':'black'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['time_to_eruption'].describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_segment_id = pd.read_csv(PATH+'test/473253715.csv')\n\ndf_segment_id.plot(figsize=(20,20),\n                  subplots=True,\n                  layout=(10,1),\n                  rot=0,\n                  lw=1,\n                  title='sergemnt id #473253715')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train.sort_values('time_to_eruption', axis=0, ascending=True).iloc[[0,-1],:])\n\nsegment_id_min = 601524801\nsegment_id_max = 1923243961\n\ndf_segment_id_min = pd.read_csv(PATH+'train/'+str(segment_id_min)+'.csv')\n\ndf_segment_id_max = pd.read_csv(PATH+'train/'+str(segment_id_max)+'.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_segment_id_min.plot(figsize=(20,20),\n                  subplots=True,\n                  layout=(10,1),\n                  rot=0,\n                  lw=1,\n                  title='sergemnt id min')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_segment_id_max.plot(figsize=(20,20),\n                  subplots=True,\n                  layout=(10,1),\n                  rot=0,\n                  lw=1,\n                  title='sergemnt id max')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_features(signal, ts, sensor_id):\n    X = pd.DataFrame()\n    f = np.fft.fft(signal)\n    f_real = np.real(f)\n    X.loc[ts, f'{sensor_id}_sum'] = signal.sum()\n    X.loc[ts, f'{sensor_id}_mean'] = signal.mean()\n    X.loc[ts, f'{sensor_id}_std'] = signal.std()\n    X.loc[ts, f'{sensor_id}_var'] = signal.var()\n    X.loc[ts, f'{sensor_id}_max'] = signal.max()\n    X.loc[ts, f'{sensor_id}_min'] = signal.min()\n    X.loc[ts, f'{sensor_id}_skew'] = signal.skew()\n    X.loc[ts, f'{sensor_id}_mad'] = signal.mad()\n    X.loc[ts, f'{sensor_id}_kurtosis'] = signal.kurtosis()\n    X.loc[ts, f'{sensor_id}_quantile99'] = np.quantile(signal, 0.99)\n    X.loc[ts, f'{sensor_id}_quantile95'] = np.quantile(signal, 0.95)\n    X.loc[ts, f'{sensor_id}_quantile85'] = np.quantile(signal, 0.85)\n    X.loc[ts, f'{sensor_id}_quantile75'] = np.quantile(signal, 0.75)\n    X.loc[ts, f'{sensor_id}_quantile55'] = np.quantile(signal, 0.55)\n    X.loc[ts, f'{sensor_id}_quantile45'] = np.quantile(signal, 0.45)\n    X.loc[ts, f'{sensor_id}_quantile25'] = np.quantile(signal, 0.25)\n    X.loc[ts, f'{sensor_id}_quantile15'] = np.quantile(signal, 0.15)\n    X.loc[ts, f'{sensor_id}_quantile05'] = np.quantile(signal, 0.05)\n    X.loc[ts, f'{sensor_id}_quantile01'] = np.quantile(signal, 0.01)\n    X.loc[ts, f'{sensor_id}_fft_real_mean'] = f_real.mean()\n    X.loc[ts, f'{sensor_id}_fft_real_std'] = f_real.std()\n    X.loc[ts, f'{sensor_id}_fft_real_max'] = f_real.max()\n    X.loc[ts, f'{sensor_id}_fft_real_min'] = f_real.min()\n    \n    return X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = list()\nseg = 0\n\nfor seg, segment_id in tqdm(enumerate(train.segment_id)):\n    signals = pd.read_csv(PATH+'train/'+str(segment_id)+'.csv')\n    train_row = []\n    \n    if seg%200 == 0:\n        print('Processing segment_id={}'.format(seg))\n        \n    for sensor in range(0, 10):\n        sensor_id = f'sensor_{sensor+1}'\n        train_row.append(build_features(signals[sensor_id].fillna(0), segment_id, sensor_id))\n        \n    train_row = pd.concat(train_row, axis=1)\n    train_set.append(train_row)\n    seg+=1\n    \ntrain_set = pd.concat(train_set)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = train_set.reset_index()\ntrain_set = train_set.rename(columns={'index':  'segment_id'})\n\ntrain_set = pd.merge(train_set, train, on='segment_id')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_files = []\nfor dirname, _, filenames in os.walk(PATH+'test/'):\n    for filename in filenames:\n        test_files.append(filename[:-4])\n        \ntest = pd.DataFrame(test_files, columns=['segment_id'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set = list()\nseg = 0\n\nfor seg, segment_id in tqdm(enumerate(test.segment_id)):\n    signals = pd.read_csv(PATH+'test/'+str(segment_id)+'.csv')\n    test_row = []\n    \n    if seg%200 == 0:\n        print('Processing segment_id={}'.format(seg))\n        \n    for sensor in range(0, 10):\n        sensor_id = f'sensor_{sensor+1}'\n        test_row.append(build_features(signals[sensor_id].fillna(0), segment_id, sensor_id))\n        \n    test_row = pd.concat(test_row, axis=1)\n    test_set.append(test_row)\n    seg+=1\n    \ntest_set = pd.concat(test_set)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set = test_set.reset_index()\ntest_set = test_set.rename(columns={'index':  'segment_id'})\n\ntest_set = pd.merge(test_set, test, on='segment_id')\n\nprint(test_set.head(3))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_set.drop(['segment_id', 'time_to_eruption'], axis=1)\ny = train_set['time_to_eruption']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      test_size=0.2,\n                                                      random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.head(3))\nprint('np.shape(X_train) = ', np.shape(X_train))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_train.head(3))\nprint('np.shape(y_train) = ', np.shape(y_train))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(max_depth=20, random_state=0)\nmodel.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_valid)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score\n\nconf_mat = r2_score(y_valid, y_pred)\nprint(conf_mat)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nmse = mean_squared_error(y_valid, y_pred)\nfig = plt.figure()\nmulreg = fig.add_subplot(1, 1, 1)\nmulreg.scatter(y_valid, y_pred, color='r')\nmulreg.set_title('Nonlinear Regression') ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = model.predict(test_set.drop(columns=['segment_id']))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame()  \nsubmission['segment_id'] = test_set['segment_id']\nsubmission['time_to_eruption'] = prediction\nsubmission.to_csv('submission.csv', header=True, index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from math import isnan\nsensor_result = {}\nfor i in range(1,11):\n    sensor_result[f\"sensor_{i}\"] =  0\n    \nprint(sensor_result)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in file_list_test:\n    df_test_stat = pd.read_csv(i)\n    for j in range(1,11):\n        if isnan(df_test_stat[f\"sensor_{j}\"].mean()):\n            sensor_result[f\"sensor_{j}\"] += 1\n\nprint(sensor_result)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.bar(sensor_result.keys(), sensor_result.values(), width=.5, color='blue')","metadata":{},"execution_count":null,"outputs":[]}]}