{"cells":[{"metadata":{},"cell_type":"markdown","source":"# A Broken Neural Network "},{"metadata":{},"cell_type":"markdown","source":"I could have tried to find a solution to this challenge using a Random Forest / XGBoost approach, but I decided to use a neural network instead because I'm a complete deep learning newbie. Since I never used that technique for regression problems, I thought it could be fun to give it a try, but after a few hours of hyper parameters tweaking and loss functions testing, my network still doesn't work (at all). Is it the loss function choice that is wrong? Is it the shape of the graph? Is it a standardization problem? Is it just the wrong approach for that kind of problem? I have no idea.\n\nDo you folks know what went wrong? Any feedback is much appreciated!"},{"metadata":{},"cell_type":"markdown","source":"## Loading & Cleaning"},{"metadata":{"collapsed":true},"source":"%matplotlib inline","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"from geopy.distance import vincenty","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"train = pd.read_csv('../input/train.csv')","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"test = pd.read_csv('../input/test.csv')","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outliers"},{"metadata":{"collapsed":true},"source":"# new york city area\nmin_lon = -74.844,\nmin_lat = 40.026\nmax_lon = -72.221\nmax_lat = 41.372","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"initial_len = train.shape[0]","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"# ruling points outside area as outliers\ntrain = train[train['pickup_longitude'].between(min_lon, max_lon)]\ntrain = train[train['pickup_latitude'].between(min_lat, max_lat)]\ntrain = train[train['dropoff_longitude'].between(min_lon, max_lon)]\ntrain = train[train['dropoff_latitude'].between(min_lat, max_lat)]","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"cleaned_len = train.shape[0]","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"# outliers removed\ninitial_len - cleaned_len","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compute distance"},{"metadata":{"collapsed":true},"source":"def get_distance(row):\n    p1 = (row['pickup_latitude'], row['pickup_longitude'])\n    p2 = (row['dropoff_latitude'], row['dropoff_longitude'])\n    return vincenty(p1, p2).meters","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"train['distance'] = train.apply(get_distance, axis=1)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"lon = train.loc[0]['pickup_longitude']\nlat = train.loc[0]['pickup_latitude']","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NaN Correction"},{"metadata":{},"source":"# check for na's\ntrain.isnull().sum()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Datetime"},{"metadata":{"collapsed":true},"source":"# to datetime\ntrain['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'])\ntrain['dropoff_datetime'] = pd.to_datetime(train['dropoff_datetime'])","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"# get day and hour information\ntrain['pickup_hour'] = train['pickup_datetime'].apply(lambda x: x.hour)\ntrain['pickup_day'] = train['pickup_datetime'].apply(lambda x: x.weekday())","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"# convert timestamps to float\ntrain['pickup_datetime'] = train['pickup_datetime'].apply(lambda x: x.timestamp())\ntrain['dropoff_datetime'] = train['dropoff_datetime'].apply(lambda x: x.timestamp())","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"# get dummy variables for categorial data\ntrain['store_and_fwd_flag'] = pd.get_dummies(train['store_and_fwd_flag'])","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Irrelevant features"},{"metadata":{"collapsed":true},"source":"train = train.drop('id', axis=1)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploration"},{"metadata":{},"cell_type":"markdown","source":"### Distance / Trip Duration"},{"metadata":{"collapsed":true},"source":"from scipy.stats import pearsonr","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"pearsonr(train['distance'], train['trip_duration'])","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"## RNN"},{"metadata":{},"source":"import keras\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Activation, LSTM, Merge\nfrom keras.optimizers import SGD\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import normalize\nfrom sklearn.model_selection import train_test_split","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"X = train.drop('trip_duration', axis=1)\ny = train['trip_duration']","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"X_train, X_test, y_train, y_test = train_test_split(X, y)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"y_norm = normalize(y.reshape(1, -1))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"X_train.shape","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"y_norm.T.shape","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"def baseline():\n    model = Sequential()\n    model.add(Dense(48, input_dim=12, activation='relu', kernel_initializer='normal'))\n    model.add(Dense(12, activation='relu'))\n    model.add(Dense(1, activation='linear', kernel_initializer='normal'))\n    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n    return model","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"estimators = []\nregressor = KerasRegressor(build_fn=baseline, epochs=5, batch_size=50, verbose=1)\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', regressor))\npipeline = Pipeline(estimators)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"kfold = KFold(n_splits=5)\nresults = cross_val_score(pipeline, X, y_norm.T, cv=kfold)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))","execution_count":null,"cell_type":"code","outputs":[]}],"nbformat":4,"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","version":"3.5.2"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat_minor":1}