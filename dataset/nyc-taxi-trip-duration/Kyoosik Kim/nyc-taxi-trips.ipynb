{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## NYC Taxi Trip Duration Modeling\n#### <i>How long will my taxi trip take?</i>\n\n<p style=\"font-size:16px\">This research explores the public dataset used for <a href=\"https://www.kaggle.com/c/nyc-taxi-trip-duration\" target=\"_blank\" rel=\"noopener noreferrer\">NYC Taxi Trip Duration</a> competition, processes the features, and builds a prediction model. Based on this research, <a href=\"https://nyc-taxi-trip.herokuapp.com/\" target=\"_blank\" rel=\"noopener noreferrer\">this Flask App</a> has been deployed and running via Heroku. Details about the app development can also be found on <a href=\"https://github.com/Q-shick/taxi_trip_duration\" target=\"_blank\" rel=\"noopener noreferrer\">this GitHub repo.</a></p>\n\n<ol style=\"font-size:16px; margin-left:40px\">\n    <li>Preparation - Reads data and imputes observations</li>\n    <li>Analysis - Understands variables to affect trip durations</li>\n    <li>Features - Processes variables and creates other variables using external data</li>\n    <li>Modeling - Builds a nueral network for prediction</li>\n</ol>","metadata":{}},{"cell_type":"markdown","source":"<br>\n<hr>\n\n## Preparation\n<p style=\"font-size:16px\">First, we need to import all the necessary Python libraries so we can read datasets and check the data quality to begin.</p>","metadata":{}},{"cell_type":"code","source":"# Basic data handling\nimport numpy as np\nimport pandas as pd \nimport json \n\n# Plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sb\nimport plotly.express as px\n\n# Statistical testing\nfrom scipy.stats import ranksums\n\n# Datetime handling\nfrom datetime import datetime as dt\nimport calendar\nimport holidays as hd\n\n# Geographical processing\nimport geopandas as gpd \nimport geopy.distance as gpy\nfrom geopy.geocoders import Nominatim\nfrom shapely.geometry import LineString, Point, Polygon, LinearRing, shape, asShape\nimport shapely.ops as so\nfrom rtree import index # for fast look-up\n\n# Model preparing\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Lasso regression\nfrom sklearn.linear_model import Lasso\n\n# Nueral network\nfrom keras.models import Model, Sequential, load_model\nfrom keras.layers import Input, Dense, Dropout, BatchNormalization, Activation, Add\nfrom keras.metrics import RootMeanSquaredError\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l1\n\n# Others\nimport os\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip '../input/nyc-taxi-trip-duration/train.zip'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"taxi_trip_data = pd.read_csv('./train.csv')\nprint(\"Dataset Rows and Columns: \", taxi_trip_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"taxi_trip_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">The following descriptions are from the competition introduction. We will create new variables from these and utilize external data such as physical map and historical weather.</p>\n\n<ul style=\"font-size:16px; margin-left:40px\">\n    <li>id - a unique identifier for each trip</li>\n    <li>vendor_id - a code indicating the provider associated with the trip record</li>\n    <li>pickup_datetime - date and time when the meter was engaged</li>\n    <li>dropoff_datetime - date and time when the meter was disengaged</li>\n    <li>passenger_count - the number of passengers in the vehicle (driver entered value)</li>\n    <li>pickup_longitude - the longitude where the meter was engaged</li>\n    <li>pickup_latitude - the latitude where the meter was engaged</li>\n    <li>dropoff_longitude - the longitude where the meter was disengaged</li>\n    <li>dropoff_latitude - the latitude where the meter was disengaged</li>\n    <li>store_and_fwd_flag - this flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip</li>\n    <li>trip_duration - duration of the trip in seconds</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"print(\"[Taxi Trip Data Types]\\n\", taxi_trip_data.info(), sep='')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">Now, we can impute observations that do not make sense in terms of trip distance and speed. There are observations that made no distance but still had some duration. Also, some observations made a trip but their speed was as unrealistic as 2 miles per hour or 100 miles per hour. These trips do not make sense and are likely to interfere with our model to generalize observations.</p>","metadata":{}},{"cell_type":"code","source":"# Straight line between pickup and dropoff points - Actual trip should be longer\ntaxi_trip_data['dist_mile'] = taxi_trip_data.apply(lambda d : \\\n                              gpy.distance((d.pickup_latitude, d.pickup_longitude), \n                                           (d.dropoff_latitude, d.dropoff_longitude)).miles, axis=1)\n\n# Speed derived from duration and distance - Converted to per hour\ntaxi_trip_data['speed'] = taxi_trip_data['dist_mile'] / taxi_trip_data['trip_duration'] * 3600 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Trips with no distance: \", sum(taxi_trip_data['dist_mile']<0.25))\ntaxi_trip_data = taxi_trip_data[taxi_trip_data['dist_mile']>0.25] \nprint(\"Trips with speed less than 3 mps: \", sum(taxi_trip_data['speed']<=4))\ntaxi_trip_data = taxi_trip_data[taxi_trip_data['speed']>4] \nprint(\"Trips with speed greater than 90 mps: \", sum(taxi_trip_data['speed']>=80))\ntaxi_trip_data = taxi_trip_data[taxi_trip_data['speed']<80] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">The removal above has taken care of most unrealistic trips, but we can make sure of it by looking into long trips. Given that NYC is a quite busy city, the speed of the trips below might make sense but their distances are too long to believe that they were recorded properly. The distances are mostly even longer than the distance from top to bottom of Manhattan, and even if the trips were entirely in busy Manhattan in rush hour it is hard to assume that a passenger would stay in the taxi instead of getting off and finding other transportations like subway.","metadata":{}},{"cell_type":"code","source":"print(\"3+ hours trips: \", sum(taxi_trip_data['trip_duration']>=7200))\ntaxi_trip_data = taxi_trip_data[taxi_trip_data['trip_duration']<7200]\nprint(\"Less than 30 seconds trips: \", sum(taxi_trip_data['trip_duration']<60))\ntaxi_trip_data = taxi_trip_data[taxi_trip_data['trip_duration']>=60]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">Lastly, we need to remove trips with no passengers.</p>","metadata":{}},{"cell_type":"code","source":"print(\"Trips with no passengers: \", sum(taxi_trip_data['passenger_count']>0))\ntaxi_trip_data = taxi_trip_data[taxi_trip_data['passenger_count']>0]\nprint(\"After removing error observations: \", taxi_trip_data.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<hr>\n\n## Analysis\n<p style=\"font-size:16px\">Because at large we have pickup/dropoff locations and pickup time (dropoff time information already in trip duration), we will focus on those variables to find busy locations and times.","metadata":{}},{"cell_type":"code","source":"print(\"Basic Statistics of Trip Duration\\n\", sep='')\ntaxi_trip_data.describe().apply(lambda s : s.apply('{0:.0f}'.format))['trip_duration']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Limit printing too long trips\ntrip_duration_hist = taxi_trip_data[taxi_trip_data['trip_duration']<=3600]\\\n                        .sample(n=10000, replace=True, random_state=123)['trip_duration'] / 60\n\nfig = plt.figure(figsize=(12, 5))\nsb.histplot(data=trip_duration_hist, x=trip_duration_hist.values, alpha=0.7, bins=60, kde=True)\nplt.axvline(x=trip_duration_hist.median(), color='g')\nplt.xlabel(\"Minutes (Median at Green Line)\")\nplt.title(\"Distribution of Trip Duration\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">First off, we want to take a look at the target variable. Trip durations are mostly short around 10 minutes with a few exceptionally long trips as seen in the right-skewed distribution.</p>","metadata":{}},{"cell_type":"code","source":"# Trip percent by vendor id\nvendor_id_bar = pd.DataFrame(taxi_trip_data['vendor_id'].value_counts(normalize=True)).reset_index().\\\n    rename(columns={'index':'vendor_id', 'vendor_id':'trips'}).\\\n        sort_values(by='vendor_id')\nvendor_id_bar['trips'] = vendor_id_bar['trips']*100\n\n# Trip percent by store and forward\nstore_and_fwd_flag_bar = pd.DataFrame(taxi_trip_data['store_and_fwd_flag'].value_counts(normalize=True)).reset_index().\\\n    rename(columns={'index':'store_and_fwd_flag', 'store_and_fwd_flag':'trips'}).\\\n        sort_values(by='store_and_fwd_flag', ascending=False)\nstore_and_fwd_flag_bar['trips'] = store_and_fwd_flag_bar['trips']*100\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\nsb.barplot(ax=ax[0], data=vendor_id_bar, x='vendor_id', y='trips')\nax[0].set_title(\"Trips by Vendor ID\")\nax[0].set_xlabel(\"Vendor ID\")\nax[0].set_ylabel(\"Trip %\")\n\nsb.barplot(ax=ax[1], data=store_and_fwd_flag_bar, x='store_and_fwd_flag', y='trips')\nax[1].set_title(\"Trips by Store and Forward\")\nax[1].set_xlabel(\"Store and Forward\")\nax[1].set_ylabel(\"Trip %\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">Next, we can see that both Vendor ID and Store and Forward Flag have two categories. While Store and Forward Flag is lopsided, Vendor ID is rather evenly divided and we like to know how much of a discriminant power the variable has against trip durations. As we already know the target variable is not normally distributed, we can think of a non-normal test to see if Vendor 1 and Vendor 2 have a different trip duration distributions. As below, the result tells that they are from different distributions.</p>","metadata":{}},{"cell_type":"code","source":"# Encoding for later use\ntaxi_trip_data['store_and_fwd_flag'] = taxi_trip_data['store_and_fwd_flag'].apply(lambda x : 1 if x=='Y' else 0)\n\n# Wilcoxon rank sum test for non-normal distributions - A small P-value means they are different\nrank_test = ranksums(taxi_trip_data[taxi_trip_data['vendor_id']==1]['trip_duration'],\n                     taxi_trip_data[taxi_trip_data['vendor_id']==2]['trip_duration'])\n\nprint(\"Vendor ID Rank Test P-value: \", rank_test[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">The following process parses pickup and dropoff datetimes into month, day, and hour. This way, we can also extract day of week.</p>","metadata":{}},{"cell_type":"code","source":"# Convert objects to datetimes\ntaxi_trip_data['pickup_datetime'] = taxi_trip_data['pickup_datetime'].\\\n    apply(lambda t : dt.strptime(t, '%Y-%m-%d %H:%M:%S'))\n\n# Parse datetimes\ntaxi_trip_data['pickup_month'] = taxi_trip_data.pickup_datetime.apply(lambda M : M.month)\ntaxi_trip_data['pickup_date'] = taxi_trip_data.pickup_datetime.apply(lambda D : D.day)\ntaxi_trip_data['pickup_hour'] = taxi_trip_data.pickup_datetime.apply(lambda h : h.hour)\ntaxi_trip_data['pickup_minute'] = taxi_trip_data.pickup_datetime.apply(lambda m : m.minute)\ntaxi_trip_data['pickup_day'] = taxi_trip_data.pickup_datetime.apply(lambda d : d.weekday())\n\n# Print out for checking\nprint(\"Pick Up Time Example: \", taxi_trip_data.pickup_datetime[0])\nprint(\"Month : \", taxi_trip_data.pickup_month[0],\n      \"\\nDate : \", taxi_trip_data.pickup_date[0],\n      \"\\nHour : \", taxi_trip_data.pickup_hour[0],\n      \"\\nMinute : \", taxi_trip_data.pickup_minute[0],\n      \"\\nDay of Week (Mon=0): \", taxi_trip_data.pickup_day[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n\nax[0].plot(np.arange(1, len(taxi_trip_data['pickup_month'].unique())+1), \\\n           taxi_trip_data['pickup_month'].value_counts().sort_index().values, '-o')\nax[0].set_xlabel(\"Month\")\nax[0].set_ylabel(\"Trips\")\nax[0].set_title(\"Trips by Month\")\n\nax[1].plot(np.arange(1, len(taxi_trip_data['pickup_hour'].unique())+1), \\\n        taxi_trip_data['pickup_hour'].value_counts().sort_index().values, '-o')\nax[1].set_xlabel(\"Hour\")\nax[1].set_ylabel(\"Trips\")\nax[1].set_title(\"Trips by Hour\")\n\nax[2].plot(np.arange(0, len(taxi_trip_data['pickup_day'].unique())), \\\n           taxi_trip_data['pickup_day'].value_counts().sort_index().values, '-o')\nax[2].set_xlabel(\"Date\")\nax[2].set_xticks(list(dict(enumerate(calendar.day_name)).keys()))\nax[2].set_xticklabels(list(dict(enumerate(calendar.day_name)).values()), rotation=90)\nax[2].set_ylabel(\"Trips\")\nax[2].set_title(\"Trips by Day\")\n\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trips_by_day = pd.DataFrame(taxi_trip_data.groupby('pickup_day').mean()['trip_duration'].reset_index())\ntrips_by_day['pickup_day'] = trips_by_day['pickup_day'].map(dict(enumerate(calendar.day_name)))\n\nplt.plot(trips_by_day['pickup_day'], trips_by_day['trip_duration'], '-o')\nplt.title(\"Trips by Day\")\nplt.xlabel(\"Date\")\nplt.xticks(trips_by_day['pickup_day'], rotation=90)\nplt.ylabel(\"Average Duration in Seconds\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">Not surprisingly, the average durations have the similar trend as the day of week because less trips on a day like Sunday mean less cars on roads resulting in faster trips. Also, we can extract holidays while we are handling datetimes.</p>","metadata":{}},{"cell_type":"code","source":"us_holidays = hd.US()\ntaxi_trip_data['holiday_ind'] = taxi_trip_data.pickup_datetime.apply \\\n    (lambda d : 1 if dt.strftime(d, \"%Y-%m-%d\") in us_holidays else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(4,4))\n\npassenger_bar = pd.DataFrame(taxi_trip_data['passenger_count'].value_counts(normalize=True)).reset_index().\\\n    rename(columns={'index':'passenger_count', 'passenger_count':'trips'})\npassenger_bar['trips'] = passenger_bar['trips']*100\n\nsb.set_color_codes(\"muted\")\nsb.barplot(data=passenger_bar, x='passenger_count', y='trips')\nplt.title(\"Passenger Counts\")\nplt.xlabel(\"Passengers\")\nplt.ylabel(\"Trip %\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Limit printing too long trips\nfig, ax = plt.subplots(2, 1, figsize=(12, 8))\n\ntrip_mile_hist = taxi_trip_data[taxi_trip_data['dist_mile']<=15].\\\n    sample(n=10000, replace=True, random_state=123)['dist_mile']\nsb.histplot(ax=ax[0], data=trip_mile_hist, x=trip_mile_hist.values, alpha=0.7, bins=60, kde=True)\nax[0].axvline(x=trip_mile_hist.median(), color='g')\nax[0].set_title(\"Distribution of Trip Distances\")\nax[0].set_xlabel(\"Miles (Median at Green Line)\")\nax[0].set_ylabel(\"Trips\")\n\ntrip_speed_hist = taxi_trip_data['speed'].sample(n=10000, replace=True, random_state=123)\n\nsb.histplot(ax=ax[1], data=trip_speed_hist, x=trip_speed_hist.values, alpha=0.7, bins=60, kde=True)\nax[1].axvline(x=trip_speed_hist.median(), color='g')\nax[1].set_title(\"Distribution of Speed\")\nax[1].set_xlabel(\"Miles per Second (Median at Green Line)\")\nax[1].set_ylabel(\"Trips\")\n\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_vars = ['trip_duration','dist_mile','passenger_count',\n             'pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude',\n             'holiday_ind','vendor_id','store_and_fwd_flag']\ntaxi_trip_corr = taxi_trip_data[corr_vars].corr()\n\nfig = plt.figure(figsize=(8,6))\nsb.heatmap(taxi_trip_corr, cmap=\"YlGnBu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">First, Speed shouldn't be considered because the variable is derived from the target variable so we won't be allowed to use the variable for prediction. The following location variables have a modest to strong relationship with trip durations.</p>\n<ul style=\"font-size:16px; margin-left:40px\">\n    <li>Distance - The longer a trip is, the longer time it takes</li>\n    <li>Latitude - The southern part of NYC (lower latitude) such as Manhattan takes longer time</li>\n    <li>Longitude - The eastern part of NYC (higher longitude) such as JFK Airport takes longer time</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"taxi_trip_corr.iloc[0,1:6]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">The DBSCAN process below finds dense pickup/dropoff spot for slow and fast trips.</p>","metadata":{}},{"cell_type":"code","source":"# Trips slower than 5 mps\nslow_trip = taxi_trip_data[taxi_trip_data['speed'] < 5] \\\n    [['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude']]. \\\n        sample(n=3000, replace=True, random_state=123)\n\n# Dense areas\npickup_cluster = DBSCAN(eps=0.0012, min_samples=20).fit(slow_trip[['pickup_longitude','pickup_latitude']])\ndropoff_cluster = DBSCAN(eps=0.0012, min_samples=20).fit(slow_trip[['dropoff_longitude','dropoff_latitude']])\n\n# -1 for all the other areas\nslow_trip['pickup_cluster'] = pickup_cluster.labels_\nslow_trip['dropoff_cluster'] = dropoff_cluster.labels_\n\nfig_slow_pickup = px.scatter_mapbox(slow_trip, \n    lon='pickup_longitude', lat='pickup_latitude',\n    center={\"lat\": 40.75, \"lon\": -73.98},\n    color='pickup_cluster', \n    mapbox_style=\"carto-positron\", zoom=11,\n    width=400, height=400)\nfig_slow_pickup.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0}, coloraxis_showscale=False)\nfig_slow_pickup.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">Most slow trips are in Manhattan, regarless of pickup/dropoff. Among dense spots are bus terminals and shopping areas.</p>","metadata":{}},{"cell_type":"code","source":"print(\"<Clusters from 3000 Samples>\")\nprint(\"Pick-up Busy Locations: \", dict(Counter(pickup_cluster.labels_)))\nprint(\"Drop-off Busy Locations: \", dict(Counter(dropoff_cluster.labels_)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Trips faster than 50 mps\nfast_trip = taxi_trip_data[taxi_trip_data['speed'] > 50] \\\n    [['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude']]. \\\n        sample(n=3000, replace=True, random_state=123)\n\n# Dense areas\npickup_cluster = DBSCAN(eps=0.001, min_samples=30).fit(fast_trip[['pickup_longitude','pickup_latitude']])\ndropoff_cluster = DBSCAN(eps=0.001, min_samples=30).fit(fast_trip[['dropoff_longitude','dropoff_latitude']])\n\n# -1 for all the other areas\nfast_trip['pickup_cluster'] = pickup_cluster.labels_\nfast_trip['dropoff_cluster'] = dropoff_cluster.labels_\n\nfig_fast_pickup = px.scatter_mapbox(fast_trip, \n    lon='pickup_longitude', lat='pickup_latitude',\n    center={\"lat\": 40.71, \"lon\": -73.89},\n    color='pickup_cluster', \n    mapbox_style=\"carto-positron\", zoom=10,\n    width=400, height=400)\nfig_fast_pickup.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0}, coloraxis_showscale=False)\nfig_fast_pickup.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">Unlike slow trips, only meaningful dense spot for fast trips is JFK Airport. From the results, we will create two features in the next. One is to indicate if a trip is in one of those slow pickup/dropoff areas and the other is to show how close to the center of the spot if it is in one of them. For this, we won't process fast trips spots because we will create another set of features that will take care of JFK Airport later.</p>","metadata":{}},{"cell_type":"code","source":"# Busy pick up areas\nslow_trip_pickup = slow_trip[slow_trip['pickup_cluster']>=0].groupby(['pickup_cluster']).\\\n    agg(['mean','count'])[['pickup_longitude','pickup_latitude']].reset_index().\\\n    sort_values(by=[('pickup_longitude','count')], ascending=False).\\\n    droplevel(level=1, axis=1).iloc[:,[0,1,3]].\\\n    rename(columns = {'pickup_cluster':'cluster','pickup_longitude':'longitude','pickup_latitude':'latitude'})\n\n# Busy drop off areas\nslow_trip_dropoff = slow_trip[slow_trip['dropoff_cluster']>=0].groupby(['dropoff_cluster']).\\\n    agg(['mean','count'])[['dropoff_longitude','dropoff_latitude']].reset_index().\\\n    sort_values(by=[('dropoff_longitude','count')], ascending=False).\\\n    droplevel(level=1, axis=1).iloc[:,[0,1,3]].\\\n    rename(columns = {'dropoff_cluster':'cluster','dropoff_longitude':'longitude','dropoff_latitude':'latitude'})\n\ndef dist_from_center(lat, long, centers, radius):\n    for _, row in centers.iterrows():\n        radius_measured = gpy.distance((lat, long), (row['latitude'], row['longitude'])).miles\n        if radius_measured < radius:\n            return row['cluster'], radius_measured\n    return -1, 0\n\n# taxi_trip_data[['busy_pickup_spot','busy_pickup_dist']] = taxi_trip_data.apply(lambda d : \\\n#     dist_from_center(d.pickup_latitude, d.pickup_longitude, slow_trip_pickup, 0.2), axis=1).tolist()\n# taxi_trip_data[['busy_dropoff_spot','busy_dropoff_dist']] = taxi_trip_data.apply(lambda d : \\\n#     dist_from_center(d.dropoff_latitude, d.dropoff_longitude, slow_trip_dropoff, 0.2), axis=1).tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">While we are handling locations, we can create two more features regarding movements as follows. Those are not directly interpretable in readable distances (mile or km) as they are geocoordinate differences. But this unit issue won't matter when it comes to modeling because all the features are to be normalized anyways.</p>","metadata":{}},{"cell_type":"code","source":"taxi_trip_data['horizontal_move'] = taxi_trip_data['dropoff_longitude'] - taxi_trip_data['pickup_longitude']\ntaxi_trip_data['vertical_move'] = taxi_trip_data['dropoff_latitude'] - taxi_trip_data['pickup_latitude']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<hr>\n\n## Features\n<p style=\"font-size:16px\">In this section, we will create features from the existing variables as well as external datasets such as NYC Congested Areas and Historical Weather. First, we can utilize the dataset we already have because we can estimate the road situation by aggregating the trips for trip count, average duration and speed.</p>\n\n<p style=\"font-size:16px\">Next, we will create multiple features from congestion data based on <a href=\"https://abc7ny.com/traffic-commuting-lincoln-tunnel-roads/1095908/\">ABC New Articles</a>. First, the fuctions below are to calculate how much of a trip is overlapped in any of the congested areas. For example, a 10 mile long trip that goes through the Brooklyn Bridge area for 1 mile will have the area valued 10%.</p>","metadata":{}},{"cell_type":"code","source":"# Read congested area data and prepare geographical inputs\ncongested_df = pd.read_csv('../input/taxi-data-temp/congested_areas.csv')\ncongested_areas = [{area[1][0] : Polygon(eval(area[1][1]))} for area in congested_df.iterrows()]\n\ndef within_congested_area(area, pickup_lon, pickup_lat, dropoff_lon, dropoff_lat):\n    \"\"\" Return miles overlapping the given area \n        if a straight line (trip) is within the area \"\"\"\n    ext = LinearRing(area.exterior.coords)\n    line = LineString([(pickup_lon, pickup_lat),(dropoff_lon, dropoff_lat)])\n    inter_p = line.intersection(ext)\n    \n    if (Point(pickup_lon, pickup_lat).within(area) == True) & (Point(dropoff_lon, dropoff_lat).within(area) == True): \n        return -1\n    elif Point(pickup_lon, pickup_lat).within(area) == True:\n        return gpy.distance((pickup_lat, pickup_lon),(inter_p.coords[0][1], inter_p.coords[0][0])).miles\n    elif Point(dropoff_lon, dropoff_lat).within(area) == True:\n        return gpy.distance((dropoff_lat, dropoff_lon),(inter_p.coords[0][1], inter_p.coords[0][0])).miles\n    elif line.intersection(ext).is_empty == False:\n        coords = [(p.x, p.y) for p in inter_p]\n        return gpy.distance((coords[0][1], coords[0][0]),(coords[1][1], coords[1][0])).miles\n    else:\n        return 0\n    \ndef congested_area_processing(df):\n    \"\"\" Call within congested area for congested areas and \n        calculate the within area percentage \"\"\"\n    for idx, row in congested_df.iterrows():\n        df[row['area']] = df.apply(lambda p : within_congested_area(congested_areas[idx][row['area']], \\\n            p.pickup_longitude, p.pickup_latitude, p.dropoff_longitude, p.dropoff_latitude), axis=1)\n        df[row['area']] = df.apply(lambda p : p[row['area']]/p['dist_mile'] if p['dist_mile'] > 0 else 0, axis=1)\n        df[row['area']] = df[row['area']].apply(lambda p : 1 if p < 0 else p)\n        \ndef congested_percent_group(df):\n    \"\"\" Group within area percentages \"\"\"\n    for idx, row in congested_df.iterrows():\n        df[row['area']+'_group']=df[row['area']].apply(lambda x : \\\n            'high' if x > 0.66 else 'mid' if x > 0.33 else 'low' if x > 0 else 'n/a')\n    \n\n# congested_area_processing(taxi_trip_data)\n# congested_percent_group(taxi_trip_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">Now we have all the percentages calculated for the trips that go through one or more congested areas. Instead of just congested portions, we can give more detailed information. The function will multiply the conegested portions with the average speeds of the areas. By doing so, we can not only provide the model with how much each trip had congested parts but also how congested the parts were. One caution here is that we will have speed 0 for trips not going through any congested areas, which could give the model a wrong signal. Thus, we should add one more feature to mark those trips as 'N/A'.</p>","metadata":{}},{"cell_type":"code","source":"taxi_trip_data = pd.read_csv('../input/taxi-data-temp/taxi_trip_temp.csv')\ngroup_names = taxi_trip_data.columns[taxi_trip_data.columns.str.contains('_group')]\n\n# Mean aggregation\ncongested_speed_mean = pd.DataFrame([taxi_trip_data.groupby([area,'pickup_day','pickup_hour']).mean()['speed'] \n                                     for area in group_names], index=congested_df['area']).reset_index()\ncongested_speed_mean = congested_speed_mean.melt(id_vars='area')\ncongested_speed_mean.columns = ['area','dist_percent','pickup_day','pickup_hour','avg_speed']\n\n# Count aggregation\ncongested_speed_count = pd.DataFrame([taxi_trip_data.groupby([area,'pickup_day','pickup_hour']).count()['speed'] \n                                      for area in group_names], index=congested_df['area']).reset_index()\ncongested_speed_count = congested_speed_count.melt(id_vars='area')\ncongested_speed_count.columns = ['area','dist_percent','pickup_day','pickup_hour','count']\n\n# Complete aggregation\ncongested_agg = pd.merge(congested_speed_mean, congested_speed_count, \\\n                         how='inner', on=['area','dist_percent','pickup_day','pickup_hour'])\ncongested_agg['avg_speed'] = congested_agg.apply(lambda s : s['avg_speed'] if s['count'] >= 5 else np.nan, axis=1)\ncongested_agg = congested_agg.sort_values(by=['area','dist_percent','pickup_day','pickup_hour'])\ncongested_agg['avg_speed'] = congested_agg['avg_speed'].fillna(method='ffill')\ncongested_agg = congested_agg[(congested_agg['dist_percent'] != 'n/a') & \\\n                              (np.isnan(congested_agg['avg_speed'])==False)]\n\ndef congested_speed_process(df, areas):\n    \"\"\" Multiply congested portion with congested speed \"\"\"\n    for area in areas:\n        df[area+'_speed'] = pd.merge(df, congested_agg[congested_agg['area']==area], how='left', \\\n                                     left_on=[area+'_group','pickup_day','pickup_hour'], \\\n                                     right_on=['dist_percent','pickup_day','pickup_hour'])['avg_speed']\n        df[area+'_speed_na'] = df[area+'_speed'].apply(lambda s : 1 if np.isnan(s)==True else 0)\n        df[area+'_speed'] = df[area+'_speed'].fillna(0)\n        \n        \n# congested_speed_process(taxi_trip_data, congested_df['area'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">As we know pickup/dropoff locations for every trip, we can consider bringing population data based on them. <a href=\"https://data.cityofnewyork.us/City-Government/New-York-City-Population-By-Neighborhood-Tabulatio/swpk-hqdp\">Population by Neighborhood Data</a> and <a href=\"https://data.cityofnewyork.us/City-Government/Neighborhood-Tabulation-Areas-NTA-/cpf4-rkhq\">NYC Neiborhood Map Data</a> allow us to do the job.","metadata":{}},{"cell_type":"code","source":"# Read NTA data\nneighborhoods = gpd.read_file('../input/taxi-data-temp/Neighborhood_Tabulation_Areas.geojson')\npopulation = pd.read_csv('../input/taxi-data-temp/New_York_City_Population_By_Neighborhood_Tabulation_Areas.csv')\npopulation = population[population['Year']==2010]\n\nprint(\"Neighborhoods Columns :\", neighborhoods.columns)\nprint(\"Population Columns :\", population.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neighbor_pop = pd.merge(neighborhoods, population, how='inner', left_on='ntacode', right_on='NTA Code')\nneighbor_pop = neighbor_pop[['Borough','ntaname','Population','geometry']]\n\n# Exclude NTA with population less than 2000\nprint(neighbor_pop[neighbor_pop['Population']<2000][['ntaname','Population']])\nneighbor_pop = neighbor_pop[neighbor_pop['Population']>=2000].reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">Searching points in geographical data is computationally expensive. Rtree may help search pickup/dropoff locations faster, which is implemented below. As the function is to return populations, we should carefully handle trips that don't have the data available by creating 'N/A' feature.</p>","metadata":{}},{"cell_type":"code","source":"def rtree_build():\n    \"\"\" Build search trees from neighborhoods' geographical data \"\"\"\n    global idx \n    idx = index.Index()\n    for fid, feature in neighbor_pop['geometry'].items():\n        idx.insert(fid, feature.bounds)\n\ndef neighbor_population(df, lon, lat):\n    \"\"\" Search points with trees and remove false positives \"\"\"\n    eps = 1e-7 # to make squares\n    all_hits = idx.intersection([lon, lat, lon+eps, lat+eps]) # rtree intersection not allowing points\n    real_hits = []\n    \n    for p in all_hits:\n        if Point(lon, lat).within(df.iloc[p]['geometry']):\n            real_hits.append(p)\n    \n    if len(real_hits) > 0:\n        return [df.loc[real_hits[0]]['Borough'], df.loc[real_hits[0]]['ntaname'], df.loc[real_hits]['Population'].mean(), 0]\n    else: \n        return ['unknown', 'unknown', 0, 1]\n\n    \n# rtree_build()\n\n# taxi_trip_data[['pickup_borough','pickup_nta','pickup_pop','pickup_pop_na']] = taxi_trip_data.apply(lambda p : \\\n#     neighbor_population(neighbor_pop, p['pickup_longitude'], p['pickup_latitude']), axis=1).tolist()\n# taxi_trip_data[['dropoff_borough','dropoff_nta','dropoff_pop','dropoff_pop_na']] = taxi_trip_data.apply(lambda p : \\\n#     neighbor_population(neighbor_pop, p['dropoff_longitude'], p['dropoff_latitude']), axis=1).tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">Lastly, we will use weather data downloaded from <a href=\"https://openweathermap.org/history-bulk\">Open Weather Map</a>. The data includes 40 years of weather data for a selected location, but there is a charge per location. Because all the boroughs mostly have virtually the same weather at any given time, we will use the weather dataset for Manhattan.</p>","metadata":{}},{"cell_type":"code","source":"# Trip count/average by neighborhood\nnta_agg = taxi_trip_data.groupby(['pickup_nta','pickup_day','pickup_hour']).\\\n    agg(['count','mean'])['trip_duration'].reset_index().\\\n    rename(columns={'count':'nta_trips','mean':'nta_mean_duration'})\n\nnta_agg['nta_trips'] = nta_agg.apply(lambda x : 0 \n    if x['nta_trips']<5 or x['nta_trips']==np.nan or x['pickup_nta']=='unknown' else x['nta_trips'], axis=1)\nnta_agg['nta_mean_duration'] = nta_agg.apply(lambda x : 0 if x['nta_trips'] == 0 else x['nta_mean_duration'], axis=1)\nnta_agg['nta_na'] = nta_agg['nta_trips'].apply(lambda x : 1 if x == 0 else 0)\n                                               \ntaxi_trip_data = pd.merge(taxi_trip_data, nta_agg, how='left', \n    on=['pickup_nta','pickup_day','pickup_hour'])\n\n# Average speed by borough, day, hour, and distance quantile\ntaxi_trip_data['dist_bins'] = pd.qcut(taxi_trip_data['dist_mile'], q=np.arange(0, 1.1, 0.1))\nspeed_agg = taxi_trip_data.groupby(['pickup_borough','pickup_day','pickup_hour','dist_bins']).\\\n    mean()['speed'].reset_index().rename(columns={'speed':'mean_speed'})\nspeed_agg['mean_speed'].fillna(method='ffill', inplace=True)\n                                               \ntaxi_trip_data = pd.merge(taxi_trip_data, speed_agg, how='left', \\\n    on=['pickup_borough','pickup_day','pickup_hour','dist_bins'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">Finally, we want to find <a href=\"https://openweathermap.org/\" target=\"_blank\" rel=\"noopener noreferrer\">weather status</a> for each trip by hour to use it for prediction. We already know that weather affects drives, so we will skip analyses and let the model figure out how to use it for prediction.</p>","metadata":{}},{"cell_type":"code","source":"taxi_trip_data = pd.read_csv('../input/taxi-data-temp/taxi_trip_temp.csv')\ntaxi_trip_data = taxi_trip_data.drop(columns=['temp','wind_deg','wind_speed','Clear', \n                                              'Clouds','Drizzle','Fog','Haze','Mist','Rain','Snow',\n                                              'rain_1h', 'rain_3h', 'snow_1h', 'snow_3h'])\ncongested_df = pd.read_csv('../input/taxi-data-temp/congested_areas.csv')\ngroup_names = taxi_trip_data.columns[taxi_trip_data.columns.str.contains('_group')]\n\nhourly_weather = pd.read_csv('../input/taxi-data-temp/nyc_weather_history.csv')\n\nhourly_weather['datetime'] = hourly_weather['dt_iso'].apply(\\\n    lambda t : dt.strptime(t[0:19], \"%Y-%m-%d %H:%M:%S\"))\n\nhourly_weather['year'] = hourly_weather.datetime.apply(lambda Y : Y.year)\nhourly_weather['month'] = hourly_weather.datetime.apply(lambda M : M.month)\nhourly_weather['date'] = hourly_weather.datetime.apply(lambda D : D.day)\nhourly_weather['hour'] = hourly_weather.datetime.apply(lambda h : h.hour)\n\nhourly_weather = hourly_weather[(hourly_weather['year'] == 2016) & (hourly_weather['month'] < 7)]\nhourly_weather.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of Weather Observations with Multiple Status: \", \\\n    sum(hourly_weather.groupby(['dt_iso'])['weather_main'].nunique() > 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Weather status values to columns - \"weather_main\" can have multiple statuses (e.g. Fog and Rain)\nhourly_weather['main_value'] = 1\nweather_main = pd.pivot_table(hourly_weather, index=hourly_weather.datetime, \\\n                     columns='weather_main', values='main_value').reset_index()\n\n# Join the statuses and precipitation columns\nweather_cols = ['datetime','month','date','hour','temp','clouds_all','wind_deg','wind_speed']\nhourly_weather = hourly_weather[weather_cols].drop_duplicates(keep='first')\nhourly_weather = hourly_weather.set_index('datetime').\\\n                    join(weather_main.reset_index().set_index('datetime'))\nhourly_weather = hourly_weather.reset_index().fillna(0).drop(columns='index')\nhourly_weather.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"taxi_trip_data = pd.merge(taxi_trip_data, hourly_weather, how='left', \n                 left_on=['pickup_month','pickup_date','pickup_hour'], right_on=['month','date','hour'])\ntaxi_trip_data = taxi_trip_data.drop(columns=['datetime','month','date','hour'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<hr>\n\n## Modeling\n<p style=\"font-size:16px\">Our ultimate goal is to predict the trip duration for given pickup and dropoff points. To do that, we need to once more prepare the data we have processed so far. This includes encoding and normalization.</p>","metadata":{}},{"cell_type":"code","source":"# Explicitly convert types to category\ntaxi_trip_data['pickup_month'] = taxi_trip_data.pickup_month.astype(\"category\")\ntaxi_trip_data['pickup_day'] = taxi_trip_data.pickup_day.astype(\"category\")\ntaxi_trip_data['pickup_hour'] = taxi_trip_data.pickup_hour.astype(\"category\")\ntaxi_trip_data['pickup_minute'] = (taxi_trip_data.pickup_minute//10).astype(\"category\")\n\ntaxi_trip_data['vendor_id'] = taxi_trip_data.vendor_id.astype(\"category\")\ntaxi_trip_data['store_and_fwd_flag'] = taxi_trip_data.store_and_fwd_flag.astype(\"category\")\n\ntaxi_trip_data['busy_pickup_spot'] = taxi_trip_data.busy_pickup_spot.astype(\"category\")\ntaxi_trip_data['busy_dropoff_spot'] = taxi_trip_data.busy_dropoff_spot.astype(\"category\")\n\n# Speed to be dropped as derived from the target variable \ntaxi_trip_data.drop(columns='speed', inplace=True)\n\n# No longer needed\ntaxi_trip_data.drop(columns='dist_bins', inplace=True)\ntaxi_trip_data.drop(columns=group_names, inplace=True)\ntaxi_trip_data.drop(columns=congested_df['area'], inplace=True)\n\n# Encode categories\ncategorical_cols = ['pickup_day','pickup_hour','pickup_minute',\n                    'vendor_id','store_and_fwd_flag',\n                    'busy_pickup_spot','busy_dropoff_spot',\n                    'pickup_borough','dropoff_borough']\ntaxi_trip_data = pd.get_dummies(taxi_trip_data, columns=categorical_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = 'trip_duration'\nfeatures = list(taxi_trip_data.columns[(taxi_trip_data.dtypes == 'float64') | \n    (taxi_trip_data.dtypes == 'int64') | (taxi_trip_data.dtypes == 'uint8')])\n\nfeatures = [item for item in features if item not in \\\n    [target, 'pickup_month','pickup_date','pickup_hour''pickup_minute']]\nprint(\"Selected Features: \", features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale observations\nscaler = StandardScaler()\nscaler.fit(taxi_trip_data[features])\ntaxi_trip_data_scaled = scaler.transform(taxi_trip_data[features])\n\n# Split into training and test set\ndf_train, df_test, Ytrain, Ytest = train_test_split(\n    taxi_trip_data_scaled, taxi_trip_data[target].to_numpy(), test_size=0.25, random_state=1234)\n\nINPUT_DIM = df_train.shape[1]\nprint(\"Number of Featuers: \", INPUT_DIM)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">Lasso regression fits the purpose of the prediction because this is basically a linear regression with regularization. It will serve as a base model so we can compare the performance with the neural network we will also build later.</p>","metadata":{}},{"cell_type":"code","source":"lasso_lr = Lasso(alpha=0.1, fit_intercept=True, max_iter=100)\nlasso_lr.fit(df_train, Ytrain)\n\nlasso_pred = lasso_lr.predict(df_test)\nlasso_pred[lasso_pred < 0] = 0\n\nlasso_features = pd.DataFrame()\nlasso_features['feature'] = features\nlasso_features['lasso_coef'] = lasso_lr.coef_\nlasso_features = lasso_features.sort_values(by='lasso_coef', ascending=False)\n\nlasso_error = np.sqrt(mean_squared_error(lasso_pred, Ytest))\n\nprint(\"Lasso Regression Root Mean Squared Log Error: %8.2f\" % lasso_error)\nprint(\"Lasso R-squared: %8.2f\" % r2_score(Ytest, lasso_lr.predict(df_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lasso_features.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lasso_features.tail(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:16px\">Finally, we will build a neural network. One of neural networks' advantages over other machine learning models is that we don't need too much feature selection/engineering as neural networks are capable of doing the jobs during training. But we still need some model optimization as seen below. Because the problem is a simple regression, we don't need a lot of tuning.</p>","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(128, input_dim=INPUT_DIM, activation='elu', kernel_initializer='he_normal'))\nmodel.add(Dropout(3e-2))\nmodel.add(Dense(64, input_dim=INPUT_DIM, activation='relu', activity_regularizer=l1(1e-4)))\nmodel.add(Dropout(2e-2))\nmodel.add(Dense(32, input_dim=INPUT_DIM, activation='elu', activity_regularizer=l1(1e-5)))\nmodel.add(Dropout(1e-2))\nmodel.add(Dense(16, input_dim=INPUT_DIM, activation='relu', activity_regularizer=l1(1e-6)))\nmodel.add(Dense(1, activation='linear'))\n\nmodel.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=5e-4), metrics=RootMeanSquaredError())\nresult = model.fit(df_train, Ytrain, validation_data=(df_test, Ytest), epochs=50, batch_size=32, verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('./model.h5')\nmodel.summary()\n\nplt.plot(result.history['root_mean_squared_error'], label='train_error')\nplt.plot(result.history['val_root_mean_squared_error'], label='val_error')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred = model.predict(df_train)\nprint(\"Training Set R-squared: %8.3f\" % r2_score(Ytrain, train_pred))\n\ntest_pred = model.predict(df_test)\nprint(\"Test Set R-squared: %8.3f\" % r2_score(Ytest, test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_plot = pd.DataFrame(zip(Ytrain, train_pred.flatten()), columns=['True', 'Pred'])\n\nplt.figure(figsize=(7,7))\nsb.scatterplot(data=pred_plot.sample(n=1000, random_state=1234), x='True', y='Pred', size=0.1, legend=None)\nplt.gca().set_aspect('equal', adjustable='box')\nplt.plot(np.linspace(0, 5000, 2), np.linspace(0, 5000, 2), color='r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<hr>\n\n## Conclusion\n<p style=\"font-size:16px\">Durations predicted are generally similar to their true values (on or close to the red line) with the r-squared almost 0.9. The model performance mainly came from adding variables, instead of heavy feature engineering such as transformation or interaction term creation. Figuring out what could help predict trip durations, carefully adding them, and harnessing the neural network's ability to find interaction and non-linearity was the core of the modeling.</p>","metadata":{}}]}