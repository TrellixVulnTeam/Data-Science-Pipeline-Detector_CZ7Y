{"cells":[{"cell_type":"markdown","source":"**New York City Taxi Trip Duration**\n\nThis notebook goes through a simple process of extracting basic features, building a model and then make predictions.\n\nI am using the TPOT package to create a pipeline. TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. I thought it is useful to add this to the list of kernels available in this competition.","metadata":{"_uuid":"d9163d403d777f932311c57e27a9bdc2a771e90a","_execution_state":"idle","_cell_guid":"9631385a-8e31-4600-b207-bca33c81e0de"}},{"cell_type":"markdown","source":"**Objective:**\n\nThe challenge is to build a model that predicts the total ride duration of taxi trips in New York City. The primary data set is released by the NYC Taxi and Limousine Commission, which includes pickup time, geo-coordinates, number of passengers, and several other variables.\nThis challenge is a little different from most because kagglers are encouraged to publish additional training data that other participants can use for their predictions.\n\nFirst things first, lets look at the data.\n\nI borrowed some feature engineering codes from Nir Mabin's [\"Dark Taxi - TripDurationPrediction\"](https://www.kaggle.com/donniedarko/darktaxi-tripdurationprediction-lb-0-385)\nand Beluga's [\"From EDA to the Top\"](https://www.kaggle.com/gaborfodor/from-eda-to-the-top-lb-0-367).\n\n**If this helped you, some up-votes would be very much appreciated - it would keep me motivated.**","metadata":{"_uuid":"a1481fd13c682a054e31df2da430fd7fc20e655f","_execution_state":"idle","_cell_guid":"b896ae0a-1ad1-4472-95e7-ffbaf296f196"}},{"cell_type":"code","source":"# Import libraries\nimport os\nmingw_path = 'g:/mingw64/bin'\nos.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\nimport xgboost as xgb\n\nimport numpy as np\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n%matplotlib inline\nfrom haversine import haversine\nimport datetime as dt\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input/new-york-city-taxi-with-osrm/\"]).decode(\"utf8\"))","outputs":[],"execution_count":null,"metadata":{"_uuid":"bc3cb35d1c95b5f81407156c85d691559df4c2fb","_execution_state":"idle","_cell_guid":"c6ca08fe-ba84-4292-9c09-4d6720e10af2"}},{"cell_type":"markdown","source":"****Load the data****","metadata":{"_uuid":"62ab351c1741632952aa349cb52c91e305e3d190","_execution_state":"idle","_cell_guid":"cbea5ce4-b349-42db-8762-c123ed704a6a"}},{"cell_type":"code","source":"# Load training data as train\n#trainDF = pd.read_csv('../input/train.csv')\n#Use the line above instead of the one below if not running on Kaggle i.e. running locally.\ntrainDF = pd.read_csv('../input/new-york-city-taxi-with-osrm/train.csv', nrows=50000)\n\n# Load testing data as test\ntestDF = pd.read_csv('../input/new-york-city-taxi-with-osrm/test.csv')\n\n# Print size as well as the top 5 observation of training dataset\nprint('Size of the training set is: {} rows and {} columns'.format(*trainDF.shape))\nprint (\"\\n\", trainDF.head(5))","outputs":[],"execution_count":null,"metadata":{"_uuid":"cc8143e47759a830349c82045653235ad9e3f97b","_execution_state":"idle","_cell_guid":"dcd8b901-2123-4f5f-ab71-b9706df5f792"}},{"cell_type":"markdown","source":"We have only 11 columns but 1458644 rows which is a reasonable size. We also have two types of ids namely id and vendor_id. Lets try to understand this data.\n\nNOTE:\nThis statement reflects if you load the whole training data. In order to be able to run the whole script on Kaggle, I had to load only 50,000 rows of the training data to show the notebook in action.","metadata":{"_uuid":"a5830681606fa6057a6ac5ce8b8ab6fec101aa3e","_execution_state":"idle","_cell_guid":"b4f97a85-79ac-4dd2-ad17-853fa59f1aa8"}},{"cell_type":"markdown","source":"****Understanding Data****","metadata":{"_uuid":"78c5c414e2aa9aead7a639ee4793d064a643d7e8","_execution_state":"idle","_cell_guid":"19117eee-b89a-4a91-a71a-40922ecb318c"}},{"cell_type":"code","source":"# Look at the summary of numerical variables for train data set\ndf = trainDF.describe()\nprint (df)","outputs":[],"execution_count":null,"metadata":{"_uuid":"9099489cd1c546977df6a8ec00eb59faf9e78d48","scrolled":true,"_execution_state":"idle","_cell_guid":"126c91ff-1488-4852-af9f-b915cde786fa"}},{"cell_type":"code","source":"# Print the shape of the data\nprint(\"Train shape : \", trainDF.shape)\nprint(\"Test shape : \", testDF.shape)","outputs":[],"execution_count":null,"metadata":{"_uuid":"25fc29b2ad674f3d46741bb3de18e77cc8905813","_execution_state":"idle","_cell_guid":"f2e90c7b-0a11-40f1-b1f6-ab7c0708ede1"}},{"cell_type":"markdown","source":"**What are the data types ?**","metadata":{"_uuid":"3f14dd4ad48140155bc9db2dae667e3984f13bf8","_execution_state":"idle","_cell_guid":"7b7efd88-9742-4537-882b-9b6c4dc6db51"}},{"cell_type":"code","source":"dtypeDF = trainDF.dtypes.reset_index()\ndtypeDF.columns = [\"Count\", \"Column Type\"]\ndtypeDF.groupby(\"Column Type\").aggregate('count').reset_index()","outputs":[],"execution_count":null,"metadata":{"_uuid":"b853e6e929a3ae90e40118d10be8fb1a7e46cc9e","_execution_state":"idle","_cell_guid":"111ef821-d49a-4a88-9c64-0db1207f3c48"}},{"cell_type":"markdown","source":"There are 3 integers, 4 floats and four categorical variables.","metadata":{"_uuid":"1b81a39c4261b0b4183c58755f212ceae5081145","_execution_state":"idle","_cell_guid":"9a6952ab-11de-40a1-b3ee-2a34ba5b01b0"}},{"cell_type":"markdown","source":"****Understanding distribution of target variable i.e trip duration.****","metadata":{"_uuid":"3f42d244578085349383ed65e49848986eb47f81","_execution_state":"idle","_cell_guid":"27d127dc-f6d9-4f78-9415-f5cacbc8f423"}},{"cell_type":"code","source":"# Plot the trip duration\nplt.figure(figsize=(8,6))\nplt.scatter(range(trainDF.shape[0]), np.sort(trainDF.trip_duration.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('trip duration', fontsize=12)\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_uuid":"28810e6fb387ee629e4fa2211a90396cb2668ddd","_execution_state":"idle","_cell_guid":"bdbe5553-b1ef-4fc8-a3fd-c9209e358def"}},{"cell_type":"markdown","source":"We can see a few outliers with one way out there compared to the other 3 outliers. Lets remove those points so that we can zoom in.","metadata":{"_uuid":"cbcb72cd175bd1b7c2b7d8272d542955ba3266ae","_execution_state":"idle","_cell_guid":"7e720e22-60d6-4dc9-a75f-1ab4ac800e84"}},{"cell_type":"code","source":"# in train dataset some trip duration are very high (I consider them outliers and remove them before replotting it)\nth = trainDF.trip_duration.quantile(0.99)\ntempDF = trainDF\ntempDF = tempDF[tempDF['trip_duration'] < th]\nplt.figure(figsize=(8,6))\nplt.scatter(range(tempDF.shape[0]), np.sort(tempDF.trip_duration.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('trip duration', fontsize=12)\nplt.show()\n\ndel tempDF","outputs":[],"execution_count":null,"metadata":{"_uuid":"ba00d0d2b723f12f7b02d93b7ec3f7b280b98114","_execution_state":"idle","_cell_guid":"8a2c8f87-bff5-468c-85e1-c5f7bc065274"}},{"cell_type":"code","source":"# Lets remove the outliers from the train data target\ntrainDF = trainDF[trainDF['trip_duration'] < th]","outputs":[],"execution_count":null,"metadata":{"_uuid":"406447b01bc8f9879a36815974fdc8d494de7b0a","_execution_state":"idle","_cell_guid":"3e48b250-389c-4d4a-9f68-c23712eb37c1","collapsed":true}},{"cell_type":"markdown","source":"Dealing with missing values","metadata":{"_uuid":"3bde232817e116ad3e5a2723e65840a8a920bc47","_execution_state":"idle","_cell_guid":"e7a0a6e6-d0d9-4cc8-9f9b-a84a28cc605b"}},{"cell_type":"code","source":"# Number of variables with missing values\nvariables_missing_value = trainDF.isnull().sum()\nvariables_missing_value ","outputs":[],"execution_count":null,"metadata":{"_uuid":"8a30f78d5efbb2a3062d3f40474855ade2d5d2ae","_execution_state":"idle","_cell_guid":"e2e1463b-4fcc-446e-9e52-8cdeb922f891"}},{"cell_type":"markdown","source":"Nice, there are no missing values in the training data. What about the test data?","metadata":{"_uuid":"f9e1b25035a3b98b34a2e5abe6127aa0cb504d20","_execution_state":"idle","_cell_guid":"8b68d25b-05b5-4949-9c23-23110af1ae80"}},{"cell_type":"code","source":"variables_missing_value = testDF.isnull().sum()\nvariables_missing_value ","outputs":[],"execution_count":null,"metadata":{"_uuid":"94cf2e30978d5d279268f7166db39f6b33716108","_execution_state":"idle","_cell_guid":"b785004b-94b9-4aac-9f50-3a691e1ba8bd"}},{"cell_type":"markdown","source":"There are no missing values in the test data either. So lets get to the modelling bit and brew that tea for NYC.\n\nFirst we have to do a quick data pre-processing and feature selection.","metadata":{"_uuid":"9d7cc45a9f840931de9d4601106905c36bdb8321","_execution_state":"idle","_cell_guid":"e3fd1357-f702-4d72-9517-2d86b4475f9d"}},{"cell_type":"markdown","source":"****Feature Generation****\n\nHere we make a very simple data pre-processing and feature selection.","metadata":{"_uuid":"d287f589fb044e91701ecfbdc76df8825827a800","_execution_state":"idle","_cell_guid":"73fd5d9c-db98-4a7c-9860-777c052db459"}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\n\nt0 = dt.datetime.now()\n\ntrain = trainDF\ntest = testDF\ndel trainDF, testDF\n\ntrain['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\ntest['pickup_datetime'] = pd.to_datetime(test.pickup_datetime)\ntrain.loc[:, 'pickup_date'] = train['pickup_datetime'].dt.date\ntest.loc[:, 'pickup_date'] = test['pickup_datetime'].dt.date\ntrain['dropoff_datetime'] = pd.to_datetime(train.dropoff_datetime)\ntrain['store_and_fwd_flag'] = 1 * (train.store_and_fwd_flag.values == 'Y')\ntest['store_and_fwd_flag'] = 1 * (test.store_and_fwd_flag.values == 'Y')\ntrain['check_trip_duration'] = (train['dropoff_datetime'] - train['pickup_datetime']).map(lambda x: x.total_seconds())\nduration_difference = train[np.abs(train['check_trip_duration'].values  - train['trip_duration'].values) > 1]\nprint('Trip_duration and datetimes are ok.') if len(duration_difference[['pickup_datetime', 'dropoff_datetime', 'trip_duration', 'check_trip_duration']]) == 0 else print('Ooops.')\n\ntrain['trip_duration'].describe()\n\ntrain['log_trip_duration'] = np.log(train['trip_duration'].values + 1)\n\n# Feature Extraction\ncoords = np.vstack((train[['pickup_latitude', 'pickup_longitude']].values,\n                    train[['dropoff_latitude', 'dropoff_longitude']].values,\n                    test[['pickup_latitude', 'pickup_longitude']].values,\n                    test[['dropoff_latitude', 'dropoff_longitude']].values))\n\npca = PCA().fit(coords)\ntrain['pickup_pca0'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 0]\ntrain['pickup_pca1'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 1]\ntrain['dropoff_pca0'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\ntrain['dropoff_pca1'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\ntest['pickup_pca0'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 0]\ntest['pickup_pca1'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 1]\ntest['dropoff_pca0'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\ntest['dropoff_pca1'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\n\n# Distance\ndef haversine_array(lat1, lng1, lat2, lng2):\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    AVG_EARTH_RADIUS = 6371  # in km\n    lat = lat2 - lat1\n    lng = lng2 - lng1\n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n    return h\n\ndef dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n    a = haversine_array(lat1, lng1, lat1, lng2)\n    b = haversine_array(lat1, lng1, lat2, lng1)\n    return a + b\n\ndef bearing_array(lat1, lng1, lat2, lng2):\n    AVG_EARTH_RADIUS = 6371  # in km\n    lng_delta_rad = np.radians(lng2 - lng1)\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    y = np.sin(lng_delta_rad) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n    return np.degrees(np.arctan2(y, x))\n\ntrain.loc[:, 'distance_haversine'] = haversine_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\ntrain.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\ntrain.loc[:, 'direction'] = bearing_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\ntrain.loc[:, 'pca_manhattan'] = np.abs(train['dropoff_pca1'] - train['pickup_pca1']) + np.abs(train['dropoff_pca0'] - train['pickup_pca0'])\n\ntest.loc[:, 'distance_haversine'] = haversine_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\ntest.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\ntest.loc[:, 'direction'] = bearing_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\ntest.loc[:, 'pca_manhattan'] = np.abs(test['dropoff_pca1'] - test['pickup_pca1']) + np.abs(test['dropoff_pca0'] - test['pickup_pca0'])\n\ntrain.loc[:, 'center_latitude'] = (train['pickup_latitude'].values + train['dropoff_latitude'].values) / 2\ntrain.loc[:, 'center_longitude'] = (train['pickup_longitude'].values + train['dropoff_longitude'].values) / 2\ntest.loc[:, 'center_latitude'] = (test['pickup_latitude'].values + test['dropoff_latitude'].values) / 2\ntest.loc[:, 'center_longitude'] = (test['pickup_longitude'].values + test['dropoff_longitude'].values) / 2\n\n# Datetime features\ntrain.loc[:, 'pickup_weekday'] = train['pickup_datetime'].dt.weekday\ntrain.loc[:, 'pickup_hour_weekofyear'] = train['pickup_datetime'].dt.weekofyear\ntrain.loc[:, 'pickup_hour'] = train['pickup_datetime'].dt.hour\ntrain.loc[:, 'pickup_minute'] = train['pickup_datetime'].dt.minute\ntrain.loc[:, 'pickup_dt'] = (train['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\ntrain.loc[:, 'pickup_week_hour'] = train['pickup_weekday'] * 24 + train['pickup_hour']\n\ntest.loc[:, 'pickup_weekday'] = test['pickup_datetime'].dt.weekday\ntest.loc[:, 'pickup_hour_weekofyear'] = test['pickup_datetime'].dt.weekofyear\ntest.loc[:, 'pickup_hour'] = test['pickup_datetime'].dt.hour\ntest.loc[:, 'pickup_minute'] = test['pickup_datetime'].dt.minute\ntest.loc[:, 'pickup_dt'] = (test['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\ntest.loc[:, 'pickup_week_hour'] = test['pickup_weekday'] * 24 + test['pickup_hour']\n\ntrain.loc[:,'week_delta'] = train['pickup_datetime'].dt.weekday + \\\n    ((train['pickup_datetime'].dt.hour + (train['pickup_datetime'].dt.minute / 60.0)) / 24.0)\ntest.loc[:,'week_delta'] = test['pickup_datetime'].dt.weekday + \\\n    ((test['pickup_datetime'].dt.hour + (test['pickup_datetime'].dt.minute / 60.0)) / 24.0)\n\n# Make time features cyclic\ntrain.loc[:,'week_delta_sin'] = np.sin((train['week_delta'] / 7) * np.pi)**2\ntrain.loc[:,'hour_sin'] = np.sin((train['pickup_hour'] / 24) * np.pi)**2\n\ntest.loc[:,'week_delta_sin'] = np.sin((test['week_delta'] / 7) * np.pi)**2\ntest.loc[:,'hour_sin'] = np.sin((test['pickup_hour'] / 24) * np.pi)**2\n\n# Speed\ntrain.loc[:, 'avg_speed_h'] = 1000 * train['distance_haversine'] / train['trip_duration']\ntrain.loc[:, 'avg_speed_m'] = 1000 * train['distance_dummy_manhattan'] / train['trip_duration']\n\ntrain.loc[:, 'pickup_lat_bin'] = np.round(train['pickup_latitude'], 3)\ntrain.loc[:, 'pickup_long_bin'] = np.round(train['pickup_longitude'], 3)\n# Average speed for regions\ngby_cols = ['pickup_lat_bin', 'pickup_long_bin']\ncoord_speed = train.groupby(gby_cols).mean()[['avg_speed_h']].reset_index()\ncoord_count = train.groupby(gby_cols).count()[['id']].reset_index()\ncoord_stats = pd.merge(coord_speed, coord_count, on=gby_cols)\ncoord_stats = coord_stats[coord_stats['id'] > 100]\n\ntrain.loc[:, 'pickup_lat_bin'] = np.round(train['pickup_latitude'], 2)\ntrain.loc[:, 'pickup_long_bin'] = np.round(train['pickup_longitude'], 2)\ntrain.loc[:, 'center_lat_bin'] = np.round(train['center_latitude'], 2)\ntrain.loc[:, 'center_long_bin'] = np.round(train['center_longitude'], 2)\ntrain.loc[:, 'pickup_dt_bin'] = (train['pickup_dt'] // (3 * 3600))\ntest.loc[:, 'pickup_lat_bin'] = np.round(test['pickup_latitude'], 2)\ntest.loc[:, 'pickup_long_bin'] = np.round(test['pickup_longitude'], 2)\ntest.loc[:, 'center_lat_bin'] = np.round(test['center_latitude'], 2)\ntest.loc[:, 'center_long_bin'] = np.round(test['center_longitude'], 2)\ntest.loc[:, 'pickup_dt_bin'] = (test['pickup_dt'] // (3 * 3600))\n\n# Clustering\nsample_ind = np.random.permutation(len(coords))[:500000]\nkmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind])\n\ntrain.loc[:, 'pickup_cluster'] = kmeans.predict(train[['pickup_latitude', 'pickup_longitude']])\ntrain.loc[:, 'dropoff_cluster'] = kmeans.predict(train[['dropoff_latitude', 'dropoff_longitude']])\ntest.loc[:, 'pickup_cluster'] = kmeans.predict(test[['pickup_latitude', 'pickup_longitude']])\ntest.loc[:, 'dropoff_cluster'] = kmeans.predict(test[['dropoff_latitude', 'dropoff_longitude']])\nt1 = dt.datetime.now()\nprint('Time till clustering: %i seconds' % (t1 - t0).seconds)\n\n# Temporal and geospatial aggregation\nfor gby_col in ['pickup_hour', 'pickup_date', 'pickup_dt_bin',\n               'pickup_week_hour', 'pickup_cluster', 'dropoff_cluster']:\n    gby = train.groupby(gby_col).mean()[['avg_speed_h', 'avg_speed_m', 'log_trip_duration']]\n    gby.columns = ['%s_gby_%s' % (col, gby_col) for col in gby.columns]\n    train = pd.merge(train, gby, how='left', left_on=gby_col, right_index=True)\n    test = pd.merge(test, gby, how='left', left_on=gby_col, right_index=True)\n\nfor gby_cols in [['center_lat_bin', 'center_long_bin'],\n                 ['pickup_hour', 'center_lat_bin', 'center_long_bin'],\n                 ['pickup_hour', 'pickup_cluster'],  ['pickup_hour', 'dropoff_cluster'],\n                 ['pickup_cluster', 'dropoff_cluster']]:\n    coord_speed = train.groupby(gby_cols).mean()[['avg_speed_h']].reset_index()\n    coord_count = train.groupby(gby_cols).count()[['id']].reset_index()\n    coord_stats = pd.merge(coord_speed, coord_count, on=gby_cols)\n    coord_stats = coord_stats[coord_stats['id'] > 100]\n    coord_stats.columns = gby_cols + ['avg_speed_h_%s' % '_'.join(gby_cols), 'cnt_%s' %  '_'.join(gby_cols)]\n    train = pd.merge(train, coord_stats, how='left', on=gby_cols)\n    test = pd.merge(test, coord_stats, how='left', on=gby_cols)\n\ngroup_freq = '60min'\ndf_all = pd.concat((train, test))[['id', 'pickup_datetime', 'pickup_cluster', 'dropoff_cluster']]\ntrain.loc[:, 'pickup_datetime_group'] = train['pickup_datetime'].dt.round(group_freq)\ntest.loc[:, 'pickup_datetime_group'] = test['pickup_datetime'].dt.round(group_freq)\n\n# Count trips over 60min\ndf_counts = df_all.set_index('pickup_datetime')[['id']].sort_index()\ndf_counts['count_60min'] = df_counts.isnull().rolling(group_freq).count()['id']\ntrain = train.merge(df_counts, on='id', how='left')\ntest = test.merge(df_counts, on='id', how='left')\n\n# Count how many trips are going to each cluster over time\ndropoff_counts = df_all \\\n    .set_index('pickup_datetime') \\\n    .groupby([pd.TimeGrouper(group_freq), 'dropoff_cluster']) \\\n    .agg({'id': 'count'}) \\\n    .reset_index().set_index('pickup_datetime') \\\n    .groupby('dropoff_cluster').rolling('240min').mean() \\\n    .drop('dropoff_cluster', axis=1) \\\n    .reset_index().set_index('pickup_datetime').shift(freq='-120min').reset_index() \\\n    .rename(columns={'pickup_datetime': 'pickup_datetime_group', 'id': 'dropoff_cluster_count'})\n\ntrain['dropoff_cluster_count'] = train[['pickup_datetime_group', 'dropoff_cluster']].merge(dropoff_counts, on=['pickup_datetime_group', 'dropoff_cluster'], how='left')['dropoff_cluster_count'].fillna(0)\ntest['dropoff_cluster_count'] = test[['pickup_datetime_group', 'dropoff_cluster']].merge(dropoff_counts, on=['pickup_datetime_group', 'dropoff_cluster'], how='left')['dropoff_cluster_count'].fillna(0)\n\n# Count how many trips are going from each cluster over time\ndf_all = pd.concat((train, test))[['id', 'pickup_datetime', 'pickup_cluster', 'dropoff_cluster']]\npickup_counts = df_all \\\n    .set_index('pickup_datetime') \\\n    .groupby([pd.TimeGrouper(group_freq), 'pickup_cluster']) \\\n    .agg({'id': 'count'}) \\\n    .reset_index().set_index('pickup_datetime') \\\n    .groupby('pickup_cluster').rolling('240min').mean() \\\n    .drop('pickup_cluster', axis=1) \\\n    .reset_index().set_index('pickup_datetime').shift(freq='-120min').reset_index() \\\n    .rename(columns={'pickup_datetime': 'pickup_datetime_group', 'id': 'pickup_cluster_count'})\n\ntrain['pickup_cluster_count'] = train[['pickup_datetime_group', 'pickup_cluster']].merge(pickup_counts, on=['pickup_datetime_group', 'pickup_cluster'], how='left')['pickup_cluster_count'].fillna(0)\ntest['pickup_cluster_count'] = test[['pickup_datetime_group', 'pickup_cluster']].merge(pickup_counts, on=['pickup_datetime_group', 'pickup_cluster'], how='left')['pickup_cluster_count'].fillna(0)\n\n# For this particular problem we can add OSRM ([Open Source Routing Machine](http://project-osrm.org/ \n# \"OSRM\")) features. This data contains the fastest routes from specific starting points in NY.\nfr1 = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_train_part_1.csv', usecols=['id', 'total_distance', 'total_travel_time',  'number_of_steps'])\nfr2 = pd.read_csv('../input/new-york-city-taxi-with-osrm//fastest_routes_train_part_2.csv', usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\ntest_street_info = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_test.csv',\n                               usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\ntrain_street_info = pd.concat((fr1, fr2))\ntrain = train.merge(train_street_info, how='left', on='id')\ntest = test.merge(test_street_info, how='left', on='id')\n\n\n","outputs":[],"execution_count":null,"metadata":{"_uuid":"34db7cb4bc2f7175a634e065edb328f781bac53d","_cell_guid":"db13e472-90de-41b4-8a67-588bca58d737"}},{"cell_type":"code","source":"feature_names = list(train.columns)\nprint(np.setdiff1d(train.columns, test.columns))\ndo_not_use_for_training = ['id', 'log_trip_duration', 'pickup_datetime', 'dropoff_datetime', 'trip_duration', 'check_trip_duration',\n                           'pickup_date', 'avg_speed_h', 'avg_speed_m', 'pickup_lat_bin', 'pickup_long_bin',\n                           'center_lat_bin', 'center_long_bin', 'pickup_dt_bin', 'pickup_datetime_group']\nfeature_names = [f for f in train.columns if f not in do_not_use_for_training]\n# print(feature_names)\nprint('We have %i features.' % len(feature_names))\ntrain[feature_names].count()\n#ytrain = np.log(train['trip_duration'].values + 1)\n#ytrain = train[train['trip_duration'].notnull()]['trip_duration_log'].values\n\n\nt1 = dt.datetime.now()\nprint('Feature extraction time: %i seconds' % (t1 - t0).seconds)","outputs":[],"execution_count":null,"metadata":{"_uuid":"989ec5b4871eadac9f097bf8d470358c9b902b8a","_cell_guid":"e577d3a7-2813-49b1-bd16-25f7c496f8e8"}},{"cell_type":"markdown","source":"****Feature check before modeling****","metadata":{"_uuid":"8f03a44ff9f5a35ccc6b7d4886ad56017101fde2","_cell_guid":"aecd2d05-b748-42f2-9383-f1b602f027ce"}},{"cell_type":"code","source":"feature_stats = pd.DataFrame({'feature': feature_names})\nfeature_stats.loc[:, 'train_mean'] = np.nanmean(train[feature_names].values, axis=0).round(4)\nfeature_stats.loc[:, 'test_mean'] = np.nanmean(test[feature_names].values, axis=0).round(4)\nfeature_stats.loc[:, 'train_std'] = np.nanstd(train[feature_names].values, axis=0).round(4)\nfeature_stats.loc[:, 'test_std'] = np.nanstd(test[feature_names].values, axis=0).round(4)\nfeature_stats.loc[:, 'train_nan'] = np.mean(np.isnan(train[feature_names].values), axis=0).round(3)\nfeature_stats.loc[:, 'test_nan'] = np.mean(np.isnan(test[feature_names].values), axis=0).round(3)\nfeature_stats.loc[:, 'train_test_mean_diff'] = np.abs(feature_stats['train_mean'] - feature_stats['test_mean']) / np.abs(feature_stats['train_std'] + feature_stats['test_std'])  * 2\nfeature_stats.loc[:, 'train_test_nan_diff'] = np.abs(feature_stats['train_nan'] - feature_stats['test_nan'])\nfeature_stats = feature_stats.sort_values(by='train_test_mean_diff')\nfeature_stats[['feature', 'train_test_mean_diff']].tail()","outputs":[],"execution_count":null,"metadata":{"_uuid":"579ca306990e7173c892d40b18a2f32265954905","_cell_guid":"acd51790-de3c-496f-94ed-1c2e09227f5a"}},{"cell_type":"markdown","source":"****Train a simple classifier****\n\nWe use the TPOT package to handle the cross validation and hyperparameters for us","metadata":{"_uuid":"284a903e1c8fdd77376dcb0ad40c80dda206375f","_execution_state":"idle","_cell_guid":"88e7cecd-c924-49ce-9148-1c60caf7b158"}},{"cell_type":"code","source":"from tpot import TPOTRegressor\nauto_classifier = TPOTRegressor(generations=3, population_size=9, verbosity=2)\nfrom sklearn.model_selection import train_test_split","outputs":[],"execution_count":null,"metadata":{"_uuid":"a78cf0849fb37f4f853863b9e2f9d44fdaf1f0e1","_execution_state":"idle","_cell_guid":"fb6ae6a0-249a-42e3-9bd6-282505f9bf41","collapsed":true}},{"cell_type":"markdown","source":"**Split the training data to train and validate**","metadata":{"_uuid":"3aca2423d4e71da6603a3d9caa581df0abbae451","_execution_state":"idle","_cell_guid":"aa28099d-91cb-4f33-b802-193b83f94a8b"}},{"cell_type":"code","source":"# K Fold Cross Validation\nfrom sklearn.model_selection import KFold\n\nX = train[feature_names].values\ny = np.log(train['trip_duration'].values + 1)  \n\n\nkf = KFold(n_splits=10)\nkf.get_n_splits(X)\n\nprint(kf)  \n\nKFold(n_splits=10, random_state=None, shuffle=False)\nfor train_index, test_index in kf.split(X):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_valid = X[train_index], X[test_index]\n    y_train, y_valid = y[train_index], y[test_index]","outputs":[],"execution_count":null,"metadata":{"_uuid":"4aa44f013328df33350cebc54c27169ef0e372a7","_execution_state":"idle","_cell_guid":"b7603ffe-f702-456a-821f-4f1d11f19533"}},{"cell_type":"code","source":"auto_classifier.fit(X_train, y_train)","outputs":[],"execution_count":null,"metadata":{"_uuid":"7512c52267292580a375a03f5bd973318c77c6cc","_execution_state":"idle","_cell_guid":"3daca28b-7ac0-4f0b-a235-f1929698c9a4"}},{"cell_type":"code","source":"#print(\"The cross-validation MSE\")\n#print(auto_classifier.score(X_valid, y_valid))","outputs":[],"execution_count":null,"metadata":{"_uuid":"d110174069ff841b8d69602022051c0c4ba09149","_execution_state":"idle","_cell_guid":"a760e6e4-ba80-4420-b046-3620c63d7680","collapsed":true}},{"cell_type":"code","source":"# Now do the prediction\ntest_result = auto_classifier.predict(test[feature_names].values)\nsub = pd.DataFrame()\nsub['id'] = test['id']\nsub['trip_duration'] = np.exp(test_result)\nsub.to_csv('NYCTaxi_TpotModels.csv', index=False)\nsub.head()","outputs":[],"execution_count":null,"metadata":{"_uuid":"eb674e18fcd03006bbe7bb7f70cba688e6ca349c","_execution_state":"idle","_cell_guid":"205919a7-80bb-477f-93c8-95f4f75ed399"}},{"cell_type":"code","source":"# Export the model\nauto_classifier.export('NYCTaxi_pipeline.py')","outputs":[],"execution_count":null,"metadata":{"_uuid":"30c05fa69862c8167d67220d5eaacc401d982db3","_execution_state":"idle","_cell_guid":"573a2b82-cced-4424-9f10-7a8e2276cedf","collapsed":true}},{"cell_type":"markdown","source":"That is it for now. You can run locally with more number of generations, population, etc. to get a better result. Because of Kaggle time limitations I could not choose parameters that take longer to run.\n\nI hope you like it. If so please up-vote. \n\n**Stay tuned !!!**","metadata":{"_uuid":"433f20ca962f987564cf825e31a3bfead0967fea","_execution_state":"idle","_cell_guid":"ff96a60b-c2ef-4731-a9eb-da327e63e697"}}],"nbformat":4,"nbformat_minor":1,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"name":"python","file_extension":".py","version":"3.6.1","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","pygments_lexer":"ipython3","nbconvert_exporter":"python"}}}