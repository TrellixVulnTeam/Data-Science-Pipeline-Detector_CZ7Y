{"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"version":"3.6.1","file_extension":".py","pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"}}},"cells":[{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import timedelta\nimport datetime as dt\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 10]\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\n\n# Any results you write to the current directory are saved as output.","cell_type":"code","metadata":{"_uuid":"bafb9d0c9e18b2b5ce4417b7ef89f33842abe83d","collapsed":true,"_cell_guid":"37b93f00-a837-4c5b-aadc-9a80e06a6f07"},"execution_count":null,"outputs":[]},{"source":"t0 = dt.datetime.now()\ntrain = pd.read_csv('../input/nyc-taxi-trip-duration/train.csv')\ntest = pd.read_csv('../input/nyc-taxi-trip-duration/test.csv')\nsample_submission = pd.read_csv('../input/nyc-taxi-trip-duration/sample_submission.csv')\ntest_1 = test.copy()","cell_type":"code","metadata":{"_uuid":"575bba9c46eded693d0b3acb037fd60ef1ba613e","_cell_guid":"f5cadcc6-50a2-49f1-b47c-571fc6cf8c95"},"execution_count":null,"outputs":[]},{"source":"## Feature Engineering","cell_type":"markdown","metadata":{"_uuid":"46fb327efb75aaeae757e1c1362bb327693f3761","_cell_guid":"92f5ad77-4c8e-4de0-b691-0ad8deb8e1c4"}},{"source":"A lot of the features have been extracted using existing models, especially the model of \"beluga\" (Cheers, mate). I also tried using the weather information as a variable but it seems that they do not serve so much of a useful purpose as far the accuracy of the result is concerned. Perhaps, I would use some sort of ensemble learning later to calculate feature importance of variables","cell_type":"markdown","metadata":{"_uuid":"532f24122cc389cae9e6b4f2673ba122efeaf080","_cell_guid":"f79349b8-1ca7-4b0c-ae4d-0ae55610b2d6"}},{"source":"### Conversion of DATETIME Features","cell_type":"markdown","metadata":{"_uuid":"7e00786095d45b6d28c3170da8e20cd98bc7a6b0","_cell_guid":"996b65f9-da95-4817-aac7-4eb537bdd8fb"}},{"source":"train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\ntest['pickup_datetime'] = pd.to_datetime(test.pickup_datetime)\ntrain.loc[:, 'pickup_date'] = train['pickup_datetime'].dt.date\ntest.loc[:, 'pickup_date'] = test['pickup_datetime'].dt.date\ntrain['dropoff_datetime'] = pd.to_datetime(train.dropoff_datetime)\n","cell_type":"code","metadata":{"_uuid":"a82bdb6ca802737fca56005c531ab764535f9f70","collapsed":true,"_cell_guid":"8fb8357c-503b-4d44-8e5a-217144994b78"},"execution_count":null,"outputs":[]},{"source":"### DateTime Features","cell_type":"markdown","metadata":{"_uuid":"ccc62afdfdd6853b25af9ae19f085cb8253d78b8","collapsed":true,"_cell_guid":"4b587dca-d665-4746-95d4-8d7332b5693e"}},{"source":"train.loc[:, 'pickup_weekday'] = train['pickup_datetime'].dt.weekday\ntrain.loc[:, 'pickup_hour_weekofyear'] = train['pickup_datetime'].dt.weekofyear\ntrain.loc[:, 'pickup_hour'] = train['pickup_datetime'].dt.hour\ntrain.loc[:, 'pickup_minute'] = train['pickup_datetime'].dt.minute\ntrain.loc[:, 'pickup_dt'] = (train['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\ntrain.loc[:, 'pickup_week_hour'] = train['pickup_weekday'] * 24 + train['pickup_hour']\n\ntest.loc[:, 'pickup_weekday'] = test['pickup_datetime'].dt.weekday\ntest.loc[:, 'pickup_hour_weekofyear'] = test['pickup_datetime'].dt.weekofyear\ntest.loc[:, 'pickup_hour'] = test['pickup_datetime'].dt.hour\ntest.loc[:, 'pickup_minute'] = test['pickup_datetime'].dt.minute\ntest.loc[:, 'pickup_dt'] = (test['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\ntest.loc[:, 'pickup_week_hour'] = test['pickup_weekday'] * 24 + test['pickup_hour']\n\ntrain.loc[:, 'pickup_dayofyear'] = train['pickup_datetime'].dt.dayofyear\ntest.loc[:,'pickup_dayofyear'] = test['pickup_datetime'].dt.dayofyear","cell_type":"code","metadata":{"_uuid":"5d59fd4ef4ad0bcfd26a7a1d140082b884e4d6b8","collapsed":true,"_cell_guid":"b418a7c2-21ff-4fa5-a2e3-d09cb7a48a6a"},"execution_count":null,"outputs":[]},{"source":"### Bearing Feature","cell_type":"markdown","metadata":{"_uuid":"ff4f1d7848f4eaee0ad6891260779e01e14c0aea","collapsed":true,"_cell_guid":"dd811ba5-3c8e-490e-a7db-32a4a4fe4661"}},{"source":"def bearing_array(lat1, lng1, lat2, lng2):\n    AVG_EARTH_RADIUS = 6371  # in km\n    lng_delta_rad = np.radians(lng2 - lng1)\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    y = np.sin(lng_delta_rad) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n    return np.degrees(np.arctan2(y, x))\n\ntrain.loc[:, 'direction'] = bearing_array(train['pickup_latitude'].values, train['pickup_longitude'].values, \n                                          train['dropoff_latitude'].values, train['dropoff_longitude'].values)\n\ntest.loc[:, 'direction'] = bearing_array(test['pickup_latitude'].values, test['pickup_longitude'].values, \n                                         test['dropoff_latitude'].values, test['dropoff_longitude'].values)\n","cell_type":"code","metadata":{"_uuid":"e09b4951c029f2a8296cc95d8a3b6bd5b7d3c713","collapsed":true,"_cell_guid":"cbb02090-2ca6-4730-956b-755147d0a98e"},"execution_count":null,"outputs":[]},{"source":"### Distance Calculation","cell_type":"markdown","metadata":{"_uuid":"80d3e8da82a961b24d1eb46ce6dc0490d176099a","collapsed":true,"_cell_guid":"3dc2ed1c-8f70-4cb8-ad63-451dfd91d30d"}},{"source":"def haversine_array(lat1, lng1, lat2, lng2):\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    AVG_EARTH_RADIUS = 6371  # in km\n    lat = lat2 - lat1\n    lng = lng2 - lng1\n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n    return h\n\ndef dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n    a = haversine_array(lat1, lng1, lat1, lng2)\n    b = haversine_array(lat1, lng1, lat2, lng1)\n    return a + b\n\ntrain.loc[:, 'distance_haversine'] = haversine_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\ntrain.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\n\n\ntest.loc[:, 'distance_haversine'] = haversine_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\ntest.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\n\n\n\ntrain.loc[:, 'center_latitude'] = (train['pickup_latitude'].values + train['dropoff_latitude'].values) / 2\ntrain.loc[:, 'center_longitude'] = (train['pickup_longitude'].values + train['dropoff_longitude'].values) / 2\ntest.loc[:, 'center_latitude'] = (test['pickup_latitude'].values + test['dropoff_latitude'].values) / 2\ntest.loc[:, 'center_longitude'] = (test['pickup_longitude'].values + test['dropoff_longitude'].values) / 2","cell_type":"code","metadata":{"_uuid":"989a82f1ed353db24e11625e4060df3e8d5b621a","_cell_guid":"97dcc6a5-b5f7-4f52-aec1-811e10463dbb"},"execution_count":null,"outputs":[]},{"source":"### PCA Features","cell_type":"markdown","metadata":{"_uuid":"fd84ca83effba816da345af081e6d5e064d7ace5","collapsed":true,"_cell_guid":"467037da-be53-422c-bacc-ae05b727edf8"}},{"source":"coords = np.vstack((train[['pickup_latitude', 'pickup_longitude']].values,\n                    train[['dropoff_latitude', 'dropoff_longitude']].values,\n                    test[['pickup_latitude', 'pickup_longitude']].values,\n                    test[['dropoff_latitude', 'dropoff_longitude']].values))\n\npca = PCA().fit(coords)\n","cell_type":"code","metadata":{"_uuid":"c20ff891223e78b7c33550ae3fdf2352912d434e","collapsed":true,"_cell_guid":"66bc5e5d-6ddf-4aa6-8cb5-296f53e633ef"},"execution_count":null,"outputs":[]},{"source":"train['pickup_pca0'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 0]\ntrain['pickup_pca1'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 1]\ntrain['dropoff_pca0'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\ntrain['dropoff_pca1'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\ntest['pickup_pca0'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 0]\ntest['pickup_pca1'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 1]\ntest['dropoff_pca0'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\ntest['dropoff_pca1'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\n\ntrain.loc[:, 'pca_manhattan'] = np.abs(train['dropoff_pca1'] - train['pickup_pca1']) + np.abs(train['dropoff_pca0'] - train['pickup_pca0'])\ntest.loc[:, 'pca_manhattan'] = np.abs(test['dropoff_pca1'] - test['pickup_pca1']) + np.abs(test['dropoff_pca0'] - test['pickup_pca0'])","cell_type":"code","metadata":{"_uuid":"6318cede3b9a64223dcc793694944101529a58fa","collapsed":true,"_cell_guid":"68dfc8ed-fd94-4f22-bdd1-707a210bae78"},"execution_count":null,"outputs":[]},{"source":"### Clustering Features","cell_type":"markdown","metadata":{"_uuid":"06cc4f5439fd0a98dfab3efa64d8b83c2ca40abf","_cell_guid":"17a2ba72-1afd-4537-aa97-1fcd050665f3"}},{"source":"sample_ind = np.random.permutation(len(coords))[:500000]\nkmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind])","cell_type":"code","metadata":{"_uuid":"f1a232f9732a63f7fbf4ccc654246c246d6c1714","collapsed":true,"_cell_guid":"69940065-a771-4139-8461-ce2b833541d9"},"execution_count":null,"outputs":[]},{"source":"train.loc[:, 'pickup_cluster'] = kmeans.predict(train[['pickup_latitude', 'pickup_longitude']])\ntrain.loc[:, 'dropoff_cluster'] = kmeans.predict(train[['dropoff_latitude', 'dropoff_longitude']])\ntest.loc[:, 'pickup_cluster'] = kmeans.predict(test[['pickup_latitude', 'pickup_longitude']])\ntest.loc[:, 'dropoff_cluster'] = kmeans.predict(test[['dropoff_latitude', 'dropoff_longitude']])\nt1 = dt.datetime.now()","cell_type":"code","metadata":{"_uuid":"d1c0f8ce7ba39ac0e3ddf13080e5852a0b5aa23a","collapsed":true,"_cell_guid":"098c70be-6a35-4713-b069-0ed56e898b20"},"execution_count":null,"outputs":[]},{"source":"## OSRM Data","cell_type":"markdown","metadata":{"_uuid":"c1526cbb487fe825ababc2d1385926ad4b177b7d","_cell_guid":"b016f6d2-452d-4b31-858d-2c7bb123465c"}},{"source":"fr1 = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_train_part_1.csv', usecols=['id', 'total_distance', 'total_travel_time',  'number_of_steps', ])\nfr2 = pd.read_csv('../input/new-york-city-taxi-with-osrm//fastest_routes_train_part_2.csv', usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\ntest_street_info = pd.read_csv('../input/new-york-city-taxi-with-osrm//fastest_routes_test.csv',\n                               usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])","cell_type":"code","metadata":{"_uuid":"2360f6c7276a1d340824da8ff911399fafa4d0a1","collapsed":true,"_cell_guid":"7ef23e1a-54f2-422f-b9ff-9fc27c3c55a6"},"execution_count":null,"outputs":[]},{"source":"train_street_info = pd.concat((fr1, fr2))\ntrain = train.merge(train_street_info, how='left', on='id')\ntest = test.merge(test_street_info, how='left', on='id')","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"source":"### Features Checking","cell_type":"markdown","metadata":{}},{"source":"train['log_trip_duration'] = np.log(train['trip_duration'].values + 1)","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"source":"feature_names = list(train.columns)\nprint(np.setdiff1d(train.columns, test.columns))\n","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"do_not_use_for_training = ['id', 'log_trip_duration', 'trip_duration', 'dropoff_datetime', 'pickup_date', \n                           'pickup_datetime', 'date']\nfeature_names = [f for f in train.columns if f not in do_not_use_for_training]\n# print(feature_names)\nprint('We have %i features.' % len(feature_names))\ntrain[feature_names].count()\n         ","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"### Features Encoding ","cell_type":"markdown","metadata":{}},{"source":"train['store_and_fwd_flag'] = train['store_and_fwd_flag'].map(lambda x: 0 if x == 'N' else 1)","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"source":"test['store_and_fwd_flag'] = test['store_and_fwd_flag'].map(lambda x: 0 if x == 'N' else 1)","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"source":"### K Fold Cross Validation","cell_type":"markdown","metadata":{}},{"source":"from sklearn.model_selection import KFold\n\nX = train[feature_names].values\ny = np.log(train['trip_duration'].values + 1)  \n\n\nkf = KFold(n_splits=10)\nkf.get_n_splits(X)\n\nprint(kf)  \n\nKFold(n_splits=10, random_state=None, shuffle=False)\nfor train_index, test_index in kf.split(X):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    \n ","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"### XgBoost Implementation","cell_type":"markdown","metadata":{}},{"source":"   \ndtrain = xgb.DMatrix(X_train, label=y_train)\ndvalid = xgb.DMatrix(X_test, label=y_test)\ndtest = xgb.DMatrix(test[feature_names].values)\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n# Try different parameters! My favorite is random search :)\nxgb_pars = {'min_child_weight': 10, 'eta': 0.04, 'colsample_bytree': 0.8, 'max_depth': 15,\n            'subsample': 0.75, 'lambda': 2, 'nthread': -1, 'booster' : 'gbtree', 'silent': 1, 'gamma' : 0,\n            'eval_metric': 'rmse', 'objective': 'reg:linear'}    ","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"source":"model = xgb.train(xgb_pars, dtrain, 500, watchlist, early_stopping_rounds=250,\n                  maximize=False, verbose_eval=15)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"#### Continued :)","cell_type":"markdown","metadata":{}},{"source":"","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]}],"nbformat":4}