{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# New York taxi trips duration\n\nThe objective of this kernel is to better understand and explain some useful techniques of exploratory data analysis(EDA) and try to achieve a good performance in training,testing, and submiting a outcome to kaggle competition. for the sake of doing that, we are going to use the New York City Trip Duration competition ,you guys can find the complete briefing of it by clicking <a href=\"https://www.kaggle.com/competitions/nyc-taxi-trip-duration/overview\">Here</a> .","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport xgboost\nfrom math import radians, cos, sin, asin, sqrt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-02T11:55:51.287672Z","iopub.execute_input":"2022-05-02T11:55:51.288042Z","iopub.status.idle":"2022-05-02T11:55:51.913501Z","shell.execute_reply.started":"2022-05-02T11:55:51.287977Z","shell.execute_reply":"2022-05-02T11:55:51.912604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/nyc-taxi-trip-duration/train.zip')\ntest = pd.read_csv('../input/nyc-taxi-trip-duration/test.zip')\n\nfast1_train = pd.read_csv('../input/nyfastestroutes/fastest_routes_train_part_1.csv')\nfast2_train = pd.read_csv('../input/nyfastestroutes/fastest_routes_train_part_2.csv')\nfast_test = pd.read_csv('../input/nyfastestroutes/fastest_routes_test.csv')\nweather = pd.read_csv('../input/weather-data-in-new-york-city-2016/weather_data_nyc_centralpark_2016(1).csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:55:51.918203Z","iopub.execute_input":"2022-05-02T11:55:51.918883Z","iopub.status.idle":"2022-05-02T11:56:21.601493Z","shell.execute_reply.started":"2022-05-02T11:55:51.918834Z","shell.execute_reply":"2022-05-02T11:56:21.600414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importing the required datasets with pandas.read_csv()","metadata":{}},{"cell_type":"code","source":"train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\ntrain['dropoff_datetime'] = pd.to_datetime(train.dropoff_datetime)\n\ntest['pickup_datetime'] = pd.to_datetime(test.pickup_datetime)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:56:21.603745Z","iopub.execute_input":"2022-05-02T11:56:21.604111Z","iopub.status.idle":"2022-05-02T11:56:23.172361Z","shell.execute_reply.started":"2022-05-02T11:56:21.604064Z","shell.execute_reply":"2022-05-02T11:56:23.171348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Parsing datetime","metadata":{}},{"cell_type":"code","source":"train['date'] = [i.date() for i in train.pickup_datetime]\ntrain['date'] = pd.to_datetime(train.date)\n\ntrain['year'] = [date.year for date in train.date]\ntrain['month'] = [date.month for date in train.date]\ntrain['day'] = [date.day for date in train.date]\ntrain['dow'] = [date.dayofweek for date in train.date]\ntrain['hour'] = [date.hour for date in train.pickup_datetime]\n\n\ntest['date'] = [i.date() for i in test.pickup_datetime]\ntest['date'] = pd.to_datetime(test.date)\n\ntest['year'] = [date.year for date in test.date]\ntest['month'] = [date.month for date in test.date]\ntest['day'] = [date.day for date in test.date]\ntest['dow'] = [date.dayofweek for date in test.date]\ntest['hour'] = [date.hour for date in test.pickup_datetime]","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:56:23.175724Z","iopub.execute_input":"2022-05-02T11:56:23.176266Z","iopub.status.idle":"2022-05-02T11:57:09.698689Z","shell.execute_reply.started":"2022-05-02T11:56:23.176213Z","shell.execute_reply":"2022-05-02T11:57:09.69784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Parsing the weather dataset and merging it with the train and test dataframes","metadata":{}},{"cell_type":"code","source":"weather['date'] = pd.to_datetime(weather.date)\n\nfor column in weather.dtypes[weather.dtypes == object].index:\n    weather.loc[:,column].replace('T',0.05,inplace = True)\n    weather.loc[:,column] = weather.loc[:,column].astype(np.float32)\n\nweather_features = ['average temperature','precipitation','snow fall','snow depth','date']\n\ntrain = pd.merge(train,weather.loc[:,weather_features],on = 'date')\ntest = pd.merge(test,weather.loc[:,weather_features],on = 'date')","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:57:09.700106Z","iopub.execute_input":"2022-05-02T11:57:09.701174Z","iopub.status.idle":"2022-05-02T11:57:12.189989Z","shell.execute_reply.started":"2022-05-02T11:57:09.701113Z","shell.execute_reply":"2022-05-02T11:57:12.188924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Parsing the fastest routes dataset and merging it with our dataframes as well","metadata":{}},{"cell_type":"code","source":"drop_cols = ['travel_time_per_step','distance_per_step','step_maneuvers','step_direction','step_location_list','starting_street','end_street','street_for_each_step','number_of_steps']\nfastest = pd.concat([fast1_train,fast2_train],ignore_index = True)\n\n# Train\nfastest.drop(axis=1 , columns = drop_cols,inplace = True)\ntrain = pd.merge(train,fastest,on = 'id')\n\n# Test\nfast_test.drop(axis=1 , columns = drop_cols,inplace = True)\ntest = pd.merge(test,fast_test,on = 'id')\n\ntrain.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:57:12.191414Z","iopub.execute_input":"2022-05-02T11:57:12.191858Z","iopub.status.idle":"2022-05-02T11:57:17.440035Z","shell.execute_reply.started":"2022-05-02T11:57:12.191816Z","shell.execute_reply":"2022-05-02T11:57:17.439313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We have parsed our data, so lets start some EDA .**\n\nFirst we are gonna group our dataset weekly","metadata":{}},{"cell_type":"code","source":"group1 = train.groupby('date')['precipitation','snow depth','total_distance','total_travel_time','dow','passenger_count'].mean()\ngroup1.reset_index(inplace = True)\ngroup2 = train.groupby('date').size()\ngroup2 = group2.reset_index()\ngroup2.columns = ['date','trips']\n\ngrouped = pd.merge(group1,group2,on = 'date') ## Week\ngrouped.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:57:17.441269Z","iopub.execute_input":"2022-05-02T11:57:17.441642Z","iopub.status.idle":"2022-05-02T11:57:17.871904Z","shell.execute_reply.started":"2022-05-02T11:57:17.441602Z","shell.execute_reply":"2022-05-02T11:57:17.871039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Ok, we have it, now we can add somemore fancy look to our EDA by grouping some interest variables by Hour</b>","metadata":{}},{"cell_type":"code","source":"group_hour1 = train.groupby('hour')['precipitation','snow depth','total_distance','total_travel_time','passenger_count'].mean()\ngroup_hour1.reset_index(inplace = True)\ngroup_hour2 = train.groupby('hour').size()\ngroup_hour2 = group_hour2.reset_index()\ngroup_hour2.columns = ['hour','trips']\n\ngrouped_hour = pd.merge(group_hour1,group_hour2,on = 'hour') ## Hour\ngrouped_hour.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:57:17.874274Z","iopub.execute_input":"2022-05-02T11:57:17.87456Z","iopub.status.idle":"2022-05-02T11:57:17.993004Z","shell.execute_reply.started":"2022-05-02T11:57:17.874516Z","shell.execute_reply":"2022-05-02T11:57:17.992112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Hour and Day of Week\ndowh1 = train.groupby(['dow','hour'])['precipitation','snow depth','total_distance','total_travel_time','passenger_count'].mean()\ndowh2 = train.groupby(['dow','hour']).size()\ndowh2.name = 'Trips'\n\ndowh = pd.concat([dowh1,dowh2],axis= 1)\n\ntrips_hm = dowh2.unstack(level = 1)\n\nam = trips_hm.iloc[:,:12]\npm = trips_hm.iloc[:,12:]","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:57:17.994483Z","iopub.execute_input":"2022-05-02T11:57:17.995161Z","iopub.status.idle":"2022-05-02T11:57:18.205981Z","shell.execute_reply.started":"2022-05-02T11:57:17.99511Z","shell.execute_reply":"2022-05-02T11:57:18.205277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of Travels relantionships\nFirst thing first, what is more important in a Taxi travels dataset more then the number of trips ? So we can start by plotting the mean of the number of trips occorrued in eachday of week,and week hour of the day to see how it looks like.","metadata":{}},{"cell_type":"code","source":"sns.set_context('poster')\n\nfig,axes  = plt.subplots(2,figsize = (14,12))\n\n\nsns.heatmap(am,square = True,cbar = False,linewidths = 1,cmap = 'Greys',ax = axes[0],annot=False,annot_kws = {'fontsize':15})\naxes[0].set_title('AM')\naxes[0].set_yticks([0.4,1.4,2.4,3.4,4.4,5.4,6.4])\naxes[0].set_xticks([0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.5,11.5])\naxes[0].set_xticklabels(['00:00','1:00','2:00','3:00','4:00','5:00','6:00','07:00','8:00','9:00','10:00','11:00'])\naxes[0].set_yticklabels(['Mon','Tue','Wed','Tur','Fri','Sat','Sun'])\naxes[0].set_ylabel('')\naxes[0].set_xlabel('')\naxes[0].tick_params(labelsize=15)\n\nsns.heatmap(pm,square = True,cbar = False,linewidths = 1,cmap = 'Greys',ax = axes[1],annot = False,annot_kws = {'fontsize':15})\naxes[1].set_title('PM')\naxes[1].set_yticks([0.4,1.4,2.4,3.4,4.4,5.4,6.4])\naxes[1].set_yticklabels(['Mon','Tue','Wed','Tur','Fri','Sat','Sun'])\naxes[1].set_ylabel('')\naxes[1].set_xticks([0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.5,11.5])\naxes[1].set_xticklabels(['12:00','13:00','14:00','15:00','16:00','17:00','18:00','19:00','20:00','21:00','22:00','23:00'])\naxes[1].set_xlabel('')\naxes[1].tick_params(labelsize=15)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:21:24.611311Z","iopub.execute_input":"2022-05-02T12:21:24.611729Z","iopub.status.idle":"2022-05-02T12:21:25.285584Z","shell.execute_reply.started":"2022-05-02T12:21:24.611686Z","shell.execute_reply":"2022-05-02T12:21:25.284498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice, now we can see that when people are going to work (between 7:00 and 9:00) the number of taxi trips ramped up, as well as in hours when people are going back to their home after work (between 18:00 and 20:00) . Also is interesting to see how taxi trips are demanded in the weekends after 23:00, we have no doubt that people like to hang out and meet their friends very often in the weekend's nights .\n\n## Handling Outliers\n\nUnfortunatelly our dataset dont have a real confident variable to represent real state of traffic jam, but we can use some feature engineering to create something that can serve as a proxy,that is, something that can represent traffic jam, in order to do that we can use the average speed, in wich we can estimate by dividing the average distance by the average trip duration , so lets do it .","metadata":{}},{"cell_type":"code","source":"train['trafficjam'] = (train.total_distance/train.trip_duration*-1)+3453","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:57:45.347667Z","iopub.execute_input":"2022-05-02T11:57:45.348256Z","iopub.status.idle":"2022-05-02T11:57:45.366661Z","shell.execute_reply.started":"2022-05-02T11:57:45.348201Z","shell.execute_reply":"2022-05-02T11:57:45.365558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\nsns.set_context('poster',font_scale = 1)\nplt.figure(figsize = (14,5))\nbins = pd.cut(train.trafficjam,10,labels = False)\nplt.scatter(y = bins,x = range(bins.size),s = 10)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:58:07.572296Z","iopub.execute_input":"2022-05-02T11:58:07.57317Z","iopub.status.idle":"2022-05-02T11:58:10.324853Z","shell.execute_reply.started":"2022-05-02T11:58:07.573088Z","shell.execute_reply":"2022-05-02T11:58:10.323704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like we have some outliers here, they represent 1200 points in our dataset of 1.400.000 datapoints, for the sake of this analysis they are not valid and can interfere badly in our analysis, so lets get rid of them assign them the value of the mean .","metadata":{}},{"cell_type":"code","source":"## Handling outliers\ntrain.trafficjam[np.abs(train.trafficjam - train.trafficjam.mean()) > (train.trafficjam.std()*3)].size\n\n## Assign mean to them\ntrain.loc[np.abs(train.trafficjam - train.trafficjam.mean()) > (train.trafficjam.std()*3),'trafficjam'] = train.trafficjam.mean()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:58:20.244034Z","iopub.execute_input":"2022-05-02T11:58:20.244379Z","iopub.status.idle":"2022-05-02T11:58:20.329548Z","shell.execute_reply.started":"2022-05-02T11:58:20.244345Z","shell.execute_reply":"2022-05-02T11:58:20.328622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Traffic Jam relantionships\nNow we can indeed se how our traffic jam variable behave by plotting it","metadata":{}},{"cell_type":"code","source":"dh_traffic = train.groupby(['dow','hour'])['trafficjam'].mean()\ndh_traffic = dh_traffic.unstack(level = 1)\n\nam = dh_traffic.iloc[:,:12]\npm = dh_traffic.iloc[:,12:]\n\nfig,axes = plt.subplots(2,figsize = (12,12))\nsns.set_context('poster',font_scale = 1.5)\n\nsns.heatmap(am,square = True,cbar = False,linewidths = 1,cmap = 'YlOrRd',ax = axes[0],annot=False,annot_kws = {'fontsize':10},fmt = 'g')\naxes[0].set_title('AM')\naxes[0].set_yticks([0.4,1.4,2.4,3.4,4.4,5.4,6.4])\naxes[0].set_xticks([0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.5,11.5])\naxes[0].set_xticklabels(['00:00','1:00','2:00','3:00','4:00','5:00','6:00','07:00','8:00','9:00','10:00','11:00'])\naxes[0].set_yticklabels(['Mon','Tue','Wed','Tur','Fri','Sat','Sun'])\naxes[0].set_ylabel('')\naxes[0].set_xlabel('')\naxes[0].tick_params(labelsize=15)\n\nsns.heatmap(pm,square = True,cbar = False,linewidths = 1,cmap = 'YlOrRd',ax = axes[1],annot = False,annot_kws = {'fontsize':10},fmt = 'g')\naxes[1].set_title('PM')\naxes[1].set_yticks([0.4,1.4,2.4,3.4,4.4,5.4,6.4])\naxes[1].set_yticklabels(['Mon','Tue','Wed','Tur','Fri','Sat','Sun'])\naxes[1].set_ylabel('')\naxes[1].set_xticks([0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.5,11.5])\naxes[1].set_xticklabels(['12:00','13:00','14:00','15:00','16:00','17:00','18:00','19:00','20:00','21:00','22:00','23:00'])\naxes[1].set_xlabel('')\naxes[1].tick_params(labelsize=15)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:21:23.888716Z","iopub.execute_input":"2022-05-02T12:21:23.888977Z","iopub.status.idle":"2022-05-02T12:21:24.609119Z","shell.execute_reply.started":"2022-05-02T12:21:23.888946Z","shell.execute_reply":"2022-05-02T12:21:24.607397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## DAY OF WEEK VARIABLES\n\n\nfeatures = ['passenger_count','total_travel_time','trips']\nsgh = grouped_hour.loc[:,features+['hour']]\n\nfor feature in features:\n    sgh[str(feature)+'_score'] = pd.qcut(sgh[feature],20,labels = False)\nsgh = sgh.drop(columns = ['passenger_count','total_travel_time','trips'])\n\nsgh.loc[:,sgh.columns != 'hour'] = sgh.loc[:,sgh.columns != 'hour'] +1 ## Verificar depois\n\nam = sgh[sgh.hour < 12]\npm = sgh[sgh.hour >= 12]\nam.set_index('hour',inplace = True)\npm.set_index('hour',inplace = True)\n\n\nsns.set_context('poster')\n\nfig,axes  = plt.subplots(2,figsize = (14,10))\n\naxes[0].set_title('Variables by hour of day')\nsns.heatmap(am.T,square = True,cbar = False,linewidths = 1,cmap = 'YlOrRd',ax = axes[0],annot=False,annot_kws = {'fontsize':15},vmax = 24)\naxes[0].set_xticks([0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.5,11.5])\naxes[0].set_xticklabels(['00:00','1:00','2:00','3:00','4:00','5:00','6:00','07:00','8:00','9:00','10:00','11:00'])\naxes[0].set_xlabel('')\naxes[0].set_yticks([0.5,1.5,2.5])\naxes[0].set_yticklabels(['Passengers','Travel time','Trips'])\n\nsns.heatmap(pm.T,square = True,cbar = False,linewidths = 1,cmap = 'YlOrRd',ax = axes[1],annot = False,annot_kws = {'fontsize':15},vmax = 24)\naxes[1].set_xticks([0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.5,11.5])\naxes[1].set_xticklabels(['12:00','13:00','14:00','15:00','16:00','17:00','18:00','19:00','20:00','21:00','22:00','23:00'])\naxes[1].set_xlabel('')\naxes[1].set_yticks([0.5,1.5,2.5])\naxes[1].set_yticklabels(['Passengers','Travel time','Trips'])\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:58:57.896127Z","iopub.execute_input":"2022-05-02T11:58:57.896567Z","iopub.status.idle":"2022-05-02T11:58:58.519127Z","shell.execute_reply.started":"2022-05-02T11:58:57.896455Z","shell.execute_reply":"2022-05-02T11:58:58.517995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation map\n\nA correlation map (aka. correlation matrix) is a very handful tool when dealing with large number of variables. it shows us lots of meaningful information about our dataset and its relationships. In this case, as expected we can se a great correlation between total_travel_time and total_distance, but besides it, we will have to explore better our dataset to understand its relationships and what kind of variables are useful to predict the total travel time.\n\nIn order to do that we are going to explore better the features and make some feature engineering as we are gonna see further.","metadata":{}},{"cell_type":"code","source":"corr_features = ['dow','precipitation','average temperature','total_distance','total_travel_time','snow fall','snow depth']\ncorr = train.loc[:,corr_features].corr()\nplt.figure(figsize = (11,11))\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.set_context(\"paper\",font_scale=1.3)\nsns.set_style('white')\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr,annot = True,square = True,cmap = cmap,mask = mask,linewidths=0.5,vmax = 0.3,cbar = False)\nplt.title('Correlation Map',fontsize = 22)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:59:06.385887Z","iopub.execute_input":"2022-05-02T11:59:06.386188Z","iopub.status.idle":"2022-05-02T11:59:07.097419Z","shell.execute_reply.started":"2022-05-02T11:59:06.386155Z","shell.execute_reply":"2022-05-02T11:59:07.09633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploring better ...","metadata":{}},{"cell_type":"code","source":"passengers = train.passenger_count.value_counts()\nsns.set_context('poster',font_scale = 0.8)\nplt.figure(figsize = (12,5))\nsns.barplot(passengers.index,passengers.values,palette = 'YlOrRd_r')\nplt.xlabel('Number of passengers')\nplt.ylabel('Number of travels')\nplt.title('Passenger count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:59:25.923063Z","iopub.execute_input":"2022-05-02T11:59:25.923862Z","iopub.status.idle":"2022-05-02T11:59:26.216543Z","shell.execute_reply.started":"2022-05-02T11:59:25.923807Z","shell.execute_reply":"2022-05-02T11:59:26.215353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the last plot showed us, it looks like its more likely that a taxi travel will carry only 1 or 2 person .\n\n## Weather interference in Taxi trips\nAnalysing our correlation map its very clear that weather features apparently dont showed any correlation with our variables of interest, but as matter of fact we are gonna explore a few more the way that weather variables can relate with total_travel_time","metadata":{}},{"cell_type":"code","source":"## SNOW DEPTH PARSING\ntrain['snow depth'] = round(train['snow depth'])\nsnow_tt = train.groupby('snow depth')['total_travel_time'].mean()\n\n## PRECIPITATION PARSING\ntrain['cprecipitation'] = pd.cut(train.precipitation,10,labels = False)\nprec_tt = train.groupby('cprecipitation')['total_travel_time'].mean()\n\n## AVERAGE TEMP PARSING\ntrain['ctemp'] = pd.cut(train['average temperature'],10)\ntrain['ctemp'] = train.ctemp.map(lambda x : round(x.mid))\ntemp_tt = train.groupby('ctemp')['total_travel_time'].mean()\n\n## SNOW FALL PARSING\ntrain['snow fall']  = train['snow fall'].astype(np.int32).map({2:1,27:1,0:0,1:1})\nsnowf_tt = train.groupby('snow fall')['total_travel_time'].mean()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:59:41.41438Z","iopub.execute_input":"2022-05-02T11:59:41.414736Z","iopub.status.idle":"2022-05-02T11:59:41.643331Z","shell.execute_reply.started":"2022-05-02T11:59:41.4147Z","shell.execute_reply":"2022-05-02T11:59:41.642315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_palette(sns.color_palette(\"hls\"))\n\nfig,axes = plt.subplots(2,2,figsize = (12,12),sharey = True)\naxes = np.array(axes).flatten()\n\nfig.suptitle('Weather variables by Total travel time')\nsns.barplot(x = snow_tt.index,y = snow_tt.values,ax = axes[0])\nsns.barplot(x = snowf_tt.index,y = snowf_tt.values,ax = axes[1])\nsns.barplot(x = temp_tt.index,y = temp_tt.values,ax = axes[2])\nsns.barplot(x = prec_tt.index,y = prec_tt.values,ax = axes[3])\n\naxes[0].set_xlabel('Average Snow depth')\naxes[1].set_xlabel('Snow Fall')\naxes[2].set_xlabel('Average Temperature')\naxes[3].set_xlabel('Average Precipitation')\n\nplt.setp(axes, ylim=(300,420))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T11:59:58.309638Z","iopub.execute_input":"2022-05-02T11:59:58.310009Z","iopub.status.idle":"2022-05-02T11:59:59.180336Z","shell.execute_reply.started":"2022-05-02T11:59:58.309969Z","shell.execute_reply":"2022-05-02T11:59:59.179276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Weekly relationships\n\nWe have already seen lot of relationships between features and hour of the day, but now its time to see if we can find some relation between days of week and some variables.","metadata":{}},{"cell_type":"code","source":"sns.set_context('paper',font_scale = 2)\n\nfig,axes = plt.subplots(3,1,figsize = (12,9),sharex = True)\nfeatures = [['trips','Trips by day of week'],['passenger_count','Passengers by day of week'],['total_distance','Total distance by day of week']]\naxes = np.array(axes).flatten()\n\naxes[0].set_title('Days of week variables analysis')\nfor ax,feature in zip(axes,features):\n    sns.violinplot(x = 'dow',y = feature[0],data = grouped,ax = ax,palette = 'Set3')\n    #ax.set_title(feature[1])\n    ax.set_xlabel('')\n    \nplt.setp(axes, xticks= grouped.dow.sort_values().unique(), xticklabels=['Sun','Mon','Tue','Wed','Tur','Fri','Sat'])\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:00:05.430216Z","iopub.execute_input":"2022-05-02T12:00:05.4306Z","iopub.status.idle":"2022-05-02T12:00:06.420837Z","shell.execute_reply.started":"2022-05-02T12:00:05.430561Z","shell.execute_reply":"2022-05-02T12:00:06.419985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the Model\nNow we can start the ML part, lets start it by importing the required tools","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import ShuffleSplit as ss\nfrom sklearn.model_selection import cross_val_score as cvs\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:00:21.765939Z","iopub.execute_input":"2022-05-02T12:00:21.766464Z","iopub.status.idle":"2022-05-02T12:00:21.772298Z","shell.execute_reply.started":"2022-05-02T12:00:21.766426Z","shell.execute_reply":"2022-05-02T12:00:21.771353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Declaring some function we are gonna use","metadata":{}},{"cell_type":"code","source":"\"\"\" \nThe Haversine formula calculates the shortest distance between two points,\non a sphere using their latitudes and longitudes measured along the surface. \n\"\"\"\ndef haversine(row):\n    lon1 = row['pickup_longitude']\n    lat1 = row['pickup_latitude']\n    lon2 = row['dropoff_longitude']\n    lat2 = row['dropoff_latitude']\n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1\n    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n    c = 2 * asin(sqrt(a)) \n    km = 6369 * c\n    return km\n\n\"\"\" \nThis function will calculate the direction of the trips given the coordinates.\n\"\"\"\ndef get_direction(row):\n    lat_1 = row['pickup_latitude']\n    long_1 = row['pickup_longitude']\n    lat_2 = row['dropoff_latitude']\n    long_2 = row['dropoff_longitude']\n    \n    AVG_EARTH_RADIUS = 6371  # in km\n    long_delta_rad = np.radians(long_2 - long_1)\n    lat_1, long_1, lat_2, long_2 = map(np.radians, (lat_1, long_1, lat_2, long_2))\n    y = np.sin(long_delta_rad) * np.cos(lat_2)\n    x = np.cos(lat_1) * np.sin(lat_2) - np.sin(lat_1) * np.cos(lat_2) * np.cos(long_delta_rad)\n    \n    return np.degrees(np.arctan2(y, x))\n\n\"\"\" \nError function used to evaluate our model.\n\"\"\"\ndef rmsle(evaluator,X,real):\n    sum = 0.0\n    predicted = evaluator.predict(X)\n    print(\"Number predicted less than 0: {}\".format(np.where(predicted < 0)[0].shape))\n\n    predicted[predicted < 0] = 0\n    for x in range(len(predicted)):\n        p = np.log(predicted[x]+1)\n        r = np.log(real[x]+1)\n        sum = sum + (p-r)**2\n    return (sum/len(predicted))**0.5","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:01:04.796125Z","iopub.execute_input":"2022-05-02T12:01:04.796666Z","iopub.status.idle":"2022-05-02T12:01:04.812916Z","shell.execute_reply.started":"2022-05-02T12:01:04.79663Z","shell.execute_reply":"2022-05-02T12:01:04.811839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aplying distance and direction functions to our datasets","metadata":{}},{"cell_type":"code","source":"train[\"distance\"] = train.apply(haversine,axis=1)\ntest[\"distance\"] = test.apply(haversine,axis=1)\n\ntrain[\"direction\"] = train.apply(get_direction,axis=1)\ntest[\"direction\"] = test.apply(get_direction,axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:01:29.459585Z","iopub.execute_input":"2022-05-02T12:01:29.460413Z","iopub.status.idle":"2022-05-02T12:05:46.578342Z","shell.execute_reply.started":"2022-05-02T12:01:29.460365Z","shell.execute_reply":"2022-05-02T12:05:46.577331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\n\ntrain[\"store_and_fwd_flag\"] = le.fit_transform(train[\"store_and_fwd_flag\"])\ntest[\"store_and_fwd_flag\"] = le.fit_transform(test[\"store_and_fwd_flag\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:05:46.580043Z","iopub.execute_input":"2022-05-02T12:05:46.580327Z","iopub.status.idle":"2022-05-02T12:05:47.264201Z","shell.execute_reply.started":"2022-05-02T12:05:46.580294Z","shell.execute_reply":"2022-05-02T12:05:47.262904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Selecting the features we are gonna use and declaring our X and y","metadata":{}},{"cell_type":"code","source":"features = [\"vendor_id\",\"passenger_count\",\"pickup_longitude\",\"pickup_latitude\",\n           \"dropoff_longitude\",\"dropoff_latitude\",\"store_and_fwd_flag\",\"day\",\n           \"dow\",\"hour\",\"average temperature\",\"precipitation\",\"snow fall\",\n           \"snow depth\",\"total_distance\",\"total_travel_time\",\"distance\",\"direction\"]\n\ntrain = train[train.trip_duration<90000]\n\nX = train[features]\ny = train.trip_duration.values","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:05:47.266118Z","iopub.execute_input":"2022-05-02T12:05:47.266715Z","iopub.status.idle":"2022-05-02T12:05:47.784552Z","shell.execute_reply.started":"2022-05-02T12:05:47.266674Z","shell.execute_reply":"2022-05-02T12:05:47.78372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading our Xgboost model and evaluating it with cross validation","metadata":{}},{"cell_type":"code","source":"reg = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n                           colsample_bytree=1, max_depth=10)\n\ncv = ss(n_splits=2, test_size=0.2, random_state=0)\n\nprint(cvs(reg, X, y, cv=cv,scoring=rmsle))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:05:47.786634Z","iopub.execute_input":"2022-05-02T12:05:47.786879Z","iopub.status.idle":"2022-05-02T12:15:38.542656Z","shell.execute_reply.started":"2022-05-02T12:05:47.786851Z","shell.execute_reply":"2022-05-02T12:15:38.541306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets train our model and predict the competition results!","metadata":{}},{"cell_type":"code","source":"reg.fit(X,y)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:15:38.544568Z","iopub.execute_input":"2022-05-02T12:15:38.545424Z","iopub.status.idle":"2022-05-02T12:21:19.05446Z","shell.execute_reply.started":"2022-05-02T12:15:38.545373Z","shell.execute_reply":"2022-05-02T12:21:19.053553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets explore the importance of the features in our model","metadata":{}},{"cell_type":"code","source":"features_importance = reg.get_booster().get_score(importance_type='weight')\ndata = pd.DataFrame(data=features_importance.values(), index=features_importance.keys(), columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n\nfig = plt.figure(figsize = (12,10))\nfig.suptitle('Features Importance')\nsns.barplot(x = data.score,y = data.index)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:21:19.055971Z","iopub.execute_input":"2022-05-02T12:21:19.056819Z","iopub.status.idle":"2022-05-02T12:21:19.437754Z","shell.execute_reply.started":"2022-05-02T12:21:19.056763Z","shell.execute_reply":"2022-05-02T12:21:19.437098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = test[features]\npredicted = reg.predict(X_test)\ntest['trip_duration'] = predicted\ntest[['id','trip_duration']].to_csv('Taxi_sub.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:21:20.839191Z","iopub.execute_input":"2022-05-02T12:21:20.840049Z","iopub.status.idle":"2022-05-02T12:21:23.886543Z","shell.execute_reply.started":"2022-05-02T12:21:20.840003Z","shell.execute_reply":"2022-05-02T12:21:23.885568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thats it! We are done!","metadata":{}}]}