{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Prequels/sequels\n\n- **ChaiEDA sessions: ChaiEDA: NYC Taxi Trip Duration (data-prep)** | [Extended Dataset](https://www.kaggle.com/neomatrix369/nyc-taxi-trip-duration-extended)\n- [ChaiEDA sessions: ChaiEDA: NYC Taxi Trip Duration - analysis](https://www.kaggle.com/neomatrix369/chaieda-nyc-taxi-trip-duration-analysis)"},{"metadata":{},"cell_type":"markdown","source":"## Installing and importing libraries and packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install swifter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bash\nrm -f dtype_diet.py\nwget https://raw.githubusercontent.com/ianozsvald/dtype_diet/master/dtype_diet.py\nls -lash *.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import dtype_diet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading datasets"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"DATA_FOLDER='/kaggle/input/nyc-taxi-trip-duration/'\ntraining_dataset = pd.read_csv(f'{DATA_FOLDER}/train.zip')\ntest_dataset = pd.read_csv(f'{DATA_FOLDER}/test.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATASET_UPLOAD_FOLDER='/kaggle/working/upload'\nEXTENDED_DATA_FOLDER='/kaggle/input/nyc-taxi-trip-duration-extended'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bash\nUPLOAD_FOLDER=/kaggle/working/upload\nmkdir -p ${UPLOAD_FOLDER}\ncp /kaggle/input/nyc-taxi-trip-duration/*.zip ${UPLOAD_FOLDER} || true\ncp /kaggle/input/chaieda-nyc-taxi-trip-duration-data-prep/*.csv ${UPLOAD_FOLDER} || true","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# dtype_diet.report_on_dataframe(training_dataset)\ntraining_dataset.info(memory_usage='deep')\ntraining_dataset['vendor_id'] = training_dataset['vendor_id'].astype('int8')\ntraining_dataset['passenger_count'] = training_dataset['passenger_count'].astype('int8')\ntraining_dataset['store_and_fwd_flag'] = training_dataset['store_and_fwd_flag'].astype('category')\ntraining_dataset['trip_duration'] = training_dataset['trip_duration'].astype('int32')\ntraining_dataset.info(memory_usage='deep')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# dtype_diet.report_on_dataframe(test_dataset)\ntest_dataset.info(memory_usage='deep')\ntest_dataset['vendor_id'] = test_dataset['vendor_id'].astype('int8')\ntest_dataset['passenger_count'] = test_dataset['passenger_count'].astype('int8')\ntest_dataset['store_and_fwd_flag'] = test_dataset['store_and_fwd_flag'].astype('category')\ntest_dataset.info(memory_usage='deep')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(training_dataset.columns)\nprint(test_dataset.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test_filename = 'train_test_extended.csv'\nextended_dataset_name = f'{EXTENDED_DATA_FOLDER}/{train_test_filename}'\nfound_extended_dataset = os.path.exists(extended_dataset_name)\nif found_extended_dataset:\n    print(f'Found {extended_dataset_name}, reusing existing one')\n    combined_dataset = pd.read_csv(extended_dataset_name)\nelse:\n    print(f'Did not find {extended_dataset_name}, will generate one starting here')\n    combined_dataset = pd.concat([training_dataset, test_dataset])\n    combined_dataset = combined_dataset.reset_index(drop=True)\ncombined_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndtype_diet.report_on_dataframe(combined_dataset)\ncombined_dataset.info(memory_usage='deep')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Additional data\n\nLoading additional data that cover information about the districts and neighbourhoods in New York City with the help of the [NYC Airbnb dataset](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data)."},{"metadata":{"trusted":true},"cell_type":"code","source":"ADDITIONAL_DATA_FOLDER='/kaggle/input/new-york-city-airbnb-open-data/'\nmore_data_dataset = pd.read_csv(f'{ADDITIONAL_DATA_FOLDER}/AB_NYC_2019.csv')\nmore_data_dataset = more_data_dataset.drop(columns=['id', 'host_id', 'host_name', 'room_type', 'price', 'minimum_nights', 'number_of_reviews', \n                                                    'last_review', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365'])\n\nmore_data_dataset = more_data_dataset.rename(columns={'neighbourhood_group': 'district'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# dtype_diet.report_on_dataframe(more_data_dataset)\nmore_data_dataset.info(memory_usage='deep')\nmore_data_dataset['district'] = more_data_dataset['district'].astype('category')\nmore_data_dataset['neighbourhood'] = more_data_dataset['neighbourhood'].astype('category')\nmore_data_dataset.info(memory_usage='deep')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Convert Latitude/Longitude\n\nIt's easier to manage (compare, sort, etc...) multiple fields when they can be clubbed/merged into a single unique (and meaningful) value. Here we combine "},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/8285599/is-there-a-formula-to-change-a-latitude-and-longitude-into-a-single-number\n# Alternative calculation: (lat * 1e7 << 16) & 0xffff0000 | lng * 1e7 & 0x0000ffff\ndef geonumber(lat: float, lng: float):\n    return ((lat + 90) * 180) + lng","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmore_data_dataset['name'] = more_data_dataset['name'].fillna('<Unknown>')\nmore_data_dataset = more_data_dataset.sort_values(by = ['latitude','longitude'])\nmore_data_dataset['geonumber'] = np.vectorize(geonumber)(more_data_dataset['latitude'], more_data_dataset['longitude'])\nmore_data_dataset['name'] = more_data_dataset['name'].apply(lambda x: x.title())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"more_data_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"more_data_dataset.to_csv(f'{DATASET_UPLOAD_FOLDER}/nyc_additional_info.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate District and Neighbourhood from Latitude and Longitude\n\nUsing the additional data dataset mapping the Latitudes and Longitudes of each Pickup and Dropoff points with the respective districts and neighbourhoods corresponding to them. But this is done slighly differently, we convert the Latitude and Longitude of a point into a single number called _geonumber_. This helps make the matching during the mapping process easier and a bit more efficient."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncombined_dataset['pickup_geonumber'] = np.vectorize(geonumber)(combined_dataset['pickup_latitude'], combined_dataset['pickup_longitude'])\ncombined_dataset['dropoff_geonumber'] = np.vectorize(geonumber)(combined_dataset['dropoff_latitude'], combined_dataset['dropoff_longitude'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tempfile\nimport numexpr\nfrom tqdm.auto import tqdm\nfrom joblib import Memory, Parallel, delayed \nimport math\n\nmemory = Memory('/kaggle/working/', compress=9, verbose=0)\n\nOFFSET = 200\n\ndef filter_more_data_dataset(filter, target_geonumber: float):\n    mid_point = round(more_data_dataset[filter].shape[0] / 2)\n    if more_data_dataset[filter].shape[0] > 0:\n        return [more_data_dataset.iloc[mid_point]['district'], more_data_dataset.iloc[mid_point]['neighbourhood']]\n    \n    min_geonumber = more_data_dataset[0:1]['geonumber'].values[0]\n    max_geonumber = more_data_dataset[-1:]['geonumber'].values[0]\n    if (target_geonumber < min_geonumber) or (target_geonumber > max_geonumber):\n        return [\"Outside NYC\", \"Outside NYC\"]\n\n    return [\"<Unknown>\", \"<Unknown>\"]\n    \n@memory.cache()\ndef get_district_neighbourhood_info(target_geonumber: float) -> (str, str):\n    if math.isnan(target_geonumber):\n        return [\"<Unknown>\", \"<Unknown>\"]\n    \n    start_geonumber = target_geonumber - OFFSET\n    end_geonumber = target_geonumber + OFFSET\n\n    expression = f'(geonumber >= {start_geonumber}) and (geonumber <= {end_geonumber})'\n    lat_long_filter = more_data_dataset.eval(expression)\n    \n    return filter_more_data_dataset(lat_long_filter, target_geonumber)\n\n\ndef process_pickup_dropoff_info(pickup_geonumber: float, dropoff_geonumber: float):\n    pickup_results = get_district_neighbourhood_info(pickup_geonumber)\n    dropoff_results = get_district_neighbourhood_info(dropoff_geonumber)\n    \n    return pickup_results + dropoff_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import swifter\nimport gc\n\ndef apply_pickup_dropoff_info(dataset):\n    pickup_geonumber = dataset['pickup_geonumber']\n    dropoff_geonumber = dataset['dropoff_geonumber']\n    return process_pickup_dropoff_info(pickup_geonumber, dropoff_geonumber)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialise(dataset: pd.DataFrame, new_field: str) -> pd.DataFrame:\n    if new_field not in dataset.columns:\n        dataset[new_field] = '<Unknown>'\n    return dataset\n\ndef set_datatype(dataset: pd.DataFrame, new_field: str, type_name: str = 'category') -> pd.DataFrame:\n    if new_field in dataset.columns:\n        dataset[new_field] = dataset[new_field].astype(type_name)\n    return dataset\n\ndef update_dataset(dataset: pd.DataFrame, start: int, end: int, results: list) -> pd.DataFrame:\n    pickup_districts = []\n    pickup_neighbourhoods = []\n    dropoff_districts = []\n    dropoff_neighbourhoods = []\n    for each in results:\n        pickup_districts.append(each[0])\n        pickup_neighbourhoods.append(each[1])\n        dropoff_districts.append(each[2])\n        dropoff_neighbourhoods.append(each[3])\n    \n    dataset.iloc[start:end]['pickup_district'] = pickup_districts.copy()\n    dataset.iloc[start:end]['pickup_neighbourhood'] = pickup_neighbourhoods.copy()\n    dataset.iloc[start:end]['dropoff_district'] = dropoff_districts.copy()\n    dataset.iloc[start:end]['dropoff_neighbourhood'] = dropoff_neighbourhoods.copy()\n    print(f'Saved rows between {start} and {end} of the dataset.')\n  \n    del pickup_districts, pickup_neighbourhoods, dropoff_districts, dropoff_neighbourhoods\n    \n    return dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a filter we will use to keep our pipeline continuous and also filter out the locations that have been mapped successfully to those that have failed to be mapped."},{"metadata":{"trusted":true},"cell_type":"code","source":"unknown_filter = (combined_dataset.pickup_district == '<Unknown>') | (combined_dataset.pickup_neighbourhood == '<Unknown>') | \\\n                 (combined_dataset.dropoff_district == '<Unknown>') | (combined_dataset.dropoff_neighbourhood == '<Unknown>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_dataset[unknown_filter]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generate a mapping between the geonumbers in the combined dataset and additional data dataset using either joblib's Parallel or Swifter to parallel process the data processing tasks. The unmapped rows in the dataset are split into smaller batches and each batch are updated as soon as the process per batch finishes. The filter takes care of reducing the number of rows that need mapping each time a row is updated."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nforce_regenerate = True\nif force_regenerate or (not found_extended_dataset):\n    batch_size = combined_dataset[unknown_filter].shape[0] // 20       \\\n                 if combined_dataset[unknown_filter].shape[0] > 10_000 \\\n                 else combined_dataset[unknown_filter].shape[0]\n    if batch_size == 0:\n        print('No data needs mapping.')\n    else:\n        combined_dataset = initialise(combined_dataset, 'pickup_district')\n        combined_dataset = initialise(combined_dataset, 'pickup_neighbourhood')\n        combined_dataset = initialise(combined_dataset, 'dropoff_district')\n        combined_dataset = initialise(combined_dataset, 'dropoff_neighbourhood')\n\n        combined_dataset = set_datatype(combined_dataset, 'pickup_district', 'str')\n        combined_dataset = set_datatype(combined_dataset, 'pickup_neighbourhood', 'str')\n        combined_dataset = set_datatype(combined_dataset, 'dropoff_district', 'str')\n        combined_dataset = set_datatype(combined_dataset, 'dropoff_neighbourhood', 'str')\n\n        processing_method = 'swifter' # 'default' ==> option is slow on this dataframe, 'swifter'= maybe a after option\n\n        for index in tqdm(range(0, combined_dataset[unknown_filter].shape[0], batch_size), \\\n                                desc=f'Pickup/dropoff (batchsize: {batch_size})'):\n            start = index\n            end = index + batch_size\n\n            if processing_method == 'swifter':\n                results = combined_dataset[unknown_filter][start:end].swifter \\\n                        .set_dask_scheduler(scheduler=\"processes\") \\\n                        .progress_bar(enable=True, desc=f'Processing: {start} to {end}') \\\n                        .allow_dask_on_strings(enable=True) \\\n                        .apply(apply_pickup_dropoff_info, axis=1)\n            else:\n                pickup_geonumbers = combined_dataset[unknown_filter][start:end]['pickup_geonumber'].values\n                dropoff_geonumbers = combined_dataset[unknown_filter][start:end]['dropoff_geonumber'].values\n\n                results = Parallel(n_jobs=-1)(\n                    delayed(process_pickup_dropoff_info)(\n                        pickup_geonumbers, dropoff_geonumbers\n                    ) for _, (pickup_geonumber, dropoff_geonumber) in \\\n                              enumerate(tqdm(zip(pickup_geonumbers, dropoff_geonumbers)))\n                )\n                del pickup_geonumbers, dropoff_geonumbers\n\n            combined_dataset[unknown_filter] = update_dataset(combined_dataset.loc[unknown_filter], start, end, results)\n            del results\n            gc.collect()\n\n    combined_dataset = set_datatype(combined_dataset, 'pickup_district', 'category')\n    combined_dataset = set_datatype(combined_dataset, 'pickup_neighbourhood', 'category')\n    combined_dataset = set_datatype(combined_dataset, 'dropoff_district', 'category')\n    combined_dataset = set_datatype(combined_dataset, 'dropoff_neighbourhood', 'category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[print(f'{field}\\n{combined_dataset[field].value_counts()}\\n') \\\n       for field in ['pickup_district', 'pickup_neighbourhood', 'dropoff_district', 'dropoff_neighbourhood']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate features from date/time related fields\n\nThe date and time fields related to pickup and dropoff between rides contain a number of time related features i.e. Month, Year, Season, etc... which can help understand the behaviour of the ride/client and also demand and usage of taxis throughout the year(s)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# The country's meteorological department follows the international standard of four seasons with some local adjustments: \n# - winter (December - February)\n# - spring (March - May)\n# - summer (June - August) \n# - fall (September - November)\n\ndate_to_season_mapping = {'1. Winter': [12, 2], '2. Spring': [3, 5], '3. Summer': [6, 8], '4. Fall': [9, 11]}\n\ndef date_to_season(dates):\n    results = []\n    date_values = pd.DatetimeIndex(dates).month.values\n    \n    for month in date_values:\n        result = 'None'\n        for each_season in date_to_season_mapping:\n            start, end = date_to_season_mapping[each_season]\n            if ((start < end) and (start <= month <= end)) or \\\n               ((start > end) and ((month >= start) or (month <= end))):\n                result = each_season\n                break\n\n        results.append(result)\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_no_to_name_mapping = [\n    '01. Jan', '02. Feb', '03. Mar', '04. Apr', '05. May', '06. Jun', '07. Jul', \n    '08. Aug', '09. Sep', '10. Oct', '11. Nov', '12. Dec'\n]\n\ndef date_to_month_name(dates):\n    month_values = pd.DatetimeIndex(dates).month.values\n    results = []\n    for month in month_values:\n        result = month_no_to_name_mapping[month - 1]\n        results.append(result)\n    return results\n\ndef weekday_or_weekend(dates):\n    results = []\n    for date_value in pd.DatetimeIndex(dates.values):\n        weekno = date_value.weekday()\n        result = \"Weekday\" if weekno < 5 else \"Weekend\"\n        results.append(result)\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import holidays\nholidays_usa = holidays.USA()\n\ndef regular_day_or_holiday(dates):\n    results = []\n    for date_value in pd.DatetimeIndex(dates.values):\n        result = \"Holiday (or Festival)\" if date_value.date() in holidays_usa else \"Regular day\"\n        results.append(result)\n    return results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Day period and time ranges\n\n- morning: 6-11:59\n- afternoon: 12-5\n- night: 6-12 (with activity)\n- sleep time: 12-5:59 (w/o activity)\n\nThanks [Mindy](https://www.kaggle.com/mindyng) for your help and also confirming the above."},{"metadata":{"trusted":true},"cell_type":"code","source":"date_to_day_period_mapping = {'1. Morning': [6, 11], '2. Afternoon': [12, 17], \n                              '3. Evening': [18, 23], '4. Night': [0, 5]}\ndef date_to_day_period(datetimes):\n    results = []\n    datetime_values = datetimes.values\n    for datetime in datetime_values:\n        _, time_of_day = datetime.split(' ')\n        hour, _, _ = time_of_day.split(':')\n        hour = int(hour)\n        result = 'None'\n        for each_day_period in date_to_day_period_mapping:\n            start, end = date_to_day_period_mapping[each_day_period]\n            if ((start < end) and (start <= hour <= end)) or \\\n               ((start > end) and ((hour >= start) or (hour <= end))):\n                result = each_day_period\n                break\n\n        results.append(result)\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nforce_regenerate = True\nif force_regenerate or (not found_extended_dataset):\n    combined_dataset['pickup_hour'] = pd.DatetimeIndex(combined_dataset['pickup_datetime']).hour\n    combined_dataset['day_period'] = date_to_day_period(combined_dataset['pickup_datetime'])\n    combined_dataset['day_name'] = pd.DatetimeIndex(combined_dataset['pickup_datetime']).day_name()\n    daynames_with_index = {\n        'Monday': '1. Monday', 'Tuesday': '2. Tuesday', 'Wednesday': '3. Wednesday', 'Thursday': '4. Thursday',\n        'Friday': '5. Friday', 'Saturday': '6. Saturday', 'Sunday': '7. Sunday'\n    }\n    combined_dataset['day_name'] = combined_dataset['day_name'].replace(daynames_with_index)\n    combined_dataset['month'] = date_to_month_name(combined_dataset['pickup_datetime'])\n    combined_dataset['financial_quarter'] = combined_dataset['month'] \n    month_to_quarter = {\n        '01. Jan': 4, '02. Feb': 4, '03. Mar': 4, '04. Apr': 1, '05. May': 1, '06. Jun': 1, '07. Jul': 2, \n        '08. Aug': 2, '09. Sep': 2, '10. Oct': 3, '11. Nov': 3, '12. Dec': 3\n    }\n    combined_dataset['financial_quarter'] = combined_dataset['financial_quarter'].replace(month_to_quarter)\n    combined_dataset['year'] = pd.DatetimeIndex(combined_dataset['pickup_datetime']).year\n    combined_dataset['season'] = date_to_season(combined_dataset['pickup_datetime'])\n    combined_dataset['weekday_or_weekend'] = weekday_or_weekend(combined_dataset['pickup_datetime'])\n    combined_dataset['regular_day_or_holiday'] = regular_day_or_holiday(combined_dataset['pickup_datetime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncombined_dataset.info(memory_usage='deep')\ncombined_dataset['pickup_hour'] = combined_dataset['pickup_hour'].astype('category')\ncombined_dataset['day_period'] = combined_dataset['day_period'].astype('category')\ncombined_dataset['day_name'] = combined_dataset['day_name'].astype('category')\ncombined_dataset['month'] = combined_dataset['month'].astype('category')\ncombined_dataset['financial_quarter'] = combined_dataset['financial_quarter'].astype('category')\ncombined_dataset['year'] = combined_dataset['year'].astype('category')\ncombined_dataset['season'] = combined_dataset['season'].astype('category')\ncombined_dataset['weekday_or_weekend'] = combined_dataset['weekday_or_weekend'].astype('category')\ncombined_dataset['regular_day_or_holiday'] = combined_dataset['regular_day_or_holiday'].astype('category')\ncombined_dataset.info(memory_usage='deep')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Save the generated fields in the new datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_filter = ~combined_dataset.trip_duration.isna()\ntrain_extended_dataset = combined_dataset[train_filter]\ntrain_extended_dataset.to_csv(f'{DATASET_UPLOAD_FOLDER}/train_extended.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_extended_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_filter = combined_dataset.trip_duration.isna()\ntest_extended_dataset = combined_dataset[test_filter]\ntest_extended_dataset.to_csv(f'{DATASET_UPLOAD_FOLDER}/test_extended.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_extended_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncombined_dataset.to_csv(f'{DATASET_UPLOAD_FOLDER}/{train_test_filename}', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Uploading newly created/updated csv to your Kaggle Dataset\n\nSetup your local environment with your Kaggle login details (`KAGGLE_KEY` and `KAGGLE_USERNAME`)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nimport os\nos.environ['KAGGLE_KEY'] = user_secrets.get_secret(\"KAGGLE_KEY\")\nos.environ['KAGGLE_USERNAME'] = user_secrets.get_secret(\"KAGGLE_USERNAME\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the `kaggle` Python client login, into your account from within the kernel."},{"metadata":{"trusted":true},"cell_type":"code","source":"import kaggle\nkaggle.api.authenticate()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get the metadata for the dataset you have already created manually - it's best to manually create it and upload the initial csv file(s) into it, to avoid subsequent issues with updating the dataset (as seen during my own end-to-end cycle).\n\nSave the metadata file as a json file but before that, add/update two keys id and id_no with the respective details as shown below and then save it."},{"metadata":{"trusted":true},"cell_type":"code","source":"OWNER_SLUG='neomatrix369'\nDATASET_SLUG='nyc-taxi-trip-duration-extended'\ndataset_metadata = kaggle.api.metadata_get(OWNER_SLUG, DATASET_SLUG)\ndataset_metadata['id'] = dataset_metadata[\"ownerUser\"] + \"/\" + dataset_metadata['datasetSlug']\ndataset_metadata['id_no'] = dataset_metadata['datasetId']\nimport json\nwith open(f'{DATASET_UPLOAD_FOLDER}/dataset-metadata.json', 'w') as file:\n    json.dump(dataset_metadata, file, indent=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally call the dataset_create_version() api and pass it the folder where the metadata file exists and also where your .csv and .fth file(s) - those file(s) that you would like to upload into your existing Dataset (as a new version)."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# !kaggle datasets version -m \"Updating datasets\" -p /kaggle/working/upload\nkaggle.api.dataset_create_version(DATASET_UPLOAD_FOLDER, 'Updating datasets')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleanup (joblib cache)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -fr /kaggle/working/joblib","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prequels/sequels\n\n- **ChaiEDA sessions: ChaiEDA: NYC Taxi Trip Duration (data-prep)** | [Extended Dataset](https://www.kaggle.com/neomatrix369/nyc-taxi-trip-duration-extended)\n- [ChaiEDA sessions: ChaiEDA: NYC Taxi Trip Duration - analysis](https://www.kaggle.com/neomatrix369/chaieda-nyc-taxi-trip-duration-analysis)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}