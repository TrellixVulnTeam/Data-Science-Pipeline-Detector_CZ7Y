{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport missingno as msno\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/nyc-taxi-trip-duration/train.zip\", compression=\"zip\")\ndf_test = pd.read_csv(\"/kaggle/input/nyc-taxi-trip-duration/test.zip\", compression=\"zip\")\ndf_sub = pd.read_csv(\"/kaggle/input/nyc-taxi-trip-duration/sample_submission.zip\", compression=\"zip\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# General Information about the dataset"},{"metadata":{},"cell_type":"markdown","source":"__First of all, it is important to know the data itself a little better. it would be nice to know:__\n  * how many datasets are relevant\n  * which columns are present in the data sets\n  * which datatypes are present \n  * how many rows are in the datasets\n  * how heavy the dataset are in terms of memory usage"},{"metadata":{"trusted":true},"cell_type":"code","source":"for name, ds in zip([\"df_train\",\"df_test\",\"df_sub\"],[df_train, df_test, df_sub]):\n    \n    print(\"---------------\")\n    print(\"{}\\n\".format(name))\n    print(ds.info())\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* the main goal of the competition is to predict the trip duration of a taxi drive based on several attributes describing the drive\n* three dataset are provided: train set, test set and submission file\n* train set/test set\n    * time features: pickup and dropoff datetime\n    * geographical features: pickup and drop off longitude/latitude\n    * others: store-and fwd Flag, passenger count\n* submission file:\n    * structure of the submission "},{"metadata":{},"cell_type":"markdown","source":"#### Another important question to get to know the data is the structure concerning missingness. Are there many values gone missing or is the dataset complete?"},{"metadata":{"trusted":true},"cell_type":"code","source":"for name, ds in zip([\"df_train\",\"df_test\",\"df_sub\"],[df_train, df_test, df_sub]):\n    \n    print(\"---------------\")\n    print(\"{}\\n\".format(name))\n    print(ds.isnull().sum())\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* no missing values in the data set"},{"metadata":{},"cell_type":"markdown","source":"For closing this first description of the datasets, it's maybe also interesting to have a look on the raw data"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([df_train.head(),df_train.tail()],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([df_test.head(),df_test.tail()],axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"cat features train set: {}\".format(df_train.select_dtypes(exclude=\"number\").columns))\nprint(\"\\n\")\nprint(\"numeric features train set: {}\".format(df_train.select_dtypes(include=\"number\").columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"cat features test set: {}\".format(df_test.select_dtypes(exclude=\"number\").columns))\nprint(\"\\n\")\nprint(\"numeric features test set: {}\".format(df_test.select_dtypes(include=\"number\").columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"pickup_datetime\"] = pd.to_datetime(df_train[\"pickup_datetime\"], format=\"%Y-%m-%d %H:%M:%S\")\ndf_train[\"dropoff_datetime\"] = pd.to_datetime(df_train[\"dropoff_datetime\"], format=\"%Y-%m-%d %H:%M:%S\")\n\ndf_test[\"pickup_datetime\"] = pd.to_datetime(df_test[\"pickup_datetime\"], format=\"%Y-%m-%d %H:%M:%S\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.auto import tqdm\n\ntqdm.pandas()\n\ndf_train[\"year\"] = df_train[\"pickup_datetime\"].progress_apply(lambda x: x.year)\ndf_train[\"month\"] = df_train[\"pickup_datetime\"].progress_apply(lambda x: x.month)\ndf_train[\"day\"] = df_train[\"pickup_datetime\"].progress_apply(lambda x: x.day)\ndf_train[\"hour\"] = df_train[\"pickup_datetime\"].progress_apply(lambda x: x.hour)\ndf_train[\"minute\"] = df_train[\"pickup_datetime\"].progress_apply(lambda x: x.minute)\n\ndf_test[\"year\"] = df_test[\"pickup_datetime\"].progress_apply(lambda x: x.year)\ndf_test[\"month\"] = df_test[\"pickup_datetime\"].progress_apply(lambda x: x.month)\ndf_test[\"day\"] = df_test[\"pickup_datetime\"].progress_apply(lambda x: x.day)\ndf_test[\"hour\"] = df_test[\"pickup_datetime\"].progress_apply(lambda x: x.hour)\ndf_test[\"minute\"] = df_test[\"pickup_datetime\"].progress_apply(lambda x: x.minute)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Which time frame is considered in the dataset?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"df_train: {}\".format(df_train[\"year\"].unique()))\nprint(\"df_test: {}\".format(df_test[\"year\"].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* the datasets focus only the year 2016"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, sub = plt.subplots(2,3,figsize=(25,6))\ncounter = 0\n\nfor feat, subplot in zip([\"month\",\"day\",\"hour\",\"month\",\"day\",\"hour\"], sub.flatten()):\n    \n    if counter<3:\n        sns.barplot(x=df_train[feat].value_counts().index, y = df_train[feat].value_counts().values, ax= subplot, palette=\"CMRmap\")\n        subplot.grid()\n        subplot.set_title(\"Train set {}\".format(feat))\n    else:\n        sns.barplot(x=df_test[feat].value_counts().index, y = df_test[feat].value_counts().values, ax= subplot, palette=\"CMRmap\")\n        subplot.grid()\n        subplot.set_title(\"Test set {}\".format(feat))\n    \n    counter+=1\n    \nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* all records are from the year 2016 and regarding the months the rides took place between January and June 2016\n* the rides are approx. equally distributed to the days per months. however at the end of the month there are fewer rides\n* regarding the time of the day, most of the clients have been driven between 18 - 23 h and the least has been transported between 0 and 6 h "},{"metadata":{},"cell_type":"markdown","source":"### to verify this hypothesis: we'll conduct some hypothesis test"},{"metadata":{},"cell_type":"markdown","source":"but first of all, we have to know which test we can use to verify\nto use anova or t-tests to verify, the data needs to be normally distributed <br>\n--> to check if this prerequisite is met, we'll have a look on the q-q-plots of the feature \"trip_duration\""},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,sub = plt.subplots(2,3,figsize=(12,6))\n\nsns.distplot(df_train[\"trip_duration\"], hist_kws={\"edgecolor\":\"black\"}, ax=sub[0][0])\nsns.distplot(np.log1p(df_train[\"trip_duration\"]), hist_kws={\"edgecolor\":\"black\"}, ax=sub[0][1])\nsns.distplot(df_train[df_train[\"trip_duration\"]<60*120][\"trip_duration\"], hist_kws={\"edgecolor\":\"black\"}, ax=sub[0][2])\n\nprob = stats.probplot(df_train[\"trip_duration\"], dist=stats.norm, plot=sub[1][0])\nprob = stats.probplot(np.log1p(df_train[\"trip_duration\"]), dist=stats.norm, plot=sub[1][1])\nprob = stats.probplot(df_train[df_train[\"trip_duration\"]<60*120][\"trip_duration\"], dist=stats.norm, plot=sub[1][2])\n\n\ncounter = 0\n\nfor name, subplot in zip([\"duration [raw data]\",\"duration [log]\", \"duration [< 2 h]\",\"probPlot duration [raw data]\",\"probPlot duration [log]\", \"probPlot duration [< 2 h]\"], sub.flatten()):\n    subplot.set_title(\"{}\".format(name))\n    subplot.grid()\n    \n    if counter < 3:\n        if name == \"duration [log]\": \n            subplot.set_xlabel(\"trip duration in log(sec)\")\n        else:\n            subplot.set_xlabel(\"trip duration in sec\")\n    \n    counter += 1\n\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* the log of duration seems to be the most suitable to use anova and t-test"},{"metadata":{},"cell_type":"markdown","source":"### Are there many outlier present in the dataset?"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_outlier(df):\n    \n    outlier_index = []\n    \n    q1 = np.quantile(df[\"trip_duration\"],0.25)\n    q3 = np.quantile(df[\"trip_duration\"],0.75)\n    IQR = q3 - q1\n    outlier_step = 1.5 * IQR\n    \n    lower_barreer = q1 - outlier_step\n    upper_barreer = q3 + outlier_step\n    \n    outlier_list_col = df[(df[\"trip_duration\"] < lower_barreer) | (df[\"trip_duration\"] > upper_barreer)].index\n    outlier_index.extend(outlier_list_col)\n    \n    return outlier_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"tuckey outlier df_train: {}\".format(df_train.iloc[get_outlier(df_train)].shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"min duration: {} sec \".format(df_train[\"trip_duration\"].min()))\nprint(\"max duration: {} hour \".format(np.round(df_train[\"trip_duration\"].max()/(60**2)),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* right skewed distribution: main part of the data is distributed between 0 and 58 min (2092 s)\n* partly heavy outliers (in total 74,2 k outlier)\n* max duration 980 h"},{"metadata":{},"cell_type":"markdown","source":"## Which distances are travelled?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import sin, cos, sqrt, atan2, radians\n\ndef get_distance(lon_1, lon_2, lat_1, lat_2):\n\n    # approximate radius of earth in km\n    R = 6373.0\n\n    lat1 = radians(lat_1)\n    lon1 = radians(lon_1)\n    lat2 = radians(lat_2)\n    lon2 = radians(lon_2)\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n\n    distance = R * c\n\n    return distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"distance\"] = df_train.progress_apply(lambda x: get_distance(x[\"pickup_longitude\"],x[\"dropoff_longitude\"],x[\"pickup_latitude\"],x[\"dropoff_latitude\"]),axis=1)\ndf_test[\"distance\"] = df_test.progress_apply(lambda x: get_distance(x[\"pickup_longitude\"],x[\"dropoff_longitude\"],x[\"pickup_latitude\"],x[\"dropoff_latitude\"]),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,sub = plt.subplots(2,3,figsize=(12,6))\n\nsns.distplot(df_train[\"distance\"], hist_kws={\"edgecolor\":\"black\"}, ax=sub[0][0])\nsns.distplot(np.log1p(df_train[\"distance\"]), hist_kws={\"edgecolor\":\"black\"}, ax=sub[0][1])\nsns.distplot(df_train[df_train[\"distance\"]<15][\"distance\"], hist_kws={\"edgecolor\":\"black\"}, ax=sub[0][2])\n\nprob = stats.probplot(df_train[\"distance\"], dist=stats.norm, plot=sub[1][0])\nprob = stats.probplot(np.log1p(df_train[\"distance\"]), dist=stats.norm, plot=sub[1][1])\nprob = stats.probplot(df_train[df_train[\"distance\"]<15][\"distance\"], dist=stats.norm, plot=sub[1][2])\n\n\ncounter = 0\n\nfor name, subplot in zip([\"distance [raw data]\",\"distance [log]\", \"distance [< 15 km]\",\"probPlot distance [raw data]\",\"probPlot distance [log]\", \"probPlot distance [< 2 h]\"], sub.flatten()):\n    subplot.set_title(\"{}\".format(name))\n    subplot.grid()\n    \n    if counter < 3:\n        if name == \"km [log]\": \n            subplot.set_xlabel(\"distance in log(km)\")\n        else:\n            subplot.set_xlabel(\"distance in km\")\n    \n    counter += 1\n\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Where do the rides take exactly place?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Map where the passengers have been picked up (blue) and dropped off (red)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import folium\nf = folium.Figure(width=1500, height=500)\nmapa = folium.Map(location = (40.7679, -73.9822), zoom_start=11).add_to(f)\n\nfor index, row in df_train.sample(1000).iterrows():\n    folium.Marker([row[\"pickup_latitude\"], row[\"pickup_longitude\"]], icon=folium.Icon(color=\"blue\")).add_to(mapa)\n    folium.Marker([row[\"dropoff_latitude\"], row[\"dropoff_longitude\"]], icon=folium.Icon(color=\"red\")).add_to(mapa)\n\n\ndisplay(mapa)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* the rides take place in the newyork region with focus on manhattan\n* some passengers set out to be dropped off in areas outside newyork or the airport"},{"metadata":{"trusted":true},"cell_type":"code","source":"import folium\nf = folium.Figure(width=1500, height=500)\nmapa = folium.Map(location = (40.7679, -73.9822), zoom_start=11).add_to(f)\n\nfor index, row in df_train[df_train[\"distance\"]>20].sample(200).iterrows():\n    folium.Marker([row[\"pickup_latitude\"], row[\"pickup_longitude\"]], icon=folium.Icon(color=\"blue\")).add_to(mapa)\n    folium.Marker([row[\"dropoff_latitude\"], row[\"dropoff_longitude\"]], icon=folium.Icon(color=\"red\")).add_to(mapa)\n\n\ndisplay(mapa)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* many of the ride > 20 are rides to the or from the airports"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.auto import tqdm\n\ntqdm.pandas()\n\ndf_train[\"pickUp_coordinates\"] = df_train.progress_apply(lambda x: (x[\"pickup_latitude\"], x[\"pickup_longitude\"]), axis=1)\ndf_train[\"dropOff_coordinates\"] = df_train.progress_apply(lambda x: (x[\"dropoff_latitude\"], x[\"dropoff_longitude\"]), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig ,sub = plt.subplots(1,1,figsize=(12,4))\n\nsns.barplot(x = df_train[\"passenger_count\"].value_counts().index, y = df_train[\"passenger_count\"].value_counts().values, ax= sub, palette=\"PuBu_r\")\nsub.grid()\nsub.set_xlabel(\"Passenger per ride\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* most of the rides are people taking a taxi alone"},{"metadata":{},"cell_type":"markdown","source":"# Multivariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = df_train[[\"trip_duration\",\"month\",\"day\",\"hour\",\"distance\"]].corr()\nmask = np.triu(np.ones_like(correlation, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nsns.heatmap(correlation, mask=mask, cmap=cmap, linecolor = \"black\",lw=0.09);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* no linear relationship observable between the features"},{"metadata":{},"cell_type":"markdown","source":"## is the trip duration time-dependent?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, sub = plt.subplots(1,3,figsize=(25,5))\n\nfor name, subplot in zip([\"month\",\"day\", \"hour\"], sub.flatten()):\n    \n    data = df_train.groupby(name)[\"trip_duration\"].mean()\n    sns.barplot(x=data.index, y=data.values, ax=subplot, palette=\"CMRmap\")\n    subplot.grid(color=\"lightgrey\")\n\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Anova"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"ANOVA month/trip-duration: {}\".format(stats.f_oneway(*[np.log1p(df_train[df_train[\"month\"]==feat][\"trip_duration\"]) for feat in df_train[\"month\"].unique()])))\nprint(\"ANOVA day/trip-duration: {}\".format(stats.f_oneway(*[np.log1p(df_train[df_train[\"day\"]==feat][\"trip_duration\"]) for feat in df_train[\"day\"].unique()])))\nprint(\"ANOVA hour/trip-duration: {}\".format(stats.f_oneway(*[np.log1p(df_train[df_train[\"hour\"]==feat][\"trip_duration\"]) for feat in df_train[\"hour\"].unique()])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* in all three time categories at least two means are significantly different from each other"},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import combinations \nhour_list = df_train[\"month\"].unique()\n\nfor feat1, feat2 in combinations(hour_list,2):\n    t, p = stats.ttest_ind(np.log1p(df_train[df_train[\"month\"]==feat1][\"trip_duration\"]),np.log1p(df_train[df_train[\"month\"]==feat2][\"trip_duration\"]))\n    if p > 0.01:\n        print(\"p-value of t-Test between {} and {}: {}\".format(feat1,feat2, np.round(p,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* only month january and february are not signific. different from each other in terms of trip duration"},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import combinations \nhour_list = df_train[\"hour\"].unique()\n\nfor feat1, feat2 in combinations(hour_list,2):\n    t, p = stats.ttest_ind(np.log1p(df_train[df_train[\"hour\"]==feat1][\"trip_duration\"]),np.log1p(df_train[df_train[\"hour\"]==feat2][\"trip_duration\"]))\n    if p > 0.01:\n        print(\"p-value of t-Test between {} and {}: {}\".format(feat1,feat2, np.round(p,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"by night and in the morning, the trip duration is signific. lower than in the time frame between 14-18 h"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, sub = plt.subplots(1,1,figsize=(12,6))\n\nsns.barplot(x=df_train.groupby(\"passenger_count\")[\"trip_duration\"].mean().index, y= df_train.groupby(\"passenger_count\")[\"trip_duration\"].mean(), ax=sub)\nsub.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"ANOVA passenger count/trip-duration: {}\".format(stats.f_oneway(*[np.log1p(df_train[df_train[\"passenger_count\"]==feat][\"trip_duration\"]) for feat in df_train[\"passenger_count\"].unique()])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import combinations \nimport warnings \nwarnings.filterwarnings(\"ignore\")\npassenger_list = df_train[\"passenger_count\"].unique()\n\nfor feat1, feat2 in combinations(passenger_list,2):\n    \n    feat1_data = np.log1p(df_train[df_train[\"passenger_count\"]==feat1][\"trip_duration\"])\n    feat2_data = np.log1p(df_train[df_train[\"passenger_count\"]==feat2][\"trip_duration\"])\n    \n    t, p = stats.ttest_ind(feat1_data, feat2_data)\n    if p > 0.01:\n        print(\"p-value of t-Test between {} and {}: {}\".format(feat1,feat2, np.round(p,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* only the combinations above are not signific. different from each other"},{"metadata":{},"cell_type":"markdown","source":"# First Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nX, y= df_train[[\"passenger_count\",\"pickup_longitude\",\"pickup_latitude\",\"month\",\"day\",\"hour\", \"distance\",\"store_and_fwd_flag\"]], df_train[\"trip_duration\"]\n\nenc = LabelEncoder()\nX[\"store_and_fwd_flag\"] = enc.fit_transform(X[\"store_and_fwd_flag\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold \nkf = KFold(n_splits=5) \nkf.get_n_splits(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import mean_squared_log_error\n\nxgboost_params = { \n   \"objective\": \"reg:squarederror\",\n   \"n_estimators\": 40,\n   \"booster\": \"gbtree\",\n   \"learning_rate\": 0.1,\n   \"subsample\": 0.75,\n   \"colsample_bytree\": 0.68,\n   \"max_depth\": 7\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_dict = {}\n\nfor fold,(train_index, test_index) in enumerate(kf.split(X)):\n\n    print(\"Training the fold {}\".format(fold+1))\n    reg = xgb.XGBRegressor(**xgboost_params)\n    reg.fit(X.values[train_index],y.values[train_index])\n    xgb_preds = abs(reg.predict(X.values[test_index]))\n    fold_dict[fold] = mean_squared_log_error(y.values[test_index], xgb_preds)\n    print(\"Result for fold {}: {}\".format(fold+1, mean_squared_log_error(y.values[test_index], xgb_preds)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"booster = reg.get_booster()\nimportance = booster.get_fscore()\nimp_dict = {X.columns[i]:float(importance.get('f'+str(i),0.)) for i in range(len(X.columns))}\nsorted_importance = {k: v for k, v in sorted(imp_dict.items(), key=lambda item: item[1])}\n\nfig, sub = plt.subplots(1,1,figsize=(12,4))\nsns.barplot(x=list(sorted_importance.keys()),y=list(sorted_importance.values()), ax=sub);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_errors = []\nval_errors = []\n\nfor i in range(1, X_train.shape[0],2*10**5):\n    print(\"Round training from sample 1 to sample {}\".format(i))\n    reg.fit(X_train.iloc[:i],y_train.iloc[:i])\n    train_preds = abs(reg.predict(X_train.iloc[:i]))\n    val_preds = abs(reg.predict(X_val))\n    train_errors.append(mean_squared_log_error(y_train.iloc[:i], train_preds))\n    val_errors.append(mean_squared_log_error(y_val, val_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, sub = plt.subplots(1,1,figsize=(12,6))\n\nsns.lineplot(x=range(1, X_train.shape[0],2*10**5), y=train_errors, label=\"Training Error\")\nsns.lineplot(x=range(1, X_train.shape[0],2*10**5), y=val_errors, label=\"Validation Error\")\n\nsub.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analysis of Learning curve:\n   * Training error and Validation error are converging --> Adding more training data doesn't improve the model\n   * the error level of ~ 0.33 is concerning the ranking not terrible\n   * concerning variance, the model generalizes well on the validation set (the gap between Training and validation error is low and get narrower by adding more training data) "},{"metadata":{},"cell_type":"markdown","source":"### Validation curve Learning Rate "},{"metadata":{"trusted":true},"cell_type":"code","source":"xgboost_params = { \n   \"objective\": \"reg:squarederror\",\n   \"n_estimators\": 40,\n   \"booster\": \"gbtree\",\n   \"subsample\": 0.75,\n   \"colsample_bytree\": 0.68,\n   \"max_depth\": 7\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_errors = []\nval_errors = []\n\nfor i in np.arange(0.05, 1.0, 0.1):\n    print(\"Learning rate {}\".format(i))\n    \n    xgboost_params[\"learning_rate\"] = i \n    \n    reg = xgb.XGBRegressor(**xgboost_params)\n    reg.fit(X_train,y_train)\n    train_preds = abs(reg.predict(X_train))\n    val_preds = abs(reg.predict(X_val))\n    train_errors.append(mean_squared_log_error(y_train, train_preds))\n    val_errors.append(mean_squared_log_error(y_val, val_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, sub = plt.subplots(1,1,figsize=(12,5))\n\nsns.lineplot(x=np.arange(0.05, 1.0, 0.1),y=train_errors, label=\"Training loss\",color=\"blue\", ax=sub)\nsns.lineplot(x=np.arange(0.05, 1.0, 0.1),y=val_errors, label=\"validation loss\",color=\"dimgrey\", ax=sub)\n\nsub.set_xticks(np.arange(0,1.1,0.1))\n\nsub.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, y_train= df_train[[\"passenger_count\",\"pickup_longitude\",\"pickup_latitude\",\"month\",\"day\",\"hour\", \"distance\",\"store_and_fwd_flag\"]], df_train[\"trip_duration\"]\n\nenc = LabelEncoder()\nX_train[\"store_and_fwd_flag\"] = enc.fit_transform(X_train[\"store_and_fwd_flag\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = df_test[[\"passenger_count\",\"pickup_longitude\",\"pickup_latitude\",\"month\",\"day\",\"hour\", \"distance\",\"store_and_fwd_flag\"]]\n\nenc = LabelEncoder()\nX_test[\"store_and_fwd_flag\"] = enc.fit_transform(X_test[\"store_and_fwd_flag\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgboost_params = { \n   \"objective\": \"reg:squarederror\",\n   \"n_estimators\": 40,\n   \"booster\": \"gbtree\",\n   \"learning_rate\": 0.1,\n   \"subsample\": 0.75,\n   \"colsample_bytree\": 0.68,\n   \"max_depth\": 7\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = xgb.XGBRegressor(**xgboost_params)\nreg.fit(X_train,y_train)\nxgb_preds = abs(reg.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = {\"id\":df_sub[\"id\"],\"trip_duration\":xgb_preds}\nf = pd.DataFrame(f)\nf.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}