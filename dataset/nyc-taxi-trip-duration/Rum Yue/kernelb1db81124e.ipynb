{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Rum Yue\n## Exercise on NYC taxi trip duration prediction"},{"metadata":{},"cell_type":"markdown","source":"### 1) Data cleaning\n### 2) Model selection & training"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%HTML\n<button onclick=\"$('.input, .prompt, .output_stderr, .output_error, .output_result').toggle();\">Toggle Code</button>","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Packages"},{"metadata":{"trusted":false},"cell_type":"code","source":"%matplotlib inline\nfrom google.cloud import bigquery\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport seaborn as sns\nimport datetime as dt\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_log_error as msle\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data cleaning"},{"metadata":{"trusted":false},"cell_type":"code","source":"#plt.style.use('fivethirtyeight')\n#plt.rcParams[\"patch.force_edgecolor\"] = True\npd.set_option('display.max_columns', 500)\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/Users/xlyue/Downloads/APMA E 4990-a7a892335efd.json\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def getdata():\n    client = bigquery.Client()\n    pd.set_option('mode.chained_assignment', None)\n\n    query = (\n            \"\"\"\n            SELECT pickup_datetime,pickup_longitude,pickup_latitude,\n            dropoff_longitude, dropoff_latitude, passenger_count,\n            DATETIME_DIFF( dropoff_datetime, pickup_datetime, SECOND) as travel_time\n            FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2016` \n            WHERE pickup_latitude IS NOT NULL AND pickup_longitude IS NOT NULL and dropoff_longitude IS NOT NULL \n            ORDER BY RAND()\n            LIMIT 1500000\"\"\"\n            )\n    df=pd.io.gbq.read_gbq(query,dialect='standard')\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#get train data \nraw=getdata()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train=raw.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Latitude and Longitude\n\nIn this part, we removed data whose location is outside of NYC based on latitude and longitude."},{"metadata":{"trusted":false},"cell_type":"code","source":"#limit latitude and longitude within the range of nyc (based on google map)\ndef lat_long(train):\n    train = train[(train.pickup_longitude > -74.25) & (train.pickup_longitude < -73.70)]\n    train = train[(train.pickup_latitude > 40.49) & (train.pickup_latitude < 40.91)]\n    train = train[(train.dropoff_longitude > -74.25) & (train.dropoff_longitude < -73.70)]\n    train = train[(train.dropoff_latitude > 40.49) & (train.dropoff_latitude < 40.91)]\n    \n    #show the plot of pickup locations\n    p_lng = train['pickup_longitude'].values\n    p_lat = train['pickup_latitude'].values\n    d_lng = train['dropoff_longitude'].values\n    d_lat = train['dropoff_latitude'].values\n    \n    plt.figure(figsize = (12, 8))\n    plt.grid(False)\n    plt.rcParams['savefig.dpi'] = 300\n    #plt.rcParams['figure.dpi'] = 300\n    plt.plot(p_lng, p_lat, '.', alpha = .4, markersize = .8)\n    plt.plot(d_lng, d_lat, '.', alpha = .4, markersize = .8)\n\n    plt.title('Pickup & Dropoff Trip Plots')\n    plt.show()\n    \n    return train","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"train=lat_long(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Passenger count \nDiscard records with zero passengers and nine passengers since these two cases are unreasonable both for taxi trip and policy."},{"metadata":{"trusted":false},"cell_type":"code","source":"def passenger(train):\n    train = train[(train.passenger_count > 0) & (train.passenger_count < 9)]\n    \n    return train","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train = passenger(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Travel time\n\nOnly kept data whose travel time is larger than 10 seconds and less than 8 hours(Longest uber trip in NYC histroy: https://nypost.com/2016/12/10/the-longest-ride-in-uber-history/). And we found that travel time subject ot lognormal distribution. \n\nBecause difference between travel times is huge (some trips only took few minutes while others took hours), we took its log value as our feature."},{"metadata":{"trusted":false},"cell_type":"code","source":"#show the distribution of travel time, remove outliers\ndef travel_time(data):\n    #remove data whose travel time is less than 10 seconds\n    data=data[data['travel_time']>10]\n    #remove data whose travel time is more than 8 hours\n    data=data[data['travel_time']<30000]\n    plt.figure(figsize=(12,6))\n    plt.grid(False)\n    plt.hist(data['travel_time'],bins=500, label = 'trip duration', rwidth = 0.9)\n    plt.xlabel('travel time (seconds)')\n    plt.xlim(-100,6000)\n    plt.title('travel time distribution')\n    plt.legend()\n    plt.show()\n    \n    plt.figure(figsize=(12,6))\n    data['travel_time']=np.log(data['travel_time'].copy())\n    plt.grid(False)\n    plt.hist(data['travel_time'],bins=100, label = 'trip duration', rwidth = 0.9)\n    plt.xlabel('travel time (log seconds)')\n    plt.title('travel time distribution (after taking log)')\n    plt.legend()\n    plt.show()\n    return data\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train=travel_time(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Time features: hours, week and month\n\nIn this section we showed that time in a day is an important feature, while weeday and month are not."},{"metadata":{"trusted":false},"cell_type":"code","source":"#analysis the distribution among time\ndef time(train):\n    train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime).copy()\n    train['month'] = train.pickup_datetime.dt.month.copy()\n    train['week'] = train.pickup_datetime.dt.week.copy()\n    train['weekday'] = train.pickup_datetime.dt.weekday.copy()\n    train['hour'] = train.pickup_datetime.dt.hour.copy()\n    train['minute'] = train.pickup_datetime.dt.minute.copy()\n    train['minute_oftheday'] = train['hour'] .copy()* 60 + train['minute'].copy()\n    train.drop(['minute'], axis=1, inplace=True)\n\n    \n    wkd_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    plt.figure(figsize = (12, 3))\n    data_wkd = train.groupby('weekday').aggregate({'pickup_datetime': 'count'}).reset_index()\n    sns.barplot(x = 'weekday', y = 'pickup_datetime', data = data_wkd)\n    plt.title('amount of trip records distribution')\n    plt.xlabel('weekday')\n    plt.xticks(range(0, 7), wkd_names)\n    plt.ylabel('amount of trips')\n    plt.show()\n    \n    mt_names = ['January', 'February', 'March', 'April', 'May', 'June']\n    plt.figure(figsize = (12, 3))\n\n    data_mt = train.groupby('month').aggregate({'pickup_datetime': 'count'}).reset_index()\n    sns.barplot(x = 'month', y = 'pickup_datetime', data = data_mt)\n\n    plt.title('amount of trip records distribution')\n    plt.xlabel('month')\n    plt.xticks(range(0, 6), mt_names)\n    plt.ylabel('amount of trips')\n    plt.show()\n    plt.figure(figsize = (12, 3))\n\n    data_h = train.groupby('hour').aggregate({'pickup_datetime': 'count'}).reset_index()\n    sns.barplot(x = 'hour', y = 'pickup_datetime', data = data_h)\n\n    plt.title('amount of trip records distribution')\n    plt.xlabel('hour of a day: 0 - 23')\n    plt.ylabel('amount of trips')\n    plt.show()\n    return train","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"train=time(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the 3 plots above we can see that the amount of trips does not vary much across weekdays and months, but does flunctuate within one day.\nSo we take time in a day as a feature."},{"metadata":{"trusted":false},"cell_type":"code","source":"#heatmap for hour & weekday\ndef heatmap(train):\n    plt.figure(figsize = (12, 3))\n    \n    sns.heatmap(data = pd.crosstab(train['weekday'], train['hour'], values = train.pickup_datetime,\n                                   aggfunc = 'count', normalize = 'index'), cmap='BuPu')\n    wkd_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    plt.title('trip amount heatmap of weekday & hour')\n    plt.xlabel('hour of a day: 0 - 23')\n    plt.ylabel('weekday')\n    plt.yticks(range(0, 7), wkd_names, rotation = '20')\n    plt.show()\n    #heatmap for weekday & month\n    mt_names = ['January', 'February', 'March', 'April', 'May', 'June']\n    fig1=plt.figure(figsize = (12, 3))\n    sns.heatmap(data = pd.crosstab(train['month'], train['weekday'], values = train.pickup_datetime, \n                                   aggfunc = 'count', normalize = 'index'), cmap='Blues')\n    plt.title('trip amount heatmap of month & weekday')\n    plt.xlabel('weekday')\n    plt.xticks(range(0, 7), wkd_names, rotation = '10')\n    plt.ylabel('month')\n    plt.yticks(range(0, 6), mt_names, rotation = '0')\n    plt.show()\n    train.drop(['pickup_datetime'], axis=1, inplace=True)\n    train.drop(['hour'],axis=1,inplace=True)\n    \n    return train","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"train=heatmap(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distance & Speed\n\nCalculated Haversine distance based on longitudes and latitudes. Removed distance that is larger than the diameter of NYC (60 km) or is too short (less than 10 meters). Demonstrate the distance distribution.\n\nCalculated approximating speed based on Haversine distance and travel time. The aim is to remove outliers whose speed is abnormally large. Those outliers might elude previous cleaning because their travel time and distance seem normal when judging seperately."},{"metadata":{"trusted":false},"cell_type":"code","source":"def haversine(lat1, lon1, lat2, lon2):\n    r=6371.0 # average radius of earth\n    lat1, lon1, lat2, lon2 = np.radians([lat1, lon1, lat2, lon2])\n    a = np.sin((lat2-lat1)/2.0)**2 + \\\n        np.cos(lat1) * np.cos(lat2) * np.sin((lon2-lon1)/2.0)**2\n    distance=2 * r * np.arcsin(np.sqrt(a))\n    return distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#compute distance based on longitudes and latitudes, show the distribution of distance, remove outliers\ndef distance(train):\n    train['distance'] = haversine(train['pickup_latitude'].values.copy(), train['pickup_longitude'].values.copy(),\\\n                                  train['dropoff_latitude'].values.copy(), train['dropoff_longitude'].values.copy())\n    #the diameter of new york city is less than 60 km \n    train=train[train['distance']<60]\n    train=train[train['distance']>0.01]\n    plt.figure(figsize = (12, 8))\n    plt.hist(train['distance'], bins=200,label = 'distance', rwidth = 0.9)\n    plt.xlabel('Direct distance between pickup and dropoff locations(km)')\n    plt.ylabel('Count')\n    plt.title('Distribution of distance')\n    plt.legend()\n    plt.show()\n    \n    train['speed']= train.distance.copy() / (np.exp(train.travel_time.copy())/3600.0)\n    train=train[train['speed']<500]\n    plt.figure(figsize=(12,8))\n    plt.hist(train.speed, bins=500,label = 'speed', rwidth = 0.9)\n    plt.xlim(-10,300)\n    plt.xlabel('Approximating average travel speed (km/h)')\n    plt.ylabel('Count')\n    plt.title('Distribution of speed')\n    plt.legend()\n    plt.show()\n    train.drop(['speed'], axis=1, inplace=True)\n    return train","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"trian=distance(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Direction feature\n\nThe direction from pickup point to dropoff point."},{"metadata":{"trusted":false},"cell_type":"code","source":"def ft_degree(lat1, lng1, lat2, lng2):\n    #r = 6371 #km\n    lng_delta_rad = np.radians(lng2 - lng1)\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    y = np.sin(lng_delta_rad) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n    \n    return np.degrees(np.arctan2(y, x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Direction feature\ndef direction(train):\n    train['direction'] = ft_degree(train['pickup_latitude'].copy().values, train['pickup_longitude'].copy().values, \\\n                               train['dropoff_latitude'].copy().values, train['dropoff_longitude'].copy().values)\n    return train","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train=direction(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def show_corr(train):\n    plt.figure(figsize = (18, 6))\n    features=train.copy()\n    sns.heatmap(data = features.corr(), annot = True, cmap = plt.cm.RdYlBu_r, linewidths=.1)\n\n    plt.title('Correlations between variables')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"show_corr(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test data preprocessing \n\nGenerate features we need."},{"metadata":{"trusted":false},"cell_type":"code","source":"def preprocess(test):\n    test['pickup_datetime'] = pd.to_datetime(test.pickup_datetime).copy()\n    test['month'] = test.pickup_datetime.dt.month.copy()\n    test['week'] = test.pickup_datetime.dt.week.copy()\n    test['weekday'] = test.pickup_datetime.dt.weekday.copy()\n    test['hour'] = test.pickup_datetime.dt.hour.copy()\n    test['minute'] = test.pickup_datetime.dt.minute.copy()\n    test['minute_oftheday'] = test['hour'] .copy()* 60 + test['minute'].copy()\n    test.drop(['minute'], axis=1, inplace=True)\n    test.drop(['hour'], axis=1, inplace=True)\n    test.drop(['pickup_datetime'], axis=1, inplace=True)\n    test['distance'] = haversine(test['pickup_latitude'].values.copy(), test['pickup_longitude'].values.copy(),\n                                  test['dropoff_latitude'].values.copy(), test['dropoff_longitude'].values.copy())\n    test['direction']= ft_degree(test['pickup_latitude'].copy().values, test['pickup_longitude'].copy().values, \n                               test['dropoff_latitude'].copy().values, test['dropoff_longitude'].copy().values)\n    return test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test=preprocess(pd.read_csv('APM4990_final_test_data_filtered.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model selection & training"},{"metadata":{},"cell_type":"markdown","source":"### Model preparation"},{"metadata":{"trusted":false},"cell_type":"code","source":"def data_split(train):\n    y = train['travel_time']\n    train.drop(['travel_time'], axis = 1, inplace =True)\n    X = train.copy()\n    z_1 = train.copy()\n    z_2 = y.copy()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n    X_train_2, X_dev, y_train_2, y_dev = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)\n    \n    return X_train, X_test, y_train, y_test, X_train_2, X_dev, y_train_2, y_dev, z_1, z_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d_set = data_split(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multivariate linear model"},{"metadata":{"trusted":false},"cell_type":"code","source":"def linear_model(d_set):\n    lr = LinearRegression()\n    lr.fit(d_set[0], d_set[2])\n    train_sc = lr.score(d_set[0], d_set[2])\n    test_sc = lr.score(d_set[1], d_set[3])\n    y_pred_lr = lr.predict(d_set[1])\n    rmse = np.sqrt(mse(d_set[3], y_pred_lr))\n    v_exp = metrics.explained_variance_score(d_set[3], y_pred_lr)\n    R_2 = metrics.r2_score(d_set[3], y_pred_lr)\n    rmsle = np.sqrt(msle(d_set[3], y_pred_lr))\n    \n    print('R^2 score is {}, rmse is {}, rmsle is {}'.format(R_2, rmse, rmsle))\n    print( )\n    print('linear training model score is {}, test model score is {}'.format(train_sc, test_sc))\n    print( )\n    print('{} of the variation in the dependent variable is explained by multivariate linear model'.format(v_exp))\n    \n    plt.plot(d_set[3], y_pred_lr, label = 'fit')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"linear_model(d_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusion \nThe model score of test set does not indicate obvious overfitting. "},{"metadata":{},"cell_type":"markdown","source":"### Random forest model"},{"metadata":{"trusted":false},"cell_type":"code","source":"def rfr(d_set):\n    rf = RandomForestRegressor(n_estimators=200, max_depth = 50, min_samples_split = 10, n_jobs = -1)\n    rf.fit(d_set[0], d_set[2])\n    train_sc = rf.score(d_set[0], d_set[2])\n    test_sc = rf.score(d_set[1], d_set[3])\n    y_pred_rf = rf.predict(d_set[1])\n    rmse = np.sqrt(mse(d_set[3], y_pred_rf))\n    v_exp = metrics.explained_variance_score(d_set[3], y_pred_rf)\n    R_2 = metrics.r2_score(d_set[3], y_pred_rf)\n    rmsle = np.sqrt(msle(d_set[3], y_pred_rf))\n    \n    print('R^2 score is {}, rmse is {}, rmsle is {}'.format(R_2, rmse, rmsle))\n    print( )\n    print('Random forest training model score is {}, test model score is {}'.format(train_sc, test_sc))\n    print( )\n    print('{} of the variation in the dependent variable is explained by random forest model'.format(v_exp))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nrfr(d_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluation on Random forest model\nRadomforest regressor takes too much time for fitting the training data set, even with original setting parameters. It is unreasonable to tune parameters using this model if we want to improve efficiency in the following sections."},{"metadata":{},"cell_type":"markdown","source":"### Light Gradient Boosting Machine (LightGBM)"},{"metadata":{},"cell_type":"markdown","source":"#### Fastness and Cross Validation (in k = 5 folders)"},{"metadata":{"trusted":false},"cell_type":"code","source":"def light(d_set):\n    lgbR = LGBMRegressor()\n    lgbR.fit(d_set[0], d_set[2])\n    train_sc = lgbR.score(d_set[0], d_set[2])\n    test_sc = lgbR.score(d_set[1], d_set[3])\n    y_pred_lgbR = lgbR.predict(d_set[1])\n    rmse = np.sqrt(mse(d_set[3], y_pred_lgbR))\n    v_exp = metrics.explained_variance_score(d_set[3], y_pred_lgbR)\n    R_2 = metrics.r2_score(d_set[3], y_pred_lgbR)\n    rmsle = np.sqrt(msle(d_set[3], y_pred_lgbR))\n    \n    print('R^2 score is {}, rmse is {}, rmsle is {}'.format(R_2, rmse, rmsle))\n    print( )\n    print('LightGBM training model score is {}, test model score is {}'.format(train_sc, test_sc))\n    print( )\n    print('{} of the variation in the dependent variable is explained by random forest model'.format(v_exp))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nlight(d_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def light_cv(d_set):\n    lgbR = LGBMRegressor()\n    cv_score_lgb = cross_val_score(lgbR, d_set[8], d_set[9], cv = 5, n_jobs = -1)\n    print('cross validation score for Lightgbm is {}'.format(cv_score_lgb))\n    print( )\n    print('average cv score {} indicates stability'.format(np.mean(cv_score_lgb)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"light_cv(d_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Hyperparameter tuning part I \n#### Pre training"},{"metadata":{"trusted":false},"cell_type":"code","source":"#easy example for several important parameters tuning of lightgbm\ndef pre_tr(d_set):\n    lgbR = LGBMRegressor()\n    n_estimators = np.arange(80, 1200, 100)\n    scores_1 = []\n    for n in n_estimators:\n        lgbR.set_params(n_estimators = n)\n        lgbR.fit(d_set[0], d_set[2])\n        scores_1.append(lgbR.score(d_set[1], d_set[3]))\n\n    plt.plot(n_estimators, scores_1)\n    plt.title(\"Effect of n_estimators\")\n    plt.xlabel(\"n_estimator\")\n    plt.ylabel(\"R^2 score\")\n    plt.show()\n\n    max_depth = np.arange(8, 99, 10)\n    scores_2 = []\n    for n in max_depth:\n        lgbR.set_params(max_depth = n)\n        lgbR.fit(d_set[0], d_set[2])\n        scores_2.append(lgbR.score(d_set[1], d_set[3]))\n\n    plt.plot(max_depth, scores_2)\n    plt.title(\"Effect of max_depth\")\n    plt.xlabel(\"max_depth\")\n    plt.ylabel(\"R^2 score\")\n    plt.show()\n\n    num_leaves = np.arange(80, 1100, 100)\n    scores_3 = []\n    for n in num_leaves:\n        lgbR.set_params(num_leaves = n)\n        lgbR.fit(d_set[0], d_set[2])\n        scores_3.append(lgbR.score(d_set[1], d_set[3]))\n\n    plt.plot(num_leaves, scores_3)\n    plt.title(\"Effect of num_leaves\")\n    plt.xlabel(\"num_leaves\")\n    plt.ylabel(\"R^2 score\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pre_tr(d_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Hyperparameter tuning part II\n#### Monte Carlo Method\n$n \\geq 1000$ iterations leads to at least $1 - \\frac{1}{\\sqrt{n}} \\approx 97\\%$ accuracy to achieve optimal model score.\nTune parameters based on above range and references: https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html"},{"metadata":{"trusted":false},"cell_type":"code","source":"def mc_tuning(d_set):\n    #number of leaves \\leq 2^(max_depth)\n    num_iterations = 200\n    results_lgb = pd.DataFrame(columns=[\"rmse\", \"n_estimators\", \"num_leaves\", \"max_depth\", \"feature_fraction\", \"bagging_fraction\", \"bagging_freq\",\n                                    \"learning_rate\", \"model score\"])\n\n    for i in range(num_iterations):\n        num_leaves = np.random.randint(400, 950)\n        max_depth = np.random.randint(12, 35)\n        feature_fraction = np.random.uniform(0.5,1)\n        bagging_fraction = np.random.uniform(0.4,1)\n        bagging_freq = np.random.randint(3,10)\n        learning_rate = np.random.uniform(0.1,0.999)\n        n_estimators = np.random.randint(999, 1300)\n        early_stopping_rounds = 5\n\n        lgb_search = LGBMRegressor(objective = 'regression', boosting_type = 'gbdt', num_leaves = num_leaves, max_depth = max_depth,\n                            feature_fraction = feature_fraction,\n                            bagging_fraction = bagging_fraction,\n                            bagging_freq = bagging_freq,\n                            learning_rate = learning_rate,\n                            n_estimators = n_estimators)\n    \n        lgb_search.fit(d_set[4], d_set[6],\n            eval_set = [(d_set[5], d_set[7])],\n            eval_metric = 'rmse', early_stopping_rounds = early_stopping_rounds)\n    \n        model_score = lgb_search.score(d_set[1], d_set[3])\n        y_preds_lgb = lgb_search.predict(d_set[1], num_iteration = lgb_search.best_iteration_)\n        rmse_lgb = np.sqrt(mse(d_set[3], y_preds_lgb))\n        if (d_set[3] < 0).any() or (y_preds_lgb < 0).any():\n            rmsle_lgb = 0\n        else:\n            rmsle_lgb = np.sqrt(msle(d_set[3], y_preds_lgb))\n        \n        print(\"the {0}th iteration has score {1} with rmse {2} and rmsle {3}.\".format(i+1, model_score, rmse_lgb, rmsle_lgb))\n        print()\n    \n        each_row = pd.DataFrame([[rmse_lgb, rmsle_lgb, n_estimators, num_leaves, max_depth, feature_fraction, bagging_fraction,\n                             bagging_freq,\n                             learning_rate, model_score]],\n                         columns = [\"rmse\", \"rmsle\", \"n_estimators\", \"num_leaves\", \"max_depth\", \"feature_fraction\", \"bagging_fraction\",\n                                \"bagging_freq\",\n                                \"learning_rate\", \"model score\"])\n    \n# Append the dataframe as a new row in result\n        results_lgb = results_lgb.append(each_row, ignore_index=True)\n    \n    return results_lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"result = mc_tuning(d_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def sc_rank(result):\n    ind = np.argmax(result['model score'].values) \n    \n    return result.iloc[[ind]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sc_rank(result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Check improvement"},{"metadata":{"trusted":false},"cell_type":"code","source":"def light_opt(d_set):\n    lgbR_opt = LGBMRegressor(objective = 'regression',\n                            boosting_type = 'gbdt',\n                            n_estimators = 1239, num_leaves = 791,\n                            max_depth = 17,\n                            feature_fraction = 0.66211,\n                            bagging_fraction = 0.833928,\n                            bagging_freq = 6,\n                            learning_rate = 0.137911)\n    lgbR_opt.fit(d_set[0], d_set[2])\n    train_sc = lgbR_opt.score(d_set[0], d_set[2])\n    test_sc = lgbR_opt.score(d_set[1], d_set[3])\n    y_pred_lgbR = lgbR_opt.predict(d_set[1])\n    rmse = np.sqrt(mse(d_set[3], y_pred_lgbR))\n    v_exp = metrics.explained_variance_score(d_set[3], y_pred_lgbR)\n    R_2 = metrics.r2_score(d_set[3], y_pred_lgbR)\n    rmsle = np.sqrt(msle(d_set[3], y_pred_lgbR))\n    \n    print('R^2 score is {}, rmse is {}, rmsle is {}'.format(R_2, rmse, rmsle))\n    print( )\n    print('LightGBM training model score is {}, test model score is {}'.format(train_sc, test_sc))\n    print( )\n    print('{} of the variation in the dependent variable is explained by random forest model'.format(v_exp))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"light_opt(d_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"lgb.plot_importance(lgbR_opt,  max_num_features=28, figsize=(8, 10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction"},{"metadata":{"trusted":false},"cell_type":"code","source":"def test_predict(test, d_set):\n    lgbR_opt = LGBMRegressor(objective = 'regression',\n                            boosting_type = 'gbdt',\n                            n_estimators = 1239, num_leaves = 791,\n                            max_depth = 17,\n                            feature_fraction = 0.66211,\n                            bagging_fraction = 0.833928,\n                            bagging_freq = 6,\n                            learning_rate = 0.137911)\n    lgbR_opt.fit(d_set[8], d_set[9])\n    predictions = lgbR_opt.predict(test[test.columns])\n    test['predictions'] = np.exp(predictions)\n    return test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission = test_predict(test, d_set)\nsubmission.to_csv(\"test_predictions\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}