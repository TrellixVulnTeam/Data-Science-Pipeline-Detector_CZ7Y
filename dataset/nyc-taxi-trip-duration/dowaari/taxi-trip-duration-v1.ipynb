{"cells":[{"metadata":{},"cell_type":"markdown","source":"해당 커널은 아래 커널을 참고하여 공부한 내용입니다. <br>\nhttps://www.kaggle.com/gaborfodor/from-eda-to-the-top-lb-0-367\n\n```\n이번 대회에서 카글은 뉴욕에서의 택시 여행의 총 운행 기간을 예측하는 모델을 만드는 데 도전하고 있다.<br>\n당신의 기본 데이터 세트는 픽업 시간, 지리 좌표, 승객 수 및 기타 여러 변수가 포함된다.\n\n- id - 각 운행에 대한 id\n- vendor_id - 운행기록 제공자(회사)에 대한 id\n- pickup_datetime - date and time when the meter was engaged\n- dropoff_datetime - date and time when the meter was disengaged\n- passenger_count - the number of passengers in the vehicle\n- pickup_longitude - the longitude where the meter was engaged\n- pickup_latitude - the latitude where the meter was engaged\n- dropoff_longitude - the longitude where the meter was disengaged\n- dropoff_latitude - the latitude where the meter was disengaged\n- store_and_fwd_flag \n    - whether the trip record was held in vehicle memory before sending <br>\n      to the vendor because the vehicle did not have a connection to the server \n    - Y=store and forward; N=not a store and forward trip\n- trip_duration - duration of the trip in seconds (운행기간 단위는 초)\n```"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# engine='c', parse_dates, infer_datetime_format\ntrain = pd.read_csv('../input/train.csv') \ntest = pd.read_csv('../input/test.csv')\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.columns)\nprint(test.columns) # dropoff_datetime, trip_duration 칼럼이 없다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.setdiff1d(train.columns, test.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"```\n# 전처리\n- 결측값은 없는가? \n    - 결측값없음. (추후 파생특성에서 결측값 발생시?)\n- id를 제외한 문자열은 없는가? \n    - store_and_fwd_flag 문자열은 라벨인코딩한다.(Y/N)\n    - pickup_datetime, dropoff_datetime은 datetime형식으로 변환한다.\n- 경도/위도의 값이 과도하게 큰 값은 없는가? 처리하지않아도 문제없는가?\n    - 그래프 표시할경우 평균값에 +-0.2를 한다.\n    - city_long_border = (-74.03, -73.75)\n    - city_lat_border = (40.63, 40.85)\n- passenger_count가 0명 값은 ? \n    - 승객이 타지않은 상태의 정보, train,test 모두 존재, 그냥 나둔다. \n- trip_duration 과도하게 큰 값은 없는가? \n    - 삭제한다.\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train내 id중복없음 :', train['id'].nunique() == train.shape[0])\n# id의 유니크 개수와 행개수가 동일하면 id중복은 없다는 것.\nprint('train과 test의 id중복없음 :', len(np.intersect1d(train.id.values, test.id.values)) == 0)\n# train과 test의 id가 중복없음.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.count().min() == train.shape[0])\nprint(test.count().min() == test.shape[0])\n# missing value없음.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.store_and_fwd_flag.unique())\nprint(test.store_and_fwd_flag.unique())\nprint(train['vendor_id'].unique())\nprint(test['vendor_id'].unique())\n# 해당 칼럼은 값이 2개","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['store_and_fwd_flag'] = 1 * (train.store_and_fwd_flag.values == 'Y')\ntest['store_and_fwd_flag'] = 1 * (test.store_and_fwd_flag.values == 'Y')\n# 문자 라벨 인코딩 Y는 1, N은 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['pickup_datetime']= pd.to_datetime(train['pickup_datetime'])\ntrain['dropoff_datetime']= pd.to_datetime(train['dropoff_datetime'])\ntest['pickup_datetime']= pd.to_datetime(test['pickup_datetime'])\n# 시간변수를 문자열에서 datetime으로 변환, csv에서 불러올때 바로 변환하는 방법도 있다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['pickup_date']= train['pickup_datetime'].dt.date\ntest['pickup_date']= test['pickup_datetime'].dt.date\n# 시,분,초을 제외한 time추출 (2016-05-01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 지역정보, 경도/위도에 대해 살펴본다.\ntrain[['pickup_longitude','dropoff_longitude', 'pickup_latitude', 'dropoff_latitude']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['idx']= train.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# longitude 값의 분포는?\nfig, ax = plt.subplots(ncols=2, sharey=True)\ntrain.plot.scatter(x='idx', y='pickup_longitude', ax=ax[0])\ntrain.plot.scatter(x='idx', y='dropoff_longitude', ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# latitude 값의 분포는?\nfig, ax = plt.subplots(ncols=2, sharey=True)\ntrain.plot.scatter(x='idx', y='pickup_latitude', ax=ax[0])\ntrain.plot.scatter(x='idx', y='dropoff_latitude', ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 위의 그래프를 확인했을때\n#  -80 < long < -60\n#  32 < lat < 45\n# 평균치 73.97 , 40.75 +-0.2 (그래프 확인시)\nlong_m= -73.97\nlat_m= 40.75\ncity_long_border = (long_m-0.2, long_m+0.2)\ncity_lat_border = (lat_m-0.2, lat_m+0.2) \n# city_long_border = (-74.03, -73.75)\n# city_lat_border = (40.63, 40.85) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N = 100000\nfig,ax= plt.subplots(ncols=2, sharex=True, sharey=True)\nax[0].scatter(train['pickup_longitude'].values[:N], train['pickup_latitude'].values[:N], \n              color='b', s=1, label='train', alpha=0.5)\nax[1].scatter(test['pickup_longitude'].values[:N], test['pickup_latitude'].values[:N], \n              color='g', s=1, label='train', alpha=0.5)\n\nplt.xlim(city_long_border) # long\nplt.ylim(city_lat_border) # lat\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 다시 조정\ncity_long_border = (-74.03, -73.75)\ncity_lat_border = (40.63, 40.85) \n\nN = 100000\nfig,ax= plt.subplots(ncols=2, sharex=True, sharey=True)\nax[0].scatter(train['pickup_longitude'].values[:N], train['pickup_latitude'].values[:N], \n              color='b', s=1, label='train', alpha=0.5)\nax[1].scatter(test['pickup_longitude'].values[:N], test['pickup_latitude'].values[:N], \n              color='g', s=1, label='train', alpha=0.5)\n\nplt.xlim(city_long_border) # long\nplt.ylim(city_lat_border) # lat\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['trip_duration'].min())\nprint(train['trip_duration'].max() / 3600)\ntrain.plot.scatter(x='idx', y='trip_duration')\n# 1초??, 979시간?? 아웃밸류임.\n# Fortunately the evaluation metric is RMSLE and not RMSE . \n# Outliers will cause less trouble. We could logtransform our target label and use RMSE during training.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 운행시간 48시간 이상 삭제 \nIdx = train[train['trip_duration'] > 48*3600].index\ntrain= train.drop(Idx, axis=0)\n# train= train[train['trip_duration'] < 48*3600]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Extraction\n- Extract 59 useful feature\n- 기존 feature 7개\n     1. ~~id~~\n     - vendor_id\n     - ~~pickup_datetime~~\n     - passenger_count\n     - pickup_longitude\n     - pickup_latitude\n     - dropoff_longitude\n     - dropoff_latitude\n     - store_and_fwd_flag\n     \n\n- 추가 feature  \n    1. 승차/하차 장소,지역 (8)\n        - 추가: 승차/하차 PCA변환_경도/위도 (4)\n        - 추가: 승차/하차 클러스터링 경도/위도 (4)   \n        - 추가: ~~승차/하차 경도/위도 bin~~ \n    - 운행거리 (6)\n        - 추가: 하버사인, 더미, 베어링, PCA연산 (4)\n        - 추가: 센터경도/위도 (2)\n        - 추가: ~~센터경도/위도 bin~~\n    - 승차시간 (5)\n        - 추가: 주일, 주년, ~~일~~, 시, 분, 초, 주시\n        - 추가: ~~초bin~~    \n    - 운행속도 (0) -> 운행거리 / 운행시간\n        - 추가: ~~하버사인 평속, 더미 평속, 로그운행시간~~  \n    - 장소와 시간에 따른 평균운행속도 (23)\n        - 추가: 시, 일, 초bin, 주시, 클러스터승차/하차에 따른 하버사인 평속 (6) \n        - 추가: 위 동일,에 따른 더미 평속 (6)\n        - 추가: 위 동일,에 따른 운행 시간 (6)\n        - 추가: 센터경도+위도, 시+센터경도+위도, 시+클러스터승차, 시+클러스터하차 (4)\n        - 추가: 클러스터승차+클러스터하차 하버사인 평속 (1)\n    - 운행횟수 (7)\n        - 추가: 위 동일, id횟수 (5)\n        - 추가: ~~pickup_datetime_group~~\n        - 추가: dropoff_cluster_count\n        - 추가: pikup_cluster_count\n    - OSMR 피쳐, 외부데이타 (3)\n        - 추가: 총 거리, 총 운행시간, 스텝수 (???)"},{"metadata":{"trusted":true},"cell_type":"code","source":"7+8+6+5+23+7+3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 위도/경도를 PCA값으로 변환 (2차원->2차원)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nimport warnings\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coords= np.vstack((train[['pickup_latitude', 'pickup_longitude']].values,\n          train[['dropoff_latitude', 'dropoff_longitude']].values,\n          test[['pickup_latitude', 'pickup_longitude']].values,\n          test[['dropoff_latitude', 'dropoff_longitude']].values         \n         ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coords.shape # 수직으로 쌓다. 행방향으로 쌓았음. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA().fit(coords)\n# PCA 주성분 분석\n# (1) 특성추출: 특성들이 통계적으로 상관관계가 없도록 데이터셋을 회전하는 기술 (2차원에서 2차원 또는 1차원으로 변환)\n# (2) 차원축소 : 고차원의 데이터셋 시각화 (10차원에서 3차원~1차원으로 변환)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PCA 지역정보\n# 기존 4개 칼럼 -> PCA로 변환된 칼럼 4개\ntrain['pickup_pca0'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 0]\ntrain['pickup_pca1'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 1]\ntrain['dropoff_pca0'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\ntrain['dropoff_pca1'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\ntest['pickup_pca0'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 0]\ntest['pickup_pca1'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 1]\ntest['dropoff_pca0'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\ntest['dropoff_pca1'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2)\nax[0].scatter(train['pickup_longitude'].values[:N], train['pickup_latitude'].values[:N],\n              color='blue', s=1, alpha=0.1)\nax[1].scatter(train['pickup_pca0'].values[:N], train['pickup_pca1'].values[:N],\n              color='green', s=1, alpha=0.1)\nax[0].set_xlim(city_long_border)\nax[0].set_ylim(city_lat_border)\npca_borders = pca.transform([[x, y] for x in city_lat_border for y in city_long_border])\nax[1].set_xlim(pca_borders[:, 0].min(), pca_borders[:, 0].max())\nax[1].set_ylim(pca_borders[:, 1].min(), pca_borders[:, 1].max())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 클러스터링 지역정보, 2가지 컬럼\nsample_ind = np.random.permutation(len(coords))[:500000]\nkmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind])\n\ntrain.loc[:, 'pickup_cluster'] = kmeans.predict(train[['pickup_latitude', 'pickup_longitude']])\ntrain.loc[:, 'dropoff_cluster'] = kmeans.predict(train[['dropoff_latitude', 'dropoff_longitude']])\ntest.loc[:, 'pickup_cluster'] = kmeans.predict(test[['pickup_latitude', 'pickup_longitude']])\ntest.loc[:, 'dropoff_cluster'] = kmeans.predict(test[['dropoff_latitude', 'dropoff_longitude']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=1, nrows=1)\nax.scatter(train.pickup_longitude.values[:N], train.pickup_latitude.values[:N], s=10, lw=0,\n           c=train.pickup_cluster[:N].values, cmap='tab20', alpha=0.2)\nax.set_xlim(city_long_border)\nax.set_ylim(city_lat_border)\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 경도/위도 bin, 센터 경도/위도 bin, 지역정보 4가지 (추후 입력특성으로 사용하지 않음.), long경도, lat위도\ntrain.loc[:, 'pickup_lat_bin'] = np.round(train['pickup_latitude'], 2)\ntrain.loc[:, 'pickup_long_bin'] = np.round(train['pickup_longitude'], 2)\ntest.loc[:, 'pickup_lat_bin'] = np.round(test['pickup_latitude'], 2)\ntest.loc[:, 'pickup_long_bin'] = np.round(test['pickup_longitude'], 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 거리정보를 구해보자.\ndef haversine_array(lat1, lng1, lat2, lng2):\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    AVG_EARTH_RADIUS = 6371  # in km\n    lat = lat2 - lat1\n    lng = lng2 - lng1\n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n    return h\n\ndef dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n    a = haversine_array(lat1, lng1, lat1, lng2)\n    b = haversine_array(lat1, lng1, lat2, lng1)\n    return a + b\n\ndef bearing_array(lat1, lng1, lat2, lng2):\n    AVG_EARTH_RADIUS = 6371  # in km\n    lng_delta_rad = np.radians(lng2 - lng1)\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    y = np.sin(lng_delta_rad) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n    return np.degrees(np.arctan2(y, x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 거리 4가지, train\ntrain.loc[:, 'distance_haversine'] = haversine_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\ntrain.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\ntrain.loc[:, 'direction'] = bearing_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\ntrain.loc[:, 'pca_manhattan'] = np.abs(train['dropoff_pca1'] - train['pickup_pca1']) + np.abs(train['dropoff_pca0'] - train['pickup_pca0'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 거리 4가지, test\ntest.loc[:, 'distance_haversine'] = haversine_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\ntest.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\ntest.loc[:, 'direction'] = bearing_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\ntest.loc[:, 'pca_manhattan'] = np.abs(test['dropoff_pca1'] - test['pickup_pca1']) + np.abs(test['dropoff_pca0'] - test['pickup_pca0'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 센터 거리정보 (승차/하차위치의 센터, 승차/하차위치의 센터bin)\n# 4가지 컬럼\ntrain.loc[:, 'center_latitude'] = (train['pickup_latitude'].values + train['dropoff_latitude'].values) / 2\ntrain.loc[:, 'center_longitude'] = (train['pickup_longitude'].values + train['dropoff_longitude'].values) / 2\ntest.loc[:, 'center_latitude'] = (test['pickup_latitude'].values + test['dropoff_latitude'].values) / 2\ntest.loc[:, 'center_longitude'] = (test['pickup_longitude'].values + test['dropoff_longitude'].values) / 2\n\ntrain.loc[:, 'center_lat_bin'] = np.round(train['center_latitude'], 2)\ntrain.loc[:, 'center_long_bin'] = np.round(train['center_longitude'], 2)\ntest.loc[:, 'center_lat_bin'] = np.round(test['center_latitude'], 2)\ntest.loc[:, 'center_long_bin'] = np.round(test['center_longitude'], 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 시간 (총 6가지)\n# 이번주 몇번째날?, \n# 이번년도 몇번째주?,\n# 몇시, 몇분, 몇번째초, 이번주 몇번째시, 몇번째초bin?\n\ntrain.loc[:, 'pickup_weekday'] = train['pickup_datetime'].dt.weekday # 해당 주에 몇번째 일\ntrain.loc[:, 'pickup_hour_weekofyear'] = train['pickup_datetime'].dt.weekofyear # 해당 년에 몇번째 주\ntrain.loc[:, 'pickup_hour'] = train['pickup_datetime'].dt.hour\ntrain.loc[:, 'pickup_minute'] = train['pickup_datetime'].dt.minute\ntrain.loc[:, 'pickup_dt'] = (train['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds() # 전체 시간을 초로 변환\ntrain.loc[:, 'pickup_week_hour'] = train['pickup_weekday'] * 24 + train['pickup_hour']\ntrain.loc[:, 'pickup_dt_bin'] = (train['pickup_dt'] // (3 * 3600)) # dt 초를 3시간 단위로 범주화\n\ntest.loc[:, 'pickup_weekday'] = test['pickup_datetime'].dt.weekday\ntest.loc[:, 'pickup_hour_weekofyear'] = test['pickup_datetime'].dt.weekofyear\ntest.loc[:, 'pickup_hour'] = test['pickup_datetime'].dt.hour\ntest.loc[:, 'pickup_minute'] = test['pickup_datetime'].dt.minute\ntest.loc[:, 'pickup_dt'] = (test['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\ntest.loc[:, 'pickup_week_hour'] = test['pickup_weekday'] * 24 + test['pickup_hour']\ntest.loc[:, 'pickup_dt_bin'] = (test['pickup_dt'] // (3 * 3600)) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['pickup_datetime','pickup_hour','pickup_minute','pickup_dt', \n       'pickup_weekday', 'pickup_hour_weekofyear','pickup_week_hour', 'pickup_dt_bin']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 거리(km)/운행시간(s) = 속도(km/s)\n# 속도 3가지\ntrain.loc[:, 'avg_speed_h'] = 1000 * train['distance_haversine'] / train['trip_duration']\ntrain.loc[:, 'avg_speed_m'] = 1000 * train['distance_dummy_manhattan'] / train['trip_duration']\ntrain['log_trip_duration'] = np.log(train['trip_duration'].values + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 시간대에 따른 속도변화 시각화\nfig, ax = plt.subplots(ncols=3, sharey=True, figsize=(15,5))\nax[0].plot(train.groupby('pickup_hour').mean()['avg_speed_h'], 'bo-', lw=2, alpha=0.7)\nax[1].plot(train.groupby('pickup_weekday').mean()['avg_speed_h'], 'go-', lw=2, alpha=0.7)\nax[2].plot(train.groupby('pickup_week_hour').mean()['avg_speed_h'], 'ro-', lw=2, alpha=0.7)\nax[0].set_xlabel('hour')\nax[1].set_xlabel('weekday')\nax[2].set_xlabel('weekhour')\nax[0].set_ylabel('average speed')\nfig.suptitle('Rush hour average traffic speed')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 지역범위에 따른 속도변화 시각화\n\ntrain.loc[:, 'pickup_lat_bin'] = np.round(train['pickup_latitude'], 3)\ntrain.loc[:, 'pickup_long_bin'] = np.round(train['pickup_longitude'], 3)\n# Average speed for regions\ngby_cols = ['pickup_lat_bin', 'pickup_long_bin']\ncoord_speed = train.groupby(gby_cols).mean()[['avg_speed_h']].reset_index()\ncoord_count = train.groupby(gby_cols).count()[['id']].reset_index()\ncoord_stats = pd.merge(coord_speed, coord_count, on=gby_cols)\ncoord_stats = coord_stats[coord_stats['id'] > 100] # 100개 이상의 값에 대한 평균치만 허용\nfig, ax = plt.subplots(ncols=1, nrows=1, figsize=(10,5))\nax.scatter(train.pickup_longitude.values[:N], train.pickup_latitude.values[:N],\n           color='black', s=1, alpha=0.5)\nax.scatter(coord_stats.pickup_long_bin.values, coord_stats.pickup_lat_bin.values,\n           c=coord_stats.avg_speed_h.values,\n           cmap='RdYlGn', s=20, alpha=0.5, vmin=1, vmax=8)\nax.set_xlim(city_long_border)\nax.set_ylim(city_lat_border)\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\nplt.title('Average speed')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Temporal and geospatial aggregation\n# 시간과 지역에 따른 평균속도 확인 (18가지)\n# 시, 일, 초bin, 주시, 클러스터 승차, 클러스터 하차에 따른 하버사인 평속 (6) \n# 위 동일,에 따른 더미 평속 (6)\n# 위 동일,에 따른 운행 시간 (6)\n\nfor gby_col in ['pickup_hour', 'pickup_date', 'pickup_dt_bin',\n               'pickup_week_hour', 'pickup_cluster', 'dropoff_cluster']:\n    gby = train.groupby(gby_col).mean()[['avg_speed_h', 'avg_speed_m', 'log_trip_duration']]\n    gby.columns = ['%s_gby_%s' % (col, gby_col) for col in gby.columns]\n    train = pd.merge(train, gby, how='left', left_on=gby_col, right_index=True)\n    test = pd.merge(test, gby, how='left', left_on=gby_col, right_index=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 시간과 지역에 따른 평균속도 확인 (5가지)\n# 센터경도+위도, 시+센터경도+위도, 시+클러스터승차, 시+클러스터하차 (4)\n# 클러스터승차+하차에 따른 하버사인 평속 (1)\n\n\nfor gby_cols in [['center_lat_bin', 'center_long_bin'],\n                 ['pickup_hour', 'center_lat_bin', 'center_long_bin'],\n                 ['pickup_hour', 'pickup_cluster'],  ['pickup_hour', 'dropoff_cluster'],\n                 ['pickup_cluster', 'dropoff_cluster']]:\n    coord_speed = train.groupby(gby_cols).mean()[['avg_speed_h']].reset_index()\n    coord_count = train.groupby(gby_cols).count()[['id']].reset_index()\n    coord_stats = pd.merge(coord_speed, coord_count, on=gby_cols)\n    coord_stats = coord_stats[coord_stats['id'] > 100]\n    coord_stats.columns = gby_cols + ['avg_speed_h_%s' % '_'.join(gby_cols), 'cnt_%s' %  '_'.join(gby_cols)]\n    train = pd.merge(train, coord_stats, how='left', on=gby_cols)\n    test = pd.merge(test, coord_stats, how='left', on=gby_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 운행횟수 (8가지)\n# 위 동일, id횟수 (5)\n# pickup_datetime_group # 시간을 60분 단위로 그룹화 시킨다.\n# dropoff_cluster_count # 해당 클러스터 지역에서 시간당(60분) 하차 횟수\n# pikup_cluster_count # 해당 클러스터 지역에서 시간당 승차 횟수\n\n# group_freq = '60min'\n# df_all = pd.concat((train, test))[['id', 'pickup_datetime', 'pickup_cluster', 'dropoff_cluster']]\n# train.loc[:, 'pickup_datetime_group'] = train['pickup_datetime'].dt.round(group_freq)\n# test.loc[:, 'pickup_datetime_group'] = test['pickup_datetime'].dt.round(group_freq)\n\n# # Count trips over 60min\n# df_counts = df_all.set_index('pickup_datetime')[['id']].sort_index()\n# df_counts['count_60min'] = df_counts.isnull().rolling(group_freq).count()['id']\n# train = train.merge(df_counts, on='id', how='left')\n# test = test.merge(df_counts, on='id', how='left')\n\n# # Count how many trips are going to each cluster over time\n# dropoff_counts = df_all \\\n#     .set_index('pickup_datetime') \\\n#     .groupby([pd.TimeGrouper(group_freq), 'dropoff_cluster']) \\\n#     .agg({'id': 'count'}) \\\n#     .reset_index().set_index('pickup_datetime') \\\n#     .groupby('dropoff_cluster').rolling('240min').mean() \\\n#     .drop('dropoff_cluster', axis=1) \\\n#     .reset_index().set_index('pickup_datetime').shift(freq='-120min').reset_index() \\\n#     .rename(columns={'pickup_datetime': 'pickup_datetime_group', 'id': 'dropoff_cluster_count'})\n\n# train['dropoff_cluster_count'] = train[['pickup_datetime_group', 'dropoff_cluster']].merge(dropoff_counts, on=['pickup_datetime_group', 'dropoff_cluster'], how='left')['dropoff_cluster_count'].fillna(0)\n# test['dropoff_cluster_count'] = test[['pickup_datetime_group', 'dropoff_cluster']].merge(dropoff_counts, on=['pickup_datetime_group', 'dropoff_cluster'], how='left')['dropoff_cluster_count'].fillna(0)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count how many trips are going from each cluster over time\n# df_all = pd.concat((train, test))[['id', 'pickup_datetime', 'pickup_cluster', 'dropoff_cluster']]\n# pickup_counts = df_all \\\n#     .set_index('pickup_datetime') \\\n#     .groupby([pd.TimeGrouper(group_freq), 'pickup_cluster']) \\\n#     .agg({'id': 'count'}) \\\n#     .reset_index().set_index('pickup_datetime') \\\n#     .groupby('pickup_cluster').rolling('240min').mean() \\\n#     .drop('pickup_cluster', axis=1) \\\n#     .reset_index().set_index('pickup_datetime').shift(freq='-120min').reset_index() \\\n#     .rename(columns={'pickup_datetime': 'pickup_datetime_group', 'id': 'pickup_cluster_count'})\n\n# train['pickup_cluster_count'] = train[['pickup_datetime_group', 'pickup_cluster']].merge(pickup_counts, on=['pickup_datetime_group', 'pickup_cluster'], how='left')['pickup_cluster_count'].fillna(0)\n# test['pickup_cluster_count'] = test[['pickup_datetime_group', 'pickup_cluster']].merge(pickup_counts, on=['pickup_datetime_group', 'pickup_cluster'], how='left')['pickup_cluster_count'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# OSMR 피쳐, 외부데이타 (3가지)\n# 총 거리, 총 운행시간, 스텝 갯수 (???)\n# 승차/하차 위치의 실제도로 최단거리\n# 예측운행시간\n# fr1 = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_train_part_1.csv',\n#                   usecols=['id', 'total_distance', 'total_travel_time',  'number_of_steps'])\n# fr2 = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_train_part_2.csv',\n#                   usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\n# test_street_info = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_test.csv',\n#                                usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\n# train_street_info = pd.concat((fr1, fr2))\n# train = train.merge(train_street_info, how='left', on='id')\n# test = test.merge(test_street_info, how='left', on='id')\n# train_street_info.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train과 test 컬럼차이 -> 입력특성에서 제외\nprint(np.setdiff1d(train.columns, test.columns)) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = list(train.columns)\nprint(np.setdiff1d(train.columns, test.columns))\ndo_not_use_for_training = ['id', 'log_trip_duration', 'pickup_datetime', 'dropoff_datetime',\n                           'trip_duration', 'idx',\n                           'pickup_date', 'avg_speed_h', 'avg_speed_m',\n                           'pickup_lat_bin', 'pickup_long_bin',\n                           'center_lat_bin', 'center_long_bin',\n                           'pickup_dt_bin', 'pickup_datetime_group']\nfeature_names = [f for f in train.columns if f not in do_not_use_for_training]\nprint('We have %i features.' % len(feature_names))\n\ny = np.log(train['trip_duration'].values + 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nXtr, Xv, ytr, yv = train_test_split(train[feature_names].values, y, test_size=0.2, random_state=1987)\ndtrain = xgb.DMatrix(Xtr, label=ytr)\ndvalid = xgb.DMatrix(Xv, label=yv)\ndtest = xgb.DMatrix(test[feature_names].values)\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\nxgb_pars = {'min_child_weight': 50, 'eta': 0.3, 'colsample_bytree': 0.3, 'max_depth': 10,\n            'subsample': 0.8, 'lambda': 1., 'nthread': 4, 'booster' : 'gbtree', 'silent': 1,\n            'eval_metric': 'rmse', 'objective': 'reg:linear'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = xgb.train(xgb_pars, dtrain, 60, watchlist, early_stopping_rounds=50,\n                  maximize=False, verbose_eval=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Modeling RMSLE %.5f' % model.best_score) # train 0.35, valid 0.385","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_dict = model.get_fscore()\nfs = ['f%i' % i for i in range(len(feature_names))]\nf1 = pd.DataFrame({'f': list(feature_importance_dict.keys()),\n                   'importance': list(feature_importance_dict.values())})\nf2 = pd.DataFrame({'f': fs, 'feature_name': feature_names})\nfeature_importance = pd.merge(f1, f2, how='right', on='f')\nfeature_importance = feature_importance.fillna(0)\nfeature_importance[['feature_name', 'importance']].sort_values(by='importance',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```\nThe following plot shows the feature elimination result and the xgboost importance. A few observations:\n\n* We had quite a few location related feature with high feature importance score although removing just one of them does not really increase the error.\n( 위치, 지역과 관련된 특성들이 높은 중요도를 가진다. 하지만 그들중 하나를 제거하더라도 오류가 증가하지는 않는다.)\n* Vendor_id is the second least used feature according to feature importance plot but removing it increases the rmse significantly. It is binary value, even a few decision trees using it captures its information.\n(벤더 id의 경우 매우낮은 순위의 중요도를 가지지만 이를 제거하면 rmse가 크게 증가한다.)\n* Direction is important from both aspect. It does not have many correlated feature and removing it would hurt the model.\n(direction 특성은 다양한 면에서 중요하다. 많은 상관관계를 가지며 해당특성을 제거하면 모델을 손상시킬수있다.)\n* There are lots of different features which could be removed without really breaking a leg.\n(그밖에 제거 할수있는 다양한 특성들이 있다.)\nIn real world projects I prefer models with less variables however in kaggle competitions I err on having more even if they only give marginal improvent.\n```\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"ypred = model.predict(dvalid)\n\nplt.scatter(ypred, yv, s=0.1, alpha=0.1)\nplt.xlabel('log(prediction)')\nplt.ylabel('log(ground truth)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(np.exp(ypred), np.exp(yv), s=0.1, alpha=0.1)\nplt.xlabel('prediction')\nplt.ylabel('ground truth')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ytest = model.predict(dtest)\n\ntest['trip_duration'] = np.exp(ytest) - 1\ntest[['id', 'trip_duration']].to_csv('submission_v2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}