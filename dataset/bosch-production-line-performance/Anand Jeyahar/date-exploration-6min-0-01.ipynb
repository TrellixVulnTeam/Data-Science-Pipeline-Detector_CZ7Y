{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"66fabb4c-2a2c-6ad0-3447-1ac095217ca2"},"source":"Checking the 'train_dates.csv'\n\n - lots of columns (1157)\n - 80%+ missing values\n - Same stations often have same date values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"59539376-020b-b14f-e9f8-6cb4225f35cb"},"outputs":[],"source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain_date_part = pd.read_csv('../input/train_date.csv', nrows=50000)\nprint(train_date_part.shape)\nprint(train_date_part.count())\nprint(train_date_part.size)\nprint(1.0 * train_date_part.count().sum() / train_date_part.size)\nprint(train_date_part[:2])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f72ecf0-c05a-685e-eba8-fb2316e3aea8"},"outputs":[],"source":"# Let's check the min and max times for each station\ndef get_station_times(dates, withId=False):\n    times = []\n    cols = list(dates.columns)\n    print(cols)\n    if 'Id' in cols:\n        cols.remove('Id')\n    for feature_name in cols:\n        if withId:\n            df = dates[['Id', feature_name]].copy()\n            df.columns = ['Id', 'time']\n        else:\n            df = dates[[feature_name]].copy()\n            df.columns = ['time']\n        df['station'] = feature_name.split('_')[1][1:]\n        df = df.dropna()\n        times.append(df)\n    return pd.concat(times)\n\nstation_times = get_station_times(train_date_part, withId=True).sort_values(by=['Id', 'station'])\nprint(station_times[:5])\nprint(station_times.shape)\nmin_station_times = station_times.groupby(['Id', 'station']).min()['time']\nmax_station_times = station_times.groupby(['Id', 'station']).max()['time']\nprint(np.mean(1. * (min_station_times == max_station_times)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"61e4c1db-00fc-06c2-e1ba-d7f087682738"},"source":"We just removed the missing values. As the observation times are almost always unique for staion, Id pair we could spare a lot of memory by reading only one time for each station.\n\nPlease note we checked only 1% of the dataset!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2cbc563a-3a6b-9a07-475a-3dc658c52b54"},"outputs":[],"source":"# Read station times for train and test\ndate_cols = train_date_part.drop('Id', axis=1).count().reset_index().sort_values(by=0, ascending=False)\ndate_cols['station'] = date_cols['index'].apply(lambda s: s.split('_')[1])\ndate_cols = date_cols.drop_duplicates('station', keep='first')['index'].tolist()\nprint(date_cols) # selected features\n\ntrain_date = pd.read_csv('../input/train_date.csv', usecols=date_cols)\nprint(train_date.shape)\ntrain_station_times = get_station_times(train_date, withId=False)\nprint(train_station_times.shape)\ntrain_time_cnt = train_station_times.groupby('time').count()[['station']].reset_index()\ntrain_time_cnt.columns = ['time', 'cnt']\nprint(train_time_cnt.shape)\n\ntest_date = pd.read_csv('../input/test_date.csv', usecols=date_cols)\nprint(test_date.shape)\ntest_station_times = get_station_times(test_date, withId=False)\nprint(test_station_times.shape)\ntest_time_cnt = test_station_times.groupby('time').count()[['station']].reset_index()\ntest_time_cnt.columns = ['time', 'cnt']\nprint(test_time_cnt.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0eb843bc-30d2-61dc-dc30-70225430787e"},"outputs":[],"source":"fig = plt.figure()\nplt.plot(train_time_cnt['time'].values, train_time_cnt['cnt'].values, 'b.', alpha=0.1, label='train')\nplt.plot(test_time_cnt['time'].values, test_time_cnt['cnt'].values, 'r.', alpha=0.1, label='test')\nplt.title('Original date values')\nplt.ylabel('Number of records')\nplt.xlabel('Time')\nfig.savefig('original_date_values.png', dpi=300)\nplt.show()\n\nprint((train_time_cnt['time'].min(), train_time_cnt['time'].max()))\nprint((test_time_cnt['time'].min(), test_time_cnt['time'].max()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"8c2f40d9-5950-99a5-0395-f8132d97a02e"},"source":"A few observations:\n\n 1. Train and test set has the same time period\n 2. There is a clear periodic pattern \n 3. The dates are transformed to 0 - 1718 with granularity of 0.01\n 4. There is a gap in the middle\n\nCould we figure out what does 0.01 mean?  Let's check a few auto correlations!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c29e43b4-a667-5ca8-3b43-39b3a83a9077"},"outputs":[],"source":"time_ticks = np.arange(train_time_cnt['time'].min(), train_time_cnt['time'].max() + 0.01, 0.01)\ntime_ticks = pd.DataFrame({'time': time_ticks})\ntime_ticks = pd.merge(time_ticks, train_time_cnt, how='left', on='time')\ntime_ticks = time_ticks.fillna(0)\n# Autocorrelation\nx = time_ticks['cnt'].values\nmax_lag = 8000\nauto_corr_ks = range(1, max_lag)\nauto_corr = np.array([1] + [np.corrcoef(x[:-k], x[k:])[0, 1] for k in auto_corr_ks])\nfig = plt.figure()\nplt.plot(auto_corr, 'k.', label='autocorrelation by 0.01')\nplt.title('Train Sensor Time Auto-correlation')\nperiod = 25\nauto_corr_ks = list(range(period, max_lag, period))\nauto_corr = np.array([1] + [np.corrcoef(x[:-k], x[k:])[0, 1] for k in auto_corr_ks])\nplt.plot([0] + auto_corr_ks, auto_corr, 'go', alpha=0.5, label='strange autocorrelation at 0.25')\nperiod = 1675\nauto_corr_ks = list(range(period, max_lag, period))\nauto_corr = np.array([1] + [np.corrcoef(x[:-k], x[k:])[0, 1] for k in auto_corr_ks])\nplt.plot([0] + auto_corr_ks, auto_corr, 'ro', markersize=10, alpha=0.5, label='one week = 16.75?')\nplt.xlabel('k * 0.01 -  autocorrelation lag')\nplt.ylabel('autocorrelation')\nplt.legend(loc=0)\nfig.savefig('train_time_auto_correlation.png', dpi=300)"},{"cell_type":"markdown","metadata":{"_cell_guid":"dca1f4be-8077-f181-1e1e-3db2dd1a2d48"},"source":"The largest peaks are at approximately 1680 ticks.  Let's call it a week ;) \n\nIn each week we could see 7 local maxima ~ days."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ee876be-a3d9-e141-eb21-0c361ce0a5a7"},"outputs":[],"source":"week_duration = 1679\ntrain_time_cnt['week_part'] = ((train_time_cnt['time'].values * 100) % week_duration).astype(np.int64)\n# Aggregate weekly stats\ntrain_week_part = train_time_cnt.groupby(['week_part'])[['cnt']].sum().reset_index()\nfig = plt.figure()\nplt.plot(train_week_part.week_part.values, train_week_part.cnt.values, 'b.', alpha=0.5, label='train count')\ny_train = train_week_part['cnt'].rolling(window=20, center=True).mean().values\nplt.plot(train_week_part.week_part.values, y_train, 'b-', linewidth=4, alpha=0.5, label='train count smooth')\nplt.title('Relative Part of week')\nplt.ylabel('Number of records')\nplt.xlim(0, 1680)\nfig.savefig('week_duration.png', dpi=300)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"19b6d807-8d39-ca29-7b36-006a7e06594e"},"outputs":[],"source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef test_stationarity(timeseries, valueCol, skip_stationarity=False, title='timeseries', **kwargs):\n\n    from statsmodels.tsa.stattools import adfuller\n    #Determing rolling statistics\n    rolmean = pd.rolling_mean(timeseries, window=12)\n    rolstd = pd.rolling_std(timeseries, window=12)\n\n    #Plot rolling statistics:\n    fig = plt.figure(figsize=(12, 8))\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation of ' + title )\n    plt.show()\n\n    if not skip_stationarity:\n        #Perform Dickey-Fuller test:\n        dftest = adfuller(timeseries[valueCol], autolag=kwargs.get('autolag', 't-stat'))\n        print('Results of Dickey-Fuller Test:')\n        dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n        for key,value in dftest[4].items():\n            dfoutput['Critical Value (%s)'%key] = value\n        print(dfoutput)\n\ndef plot_autocorrelation(timeseries_df, valueCol=None,\n                         timeCol='timestamp', timeInterval='30min', partial=False):\n    \"\"\"\n    Plot autocorrelation of the given dataframe based on statsmodels.tsa.stattools.acf\n\t\t\t(which apparently is simple Ljung-Box model)\n    Assumes:\n       default timecol == 'timestamp' if different pass a kw parameter\n\n    \"\"\"\n    import statsmodels.api as sm\n    fig = plt.figure(figsize=(12,8))\n    ax1 = fig.add_subplot(111)\n    if partial:\n        subplt = sm.graphics.tsa.plot_acf(timeseries_df[valueCol].squeeze(), lags=40, ax=ax1)\n    else:\n        subplt = sm.graphics.tsa.plot_pacf(timeseries_df[valueCol], lags=40, ax=ax1)\n    plt.show()\n    return fig\n\ndef seasonal_decompose(timeseries_df, freq=None, **kwargs):\n    import statsmodels.api as sm\n    timeseries_df.interpolate(inplace=True)\n    if not freq: freq = len(timeseries_df) - 2\n    seasonal_components = sm.tsa.seasonal_decompose(timeseries_df, freq=freq, **kwargs)\n    fig = seasonal_components.plot()\n    return fig\n\ndef create_timeseries_df(dataframe, dropColumns=list(),filterByCol=None,\n                      filterByVal=None, timeCol='date',\n                      timeInterval='30min', func=sum):\n    \"\"\"\n    # A simple function that takes df, and returns a timeseries with a temporal distribution of audit events\n    auditcode= <specify which audit event> (None means just a distribution of any audit event)\n\n    \"\"\"\n    new_df = dataframe.copy(deep=True)\n    if dropColumns:\n        new_df.drop(dropColumns, 1, inplace=True)\n    if filterByVal:\n        assert type(filterByVal) == list, \"Need a list of values for filterByVal\"\n        assert filterByCol, \"Column to be filtered by is mandatory\"\n        assert filterByCol not in dropColumns, \"Cannot group by a column that's to be dropped\"\n        assert type(filterByCol) != list, \"Only single column can be passed\"\n        new_df = new_df[new_df[filterByCol].isin(filterByVal)].groupby(timeCol).agg(func)\n        new_df.columns = filterByVal\n        new_df.index = pd.to_datetime(new_df.index)\n        new_df = new_df.resample(timeInterval, func)\n    else:\n        new_df = new_df.groupby(timeCol).agg(func)\n        new_df.index = pd.to_datetime(new_df.index)\n        new_df = new_df.resample(timeInterval, func)\n    return new_df"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3aba2028-6800-e11f-1048-01a453316190"},"outputs":[],"source":"plot_autocorrelation(train_time_cnt, valueCol='cnt') # AR model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8079ea9e-8d9d-3a79-c5a1-6a26987840f9"},"outputs":[],"source":"plot_autocorrelation(train_time_cnt, valueCol='cnt', partial=True) # partial AR model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"90d32927-5aa4-be54-67a8-98eb5d69d69f"},"outputs":[],"source":"seasonal_decompose(train_time_cnt)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"98296892-8d3d-3868-d1ae-c67ffcb28cf1"},"outputs":[],"source":"test_stationarity(train_time_cnt, valueCol='cnt', skip_stationarity=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}