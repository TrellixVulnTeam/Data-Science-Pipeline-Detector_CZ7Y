{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bike Sharing Demand\n\n\n\n## Context\n\nBike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.\n\nThe data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city. In this competition, participants are asked to combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C.\n\n\n## Goal\nForecast use of a city bikeshare system\n\n---"},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npd.options.display.max_columns = 100\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nimport lightgbm as lgbm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the head of the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/bike-sharing-demand/train.csv',header = 0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset contains the following columns:\n\n```\ndatetime - hourly date + timestamp  \nseason -  1 = spring, 2 = summer, 3 = fall, 4 = winter \nholiday - whether the day is considered a holiday\nworkingday - whether the day is neither a weekend nor holiday\nweather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy \n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds \n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \ntemp - temperature in Celsius\natemp - \"feels like\" temperature in Celsius\nhumidity - relative humidity\nwindspeed - wind speed\ncasual - number of non-registered user rentals initiated\nregistered - number of registered user rentals initiated\ncount - number of total rentals\n```\n\n'count' is the target, all other columns are features. \n\nThe value of 'count' is equal to casual plus register"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are basiclly 3 types of data:\n- data concerning the weather\n- for time infos\n- linked to the number of users\n\nNo Nan, it seems that there isn't any missing value. All types seem to be corresponding to the meaning of the data except the 'datetime' column wich is an object not a pandas' datetime\nLet's see the basic statistics:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At first glance, when we look at the temp / atemp min, it seems strange that no negative temperature is recorded..."},{"metadata":{},"cell_type":"markdown","source":"## Weather informations analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 4))\nsns.distplot(df.temp, bins=10, label='real temp.')\nsns.distplot(df.atemp, bins=10, label='feels like temp.')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One can see an offset between the real temperature and the \"feels like\" temperature. This can probably explained by the fact that temperature on a bike is different. But the distributions looks the same. More, there is a clear correlation between the 2 features."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(4, 4))\nsns.scatterplot(df.atemp, df.temp)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see if the difference between the 2 features is quite the same..."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Delta'] = df.temp - df.atemp\nsns.lineplot(x=df.index, y=df.Delta)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are few abnormal values corresponding to the spike in the above plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.Delta > 5].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[(df.atemp >= 12) & (df.atemp <= 12.5)].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All those measures have been recorded the same day. During that day 'temp' changed but not 'atemp' The 'atemp' value seems strange even during other few days... Because those 2 features are correlated, we only need to keep one. This is more safe to keep temp instead of atemp"},{"metadata":{},"cell_type":"markdown","source":"Is there also a correlation between humidity and windspeed ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 4))\nsns.distplot(df.humidity, bins=10, label='humidity')\nsns.distplot(df.windspeed, bins=10, label='windspeed')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df[['temp', 'atemp', 'humidity', 'windspeed']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Except for the temperature, there is no clear correlation between other features. Furthermore:\n- humidity vs atemp there is something strange with missing value like a void in the scatter\n- for all windspeed plots there are null values and beginning values start at 5, that's probably because the sensor can detect low values...this needs to be investigated further."},{"metadata":{},"cell_type":"markdown","source":"## Modification of time data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['casual_percentage'] = df['casual'] / df['count']\ndf['registered_percentage'] = df['registered'] / df['count']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def change_datetime(df):\n    \"\"\" Modify the col datetime to create other cols: dow, month, week...\"\"\"\n    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n    df[\"dow\"] = df[\"datetime\"].dt.dayofweek\n    df[\"month\"] = df[\"datetime\"].dt.month\n    df[\"week\"] = df[\"datetime\"].dt.week\n    df[\"hour\"] = df[\"datetime\"].dt.hour\n    df[\"year\"] = df[\"datetime\"].dt.year\n    df[\"season\"] = df.season.map({1: \"Winter\", 2 : \"Spring\", 3 : \"Summer\", 4 :\"Fall\" })\n    df[\"month_str\"] = df.month.map({1: \"Jan \", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5: \"May\", 6: \"Jun\", 7: \"Jul\", 8: \"Aug\", 9: \"Sep\", 10: \"Oct\", 11: \"Nov\", 12: \"Dec\" })\n    df[\"dow_str\"] = df.dow.map({5: \"Sat\", 6 : \"Sun\", 0 : \"Mon\", 1 :\"Tue\", 2 : \"Wed\", 3 : \"Thu\", 4: \"Fri\" })\n    df[\"weather_str\"] = df.weather.map({1: \"Good\", 2 : \"Normal\", 3 : \"Bad\", 4 :\"Very Bad\"})\n    return df\n    \n    \ndf = change_datetime(df)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Rentals / target analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(data=df['count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['y_log'] = np.log(df['count'])\nsns.kdeplot(data=df['y_log'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.pointplot(x=df[\"hour\"], y=df[\"count\"], hue=df[\"season\"])\nplt.xlabel(\"Hour Of The Day\")\nplt.ylabel(\"Users Count\") \nplt.title(\"Rentals Across Hours\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.lineplot(x=\"hour\", y=\"count\", hue=\"season\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ---------------------------------------------------------\nplt.figure(figsize=(6,3))\nplt.stackplot(range(1,25),\n              df.groupby(['hour'])['casual_percentage'].mean(), \n              df.groupby(['hour'])['registered_percentage'].mean(), \n              labels=['Casual','Registered'])\nplt.legend(loc='upper left')\nplt.margins(0,0)\nplt.title(\"Evolution of casual /registered bikers' share over hours of the day\")\n\n# ---------------------------------------------------------\nplt.figure(figsize=(6,6))\ndf_hours = pd.DataFrame(\n    {\"casual\" : df.groupby(['hour'])['casual'].mean().values,\n    \"registered\" : df.groupby(['hour'])['registered'].mean().values},\n    index = df.groupby(['hour'])['casual'].mean().index)\ndf_hours.plot.bar(rot=0)\nplt.title(\"Evolution of casual /registered bikers numbers over hours of the day\")\n\n# ---------------------------------------------------------\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.pointplot(x=df[\"dow\"], y=df[\"count\"], hue=df[\"season\"])\nplt.xlabel(\"Day of the week\")\nplt.ylabel(\"Users Count\") \nplt.title(\"Rentals Across week days\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ---------------------------------------------------------\nplt.figure(figsize=(6,3))\nplt.stackplot(range(1,8),\n              df.groupby(['dow'])['casual_percentage'].mean(), \n              df.groupby(['dow'])['registered_percentage'].mean(), \n              labels=['Casual','Registered'])\nplt.legend(loc='upper left')\nplt.margins(0,0)\nplt.title(\"Evolution of casual /registered bikers' share over weekdays\")\n\n# ---------------------------------------------------------\nplt.figure(figsize=(6,6))\ndf_hours = pd.DataFrame(\n    {\"casual\" : df.groupby(['dow'])['casual'].mean().values,\n    \"registered\" : df.groupby(['dow'])['registered'].mean().values},\n    index = df.groupby(['dow'])['casual'].mean().index)\ndf_hours.plot.bar(rot=0)\nplt.title(\"Evolution of casual /registered bikers numbers over weekdays\")\n\n# ---------------------------------------------------------\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(10, 8)\nsns.boxplot(data=df, y=\"count\", x=\"month_str\", orient=\"v\")\nax.set(xlabel=\"Months\" , ylabel=\"Count\", title=\"Count Across Month\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(x='hour', y='temp', data=df, hue='season')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ---------------------------------------------------------\nplt.figure(figsize=(6,3))\nplt.stackplot(range(1,13),\n              df.groupby(['month'])['casual_percentage'].mean(), \n              df.groupby(['month'])['registered_percentage'].mean(), \n              labels=['Casual','Registered'])\nplt.legend(loc='upper left')\nplt.margins(0,0)\nplt.title(\"Evolution of casual /registered bikers' share over months of the year\")\n\n# ---------------------------------------------------------\nplt.figure(figsize=(6,6))\ndf_hours = pd.DataFrame(\n    {\"casual\" : df.groupby(['month'])['casual'].mean().values,\n    \"registered\" : df.groupby(['month'])['registered'].mean().values},\n    index = df.groupby(['month'])['casual'].mean().index)\ndf_hours.plot.bar(rot=0)\nplt.title(\"Evolution of casual /registered bikers numbers over months of the year\")\n\n# ---------------------------------------------------------\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\n\nbars = ['casual not on working days', 'casual on working days',\\\n        'registered not on working days', 'registered on working days',\\\n        'casual not on holidays', 'casual on holidays',\\\n        'registered not on holidays', 'registered on holidays']\n\nqty = [df.groupby(['workingday'])['casual'].mean()[0], df.groupby(['workingday'])['casual'].mean()[1],\\\n      df.groupby(['workingday'])['registered'].mean()[0], df.groupby(['workingday'])['registered'].mean()[1],\\\n      df.groupby(['holiday'])['casual'].mean()[0], df.groupby(['holiday'])['casual'].mean()[1],\\\n      df.groupby(['holiday'])['registered'].mean()[0], df.groupby(['holiday'])['registered'].mean()[1]]\n\ny_pos = np.arange(len(bars))\nplt.barh(y_pos, qty, align='center')\n\nplt.yticks(y_pos, labels=bars)\n#plt.invert_yaxis()  # labels read top-to-bottom\nplt.xlabel('Mean nb of bikers')\nplt.title(\"Number of bikers on holidays / working days\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ---------------------------------------------------------\nplt.figure(figsize=(6,3))\nplt.stackplot(range(1,5),\n              df.groupby(['season'])['casual_percentage'].mean(), \n              df.groupby(['season'])['registered_percentage'].mean(), \n              labels=['Casual','Registered'])\nplt.legend(loc='upper left')\nplt.margins(0,0)\nplt.title(\"Evolution of casual /registered bikers' share over seasons\")\n\n# ---------------------------------------------------------\nplt.figure(figsize=(6,6))\ndf_hours = pd.DataFrame(\n    {\"casual\" : df.groupby(['season'])['casual'].mean().values,\n    \"registered\" : df.groupby(['season'])['registered'].mean().values},\n    index = df.groupby(['season'])['casual'].mean().index)\ndf_hours.plot.bar(rot=0)\nplt.title(\"Evolution of casual /registered bikers numbers over seasons\")\n\n# ---------------------------------------------------------\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generally speaking, the number of registered user increase between non working and working days where as the opposite can be seen when it comes to casual users (their number decrease between non working to working days). The spikes on the previous plots may corroborate this assumption because the registered users tend to use bike to go to work while casul users rent bike in the middle of the day... "},{"metadata":{},"cell_type":"markdown","source":"## Correlations"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = df[[\"temp\",\"atemp\",\"casual\",\"registered\",\"humidity\",\"windspeed\",\"count\"]].corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(7, 6))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, annot=True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5, 5))\nsns.scatterplot(df.registered, df['count'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preparation for models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# target \ny = (df[\"count\"])\n\n# drop irrelevant cols and target\ncols_dropped = [\"count\", \"datetime\", \"atemp\", \"month_str\", \"season\", \"dow_str\", \"weather_str\",\\\n                \"casual\", \"registered\", \"casual_percentage\", \"registered_percentage\", \"y_log\", \"Delta\"] \nX = df.drop(columns=cols_dropped)\n            \nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split the data set into a training part and one for testing purpose."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_rmse(reg, model_name):\n    \"\"\"Print the score for the model passed in argument and retrun scores for the train/test sets\"\"\"\n    \n    y_train_pred, y_pred = reg.predict(X_train), reg.predict(X_test)\n    rmse_train, rmse_test = np.sqrt(mean_squared_error(y_train, y_train_pred)), np.sqrt(mean_squared_error(y_test, y_pred))\n    print(model_name, f'\\t - RMSE on Training  = {rmse_train:.2f} / RMSE on Test = {rmse_test:.2f}')\n    \n    return rmse_train, rmse_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=100).fit(X_train, y_train)\n_, _ = get_rmse(rf, 'rondom forrest')\n\nfeatures = pd.DataFrame()\nfeatures[\"features\"] = X_train.columns\nfeatures[\"coefficient\"] = rf.feature_importances_\n\nfeatures.sort_values(by=[\"coefficient\"], ascending=False, inplace=True)\nfig,ax= plt.subplots()\nfig.set_size_inches(20,5)\nsns.barplot(data=features, x=\"coefficient\", y=\"features\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb = GradientBoostingRegressor(n_estimators=100).fit(X_train, y_train)\n_, _ = get_rmse(gb, 'gb')\n\nfeatures = pd.DataFrame()\nfeatures[\"features\"] = X_train.columns\nfeatures[\"coefficient\"] = gb.feature_importances_\n\nfeatures.sort_values(by=[\"coefficient\"], ascending=False, inplace=True)\nfig,ax= plt.subplots()\nfig.set_size_inches(20,5)\nsns.barplot(data=features, x=\"coefficient\", y=\"features\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_reg = xgb.XGBRegressor(n_estimators=100).fit(X_train, y_train)\n_, _ = get_rmse(xgb_reg, 'xgb_reg')\n\nfeatures = pd.DataFrame()\nfeatures[\"features\"] = X_train.columns\nfeatures[\"coefficient\"] = xgb_reg.feature_importances_\n\nfeatures.sort_values(by=[\"coefficient\"], ascending=False, inplace=True)\nfig,ax= plt.subplots()\nfig.set_size_inches(20,5)\nsns.barplot(data=features, x=\"coefficient\", y=\"features\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_reg = lgbm.LGBMRegressor(n_estimators=100).fit(X_train, y_train)\n_, _ = get_rmse(lgbm_reg, 'lgbm_reg')\n\nfeatures = pd.DataFrame()\nfeatures[\"features\"] = X_train.columns\nfeatures[\"coefficient\"] = lgbm_reg.feature_importances_\n\nfeatures.sort_values(by=[\"coefficient\"], ascending=False, inplace=True)\nfig,ax= plt.subplots()\nfig.set_size_inches(20,5)\nsns.barplot(data=features, x=\"coefficient\", y=\"features\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The same feature come first, sometimes the order vary a little bit..."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Models training and predictions"},{"metadata":{},"cell_type":"markdown","source":"## Metric  - Root Mean Squared Error"},{"metadata":{},"cell_type":"markdown","source":"Using logarithmic is an indirect way of measuring the performance of a loss function in terms of something more easily understandable"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle(y, y_,convertExp=True):\n    if convertExp:\n        y = np.exp(y),\n        y_ = np.exp(y_)\n    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))\n    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))\n    calc = (log1 - log2) ** 2\n    return np.sqrt(np.mean(calc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Please, also refer to the get_rmse function coded previously."},{"metadata":{},"cell_type":"markdown","source":"## Base line & basic models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of all the basic models used at first\nmodel_list = [\n    LinearRegression(), Lasso(), Ridge(), ElasticNet(),\n    RandomForestRegressor(), GradientBoostingRegressor(), ExtraTreesRegressor(),\n    xgb.XGBRegressor(), lgbm.LGBMRegressor()\n             ]\n\n# creation of list of names and scores for the train / test\nmodel_names = [str(m)[:str(m).index('(')] for m in model_list]\nrmse_train, rmse_test = [], []\n\n# fit and predict all models\nfor model, name in zip(model_list, model_names):\n    model.fit(X_train, y_train)\n    sc_train, sc_test = get_rmse(model, name)\n    rmse_train.append(sc_train)\n    rmse_test.append(sc_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Polynomial regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\npoly_lin_reg = Pipeline([\n    (\"poly_feat\", PolynomialFeatures(degree=3)),\n    (\"scaler\", StandardScaler()),\n    (\"linear_reg\", LinearRegression())\n])\n\npoly_lin_reg.fit(X_train, y_train)\n\nsc_train, sc_test = get_rmse(poly_lin_reg, \"Poly Linear Reg\")\n\nmodel_names.append('Poly Linear Reg')\nrmse_train.append(sc_train)\nrmse_test.append(sc_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameters tuning"},{"metadata":{},"cell_type":"markdown","source":"### Lasso"},{"metadata":{"trusted":true},"cell_type":"code","source":"rd_cv = Ridge()\nrd_params_ = {'max_iter':[1000, 2000, 3000],\n                 'alpha':[0.1, 1, 2, 3, 4, 10, 30, 100, 200, 300, 400, 800, 900, 1000]}\n\nrmsle_scorer = metrics.make_scorer(rmsle, greater_is_better=False)\nrd_cv = GridSearchCV(rd_cv,\n                  rd_params_,\n                  scoring = rmsle_scorer,\n                  cv=5)\n\nrd_cv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lasso"},{"metadata":{"trusted":true},"cell_type":"code","source":"sc_train, sc_test = get_rmse(rd_cv, \"Ridge CV\")\n\nmodel_names.append('Ridge CV')\nrmse_train.append(sc_train)\nrmse_test.append(sc_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"la_cv = Lasso()\n\nalpha  = 1/np.array([0.1, 1, 2, 3, 4, 10, 30, 100, 200, 300, 400, 800, 900, 1000])\nla_params = {'max_iter':[1000, 2000, 3000],'alpha':alpha}\n\nla_cv = GridSearchCV(la_cv, la_params, scoring = rmsle_scorer, cv=5)\nla_cv.fit(X_train, y_train).best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc_train, sc_test = get_rmse(la_cv, \"Lasso CV\")\n\nmodel_names.append('Lasso CV')\nrmse_train.append(sc_train)\nrmse_test.append(sc_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Knn regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_reg = KNeighborsRegressor()\nknn_params = {'n_neighbors':[1, 2, 3, 4, 5, 6]}\n\nknn_reg = GridSearchCV(knn_reg, knn_params, scoring = rmsle_scorer, cv=5)\nknn_reg.fit(X_train, y_train).best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc_train, sc_test = get_rmse(knn_reg, \"kNN Reg\")\n\nmodel_names.append('kNN Reg')\nrmse_train.append(sc_train)\nrmse_test.append(sc_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LinearSVR"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_reg = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"linear_svr\", LinearSVR())\n])\n\nsvm_reg.fit(X_train, y_train)\n\nsc_train, sc_test = get_rmse(svm_reg, \"SVM Reg\")\n\nmodel_names.append('SVM Reg')\nrmse_train.append(sc_train)\nrmse_test.append(sc_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Just for fun: a MLP (Multi Layer Perceptron)"},{"metadata":{},"cell_type":"markdown","source":"I don't expect this model to be reall efficient because it's probably far too complex for our needs... but that's just for fun!"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nX_train_, X_test_, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_five_layers(input_dim):\n\n    model = tf.keras.models.Sequential()\n\n    # Add the first Dense layers of 100 units with the input dimension\n    model.add(tf.keras.layers.Dense(100, input_dim=input_dim, activation='sigmoid'))\n\n    # Add four more layers of decreasing units\n    model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n    model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n    model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n    model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n\n    # Add finally the output layer with one unit: the predicted result\n    model.add(tf.keras.layers.Dense(1, activation='relu'))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model_five_layers(input_dim=X_train.shape[1])\n\n# Compile the model with mean squared error (for regression)\nmodel.compile(optimizer='SGD', loss='mean_squared_error')\n\n# Now fit the model on XXX epoches with a batch size of XXX\n# You can add the test/validation set into the fit: it will give insights on this dataset too\nmodel.fit(X_train_, y_train, validation_data=(X_test_, y_test), epochs=200, batch_size=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred, y_pred = model.predict(X_train_), model.predict(X_test_)\nrmse_train_, rmse_test_ = np.sqrt(mean_squared_error(y_train, y_train_pred)), np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"MLP reg\", f'\\t - RMSE on Training  = {rmse_train_:.2f} / RMSE on Test = {rmse_test_:.2f}')\n\n#sc_train, sc_test = get_rmse(model, \"MLP reg\")\n\nmodel_names.append('MLP reg')\nrmse_train.append(rmse_train_)\nrmse_test.append(rmse_test_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion and submission "},{"metadata":{},"cell_type":"markdown","source":"Before making a submission we have to choose the best model i.e with the smallest RMSE."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_score = pd.DataFrame({'model_names' : model_names,\n                         'rmse_train' : rmse_train,\n                         'rmse_test' : rmse_test})\ndf_score = pd.melt(df_score, id_vars=['model_names'], value_vars=['rmse_train', 'rmse_test'])\ndf_score.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 10))\nsns.barplot(y=\"model_names\", x=\"value\", hue=\"variable\", data=df_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The MLP model is indeed useless\n- All linear regression have the same poor result, regularization using L1 or L2 with Lasso & Ridge doesn't change anything which is quite obvious because those models are underfiting and need to be complexified (not regularized!). The same argument applies to ElasticNet which is a combinaison of the 2 above.\n- Gridsearch don't change anything because tuning regularization hyperparameters is not interesting for the same reason.\n- The polynomial use help to improve results a little bit.\n- Gradient Boosting & XGBoost Regressors are performing well without too overfitting.\n- ExtraTree Regressor is out\n- Random Forrest & LGBM Regressors are the best but the first one is clearly overfitting, so it would be better to choose the second one : LGBM Regressor"},{"metadata":{},"cell_type":"markdown","source":"Now let's see how do we have to sumbit our answer"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_sample = pd.read_csv(\"../input//bike-sharing-demand/sampleSubmission.csv\")\ny_sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/bike-sharing-demand/test.csv\")\ndf_test = change_datetime(df_test)\n\n# keep this col for the submission\ndatetimecol = df_test[\"datetime\"]\n\ntest_cols_dropped = ['datetime',\n 'atemp',\n 'month_str',\n 'season',\n 'dow_str',\n 'weather_str']\n\ndf_test = df_test.drop(columns=test_cols_dropped)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We train our model on the whole data set (not only the splitted part). The more data we use, the better would be the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_reg = lgbm.LGBMRegressor()\nlgbm_reg.fit(X, y)\ny_pred_final = lgbm_reg.predict(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"datetime\": datetimecol,\n        \"count\": [max(0, x) for x in y_pred_final]\n    })\nsubmission.to_csv('bike_prediction_output.csv', index=False)\n\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Submit to kaggle, this model scores 0.41233. Not bad :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}