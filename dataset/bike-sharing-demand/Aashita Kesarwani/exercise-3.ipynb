{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Exercise notebook for the third session (30 min)\n\nThis is the exercise notebook for the third session of the [Machine Learning workshop series at Harvey Mudd College](http://www.aashitak.com/ML-Workshops/). Please feel free to ask for help from the instructor and/or TAs."},{"metadata":{},"cell_type":"markdown","source":"First we import python modules:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.preprocessing import PolynomialFeatures\n\nimport warnings\nwarnings.simplefilter('ignore')","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will tackle the [Bike Sharing Demand](https://www.kaggle.com/c/bike-sharing-demand/overview) dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/'\nrides = pd.read_csv(path + 'train.csv')\nrides.head()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"              datetime  season  holiday  ...    casual  registered  count\n0  2011-01-01 00:00:00       1        0  ...         3          13     16\n1  2011-01-01 01:00:00       1        0  ...         8          32     40\n2  2011-01-01 02:00:00       1        0  ...         5          27     32\n3  2011-01-01 03:00:00       1        0  ...         3          10     13\n4  2011-01-01 04:00:00       1        0  ...         0           1      1\n\n[5 rows x 12 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>datetime</th>\n      <th>season</th>\n      <th>holiday</th>\n      <th>workingday</th>\n      <th>weather</th>\n      <th>temp</th>\n      <th>atemp</th>\n      <th>humidity</th>\n      <th>windspeed</th>\n      <th>casual</th>\n      <th>registered</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2011-01-01 00:00:00</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>9.84</td>\n      <td>14.395</td>\n      <td>81</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>13</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2011-01-01 01:00:00</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>9.02</td>\n      <td>13.635</td>\n      <td>80</td>\n      <td>0.0</td>\n      <td>8</td>\n      <td>32</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2011-01-01 02:00:00</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>9.02</td>\n      <td>13.635</td>\n      <td>80</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>27</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2011-01-01 03:00:00</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>9.84</td>\n      <td>14.395</td>\n      <td>75</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>10</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2011-01-01 04:00:00</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>9.84</td>\n      <td>14.395</td>\n      <td>75</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Data Fields**\n\n* datetime - hourly date + timestamp    \n* season -  1 = spring, 2 = summer, 3 = fall, 4 = winter   \n* holiday - whether the day is considered a holiday  \n* workingday - whether the day is neither a weekend nor holiday  \n* weather -   \n    1: Clear, Few clouds, Partly cloudy, Partly cloudy   \n    2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist   \n    3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds   \n    4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog   \n* temp - temperature in Celsius  \n* atemp - \"feels like\" temperature in Celsius  \n* humidity - relative humidity  \n* windspeed - wind speed  \n* casual - number of non-registered user rentals initiated  \n* registered - number of registered user rentals initiated  \n* count - number of total rentals  "},{"metadata":{},"cell_type":"markdown","source":"Let us look at the *datetime* values."},{"metadata":{"trusted":false},"cell_type":"code","source":"rides['datetime'].values[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we perform some feature engineering and data pre-processing similar to what we practised in the previous two sessions. "},{"metadata":{"trusted":false},"cell_type":"code","source":"from datetime import datetime\n\n# We extract 'month', 'hour', 'weekday' from the 'datetime' column\ndef extract_from_datetime(rides):\n    rides[\"date\"] = rides[\"datetime\"].apply(lambda x : x.split()[0])\n    rides[\"hour\"] = rides[\"datetime\"].apply(lambda x : x.split()[1].split(\":\")[0])\n    rides[\"weekday\"] = rides[\"date\"].apply(lambda dateString : \n                            datetime.strptime(dateString,\"%Y-%m-%d\").weekday())\n    rides[\"month\"] = rides[\"date\"].apply(lambda dateString : \n                            datetime.strptime(dateString,\"%Y-%m-%d\").month)\n    return rides\n\n# We one-hot encode the categorical features\ndef one_hot_encoding(rides):\n    dummy_fields = ['season', 'weather', 'month', 'hour', 'weekday']\n    for each in dummy_fields:\n        dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)\n        rides = pd.concat([rides, dummies], axis=1)\n    return rides\n\n# We drop the columns that are redundant now\ndef drop_features(rides):\n    features_to_drop = ['datetime', 'date', \n                        'month', 'hour', 'weekday', \n                        'season', 'weather']\n\n    rides = rides.drop(features_to_drop, axis=1)\n    return rides\n\n# Now we aggregate all the above defined functions inside a function\ndef feature_engineering(rides):\n    rides = extract_from_datetime(rides)\n    rides = one_hot_encoding(rides)\n    rides = drop_features(rides)\n    return rides\n\n# Now we apply all the above defined functions to the rides dataframe\nrides = feature_engineering(rides)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The reason we defined all the steps as functions and bundled them into another function `feature_engineering` is so as to reuse the code for processing the data from `test.csv` file for which we will make predictions at the end."},{"metadata":{"trusted":false},"cell_type":"code","source":"rides.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rides.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rides.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For all algorithms using gradient descent for minimizing the cost function, normalizing features helps speed up the learning process. This is because otherwise the features with values higher in magnitudes will dominate the updates. See [here](https://www.coursera.org/lecture/machine-learning/gradient-descent-in-practice-i-feature-scaling-xx3Da) and [here](https://gist.github.com/oskarth/3469833). We substract the qunatitative features by their mean and divide by their standard deviation to redistribute them to have mean 0 and standard deviation 1. \n$$ x' = \\frac{x - \\mu}{\\sigma} $$\n![](https://www.jeremyjordan.me/content/images/2018/01/Screen-Shot-2018-01-23-at-2.27.20-PM.png)"},{"metadata":{"trusted":false},"cell_type":"code","source":"quantitative_features = ['temp', 'atemp', 'humidity', 'windspeed']\n\n# Store scalings in a dictionary so we can convert back later\nscaled_features = {}\nfor each in quantitative_features:\n    mean, std = rides[each].mean(), rides[each].std()\n    scaled_features[each] = [mean, std]\n    rides.loc[:, each] = (rides[each] - mean)/std","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Next we extract the target variables from the dataframe\ntarget = rides[['casual', 'registered', 'count']]\ntarget = np.log1p(target)\nrides = rides.drop(['casual', 'registered', 'count'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we split the data into training and validation set."},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(rides, target,\n                                        random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train a linear regression model using the training set and calculate the $R^2$ score for both training and validation set."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You should get the following $R^2$ values for the training and validation set.  \n`R-squared score (training): 0.641\nR-squared score (validation): 0.625`"},{"metadata":{},"cell_type":"markdown","source":"Let us try polynomial regression with degree 2. First we get polynomial features."},{"metadata":{"trusted":false},"cell_type":"code","source":"poly2 = PolynomialFeatures(degree=2)\nX_poly2 = poly2.fit_transform(rides)\nX_train_poly2, X_valid_poly2, y_train_poly2, y_valid_poly2 = train_test_split(X_poly2, \n                                                    target['count'], random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train a polynomial regression model using [`LinearRegression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) on polynomial features and call it `polyreg2`. "},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train a polynomial regression coupled with Ridge and call it `polyreg2_ridge`. Tune the regularization parameter alpha."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train a polynomial regression coupled with Lasso and call it `polyreg2_lasso`. Tune the regularization parameter alpha."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let us try polynomial regression with degree 3."},{"metadata":{"trusted":false},"cell_type":"code","source":"poly3 = PolynomialFeatures(degree=3)\nX_poly3 = poly3.fit_transform(rides)\nX_train_poly3, X_valid_poly3, y_train_poly3, y_valid_poly3 = train_test_split(X_poly3, \n                                                    target['count'], random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"polyreg3 = LinearRegression().fit(X_train_poly3, y_train_poly3)\n\npolyreg3_train_score = polyreg3.score(X_train_poly3, y_train_poly3)\npolyreg3_valid_score = polyreg3.score(X_valid_poly3, y_valid_poly3)\n\nprint('R-squared score (training): {:.3f}'\n     .format(polyreg3_train_score))\nprint('R-squared score (validation): {:.3f}'\n     .format(polyreg3_valid_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This suggests the model has overfitted to the training set excessively. Nonetheless, the very high $R^2$ looks promising, so we use regularization on the the polynomial regression with degree 3 features. "},{"metadata":{},"cell_type":"markdown","source":"Train the polynomial regression for degree 3 coupled with Ridge and call it `polyreg3_ridge`. Tune the regularization parameter alpha starting with a not so high value, say 10."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train the polynomial regression for degree 3 coupled with Lasso and call it `polyreg3_lasso`. Try a few differnt values for the regularization parameter alpha."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the following function for the root mean-squared error (RMSE), compare the different regression models, preferably by plotting a graph. Similarly, plot a graph to compare the $R^2$ scores as well."},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_rmse(reg):\n    y_pred_train = reg.predict(X_train_poly)\n    train_rmse = np.sqrt(mean_squared_error(y_train_poly, y_pred_train))\n    y_pred_valid = reg.predict(X_valid_poly)\n    valid_rmse = np.sqrt(mean_squared_error(y_valid_poly, y_pred_valid))\n    return train_rmse, valid_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next steps:\n* Read the test.csv file into a dataframe\n* Feature engineer the dataframe in exactly the same way as above by using function `feature_engineering`.\n* Scale the quantitative variables the same way as above\n* Train a model\n* Predict\n* Convert the predictions using exponential (since our model is built using log for the target variable)\n* Create a dataframe for the results with the right format\n* Save it into csv file and submit"},{"metadata":{},"cell_type":"markdown","source":"### Acknowledgement:\n\nThe credits for the images used in the above are as follows.\n- Image 1: https://commons.wikimedia.org/wiki/File:Gaussian_kernel_regression.png\n\nFor the feature engineering of the data, inspiration is taken from the following two publically shared sources:\n* Udacity Github Repository: https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-bikesharing\n* Kaggle kernel by Vivek Srinivasan: https://www.kaggle.com/viveksrinivasan/eda-ensemble-model-top-10-percentile\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}