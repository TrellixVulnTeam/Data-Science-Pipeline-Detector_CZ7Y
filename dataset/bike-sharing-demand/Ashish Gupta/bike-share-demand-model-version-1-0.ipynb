{"cells":[{"metadata":{"_uuid":"4c30bb8a705adfe5d280a59470acf79a296aa744"},"cell_type":"markdown","source":"I have taken some references Vivek Srinivasan EDA & Ensemble Model (Top 10 Percentile) to build this kernel"},{"metadata":{"trusted":false,"_uuid":"d4897d15d0fefff040467aeb5064477a4f3ba378"},"cell_type":"code","source":"# Import the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\nimport warnings\nimport os\nfrom six.moves import urllib\nimport matplotlib\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2b32baa07d817b21d158944faa285346e79eff27"},"cell_type":"code","source":"#Add All the Models Libraries\n\n# Scalers\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.utils import shuffle\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\n\n# Models\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error\n\n#regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom scipy.stats import reciprocal, uniform\n\n# Cross-validation\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.model_selection import cross_validate\n\n# GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#Common data processors\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom scipy import sparse\n\n#Accuracy Score\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bf8da7c1e3036c0eae8d0dc3a6d72ffbfe2e39b7"},"cell_type":"code","source":"# to make this notebook's output stable across runs\nnp.random.seed(123)\n\n# To plot pretty figures\n%matplotlib inline\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"51c93ddefc2c2ded99cfd1dff8c7a868a1f29f88"},"cell_type":"code","source":"#merge the data for feature engineering and later split it, just before applying Data Pipeline\nTrainFile = pd.read_csv(\"../input/train.csv\") #read the data from the csv file.\nTestFile = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3a7951a622466757bf3c39eb219769a06977a253"},"cell_type":"code","source":"TrainFile.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2a2b89697406429a264d6bac02de4a1ca00b81a4"},"cell_type":"code","source":"TestFile.info()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"a07a8b3b2a1cfcbc82aa824f2b3c06100541af28"},"cell_type":"code","source":"TrainFile.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e5d33c64110237f262dc4277598a35fb337e9d5b"},"cell_type":"code","source":"TestFile.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ce7b902cc3e1a8a35305a527834ef2593865d37d"},"cell_type":"code","source":"TrainFile.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"031fd30d6da12f93eafc11bcafbc7c7459ef5148"},"cell_type":"code","source":"TestFile.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f04149bd0b52bc684432f4a36c7f65ceff90f072"},"cell_type":"code","source":"TrainFile.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cf5647a0cd4d17cdc4574a740064d691401a88ad"},"cell_type":"code","source":"TestFile.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eff1a34185e475082514abbfdbb65b572319e381"},"cell_type":"markdown","source":"Create New Columns from DataTime Column"},{"metadata":{"trusted":false,"_uuid":"b129337c8cb5f21b5f6abc18b6d01ff1ce52a32e"},"cell_type":"code","source":"DataFile = TrainFile.append(TestFile,sort=False)\nDataFile.reset_index(inplace=True)\nDataFile.drop('index',inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"003b739cb1ab4b42bb44389d8ecea380e2a0a677"},"cell_type":"code","source":"DataFile.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5e9b03f42d52b07bdaf28f895e6ab183ad5d8dfd"},"cell_type":"code","source":"DataFile['date'] = DataFile.datetime.apply(lambda x: x.split()[0])\nDataFile['hour'] = DataFile.datetime.apply(lambda x: x.split()[1].split(\":\")[0]).astype(\"int\")\nDataFile['year']  = DataFile.datetime.apply(lambda x: x.split()[0].split(\"-\")[0])\nDataFile['month'] = DataFile.date.apply(lambda x: datetime.strptime(x,\"%Y-%m-%d\").month)\nDataFile['weekday'] = DataFile.date.apply(lambda x: datetime.strptime(x,\"%Y-%m-%d\").weekday())\nDataFile = DataFile.drop([\"datetime\", \"date\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cf414029319c10f7811bbde6aac3acc4c017025"},"cell_type":"markdown","source":"Convert Season and Weather into Categorical Values"},{"metadata":{"trusted":false,"_uuid":"3b67629afa41cf692006aa09d8d6c72f76050524"},"cell_type":"code","source":"DataFile[\"season\"] = DataFile.season.map({1: \"Spring\", 2 : \"Summer\", 3 : \"Fall\", 4 :\"Winter\" })","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6569d1e1b1f5370c36e7d23b17f65e628ef5fa86"},"cell_type":"code","source":"DataFile[\"weather\"] = DataFile.weather.map({1: \" Clear + Few clouds + Partly cloudy + Partly cloudy\",\\\n                                        2 : \" Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \", \\\n                                        3 : \" Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\", \\\n                                        4 :\" Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \" })","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"394297f7179c5e18f21e85aec34ec793ba818fbc"},"cell_type":"code","source":"categoryVariableList = [\"hour\",\"weekday\",\"month\",\"season\",\"weather\",\"holiday\",\"workingday\",\"year\"]\nfor var in categoryVariableList:\n    DataFile[var] = DataFile[var].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5faca6d626925cd7385056d4141edca9ebde27fa"},"cell_type":"code","source":"import seaborn as sn\n\ncorrelation_matrix = DataFile[:].corr()\nmask = np.array(correlation_matrix)\nmask[np.tril_indices_from(mask)] = False\nfigure = plt.gcf()\nfigure.set_size_inches(20,10)\nsn.heatmap(data=correlation_matrix, mask=mask, square=True, annot=True, cbar=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3b54805e4f86d509f9f44ae120ce233da93f3f50"},"cell_type":"code","source":"#DROP the column atemp because of correlation.\nDataFile = DataFile.drop(labels='atemp', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"23f69de73adfb9727bbd694cc7826483309b17a4"},"cell_type":"code","source":"## Exploratory Outlier Analysis - Idea from https://www.kaggle.com/viveksrinivasan/eda-ensemble-model-top-10-percentile\nfig,axes = plt.subplots(nrows=2,ncols=2)\nfig.set_size_inches(20,15)\nsn.boxplot(data = DataFile, x='hour', y='count', orient = 'v', ax = axes[1][1])\nsn.boxplot(data = DataFile, x='humidity', y='count', orient = 'v', ax = axes[0][1])\nsn.boxplot(data = DataFile, x='windspeed',y='count', orient = 'v', ax = axes[1][0])\nsn.boxplot(data = DataFile, y='count', orient = 'v', ax = axes[0][0])\n\naxes[0][0].set(ylabel='count',title=\"Box Plot On Count\")\naxes[0][1].set(xlabel='humidity', ylabel='Count',title=\"Box Plot On Count for Humidity range\")\naxes[1][0].set(xlabel='windspeed', ylabel='Count',title=\"Box Plot On Count for different wind speeds\")\naxes[1][1].set(xlabel='workingday', ylabel='Count',title=\"Box Plot On Count Across Working Day\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"b65a31d907ae9abb1c70156a68f76f70e39f41df"},"cell_type":"code","source":"# Visualizations : Check the counts vs Season, Weather, Hour, Weekday - idea taken from https://www.kaggle.com/viveksrinivasan/eda-ensemble-model-top-10-percentile\nfig,(ax1,ax2,ax3)= plt.subplots(nrows=3)\nfig.set_size_inches(12, 30)\nsortOrder = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\nhueOrder = [\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\n\nhourAggregated = pd.DataFrame(DataFile.groupby([\"hour\",\"season\"],sort=True)[\"count\"].mean()).reset_index()\nsn.pointplot(x=hourAggregated[\"hour\"], y=hourAggregated[\"count\"],hue=hourAggregated[\"season\"], data=hourAggregated, join=True,ax=ax1)\nax1.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across Season\",label='big')\n\nhourAggregated = pd.DataFrame(DataFile.groupby([\"hour\",\"weather\"],sort=True)[\"count\"].mean()).reset_index()\nsn.pointplot(x=hourAggregated[\"hour\"], y=hourAggregated[\"count\"],hue=hourAggregated[\"weather\"], data=hourAggregated, join=True,ax=ax2)\nax2.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across Weather\",label='big')\n\nhourTransformed = pd.melt(DataFile[[\"hour\",\"casual\",\"registered\"]], id_vars=['hour'], value_vars=['casual', 'registered'])\nhourAggregated = pd.DataFrame(hourTransformed.groupby([\"hour\",\"variable\"],sort=True)[\"value\"].mean()).reset_index()\n\nsn.pointplot(x=hourAggregated[\"hour\"], y=hourAggregated[\"value\"],hue=hourAggregated[\"variable\"],hue_order=[\"casual\",\"registered\"], data=hourAggregated, join=True,ax=ax3)\nax3.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across User Type\",label='big')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"68ef4315e4c538b898c646a46180d22d7aaa3e6d"},"cell_type":"code","source":"DataFile = DataFile.drop(['casual','registered'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0ffc9e6ff2b133a2fb52767f1e765f986a37feb6"},"cell_type":"code","source":"# Removing the potential outliers from the training set and maintain the test set\n\ntest_set = DataFile[~pd.notnull(DataFile['count'])]\ntrain_set = DataFile[np.abs(DataFile[\"count\"]-DataFile[\"count\"].mean())<=(3*DataFile[\"count\"].std())] \ntest_set = test_set.drop('count',axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9cbdb3d27d8122c5c658ea6390acf7069370e2c7"},"cell_type":"code","source":"test_set.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7d3d29a62359bab96ecd0ed6c73f9d33af7b174b"},"cell_type":"code","source":"train_set.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a3aef592ec0ac49448183bb751a9b7065840b931"},"cell_type":"code","source":"#Check for the missing values to check if any random extraction happened? Validate that shuffle was false\n\nobs = train_set.isnull().sum().sort_values(ascending = False)\npercent = round(train_set.isnull().sum().sort_values(ascending = False)/len(train_set)*100, 2)\npd.concat([obs, percent], axis = 1,keys= ['Number of Observations', 'Percent'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ea3b0a3baa4d0cce7a4a3200033b2614e617a73b"},"cell_type":"code","source":"#Check for the missing values in test data\nobs = test_set.isnull().sum().sort_values(ascending = False)\npercent = round(test_set.isnull().sum().sort_values(ascending = False)/len(test_set)*100, 2)\npd.concat([obs, percent], axis = 1,keys= ['Number of Observations', 'Percent'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b84ecf8e3ee660fcc90f83b25fa3e34c41c336a"},"cell_type":"markdown","source":"One Hot Encoding"},{"metadata":{"trusted":false,"_uuid":"0da23412a531c004f360140538897862b235170e"},"cell_type":"code","source":"# The CategoricalEncoder class will allow us to convert categorical attributes to one-hot vectors.\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d7166c6168938263fcb593704fc8c604871b629a"},"cell_type":"code","source":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"03f3e21d9834ab4f9962f7aee7056593a4d9be1b"},"cell_type":"code","source":"# Now define x and y.\n\n#the Y Variable\ntrain_set_y = train_set[\"count\"].copy()\ntrain_set_y = train_set_y.astype(int)\n\n# Need to reshape y to apply standard scaler.\n\n#the X variables\ntrain_set_X = train_set.drop(\"count\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"43d3eb0dfdda3afb5d1b3a6064cd38100add3046"},"cell_type":"code","source":"cat_pipeline = Pipeline([\n        (\"selector\", DataFrameSelector(['season','weather'])),\n        (\"cat_encoder\", CategoricalEncoder(encoding='onehot-dense')),\n    ])\n\nnum_pipeline = Pipeline([\n        (\"selector\", DataFrameSelector([\"temp\",\"humidity\",\"windspeed\"])),\n        ('scaler', MinMaxScaler())\n    ])\nno_pipeline = Pipeline([\n        (\"selector\", DataFrameSelector(['holiday','workingday','hour','year','month','weekday']))\n    \n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4e6b118ccd498bc5910fa49532ea89b3cff62ad2"},"cell_type":"code","source":"full_pipeline = FeatureUnion(transformer_list=[\n    (\"cat_pipeline\", cat_pipeline),\n    (\"num_pipeline\",num_pipeline),\n    (\"no_pipeline\", no_pipeline)\n    ])\n\nfinal_train_X = full_pipeline.fit_transform(train_set_X)\nfinal_test_X = full_pipeline.transform(test_set)\nfinal_train_y = np.log1p(train_set_y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2727abef04a0e9cc098da1f72dfafe2a82bf25b"},"cell_type":"raw","source":"Now We Build the Models"},{"metadata":{"trusted":false,"_uuid":"72fb405f0b13bae7d3b09fcefe04aa9f67906318"},"cell_type":"code","source":"#pd.DataFrame(final_train_y).to_csv(\"test.csv\")\n#final_train_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5f6b481e29c63de3c5aa5a683ba6f13b6dac8eb3"},"cell_type":"code","source":"#Random Forest Regressor.\nforest_class = RandomForestRegressor(random_state = 42)\n\nn_estimators = [500]\nmax_features = [\"auto\",'sqrt','log2']\n\nparam_grid_forest = {'n_estimators' : n_estimators, 'max_features' : max_features}\n\n\nrand_search_forest = GridSearchCV(forest_class, param_grid_forest, cv = 4, \n                                        scoring='neg_mean_squared_log_error', refit = True, n_jobs = -1, verbose=2)\n\nrand_search_forest.fit(final_train_X, final_train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7e372a160a5f2640894fbb40e6c975ac2289730a"},"cell_type":"code","source":"random_estimator = rand_search_forest.best_estimator_\n\ny_pred_rf= random_estimator.predict(final_train_X)\nrf_msle = mean_squared_error(final_train_y, y_pred_rf)\nrf_rmsle = np.sqrt(rf_msle)\nrf_rmsle","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"48f407c69def2c16da7a5442bb044eabeba67d70"},"cell_type":"code","source":"#ADA Regressor\n\nada_boost = AdaBoostRegressor(random_state = 42)\n\nn_estimators = [500]\n\nparam_grid_ada = {'n_estimators' : n_estimators}\n\nrand_search_ada = GridSearchCV(ada_boost, param_grid_ada, cv = 4, scoring='neg_mean_squared_log_error', refit = True, n_jobs = -1, verbose = 2)\n\nrand_search_ada.fit(final_train_X, final_train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7e5b5b27d1f2ffb812ca80570c19884bf296b381"},"cell_type":"code","source":"ada_estimator = rand_search_ada.best_estimator_\n\ny_pred_ada= ada_estimator.predict(final_train_X)\nada_msle = mean_squared_error(final_train_y, y_pred_ada)\nada_rmsle = np.sqrt(ada_msle)\nada_rmsle","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4b969364b856ff542ac3afefee299324137dac98"},"cell_type":"code","source":"#Implement SVR\n\nSVR_Reg = SVR()\n\nSVR_Reg.fit(final_train_X, final_train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"63ca0739e789b3469aa0002bfa154177333e9e91"},"cell_type":"code","source":"y_pred_svr= SVR_Reg.predict(final_train_X)\nsvr_msle = mean_squared_error(final_train_y, y_pred_svr)\nsvr_rmsle = np.sqrt(svr_msle)\nsvr_rmsle","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0a6f6dc0f1c76c87a60a8608cd99b6b70efcb99f"},"cell_type":"code","source":"GB_Classifier = GradientBoostingRegressor(random_state = 42)\n\nn_estimators = [50,500]\n\nparam_grid_grad_boost_class = {'n_estimators' : n_estimators}\n\nrand_search_grad_boost_class = GridSearchCV(GB_Classifier, param_grid_grad_boost_class, cv = 4, scoring='neg_mean_squared_log_error', \n                               refit = True, n_jobs = -1, verbose = 2)\n\nrand_search_grad_boost_class.fit(final_train_X, final_train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b1ffcda424587b35faf416310d25a32a16614684"},"cell_type":"code","source":"y_pred_gb= rand_search_grad_boost_class.predict(final_train_X)\ngb_msle = mean_squared_error(final_train_y, y_pred_gb)\ngb_rmsle = np.sqrt(gb_msle)\ngb_rmsle","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b523df840860d8fa210224f0abe531d7fc74fc3b"},"cell_type":"markdown","source":"Random Forest gives the best prediction"},{"metadata":{"trusted":false,"_uuid":"d50bcd59c431591e14b813ed52f0f2487ea3c162"},"cell_type":"code","source":"pred=random_estimator.predict(final_test_X)\nd={'datetime':TestFile['datetime'],'count':np.exp(pred)}\nans=pd.DataFrame(d)\nans.to_csv('answer.csv',index=False) ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}