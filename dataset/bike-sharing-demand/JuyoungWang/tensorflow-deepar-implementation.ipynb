{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bike sharing problem\n\nYou can get the dataset here: https://www.kaggle.com/competitions/bike-sharing-demand\nIt is worth mentioning the neural network-based models I am presenting were NOT hyperparameter-tunned, as I am using M1 macbook. For any inquiery, and README.md, please comment on \"https://github.com/juyoungwang/Tensorflow_DeepAR\".\n\nThis is a toy code I made to do some bike-sharing demand time series forecasting, using the manually implemented DeepAR (https://arxiv.org/abs/1704.04110) architecture with Tensorflow (Zhang et al. already have nice Torch implementation for this, which is the reason I decided to make the Tenworflow one).\n\nTo run the code, you should first put your data downloaded from https://www.kaggle.com/competitions/bike-sharing-demand into the data folder of the repository.\n\nI have three neural-network-based time series forecasting methods implemented below:\n\n* A naive LSTM-based one, which has only one LSTM as the intermediate layer.\n* DeepAR architecture, without ancestral sampling functionality (which does not use the previous time step forecasting value as the new input to predict the next time step demand).\n* DeepAR architecture, with ancestral sampling functionality (the one which uses previous time step forecast as a new input for the next step forecasts.). \n\nMy DeepAR implementation differs the other DeepAR tensorflow or pytorch implementaions in the following aspects:\n* My implmentation handles categorical features, by applying embedding layers to each of them in a seperate manner.\n* After applying embedding, both ordinal and categorical embedded features are concatenated to be used as the input for the LSTM.\n* As I used min-max scaler for data, I assumed forecasting value always higher than 0. Because of this reason, I applied \"relu\" activation for the sake of mean value prediction.\n* Sampling-based forecasting functionality is not implemented, due to my limited computational resource (M1 macbook with no GPU). Though, its implementation is quiet straight forward.\n\nAs the model with ancestral sampling functionality is NON-SEQUENTIAL, I had to use custom training loop with state transfer functionality of keras.lstm. To the best of my knowledge, existing tensorflow implementations were mostly using sequential model supported by Keras, and they did not have properly implemented this ancestral sampling part, which is the reason I made this implementation.\n\n**NOTES**: \n* I have not done any single hyperparameter tunning of the models. These are just implemented for fun, using my M1 macbook, which does not even have CUDA.\n* Maybe modularizing the codes would be my next step, though, now I am preparing coding interviews, as I am looking for AI/ML/OR jobs in korea to complete my military service.\n* To \"best\" train the model, because of the way training data is designed (only approx the first 20 days of months are contained), a careful time series window selection is required. Such a step is skipped in this notebook.\n\nLet's start with the basics.\n\n## 1. GBR\n### 1.1. Raw data processor for gradient boosted regression algorithm\nWe will:\n* drop [\"casual\", \"registered\"] columns, as they are unknown before observing the true demand data.\n* separate datetime into year, month, day and hour, since we would like to use them as covariates information. Note that the dataset is registered in hourly basis, meaning, we will only consider year, month, day and hour as time covariates.","metadata":{"id":"sKRo0dbrtEXa"}},{"cell_type":"code","source":"import seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set(rc={'figure.figsize':(15,7.5)})","metadata":{"id":"U7-FnAchtEXc","execution":{"iopub.status.busy":"2022-06-25T00:18:59.576546Z","iopub.status.idle":"2022-06-25T00:18:59.577025Z","shell.execute_reply.started":"2022-06-25T00:18:59.576788Z","shell.execute_reply":"2022-06-25T00:18:59.576811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntrain = pd.read_csv(\"../input/bike-sharing-demand/train.csv\", parse_dates = [\"datetime\"])\ntest = pd.read_csv(\"../input/bike-sharing-demand/test.csv\", parse_dates = [\"datetime\"])\nfeatures = list(train)[:-3]\nlabel = list(train)[-1]\n\ndef preprocessor(df, features):\n    df[\"year\"] = pd.to_datetime(df['datetime']).dt.year\n    df[\"month\"] = pd.to_datetime(df['datetime']).dt.month\n    df[\"day\"] = pd.to_datetime(df['datetime']).dt.day\n    df[\"hour\"] = pd.to_datetime(df['datetime']).dt.hour\n    try:\n        return df[[\"year\", \"month\", \"day\", \"hour\"] + features[1:]], df[\"count\"]\n    except:\n        return df[[\"year\", \"month\", \"day\", \"hour\"] + features[1:]]","metadata":{"id":"yZvabFsItEXd","execution":{"iopub.status.busy":"2022-06-25T00:18:59.574144Z","iopub.status.idle":"2022-06-25T00:18:59.57465Z","shell.execute_reply.started":"2022-06-25T00:18:59.574403Z","shell.execute_reply":"2022-06-25T00:18:59.574428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x, train_y = preprocessor(train, features)\ntest_x = preprocessor(test, features)","metadata":{"id":"PlYgCVVttEXe","execution":{"iopub.status.busy":"2022-06-25T00:18:59.57862Z","iopub.status.idle":"2022-06-25T00:18:59.579095Z","shell.execute_reply.started":"2022-06-25T00:18:59.578856Z","shell.execute_reply":"2022-06-25T00:18:59.578879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Code below shows there is no nan, neither null data in our dataset:","metadata":{"id":"iyZqJlIRtEXe"}},{"cell_type":"code","source":"print(train_x.info())\nprint(train_y.isnull().sum())\nprint(test_x.info())","metadata":{"id":"0YSTc4d6tEXe","outputId":"e5177151-940e-49fb-e954-9d95dcccad00","execution":{"iopub.status.busy":"2022-06-25T00:18:59.580864Z","iopub.status.idle":"2022-06-25T00:18:59.581344Z","shell.execute_reply.started":"2022-06-25T00:18:59.581086Z","shell.execute_reply":"2022-06-25T00:18:59.581108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2. Gradient boosting regressor\nI brought the code below from https://www.kaggle.com/code/kongnyooong/bike-sharing-demand-for-korean-beginners/notebook, just for the comparison purposes with the neural network-based method. I was about to implement this myself, but since there is already implemented one, why? For further details, please check the code above.","metadata":{"id":"_Zl_GucjtEXf"}},{"cell_type":"markdown","source":"## 2. Deep leearning\n\n### 2.1. Data processing\n* Unlike tree-based methods, such as XGBoost, neural network requires (or it better performs with) data normalization (or standardization).\n* For the sake of data-scaling, we will perform minmax normalization.\n* For ancestral sampling purpose, we will append $z_{t-1}$ to input vector $x_t$, i.e. the RNN input vector will be $(z_{t-1},x_t)$ for timestep $t$, where $(z_s)_s$ is target series, and $(x_s)_s$ is known covariates.","metadata":{"id":"PfU-7InxtEXg"}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\ntrain = pd.read_csv(\"../input/bike-sharing-demand/train.csv\", parse_dates = [\"datetime\"])\ntest = pd.read_csv(\"../input/bike-sharing-demand/test.csv\", parse_dates = [\"datetime\"])\nfeatures = list(train)[:-3]\nlabel = list(train)[-1]\n\ndef preprocessor(df, features):\n    df[\"year\"] = pd.to_datetime(df['datetime']).dt.year\n    df[\"month\"] = pd.to_datetime(df['datetime']).dt.month\n    df[\"day\"] = pd.to_datetime(df['datetime']).dt.day\n    df[\"hour\"] = pd.to_datetime(df['datetime']).dt.hour\n    try:\n        return df[[\"year\", \"month\", \"day\", \"hour\"] + features[1:]], df[\"count\"]\n    except:\n        return df[[\"year\", \"month\", \"day\", \"hour\"] + features[1:]]\n\ntrain_x, train_y = preprocessor(train, features)\ntest_x = preprocessor(test, features)","metadata":{"id":"KEhevNdztEXg","execution":{"iopub.status.busy":"2022-06-25T00:19:33.512954Z","iopub.execute_input":"2022-06-25T00:19:33.513305Z","iopub.status.idle":"2022-06-25T00:19:34.206898Z","shell.execute_reply.started":"2022-06-25T00:19:33.513273Z","shell.execute_reply":"2022-06-25T00:19:34.205966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the sake of implementational convenience, we will assume year, month, day, hour are continuous features. However, we strongly agree it is highly debatable if we would like to treat them as categorical feature or not.","metadata":{"id":"nyIlMwsStEXh"}},{"cell_type":"code","source":"train_x.head()","metadata":{"id":"Tw4ZCbAEtEXh","outputId":"d50831b2-13ec-407b-b049-65f1d821c4ee","execution":{"iopub.status.busy":"2022-06-25T00:19:35.564059Z","iopub.execute_input":"2022-06-25T00:19:35.564651Z","iopub.status.idle":"2022-06-25T00:19:35.586852Z","shell.execute_reply.started":"2022-06-25T00:19:35.564617Z","shell.execute_reply":"2022-06-25T00:19:35.585933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that there is high concentration of windspeed = 0, which is theoretically impossible.","metadata":{"id":"36tLfVeztEXh"}},{"cell_type":"code","source":"plt.hist(train_x[\"windspeed\"])","metadata":{"id":"VXlBqrBBtEXi","outputId":"4d1107d4-4c82-4e84-e1fb-028299848ea6","execution":{"iopub.status.busy":"2022-06-25T00:19:38.133428Z","iopub.execute_input":"2022-06-25T00:19:38.133778Z","iopub.status.idle":"2022-06-25T00:19:38.351194Z","shell.execute_reply.started":"2022-06-25T00:19:38.133749Z","shell.execute_reply":"2022-06-25T00:19:38.350275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Assuming windspeed unavailable data were replaced by 0, let as drop this column. \n* Furthermore, both temp and atemp are almost linearly related with each other. Let us also drop atemp column, to reduce any issues due to high correlation.\n* Lastly, we will change the domain of season feature to be between 0 and 3, instead of using 1 and 4.","metadata":{"id":"H6iBcSpztEXi"}},{"cell_type":"code","source":"cont_feat = ['year', 'month', 'day', 'hour', 'temp', 'humidity']\ndisc_feat = ['season', 'holiday', 'workingday', 'weather']\ntrain_x['season'] = train_x['season'] - 1\ntrain_x['weather'] = train_x['weather'] - 1\ntest_x['season'] = test_x['season'] - 1\ntest_x['weather'] = test_x['weather'] - 1\ntrain_x = train_x[cont_feat + disc_feat]\ntest_x = test_x[cont_feat + disc_feat]","metadata":{"id":"eOAeUE-BtEXj","outputId":"3b63b31d-2cba-4ff7-f319-abb9537f2bb2","execution":{"iopub.status.busy":"2022-06-25T00:19:40.114299Z","iopub.execute_input":"2022-06-25T00:19:40.114653Z","iopub.status.idle":"2022-06-25T00:19:40.127636Z","shell.execute_reply.started":"2022-06-25T00:19:40.114625Z","shell.execute_reply":"2022-06-25T00:19:40.126586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now have to apply MinMax scaling technique to continuous features:","metadata":{"id":"_xvYDb8wtEXj"}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nMMSc_feature = MinMaxScaler()\nMMSc_feature.fit(train_x[cont_feat])\ntrain_x[cont_feat] = MMSc_feature.transform(train_x[cont_feat])\ntest_x[cont_feat] = MMSc_feature.transform(test_x[cont_feat])","metadata":{"id":"c48_0AputEXj","execution":{"iopub.status.busy":"2022-06-25T00:19:43.138402Z","iopub.execute_input":"2022-06-25T00:19:43.138745Z","iopub.status.idle":"2022-06-25T00:19:43.20961Z","shell.execute_reply.started":"2022-06-25T00:19:43.138717Z","shell.execute_reply":"2022-06-25T00:19:43.208681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MMSc_label = MinMaxScaler()\nMMSc_label.fit(np.asarray(train_y).reshape(-1,1))\ntrain_y = MMSc_label.transform(np.asarray(train_y).reshape(-1,1))","metadata":{"id":"rOK0AX-VtEXk","execution":{"iopub.status.busy":"2022-06-25T00:19:43.822903Z","iopub.execute_input":"2022-06-25T00:19:43.823304Z","iopub.status.idle":"2022-06-25T00:19:43.829886Z","shell.execute_reply.started":"2022-06-25T00:19:43.823247Z","shell.execute_reply":"2022-06-25T00:19:43.828733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let us separate train dataset into both \"other train\" and \"validation\".","metadata":{"id":"D5w6dERAtEXk"}},{"cell_type":"code","source":"train = train_x.copy()\ntrain[\"label\"] = train_y\nn = len(train)\ntrain_df = train[0:int(n*0.7)]\nval_df = train[int(n*0.7):int(n*0.9)]\ntest_df = train[int(n*0.9):]","metadata":{"id":"j5sFR19ytEXl","execution":{"iopub.status.busy":"2022-06-25T00:19:45.552596Z","iopub.execute_input":"2022-06-25T00:19:45.552944Z","iopub.status.idle":"2022-06-25T00:19:45.561726Z","shell.execute_reply.started":"2022-06-25T00:19:45.552916Z","shell.execute_reply":"2022-06-25T00:19:45.560791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is shown in a variety of literature, that time series forcasting performance can be drastically enhanced by using the \"previous target series value\" as inputs. However, for the sake of simplicity of the code, we will skip such a step.","metadata":{"id":"_xmudrnKtEXl"}},{"cell_type":"markdown","source":"### 2.2. Tensorflow implementation for LSTM-based models\n\n* Normalized log-transformed label has range 0 to 1. Since > 0, we would like to use ReLU() as the output activation function.\n\n### 2.2.1. Time series window generation\n\nBelow, we will make tensorflow iterators for windowed arrays of format (batch_size, time steps, features).","metadata":{"id":"94teIkQhtEXl"}},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Embedding, Dropout, Bidirectional, LSTM, Dense, TimeDistributed, Softmax, Multiply, Lambda, Reshape\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, TensorBoard\nimport tensorflow_probability as tfp\n","metadata":{"id":"FX-IML_utEXl","execution":{"iopub.status.busy":"2022-06-25T00:19:55.278916Z","iopub.execute_input":"2022-06-25T00:19:55.27947Z","iopub.status.idle":"2022-06-25T00:19:55.286015Z","shell.execute_reply.started":"2022-06-25T00:19:55.279442Z","shell.execute_reply":"2022-06-25T00:19:55.285235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer, train = True):\n  dataset = tf.data.Dataset.from_tensor_slices(series)\n  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n  dataset = dataset.shuffle(shuffle_buffer)\n  if train:\n    dataset = dataset.batch(batch_size).prefetch(1)\n  else:\n    dataset = dataset.batch(series.shape[0]).prefetch(1)\n  return dataset","metadata":{"id":"yVz4zNl_tEXm","execution":{"iopub.status.busy":"2022-06-25T00:20:12.091121Z","iopub.execute_input":"2022-06-25T00:20:12.091603Z","iopub.status.idle":"2022-06-25T00:20:12.105634Z","shell.execute_reply.started":"2022-06-25T00:20:12.091562Z","shell.execute_reply":"2022-06-25T00:20:12.104535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window_size = 24\nbatch_size = 32\nshuffle_buffer = 1000\ntrain_dat = windowed_dataset(train_df, window_size, batch_size, shuffle_buffer)\nvalidation_dat = windowed_dataset(val_df, window_size, batch_size, shuffle_buffer)\ntest_dat = windowed_dataset(test_df, window_size, batch_size, shuffle_buffer, train = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T00:20:14.685624Z","iopub.execute_input":"2022-06-25T00:20:14.685961Z","iopub.status.idle":"2022-06-25T00:20:17.934429Z","shell.execute_reply.started":"2022-06-25T00:20:14.685932Z","shell.execute_reply":"2022-06-25T00:20:17.933419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each train iterate are of the form:","metadata":{"id":"YQDIbXc5tEXn"}},{"cell_type":"code","source":"print(next(train_dat.as_numpy_iterator()).shape)","metadata":{"id":"8Eaip7nktEXn","outputId":"dcde06a5-cb4a-42eb-f7e0-0253970d9caf","execution":{"iopub.status.busy":"2022-06-25T00:18:59.592815Z","iopub.status.idle":"2022-06-25T00:18:59.593584Z","shell.execute_reply.started":"2022-06-25T00:18:59.593337Z","shell.execute_reply":"2022-06-25T00:18:59.59336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dat","metadata":{"id":"Biti6fhZtEXn","outputId":"2516d859-61e5-498c-8714-a94813994203"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"which looks like:","metadata":{"id":"syKFvMuptEXo"}},{"cell_type":"code","source":"pd.DataFrame(data = next(train_dat.as_numpy_iterator())[0, :, :]).head(5)","metadata":{"id":"E6rJqEwAtEXo","outputId":"f47d1101-6fe8-43f2-af3f-f8ef34474fdf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"while validation and test are of shape:","metadata":{"id":"qfn0xEWhtEXo"}},{"cell_type":"code","source":"print(next(validation_dat.as_numpy_iterator()).shape)\nprint(next(test_dat.as_numpy_iterator()).shape)","metadata":{"id":"h5lHRXogtEXo","outputId":"03326cbf-6027-4e33-aee8-9826f6fef35e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And each batch looks like:","metadata":{"id":"hqK0wMSTtEXp"}},{"cell_type":"code","source":"pd.DataFrame(data = next(train_dat.as_numpy_iterator())[0, :, :]).head(5)","metadata":{"id":"yIHfq4cVtEXq","outputId":"9c82f40b-7ce3-4875-bbff-b18724a0a3e9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that columns from\n* 0 to 5 are ordinal features,\n* 6 to 9 are categorical features,\n* 10 is label","metadata":{"id":"rTyHplDNtEXt"}},{"cell_type":"markdown","source":"Let us define list of columns corresponding to:\n* Ordinal features (=continuous features).\n* Categorical features (=discrete features).\n* Label\n\nusing the code below:","metadata":{"id":"MUp-DVv6tEXu"}},{"cell_type":"code","source":"cont_feat_tf = list(range(0,6))\ndisc_feat_tf = list(range(6,10))\nlabel_tf = [10]\n# pd.DataFrame(data = next(train_dat.as_numpy_iterator())[0, :, :]).iloc[:, cont_feat_tf]","metadata":{"id":"Wf3EoKYVtEXw","execution":{"iopub.status.busy":"2022-06-25T00:18:59.744285Z","iopub.execute_input":"2022-06-25T00:18:59.744927Z","iopub.status.idle":"2022-06-25T00:18:59.74969Z","shell.execute_reply.started":"2022-06-25T00:18:59.744895Z","shell.execute_reply":"2022-06-25T00:18:59.748522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2.2. Naive LSTM-based model\n\nNow we will build a naive LSTM-based model for the forcasting problem. First, let's define the loss.","metadata":{"id":"Mu3qT710tEXx"}},{"cell_type":"code","source":"def rmse(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))","metadata":{"id":"CJlQNnp9tEXx","execution":{"iopub.status.busy":"2022-06-25T00:19:00.821097Z","iopub.execute_input":"2022-06-25T00:19:00.821512Z","iopub.status.idle":"2022-06-25T00:19:00.826698Z","shell.execute_reply.started":"2022-06-25T00:19:00.821476Z","shell.execute_reply":"2022-06-25T00:19:00.825706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will define the embedding dimension for discrete features. We will follow a general rule from https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html, which suggests:\n$$\\text{Embedding dimension} = \\sqrt[4]{\\text{Number of categories}}$$\n\nDecimals will be rounded to the closest integers.","metadata":{"id":"fSH7SaGRtEXx"}},{"cell_type":"code","source":"def embedding_dimension(cat_num):\n    return round(cat_num**0.25)\n\ncat_nums = [4, 2, 2, 4] # 4 for season, 2 for holiday, 2 for working day, 4 for weather","metadata":{"id":"8IuThiFJtEXy","execution":{"iopub.status.busy":"2022-06-25T00:19:02.325023Z","iopub.execute_input":"2022-06-25T00:19:02.325522Z","iopub.status.idle":"2022-06-25T00:19:02.331408Z","shell.execute_reply.started":"2022-06-25T00:19:02.325481Z","shell.execute_reply":"2022-06-25T00:19:02.330423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below, we have list of hyper parameters:","metadata":{"id":"cxBjDhDttEXy"}},{"cell_type":"code","source":"embedding_nums = [embedding_dimension(i) for i in cat_nums]\ndropout_rates = 0\nlstm_units = 64\nl1_penalty = 0","metadata":{"id":"Q5kmbdMutEXy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's define the model:","metadata":{"id":"p84VO7dAtEXy"}},{"cell_type":"code","source":"from tensorflow.keras.regularizers import l1\n\n# Input layer\ninp = tf.keras.Input((None, train.shape[1]-1), name = 'Input')\n\n# Continous features\ncont = K.stack([inp[:, :, i] for i in cont_feat_tf], axis = 2)\n\n# Discrete features\ncats_emb = []\nfor d_f in disc_feat_tf:\n    cat_inp = inp[:, :, d_f]\n    d_fm6 = d_f - 6\n    cats_emb.append(Embedding(cat_nums[d_fm6], embedding_nums[d_fm6], name = \"Embedding_{0}\".format(disc_feat[d_fm6]))(cat_inp))\nsub_embs = tf.concat([cont, K.stack(cats_emb, axis = 2)[:, :, :, 0]], axis =\n 2)\nfull_embs = Dropout(dropout_rates)(sub_embs)\n\nLSTM_out = tf.keras.layers.LSTM(lstm_units, return_sequences=True, name = \"LSTM\")(full_embs)\nOutput_dense = Dense(1, activation='relu', name='Output', kernel_regularizer=l1(l1_penalty))\nOutput = TimeDistributed(Output_dense, name='Output_timedistributed')(LSTM_out)\n","metadata":{"id":"y0K32P16tEXz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below, let's compile the model:","metadata":{"id":"dqpzlDYXtEXz"}},{"cell_type":"code","source":"lstm_model = tf.keras.Model(inputs=inp, outputs=Output, name='Basic_LSTM')\nlstm_model.compile(optimizer='adam', loss=rmse)","metadata":{"id":"LLfWovhwtEX0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now, let's train the model:","metadata":{"id":"VHDCYVfMtEX0"}},{"cell_type":"code","source":"epoch_num = 20\nval_loss = []\nfor epoch in range(epoch_num):\n    print(\"We are at epoch = {0}\".format(epoch))\n    for train_batch in train_dat.as_numpy_iterator():\n        tr_x = train_batch[:, :, :-1]\n        tr_y = np.expand_dims(train_batch[:, :, -1], axis=-1)\n        lstm_model.fit(tr_x, tr_y, epochs=1, verbose=0, shuffle=False)\n    vl_track = []\n    for validation_batch in validation_dat.as_numpy_iterator():\n        val_x = validation_batch[:, :, :-1]\n        val_y = np.expand_dims(validation_batch[:, :, -1], axis=-1)\n        vl_track.append(rmse(lstm_model.predict(val_x), val_y))\n    val_loss.append(np.mean(vl_track))","metadata":{"id":"0sg0cioHtEX0","outputId":"a7cf9407-f760-4df5-f433-773931f605ce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nplt.plot(val_loss)\nplt.title(\"Validation loss curve\")\nplt.show()","metadata":{"id":"pcTts5N5tEX0","outputId":"939f9ca5-9367-43b8-c351-ebcdc153d81c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below is a prediction results for the first 50 timesteps:","metadata":{"id":"ubpl-hkAtEX0"}},{"cell_type":"code","source":"pred_val = MMSc_label.inverse_transform(lstm_model(np.expand_dims(np.asarray(val_df.iloc[:50, :-1]), axis = 0))[:, :, 0])[0, :]\ntrue_val = MMSc_label.inverse_transform(np.asarray(val_df.iloc[:50, -1]).reshape(-1,1))[:, 0]\nplt.plot(pred_val, 'r', label = \"Prediction\")\nplt.plot(true_val, 'b', label = \"True value\")\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{"id":"z1E2ZRpUtEX1","outputId":"24ea40e9-bcb5-4cad-9266-fc083d532311"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2.3. DeepAR without ancestral sequence sampling functionality\n\nDeepAR is a probabilistic time series forecasting model developed by Salinas et al. Here, I am just providing a simple implementation for this, in order to assess the performance of the model compared to the naive LSTM-based one.\n\nTo avoid any numerical issues, we will sum 1e-4 to standard deviation.\n\nAlso, my M1 macbook does not support tensorflow probability for some weird reason. I will borrow an existing implementaion of gaussian negative likelihood loss, and motify the code accordingly.","metadata":{"id":"3n7BaC_RtEX1"}},{"cell_type":"code","source":"# https://fairyonice.github.io/Create-a-neural-net-with-a-negative-log-likelihood-as-a-loss.html\ndef nll_gaussian(y_true, y_pred):\n    ## element wise square\n    mu = y_pred[:, :, 0]\n    sigma = y_pred[:, :, 1] + 1e-4\n    y_true = y_true[:, :, 0]\n    square = tf.square(mu - y_true)## preserve the same shape as y_pred.shape\n    ms = tf.divide(square,sigma) + K.log(sigma)\n    ms = tf.reduce_mean(ms)\n    return ms","metadata":{"id":"RrfNoWhAtEX1","execution":{"iopub.status.busy":"2022-06-25T00:19:08.425664Z","iopub.execute_input":"2022-06-25T00:19:08.426011Z","iopub.status.idle":"2022-06-25T00:19:08.432102Z","shell.execute_reply.started":"2022-06-25T00:19:08.425982Z","shell.execute_reply":"2022-06-25T00:19:08.431158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model hyperparameters are:","metadata":{"id":"knRVr2BjtEX1"}},{"cell_type":"code","source":"embedding_nums = [embedding_dimension(i) for i in cat_nums]\ndropout_rates = 0.1\nlstm_units = 40\nl1_penalty = 0.001","metadata":{"id":"b2bmjpzJtEX1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's define the model below.","metadata":{"id":"TlDDWMpItEX1"}},{"cell_type":"code","source":"from tensorflow.keras.regularizers import l1\n\n# Input layer\ninp = tf.keras.Input((None, train.shape[1]-1), name = 'Input')\n\n# Continous features\ncont = K.stack([inp[:, :, i] for i in cont_feat_tf], axis = 2)\n\n# Discrete features\ncats_emb = []\nfor d_f in disc_feat_tf:\n    cat_inp = inp[:, :, d_f]\n    d_fm = d_f - disc_feat_tf[0]\n    cats_emb.append(Embedding(cat_nums[d_fm], embedding_nums[d_fm], name = \"Embedding_{0}\".format(disc_feat[d_fm]))(cat_inp))\nsub_embs = tf.concat([cont, tf.concat(cats_emb, axis = 2)], axis = 2)\nfull_embs = Dropout(dropout_rates)(sub_embs)\n\nLSTM_out_1 = tf.keras.layers.LSTM(lstm_units, return_sequences=True, name = \"LSTM_1\")(full_embs)\nLSTM_out_2 = tf.keras.layers.LSTM(lstm_units, return_sequences=True, name = \"LSTM_2\")(LSTM_out_1)\nLSTM_out_3 = tf.keras.layers.LSTM(lstm_units, return_sequences=True, name = \"LSTM_3\")(LSTM_out_2)\n\nOutput_dense_mu = Dense(1, activation='linear', name='Output_mu', kernel_regularizer=l1(l1_penalty))\nOutput_dense_sigma = Dense(1, activation='softplus', name='Output_sigma', kernel_regularizer=l1(l1_penalty))\nmu = TimeDistributed(Output_dense_mu, name='Mu_TD')(LSTM_out_3)\nsigma = TimeDistributed(Output_dense_sigma, name='Sigma_TD')(LSTM_out_3)\n\nDeepAR_model = tf.keras.Model(inputs=inp, outputs=tf.concat([mu, sigma], axis = 2), name='DeepAR')\nDeepAR_model.compile(optimizer='adam', loss=nll_gaussian)\n","metadata":{"id":"mjZF2Jp8tEX1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch_num = 20\nval_loss = []\nfor epoch in range(epoch_num):\n    print(\"We are at epoch = {0}\".format(epoch))\n    for train_batch in train_dat.as_numpy_iterator():\n        tr_x = train_batch[:, :, :-1]\n        tr_y = np.expand_dims(train_batch[:, :, -1], axis=-1)\n        DeepAR_model.fit(tr_x, tr_y, epochs=1, verbose=0, shuffle=False)\n#     DeepAR_model.save_weights(os.getcwd() + \"/DeepAR_weights/{0}.pth\".format(epoch))\n    vl_track = []\n    for validation_batch in validation_dat.as_numpy_iterator():\n        val_x = validation_batch[:, :, :-1]\n        val_y = np.expand_dims(validation_batch[:, :, -1], axis=-1)\n        vl_track.append(rmse(DeepAR_model.predict(val_x), val_y))\n    val_loss.append(np.mean(vl_track))","metadata":{"id":"cz54axqRtEX2","outputId":"d6657719-749b-4303-fbca-5fdb55399855"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below, we have the prediction results for the validation dataset:","metadata":{"id":"2GS0Uw2HtEX2"}},{"cell_type":"code","source":"pred_val = MMSc_label.inverse_transform(DeepAR_model(np.expand_dims(np.asarray(val_df.iloc[:50, :-1]), axis = 0))[:, :, 0])[0, :]\npred_sigma = MMSc_label.inverse_transform(np.sqrt(DeepAR_model(np.expand_dims(np.asarray(val_df.iloc[:50, :-1]), axis = 0))[:, :, 1]))[0, :]\ntrue_val = MMSc_label.inverse_transform(np.asarray(val_df.iloc[:50, -1]).reshape(-1,1))[:, 0]\n\nz_score = 1.65\nLB = np.clip(pred_val - z_score*pred_sigma, a_min = 0, a_max = None)\nUB = np.clip(pred_val + z_score*pred_sigma, a_min = 0, a_max = None)","metadata":{"id":"ZzM6uTTqtEX2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot below shows approx 90% of prediction interval surrounding mean value forecasts:","metadata":{"id":"TTRluNaptEX2"}},{"cell_type":"code","source":"plt.plot(pred_val, 'r', label = \"Prediction\")\nplt.fill_between(range(50), LB, UB, color='chocolate', alpha=0.2)\nplt.plot(true_val, 'b', label = \"True value\")\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{"id":"RiMmG0gdtEX2","outputId":"5109d8f1-17ce-4057-bc96-d4b489357721"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: A rigorous implementation of DeepAR requires to feed value of previous RNN output as the new input to the forecasts, which is the step I skipped, since I just wanted to play with the dataset.","metadata":{"id":"ofO7cThltEX2"}},{"cell_type":"markdown","source":"### 2.2.4. DeepAR with ancestral forecasting functionality\n\nBelow, we will make DeepAR implementation using Tensorflow. \nDuring the training phase, the network will receive:\n$$(z_{t-1},x_t)$$\nwith aim to predict\n$$z_t$$\nwhere $(z_t)_t$ is target series, and $(x_t)_t$ is covariates.\n\nWe will be using negative Gaussian log likelihood as the objective function.","metadata":{"id":"vuf317u3tEX3"}},{"cell_type":"code","source":"train = train_x.copy()\ntrain[\"label\"] = train_y\nn = len(train)\ntrain_df = train[0:int(n*0.7)]\nval_df = train[int(n*0.7):int(n*0.9)]\ntest_df = train[int(n*0.9):]","metadata":{"id":"NR4irOPEtEX3","execution":{"iopub.status.busy":"2022-06-25T00:19:59.013885Z","iopub.execute_input":"2022-06-25T00:19:59.014219Z","iopub.status.idle":"2022-06-25T00:19:59.022897Z","shell.execute_reply.started":"2022-06-25T00:19:59.014192Z","shell.execute_reply":"2022-06-25T00:19:59.021959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_list = ['prev_label', 'year', 'month', 'day', 'hour', 'temp', 'humidity', 'season', 'holiday', 'workingday', 'weather', 'label']\ntrain_df[\"prev_label\"] = train_df[\"label\"].shift(1).fillna(0)\ntrain_df = train_df[col_list]\nval_df[\"prev_label\"] = val_df[\"label\"].shift(1).fillna(0.469262)\nval_df = val_df[col_list]\ntest_df[\"prev_label\"] = test_df[\"label\"].shift(1).fillna(0.389344)\ntest_df = test_df[col_list]","metadata":{"id":"DeasvWx2tEX3","outputId":"e3464214-6ac9-44d6-820d-28ace290fae9","execution":{"iopub.status.busy":"2022-06-25T00:20:00.155468Z","iopub.execute_input":"2022-06-25T00:20:00.155937Z","iopub.status.idle":"2022-06-25T00:20:00.181482Z","shell.execute_reply.started":"2022-06-25T00:20:00.155895Z","shell.execute_reply":"2022-06-25T00:20:00.180506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train dataset looks like:","metadata":{"id":"rEBd5v-StEX3"}},{"cell_type":"code","source":"train_df.head(5)","metadata":{"id":"hrlmaFGltEX3","outputId":"a410d745-b4ee-402f-96f9-5d2fb6c79351","execution":{"iopub.status.busy":"2022-06-25T00:20:28.638939Z","iopub.execute_input":"2022-06-25T00:20:28.639299Z","iopub.status.idle":"2022-06-25T00:20:28.656352Z","shell.execute_reply.started":"2022-06-25T00:20:28.639248Z","shell.execute_reply":"2022-06-25T00:20:28.655325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's define the iterator:","metadata":{"id":"qoeGrM5xtEX3"}},{"cell_type":"code","source":"window_size = 24\nbatch_size = 32\nshuffle_buffer = 1000\ntrain_dat = windowed_dataset(train_df, window_size, batch_size, shuffle_buffer)\nvalidation_dat = windowed_dataset(val_df, window_size, batch_size, shuffle_buffer)\ntest_dat = windowed_dataset(test_df, window_size, batch_size, shuffle_buffer, train = False)","metadata":{"id":"Efx7zJGytEX4","execution":{"iopub.status.busy":"2022-06-25T00:20:30.873991Z","iopub.execute_input":"2022-06-25T00:20:30.874365Z","iopub.status.idle":"2022-06-25T00:20:30.926689Z","shell.execute_reply.started":"2022-06-25T00:20:30.874334Z","shell.execute_reply":"2022-06-25T00:20:30.925748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that unlike the previous analysis we did, we now have \"previous label\" as also part of features. Note that the very initial value of prev_label was replaced by 0, as this is not known.\n\nBelow, we have DeepAR model parameter definition:","metadata":{"id":"3hMNEEKjtEX4"}},{"cell_type":"code","source":"def embedding_dimension(cat_num):\n    return round(cat_num**0.25)","metadata":{"id":"FPqI9-eKtEX4","execution":{"iopub.status.busy":"2022-06-25T00:20:32.261943Z","iopub.execute_input":"2022-06-25T00:20:32.26235Z","iopub.status.idle":"2022-06-25T00:20:32.267466Z","shell.execute_reply.started":"2022-06-25T00:20:32.262316Z","shell.execute_reply":"2022-06-25T00:20:32.266178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DeepAR_tf_params():\n    def __init__(self):\n        self.cat_nums = None\n        self.cont_feat = None\n        self.cont_feat_tf = None\n        self.disc_feat = None\n        self.disc_feat_tf = None\n        self.dropout_rates = None\n        self.embedding_nums = None\n        self.label_tf = None\n        self.lstm_num = None\n        self.lstm_units = None\n        self.l1_penalty = None\n\ncont_feat = ['prev_label', 'year', 'month', 'day', 'hour', 'temp', 'humidity']\ndisc_feat = ['season', 'holiday', 'workingday', 'weather']\n\nparams = DeepAR_tf_params()\nparams.cat_nums = [4, 2, 2, 4] # Due to numerical issue, we are expanding cat_num\nparams.cont_feat = cont_feat\nparams.disc_feat = disc_feat\nparams.cont_feat_tf = list(range(0,7))\nparams.disc_feat_tf = list(range(7,11))\nparams.dropout_rates = 0\nparams.embedding_nums = [embedding_dimension(i) for i in params.cat_nums]\nparams.lstm_num = 3\nparams.label_tf = [11]\nparams.lstm_units = 40\nparams.l1_penalty = 0\nparams.lr = 0.0003","metadata":{"id":"a5TayRF-tEX4","execution":{"iopub.status.busy":"2022-06-25T00:20:33.265932Z","iopub.execute_input":"2022-06-25T00:20:33.266283Z","iopub.status.idle":"2022-06-25T00:20:33.275495Z","shell.execute_reply.started":"2022-06-25T00:20:33.266235Z","shell.execute_reply":"2022-06-25T00:20:33.274566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's build DeepAR model. We will:\n* Train the network using the true label.\n* Assess the validation performance via ancestral sampling with 6 step forecasting results.\n\n**NOTE**: The model assumes we have at least one categorical and one ordinal features, with the last feature dimension being the label.","metadata":{"id":"qUEBOOYVtEX4"}},{"cell_type":"code","source":"from tensorflow.keras.regularizers import l1\n\nclass DeepAR_tf(tf.keras.Model):\n  def __init__(self, D_p:DeepAR_tf_params):\n    super().__init__(self)\n    self.params = D_p\n    self.cat_embeddings = [Embedding(self.params.cat_nums[d_f - self.params.disc_feat_tf[0]], self.params.embedding_nums[d_f - self.params.disc_feat_tf[0]], name = \"Embedding_{0}\".format(self.params.disc_feat[d_f - self.params.disc_feat_tf[0]])) for d_f in self.params.disc_feat_tf]\n    self.lstms = [tf.keras.layers.LSTM(self.params.lstm_units, return_sequences=True, return_state=True, name = \"LSTM_{0}\".format(i)) for i in range(self.params.lstm_num)]\n    self.mu = Dense(1, activation='linear', name='mu', kernel_regularizer=l1(self.params.l1_penalty))\n    self.sigma = Dense(1, activation='softplus', name='sigma', kernel_regularizer=l1(self.params.l1_penalty))\n\n  def call(self, inputs, states=None, return_state=False, training=False):\n    x = inputs\n    continuous_features = K.stack([x[:, :, i] for i in self.params.cont_feat_tf], axis = 2)\n    discrete_features = tf.concat([self.cat_embeddings[i - self.params.disc_feat_tf[0]](tf.cast(x[:, :, i], tf.int64), training = training) for i in self.params.disc_feat_tf], axis = 2)\n    x = tf.concat([continuous_features, discrete_features], axis = 2)\n    x = Dropout(self.params.dropout_rates)(x)\n    if states is None:\n      states = [self.lstms[i].get_initial_state(x) for i in range(self.params.lstm_num)]\n    for i in range(self.params.lstm_num):\n      x, memory_i, carry_i = self.lstms[i](x, initial_state=states[i], training=training)\n      states[i] = [memory_i, carry_i]\n    mu = self.mu(x, training = training)\n    sigma = self.sigma(x, training = training)\n    x = tf.concat([mu, sigma], axis = 2)\n    if return_state:\n      return x, states\n    else:\n      return x","metadata":{"id":"dcTdv4s4tEX4","execution":{"iopub.status.busy":"2022-06-25T00:24:10.184025Z","iopub.execute_input":"2022-06-25T00:24:10.1844Z","iopub.status.idle":"2022-06-25T00:24:10.200519Z","shell.execute_reply.started":"2022-06-25T00:24:10.184369Z","shell.execute_reply":"2022-06-25T00:24:10.199503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DeepAR_tf_model = DeepAR_tf(params)\nfor i in train_dat.take(1):\n    for t in range(i.shape[1]):\n        tr_x = K.expand_dims(i[:, t, :-1], axis = 1)\n        tr_y = i[:, t, -1]\n        example_pred = DeepAR_tf_model(tr_x)","metadata":{"id":"n_MsM9-ttEX4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DeepAR_tf_model.summary()","metadata":{"id":"bHqaD72WtEX4","outputId":"c9cc2440-559b-44d8-edef-d6e328a4b643"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's compile the model:","metadata":{"id":"m4Y2E9FuyJpr"}},{"cell_type":"code","source":"# https://fairyonice.github.io/Create-a-neural-net-with-a-negative-log-likelihood-as-a-loss.html\ndef nll_gaussian(y_true, y_pred):\n    ## element wise square\n    mu = y_pred[:, 0, 0]\n    sigma = y_pred[:, 0, 1] + 1e-4\n    y_true = tf.cast(y_true, tf.float32)\n    square = tf.square(mu - y_true)## preserve the same shape as y_pred.shape\n    ms = tf.divide(square,sigma) + K.log(sigma)\n    ms = tf.reduce_mean(ms)\n    return ms","metadata":{"id":"hKEX-ixUyOH6","execution":{"iopub.status.busy":"2022-06-25T00:21:30.7157Z","iopub.execute_input":"2022-06-25T00:21:30.716048Z","iopub.status.idle":"2022-06-25T00:21:30.723053Z","shell.execute_reply.started":"2022-06-25T00:21:30.716019Z","shell.execute_reply":"2022-06-25T00:21:30.722013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**NOTE**: Now, I need to write the custom training loop below (get loss += for each time step, and update the weight at the end).\n\nTry to do this like in Pytorch.\n","metadata":{"id":"dSQ1y9v33fcl"}},{"cell_type":"code","source":"def grad(model, inputs, targets):\n  loss_value = tf.zeros(1)\n  stat = None\n  with tf.GradientTape() as tape:\n    for t in range(inputs.shape[1]):\n      features = K.expand_dims(inputs[:, t, :], axis = 1)\n      if stat == None:\n        y_pred, stat = model(features, return_state = True, training = True)\n      else:\n        y_pred, stat = model(features, states = stat, return_state = True, training = True)\n      loss_value += nll_gaussian(targets[:, t], y_pred)\n  return loss_value, tape.gradient(loss_value, model.trainable_variables)","metadata":{"id":"SM2KDfyq3tCJ","execution":{"iopub.status.busy":"2022-06-25T00:21:32.141054Z","iopub.execute_input":"2022-06-25T00:21:32.141451Z","iopub.status.idle":"2022-06-25T00:21:32.149184Z","shell.execute_reply.started":"2022-06-25T00:21:32.14142Z","shell.execute_reply":"2022-06-25T00:21:32.148239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Note: Rerunning this cell uses the same model variables\nDeepAR_tf_model = DeepAR_tf(params)\noptimizer = tf.keras.optimizers.Adam(learning_rate=params.lr)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T00:24:15.224793Z","iopub.execute_input":"2022-06-25T00:24:15.225501Z","iopub.status.idle":"2022-06-25T00:24:15.254838Z","shell.execute_reply.started":"2022-06-25T00:24:15.225453Z","shell.execute_reply":"2022-06-25T00:24:15.253782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Keep results for plotting\ntrain_loss_results = []\n\nnum_epochs = 30\n\nfor epoch in range(num_epochs):\n  epoch_loss_avg = tf.keras.metrics.Mean()\n\n  # Training loop - using batches of 32\n  for i in train_dat:\n    x = i[:, :, :-1]\n    y = i[:, :, -1]\n    # Optimize the model\n    loss_value, grads = grad(DeepAR_tf_model, x, y)\n    optimizer.apply_gradients(zip(grads, DeepAR_tf_model.trainable_variables))\n\n    # Track progress\n    epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n\n  # End epoch\n  train_loss_results.append(epoch_loss_avg.result())\n\n  print(\"Epoch {:03d}: Loss: {:.3f}\".format(epoch, epoch_loss_avg.result()))","metadata":{"id":"Km7SDlnXz6WH","outputId":"d07a6b9b-e666-4d4b-cca1-c3e31ddb0545","execution":{"iopub.status.busy":"2022-06-25T00:24:15.905833Z","iopub.execute_input":"2022-06-25T00:24:15.90665Z","iopub.status.idle":"2022-06-25T00:24:16.317217Z","shell.execute_reply.started":"2022-06-25T00:24:15.9066Z","shell.execute_reply":"2022-06-25T00:24:16.31429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_starting = 0\ni = K.expand_dims(val_df.iloc[:50, :], axis = 0)\nstates = None\nx = i[:, :, :-1].numpy()\ny_hat = tf.zeros(i[:, :, -1].shape).numpy()\ny = i[:, :, -1].numpy()\nstd = tf.zeros(i[:, :, -1].shape).numpy()\nfor t in range(x.shape[1]):\n  features = K.expand_dims(x[:, t, :], axis = 1)\n  if states == None:\n    y_pred, states = DeepAR_tf_model(features, return_state = True, training = False)\n  else:\n    y_pred, states = DeepAR_tf_model(features, states = states, return_state = True, training = False)\n  if t < x.shape[1]-1 and t >= f_starting:\n    x[:, t+1, 0] = y_pred[:, 0, 0]\n    y_hat[:, t] = y_pred[:, 0, 0]\n    std[:, t] = y_pred[:, 0, 1]\n  elif t < f_starting:\n    y_hat[:, t] = x[:, t+1, 0]\n    std[:, t] = 0*y_pred[:, 0, 1]\n  else:\n    y_hat[:, t] = y_pred[:, 0, 0]\n    std[:, t] = y_pred[:, 0, 1]\n      ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_val = MMSc_label.inverse_transform(y_hat)\npred_sigma = MMSc_label.inverse_transform(np.sqrt(std))\ntrue_val = MMSc_label.inverse_transform(y)\n\nz_score = 1.65\nLB = np.clip(pred_val - z_score*pred_sigma, a_min = 0, a_max = None)\nUB = np.clip(pred_val + z_score*pred_sigma, a_min = 0, a_max = None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(pred_val[0, :], 'r', label = \"Prediction\")\nplt.fill_between(range(y_hat.shape[1]), LB[0, :], UB[0, :], color='chocolate', alpha=0.2)\nplt.plot(true_val[0, :], 'b', label = \"True value\")\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"real_test = pd.read_csv(\"../input/bike-sharing-demand/test.csv\", parse_dates = [\"datetime\"])\nreal_test[\"label\"] = np.zeros(real_test.shape[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = list(real_test)[:-3]\nlabel = list(real_test)[-1]\n\ndef preprocessor(df):\n    df[\"year\"] = pd.to_datetime(df['datetime']).dt.year\n    df[\"month\"] = pd.to_datetime(df['datetime']).dt.month\n    df[\"day\"] = pd.to_datetime(df['datetime']).dt.day\n    df[\"hour\"] = pd.to_datetime(df['datetime']).dt.hour\n    return df[['year', 'month', 'day', 'hour', 'temp', 'humidity', 'season', 'holiday', 'workingday', 'weather', 'label']]\n\ntest_true = preprocessor(real_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cont_feat = ['year', 'month', 'day', 'hour', 'temp', 'humidity']\ndisc_feat = ['season', 'holiday', 'workingday', 'weather']\ntest_true['season'] = test_true['season'] - 1\ntest_true['weather'] = test_true['weather'] - 1\ntest_true = test_true[cont_feat + disc_feat + [\"label\"]]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_list = ['prev_label', 'year', 'month', 'day', 'hour', 'temp', 'humidity', 'season', 'holiday', 'workingday', 'weather', 'label']\ntest_true[\"prev_label\"] = test_true[\"label\"].shift(1).fillna(0.19526038)\ntest_true = test_true[col_list]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_true[cont_feat] = MMSc_feature.transform(test_true[cont_feat])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_starting = 0\ni = K.expand_dims(test_true, axis = 0)\nstates = None\nx = i[:, :, :-1].numpy()\ny_hat = tf.zeros(i[:, :, -1].shape).numpy()\nstd = tf.zeros(i[:, :, -1].shape).numpy()\nfor t in range(x.shape[1]):\n  features = K.expand_dims(x[:, t, :], axis = 1)\n  if states == None:\n    y_pred, states = DeepAR_tf_model(features, return_state = True, training = False)\n  else:\n    y_pred, states = DeepAR_tf_model(features, states = states, return_state = True, training = False)\n  if t < x.shape[1]-1 and t >= f_starting:\n    x[:, t+1, 0] = np.clip(y_pred[:, 0, 0], a_min = 0, a_max = None)\n    y_hat[:, t] = np.clip(y_pred[:, 0, 0], a_min = 0, a_max = None)\n    std[:, t] = y_pred[:, 0, 1]\n  elif t < f_starting:\n    y_hat[:, t] = x[:, t+1, 0]\n    std[:, t] = 0*y_pred[:, 0, 1]\n  else:\n    y_hat[:, t] = np.clip(y_pred[:, 0, 0], a_min = 0, a_max = None)\n    std[:, t] = y_pred[:, 0, 1]\n      ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_val = np.round(MMSc_label.inverse_transform(y_hat), decimals = 0)\npred_sigma = MMSc_label.inverse_transform(np.sqrt(std))\n\nz_score = 1.65\nLB = np.clip(pred_val - z_score*pred_sigma, a_min = 0, a_max = None)\nUB = np.clip(pred_val + z_score*pred_sigma, a_min = 0, a_max = None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"init_pred = 100\nfin_pred = 160\nplt.plot(pred_val[0, init_pred:fin_pred], 'r', label = \"Prediction\")\nplt.fill_between(range(fin_pred-init_pred), LB[0, init_pred:fin_pred], UB[0, init_pred:fin_pred], color='chocolate', alpha=0.2)\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_test[\"count\"] = pred_val[0, :]\nsubmission = real_test[[\"datetime\", \"count\"]]\nsubmission.to_csv(\"sub.csv\", index = False, header = True)","metadata":{},"execution_count":null,"outputs":[]}]}