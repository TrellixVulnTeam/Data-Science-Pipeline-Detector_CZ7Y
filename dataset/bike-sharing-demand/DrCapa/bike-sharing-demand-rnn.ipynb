{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intro Bike Sharing Demand Competition\nSource: https://www.kaggle.com/c/bike-sharing-demand\n\nThis notebook is a starter code for all beginners and easy to understand. We will give an introduction to analysis and feature engineering.<br> \nTherefore we focus on\n* a simple analysis of the data,\n* create new features,\n* encoding and\n* scale data.\n\nWe use categorical feature encoding techniques, compare <br>\nhttps://www.kaggle.com/drcapa/categorical-feature-encoding-challenge-xgb <br>\n\nFor this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period."},{"metadata":{},"cell_type":"markdown","source":"# Load Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport datetime\nimport calendar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, SimpleRNN\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.optimizers import RMSprop,Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Input path"},{"metadata":{"trusted":true},"cell_type":"code","source":"path_in = '../input/'\nprint(os.listdir(path_in))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(path_in + 'train.csv', parse_dates = ['datetime'],\n                         index_col='datetime', infer_datetime_format=True)\ntest_data = pd.read_csv(path_in + 'test.csv', parse_dates = ['datetime'],\n                        index_col='datetime', infer_datetime_format=True)\nsamp_subm = pd.read_csv(path_in+'sampleSubmission.csv', parse_dates = ['datetime'],\n                        index_col='datetime', infer_datetime_format=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_bar(data, feature):\n    \"\"\" Plot distribution \"\"\"\n    \n    fig = plt.figure(figsize=(5,3))\n    sns.barplot(x=feature, y='count', data=data, palette='Set3',orient='v')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_timeseries(data, feature):\n    \"\"\" Plot timeseries \"\"\"\n    \n    fig = plt.figure(figsize=(16,9))\n    plt.plot(data.index, data[feature])\n    plt.title(feature)\n    plt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_timeseries_train_and_predict(train, predict, year, month):\n    \"\"\" Compare train and predict data for a month \"\"\"\n    \n    start_date = datetime.datetime(year, month, 1, 0, 0, 0)\n    last_day_of_month = calendar.monthrange(year, month)[1]\n    end_date = datetime.datetime(year, month, last_day_of_month, 23, 0, 0)\n    \n    fig = plt.figure(figsize=(16,9))\n    plt.plot(train[start_date: end_date].index, train.loc[start_date:end_date, 'count'], 'b', label = 'train')\n    plt.plot(predict[start_date: end_date].index, predict.loc[start_date:end_date, 'count'], 'r', label = 'predict')\n    plt.title('Train and Predict')\n    plt.legend()\n    plt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(y_true, y_pred):\n    \"\"\" root_mean_squared_error \"\"\"\n    \n    return K.sqrt(K.mean(K.square(y_pred - y_true)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA\nYou are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters\nnum_months_per_year = 12\nyear_list = [2011, 2012]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samp_subm.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing Timestamps\nThere are losts of missing hours in the train dataset. We expect $ 2 years*12 months*19 days *24 hours = 10944 timesteps$. We count 10886 timesteps so there are 58 missing. Every month in the train data set hast 456 timestamps.  "},{"metadata":{},"cell_type":"markdown","source":"Fill missing timestamps:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_temp = pd.DataFrame(columns=train_data.columns)\n\nfor year in year_list:\n    for month in range(num_months_per_year):\n        start_date = datetime.datetime(year, month+1, 1, 0, 0, 0)\n        end_date = datetime.datetime(year, month+1, 19, 23, 0, 0)\n        # Fill missing timestamps\n        temp = train_data[start_date:end_date].resample('H').asfreq()\n        # Handle missing values\n        features_fill_zero = ['casual', 'registered', 'count']\n        temp[features_fill_zero] = temp[features_fill_zero].fillna(0)\n        features_fill_bbfil = ['season', 'holiday', 'workingday', 'weather']\n        temp[features_fill_bbfil] = temp[features_fill_bbfil].fillna(method='bfill')\n        features_fill_linear = ['temp', 'atemp', 'humidity', 'windspeed']\n        temp[features_fill_linear] = temp[features_fill_linear].interpolate(method='linear')\n        \n        train_data_temp = train_data_temp.append(temp)\n        \ntrain_data = train_data_temp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test Data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_temp = pd.DataFrame(columns=test_data.columns)\nfor year in year_list:\n    for month in range(num_months_per_year):\n        start_date = datetime.datetime(year, month+1, 20, 0, 0, 0)\n        last_day_of_month = calendar.monthrange(year,month+1)[1]\n        end_date = datetime.datetime(year, month+1, last_day_of_month, 23, 0, 0)\n        # Fill missing timestamps\n        temp = test_data[start_date:end_date].resample('H').asfreq()\n        # Handle missing values\n        features_fill_bbfil = ['season', 'holiday', 'workingday', 'weather']\n        temp[features_fill_bbfil] = temp[features_fill_bbfil].fillna(method='bfill')\n        features_fill_linear = ['temp', 'atemp', 'humidity', 'windspeed']\n        temp[features_fill_linear] = temp[features_fill_linear].interpolate(method='linear')\n        \n        test_data_temp = test_data_temp.append(temp)\n        \ntest_data = test_data_temp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The datetime and the seasons are cyclic features. So we can use a cyclic encoding for it."},{"metadata":{},"cell_type":"markdown","source":"# Create new features\nBased on the datetime we create new features for the month, the weekday the hour and the year. These are also cyclic features."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['weekday'] = train_data.index.weekday\ntrain_data['hour'] = train_data.index.hour\n# train_data['month'] = train_data.index.month\n# train_data['year'] = train_data.index.year\ntest_data['weekday'] = test_data.index.weekday\ntest_data['hour'] = test_data.index.hour\n# test_data['month'] = test_data.index.month\n# test_data['year'] = test_data.index.year","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Count Monthly Mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"for year in year_list:\n    for month in range(num_months_per_year): \n        start_date = datetime.datetime(year, month+1, 1, 0, 0, 0)\n        end_date = datetime.datetime(year, month+1, 19, 23, 0, 0)\n        count_mean = train_data[start_date:end_date]['count'].mean()\n        train_data.loc[start_date:end_date, 'count_mean'] = count_mean\n        \n        start_date = datetime.datetime(year, month+1, 20, 0, 0, 0)\n        last_day_of_month = calendar.monthrange(year,month+1)[1]\n        end_date = datetime.datetime(year, month+1, last_day_of_month, 23, 0, 0)\n        test_data.loc[start_date:end_date, 'count_mean'] = count_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Season"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar(train_data, 'season')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Weekday"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar(train_data, 'weekday')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Hour\nAdd new feature hour_group by group of hours."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar(train_data, 'hour')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def hour_group(s):\n    if((0<=s) & (s<=6)):\n        return 1\n    elif((s==7) | (s==9)):\n        return 2\n    elif((s==8) | (s==16) | (s==19)):\n        return 3\n    elif((10<=s) & (s<=15)):\n        return 4\n    elif((s==17) | (s==18)):\n        return 5\n    elif(20<=s):\n        return 6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['hour_group'] = train_data['hour'].apply(hour_group)\ntest_data['hour_group'] = test_data['hour'].apply(hour_group)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding \n## Cyclic features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# features_cyc = ['hour', 'weekday']\n# for feature in features_cyc:\n#     train_data[feature+'_sin'] = np.sin((2*np.pi*train_data[feature])/max(train_data[feature]))\n#     train_data[feature+'_cos'] = np.cos((2*np.pi*train_data[feature])/max(train_data[feature]))\n#     test_data[feature+'_sin'] = np.sin((2*np.pi*test_data[feature])/max(test_data[feature]))\n#     test_data[feature+'_cos'] = np.cos((2*np.pi*test_data[feature])/max(test_data[feature]))\n# train_data = train_data.drop(features_cyc, axis=1)\n# test_data = test_data.drop(features_cyc, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## One Hot Encoding for categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# features_one_hot = ['weekday', 'weather']\n# train_data[features_one_hot] = train_data[features_one_hot].astype(int).astype(str)\n# test_data[features_one_hot] = test_data[features_one_hot].astype(int).astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_data = pd.get_dummies(train_data)\n# test_data = pd.get_dummies(test_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scale Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"scale_features = ['temp', 'atemp', 'humidity', 'hour', 'windspeed']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scaler = MinMaxScaler()\n# train_data[scale_features] = scaler.fit_transform(train_data[scale_features])\n# test_data[scale_features] = scaler.transform(test_data[scale_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict Monthly"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features\nfeature_list = ['holiday', 'workingday', 'weather', 'temp', 'atemp',\n                'humidity', 'windspeed', 'hour_group',\n                'hour', 'weekday', 'count_mean']\nno_features = ['casual', 'registered']#, 'count']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict One Month"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['workingday'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters\nlookback = 3\nhorizon = 2\nmonth = 1\nyear = 2011","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_data(data, lookback, horizon, start, end, kind):\n    X, y = [], []\n    \n    if kind == 'train':\n        start_shifted = start\n    else:\n        start_shifted = start - datetime.timedelta(hours=lookback)\n        \n    temp = data[start_shifted:end].copy()\n    temp.index = range(len(temp.index))\n    \n    start_ix = lookback\n    \n    n_samples = int((len(temp.index)-lookback)/horizon)\n    \n    for i in range(n_samples):\n        end_ix = start_ix+horizon\n        seq_X = temp[(start_ix-lookback):start_ix][data.columns.difference(no_features)].values\n        seq_y = temp[start_ix:end_ix]['count'].values\n        \n        X.append(seq_X)\n        y.append(seq_y)\n        \n        start_ix = end_ix\n    \n    return np.array(X), np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters\nlookback = 24\nhorizon = 1\nmonth = 12\nyear = 2012\n\n# define time range\nstart_train = datetime.datetime(year, month, 1, 0, 0, 0)\nend_train = datetime.datetime(year, month, 19, 23, 0, 0)\nstart_test = datetime.datetime(year, month, 20, 0, 0, 0)\nlast_day_of_month = calendar.monthrange(year,month)[1]\nend_test = datetime.datetime(year, month, last_day_of_month, 23, 0, 0)\n\n\n# concate train and test data\ndata = pd.concat([train_data[start_train:end_train],\n                  test_data[start_test:end_test]])\n\n# scale data\nfeatures_no_scale = ['season', 'holiday', 'workingday', 'count_mean']\ndata_scaled = data.copy()\ndata_mean = data_scaled.mean(axis=0)\ndata_scaled[data.columns.difference(features_no_scale)] -= data_mean\ndata_std = data_scaled.std(axis=0)\ndata_scaled[data.columns.difference(features_no_scale)] /= data_std\n\n# create train and test data\nX_train, y_train = create_data(data_scaled, lookback, horizon, start_train, end_train, 'train')\nprint(X_train[0])\n# define model\nn_steps = X_train.shape[1]\nn_features = X_train.shape[2]\nmodel = Sequential()\nmodel.add(LSTM(64, return_sequences=True, input_shape=(n_steps, n_features)))\nmodel.add(SimpleRNN(64, return_sequences=True, input_shape=(n_steps, n_features)))\nmodel.add(LSTM(32))\nmodel.add(Dense(horizon))\n\n# define optimizer and compile model\noptimizer = Adam(lr=1e-4)\nmodel.compile(optimizer=optimizer, loss='mse', metrics = ['mse'])\n\n# fit model\nhistory = model.fit(X_train, y_train,\n                    epochs=1, verbose=1)\n\n# predict y_test\nstart_test_temp = start_test\n\nn_days = last_day_of_month-19\nfor i in range(n_days*24+1):\n    end_test_temp = start_test+datetime.timedelta(hours=i)\n    #print(start_test_temp, end_test_temp)\n    X_test, y_test = create_data(data_scaled, lookback, horizon, start_test_temp, end_test_temp, 'test')\n    if X_test.size != 0: \n        y_test = model.predict(X_test, verbose=0)\n\n        data_scaled.loc[start_test_temp:(end_test_temp-datetime.timedelta(hours=1)), 'count'] = y_test[0]\n        start_test_temp = end_test_temp\n\n# write in submission data        \n#samp_subm[start_test:end_test]['count'] = data_scaled[start_test:end_test]['count']*data_std['count']+data_mean['count']\nsamp_subm.loc[start_test:end_test, 'count'] = data_scaled.loc[samp_subm[start_test:end_test].index]['count']*data_std['count']+data_mean['count']\nsamp_subm['count'] = np.where(samp_subm['count']<0, 0, samp_subm['count'])\nsamp_subm['count'] = samp_subm['count'].interpolate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = history.history['loss']\nacc = history.history['mse']\nepochs = range(1, len(loss)+1)\nplt.plot(epochs, loss, 'ro', label='loss_train')\nplt.plot(epochs, acc, 'b', label='accuracy_train')\nplt.title('value of the loss function')\nplt.xlabel('epochs')\nplt.ylabel('value of the loss function')\nplt.legend()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_timeseries_train_and_predict(train_data, samp_subm, year, month)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict All Months"},{"metadata":{"trusted":true},"cell_type":"code","source":"# parameters\nlookback = 24\nhorizon = 1\nn_months = 12\nyear_list = [2011, 2012]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fit Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_samples = 0#int(19*24)-lookback\nn_features = len(train_data[train_data.columns.difference(no_features)].columns)\nprint('features used: ', train_data[train_data.columns.difference(no_features)].columns)\nX_train, y_train = np.empty(shape=(n_samples, lookback, n_features)), np.empty(shape=(n_samples, horizon))\n\nfor year in year_list:\n    for month in range(1,n_months+1):\n        #print('year: ', year, ' month: ', month)\n        start_train = datetime.datetime(year, month, 1, 0, 0, 0)\n        end_train = datetime.datetime(year, month, 19, 23, 0, 0)\n        \n        data = train_data[start_train:end_train]\n        \n        # scale data\n        features_no_scale = ['season', 'holiday', 'workingday', 'count_mean']\n        data_scaled = data.copy()\n        data_mean = data_scaled.mean(axis=0)\n        data_scaled[data.columns.difference(features_no_scale)] -= data_mean\n        data_std = data_scaled.std(axis=0)\n        data_scaled[data.columns.difference(features_no_scale)] /= data_std\n                \n        # create train and test data\n        X_temp, y_temp = create_data(data_scaled, lookback, horizon, start_train, end_train, 'train')\n    \n        X_train = np.vstack((X_train, X_temp))\n        y_train = np.vstack((y_train, y_temp))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = X_train.shape[1]\nn_features = X_train.shape[2]\nmodel = Sequential()\nmodel.add(LSTM(64, return_sequences=True, input_shape=(n_steps, n_features), dropout=0.0, recurrent_dropout=0.2,))\nmodel.add(LSTM(64, return_sequences=True, input_shape=(n_steps, n_features), dropout=0.0, recurrent_dropout=0.2,))\n#model.add(LSTM(164, return_sequences=True, input_shape=(n_steps, n_features)))\n#model.add(LSTM(164, return_sequences=True, input_shape=(n_steps, n_features)))\nmodel.add(LSTM(32))\nmodel.add(Dense(horizon))\n\n# define optimizer and compile model\noptimizer = Adam(lr=1e-4)\nmodel.compile(optimizer=optimizer, loss='mse', metrics = ['mse'])\n\n# fit model\nhistory = model.fit(X_train, y_train,\n                    validation_split=0.1,\n                    epochs=150, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss)+1)\nplt.plot(epochs, loss, 'ro', label='loss_train')\nplt.plot(epochs, val_loss, 'b', label='loss_val')\nplt.title('value of the loss function')\nplt.xlabel('epochs')\nplt.ylabel('value of the loss function')\nplt.legend()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"for year in year_list:\n    for month in range(1,n_months+1):\n        #print('year: ', year, ' month: ', month)\n        start_train = datetime.datetime(year, month, 1, 0, 0, 0)\n        end_train = datetime.datetime(year, month, 19, 23, 0, 0)\n        start_test = datetime.datetime(year, month, 20, 0, 0, 0)\n        last_day_of_month = calendar.monthrange(year,month)[1]\n        end_test = datetime.datetime(year, month, last_day_of_month, 23, 0, 0)\n\n        # concate train and test data\n        data = pd.concat([train_data[start_train:end_train],\n                          test_data[start_test:end_test]])\n        \n        # scale data\n        features_no_scale = ['season', 'holiday', 'workingday', 'count_mean']\n        data_scaled = data.copy()\n        data_mean = data_scaled.mean(axis=0)\n        data_scaled[data.columns.difference(features_no_scale)] -= data_mean\n        data_std = data_scaled.std(axis=0)\n        data_scaled[data.columns.difference(features_no_scale)] /= data_std\n        \n        # predict y_test\n        start_test_temp = start_test\n\n        n_days = last_day_of_month-19\n        for i in range(n_days*24+1):\n            end_test_temp = start_test+datetime.timedelta(hours=i)\n        \n            X_test, y_test = create_data(data_scaled, lookback, horizon, start_test_temp, end_test_temp, 'test')\n            if X_test.size != 0: \n                y_test = model.predict(X_test, verbose=0)\n\n                data_scaled.loc[start_test_temp:(end_test_temp-datetime.timedelta(hours=1)), 'count'] = y_test[0]\n                start_test_temp = end_test_temp\n        \n        # write submission file\n        #samp_subm.loc[start_test:end_test, 'count'] = data_scaled[start_test:end_test]['count']*data_std['count']+data_mean['count']\n        samp_subm.loc[start_test:end_test, 'count'] = data_scaled.loc[samp_subm[start_test:end_test].index]['count']*data_std['count']+data_mean['count']\n        samp_subm['count'] = np.where(samp_subm['count']<0, 0, samp_subm['count'])\n        samp_subm['count'] = samp_subm['count'].interpolate()        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate Output"},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'datetime': samp_subm.index,\n                       'count': samp_subm['count']})\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyse Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_timeseries_train_and_predict(train_data, samp_subm, 2012, 12)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}