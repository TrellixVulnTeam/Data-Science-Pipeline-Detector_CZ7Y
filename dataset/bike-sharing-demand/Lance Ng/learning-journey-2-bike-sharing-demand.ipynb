{"cells":[{"metadata":{},"cell_type":"markdown","source":"**About Me**\n\nThis notebook seeks to outline my learning journey. I am considerably new to Data Science and am actively looking for a way to make a career switch into the industry. Luckily in my country, there is an apprenticeship programme that helps people do exactly that. However, during the last application, I was unable to complete the technical assessment in time. It took me a couple of weeks to figure out the solution to the case study, by then the application has already been closed.\n\nDuring my research, I came across the \"Bike Sharing Demand\" kaggle problem. It is almost identical to the case study during my application, and therefore I have chosen to publish it as my next Learning Journey. You can find more details from the original competition here: https://www.kaggle.com/c/bike-sharing-demand/data\n\n\n**#STEP 1: IMPORTING LIBRARIES AND DATASET**\n\nWe start off as usual by importing the dataset and relevant libraries. Then take a brief look at the dataset.\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#STEP 1: IMPORTING LIBRARIES AND DATASET\n\n# Importing the libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_log_error\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the dataset from Kaggle\ntraindf = pd.read_csv('../input/bike-sharing-demand/train.csv')\ntestdf = pd.read_csv('../input/bike-sharing-demand/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print first 5 rows of traindf \ntraindf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print first 5 rows of testdf \ntestdf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Brief look at train set\")\ntraindf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**#STEP 2: EDA AND DATA PRE-PROCESSING**\n\nWe can see that the first column 'datetime' is of 'object' datatype. That means we should convert it into datetime datatype. We can then set them as our dataframe index and perform timeseries analysis.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf['datetime'] = pd.to_datetime(traindf['datetime'])\ntraindf = traindf.set_index('datetime')\n\n# Creating relevant datetime columns\ntraindf['year'] = traindf.index.year\ntraindf['month'] = traindf.index.month\ntraindf['hour'] = traindf.index.hour\n\ntraindf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the data description and info provided, we can also see that 'season' and 'weather' are categorical features. This means that we should encode them before moving on to further analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding categorical data \n\ntraindf['spring'] = (traindf['season']==1)*1\ntraindf['summer'] = (traindf['season']==2)*1\ntraindf['fall'] = (traindf['season']==3)*1\ntraindf['winter'] = (traindf['season']==4)*1\n\ntraindf['clear'] = (traindf['weather']==1)*1\ntraindf['cloudy'] = (traindf['weather']==2)*1\ntraindf['light_snow'] = (traindf['weather']==3)*1\ntraindf['heavy_snow'] = (traindf['weather']==4)*1\n\ntraindf = traindf.drop(['season'],axis=1)\ntraindf = traindf.drop(['weather'],axis=1)\n\ntraindf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There will be further data processing that needs to be done, but let's plot a correlation matrix to find out more about the relationships between these features. We can do this by using the Seaborn heatmap","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for Correlation\ncor = traindf.corr()\nsns.set(font_scale=1.25)\nf, ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(cor, cmap=\"YlGnBu\", annot=True, fmt='.2f', square =True, cbar=False);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the above heatmap as well as feature details provided in the case studies, we can make the following assumptions:\n* 'holiday' is redundant as it does not provide us with additional information that 'workingday' did not\n* 'atemp' is redundant as it does not provide us with additional information that 'temp' did not\n* 'humidity' has an inverse relation with 'count' - we infer that the higher the humidity, the less comfortable it is for people to ride bikes\n* 'casual' and 'registered' adds up to make 'count' - as our target variable is count, we can drop the former 2 columns\n* 'summer', 'winter', 'cloud' and 'heavy_snow' does not appear to have strong influence over the 'count' variable\n\nSo let's proceed to drop the irrelevant columns, to make our model more robust with fewer features. Let's also take the chance to check for missing data among our remaining features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping redundant columns\ncols_to_drop = ['holiday', 'atemp', 'summer', 'winter','cloudy','heavy_snow','casual','registered']\ntraindf = traindf.drop(cols_to_drop, axis=1)\n\n# Check for missing data\ntotal = traindf.isnull().sum().sort_values(ascending=False)\npercent = (traindf.isnull().sum()/traindf.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nprint(missing_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**#STEP 3: TRAIN TEST SPLIT FOR TRAIN DATASET**\n\nNow that we are done preparing the train dataset, we can begin with the next step of modelling. To start off, we will split the trainset so that we can perform cross validation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train_test_split using traindf\ntraindf_X = traindf.drop(['count'],axis=1)\ntraindf_y = traindf[['count']]\nX_train, X_test, y_train, y_test = train_test_split(traindf_X,traindf_y,test_size=.2, random_state=8)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**#STEP 4: MODEL SELECTION AND HYPERPARAMETER TUNING**\n\nLet's perform initial testing based on below 4 models. This is a technique picked out from Raj Mehrotra's notebook: https://www.kaggle.com/rajmehra03/bike-sharing-demand-rmsle-0-3194. I found this to be helpful in quickly selecting an appropriate model for the problem at hand, before doing a deep-dive into hyperparameter tuning.****\n\nAs the nature of this case study is to forecast bike sharing demand. It is important to note that under-forecasting has a more severe consequence than over-forecasting. This is because under-forecasting will result in loss of business opportunity and stunted growth in business. With this in mind, we have chosen to employ RMSLE as our evaluation metric instead of the common RMSE. This is because RMSLE incurs a larger penalty for underestimation - you can read more about it here: https://medium.com/analytics-vidhya/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initial testing based on several models\nmodels=[RandomForestRegressor(),AdaBoostRegressor(),SVR(),KNeighborsRegressor()]\nmodel_names=['RandomForestRegressor','AdaBoostRegressor','SVR','KNeighborsRegressor']\n\n# Compiling initial base results using RMSLE\nrmsle=[]\nmodel_result={}\nfor model in range (len(models)):\n    clf=models[model]\n    clf.fit(X_train,y_train.values.ravel())\n    test_pred=clf.predict(X_test)\n    rmsle.append(np.sqrt(mean_squared_log_error(test_pred,y_test)))\nmodel_result={'Modelling Algo':model_names,'RMSLE':rmsle}   \nrmsle_frame=pd.DataFrame(model_result)\nrmsle_frame","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the RandomForestRegressor easily outperforms the rest of the models with a low RMSLE score of 0.344. So let's move on and commit to the RandomForestRegressor model, and perform hyperparameter tuning. For the purposes of efficiency, I will not run the hyperparameter tuning here as it takes significant runtime.\n\nI have performed GridSearchCV using below parameters:\n* 'n_estimators':[300,500,700]\n* 'bootstrap':[True,False]\n* 'max_depth':[None, 25, 50, 75, 100]\n* 'min_samples_leaf':[1,2,4]\n* 'min_samples_split':[2,5,10]\n* 'max_features':[\"auto\",'sqrt','log2']\n\nAfter an hour of runtime on my own machine, the best parameters appear to be:\n* 'n_estimators':[300]\n* 'bootstrap':[True]\n* 'max_depth':[50]\n* 'min_samples_leaf':[2]\n* 'min_samples_split':[2]\n* 'max_features':['auto']\n\nAs you can see, we will achieve a slight improved score of RMSLE: 0.339","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the best parameters to traindf\nparams_dict={'n_estimators':[300],'bootstrap':[True],'max_depth':[50],'min_samples_leaf':[2],'min_samples_split':[2],'n_jobs':[-1],'max_features':['auto']}\nclf_rf=GridSearchCV(estimator=RandomForestRegressor(),param_grid=params_dict,scoring='neg_mean_squared_log_error',cv=5)\nclf_rf.fit(X_train,y_train.values.ravel())\npred=clf_rf.predict(X_test)\nprint((np.sqrt(mean_squared_log_error(pred,y_test))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**STEP 5: BUILDING DATA PIPELINE AND PREDICTING RESULT FOR TEST DATASET**\n\nNow that we have the model, we will now need to feed in the testdf. However, we'll have to perform all the data preprocessing step that we did for traindf previously. As part of my learning journey, I have also learnt to build data Pipeline. I have read that Pipelines ensure our code to be reusable for future dataset. So let's get started!\n\nBefore building any data Pipeline, it is important to list down the steps that needs to be performed:\n* Convert 'datetime' column to datetime datatype, set as index, and create 'year', 'month', 'day'\n* Create binary columns for 'spring', 'fall', 'clear', 'light_snow'\n* Drop irrelevant columns of 'holiday', 'atemp', 'weather', 'season', 'datetime'\n* Fitting into RandomForestRegressor with best parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting 'datetime' datatype and set as index\nclass DatetimeConverter(BaseEstimator):\n    def __init__(self):\n        pass\n    def fit(self, documents, y=None):\n        return self\n    def transform(self, x_dataset):\n        x_dataset['datetime'] = pd.to_datetime(x_dataset['datetime'])\n        x_dataset = x_dataset.set_index('datetime')\n        x_dataset['year'] = x_dataset.index.year\n        x_dataset['month'] = x_dataset.index.month\n        x_dataset['hour'] = x_dataset.index.hour        \n        \n        return x_dataset\n\n# Creating custom class for binary encoding\nclass BinaryEncoder(BaseEstimator):\n    def __init__(self):\n        pass\n    def fit(self, documents, y=None):\n        return self\n    def transform(self, x_dataset):\n        x_dataset['spring'] = (x_dataset['season'] == 1)*1\n        x_dataset['fall'] = (x_dataset['season'] == 3)*1\n        x_dataset['clear'] = (x_dataset['weather'] == 1)*1\n        x_dataset['light_snow'] = (x_dataset['weather'] == 3)*1\n        \n        return x_dataset\n\n# Create transformer to drop irrelevant columns\ndrop_col = ColumnTransformer(remainder='passthrough',\n                                transformers=[('drop_columns', 'drop', ['holiday', 'atemp', 'weather', 'season'])])\n\nmodel_pipeline = Pipeline(steps=[('converting_datetime', DatetimeConverter()),\n                                 ('create_binary_columns', BinaryEncoder()),\n                                 ('drop_columns', drop_col),\n                                 ('random_forest_regressor', RandomForestRegressor(n_estimators=300,\n                                                                                   bootstrap=True,\n                                                                                   max_depth=50,\n                                                                                   min_samples_leaf=2,\n                                                                                   min_samples_split=2,\n                                                                                   max_features='auto'))])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re-importing the dataset from Kaggle\ntraindf = pd.read_csv('../input/bike-sharing-demand/train.csv')\ntestdf = pd.read_csv('../input/bike-sharing-demand/test.csv')\ntraindf_X = traindf.drop(['count','casual','registered'],axis=1)\ntraindf_y = traindf[['count']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have built our model pipeline and re-imported our dataset, we are now ready to feed the data and generate the output file. The final result when submitted to the kaggle challenge is 0.479. This is definitely not the best result on the scoreboard, but it idd satisfied my objective for publishing this notebook of mine. Thanks for reading.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_pipeline.fit(traindf_X,traindf_y.values.ravel())\nsubmission=pd.DataFrame(model_pipeline.predict(testdf), index=testdf['datetime'])\nsubmission.rename(columns={0:'count'}, inplace=True)\nsubmission.to_csv('submission.csv', index=True)\nprint(submission)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}