{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://fscl01.fonpit.de/userfiles/7611461/image/mobility-electric/mobility-electric_02-w810h462.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Bike Sharing Demand","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Introduction \n\n#### 본 대회는 Regression에 관련된 문제로, 도시 전역의 키오스크 위치 네트워크를 통해 멤버십, 렌탈 및 자전거 반품을 얻는 프로세스가 자동화되는 자전거를 임대하는 수단인 '자전거 공유 시스템'의 대여 정확한 수요 예측이 목표입니다.\n\n#### 본 커널은 다른 분들이 올려주신 커널들을 참고하여 만들었습니다.\n\n#### Baseline은 유튜브 today오늘코드님의 '자전거 수요예측' 영상을 보고 작성하였습니다. \n\n#### https://www.youtube.com/watch?v=Q_MbN-vu_2w&t=2s\n\n#### 해당 영상과 캐글러들의 여러 커널을 보고 공부하며 쉽고, 좋은 성능을 낼 수 있는 커널을 만들고자 하였습니다.\n\n#### 실수나 혹은 지적할만한 문제가 있다면 Feedback 해주시면 감사하겠습니다 !","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 본 커널의 순서입니다.\n\n#### 1. Import & Data check\n\n#### 2. EDA\n\n#### 3. Feature Engineering\n\n#### 4. Modeling & Make Submission","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 데이터에 대한 설명은 변수가 어렵지 않으므로 생략하도록 하겠습니다!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Import & Data check","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom scipy import stats\n\nimport missingno as msno\nplt.style.use('seaborn')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nmpl.rcParams['axes.unicode_minus'] = False\n\n%matplotlib inline\n\n# 기본적인 모듈을 import 합니다. \n# mlp.rcParams['axes.unicode_minus'] = False 코드는 그래프에서 마이너스 폰트가 깨지는 문제에 대한 대처입니다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/bike-sharing-demand/train.csv\", parse_dates = [\"datetime\"])\ndf_test = pd.read_csv(\"../input/bike-sharing-demand/test.csv\", parse_dates = [\"datetime\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df_train.columns:\n    msperc = 'column: {:>10}\\t Percent of NaN value: {:.2f}%'.format(col, 100 * (df_train[col].isnull().sum() / df_train[col].shape[0]))\n    print(msperc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df_test.columns:\n    msperc = 'column: {:>10}\\t Percent of NaN value: {:.2f}%'.format(col, 100 * (df_test[col].isnull().sum() / df_test[col].shape[0]))\n    print(msperc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(df_train, figsize=(12,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"year\"] = df_train[\"datetime\"].dt.year\ndf_train[\"month\"] = df_train[\"datetime\"].dt.month\ndf_train[\"day\"] = df_train[\"datetime\"].dt.day\ndf_train[\"hour\"] = df_train[\"datetime\"].dt.hour\ndf_train[\"minute\"] = df_train[\"datetime\"].dt.minute\ndf_train[\"second\"] = df_train[\"datetime\"].dt.second\n\ndf_test[\"year\"] = df_test[\"datetime\"].dt.year\ndf_test[\"month\"] = df_test[\"datetime\"].dt.month\ndf_test[\"day\"] = df_test[\"datetime\"].dt.day\ndf_test[\"hour\"] = df_test[\"datetime\"].dt.hour\ndf_test[\"minute\"] = df_test[\"datetime\"].dt.minute\ndf_test[\"second\"] = df_test[\"datetime\"].dt.second\n\ndf_train.shape\n\n# datetime의 데이터들을 편하게 알아보기위해 년,월,일,시,분,초 단위로 나눠준다.\n# 확인결과 column이 18로 늘어난것을 볼 수 있다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure, ((ax1,ax2,ax3),(ax4,ax5,ax6)) = plt.subplots(nrows = 2, ncols = 3)\nfigure.set_size_inches(18,10)\n\nsns.barplot(data=df_train, x = \"year\", y = \"count\", ax = ax1)\nsns.barplot(data=df_train, x = \"month\", y = \"count\", ax = ax2)\nsns.barplot(data=df_train, x = \"day\", y = \"count\", ax = ax3)\nsns.barplot(data=df_train, x = \"hour\", y = \"count\", ax = ax4)\nsns.barplot(data=df_train, x = \"minute\", y = \"count\", ax = ax5)\nsns.barplot(data=df_train, x = \"second\", y = \"count\", ax = ax6)\n\nax1.set(ylabel = \"count\", title = \"Rental amount by year\")\nax2.set(ylabel = \"count\", title = \"Rental amount by month\")\nax3.set(ylabel = \"count\", title = \"Rental amount by day\")\nax4.set(ylabel = \"count\", title = \"Rental amount by hour\")\n\n# 각 기준별 대여량을 직관적으로 살펴보기 위해 barplot을 사용하여 시각화 해줍니다.\n# 연도별 대여량을 보면 11년도보다 12년도가 대여량이 늘어난것을 알 수 있습니다. \n# 월별 대여량을 보면 대체로 겨울보다 여름에 대여량이 높은것으로 보입니다.\n# 일별 대여량은 1~19일까지의 데이터만 포함된다. 나머지는 test데이터에 있으며 일단 보류하도록 합니다.\n# 시간별 대여량은 오전8시와 오후5시, 오후6시 가량이 높은데, 출퇴근시간에 이용하는 고객이 많아서 그런것으로 판단됩니다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"dayofweek\"] = df_train[\"datetime\"].dt.dayofweek\ndf_test[\"dayofweek\"] = df_test[\"datetime\"].dt.dayofweek\ndf_train.shape\n\n# 요일 변수를 추가로 생성해줍니다. \n# 제대로 생성되어 column의 수가 19개로 늘어났습니다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"dayofweek\"].value_counts()\n\n# 0~6까지의 범주의 갯수를 세어줍니다. \n# 0 = 월요일 ~ 6 = 일요일입니다.\n# 5,6(토, 일)의 대여량이 조금더 많은 것으로 보아 앞서 살펴본 workingday의 Boxplot 내용과 어느정도 일치하는 것으로 보입니다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(nrows = 5)\nfig.set_size_inches(18,25)\n\nsns.pointplot(data = df_train, x = \"hour\", y = \"count\", ax = ax1)\nsns.pointplot(data = df_train, x = \"hour\", y = \"count\", hue = \"workingday\", ax = ax2)\nsns.pointplot(data = df_train, x = \"hour\", y = \"count\", hue = \"dayofweek\", ax = ax3)\nsns.pointplot(data = df_train, x = \"hour\", y = \"count\", hue = \"weather\", ax = ax4)\nsns.pointplot(data = df_train, x = \"hour\", y = \"count\", hue = \"season\",  ax = ax5)\n\n# 시간의 흐름에 따른 pointplot을 그려줍니다.\n# 첫번째 plot은 역시 앞서 보았던 결과들과 일치합니다.\n# 두번째 plot을 보면 근무일때는 출퇴근시간에 집중되어 있지만, 아닌 경우는 그래프가 낮시간에 전체적으로 평이합니다. \n# 세번째 plot을 보면 역시 두번째 plot과 마찬가지로 토,일은 낮시간에 전체적으로 평이하고, 나머지 요일은 출퇴근 시간에 집중됩니다.\n# 네번째 plot을 보면 맑은날이 역시 가장 높은 대여량을 보이고, 그 다음으로 안개, 가벼운 눈비가 오늘 날씨순으로 대여량이 많습니다. 악천후인 날씨는 없다고 봐도 무방할 정도입니다.\n# 마지막 plot을 보면 봄이 가장적으며, 여름 가을 겨울은 조금의 차이가 나는 것으로 보입니다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_data = df_train[[\"temp\", \"atemp\", \"casual\", \"registered\", \"humidity\", \"windspeed\", \"count\"]]\n\ncolormap = plt.cm.PuBu\n \nf , ax = plt.subplots(figsize = (12,10))\nplt.title('Correlation of Numeric Features with Rental Count',y=1,size=18)\nsns.heatmap(corr_data.corr(), vmax=.8, linewidths=0.1,square=True,annot=True,cmap=colormap,\n            linecolor=\"white\",annot_kws = {'size':14})\n\n# 수치형 데이터들의 상관관계를 알아보기위해 Heatmap을 그려봅니다.\n# count의 열을 보면 가장 눈에 띄게 진한 변수는 registered입니다. 이 변수는 test데이터엔 존재하지 않습니다.\n# 그 다음으로 상관계수가 높은 변수는 casual이다.\n# 온도, 습도, 풍속은 거의 관계가 없다고 볼 수 있다.\n# temp와 atemp는 온도, 체감온도인데 상관계수가 매우 높은걸로 봐서 다중공선성을 보인다고 할 수 있다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize=(12,5))\n\ntemp_scatter_plot = pd.concat([df_train['count'],df_train['temp']],axis = 1)\nsns.regplot(x='temp',y = 'count',data = temp_scatter_plot,scatter= True, fit_reg=True, ax=ax1)\nwindspeed_scatter_plot = pd.concat([df_train['count'],df_train['windspeed']],axis = 1)\nsns.regplot(x='windspeed',y = 'count',data = windspeed_scatter_plot,scatter= True, fit_reg=True, ax=ax2)\nhumidity_scatter_plot = pd.concat([df_train['count'],df_train['humidity']],axis = 1)\nsns.regplot(x='humidity',y = 'count',data = humidity_scatter_plot,scatter= True, fit_reg=True, ax=ax3)\n\n# 유의하지 않다고 판단된 변수에 대해서 Scatterplot을 그려봅니다.\n# windspeed의 경우 0에 많은 데이터가 몰려있습니다. \n# 일반적으로 풍속이 0인 경우가 흔치 않으므로 Null데이터를 0으로 대체한게 아닌가 생각해볼 수 있습니다. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows = 2, figsize = (18,14))\n\nplt.sca(axes[0])\nplt.xticks(rotation = 30, ha = \"right\")\naxes[0].set(ylabel = \"count\", title = \"train windspeed\")\nsns.countplot(data = df_train, x = \"windspeed\", ax = axes[0])\n\nplt.sca(axes[1])\nplt.xticks(rotation = 30, ha = \"right\")\naxes[1].set(ylabel = \"count\", title = \"test windspeed\")\nsns.countplot(data = df_test, x = \"windspeed\", ax = axes[1])\n\n# 풍속에 대한 자세한 시각화를 해준다.\n# 0에대한 값이 매우많다.\n# Feature engineering에서 고쳐주도록 한다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def concatenate_year_month(datetime):\n    return \"{0}-{1}\".format(datetime.year, datetime.month)\n\ndf_train[\"year_month\"] = df_train[\"datetime\"].apply(concatenate_year_month)\ndf_test[\"year_month\"] = df_test[\"datetime\"].apply(concatenate_year_month) \n    \nprint(df_train.shape)\ndf_train[[\"datetime\", \"year_month\"]].head()\n\n# 연도별 데이터의 대여 변화를 좀더 자세히 보기위해 year와 month 데이터를 붙여줍니다. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (18,4))\n\nsns.barplot(data = df_train, y = \"count\", x = \"year_month\")\n\n# 2011년에 비해 2012년의 대여량이 높아졌던것을 확인했었는데, 두 년도를 월별로 시각화하여 좀 더 연속적으로 보도록합니다.\n# 역시나 2012년의 대여량이 대체적으로 높으며, 두 연도 모두 따뜻한 계절의 대여량이 더 높습니다.\n# 전체적으로 보면 갈수록 대여량이 증가하는 추세입니다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter \n\ndef detect_outliers(df, n, features):\n    outlier_indices = []\n    for col in features:\n        Q1 = np.percentile(df[col], 25)\n        Q3 = np.percentile(df[col], 75)\n        IQR = Q3 - Q1\n        \n        outlier_step = 1.5 * IQR\n        \n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index\n        outlier_indices.extend(outlier_list_col)\n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(k for k, v in outlier_indices.items() if v > n)\n        \n    return multiple_outliers\n        \nOutliers_to_drop = detect_outliers(df_train, 2, [\"temp\", \"atemp\", \"casual\", \"registered\", \"humidity\", \"windspeed\", \"count\"])\n\n# train 데이터의 이상치를 탐색합니다.\n# IQR(튜키의 방법)을 이용한 함수를 지정하여 이상치 탐색을 수행합니다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.loc[Outliers_to_drop]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)\ndf_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_num = df_train[[\"count\", \"temp\", \"atemp\", \"casual\", \"registered\", \"humidity\", \"windspeed\"]]\n\nfor col in df_train_num:\n    print('{:15}'.format(col), \n          'Skewness: {:05.2f}'.format(df_train[col].skew()) , \n          '   ' ,\n          'Kurtosis: {:06.2f}'.format(df_train[col].kurt())  \n         )\n\n# 첨도 왜도 확인","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows = 5, ncols = 2, figsize=(16, 18))\nsns.boxplot(data = df_train, y=\"count\", x = \"season\", orient = \"v\", ax = axes[0][0])\nsns.boxplot(data = df_train, y=\"count\", x = \"holiday\", orient = \"v\", ax = axes[0][1])\nsns.boxplot(data = df_train, y=\"count\", x = \"workingday\", orient = \"v\", ax = axes[1][0])\nsns.boxplot(data = df_train, y=\"count\", x = \"weather\", orient = \"v\", ax = axes[1][1])\nsns.boxplot(data = df_train, y=\"count\", x = \"dayofweek\", orient = \"v\", ax = axes[2][0])\nsns.boxplot(data = df_train, y=\"count\", x = \"month\", orient = \"v\", ax = axes[2][1])\nsns.boxplot(data = df_train, y=\"count\", x = \"year\", orient = \"v\", ax = axes[3][0])\nsns.boxplot(data = df_train, y=\"count\", x = \"hour\", orient = \"v\", ax = axes[3][1])\nsns.boxplot(data = df_train, y=\"count\", x = \"minute\", orient = \"v\", ax = axes[4][0])\n\naxes[0][0].set(ylabel = \"count\", title = \"Rental count by season\")\naxes[0][1].set(ylabel = \"count\", title = \"Rental count by holiday\")\naxes[1][0].set(ylabel = \"count\", title = \"Rental count by workingday\")\naxes[1][1].set(ylabel = \"count\", title = \"Rental count by weather\")\naxes[2][0].set(ylabel = \"count\", title = \"Rental count by dayofweek\")\naxes[2][1].set(ylabel = \"count\", title = \"Rental count by month\")\naxes[3][0].set(ylabel = \"count\", title = \"Rental count by year\")\naxes[3][1].set(ylabel = \"count\", title = \"Rental count by hour\")\naxes[4][0].set(ylabel = \"count\", title = \"Rental count by minute\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 1, figsize = (10,6))\ng = sns.distplot(df_train[\"count\"], color = \"b\", label=\"Skewness: {:2f}\".format(df_train[\"count\"].skew()), ax=ax)\ng = g.legend(loc = \"best\")\n\nprint(\"Skewness: %f\" % df_train[\"count\"].skew())\nprint(\"Kurtosis: %f\" % df_train[\"count\"].kurt())\n\n# 첨도와 왜도 확인","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"count_Log\"] = df_train[\"count\"].map(lambda i:np.log(i) if i>0 else 0)\n\nf, ax = plt.subplots(1, 1, figsize = (10,6))\ng = sns.distplot(df_train[\"count_Log\"], color = \"b\", label=\"Skewness: {:2f}\".format(df_train[\"count_Log\"].skew()), ax=ax)\ng = g.legend(loc = \"best\")\n\nprint(\"Skewness: %f\" % df_train['count_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['count_Log'].kurt())\n\ndf_train.drop('count', axis= 1, inplace=True)\n\n# 치우침 해소를위해 로그를 취한다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainWind0 = df_train.loc[df_train[\"windspeed\"] == 0]\ntrainWindNot0 = df_train.loc[df_train[\"windspeed\"] != 0]\n\n# 풍속 변수의 0값 데이터들을 만져주기 위해 0인 값과 아닌 값들로 나눠줍니다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n# RandomForest로 값을 예측해보도록 합니다.\ndef predict_windspeed(data):\n    dataWind0 = data.loc[data[\"windspeed\"] == 0]\n    dataWindNot0 = data.loc[data[\"windspeed\"] != 0]\n    # 풍속 데이터를 0인 값과 아닌 값들로 나눠줍니다.\n    wcol = [\"season\", \"weather\", \"humidity\", \"day\", \"temp\", \"atemp\"]\n    # 예측에 필요한 변수들을 지정합니다.\n    dataWindNot0[\"windspeed\"] = dataWindNot0[\"windspeed\"].astype(\"str\")\n    # 풍속이 0이 아닌 값들을 string데이터로 변환합니다.\n    rf_wind = RandomForestClassifier()\n    rf_wind.fit(dataWindNot0[wcol], dataWindNot0[\"windspeed\"])\n    wind0 = rf_wind.predict(X=dataWind0[wcol])\n    # 랜덤포레스트를 사용하여 wcol의 변수들로 0인 값들을 학습시키고 예측합니다.\n    \n    predictWind0 = dataWind0\n    predictWindNot0 = dataWindNot0\n    # 예측한 값을 넣어줄 데이터프레임을 만듭니다.\n    predictWind0[\"windspeed\"] = wind0\n    # 예측값들을 넣어줍니다.\n    data = predictWindNot0.append(predictWind0)\n    # 0이 아닌 데이터들과 예측값들을 합쳐줍니다.\n    data[\"windspeed\"] = data[\"windspeed\"].astype(\"float\")\n    # 풍속의 데이터들을 다시 float 타입으로 변환합니다.\n    data.reset_index(inplace = True)\n    data.drop(\"index\", inplace = True, axis = 1)\n    \n    return data\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = predict_windspeed(df_train)\ndf_test = predict_windspeed(df_test)\n\nfig, (ax1, ax2) = plt.subplots(nrows = 2, figsize = (18,14))\n\nplt.sca(ax1)\nplt.xticks(rotation = 30, ha = \"right\")\nax1.set(ylabel = \"count\", title = \"train windspeed\")\nsns.countplot(data = df_train, x = \"windspeed\", ax = ax1)\n\nplt.sca(ax2)\nplt.xticks(rotation = 30, ha = \"right\")\nax1.set(ylabel = \"count\", title = \"test windspeed\")\nsns.countplot(data = df_test, x = \"windspeed\", ax = ax2)\n\n# 예측된 풍속데이터 시각화, rotation은 글씨의 각도 조절 파라미\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.get_dummies(df_train, columns = [\"weather\"], prefix = \"weather\")\ndf_test = pd.get_dummies(df_test, columns = [\"weather\"], prefix = \"weather\")\n\ndf_train = pd.get_dummies(df_train, columns = [\"season\"], prefix = \"season\")\ndf_test = pd.get_dummies(df_test, columns = [\"season\"], prefix = \"season\")\n\n#onehotencoding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_data = df_train[[\"count_Log\", \"windspeed\"]]\ncorr_data.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datetime_test = df_test['datetime']\n\ndf_train.drop([\"datetime\", \"registered\",\"casual\",\"holiday\", \"year_month\", \"minute\", \"second\"], axis = 1, inplace = True)\ndf_test.drop([\"datetime\",\"holiday\", \"year_month\", \"minute\", \"second\"], axis = 1, inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model & Make Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \nfrom sklearn import metrics \nX_train = df_train.drop(\"count_Log\", axis = 1).values \ntarget_label = df_train[\"count_Log\"].values \nX_test = df_test.values \nX_tr, X_vld, y_tr, y_vld = train_test_split(X_train, target_label, test_size = 0.2, random_state = 2000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nregressor = GradientBoostingRegressor(n_estimators=2000, learning_rate=0.05,\n                                   max_depth=4, \n                                   min_samples_leaf=15, min_samples_split=10, random_state =42) \n\nregressor.fit(X_tr,y_tr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat = regressor.predict(X_tr)\nplt.scatter(y_tr, y_hat, alpha = 0.2)\nplt.xlabel('Targets (y_tr)',size=18)\nplt.ylabel('Predictions (y_hat)',size=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_test = regressor.predict(X_vld)\nplt.scatter(y_vld, y_hat_test, alpha=0.2)\nplt.xlabel('Targets (y_vld)',size=18)\nplt.ylabel('Predictions (y_hat_test)',size=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification\n\nmodels=[GradientBoostingRegressor()]\nmodel_names=['regressor']\nrmsle=[]\nd={}\nfor model in range (len(models)):\n    clf=models[model]\n    clf.fit(X_tr,y_tr)\n    test_pred=clf.predict(X_vld)\n    rmsle.append(np.sqrt(mean_squared_log_error(test_pred,y_vld)))\nd={'Modelling Algo':model_names,'RMSLE':rmsle}   \nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score \naccuracies = cross_val_score(estimator = regressor, X = X_tr, y = y_tr, cv = 8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracies.mean()) \nprint(accuracies.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"use_logvals = 1 \n\npred_xgb = regressor.predict(X_test) \n\nsub_xgb = pd.DataFrame() \nsub_xgb['datetime'] = datetime_test \nsub_xgb['count'] = pred_xgb \nif use_logvals == 1:\n    sub_xgb['count'] = np.exp(sub_xgb['count'])\n    \nsub_xgb.to_csv('xgb.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}