{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-29T19:19:12.297411Z","iopub.execute_input":"2021-05-29T19:19:12.297742Z","iopub.status.idle":"2021-05-29T19:19:12.306293Z","shell.execute_reply.started":"2021-05-29T19:19:12.297713Z","shell.execute_reply":"2021-05-29T19:19:12.305254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  **Getting train and test data**","metadata":{}},{"cell_type":"code","source":"train_df= pd.read_csv('/kaggle/input/bike-sharing-demand/train.csv')\ntest_df = pd.read_csv(\"/kaggle/input/bike-sharing-demand/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:19:12.845445Z","iopub.execute_input":"2021-05-29T19:19:12.845807Z","iopub.status.idle":"2021-05-29T19:19:12.876592Z","shell.execute_reply.started":"2021-05-29T19:19:12.845778Z","shell.execute_reply":"2021-05-29T19:19:12.875777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above data we can observe that there are both kinds of variables present in the feature\n1. Categorical Variables\n2. Numerical Variables\n1st caegory includes columns: season, holiday, working_day, datetime, \n2nd category includes columns: temp, atmep, humidity, windspeed, registered, causal","metadata":{}},{"cell_type":"markdown","source":"## Checking the number of unique number for categorical variables","metadata":{}},{"cell_type":"code","source":"map_to_uniqueness = {}\nfor cols in train_df.columns: \n    map_to_uniqueness[cols] = train_df[cols].nunique()\nprint(map_to_uniqueness)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:19:12.959664Z","iopub.execute_input":"2021-05-29T19:19:12.960046Z","iopub.status.idle":"2021-05-29T19:19:12.973603Z","shell.execute_reply.started":"2021-05-29T19:19:12.959985Z","shell.execute_reply":"2021-05-29T19:19:12.972645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking null values","metadata":{}},{"cell_type":"code","source":"null_values = {}\nfor cols in train_df.columns:\n    null_values[cols] = train_df[cols].isnull().sum()\nprint(null_values)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:19:13.054227Z","iopub.execute_input":"2021-05-29T19:19:13.054572Z","iopub.status.idle":"2021-05-29T19:19:13.06463Z","shell.execute_reply.started":"2021-05-29T19:19:13.054542Z","shell.execute_reply":"2021-05-29T19:19:13.063631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no null values in any column. Yayy! We are good to go without bothering about imputations and methods!","metadata":{}},{"cell_type":"markdown","source":"## Taking care of datetime series","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nsns.distplot(train_df['count'], bins = 100);\nprint(train_df[\"count\"].skew())\n\ntrain_df.datetime = pd.to_datetime(train_df.datetime)\ntrain_df[\"month\"] = train_df.datetime.dt.month\ntrain_df[\"year\"] = train_df.datetime.dt.year\ntrain_df[\"hour\"] = train_df.datetime.dt.hour\n\ntest_df.datetime = pd.to_datetime(test_df.datetime)\ntest_df[\"month\"] = test_df.datetime.dt.month\ntest_df[\"year\"] = test_df.datetime.dt.year\ntest_df[\"hour\"] = test_df.datetime.dt.hour","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:19:14.131391Z","iopub.execute_input":"2021-05-29T19:19:14.131758Z","iopub.status.idle":"2021-05-29T19:19:14.647155Z","shell.execute_reply.started":"2021-05-29T19:19:14.131725Z","shell.execute_reply":"2021-05-29T19:19:14.64608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that data is not normally distributed so we need to take care of skewness and transform out target variable to a normal distributed target variable. It might also come to mind that why I did not create another column of day but but but since it is already given in the question statemnet that for the training data the days would be from 1 to 19 and for test data, it would be 20 above and since datetime series comes under categorical variable so while one hot encoding training and test variable the two different categories will create a problem for us and we want to simple model.","metadata":{}},{"cell_type":"markdown","source":"## Let's draw some correlation from the data","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ncor = train_df.corr()\nf, ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(cor, vmax=.8, square=True, annot= True);","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:19:14.649013Z","iopub.execute_input":"2021-05-29T19:19:14.649411Z","iopub.status.idle":"2021-05-29T19:19:16.163697Z","shell.execute_reply.started":"2021-05-29T19:19:14.649371Z","shell.execute_reply":"2021-05-29T19:19:16.16273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there a correlation between temp and atemp so we don't need both of them. Looking at the correlation we can also see that registered has a string correlation with target i.e count and causal has a strong correlation with registred so we do not need both of them. Let's get rid of them!","metadata":{}},{"cell_type":"markdown","source":"## Let's do some feature analysis","metadata":{}},{"cell_type":"code","source":"# Removing colums from train which are not in test_df and column which are too much correlated with some other variable\ncols = [\"casual\", \"registered\", \"atemp\", \"datetime\"]\ntrain_df = train_df.drop(cols, axis=1)\ntest_df = test_df.drop([\"atemp\", \"datetime\"], axis =1)\ny = train_df.pop(\"count\")","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:19:16.165401Z","iopub.execute_input":"2021-05-29T19:19:16.165702Z","iopub.status.idle":"2021-05-29T19:19:16.174358Z","shell.execute_reply.started":"2021-05-29T19:19:16.165671Z","shell.execute_reply":"2021-05-29T19:19:16.173488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's have a look at the histogram!","metadata":{}},{"cell_type":"code","source":"# Draw a histogram of training data\n\ntrain_cols = [col for col in list(train_df)]\ntrain_df[train_cols].hist(figsize=(20,20), bins=100, color='blue', alpha=0.5)\nplt.show()\n\n# Just to take a quicklook at the data by looking at their distributions\n\nprint(len(train_df[train_df[\"holiday\"] == 1].holiday))\nprint(len(train_df[train_df[\"holiday\"] == 0].holiday))\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:19:16.175909Z","iopub.execute_input":"2021-05-29T19:19:16.17618Z","iopub.status.idle":"2021-05-29T19:19:19.716399Z","shell.execute_reply.started":"2021-05-29T19:19:16.176154Z","shell.execute_reply":"2021-05-29T19:19:19.715433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\ntest_cols = [col for col in list(test_df)]\ntest_df[test_cols].hist(figsize=(20,20), bins=100, color='red', alpha=0.5)\nplt.show()\nprint(test_df.columns)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:19:19.717552Z","iopub.execute_input":"2021-05-29T19:19:19.717818Z","iopub.status.idle":"2021-05-29T19:19:23.574125Z","shell.execute_reply.started":"2021-05-29T19:19:19.717793Z","shell.execute_reply":"2021-05-29T19:19:23.57314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see from above histograms that both the test data and train data are distributed in pretty much the same way.","metadata":{}},{"cell_type":"code","source":"#standardizing data\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:19:23.575319Z","iopub.execute_input":"2021-05-29T19:19:23.575606Z","iopub.status.idle":"2021-05-29T19:19:23.578956Z","shell.execute_reply.started":"2021-05-29T19:19:23.575577Z","shell.execute_reply":"2021-05-29T19:19:23.578187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's look for bivariate variables","metadata":{}},{"cell_type":"markdown","source":"From above we feel tempted to get rid of few of the outliers. We can say that temp>40 falls prey to outlier category and so is windspeed>50 and when weather is of 4th category that is in winters. We will say, it's not worth it, let's live with it","metadata":{}},{"cell_type":"markdown","source":"In search of Normality!","metadata":{}},{"cell_type":"code","source":"\nfrom scipy.stats import norm\nfrom scipy import stats\nsns.distplot(y, fit=norm);\nfig = plt.figure()\nres = stats.probplot(y, plot=plt)\n\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-29T19:19:23.579756Z","iopub.execute_input":"2021-05-29T19:19:23.580338Z","iopub.status.idle":"2021-05-29T19:19:24.233377Z","shell.execute_reply.started":"2021-05-29T19:19:23.580301Z","shell.execute_reply":"2021-05-29T19:19:24.232385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting categorical and numerical data","metadata":{}},{"cell_type":"code","source":"numerical_cols = [\"windspeed\", \"temp\", \"humidity\"]\ncat_cols = [cols for cols in train_df.columns.to_list() if cols not in numerical_cols]","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:19:24.235417Z","iopub.execute_input":"2021-05-29T19:19:24.235707Z","iopub.status.idle":"2021-05-29T19:19:24.240157Z","shell.execute_reply.started":"2021-05-29T19:19:24.235679Z","shell.execute_reply":"2021-05-29T19:19:24.239089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[cat_cols] = train_df[cat_cols].astype('category')\ntest_df[cat_cols] = test_df[cat_cols].astype('category')","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:19:24.241499Z","iopub.execute_input":"2021-05-29T19:19:24.241786Z","iopub.status.idle":"2021-05-29T19:19:24.270649Z","shell.execute_reply.started":"2021-05-29T19:19:24.241758Z","shell.execute_reply":"2021-05-29T19:19:24.269802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the data into train and test set","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(train_df, y, random_state=5, test_size = 0.2)\n\"\"\"\ncols = [\"temp\", \"humidity\", \"windspeed\"]\nfor col in cols:\n    X_train[cols] = np.log1p(X_train[cols]) \n    X_valid[cols] = np.log1p(X_valid[cols])\n    refined_test_df[cols] = np.log1p(refined_test_df[cols])   \n\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:19:24.271928Z","iopub.execute_input":"2021-05-29T19:19:24.272239Z","iopub.status.idle":"2021-05-29T19:19:24.28582Z","shell.execute_reply.started":"2021-05-29T19:19:24.27221Z","shell.execute_reply":"2021-05-29T19:19:24.284851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transforming the train and validation data to make their distribution normal","metadata":{}},{"cell_type":"code","source":"y_train = np.log1p(y_train) \ny_valid = np.log1p(y_valid)\nprint(X_train.shape)\nprint(X_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:19:24.287195Z","iopub.execute_input":"2021-05-29T19:19:24.287486Z","iopub.status.idle":"2021-05-29T19:19:24.2926Z","shell.execute_reply.started":"2021-05-29T19:19:24.287458Z","shell.execute_reply":"2021-05-29T19:19:24.291951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model tarining","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nnum_pipeline = Pipeline([\n        ('std_scaler', StandardScaler())\n])\nfull_pipeline = ColumnTransformer([\n    ('cat', OneHotEncoder(), cat_cols),\n    ('num', num_pipeline, numerical_cols)\n])\n\n\nX_train_prepared = full_pipeline.fit_transform(X_train)\nX_valid_prepared = full_pipeline.transform(X_valid)\n\nprint(X_train_prepared.shape)\nprint(X_valid_prepared.shape)\n\nxg = XGBRegressor(random_state = 0, learning_rate = 0.2, n_estimators = 150)\n\nxg.fit(X_train_prepared, y_train)\ny_pred = xg.predict(X_valid_prepared)\ny_train_pred = xg.predict(X_train_prepared)\n     \nprint(\"These are rmsle for train and validation sets {}, {}\"\n      .format(np.sqrt(mean_squared_log_error(y_train_pred, y_train)), \n              np.sqrt(mean_squared_log_error(y_pred, y_valid))\n             ))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:19:24.293452Z","iopub.execute_input":"2021-05-29T19:19:24.293697Z","iopub.status.idle":"2021-05-29T19:19:24.863518Z","shell.execute_reply.started":"2021-05-29T19:19:24.293672Z","shell.execute_reply":"2021-05-29T19:19:24.862445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nX_test_prepared = full_pipeline.transform(test_df)\nres  = xg.predict(X_test_prepared)\nprint(res)\nres = np.expm1(res).astype(int)\nprint(res)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:19:24.864935Z","iopub.execute_input":"2021-05-29T19:19:24.865334Z","iopub.status.idle":"2021-05-29T19:19:24.90341Z","shell.execute_reply.started":"2021-05-29T19:19:24.865292Z","shell.execute_reply":"2021-05-29T19:19:24.902303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/bike-sharing-demand/sampleSubmission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:19:24.905168Z","iopub.execute_input":"2021-05-29T19:19:24.905952Z","iopub.status.idle":"2021-05-29T19:19:24.920142Z","shell.execute_reply.started":"2021-05-29T19:19:24.905903Z","shell.execute_reply":"2021-05-29T19:19:24.919051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'datetime': submission.datetime,\n                       'count': res })\nprint(output.shape)\noutput.to_csv('submission.csv', index=False)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-05-29T19:19:24.921638Z","iopub.execute_input":"2021-05-29T19:19:24.922312Z","iopub.status.idle":"2021-05-29T19:19:24.947452Z","shell.execute_reply.started":"2021-05-29T19:19:24.922267Z","shell.execute_reply":"2021-05-29T19:19:24.946433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}