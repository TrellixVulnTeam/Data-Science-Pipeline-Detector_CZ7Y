{"cells":[{"metadata":{},"cell_type":"markdown","source":"#                      Bike Share Challenge:"},{"metadata":{},"cell_type":"markdown","source":"## Part I:"},{"metadata":{},"cell_type":"markdown","source":"**Goal of challenge**:\n   You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.\n   \n   As we have to simply provide a prediction on the total, we need to predict the number of casual rides + the number of registered rides.\n   \n   **The dataset**:<br>\nYou are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month.\n\n**Data fields:**\n\n- datetime - hourly date + timestamp<br>\n- season -  1 = spring, 2 = summer, 3 = fall, 4 = winter <br>\n- holiday - whether the day is considered a holiday<br>\n- workingday - whether the day is neither a weekend nor holiday<br>\n- weather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy<br>\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist<br>\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds<br>\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog <br>\n- temp - temperature in Celsius<br>\n- atemp - \"feels like\" temperature in Celsius<br>\n- humidity - relative humidity<br>\n- windspeed - wind speed<br>\n- casual - number of non-registered user rentals initiated<br>\n- registered - number of registered user rentals initiated<br>\n- count - number of total rentals\n\n**Initial thoughts:**<br>\n- Bike share demand is usually higher on weekends (free floating); however we are only provided with the first 19 days of the month so we can create a variable which says how many weekends in the month provided;<br>\n- Bike share demand is very correlated with good weather (our good weather indicators are: season, weather (1-2-3-4), temp, atemp, humidity, windspeed\n- Casual users: they usually use more on the weekend; they tend to be higher during tourist seasons\n- Registered users: they tend to use more always\n- Total rentals over seasons: usually cyclical over the seasons\n- Holidays: higher number of riders"},{"metadata":{},"cell_type":"markdown","source":"**Load relevant libraries:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nplt.rc(\"font\", size=18)\nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/bike-sharing-demand/train.csv')\ntest = pd.read_csv('../input/bike-sharing-demand/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Unique values per variable:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_values = {}\nfor i in range(1, len(train.columns)-3):\n    unique_values[train.columns[i]] = train[train.columns[i]].unique()\nunique_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part II: EDA and Visualisation"},{"metadata":{},"cell_type":"markdown","source":"**Preparing Out Data For Vis:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Datetime:\n\ndatasets = [train, test]\n\nfor dataset in datasets:\n    dataset['datetime'] = pd.to_datetime(dataset.datetime)\n    dataset['hour'] = dataset['datetime'].apply(lambda x: x.hour)\n    dataset['day'] = dataset['datetime'].apply(lambda x: x.day)\n    dataset['weekday'] = dataset['datetime'].apply(lambda x: x.weekday())\n    dataset['month'] = dataset['datetime'].apply(lambda x: x.month)\n    dataset['year'] = dataset['datetime'].apply(lambda x: x.year)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Names for categorical data:\ntrain_c = train.copy()\ntrain_c['weather'] = train_c['weather'].map({1: 'Good', 2: 'Medium', 3: 'Bad', 4: 'Very Bad'})\ntrain_c['weekday'] = train_c['weekday'].map({0: 'Mon', 1: 'Tue', 2: 'Wed', 3: 'Thur', 4: 'Fri',\n                                            5: 'Sat', 6: 'Sun'})\ntrain_c['month'] = train_c['month'].map({1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', \n                                        6: 'Jun', 7: 'July', 8: 'Aug', 9: 'Sept', 10: 'Oct',\n                                         11: 'Nov', 12: 'Dec'})\ntrain_c['season'] = train_c['season'].map({1: 'Spring', 2: 'Summer', 3: 'Fall', 4: 'Winter'})\ntrain_c['workingday'] = train_c['workingday'].map({0: 'No', 1: 'Yes'})\ntrain_c['holiday'] = train_c['holiday'].map({0: 'No', 1: 'Yes'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Season EDA:**|"},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import mean\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize = (12,8))\nsns.barplot(x = 'season', y = 'count', data = train_c, ci=None, color='salmon',\n            hue = 'year', estimator = mean, ax =ax[0,0])\nax[0,0].set_title('Mean Count by Season hue: Year')\nsns.barplot(x = 'season', y = 'count', data = train_c, ci=None, \n            color = 'salmon', hue = 'weather', estimator = mean, ax = ax[0,1])\nax[0,1].set_title('Mean Count by Season hue: Weather')\nsns.barplot(x = 'month', y = 'count', data = train_c, ci=None, \n            color = 'indigo', hue = 'year', estimator = mean, ax = ax[1,0])\nax[1,0].set_title('Mean Count by Month hue: Year')\nsns.barplot(x = 'month', y = 'count', data = train_c, ci=None, \n            color = 'indigo', hue = 'weather', estimator = mean, ax = ax[1,1])\nax[1,1].set_title('Mean Count by Season hue: Weather')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preliminary observation:\n* We can see a big shift up from 2011 to 2012\n* The very bad weather is associated with January, and yet this is associated with Spring\n(The classification of the winter months as Spring is interesting).\n* We can see that the months / seasons with worse weather indicators have a lower count of rides"},{"metadata":{},"cell_type":"markdown","source":"**Humidity, Temperature:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=4, figsize = (16,5))\nsns.distplot(train_c['windspeed'], ax=ax[0])\nax[0].set_title('Distplot windspeed')\nsns.distplot(train_c['temp'], ax=ax[1])\nax[1].set_title('Distplot temperature')\nsns.distplot(train_c['atemp'], ax=ax[2])\nax[2].set_title('Distplot atemperature')\nsns.distplot(train_c['humidity'], ax=ax[3])\nax[3].set_title('Distplot humidity')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comments:\n* For atemp('feels like temperature') we can see some spikes around the 30 celcius mark;\n* For temp we can see spikes around the 16 degrees marks\n* Other than that the two displots for temperature would show a *relatively* normal distribution;\n* For windspeed we would see a normal distribution except for the spike at 0 which seems to indicate to be an outlier; let's look into these distributions by looking at their outliers a bit closer."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=4, ncols=1, figsize = (12,12))\nsns.boxplot(x='season',y='windspeed', hue= 'weather', data=train_c, palette='winter', ax = ax[0])\nax[0].set_title('Boxplot Wincdspeed by Season: Hue Weather')\nsns.boxplot(x='season',y='temp', hue= 'weather', data=train_c, palette='winter', ax = ax[1])\nax[1].set_title('Boxplot Temperature by Season: Hue Weather')\nsns.boxplot(x='season',y='atemp', hue= 'weather', data=train_c, palette='winter', ax = ax[2])\nax[2].set_title('Boxplot ATemperature by Season: Hue Weather')\nsns.boxplot(x='season',y='humidity', hue= 'weather', data=train_c, palette='winter', ax = ax[3])\nax[3].set_title('Boxplot Humidity by Season: Hue Weather')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Comments:*\n- What we can see here is that out of whisker bounds instances tend to be the lower end for humidity are fall under the occasions of bad weather; particularly prevalent are the Summer and Fall seasons;\n- Winter, for temp atemp and humidity seems to have the least amount of outliers;\n- For temperature we see highest amount of outliers with Fall season for good weather; same with atemp;"},{"metadata":{},"cell_type":"markdown","source":"**Day of week and times:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, figsize = (12,8))\ngrouped_hours = pd.DataFrame(train_c.groupby(['hour'], sort=True)['casual', 'registered', 'count'].mean())\ngrouped_hours.plot(ax=ax)\nax.set_xticks(grouped_hours.index.to_list())\nax.set_xticklabels(grouped_hours.index)\nplt.xticks(rotation=45)\nplt.title('Avg Count by Hour')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Preliminary observations:*\n- We can see that registered users follow commuter patterns, whilst casual users do not - they have higher peaks during the afternoon (potentially from weekend use);"},{"metadata":{},"cell_type":"markdown","source":"**Let's look at by day of week:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, figsize = (12,8))\nsns.barplot(x = 'weekday', y = 'count', data = train_c, ci=None, \n            color = 'indigo', estimator = mean, ax = ax)\nax.set_title('Avg Count by Weekday')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Similar usage overall indicating that higher usage during weekends will compensate for commuter usage during weekdays"},{"metadata":{},"cell_type":"markdown","source":"**Can we see a commuter trend?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize = (15,8))\n\nworkingday = train_c.loc[train_c.workingday == 'Yes']\nnot_workingday = train_c.loc[train_c.workingday == 'No']\ngrouped_workingday = pd.DataFrame(workingday.groupby(['hour'], sort=True)['count'].mean())\ngrouped_notworkingday = pd.DataFrame(not_workingday.groupby(['hour'], sort=True)['count'].mean())\n\ngrouped_workingday.plot(ax=ax[0])\nax[0].set_xticks(grouped_workingday.index.to_list())\nax[0].set_xticklabels(grouped_workingday.index)\nax[0].tick_params(labelrotation=45)\nax[0].set_title('Avg Count by Hour - Working Day')\n\ngrouped_notworkingday.plot(ax=ax[1])\nax[1].set_xticks(grouped_notworkingday.index.to_list())\nax[1].set_xticklabels(grouped_notworkingday.index)\nax[1].tick_params(labelrotation=45)\nax[1].set_title('Avg Count by Hour - Not Working Day')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Preliminary observations:*\n- We can see that the different patterns are very clear here: commuter for working days and leisure for non working days meaning that most usage is during the aft (esp. since weekend)"},{"metadata":{},"cell_type":"markdown","source":"## Part III: Any Outliers?"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"ticks\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data=train_c,\n                  y_vars=['count'],\n                  x_vars=['temp', 'atemp', 'humidity', 'windspeed'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data=train_c,\n                  y_vars=['registered'],\n                  x_vars=['temp', 'atemp', 'humidity', 'windspeed'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data=train_c,\n                  y_vars=['casual'],\n                  x_vars=['temp', 'atemp', 'humidity', 'windspeed'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In general we can see that with the casual users temperature seems to be a bigger driver than registered ones."},{"metadata":{},"cell_type":"markdown","source":"**Creating a train without outliers train set:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = train.quantile(0.25)\nQ3 = train.quantile(0.75)\nIQR = Q3 - Q1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['datetime'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_without_outliers =train[~((train < (Q1 - 1.5 * IQR)) |(train > (Q3 + 1.5 * IQR))).any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train original shape\", train.shape[0])\nprint(\"train_without_outliers observations\", train_without_outliers.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's review some of the outliers we saw with our boxplots in the previous analysis of temperature, windspeed and humidity:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=4, ncols=2, figsize = (12,12))\n\nsns.boxplot(x='season',y='windspeed', data=train, palette='winter', ax = ax[0,0])\nax[0,0].set_title('Boxplot Wincdspeed by Season WITH OUTLIER')\nsns.boxplot(x='season',y='windspeed', data=train_without_outliers, palette='winter', ax = ax[0,1])\nax[0,1].set_title('Boxplot Wincdspeed by Season WITHOUT OUTLIER')\n\nsns.boxplot(x='season',y='temp', data=train, palette='winter', ax = ax[1,0])\nax[1,0].set_title('Boxplot Temperature by Season WITH OUTLIERS')\nsns.boxplot(x='season',y='temp', data=train_without_outliers, palette='winter', ax = ax[1,1])\nax[1,1].set_title('Boxplot Temperature by Season WITHOUT OUTLIERS')\n\n\nsns.boxplot(x='season',y='atemp', data=train, palette='winter', ax = ax[2,0])\nax[2,0].set_title('Boxplot ATemperature WITH OUTLIERS')\nsns.boxplot(x='season',y='atemp', data=train_without_outliers, palette='winter', ax = ax[2,1])\nax[2,1].set_title('Boxplot ATemperature by Season WITHOUT OUTLIERES')\n\nsns.boxplot(x='season',y='humidity', data=train, palette='winter', ax = ax[3,0])\nax[3,0].set_title('Boxplot Humidity by Season WITH OUTLIERS')\nsns.boxplot(x='season',y='humidity', data=train_without_outliers, palette='winter', ax = ax[3,1])\nax[3,1].set_title('Boxplot Humidity by Season WITHOUT OUTLIERS')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Comments:**\n* Here we can see that the simple method for removing outliers has worked well wiuth windspeed;\n* Its effect on temperature and atemperature however have not been very successful;\n* This comes down to the fact that *whilst on an aggregate level of, say, windspeed we are able to remove outliers, this may not result on a more granular level (of looking at it on a season by season basis)*\n* Note: arguably, if one were to apply a removing outliers method, it would make sense to do this on a season by season basis if we are looking at variables such as weather;\n* We will therefore go ahead with the regression analysis without the non-outlier set first, but then consider it later to see if it makes any great change."},{"metadata":{},"cell_type":"markdown","source":"**How to deal with outliers?**"},{"metadata":{},"cell_type":"markdown","source":"https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html"},{"metadata":{},"cell_type":"markdown","source":"Summary of scalers:\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PowerTransformer\n\n**MinMaxScaler:**\n- Rescales the data set such that all feature values are in the range [0, 1]\n- MinMaxScaler is very sensitive to the presence of outliers.\n\n**MaxAbScaler:**\n- Differs from the previous scaler such that the absolute values are mapped in the range [0, 1]. On positive only data, this scaler behaves similarly to MinMaxScaler and therefore also suffers from the presence of large outliers.\n\n**RobustScaler:**\n- The centering and scaling statistics of this scaler are based on percentiles and are therefore not influenced by a few number of very large marginal outliers.\n\n**PowerTransformer:**\n- PowerTransformer applies a power transformation to each feature to make the data more Gaussian-like\n- Currently, PowerTransformer implements the Yeo-Johnson and Box-Cox transforms.\n- The power transform finds the optimal scaling factor to stabilize variance and mimimize skewness through maximum likelihood estimation.\n- By default, PowerTransformer also applies zero-mean, unit variance normalization to the transformed output. Note that Box-Cox can only be applied to strictly positive data. Income and number of households happen to be strictly positive, but if negative values are present the Yeo-Johnson transformed is to be preferred.\n\n**QuantileTransformer:**\n- has an additional output_distribution parameter allowing to match a Gaussian distribution instead of a uniform distribution. Note that this non-parametetric transformer introduces saturation artifacts for extreme values.\n\n**QuantileTransformer (uniform output):**\n- QuantileTransformer applies a non-linear transformation such that the probability density function of each feature will be mapped to a uniform distribution. In this case, all the data will be mapped in the range [0, 1], even the outliers which cannot be distinguished anymore from the inliers.\n- As RobustScaler, QuantileTransformer is robust to outliers in the sense that adding or removing outliers in the training set will yield approximately the same transformation on held out data. But contrary to RobustScaler, QuantileTransformer will also automatically collapse any outlier by setting them to the a priori defined range boundaries (0 and 1).\n\n**Normalizer:**\n- The Normalizer rescales the vector for each sample to have unit norm, independently of the distribution of the samples."},{"metadata":{},"cell_type":"markdown","source":"For this project, given the presence of outliers, we will consider the use of RobustScaler()."},{"metadata":{},"cell_type":"markdown","source":"### Conclusions from overall EDA:"},{"metadata":{},"cell_type":"markdown","source":"- Casual users tend to be non working day users\n- Non working day users do not use it for commuter times, rather usage is high in the early afternoon not commuter hours\n- Weather is an important factor for usage but plays a stronger role on casual users\n- Usage by weekday stays roughly the same\n- It changes though by season wiht Spring - which has the most intense weather - reporting to be the season with the least rides"},{"metadata":{},"cell_type":"markdown","source":"## Part IV: Correlations"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr()\nmask = np.array(train.corr())\nmask[np.tril_indices_from(mask)] = False\nfig,ax= plt.subplots()\nfig.set_size_inches(30,15)\nsns.heatmap(train.corr(), mask = mask, vmax = 0.8, square=True, annot=True, center = 0, \n            cmap=\"RdBu_r\", linewidths=.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* For casual and registered: weather seems to be the most contributing factor\n* With casual we also see strong associations with working day (negative)"},{"metadata":{},"cell_type":"markdown","source":"### Identifying the Most Important Factor:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X= train.drop(['count', 'casual', 'registered'], axis = 1)\ny = train['count']\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.3, random_state=5)\n\nrf = RandomForestRegressor(n_estimators=100, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Graphical representation of most important factors:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib as mp\nplt.subplots(figsize=(15,10))\ncore_variables = pd.Series(rf.feature_importances_, index=X.columns)\ncore_variables = core_variables.nlargest(8)\n\n# Colorize the graph based on likeability:\nlikeability_scores = np.array(core_variables)\n \ndata_normalizer = mp.colors.Normalize()\ncolor_map = mp.colors.LinearSegmentedColormap(\n    \"my_map\",\n    {\n        \"red\": [(0, 1.0, 1.0),\n                (1.0, .5, .5)],\n        \"green\": [(0, 0.5, 0.5),\n                  (1.0, 0, 0)],\n        \"blue\": [(0, 0.50, 0.5),\n                 (1.0, 0, 0)]\n    }\n)\n\nplt.title('Most Important Features')\n\n#make the plot\ncore_variables.plot(kind='barh', color=color_map(data_normalizer(likeability_scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now let's do it with the without outliers dataset:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\ncontinuous_features = ['temp','atemp', 'humidity', 'windspeed']\ndata = [train_without_outliers]\nfor dataset in data:\n    for col in continuous_features:\n        transf = dataset[col].values.reshape(-1,1)\n        scaler = preprocessing.StandardScaler().fit(transf)\n        dataset[col] = scaler.transform(transf)\ntrain_without_outliers.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X= train_without_outliers.drop(['count', 'casual', 'registered'], axis = 1)\ny = train_without_outliers['count']\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.3, random_state=5)\n\nrf_without_outliers = RandomForestRegressor(n_estimators=100, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_without_outliers.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Importance Without Outliers:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15,10))\ncore_variables_without_outliers = pd.Series(rf_without_outliers.feature_importances_, index=X.columns)\ncore_variables_without_outliers = core_variables_without_outliers.nlargest(8)\n\n# Colorize the graph based on likeability:\nlikeability_scores = np.array(core_variables)\n \ndata_normalizer = mp.colors.Normalize()\ncolor_map = mp.colors.LinearSegmentedColormap(\n    \"my_map\",\n    {\n        \"red\": [(0, 1.0, 1.0),\n                (1.0, .5, .5)],\n        \"green\": [(0, 0.5, 0.5),\n                  (1.0, 0, 0)],\n        \"blue\": [(0, 0.50, 0.5),\n                 (1.0, 0, 0)]\n    }\n)\n\n#make the plot\ncore_variables_without_outliers.plot(kind='barh', color=color_map(data_normalizer(likeability_scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now let's compare the two plots:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize = (20,10))\ncore_variables.plot(kind='barh', color=color_map(data_normalizer(likeability_scores)), ax=ax[0])\nax[0].set_title('With outliers significance plot')\ncore_variables_without_outliers.plot(kind='barh', color=color_map(data_normalizer(likeability_scores)), ax=ax[1])\nax[1].set_title('Without outliers significance plot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**\n* With this random forest regressor feature importance we can see that with or without the outliers trai dataset the most import feature by far is *time*.\n* Notable change would be the difference between weekday and working day in importance - with the latter being more important for the original dataset.\n* For this reason we will continue with the normal, complete dataset; however, in our training below we will select only the most important variables."},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Regressor: Applying Standard Scalers"},{"metadata":{},"cell_type":"markdown","source":"**Using Robust Scaler:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['count', 'casual', 'registered'], axis =1)\ny = train['count']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer = RobustScaler().fit(X_train)\nrescaled_X_train = transformer.transform(X_train)\n\ntransformer = RobustScaler().fit(X_test)\nrescaled_X_test = transformer.transform(X_test)\n\ny_train= y_train.values.reshape(-1,1)\ny_test= y_test.values.reshape(-1,1)\n\ntransformer = RobustScaler().fit(y_train)\nrescaled_y_train = transformer.transform(y_train)\n\ntransformer = RobustScaler().fit(y_test)\nrescaled_y_test = transformer.transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=100)\nrf.fit(rescaled_X_train, rescaled_y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn import metrics\nrf_prediction = rf.predict(rescaled_X_test)\nprint('MSE:', metrics.mean_squared_error(rescaled_y_test, rf_prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(rescaled_y_test,rf_prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using MinMax Scaler:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['count', 'casual', 'registered'], axis =1)\ny = train['count']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train= y_train.values.reshape(-1,1)\ny_test= y_test.values.reshape(-1,1)\n\nsc_X = MinMaxScaler()\nsc_y = MinMaxScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.fit_transform(X_test)\ny_train = sc_y.fit_transform(y_train)\ny_test = sc_y.fit_transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=100)\nrf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_prediction = rf.predict(X_test)\nprint('MSE:', metrics.mean_squared_error(y_test, rf_prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test,rf_prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using Standard Scaler**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"continuous_features= ['temp','atemp', 'humidity', 'windspeed', 'count']\ntrain_copy = train.copy()\nfor col in continuous_features:\n    transf = train_copy[col].values.reshape(-1,1)\n    scaler = preprocessing.StandardScaler().fit(transf)\n    train_copy[col] = scaler.transform(transf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_copy.drop(['count', 'casual', 'registered'], axis =1)\ny = train_copy['count']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer = StandardScaler().fit(X_train)\nstandard_X_train = transformer.transform(X_train)\n\ntransformer = StandardScaler().fit(X_test)\nstandard_X_test = transformer.transform(X_test)\n\ny_train= y_train.values.reshape(-1,1)\ny_test= y_test.values.reshape(-1,1)\n\ntransformer = StandardScaler().fit(y_train)\nstandard_y_train = transformer.transform(y_train)\n\ntransformer = StandardScaler().fit(y_test)\nstandard_y_test = transformer.transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=100)\nrf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_prediction = rf.predict(standard_X_test)\nprint('MSE:', metrics.mean_squared_error(standard_y_test, rf_prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(standard_y_test,rf_prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission 1:"},{"metadata":{},"cell_type":"markdown","source":"For this, therefore, we will use the MinMax() as that performed the best in terms of MSE and the shape of the scatter plot. However, let's try this time using the without outliers dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_without_outliers[['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp','humidity', 'year', \n                            'month', 'day', 'hour', 'weekday','windspeed']]\ny = train_without_outliers['count']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's also decrease the test size:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train= y_train.values.reshape(-1,1)\ny_test= y_test.values.reshape(-1,1)\n\nsc_X = MinMaxScaler()\nsc_y = MinMaxScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.fit_transform(X_test)\ny_train = sc_X.fit_transform(y_train)\ny_test = sc_y.fit_transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=100)\nrf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_prediction = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('MSE:', metrics.mean_squared_error(y_test, rf_prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp','humidity', 'year', 'month', 'day', 'hour',\n     'weekday','windspeed']] = sc_X.fit_transform(test[['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp','humidity', \n                                                          'year', 'month', 'day', 'hour', 'weekday','windspeed']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred= rf.predict(test[['season', 'holiday', 'workingday', \n                            'weather', 'temp', 'atemp','humidity', 'year', 'month', 'day', \n                            'hour', 'weekday','windspeed']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred=test_pred.reshape(-1,1)\ntest_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = sc_y.inverse_transform(test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = pd.DataFrame(test_pred, columns=['count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission1 = pd.concat([test['datetime'], test_pred],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission1.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission1['count'] = submission1['count'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission1.to_csv('submission1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Score:**\nScore: (private leaderboard for now): 0.49759"},{"metadata":{},"cell_type":"markdown","source":"## Submission 1:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_without_outliers[['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp','humidity', 'year', \n                            'month', 'day', 'hour', 'weekday','windspeed']]\ny = train_without_outliers['count']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rescaled_X_train = RobustScaler().fit_transform(X_train)\n\nrescaled_X_test = RobustScaler().fit_transform(X_test)\n\ny_train= y_train.values.reshape(-1,1)\ny_test= y_test.values.reshape(-1,1)\n\nrescaled_y_train = RobustScaler().fit_transform(y_train)\n\nrescaled_y_test = RobustScaler().fit_transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=100)\nrf.fit(rescaled_X_train, rescaled_y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_prediction = rf.predict(rescaled_X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp','humidity', 'year', 'month', 'day', 'hour',\n     'weekday','windspeed']] = sc_X.fit_transform(test[['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp','humidity', \n                                                          'year', 'month', 'day', 'hour', 'weekday','windspeed']])\n\ntest_pred= rf.predict(test[['season', 'holiday', 'workingday', \n                            'weather', 'temp', 'atemp','humidity', 'year', 'month', 'day', \n                            'hour', 'weekday','windspeed']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred=test_pred.reshape(-1,1)\ntest_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = transformer.inverse_transform(test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some Comments:"},{"metadata":{},"cell_type":"markdown","source":"* It seems that in doing well with this bike prediction demand model comes down to how we treat with the outlier variables.\n* This could be the nature of dealing with demand data in which outliers can be present and extremely influential in our models\n* We looked at different ways of dealing with outliers and found that the best was the MinMax() standard scaler preprocessing method\n* The MinMax method proves to be the most effective\n* When for example we use the RobustScaler() we get values that are very off;\n* It would be intersting to find out more into detail about the dynamics of why RobustScaler() is less effective than MinMax() by such a high degree.\n* The code/idea to create a without outliers training I saw from other submissions; I will look to update this to see if there are more optimal ways of doing this in future commits.\n<br>\n\n**In any case, future work on this will require more attention to be dealt on the cases of outliers and how to effectively deal with them.**"}],"metadata":{"kernelspec":{"display_name":"bts36","language":"python","name":"my_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}