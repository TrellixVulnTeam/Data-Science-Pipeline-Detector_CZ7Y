{"cells":[{"metadata":{"_uuid":"ec89ae50fb058b7e01e0a03cdf2ea818abdff134"},"cell_type":"markdown","source":"# This python code is for project : Bike Renting \n#### The objective of this Case is Predication of bike rental count on daily based on the environmental and seasonal settings\n#### This is more to demonstarte all the steps involved in any machine learning project\n#### For the sake of simplicity, I am using day.csv only for the analysis."},{"metadata":{"_uuid":"2ac23b558458af10e9d19927d07f5d17e2fe22e6"},"cell_type":"markdown","source":"### I am going to divide whole project in to 8 parts:\n#### 1.) Define and categorize problem statement\n#### 2.) Gather the data\n#### 3.) Prepare data for consumption\n#### 4.) Perform Exploratory Data Analysis\n#### 5.) Models Building\n#### 6.) Evaluate and compare Model performances and choose the best model\n#### 7.) Hypertune the selected model\n#### 8.) Produce sample output with tuned model"},{"metadata":{"trusted":false,"_uuid":"ded2dc6aa6f24d0e30f9063710a6a1a59ffe9f77"},"cell_type":"code","source":"## ----------- Part 1: Define and categorize the problem statement --------------\n#### The problem statement is to \"Predict the daily bike rental count based on the environmental and seasonal settings\"\n##### This is clearly a 'Supervised machine learning regression problem' to predict a number based on the input features\n\n## ----------- Part 1 ends here ----------------- ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4195b40befdfea1cba8e88737747daf38e6d6fe"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3f8566357184ad515a6bc9e70e8d5e095c72824"},"cell_type":"code","source":"##------------- Import all the required libraries--------------\n\n## Import all the required libraries\nimport os\nimport pandas as pd\nimport numpy as np\n\n#---- for model building\nimport sklearn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n#from sklearn.cross_validation import train_test_split\n\n#---- for visualization---\nimport matplotlib.pyplot as plt \nimport seaborn as sn\n\n#------ for model evaluation -----\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\n#---- For handling warnings\nimport sys\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45cc44236012493c7ed38f949d28fd316def9ff8"},"cell_type":"code","source":"## ------------------- Part 2: Gather the data -----------------\n\n### Here data is provided as .csv file with the problem.\n### Let's import the data \n\n#bike = pd.read_csv(\"/input../day.csv\")\nbike =pd.read_csv('../input/train.csv')\nbike_test =pd.read_csv('../input/test.csv')\nbike.head()\n\n##---------- Part 2 ends here --------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"defc5fc09cce08a229a617ccdce8932fd9d09f98"},"cell_type":"code","source":"# ------------Part 3 : Prepare the data for consumption(Data Cleaning) ---------------\n#### 3a.) Check the shape/properties of the data\n#### 3b.) Completing -- Perform missing value analysis and impute missing values if necessary\n#### 3c.) Correcting -- Check for any invalid data inputs , for outliers or for any out of place data\n#### 3d.) Creating -- Feature extraction . Extract any new features from existing features if required\n#### 3e.) Converting -- Converting data to proper formats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72368e9f78cee8801397e112092bba5cf8ec607c"},"cell_type":"code","source":"#### --------3a.) Check the shape/properties of the data\n## Check the shape of the data\nbike.shape\n\n# what we can infer:\n## ->the dataset has 731 observations and 16 features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75dbef7e25c7e85f67510be8debee294c38a9d34"},"cell_type":"code","source":"## Check the properties of the data\nbike.info()\n# what we can infer:\n# ->There are no null values in the dataset\n# -> The datatypes are int,float and object ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b389df06b849a46e32f9d9240b4ea59b5865a2a"},"cell_type":"code","source":"# -------------- 3b.) Completing -- Perform missing value analysis and impute missing values if necessary\n# Although we have already seen above thatthere are no null values in the dataset. Lets try other way to confirm this\n#Checking nulls\nbike.isnull().sum().sort_values(ascending=False)\n\n# what we can infer:\n# ->There are no null values in the dataset.If it had, then eithere the rows/columns had to be dropped or the null values be imputed based on the % of null values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b5af9c3512f779ed87cf62825b736b53144e756"},"cell_type":"code","source":"#### ------------------3c.) Correcting -- Check for any invalid data inputs , for outliers or for any out of place data\n# From above observations data doesnot seem to have any invalid datatypes to be handled\n# Let's check for the outliers in EDA step","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cb094b47a1130648dae6915095da5fa4cccb088"},"cell_type":"code","source":"#### -----------------3d.) Creating -- Feature extraction . Extract any new features from existing features if required\nbike.head()\nbike.datetime.describe()\n## We can see that here we have 'datetime', which gives us the exact date. This features has 2 years of data(2011, 2012), all through 12 months(1 to 12) of a year\n## However, date(day of month) information is not saperately given.\n## Lets extract 'date','mnth','weekday' and 'yr' from 'datetime' column\nbike['datetime'] = pd.to_datetime(bike['datetime'])\nbike['date'] = bike['datetime'].dt.day\nbike['mnth'] = bike['datetime'].dt.month\nbike['yr'] = bike['datetime'].dt.year\nbike['weekday'] = bike['datetime'].dt.weekday\n\n#--convert year 2011 : 1 and 2012 : 2\nbike['yr']=bike.yr.replace({2011,2012},{1,2})\n\n\n## Now, 'dteday' column is not required, since we already have year, month, date info in other columns. So lets drop it.\nbike = bike.drop(columns=['datetime'])\n\n#--------repeating the same operation for test data ------------\nbike_test\nbike_test['datetime'] = pd.to_datetime(bike_test['datetime'])\nbike_test['date'] = bike_test['datetime'].dt.day\nbike_test['mnth'] = bike_test['datetime'].dt.month\nbike_test['yr'] = bike_test['datetime'].dt.year\nbike_test['weekday'] = bike_test['datetime'].dt.weekday\nbike_test['yr']=bike_test.yr.replace({2011,2012},{1,2})\nbike_test = bike_test.drop(columns=['datetime'])\n#---------------------------------------------------------------\n\nbike.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b43d45f925022eb612ec1da07048feec7509768e"},"cell_type":"code","source":"#### 3e.) ------- Converting -- Converting data to proper formats\n#We can clearly see that \"season\", \"yr\",\"mnth\",\"holiday\",\"weekday\",\"workingday\",\"weather\",\"date\" are categories,rather than continous variable.\n#Let them convert to categories\ncategoryFeatureList = [\"season\", \"yr\",\"mnth\",\"holiday\",\"weekday\",\"workingday\",\"weather\",\"date\"]\nfor var in categoryFeatureList:\n    bike[var] = bike[var].astype(\"category\")\n    bike_test[var] = bike[var].astype(\"category\")\nbike.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4a06b50ca7ba8e2f7a8fd140f523688e7855c7bf"},"cell_type":"code","source":"# ------------Part 3 : Prepare the data for consumption(Data Cleaning) ENDS here---------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bc2fd03ba7d68cc2709ca26143158218969eed66"},"cell_type":"code","source":"# ------------Part 4 : Exploratory Data Analysis(EDA) STARTS here -----------","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dee48170d7f57f1612aec575ba0fc3ef2bde7dea"},"cell_type":"code","source":"#----- 4 a.) Outlier Analysis -----------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6685f993d88d463b3f871a2323e88b8b865fe35c"},"cell_type":"code","source":"## -- Lets do the outlier analysis ----\n## -- Visualize continous variables(cnt,temp,atemp,humidity,windspeed) and \n##  count with respect to categorical variables(\"season\", \"yr\",\"mnth\",\"holiday\",\"weekday\",\"workingday\",\"weathersit\",\"date\")with boxplots ---\nfig, axes = plt.subplots(nrows=3,ncols=4)\nfig.set_size_inches(20,15)\n\n#-- Plot total counts on y bar\nsn.boxplot(data=bike, y=\"count\",ax=axes[0][0])\n\n#-- Plot temp on y bar\nsn.boxplot(data=bike, y=\"temp\",ax=axes[0][1])\n\n#-- Plot atemp on y bar\nsn.boxplot(data=bike, y=\"atemp\",ax=axes[0][2])\n\n#-- Plot hum on y bar\nsn.boxplot(data=bike, y=\"humidity\",ax=axes[0][3])\n\n#-- Plot windspeed on y bar\nsn.boxplot(data=bike, y=\"windspeed\",ax=axes[1][0])\n\n#-- Plot total counts on y-bar and 'yr' on x-bar\nsn.boxplot(data=bike,y=\"count\",x=\"yr\",ax=axes[1][1])\n\n#-- Plot total counts on y-bar and 'mnth' on x-bar\nsn.boxplot(data=bike,y=\"count\",x=\"mnth\",ax=axes[1][2])\n\n#-- Plot total counts on y-bar and 'date' on x-bar\nsn.boxplot(data=bike,y=\"count\",x=\"date\",ax=axes[1][3])\n\n#-- Plot total counts on y-bar and 'season' on x-bar\nsn.boxplot(data=bike,y=\"count\",x=\"season\",ax=axes[2][0])\n\n#-- Plot total counts on y-bar and 'weekday' on x-bar\nsn.boxplot(data=bike,y=\"count\",x=\"weekday\",ax=axes[2][1])\n\n#-- Plot total counts on y-bar and 'workingday' on x-bar\nsn.boxplot(data=bike,y=\"count\",x=\"workingday\",ax=axes[2][2])\n\n#-- Plot total counts on y-bar and 'weathersit' on x-bar\nsn.boxplot(data=bike,y=\"count\",x=\"weather\",ax=axes[2][3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dac4228664f0e4574b5f370601230f17989a8d52"},"cell_type":"code","source":"# what we can infer from above boxplots:\n# -> There are many outliers.\n# Lets keep these outliers for now, till we complete full EDA(will remove the outliers in next update of kernel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6669970f5d327bbaa2e39d70fcc475ab3a4aa470"},"cell_type":"code","source":"#---- 4b.) Correlation Analysis\n#--- Explore continous features\n#--- Explore categorical features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9fc747b317265bcdc87b9d2e78b45496694cef5"},"cell_type":"code","source":"#------------- Explore continous features -----------------\n##Explore the correlation btwn the independent continous features with target variabe\ncorr=bike[['temp','atemp','humidity','windspeed']].corrwith(bike['count'])\ncorr.plot.bar(figsize=(8,8), title='Correlation of features with the response variable count_of_rented_bikes', grid=True, legend=False, style=None, fontsize=None, colormap=None, label=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94872ff446318083ac84d5f97c799894bc38d96e"},"cell_type":"code","source":"##------heatmap for correlation matrix---------##\n##to check multicollinearity ---##\n\n#correlation matrix\nsn.set(style='white')\n#compute correlation matrix\ncorr =bike.drop(columns=['count']).corr()\n#generate a mask for upper triangle#\nmask =np.zeros_like(corr, dtype=np.bool)\nmask[np.tril_indices_from(mask)]=True\n#setuop the matplotlab figure\nf,ax=plt.subplots(figsize=(10,10))\n#generate a custom diverging colormap\ncmap=sn.diverging_palette(220, 10, s=75, l=50, sep=10, n=6, center='light', as_cmap=True)\n#heatmap\nsn.heatmap(corr, vmin=None, vmax=None, cmap=cmap, center=0, robust=False, fmt='.2g', linewidths=0, linecolor='white', square=True, mask=mask, ax=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bc3e5f722d59256839a97190d5fd6e439dda05ce"},"cell_type":"code","source":"#Clearly, from above heatmap, we can se that the dataset has multicolinearity. 'temp' and 'atemp' are highly correlated.\n#Will need to drop one of them.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fa354a26696f927250ebb11fb4e4e8b9dc90907"},"cell_type":"code","source":"#Visualize the relationship among all continous variables using pairplots\nNumericFeatureList=[\"temp\",\"atemp\",\"humidity\",\"windspeed\"]\nsn.pairplot(bike,hue = 'yr',vars=NumericFeatureList)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7653660616bcdc3094fb4d2e5e7d1fd7a8790dd"},"cell_type":"code","source":"#Lets explore some more, the relationship btwn independent continous variables and dependent variable using JOINT PLOTs\n#graph individual numeric features by count of rented bikes\nfor i in NumericFeatureList:\n    sn.jointplot(i, \"count\", data=bike, kind='reg', color='g', size=4, ratio=2, space=0.2, dropna=True, xlim=None, ylim=None, joint_kws=None, marginal_kws=None, annot_kws=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf44a7965e178cf97292e531865684c4c28b1db2"},"cell_type":"code","source":"# Check the distribution plot of target variable 'count'\nsn.distplot(bike[\"count\"],color ='r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2e0afe2392dc68efa92c58972df68d3b080da61d"},"cell_type":"code","source":"# what we can infer from above analysis of continous variables:\n# -> Target variable 'cnt' is almost normally distributed, which is a good thing.\n# -> From correlation with dependent variable cnt, we can see that 'casual','registered' are very highly correlated to cnt. These are actually 'leak variablles'. Needs to be dropped from the dataset\n# -> 'hum' has low correlation with 'cnt'. For ow, lets keep it.\n# -> atemp and temp has good correlation with 'cnt'\n# -> From heatmap, we can see that atemp and temp are highly correlated. So we need to drop 1 to remove multicollinearity.\n# -> Since, as seen from jointplot,p(atemp) < p(temp), we can drop 'temp' and retain 'atemp' in the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"692c4d5190dc509cf727cdef81b6b39ca1b3e2c2"},"cell_type":"code","source":"#------------- Explore categorical features ------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"915083a459a3d944925cb9534a3eb7e81498bd27"},"cell_type":"code","source":"##checking the pie chart distribution of categorical variables\n#bike_piplot = bike.drop(columns=['instant','dteday','temp','atemp','hum','windspeed','casual','registered','cnt'])\nbike_piplot=bike[categoryFeatureList]\nplt.figure(figsize=(15,12))\nplt.suptitle('pie distribution of categorical features', fontsize=20)\nfor i in range(1,bike_piplot.shape[1]+1):\n    plt.subplot(3,3,i)\n    f=plt.gca()\n    f.set_title(bike_piplot.columns.values[i-1])\n    values=bike_piplot.iloc[:,i-1].value_counts(normalize=True).values\n    index=bike_piplot.iloc[:,i-1].value_counts(normalize=True).index\n    plt.pie(values,labels=index,autopct='%1.1f%%')\n#plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4726d69a238b7f9cfc6abc2f6f2a202469b76a48"},"cell_type":"code","source":"#What we can infer from above piplot:\n#-> Most of the categorical variables are uniformally distributed, except 'holiday','weathersit','workingday'\n#-> This makes sense for 'weathersit', as extreme weather is rare and hence %percentage of extreme weather in whole dataset is low\n#-> This makes sense for 'holiday', as number of holidays are less in comparison to working days\n#-> This makes sense for 'workingday' for the same reason as above\n#-> So, categorical data seems o be pretty much uniformly distributed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5dc2a8b53f0ebb888eec8394fccc5ed759e02d1a"},"cell_type":"code","source":"#graph individual categorical features by count\nfig, saxis = plt.subplots(3, 3,figsize=(16,12))\n\nsn.barplot(x = 'season', y = 'count',hue= 'yr', data=bike, ax = saxis[0,0], palette =\"Blues_d\")\nsn.barplot(x = 'yr', y = 'count', order=[0,1,2,3], data=bike, ax = saxis[0,1], palette =\"Blues_d\")\nsn.barplot(x = 'mnth', y = 'count', data=bike, ax = saxis[0,2])\nsn.barplot(x = 'holiday', y = 'count',  data=bike, ax = saxis[1,0])\nsn.barplot(x = 'weekday', y = 'count',  data=bike, ax = saxis[1,1])\nsn.barplot(x = 'workingday', y = 'count', data=bike, ax = saxis[1,2])\nsn.barplot(x = 'weather', y = 'count', data=bike, ax = saxis[2,0])\nsn.barplot(x = 'date', y = 'count' , data=bike, ax = saxis[2,1])\n#sn.pointplot(x = 'weathersit', y = 'cnt', data=bike, ax = saxis[2,0])\nsn.pointplot(x='date', y='count', hue='yr', data=bike, markers='o', linestyles='-', dodge=False, join=True,ax = saxis[2,2])\n#sn.pointplot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8c7c08079525a2ebd8b54cc14ee3e8d92988cccb"},"cell_type":"code","source":"#--- Lets see how these categorical variables individually ffects the count of rented bikes\n# Does 'yr' affects count of rented bikes\n#--> YES. the count have an upward trend wrt year\n\n#Does 'season' affects count of rented bikes\n#--> YES, it seems ppl rent more bikes during season 3 and 2, i.e. highest in fall and summer and less in winter and springs. This makes sense as weather is good to ride during summer and fall.\n\n#Does 'month' affects count of rented bikes\n#-->YES.ppl are likely to rent bikes more btwn the months May- October and lowest in month of Jan,Feb and Dec(in that order). This again makes sense, as this trend is in sync with favourable weather conditions\n\n#Does 'holiday' affects count of rented bikes\n#--> YES. ppl rent more bikes on non-holiday than holiday. It makes sense as bikers who commute to work/school will be less on holiday.\n\n#Does 'weekday' affects count of rented bikes\n#--> To some extent Yes. ppl seems to rent lesser bikes on Sat/ Sun. ie. over the weekend. Again makes sense as school and offices are closed on weekend.\n#Monday also has lesser count of rented bikes. It may be possible the ppl visit to other places/cities over weekend and travel back in car on Monday, istead of renting bikes.\n\n#Does 'weather' affects count of rented bikes\n#--> Most definately YES. noone rented bike on extreme weather(season=4). ppl rent maximum bikes during a clear day (weathersit=1)\n\n#Does 'date' affects count of rented bikes\n#--> Well there is no set trends. It seems to be random. Let explore bit more of it over the 12 months using pointplot\n#-->","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6b42191dc03a1b0454279920b59b80f7dadde6d"},"cell_type":"code","source":"#-- exploring some more pairplots, to see the trends over the years\nfig, saxis = plt.subplots(2, 2,figsize=(16,12))\nsn.pointplot(x='season', y='count', hue='yr', data=bike, markers='o', linestyles='-', dodge=False, join=True,ax = saxis[0,0])\nsn.pointplot(x='holiday', y='count', hue='yr', data=bike, markers='o', linestyles='-', dodge=False, join=True,ax = saxis[0,1])\nsn.pointplot(x='weekday', y='count', hue='mnth', data=bike, markers='o', linestyles='-', dodge=False, join=True,ax = saxis[1,0])\nsn.pointplot(x='workingday', y='count', hue='yr', data=bike, markers='o', linestyles='-', dodge=False, join=True,ax = saxis[1,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dbedc6a7415dceeb03a6f71069eda5554bf9c645"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"371a9052d3b38e1eb98ecbf8ea1e6580b3ba99c0"},"cell_type":"code","source":"#------ Exploratory Data Analysis ENDS Here------------------\n# Final observations:\n#1.) 'casual' and 'registered' are leak variables. They need to be dropped from the dataset\n#2.) 'atemp' and 'temp' are very strongly correlated . Drop 'atemp' from the dataset(since it has higher p-value than 'temp')\n#3.) 'date' does not seem to have any affect on count of bikes, it can be dropped from the dataset\n#------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f0386827d4335c64e39e092cd2f5b3bbcd00a68"},"cell_type":"code","source":"#---- Drop the features mentioned above(as part of feature engineering)\ntrain = bike.drop(columns=['temp','casual','registered'])\ntest = bike_test.drop(columns=['temp'])\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1ac9545d935827abaa451b2a6524e2a20424d48b"},"cell_type":"code","source":"#----------Part 5 : Model Builing starts here ----------------------\n#Train the models with both datasets(before and after feature engineering)\n#Note: Just to show how feature engineering improves the result, I am going to train and test 1st model(linear regression model) with both 'before feature engineering' and 'after feature engineering' data and compare the results\n# For subsequent models,I'll only use the dataset with feature engineering implemented","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d0f378e40faf4999fbff91bd12af5e8611977fe"},"cell_type":"code","source":"# 1.) I am selecting 3 models to test and evaluate\n #   -> Linear Regression Model\n #   -> Random Forrest (ensemble method using bagging technique)\n #   -> Gradient Boosting (ensemble method using boosting technique)\n#2.) Cross validation    \n#3.) All these 3 models will be compared and evaluated(with and without feature engineering)\n#4.) We'll choose the best out of 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3182fa5510232e5edf60dea7197f8555725ac44"},"cell_type":"code","source":"#----- 5a.) -- Selecting train and test datasets for cross validations\n#split train data in to test and train(after featr engineering)\n#train, test = train_test_split(bike_aftr_ftr_eng, test_size=0.20, random_state = 5)\n\ntrain_data = train[:80]\ntest_data = train[20:]\nX_train = train_data.drop(columns=['count'])\nY_train = train_data['count']\nX_test = test_data.drop(columns=['count'])\nY_test = test_data['count']\n\n#--- *AFT <=> After Feature Engineering------","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9eeb6cf0f61961f4b69d8c719c823f0b93eac9b5"},"cell_type":"code","source":"#------- 5b.) Define a dataframe to store performance metrices of the models ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c3cef99c2dd7ccbb190463151f52f2301d55ee3"},"cell_type":"code","source":"#--- define a function which takes model, predicted and test values and returns evalution matrix: R-squared value,RootMeanSquared,MeanAbsoluteError\ndef model_eval_matrix(model,X_test,Y_test,Y_predict):\n    r_squared = model.score(X_test, Y_test)\n    mse = mean_squared_error(Y_predict, Y_test)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(Y_predict, Y_test)\n    return r_squared,mse,rmse, mae","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e6e6b6a791bf496cad9c42cb86d15c6cea4c7a7f"},"cell_type":"code","source":"#-------- 5c.) Define and fit models ---------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ec56a765cd27909a9030756492248c342e84a71"},"cell_type":"code","source":"#--Define Linear regession model --\nlrm_regressor = LinearRegression()\nlrm_regressor.fit(X_train, Y_train)\nY_predict_lrm =lrm_regressor.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e51e2bf9b02938b152f5768ba0776763f1b11e92"},"cell_type":"code","source":"#------- Random Forest Model (Ensemble method using Bagging technique) --------------\nforest_reg = RandomForestRegressor(random_state=1)\nforest_reg.fit(X_train, Y_train)\nY_predict_forest =forest_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e39d1e2f4e84b06ab259b4e7c5091cd0b156bb5c"},"cell_type":"code","source":"## ----------- Building XGBoost Model (Ensemble method using Boosting technique) ---------------\n#xgb_reg = GradientBoostingRegressor(random_state=1) # without parameter hypertuning\n# Following model is with parameter hypertuning\nxgb_reg = GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=300, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=1, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=100, warm_start=False, presort='auto')\nxgb_reg.fit(X_train, Y_train)\nY_predict_xgb = xgb_reg.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dcd120b028b664bdaa354d07e4ea9f39f62b989e"},"cell_type":"code","source":"#-------Part 5 ENDS here ------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a09eed659d94ed36e67f49d1d419a4c1471d2c78"},"cell_type":"code","source":"#-------Part 6 : Model comparisions STARTS here---------------------------","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"21af50abc0ea598a438d7290b77a153ab3ee91bf"},"cell_type":"code","source":"#---Stroring all model performances in dataframe to compare----\nmetric=[]\nml_models=['Linear Regression','Random Forest','Gradient Boosting']\nfitted_models= [lrm_regressor,forest_reg,xgb_reg]\nY_Predict =[Y_predict_lrm,Y_predict_forest,Y_predict_xgb]\ni=0\nfor mod in ml_models:\n    R_SQR,MSE,RMSE,MAE = model_eval_matrix(fitted_models[i],X_test,Y_test,Y_Predict[i])\n    metric.append([mod,R_SQR,MSE,RMSE,MAE])\n    i=i+1\ndf_mod_performance=pd.DataFrame(metric,columns =['Model','R-Squared','MeanSquaredError','RootMeanSquaredError','MeanAbsoluteError'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"482db03a0f0ca9c2ec593b5d16a8217aabc4a3a5"},"cell_type":"code","source":"df_mod_performance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b427875c7216212321a5deb3970376cb6c2fdcd"},"cell_type":"code","source":"#------ Comparing the performance matrix values of the models-----\n#fig, saxis = plt.subplots(2, 2,figsize=(16,12))\n#a=sn.pointplot(y='Model', x='R-Squared', rotate =90,data=df_mod_performance, markers='o', linestyles='-', dodge=False, join=True,ax = saxis[0,0])\n#a.set_xticklabels(a.get_xticklabels(), rotation=45)\n#sn.pointplot(y='Model', x='MeanSquaredError', data=df_mod_performance, markers='o', linestyles='-', dodge=False, join=True,ax = saxis[0,1])\n#sn.pointplot(y='Model', x='RootMeanSquaredError', data=df_mod_performance, markers='o', linestyles='-', dodge=False, join=True,ax = saxis[1,0])\n#sn.pointplot(y='Model', x='MeanAbsoluteError', data=df_mod_performance, markers='o', linestyles='-', dodge=False, join=True,ax = saxis[1,1])\n#plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"73389cf39a772e95d075b7b49d659f68dfd2e9b7"},"cell_type":"code","source":"#What can be inferred from above observations:\n#-->It is evidently clear that gradient boost gives the best performance out of all the models\n#-->Hence we'll consider Gradient Boosting as our final model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a09b455ee1697dc46e4bc103888ff59481fa946c"},"cell_type":"code","source":"#---------Part 6 : Model comparisions ENDS here ---------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cc2975cbcd168c9b7ebd754646849a50f62ac4a5"},"cell_type":"code","source":"#---------Part 7 : Hypertune the selected model starts here ------------\n\n#Now, Gradient Boosting is the final model, parameter hypertuning can be performed on the model to find the best parameters which will give the maximum performance.\n#Functions like GRIDSearchCV from GridSearch library of python can be used for this.\n\n#However, I tried here simple approach of ‘hit and trial’, where I changed parameter few times and found a set which gave me maximum performance.\n\n#Before parameter tuning:\n#-----> Gradient Boosting\n#-----> R-Squared :0.897838\n#-----> MSE: 387939.616482\n#-----> RMSE: 622.847988\n#-----> MAE: 460.576495\n\n#Before parameter tuning:\n#-----> Gradient Boosting\n#-----> R-Squared :0.913779\n#-----> MSE: 327408.191428\n#-----> RMSE: 572.195938\n#-----> MAE: 415.264316\n\n#Evident here, hypertuning the parameter boosted the model performance. So, we lock the parameters as below:\n#-->loss='ls',\n#-->learning_rate=0.1, \n#-->n_estimators=300, \n#-->subsample=1.0, \n#-->criterion='friedman_mse', \n#-->min_samples_split=2, \n#-->min_samples_leaf=1, \n#-->min_weight_fraction_leaf=0.0, \n#-->max_depth=3, \n#-->min_impurity_decrease=0.0, \n#-->min_impurity_split=None, \n#-->init=None, \n#-->random_state=1, \n#-->max_features=None, \n#-->alpha=0.9, \n#-->verbose=0, \n#-->max_leaf_nodes=100, \n#-->warm_start=False, \n#-->presort='auto'\n\n#Lets produce the output using this model\n\n#---------Part 7 : Hypertune the selected model ENDS here ------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f5c34d44b11a8c37103ecfe395a54a193b9705b"},"cell_type":"code","source":"#--------Part 8 : Produce sample output with tuned model STARTS here----------------------\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4e6dd8b7d24dd5497bd5046798f2cbb88d8bc2a"},"cell_type":"code","source":"Y_predict_xgb_final = xgb_reg.predict(test)\nfinal_bike_prediction_df=test\n#final_bike_prediction_df['ActualCount'] = Y_test\nfinal_bike_prediction_df['PredictedCount'] = Y_predict_xgb_final\nfinal_bike_prediction_df['PredictedCount'] = round(final_bike_prediction_df['PredictedCount'])\n#--- Sample output(with actual counts and predicted counts) ---\n#final_bike_prediction_df\nfinal_bike_prediction_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6a19f26f542ff0fa0900fbb53f31f935a9f2c51"},"cell_type":"code","source":"#-----Plotting the distributions of 'ActualCount' and 'PredictedCount'\n#fig, saxis = plt.subplots(2, 2,figsize=(16,12))\n#sn.distplot(final_bike_prediction_df[\"ActualCount\"],color ='r', ax = saxis[0,0])\n#sn.distplot(final_bike_prediction_df[\"PredictedCount\"],color ='g',ax = saxis[0,1])\n\n#--- As clearly evident from the below charts the distributions of both the counts are very similar.\n#--This seems a fair model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0cbc8c72a2e449e67a82c85130ece24226a6e2a7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cac484e1ad938fe0aa8ae3c6258cf84ff673ceb8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cba974b486f8b030c45317eef1fab36e625f00c6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bd51d0ab7d4e4db22adbf3082a07581416d65665"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a0cbfef0bb931ba9251e12b1a9df9db3776703d6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"158e1c12eea1a2d5aa844029930d597ea8367751"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1bc7b82f819ad3b691d0fb115c8353da863d9d1e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bc3b09d7259c8b8b33db2fc6c6a2a98059ab5369"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"26897c776f571d0a4788f6290d1e2cc599252fa3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ffd42b96ddd534d00e2819e77a1e871baf19b4a6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"595816dd437a49ba2885644fe4f1d6594d93fdfe"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0a712cf4a94b783e4a2d01edf35e8e2e43d81538"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f3583a070cf218c39466c6c511d0f17296b22f30"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1cc633b3354fb58d03d4031eaca38f0d5f7bb358"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"242ef3c0173df52341560451b433cf955d3740d3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"59aab7baa13bb4613cb16b7c48cef395dc67a07b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e968d39679e150e0846475c25ce094eabf3fb6b0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f0636311b2a060c6fdcfb3179e57c5b06e7d2339"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"561f599fdcb16b8bc18d5dcc7fe611d4b6ea88c3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"efd43984637648e75f0d82922c66707452ad4385"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3495e283b04de2466bb01eec38d94a74c000d310"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6caa4f55a6640d85220cb242b62d4883b0bdf066"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3d65fc05571aec2b8eec7cd19f010bd3579f3b33"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6a47e2e8b9413fae8d841aef8a196e4d2d6bc58f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4ea8922bc0be104f99a821d5ec83444f2029d1d4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cfe7ca389d01ef9eaada9fe0648734b4df248645"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bc0a08d22981e6cc8f555a2911d175674c198bfa"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}