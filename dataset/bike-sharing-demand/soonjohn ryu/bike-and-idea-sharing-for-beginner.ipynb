{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load basic library\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1'st Step for beginner : First meeting with Data\n\n## Load everything you need , and let's see what a dataset look like.\n* I worked as a process engineer for a ten years, So I'm not familiar with computer science. That mean, I will use simple and easy code\n* 1'st I loaded a basic library and I will load csv file(train and test set)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/bike-sharing-demand/train.csv')\ntest = pd.read_csv('../input/bike-sharing-demand/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It's a first meething\n* See head or tail.\n* See shape.\n* See info & describe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info(), test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Casual, registered value is not include the testset\n## I will remove this value(If it's useless, it's bound to be thrown away.)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# It's just first impression\n\n* we need to go deeper... How?\n* we have a visualization tool!  matplot and seaborn bring us to deeper understanding\n\n\n\n## But, I don't like the shape of datetime, so before we go, i like to transfer the shape of datetime columns\n## Let's divide the datetime in to year, month, day(weekday), hour","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a tempdate\n\ntrain['tempdate'] = train.datetime.apply(lambda x : x.split())\ntrain['tempdate'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use apply function\ntrain['year'] = train.tempdate.apply(lambda x : x[0].split('-')[0])\ntrain['month'] = train.tempdate.apply(lambda x : x[0].split('-')[1])\ntrain['day'] = train.tempdate.apply(lambda x : x[0].split('-')[2])\ntrain['hour'] = train.tempdate.apply(lambda x : x[1].split(':')[0])\n# check the year\ntrain['year']\n# transfer proper type\ntrain['year'] = pd.to_numeric(train['year'])\ntrain['month'] = pd.to_numeric(train['month'])\ntrain['day'] = pd.to_numeric(train['day'])\ntrain['hour'] = pd.to_numeric(train['hour'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# When we think about the bike sharing. Day is not a important factor, but weekday is import factor.(I think so..)\ntrain['weekday'] = train.tempdate.apply(lambda x : datetime.strptime(x[0],'%Y-%m-%d').weekday())\n# Checking is important for beginner(it's make me comportable)\ntrain['weekday']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I don't need more tempdate. Drop in the castle!!!\ntrain = train.drop(['tempdate'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2'nd Step for beginner : Visualization\n\n* It's a time that we use visualization tool\n* I think Seaborn is the beautiful tool, especially for beginner","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"figure = plt.figure(figsize = (10,10))\n\nax1 = plt.subplot(2,2,1)\nax1 = sns.barplot(x = 'season', y='count', data=train)\n\nax2 = plt.subplot(2,2,2)\nax2 = sns.barplot(x = 'holiday', y='count', data=train )\n\nax3 = plt.subplot(2,2,3)\nax3 = sns.barplot(x = 'workingday', y='count', data=train )\n\nax4 = plt.subplot(2,2,4)\nax4 = sns.barplot(x = 'weather', y='count', data=train )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure = plt.figure(figsize = (10,10))\n\nax1 = plt.subplot(2,2,1)\nax1 = sns.barplot(x = 'year', y='count', data=train)\n\nax2 = plt.subplot(2,2,2)\nax2 = sns.barplot(x = 'month', y='count', data=train )\n\nax3 = plt.subplot(2,2,3)\nax3 = sns.barplot(x = 'day', y='count', data=train )\n\nax4 = plt.subplot(2,2,4)\nax4 = sns.barplot(x = 'hour', y='count', data=train )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## hmm.... Season is right?\n## I want change the season, because January and February are not spring. It is winter.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# make function for change season\n\ndef seson_change(month):\n    if month in [12,1,2]:\n        return 4\n    elif month in [3,4,5]:\n        return 1\n    elif month in [6,7,8]:\n        return 2\n    elif month in [9,10,11]:\n        return 3\n\ntrain['season'] = train.month.apply(seson_change)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One more visualization\nfigure = plt.figure(figsize = (10,10))\n\nax1 = plt.subplot(2,2,1)\nax1 = sns.barplot(x = 'season', y='count', data=train)\n\nax2 = plt.subplot(2,2,2)\nax2 = sns.barplot(x = 'holiday', y='count', data=train )\n\nax3 = plt.subplot(2,2,3)\nax3 = sns.barplot(x = 'workingday', y='count', data=train )\n\nax4 = plt.subplot(2,2,4)\nax4 = sns.barplot(x = 'weather', y='count', data=train )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure = plt.figure(figsize = (10,10))\n\nax1 = plt.subplot(2,2,1)\nax1 = sns.distplot(train.temp)\n\nax2 = plt.subplot(2,2,2)\nax2 = sns.distplot(train.atemp)\n\nax3 = plt.subplot(2,2,3)\nax3 = sns.distplot(train.humidity)\n\nax4 = plt.subplot(2,2,4)\nax4 = sns.distplot(train.windspeed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We see the all value.\n* Next step. we need to see the correlation(it's basic)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"figure = plt.figure(figsize = (20,20))\n\nax = sns.heatmap(train.corr(), annot=True,vmin =0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x='temp', y='atemp', data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Of course, atemp & temp have very strong correlation.(I think we don't need two value, atemp more proper to predict bike sharing)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw mix value graph\nfigure = plt.figure(figsize = (10,10))\nlist = ['season','holiday','weekday','weather']\ni=0\n\nfor j in list:\n    i = i+1\n    plt.subplot(2,2,i)\n    sns.pointplot(x='hour', y='count', hue=j,data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,10))\n\nax1 = plt.subplot(2,1,1)\nax1 = sns.pointplot(x='month',y='count', hue='weather',data= train)\n\nax2 = plt.subplot(2,1,2)\nax2 = sns.barplot(x='month',y='count',data=train, hue='weather')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3'th for beginner : Preprocessing\n\n* visulization is enough(Actually not enough)\n* It's a time we prepair data for a model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing It should proceed simultaneously.\n## So, before preprocessing data, we need to concat the train, test value.(Total)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# before preprocessing we need to load the first train data\n\ntrain = pd.read_csv('../input/bike-sharing-demand/train.csv')\ntest = pd.read_csv('../input/bike-sharing-demand/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concentrate the train, test\nTotal = pd.concat([train,test],axis=0)\nTotal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Do the same process, once more for preprocessing\n\nTotal['tempdate'] = Total.datetime.apply(lambda x : x.split())\n\nTotal['year'] = Total.tempdate.apply(lambda x : x[0].split('-')[0])\nTotal['month'] = Total.tempdate.apply(lambda x : x[0].split('-')[1])\nTotal['day'] = Total.tempdate.apply(lambda x : x[0].split('-')[2])\nTotal['hour'] = Total.tempdate.apply(lambda x : x[1].split(':')[0])\nTotal['weekday'] = Total.tempdate.apply(lambda x : datetime.strptime(x[0], \"%Y-%m-%d\").weekday())\n\nTotal['year'] = pd.to_numeric(Total.year)\nTotal['month'] = pd.to_numeric(Total.month)\nTotal['day'] = pd.to_numeric(Total.day)\nTotal['hour'] = pd.to_numeric(Total.hour)\n\nTotal = Total.drop('tempdate',axis=1)\n\nTotal['season'] = Total.month.apply(seson_change)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## When we see the wind value, we find there are many '0' value\n## But you know, wind is important thing for rider. So we will make a wind in the forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# load forest\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# See the wind\nsns.distplot(train['windspeed'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first, seperate wind '0'&'1' value\nTotal_wind_speed0 = Total[Total['windspeed'] == 0]\nTotal_wind_speed1 = Total[Total['windspeed'] != 0]\n\n# When we make a wind in the forest, there are useless value in the train data\ntemp_drop_columns = ['windspeed','casual','registered','count','datetime','holiday','workingday']\n\n# Save after drop columns\nTotal_wind_speed0_df = Total_wind_speed0.drop(temp_drop_columns, axis = 1)\nTotal_wind_speed1_df = Total_wind_speed1.drop(temp_drop_columns, axis = 1)\n\n# Save the value for predict\nTotal_wind_speed1_df_speed = Total_wind_speed1['windspeed']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# simple train forest\n\nrf = RandomForestRegressor()\n\nrf.fit(Total_wind_speed1_df, Total_wind_speed1_df_speed)\n\n# make wind using forest\nTotal_wind_speed0_df_speed = rf.predict(Total_wind_speed0_df)\n\n# Save wind\nTotal_wind_speed0['windspeed'] = Total_wind_speed0_df_speed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remake the Total with wind\nTotal = pd.concat([Total_wind_speed0,Total_wind_speed1],axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Wind story is done\n\n## Next story is about cat.(category value)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This columns have several values, but it's a category value. \n# I will divided several columns by using get dummies function\n\ncat_columns = [ 'season', 'weather','year', 'hour', 'weekday' ] \n\n# Of coures, there are several category value that not contain cat_columns, but don't worry. i will drop the columns\n\nfor i in cat_columns:\n    Total[i] = Total[i].astype('category')\n\n# get_dummies\nweather_df = pd.get_dummies(Total['weather'],prefix = 'w')\nseason_df = pd.get_dummies(Total['season'],prefix='s')\nyear_df = pd.get_dummies(Total['year'], prefix = 'y')\nhour_df = pd.get_dummies(Total['hour'], prefix = 'h')\nweekday_df = pd.get_dummies(Total['weekday'], prefix='we')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sum dummies data\nTotal = pd.concat([Total,weather_df], axis=1)\nTotal = pd.concat([Total,season_df], axis=1)\nTotal = pd.concat([Total,year_df], axis=1)\nTotal = pd.concat([Total,hour_df], axis=1)\nTotal = pd.concat([Total,weekday_df], axis=1)\n\n# Data Checking is alway right \nTotal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Total.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## and I think numeric value is too large. so i will make small","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# If you use log, '0' value tranfered infinity value(it make error)\n# So, log1p make log(data +1) it's avoid error\nTotal['atemp'] = np.log1p(Total['atemp'])\nTotal['humidity'] = np.log1p(Total['humidity'])\nTotal['windspeed'] = np.log1p(Total['windspeed'])\n\n\n#checking is alway right.\nTotal.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# OK, well done, just remain simple process ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# divide values(seperate by using 'count'values)\ntrain = Total[pd.notnull(Total['count'])]\ntest = Total[~pd.notnull(Total['count'])]\n\nprint(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# If it's useless, it's bound to be thrown away.\n# Drop the columns, Drop the beat","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_columns = ['casual', 'count','temp', 'datetime','registered','weather','season','year','month','day','hour','weekday']\n\n# But, before we drop, we have to save some value(datetime need for submission, count need for prediction)\n\ntest_datetime = test['datetime']\ny_label = train['count']\n\ntrain = train.drop(drop_columns,axis=1)\ntest = test.drop(drop_columns,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape, test.shape)\nprint(y_label.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4'th for beginner : Model make & predict\n\n* Use some regression model and predict submission value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# load library for predict\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save some model\nlr = LinearRegression()\nridge = Ridge()\nlasso = Lasso()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make label value transfer logvlaue for predict\ny_label_log = np.log1p(y_label)\n\n# train model\nlr.fit(train,y_label_log)\nrf.fit(train,y_label_log)\nridge.fit(train,y_label_log)\nlasso.fit(train,y_label_log)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict easy\nlr_preds = lr.predict(train)\nrf_preds = rf.predict(train)\nridge_preds = ridge.predict(train)\nlasso_preds = lasso.predict(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will use np.exp for transfer log value\n# and just use MAE because it's easy\nprint(\"lr MAE :{:.5f}\".format(metrics.mean_absolute_error(np.exp(y_label_log),np.exp(lr_preds))))\nprint(\"rf MAE :{:.5f}\".format(metrics.mean_absolute_error(np.exp(y_label_log),np.exp(rf_preds))))\nprint(\"ridge MAE :{:.5f}\".format(metrics.mean_absolute_error(np.exp(y_label_log),np.exp(ridge_preds))))\nprint(\"lasso MAE :{:.5f}\".format(metrics.mean_absolute_error(np.exp(y_label_log),np.exp(lasso_preds))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble is better than 1 model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# I just borrowed idea from EDA & Ensemble Model (Top 10 Percentile) kernel\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngb = GradientBoostingRegressor()\ngb_params={'max_depth':range(1,11,1),'n_estimators':[1,10,100]}\nacc_scorer = metrics.make_scorer(metrics.mean_absolute_error,greater_is_better=False)\ngrid_gb=GridSearchCV(gb,gb_params,scoring=acc_scorer,cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_gb.fit(train,y_label_log)\npreds = grid_gb.predict(train)\nprint(\"GB MAE :{:.5f}\".format(metrics.mean_absolute_error(np.exp(y_label_log),np.exp(preds))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* rf model value is better than gb.. but when i submit two model predict data. GBR model has a more high score than rf model\n* I think MAE value is not proper in case. I need to use RMSE.\n* BUT, Anyway I will make submission file\n\n# Final step for beginner : Make submission file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predsTest = grid_gb.predict(test)\n\n# make dataFrame\nsubmission = pd.DataFrame({\n        \"datetime\": test_datetime,\n        \"count\": [max(0, x) for x in np.exp(predsTest)]\n    })\n\n# make file\nsubmission.to_csv('bike_and_idea_sharing_for_beginner.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The end. Happy kaggle~","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}