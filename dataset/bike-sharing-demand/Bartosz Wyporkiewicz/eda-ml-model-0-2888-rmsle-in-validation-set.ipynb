{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bike Sharing Demand\n\nForecast use of a city bikeshare system"},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nBike-sharing systems are present all around the world in many major cities. Its' automated nature allows collecting a big amount of live information about the rental type, time and even location. Based on this data, analysts can predict the future bike demand across different time, location and conditions.\n\nThe following dataset has been collected and published by 'Capital Bikeshare' program in Washington, D.C. in the US.\n\nThe dataset contains hourly rental data spanning two years regarding public bicycles usage. The training set contains data covering a period from 1st of each month to 19th, whereas test set covers the rest of the month.\n\nIn the following kernel, we will do explanatory data analysis to find the common patterns and behaviour of customers. In the later parts, we will also try to predict the total count of bikes rented during each hour covered in the test set using machine learning models and test it in the Kaggle submission platform."},{"metadata":{},"cell_type":"markdown","source":"## Kernel setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n\n\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport warnings\n\nsns.set(rc={'figure.figsize': (16.5, 8.27)})\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATASET_LOCATION = '../input/bike-sharing-demand'\n\ntrain_set = pd.read_csv(f'{DATASET_LOCATION}/train.csv', keep_default_na=True)\ntest_set = pd.read_csv(f'{DATASET_LOCATION}/test.csv', keep_default_na=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_set.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although most columns are self-explanatory, some might be a bit more mysterious.\n\n|   Column   |                   Description                   |\n|:----------:|:-----------------------------------------------:|\n|  datetime  |                 Date + Timestamp                |\n|   season   |                  Current season                 |\n|   holiday  |    Indicator whether current date is holiday    |\n| workingday |  Indicator whether current date is working date |\n|   weather  |                 Current weather                 |\n|    temp    |              Temperature in Celcius             |\n|    atemp   |       \"Feels like\" temperature in Celsius       |\n|  humidity  |        Relative humidity (Measured in %)        |\n|  windspeed |           Windspeed (Measured in km/h)          |\n|   casual   | Number of non-registered user rentals initiated |\n| registered |   Number of registered user rentals initiated   |\n|    count   |             Number of total rentals             |\n\nAlso, few columns are already converted to numerical values hence needs a bit more clarification.\n\n**Season**\n\n| Category | Description |\n|:--------:|:-----------:|\n|     1    |    Spring   |\n|     2    |    Summer   |\n|     3    |    Autumn   |\n|     4    |    Winter   |\n\n**Weather**\n\n| Category |                                       Description                                       |\n|:--------:|:---------------------------------------------------------------------------------------:|\n|     1    |                     Clear, Few clouds, Partly cloudy, Partly cloudy                     |\n|     2    |                  Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist                  |\n|     3    | Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds |\n|     4    |                Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog               |"},{"metadata":{},"cell_type":"markdown","source":"It is worth noticing that the DateTime is not a proper ISO-8601 date and will be needed proper conversion."},{"metadata":{},"cell_type":"markdown","source":"## Insight"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_set.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_set.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is fairly well prepared as there are no missing values. It will make data preparation easier as missing values always bring uncertainty to the prediction model."},{"metadata":{"trusted":false},"cell_type":"code","source":"train_set.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As mentioned earlier, DateTime will require the most preprocessing as it is an object value and not a proper ISO-8601 date.\n\nThe rest of the columns are straightforward - **temp**, **atemp** and **windspeed** are floating decimals whereas the rest of the columns contain just integers."},{"metadata":{"trusted":false},"cell_type":"code","source":"train_set.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The **temp** varies from 0.82C to 41C. **atemp** broadens this range from 0.76C to 45.45C.\n\nThe **windspeed** varies from 0 to nearly 57 km/h.\n\nThe **humidity** varies from 0 to 100%.\n\nDuring the last two years the average whether rather clear/moderately cloudy."},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_set_dt = train_set.copy()\ntrain_set_dt['datetime-iso8601'] = pd.to_datetime(train_set['datetime'])\ntrain_set_dt = train_set_dt.drop(columns=['datetime'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Season"},{"metadata":{"trusted":false},"cell_type":"code","source":"f, axes = plt.subplots(1, 3)\n\n# Total rentals\ntotal_rentals = train_set_dt[['season', 'count']].groupby('season').sum().reset_index()\nseason_count = sns.barplot('season', 'count', data=total_rentals, ax=axes[0])\nseason_count.set(xlabel='Season', ylabel='Number of total rentals', title='Total rentals')\n\n# Casual rentals\ncasual_rentals = train_set_dt[['season', 'casual']].groupby('season').sum().reset_index()\nseason_casual = sns.barplot('season', 'casual', data=casual_rentals, ax=axes[1])\nseason_casual.set(xlabel='Season', ylabel='Number of casual rentals', title='Casual rentals')\n\n# Casual rentals\nregistered_rentals = train_set_dt[['season', 'registered']].groupby('season').sum().reset_index()\nseason_registered = sns.barplot('season', 'registered', data=registered_rentals, ax=axes[2])\nseason_registered.set(xlabel='Season', ylabel='Number of registered rentals', title='Registered rentals');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most popular season for bike-sharing is always the Fall - with more than 600.000 records, followed by summer (around 580.000 rentals). Surprisingly, winter is always a more popular season than spring. During that season time, people still rent nearly twice as much as during springtime.\n\nA total, casual and registered number of rentals during all seasons are very closely correlated (nearly linear). Interestingly, only during wintertime number of registered rentals is higher than during summer."},{"metadata":{},"cell_type":"markdown","source":"### Temperature"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_set_dt['binned_temp'] = pd.cut(train_set_dt['temp'], 20)\nbinned_temp_count = train_set_dt[['binned_temp', 'count']].groupby('binned_temp').sum().reset_index()\ntemp_count = sns.barplot('binned_temp', 'count', data=binned_temp_count, palette='coolwarm')\ntemp_count.set(xlabel='Temperature', ylabel='Number of total rentals', title='Total rentals / temperature');\ntemp_count.set_xticklabels(temp_count.get_xticklabels(), rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It should not come as a surprise that the warm temperature between 23C - 30C is the most lucrative for bike-sharing companies. Nonthiswising, this collides with the previous prediction that winter is a more popular season for bike-sharing than spring. If we assume that during the winter the temperature range is between 0C and 10C whereas spring temperature could range from 10 to 19 then the spring should bring more customers than winter!\n\nThere might be few reasons for this anomaly, the season categories could be wrongly described in the provided dataset or those could mean something different (i.e. company is located in a different location than the dataset was taken)."},{"metadata":{},"cell_type":"markdown","source":"### Holiday"},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_proportion_of_cols(col_1, col_2):\n    num_of_hols = train_set_dt.groupby([col_1])[col_1].count()\n    return train_set_dt[[col_1, col_2]].groupby(col_1).sum().apply(lambda g: g / num_of_hols).reset_index()\n\n\nf_2, axes_2 = plt.subplots(1, 4)\n\n# Holiday\n# Count\ntotal_per_holiday = get_proportion_of_cols('holiday', 'count')\nholiday_count = sns.barplot('holiday', 'count', data=total_per_holiday, ax=axes_2[0])\nholiday_count.set(xlabel='Holiday', ylabel='Number of total rentals per holiday', title='Proportion of total rentals/holiday')\n\n# Casual\ncasual_per_holiday = get_proportion_of_cols('holiday', 'casual')\nholiday_casual = sns.barplot('holiday', 'casual', data=casual_per_holiday, ax=axes_2[1])\nholiday_casual.set(xlabel='Holiday', ylabel='Number of casual rentals per holiday', title='Proportion of casual rentals/holiday')\n\n# Working day\n# Count\ntotal_per_working_day = get_proportion_of_cols('workingday', 'count')\nworking_day_count = sns.barplot('workingday', 'count', data=total_per_working_day, ax=axes_2[2])\nworking_day_count.set(xlabel='Working day', ylabel='Number of total rentals per working day', title='Proportion of total rentals/working day')\n\n# Casual\ncasual_per_working_day = get_proportion_of_cols('workingday', 'casual')\nworking_day_casual = sns.barplot('workingday', 'casual', data=casual_per_working_day, ax=axes_2[3])\nworking_day_casual.set(xlabel='Working day', ylabel='Number of casual rentals per working day', title='Proportion of casual rentals/working day');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of total rentals versus holidays/working days would not be very exciting as there are more working days than free days.\n\nWhat might be interesting is that the proportion of bike users does not change significantly whether it is a holiday or not. During holiday there are around 185 rentals per total number of holidays and 190 rentals per total number of remaining days.\n\nLooking at the number of casual rentals we can notice a significant spike during holidays. It makes sense as during holiday people will be more willing to rent a shared bike.\n\nSimilarly, the number of casual rentals per non-working day is significantly higher - nearly 60 casual shared bike rentals per total number of the non-working day whereas during working days it is just around 25. That is 240% of the norm!\n\nTaking everything in the account - the most useful information is that there is a spike in demand for shared bikes during holidays and weekends which balances out the number of registered rentals during non-working days."},{"metadata":{},"cell_type":"markdown","source":"### Rentals over time"},{"metadata":{"trusted":false},"cell_type":"code","source":"count_time_series = sns.lineplot('datetime-iso8601', 'count', data=train_set_dt)\ncount_time_series.set(xlabel='Date', ylabel='Number of rentals', title='Number of rentals over time');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the start of data collection in January 2011 till the beginning of 2013, the average number of total bike rentals increased 3 fold. It is easy to perceive that the total number of rentals might increase in the future."},{"metadata":{},"cell_type":"markdown","source":"### Rentals in each month"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_set_dt['month'] = train_set_dt['datetime-iso8601'].apply(lambda datetime: datetime.strftime('%m')).astype(np.str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rentals_during_month = pd.DataFrame({\n    'Month': train_set_dt['month'],\n    'Count': train_set_dt['count'],\n    'Casual': train_set_dt['casual'],\n    'Registed': train_set_dt['registered']}\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rentals_during_month_plt = sns.barplot(x='Month', y='value', hue='Rental type', data=pd.melt(rentals_during_month, ['Month'], var_name=\"Rental type\"))\nrentals_during_month_plt.set(xlabel='Month', ylabel='Number of rentals', title='Number of rentals during each month');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Barplot of casual and registered number of rentals in each month provides some clarification to previous conclusions regarding bike usage.\n\nNext time, we observe that number of casual rentals is normally distributed with its' peak in June - August. This can mean that not only tourists can make those extra number of casual rentals (it is difficult to conclude this from data), but also that the warmer months encourage people to spontaneously rent a bike.\n\nA number of registered rentals in January - March / early April is significantly lower than in remaining months. It is difficult to conclude why there is such a massive drop of registered users in December - January. One explanation would be weather conditions - although there might be some snow in December in Washington DC, January and February are expected to have the highest snowfall."},{"metadata":{},"cell_type":"markdown","source":"### Rentals during week"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_set_dt['weekday'] = train_set_dt['datetime-iso8601'].apply(lambda datetime: datetime.strftime('%A')).astype(np.str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rentals_during_week = pd.DataFrame({\n    'Day': train_set_dt['weekday'],\n    'Count': train_set_dt['count'],\n    'Casual': train_set_dt['casual'],\n    'Registed': train_set_dt['registered']}\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rentals_during_week_plt = sns.barplot(x='Day', y='value', hue='Rental type', data=pd.melt(rentals_during_week, ['Day'], var_name=\"Rental type\"))\nrentals_during_week_plt.set(xlabel='Day', ylabel='Number of rentals', title='Number of rentals during week');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another intriguing plot is the distribution of all types of rentals across the whole week.\n\nThroughout the weekend the common number of casual rentals is around 60, whereas the same type of rental during the week (except Friday) does not exceed 25.\n\nOn the other hand, the number or registered rentals is significantly less during the weekend. Through the week it is approximately 160 rentals per day whereas during the weekend it drops to around 125 rentals."},{"metadata":{},"cell_type":"markdown","source":"### Rentals during day"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_set_dt['time'] = train_set_dt['datetime-iso8601'].apply(lambda datetime: datetime.strftime('%H:%M')).astype(np.str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rentals_during_day = pd.DataFrame({\n    'Hour': train_set_dt['time'],\n    'Count': train_set_dt['count'],\n    'Casual': train_set_dt['casual'],\n    'Registed': train_set_dt['registered']}\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rentals_during_day_plt = sns.lineplot(x='Hour', y='value', hue='Rental type', data=pd.melt(rentals_during_day, ['Hour'], var_name=\"Rental type\"))\nrentals_during_day_plt.set(xlabel='Hour', ylabel='Number of rentals', title='Number of rentals during day');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The chart above shows the number of rentals during the day.\n\nWhat is probably the most interesting is how different the usage density is for casual and registered rental types.\n\nRegistered rentals have two spikes - between 7:00 a.m. and 9:00 a.m. and between 16:30 and 17:30. This means that registered rentals are highly likely to be made by people commuting to and from the workplace or school/university. It can be concluded that registered rentals are made by people that have a pre-defined route that they need to follow daily.\n\nCasual rental distribution is substantially different. It is taking off at around 7:00 a.m. and slowly grows to reach its' peak at around 1 p.m. The plateau lasts till 5 p.m. and then slowly decreases. Casual rentals can be made by tourists as in contract to registered rentals, there are no spikes, but rather normally distributed usage during the day."},{"metadata":{},"cell_type":"markdown","source":"### Rentals during workday / weekend"},{"metadata":{"trusted":false},"cell_type":"code","source":"rentals_day_workingday = pd.melt(train_set_dt[['time', 'workingday', 'registered', 'casual']], ['time', 'workingday'], var_name=\"Rental type\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rentals_day_workingday_plt = sns.FacetGrid(data=rentals_day_workingday, col=\"Rental type\", row='workingday', aspect=2.5)\nrentals_day_workingday_plt = rentals_day_workingday_plt.map(sns.lineplot, \"time\", \"value\")\nrentals_day_workingday_plt.set(ylabel='Number of rentals', xlabel='Hour');\n[plt.setp(ax.get_xticklabels(), rotation=90) for ax in rentals_day_workingday_plt.axes.flat];","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To verify the previous assumptions a facet grid can be created. There are two line plots for different sets of configuration.\n\nPeople that make registered rentals outside working day still use their bike rental services, but there are no spikes during rush hours. Now the plot is more similar to casual rentals plot - it grows steadily from 8 a.m. reaching plateau level at around 11:00 a.m. It is starting levelling off at around 4 p.m. \n\nDuring the peak, there are around 250 rentals during one hour slot and the average number of rentals during the plateau time is about 200.\n\nA total number of casual rentals during the working day is incomparably less than during working days. It stays at a similar level all day reaching less than 80 rentals at its' peak."},{"metadata":{},"cell_type":"markdown","source":"### Rentals during holiday"},{"metadata":{"trusted":false},"cell_type":"code","source":"rentals_day_holiday = pd.melt(train_set_dt[['time', 'holiday', 'registered', 'casual']], ['time', 'holiday'], var_name=\"Rental type\")\nrentals_day_holiday_plt = sns.FacetGrid(data=rentals_day_holiday, col=\"Rental type\", row='holiday', aspect=2.5)\nrentals_day_holiday_plt = rentals_day_holiday_plt.map(sns.lineplot, \"time\", \"value\")\nrentals_day_holiday_plt.set(ylabel='Number of rentals', xlabel='Hour');\n[plt.setp(ax.get_xticklabels(), rotation=90) for ax in rentals_day_holiday_plt.axes.flat];","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analysis of registered and casual rentals during and after a holiday is even more interesting.\n\nThe spikes during rush hours are still present for registered rentals - although now the spikes reach 200 and 280 rentals respectively. During working days those were nearly 2.5 times higher. Also, the time between commuting hours shows that the number of registered rentals is much higher than during working days. Now the plateau reaches approximately 220 rentals during a single hour. For working days that figure was close to 140, which is a significant raise.\n\nNumber of casual rentals during the holiday is very close to the same type of rental outside holiday time.\n\nThe conclusion might be that a significant number of people work during holiday time. On the other hand, casual rentals are mostly done by tourists. For them, it is not as important whether it is a holiday or not and thus a casual number of rentals remain at a similar level throughout the day."},{"metadata":{},"cell_type":"markdown","source":"### Pairwise correlation"},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.heatmap(train_set_dt.corr(), cmap=\"Blues\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pairwise correlation graph shows a linear correlation between two variables.\n\nSome correlations are quite obvious:\n* Temperature and 'Feels like' temperature is strongly, linearly correlated\n* Casual, registered and total number of rentals are correlated\n\nFew correlations might be a bit more useful:\n* Temperature is correlated with a number of all kinds of rentals, although it has a higher impact on casual rentals\n* Humidity does not seem to have a significant linear impact on rental demand\n* Working-days is affecting registered users, but not that significantly casual users"},{"metadata":{},"cell_type":"markdown","source":"## Outlier detection"},{"metadata":{},"cell_type":"markdown","source":"### Count"},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.stats import stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"count_z_score = np.abs(stats.zscore(train_set['count']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f\"\"\"\nMin value: {np.amin(count_z_score)}\nMax value: {np.amax(count_z_score)}\n\"\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_set = train_set.drop(*np.where(count_z_score > 3.92))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Atemp"},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.stats import stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"atemp_z_score = np.abs(stats.zscore(train_set['atemp']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f\"\"\"\nMin value: {np.amin(atemp_z_score)}\nMax value: {np.amax(atemp_z_score)}\n\"\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_set = train_set.drop(*np.where(atemp_z_score > 2.6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Windspeed"},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.stats import stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"windspeed_z_score = np.abs(stats.zscore(train_set['windspeed']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f\"\"\"\nMin value: {np.amin(windspeed_z_score)}\nMax value: {np.amax(windspeed_z_score)}\n\"\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_set = train_set.drop(*np.where(windspeed_z_score > 5.1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preprocessing"},{"metadata":{},"cell_type":"markdown","source":"### Data cleaning"},{"metadata":{},"cell_type":"markdown","source":"There are several steps involved in data cleaning phase.\n\nWe will convert all columns to numerical format and use one-hot encoding for categorical values."},{"metadata":{},"cell_type":"markdown","source":"#### Datetime"},{"metadata":{},"cell_type":"markdown","source":"First, DateTime column is of type *object* and has to be changed:\n* Create column Day\n* Create column Month\n* Create column Year\n* Create column Day of week\n* Create column hour\n* Delete column DateTime"},{"metadata":{"trusted":false},"cell_type":"code","source":"class DateTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        X['iso_datetime'] = pd.to_datetime(X['datetime'])\n        X['day'] = X['iso_datetime'].apply(lambda datetime: datetime.strftime('%d')).astype(\"int\")\n        X['weekday'] = X['iso_datetime'].apply(lambda datetime: datetime.strftime('%w')).astype(\"int\")\n        X['month'] = X['iso_datetime'].apply(lambda datetime: datetime.strftime('%m')).astype(\"int\")\n        X['year'] = X['iso_datetime'].apply(lambda datetime: datetime.strftime('%Y')).astype(\"int\")\n        X['hour'] = X['iso_datetime'].apply(lambda datetime: datetime.strftime('%H')).astype(\"int\")\n        return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The code is very simple and self-explanatory. First, we create a valid DateTime object out of provided DateTime column and then we create some columns with DateTime properties."},{"metadata":{},"cell_type":"markdown","source":"#### Categorical columns"},{"metadata":{},"cell_type":"markdown","source":"There is a number of ways to convert categorical values into a sparse matrix. SKLearn provides a transformer called OneHotEncoder, nevertheless, it outputs NumPy array instead of Pandas DataFrame.\n\nFor this reason, I created a simple wrapper that uses Pandas get_dummies method to create one-hot encoding."},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass OneHotEncoderTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, columns) -> None:\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        X = pd.get_dummies(X, columns=self.columns)\n        return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Column removal"},{"metadata":{},"cell_type":"markdown","source":"The final class is just a tiny wrapper around Pandas drop method.\n\nAlthough we can mangle data without those classes, I tend to wrap my core methods within a class as later on, I can use SKlearn pipeline API to clean, prepare and add new features with just one line invocation function."},{"metadata":{"trusted":false},"cell_type":"code","source":"class LabelRemover(BaseEstimator, TransformerMixin):\n    def __init__(self, label):\n        self.label = label\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.drop(columns=self.label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"The most interesting part of data preparation for many people can be feature engineering.\n\nIt takes a lot of creativity to create new features that will actually improve the performance of the ML model.\n\nDuring the data analysis, we have noticed many correlations and interesting facts from the given dataset. We can use those to create new features."},{"metadata":{},"cell_type":"markdown","source":"#### Weekend feature"},{"metadata":{},"cell_type":"markdown","source":"As the title suggest, Pandas dataframe object can be used to extract day of week from the DateTime object. Based on it, a new column can be added that will check whether the day is either Saturday or Sunday."},{"metadata":{"trusted":false},"cell_type":"code","source":"class FeatureWeekendTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        X['weekend'] = X['iso_datetime'] \\\n        .apply(lambda datetime: datetime.strftime('%w')).astype(\"int\") \\\n        .apply(lambda datetime: datetime in (0, 6)) * 1\n        return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Peak hour feature"},{"metadata":{},"cell_type":"markdown","source":"From examining the findings of bike usage it is clear that many registered rentals are made to commute to workplace/university etc.\nBased on the hour of data it is straightforward to create a boolean feature that will indicate if it is a peak/rush hour or not."},{"metadata":{"trusted":false},"cell_type":"code","source":"class FeaturePeakHourTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        X['peak_hour'] = X['hour'].apply(lambda datetime: datetime in (7, 8, 17, 18)) * 1\n        return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Is it 2011 feature"},{"metadata":{},"cell_type":"markdown","source":"Seemingly, this feature might not bring too much benefit, but from the previous analysis, it is clear that there were significantly more rentals in 2012 rather than 2011."},{"metadata":{"trusted":false},"cell_type":"code","source":"class FeatureIs2011Transformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        X['is_2011'] = X['year'].apply(lambda datetime: datetime == 2011) * 1\n        return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data transformation pipeline"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\n\none_hot_cols = ['season', 'weather', 'holiday', 'workingday', 'peak_hour', 'weekend', 'is_2011']\nprep_pipeline = Pipeline(steps=[\n    ('date_transformer', DateTransformer()),\n    ('feature_weekend_transformer', FeatureWeekendTransformer()),\n    ('feature_peak_hour_transformer', FeaturePeakHourTransformer()),\n    ('feature_is_2011_transformer', FeatureIs2011Transformer()),\n    ('one_hot_encoder_transformer', OneHotEncoderTransformer(one_hot_cols)),\n    ('label_remover', LabelRemover(['iso_datetime', 'datetime']))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_set_prep = prep_pipeline.fit_transform(train_set)\ntest_set_prep = prep_pipeline.fit_transform(test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_set_prep.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train/Test split"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\ntrain_label = train_set['count']\ntrain_set_prep_clean = train_set_prep.drop(columns=['casual', 'registered', 'count'])\nX_train, X_test, Y_train, Y_test = train_test_split(train_set_prep_clean, train_label, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f\"\"\"\nX_train size: {X_train.shape}\nX_test size: {X_test.shape}\nY_train size: {Y_train.shape}\nY_test size: {Y_test.shape}\n\"\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Models comparison"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.linear_model import SGDRegressor\n\n\nregression_models = {\n    'BaggingRegressor': BaggingRegressor(),\n    'ExtraTreesRegressor': ExtraTreesRegressor(),\n    'KNeighborsRegressor': KNeighborsRegressor(),\n    'RandomForestRegressor': RandomForestRegressor()\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"columns = ['Classifier', 'RMSLE']\ndf_scores = pd.DataFrame(columns=columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error\n\n\nfor name, model in regression_models.items():\n    model.fit(X_train, Y_train)\n    Y_predicted = model.predict(X_test)\n    rmsle = np.sqrt(mean_squared_log_error(Y_test, Y_predicted))\n    df_row = pd.DataFrame([[name, rmsle]], columns=columns)\n    df_scores = df_scores.append(df_row, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_scores.sort_values(by=['RMSLE'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first three classifiers provide the best RMSLE, those are RandomForestRegressor, ExtraTreesRegressor and BaggingRegressor.\n\nAlthough default parameters are usually sufficient, those can be tuned to improve the overall score of the model."},{"metadata":{},"cell_type":"markdown","source":"## Boosting regressors"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n\nregression_models = {\n    'AdaBoostRegressor': AdaBoostRegressor(n_estimators=4000, learning_rate=.01),\n    'GradientBoostingRegressor': GradientBoostingRegressor(n_estimators=4000, alpha=.01)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"columns = ['Classifier', 'RMSLE']\ndf_scores = pd.DataFrame(columns=columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error\n\n\nfor name, model in regression_models.items():\n    model.fit(X_train, np.log1p(Y_train))\n    Y_predicted = model.predict(X_test)\n    rmsle = np.sqrt(mean_squared_log_error(Y_test, np.exp(Y_predicted)))\n    df_row = pd.DataFrame([[name, rmsle]], columns=columns)\n    df_scores = df_scores.append(df_row, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_scores.sort_values(by=['RMSLE'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most promising regressor is GradientBoostingRegressor. It requires further data preprocessing (using log1p function) so the optimizing function can converge, but it achieved a better result than the previous models."},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameters tuning"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RandomForestRegressor"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_model = RandomForestRegressor()\nrf_param_grid = {\n    'n_estimators': [100, 200, 500, 700, 1000],\n    'ccp_alpha': [0, .001],\n    'min_impurity_decrease': [0, .001, .01],\n    'ccp_alpha': [0, .001, .002]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_grid_search = GridSearchCV(estimator=rf_model, param_grid=rf_param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_log_error')\nrf_grid_search.fit(X_train, Y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Y_predicted = rf_grid_search.predict(X_test)\nnp.sqrt(mean_squared_log_error(Y_test, Y_predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GradientBoostingRegressor"},{"metadata":{"trusted":false},"cell_type":"code","source":"gb_model = GradientBoostingRegressor()\ngb_param_grid = {\n    'n_estimators': [900, 1000, 2000, 3000, 4000],\n    'alpha': [.01, .02, .03],\n    'max_depth': [3, 4, 6, 8]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gb_grid_search = GridSearchCV(estimator=gb_model, param_grid=gb_param_grid, cv=5, n_jobs=-1, scoring='neg_root_mean_squared_error')\ngb_grid_search.fit(X_train, np.log1p(Y_train));","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gb_grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Y_predicted = gb_grid_search.predict(X_test)\nnp.sqrt(mean_squared_log_error(Y_test, np.exp(Y_predicted)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BaggingRegressor"},{"metadata":{"trusted":false},"cell_type":"code","source":"br_model = BaggingRegressor()\nbr_param_grid = {\n    'n_estimators': [100, 200, 500, 700, 1000],\n    'max_features': [.7, .8, .9, 1]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"br_grid_search = GridSearchCV(estimator=br_model, param_grid=br_param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_log_error')\nbr_grid_search.fit(X_train, Y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"br_grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Y_predicted = br_grid_search.predict(X_test)\nnp.sqrt(mean_squared_log_error(Y_test, Y_predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Learning curve "},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom yellowbrick.model_selection import LearningCurve","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RandomForestRegressor"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_model = RandomForestRegressor(**rf_grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"visualizer = LearningCurve(rf_model, scoring='neg_mean_squared_log_error')\nvisualizer.fit(X_train, Y_train)\nvisualizer.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GradientBoostingRegressor"},{"metadata":{"trusted":false},"cell_type":"code","source":"gb_model = GradientBoostingRegressor(**gb_grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"visualizer = LearningCurve(gb_model, scoring='neg_root_mean_squared_error')\nvisualizer.fit(X_train, np.log1p(Y_train))\nvisualizer.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BaggingRegressor"},{"metadata":{"trusted":false},"cell_type":"code","source":"br_model = BaggingRegressor(**br_grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"visualizer = LearningCurve(br_model, scoring='neg_mean_squared_log_error')\nvisualizer.fit(X_train, Y_train)\nvisualizer.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above plots show cross-validation score in each Nth training instance along with the training score.\n\nAs might be seen in the above plots, the model is far from being perfect. It converges in about 5000 training instances reaching plateau thereafter.\n\nAdding more training data is unlikely to help as the function is not converging noticeably after 6000 training instances.\n\nAll regressor models seem to have similar performance over time."},{"metadata":{},"cell_type":"markdown","source":"## Ensemble methods"},{"metadata":{},"cell_type":"markdown","source":"Another way to improve the score of the model is by using ensemble methods.\n\nSimply put, all three previous models can be combined and use 'voting' to predict the most probable outcome (an outcome that is the most confident by any of the learning models)."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"regressors = [\n    ('RandomForestRegressor', rf_model),\n    ('BaggingRegressor', br_model)\n]\nvoting_reg = VotingRegressor(regressors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"voting_reg.fit(X_train, Y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Y_predicted = voting_reg.predict(X_test)\nnp.sqrt(mean_squared_log_error(Y_test, Y_predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance is slightly worse than GradientBoostingRegressor, hence the latter will be used for submission."},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{},"cell_type":"markdown","source":"With all data prepared and machine learning model fitted, prediction can be saved to CSV file."},{"metadata":{"trusted":false},"cell_type":"code","source":"gb_model = GradientBoostingRegressor(**gb_grid_search.best_params_)\ngb_model.fit(X_train, np.log1p(Y_train))\ntest_Y_final = gb_model.predict(test_set_prep)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_count = pd.DataFrame({\n    'datetime': test_set['datetime'],\n    'count': np.exp(test_Y_final)\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_count.to_csv(index=False, path_or_buf='./submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best RMSLE received in validation set was ~0.2888 and ~0.4197 in test set (submission)."}],"metadata":{"kernelspec":{"display_name":"python-ml","language":"python","name":"python-ml"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"toc-autonumbering":false,"toc-showcode":false,"toc-showmarkdowntxt":false,"pycharm":{"stem_cell":{"cell_type":"raw","source":[],"metadata":{"collapsed":false}}}},"nbformat":4,"nbformat_minor":4}