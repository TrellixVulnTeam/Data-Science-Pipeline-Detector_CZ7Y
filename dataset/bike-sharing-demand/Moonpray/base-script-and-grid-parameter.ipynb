{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"nbconvert_exporter":"python","file_extension":".py","version":"3.6.0","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","name":"python","mimetype":"text/x-python"}},"cells":[{"metadata":{},"source":"** 분석 포인트 **\n1. Kaggle Problem에서 제시하는 **Evaluation에 맞춰 제출 값을 조정**해준다.\n2. **Grid parameter를 적용**하여 최적의 Model Parameter를 선정한다.","cell_type":"markdown"},{"metadata":{},"source":"# Introduction","cell_type":"markdown"},{"metadata":{},"source":"##### 1. Scoring \n   * Kaggle에 제출하는 Score와 Python에서 구한 Score가 동일할 때가 가장 최적의 **Analyzing State** 이다.\n        * 이를 위해 필요한것은 sklearn.metrics 의 make_scorer 함수를 사용하여 kaggle에 있는 평가방식에 맞춰 cross_validation에 적용하는것이다.\n        * rmsle 검색\n    * Scoring을 맞춰 이 값을 **최소(최대)화** 하는것이 분석의 최종 목적\n    \n##### 2. Data Analyzing Start\n   * 데이터 분석의 시작은 **Raw Data 자체**를 꾸준히 보는것이다.\n        * 이를 위한 방법으로 여러가지가 있다.\n            1. Excel을 위한 방법 ( 오랜시간 Data를 볼 수 있을 때 좋음 )\n            2. Python Visualization\n            \n##### 3. Visualization\n   * fig, (axis1,axis2,...) = **plt.subplots**(nrows=1, ncols=2, figsize=(18,4))\n        * plt.subplots()를 사용하여 plot의 **배치**, **크기**를 조절한다.\n        \n   * **sns.countplot**(data= ,x='',ax=) \n        * x를 기준으로 row의 **개수**를 count해준다. x가 \"범주형\"자료일때 자주 사용된다.\n        \n   * **sns.barplot**(data=, x=, y=, hue=)\n        * x를 기준으로 y값의 **합(평균)** 을 계산해준다. y값이 수치변수 일때 사용하면 좋다.\n        \n   * **sns.pointplot**(data=, x=, y=, hue=)\n        * barplot과 동일하게 사용되지만 **추세**가 있는 data에 사용된다.\n        \n   * **sns.Facetplot**(data=, hue=)\n        * **범주형 data**의 요소들의 비율을 선으로 나타내준다. 범주형 데이터만을 볼때 좋다.\n        \n   * grid= **sns.FacetGrid**(train, col='Survived', row='Pclass', size=2.2, aspect=1.6)\n   * grid.**map**(plt.hist, 'Age', alpha=.5, bins=20)\n   * grid.**add_legend()**\n        * plt.subplots() 후 sns.barplot ~~ 이런 방식이아닌 seaborn함수 내의 plot조정, 설정함수를 사용할 수 도 있다.\n        * **중복기준!!!!!** 의 사용에서 용이하다. 위의 예시에서는 Survived 와 Pclass 라는 두 범주형 데이터를 기준으로 hist그램을 그리는데 이때의 x축은 'Age'변수를 사용하는 것이다.\n        * x축은 **수치형** y축은 여러개의 **범주형**을 사용 할때 좋다.\n        \n        \n   * 이 모든 방법들에서 중요한것은 단순히 한 data만 보는 것이 아닌 data의 **조작(groupby, apply)**을 통해 효과적인 시각화를 이끌어 내야한다.\n    \n\n\n##### 4. 시간변수, Datetime을 다루는 방법\n   * 우선 파일에서 Date Field를 읽을 때는 parse_dates=['datetime']을 사용해야한다.\n        * data['datetime'].dt.year(month, dayofweek ...)을 사용가능\n   * Datatime은 보통 **종속변수**에 많은 영향을 끼친다. 따라서 year,month,day,hour,minute,second로 값을 나눠 모든 변수를 분석하고 *scoring*하는 것은 중요하다. ","cell_type":"markdown"},{"metadata":{},"source":"## Bicycle \n\n* Problem definition   \n날짜를 기준으로 몇개의 자전거가 빌려질지 예상하는 문제이다.   \n주어진 문제에서는 총 12개의 data field가 주어진다,   \n주로 대여날짜, 주말(시간적요소)과 날씨(환경적요소)같은 Field가 주어져있다.   \n\n* Problem solution\n우선 주어진 문제에서 평가방식을 살펴보면 Root Mean Squared Logarithmic Error를 사용한다.   \n   $\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 }$\n이 방법의 중요한 점은 Log(x)를 채점방식으로 사용했다는 것이다. 이에 맞춘 **data configuration**도 추후 진행할것이다.    \n\n\n### 제출하는 test set의 평가 방식    \n\nPredict :P   \nAction :A   \n\n1. **| P- A |** = Mean absoule Error   \n\n2. **(P-A)^2** = Mena Squared Error   \n\n3. **(squrt(P-A))^2** = Root Mean Squared Error   \n\n4. **log** 를 사용하는 평가방식은 \"값\" 의 오차(분산)이 매우 넓을 경우에 사용한다.\n    * 이때, logX +1 을 해주는데 log를 사용할때 log1 미만에서는 y값이 \"음수\"가 발생하기 때문이다.\n\n#### Data Fields\n* **1. datetime**\n    - Evaluation에서 평가될때 이 datetime에 따른 count를 생각한다. 즉, datatime의 값은 count에 큰 영향을 줄 것이다.\n* **2. season**\n    - 계절적 요인\n* **3. holiday**\n    -\n* **4. workingday**\n    -\n* **5. weather**  \n    - \n* **6. temp**   \n    - \n* **7. atemp**    \n    - \n* **8. humidity**    \n    - \n* **9. windspeed**      \n    - \n* **10. casual**       \n    - \n* **11. registered**   \n    - \n* **12. count**   \n    -\n\n### Data Analyzing Process\n0. 우선 Data Load -> Preprocessing -> Score 의 기본틀을 만든다.\n1. train Data set을 Load후 Data의 Fields를 **직관적**으로 살펴본다.\n2. 변수 1개 1개를 **시각화**하면서 살펴본다.\n3. 정한 기준에 맞게 변수를 **Encoding** 한다.\n4. **Predict** 틀을 만들고 **Submit**한다.","cell_type":"markdown"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport os\n\nseed=37","cell_type":"code"},{"metadata":{},"source":"# Load Data","cell_type":"markdown"},{"metadata":{},"execution_count":null,"outputs":[],"source":"train = pd.read_csv('../input/train.csv',parse_dates=['datetime'])\nprint(train.shape)\ntrain.head()","cell_type":"code"},{"metadata":{},"execution_count":null,"outputs":[],"source":"test = pd.read_csv('../input/test.csv', parse_dates=['datetime'])\nprint(test.shape)\ntrain.head()","cell_type":"code"},{"metadata":{},"source":"# Visualization and Explore","cell_type":"markdown"},{"metadata":{},"source":"### dateime","cell_type":"markdown"},{"metadata":{"scrolled":true},"execution_count":null,"outputs":[],"source":"train['datetime-year'] = train['datetime'].dt.year\ntrain['datetime-month'] = train['datetime'].dt.month\ntrain['datetime-day'] = train['datetime'].dt.day\ntrain['datetime-hour'] = train['datetime'].dt.hour\ntrain['datetime-minute'] = train['datetime'].dt.minute\ntrain['datetime-second'] = train['datetime'].dt.second\n\nprint(train.shape)\ntrain.head()","cell_type":"code"},{"metadata":{},"source":"### Explore datetime by Visualization","cell_type":"markdown"},{"metadata":{},"execution_count":null,"outputs":[],"source":"fig, ((axis1, axis2, axis3), (axis4, axis5, axis6)) = plt.subplots(nrows=2, ncols=3, figsize= (18,8))\n\nprint('List of Years =' + str(train['datetime-year'].unique())) # unique()로 범주형 data에서 유일한 값만 출력\nprint('List of Month =' + str(train['datetime-month'].unique()))\nprint('List of Day =' + str(train['datetime-day'].unique()))\nprint('List of hours =' + str(train['datetime-hour'].unique()))\nprint('List of minutes =' + str(train['datetime-minute'].unique()))\nprint('List of seconds =' + str(train['datetime-second'].unique()))\n\nsns.countplot(data=train, x='datetime-year', ax=axis1)\nsns.countplot(data=train, x='datetime-month', ax=axis2)\nsns.countplot(data=train, x='datetime-day', ax=axis3)\nsns.countplot(data=train, x='datetime-hour', ax=axis4)\nsns.countplot(data=train, x='datetime-minute', ax=axis5)\nsns.countplot(data=train, x='datetime-second', ax=axis6)\n# countplot() 는 x축을 기준으로 dataframe에서 몇개의 row(value)를 가지고 있는지 나타내준다.","cell_type":"code"},{"metadata":{},"source":"#### 'Datetime' Learned\n\n1. 연도, 달, 일자, 시간 별로 count의 큰차이는 보이지 않는다.\n2. datetime-minute 과 datetime-second는 모든 value가 0 으로 분석의 가치가 없다.\n3. datetime-day의 field 가 **(0~19)** 뿐이다. 이는 train data와 test data를 일자를 기준으로 나눴다고 생각할 수 있다.\n    * 따라서 datetime-day column을 model시키면 **OverFitting**이 발생할 것이다.","cell_type":"markdown"},{"metadata":{},"execution_count":null,"outputs":[],"source":"fig, (axis1,axis2,axis3) = plt.subplots(1,3 , figsize=(18,4))\n\nsns.barplot(data=train, x='datetime-year', y='count' ,ax= axis1)\nsns.barplot(data=train, x='datetime-month', y='count', ax= axis2)\nsns.barplot(data=train, x='datetime-hour', y='count',ax= axis3)\n\n# barplot은 기본적인 bar 그래프이다. x값(범주형)에 대해 y값(연속형) 데이터의 mean을 출력해준다.","cell_type":"code"},{"metadata":{},"source":"#### 'Datetime' Learned2\n\n1. 2011년도 보다 2012년에 더 많은 자전거 대여가 이루어 졌다.\n2. 여름철 (5~10)이 겨울철 보다 상대적으로 많은 대여가 이루어진다.\n3. 주로 아침시간(**출근시간**)과 퇴근시간(**17~18**)에 대여가 빈번히 일어난다.","cell_type":"markdown"},{"metadata":{},"source":"### Explore 'Datetime' (Year and Month) \n* In Fiture seeking page, Data should be **splited** as possible as small and should be dealed by **grouping** thinking","cell_type":"markdown"},{"metadata":{},"execution_count":null,"outputs":[],"source":"fig, (axis1, axis2) = plt.subplots(1,2 ,figsize=(18,4))\n\nsns.barplot(data=train, x='datetime-year', y='count', ax=axis1)\nsns.barplot(data=train, x='datetime-month', y='count', ax=axis2)\n\ntrain['datetime-year_month'] = train['datetime'].apply(lambda dt: str(dt.year) + \"-\" + str(dt.month))\n# apply()는 train['datetime']의 각 row(value) 하나씩에 함수를 적용시켜준다.\n# datetime field 전체에 str()를 적용해주면 이상한 모양으로 문자열을 변형 시키기 때문에 apply로 각 value마다 str()을 적용시킨다,\n\nfig, axis3 = plt.subplots(figsize=(18,4))\n\nsns.barplot(data=train, x='datetime-year_month', y='count', ax=axis3)","cell_type":"code"},{"metadata":{},"source":"#### 'datetime-year_month' learned\n1. 1월과 2월의 차이가 크다고 판단할 수 있지만, 실제로는 2011년과 2012년의 전체적인 값의 차이라 볼 수 있다.\n2. 또한 7월에 value가 떨어진다 판단할 수 있지만, 실제로는 2012년에만 value가 떨어졌다.\n3. 2011년과 2012년 모두 한 year안에서 month 별 value의 흐름이 있다고 볼 수 있다. 하지만, 전체적으로 봤을때 1월과 2월이라서 항상 value가 작다고는 볼 수 없다. ( 2012년의 1,2월이 2011년 12월보다 value가 크기 때문에 **model의 error 야기** )","cell_type":"markdown"},{"metadata":{},"source":"### Explore 'datetime-year, month / \"Hour\"! ","cell_type":"markdown"},{"metadata":{},"execution_count":null,"outputs":[],"source":"def bin_hour(hour):\n    if hour<=7:\n        return \"others\"\n    elif hour <= 12:\n        return \"morning\"\n    elif hour <= 17:\n        return \"afternoon\"\n    elif hour <= 22:\n        return \"night\"\n    else:\n        return \"others\"\n\ntrain['datetime-bin_hour'] = train['datetime-hour'].apply(bin_hour)\n\nprint(train.shape)\ntrain.head()","cell_type":"code"},{"metadata":{},"execution_count":null,"outputs":[],"source":"fig, (axis1,axis2) = plt.subplots(2,1, figsize=(24,12))\n\nsns.pointplot(data=train, x='datetime-month', y='count', hue='datetime-bin_hour', ax=axis1)\nsns.pointplot(data=train, x='datetime-year_month', y='count', hue='datetime-bin_hour', ax=axis2)","cell_type":"code"},{"metadata":{},"source":"#### 'datetime- Hour! ' learned\n1. 새벽 시간대 ( 22~07시 에는 자전거 이용빈도가 많이 떨어진다. )\n2. month 만 봤을때는 night 이용이 morning 보다 높지만 전체적으로 봤을때 항상 그런것은 아니다.\n3. 2011년도와 2012년도에 달마다 변화하는 양상이 조금 다르다.\n  * 2012년도 2~4월은 오후에 count가 급격하게 상승하나, 2011년은 그렇지 않다.\n  * 2012년도 7월은 오후에 count가 급격하게 하락하나, 2011년은 그렇지 않다.\n  * 2011년도 7월은 아침에 count가 변동이 없으나, 2012년은 하락한다.\n  * 2012년도 11월은 저녁에 count가 급격하게 하락하나, 2011년은 그렇지 않다.","cell_type":"markdown"},{"metadata":{},"source":"### Explore Hour / Woring Day + DayofWeek(주말,주중)","cell_type":"markdown"},{"metadata":{},"execution_count":null,"outputs":[],"source":"days = {0: 'Monday', 1: 'Tuesday', 2: \"Wednesday\", 3: \"Thursday\", 4: \"Friday\", 5: \"Saturday\", 6: \"Sunday\"}\n\ntrain['datetime-dayofweek'] = train['datetime'].dt.dayofweek\ntrain['datetime-dayofweek'] = train['datetime-dayofweek'].apply(lambda day: days[day])\n# 숫자형 data인 dt.dayofweek를 문자열로 변경시키는 방법 \n# Dictionary 자료형을 이용하기!!!!!!!!!\n\nfigures, (ax1, ax2, ax3) = plt.subplots(nrows=3)\nfigures.set_size_inches(18, 12)\n\nsns.pointplot(data=train, x=\"datetime-hour\", y=\"count\", ax=ax1)\nsns.pointplot(data=train, x=\"datetime-hour\", y=\"count\", hue=\"workingday\", ax=ax2)\nsns.pointplot(data=train, x=\"datetime-hour\", y=\"count\", hue=\"datetime-dayofweek\", ax=ax3)","cell_type":"code"},{"metadata":{},"source":"#### 'datetime- dayofweek' learned\n1. Working Day에 따른 자전거 대여 양상이 뚜렷하게 차이난다.\n     * **Working day : 1**은 평일인데 이때는 (6~9 / 16~18) 시의 대여량이 많다.\n     * **Working day : 0**은 주말인데 오후시간(12~16) 시의 대여량이 높은것을 확인가능하다.\n     \n2. 요일마다의 대여양상도 비슷해 보이는데 몇가지 차이를 찾아낼 수 있다.\n    * Friday는 평일이지만 Working day와 비슷한 양상을 띈다. 그다음은 Monday가 영향을 받는다.\n    * 일요일과 토요일을 비교했을때 토요일이 더 높은 대여율을 보인다.","cell_type":"markdown"},{"metadata":{},"source":"### Explore Count\n* Label ficture이 되는 Count의 모양을 변형시켜 model 에 더 정확한 값을 얻을 수 있다.\n* 가장 기본이 되는 분포 모양은 **가우시안 분포** 로 _실상황_ 에서 일반적이라 생각되는 분포이다.","cell_type":"markdown"},{"metadata":{},"execution_count":null,"outputs":[],"source":"fig, (axis1, axis2, axis3) = plt.subplots(1,3, figsize=(18,4))\n\ntrain['log_count'] = np.log(train['count'])\ntrain['log_count_plus'] = np.log(train['count']+1)\n\nsns.distplot(train['count'], ax=axis1)\nsns.distplot(train['log_count'], ax=axis2)\nsns.distplot(train['log_count_plus'], ax=axis3)","cell_type":"code"},{"metadata":{},"source":"# Preprocessing","cell_type":"markdown"},{"metadata":{},"source":"### Merge data to Combi","cell_type":"markdown"},{"metadata":{},"execution_count":null,"outputs":[],"source":"train = pd.read_csv('../input/train.csv', parse_dates=['datetime'])\ntest = pd.read_csv('../input/test.csv', parse_dates=['datetime'])","cell_type":"code"},{"metadata":{},"execution_count":null,"outputs":[],"source":"combi = pd.concat([train, test])\n\nprint(combi.shape)\ncombi.head()","cell_type":"code"},{"metadata":{},"source":"### Data Featuring","cell_type":"markdown"},{"metadata":{},"source":"#### datetime","cell_type":"markdown"},{"metadata":{"scrolled":true},"execution_count":null,"outputs":[],"source":"combi['datetime-year'] = combi['datetime'].dt.year\ncombi['datetime-month'] = combi['datetime'].dt.month\ncombi['datetime-day'] = combi['datetime'].dt.day\ncombi['datetime-hour'] = combi['datetime'].dt.hour\ncombi['datetime-minute'] = combi['datetime'].dt.minute\ncombi['datetime-second'] = combi['datetime'].dt.second\n\nprint(combi.shape)\ncombi[['datetime','datetime-year','datetime-month','datetime-day','datetime-hour','datetime-minute','datetime-second']].head()","cell_type":"code"},{"metadata":{},"source":"#### year+month","cell_type":"markdown"},{"metadata":{},"execution_count":null,"outputs":[],"source":"combi['datetime-year_month'] = combi['datetime'].apply(lambda dt: str(dt.year) + '-' + str(dt.month))\n\nyear_month = pd.get_dummies(combi['datetime-year_month'], prefix=\"datetime-year_month\").astype(np.bool)\nprint(year_month.shape)","cell_type":"code"},{"metadata":{},"execution_count":null,"outputs":[],"source":"combi2 = combi.copy()","cell_type":"code"},{"metadata":{},"execution_count":null,"outputs":[],"source":"combi = pd.concat([combi2, year_month], axis=1)\nprint(combi.shape)\ncombi.head()","cell_type":"code"},{"metadata":{},"source":"#### dayofweek","cell_type":"markdown"},{"metadata":{},"execution_count":null,"outputs":[],"source":"dayofweek = combi['datetime'].dt.dayofweek\ndayofweek = pd.get_dummies(dayofweek, prefix='datetime-dayofweek').astype(np.bool)","cell_type":"code"},{"metadata":{},"execution_count":null,"outputs":[],"source":"combi2= combi.copy()","cell_type":"code"},{"metadata":{},"execution_count":null,"outputs":[],"source":"combi = pd.concat([combi2, dayofweek], axis=1)\nprint(combi.shape)\ncombi.head()","cell_type":"code"},{"metadata":{},"source":"#### bin_time","cell_type":"markdown"},{"metadata":{},"execution_count":null,"outputs":[],"source":"def bin_hour(hour):\n    if hour <= 7:\n        return 0\n    elif hour <= 15:\n        return 1\n    elif hour <= 21:\n        return 2\n    else:\n        return 0\n    \n    \ncombi['datetime-bin_hour'] = combi['datetime-hour'].apply(bin_hour)\nbin_hour_dummies = pd.get_dummies(combi['datetime-bin_hour'], prefix='datetime-hour_bin').astype(np.bool)\nprint(bin_hour_dummies.shape)","cell_type":"code"},{"metadata":{},"execution_count":null,"outputs":[],"source":"combi2 = combi.copy()","cell_type":"code"},{"metadata":{},"execution_count":null,"outputs":[],"source":"combi = pd.concat([combi2, bin_hour_dummies],axis=1)\nprint(combi.shape)\ncombi.head()","cell_type":"code"},{"metadata":{},"source":"### Split data to Train/Test","cell_type":"markdown"},{"metadata":{},"execution_count":null,"outputs":[],"source":"train = combi[combi['count'].notnull()]\n\nprint(train.shape)\ntrain.head()","cell_type":"code"},{"metadata":{},"execution_count":null,"outputs":[],"source":"test = combi[combi['count'].isnull()]\n\nprint(test.shape)\ntest.head()","cell_type":"code"},{"metadata":{},"source":"# Scoring","cell_type":"markdown"},{"metadata":{},"execution_count":null,"outputs":[],"source":"bin_hour_dummies.columns","cell_type":"code"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"feature_names = train.columns\nfeature_names = list(feature_names)\n\nremove_content =['casual','registered','count','datetime-year', 'datetime-month','datetime',\n                 'datetime-day', 'datetime-minute', 'datetime-second', 'datetime-year_month',\n                'datetime-bin_hour']\nfor x in remove_content:\n    feature_names.remove(x)\n    \nlabel_name= 'count'","cell_type":"code"},{"metadata":{},"execution_count":null,"outputs":[],"source":"X_train = train[feature_names]\n\nprint(X_train.shape)\nX_train.head()","cell_type":"code"},{"metadata":{},"source":"### Convert label(Count) into log(Count +1)\n* Evaluation에 맞게 score을 계산하므로 model을 만들때 y_train(count에 값들)도 Evaluation에 맞게 변형시켜줘야한다.","cell_type":"markdown"},{"metadata":{},"execution_count":null,"outputs":[],"source":"y_train = np.log(train[label_name]+1)\n\nprint(y_train.shape)\ny_train.head()","cell_type":"code"},{"metadata":{},"execution_count":null,"outputs":[],"source":"X_test = test[feature_names]\n\nprint(X_test.shape)\nX_test.head()","cell_type":"code"},{"metadata":{},"source":"### 평가 방식에 맞는 scoring 함수 적용하기\n$\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 }$","cell_type":"markdown"},{"metadata":{},"execution_count":null,"outputs":[],"source":"from sklearn.metrics import make_scorer\n\ndef rmsle(predict, actual):\n    predict = np.array(predict)\n    actual = np.array(actual)\n    \n    log_predict = predict + 1\n    log_actual = actual + 1\n    \n    difference = log_predict - log_actual\n    difference = np.square(difference) # 제곱함수\n    difference = np.mean(difference) # 전체합 / N = mean\n    \n    score = np.sqrt(difference)\n    \n    return score\n\nrmsle_score = make_scorer(rmsle)\nrmsle_score","cell_type":"code"},{"metadata":{},"source":"### 머신러닝 scoring 적용","cell_type":"markdown"},{"metadata":{},"execution_count":null,"outputs":[],"source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.cross_validation import cross_val_score\n\n# make model\nmodel = RandomForestRegressor(random_state=seed)\n\n# score\nscore = cross_val_score(model, X_train, y_train,scoring=rmsle_score, cv=20).mean()\n\nprint('score is {score: .5f}' .format(score=score))","cell_type":"code"},{"metadata":{},"source":"## Find Optimal Parameters\n\n머신러닝 model들의 tunining을 하는 방법 : **Hyper Parameter**   \n   hyper parameter 이란?   \n인공지능 조차 자동으로 찾을 수 없는 parameter들을 사용자 들이 값을 지정해서 model을 향상시키는 방법\n\n_**장점**_    \n    * 모든 도메인, 지식, 기술에 적용가능하다.\n    \n_**단점**_   \n    * 별로 많은 값이 오르지는 않는다.\n    \n** sklearn.ensemble.RandomForestRegressor **  \n1. n_estimations = 트리의 개수 \n    * 특징 : 높으면 높을수록 좋다, \n    * But) 속도의 증가 So, 어느 수준 이상에서는 많이 올라가지 않는다.\n    \n2. max_feature='auto' :선택하는 feature의 개수 \n    * 특징 : 높을수록 training set에 너무 적합한 model이 형성된다. \n    * But) 너무 많이하면 test set에 맞지않는 Model 이형성된다.\n    \nGrid Search는 optimal을 찾는 것은아니다. Why? parameter의 범위를 사람들이 정해줘야하기 때문이다.   \n**Grid Search-Random Search** But) 돌리다보면 어느정도 parameter들의 범위가 나온다.(Score을 기준으로) 하나씩 구해지는 기준에따라 범위를 정해놓고 First Search -> Second Search -> Final Search를 돌리면 Optimal parameter를 구할 수 있다.","cell_type":"markdown"},{"metadata":{},"source":"### Case1 : Grid Search","cell_type":"markdown"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.cross_validation import cross_val_score\n\nn_estimators = 300\nmax_depth_list = [1,3, 10, 30, 50, 100]\nmax_features_list = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n\ngrid_parameters_list = []\n\nfor max_depth in max_depth_list:\n    for max_features in max_features_list:\n        model = RandomForestRegressor(n_estimators=n_estimators,\n                                     max_depth=max_depth,\n                                     max_features=max_features,\n                                     n_jobs=-1, # 자동으로 가지치기를 하지않는다(?)\n                                     random_state = seed)\n        \n        score = cross_val_score(model, X_train, y_train, scoring=rmsle_score, cv=20).mean()\n        \n        print('n_estimators = {0}, max_depth = {1:2}, max_features ={2}, Score(RMSLE) ={3: .6f}' \\\n             .format(n_estimators,max_depth,max_features, score))\n        \n        parameter = {\n            'n_estimators' : n_estimators,\n            'max_depth' : max_depth,\n            'max_features' : max_features,\n            'score' : score\n        }\n        \n        grid_parameters_list.append(parameter)\n        \ngrid_parameters_list = pd.DataFrame.from_dict(grid_parameters_list)\n# list를 dataFrame형태로 변경한다. 이때 기준은 dict로 사용 왜냐하면 list의 각 요소는 dict형태이기 때문에\ngrid_parameters_list.sort_values('score', ascending=True, inplace=True)\n# dataFrame의 정렬에서는 sort_values을 사용하여 'score' field를 기준으로 정렬\n\ngrid_parameters_list.head(10)","cell_type":"code"},{"metadata":{},"source":"### Case2 : Random Search(First)","cell_type":"markdown"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.cross_validation import cross_val_score\n\nnum_epoch = 100\ncoarse_parameters_list = []\n\nn_estimators = 300\n\nfor epoch in range(num_epoch):\n    np.random.seed(epoch)\n    max_depth = int(np.random.uniform(1, 100))\n    \n    np.random.seed(epoch)\n    max_features = np.random.uniform(0.1, 1.0)\n\n    model = RandomForestRegressor(n_estimators=n_estimators,\n                                  max_depth=max_depth,\n                                  max_features=max_features,\n                                  random_state=seed,\n                                  n_jobs=-1)\n\n    score = cross_val_score(model, X_train, y_train, scoring=rmsle_score, cv=20).mean()\n\n    print(\"epoch = {0:2}, n_estimators = {1}, max_depth = {2:2}, max_features = {3:.6f}, Score(RMSLE) = {4:.5f}\"\\\n          .format(epoch, n_estimators, max_depth, max_features, score))\n\n    parameters = {\n        'n_estimators': n_estimators,\n        'max_depth': max_depth,\n        'max_features': max_features,\n        'score': score,\n    }\n\n    coarse_parameters_list.append(parameters)\n    \ncoarse_parameters_list = pd.DataFrame.from_dict(coarse_parameters_list)\ncoarse_parameters_list.sort_values('score', ascending=True, inplace=True)\n\ncoarse_parameters_list.head(10)","cell_type":"code"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"# Coarse search 결과, 다음의 범위 안에 optimal parameter가 있다는 사실을 찾을 수 있다.\nminimum_max_depth = 30\nmaximum_max_depth = 60\n\nminimum_max_features = 0.4\nmaximum_max_features = 0.6","cell_type":"code"},{"metadata":{},"source":"### Case2-2 Random Search(Final)","cell_type":"markdown"},{"metadata":{"collapsed":true},"execution_count":null,"outputs":[],"source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.cross_validation import cross_val_score\n\nnum_epoch = 100\nfiner_parameters_list = []\n\nn_estimators = 300\n\nfor epoch in range(num_epoch):\n    np.random.seed(epoch)\n    max_depth = int(np.random.uniform(minimum_max_depth, maximum_max_depth))\n\n    np.random.seed(epoch)\n    max_features = np.random.uniform(minimum_max_features, maximum_max_features)\n    \n    model = RandomForestRegressor(n_estimators=n_estimators,\n                                  max_depth=max_depth,\n                                  max_features=max_features,\n                                  random_state=seed,\n                                  n_jobs=-1)\n\n    score = cross_val_score(model, X_train, y_train, scoring=rmsle_score, cv=20).mean()\n\n    print(\"epoch = {0:2}, n_estimators = {1}, max_depth = {2:2}, max_features = {3:.6f}, Score(RMSLE) = {4:.5f}\"\\\n          .format(epoch, n_estimators, max_depth, max_features, score))\n\n    parameters = {\n        'n_estimators': n_estimators,\n        'max_depth': max_depth,\n        'max_features': max_features,\n        'score': score,\n    }\n\n    finer_parameters_list.append(parameters)\n\nfiner_parameters_list = pd.DataFrame.from_dict(finer_parameters_list)\nfiner_parameters_list.sort_values('score', ascending=True, inplace=True)\n\nfiner_parameters_list.head(10)","cell_type":"code"},{"metadata":{},"source":"### User hyperparameter above","cell_type":"markdown"},{"metadata":{},"execution_count":null,"outputs":[],"source":"from sklearn.ensemble import RandomForestRegressor\n\n#optimal_hyperparameters = finer_parameters_list.iloc[0]\n\nn_estimators = 300\n#max_depth = optimal_hyperparameters[\"max_depth\"]\nmax_depth = 46\n# max_features = optimal_hyperparameters[\"max_features\"]\nmax_features = 0.509763\n\nmodel = RandomForestRegressor(n_estimators=n_estimators,\n                              max_depth=max_depth,\n                              max_features=max_features,\n                              random_state=seed,\n                              n_jobs=-1)\n\nmodel","cell_type":"code"},{"metadata":{},"execution_count":null,"outputs":[],"source":"from sklearn.cross_validation import cross_val_score\n\n%time score = cross_val_score(model, X_train, y_train, scoring=rmsle_score, cv=20).mean()\n\nprint(\"Score = {score:.5f}\".format(score=score))","cell_type":"code"},{"metadata":{},"source":"## Submit","cell_type":"markdown"},{"metadata":{},"execution_count":null,"outputs":[],"source":"model.fit(X_train, y_train)\n\npredictions = model.predict(X_test)\n\n## 가우시안 분포에 맞게 수정한 label(=count)를 원래대로 돌려논다.\npredictions = np.exp(predictions)-1\n\nprint(predictions.shape)\npredictions[:3]","cell_type":"code"}],"nbformat_minor":1,"nbformat":4}