{"cells":[{"metadata":{"id":"ZvkFzlNTldWk"},"cell_type":"markdown","source":"# Overview\nBike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.\n\nThe data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city. In this competition, participants are asked to combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C.\n\n"},{"metadata":{"id":"Y_HQi5nzlhU_"},"cell_type":"markdown","source":"### About Dataset\n\ndatetime - hourly date + timestamp  \n\nseason -  1 = spring, 2 = summer, 3 = fall, 4 = winter \n\nholiday - whether the day is considered a holiday\n\nworkingday - whether the day is neither a weekend nor holiday\n\nweather -\n\n1: Clear, Few clouds, Partly cloudy, Partly cloudy \n\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \n\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds \n\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n\ntemp - temperature in Celsius\n\natemp - \"feels like\" temperature in Celsius\n\nhumidity - relative humidity\n\nwindspeed - wind speed\n\ncasual - number of non-registered user rentals initiated\n\nregistered - number of registered user rentals initiated\n\ncount - number of total rentals"},{"metadata":{"id":"a-1l05BwSXy3","trusted":true},"cell_type":"code","source":"# pip install pandas\n# pip install \n# pip install scikit-learn\n# pip install matplotlib\n# pip install seaborn","execution_count":null,"outputs":[]},{"metadata":{"id":"w7maOlglgAG_","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"id":"n9dcs1eJnzoR"},"cell_type":"markdown","source":"Creating a dictionary to store both train and test data together for preprocessing"},{"metadata":{"id":"k4btdjSBgAHD","trusted":true},"cell_type":"code","source":"dfs = {}\nfor name in ['train', 'test']:\n    df = pd.read_csv('../input/bike-sharing-demand/%s.csv' % name)\n    df['_data'] = name\n    dfs[name] = df","execution_count":null,"outputs":[]},{"metadata":{"id":"YIfY8PTlgAHF","outputId":"e94bac14-e42c-46b2-f6bd-7c0de45d76b2","trusted":true},"cell_type":"code","source":"# combine train and test data into one df\ndf = dfs['train'].append(dfs['test'])\n\n# lowercase column names\ndf.columns = map(str.lower, df.columns)\n\n# parse datetime colum & add new time related columns\ndt = pd.DatetimeIndex(df['datetime'])\ndf.set_index(dt, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"ASp7DGY13X6V","outputId":"b71d5765-b170-4579-f196-4e91a95545a4","trusted":true},"cell_type":"code","source":"df.tail(20)","execution_count":null,"outputs":[]},{"metadata":{"id":"OhSszIh6of1k","outputId":"8ddebb54-eac4-4c44-be19-9940b883651c","trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"Ne-NZJ9Joiuy","outputId":"5e807db2-811c-48ee-fb19-9782bc79ff8b","trusted":true},"cell_type":"code","source":"df.isnull().sum() ","execution_count":null,"outputs":[]},{"metadata":{"id":"J0fqxrFGJuWg","outputId":"60dca9e2-23c9-4dc0-8795-e3366d887729","trusted":true},"cell_type":"code","source":"#corelation matrix.\ncor_mat= df[:].corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"vDnwV93voSln"},"cell_type":"markdown","source":""},{"metadata":{"id":"6KrkQDCUpcCT","outputId":"740f68bb-215b-4949-8f97-4f6a67c52b32","trusted":true},"cell_type":"code","source":"df[['count','casual','registered']].hist(figsize=(20,10), grid=False, layout=(1, 3), bins = 30)","execution_count":null,"outputs":[]},{"metadata":{"id":"PTPE3_vjq6-q"},"cell_type":"markdown","source":"When our original continuous data do not follow the bell curve, we can log transform this data to make it as “normal” as possible so that the statistical analysis results from this data become more valid. In other words, the log transformation reduces or removes the skewness of our original data"},{"metadata":{"id":"6ham3WWggAHI","trusted":true},"cell_type":"code","source":"# logarithmic transformation of dependent cols\n# (adding 1 first so that 0 values don't become -inf)\nfor col in ['casual', 'registered', 'count']:\n    df['%s_log' % col] = np.log(df[col] + 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"93cUS5m3pwN7","outputId":"7d5f803c-9eb8-4303-a785-54b7ca224a86","trusted":true},"cell_type":"code","source":"df[['count_log','casual_log','registered_log']].hist(figsize=(20,10), grid=False, layout=(1, 3), bins = 30)","execution_count":null,"outputs":[]},{"metadata":{"id":"uhjaeMMLgAHK","trusted":true},"cell_type":"code","source":"df['date'] = dt.date\ndf['day'] = dt.day\ndf['month'] = dt.month\ndf['year'] = dt.year\ndf['hour'] = dt.hour\ndf['dow'] = dt.dayofweek\ndf['woy'] = dt.weekofyear","execution_count":null,"outputs":[]},{"metadata":{"id":"vmroXYmBi9Ac","outputId":"87febc84-3544-4dbe-b429-19ef819e398b","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"3jpQTfFrgAHQ","outputId":"6422e015-9583-408a-d1e5-a5d1496ff5e3","trusted":true},"cell_type":"code","source":"# add a count_season column using join\nby_season = df[df['_data'] == 'train'].groupby('season')[['count']].agg(sum)\nby_season.columns = ['count_season']\ndf = df.join(by_season, on='season')\n\nprint(by_season)","execution_count":null,"outputs":[]},{"metadata":{"id":"Ijxz86cTJF_S","outputId":"020d0fba-277e-495f-ec38-e847027dbbf0","trusted":true},"cell_type":"code","source":"#sns.factorplot(x='season',data=df,kind='count',size=5,aspect=1)\nsns.factorplot(x='season',data=df,kind='count',size=5,aspect=1.5)","execution_count":null,"outputs":[]},{"metadata":{"id":"begIJfwBTlzs"},"cell_type":"markdown","source":"The demand of bike is evenly distributed across seasons"},{"metadata":{"id":"ROitswuegAHV","trusted":true},"cell_type":"code","source":"def get_day(day_start):\n    day_end = day_start + pd.offsets.DateOffset(hours=23)\n    return pd.date_range(day_start, day_end, freq=\"H\")\n#considering one entire day\n#H= hourly frequency\n# tax day\ndf.loc[get_day(pd.datetime(2011, 4, 15)), \"workingday\"] = 1\ndf.loc[get_day(pd.datetime(2012, 4, 16)), \"workingday\"] = 1\n# thanksgiving friday\ndf.loc[get_day(pd.datetime(2011, 11, 25)), \"workingday\"] = 0\ndf.loc[get_day(pd.datetime(2012, 11, 23)), \"workingday\"] = 0\n# tax day\ndf.loc[get_day(pd.datetime(2011, 4, 15)), \"holiday\"] = 0\ndf.loc[get_day(pd.datetime(2012, 4, 16)), \"holiday\"] = 0\n\n# thanksgiving friday\ndf.loc[get_day(pd.datetime(2011, 11, 25)), \"holiday\"] = 1\ndf.loc[get_day(pd.datetime(2012, 11, 23)), \"holiday\"] = 1\n\n#storms\ndf.loc[get_day(pd.datetime(2012, 5, 21)), \"holiday\"] = 1\n#tornado\ndf.loc[get_day(pd.datetime(2012, 6, 1)), \"holiday\"] = 1","execution_count":null,"outputs":[]},{"metadata":{"id":"bzha7n2bgAHY","outputId":"6c193b14-401b-4ecf-9e67-bedf9a1163ce","trusted":true},"cell_type":"code","source":"# rentals by hour, split by working day (or not)\nby_hour = df[df['_data'] == 'train'].copy().groupby(['hour', 'workingday'])['count'].agg('sum').unstack()\n\nby_hour.plot(kind='bar', figsize=(8,4), width=0.8);","execution_count":null,"outputs":[]},{"metadata":{"id":"nxXK-pICzINc"},"cell_type":"markdown","source":"Extracting the peak hours for every wave in demand of bike"},{"metadata":{"id":"3xrBUXnzgAHa","outputId":"fccf8927-84dc-46e5-88e9-402f920fbd64","trusted":true},"cell_type":"code","source":"df['peak'] = df[['hour', 'workingday']].apply(lambda x: (0, 1)[(x['workingday'] == 1 and  ( x['hour'] == 8 or 17 <= x['hour'] <= 18 or x['hour'] == 12)) or (x['workingday'] == 0 and  10 <= x['hour'] <= 19)], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"ObAn1-K0z1FA","outputId":"b9b6830a-1580-469c-f1b9-f1cd3f79499d","trusted":true},"cell_type":"code","source":"df[(df['year'] == 2012) & (df['month'] == 12) & (df['day']==25)]","execution_count":null,"outputs":[]},{"metadata":{"id":"dg2N9QbpgAHc","outputId":"0d524d1c-6fe8-4bbd-f0f3-f46f50ebd72b","trusted":true},"cell_type":"code","source":"#sandy\ndf['holiday'] = df[['month', 'day', 'holiday', 'year']].apply(lambda x: (x['holiday'], 1)[x['year'] == 2012 and x['month'] == 10 and (x['day'] in [30])], axis = 1)\n\n#christmas day and others\ndf['holiday'] = df[['month', 'day', 'holiday']].apply(lambda x: (x['holiday'], 1)[x['month'] == 12 and (x['day'] in [24, 26, 31])], axis = 1)\ndf['workingday'] = df[['month', 'day', 'workingday']].apply(lambda x: (x['workingday'], 0)[x['month'] == 12 and x['day'] in [24, 31]], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"Ns-etirw4SGd"},"cell_type":"markdown","source":"Creating columns whre people want to go out cycling more often"},{"metadata":{"id":"XkCg_e1ngAHf","outputId":"9f29764d-d0bd-46de-83a8-00eebd2661e4","trusted":true},"cell_type":"code","source":"df['ideal'] = df[['temp', 'windspeed']].apply(lambda x: (0, 1)[x['temp'] > 27 and x['windspeed'] < 30], axis = 1)\ndf['sticky'] = df[['humidity', 'workingday']].apply(lambda x: (0, 1)[x['workingday'] == 1 and x['humidity'] >= 60], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"6fkTex9OJgFv","outputId":"4c3d636c-512d-4868-8874-0df7e0be07d8","trusted":true},"cell_type":"code","source":"# can also be visulaized using histograms for all the continuous variables.\ndf.temp.unique()\nfig,axes=plt.subplots(2,2)\naxes[0,0].hist(x=\"temp\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\naxes[0,0].set_title(\"Variation of temp\")\naxes[0,1].hist(x=\"atemp\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\naxes[0,1].set_title(\"Variation of atemp\")\naxes[1,0].hist(x=\"windspeed\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\naxes[1,0].set_title(\"Variation of windspeed\")\naxes[1,1].hist(x=\"humidity\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\naxes[1,1].set_title(\"Variation of humidity\")\nfig.set_size_inches(10,10)","execution_count":null,"outputs":[]},{"metadata":{"id":"KMJIP85M4mcJ","outputId":"f653b2ad-f024-4132-acea-82108e6f7aad","trusted":true},"cell_type":"code","source":"windspeed = df[df['_data'] == 'train'].copy().groupby(['windspeed', 'workingday'])['count'].agg('sum').unstack()\nwindspeed.plot(kind='bar', figsize=(8,4), width=0.8);","execution_count":null,"outputs":[]},{"metadata":{"id":"_1SdX_Cj5Z0x","outputId":"b1c30ef7-ffce-49e7-f0e4-f3799a0209f5","trusted":true},"cell_type":"code","source":"tempera = df[df['_data'] == 'train'].copy().groupby(['temp', 'workingday'])['count'].agg('sum').unstack()\ntempera.plot(kind='bar', figsize=(8,4), width=0.8);","execution_count":null,"outputs":[]},{"metadata":{"id":"H4cwD3kW5pra","outputId":"1f675131-3030-4135-d63b-c98a2eeff434","trusted":true},"cell_type":"code","source":"humid = df[df['_data'] == 'train'].copy().groupby(['humidity', 'workingday'])['count'].agg('sum').unstack()\nhumid.plot(kind='bar', figsize=(15,4), width=0.8);","execution_count":null,"outputs":[]},{"metadata":{"id":"ad2kTMlcJmYh","outputId":"12920428-5ab8-4363-fd5a-1e6283f16fd7","trusted":true},"cell_type":"code","source":"#corelation matrix.\ncor_mat= df[:].corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"ONPiv_VvgAHj","trusted":true},"cell_type":"code","source":"def getrmsle(y_pred, y_actual):\n    diff = np.log(y_pred + 1) - np.log(y_actual + 1)\n    mean_error = np.square(diff).mean()\n    return np.sqrt(mean_error)\n\n\ndef get_data():\n    data = df[df['_data'] == 'train'].copy()\n    return data\n\n\ndef train_test_split_custom(data, midway_day=15):\n    train = data[data['day'] <= midway_day]\n    val = data[data['day'] > midway_day]\n\n    return train, val\n\n\ndef prepare_distributed_data(data, input_cols):\n    X = data[input_cols].values\n    y_registered = data['registered_log'].values\n    y_casual = data['casual_log'].values\n\n    return X, y_registered, y_casual\n\n\ndef predict_on_validation_set(model, input_cols):\n    data = get_data()\n\n    train, val = train_test_split_custom(data)\n\n    X_train, y_train_registered, y_train_casual = prepare_distributed_data(train, input_cols)\n    X_val, y_val_registered, y_val_casual = prepare_distributed_data(val, input_cols)\n\n    model_registered = model.fit(X_train, y_train_registered)\n    y_pred_registered = np.exp(model_registered.predict(X_val)) - 1\n\n    model_casual = model.fit(X_train, y_train_casual)\n    y_pred_casual = np.exp(model_casual.predict(X_val)) - 1\n\n    y_pred_combined = np.round(y_pred_registered + y_pred_casual)\n    y_pred_combined[y_pred_combined < 0] = 0\n\n    y_val_combined = np.exp(y_val_registered) + np.exp(y_val_casual) - 2\n\n    score = getrmsle(y_pred_combined, y_val_combined)\n    return (y_pred_combined, y_val_combined, score)\n\ndf_test = df[df['_data'] == 'test'].copy()\n\n# predict on test set & transform output back from log scale\ndef predict_on_test_data(model, x_cols):\n    # prepare training set\n    df_train = df[df['_data'] == 'train'].copy()\n    X_train = df_train[x_cols].values\n    y_train_cas = df_train['casual_log'].values\n    y_train_reg = df_train['registered_log'].values\n\n    # prepare test set\n    X_test = df_test[x_cols].values\n\n    casual_model = model.fit(X_train, y_train_cas)\n    y_pred_cas = casual_model.predict(X_test)\n    y_pred_cas = np.exp(y_pred_cas) - 1\n    registered_model = model.fit(X_train, y_train_reg)\n    y_pred_reg = registered_model.predict(X_test)\n    y_pred_reg = np.exp(y_pred_reg) - 1\n    # add casual & registered predictions together\n    return y_pred_cas + y_pred_reg","execution_count":null,"outputs":[]},{"metadata":{"id":"J9481kKtgAHl","outputId":"370a4f50-5e41-4fc2-cf7d-c7965d2cfaf7","trusted":true},"cell_type":"code","source":"params = {'n_estimators': 500, 'max_depth': 100, ' min_samples_leaf': 2, 'min_samples_split' : 5, 'n_jobs': -1,'bootstrap':True,'max_features': 'auto'}\nrf_model = RandomForestRegressor(n_estimators =500,\n min_samples_split = 5,\n min_samples_leaf = 2,\n max_features= 'auto',\n max_depth = 100,\n bootstrap = True)\nrf_cols = [\n    'weather', 'temp', 'atemp', 'windspeed',\n    'workingday', 'season', 'holiday', 'sticky',\n    'hour', 'dow', 'woy', 'peak'\n    ]\n\n(rf_p, rf_val, rf_score) = predict_on_validation_set(rf_model, rf_cols)\nprint (rf_score)","execution_count":null,"outputs":[]},{"metadata":{"id":"jA-7tfYlgAHp","outputId":"0b783a12-22ca-4d55-f223-0e60459fb4a9","trusted":true},"cell_type":"code","source":"df[rf_cols].corr()","execution_count":null,"outputs":[]},{"metadata":{"id":"cv6E9QCZgAHr","outputId":"a3dd2700-a6a6-468a-a722-709bddb64058","trusted":true},"cell_type":"code","source":"params = {'n_estimators': 166, 'max_depth': 5, 'random_state': 0, 'min_samples_leaf' : 10, 'learning_rate': 0.1, 'subsample': 0.7, 'loss': 'ls'}\ngbm_model = GradientBoostingRegressor(**params)\ngbm_cols = [\n    'weather', 'temp', 'atemp', 'humidity', 'windspeed',\n    'holiday', 'workingday', 'season',\n    'hour', 'dow', 'year', 'ideal', 'count_season',\n]\n\n(gbm_p, gbm_val, gbm_score) = predict_on_validation_set(gbm_model, gbm_cols)\nprint (gbm_score)","execution_count":null,"outputs":[]},{"metadata":{"id":"NArwtoeXgAHt","outputId":"e199e6d8-6da1-49cc-e5fd-d32fad9ec617","trusted":true},"cell_type":"code","source":"df[gbm_cols].corr()","execution_count":null,"outputs":[]},{"metadata":{"id":"hA6w-vgOgAHw","outputId":"08adf158-5d51-417e-85a9-cbd97d94705f","trusted":true},"cell_type":"code","source":"# the blend gives a better score on the leaderboard, even though it does not on the validation set\ny_p = np.round(.2*rf_p + .8*gbm_p)\nprint (getrmsle(y_p, rf_val))","execution_count":null,"outputs":[]},{"metadata":{"id":"357tik5agAHy","trusted":true},"cell_type":"code","source":"rf_pred = predict_on_test_data(rf_model, rf_cols)\ngbm_pred = predict_on_test_data(gbm_model, gbm_cols)\ny_pred = np.round(.2*rf_pred + .8*gbm_pred)\n# output predictions for submission\ndf_test['count'] = y_pred\nfinal_df = df_test[['datetime', 'count']].copy()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Write to file "},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.to_csv('submit.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"FLe57cpmT5C6"},"cell_type":"markdown","source":"## Conclusion\nStand alone my Gradient Boosting model was better compared to random forest model. But, since this a competition to increasing my model's accuracy I took complementary summation of bith model wherer (RF:02 and GBR :08) which decreased my RMSLE to 0.3181 and thus increasing my score"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}