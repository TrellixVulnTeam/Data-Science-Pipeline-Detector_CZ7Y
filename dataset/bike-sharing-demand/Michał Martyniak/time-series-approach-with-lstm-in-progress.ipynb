{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Favorite lib :)\n!pip install livelossplot","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom IPython.display import display\nfrom pathlib import Path\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nfrom livelossplot.keras import PlotLossesCallback\n\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 50)\npd.set_option('display.width', 1000)\n\nROOT_DIR = Path(\"/kaggle/input/bike-sharing-demand\")\nTRAIN_DATA_PATH = ROOT_DIR / \"train.csv\"\nTEST_DATA_PATH = ROOT_DIR / \"test.csv\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ideas gathered during research\n* Multivariate CNN (1D): https://machinelearningmastery.com/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting/\n* LSTM-based: https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/\n* GRU?\n* RL?\n* Separate models for registered and casual users\n\n\n### TODO\n* Read: https://arxiv.org/pdf/1901.00069.pdf\n* Use dropout\n* Play with weights initialization\n* Predict on test data (prepend train to make historical context)\n\n### Unused\n* https://www.youtube.com/watch?v=d4Sn6ny_5LI\n    * Univariate: Holt's Winter Method, Moving average\n    * Multivariate: Vector Auto Regression, RNNs"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def expanded_index_datetime_col(data):\n    data = data.copy()\n    data[\"hour\"] = data.index.hour\n    data[\"weekday\"] = data.index.weekday\n    data[\"day\"] = data.index.day\n    data[\"month\"] = data.index.month\n    data[\"year\"] = data.index.year\n    return data\n\ndef replaced_with_onehot_cols(data, col_names):\n    data = data.copy()\n    \n    for col_name in col_names:\n        one_hot = pd.get_dummies(data[col_name], prefix=col_name)\n        data = data.join(one_hot)\n        \n        # Original column is not needed anymore\n        del data[col_name]\n    return data\n\ndef display_cols(df):\n    print(f\"Columns: ({len(df.columns)}) {list(df.columns)}\")\n\n# We want to predict \"count\" for last time step in the input, not for the next time step!\n# Therefore we need to shift i1t by one step\n# https://machinelearningmastery.com/how-to-use-the-timeseriesgenerator-for-time-series-forecasting-in-keras/\n# y_train = train[[\"count\"]].values\n# y_train = np.roll(y_train, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load train/val set"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_VAL = pd.read_csv(TRAIN_DATA_PATH, parse_dates=True, index_col=\"datetime\")\ndisplay(TRAIN_VAL)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare train/val dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_val = expanded_index_datetime_col(TRAIN_VAL)\ntrain_val = replaced_with_onehot_cols(train_val, col_names=[\"season\", \"holiday\", \"workingday\", \"weather\", \"weekday\", \"month\", \"year\"]) # \"year\"\ntrain_val = train_val.drop([\"casual\", \"registered\"], axis=1)\n\ndisplay(train_val.head())\ntrain_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_val[train_val[\"day\"] <= 16]\nprint(\"Days range: {}-{}\".format(train[\"day\"].min(), train[\"day\"].max()))\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val = train_val[train_val[\"day\"] > 16]\n\nprint(\"Days range: {}-{}\".format(val[\"day\"].min(), val[\"day\"].max()))\ndisplay(val.head(0))\nval.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalized_cols(df, scaler):\n    df = df.copy()\n    return pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns, index=df.index)\n\nscaler = MinMaxScaler()\n\nx_train = normalized_cols(df=train.drop(\"count\", axis=1), scaler=scaler)\ny_train = normalized_cols(df=train[[\"count\"]], scaler=scaler)\n\nx_val = normalized_cols(df=val.drop(\"count\", axis=1), scaler=scaler)\ny_val = normalized_cols(df=val[[\"count\"]], scaler=scaler)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,9))\nplt.plot(y_train.index, y_train['count'], 'blue', label='train')\nplt.plot(y_val.index, y_val['count'], 'orange', label='val')\nplt.legend()\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x = [[1,1], [2,2], [3, 3], [4, 4],  [5,5], [1,1], [2,2], [3, 3], [4, 4],  [5,5], [6, 6]]\n# y = [11,    12,    13,     14,      15,    111,   112,   113,    1114,    115,   116]\n# g = TimeseriesGenerator(x, y, length=5, batch_size=1)\n# gg = TimeseriesGenerator(x, y, length=2, batch_size=1)\n# g[1]\n# c = g + gg\n\ndef gen(X, Y):\n    start_idx = 0\n    window_size = 2 * 24 # 2 days (if there are no gaps)\n     \n    while start_idx + window_size <= X.size:\n        # Each row contains feature set\n        rows = X[start_idx:start_idx + window_size]\n        \n        # Remove \"day\" from features (helper column only)\n        sequence_of_features = rows.drop(\"day\", axis=1)\n        \n        # Value for last element in input sequence (NOT from the future)\n        target = Y[\"count\"][start_idx + window_size - 1]\n        yield sequence_of_features.values, target\n        \n        \n        current_day = X[\"day\"][start_idx]\n        if start_idx + window_size < len(X):\n            last_day = X[\"day\"][start_idx + window_size]\n        else:\n            # Prevent IndexError\n            break\n            \n        # Don't allow for making sequences containing time gaps\n        if current_day > last_day:\n            # skip to next month if day number would decrease.\n            start_idx += window_size\n        else:\n            # shift next yielded window by stride (offset)\n            start_idx += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import Sequence #, to_categorical, plot_model\n\nclass BatchGenerator(Sequence):\n\n    def __init__(self, xy_gen, batch_size=32):\n        self.batch_size = batch_size\n        self.xy = list(xy_gen)\n        \n    def __len__(self):\n        \"\"\"Returns number of batches\"\"\"\n        return len(self.xy) // self.batch_size\n    \n    def __getitem__(self, idx):\n        batch_range = range(idx * self.batch_size, (idx + 1) * self.batch_size)        \n        batch_with_sequences_of_features = []\n        batch_with_targets = []\n        \n        for seq_id in batch_range:\n            seq_of_features, target = self.xy[seq_id]\n            batch_with_sequences_of_features.append(seq_of_features)\n            batch_with_targets.append(target)\n        \n        # Prepare randomized indexes for shuffling mini-batches\n        indices = np.arange(self.batch_size)\n        np.random.shuffle(indices)\n        \n        # Convert to numpy and shuffle\n        batch_with_sequences_of_features = np.array(batch_with_sequences_of_features)[indices]\n        batch_with_targets = np.array(batch_with_targets)[indices]\n        \n        return batch_with_sequences_of_features, batch_with_targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen = BatchGenerator(gen(x_train, y_train), batch_size=64)\nval_gen = BatchGenerator(gen(x_val, y_val), batch_size=64)\nlen(train_gen), len(val_gen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seq_length = 1 #24 * 3 # 2 days\n# num_features = x_train[0].size\n# batch_size = 64\n\n# train_generator = TimeseriesGenerator(x_train, y_train, length=seq_length, batch_size=batch_size)\n# print(f\"Lookback: {seq_length} | Number of features: {num_features}\")\n\n# for idx in range(len(train_generator)):\n#     print(train_generator[idx])\n# train_generator[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_generator[1]\n# int(len(train)/(19*24))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Input, LSTM, Dense, Dropout, SimpleRNN\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping\nimport keras.backend as K\n\ndef rmse(y_true, y_pred):\n    \"\"\" root_mean_squared_error \"\"\"\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n\ninput = Input(shape=(48, 38))\n# _ = SimpleRNN(4, activation='relu')(input)\n_ = LSTM(32, activation='relu')(input)\n_ = Dropout(0.4)(_)\n_ = Dense(16, activation='relu')(_)\n_ = Dropout(0.4)(_)\n_ = Dense(8, activation='relu')(_)\noutput = Dense(1, activation='relu')(_)\n\nmodel = Model(inputs=input, outputs=output)\nmodel.compile(optimizer='adam', loss='mse', metrics=[rmse])\n\nhistory = model.fit_generator(train_gen, validation_data=val_gen,\n                              epochs=25, verbose=1, \n                              callbacks=[\t\n                                    EarlyStopping(monitor=\"val_loss\"),\n                                    PlotLossesCallback(), \n                              ]\n                             )","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"loss = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(1, len(loss)+1)\nplt.plot(epochs, loss, 'bo', label='loss_train')\nplt.plot(epochs, loss_val, 'b', label='loss_val')\nplt.title('value of the loss function')\nplt.xlabel('epochs')\nplt.ylabel('value of the loss function')\nplt.legend()\nplt.grid()\nplt.show()\n\n# data.info()\n# data[['temp', 'casual', 'registered']].plot(alpha=0.5)\n# data.describe().drop(\"count\")\n# In case there would be some columns with null values: df['foo’].fillna(value=df['foo'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_of_features = x_val[0:48].drop(\"day\", axis=1)\npred = model.predict(np.expand_dims(sequence_of_features.values))\n\nplt.plot(np.squeeze(pred), 'b', label='val pred')\nplt.plot(np.squeeze(y_val[\"count\"][0:48]), 'r', label='val gt')\n\n# pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel.predict(np.expand_dims(x_test, axis=1))\n# split datetime column into year, day, month, hour\n# use minmax scaler\n# split 19-day train set into train and val (last x days)\n# onehot - get dummies\n# build generator\n# prepare simple model\n# prepare callbacks: livelossplot, earlystopping, etc.\n# fit model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['hour'] = df.index.hour #create column containing the hour\ndf['dayofweek'] = df.index.dayofweek","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_csv(TEST_DATA_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n# plot each column\nplt.figure()\nplt.subplot(3, 1, 1)\nplt.plot(data[\"temp\"], 1, 1)\nplt.subplot(3, 1, 2)\nplt.plot(data[\"windspeed\"], 1, 2)\nplt.subplot(3,1,3)\nplt.plot(data[\"casual\"], 1, 3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}