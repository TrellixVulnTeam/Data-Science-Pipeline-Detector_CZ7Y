{"cells":[{"metadata":{},"cell_type":"markdown","source":"## MichaÅ‚ Martyniak (martyniak.tech)\n\n\n#### Possible enhancements:\n1. Focus more on feature engineering\n2. Try multivariate time series forecasting with LSTM (almost done in my another public notebook)\n3. Use hyper-parameter optimization with grid search, different kernel initialization methods, etc.\n4. Feature importance/interpretation (NLS, PyTorch): https://arxiv.org/abs/1910.05206"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# https://github.com/stared/livelossplot\n!pip install livelossplot tensorflow-gpu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom pandas import DataFrame\nfrom typing import List, NamedTuple, Tuple\nimport seaborn as sns\n\nfrom IPython.display import display\nfrom pathlib import Path\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom livelossplot.keras import PlotLossesCallback\n\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 50)\npd.set_option('display.width', 1000)\n\nROOT_DIR = Path(\"/kaggle/input/bike-sharing-demand\")\nTRAIN_DATA_PATH = ROOT_DIR / \"train.csv\"\nTEST_DATA_PATH = ROOT_DIR / \"test.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def expanded_index_datetime_col(data: DataFrame) -> DataFrame:\n    data = data.copy()\n    data[\"hour\"] = data.index.hour\n    data[\"weekday\"] = data.index.weekday\n    data[\"month\"] = data.index.month\n    data[\"year\"] = data.index.year\n    return data\n\ndef replaced_with_onehot_cols(data: DataFrame, col_names: List[str]) -> DataFrame:\n    data = data.copy()\n    \n    for col_name in col_names:\n        one_hot = pd.get_dummies(data[col_name], prefix=col_name)\n        data = data.join(one_hot)\n        \n        # Original column is not needed anymore\n        del data[col_name]\n    return data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def load(path: Path) -> DataFrame:\n    return pd.read_csv(path, parse_dates=True, index_col=\"datetime\")\n\ndef correlation(df: DataFrame) -> DataFrame:\n    corr = df.corr()\n    return sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values)\n\ndef prepare(df: DataFrame) -> DataFrame:\n    df = df.copy()\n    df = expanded_index_datetime_col(df)\n    df = replaced_with_onehot_cols(df, col_names=[\"season\", \"holiday\", \"workingday\", \"weather\", \"weekday\", \"month\", \"year\"])\n    df = df.drop([\"casual\", \"registered\", \"atemp\"], axis=1, errors=\"ignore\")\n    return df\n\noriginal_train: DataFrame = load(TRAIN_DATA_PATH)\ndisplay(original_train.describe())\ndisplay(correlation(original_train))\n\ntrain_val: DataFrame = prepare(original_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_cols(df: DataFrame, scaler) -> DataFrame:\n    df = df.copy()\n    return DataFrame(scaler.fit_transform(df.values), columns=df.columns, index=df.index)\n\nx_scaler = MinMaxScaler()\nx_trainval = train_val.drop(\"count\", axis=1)\nx_trainval = normalize_cols(df=x_trainval, scaler=x_scaler)\n\ny_scaler = MinMaxScaler()\ny_trainval = train_val[[\"count\"]]\ny_trainval = normalize_cols(df=y_trainval, scaler=y_scaler)\n\nx_train, x_val, y_train, y_val = train_test_split(x_trainval, y_trainval, test_size=0.1)\n_, NUM_FEATURES = x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport tensorflow.keras.backend as K\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle_K(y, y_hat):\n    return K.sqrt(K.mean(K.square(tf.math.log1p(y) - tf.math.log1p(y_hat))))\n\ndef small_model():\n    input = Input(shape=(NUM_FEATURES, ))\n    _ = Dense(8, activation='relu')(input)\n    output = Dense(1, activation='relu')(_)\n    return Model(inputs=input, outputs=output)\n\ndef medium_model():\n    input = Input(shape=(NUM_FEATURES, ))\n    _ = Dense(32, activation='relu')(input)\n    _ = Dropout(0.4)(_)\n    _ = Dense(32, activation='relu')(_)\n    _ = Dropout(0.4)(_)\n    _ = Dense(16, activation='relu')(_)\n    output = Dense(1, activation='relu')(_)\n    return Model(inputs=input, outputs=output)\n\ndef large_model():\n    \"\"\"Far too many parameters for this problem.\"\"\"\n    input = Input(shape=(NUM_FEATURES, ))\n    _ = Dense(64, activation='relu')(input)\n    _ = Dropout(0.5)(_)\n    _ = Dense(64, activation='relu')(_)\n    _ = Dropout(0.5)(_)\n    _ = Dense(64, activation='relu')(_)\n    _ = Dropout(0.5)(_)\n    _ = Dense(64, activation='relu')(_)\n    output = Dense(1, activation='relu')(_)\n    return Model(inputs=input, outputs=output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compile(model: Model) -> Model:\n    model.compile(optimizer='adam', loss=rmsle_K, metrics=['mse'])\n    return model\n\nclass TrainingResult(NamedTuple):\n    model: Model\n    train_loss: float\n    val_loss: float\n\ndef train(model, x: np.ndarray, y: np.ndarray, \n          val_data=Tuple[np.ndarray, np.ndarray]) -> TrainingResult:\n    history = model.fit(x, y,\n                    validation_data=val_data,\n                    epochs=200, \n                    batch_size=64,\n                    verbose=1, \n                    callbacks=[\n                        PlotLossesCallback(), \n                        ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=5, min_lr=0.000001, verbose=1),\n                        EarlyStopping(monitor=\"val_loss\", patience=10, verbose=1),\n                    ])\n    return TrainingResult(model, \n                          train_loss=history.history['loss'][-1], \n                          val_loss=history.history['val_loss'][-1])\n\nmodels = {\n    \"small\": small_model(),\n    \"medium\": medium_model(),\n    \"large\": large_model()\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Run trainings"},{"metadata":{"trusted":true},"cell_type":"code","source":"results = {}\nfor name, model in models.items():\n    result = train(compile(model), x_train.values, y_train.values, val_data=(x_val.values, y_val.values))\n    results[name] = result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Show losses"},{"metadata":{"trusted":true},"cell_type":"code","source":"for name, result in results.items():\n    print(name, result.train_loss, result.val_loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict on train/val"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_predictions(model: Model, x: DataFrame, y: DataFrame, title=\"preds\"):\n    norm_preds = model.predict(x.values)\n    predictions = y_scaler.inverse_transform(norm_preds)\n    targets = y_scaler.inverse_transform(y[[\"count\"]])\n    \n    x = x.sort_index()\n    fig = plt.figure(figsize=(25,9))\n    plt.title(title, fontsize=24)\n    plt.plot(x.index, targets, label=\"target\")\n    plt.plot(x.index, predictions, alpha=0.7, label=\"prediction\")\n    plt.legend()\n    plt.show()\n\n\ns = slice(400, 500)\nmodel = results[\"small\"].model\n# model = results[\"medium\"].model\n# model = results[\"large\"].model\nshow_predictions(model, x_train[s], y_train[s], title=\"Predictions on train set\")\nshow_predictions(model, x_val[s], y_val[s], title=\"Predictions on validation set\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(df: DataFrame, normalize=True) -> np.array:\n    if normalize:\n        x_test = x_scaler.fit_transform(df.values)\n    else:\n        x_test = df.values\n    preds = model.predict(x_test)\n    if normalize:\n        return y_scaler.inverse_transform(preds)\n    return preds\n\ndef save_submission(test_df: DataFrame, preds: np.array, path: str):\n    submission = test_df.copy()\n    submission[\"datetime\"] = test_df.index\n    submission[\"count\"] = preds\n    submission = submission[[\"datetime\", \"count\"]]\n    submission.to_csv(path, index=False)\n\n\noriginal_test: DataFrame = load(TEST_DATA_PATH)\ntest: DataFrame = prepare(original_test)\nprint(list(x_train.columns))\nprint(list(test.columns))\nassert list(x_train.columns) == list(test.columns)\ntest = normalize_cols(df=test, scaler=x_scaler)\n\npredictions = evaluate(test)\nsave_submission(test, predictions, path=f\"/kaggle/working/v13.csv\")\n\nfig = plt.figure(figsize=(25,9))\nplt.title(\"Predictions on test set\", fontsize=24)\nplt.plot(test.index, predictions)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Quick glimpse of other methods"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn import tree\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\nregressors = {\n    \"Linear\": LinearRegression(),\n    \"SVR\": SVR(),\n    \"XGB\": XGBRegressor(),\n    \"RandomForest\": RandomForestRegressor(),\n    \"NN\": results[\"large\"].model\n}\n\ndef evaluate_model(model, x: DataFrame) -> np.ndarray:\n    norm_preds = model.predict(x.values)\n    predictions = y_scaler.inverse_transform(norm_preds.reshape(-1, 1))\n    return predictions\n    \ndef plot(xs, y_predictions, y_targets=None):\n    plt.plot(xs, y_predictions, alpha=0.5, color='r', label='prediction')\n    if y_targets is not None:\n        plt.plot(xs, y_targets, color='b', label=\"target\")\n    plt.legend()\n    plt.show()\n\ns = slice(1000, 1200)\n\nfor name, regressor in regressors.items():\n    if name == \"NN\":\n        regr_model = regressor\n    else:\n        regr_model = regressor.fit(x_train.values, y_train.values)    \n    \n    # Validation set\n    predictions = evaluate_model(regr_model, x_val[s])\n    targets = y_scaler.inverse_transform(y_val[s][[\"count\"]])\n    \n    fig = plt.figure(figsize=(25,9))\n    plt.title(name, fontsize=24)\n    plot(x_val[s].sort_index().index, predictions, targets)\n    \n    # Test set\n    predictions = evaluate_model(regr_model, test)\n    plot(test.index, predictions)\n    save_submission(test, predictions, path=f\"/kaggle/working/{name}_v13.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}