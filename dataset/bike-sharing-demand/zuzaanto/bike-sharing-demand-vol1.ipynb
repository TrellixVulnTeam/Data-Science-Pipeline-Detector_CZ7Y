{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Bike sharing demand - brief data analysis**\n#### In this section I'll try to get an idea about the data, correlations, disproportions etc.\n\n#### I'll try to choose the variables I'll use as an input, maybe produce some new ones.\n\n#### I'll also try to work my way up the train-val split.\n\nSome of the graphs posted here were directly inspired by [Chun's notebook](https://www.kaggle.com/chun1182/bike-shared-xgboost), [Vivek Srinivasan's notebook](https://www.kaggle.com/viveksrinivasan/eda-ensemble-model-top-10-percentile) and [Raj Mehrotra's notebook](https://www.kaggle.com/rajmehra03/bike-sharing-demand-rmsle-0-3194/)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n%matplotlib inline\nsns.set(style='whitegrid',color_codes=True)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# sampleSubmission = pd.read_csv(\"../input/bike-sharing-demand/sampleSubmission.csv\")\ntest = pd.read_csv(\"../input/bike-sharing-demand/test.csv\")\ntrain = pd.read_csv(\"../input/bike-sharing-demand/train.csv\")\ndf = train.copy()\ntest_df = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at first 5 rows of train data\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Some basic things to remember:**\n* All features are numeric.\n \n* They are not balanced, which may be an issue, TODO.\n \n* Datetime is treated as ID.\n\n* We only care about total count. *However, discriminating registered and casual users may prove helpful? (as with the whales;) TODO verify.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# (safety first)\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"disclaimer: The following stats are accumulative and do not reflect any correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# BY SEASON\nprint(\"Season:\")\ndf.season.value_counts()\nsns.factorplot(x='season',data=df,kind='count',size=3,aspect=1)\n# by holiday\nprint(\"holiday\")\nprint(df.holiday.value_counts())\nsns.factorplot(x='holiday',data=df,kind='count',size=3,aspect=1) # majority of data is for non holiday days.\nprint(\"working day\")\nprint(df.workingday.value_counts())\nsns.factorplot(x='workingday',data=df,kind='count',size=3,aspect=1) # majority of data is for working days.\nprint(\"weather (0 is the clearest)\")\nprint(df.weather.value_counts())\nsns.factorplot(x='weather',data=df,kind='count',size=3,aspect=1)  \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# distribution and outliers of continuous variables\nsns.boxplot(data=df[['temp', 'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count']])\nfig=plt.gcf()\nfig.set_size_inches(9,9)\n# or maybe let's see the histograms\ndf.temp.unique()\nfig,axes=plt.subplots(2,2)\naxes[0,0].hist(x=\"temp\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\naxes[0,0].set_title(\"Variation of temp\")\naxes[0,1].hist(x=\"atemp\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\naxes[0,1].set_title(\"Variation of atemp\")\naxes[1,0].hist(x=\"windspeed\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\naxes[1,0].set_title(\"Variation of windspeed\")\naxes[1,1].hist(x=\"humidity\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\naxes[1,1].set_title(\"Variation of humidity\")\nfig.set_size_inches(10,10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now what we all have been waiting for..."},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"cor_mat= df[:].corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ok, so..\n1. Atemp and temp are correlated in a very similar extent, and they are very correlated with each other. Let's get rid of one of those. Based on my intuition, I'll keep atemp. (I'd say this correlation is most likely non-linear tho, extreme temperatures are probably bad -> some combination of temperature and season is probably what we care about.)\n2. \"Casual\" and \"registered\" are not \"factors\" here, so ignore those\n3. Of the important factors here are also humidity, season, weather and windspeed.\n4. Casual users are more affected by temperature than regular ones. They're also slightly more discouraged by high humidity, bad weather and a tiny bit by the wind.\n5. Casual users care less about the season tho: regulars use other transportation in the winter, that's my guess.\n6. Casual users don't ride bikes on working days, opposed to regulars. In the holidays the disproportion is smaller, but still: more random bikers, less regulars.\n7. Also, \"humidity\" and \"weather\" are pretty strongly correlated. Humidity has more impact and should be kept, but \"weather\" may not be a crucial factor here after all.\n\nNote: We still have to check the hour's effect on the demand."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"hour\"] = [t.hour for t in pd.DatetimeIndex(df.datetime)]\ndf[\"day\"] = [t.day for t in pd.DatetimeIndex(df.datetime)]\ndf[\"month\"] = [t.month for t in pd.DatetimeIndex(df.datetime)]\ndf['year'] = [t.year for t in pd.DatetimeIndex(df.datetime)]\ndf['year'] = df['year'].map({2011:0, 2012:1})\ndf.head()\n# same for test\ntest_df[\"hour\"] = [t.hour for t in pd.DatetimeIndex(test_df.datetime)]\ntest_df[\"day\"] = [t.day for t in pd.DatetimeIndex(test_df.datetime)]\ntest_df[\"month\"] = [t.month for t in pd.DatetimeIndex(test_df.datetime)]\ntest_df['year'] = [t.year for t in pd.DatetimeIndex(test_df.datetime)]\ntest_df['year'] = test_df['year'].map({2011:0, 2012:1})\ncor_mat= df[:].corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok. So, we don't care about \"day\" (also proved in cell 17 [here](https://www.kaggle.com/viveksrinivasan/eda-ensemble-model-top-10-percentile#Ensemble-Model---Gradient-Boost)). We do care about \"month\", but more or less the same way as about the \"season\".\nWe do however care about hour, there is a strong correlation here. Also, it seems like the year matters: the service getting more popular with time might be an explanation.\n\nSo for now let's drop the inputs we decided are redundant."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('datetime',axis=1,inplace=True)\ntest_df.drop('datetime',axis=1,inplace=True)\ndf.drop('temp',axis=1,inplace=True)\ntest_df.drop('temp',axis=1,inplace=True)\n# let's not drop day for now, we shall need it for the split\n# df.drop('day',axis=1,inplace=True)\n# test_df.drop('day',axis=1,inplace=True)\nprint(df.head())\nprint(test_df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train-validation split\n#### Now, let's split data into training and validation\nTest data contains last 10 days of each month. Train set consists of first 19 days of each month. So for validation, let's get 4 days of each month - hopefully this way we can avoid domain shift. I'll just do it manually."},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_df = df[df[\"day\"] < 5]\nvalid_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = df[df[\"day\"] >= 5]\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train_df.copy()\nx_train.drop('count',axis=1,inplace=True)\nx_train.drop(\"casual\",axis=1,inplace=True)\nx_train.drop(\"registered\",axis=1,inplace=True)\nx_valid = valid_df.copy()\nx_valid.drop('count',axis=1,inplace=True)\nx_valid.drop(\"casual\",axis=1,inplace=True)\nx_valid.drop(\"registered\",axis=1,inplace=True)\ny_train = train_df[\"count\"]\ny_valid = valid_df[\"count\"]\nprint(x_train.head())\nprint(x_valid.head())\nprint(y_train.head())\nprint(y_valid.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1st model choice: RF and other regressors\nOk, from other people's notebooks, Random Forests seem like a way to go... let's compare it with some other regression models."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error \n\nmodels=[RandomForestRegressor(),AdaBoostRegressor(),BaggingRegressor(),SVR(),KNeighborsRegressor()]\nmodel_names=['RandomForestRegressor','AdaBoostRegressor','BaggingRegressor','SVR','KNeighborsRegressor']\nrmsle=[]\nd={}\nfor model in range (len(models)):\n    clf=models[model]\n    clf.fit(x_train,y_train)\n    valid_pred=clf.predict(x_valid)\n    rmsle.append(np.sqrt(mean_squared_log_error(valid_pred,y_valid)))\nd={'Modelling Algo':model_names,'RMSLE':rmsle}   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmsle_frame=pd.DataFrame(d)\nrmsle_frame","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now for curiosity, I'll try regressing separately to registered and casual."},{"metadata":{"trusted":true},"cell_type":"code","source":"registered_train = train_df[\"registered\"]\ncasual_train = train_df[\"casual\"]\n\nregistered_valid = valid_df[\"registered\"]\ncasual_valid = valid_df[\"casual\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# registered\nrmsle = []\nd = {}\nreg_counts = []\ncas_counts = []\n\nfor model in range (len(models)):\n    clf = models[model]\n    clf.fit(x_train,registered_train)\n    valid_pred = clf.predict(x_valid)\n    reg_counts.append(valid_pred)\n    rmsle.append(np.sqrt(mean_squared_log_error(valid_pred,registered_valid)))\nd={'Modelling Algo':model_names,'RMSLE':rmsle}   \nprint(pd.DataFrame(d))\n# casual\nrmsle = []\nd = {}\nfor model in range (len(models)):\n    clf = models[model]\n    clf.fit(x_train,casual_train)\n    valid_pred = clf.predict(x_valid)\n    cas_counts.append(valid_pred)\n    rmsle.append(np.sqrt(mean_squared_log_error(valid_pred,casual_valid)))\nd={'Modelling Algo':model_names,'RMSLE':rmsle}   \nprint(pd.DataFrame(d))\ndel d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see total count error now\nrmsle = []\nd = {}\nfor model in range(len(models)):\n    total_pred = [reg_c+cas_c for reg_c, cas_c in zip(reg_counts[model],cas_counts[model])]\n    rmsle.append(np.sqrt(mean_squared_log_error(total_pred,y_valid)))\nd={'Modelling Algo':model_names,'RMSLE':rmsle}   \nprint(\"TOTAL COUNTS SCORE\")\nprint(pd.DataFrame(d))\ndel d","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random forests do seem the best from these regression models. Let's try and find better parameters..."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nno_of_test=[200,300,400,500,600,700]\nparams_dict={'n_estimators':no_of_test,'n_jobs':[-1],'max_features':[\"auto\",'sqrt','log2']}\nclf_rf=GridSearchCV(estimator=RandomForestRegressor(),param_grid=params_dict,scoring='neg_mean_squared_log_error')\nclf_rf.fit(x_train,y_train)\nprint(\"RMSE on training set\")\npred_tr=clf_rf.predict(x_train)\nprint((np.sqrt(mean_squared_log_error(pred_tr,y_train))))\nprint(\"RMSE on validation set\")\npred=clf_rf.predict(x_valid)\nprint((np.sqrt(mean_squared_log_error(pred,y_valid))))\nprint(clf_rf.best_params_)\ndel pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# registered\nno_of_test=[300] # to simplify computations; experiments show it doesn't affect the score too much\nparams_dict={'n_estimators':no_of_test,'n_jobs':[-1],'max_features':[\"auto\",'sqrt','log2']}\nclf_rf_reg=GridSearchCV(estimator=RandomForestRegressor(),param_grid=params_dict,scoring='neg_mean_squared_log_error')\nclf_rf_reg.fit(x_train,registered_train)\nreg_pred=clf_rf_reg.predict(x_valid)\nreg_train_pred = clf_rf_reg.predict(x_train)\nprint(\"REGISTERED USER COUNT ERROR\")\nprint((np.sqrt(mean_squared_log_error(reg_pred,registered_valid))))\nprint(clf_rf_reg.best_params_)\n# casual\nno_of_test=[500]\nparams_dict={'n_estimators':no_of_test,'n_jobs':[-1],'max_features':[\"auto\",'sqrt','log2']}\nclf_rf_cas=GridSearchCV(estimator=RandomForestRegressor(),param_grid=params_dict,scoring='neg_mean_squared_log_error')\nclf_rf_cas.fit(x_train,casual_train)\ncas_pred=clf_rf_cas.predict(x_valid)\ncas_train_pred = clf_rf_cas.predict(x_train)\nprint(\"CASUAL USER COUNT ERROR\")\nprint((np.sqrt(mean_squared_log_error(cas_pred,casual_valid))))\nprint(clf_rf_cas.best_params_)\n# total\ntotal_pred = [cas_pred_single+reg_pred_single for reg_pred_single, cas_pred_single in zip(reg_pred, cas_pred)]\ntotal_train_pred = [cas_pred_single+reg_pred_single for reg_pred_single, cas_pred_single in zip(reg_train_pred, cas_train_pred)]\nprint(\"TOTAL COUNTS ERROR - validation\")\nprint((np.sqrt(mean_squared_log_error(total_pred,y_valid))))\nprint(\"TOTAL COUNTS ERROR - training\")\nprint((np.sqrt(mean_squared_log_error(total_train_pred,y_train))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2nd model: Now let's try MLP"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPRegressor\nclf = MLPRegressor(hidden_layer_sizes=(80, 80, 60, 60, 40), activation='relu', solver='adam', alpha=0.001, batch_size=40, learning_rate='adaptive', learning_rate_init=0.001, power_t=0.5, max_iter=1000, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=20)\nclf.fit(x_train,y_train)\ntrain_pred=clf.predict(x_train)\ntrain_pred[train_pred < 0] = 0 #making sure there are no negative values\nrmsle_tr = np.sqrt(mean_squared_log_error(abs(train_pred),y_train))\nvalid_pred=clf.predict(x_valid)\nvalid_pred[valid_pred < 0] = 0 #making sure there are no negative values\nrmsle = np.sqrt(mean_squared_log_error(abs(valid_pred),y_valid))\nprint(\"RMSLE on validation\")\nprint(rmsle)\nprint(\"RMSLE on training\")\nprint(rmsle_tr)\nprint(\"nr of iterations\")\nprint(clf.n_iter_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not too bad, let's see what changes when we train it separately for registered and casual users."},{"metadata":{"trusted":true},"cell_type":"code","source":"# registered\n# clf_mlp_reg= MLPRegressor(hidden_layer_sizes=(80, 80, 60,40), activation='relu', solver='adam', alpha=0.0001, batch_size=40, learning_rate='adaptive', learning_rate_init=0.001, power_t=0.5, max_iter=1000, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\nclf_mlp_reg = clf\nclf_mlp_reg.fit(x_train,registered_train)\nreg_pred=clf_mlp_reg.predict(x_valid)\nreg_pred[reg_pred < 0] = 0 #making sure there are no negative values\n\nprint(\"REGISTERED USER COUNT ERROR\")\nprint((np.sqrt(mean_squared_log_error(reg_pred,registered_valid))))\n# casual\n# clf_mlp_cas=MLPRegressor(hidden_layer_sizes=(80, 80, 60,40), activation='relu', solver='adam', alpha=0.0001, batch_size=40, learning_rate='adaptive', learning_rate_init=0.001, power_t=0.5, max_iter=1000, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\nclf_mlp_cas = clf\nclf_mlp_cas.fit(x_train,casual_train)\ncas_pred=clf_mlp_cas.predict(x_valid)\ncas_pred[cas_pred < 0] = 0 #making sure there are no negative values\nprint(\"CASUAL USER COUNT ERROR\")\nprint((np.sqrt(mean_squared_log_error(cas_pred,casual_valid))))\n# total\ntotal_pred = [cas_pred_single+reg_pred_single for reg_pred_single, cas_pred_single in zip(reg_pred, cas_pred)]\nprint(\"TOTAL COUNTS ERROR\")\nprint((np.sqrt(mean_squared_log_error(total_pred,y_valid))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's export test predictions from the best model."},{"metadata":{"trusted":true},"cell_type":"code","source":"best_clf = clf_rf\n\npred=best_clf.predict(test_df)\nd={'datetime':test['datetime'],'count':pred}\nans=pd.DataFrame(d)\nans.to_csv('answer_bestsinglemodel.csv',index=False) # saving to a csv file for predictions on kaggle.\n\nbest_cas_clf = clf_rf_cas\nbest_reg_clf = clf_rf_reg\n\npred_cas=best_cas_clf.predict(test_df)\npred_reg=best_reg_clf.predict(test_df)\npred = [pred_reg_s + pred_cas_s for pred_reg_s, pred_cas_s in zip(pred_reg, pred_cas)]\nd={'datetime':test['datetime'],'count':pred}\nans=pd.DataFrame(d)\nans.to_csv('answer_besttwomodels.csv',index=False) # saving to a csv file for predictions on kaggle.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\nRMSLE:\n\nOn test set:\n* single RF model: 0.49458\n* two RF models (for casual and registered): 0.49267\n\n..which takes ca. 1400th position of the leaderboard, of total 3242 participating teams.\n\nOn validation set (ca. 25% of all training data):\n* single RF model: 0.38816\n* two RF models (for casual and registered): 0.38387\n* single MLP: 0.36067\n\nOn training set:\n* single RF model: 0.15186\n* two RF models (for casual and registered): 0.15151\n* single MLP: 0.15044 \n\n\n\n# Conclusions and future work\nHere, unsurprisingly (if you looked at other people's work), Random Forests proved best suited. The striking thing is the difference between scores for casual and registered users. The former's behaviour proved much harder to anticipate, both by the regression random forest model, and by MLP. \n\n**About the difference between train, validation and test score**: differences between sets are significant. Especially training and validation score are very different - which suggests overfitting. In case of RF, changing the number of trees / their depth (also regularization) probably could mitigate this problem. Test score and validation score also are different, but here the difference is smaller. It seems to suggest domain shift, i.e. that validation and test set are too different from each other - despite my efforts to create them analogically. The fault could be that I didn't take into account balancing holidays of test set and validation, and ex. there were almost none in validation and therefore it wasn't a representative portion for the generalization purposes. Still, when I looked through notebooks of other users, I noticed some of them had much bigger differences between scores they claimed to have on validation and their positions on the leaderboard.\n\n### Random Forest\n\nI believe the RF score may be significantly improved by hand-picking input parameters more wisely, i.e. by maximizing correlations between input variables and total bikers' count. As it is now, the only modifications made were: drop of temp, and creation of numerical day, month and year variables. It probably would be beneficial to ex. merge \"season\" and \"weather\" categories, into categories such as \"spring and sunny\", \"spring and rainy\", \"winter and cloudy\" etc. \n\nI performed a GridSearch, but it was not too thorough. Tuning more parameters and in a wider range may improve that score too. Also, RandomSearch instead of GridSearch always seems like a more reasonable idea, so it probably should be attempted as well.\n\nAnother improvement could be achieved by making sure the inputs are well balanced, i.e. the model treats the inputs with similar/proportional importance. Also, removing outliers (ex. outside of range of mean val +- 2x std dev) may have improved the regression!\n\n### Multi-layer Perceptron\n\nIntuition tells me that RF cares about the right inputs (highly correlated to target value and ideally not correlated to each other) more than MLP. I was able to improve MLP significantly by tweaking the architecture (numbers of layers and neurons in hidden layers), batch size, learning rate, number of epochs, early stopping etc.. without touching parameters. The network by definition should eventually be able to adjust to inputs, even to learn that some of them are not helpful after all. Especially if we are using L1 and L2 regularization (which I am here). That is why it stays my favourite method, even if it didn't achieve the best score in this case. Note, it probably would achieve the best score, if I played around with all the hyperparameters. \n\n\nNOTE: Obviously, more graphs could be created, but in the notebooks cited above I found so many, that I found it unnecessary to copy all of them here."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}