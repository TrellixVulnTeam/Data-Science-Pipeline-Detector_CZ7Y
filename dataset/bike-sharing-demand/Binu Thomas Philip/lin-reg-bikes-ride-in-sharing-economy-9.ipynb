{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Recently I published a self help book titled Inspiration: Thoughts on Spirituality, Technology, Wealth, Leadership and Motivation. The preview of the book can be read from the Amazon link https://lnkd.in/gj7bMQA\n\n### You can refer to my other notebooks from https://www.kaggle.com/binuthomasphilip/code","metadata":{}},{"cell_type":"markdown","source":"Today we live in era of Sharing Economy.The biggest company like Amazon,Airbnb,Uber,Swiggy etc dont own much physical assets.But they are using software to optimize the use of the assets available.In this kernel we will be covering following topics.\n\n### 1.Data Import and Preprocessing \n\n### 2.Exploratory Data Analysis \n\n### 3.Checking Multi Linear Regression Assumptions like\n\n      -Normality\n\n      -Linear Correlation\n\n      -Multicollinearity\n\n      -Autocorrelation\n\n      -Sample Size\n\n### 4.Drop Irrevalent features\n\n### 5.Creating and Modifying features\n\n### 6.Create Dummy Variables\n\n### 7.Train Test Split\n\n### 8.Fit and Score Model\n\n### 9.Present the Results","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.Data Import and Preprocessing ","metadata":{}},{"cell_type":"markdown","source":"### Importing Python Modules","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport math","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setting Pandas Display ","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing Dataset","metadata":{}},{"cell_type":"code","source":"\ndf1=pd.read_csv('../input/bike-sharing-demand/train.csv',parse_dates=['datetime'],index_col=0)\ndf_test=pd.read_csv('../input/bike-sharing-demand/test.csv',parse_dates=['datetime'],index_col=0)\ndf1.head(2)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Datetime Conversion","metadata":{}},{"cell_type":"code","source":"\n#df1.info()\ndef add_feature(df1):\n    df1['year']=df1.index.year\n    df1['month']=df1.index.month\n    df1['dayofmonth']=df1.index.day\n    df1['dayofweek']=df1.index.dayofweek\n    df1['hour']=df1.index.hour","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"add_feature(df1)\nadd_feature(df_test)\ndf1.head(2)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Shape of Data ","metadata":{}},{"cell_type":"code","source":"df1.shape","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Making Copy of Dataset","metadata":{}},{"cell_type":"code","source":"df =df1.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Renaming Columns ","metadata":{}},{"cell_type":"code","source":"df = df.rename(columns={'count':'demand'})","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dropping unwanted columns","metadata":{}},{"cell_type":"code","source":"df.reset_index(drop=True, inplace=True)\ndf = df.drop(['casual','registered'],axis=1)\ndf.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have dropped the index, Datetime,Casual and Registered columns from our dataset.The column count is a sum of Casual and Registered,We will be predicting the column count so we habe dropped the columns casual and registered from our dataset.","metadata":{}},{"cell_type":"markdown","source":"### Checking Missing Values","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are no missing values in the dataset.So we can proceed further.","metadata":{}},{"cell_type":"markdown","source":"# 2.Exploratory Data Analysis ","metadata":{}},{"cell_type":"markdown","source":"### Histogram","metadata":{}},{"cell_type":"code","source":"df.hist(figsize = (15,10))\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the demand remains same for all the fours seasons.So we can drop this feature while predicting the demand.The count(demand) is not normally distributed.We need to transform the count(demand) feature.","metadata":{}},{"cell_type":"markdown","source":"### Vizualise the Continous features Vs Demand (Count)","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.subplot(2,2,1,)\nplt.title('Temperature Vs Demand')\nplt.xlabel('Temperature')\nplt.ylabel('Demand')\nplt.scatter(df['temp'],df['demand'],s=2,c='g')\n\nplt.subplot(2,2,2)\nplt.title('Abs Temperature Vs Demand')\nplt.xlabel('Abs Temperature')\nplt.ylabel('Demand')\nplt.scatter(df['atemp'],df['demand'],s=2,c='b')\n\nplt.subplot(2,2,3)\nplt.title('Humidity Vs Demand')\nplt.xlabel('Humidity')\nplt.ylabel('Demand')\nplt.scatter(df['humidity'],df['demand'],c='r')\n\nplt.subplot(2,2,4)\nplt.title('Windspeed Vs Demand')\nplt.xlabel('Windspeed')\nplt.ylabel('Demand')\nplt.scatter(df['windspeed'],df['demand'],c='c')\n\nplt.tight_layout()\n\npass","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plots we can see that temperature and Windspeed Have an affect on Bike demand.\n\nTemperature and Absolute temperature plots are alsmost similar in nature.This indicated high colinearity between the two feature.We can drop one of the features to avoid the colinearity.","metadata":{}},{"cell_type":"markdown","source":"### Visualse the categorical features","metadata":{}},{"cell_type":"code","source":"colors = ['g','r','m','b']\n\nplt.figure(figsize=(15,10))\nplt.subplot(3,3,1,)\nplt.title('Seasons Vs Demand')\nplt.xlabel('Seasons')\nplt.ylabel('Demand')\ncat_list=df['season'].unique()\ncat_average=df.groupby('season').mean()['demand']\nplt.bar(cat_list,cat_average,color=colors)\n\nplt.subplot(3,3,2,)\nplt.title('Holiday Vs Demand')\nplt.xlabel('Holiday')\nplt.ylabel('Demand')\ncat_list=df['holiday'].unique()\ncat_average=df.groupby('holiday').mean()['demand']\nplt.bar(cat_list,cat_average,color=colors)\n\nplt.subplot(3,3,3,)\nplt.title('Working Day Vs Demand')\nplt.xlabel('Working Day')\nplt.ylabel('Demand')\ncat_list=df['workingday'].unique()\ncat_average=df.groupby('workingday').mean()['demand']\nplt.bar(cat_list,cat_average,color=colors)\n\nplt.subplot(3,3,4,)\nplt.title('Weather Vs Demand')\nplt.xlabel('Weather')\nplt.ylabel('Demand')\ncat_list=df['weather'].unique()\ncat_average=df.groupby('weather').mean()['demand']\nplt.bar(cat_list,cat_average,color=colors)\n\nplt.subplot(3,3,5,)\nplt.title('Year Vs Demand')\nplt.xlabel('Year')\nplt.ylabel('Demand')\ncat_list=df['year'].unique()\ncat_average=df.groupby('year').mean()['demand']\nplt.bar(cat_list,cat_average,color=colors)\n\nplt.subplot(3,3,6,)\nplt.title('Month Vs Demand')\nplt.xlabel('Month')\nplt.ylabel('Demand')\ncat_list=df['month'].unique()\ncat_average=df.groupby('month').mean()['demand']\nplt.bar(cat_list,cat_average,color=colors)\n\nplt.subplot(3,3,7,)\nplt.title('Day Of Month Vs Demand')\nplt.xlabel('Day Of Month')\nplt.ylabel('Demand')\ncat_list=df['dayofmonth'].unique()\ncat_average=df.groupby('dayofmonth').mean()['demand']\nplt.bar(cat_list,cat_average,color=colors)\n\nplt.subplot(3,3,8,)\nplt.title('Day Of Week Vs Demand')\nplt.xlabel('Day Of Week')\nplt.ylabel('Demand')\ncat_list=df['dayofweek'].unique()\ncat_average=df.groupby('dayofweek').mean()['demand']\nplt.bar(cat_list,cat_average,color=colors)\n\nplt.subplot(3,3,9,)\nplt.title('Hour Vs Demand')\nplt.xlabel('Hour')\nplt.ylabel('Demand')\ncat_list=df['hour'].unique()\ncat_average=df.groupby('hour').mean()['demand']\nplt.bar(cat_list,cat_average,color=colors)\n\n\nplt.tight_layout()\npass","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the features working day,holiday,day of month,day of week, dont have any affect on the bike demand.So we can drop these features from our prediction model.Also we have data for only two years.So the column of year can also be dropped from our dataset.From the Hour plot we can clearly make out that the demand for bikes is highest at 8 am and 5 pm.This is due to more demand during office going hours.","metadata":{}},{"cell_type":"markdown","source":"# 3.Checking Multi Linear Regression Assumptions","metadata":{}},{"cell_type":"markdown","source":"### Creating a Copy of Dataframe","metadata":{}},{"cell_type":"code","source":"df_prep = df.copy()\n#df_prep","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking Outliers","metadata":{}},{"cell_type":"code","source":"df_prep['demand'].describe()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above stats we can see that 50% of the values lie between demand value 42 to 284.Now we will analyse the outliers in the dataset.","metadata":{}},{"cell_type":"markdown","source":"### Outliers Based On Percentile","metadata":{}},{"cell_type":"code","source":"df_prep['demand'].quantile([0.05,0.1,0.15,0.9,0.99])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the data we can say 5% of the time the demand was less than 5 and 1 % of the time the demand was more than 774.","metadata":{}},{"cell_type":"markdown","source":"### Checking Multicollinearity","metadata":{}},{"cell_type":"code","source":"correlation = df_prep[['temp','atemp','humidity','demand']].corr()\ncorrelation","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there is a high correlation between temp and atemp.There is a neegative correlation between the demand and humidity.","metadata":{}},{"cell_type":"markdown","source":"# 4.Dropping Unwanted Features","metadata":{}},{"cell_type":"code","source":"df_prep = df_prep.drop(['atemp','holiday','workingday','year','dayofmonth','dayofweek'],axis=1)\ndf_prep.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we have dropped features like atemp,holiday,workingday,year,dayofmonth and dayofweek because they have no impact on the outcome of our prediction.","metadata":{}},{"cell_type":"markdown","source":"# 5.Creating and Modifying Features","metadata":{}},{"cell_type":"markdown","source":"### Checking Autocorrelation","metadata":{}},{"cell_type":"code","source":"df2 = pd.to_numeric(df_prep['demand'],downcast='float')\nplt.acorr(df2,maxlags=12)\npass","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plot we can see that there is autocorrelation in the demand. Auto correlation means the value of demand at time t is dependent on the value at time t-1 or t-2 ... The idependent variable demand has auto correlation and it has a log normal distribution.We need to fix this to improve our results.","metadata":{}},{"cell_type":"markdown","source":"### Transforming Demand to Normal Distribution","metadata":{}},{"cell_type":"code","source":"\nd1 = df_prep['demand']\nd2 = np.log(d1)\n\nplt.figure(figsize=(15,10))\nplt.subplot(1,2,1,)\nplt.title('Actual Demand Plot')\nplt.xlabel('Frequncy')\nplt.ylabel('Demand')\nd1.hist(rwidth=0.9)\n\nplt.subplot(1,2,2,)\nplt.title('Log Transform Demand Plot')\nplt.xlabel('Frequncy')\nplt.ylabel('Demand')\nd2.hist(rwidth=0.9)\n\npass","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plot we can see that by doing a log transform on the demand we can convert it to a normal distribution curve.","metadata":{}},{"cell_type":"markdown","source":"### Demand into Log Values","metadata":{}},{"cell_type":"code","source":"df_prep['demand'] = np.log(df_prep['demand'])\ndf_prep.head(2)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Taking Care of Auto Correlation","metadata":{}},{"cell_type":"code","source":"t_1 = df_prep['demand'].shift(+1).to_frame()\nt_1.columns = ['t-1']\n\nt_2 = df_prep['demand'].shift(+2).to_frame()\nt_2.columns = ['t-2']\n\nt_3 = df_prep['demand'].shift(+3).to_frame()\nt_3.columns = ['t-3']","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Concanating t_1,t-2,t-3 with Demand Dataframe","metadata":{}},{"cell_type":"code","source":"df_prep_lag = pd.concat([df_prep,t_1,t_2,t_3],axis=1)\ndf_prep_lag = df_prep_lag.dropna()\ndf_prep_lag.head(2)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above table one can observe that we have created addition column of features t-1,t-2,t-3 to take care of autocorrelation of the demand feature This additional created features will be used to predict the demand.","metadata":{}},{"cell_type":"markdown","source":"### You can refer to my other notebooks from https://www.kaggle.com/binuthomasphilip/code","metadata":{}},{"cell_type":"markdown","source":"# 6.Creating Dummy Variables","metadata":{}},{"cell_type":"code","source":"df_prep_lag.dtypes","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For making the dummies command to work we need the categorical data in object or category type.Let covert the type of data.","metadata":{}},{"cell_type":"code","source":"df_prep_lag['season'] = df_prep_lag['season'].astype('category')\ndf_prep_lag['weather'] = df_prep_lag['weather'].astype('category')\ndf_prep_lag['month'] = df_prep_lag['month'].astype('category')\ndf_prep_lag['hour'] = df_prep_lag['hour'].astype('category')\ndummy_df = pd.get_dummies(df_prep_lag,drop_first=True)\ndummy_df.head(2)\n#dummy_df.shape","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we have created dummy variables and avoided the dummy variable trap by removing the unwanted column of categorical data.","metadata":{}},{"cell_type":"markdown","source":"# 7.Train Test Split","metadata":{}},{"cell_type":"code","source":"y = df_prep_lag[['demand']]\nX = df_prep_lag.drop(['demand'],axis=1)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the training set at 70%\ntr_size = 0.7*len(X)\ntr_size = int(tr_size)\n\nX_train = X.values[0:tr_size]\nX_test = X.values[tr_size:len(X)]\n\ny_train = y.values[0:tr_size]\ny_test = y.values[tr_size:len(y)]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our demand data is a time series data.So the sequence of the data for features are important to predict demand.So while splitting the data we have maintained the sequence of the data.","metadata":{}},{"cell_type":"markdown","source":"# 8.Fit and Model Evaluation","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression \n\nstd_reg = LinearRegression()\nstd_reg.fit(X_train,y_train)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### R Squared Error ","metadata":{}},{"cell_type":"code","source":"r2_train = std_reg.score(X_train,y_train)\nr2_test = std_reg.score(X_test,y_test)\nprint('R Suared Error for Train set:',r2_train)\nprint('R Suared Error for Test set:',r2_test)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The R square value for the test set is higher.Which tells us that there is no overfitting in the model.","metadata":{}},{"cell_type":"code","source":"# Create Predictions \ny_predict = std_reg.predict(X_test)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RMSE","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nrmse = math.sqrt(mean_squared_error(y_test,y_predict))\nprint('RMSE of the model:',rmse)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Low value of RMSE tells us that our model is quite good.","metadata":{}},{"cell_type":"markdown","source":"### RMSLE ","metadata":{}},{"cell_type":"code","source":"y_test_e = []\ny_predict_e = []\n\n\nfor i in range(0,len(y_test)):\n    y_test_e.append(math.exp(y_test[i]))\n    y_predict_e.append(math.exp(y_predict[i]))\n    \n# Calculate the sum\nlog_sq_sum = 0.0\n\nfor i in range(0,len(y_test_e)):\n    log_a = math.log(y_test_e[i] +1)\n    log_p = math.log(y_predict_e[i] +1)\n    log_diff = (log_p - log_a)**2\n    log_sq_sum = log_sq_sum + log_diff\n    \nrmsle = math.sqrt(log_sq_sum/len(y_test))\nprint(rmsle)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9.Conclusion:\n\n1.We have worked with the time series data of bike rides.We have done exploratory data analysis and found out the affect continous and categorical data on Bike Demand.\n\n2.From the above plots we can see that temperature and Windspeed have an affect on Bike demand.Temperature and Absolute temperature plots are alsmost similar in nature.This indicated high colinearity between the two feature.We dropped one feature to avoid multi collinearity.\n\n3.We can see that the features working day,holiday,day of month,day of week, dont have any affect on the bike demand.So we can drop these features from our prediction model.Also we have data for only two years.So the column of year can also be dropped from our dataset.From the Hour plot we can clearly make out that the demand for bikes is highest at 8 am and 5 pm.This is due to more demand during office going hours.\n\n4.We have done a log transformation on the predicted variable Demand to make it into normal distribution.This helps in improving the accuracy of the model.Demand is autolinear and we have used values of t-1, t-2  and t-3 demand values to over come auto colinearity problem.\n\n5.We used a Linear regression model to predict the bike demand.We can see that we got good accuracy from the R square,RMSE and RMSLE values.","metadata":{}},{"cell_type":"markdown","source":"### You can refer to my other notebooks from https://www.kaggle.com/binuthomasphilip/code","metadata":{}}]}