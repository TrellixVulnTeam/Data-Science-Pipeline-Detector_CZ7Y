{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Welcome to my Tutorial Notebook\n* I am supper excited to share with you guys the results of new insights for the data preparation and building two models such as:\n*  Regression using SKLearn's Neural Network (NN)\n* Train unisng Keras API with Tensorflow as Backend\n\n# Kaggle Bike Sharing Demand Dataset\nModified 'count' to log1p(count) for training\n\nLog can be used when target represents a count (that is non-negative values)\n\nModel now predicts as log1p(count). We need to convert it back to actual count using expm1(predicted_target)\n\n\n\nInput Features: ['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed', 'year', 'month', 'day', 'dayofweek','hour']\nTarget Feature: [log1p('count')]\n\nObjective: We are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period (Ref: Kaggle.com)"},{"metadata":{},"cell_type":"markdown","source":"# In this Notebook, we will go through some steps and different insights such as:\n# Contains\n# Regression using SKLearn's Neural Network (NN)\n* Data Underestanding\n* Data Visualization\n* Data Preparation:\n 1. One Hot Encode all the Categorical Features\n 2. Standardize or Normalize all the Numeric Features\n* Train using SKLearn's MLPRegressor (Multi-Layer Perceptron)/Regression using SKLearn's Neural Network (NN)\n* Prediction\n* Evaluating the Results\n* Submission\n# New Insight for Modeling\n# Train unisng Keras API with Tensorflow as Backend\n* Data Underestanding\n* Data Visualization\n* Data Preparation:\n 1. One Hot Encode all the Categorical Features\n 2. Standardize or Normalize all the Numeric Features\n* Train unisng Keras API with Tensorflow as Backend\n* Prediction\n* Evaluating the Results\n* Submission\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Let's import the Necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Example\n# Converts to log1p(count)\n# Print original count back using expm1\nprint('Test log and exp')\ntest_count = 100\nprint('original value', test_count)\nx = np.log1p(test_count) # log (x+1)\nprint('log1p', x)\nprint('expm1', np.expm1(x)) # exp(x) - 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['count', 'season', 'holiday', 'workingday', 'weather', 'temp',\n       'atemp', 'humidity', 'windspeed', 'year', 'month', 'day', 'dayofweek','hour']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading the datasets\ndf = pd.read_csv('../input/bike-sharing-demand/train.csv', parse_dates=['datetime'],index_col=0)\ndf_test = pd.read_csv('../input/bike-sharing-demand/test.csv', parse_dates=['datetime'],index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to convert datetime to numeric for training.\n# Let's extract key features into separate numeric columns\ndef add_features(df):\n    df['year'] = df.index.year\n    df['month'] = df.index.month\n    df['day'] = df.index.day\n    df['dayofweek'] = df.index.dayofweek\n    df['hour'] = df.index.hour","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"add_features(df)\nadd_features(df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(df['2011']['count'],label='2011')\nplt.plot(df['2012']['count'],label='2012')\nplt.xticks(fontsize=14, rotation=45)\nplt.xlabel('Date')\nplt.ylabel('Rental Count')\nplt.title('2011 and 2012 Rentals (Year to Year)')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(df['2011']['count'].map(np.log1p),label='2011')\nplt.plot(df['2012']['count'].map(np.log1p),label='2012')\nplt.xticks(fontsize=14, rotation=45)\nplt.xlabel('Date')\nplt.ylabel('Log(Rental Count)')\nplt.title('2011 and 2012 Rentals (Year to Year)')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.boxplot([df['count']], labels=['count'])\nplt.title('Box Plot - Count')\nplt.ylabel('Target')\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see how the data distribution changes with log1p\n# Evenly distributed\nplt.boxplot([df['count'].map(np.log1p)], labels=['log1p(count)'])\nplt.title('Box Plot - log1p(Count)')\nplt.ylabel('Target')\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"count\"] = df[\"count\"].map(np.log1p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save all data\ndf.to_csv('bike_all.csv',index=True,index_label='datetime',columns=columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and Validation Set\n* Target Variable as first column followed by input features\n* Training, Validation files do not have a column header"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training = 70% of the data\n# Validation = 30% of the data\n# Randomize the datset\nnp.random.seed(5)\nl = list(df.index)\nnp.random.shuffle(l)\ndf = df.loc[l]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = df.shape[0]\ntrain = int(.7 * rows)\ntest = rows-train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows, train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Write Training Set\ndf.iloc[:train].to_csv('bike_train.csv'\n                          ,index=False,header=False\n                          ,columns=columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Write Validation Set\ndf.iloc[train:].to_csv('bike_validation.csv'\n                          ,index=False,header=False\n                          ,columns=columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test Data has only input features\ndf_test.to_csv('bike_test.csv',index=True,index_label='datetime')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(','.join(columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Write Column List\nwith open('bike_train_column_list.txt','w') as f:\n    f.write(','.join(columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Regression using SKLearn's Neural Network (NN)\n* One-Hot Encode categorical features, Standardize numeric features\n* Objective:\n\n* Train a bike rental prediction model\n* NN requires one hot encoding of categorical data\n* NN also requires features to be on similar scale\n* Perform one-hot encoding of all categorical features: ['season', 'holiday', 'workingday', 'weather', 'year', 'month', 'day', 'dayofweek', 'hour']\n* Verify model performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nimport numpy as np\n# Set random seed\nnp.random.seed(0)\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# NN\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, KBinsDiscretizer\n# Column Transformer\nfrom sklearn.compose import ColumnTransformer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_list_file = 'bike_train_column_list.txt'\ntrain_file = 'bike_train.csv'\nvalidation_file = 'bike_validation.csv'\ntest_file = 'bike_test.csv'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# One Hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# One Hot Encode all Categorical Features\n# Let's define all the categorical features\ncategorical_features = ['season','holiday','workingday','weather','year','month','dayofweek','hour']\n\n# Standardize Features\nstandardize_features = ['temp', 'atemp', 'humidity', 'windspeed']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ''\nwith open(column_list_file,'r') as f:\n    columns = f.read().split(',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Specify the column names as the file does not have column header\ndf_train = pd.read_csv(train_file,names=columns)\ndf_validation = pd.read_csv(validation_file,names=columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_validation.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_train.iloc[:,1:] # Features: 1st column onwards \ny_train = df_train.iloc[:,0].ravel() # Target: 0th column\n\nX_validation = df_validation.iloc[:,1:]\ny_validation = df_validation.iloc[:,0].ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features to one-hot encode\ncategorical_features+['day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features to standardize\nstandardize_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Column Transformer/New Idea \n* Chain all data transformations\n* Easy and straight forward"},{"metadata":{"trusted":true},"cell_type":"code","source":"colTransformer = ColumnTransformer([('onehot',\n                                     OneHotEncoder(categories='auto',sparse=False),\n                                     categorical_features),\n                                    ('onehotday',\n                                     OneHotEncoder(categories=[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]],\n                                                   sparse=False),\n                                     ['day']),\n                                    ('standardize',\n                                    StandardScaler(),standardize_features)\n                                   ],\n                                   remainder=\"passthrough\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colTransformer.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_encoded = colTransformer.transform(X_train)\nX_validation_encoded = colTransformer.transform(X_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Data',X_train.shape, 'OneHot Encoded',X_train_encoded.shape)\nprint('Val Data',X_validation.shape, 'OneHot Encoded',X_validation_encoded.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_encoded[:1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_validation_encoded[:1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train a neural network regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_regressor = MLPRegressor(random_state=5, \n                            hidden_layer_sizes=[100],\n                            activation='relu',\n                            max_iter=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_regressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nnn_regressor.fit(X_train_encoded,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare actual vs predicted performance with dataset not seen by the model before\ndf = pd.read_csv(validation_file,names=columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = nn_regressor.predict(X_validation_encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['count_predicted'] = result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['count_predicted'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert log(count) to count\ndf['count'] = df['count'].map(np.expm1)\ndf['count_predicted'] = df['count_predicted'].map(np.expm1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Actual Vs Predicted\nplt.plot(df['count'], label='Actual')\nplt.plot(df['count_predicted'],label='Predicted')\nplt.xlabel('Sample')\nplt.ylabel('Count')\nplt.xlim([100,150])\nplt.title('Validation Dataset - Predicted Vs. Actual')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Over prediction and Under Prediction needs to be balanced\n# Training Data Residuals\nresiduals = (df['count'] - df['count_predicted'])\n\nplt.hist(residuals)\nplt.grid(True)\nplt.xlabel('Actual - Predicted')\nplt.ylabel('Count')\nplt.title('Residuals Distribution')\nplt.axvline(color='r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"value_counts = (residuals > 0).value_counts(sort=False)\nprint(' Under Estimation: {0:.2f}'.format(value_counts[True]/len(residuals)))\nprint(' Over  Estimation: {0:.2f}'.format(value_counts[False]/len(residuals)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.metrics as metrics\nprint(\"RMSE: {0:.2f}\".format(metrics.mean_squared_error(df['count'],\n                                                    df['count_predicted'])**.5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Metric Use By Kaggle\ndef compute_rmsle(y_true, y_pred):\n    if type(y_true) != np.ndarray:\n        y_true = np.array(y_true)\n        \n    if type(y_pred) != np.ndarray:\n        y_pred = np.array(y_pred)\n     \n    return(np.average((np.log1p(y_pred) - np.log1p(y_true))**2)**.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"RMSLE: {0:.2f}\".format(compute_rmsle(df['count'],df['count_predicted'])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optional Test Data\n# Prepare Data for Submission to Kaggle\ndf_test = pd.read_csv(test_file,parse_dates=['datetime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test =  df_test.iloc[:,1:] # Exclude datetime for prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform data first with column transformer\nresult = nn_regressor.predict(colTransformer.transform(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert result to actual count\ndf_test[\"count\"] = np.expm1(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[df_test[\"count\"] < 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[['datetime','count']].to_csv('My_New_Insight_Predicted_Count.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NEW Insight\n# Regression using TensorFlow\nBuild the Neural Network using Keras - Easy and Portable across different implementations\nhttps://keras.io/\n\n\n\n\n# Objective:\n\n* Train a bike rental prediction model\n* NN requires one hot encoding of categorical data\n* NN also requires features to be on similar scale\n* Perform one-hot encoding of all categorical features: ['season', 'holiday', 'workingday', 'weather', 'year', 'month', 'day', 'dayofweek', 'hour']\n* Verify model performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://keras.io/\n# https://github.com/keras-team/keras/issues/2743\nimport sys\nimport numpy as np\n# Set random seed\nnp.random.seed(0)\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# Column Transformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, KBinsDiscretizer\n\n# Keras Library\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_list_file = 'bike_train_column_list.txt'\ntrain_file = 'bike_train.csv'\nvalidation_file = 'bike_validation.csv'\ntest_file = 'bike_test.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One Hot Encode all Categorical Features\n# Let's define all the categorical features\ncategorical_features = ['season','holiday','workingday','weather','year','month','dayofweek','hour']\n\n# Separated day\ncategorical_day = ['day']\n\n# Standardize Features\nstandardize_features = ['temp', 'atemp', 'humidity', 'windspeed']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ''\nwith open(column_list_file,'r') as f:\n    columns = f.read().split(',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Specify the column names as the file does not have column header\ndf_train = pd.read_csv(train_file,names=columns)\ndf_validation = pd.read_csv(validation_file,names=columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_validation.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_train.iloc[:,1:] # Features: 1st column onwards \ny_train = df_train.iloc[:,0].ravel() # Target: 0th column\n\nX_validation = df_validation.iloc[:,1:]\ny_validation = df_validation.iloc[:,0].ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colTransformer = ColumnTransformer([('onehot',\n                                     OneHotEncoder(categories='auto',sparse=False),\n                                     categorical_features),\n                                    ('onehotday',\n                                     OneHotEncoder(categories=[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]],\n                                                   sparse=False),\n                                     ['day']),\n                                    ('standardize',\n                                    StandardScaler(),standardize_features)\n                                   ],\n                                   remainder=\"passthrough\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colTransformer.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_encoded = colTransformer.transform(X_train)\nX_validation_encoded = colTransformer.transform(X_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Data',X_train.shape, 'OneHot Encoded',X_train_encoded.shape)\nprint('Val Data',X_validation.shape, 'OneHot Encoded',X_validation_encoded.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_encoded[:1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_validation_encoded[:1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Model using Keras\nReference: https://keras.io/getting-started/sequential-model-guide/"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dimension of input data\n# We need to specify number of features when configuring the first hidden layer\nX_train_encoded.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n# 1 hidden layer with 100 neurons with relu activation\n# output layer - regression, so no activation\nmodel.add(Dense(100, input_dim=X_train_encoded.shape[1],activation='relu'))\nmodel.add(Dense(1,activation=None))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Need to compile the model, specify the optimizer and loss function to use\n# For a mean squared error regression problem\nmodel.compile(optimizer='adam',\n              loss='mse')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* One creative idea to avoid from overfitting is using the Early Sopping Method"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can optionally configure early stopping to prevent overfitting - stop when validation loss does not improve\nearly_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train_encoded, y_train, epochs=20, batch_size=32, \n          validation_data=(X_validation_encoded,y_validation),callbacks=[early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x=history.epoch,y=history.history['loss'],label='Training Error')\nplt.scatter(x=history.epoch,y=history.history['val_loss'],label='Validation Error')\nplt.grid(True)\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Training Vs Validation Error')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare actual vs predicted performance with dataset not seen by the model before\ndf = pd.read_csv(validation_file,names=columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = model.predict(X_validation_encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['count_predicted'] = result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['count_predicted'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['count'] = df['count'].map(np.expm1)\ndf['count_predicted'] = df['count_predicted'].map(np.expm1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Actual Vs Predicted\nplt.plot(df['count'], label='Actual')\nplt.plot(df['count_predicted'],label='Predicted')\nplt.xlabel('Sample')\nplt.ylabel('Count')\nplt.xlim([100,150])\nplt.title('Validation Dataset - Predicted Vs. Actual')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Over prediction and Under Prediction needs to be balanced\n# Training Data Residuals\nresiduals = (df['count'] - df['count_predicted'])\n\nplt.hist(residuals)\nplt.grid(True)\nplt.xlabel('Actual - Predicted')\nplt.ylabel('Count')\nplt.title('Residuals Distribution')\nplt.axvline(color='r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"value_counts = (residuals > 0).value_counts(sort=False)\nprint(' Under Estimation: {0:.2f}'.format(value_counts[True]/len(residuals)))\nprint(' Over  Estimation: {0:.2f}'.format(value_counts[False]/len(residuals)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.metrics as metrics\nprint(\"RMSE: {0:.2f}\".format(metrics.mean_squared_error(df['count'],\n                                                    df['count_predicted'])**.5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Metric Use By Kaggle\ndef compute_rmsle(y_true, y_pred):\n    if type(y_true) != np.ndarray:\n        y_true = np.array(y_true)\n        \n    if type(y_pred) != np.ndarray:\n        y_pred = np.array(y_pred)\n     \n    return(np.average((np.log1p(y_pred) - np.log1p(y_true))**2)**.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"RMSLE: {0:.2f}\".format(compute_rmsle(df['count'],df['count_predicted'])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optional Test Data\n# Prepare Data for Submission to Kaggle\ndf_test = pd.read_csv(test_file,parse_dates=['datetime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test =  df_test.iloc[:,1:] # Exclude datetime for prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform data first with column transformer\nresult = model.predict(colTransformer.transform(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert result to actual count\ndf_test[\"count\"] = np.expm1(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def adjust_count(x):\n    if x < 0:\n        return 0\n    else:\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[df_test[\"count\"] < 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['count'] = df_test['count'].map(adjust_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[df_test[\"count\"] < 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[['datetime','count']].to_csv('Mew_Insight_Keras_Predicted_Count.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}