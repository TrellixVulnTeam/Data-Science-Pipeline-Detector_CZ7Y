{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n#Import libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nfrom matplotlib import style\nimport math\nimport seaborn as sns \nimport missingno as msno\nfrom datetime import datetime\nimport statsmodels.formula.api as sm\n\n#import the necessary modelling algos.\n\n#classifiaction.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n#regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV,LassoCV\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\n\n#model selection\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n#evaluation metrics\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification\n\n\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read Training and Test Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data=pd.read_csv(r'../input/bike-sharing-demand/train.csv')\ntest_data=pd.read_csv(r'../input/bike-sharing-demand/test.csv')\ndf=train_data.copy()\ntest_df=test_data.copy()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Identify Unique Columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **A SHORT DESCRIPTION OF THE FEATURES**\n\ndatetime - hourly date + timestamp\n\nseason - 1 = spring, 2 = summer, 3 = fall, 4 = winter\n\nholiday - whether the day is considered a holiday\n\nworkingday - whether the day is neither a weekend nor holiday\n\nweather -\n\n1: Clear, Few clouds, Partly cloudy, Partly cloudy\n\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n\ntemp - temperature in Celsius\n\natemp - \"feels like\" temperature in Celsius\n\nhumidity - relative humidity\n\nwindspeed - wind speed\n\ncasual - number of non-registered user rentals initiated\n\nregistered - number of registered user rentals initiated\n\ncount - number of total rentals"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*ALL THE VARIABLES OR FEATURES ARE NUMERIC AND THE TARGET VARIABLE THAT WE HAVE TO PREDICT IS THE count VARIABLE. HENCE THIS IS A TYPICAL EXAMPLE OF A **REGRESSION PROBLEM** AS THE count VARIABLE IS CONTINUOUS VARIED.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **A. Data Preprocessing**\ncheck Null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().values.any()\n\ndf.isnull().sum()  # implies no null values and hence no imputation needed ::).","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"no missing value."},{"metadata":{},"cell_type":"markdown","source":"# **# Data Exploration**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us consider season.\ndf.season.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='season', y='count', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot(x='season',data=df,kind='count',size=5,aspect=1.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#holiday\ndf.holiday.value_counts()\nsns.barplot(x='holiday', y='count', data=df)\nsns.factorplot(x='holiday',data=df,kind='count',size=5,aspect=1) # majority of data is for non holiday days.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#workingday\ndf.workingday.value_counts()\nsns.barplot(x='workingday', y='count', data=df)\nsns.factorplot(x='workingday',data=df,kind='count',size=5,aspect=1) # majority of data is for working days.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#weather\ndf.workingday.value_counts()\nsns.barplot(x='weather', y='count', data=df)\nsns.factorplot(x='weather',data=df,kind='count',size=5,aspect=1) # majority of data is for weather.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data=df[['temp',\n       'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count']])\nfig=plt.gcf()\nfig.set_size_inches(10,10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfWithoutOutliers = df[np.abs(df[\"count\"]-df[\"count\"].mean())<=(3*df[\"count\"].std())] \ndisplay(\"Shape Of the dataframe before Ouliers: \",df.shape)\ndisplay(\"Shape Of the dataframe after Ouliers: \",dfWithoutOutliers.shape)\ndf =dfWithoutOutliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# can also be visulaized using histograms for all the continuous variables.\ndf.temp.unique()\nfig,axes=plt.subplots(2,2)\naxes[0,0].hist(x=\"temp\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\naxes[0,0].set_title(\"Variation of temp\")\naxes[0,1].hist(x=\"atemp\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\naxes[0,1].set_title(\"Variation of atemp\")\naxes[1,0].hist(x=\"windspeed\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\naxes[1,0].set_title(\"Variation of windspeed\")\naxes[1,1].hist(x=\"humidity\",data=df,edgecolor=\"black\",linewidth=2,color='#ff4125')\naxes[1,1].set_title(\"Variation of humidity\")\nfig.set_size_inches(10,10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **B. Data Wrangling **"},{"metadata":{"trusted":true},"cell_type":"code","source":"#corelation matrix.\ncor_mat= df[:].corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"INFERENCES FROM THE ABOVE HEATMAP--\nself realtion i.e. of a feature to itself is equal to 1 as expected.\n\ntemp and atemp are highly related as expected.\n\nhumidity is inversely related to count as expected as the weather is humid people will not like to travel on a bike.\n\nalso note that casual and working day are highly inversely related as you would expect.\n\nAlso note that count and holiday are highly inversely related as you would expect.\n\nAlso note that temp(or atemp) highly effects the count.\n\nAlso note that weather and count are highly inversely related. This is bcoz for uour data as weather increases from (1 to 4) implies that weather is getting more worse and so lesser people will rent bikes.\n\nregistered/casual and count are highly related which indicates that most of the bikes that are rented are registered.\n\nsimilarly we can draw some more inferences like weather and humidity and so on... ."},{"metadata":{},"cell_type":"markdown","source":"FEATURE ENGINEERING AND GET SOME NEW FEATURES AND DROP SOME USELESS OR LESS RELEVANT FEATURES"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # seperating season as per values. this is bcoz this will enhance features.\nseason=pd.get_dummies(df['season'],prefix='season')\ndf=pd.concat([df,season],axis=1)\ndf.head()\nseason=pd.get_dummies(test_df['season'],prefix='season')\ntest_df=pd.concat([test_df,season],axis=1)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # # same for weather. this is bcoz this will enhance features.\nweather=pd.get_dummies(df['weather'],prefix='weather')\ndf=pd.concat([df,weather],axis=1)\ndf.head()\nweather=pd.get_dummies(test_df['weather'],prefix='weather')\ntest_df=pd.concat([test_df,weather],axis=1)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # # now can drop weather and season.\ndf.drop(['season','weather'],inplace=True,axis=1)\ndf.head()\ntest_df.drop(['season','weather'],inplace=True,axis=1)\ntest_df.head()\n\n\n# # # also I dont prefer both registered and casual but for ow just let them both.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"hour\"] = [t.hour for t in pd.DatetimeIndex(df.datetime)]\ndf[\"day\"] = [t.dayofweek for t in pd.DatetimeIndex(df.datetime)]\ndf[\"month\"] = [t.month for t in pd.DatetimeIndex(df.datetime)]\ndf['year'] = [t.year for t in pd.DatetimeIndex(df.datetime)]\ndf['year'] = df['year'].map({2011:0, 2012:1})\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[\"hour\"] = [t.hour for t in pd.DatetimeIndex(test_df.datetime)]\ntest_df[\"day\"] = [t.dayofweek for t in pd.DatetimeIndex(test_df.datetime)]\ntest_df[\"month\"] = [t.month for t in pd.DatetimeIndex(test_df.datetime)]\ntest_df['year'] = [t.year for t in pd.DatetimeIndex(test_df.datetime)]\ntest_df['year'] = test_df['year'].map({2011:0, 2012:1})\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now can drop datetime column.\ndf.drop('datetime',axis=1,inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find Correlation for NEW FEATURES"},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_mat= df[:].corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['casual','registered'],axis=1,inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*COUNT VARIATION WITH DIFFERENT FEATURES*"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot(x=\"hour\",y=\"count\",data=df,kind='bar',size=5,aspect=1.5)\n# note that time of day affects wheteher people take bike or not. like night time lesser bikes used and using for office commute\n\nsns.factorplot(x=\"day\",y='count',kind='bar',data=df,size=5,aspect=1)\n# note that day has lesser affects wheteher people take bike or not. \n\nsns.factorplot(x=\"month\",y=\"count\",data=df,kind='bar',size=5,aspect=1.5)\n# note that month affects season and that effects wheteher people take bike or not. like climate conditions rainy,hazy etc... .\n\nsns.factorplot(x=\"year\",y=\"count\",data=df,kind='bar',size=5,aspect=1.5)\n# 0 for 2011 and 1 for 2012. Hence demand has increased over the years.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for temp using scatter plot as random values \nplt.scatter(x=\"temp\",y=\"count\",data=df,color='green')\n\n# note that this way this is hard to visualze. \n# a better way is to convert the 'temp' variable into intervals or so called bins and then treat it like a discrete variable\n\ndf_temp=df.copy()\ndf_temp.temp.describe()\ndf_temp['temp_bin']=np.floor(df_temp['temp'])//5\ndf_temp['temp_bin'].unique()\n# now we can visualize as follows\nsns.factorplot(x=\"temp_bin\",y=\"count\",data=df_temp,kind='bar')\n#now the demand is highest for bins 6 and 7 which is about tempearure 30-35(bin 6) and 35-40 (bin 7).","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **DATA MODELLING**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns.to_series().groupby(df.dtypes).groups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying Machine Learning Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = df.iloc[:, :], df['count']\nX = X.drop('count',axis=1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalize the train set\n#def norm_func(i):\n    #x = (i-i.min())\t/ (i.max()-i.min())\n    #return (x)\nfrom sklearn.preprocessing import StandardScaler\nscl= StandardScaler()\n#X = scl.fit_transform(X)\n#y = scl.fit_transform(y)\n\nfrom sklearn.model_selection import  train_test_split\n#x_train,x_test,y_train,y_test=train_test_split(df.drop('count',axis=1),df['count'],test_size=0.25,random_state=42)\n#x_train,x_test,y_train,y_test=train_test_split(df.drop('count',axis=1),df['count'],test_size=0.2,random_state=0)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nX_train = scl.fit_transform(X_train)\nX_test = scl.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **C. Ridge Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"alphas = 10**np.linspace(10,-2,100)*0.5\nalphas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefs = []\n#from sklearn.linear_model import Ridge\nfor a in alphas:\n    model = Ridge(alpha=a, normalize=True)\n    model.fit(X_train,y_train)\n    coefs.append(model.coef_)\n    score = model.score(X_train,y_train)\n    pred2 = model.predict(X_test)\n    mse = mean_squared_error(y_test, pred2) \n    print(\"Alpha:{0:.6f}, R2:{1:.3f}, MSE:{2:.2f}, RMSE:{3:.2f}\".format(a, score, mse, np.sqrt(mse)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(coefs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.gca()\nax.plot(alphas, coefs)\nax.set_xscale('log')\nplt.axis('tight')\nplt.xlabel('alpha')\nplt.ylabel('weights')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Alpha:0.015269, R2:0.396, MSE:19938.35, RMSE:141.20\nridge_mod=Ridge(alpha=0.015269, normalize=True).fit(X_train,y_train)\npred2 = ridge_mod.predict(X_test)\nscore = model.score(X_test,y_test)\nRmse = mean_squared_error(y_test,pred2)\nprint(\"R2:{0:.3f}, MSE:{1:.2f}, RMSE:{2:.2f}\"\n   .format(score, Rmse,np.sqrt(mse))) \nprint(pd.Series(ridge_mod.coef_, index = X.columns)) # Print coefficients\nprint(mean_squared_error(y_test, pred2))          # Calculate the test MSE\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Alpha:0.015269, R2:0.396, MSE:19938.35, RMSE:141.20\nridge_cv=RidgeCV(alphas=alphas,scoring = 'neg_mean_squared_error', normalize = True, store_cv_values=True)\nridge_cv_mod = ridge_cv.fit(X_train,y_train)\nprint(ridge_cv_mod.alpha_)\n#0.01\n#print(np.mean(ridge_mod.cv_values_, axis=0))\n\n\nprint(ridge_mod.intercept_)\nprint(ridge_mod.coef_)\n\nprint(ridge_cv_mod.intercept_)\nprint(ridge_cv_mod.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(y_test,pred2)\nplt.title('Residual Analysis - Ridge_Regression')\nplt.xlabel('Observed')\nplt.ylabel('Residual')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint(\"MAE:\", metrics.mean_absolute_error(y_test,pred2))\nprint('MSE:', metrics.mean_squared_error(y_test, pred2))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*The 10-Fold Cross-Validation Error 2*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# grid search hyperparameters for ridge regression\nfrom numpy import arange\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import Ridge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model\nmodel = Ridge()\n# define model evaluation method\ncv = RepeatedKFold(n_splits=10, random_state=1)\n# define grid\ngrid = dict()\ngrid['alpha'] = arange(0, 1, 0.01)\n# define search\nsearch = GridSearchCV(model, grid, scoring='neg_mean_squared_error', cv=cv)\n# perform the search\nresults = search.fit(X_train, y_train)\n# summarize\nprint('MSE: %.3f' % results.best_score_)\nprint('Config: %s' % results.best_params_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **D. Lasso Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nX_train = scl.fit_transform(X_train)\nX_test = scl.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefs = []\n#from sklearn.linear_model import Ridge\nfor a in alphas:\n    Lmodel = Lasso(alpha=a, normalize=True)\n    Lmodel.fit(X_train,y_train)\n    coefs.append(Lmodel.coef_)\n    Lscore = Lmodel.score(X_train,y_train)\n    Lpred2 = Lmodel.predict(X_test)\n    mse = mean_squared_error(y_test, Lpred2) \n    print(\"Alpha:{0:.6f}, R2:{1:.3f}, MSE:{2:.2f}, RMSE:{3:.2f}\".format(a, Lscore, mse, np.sqrt(mse)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LassoCV\nfrom sklearn.datasets import make_regression\n\n#Alpha:0.005000, R2:0.396, MSE:19959.38, RMSE:141.28\nlasso_cv=LassoCV(cv=20)\nlasso_cv_mod = lasso_cv.fit(X_train,y_train)\nprint(lasso_cv_mod.alpha_)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Lmodel.intercept_)\nprint(Lmodel.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(coefs)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Alpha:0.005000, R2:0.400, MSE:19959.38, RMSE:141.28\n\nax = plt.gca()\nax.plot(alphas*2, coefs)\nax.set_xscale('log')\nplt.axis('tight')\nplt.xlabel('alpha')\nplt.ylabel('weights')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Cross Validation Lasso**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nX_train = scl.fit_transform(X_train)\nX_test = scl.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate an lasso regression model on the dataset\nfrom numpy import mean\nfrom numpy import std\nfrom numpy import absolute\nfrom sklearn.model_selection import cross_val_score\n\n#Alpha:0.005000, R2:0.396, MSE:19959.38, RMSE:141.28\nLmodel = Lasso(alpha=0.005000, normalize=True)\nLmodel.fit(X_train,y_train)\ncoefs.append(Lmodel.coef_)\nLscore = Lmodel.score(X_train,y_train)\nLpred2 = Lmodel.predict(X_test)\nmse = mean_squared_error(y_test, Lpred2) \n\n\n# define model evaluation method\nLcv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define grid\ngrid = dict()\ngrid['alpha'] = arange(0, 1, 0.01)\n# define search\nsearch = GridSearchCV(Lmodel, grid, scoring='neg_mean_absolute_error', cv=Lcv, n_jobs=-1)\n# evaluate model\nscores = cross_val_score(Lmodel, X_train, y_train, scoring='neg_mean_absolute_error', cv=Lcv, n_jobs=-1)\n\n# perform the search\nresults = search.fit(X_train, y_train)\n# summarize\nprint('MAE: %.3f' % results.best_score_)\nprint('Config: %s' % results.best_params_)\n\n# force scores to be positive\nscores = absolute(scores)\nprint('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(y_test,Lpred2)\nplt.title('Residual Analysis - Lasso Regression')\nplt.xlabel('Observed')\nplt.ylabel('Residual')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MAE:\", metrics.mean_absolute_error(y_test,Lpred2))\nprint('MSE:', metrics.mean_squared_error(y_test, Lpred2))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, Lpred2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10 fold CV\nfrom sklearn.linear_model import LassoCV\n## define model evaluation method\ncv = RepeatedKFold(n_splits=10, random_state=1)\n# define model\nmodel = LassoCV(alphas=arange(0, 1, 0.01), cv=cv)\n# fit model\nmodel.fit(X_train, y_train)\n# summarize chosen configuration\nprint('alpha: %f' % model.alpha_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **E1. Regression Tree**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nreg4 = DecisionTreeRegressor()\nreg4.fit(X_train,y_train)\nreg4.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Dec_Tree = reg4.predict(X_test)\nDec_Tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(y_test,Dec_Tree)\nplt.title('Residual Analysis - Decision Tree Regression')\nplt.xlabel('Observed')\nplt.ylabel('Residual')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MAE:\", metrics.mean_absolute_error(y_test,Dec_Tree ))\nprint('MSE:', metrics.mean_squared_error(y_test, Dec_Tree))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, Dec_Tree)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10 Fold\nparameters = {'max_depth':range(3,20)}\nclf = GridSearchCV(reg4, parameters,scoring='neg_mean_squared_error', cv=10)\nclf.fit(X_train, y_train)\ntree_model = clf.best_estimator_\nprint (clf.best_score_, clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **E2. Decision Tree with Pruning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pruning the Tree\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Minimum observations at the internal node approach\nregtree2 = DecisionTreeRegressor(min_samples_split = 3)\nregtree2.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction\ntest_pred2 = regtree2.predict(X_test)\ntrain_pred2 = regtree2.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Error on test dataset\nmean_squared_error(y_test, test_pred2)\nr2_score(y_test, test_pred2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Error on train dataset\nmean_squared_error(y_train, train_pred2)\nr2_score(y_train, train_pred2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Minimum observations at the leaf node approach\nregtree3 = DecisionTreeRegressor(min_samples_leaf = 3)\nregtree3.fit(X_train, y_train)\n\n# Prediction\ntest_pred3 = regtree3.predict(X_test)\ntrain_pred3 = regtree3.predict(X_train)\n\n# measure of error on test dataset\nmean_squared_error(y_test, test_pred3)\nr2_score(y_test, test_pred3)\n\n# measure of error on train dataset\nmean_squared_error(y_train, train_pred3)\nr2_score(y_train, train_pred3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10 Fold DT pruning with leaf node apporoach\nparameters = {'max_depth':range(3,20)}\nclf = GridSearchCV(regtree3, parameters,scoring='neg_mean_squared_error' ,cv=10)\nclf.fit(X_train, y_train)\ntree_model = clf.best_estimator_\nprint (clf.best_score_, clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10 Fold DT pruning internal node approach\nparameters = {'max_depth':range(3,20)}\nclf = GridSearchCV(regtree2, parameters,scoring='neg_mean_squared_error', cv=10)\nclf.fit(X_train, y_train)\ntree_model = clf.best_estimator_\nprint (clf.best_score_, clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **F. Random Forest**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nforest = RandomForestRegressor(n_estimators = 400, criterion='mse',random_state=1, n_jobs=-1)\nforest.fit(X_train, y_train)\ny_train_pred = forest.predict(X_train)\ny_test_pred = forest.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\n#Root_Mean_Square_Log_Error(RMSE) is accuracy criteria for this problem\nprint('RMSLE train: %.3f' % np.sqrt(mean_squared_error(np.log(y_train + 1), np.log(y_train_pred + 1))))\nprint('RMSLE test: %.3f' % np.sqrt(mean_squared_error(np.log(y_test + 1), np.log(y_test_pred + 1))))\nprint('R2 train: %.3f' % r2_score(y_train, y_train_pred))\nprint('R2 test: %.3f' % r2_score(y_test, y_test_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = RandomForestClassifier()\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom numpy import mean\nfrom numpy import std\n# evaluate the model\nmodel = RandomForestRegressor()\n# evaluate the model\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(forest, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise')\n# report performance\nprint('MSE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(y_test,y_test_pred)\nplt.title('Residual Analysis - Random Forest Regression')\nplt.xlabel('Observed')\nplt.ylabel('Residual')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best result given by Random Forest across A-H."},{"metadata":{},"cell_type":"markdown","source":"# **G. Gradient Boosting**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.ensemble import GradientBoostingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model1 = GradientBoostingRegressor()\n\nmodel1 = GradientBoostingRegressor(n_estimators=250, learning_rate=0.1, max_depth=7,subsample=0.9, random_state=42,loss='ls', verbose=2).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model on the whole dataset\nmodel1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the evaluation procedure\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate the model\nn_scores = cross_val_score(model1, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n# report performance\nprint('MSE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **H. Optimal Model**"},{"metadata":{},"cell_type":"markdown","source":"Random Forest with 10 fold cross validation was giving the least error and high accuracy across all the models from A-H\nSimilar apporach within train and validation can be apply to the given test data"},{"metadata":{},"cell_type":"markdown","source":"# **Compute the MSE for the Test Data & Compare with the CV Error**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#x_train,x_test,y_train,y_test=train_test_split(df.drop('count',axis=1),df['count'],test_size=0.20,random_state=0)\nX_train,X_test,y_train,y_test=train_test_split(df.drop('count',axis=1),df['count'],test_size=0.25,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models=[RandomForestRegressor(),AdaBoostRegressor(),BaggingRegressor(),KNeighborsRegressor()]\nmodel_names=['RandomForestRegressor','AdaBoostRegressor','BaggingRegressor','KNeighborsRegressor']\nrmsle=[]\nmse=[]\nd={}\nfor model in range (len(models)):\n    clf=models[model]\n    clf.fit(X_train,y_train)\n    test_pred=clf.predict(X_test)\n    rmsle.append(np.sqrt(mean_squared_log_error(test_pred,y_test)))\n    mse.append(mean_squared_error(test_pred,y_test))\nd={'Modelling Algo':model_names,'RMSLE':rmsle,'MSE':mse}   \nd\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmsle_frame=pd.DataFrame(d)\nrmsle_frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot(x='Modelling Algo',y='RMSLE',data=rmsle_frame,kind='bar',size=5,aspect=2)\nsns.factorplot(x='Modelling Algo',y='MSE',data=rmsle_frame,kind='bar',size=5,aspect=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for random forest regresion.\nno_of_test=[1000]\nparams_dict={'n_estimators':no_of_test,'n_jobs':[-1],'max_features':[\"auto\",'sqrt','log2']}\nclf_rf=GridSearchCV(estimator=RandomForestRegressor(),param_grid=params_dict,scoring='neg_mean_squared_log_error')\nclf_rf.fit(X_train,y_train)\npred=clf_rf.predict(X_test)\nprint((np.sqrt(mean_squared_log_error(pred,y_test))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_rf.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RANDOM FORETS REGRESSOR GIVES THE LEAST RMSLE. HENCE WE USE IT TO MAKE PREDICTIONS"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=clf_rf.predict(test_df.drop('datetime',axis=1))\nd={'datetime':test_data['datetime'],'count':pred}\nans=pd.DataFrame(d)\nans.to_csv('submission.csv',index=False) # saving to a csv file for predictions.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}