{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom datetime import datetime as dt\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom catboost import CatBoostRegressor, Pool\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport re, string, sys, pprint\nfrom datetime import datetime\n\nsns.set()\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n#========================================================\n#submission line with output table in submission format, this file will show up in output folder on right\n# output.to_csv('submission.csv,index=False')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"See, fork, and run a random forest benchmark model through Kaggle Scripts\n\nYou are provided hourly rental data spanning two years 1/1/2011-31/12/2012. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.\n\n\n\nData Fields\ndatetime - hourly date + timestamp  \nseason -  1 = spring, 2 = summer, 3 = fall, 4 = winter \nholiday - whether the day is considered a holiday\nworkingday - whether the day is neither a weekend nor holiday\nweather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \ntemp - temperature in Celsius\natemp - \"feels like\" temperature in Celsius\nhumidity - relative humidity\nwindspeed - wind speed\ncasual - number of non-registered user rentals initiated\nregistered - number of registered user rentals initiated\ncount - number of total rentals"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset = pd.read_csv('/kaggle/input/bike-sharing-demand/train.csv', index_col=False)\ntestset = pd.read_csv('/kaggle/input/bike-sharing-demand/test.csv', index_col=False)\n\ntrain=trainset.copy()\ntest=testset.copy()\ndisplay(train.head(3))\ndisplay(test.head(3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def exploreset(mydataset):\n    print(\"\\n=============\\n\")\n    print(\"COLUMNS IN DATASET: \",\"\\n\", mydataset.columns,\"\\n\")\n    print(\"ROWS/INDEX IN DATASET: \",\"\\n\", mydataset.index,\"\\n\")\n    print('SHAPE OF DATASET: ',\"\\n\",mydataset.shape,\"\\n\")\n    print('SIZE OF DATASET: ',\"\\n\",mydataset.size)\n    print(\"\\n=============\\n\")\n    print('COUNT of ISNA in each COLUMN',\"\\n-----------\")\n    print(mydataset.isna().sum(axis=0))\n    print(\"\\n=============\\n\")\n    print('DATASET INFO',\"\\n-----------\")\n    print(mydataset.info(),\"\\n\")\n    print(\"\\n=============\\n\")\n    print('DATASET DESCRIPTION',\"\\n-----------\")\n    print(mydataset.describe(),\"\\n\")\n    print(\"\\n=============\\n\")\n    print('DATASET CORRELATIONS',\"\\n-----------\")\n    print(mydataset.corr(),\"\\n\")\n\n        \ndef ISNAValues(mydataset):\n    print('COLUMNS with ISNA VALUES',\"\\n===\")\n    for column_name, isna in mydataset.isna().items():\n        try:\n            if isna.sum() > 0:\n\n                    print(\"//\", column_name, ' type     : ', type(mydataset[column_name][1]))\n                    print(\"//\", column_name, ' na count : ', isna.sum(),\"\\n\")\n\n        except:\n            print(\"Error\",\"\\n\")\n\n\ndef uniquevalues(mydataset):\n    print(\"\\n=============\\n\")\n   \n    print('COUNT of UNIQUE VALUES in each COLUMN',\"\\n-----------\")\n    print(mydataset.nunique(),\"\\n\")\n \n    print('DETAILS OF COLUMNS WITH UNIQUE VALUES >0',\"\\n===\")\n    for column_name, nunique in mydataset.nunique().items():\n        if nunique >0:\n            print(column_name, ':', mydataset[column_name].unique(),'// uniq : ', round(mydataset[column_name].nunique(),1))\n            print(mydataset[column_name].value_counts(),\"\\n====\\n\")\n            \n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_analysis_imports(mydataset):\n    exploreset(mydataset)\n    ISNAValues(mydataset)\n    uniquevalues(mydataset)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('TRAIN DATA')\nrun_analysis_imports(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train set data\n\nno na values \n\nCount of unique values show\n\ndatetime      10886 unique datetime values, (hourly date + timestamp) is an object and needs processing\nseason            4 categorical data where season - 1 = spring, 2 = summer, 3 = fall, 4 = winter holiday\nholiday           2 categorical data where 1 = yes/0 = no\nworkingday        2 categorical data where 1 = yes/0 = no (whether the day is neither a weekend nor holiday ie. may overlap)\n\nweather           4  categorical data where (1, 2, 3, 4)\n    1: Clear, Few clouds, Partly cloudy, Partly cloudy \n    2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \n    3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds \n    4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \ntemp             49 continuous, temperature in Celsius\natemp            60 continuous, \"feels like\" temperature in Celsius \nhumidity         89 discrete, relative humidity\nwindspeed        28 continuous, wind speed \n\ncasual          309 discrete, number of non-registered user rentals initiated\nregistered      731 discrete, number of registered user rentals initiated\ncount           822 discrete, number of total rentals (ie casual + registered) (RESPONSE)\n\n\nDATASET DESCRIPTION (2 yrs of hourly data for 20 days/mth)\n\nWeather hourly mean is 1.4 indicating that the rental coverage area is one that tends to have clear or misty weather more often than snow, or thunderstorms\n\nHourly Temp ranges from approx. 0 to 45 degree celcius with the mean temperature around 20 deg celcius\n\nMean hourly rental by registered users is 155 much higher than mean hourly rental by casual users 36\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('TEST DATA')\nrun_analysis_imports(test)\n\n\n# columns in train but not in test data >>  \n# casual - number of non-registered user rentals initiated\n# registered - number of registered user rentals initiated\n# count - number of total rentals\n\n#count of each date in test data need to be predicted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##Test set data\n\nno na values\nMatching columns and data types to train set data except for casual, registered, count are not available\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature enhancement of date/time 2011-01-20 00:00:00 \n\ndisplay(test.sample(5))\ndisplay(train.sample(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train, Test Data setup + Adding New Features "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical 'am' column \n\ndef ampm(row):\n    am_list = list(range(12))\n    if row['hour'] in am_list:\n        return 1\n    else:\n        return 0  \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical 'peak hour' column \n# peak hour column for working days\n\ndef peak_hour(row):\n    ph_list = [7, 8, 9, 16, 17, 18, 19]\n    if all([row['holiday']==0, row['workingday']==1, row['hour'] in ph_list]):\n        return 1\n    else:\n        return 0  \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical 'atemp'  column\n\n# group temps in 5deg intervals (e.g. 1: 0=<atemp<4.99, 2: 5=<atemp<10.99, ... 9: atemp=>40)\n\ndef cat_atemp(atemp):\n\n    if 0<= atemp <14.99:\n        return 1\n    elif 5<= atemp <29.99:\n        return 2\n    else:\n        return 3\n\n#     if 0<= atemp <4.99:\n#         return 1\n#     elif 5<= atemp <9.99:\n#         return 2\n#     elif 10<= atemp <14.99:\n#         return 3\n#     elif 15<= atemp <19.99:\n#         return 4\n#     elif 20<= atemp <24.99:\n#         return 5\n#     elif 25<= atemp <29.99:\n#         return 6\n#     elif 30<= atemp <34.99:\n#         return 7\n#     elif 35<= atemp <39.99:\n#         return 8\n#     else:\n#         return 9\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical 'ph2' column\n# Weekend peak hours (9am to 9pm )\n# Holiday rentals are not significant (very much below mean daily rentals)\n\ndef peak_hour2(row):\n    ph2_list = range(9,22)\n    weekend = [5,6]\n    if all([row['day of week']in weekend, row['hour'] in ph2_list]):\n        return 1\n    else:\n        return 0  \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=train.copy()\n\n\n# convert ['datetime'] to datetime obj\ndata['datetime'] = data.apply(lambda eachrow: dt.strptime(eachrow['datetime'], \"%Y-%m-%d %H:%M:%S\"), axis=1)\n\n\n# add 'year', 'month', 'day', 'hour' columns\ndata['year'] = data['datetime'].dt.year\ndata['month'] = data['datetime'].dt.month\ndata['day'] = data['datetime'].dt.day\ndata['hour'] = data['datetime'].dt.hour\n\n\n# add 'day of week' column\ndata['day of week'] = data['datetime'].dt.dayofweek\n\n\n# adding new features\ndata['am morning'] = data.apply(lambda row: ampm(row), axis=1)\ndata['peak hour'] = data.apply(lambda row: peak_hour(row), axis=1)\ndata['peak hour2'] = data.apply(lambda row: peak_hour2(row), axis=1)\ndata['cat atemp'] = data.apply(lambda eachrow: cat_atemp(eachrow['atemp']), axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#new grouped dataset\ngrouped1 = data.groupby(['weather']).sum()\ngrouped2 = data.groupby(['season']).sum() \ngrouped3 = data.groupby(['hour']).sum()\n\n#drop columns that have no meaning when summed\ngrouped1.drop(['holiday','workingday','temp','atemp','humidity','windspeed','season'],axis=1,inplace=True)\ngrouped2.drop(['holiday','workingday','temp','atemp','humidity','windspeed','weather'],axis=1,inplace=True)\ngrouped3.drop(['holiday','workingday','temp','atemp','humidity','windspeed','weather'],axis=1,inplace=True)\ndisplay(grouped1)\ndisplay(grouped2)\ndisplay(grouped3)\n\n# print(grouped.index, '\\n')\n# grouped.loc[(1,4)]\n\n# It appears that rentals are more affected by weather than by season. \n# Casual users drop significantly in poor weather \n# Registered rentals remain mostly consistent across seasons but drops when it is (1), spring. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group_hour_workday1 = data.loc[data['workingday']==1].groupby(['hour']).sum()\ngroup_hour_workday0 = data.loc[data['workingday']==0].groupby(['hour']).sum()\n\nfig, (ax1,ax2,ax3,ax4) = plt.subplots(4,1, figsize=(10,20), sharex=False)\n\nax1 = sns.barplot(x=group_hour_workday1.index, y='casual', data=group_hour_workday1, color='blue', ax=ax1)\nax1.set_xlabel(None)\nax2 = sns.barplot(x=group_hour_workday1.index, y='registered', data=group_hour_workday1, color='red', ax=ax2)\nax2.set_xlabel(None)\nax3 = sns.barplot(x=group_hour_workday0.index, y='casual', data=group_hour_workday0, color='mediumblue', ax=ax3)\nax3.set_xlabel(None)\nax4 = sns.barplot(x=group_hour_workday0.index, y='registered', data=group_hour_workday0, color='darkred', ax=ax4)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Heatmap without categorical columns except for binary types\n\ndata_corr = data.drop(columns=['datetime','weather','temp','count','year','day','hour','day of week'])\ncorrmat = data_corr.corr()\nfig, ax = plt.subplots(figsize=(12,12))\n\nsns.heatmap(corrmat, annot=True,fmt='.2f', annot_kws={'size':10},square=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as expected, there is presence of multicollinearity between the variables temp and atemp ('feels like temp') and we will drop the predictor temp there is high correlation between registered and count because registered users comprise a significant percentage of total rentals. however, we do not drop the predictor at this stage as we would like to understand what drives casual vs registered usage\n\nFrom the heatmap, the top 3 variables most correlated with the number of rental is atemp, season, windspeed. All other variable have low correlation with count <0.05"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Splitting --- for independent 'registered' and 'casual' modelling\n\ncols_to_drop = ['datetime','temp','registered','casual','count','cat atemp','am morning']\n\ny_data_reg = data['registered']\nX_data_reg = data.drop(cols_to_drop, axis=1)\nX_train_reg,X_test_reg,y_train_reg,y_test_reg = train_test_split(X_data_reg,y_data_reg, \n                                                                 test_size=0.1)\n                        \ny_data_cas = data['casual']\nX_data_cas = data.drop(cols_to_drop, axis=1)                    \nX_train_cas,X_test_cas,y_train_cas,y_test_cas = train_test_split(X_data_cas,y_data_cas,\n                                                                 test_size=0.1)                        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full = [0,1,2,3,7,8,9,10,11,12,13]  \npart = [0,1,2,3,12,13]            # no month,day,hour,dayofweek,cat_atemp\ncat_features_reg = None\ncat_features_cas = None\n\n\nX_train_reg_pool = Pool(X_train_reg,\n                        y_train_reg, \n                        cat_features=cat_features_reg)\nX_test_reg_pool = Pool(X_test_reg,\n                       y_test_reg,\n                       cat_features=cat_features_reg) \n\n\nX_train_cas_pool = Pool(X_train_cas,\n                        y_train_cas,\n                        cat_features=cat_features_cas)\nX_test_cas_pool = Pool(X_test_cas,\n                       y_test_cas,\n                       cat_features=cat_features_cas) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param = {\n        'n_estimators':500,\n        'depth':3, \n        'learning_rate':1, \n        'loss_function':'RMSE'\n        }\n\nmodel1 = CatBoostRegressor(n_estimators=3000,depth=8,learning_rate=0.1,loss_function='RMSE',l2_leaf_reg=20)\nmodel2 = CatBoostRegressor(n_estimators=3000,depth=8,learning_rate=0.1,loss_function='RMSE',l2_leaf_reg=20)\n\nmodel1.fit(X_train_reg_pool)\nmodel2.fit(X_train_cas_pool)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pred_scores(data,label,model):\n    \n    pred = [max(0,round(num)) for num in model.predict(data)]\n    \n    return  {\n            \"RMSE\": np.sqrt(metrics.mean_squared_error(label,pred)), \n            \"R-Squared\": metrics.r2_score(label,pred),\n#             \"Predictions\": pred[50:200]\n            }\n\nprint(\"TRAIN-registered:\", pred_scores(X_train_reg_pool, y_train_reg, model1))\nprint(\"TRAIN-casual:\", pred_scores(X_train_cas_pool, y_train_cas, model2))\nprint(\"TEST-registered:\", pred_scores(X_test_reg_pool, y_test_reg, model1))\nprint(\"TEST-casual:\", pred_scores(X_test_cas_pool, y_test_cas, model2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test.csv data setup\n\ntdata=test.copy()\n\ntdata['datetime'] = tdata.apply(lambda eachrow: dt.strptime(eachrow['datetime'], \"%Y-%m-%d %H:%M:%S\"), axis=1)\n\ntdata['year'] = tdata['datetime'].dt.year\ntdata['month'] = tdata['datetime'].dt.month\ntdata['day'] = tdata['datetime'].dt.day\ntdata['hour'] = tdata['datetime'].dt.hour\n\ntdata['day of week'] = tdata['datetime'].dt.dayofweek\n\ntdata['am morning'] = tdata.apply(lambda row: ampm(row), axis=1)\ntdata['peak hour'] = tdata.apply(lambda row: peak_hour(row), axis=1)\ntdata['peak hour2'] = data.apply(lambda row: peak_hour2(row), axis=1)\ntdata['cat atemp'] = tdata.apply(lambda eachrow: cat_atemp(eachrow['atemp']), axis=1)\n\ntest_data = tdata.drop(columns=['datetime','temp','cat atemp'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test data to model (Pool)\n\ntest_reg_pool = Pool(test_data,\n                        cat_features=cat_features_reg) \n\ntest_cas_pool = Pool(test_data,\n                        cat_features=cat_features_cas)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test predictions\n\npred_reg = [max(0,round(num)) for num in model1.predict(test_reg_pool)]\npred_cas = [max(0,round(num)) for num in model2.predict(test_cas_pool)]\n\nprint(pred_reg[50:100])\nprint(pred_cas[50:100])\n\npred_total = [pred_reg[i] + pred_cas[i] for i in range(len(pred_reg))]\nprint(pred_total[50:100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame(test['datetime'], columns=['datetime', 'count'])\noutput['count'] = pred_total\noutput.to_csv('submission.csv', index=False)\n\noutput","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}