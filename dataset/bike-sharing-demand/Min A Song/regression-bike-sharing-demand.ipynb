{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Regression with Bike Sharing Demand Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Data Loading and Preprocessing"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"bike_df = pd.read_csv('../input/train.csv')\nbike_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 'datetime' feature needs to be transformed as an object and to be splited into year, month, day and time. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform string into datetime type\nbike_df['datetime'] = bike_df['datetime'].apply(pd.to_datetime)\n\n# Extract year, month, day and time from the datetime type\nbike_df['year'] = bike_df['datetime'].apply(lambda x : x.year)\nbike_df['month'] = bike_df['datetime'].apply(lambda x : x.month)\nbike_df['day'] = bike_df['datetime'].apply(lambda x : x.day)\nbike_df['hour'] = bike_df['datetime'].apply(lambda x : x.hour)\nbike_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I will delete the datetime feature. \n\nI will also delete casual and registered features as the sum of casual and registered equals to count."},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_df.drop(['datetime', 'casual', 'registered'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will create a prediction performance evaluation function. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# Calculate RMSLE (Root Mean Square Log Error) with using not log() but log1p() due to issues including NaN\ndef rmsle(y, pred):\n    log_y = np.log1p(y)\n    log_pred = np.log1p(pred)\n    squared_error = (log_y - log_pred) ** 2\n    rmsle = np.sqrt(np.mean(squared_error))\n    return rmsle\n\n# Calculate RMSE\ndef rmse(y, pred):\n    return np.sqrt(mean_squared_error(y, pred))\n\n# Calculate MSE, RMSE and RMSLE\ndef evaluate_regr(y, pred):\n    rmsle_val = rmsle(y, pred)\n    rmse_val = rmse(y, pred)\n    mse_val = mean_absolute_error(y, pred)\n    print('RMSLE: {:.3f}, RMSE: {:.3f}, MSE: {:.3f}'.format(rmsle_val, rmse_val, mse_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Log Transformation, Feature Encoding and Model training/prediction/evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\n\ny_target = bike_df['count']\nX_features = bike_df.drop(['count'], axis=1, inplace=False)\n\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=2019)\n\nlr_reg = LinearRegression()\nlr_reg.fit(X_train, y_train)\npred = lr_reg.predict(X_test)\n\nevaluate_regr(y_test, pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Considering count values, such prediction errors seems to be relatively high. \n\nLet's check actual and predicted values in top 5 errors!"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_error_data(y_test, pred, n_tops=5):\n    result_df = pd.DataFrame(y_test.values, columns=['real_count'])\n    result_df['predicted_count'] = np.round(pred)\n    result_df['diff'] = np.abs(result_df['real_count'] - result_df['predicted_count'])\n    \n    print(result_df.sort_values('diff', ascending=False)[:n_tops])\n    \nget_top_error_data(y_test, pred, n_tops=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As I mentioned, prediction errors are so high. \n\nLet's check Target distribution. "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_target.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Target distribution is skewed!\n\nLog transformation fot the Target seems to be needed. "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_log_transform = np.log1p(y_target)\ny_log_transform.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Skewness is somewhat improved.\n\nLet's train the model again!"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_target_log = np.log1p(y_target)\n\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target_log, test_size=0.3, random_state=2019)\n\nlr_reg = LinearRegression()\nlr_reg.fit(X_train, y_train)\npred = lr_reg.predict(X_test)\n\n# Convert the transformed y_test values into the original values\ny_test_exp = np.expm1(y_test)\n\n# Convert the transformed predicted values into the original values\npred_exp = np.expm1(pred)\n\nevaluate_regr(y_test_exp, pred_exp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RMSLE is lower, but RMSE is higher. \n\nLet's check coefficient values of features!"},{"metadata":{"trusted":true},"cell_type":"code","source":"coef = pd.Series(lr_reg.coef_, index=X_features.columns)\ncoef_sort = coef.sort_values(ascending=False)\nsns.barplot(x=coef_sort.values, y=coef_sort.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Year feature has an exceptional coefficient value, which results from being int type. \n\nSo, categorical features including year need to be transformed into one-hot encoding. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_features_ohe = pd.get_dummies(X_features, columns=['year', 'month', 'hour', 'holiday', 'workingday', 'season', 'weather'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_features_ohe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_features_ohe, y_target_log, test_size=0.3, random_state=2019)\n\ndef get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1=False):\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    if is_expm1:\n        y_test = np.expm1(y_test)\n        pred = np.expm1(pred)\n    print('###', model.__class__.__name__, '###')\n    evaluate_regr(y_test, pred)\n\nlr_reg = LinearRegression()\nridge_reg = Ridge(alpha=10)\nlasso_reg = Lasso(alpha=0.01)\n\nfor model in [lr_reg, ridge_reg, lasso_reg]:\n    get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After one-hot encoding, prediction performance is improved. \n\nLet's check features with high coefficient values."},{"metadata":{"trusted":true},"cell_type":"code","source":"coef = pd.Series(lr_reg.coef_, index=X_features_ohe.columns)\ncoef_sort = coef.sort_values(ascending=False)[:15]\nsns.barplot(x=coef_sort.values, y=coef_sort.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Season, month and weather features have high coefficient values.\n\nLet's apply Regression Tree models including RandomForest, GBM, XGBoost and LightGBM."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_features_ohe, y_target_log, test_size=0.3, random_state=2019)\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nrf_reg = RandomForestRegressor(n_estimators=500)\ngbm_reg = GradientBoostingRegressor(n_estimators=500)\nxgb_reg = XGBRegressor(n_estimators=500)\nlgbm_reg = LGBMRegressor(n_estimators=500)\n\nfor model in [rf_reg, gbm_reg, xgb_reg, lgbm_reg]:\n    get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Regression prediction performances are improved!"},{"metadata":{},"cell_type":"markdown","source":"## 3. Prediction on Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/sampleSubmission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = pd.read_csv('../input/test.csv')\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform string into datetime type\nX_test['datetime'] = X_test['datetime'].apply(pd.to_datetime)\n\n# Extract year, month, day and time from the datetime type\nX_test['year'] = X_test['datetime'].apply(lambda x : x.year)\nX_test['month'] = X_test['datetime'].apply(lambda x : x.month)\nX_test['day'] = X_test['datetime'].apply(lambda x : x.day)\nX_test['hour'] = X_test['datetime'].apply(lambda x : x.hour)\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.drop(['datetime'], axis=1, inplace=True)\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_ohe = pd.get_dummies(X_test, columns=['year', 'month', 'hour', 'holiday', 'workingday', 'season', 'weather'])\nX_test_ohe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = lgbm_reg.predict(X_test_ohe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['count'] = np.round(prediction, 0).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('./My_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}