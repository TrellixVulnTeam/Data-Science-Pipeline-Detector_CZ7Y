{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_log_error\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/bike-sharing-demand/train.csv')\ntest = pd.read_csv('../input/bike-sharing-demand/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will convert the current datetime column into Machine Learning friendly format i.e Year, Month, Day, Hour."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['year'] = [t.year for t in pd.DatetimeIndex(train.datetime)]\ntrain['month'] = [t.month for t in pd.DatetimeIndex(train.datetime)]\ntrain['day'] = [t.day for t in pd.DatetimeIndex(train.datetime)]\ntrain['hour'] = [t.hour for t in pd.DatetimeIndex(train.datetime)]\n\ntest['year'] = [t.year for t in pd.DatetimeIndex(test.datetime)]\ntest['month'] = [t.month for t in pd.DatetimeIndex(test.datetime)]\ntest['day'] = [t.day for t in pd.DatetimeIndex(test.datetime)]\ntest['hour'] = [t.hour for t in pd.DatetimeIndex(test.datetime)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In above output we can see we converted the datetime column in Machine Learning firendly format now we will drop the datetime column"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop('datetime',axis=1,inplace=True)\ntest.drop('datetime',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Visualisation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(11.7,8.27)})\nfig, ax = plt.subplots(2,2)\nsns.barplot(train['season'],train['count'],ax=ax[0,0]);\nsns.barplot(train['holiday'],train['count'],ax=ax[0,1]);\nsns.barplot(train['workingday'],train['count'],ax=ax[1,0]);\nsns.barplot(train['weather'],train['count'],ax=ax[1,1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above plots show us intuitively how count parameter differs with workingday, weather, season, holiday"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(11.7,8.27)})\nfig, ax = plt.subplots(2,2)\nsns.distplot(train['temp'],ax=ax[0,0]);\nsns.distplot(train['atemp'],ax=ax[0,1]);\nsns.distplot(train['humidity'],ax=ax[1,0]);\nsns.distplot(train['windspeed'],ax=ax[1,1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This are distribution plot of humidity, windspeed, temp and atemp"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(15,10)})\nsns.heatmap(train.corr(),annot=True,linewidths=0.5);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We cans see that registered/casual is highly correlated with the count which means most of the bike were registered."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['casual','registered'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Correlation Plot**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(20,5)})\nsns.barplot(x=train['month'],y=data['count']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above plot explains the demand of the bicycle according to month."},{"metadata":{},"cell_type":"markdown","source":"**Data Transformation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"season = pd.get_dummies(train['season'],prefix='season')\ntrain = pd.concat([train,season],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop('season',axis=1,inplace=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather = pd.get_dummies(train['weather'],prefix='weather')\n\ntrain = pd.concat([train,weather],axis=1)\n\ntrain.drop('weather',axis=1,inplace=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting data into Train and Test split**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns.to_series().groupby(data.dtypes).groups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop('count',axis=1)\ny = train['count']\nX = sc.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Building**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()\nmodel_rf = rf.fit(X_train,y_train)\ny_pred_rf = model_rf.predict(X_test)\nnp.sqrt(mean_squared_log_error(y_test,y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above Random Forest model is using default parameter of the Random forest model to reduse RMSLE more we will do hyperparameter tuning the parameters considered are listed below"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = [int(x) for x in range(200,2000,100)]\nmax_feature = ['auto','sqrt']\nmin_sample_split = [2,5,10]\nmin_sample_leaf = [1,2,4]\nmax_depth = [int(x) for x in range(10,110,11)]\nmax_depth.append(None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_grid = {'n_estimators': n_estimators,\n              'max_depth': max_depth,\n              'max_features': max_feature,\n              'min_samples_leaf': min_sample_leaf,\n              'min_samples_split': min_sample_split}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_grid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will find the optimal solution using RandomizedSearchCV, the estimator will be Random Forest and parameters will be all the parameters in random_grid"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_tune = RandomForestRegressor()\nfrom sklearn.model_selection import RandomizedSearchCV\nrf_random = RandomizedSearchCV(estimator=rf_tune,param_distributions=random_grid,n_iter=100,cv=5,verbose= 2,n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"by running **rf_random(X_train,y_train)** we will get the optimal parameter as below\n\n* max_depth=87\n* max_features='auto'\n* min_samples_leaf=1\n* min_samples_split=2\n* n_estimators=1300\n\nnow we will make final Random forest model with hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_rf = RandomForestRegressor(max_depth=87,max_features='auto',min_samples_leaf=1,min_samples_split=2,n_estimators=1300)\nfinal_model_rf = final_rf.fit(X_train,y_train)\ny_final_pred = final_model_rf.predict(X_test)\nnp.sqrt(mean_squared_log_error(y_test,y_final_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}