{"cells":[{"metadata":{"_uuid":"b3c684e453b81c1899e3eefa7921807cc1187ef1"},"cell_type":"markdown","source":"Simplified and optimized version of code can be found at my profile: https://www.kaggle.com/valeriyparubets/sklearn-gbr-0-413\nHere is an explanation about how it's done\nAlso, the same in Russian is here: https://github.com/Vzzzz/kaggle-bike","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nfrom datetime import datetime\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":false},"cell_type":"code","source":"trainData = pd.read_csv('../input/train.csv')\ntestData = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"660ddffeed6968b757c8194a8e621b736848d319"},"cell_type":"code","source":"trainData.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51e0fd81e18d57474af5942085cb3f5090ec33f7"},"cell_type":"markdown","source":"First, let's visualize distribution and rolling mean of bikes rented for the hole period:","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"3c4ae8d973f01d0ff5b086a3c4ca6431252f4d6f"},"cell_type":"code","source":"fig, axes = plt.subplots(figsize=(15, 4), ncols=2, nrows=1)\nsn.distplot(trainData[\"count\"],ax=axes[0])\nplt.plot(pd.rolling_mean(trainData['count'], 100))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3761b3914176181b14d78fc081535b08157b87c2"},"cell_type":"markdown","source":"With this form of distribution it's better to use logarithm of count. Thus we simplify the distribution:","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"b07cc24e8bb76784f4deb8a1132f39dd332e12d1"},"cell_type":"code","source":"trainData['logcount'] = trainData['count'].apply(lambda x: np.log1p(x))\nfig, axes = plt.subplots(figsize=(15, 8))\nsn.distplot(trainData[\"logcount\"], ax=axes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a026e6458e133ea1c99bbeb6c13f8581b153bec3"},"cell_type":"markdown","source":"As a first step of feature engineering we separate date-time column on a set of few features: 'date', 'month', 'hour', 'weekday'","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"4f4dd10a3f77c64ac5d72b70421811a27f42b27e"},"cell_type":"code","source":"trainData['date'] = trainData.datetime.apply(lambda x : x.split()[0])\ntrainData['hour'] = trainData.datetime.apply(lambda x : x.split()[1].split(\":\")[0])\ntrainData['weekday'] = trainData.date.apply(lambda dateString : datetime.strptime(dateString, '%Y-%m-%d').weekday())\ntrainData['month'] = trainData.date.apply(lambda dateString : datetime.strptime(dateString, '%Y-%m-%d').month)\n\ntestData['date'] = testData.datetime.apply(lambda x : x.split()[0])\ntestData['hour'] = testData.datetime.apply(lambda x : x.split()[1].split(\":\")[0])\ntestData['weekday'] = testData.date.apply(lambda dateString : datetime.strptime(dateString, '%Y-%m-%d').weekday())\ntestData['month'] = testData.date.apply(lambda dateString : datetime.strptime(dateString, '%Y-%m-%d').month)\n\ntimeColumn = testData['datetime']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b73d549ecf9b17738fe824b6e80edf18476c4fe1"},"cell_type":"markdown","source":"Also here we can try to train a regressor. It will give approximatelly 0.465","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"72e3f7f8f498d3247bc97b606c5ba5ba16d0fa74"},"cell_type":"code","source":"import xgboost as xgb\n\nX = trainData.drop(['count', 'datetime', 'registered', 'casual', 'date', 'logcount'], axis=1).values\nY = trainData['logcount'].values\n\ntestX = testData.drop(['datetime', 'date'], axis=1).values\n\ntrainMatrix = xgb.DMatrix(X, label=Y)\n\nmax_depth = 5\nmin_child_weight = 8\nsubsample = 0.9\nnum_estimators = 1000\nlearning_rate = 0.1\n\nclf = xgb.XGBRegressor(max_depth=max_depth,\n                min_child_weight=min_child_weight,\n                subsample=subsample,\n                n_estimators=num_estimators,\n                learning_rate=learning_rate)\n\nclf.fit(X,Y)\n\npred = clf.predict(testX)\npred = np.expm1(pred)\n\nsubmission = pd.DataFrame({\n        \"datetime\": timeColumn,\n        \"count\": pred\n    })\nsubmission.to_csv('XGBNoFE.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"0368da29f91bd5684be879e8b910eacad28bd8a1"},"cell_type":"markdown","source":"Let's continue with further feature engineering.\nLet's see distribution of working and non-working days, seasons and daytime","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"87f9e74184e7d25a67beb68212315b37b3e90df0"},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1,ncols=2)\nfig.set_size_inches(15, 8)\nsn.boxplot(data=trainData, y='count', x='season', ax=axes[0])\nsn.boxplot(data=trainData, y='count', x='workingday', ax=axes[1])\naxes[0].set(xlabel='season', ylabel='count')\naxes[1].set(xlabel='workingday', ylabel='count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6d40596d24556cfbdf88e321689a90b29290e9e1"},"cell_type":"code","source":"fix, axes = plt.subplots(figsize=(15, 10))\nsn.boxplot(data=trainData, y='count', x='hour', ax=axes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3dd9e9c2839821f718d4c15f6e3d1c6a82dca92"},"cell_type":"markdown","source":"With a rule of three sigmas let's clear dataset of \"anomaly\" entries. So we drop ~1% of data that doesn't suits this distribution","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"82569ad22e4ed89f33b7fa00367082b0dd06addf"},"cell_type":"code","source":"trainDataWithoutOutliers = trainData[np.abs(trainData['count']-trainData['count'].mean())\n                                     <=(3*trainData['count'].std())] \nprint(trainDataWithoutOutliers.shape)\ntrainData = trainDataWithoutOutliers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a118a0928befb3e6cbcab7a770ee10ea5c010f2"},"cell_type":"markdown","source":"Also we can see the correlation between features. This will help to decide which of them better drop.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"52b9bbb72f4284f744a2d9cf5cb624b83ee51a56"},"cell_type":"code","source":"corrMat = trainData.corr()\nmask = np.array(corrMat)\nmask[np.tril_indices_from(mask)] = False\nfig, ax= plt.subplots(figsize=(20, 10))\nsn.heatmap(corrMat, mask=mask,vmax=1., square=True,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d90547749500ba6713a946a96db5b0d04b14d8b"},"cell_type":"markdown","source":"Also we may build some dependencies between features using plots like these::","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"82e7a9ef4b4da0aae02b262712b33932af016b32"},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(15, 15))\n\nmeanMonthly = pd.DataFrame(trainData.groupby('month')['count'].mean()).reset_index().sort_values(by='count', ascending=False)\nsn.barplot(data=meanMonthly, x='month', y='count', ax=axes[0])\naxes[0].set(xlabel='month', ylabel='count')\n\nhoursSeasonly = pd.DataFrame(trainData.groupby(['hour', 'season'], sort=True)['count'].mean()).reset_index()\nsn.pointplot(x=hoursSeasonly['hour'], y=hoursSeasonly['count'], hue=hoursSeasonly['season'], data=hoursSeasonly, join=True, ax=axes[1])\naxes[1].set(xlabel='hour', ylabel='count')\n\nhoursDayly = pd.DataFrame(trainData.groupby(['hour','weekday'], sort=True)['count'].mean()).reset_index()\nsn.pointplot(x=hoursDayly['hour'], y=hoursDayly['count'], hue=hoursDayly['weekday'], data=hoursDayly, join=True,ax=axes[2])\naxes[2].set(xlabel='hour', ylabel='count')\n\nhoursSeasonly = pd.DataFrame(trainData.groupby(['hour', 'month'], sort=True)['count'].mean()).reset_index()\nsn.pointplot(x=hoursSeasonly['hour'], y=hoursSeasonly['count'], hue=hoursSeasonly['month'], data=hoursSeasonly, join=True, ax=axes[3])\naxes[1].set(xlabel='hour', ylabel='count')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47dcac1c9640d4ad8fc5573bc83c5edd7c433c3b"},"cell_type":"markdown","source":"With this information we chose the list of features that can be used for training","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"ce99f74869131d76f4c878599a84411bbf01418e"},"cell_type":"code","source":"X = trainData.drop(['date', 'temp', 'casual', 'registered', 'logcount', 'datetime', 'count'], axis=1)\n\nseason_df = pd.get_dummies(trainData['season'], prefix='s', drop_first=True)\nweather_df = pd.get_dummies(trainData['weather'], prefix='w', drop_first=True)\nhour_df = pd.get_dummies(trainData['hour'], prefix='h', drop_first=True)\nweekday_df = pd.get_dummies(trainData['weekday'], prefix='d', drop_first=True)\nmonth_df = pd.get_dummies(trainData['month'], prefix='m', drop_first=True)\n\nX = X.join(season_df)\nX = X.join(weather_df)\nX = X.join(hour_df)\nX = X.join(weekday_df)\nX = X.join(month_df)\n\nX = X.values\nY=trainData['logcount'].values\nprint(X.shape)\n\ntestX = testData.drop(['date', 'temp', 'datetime'], axis=1)\n\nseason_df = pd.get_dummies(testData['season'], prefix='s', drop_first=True)\nweather_df = pd.get_dummies(testData['weather'], prefix='w', drop_first=True)\nhour_df = pd.get_dummies(testData['hour'], prefix='h', drop_first=True)\nweekday_df = pd.get_dummies(testData['weekday'], prefix='d', drop_first=True)\nmonth_df = pd.get_dummies(testData['month'], prefix='m', drop_first=True)\n\ntestX = testX.join(season_df)\ntestX = testX.join(weather_df)\ntestX = testX.join(hour_df)\ntestX = testX.join(weekday_df)\ntestX = testX.join(month_df)\n\ntestX = testX.values\nprint(testX.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d510bf79dd33ae0b4333330599823bdbf0304646"},"cell_type":"markdown","source":"And we train some models","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"513709f8dda970e7759d6181bcb9f5e3e11342d8"},"cell_type":"code","source":"clf=xgb.XGBRegressor(max_depth=8,min_child_weight=6,gamma=0.4,colsample_bytree=0.6,subsample=0.6)\nclf.fit(X,Y)\n\npred = clf.predict(testX)\npred = np.expm1(pred)\n\nsubmission = pd.DataFrame({\n        \"datetime\": timeColumn,\n        \"count\": pred\n    })\nsubmission.to_csv('XGBwithFE.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da1fd35d4d643a67ea3ae39f22f6db0c187a79ec"},"cell_type":"markdown","source":"Also we may use some sklearn models with their's optimization","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"49e92362729a09ad3ec3ac87e25706c4eba89585"},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import make_scorer\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e8fba38a56961a0ce5bd357f3df88fbed8f5321"},"cell_type":"markdown","source":"Competition's metrics based on RLMSE, so if we want to optimize classifier we'd better use custom loss function:","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"26bbfff1d94b53ea0facd59ef32b9fb9bc7e02cc"},"cell_type":"code","source":"def loss_func(truth, prediction):\n    truth = np.expm1(truth)\n    prediction = np.expm1(prediction)\n    log1 = np.array([np.log(x + 1) for x in truth])\n    log2 = np.array([np.log(x + 1) for x in prediction])\n    return np.sqrt(np.mean((log1 - log2)**2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"340b2898232f0ab08cf572bd916ec87c6bbc05b4"},"cell_type":"code","source":"param_grid = {\n    'n_estimators': [50, 80, 100, 120],\n    'max_depth': [None, 1, 2, 5],\n    'max_features': ['sqrt', 'log2', 'auto']\n}\n\nscorer = make_scorer(loss_func, greater_is_better=False)\n\nregr = RandomForestRegressor(random_state=42)\n\nrfr = GridSearchCV(regr, param_grid, cv=4, scoring=scorer, n_jobs=4).fit(X, Y)\nprint('\\tParams:', rfr.best_params_)\nprint('\\tScore:', rfr.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"213d50c37a598cfb55a65bb6e6437cd9a8f3e634"},"cell_type":"code","source":"pred = rfr.predict(testX)\npred = np.expm1(pred)\n\nsubmission = pd.DataFrame({\n        \"datetime\": timeColumn,\n        \"count\": pred\n    })\nsubmission.to_csv('RandomForest.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a560d1bcb59484181d814e75ccb8b3cccc6ebce"},"cell_type":"markdown","source":"Code below runs too long, Usually it return the following:\n('Params:', {'n_estimators': 2000, 'learning_rate': 0.01, 'max_depth': 4})\n('Score:', -0.09649149584358846)","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"01230efd792e901e78cc88b95cb9668eb3117f78"},"cell_type":"code","source":"#\n#param_grid = {\n#    'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n#    'n_estimators': [100, 1000, 1500, 2000, 4000],\n#    'max_depth': [1, 2, 3, 4, 5, 8, 10]\n#}\n#\n#scorer = make_scorer(loss_func, greater_is_better=False)\n#\n#gb = GradientBoostingRegressor(random_state=42)\n#\n#gbr = GridSearchCV(gb, param_grid, cv=4, scoring=scorer, n_jobs=3).fit(X, Y)\n#print('\\tParams:', gbr.best_params_)\n#print('\\tScore:', gbr.best_score_)\n\ngbr = GradientBoostingRegressor(n_estimators=2000, learning_rate=0.01, max_depth=4)\n\ngbr.fit(X, Y)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"cb71e3c9f287ea1d4b4c376b36ccafeff6dd9a53"},"cell_type":"code","source":"pred = gbr.predict(testX)\npred = np.expm1(pred)\n\nsubmission = pd.DataFrame({\n        \"datetime\": timeColumn,\n        \"count\": pred\n    })\nsubmission.to_csv('GradientBoost.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}