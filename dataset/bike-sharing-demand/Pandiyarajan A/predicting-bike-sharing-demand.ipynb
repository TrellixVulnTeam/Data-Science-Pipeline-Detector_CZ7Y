{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pa\nfrom datetime import datetime as dt\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"PLOTS_PATH = \"../input/bike-sharing-demand/plots/\"\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(PLOTS_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving plot\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n    print(\"plot saved\", fig_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIR_PATH = \"../input/bike-sharing-demand/\"\nTRAIN_FILE = \"train.csv\"\nTEST_FILE = \"test.csv\"\n\ndef load_csv_data(file_path, file_name):\n    return pa.read_csv(os.path.join( file_path, file_name) )\n\ntrain_data = load_csv_data(DATA_DIR_PATH,TRAIN_FILE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analysis\nlets have a look at the data and figure out what can we infer from it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy dataset for analysis and manupulations.\nanalysis_data_set = train_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the metadata of the data.\nanalysis_data_set.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation:\n\n* We have 12 columns. 11 Features and 1 Target. \n* Data types : 3 Float, 8 int and 1 Object.\n\nWe need take care of Object datatype. i.e., _datetime_ feature. Because most of the model training algorithims accept features as numeric datatypes. \n\nTo get a better understanding of the data. Lets print the first 5 rows of the data table. "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"analysis_data_set.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_datetime_ feature is datetime formatted. Parse the required information for our model and drop the feature.\n\nExtract _\"weekday\",\"month\",\"hour\",\"year\"_ from _datetime_ object and add them as seperate features in our dataset. Then drop the _datatime_ feature. We must do the same process for validation and test instances."},{"metadata":{"slideshow":{"slide_type":"-"},"trusted":true},"cell_type":"code","source":"# \"weekday\",\"month\",\"hour\",\"year\" extraction\ndef transform(X):\n    c_datetime = X[\"datetime\"].unique()\n    formatted_dt = [dt.strptime(x, \"%Y-%m-%d %H:%M:%S\") for x in c_datetime]\n    X[\"weekday\"] = extract_part_from_date(formatted_dt,\"%w\")\n    X[\"month\"] = extract_part_from_date(formatted_dt,\"%m\")\n    X[\"hour\"] = extract_part_from_date(formatted_dt,\"%H\")\n    X[\"year\"] = extract_part_from_date(formatted_dt,\"%Y\")\n\n    int_features = [\"weekday\",\"month\",\"hour\",\"year\"]\n    for feature in int_features:\n        X[feature] = X[feature].astype(np.int64)\n\n    return X\n\ndef extract_part_from_date(dates, code_str):\n    return [date.strftime(code_str) for date in dates]\n\nanalysis_data_set = transform(analysis_data_set)    \nanalysis_data_set = analysis_data_set.drop([\"datetime\"], axis=1)\n\n# print first 6 instances of the dataset\nanalysis_data_set.head(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Discover and visualize the data to gain insights"},{"metadata":{},"cell_type":"markdown","source":"### Finding missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\nmsno.matrix(analysis_data_set,figsize=(12,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation\n\nlets find the pairwise correlation of all features in the our dataset. Non-numeric features are ignored automatically by the `.corr()` function in pandas."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix=analysis_data_set.corr()\ncorr_matrix[\"count\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cmap = sns.cubehelix_palette(light=1, as_cmap=True)\nsns.heatmap(corr_matrix, cmap=cmap)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [\"temp\",\"season\",\"windspeed\",\"humidity\",\"weather\",\"count\"]\ng = sns.pairplot(analysis_data_set,\n                 vars= features,\n                 palette = 'husl',\n                 height=1.4,\n                 aspect = 1.5,\n                 kind='reg',\n                 diag_kind = 'kde',\n                 diag_kws=dict(shade=True),\n                 plot_kws=dict(scatter_kws={'s':.2}),\n                 \n            )\ng.set(xticklabels=[])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inference:\n* _humidity, weather_ have strong negative relation \n* and _temp, season, windspeed_ have strong positive relation the target (_count_). \n* Other features such as  _weekday, workingday, year, month_ have considered corelation value.   \n* _atemp_ is highly correlated with _temp_, as how _registered, casual_ are correlated, so we can ignore _atemp, registered, casual_ features for our model.  "},{"metadata":{},"cell_type":"markdown","source":"### Outliers\n\nIdentify outliers in target and in the features that have strong corelation with the target."},{"metadata":{"trusted":true},"cell_type":"code","source":"# \"temp\",\"season\",\"windspeed\",\"humidity\",\"weather\",\"count\"\ndef bxplot(data):\n    fig, axes =  plt.subplots(nrows=3,ncols=2)\n    fig.set_size_inches(12,10)\n    sns.boxplot(y=data['count'],ax=axes[0][0])\n    sns.boxplot(x=data['temp'],y=data['count'],ax=axes[0][1])\n    sns.boxplot(x=data['season'],y=data['count'],ax=axes[1][0])\n    sns.boxplot(x=data['windspeed'],y=data['count'],ax=axes[1][1])\n    sns.boxplot(x=data['humidity'],y=data['count'],ax=axes[2][0])\n    sns.boxplot(x=data['weather'],y=data['count'],ax=axes[2][1])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bxplot(analysis_data_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove values on all features which are above 3 Standard deviations"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import zscore\nanalysis_data_set = analysis_data_set[(np.abs(zscore(analysis_data_set)) < 3).all(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bxplot(analysis_data_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Addtional informations on the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# hour \nsns.catplot(x='hour',y='count', hue=\"season\", data=analysis_data_set, kind=\"point\", aspect=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inference :\n\n* Across all the season the rental timing seems to be similar, yet the number of rentals varies by season. \n* The green season has almost double the count of blue during evening ( 17:00 - 19:00 )\n* During morning hours ( 7:00 - 9:00 ) the count variance small comparied to evening "},{"metadata":{"trusted":true},"cell_type":"code","source":"# day\nsns.catplot(x='hour',y='count',hue=\"weekday\",data=analysis_data_set, kind=\"point\", aspect=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inference :\n\n* The count is high during weekdays and low during weekends\n* On weekdays, morning and evening the count is at the peak. We may assume that is due to office goers.\n* Whereas on the weekends the count is flat and spread across the midday. Could be leisure rental."},{"metadata":{"trusted":true},"cell_type":"code","source":"# month wise rent\nsns.barplot(x=\"month\", y=\"count\", data=analysis_data_set, capsize=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inference :\n* Count is lower on the year start and gradually increases during the mid year.\n* Then the Count starts decreasing smoothly and reaches little more than the average count during month end.  "},{"metadata":{},"cell_type":"markdown","source":"With all the analysis, observations and inference we have a better idea on the features and the manupulations to be carried out on the data before we feed it into out machine learning algorithms for training and predictions. "},{"metadata":{},"cell_type":"markdown","source":"# Prepare the Data for Machine Learning Algorithms"},{"metadata":{},"cell_type":"markdown","source":"### Train and Test split \n\nStratified sampling. Spliting data for test and train set by stratifying based on season i.e., equal number of intances are fetched from each seasons."},{"metadata":{"trusted":true},"cell_type":"code","source":"# train and test split \nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nstratified = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\nfor train_i, validation_i in stratified.split(train_data, train_data['season']):\n    strat_train_set = train_data.loc[train_i]\n    strat_validation_set = train_data.loc[validation_i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, validation = train_test_split(train_data, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = strat_train_set.drop([\"count\"],axis=1).copy()\ntrain_Y = strat_train_set[\"count\"].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transformation Pipelines\n\nLets create custome transformation classes for \n* Date extraction\n* Columns drop\n* Outliers removals"},{"metadata":{},"cell_type":"markdown","source":"### Date extraction transformer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass DatePreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        return None\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        c_datetime = X[\"datetime\"].unique()\n        formatted_dt = [dt.strptime(x, \"%Y-%m-%d %H:%M:%S\") for x in c_datetime]\n        X[\"weekday\"] = self.extract_part_from_date(formatted_dt,\"%w\")\n        X[\"month\"] = self.extract_part_from_date(formatted_dt,\"%m\")\n        X[\"hour\"] = self.extract_part_from_date(formatted_dt,\"%H\")\n        X[\"year\"] = self.extract_part_from_date(formatted_dt,\"%Y\")\n        \n        int_features = [\"weekday\",\"month\",\"hour\",\"year\"]\n        for feature in int_features:\n            X[feature] = X[feature].astype(np.int64)\n        return X\n    \n    def extract_part_from_date(self, dates, code_str):\n        return [date.strftime(code_str) for date in dates]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transformer to drop columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DropColumns(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, columns):\n        self.columns = columns\n    \n    def fit(self,X):\n        return self\n    \n    def transform(self, X):\n        X = X.drop(self.columns, axis=1)\n        return X ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pipeline class to help with sequences of transformations\n\nPlace our custome transformation classes in the pipeline "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\ndrop_cols = ['datetime','registered','casual','atemp']\nnum_pipline = Pipeline([\n        (\"date_processor\", DatePreprocessor()),\n        (\"drop_cols\", DropColumns(columns = drop_cols) )\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nnum_attribs = list(train_X)\n\ncat_attribs = [\"season\",\"weather\"]\n\ncat_pipeline = ColumnTransformer([\n    (\"num_pipline\", num_pipline, num_attribs), # Handling Text and Categorical Attributes\n    (\"cat_pipline\", OneHotEncoder(sparse=False, categories='auto'), cat_attribs) \n])\n\ntrain_prepared = cat_pipeline.fit_transform(train_X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_prepared = train_prepared[(np.abs(zscore(train_prepared)) < 3).all(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Select and Train a model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor()\nrfr.fit(train_prepared,train_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_x = validation.drop(['count'],axis=1)\nvalidation_y = validation['count'].copy()\n\nnum_attribs = list(validation_x)\ncat_attribs = [\"season\",\"weather\"]\ndrop_cols = ['datetime','registered','casual','atemp','holiday']\nvalidation_x_prepared = cat_pipeline.transform(validation_x)\nrfr_prediction = rfr.predict(validation_x_prepared)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  RMSE"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfr_prediction[:2], validation_y[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nmse = mean_squared_error(validation_y,rfr_prediction)\nlin_rmse = np.sqrt(mse)\nlin_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Better Evaluation Using Cross-Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(rfr, validation_x_prepared, validation_y, cv=10, scoring=\"neg_mean_squared_error\")\nrmse_scores = np.sqrt(-scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores.std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Previously we used RandomForestRegressor, now lets try various other algorithms and find out which model performs better on our data. Then we pick that model and do some fine tuning.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\n\nmodels=[RandomForestRegressor(),AdaBoostRegressor(),BaggingRegressor(),SVR(),KNeighborsRegressor()]\nmodel_names=['RandomForestRegressor','AdaBoostRegressor','BaggingRegressor','SVR','KNeighborsRegressor']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_x = strat_validation_set.drop(['count'],axis=1)\nvalidation_y = strat_validation_set['count'].copy()\nnum_attribs = list(validation_x)\ncat_attribs = ['season','weather']\ndrop_cols = ['datetime','registered','casual','atemp','holiday']\n\nrmses=[]\nstds = []\nd={}\ntrained_models = []\nfor model in models:\n    model.fit(train_prepared,train_Y)\n    trained_models.append(model)\n    validation_x_prepared = cat_pipeline.transform(validation_x)\n    prediction = model.predict(validation_x_prepared)\n    # validation\n    scores = cross_val_score(model, validation_x_prepared, validation_y, cv=10, scoring=\"neg_mean_squared_error\")\n    rmse_scores = np.sqrt(-scores)\n    rmses.append(rmse_scores.mean())\n    stds.append(rmse_scores.std())\n    \nd = {\"Models\": model_names, \"mean rsme score\": rmses, \"Standard deviation\": stds}\ndf = pa.DataFrame(d)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can pick a model which has lower mean rsme score and it good to consider the standard deviation as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['mean rsme score'].idxmin()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RandomForestRegressor has performed better on our data than the other algorithms. So lets proceed with RandomForestRegressor with little fine tuning. "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = trained_models[0]\nmodel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Regressor performs better\nLets Fine-Tune this Model. We could find best parameters for our models using GridSearchCV or RandomizedSearchCV. Lets try both."},{"metadata":{},"cell_type":"markdown","source":"# Parameter Tuning"},{"metadata":{},"cell_type":"markdown","source":"#### Find specified parameter values for our model using GridSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators': [3,10,20,30]}\n]\n\nrfr = RandomForestRegressor(random_state=42)\ngrid_search = GridSearchCV(rfr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(train_prepared,train_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above results we can see with 'n_estimators': 30 the msre is 43.468935429069326. We could try increasing the n_estimators in multiples of 10 and check."},{"metadata":{},"cell_type":"markdown","source":"#### Now finding specified parameter values for our model using RandomizedSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n    'n_estimators': randint(1,200)\n}\n\nrfr = RandomForestRegressor(random_state=42)\nrandom_search = RandomizedSearchCV(rfr, param_distributions=param_distribs,cv=10, scoring='neg_mean_squared_error')\nrandom_search.fit(train_prepared,train_Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvres = random_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'],cvres['params']):\n    print(np.sqrt(-mean_score), params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With RandomizedSearchCV we have identified a precise n_estimators."},{"metadata":{},"cell_type":"markdown","source":"# Evaluate Your System on the Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = load_csv_data(DATA_DIR_PATH,TEST_FILE)\ntest_data.columns\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_x.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We are adding two colums for the sake of transformers since we had \"casual,registered\" columns in train set we expect text set also to have the same number of columns. "},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = random_search.best_estimator_\n\nX_test = test_data.copy()\nX_test['casual'] = 0\nX_test['registered'] = 0\n\nnum_attribs = list(X_test)\n\nX_prepared = cat_pipeline.transform(X_test)\nprediction = final_model.predict(X_prepared)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(prediction),len(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(prediction[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out_dict = {\"date\":test_data.datetime, \"count\" : prediction }\nout_dict_df = pa.DataFrame(out_dict)\nout_dict_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## save to csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"# out_dict_df.to_csv('../input/bike-sharing-demand/bike-sharing-demand-estimation.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ** Please share your valuable comments and suggestions. Thank you! **"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}