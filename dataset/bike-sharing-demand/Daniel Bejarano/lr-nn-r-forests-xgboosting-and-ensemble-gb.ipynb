{"cells":[{"metadata":{"_uuid":"81611031c7cb17549a1347ff790ee66f9ffd19b5"},"cell_type":"markdown","source":"## BIKE SHARING DEMAND COMPETITION\n\n#### Source: Kaggle | Date: 11/26/18 \n\n#### Overview\nPredict Bike Sharing Demand on days 20th-to-end-of-month using hourly training data for days 1-19 of the month. **Only information available prior to the rental period can be used to predict demand.**"},{"metadata":{"_uuid":"e529ae8487d8504831a1ef2299f49d72ff1eb094"},"cell_type":"markdown","source":"## 1. Load Libraries and Datasets"},{"metadata":{"_uuid":"8275029487b5f2c6d4345922e73d87194370ff64"},"cell_type":"markdown","source":"The following libraries will be used throughout the Exploratory Data Analysis (EDA) and Predictive Data Analysis (PDA)"},{"metadata":{"trusted":false,"_uuid":"4c083f04da85b2f3430130469b639df64e23acc8"},"cell_type":"code","source":"# General\nimport warnings\nfrom datetime import datetime\nimport os\n\n# Data Libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Linear Regression\nimport statsmodels.api as sm\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_log_error\nimport statsmodels.formula.api as smf\n\n# Neural Networks\nfrom keras.models import Model\nfrom keras import Input, layers\n\n# Random Forests\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\n\n# XGBoost\nimport xgboost as xg\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n%matplotlib inline \n#to show images within Jupyter Notebook without having to call \"show()\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6c405dfc43d2d452c2e08d98a3694d1f1ddc667"},"cell_type":"markdown","source":"#### Load train datasets and sample submission file already saved in directory"},{"metadata":{"trusted":true,"_uuid":"fd341fd4a864a2f4200bf60d75db149b70f96556"},"cell_type":"code","source":"# Load Datasets from Computer\n# train_df = pd.read_csv('train.csv')\n# test_df = pd.read_csv('test.csv')\n# sample_submission = pd.read_csv('sampleSubmission.csv')\n\n# Load Datasets from Kaggle Kernel\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nsample_submission = pd.read_csv(\"../input/sampleSubmission.csv\")\n\n# Visualize Datasets as Tables\ndisplay(train_df.head(3))\nprint('Training Set Size: ', train_df.shape)\ndisplay(test_df.head(3))\nprint('Test Set Size: ', test_df.shape)\ndisplay(sample_submission.head(3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e9f25c9a3bbb47b1860e82c1339ef42a6a2091a"},"cell_type":"markdown","source":"### Datasets Description\n\n#### Features:\n- datetime - hourly date & timestamp <br>\n- season - 1 = spring, 2 = summer, 3 = fall, 4 = winter <br>\n- holiday - 1 = holiday, 0 = non-holiday<br>\n- workingday - 1 = working day, 0 = weekend <br>\n- weather\n    1. Clear, Few clouds, Partly cloudy, Partly cloudy\n    2. Mist & Cloudy, Mist & Broken clouds, Mist & Few clouds, Mist\n    3. Light Snow, Light Rain & Thunderstorm & Scattered clouds, Light Rain & Scattered clouds\n    4. Heavy Rain & Ice Pallets & Thunderstorm & Mist, Snow & Fog\n<br>\n- temp - temperature in Celsius<br>\n- atemp - \"feels like\" temperature in Celsius<br>\n- humidity - relative humidity<br>\n- windspeed - wind speed<br>\n- casual - number of non-registered user rentals initiated<br>\n- registered - number of registered user rentals initiated<br>\n- count - number of total rentals (Dependent Variable)<br>\n\n#### Training Set:\nContains 10,886 observations of 12 variables. The dependent variable (what we will be predicting) is the feature Count. It contains hourly observations for days 1-19 of every month from 2011-01-01 to 2012-12-19.\n\n#### Test Set:\nContains 6,493 observations of 9 variables. It includes hourly observations for days 20-end_of_month from 2011-01-20 to 2012-12-31.\n\nIt does not include the Casual and Registered features (divides renters into registered and non-registered users). Since we won't be able to use them to predict on the testing set, and since Count is lineraly dependent on them (Count = Casual + Registered) we will be dropping them as they won't be of much help in our predictions.\n\n#### Sample Submission:\nWe will be predicting the count of bicycles rented per hour on the test set."},{"metadata":{"_uuid":"a28b2032c7c0e581e0a0fbe4856c479711eb1118"},"cell_type":"markdown","source":"### Data Types\nLet's look at what type of variables we are dealing with in our datasets. \n\nWe notice for the most part is numeric. Some of those numeric variables are categorical ones represented as integers: Season, Holiday, Workingday and Weather"},{"metadata":{"trusted":false,"_uuid":"6e9cbf54024a1bb747fd9b937a9948b803628521"},"cell_type":"code","source":"print(train_df.info())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74d25650b4031508656a9502f29ed56d7d3c64d8"},"cell_type":"markdown","source":"## 2. Exploratory Data Analysis (EDA) \n\nLet's first look at a pairplot of the variables to get a feel for what the dataset contains. I already show it with Casual and Registered features dropped. Also, Atemp is very highly correlated w/ Temp, so it was dropped as well to avoid multicolinearity.\n\n**REMEMBER that ONLY the training set should be used during EDA. This prevents us from learning information about the test set that could bias us to reach particular conclusions that are specific to the test set and would not necessarily generalize well.**"},{"metadata":{"_uuid":"b5c77954f7cb8728b6e6cafbaadcbefe5a02f5f5"},"cell_type":"markdown","source":"### Feature Engineering - Part 1"},{"metadata":{"trusted":false,"_uuid":"8c78a2ef7b7c5dfe5a15c8027683ca3687ee7828"},"cell_type":"code","source":"undesired_feat1 = ['casual', 'registered', 'atemp']\ntrain_df.drop(undesired_feat1, inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38cdea365f616fdea662817f9c4af90c235bd9e4"},"cell_type":"markdown","source":"### Visualization - Pair Plots"},{"metadata":{"trusted":false,"_uuid":"af583da124582a795e18256f5c2ce6ef313dac82"},"cell_type":"code","source":"sns.set()\nsns.pairplot(train_df, hue='weather')\nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da067432070c0dbbf94b1486d99721a55670bf84"},"cell_type":"markdown","source":"#### Analysis:\n\n1. Count is skewed right. Since a good amount of statistical models assume a Gaussian distribution we will try a some transformations to normalize the data\n2. Temp and Winspeed seem to have a positive and negative correlation with Count, respectively.\n3. There is not such a considerable difference between workinday and weekday ridership. It does seem to depend on the weather, where most of the cloudy day ridership occurs on the weekends and holidays.\n\nOne thing that is not present in our pairs plot is Datetime. Let's extract its components into new columns"},{"metadata":{"_uuid":"bdec0e7ee35dbb0c15e9c76920061e943fce3b76"},"cell_type":"markdown","source":"### Feature Engineering - Part 2"},{"metadata":{"trusted":false,"_uuid":"2c6456ad0b63bf125995707a584fc656dfb1f6ac"},"cell_type":"code","source":"# Change Datetime to date and time values and make datetime the axis\ntrain_df['datetime'] = train_df['datetime'].apply(lambda x:datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n\n# Add columns for month, hour, day and year\ntrain_df['year'] = train_df.datetime.apply(lambda x: x.year)\ntrain_df['month'] = train_df.datetime.apply(lambda x: x.month)\ntrain_df['day'] = train_df.datetime.apply(lambda x: x.day)\ntrain_df['hour'] = train_df.datetime.apply(lambda x: x.hour)\ntrain_df['weekday'] = train_df.datetime.apply(lambda x: x.weekday())\n\n# Set datetime as the index\ntrain_df.set_index('datetime', inplace=True)\n\ntrain_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24aa799a1ddd28d9e2208690229ce7ac022285f7"},"cell_type":"markdown","source":"### Visualization of Count\nGet an understanding of how Count relates to other variables"},{"metadata":{"_uuid":"8a0e6133bf66db9552d9ac53f5c5a797c7869d1f"},"cell_type":"markdown","source":"#### Rolling Sum Over 24 Hours"},{"metadata":{"trusted":false,"_uuid":"282b07d22883bb59a851d4394c8b7d2e3164f8bf"},"cell_type":"code","source":"fig,ax = plt.subplots()\nfig.set_size_inches(12,5)\nsns.scatterplot(x = train_df.index, y = train_df['count'].rolling(24).sum())\nax.set(ylabel='Count Rolling Sum', xlabel='Time', title='Ridership Rolling Sum Over 24 Hours')\nax.set_xlim(train_df.index.min(), train_df.index.max())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1f1be8e6843f6cd089b268709857b1ecf44dd94"},"cell_type":"markdown","source":"#### Analysis:\nCount cycles during each month, as well as by season, but it also shows a clear increasing tendency overall with time."},{"metadata":{"_uuid":"5bd8874cd4b08925f014c9d87ade147e76e4ddac"},"cell_type":"markdown","source":"#### Count Outliers"},{"metadata":{"trusted":false,"_uuid":"8d25a691c95f2c6f0897760ac258785a660f6abb"},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=3,ncols=2)\nfig.set_size_inches(14, 15)\n\n# As a function of time of day\nsns.boxplot('temp', 'count', data=train_df, ax=axes[0][0])\naxes[0][0].set(ylabel='Count', xlabel='Temp', title='Count vs Temperature')\nfor item in axes[0][0].get_xticklabels():\n    item.set_rotation(90)\n    \n# As a function of month\nsns.boxplot('month', 'count', data=train_df, ax=axes[0][1])\naxes[0][1].set(ylabel='Count', xlabel='Month', title='Count vs Month')\n\n# As a function of hour in the day\nsns.boxplot('hour', 'count', data=train_df, ax=axes[1][0])\naxes[1][0].set(ylabel='Count', xlabel='Time of Day', title='Count vs Time-of-Day')\n\n# As a function of the day of the month\nsum_count_per_hour = pd.DataFrame(train_df.groupby('hour')['count'].sum()).reset_index()\nsns.lineplot(x='hour', y='count', data=sum_count_per_hour, ax=axes[1][1])\naxes[1][1].set(ylabel='Sum of Count', xlabel='Time of Day', title='Count vs Sum of Count Per Time of Day')\n\n# As a function of day of the week\nsns.boxplot(x='weekday', y='count', data=train_df, ax=axes[2][0])\naxes[2][0].set(ylabel='Count', xlabel='Weekday', title='Count vs Weather')\n\n# As a function of weather\nsns.boxplot('weather', 'count', data=train_df, ax=axes[2][1])\naxes[2][1].set(ylabel='Count', xlabel='Weather', title='Count vs Weather')\n\nplt.tight_layout() # to leave some space between graphs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4fc6def98397e3ca0457f0327650fe1178d0c89"},"cell_type":"markdown","source":"#### Analysis:\n\n1. Rentals show a tendency to increase with higher temperatures, up to about 36 deegres-C.\n2. More rentals occur during the Spring and Summer months.\n3. From the features presented above, the one offering the largest variance amongst its values is Hour. It can be inferred that most rentals are for people to commute to work since the average rentals at 8 AM and 5 PM are the highest. The line plot, however, does show that the cummulative rentals between 11 AM and 8 PM are high enough to conclude that there is considerable non-commuting renting throughout the day, but it's less predictable, as can be seen by the large number of outliers between 10 AM and 3 PM.\n4. On average, rentals are faily uniform from one day of the week to the next.\n4. One assumption we could make is that a significant amount of users have alternative commmuniting options, since commuting is a big part of rentals, and the average Count drops considerably from good to bad weather. We will explore this later. \n5. Based on the amount of outliers, it might make sense to remove the most extreme values."},{"metadata":{"_uuid":"7f5f2b29cd79e06cca17485bdb284479d973f418"},"cell_type":"markdown","source":"Let's see if the same outliers and distributions can be observed by re-scaling Count"},{"metadata":{"_uuid":"00d2418b09921084cfe3cc9f5684143780e2eb94"},"cell_type":"markdown","source":"#### Count Distribution\n"},{"metadata":{"trusted":false,"_uuid":"51f14fdbe77f1d4c6fd2b0396f9b77e74843d300"},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2)\nfig.set_size_inches(14, 5)\n\nsns.distplot(train_df['count'], ax=axes[0])\naxes[0].set(title='Original Distribution')\n\n# Define Data Variations\nlog_count = np.log(train_df['count'])\nx_p5 = np.sqrt(train_df['count'])\n\nsns.distplot(log_count, ax=axes[1])\naxes[1].set(title='Modified Distribution')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f839e6d8e7ac5759ec3a0fe28dc8e2659f0b194d"},"cell_type":"markdown","source":"#### Analysis:\nThe data is skewed to the right. After attempting several transformations (such as x^1/2, x^1/3) a log transformation provides a distribution that looks fairly Gaussian in comparison w/ the original"},{"metadata":{"_uuid":"ea931064738d7ec25e315eba7675331408d482a1"},"cell_type":"markdown","source":"### Feature Engineering - Part 3: Outliers and Data Transformation\n\nThis will consist of two parts:\n1. Remove outliers - We will define outliers as those points beyond the 99 percentile, which is 3 standard deviations from the mean.\n2. Substitute Count with its log transformation: log(count).\n\nOne needs to be cautious when removing outliers. Just because it's an extreme value doesn't mean it's an error or that it's not informative. When doing data INFERENCE, we seek to understand the dependent and independent variables as much as possible, and their relationship. When the focus is PREDICTION, the goal is developing a model that predicts accurately on unseen data. In this case our interest is a little on the former, but more so on the latter, so we will proceed to test our model with and without outliers. \n\nTransformations may not always provide benefits either, so we will try fitting a model with and without the data transformation as well. "},{"metadata":{"trusted":false,"_uuid":"4d0c255c311e3c0ee98715b1cbd15ab34db61c6d"},"cell_type":"code","source":"# Keep both log_count and count in dataset to be able to go back and forth for intepretability\ntrain_df['log_count'] = np.log(train_df['count']) \n\n# Remove Outliers\ncount_mean = np.mean(train_df['count'])\ncount_std = np.std(train_df['count'])\nthree_std = count_mean + 3*count_std\n\ntrain_df = train_df[train_df['count'] < three_std]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4563c03f0ce688adcfca2dd7c5d503c91fb40dce"},"cell_type":"markdown","source":"### Correlation Analysis\n\nTo make our colinearity (when two or more predictors are closely related to each other) analysis more robust, we create and examine a correlation matrix."},{"metadata":{"trusted":false,"_uuid":"028b729343bc6f78936f377f2f80d908e13eeddb"},"cell_type":"code","source":"mask = np.array(train_df.corr())\nmask[np.tril_indices_from(mask)] = False\nfig,ax= plt.subplots()\nfig.set_size_inches(17,10)\nsns.heatmap(train_df.corr(), mask=mask, square=True,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"840be73b83cf72023a1cb2d4ce8e0dbfee1e3bf0"},"cell_type":"markdown","source":"#### Deductions\nThere are only a few variables with high correlation. Some are not very interesting, like Month being highly correlated with Season, or Humidity being correlated with Weather.\n\nOthers, however, might be less obvious and more relevant. First, Count correlates at 0.39 with Temp. Another is how Count correlates with Hour. Even more surprising is that Log_count is highly correlated with Hour. It looks like the data transformation may have turned some of these predictors into more powerful ones. "},{"metadata":{"_uuid":"0004e66825b6e5d88b355971b44c20b53167c145"},"cell_type":"markdown","source":"### Some Remaining Questions\n\nSome questions that came up to me during the analysis done so far:\n\n1. How does Count vary depending on the day of the week?\n2. Are there times during the year when more percentage of the bike rentals are on a workday vs the weekend?\n3. How much does Temp affect Count throughout the day?\n    - To answer this question we need to convert Temp into a categorical variable: We into 4 segments: 0-10 ,11-20, 21-30, 31-max\n4. How would each of this differ between Count and Log_Count?\n    - The answer to this question is that the relative change between weather types, workingday, and Temp remain after transforming the data. Being that Count is easier to interpret than Log_Count, we will only plot Count. \n\nLet's plot those relationships mentioned above to come up with some insights."},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"30f7ff4444f1452ff27aa78c7cfabb78d659687b"},"cell_type":"code","source":"fig,axes = plt.subplots(nrows=3, ncols=1)\nfig.set_size_inches(15, 18)\n\n# Question 1:\n# 0=Monday, 6=Sunday\nday_of_week = train_df.groupby(['weekday', 'hour'], sort=True).mean().reset_index()\nsns.pointplot(x='hour', y='count', data=day_of_week, hue='weekday', join=True, ax=axes[0], scale=1.5, palette='husl')\naxes[0].set(xlabel='Time of the Day', ylabel='Count', title='Count vs Hour, Separated by Weather Type')\n\n# Question 2:\nyearly_workingday = train_df.groupby(['workingday', 'month'], sort=True).mean().reset_index()\nsns.pointplot(x='month', y='count', data=yearly_workingday, hue='workingday', join=True, ax=axes[1], scale=1.5, palette='husl')\naxes[1].set(xlabel='Month', ylabel='Count', title='Count vs Month, Separated by Work vs. Weekend Day')\n\n# Question 3: \n# cat_temp = pd.cut(train_df['temp'], 4, labels=[\"cold\", \"cool\", \"warm\", \"hot\"])\n# train_df['cat_temp'] = cat_temp\nhourly_temp = train_df.groupby(['weather', 'hour'], sort=True).mean().reset_index()\nsns.pointplot(x='hour', y='count', data=train_df, hue='weather', join=True, ax=axes[2], scale=1.5, palette='husl')\naxes[2].set(xlabel='Time of the Day', ylabel='Count', title='Count vs Hour, Separated by Temp Type')\n\nplt.tight_layout() # to leave some space between graphs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"720849b25a3c3a5e72c7cc2fade70ce02fe9a34d"},"cell_type":"markdown","source":"#### Analysis:\n1. As previously observed, during the week most of the rentals are customers commuting to/from work. The weekend shows high rentals throghout mid-day and into the afternoon.\n2. Hourly rentals vary on different months of the year, but they are not very different between working and non-working days. We do see a trend that more \"recreational\" rides (non-working days) occur during the summer when compared to workingday rentals. The opposite is true during the winter.\n3. Weather seems to affect rentals uniformly regardless of the time of the day. The extreme cases are when rentals are very low and when they are very high. The gaps between good and bad weather rentals are smaller/larger at those points in time, respectively."},{"metadata":{"_uuid":"ebc6ab06337b0d9d5ff6beff6b0568da7ae1d4e2"},"cell_type":"markdown","source":"**The EDA above should be sufficient to give us an idea of how variables relate to each other, especially how Count relates to the rest. This information is highly important when choosing the model, doing feature selection or any other type of feature engineering, and when analyzing the results from our models.**\n\n**Before we move on to modeling, let's define functions for pre-processing data to get it in the appropriate form for our models. We will use functions since we might want to use different pre-processing steps for different models, and since we will do this both for our train and test sets. Functions will therefore save time and avoid confusion.**"},{"metadata":{"_uuid":"9dda7fefd9c84797286eb548d837539de126f276"},"cell_type":"markdown","source":"## 3. Data Preprocessing\n\n### Define functions to Preprocess Data"},{"metadata":{"trusted":false,"_uuid":"f4c5667867e9c9c53fb7b6ec836210e003406267"},"cell_type":"code","source":"def load_datasets(file_names):\n    '''loads a list of files as pandas dataframes'''\n    files_list = []\n    for i in file_names:\n        df_i = pd.read_csv(\"../input/{}\".format(i))\n        files_list.append(df_i)\n    return(files_list)\n\ndef drop_features(df, features):\n    '''Drop specified list of features'''\n    df.drop(features, inplace=True, axis=1)\n    return(df)\n\ndef process_datetime(df):\n    '''Change Datetime to date and time values, set those values as columns, and\n    set original datetime column as index'''\n    df['datetime'] = df['datetime'].apply(lambda x:datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n    # Add columns for month, hour, day and year\n    df['year'] = df.datetime.apply(lambda x: x.year)\n    df['month'] = df.datetime.apply(lambda x: x.month)\n    df['day'] = df.datetime.apply(lambda x: x.day)\n    df['hour'] = df.datetime.apply(lambda x: x.hour)\n    df['weekday'] = df.datetime.apply(lambda x: x.weekday())\n    df.set_index('datetime', inplace=True)\n    return(df)\n\ndef log_transform(df):\n    '''Keep both log_count and count in dataset to be able to go back and \n    forth for intepretability'''\n    df['count'] = np.log(df['count']) \n    return(df)\n\ndef remove_outliers(df):\n    '''Remove Outliers'''\n    count_mean = np.mean(df['count'])\n    count_std = np.std(df['count'])\n    three_std = count_mean + 3*count_std\n    df = df[train_df['count'] < three_std]\n    return(df)\n\ndef norm_scale(df, cont_features):\n    '''Normalize and Scale Continuous Features'''\n    df[cont_features] -= np.mean(df[cont_features])\n    df[cont_features] /= np.std(df[cont_features])\n    return(df)\n\ndef cat_into_one_hot(df, cat_features):\n    '''Takes list of categorical features and turns them into One-Hot Encoding'''\n    df = pd.get_dummies(df, columns=cat_features, drop_first=True)\n    return(df)\n\ndef split_train_val(df, val_size):\n    '''Separates the training set into a training and a validation set\n    val_size should be a number between 0 and 1'''\n    split_at = int(df.shape[0] // (1/(1-val_size)))\n    df_train = df[:split_at][:]\n    df_val = df[split_at:][:]\n    return(df_train, df_val)\n    \ndef split_x_y(df_train, df_val):\n    '''Takes a training set and a validation set and returns X_train, y_train, X_val, y_val'''\n    x_train_df = df_train.drop('count', axis=1)\n    y_train_df = df_train['count']\n    x_val_df = df_val.drop('count', axis=1)\n    y_val_df = df_val['count']\n    return(x_train_df, x_val_df, y_train_df, y_val_df)\n\ndef exp_transform(df):\n    '''To convert Count back to non-log values'''\n    if type(df) == pd.core.series.Series:\n        df = np.exp(df)\n    elif type(df) == np.ndarray:\n        df = np.exp(df)\n    else:\n        df['count'] = np.exp(df['count'])\n    return(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed089bbf4298a45299ee74931224bc71783ed89b"},"cell_type":"markdown","source":"Let's load and preprocess the data. The end result will be what we walked through on the EDA section above, with 3 major additions: \n1. All categorical variables (those which can only take on discrete values, whether they come in numerical or string form) will be converted to one-hot-encodings. This is highly recommended for certain ML algorithms because, say, for Season, a value of 4 does not mean anything in relation to a value of 1 other than the fact they are two different categories. The fact that winter is 4 does not mean it's 4 times larger than spring. \n2. We will perform normalization and standardization of features before inputting them into some of the models. Neural Networks, for instance, are particularly sensitive to features in different scales. This is reasonable the features are normally distributed, which in this case it's not true for all.\n3. Our training dataset will be split into X_train, X_val, y_train and y_val."},{"metadata":{"_uuid":"fbf69e13259ae67587b78f363d32dddc21ae5959"},"cell_type":"markdown","source":"### Data Preprocessing for train and test sets\nThese functions should take the input data and transform it to a form that can be fed into the various predictive models we will fit."},{"metadata":{"trusted":false,"_uuid":"dd07d3bcf71861b1c07fdd2a3db94d5db6de2d72"},"cell_type":"code","source":"# Load Datasets\ntrain_df, test_df = load_datasets(['train.csv', 'test.csv'])\n\n# Drop Uninportant Features\ntrain_df = drop_features(train_df, ['casual', 'registered', 'atemp'])\ntest_df = drop_features(test_df, 'atemp')\n\n# Process Datetime\ntrain_df = process_datetime(train_df)\ntest_df = process_datetime(test_df)\n\n# # Remove Outliers - Only for Training Data\n# train_df = remove_outliers(train_df)\n\n# Normalize and Standardize Continuous Variables\ncont_features = ['temp', 'humidity', 'windspeed']\ntrain_df = norm_scale(train_df, cont_features)\ntest_df = norm_scale(test_df, cont_features)\n\n# Transform Data to Log - Only for Training Data\ntrain_df = log_transform(train_df)\n\n# Change Categorical into One_Hot - 'day' is not included bc it's different between train & test\ncat_features = ['season', 'holiday', 'workingday', 'weather', 'month', 'hour', 'weekday']\ntrain_df = cat_into_one_hot(train_df, cat_features)\ntest_df = cat_into_one_hot(test_df, cat_features)\n\n# Split into train train and validation sets\ntrain_train_df, train_val_df = split_train_val(train_df, 0.2)\n\n# Split into X and y\nX_train, X_val, y_train, y_val = split_x_y(train_train_df, train_val_df)\n\n# Check datasets shapes\nprint('We check the datasets shapes to ensure our pre-processing function did its job correctly: ', '\\n')\nprint('X_train Shape: ', X_train.shape)\nprint('X_val Shape: ', X_val.shape)\nprint('y_train Shape: ', y_train.shape)\nprint('y_val Shape: ', y_val.shape)\nprint('Test Set Shape: ', test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"822989bd84381e1ea65be7ad8b44095fb23dc584"},"cell_type":"markdown","source":"## 4. Predictive Data Analytics (PDA)"},{"metadata":{"_uuid":"06c73303d12f7a6107f09221d6a65d313f69fcd0"},"cell_type":"markdown","source":"### Model A: Liner Regression"},{"metadata":{"_uuid":"1fbcf079e3c8a86e0bfd32b50d04eae90c24a164"},"cell_type":"markdown","source":"We will make copies of the datasets in case we want to make some alterations, like dropping features that are not predictive enough. This way we can quickly test performance of the model on differently featured engineered sets.\n"},{"metadata":{"trusted":false,"_uuid":"a0618da90815a9c0be1745aca9bddf1e24637128"},"cell_type":"code","source":"X_train_lr = X_train.copy()\ny_train_lr = y_train.copy()\nX_val_lr = X_val.copy()\ny_val_lr = y_val.copy()\n\nfeats_to_drop = ['weather_4', 'month_4', 'weekday_1', 'weekday_2']\nX_train_lr.drop(feats_to_drop, inplace=True, axis=1)\nX_val_lr.drop(feats_to_drop, inplace=True, axis=1)\n\nX_train_lr_intercept = sm.add_constant(X_train_lr, has_constant='add') # this adds an intercept term column\nest = sm.OLS(y_train_lr, X_train_lr_intercept) \nest2 = est.fit()\nX_val_lr_intercept = sm.add_constant(X_val_lr, has_constant='add')\npred_lr = est2.predict(X_val_lr_intercept)\nprint(est2.summary())\n\n# Convert back from Log_Count to Count\npred_lr = exp_transform(pred_lr)\ny_val_lr = exp_transform(y_val_lr)\n\n# Evaluate Model\npred_lr[pred_lr<0] = 0\nmsle = np.sqrt(mean_squared_log_error(y_val_lr, pred_lr))\nprint('MSLE: ', msle)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a347e05e224a75910358006167e40b10d1104b38"},"cell_type":"markdown","source":"#### Comments on how to interpret results:\nYou should not be able to predict the errors (there should not be observable trends). If your model did a good job at explaining/predicting the response, then the error left would be stochastic (the portion of the error that is inherent to real life randomness. \n\nAlso, you should not be able to predict residuals based off another variable. If so, then that variable should be included in your model. Lastly, adjacent residuals should not be correlated with eachother. This is called autocorrelation and means the deterministic portion of your model is not capturing that information (often found in time series).\n\nLet's take a look at a residuals plot and determine whether a linear model that predicts the Log_Count is better or worse than predicting Count without any log transformations."},{"metadata":{"_uuid":"0d061bf0ede17b3102e1a337fc031f468313c9fb"},"cell_type":"markdown","source":"#### Plot Residuals vs Predictions - WITH log transformation of Count"},{"metadata":{"trusted":false,"_uuid":"b9da3b9d0b1835e1c0500f3b63f954039d94c1b3"},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2)\nfig.set_size_inches(12, 5)\nsns.distplot(pred_lr, ax=axes[0])\naxes[0].set(xlabel='Predicted Count', ylabel='P(count)', title='Val Set Predictions Dist' )\n\nresiduals = (y_val_lr - pred_lr)\nsns.scatterplot(x=pred_lr, y=residuals, ax=axes[1])\naxes[1].set(xlabel='Predicted Count', ylabel='Residuals', title='Residuals vs Predictions')\n\nprint('LINEAR REGRESSION WITH LOG TRANSFORMATION ON DATA')\nprint('MSLE: ', msle)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e75ba7452c7eac89db154ec2dc72f2514155606d"},"cell_type":"markdown","source":"#### Plot Residuals vs Predictions - WITHOUT log transformation of Count"},{"metadata":{"trusted":false,"_uuid":"5ed92a7d02de57e503282136d1d38e60805a1cd5"},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2)\nfig.set_size_inches(12, 5)\nsns.distplot(pred_lr, ax=axes[0])\naxes[0].set(xlabel='Predicted Count', ylabel='P(count)', title='Val Set Predictions Dist' )\n\nresiduals = (y_val_lr - pred_lr)\nsns.scatterplot(x=pred_lr, y=residuals, ax=axes[1])\naxes[1].set(xlabel='Predicted Count', ylabel='Residuals', title='Residuals vs Predictions')\n\nprint('LINEAR REGRESSION WITHOUT LOG TRANSFORMATION ON DATA')\nprint('MSLE:', msle)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3dcde29762a1144f00080f94587c7ed1ca43f3b3"},"cell_type":"markdown","source":"#### Analysis:\n\nThe following variations of the input data generated these results:\n- Using Count instead of Log-Count provides a score of 0.99, which is almost double than the 0.56 obtained by fitting a model that predicts the log of Count. \n- Removing outliers negatively impacted the model performance. Therefore, we will keep outliers.\n- Normalizing and Standardizing had no effect on the model. We will keep the original values.\n- Not converting categorical variables into one-hot vectors doubled the error rate.\n\nAfter choosing log(count) as the better independent variable:\n1. The linear model does a good job at explaining the response variable Count, with an Adjusted R^2 score of 0.825.\n2. Dropping those features with low predictive power (high p-values) did not result in any changes to the error rate or R^2 score, so we will keep them for simplicity.\n3. The error score from regularized models (both L1 and L2) came up to be higher than with no regularization, so we will keep the unregularized model. \n\n    \n**Note: Some of the predictions were negative. In reality we can't have negative bike rentals, so these values were substituted by zeros.**"},{"metadata":{"_uuid":"add5b6247fe2f1f7fc4e4aa3298bb26ac00c714e"},"cell_type":"markdown","source":"#### From looking at the \"Residuals vs Predictions\" plots, we observe discernable patterns. This means not all of the non-random portion of the error is being captured. Let's try some non-linear models next and perform a similar analysis as above."},{"metadata":{"_uuid":"f5e2b1d75f9eb1dafed39a88315527e147d16afa"},"cell_type":"markdown","source":"### Model B: Neural Network"},{"metadata":{"trusted":false,"_uuid":"a1f170b54922aef9ee8c537251877ab3ab2e1ffc"},"cell_type":"code","source":"# # Build Network\ninput_tensor = Input(shape=(X_train.shape[1],))\nx = layers.Dense(80, activation='relu')(input_tensor)\nx = layers.Dense(80, activation='relu')(x)\noutput_tensor = layers.Dense(1)(x)\nnn_model = Model(input_tensor, output_tensor)\nnn_model.summary()\n\n# Compile and Fit\nnn_model.compile(optimizer='rmsprop', loss='msle')\nhistory = nn_model.fit(X_train, y_train, validation_split=0.2, epochs=10)\npred_nn = nn_model.predict(X_val)\n\n# # Convert back from Log_Count to Count\n# pred = exp_transform(pred)\n# y_val = exp_transform(y_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd6c650b4ccaea957f898991b41d5781e2c43e94"},"cell_type":"markdown","source":"#### Plot Residuals vs Predictions"},{"metadata":{"trusted":false,"_uuid":"875b251a5543cc3d775d421908eec4d5d22b683c"},"cell_type":"code","source":"# Evaluate Model\npred_nn = np.ravel(pred_nn)\npred_nn[pred_nn<0] = 0\nmsle = np.sqrt(mean_squared_log_error(y_val, pred_nn))\n\n# Plot Training and Validation MSLE\nmsle_train = history.history['loss']\nmsle_val = history.history['val_loss']\nepochs = range(1, len(msle_train) + 1)\nplt.figure()\nplt.plot(epochs, msle_train, 'bo', label='Training msle')\nplt.plot(epochs, msle_val, 'b', label='Validation msle')\nplt.title('Training and Validation Loss')\nplt.legend()\n\n# Plot Predictions and Residuals\nfig, axes = plt.subplots(nrows=1, ncols=2)\nfig.set_size_inches(15, 5)\n\nsns.distplot(pred_nn, ax=axes[0])\naxes[0].set(xlabel='Predicted Count', ylabel='P(count)', title='Val Set Predictions Dist' )\n\nresiduals = (y_val - pred_nn)\nsns.scatterplot(x=pred_nn, y=residuals, ax=axes[1])\naxes[1].set(xlabel='Predicted Count', ylabel='Residuals', title='Residuals vs Predictions')\n\nprint('NEURAL NETWORKS MODEL RESULTS')\nprint('MSLE: ', msle)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e7cab2cb8f55564ea6342a87cebe5bf27995eac"},"cell_type":"markdown","source":"**The results from our Neural Network are not great at all. Let's try some other models.**"},{"metadata":{"_uuid":"d06e0febe0d72f85af1a6dee0e6f5caadc93419e"},"cell_type":"markdown","source":"### Model C: Random Forests"},{"metadata":{"trusted":false,"_uuid":"e0ea7ef0a4f983dae7518e77913efc55700113b5"},"cell_type":"code","source":"rfc = RandomForestRegressor(n_estimators = 50)\nrfc.fit(X_train, y_train)\nrfc.feature_importances_\npred_rf = rfc.predict(X_val)\n\n# Evaluate Model on Validation Set\npred_rf.resize(y_val.shape)\nmlse_rf = np.sqrt(mean_squared_log_error(y_val, pred_rf))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7babd309420cc5082e09831313dd42b29ca4e579"},"cell_type":"markdown","source":"#### Plot Residuals vs Predictions"},{"metadata":{"trusted":false,"_uuid":"84d940a3bd9b490ca0d4504a1adac97b71e564df"},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2)\nfig.set_size_inches(12, 5)\nsns.distplot(pred_rf, ax=axes[0])\naxes[0].set(xlabel='Predicted Count', ylabel='P(count)', title='Val Set Predictions Dist' )\n\nresiduals = (y_val - pred_rf)\nsns.scatterplot(x=pred_rf, y=residuals, ax=axes[1])\naxes[1].set(xlabel='Predicted Count', ylabel='Residuals', title='Residuals vs Predictions')\n\nprint('RANDOM FORESTS MODEL RESULTS')\nprint('MSLE: ', msle)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56eef3be765e3e170d3c06c04743479001513379"},"cell_type":"markdown","source":"### Model D: XGRegressor"},{"metadata":{"trusted":false,"_uuid":"266f660e014f7176a5ea2a519e31b7cce04df90a"},"cell_type":"code","source":"xgr_params = [{'max_depth': [8], 'min_child_weight':[4], 'gamma':[0.0001]}] #per previous run\n\nxgr = xg.XGBRegressor()\n\ngrid_xgr = GridSearchCV(xgr, param_grid=xgr_params, cv=5, refit=True, verbose=1)\ngrid_xgr.fit(X_train, y_train)\npred_xgr = grid_xgr.predict(X_val)\n\nbest_score = grid_xgr.best_score_\nbest_params = grid_xgr.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"929c725e1800691f6789f430e137267b3edf0821"},"cell_type":"markdown","source":"#### Plot Residuals vs Predictions"},{"metadata":{"trusted":false,"_uuid":"35dd0bddb834a03fc19a7c9697065a9a0a136e96"},"cell_type":"code","source":"# Evaluate Model on Validation Set\npred_xgr[pred_xgr<0] = 0\nmsle_xgr = np.sqrt(mean_squared_log_error(y_val, pred_xgr))\n\nfig, axes = plt.subplots(nrows=1, ncols=2)\nfig.set_size_inches(12, 5)\nsns.distplot(pred_xgr, ax=axes[0])\naxes[0].set(xlabel='Predicted Count', ylabel='P(count)', title='Val Set Predictions Dist' )\n\nresiduals = (y_val - pred_xgr)\nsns.scatterplot(x=pred_xgr, y=residuals, ax=axes[1])\naxes[1].set(xlabel='Predicted Count', ylabel='Residuals', title='Residuals vs Predictions')\n\nprint('XGBOOST REGRESSOR WITHOUT LOG TRANSFORMATION MODEL RESULTS')\nprint('MSLE: ', msle_xgr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c1ee98aae386a95930c108d43db8dbb9b18a826"},"cell_type":"markdown","source":"### Model E: Ensemble Gradient Boosting"},{"metadata":{"trusted":false,"_uuid":"3568520ff869b594665e8bcb02c79e8766c8e1a0"},"cell_type":"code","source":"egb_params = [{'learning_rate': [0.1], 'max_depth':[6]}] # from previous run\n\negb = GradientBoostingRegressor(n_estimators=50, loss='ls')\n\ngrid_egb = GridSearchCV(egb, param_grid=egb_params, cv=5, refit=True, verbose=1)\ngrid_egb.fit(X_train, y_train)\nbest_score = grid_egb.best_score_\nbest_params = grid_egb.best_params_\nprint(' The Best Score :', best_score, '\\n', 'The Best Params : ', best_params)\npred_egb = grid_egb.predict(X_val)\npred_egb[pred_egb<0] = 0\n\n# Evaluate Model on Validation Set\n# pred.resize(y_val_xg.shape)\nmsle_egb = np.sqrt(mean_squared_log_error(y_val, pred_egb))\nprint('MSLE Score: ', msle_egb)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2aa09e652f4cabf6c3d1c80d37d5b2598d67e7b2"},"cell_type":"markdown","source":"#### Plot Residuals vs Predictions"},{"metadata":{"trusted":false,"_uuid":"a1ff4c162e59a69432e85b0cccdacde3a230bde8"},"cell_type":"code","source":"# Evaluate Model on Validation Set\npred_egb[pred_egb<0] = 0\nmsle_egb = np.sqrt(mean_squared_log_error(y_val, pred_egb))\n\nfig, axes = plt.subplots(nrows=1, ncols=2)\nfig.set_size_inches(12, 5)\nsns.distplot(pred_egb, ax=axes[0])\naxes[0].set(xlabel='Predicted Count', ylabel='P(count)', title='Val Set Predictions Dist' )\n\nresiduals = (y_val - pred_egb)\nsns.scatterplot(x=pred_egb, y=residuals, ax=axes[1])\naxes[1].set(xlabel='Predicted Count', ylabel='Residuals', title='Residuals vs Predictions')\n\nprint('ENSEMBLE GRADIENT BOOSTING WITH LOG TRANSFORMATION MODEL RESULTS')\nprint('MSLE: ', msle_egb)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fa7092c9680b275f8ffe92fac47242ac5ceef1f"},"cell_type":"markdown","source":"### [Extra] Model F: Recurrent Neural Network (RNN) and Additinoal Feature Engineering\n\nI tried a couple of other models just to see what kind of results they would give. They weren't great, but if you're interested here I describe both:\n\n1. **RNN** - A rolling model that used data from the previous 24 hours to predict Count. It incorporated the Count variable from those 24 hours. Therefore, when it got to the 20th day of the month, it took data from day 19. This meant, however, that on day 21 it used data from day 20, which were all predictions obtained previously. The Count predictions ended up being too low. My hypothesis is that the RNN model (which used GRU cells) assigned heavy neuron weights on the Count input from the previous 24 hours because it was a strong predictor, but it got stuck predicting values very similar to the previous Counts. This, as we saw in our EDA, is not a behavior observed on the data.\n2. **Additional Feature** - Similarly to above, I incorporated Count from previous observations and used XGBoost. The model used Count from exactly 24 hours earlier (same time, previous day), but just that one Count value, not a window of 24 observations as in the RNN model above. To predict on the test set it used data from the training set only for day 20. Day 21 used the predictions from day 20, and so on. Therefore it resulted in a similar pattern as the RNN model: relying too heavily on the Count_From_24_Hours_Ago feature and then getting stuck on very similar values. This could be circumvented in various ways. One would be to heavily regulate the weight for that particular input, for instance.\n\n#### With all this in mind, let's move on to the final results..."},{"metadata":{"_uuid":"9ebe11eec505c7a68e4171f0aafa18c530d2bade"},"cell_type":"markdown","source":"## 5. Generate Results and Export Models\nBased on the 7 models attempted, and their set of variations, Linear Regression and Ensemble Gradient Boosting performed best. Therefore, we will submit results for those two models and cross our fingers (that's important, don't forget the cross-fingers step).\n\nIn order to produce the final results, we need to do one last thing: train the models on the entire training set. So far we have been using a subset of it, so that we could use the remaining portion (20%) for validation. So let's train Linear Regression and Ensemble Gradient Boosting on the entire set."},{"metadata":{"trusted":false,"_uuid":"4d96b06e0ff30c553ffe983003dab91fdeaa07c9"},"cell_type":"code","source":"# Concatenate train and validation sets into one\nX_train_all = pd.concat([X_train, X_val], axis=0)\ny_train_all = pd.concat([y_train, y_val], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"397021ca69a2c4e299463f138db2b3b4d6a46c99"},"cell_type":"markdown","source":"### 5.1 Linear Regression on Whole Training Set"},{"metadata":{"trusted":false,"_uuid":"13cfbf19595874dbb0209e0a20d6fe33464036ab"},"cell_type":"code","source":"X_train_lr_all = sm.add_constant(X_train_all, has_constant='add')\ntest_df_lr = sm.add_constant(test_df, has_constant='add')\nest = sm.OLS(y_train_all, X_train_lr_all) \nest2 = est.fit()\npred_lr = est2.predict(test_df_lr)\n\n# Convert back from Log_Count to Count\npred_lr = exp_transform(pred_lr)\npred_lr[pred_lr<0] = 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90ad76e572a62a2c5715710cc61732f2519289db"},"cell_type":"markdown","source":"### 5.2 Ensemble Gradient Boosting on Whole Training Set"},{"metadata":{"trusted":false,"_uuid":"a9737c7aaa7004c73e51b8aae615b2338b211c3f"},"cell_type":"code","source":"egb_params = [{'learning_rate': [0.1], 'max_depth':[6]}] # from previous run\n\negb = GradientBoostingRegressor(n_estimators=250, loss='ls')\ngrid_egb = GridSearchCV(egb, param_grid=egb_params, verbose=1)\ngrid_egb.fit(X_train_all, y_train_all)\n\npred_egb = grid_egb.predict(test_df)\npred_egb = exp_transform(pred_egb)\npred_egb[pred_egb<0] = 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75d6be116503a9cfbfec6e2b74326fac6910dfc5"},"cell_type":"markdown","source":"### 5.3 Save Files with Final Predictions on Test Set"},{"metadata":{"trusted":false,"_uuid":"684c8656a8e0d9d99688da8fc0a4ba1d18cf1ae0"},"cell_type":"code","source":"# # Make a copy of the sample submission file for each model\n# submission_LR = sample_submission.copy()\n# submission_EGB = sample_submission.copy()\n\n# # Add predictions to submission file\n# submission_LR['count'] = np.array(pred_lr)\n# submission_EGB['count'] = pred_egb\n\n# # Save Files\n# submission_LR.to_csv('submission_LR', header=True, index=False)\n# submission_EGB.to_csv('submission_EGB', header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70bfa6dc0af1b0e8cd3d4418e2d6825b53fd1fa4"},"cell_type":"markdown","source":"## Final Results\n\nAfter submitting both models to Kaggle, Ensemble Gradient Boosting with log-transformed Count had the lowest error rate at 0.43. This could be improved further by increasing the number of estimators and exploring other values for its parameters (we tested only a few combinations with GridSearchCV).\n\nA shout out to Vivek Srinivasan and his Kernel \"EDA & Ensemble Model (Top 10 Percentile)\". It provided great insights that helped me structure and improve my EDA. "},{"metadata":{"_uuid":"eb2029f70dc5ea360528dc802eb7322c307cf15a"},"cell_type":"markdown","source":"### Kindly share if you found this useful."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}