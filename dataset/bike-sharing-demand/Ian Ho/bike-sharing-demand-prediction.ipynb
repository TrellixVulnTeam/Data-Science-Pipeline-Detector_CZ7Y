{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Bike Sharing Demand Prediction\n\nAuthor: Ian Ho","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This is a notebook for the Bike Sharing Demand Prediction competition on Kaggle. As with most competitions, there is a need to cover both data processing and modelling to achieve good and robust results. However, this notebook will primarily focus on the modelling and optimization portions, while quickly glossing over the preprocessing portions. Nonetheless, proper pipeline methods will be employed for the preprocessing, and viewers are welcome to try alternative methods to evaluate the effects on test errors. \n\nIn particular, I will be highlighting the use of Bayesian Optimization in finding the optimal hyperparameters for the XGBoost Regressor.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Results","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This notebook took only about 3 hours to complete, but it has a leaderboard error of 0.39 which places it well in the top 10%!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Dependencies","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom xgboost import XGBRegressor\n\nfrom bayes_opt import BayesianOptimization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/bike-sharing-demand/train.csv\")\nsubmit = pd.read_csv(\"/kaggle/input/bike-sharing-demand/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Extracting features from datetime","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to get information about day, week, hour, year from datetime\n\ndef date_extractor(data):\n\n    data['datetime'] = pd.to_datetime(data['datetime'])\n    data[\"date\"] = data[\"datetime\"].apply(lambda x: x.date())\n    data[\"hour\"] = data[\"datetime\"].apply(lambda x: x.hour)\n    data[\"weekday\"] = data[\"datetime\"].apply(lambda x:x.isoweekday())\n    data[\"month\"] = data[\"datetime\"].apply(lambda x:x.month)\n    data[\"year\"] = data[\"datetime\"].apply(lambda x:x.year)\n\n    data.drop(['datetime'], axis=1, inplace=True)\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = date_extractor(train)\nsubmit = date_extractor(submit)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping columns ```casual``` and ```registered``` as they do not appear in the test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_col = ['casual', 'registered']\nfor col in drop_col:\n    train.drop(col, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating pipeline for Data Transformation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\ncategorical_transformer = Pipeline(steps=[('onehotencoder', OneHotEncoder(handle_unknown='ignore'))])\n\nnumeric_features = ['temp', 'atemp', 'humidity', 'windspeed']\ncategorical_features = ['season', 'holiday', 'workingday', 'weather', 'hour', 'weekday', 'month', 'year']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Isolating label column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['count']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping irrelevant columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['date', 'count'], axis=1, inplace=True)\nsubmit.drop(['date'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transforming Data using Pipelines","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = preprocessor.fit_transform(train)\nX_submit = preprocessor.transform(submit)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### User-defined function for RMSLE \n\nRMSLE = Root-mean-squared-log-error\n\n$RMSLE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 }$","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def RMSLE(y_true, y_pred):\n    \n    pairs = list(zip(np.log(y_true+1), np.log(y_pred+1)))\n    \n    return round(np.sqrt(sum(map(lambda x: (x[0] - x[1]) ** 2, pairs)) / len(y_true)), 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modelling Baseline XGBoost Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bayesian Optimization for Hyperparameter Tuning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- In order to use Bayesian Optimization for RMSLE minimization, I customised the ```XGB_error``` to return the negative of RMSLE, so that when Bayesian Optimization tries to maximise this value, it will be minimizing the error. Obviously, this is a quick fix but it works pretty well looking at the iterations!\n\n\n- I decide to search for the following hyperparameters, as defined according to the [XGBoost documentation](https://xgboost.readthedocs.io/en/latest/parameter.html):\n    - ```max_depth``` : Maximum depth of a tree. Increasing this value will make the model more complex and more prone to overfitting\n    - ```min_child_weight``` : This is the minimum sum of instance weight needed in a child. The larger ```min_child_weight``` is, the more conservative the algorithm will be\n    - ```gamma``` : Minimum loss reduction required to make a further partition on a leaf node of the tree\n    - ```colsample_by_tree``` : The subsample ratio of columns when constructing each tree.\n    - ```subsample``` : Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost will randomly sample half of training prior to growing trees, to prevent overfitting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def XGB_error(max_depth, min_child_weight, gamma, colsample_bytree, subsample, X_train, X_test, y_train, y_test):\n    \n    XGB = XGBRegressor(\n        max_depth=max_depth, \n        min_child_weight=min_child_weight, \n        gamma=gamma, \n        colsample_bytree=colsample_bytree, \n        subsample=subsample\n    )\n    \n    y_train = np.log(y_train)\n    XGB.fit(X_train, y_train)\n    y_pred = XGB.predict(X_test)\n    y_pred = np.exp(y_pred)\n    \n    return -RMSLE(y_test, y_pred)\n\n\ndef optimize_XGB(X_train, X_test, y_train, y_test):\n    \n    def XGB_wrapper(max_depth, min_child_weight, gamma, colsample_bytree, subsample):\n        \n        return XGB_error(\n            max_depth=int(max_depth), \n            min_child_weight=min_child_weight, \n            gamma=gamma, \n            colsample_bytree=colsample_bytree, \n            subsample=subsample,\n            X_train=X_train, \n            X_test=X_test, \n            y_train=y_train, \n            y_test=y_test\n        )\n    \n    optimizer = BayesianOptimization(\n        f=XGB_wrapper,\n        pbounds={\n            \"max_depth\": (0, 20), \n            \"min_child_weight\": (1, 10), \n            \"gamma\": (0.2, 0.8), \n            \"colsample_bytree\": (0.2, 0.9), \n            \"subsample\": (0.2, 0.9)\n        },\n        random_state=0,\n        verbose=2\n    )\n        \n    optimizer.maximize(n_iter=50)\n\n    print(\"Final result:\", optimizer.max)\n    print()\n    final_params = optimizer.max['params']\n    for k, v in final_params.items():\n        print(k,'=',v,',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n\noptimize_XGB(X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Retrain with all the data, and with optimal hyperparameters, make submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB = XGBRegressor(\n    colsample_bytree = 0.8420564768063263 ,\n    gamma = 0.3169832518324276 ,\n    max_depth = 6 ,\n    min_child_weight = 8.956069480250676 ,\n    subsample = 0.8715769930248918 ,)\n\ny_logged = np.log(y)\nXGB.fit(X, y_logged)\n\ny_submit = XGB.predict(X_submit)\ny_submit = np.exp(y_submit)\n\nsubmit = pd.read_csv(\"/kaggle/input/bike-sharing-demand/test.csv\")\nsubmission = pd.concat([submit['datetime'], pd.DataFrame(y_submit)], axis=1)\nsubmission.columns = ['datetime', 'count']\nsubmission.set_index('datetime', inplace=True)\nsubmission.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}