{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Bike Sharing Demand\n### Forecast use of a city bikeshare system\n\n(Kaggle competition link: https://www.kaggle.com/c/bike-sharing-demand/overview)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection, linear_model, ensemble, pipeline, preprocessing, metrics\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the train data and test data\nraw_data = pd.read_csv('/kaggle/input/bike-sharing-demand/train.csv')\nraw_val = pd.read_csv('/kaggle/input/bike-sharing-demand/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_val.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Description of data features:\n***datetime*** - hourly date + timestamp  \n\n***season*** -  1 = spring, 2 = summer, 3 = fall, 4 = winter \n\n***holiday*** - whether the day is considered a holiday\n\n***workingday*** - whether the day is neither a weekend nor holiday\n\n***weather*** - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n    \n***temp*** - temperature in Celsius\n\n***atemp*** - \"feels like\" temperature in Celsius\n\n***humidity*** - relative humidity\n\n***windspeed*** - wind speed\n\n***casual*** - number of non-registered user rentals initiated\n\n***registered*** - number of registered user rentals initiated\n\n***count*** - number of total rentals","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data inspection:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(raw_data.shape, raw_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_val.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation \nraw_data.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualusation of correlation \ncorr = raw_data.corr()\nplt.figure(figsize=(11, 9))\nax = sns.heatmap(corr, square=True,annot=True,cbar=True, linewidths=.5)\ni, k = ax.get_ylim()\nax.set_ylim(i+0.5, k-0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preprocessing:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features 'registered', 'casual' and target variable 'count'\n# have a strong linear dependence (a+b=c)\nnp.all(raw_data.registered + raw_data.casual == raw_data['count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting column to datetime type \nraw_data.datetime = raw_data.datetime.apply(pd.to_datetime)\nraw_val.datetime = raw_val.datetime.apply(pd.to_datetime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding new columns - month, hour and year\nraw_data['month'] = raw_data.datetime.apply(lambda x : x.month)\nraw_data['hour'] = raw_data.datetime.apply(lambda x : x.hour)\nraw_data['year'] = raw_data.datetime.apply(lambda x : x.year)\n\nraw_val['month'] = raw_val.datetime.apply(lambda x : x.month)\nraw_val['hour'] = raw_val.datetime.apply(lambda x : x.hour)\nraw_val['year'] = raw_val.datetime.apply(lambda x : x.year)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Delelting of ununnecessary features\nraw_data = raw_data.drop(['datetime', 'casual', 'registered'], axis=1)\nval_data = raw_val.drop(['datetime'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividing to train sample and label sample\ndata_labels = raw_data['count']\ndata = raw_data.drop(['count'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spliting data into random train and test subsets\nX_train, X_test, y_train, y_test = model_selection.train_test_split(data,\n                                                                    data_labels,\n                                                                    test_size=0.1,\n                                                                    random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Noticing colums with different type of features\nbinary_col = ['holiday', 'workingday']\nnumeric_col = ['temp', 'atemp', 'humidity', 'windspeed', 'hour']\ncategor_col = ['season', 'weather', 'month', 'year']\n\n# Function for getting index of columns (for FunctionTransformer)\ndef get_ind(names_col):\n    return [X_train.columns.get_loc(i) for i in names_col]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Encoding, scaling will be in pipelines","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Linear regression (SGD):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choosing a model\nsgd_reg = linear_model.SGDRegressor(random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating pipeline for data transforming and further estimation\nsgd_estimator = pipeline.Pipeline(steps=[\n    ('feature_processing', pipeline.FeatureUnion(transformer_list=[\n        \n        #binary\n        ('binary_variables_processing', preprocessing.FunctionTransformer(lambda data: data.iloc[:, get_ind(binary_col)])),\n        \n        #numeric\n        ('numeric_variables_processing', pipeline.Pipeline(steps=[\n            ('selecting', preprocessing.FunctionTransformer(lambda data: data.iloc[:, get_ind(numeric_col)])),\n            ('scaling', preprocessing.StandardScaler(with_mean=0.))])),\n        \n        #categorical\n        ('categorical_variables_processing', pipeline.Pipeline(steps=[\n            ('selecting', preprocessing.FunctionTransformer(lambda data: data.iloc[:, get_ind(categor_col)])),\n            ('hot_encoding', preprocessing.OneHotEncoder(handle_unknown='ignore'))])),\n        ])),\n    \n    ('model_fitting', sgd_reg)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Available parametrs of created estimator\nprint(*sgd_estimator.get_params().keys(), sep='\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variation of paramers for grid_search\nparametrs_sgd = {'model_fitting__alpha' : [0.00001, 0.0001, 0.001, 0.01, 0.1],\n                 'model_fitting__eta0' : [0.001, 0.05, 0.1, 0.5], # initial gradient step\n                 'model_fitting__max_iter' : [500, 1000, 2000],\n                 'model_fitting__penalty' : ['l2']}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Available scores of grid_search\nprint(*metrics.SCORERS.keys(), sep='\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choosing grid_search for searching best model's parametrs and cross-validation\ngrid_cv_sgd = model_selection.GridSearchCV(sgd_estimator,\n                                           parametrs_sgd,\n                                           scoring='neg_mean_absolute_error',\n                                           cv=5,\n                                           n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fiting grid_search by data\ngrid_cv_sgd.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_cv_sgd.best_score_)\nprint(grid_cv_sgd.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_rmsle(y_true, y_pred):\n    '''Func for counting Root Mean Squared Logarithmic Error (RMSLE)'''\n    \n    # scaling to (0. 1)\n    y_true_scaled = preprocessing.minmax_scale(y_true,feature_range=(0,1))    \n    y_pred_scaled = preprocessing.minmax_scale(y_pred, feature_range=(0,1))\n    \n    return np.sqrt(metrics.mean_squared_log_error(y_true_scaled,\n                                                  y_pred_scaled))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_sgd = grid_cv_sgd.best_estimator_.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSLE on train data\nprint('RMSLE on train data: {}'.format(get_rmsle(y_train, y_train_pred_sgd)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### score on test data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred_sgd = grid_cv_sgd.best_estimator_.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSLE on test data\nprint('RMSLE on test data: {}'.format(get_rmsle(y_test, y_test_pred_sgd)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare several test labels and predicted labels just by view \nprint(y_test[:13].to_numpy())\nprint(y_test_pred_sgd[:13].round())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display a graph of points in the space of correct labels and predictions \n# on train and test data. A diagonal cloud of point is expected for a good regression model.\\\n# (2 scatter plots on 1 graph)\nplt.figure(figsize=(7, 5))\nplt.grid(True)\nplt.xlim(-100,1100)\nplt.ylim(-100,1100)\nplt.scatter(y_train, y_train_pred_sgd, alpha=0.5, color='red', label='train')\nplt.scatter(y_test, y_test_pred_sgd, alpha=0.5, color='blue', label='test')\nplt.legend()\nplt.title('SGD Regressor')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, the linear model SGD Regressor showed not really impressive result.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Random Forest:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choosing a model\nrf_reg = ensemble.RandomForestRegressor(random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating pipeline for data transforming and further estimation\n# (notice: numeric data doesn't need scaling with random forest, but just let it be)\nrf_estimator = pipeline.Pipeline(steps=[\n    ('feature_processing', pipeline.FeatureUnion(transformer_list=[\n        \n        #binary\n        ('binary_variables_processing', preprocessing.FunctionTransformer(lambda data: data.iloc[:, get_ind(binary_col)])),\n        \n        #numeric\n        ('numeric_variables_processing', pipeline.Pipeline(steps=[\n            ('selecting', preprocessing.FunctionTransformer(lambda data: data.iloc[:, get_ind(numeric_col)])),\n            ('scaling', preprocessing.StandardScaler(with_mean=0.))])),        \n        \n        #categorical\n        ('categorical_variables_processing', pipeline.Pipeline(steps=[\n            ('selecting', preprocessing.FunctionTransformer(lambda data: data.iloc[:, get_ind(categor_col)])),\n            ('hot_encoding', preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False))])),\n        ])),\n    \n    ('model_fitting', rf_reg)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Available parametrs of created estimator\nprint(*rf_estimator.get_params().keys(), sep='\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Variation of paramers for grid_search\nrf_parametrs = {'model_fitting__n_estimators' : [500, 1000],\n                'model_fitting__max_depth' : [20, 60]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choosing grid_search for searching best model's parametrs and cross-validation\ngrid_cv_rf = model_selection.GridSearchCV(rf_estimator,\n                                          rf_parametrs,\n                                          scoring='neg_mean_absolute_error',\n                                          cv=5,\n                                          n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Fiting grid_search by data\ngrid_cv_rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_cv_rf.best_score_)\nprint(grid_cv_rf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dirty hack for getting column name of train data after scalling in order to\n# counting feature's importances (see below)\ntemp_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False)\ntemp_train_X_encoded = temp_encoder.fit_transform(X_train[categor_col])\ntemp_column_name = temp_encoder.get_feature_names(categor_col)\ntemp_X_train_drop = X_train.drop(categor_col, axis=1)\ntrain_column_names = np.concatenate([np.array(list(temp_X_train_drop)), temp_column_name])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see which features are most important\nfeature_importances = grid_cv_rf.best_estimator_.named_steps['model_fitting'].feature_importances_\n\n# puting features and their importances into DataFrame\nfeature_importan_df = pd.DataFrame({'feature' : train_column_names, \n                                    'feature_importances' : feature_importances.round(4)})\n# showing importance of features (first ten)\nfeature_importances_df = feature_importan_df.sort_values('feature_importances',\n                                                         ascending=False).head(10)\nfeature_importances_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Another dirty hack for getting all scalling train data with column name in order to\n# shoing plot of dependence of the most important features from count (see below)\none_hot_encoded_data = X_train\nfor cat in categor_col:      \n    temp_one_hot_encoded_data = pd.get_dummies(X_train[cat],prefix=cat)    \n    one_hot_encoded_data = pd.concat([one_hot_encoded_data,temp_one_hot_encoded_data],axis=1)\n    one_hot_encoded_data.drop(cat, axis=1, inplace=True)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot of dependence of the most important features from count\nplt.figure(figsize=(15, 8))\ncolors = ['green', 'blue', 'red', 'yellow', 'pink', 'grey']\nfor i in range(6):\n    plt.subplot(2, 3, i+1)\n    plt.grid(True)\n    plt.scatter(one_hot_encoded_data[feature_importances_df.iloc[i, 0]],\n                y_train, color=colors[i], label='train')    \n    plt.ylabel('count')\n    plt.xlabel(feature_importances_df.iloc[i, 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSLE on train data\ny_train_pred = grid_cv_rf.best_estimator_.predict(X_train)\nprint('RMSLE on train data: {}'.format(get_rmsle(y_train, y_train_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### score on test data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSLE on test data\ny_test_pred = grid_cv_rf.best_estimator_.predict(X_test)\nprint('RMSLE on test data: {}'.format(get_rmsle(y_test, y_test_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare several test labels and predicted labels just by view \nprint(y_test[:13].to_numpy())\nprint(y_test_pred[:13].round())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display a graph of points in the space of correct labels and predictions \n# on train and test data. A diagonal cloud of point is expected for a good regression model.\n# (2 scatter plots on 1 graph)\nplt.figure(figsize=(7, 5))\nplt.grid(True)\nplt.xlim(-100,1100)\nplt.ylim(-100,1100)\nplt.scatter(y_train, y_train_pred, alpha=0.5, color = 'red', label='train')\nplt.scatter(y_test, y_test_pred, alpha=0.5, color = 'blue', label='test')\nplt.legend()\nplt.title('Random Forest')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, the Random Forest Regressor showed better result.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Final prediction and submision:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predictions on validation sample\ny_val_pred = grid_cv_rf.best_estimator_.predict(val_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making submission file (saving results in the csv-file)\nanswer = pd.DataFrame({'datetime' : raw_val.datetime,\n                       'count' : y_val_pred})\nanswer.to_csv('my_submission.csv', index=False)\nprint('Your submission was successfully saved!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}