{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bike Sharing Predictions\n[https://www.kaggle.com/terminate9298/bike-sharing-predictions](http://)\n<br>Github Link\n[https://github.com/terminate9298](http://)\n<br>Website\n[https://cv.kaus98.ml](http://)\n<br>\nPlease Upvote\n\nThis IPython include Predictions with multiple Regression Techniques and various Visualisations..."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Additional Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import accuracy_score\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.layers import Dropout\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Descriptions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_data(data , info=True ,shape = True, percentage =True,describe = True , sample=True , columns = False):\n    if columns == True:\n        print('\\nColumns of Data...')\n        print(data.columns)\n        return \n    if shape ==True:\n        print('Shape of Data is...')\n        print(data.shape)\n    if info==True:\n        print('\\nInfo of Data...')\n        print(data.info())\n    if percentage ==True:\n        print('\\nPercentage of Data Missing ...')\n        print((data.isnull().sum()/data.shape[0])*100)\n    if describe == True:\n        print('\\nDescription of data...')\n        display(data.describe())\n    if sample == True:\n        print('\\nSample of Data...')\n        display(data.sample(10).T)\n    \n\ndefine_data(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Columns in Test File"},{"metadata":{"trusted":true},"cell_type":"code","source":"define_data(train  , columns = True)\ndefine_data(test  , columns = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Divide DateTime Column to various Columns\ndef add_dates(data , column , suffix='time_' , year = True , month = True , day = False ,dayofweek = True, hour = True , minute = False  , second = False , date = False , time = False):\n    data['add_date_date_time'] = pd.to_datetime(data[column])\n    if year == True:\n        data[suffix+'year']=data['add_date_date_time'].dt.year\n    if month == True:\n        data[suffix+'month']=data['add_date_date_time'].dt.month\n    if day == True:\n        data[suffix+'day']=data['add_date_date_time'].dt.day\n    if hour == True:\n        data[suffix+'hour']=data['add_date_date_time'].dt.hour\n    if minute == True: \n        data[suffix+'minute']=data['add_date_date_time'].dt.minute\n    if date == True:\n        data[suffix+'date']=data['add_date_date_time'].dt.date\n    if time == True:\n        data[suffix+'time']=data['add_date_date_time'].dt.time\n    if second == True:\n        data[suffix+'second']=data['add_date_date_time'].dt.second\n    if dayofweek == True:\n        data[suffix+'dayofweek']=data['add_date_date_time'].dt.dayofweek\n    data = data.drop(columns = ['add_date_date_time'] , axis =1)\n    return data\ntrain = add_dates(train , column = 'datetime') \ndefine_data(train , columns = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unique_count(data , columns = []):\n    for col in columns :\n        print('Unique Data Percentage in ',col)\n        print((data[col].value_counts()/data.shape[0])*100)\n        print('\\n')\nunique_count(train , columns = ['season','weather','time_year', 'time_dayofweek'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_unique_data(data):\n    for i in data.columns:\n        unique_cols_data = data[i].unique()\n        if len(unique_cols_data)<20:\n            print('Correct Type on Column -> ',i)\n            print('Unique data in this Column is -> ',unique_cols_data)\n            print('\\n')\ndisplay_unique_data(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train.corr().style.format(\"{:.2%}\").highlight_min())\n# f,ax = plt.subplots(figsize=(15, 15))\n# sns.heatmap(train.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def joint_plots(data , col,columns  = []):\n    plt.figure(figsize=(16,16))\n    for i in columns:\n        sns.jointplot( x=col , y=i , data=data , height=10, ratio=3 , color='g')\n        plt.show()\njoint_plots(train , columns = ['temp' , 'atemp'  ,'humidity' , 'windspeed' ] , col = 'count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_bar(data, col ,  feature=[]):\n    length = len(feature)*4\n    plt.figure(figsize=(20,length))\n    for i,j in zip(feature,range(1,len(feature)*2-1,2)):\n        plt.subplot(10,2,j)\n        #fig = plt.figure(figsize=(9,8))\n        sns.barplot(x=i, y=col, data=data, palette='Set2',orient='v')\n        plt.plot()\n        plt.subplot(10,2,j+1)\n        sns.boxplot(x=i, y=col, data=data, palette='Set2'  , width=.4)\n        plt.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar(train, col = 'count',feature =['time_hour','time_month','time_dayofweek','time_year','weather', 'holiday' , 'workingday' , 'season' ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### New Columns and Other Chnages"},{"metadata":{"trusted":true},"cell_type":"code","source":"def hour_group(s):\n    if((0<=s) & (s<=6)):\n        return 1\n    elif((s==7) | (s==9)):\n        return 2\n    elif((s==8) | (s==16) | (s==19)):\n        return 3\n    elif((10<=s) & (s<=15)):\n        return 4\n    elif((s==17) | (s==18)):\n        return 5\n    elif(20<=s):\n        return 6\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['time_hour_group'] = train['time_hour'].apply(hour_group).astype(str)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def new_col_categorical(data , columns = [] , remove_original = True):\n    for i in columns:\n        unique_cols = data[i].unique()\n        if len(unique_cols) < 20:\n            print('\\nCorrect Type on Column -> ',i)\n            print('Unique data in this Column is -> ',unique_cols)\n        else:\n            return data\n    if remove_original == False:\n        original_data = data[columns]\n    data = pd.get_dummies(data , columns = columns)\n    if remove_original == False:\n        data = pd.concat([data,original_data] , axis=1)\n    return data\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = new_col_categorical(train,columns=['season','weather','time_year', 'time_dayofweek' , 'time_month','time_hour_group'] , remove_original = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train[train.holiday == train.workingday].sample(10)\n# on Saterday and Sunday there is holiday and thats why both are 0\ntrain['weekend'] = train['time_dayofweek_5']+train['time_dayofweek_6'] \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"define_data(train, columns = True )\n# train_x_new = train.drop(columns =['datetime','count', 'season_1','casual','registered',\n#        'season_2', 'season_3', 'season_4', 'weather_1', 'weather_2',\n#        'weather_3', 'weather_4', 'time_year_2011', 'time_year_2012',\n#        'time_dayofweek_0', 'time_dayofweek_1', 'time_dayofweek_2',\n#        'time_dayofweek_3', 'time_dayofweek_4', 'time_dayofweek_5',\n#        'time_dayofweek_6', 'time_month_1', 'time_month_2', 'time_month_3',\n#        'time_month_4', 'time_month_5', 'time_month_6', 'time_month_7',\n#        'time_month_8', 'time_month_9', 'time_month_10', 'time_month_11',\n#        'time_month_12'] , axis = 1)\ntrain_x_new = train.drop(columns =['datetime','count', 'casual','registered',\n       'season', 'weather', 'time_year',\n       'time_dayofweek', 'time_month','time_hour_group'] , axis = 1)\ntrain_y_new = train['count']\ndefine_data(train_x_new, columns = True )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Processing Test Data\ntest = add_dates(test , column = 'datetime') \ntest['time_hour_group'] = test['time_hour'].apply(hour_group).astype(str)\ntest = new_col_categorical(test,columns=['season','weather','time_year', 'time_dayofweek' , 'time_month','time_hour_group'] , remove_original = False)\ntest['weekend'] = test['time_dayofweek_5']+test['time_dayofweek_6'] \n# test_x_new = test.drop(columns =['datetime', 'season_1',\n#        'season_2', 'season_3', 'season_4', 'weather_1', 'weather_2',\n#        'weather_3', 'weather_4', 'time_year_2011', 'time_year_2012',\n#        'time_dayofweek_0', 'time_dayofweek_1', 'time_dayofweek_2',\n#        'time_dayofweek_3', 'time_dayofweek_4', 'time_dayofweek_5',\n#        'time_dayofweek_6', 'time_month_1', 'time_month_2', 'time_month_3',\n#        'time_month_4', 'time_month_5', 'time_month_6', 'time_month_7',\n#        'time_month_8', 'time_month_9', 'time_month_10', 'time_month_11',\n#        'time_month_12'] , axis = 1)\n\ntest_x_new = test.drop(columns =['datetime',\n       'season', 'weather', 'time_year',\n       'time_dayofweek', 'time_month','time_hour_group'] , axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('For Train Data .. ')\ndefine_data(train_x_new, columns = True )\nprint('For Test Data .. ')\ndefine_data(test_x_new , columns = True )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MINMAX Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\ntrain_x_new = scaler.fit_transform(train_x_new)\ntrain_y_new = np.log1p(train_y_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x_new_1 = scaler.transform(test_x_new)\ntest_x_new_2 = scaler.fit_transform(test_x_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train , X_test , Y_train , Y_test = train_test_split(train_x_new , train_y_new , test_size = .15 , random_state = 65 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_0_error =0\nvalid_1_error =0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def score_diff(valid_0_error , valid_1_error , valid_0_error_new , valid_1_error_new):\n    if valid_0_error == 0:\n        print('First Observaton')\n        print('Train Error is : ',valid_0_error_new)\n        print('Test Error is : ',valid_1_error_new)\n        print('Diffence Between Train and Test is : ',((valid_1_error_new-valid_0_error_new)/valid_0_error_new)*100 ,' %')\n    else:\n        if valid_0_error_new > valid_0_error:\n            print('Train Error is : ',valid_0_error_new)\n            print('Test Error is : ',valid_1_error_new)\n            print('Train Error have Gone up by  : ',((valid_0_error_new-valid_0_error)/valid_0_error)*100,'%')\n            print('Test Error have Gone up by  : ',((valid_1_error_new-valid_1_error)/valid_1_error)*100,'%')\n            print('Diffence Between Train and Test is : ',((valid_1_error_new-valid_0_error_new)/valid_0_error_new)*100 ,' %')\n            print('Earlier Diffence Between Train and Test is : ',((valid_1_error-valid_0_error)/valid_0_error)*100 ,' %')\n        if valid_0_error_new < valid_0_error:\n            print('Train Error is : ',valid_0_error_new)\n            print('Test Error is : ',valid_1_error_new)\n            print('Train Error have Down up by  : ',((valid_0_error-valid_0_error_new)/valid_0_error_new)*100,'%') \n            print('Test Error have Down up by  : ',((valid_1_error-valid_1_error_new)/valid_1_error_new)*100,'%') \n            print('Diffence Between Train and Test is : ',((valid_1_error_new-valid_0_error_new)/valid_0_error_new)*100 ,' %')\n            print('Earlier Diffence Between Train and Test is : ',((valid_1_error-valid_0_error)/valid_0_error)*100 ,' %')\n        if valid_0_error_new == valid_0_error:\n            print('No Differnce in new Obseravtion')\n            print('Train Error is : ',valid_0_error_new)\n            print('Test Error is : ',valid_1_error_new)\n            print('Diffence Between Train and Test is : ',((valid_1_error_new-valid_0_error_new)/valid_0_error_new)*100 ,' %')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = lgb.Dataset(X_train , label = Y_train)\nval_set = lgb.Dataset( X_test, label = Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams = {\n        \"objective\" : \"regression\", \n        \"metric\" : \"mae\", \n        \"num_leaves\" : 60, \n        \"learning_rate\" : 0.01, \n        \"bagging_fraction\" : 0.9,\n        \"bagging_seed\" : 0, \n        \"num_threads\" : 4,\n        \"colsample_bytree\" : 0.5, \n        'lambda_l2':9\n}\n\nmodel = lgb.train(  params, \n                    train_set = train_set,\n                    num_boost_round=10000,\n                    early_stopping_rounds=200,\n                    verbose_eval=100, \n                    valid_sets=[train_set,val_set]\n                  )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlgb_pred_test = model.predict(X_test, num_iteration=model.best_iteration)\nlgb_pred_train = model.predict(X_train, num_iteration=model.best_iteration)\nlgb_pred_normal = model.predict(test_x_new_1, num_iteration=model.best_iteration)\nlgb_pred_fit = model.predict(test_x_new_2, num_iteration=model.best_iteration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(lgb_pred)\n# print(np.array(Y_test))\nvalid_0_error_new = sqrt(mean_squared_error(np.array(Y_train),lgb_pred_train))\nvalid_1_error_new = sqrt(mean_squared_error(np.array(Y_test),lgb_pred_test))\nscore_diff(valid_0_error , valid_1_error , valid_0_error_new , valid_1_error_new)\nvalid_0_error = valid_0_error_new\nvalid_1_error = valid_1_error_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb.plot_importance(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nn_estimators=100\nxgb = XGBRegressor(n_estimators=n_estimators,max_depth=4,learning_rate =0.01 , booster = 'gbtree')\nxgb.fit(X_train ,Y_train ,eval_set=[(X_train, Y_train), (X_test, Y_test)] , verbose = False)\nscore = xgb.evals_result()\nvalid_0_error_new = np.amin(score['validation_0']['rmse'])\nvalid_1_error_new = np.amin(score['validation_1']['rmse'])\nscore_diff(valid_0_error , valid_1_error , valid_0_error_new , valid_1_error_new)\nvalid_0_error = valid_0_error_new\nvalid_1_error = valid_1_error_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel = RandomForestRegressor(random_state=65, n_estimators=200, min_samples_split=4)\nresult = model.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nstart_point = n_estimators-100\nr = range(start_point,n_estimators)\nplt.figure(figsize=(16,8))\nplt.plot(r , score['validation_0']['rmse'][start_point:]  ,'r' ,label ='Train')\nplt.plot(r , score['validation_1']['rmse'][start_point:]  , 'g' , label = 'Test' )\nplt.legend(fontsize='x-large')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nn_estimators=3000\nxgb = XGBRegressor(n_estimators=n_estimators,max_depth=4,learning_rate =0.01 , booster = 'gbtree')\nxgb.fit(train_x_new , train_y_new ,eval_set=[(X_train, Y_train), (X_test, Y_test)] , verbose = False)\npred_normal = xgb.predict(test_x_new_1)\npred_fit = xgb.predict(test_x_new_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel = RandomForestRegressor(random_state=65, n_estimators=n_estimators-2000)\nmodel.fit(train_x_new , train_y_new)\nrfr_pred_normal = model.predict(test_x_new_1)\nrfr_pred_fit = model.predict(test_x_new_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npred_normal = np.expm1(pred_normal)\npred_fit = np.expm1(pred_fit)\nrfr_pred_normal = np.expm1(rfr_pred_normal)\nrfr_pred_fit = np.expm1(rfr_pred_fit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nNN_model = Sequential()\n\n# The Input Layer :\nNN_model.add(Dense(128, kernel_initializer='normal',input_dim = train_x_new.shape[1], activation='relu'))\n\n# The Hidden Layers :\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dropout(0.3))\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dropout(0.3))\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dropout(0.3))\n# The Output Layer :\nNN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n# Compile the network :\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nNN_model.summary()\n\nhistory = NN_model.fit(train_x_new,train_y_new, epochs=50, batch_size=64,  verbose=1, validation_split=0.2)\nstart_point = 150-100\nr = range(start_point,150)\nplt.figure(figsize=(16,8))\nplt.plot( history.history['loss']  ,'r' ,label ='Train')\nplt.plot( history.history['val_loss']  , 'g' , label = 'Test' )\nplt.legend(fontsize='x-large')\nvalid_0_error_new = history.history['loss'][-1]\nvalid_1_error_new = history.history['val_loss'][-1]\nscore_diff(valid_0_error , valid_1_error , valid_0_error_new , valid_1_error_new)\nvalid_0_error = valid_0_error_new\nvalid_1_error = valid_1_error_new\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ANN_pred_normal = NN_model.predict(test_x_new_1)\nANN_pred_fit = NN_model.predict(test_x_new_2)\nANN_pred_normal = np.expm1(ANN_pred_normal)\nANN_pred_fit = np.expm1(ANN_pred_fit)\nANN_pred_fit = ANN_pred_fit.reshape(6493)\nANN_pred_normal = ANN_pred_normal.reshape(6493)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'datetime': test.datetime,'count': pred_normal})\noutput.to_csv('xgb_pred_normal.csv', index=False)\noutput = pd.DataFrame({'datetime': test.datetime,'count': pred_fit})\noutput.to_csv('xgb_pred_fit.csv', index=False)\noutput = pd.DataFrame({'datetime': test.datetime,'count': rfr_pred_normal})\noutput.to_csv('rfr_pred_normal.csv', index=False)\noutput = pd.DataFrame({'datetime': test.datetime,'count': rfr_pred_fit})\noutput.to_csv('rfr_pred_fit.csv', index=False)\noutput = pd.DataFrame({'datetime': test.datetime,'count': ANN_pred_normal})\noutput.to_csv('ANN_pred_normal.csv', index=False)\noutput = pd.DataFrame({'datetime': test.datetime,'count': ANN_pred_fit})\noutput.to_csv('ANN_pred_fit.csv', index=False)\noutput = pd.DataFrame({'datetime': test.datetime,'count': lgb_pred_normal})\noutput.to_csv('lgb_pred_normal.csv', index=False)\noutput = pd.DataFrame({'datetime': test.datetime,'count': lgb_pred_fit})\noutput.to_csv('lgb_pred_fit.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}