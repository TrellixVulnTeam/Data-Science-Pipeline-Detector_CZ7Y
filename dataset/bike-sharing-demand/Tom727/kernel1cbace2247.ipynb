{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# ライブラリの呼び出し\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns #시각화를 위한 라이브러리\nimport matplotlib.pyplot as plt\nimport calendar \nfrom datetime import datetime\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 訓練、テストデータセットの形とデータのカラムの属性と値の数を把握\n\ntrain = pd.read_csv('/kaggle/input/bike-sharing-demand/train.csv')\ntest = pd.read_csv('/kaggle/input/bike-sharing-demand/test.csv')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"カラムの説明\n\ndatetime - hourly date + timestamp  \nseason -  1 = spring, 2 = summer, 3 = fall, 4 = winter \nholiday - whether the day is considered a holiday\nworkingday - whether the day is neither a weekend nor holiday\nweather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy \n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds \n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \ntemp - temperature in Celsius\natemp - \"feels like\" temperature in Celsius\nhumidity - relative humidity\nwindspeed - wind speed\ncasual - number of non-registered user rentals initiated\nregistered - number of registered user rentals initiated\ncount - number of total rentals\n\"\"\"\n\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 中身の確認\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# データの前処理と可視化\n# split関数を使用して年 - 月 - 日と時間を分離\ntrain['tempDate'] = train.datetime.apply(lambda x:x.split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 分離したtempDateの年-月-日を利用してyear、month、dayとweekdayのカラムを抽出\n\ntrain['year'] = train.tempDate.apply(lambda x:x[0].split('-')[0])\ntrain['month'] = train.tempDate.apply(lambda x:x[0].split('-')[1])\ntrain['day'] = train.tempDate.apply(lambda x:x[0].split('-')[2])\n\n#weekdayはcalendarとdatetimeを利用\ntrain['weekday'] = train.tempDate.apply(lambda x:calendar.day_name[datetime.strptime(x[0],\"%Y-%m-%d\").weekday()])\n\ntrain['hour'] = train.tempDate.apply(lambda x:x[1].split(':')[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 分離して抽出された属性が文字列であるため数値データに変換\n\ntrain['year'] = pd.to_numeric(train.year,errors='coerce')\ntrain['month'] = pd.to_numeric(train.month,errors='coerce')\ntrain['day'] = pd.to_numeric(train.day,errors='coerce')\ntrain['hour'] = pd.to_numeric(train.hour,errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 変換をしたため確認\n\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tempDateのカラムを除去\n\ntrain = train.drop('tempDate',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 各カラムとcountの相関\n\n#year - count\nfig = plt.figure(figsize=[12,10])\nax1 = fig.add_subplot(2,2,1)\nax1 = sns.barplot(x='year',y='count',data=train.groupby('year')['count'].mean().reset_index())\n\n#month - count\nax2 = fig.add_subplot(2,2,2)\nax2 = sns.barplot(x='month',y='count',data=train.groupby('month')['count'].mean().reset_index())\n\n#day - count\nax3 = fig.add_subplot(2,2,3)\nax3 = sns.barplot(x='day',y='count',data=train.groupby('day')['count'].mean().reset_index())\n\n#hour - count\nax4 = fig.add_subplot(2,2,4)\nax4 = sns.barplot(x='hour',y='count',data=train.groupby('hour')['count'].mean().reset_index())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#season - count\nfig = plt.figure(figsize=[12,10])\nax1 = fig.add_subplot(2,2,1)\nax1 = sns.barplot(x='season',y='count',data=train.groupby('season')['count'].mean().reset_index())\n\n#holiday - count\nax2 = fig.add_subplot(2,2,2)\nax2 = sns.barplot(x='holiday',y='count',data=train.groupby('holiday')['count'].mean().reset_index())\n\n#workingday - count\nax3 = fig.add_subplot(2,2,3)\nax3 = sns.barplot(x='workingday',y='count',data=train.groupby('workingday')['count'].mean().reset_index())\n\n#weather - count\nax4 = fig.add_subplot(2,2,4)\nax4 = sns.barplot(x='weather',y='count',data=train.groupby('weather')['count'].mean().reset_index())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def badToRight(month):\n    if month in [12,1,2]:\n        return 4\n    elif month in [3,4,5]:\n        return 1\n    elif month in [6,7,8]:\n        return 2\n    elif month in [9,10,11]:\n        return 3\n    \ntrain['season'] = train.month.apply(badToRight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1つの列と結果の値を比較\n\n#season - count\nfig = plt.figure(figsize=[12,10])\nax1 = fig.add_subplot(2,2,1)\nax1 = sns.barplot(x='season',y='count',data=train.groupby('season')['count'].mean().reset_index())\n\n#holiday - count\nax2 = fig.add_subplot(2,2,2)\nax2 = sns.barplot(x='holiday',y='count',data=train.groupby('holiday')['count'].mean().reset_index())\n\n#woikingday - count\nax3 = fig.add_subplot(2,2,3)\nax3 = sns.barplot(x='workingday',y='count',data=train.groupby('workingday')['count'].mean().reset_index())\n\n#weather - count\nax4 = fig.add_subplot(2,2,4)\nax4 = sns.barplot(x='weather',y='count',data=train.groupby('weather')['count'].mean().reset_index())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 相関係数をheatmapを介して可視化\n\nfig = plt.figure(figsize=[20,20])\nax = sns.heatmap(train.corr(),annot=True,square=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# heatmap相関を参照して二つの異なるカラムとcountを視覚化\n\n#hour season - count\nfig = plt.figure(figsize=[12,10])\nax1 = fig.add_subplot(2,2,1)\nax1 = sns.pointplot(x='hour',y='count',hue='season',data=train.groupby(['season','hour'])['count'].mean().reset_index())\n\n#hour holiday - count\nax2 = fig.add_subplot(2,2,2)\nax2 = sns.pointplot(x='hour',y='count',hue='holiday',data=train.groupby(['holiday','hour'])['count'].mean().reset_index())\n\n#hour weekday - count\nax3 = fig.add_subplot(2,2,3)\nax3 = sns.pointplot(x='hour',y='count',hue='weekday',hue_order=['Sunday','Monday','Tuesday','Wendnesday','Thursday','Friday','Saturday'],data=train.groupby(['weekday','hour'])['count'].mean().reset_index())\n\n#hour weather - count\nax4 = fig.add_subplot(2,2,4)\nax4 = sns.pointplot(x='hour',y='count',hue='weather',data=train.groupby(['weather','hour'])['count'].mean().reset_index())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 最後の可視化に異常値があるため確認\n\ntrain[train.weather==4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#month, weather - count \nfig = plt.figure(figsize=[12,10])\nax1 = fig.add_subplot(2,1,1)\nax1 = sns.pointplot(x='month',y='count',hue='weather',data=train.groupby(['weather','month'])['count'].mean().reset_index())\n\n#month count\nax2 = fig.add_subplot(2,1,2)\nax2 = sns.barplot(x='month',y='count',data=train.groupby('month')['count'].mean().reset_index())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nWindspeed分布を表現したグラフでWindspeedが0である値が多かったが、\nこれは実際には0であったかor値を正しく測定できなくて0である二つの場合がある\n後者の場合を考えて、windspeed値を付与する\n\"\"\"\n\n# 文字列をカテゴリー化し、それぞれに対応する数値に変換\n\ntrain['weekday']= train.weekday.astype('category')\nprint(train['weekday'].cat.categories)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#0:Sunday --> 6:Saturday\ntrain.weekday.cat.categories = ['5','1','6','0','4','2','3']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ランダムフォレストを使ってwindspeedを付与\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Windspeedが0のデータフレーム\nwindspeed_0 = train[train.windspeed == 0]\n# Windspeedが0でないデータフレーム\nwindspeed_Not0 = train[train.windspeed != 0]\n\n# Windspeedが0であるデータフレームから不要なカラムを除去\nwindspeed_0_df = windspeed_0.drop(['windspeed','casual','registered','count','datetime'],axis=1)\n\n# Windspeedが0でないデータフレームはそのまま学習\nwindspeed_Not0_df = windspeed_Not0.drop(['windspeed','casual','registered','count','datetime'],axis=1)\nwindspeed_Not0_series = windspeed_Not0['windspeed'] \n\n# モデルに0以外のデータフレームと結果の値を学習させる\nrf = RandomForestRegressor()\nrf.fit(windspeed_Not0_df,windspeed_Not0_series)\n\n# 学習したモデルからWindspeedが0であるデータフレームのWindspeedを導出\npredicted_windspeed_0 = rf.predict(windspeed_0_df)\n\n# 導出された値を元のデータフレームに挿入\nwindspeed_0['windspeed'] = predicted_windspeed_0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 分割したデータフレームを元の形に復元\ntrain = pd.concat([windspeed_0,windspeed_Not0],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 時間ごとのソートのためにstring typeのdatetimeを変換\ntrain.datetime = pd.to_datetime(train.datetime,errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 統合したデータをdatetime順に並べ替える\ntrain = train.sort_values(by=['datetime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# windspeedを修正した後、再び相関係数を分析\nfig = plt.figure(figsize=[20,20])\nax = sns.heatmap(train.corr(),annot=True,square=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=[5,5])\nsns.distplot(train['windspeed'],bins=np.linspace(train['windspeed'].min(),train['windspeed'].max(),10))\nplt.suptitle(\"Filled by Random Forest Regressor\")\nprint(\"Min value of windspeed is {}\".format(train['windspeed'].min()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 同様の前処理をtestとtrainの統合データで行う\n\ntrain = pd.read_csv('/kaggle/input/bike-sharing-demand/train.csv')\ntest = pd.read_csv('/kaggle/input/bike-sharing-demand/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine = pd.concat([train,test],axis=0)\ncombine.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine['tempDate'] = combine.datetime.apply(lambda x:x.split())\ncombine['weekday'] = combine.tempDate.apply(lambda x: calendar.day_name[datetime.strptime(x[0],\"%Y-%m-%d\").weekday()])\ncombine['year'] = combine.tempDate.apply(lambda x: x[0].split('-')[0])\ncombine['month'] = combine.tempDate.apply(lambda x: x[0].split('-')[1])\ncombine['day'] = combine.tempDate.apply(lambda x: x[0].split('-')[2])\ncombine['hour'] = combine.tempDate.apply(lambda x: x[1].split(':')[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine['year'] = pd.to_numeric(combine.year,errors='coerce')\ncombine['month'] = pd.to_numeric(combine.month,errors='coerce')\ncombine['day'] = pd.to_numeric(combine.day,errors='coerce')\ncombine['hour'] = pd.to_numeric(combine.hour,errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine['season'] = combine.month.apply(badToRight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine.weekday = combine.weekday.astype('category')\ncombine.weekday.cat.categories = ['5','1','6','0','4','2','3']\ndataWind0 = combine[combine['windspeed']==0]\ndataWindNot0 = combine[combine['windspeed']!=0]\n\ndataWind0.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataWind0_df = dataWind0.drop(['windspeed','casual','registered','count','datetime','tempDate'],axis=1)\n\ndataWindNot0_df = dataWindNot0.drop(['windspeed','casual','registered','count','datetime','tempDate'],axis=1)\ndataWindNot0_series = dataWindNot0['windspeed']\n\ndataWindNot0_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf2 = RandomForestRegressor()\nrf2.fit(dataWindNot0_df,dataWindNot0_series)\npredicted = rf2.predict(dataWind0_df)\nprint(predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataWind0['windspeed'] = predicted\ncombine = pd.concat([dataWind0,dataWindNot0],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 不要なカラムの除去\ncategorizational_columns = ['holiday','humidity','season','weather','workingday','year','month','day','hour']\ndrop_columns = ['datetime','casual','registered','count','tempDate']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#数値に変換\nfor col in categorizational_columns:\n    combine[col] = combine[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 統合したデータセットからcountの有無により訓練とテストセットを分離し、それぞれをdatetimeに並べ替え\ntrain = combine[pd.notnull(combine['count'])].sort_values(by='datetime')\ntest = combine[~pd.notnull(combine['count'])].sort_values(by='datetime')\n\n# データを訓練した結果\ndatetimecol = test['datetime']\nyLabels = train['count'] #count\nyLabelsRegistered = train['registered'] #登録者\nyLabelsCasual = train['casual'] #一時的なユーザー","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 必要ないcolumnを除去した後のtrainとtest\ntrain = train.drop(drop_columns,axis=1)\ntest = test.drop(drop_columns,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nこの問題では、RMSLE方式を利用して、適切に予測がされたか評価することになる。\nRMSLEは、以下のリンクを参照して、利用。\nhttps://programmers.co.kr/learn/courses/21/lessons/943#\n\nRMSLE\n過大評価された項目ではなく、過小評価された項目にペナルティを与える方式\n誤差を二乗してヒョンギュンした値の平方根で値が小さくなるほど精度が高い\n0に近い値が出てくるほど精度が高い\n\"\"\"\n\n# y is predict value y_ is actual value\ndef rmsle(y, y_,convertExp=True):\n    if convertExp:\n        y = np.exp(y), \n        y_ = np.exp(y_)\n    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))\n    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))\n    calc = (log1 - log2) ** 2\n    return np.sqrt(np.mean(calc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 線形回帰モデル\n# 線形回帰モデルは、触れるだけの内部attrがない\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso\n\n\nlr = LinearRegression()\n\n\"\"\"\n下のカーネルを参照してyLabelsをログ化しようとして、なぜnp.logではなく、np.log1pを活用するか？\nnp.log1pはnp.log（1+ x）と同じです。理由は、もしあれば、xの値が0であるが、これをlogになると、（ - ）無限大に収束するので、np.log1pを活用する。\n\"\"\"\nyLabelslog = np.log1p(yLabels)\n#線形モデルに、私たちのデータを学習\nlr.fit(train,yLabelslog)\n#結果値導出\npreds = lr.predict(train)\n#rmsle関数のelementにnp.exp（）指数関数をとる理由は、私たちのpreds値に得られたのは、一度logをした値であるため、元のモデルには、logをしていない元の値を入れるウィハムイム。\nprint('RMSLE Value For Linear Regression: {}'.format(rmsle(np.exp(yLabelslog),np.exp(preds),False)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nGridSearchCVを活用すれば、私たちが利用するようになるの各モデルごとに変更する必要がパラメータチューニング時にどのパラメータが最適値を出すのかなどを知ることができている。\n\nGridSearchCV参照：\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\nhttps://datascienceschool.net/view-notebook/ff4b5d491cc34f94aea04baca86fbef8/\n\"\"\"\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\n\n#RidgeモデルはL2制約を持つ線形回帰モデルで改善されたモデルであり、そのモデルで有意深くチューニングする必要があるパラメータは、alpha値である。\nridge = Ridge()\n\n#私たちは、チューニングしたいRidgeのパラメータのうちの特定のパラメータに配列の値に引き渡すと、テストした後どのようなパラメータが最適の値であるか知らせる\nridge_params = {'max_iter':[3000],'alpha':[0.001,0.01,0.1,1,10,100,1000]}\nrmsle_scorer = metrics.make_scorer(rmsle,greater_is_better=False)\ngrid_ridge = GridSearchCV(ridge,ridge_params,scoring=rmsle_scorer,cv=5)\n\ngrid_ridge.fit(train,yLabelslog)\npreds = grid_ridge.predict(train)\nprint(grid_ridge.best_params_)\nprint('RMSLE Value for Ridge Regression {}'.format(rmsle(np.exp(yLabelslog),np.exp(preds),False)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#結果についてGridSearchCVの変数であるgrid_ridge変数にcv_result_を通じてalpha値の変化に応じて、平均値の変化を把握可能\ndf = pd.DataFrame(grid_ridge.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RidgeモデルはL1制約を持つ線形回帰モデルで改善されたモデルであり、そのモデルで有意深くチューニングする必要があるパラメータは、alpha値である。\nlasso = Lasso()\n\nlasso_params = {'max_iter':[3000],'alpha':[0.001,0.01,0.1,1,10,100,1000]}\ngrid_lasso = GridSearchCV(lasso,lasso_params,scoring=rmsle_scorer,cv=5)\ngrid_lasso.fit(train,yLabelslog)\npreds = grid_lasso.predict(train)\nprint('RMSLE Value for Lasso Regression {}'.format(rmsle(np.exp(yLabelslog),np.exp(preds),False)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor()\n\nrf_params = {'n_estimators':[1,10,100]}\ngrid_rf = GridSearchCV(rf,rf_params,scoring=rmsle_scorer,cv=5)\ngrid_rf.fit(train,yLabelslog)\npreds = grid_rf.predict(train)\nprint('RMSLE Value for RandomForest {}'.format(rmsle(np.exp(yLabelslog),np.exp(preds),False)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\ngb = GradientBoostingRegressor()\ngb_params={'max_depth':range(1,11,1),'n_estimators':[1,10,100]}\ngrid_gb=GridSearchCV(gb,gb_params,scoring=rmsle_scorer,cv=5)\ngrid_gb.fit(train,yLabelslog)\npreds = grid_gb.predict(train)\nprint('RMSLE Value for GradientBoosting {}'.format(rmsle(np.exp(yLabelslog),np.exp(preds),False)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predsTest = grid_gb.predict(test)\nfig,(ax1,ax2)= plt.subplots(ncols=2)\nfig.set_size_inches(12,5)\nsns.distplot(yLabels,ax=ax1,bins=50)\nsns.distplot(np.exp(predsTest),ax=ax2,bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"datetime\": datetimecol,\n        \"count\": [max(0, x) for x in np.exp(predsTest)]\n    })\nsubmission.to_csv('bike_predictions_gbm_separate_without_fe.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}