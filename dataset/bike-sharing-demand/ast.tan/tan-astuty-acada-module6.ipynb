{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the necessary modules\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the train data\n\ntrain = pd.read_csv('/kaggle/input/bike-sharing-demand/train.csv', index_col=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Metadata from Kaggle\n\n# datetime - hourly date + timestamp  \n# season -  1 = spring, 2 = summer, 3 = fall, 4 = winter \n# holiday - whether the day is considered a holiday\n# workingday - whether the day is neither a weekend nor holiday\n# weather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy \n# 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \n# 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds \n# 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n# temp - temperature in Celsius\n# atemp - \"feels like\" temperature in Celsius\n# humidity - relative humidity\n# windspeed - wind speed\n# casual - number of non-registered user rentals initiated\n# registered - number of registered user rentals initiated\n# count - number of total rentals\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Understand the data\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Understand the data\n\ntrain.info()\n\n# At a glance, it looks like there is no missing values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Understand the data\n\n(train == 0).any()\n\n# there are some columns where value = 0 which make sense\n# the only column which having 0 is questionable is 'count'\n# although there is 0 values in casual and registered count, there is no zero values in total count, which is ok","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Understand the data\n\ntrain.min()\n\n# there is no unusual / negative values that we should not be expecting. For eg: count columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Understand the data\n\nprint(train.datetime.min())\nprint(train.datetime.max())\nprint(train.datetime.nunique())\n\n# data is from 1st Jan 2011 to 19 Dec 2012\n# no duplicate datetime entry\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Understand the data\n\n# although all data (except datetime) are numerical/continuous, \n# season, holiday, workingday, weather are in fact categorical\n# holiday and workingday are already encoded into 0/1 binary\n# season and weather are encoded in ordinal fashion - is it better to convert this to one-hot encoding?\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Understand the data\n\nprint(train.holiday.value_counts(), \"\\n\")\nprint(train.workingday.value_counts(), \"\\n\")\nprint(train.season.value_counts(), \"\\n\")\nprint(train.weather.value_counts())\n\n# no weird values for season and weather (all as per the metadata)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize / Preliminary Observation\n\nsns.pairplot(x_vars=['temp', 'atemp', 'humidity', 'windspeed'], \n                     y_vars='count', data=train, diag_kind=None, height=7, aspect=0.7)\n\n\n\n# at a glance, there does not seem to be any strong pattern / relationship between temperature, humidity and wind speed with total rentals\n# however, we can see instances of more rentals when temperature and humidity is moderate (not too low/high)\n# and also inverse relationship between wind speed and rentals\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize / Preliminary Observation\n\nsns.pairplot(x_vars=['weather', 'season', 'holiday', 'workingday'], \n                     y_vars='count', data=train, diag_kind=None, height=7, aspect=0.7)\n\n# higher rental counts observed when weather is conducive (ie clear/few cloud) and almost none when it is rainy/stormy/snowy  \n# higher rental counts on fall/winter (which is quite contradictory to above observation on weather)\n# likewise, higher rental counts observed for non-holiday / workig day\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add date and time columns for additional level of detail\n\nimport datetime\n\ntrain['year_month'] = pd.to_datetime(train['datetime']).dt.to_period('M')\ntrain['year'] = pd.DatetimeIndex(train['datetime']).year\ntrain['month'] = pd.DatetimeIndex(train['datetime']).month\ntrain['hour'] = pd.DatetimeIndex(train['datetime']).hour\n\n\ndisplay(train.head())\nprint(train.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Investigate the season (because observation on season above seems to be contradictory)\n\ntrain.groupby(['season','month'])['count'].sum()\n\n# Season appears to be labelled wrongly, does not match the right season","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize / Preliminary observation\n\nsns.lineplot(x='month', y='count', hue='year', data=train, ci=None, estimator='sum')\n\n# no of rentals registering growth in 2012 vs 2011\n# generally, no of rentals peaking in warmer summer and early fall months","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize / Preliminary observation\n\nplt.figure(figsize=(20,8))\nsns.barplot(x='year_month', y='count', data=train, estimator=sum, ci=None, color='lightgreen')\n\n# no of rentals registering growth in 2012 vs 2011\n# generally, no of rentals peaking in warmer summer and early fall months\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize / Preliminary Observation\n\nplt.figure(figsize=(20,8))\nsns.barplot(x='hour', y='count', data=train, estimator=sum, ci=None, hue='year')\n\n# Rental counts peaking at 8 am and 5-6 pm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check correlation between variables\n\ncorrmat = train.corr()\n\nplt.figure(figsize=(12,9))\nsns.heatmap(corrmat, annot=True, fmt='.2f')\n\n# features with the highest correlation with count = temp, atemp, humidity, hour\n# ignore casual / registered -> high correlation because both add up to count\n# even though windspeed, holiday, workingday, month and weather do not show high correlation, there is some relationship with count, just that it is not linear as we observe from the earlier visualizations\n\n# features with high collinearity:\n# season/month\n# temp/atemp\n# humidity/weather\n\n# interesting to observe that casual rental is more correlated with tempetature (temp/atemp) --> rent on a whim depending on temperature\n# and registered rental is more correlated with hour --> rent for regular use at similar timing\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define x_train and y_train\n# remove season (labelled wrongly and also linked to month)\n# remove temp (correlated with atemp)\n# remove casual and registered (add up to count)\n# for time period - retain year, month and hour because results vary by month/hour and also year with YOY growth\n\nx_train = train.drop(['datetime', 'season', 'temp', 'casual', 'registered', 'count', 'year_month'], axis=1)\n\ny_train = train['count']\n\nprint(x_train.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rearrange columns\n\nx_train = x_train[['atemp', 'humidity', 'windspeed', 'hour', 'month', 'holiday', 'workingday', 'year', 'weather']]\nprint(x_train.columns)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make pipeline:\n# no need to impute missing values as there are none\n# perform scaling on numerical columns (exclude those to be encoded)\n# one-hot encoding on weather\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\n\nscaling_transformer = make_column_transformer(\n(StandardScaler(), list(range(0,5))),\n    remainder='passthrough'\n)\n\nencoding_transformer = make_column_transformer(\n(OneHotEncoder(handle_unknown='ignore'), [7,8]),\n    remainder='passthrough'\n)\n\nfull_transformer = make_pipeline(\nscaling_transformer, encoding_transformer,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define formula to get prediction scores\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ndef get_prediction_scores(x_train, y_train, model=LinearRegression()):\n    \n    full_pipeline = make_pipeline(full_transformer, model)\n    \n    full_pipeline.fit(x_train, y_train)\n    \n    y_pred_train = full_pipeline.predict(x_train)\n    \n    print(f'Train rMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train))}')\n    print(f'Train r2: {r2_score(y_train, y_pred_train)}')\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test Model 1 : Linear Regression\n\nmodel=LinearRegression()\n\nget_prediction_scores(x_train, y_train, model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test Model 2 : Ridge\n\nmodel=Ridge(alpha=1)\n\nget_prediction_scores(x_train, y_train, model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test Model 3 : Lasso\n\nmodel=Lasso(alpha=1)\n\nget_prediction_scores(x_train, y_train, model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test Model 4 : ElasticNet\n\nmodel=ElasticNet(alpha=1)\n\nget_prediction_scores(x_train, y_train, model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test Model 5 : Decision Tree\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nmodel=DecisionTreeRegressor(max_depth=10)\n\nget_prediction_scores(x_train, y_train, model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test Model 6 : Random Forest\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(max_depth=10)\n\nget_prediction_scores(x_train, y_train, model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random forest regressor seem to produce the best predictive accuracy compared to other models\n# we shall do cross-validation on the training data\n\nfrom sklearn.model_selection import cross_val_score\n\nchosen_pipeline = make_pipeline(full_transformer, RandomForestRegressor(max_depth=10))\n\ncv_score = cross_val_score(chosen_pipeline, x_train, y_train, cv=10)\n\nprint(f'cv_score: {cv_score}')\nprint(f'mean cv_score: {cv_score.mean()}') # or np.mean(cv_score)\nprint(f'variance cv_score: {cv_score.var()}') # or np.var(cv_score)\n\n# quite consistent results with low variance\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We shall now try to fine tune the hyperparameter using cross validation and grid search\n\nfrom sklearn.model_selection import GridSearchCV\n\nchosen_pipeline.named_steps\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param = {'randomforestregressor__max_depth': [5,8,10]}\n\ngrid_search = GridSearchCV(chosen_pipeline, param, cv=5)\ngrid_search.fit(x_train, y_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_estimator_\n\n# now that we have chosen the model (RandomForestRegressor with max_depth=10)\n# we shall use this to predict y_test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the test data\n\ntest = pd.read_csv('/kaggle/input/bike-sharing-demand/test.csv', index_col=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform the same exploratory data analysis as train data\n\ndisplay(test.head())\nprint(test.info())\n\n# Likewise, at a glance, it looks like there is no missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform the same exploratory data analysis as train data\n\n(test == 0).any()\n\n# there is no unusual/unexpected zero values => ok\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform the same exploratory data analysis as train data\n\ntest.min()\n\n# there is also no unusual / negative values that we should not be expecting\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform the same exploratory data analysis as train data\n\nprint(test.datetime.min())\nprint(test.datetime.max())\nprint(test.datetime.nunique())\n\n# data is from 20th Jan 2011 to 31 Dec 2012 => as per the data explanation that test data is from the 20th of the month till end of the month\n# no duplicate datetime entry","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform the same exploratory data analysis as train data\n\nprint(test.holiday.value_counts(), \"\\n\")\nprint(test.workingday.value_counts(), \"\\n\")\nprint(test.season.value_counts(), \"\\n\")\nprint(test.weather.value_counts())\n\n# likewise, no weird values for season and weather (all as per the metadata)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add date and time columns for additional level of detail\n\ntest['year_month'] = pd.to_datetime(test['datetime']).dt.to_period('M')\ntest['year'] = pd.DatetimeIndex(test['datetime']).year\ntest['month'] = pd.DatetimeIndex(test['datetime']).month\ntest['hour'] = pd.DatetimeIndex(test['datetime']).hour\n\n\ndisplay(test.head())\nprint(test.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define x_test\n\nx_test = test.drop(['datetime', 'season', 'temp', 'year_month'], axis=1)\n\n\nprint(x_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rearrange columns\n\nx_test = x_test[['atemp', 'humidity', 'windspeed', 'hour', 'month', 'holiday', 'workingday', 'year', 'weather']]\nprint(x_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now that we have chosen the model (RandomForestRegressor with max_depth=10)\n# we shall use this to predict y\n\ndef get_predicted_y(x_train, y_train, x_test, model=RandomForestRegressor(max_depth=10)):\n    \n    new_pipeline = make_pipeline(full_transformer, model)\n    \n    new_pipeline.fit(x_train, y_train)\n    \n    y_test = pd.Series(new_pipeline.predict(x_test), name='count')\n    \n    y_test = pd.concat([test['datetime'], y_test], axis=1)\n    \n    y_test.to_csv('submission.csv', index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=RandomForestRegressor(max_depth=10)\n\nget_predicted_y(x_train, y_train, x_test, model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}