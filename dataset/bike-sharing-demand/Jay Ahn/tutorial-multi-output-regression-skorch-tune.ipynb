{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Before begins\n\nThis notebook is written in google colab.\n\nTo see some interactive plots, please enter the colab link Below.\n\n<a href=\"https://colab.research.google.com/drive/1FihAHMXlpPxLwlpa-B261IcY1zisMrEL?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>","metadata":{"id":"NEcrcUgibWiJ"}},{"cell_type":"markdown","source":"# Overview\n\n<br>\n\n## Competition description\n\n<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/3948/logos/thumb76_76.png\" width=50 align='left' alt=\"Open in Colab\"/></a>\n&nbsp; \n<font size=\"5\">[Bike Sharing Demand](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)</font>\n\n<br>\n\n- Problem type: Multi-output regression\n  - Forecast demand of a city bikeshare system\n- Evaluation metric: Root Mean Squared Log Error (RMSLE)\n\n<br>\n\n## Notebook description\n\nThis notebook provides the '**proper workflow**' for kaggle submission.\n\nThe workflow is divided into three main steps.\n1. Data preprocessing\n2. Model selection (hyper parameter tuning, model combination, model comparison)\n3. Training final model & Prediction on Test-set\n\n### Used Models and Tools\n- Model\n  - Linear regression (Scikit-learn)\n  - Support vector machine (Scikit-learn)\n  - Random forest (Scikit-learn)\n  - Xgboost (Scikit-learn)\n  - Multi-layer perceptron (Skorch, PyTorch)\n  - Factorization machine (Skorch, PyTorch)\n- Hyper-parameter Tuning\n  - Grid Search (Ray Tune)\n  - Bayesian optimization (Ray Tune)\n\n\n**Warnings**:\n- The purpose of this notebook\n  - This notebook focuses on the 'procedure' rather than the 'result'. \n  - But since this is a competition, it cannot be avoided that the score is important. Following this notebook, you will get the top 15% (score: 0.12519) result in this competition\n\n- The readers this notebook is intended for\n  - Who are aware of the basic usage of data processing tools (e.g., numpy, pandas)\n  - Who are aware of the basic concepts of machine learning models \n","metadata":{"id":"yf1scTfmyUHQ"}},{"cell_type":"markdown","source":"# 0. Preliminaries","metadata":{"id":"QBk8iQfSbbQt"}},{"cell_type":"markdown","source":"### > Set Configurations \n\n- Set the configurations for this notebook","metadata":{"id":"pTCt72CQWKBa"}},{"cell_type":"code","source":"config = {\n    'data_name': 'bike-sharing-demand',\n    'random_state': 2022\n}","metadata":{"id":"rYO_8wG_bfv3","execution":{"iopub.status.busy":"2022-05-16T04:57:36.521852Z","iopub.execute_input":"2022-05-16T04:57:36.522652Z","iopub.status.idle":"2022-05-16T04:57:36.527162Z","shell.execute_reply.started":"2022-05-16T04:57:36.522599Z","shell.execute_reply":"2022-05-16T04:57:36.526425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### > Install Libraries","metadata":{"id":"aNWeoTAEWLe-"}},{"cell_type":"code","source":"!pip install tune_sklearn ray[tune] skorch -q","metadata":{"id":"zUFVupxjWMsD","execution":{"iopub.status.busy":"2022-05-16T04:51:16.499808Z","iopub.execute_input":"2022-05-16T04:51:16.500218Z","iopub.status.idle":"2022-05-16T04:51:30.092914Z","shell.execute_reply.started":"2022-05-16T04:51:16.500183Z","shell.execute_reply":"2022-05-16T04:51:30.092061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data preprocessing\n\nThe data preprocessing works are divided into 9 steps here.\n\nSome of these steps are mandatory and some are optional.\n\nOptional steps are marked separately.\n\nIt is important to go through each step in order.\nBe careful not to reverse the order.","metadata":{"id":"gH2oJD3ezQD6"}},{"cell_type":"markdown","source":"## 1-1. Load Dataset\n\nLoad train-set and test-set on working environment\n","metadata":{"id":"1O76WdmtbsTI"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv('../input/{}/train.csv'.format(config['data_name']))\ntest = pd.read_csv('../input/{}/test.csv'.format(config['data_name']))","metadata":{"id":"OmHiIN8l4HrR","execution":{"iopub.status.busy":"2022-05-16T04:59:53.476447Z","iopub.execute_input":"2022-05-16T04:59:53.476805Z","iopub.status.idle":"2022-05-16T04:59:53.555749Z","shell.execute_reply.started":"2022-05-16T04:59:53.476768Z","shell.execute_reply":"2022-05-16T04:59:53.554902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### > Concatenate the 'train' and 'test' data for preprocessing\n\nData preprocessing work should be applied equally for train-set and test-set.\n\nIn order to work at once, exclude the output variables ['casual', 'registered', 'count'] from 'train' and combine it with 'test'.","metadata":{"id":"8QTiBSXqbtpl"}},{"cell_type":"code","source":"all_features = pd.concat([train.drop(['casual', 'registered', 'count'], axis=1), test], axis=0)\ny_train = train[['casual', 'registered', 'count']]","metadata":{"id":"nSlpsUSQbpzP","execution":{"iopub.status.busy":"2022-05-16T04:59:58.794208Z","iopub.execute_input":"2022-05-16T04:59:58.79451Z","iopub.status.idle":"2022-05-16T04:59:58.815808Z","shell.execute_reply.started":"2022-05-16T04:59:58.794477Z","shell.execute_reply":"2022-05-16T04:59:58.814943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1-2. Missing Value Treatment\n\nMissing (NA) values in Data must be treated properly before model training.\n\nThere are three main treatment methods:\n1. Remove the variables which have NA values\n2. Remove the rows (observations) which have NA values\n3. Impute the NA values with other values\n\nWhich of the above methods is chosen depends on the analyst's discretion.\nIt is important to choose the appropriate method for the situation.","metadata":{"id":"Fia79Yb4YfnR"}},{"cell_type":"markdown","source":"### > Check missing values in each variable\n\nThere is no missing values in the data-set","metadata":{"id":"B3VvJe4ba9eH"}},{"cell_type":"code","source":"all_features.isnull().sum()","metadata":{"id":"BRVmI9DA8pfA","outputId":"97fe2a0b-d2fb-4e13-951f-59fb5c6fa8f7","execution":{"iopub.status.busy":"2022-05-16T05:00:01.587753Z","iopub.execute_input":"2022-05-16T05:00:01.588156Z","iopub.status.idle":"2022-05-16T05:00:01.600401Z","shell.execute_reply.started":"2022-05-16T05:00:01.588126Z","shell.execute_reply":"2022-05-16T05:00:01.599352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1-3. Variable modification","metadata":{"id":"RETuULj8Wwjs"}},{"cell_type":"markdown","source":"### > datetime variable\n\nFrom 'datetime' variable, we can extract the following informations \n- year\n- month \n- day\n- hour\n- name_of_day","metadata":{"id":"EUJdymg-Pqek"}},{"cell_type":"code","source":"all_features.head()","metadata":{"id":"rhmX2V71bE6a","outputId":"9e8161d0-bc34-4e64-f609-726584acd938","execution":{"iopub.status.busy":"2022-05-16T05:00:03.226428Z","iopub.execute_input":"2022-05-16T05:00:03.226725Z","iopub.status.idle":"2022-05-16T05:00:03.248814Z","shell.execute_reply.started":"2022-05-16T05:00:03.226693Z","shell.execute_reply":"2022-05-16T05:00:03.248085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_features['datetime'] = pd.to_datetime(all_features['datetime'])\n\nall_features['year'] = all_features['datetime'].dt.year\nall_features['month'] = all_features['datetime'].dt.month\nall_features['day'] = all_features['datetime'].dt.day\nall_features['hour'] = all_features['datetime'].dt.hour\nall_features['day_name'] = all_features['datetime'].dt.day_name()\n\nall_features.drop(['datetime'], axis=1, inplace=True)","metadata":{"id":"QRRN_TsTPHKp","execution":{"iopub.status.busy":"2022-05-16T05:00:03.995Z","iopub.execute_input":"2022-05-16T05:00:03.995537Z","iopub.status.idle":"2022-05-16T05:00:04.042755Z","shell.execute_reply.started":"2022-05-16T05:00:03.995503Z","shell.execute_reply":"2022-05-16T05:00:04.041831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### > Drop unuseful variables\n\n#### >> 'day'\n\nIn this competition, 1st to the 19th day of each month are divided into train data, and from the 20th to the end of the month are divided into test data.\nThus 'day' variable which indicates the day of the bike rental does not provide any sufficient information in training and prediction\n\n\n","metadata":{"id":"liEdEOyedgNS"}},{"cell_type":"code","source":"all_features.drop(['day'], axis=1, inplace=True)","metadata":{"id":"XXd-FQWedt5j","execution":{"iopub.status.busy":"2022-05-16T05:00:04.814142Z","iopub.execute_input":"2022-05-16T05:00:04.814437Z","iopub.status.idle":"2022-05-16T05:00:04.821927Z","shell.execute_reply.started":"2022-05-16T05:00:04.814405Z","shell.execute_reply":"2022-05-16T05:00:04.821176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1-4. Variable transformation","metadata":{"id":"dV_G-WmxV2zx"}},{"cell_type":"markdown","source":"### > int to object","metadata":{"id":"aAqAIz2XdFBe"}},{"cell_type":"code","source":"# int -> object\nvars = ['season', 'holiday', 'workingday', 'weather', 'year', 'month', 'hour']\nall_features[vars] = all_features[vars].astype('object')","metadata":{"id":"jLT15WzdV7YM","execution":{"iopub.status.busy":"2022-05-16T05:00:05.332095Z","iopub.execute_input":"2022-05-16T05:00:05.332519Z","iopub.status.idle":"2022-05-16T05:00:05.345047Z","shell.execute_reply.started":"2022-05-16T05:00:05.332488Z","shell.execute_reply":"2022-05-16T05:00:05.344071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1-5. Dummify categorical variables\n\nIn the case of linear modeling without regularization, the first or last column should be dropped (to prevent linear dependency), but here, for the convenience of using the factorization model, one-hot encoding method is used that does not drop any columns.","metadata":{"id":"hJayzx_-Nzck"}},{"cell_type":"code","source":"data_set = pd.get_dummies(all_features, drop_first=False)","metadata":{"id":"CsctjSYSQClH","execution":{"iopub.status.busy":"2022-05-16T05:00:05.968759Z","iopub.execute_input":"2022-05-16T05:00:05.969296Z","iopub.status.idle":"2022-05-16T05:00:06.017152Z","shell.execute_reply.started":"2022-05-16T05:00:05.969237Z","shell.execute_reply":"2022-05-16T05:00:06.016465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1-6. Scaling continuous variables\n\n\nMinMaxScaling maps all variables from 0 to 1 in order to consider only relative information, not absolute magnitudes of the values.\n\nBesides, it is known that scaling is often more stable in parameter optimization when training a model.","metadata":{"id":"Yo_M-blKP1pP"}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndata_set = scaler.fit_transform(data_set)","metadata":{"id":"W-F7cdreP1ZV","execution":{"iopub.status.busy":"2022-05-16T05:00:06.180041Z","iopub.execute_input":"2022-05-16T05:00:06.180336Z","iopub.status.idle":"2022-05-16T05:00:07.476581Z","shell.execute_reply.started":"2022-05-16T05:00:06.180305Z","shell.execute_reply":"2022-05-16T05:00:07.475843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1-7. Split Train & Test set","metadata":{"id":"3Bds2C0rQb_c"}},{"cell_type":"code","source":"n_train = train.shape[0]\nX_train = data_set[:n_train].astype(np.float32)\nX_test = data_set[n_train:].astype(np.float32)\ny_train = y_train.astype(np.float32)","metadata":{"id":"AZj86aMgaBrp","execution":{"iopub.status.busy":"2022-05-16T05:00:07.478011Z","iopub.execute_input":"2022-05-16T05:00:07.478362Z","iopub.status.idle":"2022-05-16T05:00:07.485306Z","shell.execute_reply.started":"2022-05-16T05:00:07.478332Z","shell.execute_reply":"2022-05-16T05:00:07.48453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1-8. Outlier Detection (*optional*)\n\nDetect and remove outlier observations that exist in the train-set.\n\n- Methodology: [Isolation Forest](https://ieeexplore.ieee.org/abstract/document/4781136/?casa_token=V7U3M1UIykoAAAAA:kww9pojtMeJtXaBcNmw0eVlJaXEGGICi1ogmeHUFMpgJ2h_XCbSd2yBU5mRgd7zEJrXZ01z2)\n  - How it works\n    - Isolation Forest applies a decision tree that repeats splits based on the 'random criterion' for the given data unitl only one observation remains in every terminal node (this is defined as 'isolation').\n    - Based on the number of splits used for isolation, 'normality' is defined. A smaller value means a higher degree of outlierness.\n    - By applying this decision tree several times, the average of the measured 'normality' values ​​is derived as the final 'normality' value.\n  - Assumptions\n    - Outliers require relatively few splits to be isolated.\n    - For normal data, the number of splits required to be isolated is relatively large.\n  - Outlier determination\n    - Determines whether it is an outlier or not based on the measured 'normality' value.\n      - sklearn's IsolationForest package determines based on '0' \n      - I, personally, think it is better to set the discriminant criterion by considering the 'distribution' of the 'normality' values.\n      - The details of the method is given below.","metadata":{"id":"xXpd4YS_Rooe"}},{"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\nclf = IsolationForest(\n    n_estimators=100,\n    max_samples='auto',\n    n_jobs=-1,\n    random_state=config['random_state'])\n\nclf.fit(X_train)\nnormality_df = pd.DataFrame(clf.decision_function(X_train), columns=['normality'])","metadata":{"id":"CT4-2FOcRnkM","execution":{"iopub.status.busy":"2022-05-16T05:00:07.486726Z","iopub.execute_input":"2022-05-16T05:00:07.487187Z","iopub.status.idle":"2022-05-16T05:00:08.768732Z","shell.execute_reply.started":"2022-05-16T05:00:07.487156Z","shell.execute_reply":"2022-05-16T05:00:08.767192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The dicriminant value \n  - The discriminant value (threshold) is defined by calculating the 1st quartile ($q_1$) and 3rd quartile ($q_3$) on the distribution of the measured normality values.\n    - with $k=1.5$\n\n$$threshold = q_1 - k*(q_3 - q_1)$$\n\n\n- Motivation\n  - This discriminant method is adapted from Tukey's boxplot idea.\nIn the distribution of any continuous variable, Tukey designates observations smaller than that value or larger than q_3 + k*(q_3 - q_1) as outliers.\n\n- How we do \n  - Our methodology does not apply the above method to a specific variable, but applies the method to the obtained normality.\n\n  - That is, it is based on the assumption that an outlier will be far left from the other observations in the measured normality distribution.","metadata":{"id":"1RNy0a3L77UI"}},{"cell_type":"code","source":"def outlier_threshold(normality, k=1.5):\n  q1 = np.quantile(normality, 0.25)\n  q3 = np.quantile(normality, 0.75)  \n  threshold = q1 - k*(q3-q1)\n  return threshold\n\nthreshold = outlier_threshold(normality_df['normality'].values, k=1.5)\n\nimport plotly.express as px\nfig = px.histogram(normality_df, x='normality', width=400, height=400)\nfig.add_vline(x=threshold, line_width=3, line_dash=\"dash\", line_color=\"red\")\nfig.show()","metadata":{"id":"fnuzws26Sm_7","outputId":"45cff69e-e2fb-4c9e-af74-42a839e8f24b","execution":{"iopub.status.busy":"2022-05-16T05:00:08.771096Z","iopub.execute_input":"2022-05-16T05:00:08.771396Z","iopub.status.idle":"2022-05-16T05:00:11.981848Z","shell.execute_reply.started":"2022-05-16T05:00:08.77136Z","shell.execute_reply":"2022-05-16T05:00:11.980768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\npx.box(normality_df, x='normality', orientation='h', width=400, height=400)","metadata":{"id":"HFc_y-fSUp4Y","outputId":"d0f2e2f7-31b7-4020-844f-929f70a84e9e","execution":{"iopub.status.busy":"2022-05-16T05:00:11.983446Z","iopub.execute_input":"2022-05-16T05:00:11.984669Z","iopub.status.idle":"2022-05-16T05:00:12.134774Z","shell.execute_reply.started":"2022-05-16T05:00:11.984613Z","shell.execute_reply":"2022-05-16T05:00:12.133561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train[normality_df['normality'].values>=threshold]\ny_train = y_train[normality_df['normality'].values>=threshold]\n\nprint('{} out of {} observations are removed from train_set'.format(train.shape[0] - X_train.shape[0], train.shape[0]))","metadata":{"id":"sTVOUiyma73J","outputId":"5558ca5c-5d16-4aaf-dcfb-9dc9e216504d","execution":{"iopub.status.busy":"2022-05-16T05:00:15.205777Z","iopub.execute_input":"2022-05-16T05:00:15.206046Z","iopub.status.idle":"2022-05-16T05:00:15.216455Z","shell.execute_reply.started":"2022-05-16T05:00:15.206019Z","shell.execute_reply":"2022-05-16T05:00:15.215217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1-9. Output variable consideration\n\n","metadata":{"id":"bC41gUQfiq66"}},{"cell_type":"markdown","source":"### > Multi-output variables\n\nAccording to the data description, the output variable 'count' is divided into two variables 'casual' and 'registered'. (i.e., count = registered + casual)\n\n#### >> Distribution differences between 'registerd' and 'casual'\n\n- Along 'hour' \n  - From the figure below, we can observe that the distributions between 'registered' and 'casual' are different along the 'hour'.\n  - 'registered' are more likely to rent bikes at commute time and 'casual' are more likely to rent bikes at daytime","metadata":{"id":"j_0_hVlTk_Vu"}},{"cell_type":"code","source":"df = pd.concat([all_features[:train.shape[0]], train[['casual', 'registered', 'count']]], axis=1)\n\ndf_melt = pd.melt(df, id_vars=['hour'], value_vars=['casual', 'registered'])\n\nfig = px.box(df_melt, x='hour', y='value', color='variable', width=600)\nfig.show()","metadata":{"id":"iRJX5VPDmhHk","outputId":"5aacbc5f-d0f6-4426-aec5-e0fbc14799eb","execution":{"iopub.status.busy":"2022-05-16T05:00:19.945014Z","iopub.execute_input":"2022-05-16T05:00:19.946313Z","iopub.status.idle":"2022-05-16T05:00:20.210073Z","shell.execute_reply.started":"2022-05-16T05:00:19.946262Z","shell.execute_reply":"2022-05-16T05:00:20.208675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- By 'workingday'\n  - From the figure below, we can observe that 'registered' tend to rent bikes on working days on the contrary 'casual' are more likely to rent bikes on not working days","metadata":{"id":"YjrQ4OlPo3Sp"}},{"cell_type":"code","source":"df = pd.concat([all_features[:train.shape[0]], train[['casual', 'registered', 'count']]], axis=1)\n\ndf_melt = pd.melt(df, id_vars=['workingday'], value_vars=['casual', 'registered'])\n\nfig = px.box(df_melt, x='workingday', y='value', color='variable', width=600)\nfig.show()","metadata":{"id":"G9AM53Z8o6IA","outputId":"f725ea20-8c82-4fad-9bff-c0b94831d7ca","execution":{"iopub.status.busy":"2022-05-16T05:00:23.02214Z","iopub.execute_input":"2022-05-16T05:00:23.022459Z","iopub.status.idle":"2022-05-16T05:00:23.274145Z","shell.execute_reply.started":"2022-05-16T05:00:23.022428Z","shell.execute_reply":"2022-05-16T05:00:23.272903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus intead of 'count', 'casual' and 'registered' are obtained as our output variables.\n\nAt the final prediction, the predicted values for 'casual' and 'registered' will be sumed up. ","metadata":{"id":"Xh9ZU-Usp2Fh"}},{"cell_type":"markdown","source":"### > Output variable transformation\n\n- Motivation\n  - Since this is a 'count regression', whose target variable indicates count and does not contain negative values, appropriate consideration should be given.\n\n- Statistics vs Machine learning\n  - From the perspective of Statistics, generalized linear model with proper link function is recommended instead of variable transformation like log-transformation ([Do not log-transform count data](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2010.00021.x)).\n\n  - But from the perspecitve of Machine learning, the prediction peformance is the priority. Thus some complicated models like Random Forest or Multi-layer perceptron should be considered although these models do not meet the statistical properties.\n\n- In this notebook\n  - In this notebook, log-transformation is performed on the output variables to apply a model such as a random forest.\n\n  - For those who want to approach it from a statistical point of view, it is recommended to use sklearn's [PoissonRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PoissonRegressor.html) model with out log-transformation.","metadata":{"id":"4QVqkcM5k8D0"}},{"cell_type":"code","source":"import plotly.express as px\n\ny_train_two = y_train[['casual', 'registered']].values.astype(np.float32)\n\ny_train_two_trans = np.log1p(y_train_two)","metadata":{"id":"65ZBlWR4jLkb","execution":{"iopub.status.busy":"2022-05-16T05:00:41.497558Z","iopub.execute_input":"2022-05-16T05:00:41.497847Z","iopub.status.idle":"2022-05-16T05:00:41.504658Z","shell.execute_reply.started":"2022-05-16T05:00:41.497817Z","shell.execute_reply":"2022-05-16T05:00:41.503098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Model Selection\n\nOur goal is to build a model that predicts the bike rental count (y) by summing up the values of 'registered' and 'casual' in every hour by using informations about that hour. The formula can be expressed as:\n\n$y = g_{registerd}(x) + h_{casual}(x) + \\epsilon$\n\nThis is a Multi-output regression problem. \nWith some adjustments, various machine learning models can be obtained. This notebook uses the following models.\n- Linear model (Poisson regression)\n- Support vector machine\n- Random forest\n- Xgboost\n- Multi-layer perceptron\n- Factorization machine\n\nHowever, we have to \"choose\" one final methodology to make predictions on the test-set.\nTo do so, a “fair evaluation” of the models is essential. \"Fair evaluation\" must satisfy the following two conditions.\n\n1. Select best hyperparameters for each model\n  - Bad hyperparmeter values could lead to the bad model performance.\n2. Same evaluation method\n  - If the evaluation method is not the same, comparison between models itself is impossible.\n\nWhen comparing models through an evaluation method that satisfies the above two conditions,\nOnly then the final model can be selected.\n\n\n","metadata":{"id":"3UbJJrv6i3kM"}},{"cell_type":"markdown","source":"## 2-1. Hyper parameter tuning by using Tune_SKlearn (Ray Tune)\n\n- Package: tune_sklearn\n  - This package makes it easy to apply [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) to sklearn models.\n  - Ray Tune is a python package that provides various hyperparameter tuning algorithms (HyperOpt, BayesianOptimization, ...).\n- Tuning procedure\n  - Define an appropriate search space for each model's hyperparameters.\n  - 5-fold CV (Cross Validation) is performed for each specific hyper-parameter value combination of the search space by using the hyper-parameter tuning algorithm (HyperOpt)\n    - Training: Training by using Scikit-Learn and Skorch packages\n    - Validation: Evaluate the model using an appropriate evaluation metric\n  - The hyperparameter with the highest average score of the CV result is designated as the optimal hyperparameter of the model.\n    - Save this CV result and use for model comparison\n\n","metadata":{"id":"TpffTJyuKCip"}},{"cell_type":"markdown","source":"### > Define a scoring function for hyper parameter tuning\n\nOur goal is to obatin the best hyperparameter in terms of RMSLE. \n\nBut since we perform Multi-output regression, a custom RMSLE function that computes the loss between true_y (count) and sum of 'registered' and 'casual' should be defined.","metadata":{"id":"305ulKi1qA62"}},{"cell_type":"code","source":"from sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\n\ndef neg_rmsle_custom(y_true_two_trans, y_pred_two):\n  try:\n    score = np.negative(mean_squared_error(np.log1p(np.expm1(y_true_two_trans).sum(axis=1)), \n                                         np.log1p(np.expm1(np.maximum(y_pred_two, 0)).sum(axis=1)), squared=False))\n  except:\n    score = np.nan\n  return score  \n\ntarget_metric = make_scorer(neg_rmsle_custom, greater_is_better=True)","metadata":{"id":"SslZlSJkqJS4","execution":{"iopub.status.busy":"2022-05-16T05:00:50.06468Z","iopub.execute_input":"2022-05-16T05:00:50.064978Z","iopub.status.idle":"2022-05-16T05:00:50.073294Z","shell.execute_reply.started":"2022-05-16T05:00:50.06494Z","shell.execute_reply":"2022-05-16T05:00:50.07181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### > Make a dataframe for containing CV results","metadata":{"id":"1yTxo63m_zeq"}},{"cell_type":"code","source":"model_list = []\nfor name in ['linear', 'svm', 'rf', 'xgb', 'mlp', 'fm']:\n  model_list.append(np.full(5, name))\n  \nbest_cv_df = pd.DataFrame({'model': np.hstack((model_list)), 'RMSLE':None, 'best_hyper_param':None})","metadata":{"id":"yecAASyT5ljV","execution":{"iopub.status.busy":"2022-05-16T05:00:51.611329Z","iopub.execute_input":"2022-05-16T05:00:51.611678Z","iopub.status.idle":"2022-05-16T05:00:51.618683Z","shell.execute_reply.started":"2022-05-16T05:00:51.61164Z","shell.execute_reply":"2022-05-16T05:00:51.617821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### > Linear model","metadata":{"id":"2MZsMccOKY2B"}},{"cell_type":"code","source":"from tune_sklearn import TuneSearchCV\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.linear_model import SGDRegressor\n\n# Define a search space\nparameters = {\n    'estimator__alpha': list(np.geomspace(1e-10, 1e-6, 5)),\n    'estimator__max_iter': [1000],\n    'estimator__tol': [1e-5, 1e-4, 1e-3],\n    'estimator__loss': ['squared_error'],  \n    'estimator__random_state': [config['random_state']]\n}\n\n# Define a Multi-output regressor\nbase_regr = SGDRegressor()\nregressor = MultiOutputRegressor(base_regr)\n\n# Specify a hyper parameter tuning algorithm\ntune_search = TuneSearchCV(\n    regressor,\n    parameters,\n    search_optimization='hyperopt',\n    n_trials=5,\n    n_jobs=-1,\n    scoring={'RMSLE':target_metric},\n    cv=5,\n    refit='RMSLE',\n    verbose=1,\n    random_state=config['random_state']\n    )\n\n# Run hyper parameter tuning\nX = X_train\ny = y_train_two_trans\ntune_search.fit(X, y)\n\n# Save the tuning results \nmodel_name = 'linear'\n\n## Save the optimal hyper parmater values\nbest_cv_df.loc[best_cv_df['model']==model_name, 'best_hyper_param'] = str(tune_search.best_params_)\n\n## Save the CV results\ncv_df = pd.DataFrame(tune_search.cv_results_)\ncv_values = cv_df.loc[tune_search.best_index_, cv_df.columns.str.startswith('split')].values\nbest_cv_df.loc[best_cv_df['model']==model_name, 'RMSLE'] = cv_values[:5]\n\n# Visualize the tuning results with parallel coordinate plot\ntune_result_df = pd.concat([pd.DataFrame(tune_search.cv_results_['params']), cv_df.loc[:,cv_df.columns.str.startswith('mean')] ], axis=1)\nimport plotly.express as px\npx.parallel_coordinates(tune_result_df, color='mean_test_RMSLE')","metadata":{"id":"2o4dQv1oreS4","outputId":"527517e4-82d5-4496-e67b-f8eabaca3862","execution":{"iopub.status.busy":"2022-05-16T05:00:53.669806Z","iopub.execute_input":"2022-05-16T05:00:53.670136Z","iopub.status.idle":"2022-05-16T05:01:22.87917Z","shell.execute_reply.started":"2022-05-16T05:00:53.670093Z","shell.execute_reply":"2022-05-16T05:01:22.878143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Support vector machine","metadata":{"id":"db18WC2yKc9p"}},{"cell_type":"code","source":"from tune_sklearn import TuneSearchCV\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\n\n# Define a search space\nparameters = {\n    'estimator__alpha': list(np.geomspace(1e-7, 1e-3, 3)),\n    'estimator__epsilon': list(np.geomspace(1e-5, 1e-1, 3)),\n    'estimator__loss': ['huber', 'epsilon_insensitive'],\n    'estimator__tol': [1e-5, 1e-4, 1e-3],\n    'estimator__max_iter': [1000],\n    'estimator__random_state': [config['random_state']]\n}\n\n\n# Define a Multi-output regressor\nbase_regr = SGDRegressor()\nregressor = MultiOutputRegressor(base_regr)\n\n# Specify a hyper parameter tuning algorithm\ntune_search = TuneSearchCV(\n    regressor,\n    parameters,\n    search_optimization='hyperopt',\n    n_trials=10,\n    n_jobs=-1,\n    scoring={'RMSLE':target_metric},\n    cv=5,\n    refit='RMSLE',\n    verbose=1,\n    random_state=config['random_state']\n    )\n\n# Run hyper parameter tuning\nX = X_train\ny = y_train_two_trans\ntune_search.fit(X, y)\n\n# Save the tuning results \nmodel_name = 'svm'\n\n## Save the optimal hyper parmater values\nbest_cv_df.loc[best_cv_df['model']==model_name, 'best_hyper_param'] = str(tune_search.best_params_)\n\n## Save the CV results\ncv_df = pd.DataFrame(tune_search.cv_results_)\ncv_values = cv_df.loc[tune_search.best_index_, cv_df.columns.str.startswith('split')].values\nbest_cv_df.loc[best_cv_df['model']==model_name, 'RMSLE'] = cv_values[:5]\n\n# Visualize the tuning results with parallel coordinate plot\ntune_result_df = pd.concat([pd.DataFrame(tune_search.cv_results_['params']), cv_df.loc[:,cv_df.columns.str.startswith('mean')] ], axis=1)\nimport plotly.express as px\npx.parallel_coordinates(tune_result_df, color='mean_test_RMSLE')","metadata":{"id":"VsI-OMilwGxb","outputId":"ce7407c2-df4d-4854-9c4e-88686e067c32","execution":{"iopub.status.busy":"2022-05-16T05:01:33.390766Z","iopub.execute_input":"2022-05-16T05:01:33.391125Z","iopub.status.idle":"2022-05-16T05:02:03.586265Z","shell.execute_reply.started":"2022-05-16T05:01:33.391084Z","shell.execute_reply":"2022-05-16T05:02:03.584997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random forest","metadata":{"id":"TZV1XLbAKffy"}},{"cell_type":"code","source":"from tune_sklearn import TuneGridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\n\n# Define a search space\nparameters = {\n    'estimator__n_estimators': [100, 300],\n    'estimator__criterion': ['squared_error'],\n    'estimator__max_depth': [25, 30, 35],\n    'estimator__max_features': ['auto'],\n    'estimator__random_state': [config['random_state']]\n}\n\n# Define a Multi-output regressor\nbase_regr = RandomForestRegressor()\nregressor = MultiOutputRegressor(base_regr)\n\n# Specify a hyper parameter tuning algorithm\ntune_search = TuneSearchCV(\n    regressor,\n    parameters,\n    search_optimization='hyperopt',\n    n_trials=6,\n    n_jobs=-1,\n    scoring={'RMSLE':target_metric},\n    cv=5,\n    refit='RMSLE',\n    verbose=1,\n    random_state=config['random_state']\n    )\n\n# Run hyper parameter tuning\nX = X_train\ny = y_train_two_trans\ntune_search.fit(X, y)\n\n# Save the tuning results \nmodel_name = 'rf'\n\n## Save the optimal hyper parmater values\nbest_cv_df.loc[best_cv_df['model']==model_name, 'best_hyper_param'] = str(tune_search.best_params_)\n\n## Save the CV results\ncv_df = pd.DataFrame(tune_search.cv_results_)\ncv_values = cv_df.loc[tune_search.best_index_, cv_df.columns.str.startswith('split')].values\nbest_cv_df.loc[best_cv_df['model']==model_name, 'RMSLE'] = cv_values[:5]\n\n# Visualize the tuning results with parallel coordinate plot\ntune_result_df = pd.concat([pd.DataFrame(tune_search.cv_results_['params']), cv_df.loc[:,cv_df.columns.str.startswith('mean')] ], axis=1)\nimport plotly.express as px\npx.parallel_coordinates(tune_result_df, color='mean_test_RMSLE')","metadata":{"id":"10x4_Hh9hqMN","outputId":"e1e0ce6a-6d11-4a15-b57c-d092fc2a4db9","execution":{"iopub.status.busy":"2022-05-16T05:02:07.52155Z","iopub.execute_input":"2022-05-16T05:02:07.522229Z","iopub.status.idle":"2022-05-16T05:08:33.821244Z","shell.execute_reply.started":"2022-05-16T05:02:07.522151Z","shell.execute_reply":"2022-05-16T05:08:33.819881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost","metadata":{"id":"NGiWtM5NKhA-"}},{"cell_type":"code","source":"from tune_sklearn import TuneSearchCV\nfrom xgboost import XGBRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\n\n# Define a search space\nparameters = {\n    'estimator__n_estimators': [50, 100, 200],\n    'estimator__learning_rate': [0.01, 0.1, 0.2],\n    'estimator__min_child_weight': [5, 10, 15],\n    'estimator__gamma': list(np.geomspace(1e-2, 1, 3)),\n    'estimator__subsample': [0.6, 1.0],\n    'estimator__colsample_bytree': [0.6, 1.0],\n    'estimator__max_depth': [15, 20, 25],\n    'estimator__objective': ['reg:squarederror'],\n    'estimator__random_state': [config['random_state']]\n}\n\n# Define a Multi-output regressor\nbase_regr = XGBRegressor(verbosity=0)\nregressor = MultiOutputRegressor(base_regr)\n\n# Specify a hyper parameter tuning algorithm\ntune_search = TuneSearchCV(\n    regressor,\n    parameters,\n    search_optimization='hyperopt',\n    n_trials=10,\n    n_jobs=-1,\n    scoring={'RMSLE':target_metric},\n    cv=5,\n    refit='RMSLE',\n    verbose=1,\n    random_state=config['random_state']\n    )\n\n# Run hyper parameter tuning\nX = X_train\ny = y_train_two_trans\ntune_search.fit(X, y)\n\n# Save the tuning results \nmodel_name = 'xgb'\n\n## Save the optimal hyper parmater values\nbest_cv_df.loc[best_cv_df['model']==model_name, 'best_hyper_param'] = str(tune_search.best_params_)\n\n## Save the CV results\ncv_df = pd.DataFrame(tune_search.cv_results_)\ncv_values = cv_df.loc[tune_search.best_index_, cv_df.columns.str.startswith('split')].values\nbest_cv_df.loc[best_cv_df['model']==model_name, 'RMSLE'] = cv_values[:5]\n\n# Visualize the tuning results with parallel coordinate plot\ntune_result_df = pd.concat([pd.DataFrame(tune_search.cv_results_['params']), cv_df.loc[:,cv_df.columns.str.startswith('mean')] ], axis=1)\nimport plotly.express as px\npx.parallel_coordinates(tune_result_df, color='mean_test_RMSLE')","metadata":{"id":"YtJ5wlpu9j21","outputId":"e169bbe7-9527-4378-83c0-cd3fcab01a8e","execution":{"iopub.status.busy":"2022-05-16T05:08:45.513449Z","iopub.execute_input":"2022-05-16T05:08:45.51376Z","iopub.status.idle":"2022-05-16T05:16:50.45829Z","shell.execute_reply.started":"2022-05-16T05:08:45.513728Z","shell.execute_reply":"2022-05-16T05:16:50.45694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multi-layer perceptron","metadata":{"id":"eMEkfLDXKmtD"}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom skorch.regressor import NeuralNetRegressor\nfrom skorch.callbacks import EarlyStopping\nfrom skorch.callbacks import EpochScoring\nfrom skorch.callbacks import Checkpoint\nfrom tune_sklearn import TuneSearchCV\nfrom sklearn.multioutput import MultiOutputRegressor\n\n# Define the model structure \n# - Note that MLP does not need to implement the 'MultiOutputRegressor' function from sklearn \n# - By adjusting the number of output layer unit, we can get the Multi-outputs\nclass MLP(nn.Module):\n    def __init__(self, num_inputs=X_train.shape[1], num_outputs=2, layer1=512, layer2=256, dropout1=0, dropout2=0):\n        super(MLP, self).__init__()\n\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(num_inputs, layer1),\n            nn.LeakyReLU(),\n            nn.Dropout(dropout1),\n            nn.Linear(layer1, layer2),\n            nn.LeakyReLU(),\n            nn.Dropout(dropout2),\n            nn.Linear(layer2, num_outputs)\n            )\n    def forward(self, x):\n        x = self.linear_relu_stack(x)\n        return x  \n\ndef try_gpu(i=0): \n    return f'cuda:{i}' if torch.cuda.device_count() >= i + 1 else 'cpu'\n\n# Set model configurations\n# - We can set the custom scoring function for early stopping by using 'callbacks'\n# - Set the scoring function as 'target_metric' which is the custom evaluation function we made above\nmlp = NeuralNetRegressor(\n    MLP(num_inputs=X_train.shape[1], num_outputs=2),\n    optimizer=torch.optim.Adam,\n    criterion=nn.MSELoss,\n    iterator_train__shuffle=True,\n    device=try_gpu(),\n    verbose=0,\n    callbacks=[EpochScoring(target_metric, lower_is_better=False, on_train=False, name='valid_neg_rmsle_custom'),\n               EarlyStopping(monitor='valid_neg_rmsle_custom', patience=5,\n                             threshold=1e-3, lower_is_better=False),\n               Checkpoint(monitor='valid_neg_rmsle_custom_best')]\n                          )\n\n# Define a search space\nparameters = {\n    'lr': list(np.geomspace(1e-5, 1e-2, 4)),\n    'module__layer1': [128, 256],\n    'module__layer2': [128, 256],\n    'module__dropout1': [0, 0.1],\n    'module__dropout2': [0, 0.1],\n    'optimizer__weight_decay': list(np.append(0, np.geomspace(1e-6, 1e-1, 6))),\n    'max_epochs': [1000],\n    'batch_size': [128],\n    'callbacks__EarlyStopping__threshold': [1e-3, 1e-4]\n    }\n\ndef use_gpu(device):\n    return True if not device == 'cpu' else False \n\n# Specify the hyper parameter tuning algorithm\ntune_search = TuneSearchCV(\n    mlp,\n    parameters,\n    search_optimization='hyperopt',\n    n_trials=15,\n    n_jobs=-1,\n    scoring={'RMSLE': target_metric},\n    cv=5,\n    refit='RMSLE',\n    verbose=1,\n    random_state=config['random_state']\n    )\n\n# Run hyper parameter tuning\nX = X_train\ny = y_train_two_trans\ntune_search.fit(X, y)\n\n# Save the tuning results \nmodel_name = 'mlp'\n\n## Save the optimal hyper parmater values\nbest_cv_df.loc[best_cv_df['model']==model_name, 'best_hyper_param'] = str(tune_search.best_params_)\n\n## Save the CV results\ncv_df = pd.DataFrame(tune_search.cv_results_)\ncv_values = cv_df.loc[tune_search.best_index_, cv_df.columns.str.startswith('split')].values\nbest_cv_df.loc[best_cv_df['model']==model_name, 'RMSLE'] = cv_values[:5]\n\n# Visualize the tuning results with parallel coordinate plot\ntune_result_df = pd.concat([pd.DataFrame(tune_search.cv_results_['params']), cv_df.loc[:,cv_df.columns.str.startswith('mean')] ], axis=1)\ntune_result_df.rename({\n    'callbacks__EarlyStopping__threshold':'Earlystoping_threshold',\n    'optimizer__weight_decay': 'weight_decay'\n    }, axis=1, inplace=True)\nimport plotly.express as px\npx.parallel_coordinates(tune_result_df, color='mean_test_RMSLE')","metadata":{"id":"1CCNhjAwPGK-","outputId":"a7921bf1-17ea-4d02-cbf7-7b5959d61f96","execution":{"iopub.status.busy":"2022-05-16T05:19:07.822332Z","iopub.execute_input":"2022-05-16T05:19:07.822712Z","iopub.status.idle":"2022-05-16T05:28:14.696738Z","shell.execute_reply.started":"2022-05-16T05:19:07.822668Z","shell.execute_reply":"2022-05-16T05:28:14.695833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Factorization Machine","metadata":{"id":"KenbYlKE5KUo"}},{"cell_type":"code","source":"def prepro_for_fm(X_train, X_test, bin_method='sturges'):\n  n_train = X_train.shape[0]\n  all = np.vstack((X_train, X_test))\n\n  col_num_uniq = np.apply_along_axis(lambda x: len(np.unique(x)), 0,  all)\n  remain_iidx = (col_num_uniq<=2)\n  to_bin_iidx = (col_num_uniq>2)\n\n  all_remain = all[:,remain_iidx]\n  all_to_bin = all[:,to_bin_iidx]\n  \n  for iter in range(all_to_bin.shape[1]):\n    bin_size = len(np.histogram(all_to_bin[:,iter], bins=bin_method)[0])\n    all_to_bin[:,iter] = pd.cut(all_to_bin[:,iter], bins=bin_size, labels=False)\n\n  all_to_bin_df = pd.DataFrame(all_to_bin).astype('object')\n  all_to_bin_array = pd.get_dummies(all_to_bin_df, drop_first=False).to_numpy()\n\n  all_array = np.hstack((all_to_bin_array, all_remain)).astype(np.int64)\n  field_dims = all_array.shape[1]\n  all_fm = np.vstack((np.apply_along_axis(lambda x: np.where(x==1), 1, all_array)))\n\n  return all_fm[:n_train], all_fm[n_train:], field_dims\n\n\nX_train_fm, X_test_fm, field_dims = prepro_for_fm(X_train, X_test, bin_method='sturges')","metadata":{"id":"zRUWGcUk1ee4","execution":{"iopub.status.busy":"2022-05-16T05:28:47.800722Z","iopub.execute_input":"2022-05-16T05:28:47.801334Z","iopub.status.idle":"2022-05-16T05:28:48.083758Z","shell.execute_reply.started":"2022-05-16T05:28:47.801259Z","shell.execute_reply":"2022-05-16T05:28:48.082859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom skorch.regressor import NeuralNetRegressor\nfrom skorch.callbacks import EarlyStopping\nfrom skorch.callbacks import EpochScoring\nfrom skorch.callbacks import Checkpoint\nfrom tune_sklearn import TuneSearchCV\nfrom sklearn.multioutput import MultiOutputRegressor\n\n# Define the model structure \n# - Note that FM does not need to implement the 'MultiOutputRegressor' function from sklearn \n# - By adjusting the number of output layer unit, we can get the Multi-outputs\nclass FM(nn.Module):\n    def __init__(self, num_inputs=field_dims, num_factors=20, output_dim=2):\n        super(FM, self).__init__()\n        self.output_dim = output_dim\n        for i in range(output_dim):\n          setattr(self, f'embedding_{i}', nn.Embedding(num_inputs, num_factors))\n        self.fc = nn.Embedding(num_inputs, output_dim)\n        self.bias = nn.Parameter(torch.zeros((output_dim,)))\n\n    def forward(self, x):\n        square_of_sum_list = []\n        sum_of_square_list = []\n        for i in range(self.output_dim):\n          square_of_sum_list.append(torch.sum(getattr(self, f'embedding_{i}')(x), dim=1)**2)\n          sum_of_square_list.append(torch.sum(getattr(self, f'embedding_{i}')(x)**2, dim=1))\n        square_of_sum = torch.stack(square_of_sum_list, dim=1)\n        sum_of_square = torch.stack(sum_of_square_list, dim=1)\n        x = self.bias + self.fc(x).sum(dim=1) + 0.5 * (square_of_sum - sum_of_square).sum(dim=2)\n        return x\n\ndef try_gpu(i=0): \n    return f'cuda:{i}' if torch.cuda.device_count() >= i + 1 else 'cpu'\n\n# Set model configurations\n# - We can set the custom scoring function for early stopping by using 'callbacks'\n# - Set the scoring function as 'target_metric' which is the custom evaluation function we made above\nfm = NeuralNetRegressor(\n    FM(num_inputs=field_dims, output_dim=2),\n    optimizer=torch.optim.Adam,\n    criterion=nn.MSELoss,\n    iterator_train__shuffle=True,\n    device=try_gpu(),\n    verbose=0,\n    callbacks=[EpochScoring(target_metric, lower_is_better=False, on_train=False, name='valid_neg_rmsle_custom'),\n               EarlyStopping(monitor='valid_neg_rmsle_custom', patience=5,\n                             threshold=1e-4, lower_is_better=False),\n               Checkpoint(monitor='valid_neg_rmsle_custom_best')]\n                          )\n\n# Define a search space\nparameters = {\n    'lr': [0.05, 0.1, 0.5],\n    'module__num_factors': [10, 20, 100],\n    'optimizer__weight_decay': [0.005, 0.01, 0.05, 0.1],\n    'max_epochs': [1000],\n    'batch_size': [128, 256],\n    'callbacks__EarlyStopping__threshold': [1e-3]\n    }\n\n\ndef use_gpu(device):\n    return True if not device == 'cpu' else False \n\n# Specify the hyper parameter tuning algorithm\ntune_search = TuneSearchCV(\n    fm,\n    parameters,\n    search_optimization='hyperopt',\n    n_trials=10,\n    n_jobs=-1,\n    scoring={'RMSLE': target_metric},\n    cv=5,\n    refit='RMSLE',\n    verbose=1,\n    random_state=config['random_state']\n    )\n\n# Run hyper parameter tuning\nX = X_train_fm\ny = y_train_two_trans\ntune_search.fit(X, y)\n\n# Save the tuning results \nmodel_name = 'fm'\n\n## Save the optimal hyper parmater values\nbest_cv_df.loc[best_cv_df['model']==model_name, 'best_hyper_param'] = str(tune_search.best_params_)\n\n## Save the CV results\ncv_df = pd.DataFrame(tune_search.cv_results_)\ncv_values = cv_df.loc[tune_search.best_index_, cv_df.columns.str.startswith('split')].values\nbest_cv_df.loc[best_cv_df['model']==model_name, 'RMSLE'] = cv_values[:5]\n\n# Visualize the tuning results with parallel coordinate plot\ntune_result_df = pd.concat([pd.DataFrame(tune_search.cv_results_['params']), cv_df.loc[:,cv_df.columns.str.startswith('mean')] ], axis=1)\ntune_result_df.rename({\n    'callbacks__EarlyStopping__threshold':'Earlystoping_threshold',\n    'optimizer__weight_decay': 'weight_decay'\n    }, axis=1, inplace=True)\ntune_result_df = tune_result_df[~tune_result_df['mean_test_RMSLE'].isnull()]\nimport plotly.express as px\npx.parallel_coordinates(tune_result_df, color='mean_test_RMSLE')","metadata":{"id":"iY8A8Zv4nJ4C","outputId":"a38b75fe-770e-46b4-ce6a-578072fee849","execution":{"iopub.status.busy":"2022-05-16T05:30:37.651635Z","iopub.execute_input":"2022-05-16T05:30:37.651963Z","iopub.status.idle":"2022-05-16T05:34:46.748936Z","shell.execute_reply.started":"2022-05-16T05:30:37.651933Z","shell.execute_reply":"2022-05-16T05:34:46.747194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2-2. Model comparison based on CV results with best hyper parameters\n\nCompare the CV results (measured using the optimal hyper parameter values)\n\nThe figure below shows that the rf, xgb, mlp, and fm models show superior performance compared to the linear and svm models.\n\n","metadata":{"id":"v4BlrQ6XK0ol"}},{"cell_type":"code","source":"fig = px.box(best_cv_df, x='model', y='RMSLE', color='model', width=600)\nfig.show()","metadata":{"id":"SuEiuzt7y_xZ","outputId":"4bdb1883-1787-4be7-9e4e-dc165b322e81","execution":{"iopub.status.busy":"2022-05-16T05:34:50.951782Z","iopub.execute_input":"2022-05-16T05:34:50.952071Z","iopub.status.idle":"2022-05-16T05:34:51.051038Z","shell.execute_reply.started":"2022-05-16T05:34:50.952042Z","shell.execute_reply":"2022-05-16T05:34:51.049875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2-3. Model Combination\n\nAlthough it is possible to select a final model among the models above, it has been observed that in many cases the combination of predicted values ​​from multiple models leads to improved prediction performance. ([Can multi-model combination really enhance the prediction skill of probabilistic ensemble forecasts?](https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.210?casa_token=OwyF2RbEywAAAAAA:gahpwGRdOWzLXyafYQQt_voHOF8MedTBLd1SBv4vkdT3ZTLVoKZQj3zl-KbrhSkX5x8CndeCxwBoL_-S))\n\nFor regression problems, the final predicted values are derived by combining the predicted values in a 'proper way'.\n\nThis notebook uses the following two model combination methods.\n\n1. Simple Average\n2. Stacked Generalization (Stacking)\n\n\nModel comparison needs to be done with single models (e.g., rf, xgb,...).\nSo model performance are measured by applying the same CV method as above.","metadata":{"id":"4n8Ah9ahKDDZ"}},{"cell_type":"markdown","source":"### > Simple Average\n\nThe simple average method derives the final prediction value by 'averaging' the predicted values of multiple models.\n\nThe top 3 models (svm, rf, xgb) of the above CV results are selected as base estimators used for the combination of predicted values.\n\nFor example,\n- Base Estimations\n  - $f_{svm}(x)$ = 0.85\n  - $f_{rf}(x)$ = 0.75\n  - $f_{xgb}(x)$ = 0.80\n- Final Estimation\n  - $f_{average}(x)$  = 0.8 (= 0.85 + 0.75 + 0.80 + / 3)\n","metadata":{"id":"Sf4wsWU4EHW-"}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom tqdm import notebook\nfrom sklearn.metrics import mean_squared_error\n\ndef CV_ensemble(ensemble_name, ensemble_func, estimators, X_train, y_train, n_folds=5, shuffle=True, random_state=2022):\n  kf = KFold(n_splits=5, random_state=random_state, shuffle=True)\n\n  res_list = []\n  for train_idx, valid_idx in notebook.tqdm(kf.split(X_train), total=kf.get_n_splits(), desc='Eval_CV'):\n    X_train_train, X_valid = X_train[train_idx], X_train[valid_idx]\n    y_train_train, y_valid = y_train[train_idx], y_train[valid_idx]\n\n    ensemble_pred = ensemble_func(estimators, X_train_train, y_train_train, X_valid)\n    neg_rmsle = neg_rmsle_custom(y_valid, ensemble_pred)\n\n    res_list.append([ensemble_name, neg_rmsle])\n  res_df = pd.DataFrame(np.vstack((res_list)))\n  res_df.columns = ['model', 'RMSLE']\n  return res_df\n\ndef average_reg_multi_output(estimators, X_train, y_train, X_test):\n  preds = []\n  for iter in range(len(estimators)):\n    try:\n      estimators[iter].module__num_factors\n    except: # for other models\n      estimators[iter].fit(X_train, y_train)\n      preds.append([estimators[iter].predict(X_test)])\n    else: # for factorization machine\n      X_train_fm, X_test_fm, _ = prepro_for_fm(X_train, X_test)\n      estimators[iter].fit(X_train_fm, y_train)\n      preds.append([estimators[iter].predict(X_test_fm)])\n    \n  avg_pred = np.vstack((preds)).mean(axis=0)\n  return avg_pred","metadata":{"id":"BJ6YHo9Bfl2Q","execution":{"iopub.status.busy":"2022-05-16T05:35:02.739731Z","iopub.execute_input":"2022-05-16T05:35:02.740002Z","iopub.status.idle":"2022-05-16T05:35:02.754062Z","shell.execute_reply.started":"2022-05-16T05:35:02.739974Z","shell.execute_reply":"2022-05-16T05:35:02.752922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\n\nlinear = MultiOutputRegressor(SGDRegressor()).set_params(**eval(best_cv_df.loc[best_cv_df['model']=='linear', 'best_hyper_param'].values[0]))\nsvm = MultiOutputRegressor(SGDRegressor()).set_params(**eval(best_cv_df.loc[best_cv_df['model']=='svm', 'best_hyper_param'].values[0]))\nrf = MultiOutputRegressor(RandomForestRegressor()).set_params(**eval(best_cv_df.loc[best_cv_df['model']=='rf', 'best_hyper_param'].values[0]))\nxgb = MultiOutputRegressor(XGBRegressor()).set_params(**eval(best_cv_df.loc[best_cv_df['model']=='xgb', 'best_hyper_param'].values[0]))\nmlp = mlp.set_params(**eval(best_cv_df.loc[best_cv_df['model']=='mlp', 'best_hyper_param'].values[0]))\nfm = fm.set_params(**eval(best_cv_df.loc[best_cv_df['model']=='fm', 'best_hyper_param'].values[0]))\n\nestimators = [xgb, mlp, fm]\nestimators_name = 'xgb_mlp_fm'\nensemble_name = 'average' + '_by_' + estimators_name\n\nX = X_train\ny = y_train_two_trans\n\nres_df = CV_ensemble(ensemble_name, average_reg_multi_output, estimators, X, y, n_folds=5, shuffle=True, random_state=config['random_state'])\nbest_cv_df = best_cv_df.append(res_df).reset_index(drop=True)","metadata":{"id":"oZc9d1u2dSSa","outputId":"b841f92e-378b-4b23-b8cc-eb11329c642c","execution":{"iopub.status.busy":"2022-05-16T05:35:12.754481Z","iopub.execute_input":"2022-05-16T05:35:12.755662Z","iopub.status.idle":"2022-05-16T05:37:47.680447Z","shell.execute_reply.started":"2022-05-16T05:35:12.755585Z","shell.execute_reply":"2022-05-16T05:37:47.679456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.box(best_cv_df, x='model', y='RMSLE', color='model', width=600)\nfig.show()","metadata":{"id":"F_0XuYrXhXmV","outputId":"23b14900-6d34-4d9d-d55e-2d6aedc9558b","execution":{"iopub.status.busy":"2022-05-16T05:37:47.683247Z","iopub.execute_input":"2022-05-16T05:37:47.683617Z","iopub.status.idle":"2022-05-16T05:37:47.779222Z","shell.execute_reply.started":"2022-05-16T05:37:47.683552Z","shell.execute_reply":"2022-05-16T05:37:47.778294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### > Stacked generalization (Stacking)\n\nIn the [Stacked generalization](https://www.jair.org/index.php/jair/article/view/10228), the predicted values of base estimators are treated as the 'input data', and y (SalePrice) of each row is treated as the 'output variable'. \nThe 'Meta Learner' is learned with these data and the predicted values of this model are derived as the final prediction values.\n\n- The 'Meta Learner' can be any of the regression models. However, this notebook uses a linear regression model for simplicity.\n\n- As input data for 'Meta Learner', predicted values for validation data in CV of base estimators are obtained.\n\n- Trained meta-learner predicts the final predicted values for the test-set by using the predicted values of the baes estimators for the test-set as input data.\n\nThe procedure is as follows:\n1. (Base estimators) Run CV on Train-set\n2. (Meta Learner) Train on CV predictions (predicted values on validation data of CV) with corresponding y values\n3. (Base estimators) Train on Train-set\n4. (Base estimators) Predict on Test-set\n5. (Meta Learner) Predict on predicted values from step 4.\n\n<img align='top' src='https://drive.google.com/uc?export=view&id=1uDxSIIFt8rUJkuIwRYU4lALvOPqlXPG5' width='600' height='400'>\n\n\nFor example,\n- Base Estimations\n  - $f_{svm}(x)$ = 0.85\n  - $f_{rf}(x)$ = 0.75\n  - $f_{xgb}(x)$ = 0.80\n- Meta Learner (linear regression)\n  - Parameter\n    - intercept = 0.1\n    - coefficient = [0.3, 0.1, 0.6]\n  - $f_{stack}(x) = 0.795 = -0.1 + 0.4*0.85 + 0.1*0.75 + 0.6*0.80$","metadata":{"id":"5eKbDaOmEPVc"}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom tqdm import notebook\n\ndef stack_reg_multi_output(estimators, X_train, y_train, X_test, n_folds=5, shuffle=True, random_state=2022):\n  num_estimators = len(estimators)-1\n  num_output = y_train.shape[1]\n\n  final_estimator = estimators[-1]\n  kf = KFold(n_splits=n_folds, random_state=random_state, shuffle=shuffle)\n  preds = []\n  y_valid_list = []\n  # Get CV predictions\n  for train_idx, valid_idx in notebook.tqdm(kf.split(X_train), total=kf.get_n_splits(), desc='Stack_CV'):\n    X_train_train, X_valid = X_train[train_idx], X_train[valid_idx]\n    y_train_train, y_valid = y_train[train_idx], y_train[valid_idx]\n    \n    valid_preds = []\n    for iter in range(num_estimators):\n      try:\n        estimators[iter].module__num_factors\n      except: # for other models\n        estimators[iter].fit(X_train_train, y_train_train)\n        valid_preds.append([estimators[iter].predict(X_valid)])\n      else: # for factorization machine\n        X_train_train_fm, X_valid_fm, _ = prepro_for_fm(X_train_train, X_valid)\n        estimators[iter].fit(X_train_train_fm, y_train_train)\n        valid_preds.append([estimators[iter].predict(X_valid_fm)])\n    \n    preds.append(np.hstack((np.vstack((valid_preds)))))\n    y_valid_list.append(y_valid)\n\n  cv_preds = np.vstack((preds))\n  cv_y = np.vstack((y_valid_list))\n  \n  # Get test predictions\n  test_preds =[]\n  for iter in range(num_estimators):\n    try:\n      estimators[iter].module__num_factors\n    except: # for other models\n      estimators[iter].fit(X_train, y_train)\n      test_preds.append([estimators[iter].predict(X_test)])\n    else: # for factorization machine\n      X_train_fm, X_test_fm, _ = prepro_for_fm(X_train, X_test)\n      estimators[iter].fit(X_train_fm, y_train)\n      test_preds.append([estimators[iter].predict(X_test_fm)])\n\n  test_preds_mat = np.hstack((np.vstack((test_preds))))\n\n  # Fit the final estimator on prediction values\n  final_estimator.fit(cv_preds, cv_y)\n  print('Training RMSLE: {}'.format(target_metric(final_estimator, cv_preds, cv_y)))\n  for iter in range(num_output):\n    print('Estimated coefficients of model {}: {} \\n intercept: {}'.format(iter, final_estimator.estimators_[iter].coef_, \n                                                                           final_estimator.estimators_[iter].intercept_))\n  test_ensemble_pred = final_estimator.predict(test_preds_mat)\n  return test_ensemble_pred","metadata":{"id":"ixki1gq_3Wwk","execution":{"iopub.status.busy":"2022-05-16T05:37:47.780583Z","iopub.execute_input":"2022-05-16T05:37:47.780885Z","iopub.status.idle":"2022-05-16T05:37:47.800615Z","shell.execute_reply.started":"2022-05-16T05:37:47.780852Z","shell.execute_reply":"2022-05-16T05:37:47.799167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.linear_model import LinearRegression\n\nimport warnings\nwarnings.filterwarnings(action='ignore', category=UserWarning)\n\n\nlinear = MultiOutputRegressor(SGDRegressor()).set_params(**eval(best_cv_df.loc[best_cv_df['model']=='linear', 'best_hyper_param'].values[0]))\nsvm = MultiOutputRegressor(SGDRegressor()).set_params(**eval(best_cv_df.loc[best_cv_df['model']=='svm', 'best_hyper_param'].values[0]))\nrf = MultiOutputRegressor(RandomForestRegressor()).set_params(**eval(best_cv_df.loc[best_cv_df['model']=='rf', 'best_hyper_param'].values[0]))\nxgb = MultiOutputRegressor(XGBRegressor()).set_params(**eval(best_cv_df.loc[best_cv_df['model']=='xgb', 'best_hyper_param'].values[0]))\nmlp = mlp.set_params(**eval(best_cv_df.loc[best_cv_df['model']=='mlp', 'best_hyper_param'].values[0]))\nfm = fm.set_params(**eval(best_cv_df.loc[best_cv_df['model']=='fm', 'best_hyper_param'].values[0]))\n\nreg = MultiOutputRegressor(LinearRegression())\n\nestimators = [xgb, mlp, fm, reg]\n\nestimators_name = 'xgb_mlp_fm'\nensemble_name = 'stack_linear' + '_by_' + estimators_name\nensemble_func = stack_reg_multi_output\nX = X_train\ny = y_train_two_trans\n\nres_df = CV_ensemble(ensemble_name, ensemble_func, estimators, X, y, n_folds=5, shuffle=True, random_state=config['random_state'])\nbest_cv_df = best_cv_df.append(res_df).reset_index(drop=True)","metadata":{"id":"hwvAfZIfiEnV","outputId":"aeb26be8-5053-474e-b3e7-911ea4f5aba0","execution":{"iopub.status.busy":"2022-05-16T05:37:47.802543Z","iopub.execute_input":"2022-05-16T05:37:47.802921Z","iopub.status.idle":"2022-05-16T05:50:41.565559Z","shell.execute_reply.started":"2022-05-16T05:37:47.802875Z","shell.execute_reply":"2022-05-16T05:50:41.562785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2-4. Model Comparison based on CV results including model combination methods\n\nFrom the figure below, 'xgb' shows the best performance among single models.\nAmong the model combination methodologies, it can be seen that the 'stack_ridge_by_rf_xgb_mlp_fm' method shows the best performance.","metadata":{"id":"jW2701CE7RJp"}},{"cell_type":"code","source":"import plotly.express as px\nfig = px.box(best_cv_df, x='model', y='RMSLE', color='model', width=1000)\nfig.show()","metadata":{"id":"yg9dn4HuzAtL","outputId":"4d886f19-f365-4cca-9e98-bdfa458b51e9","execution":{"iopub.status.busy":"2022-05-16T05:50:48.946145Z","iopub.execute_input":"2022-05-16T05:50:48.946457Z","iopub.status.idle":"2022-05-16T05:50:49.049407Z","shell.execute_reply.started":"2022-05-16T05:50:48.946417Z","shell.execute_reply":"2022-05-16T05:50:49.048333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Make a prediction with the best model","metadata":{"id":"yDnorQYRQ0hi"}},{"cell_type":"code","source":"from sklearn.linear_model import SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\n\nlinear = MultiOutputRegressor(SGDRegressor()).set_params(**eval(best_cv_df.loc[best_cv_df['model']=='linear', 'best_hyper_param'].values[0]))\nsvm = MultiOutputRegressor(SGDRegressor()).set_params(**eval(best_cv_df.loc[best_cv_df['model']=='svm', 'best_hyper_param'].values[0]))\nrf = MultiOutputRegressor(RandomForestRegressor()).set_params(**eval(best_cv_df.loc[best_cv_df['model']=='rf', 'best_hyper_param'].values[0]))\nxgb = MultiOutputRegressor(XGBRegressor()).set_params(**eval(best_cv_df.loc[best_cv_df['model']=='xgb', 'best_hyper_param'].values[0]))\nmlp = mlp.set_params(**eval(best_cv_df.loc[best_cv_df['model']=='mlp', 'best_hyper_param'].values[0]))\nfm = fm.set_params(**eval(best_cv_df.loc[best_cv_df['model']=='fm', 'best_hyper_param'].values[0]))\n\nestimators = [xgb, mlp, fm, reg]\nestimators_name = 'xgb_mlp_fm'\n\n\nreg = MultiOutputRegressor(LinearRegression())\n\nensemble_func = stack_reg_multi_output\nensemble_name = 'stack_linear_by_{}'.format(estimators_name)\n\n\nX = X_train\ny = y_train_two_trans\n\npred_two = ensemble_func(estimators, X, y, X_test, n_folds=5, shuffle=True, random_state=config['random_state'])\npred_trans = np.expm1(pred_two).sum(axis=1)\nres_df = pd.DataFrame({'datetime': test['datetime'], 'count': pred_trans})\n\nres_df.to_csv('submission.csv', index=False)","metadata":{"id":"aDiA_VtdPvA3","outputId":"46222cf7-93af-4335-f760-fa9c3121bc7f","execution":{"iopub.status.busy":"2022-05-16T05:52:19.234307Z","iopub.execute_input":"2022-05-16T05:52:19.234681Z","iopub.status.idle":"2022-05-16T05:55:34.430667Z","shell.execute_reply.started":"2022-05-16T05:52:19.234644Z","shell.execute_reply":"2022-05-16T05:55:34.429708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}