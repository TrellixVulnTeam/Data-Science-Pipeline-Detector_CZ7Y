{"cells":[{"metadata":{"scrolled":false,"_kg_hide-input":false,"_cell_guid":"26a9c0c8-ed32-45ff-b56a-f9e2bc5392f9","_kg_hide-output":false,"_uuid":"8d1f8385cae4e40d7841a9ae9b8110102f70bb97"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime\nimport seaborn as sns\nsns.set(font_scale=1.5)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom subprocess import check_output\n\nprint(check_output([\"ls\", \"../input/\"]).decode(\"utf8\"))\n\n    \n    \n    \ndf_train= pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\n\ndf_joined = df_train.append(df_test,ignore_index=True)\n\nprint(df_joined.head(100))\nprint(df_test.head(100))\n\nprint(df_train.describe())\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"02d66fce-5966-4a3e-aece-10cefc8b725a","_uuid":"80ba973a96f3ecac352ba8c1ae867412c77ec758"},"source":"# we will need this error function later.\ndef rmsle(y, y_,convertExp=True):\n    if convertExp:\n        y = np.exp(y),\n        y_ = np.exp(y_)\n    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))\n    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))\n    calc = (log1 - log2) ** 2\n    return np.sqrt(np.mean(calc))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"scrolled":false,"collapsed":true,"_cell_guid":"4e328f45-8de2-4185-aa4b-1545c127ca20","_uuid":"03b630fe9e2411285c691b3957d4f9fc39f20cd6"},"source":"# Let's engineer the data\ndf_joined['Month'] = df_joined['datetime'].map(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S').month)\ndf_joined['Hour'] = df_joined['datetime'].map(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S').hour)\ndf_joined['Weekday'] = df_joined['datetime'].map(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S').weekday())\n\n#df_joined['Weekday'] = df_joined['Weekday'].map({0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'})\n#df_joined['season'] = df_joined['season'].map({1:'Spring',2:'Summer',3:'Fall',4:'Winter'})\n#df_joined['weather'] = df_joined['weather'].map({1:'Clear',2:'Mist',3:'Light Rain',4:'Heavy Rain'})\n\ndatetimecol = df_test[\"datetime\"]\n\ndf_joined.drop('datetime',inplace=True,axis=1)\ndf_train = df_joined[ df_joined['count'].notnull()]\ndf_test = df_joined[ df_joined['count'].isnull()]\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"231ae788-f5c7-41e1-a11a-6e1c11ed5102","_uuid":"000262c111cdbc777cc26ef5ab13c753401eb454"},"source":"# Let's add some visualizations, inspired by those of Vivek Srinivasan. Thanks Vivek!\n\nfig,axes= plt.subplots(2,2,figsize=(20,20))\nsns.boxplot(data=df_train, y='count',x='weather',ax=axes[0,0])\nsns.boxplot(data=df_train, y='count',x='season',ax=axes[0,1])\nsns.boxplot(data=df_train, y='count',x='Weekday',ax=axes[1,0])\nsns.boxplot(data=df_train, y='count',x='Month',ax=axes[1,1])\n\ndf_Month_grouped = df_train[['count','Month']].groupby('Month',as_index=False).mean()\ndf_Hour_season_grouped = (df_train.groupby([\"Hour\",\"season\"],as_index=False)[\"count\"].mean())\ndf_Hour_Weekday_grouped = (df_train.groupby([\"Hour\",\"Weekday\"],as_index=False)[\"count\"].mean())\ndf_Hour_Month_grouped = (df_train.groupby([\"Hour\",\"Month\"],as_index=False)[\"count\"].mean())\ndf_Hour_weather_grouped = (df_train.groupby([\"Hour\",\"weather\"],as_index=False)[\"count\"].mean())\n\nfig,axes= plt.subplots(5,1,figsize=(30,30))\nsns.barplot( x = df_Month_grouped['Month'], y = df_Month_grouped['count'], ax=axes[0])\nsns.pointplot( y = df_Hour_season_grouped['count'], x = df_Hour_season_grouped['Hour'],  hue=df_Hour_season_grouped['season'], join=True,ax=axes[1])\nsns.pointplot( y = df_Hour_Weekday_grouped['count'], x = df_Hour_Weekday_grouped['Hour'],  hue=df_Hour_Weekday_grouped['Weekday'], join=True,ax=axes[2])\nsns.pointplot( y = df_Hour_Month_grouped['count'], x = df_Hour_Month_grouped['Hour'],  hue=df_Hour_Month_grouped['Month'], join=True,ax=axes[3])\nsns.pointplot( y = df_Hour_weather_grouped['count'], x = df_Hour_weather_grouped['Hour'],  hue=df_Hour_weather_grouped['weather'], join=True,ax=axes[4])\n\n\n#sns.countplot(data=df_train, x='Hour', hue='season',ax=axes[0,1])\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"4f10d3b1-e848-414d-87ea-5bdb5503f377","_uuid":"605b958a60be553504fc72d0a40545e2b78104a6"},"cell_type":"markdown","source":"We see that bike rentals peak around 8 am and 5 pm on weekdays -- clearly due to people who rent bikes to commute for work."},{"metadata":{"_cell_guid":"631e40e7-5e46-4cf5-b535-b04330d62d0e","_uuid":"123a7a3bc94d0dd5f3b7d411f91225e2a90b9c07"},"source":"sns.heatmap(data=df_train.corr(), linewidths=2)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"a9e707b2-f8a1-4be9-8f4c-fa06d3abdfe4","_uuid":"53d887212c0cfb5f46134a55dbbf2013d74ae9fb"},"cell_type":"markdown","source":"We observe that temp and atemp are very strongly correlated. This might cause multicorrelation, hence we will drop atemp column from the input data. Also, registered-count and casual-count are very correlated as expected. These are leakege variables, hence we will get rid of them too."},{"metadata":{"_cell_guid":"e586cd72-fadb-49db-95f9-6af7d6973411","_uuid":"fbe7257bbf966f7d073407a4bd82395531723671"},"source":"df_test.drop(['casual','registered','count','atemp'],inplace=True,axis=1)\ndf_train.drop(['casual','registered','atemp'],inplace=True,axis=1)\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"3a0c4ffa-26a3-4f5f-b2ce-8629f9535954","_uuid":"5e599b304d11534e31b55843e9ad1f50edff58e3"},"cell_type":"markdown","source":"Now that we have sufficiently visualized the data, let's start the learning step. First, we split the training data.\n"},{"metadata":{"collapsed":true,"_cell_guid":"714f5888-3f45-42a9-88b5-00769af8e1bf","_uuid":"87bc330acade013b6a628d07e947704c39e40f9e"},"source":"from sklearn.model_selection import train_test_split\nX_train,X_cv,y_train,y_cv = train_test_split(df_train.drop('count',axis=1),df_train['count'],test_size=0.2)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"bc6e85a5-7392-4aff-8ecc-a11ac331848c","_uuid":"2326af554018719da13ace4b3a173fbdc8d42b48"},"cell_type":"markdown","source":"And now, let's try our favorite ML algorithms."},{"metadata":{"_cell_guid":"e7b3f0ba-961a-4946-8efa-e45643a33cfb","_uuid":"72becf4b2869999fadd163ea86742a5ea9b8e5c5"},"source":"print(df_train.columns)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"scrolled":false,"_cell_guid":"2c58929c-d893-4e5a-98b4-07b2db045f69","_uuid":"de378f9fa24bfde0ef5c0a7effa813c9e0948dcf"},"source":"from sklearn.ensemble import RandomForestRegressor\n\nRFR = RandomForestRegressor(n_estimators = 200, max_depth=10 )\nRFR.fit(X_train, np.log1p(y_train))\npreds = RFR.predict(X = X_cv)\nprint('Random forest rmsle =', rmsle(np.exp([max(x,0) for x in preds]),y_cv,False))\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"27a61fed-6c06-451d-92d6-28d50c1bcecc","_uuid":"c91e9c060f0e689f8d0ac571ac5ecf18f658a7ce"},"source":"from sklearn.ensemble import GradientBoostingRegressor\nGBM = GradientBoostingRegressor(n_estimators=2000)\nGBM.fit(X_train, np.log1p(y_train))\npreds = GBM.predict(X = X_cv)\nprint('GBM rmsle =', rmsle( np.exp([max(x,0) for x in preds]) ,y_cv,False))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"763cc3ec-28c0-4597-a056-69aff3b76a6b","_uuid":"3332b8c20be0a896610b651a683e3bf50895641d"},"source":"finalEstimator = GradientBoostingRegressor(n_estimators=2000)\nGBM.fit(df_train.drop('count',axis=1),np.log1p(df_train['count']))\npreds = GBM.predict(df_test)\n\n\nsubmission = pd.DataFrame({\n        \"datetime\": datetimecol,\n        \"count\": [max(0, x) for x in np.exp(preds)]\n    })\nsubmission.to_csv('bike_predictions_gbm_separate_without_fe.csv', index=False)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"b5dd3669-73fc-4f7d-9358-d8d591a6cdfb","_uuid":"49dc1d48b2e1a0d78426b49a9b3040cf6c77f352"},"cell_type":"markdown","source":""}],"nbformat":4,"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","version":"3.6.1"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat_minor":1}