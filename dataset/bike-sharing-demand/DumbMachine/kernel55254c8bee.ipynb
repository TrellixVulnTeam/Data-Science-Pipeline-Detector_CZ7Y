{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":9,"outputs":[{"output_type":"stream","text":"['test.csv', 'train.csv', 'sampleSubmission.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_train_log = np.log1p(self.y_train)\nsomething = CustomClassifiers(X_train, y_train,X_train, y_train)\nsomething.baseline_models()","execution_count":39,"outputs":[{"output_type":"stream","text":"[Lasso] Accuracy Score: 1.0326835496411915\n[GradientBoostingRegressor] Accuracy Score: 0.34875476792344495\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\ngbm = GradientBoostingRegressor(n_estimators=4000,alpha=0.01); ### Test 0.41\nyLabelsLog = np.log1p(y_train)\ngbm.fit(X_train,yLabelsLog)\npreds = gbm.predict(X= X_train)\nprint (\"RMSLE Value For Gradient Boost: \",rmsle(np.exp(yLabelsLog),np.exp(preds),False))","execution_count":43,"outputs":[{"output_type":"stream","text":"RMSLE Value For Gradient Boost:  0.19825429204404965\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\ngbm = XGBClassifier(n_estimators=400,alpha=0.01); ### Test 0.41\nyLabelsLog = np.log1p(y_train)\ngbm.fit(X_train,yLabelsLog)\npreds = gbm.predict(X= x_train)\nprint (\"RMSLE Value For Gradient Boost: \",rmsle(np.exp(yLabelsLog),np.exp(preds),False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['year'].dtype","execution_count":57,"outputs":[{"output_type":"execute_result","execution_count":57,"data":{"text/plain":"dtype('int64')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import asyncio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['year'] = train['datetime'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S').year).astype('int')\ntrain['month'] = train['datetime'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S').month).astype('int')\ntrain['day'] = train['datetime'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S').day).astype('int')\ntrain['hour'] = train['datetime'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S').hour).astype('int')\n\ndrop_features  = ['datetime', 'count', 'casual', 'registered', 'temp', 'windspeed']\ncategoricalFeatureNames = [\"season\",\"holiday\",\"workingday\",\"weather\",\"month\",\"year\",\"hour\"]\nfor var in categoricalFeatureNames:\n    train[var] = train[var].astype(\"int\")\n    \ntest['year'] = test['datetime'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S').year).astype('int')\ntest['month'] = test['datetime'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S').month).astype('int')\ntest['day'] = test['datetime'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S').day).astype('int')\ntest['hour'] = test['datetime'].apply(lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S').hour).astype('int')\n\ntest_drop_features  = ['datetime', 'temp', 'windspeed']\ncategoricalFeatureNames = [\"season\",\"holiday\",\"workingday\",\"weather\",\"month\",\"year\",\"hour\"]\nfor var in categoricalFeatureNames:\n    test[var] = test[var].astype(\"int\")\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(drop_features, axis=1), train['count'], test_size = 0.3, random_state = 100)","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"something.baseline_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef rmsle(y, y_, convertExp=True):\n    if convertExp:\n        y = np.exp(y),\n        y_ = np.exp(y_)\n    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))\n    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))\n    calc = (log1 - log2) ** 2\n    return np.sqrt(np.mean(calc))\n\nimport warnings\n\nimport numpy as np\nfrom lightgbm import LGBMClassifier\nfrom sklearn import metrics\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import (AdaBoostClassifier, BaggingClassifier,\n                              ExtraTreesClassifier, GradientBoostingClassifier,\n                              GradientBoostingRegressor,\n                              RandomForestClassifier, RandomForestRegressor,\n                              VotingClassifier)\nfrom sklearn.linear_model import Lasso, LinearRegression, Ridge, SGDClassifier\nfrom sklearn.metrics import (accuracy_score, confusion_matrix, precision_score,\n                             r2_score, recall_score, mean_squared_error,\n                             roc_auc_score)\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\n\n# from Statistica.utils.metrics import rmsle\n\n\nclass CustomClassifiers():\n    '''\n    Class to Create and Run multiple Classifiers from Sklearn\n    ---------------------------------------------------------\n    Currently Supports:\n    - Linear Models:\n        - Linear Regression\n        - Logistic Regression\n        - LDA\n\n    - Non Linear Models:\n        - KNN\n        - SVM\n        - Naive Bayes\n        - SGDClassifier\n\n    - Regularization Models:\n        - Ridge\n        - Lasso\n\n    - Ensemble Models:\n        - Random Forest\n        - Gradient Boost\n    '''\n\n    def __init__(\n        self,\n        x_train,\n        y_train,\n        x_test,\n        y_test,\n        # classifier,\n        problem_type='Classification',\n        n_classifiers=100,\n        range_values=False,\n        exec_all=False,\n        classifiers_list=None\n    ):\n        '''\n        Initiaizing the CustomClassifier Class:\n\n        Parameters:\n        ----------------------------------------\n        x_train: pd.DataFrame\n            Features Data.\n        y_train: pd.DataFrame\n            Target Data.\n        x_test: pd.DataFrame\n            Test Features Data.\n        y_Test: pd.DataFrame\n            Test Target Data.\n        n_classifiers: int\n            n_classifiers argumnet for sklearn Classifiers.\n        range_values: bool\n            Get model fits on range of values in range 0..n_classifiers.\n        exec_all: bool\n            Executes all the implemented models.\n        classfiers_list: list\n            List of the Models to be executed.\n\n        '''\n        self.x_train = x_train\n        self.y_train = y_train\n        self.x_test = x_test\n        self.y_test = y_test\n        self.n_classifiers = n_classifiers\n        self.range_value = range_values\n        self.exec_all = exec_all\n        self.classifiers_list = classifiers_list\n\n        if self.classifiers_list is None:\n            self.exec_all = True\n\n        self.baseline_results = []\n\n        self.many_results = []\n        self.problem_type = problem_type\n        self.best = None\n\n    def set_classifier(self, classifier):\n        '''\n        Function returns the Classifier from the Same string name.\n        ----------------------------------------------------------\n        Parameters:\n        !INFO: The names of Classifiers are the same as the OG imports in lowercase.!\n        - self: CustomClassifiers Object\n            Refernece to the Instance Variable.\n        - classifier: string\n            Name of the Classifier\n\n        Returns:\n            Required Classifier\n        '''\n\n        if classifier == 'LinearRegression':\n            return LinearRegression\n        elif classifier == 'LogisticRegression':\n            return LogisticRegression\n        elif classifier == 'KNN':\n            return knn\n        elif classifier == 'SVM':\n            return svm\n        elif classifier == 'LDA':\n            return lda\n        elif classifier == 'DecisionTreeClassifier':\n            return decisiontreeclassifier\n        elif classifier == 'RandomForestClassifier':\n            return randomforestclassifier\n        elif classifier == 'Lasso':\n            return Lasso\n        elif classifier == 'Ridge':\n            return ridge\n        elif classifier == 'SGDClassifier':\n            return sgdclassifier\n        elif classifier == 'ExtraTreesClassifier':\n            return ExtraTreesClassifier\n        elif classifier == 'BaggedDecisionTreeClassifier':\n            return BaggedDecisionTreeClassifier\n        elif classifier == 'AdaBoostClassifier':\n            return AdaBoostClassifier\n        # ! UnderDev\n        elif classifier == 'ViolinThing':\n            return violinthing\n\n        elif classifier == 'XGBClassifier':\n            return XGBClassifier\n        elif classifier == 'LGBMClassifier':\n            return LGBMClassifier\n        elif classifier == \"GradientBoostingRegressor\":\n            return GradientBoostingRegressor\n        else:\n            raise NotImplementedError('{} Is not yet implemented'.format(classifier))\n\n    def set_metric(self, metric=None):\n        if metric:\n            if metric == 'accuracy_score':\n                return accuracy_score\n            elif metric == \"confusion_matrix\":\n                return confusion_matrix\n            elif metric == \"precision_score\":\n                return precision_score\n            elif metric == \"mean_squared_error\":\n                return mean_squared_error\n            elif metric == \"recall_score\":\n                return recall_score\n            elif metric == \"r2_score\":\n                return r2_score\n            elif metric == \"roc_auc_score\":\n                return roc_auc_score\n        else:\n            if self.problem_type == \"Classification\":\n                return rmsle\n            elif self.problem_type == \"Regression\":\n                # options = [Mean Absolute Error. Mean Squared Error. R^2.]\n                return accuracy_score\n            else:\n                raise NotImplementedError\n\n    def baseline_models(self, ret=False):\n        model_list = [\n            'Lasso',\n            'GradientBoostingRegressor',\n            # 'AdaBoost'\n        ]\n        for model in model_list:\n            clf = self.set_classifier(model)()\n            metric = self.set_metric()\n            if metric.__name__ == 'rmsle':\n                y_train_log = np.log1p(self.y_train)\n                clf.fit(self.x_train, y_train_log)\n                pred = clf.predict(self.x_test)\n                acc = self.rmsle(np.exp(y_train_log), np.exp(pred), False)\n                self.baseline_results.append([\n                    model, acc\n                ])\n        for result in self.baseline_results:\n            print(f\"[{result[0]}] Accuracy Score: {result[1]}\")\n        return\n        acc = self.set_metric()(self.y_test, pred)\n        # try:\n        #     clf.fit(self.x_train, self.y_train)\n        #     pred = clf.predict(self.x_test)\n        #     acc = accuracy_score(self.y_test, pred)\n        # except ValueError:\n        #     yLabelsLog = np.log1p(self.y_train)\n        #     clf.fit(self.x_train, self.y_train)\n        #     pred = clf.predict(self.x_test)\n        #     # acc = self.set_metric()(self.y_test, pred)\n        #     acc = self.rmsle(np.exp(yLabelsLog), np.exp(pred), False)\n        self.baseline_results.append([\n            model, acc\n        ])\n        for result in self.baseline_results:\n            print(f\"[{result[0]}] Accuracy Score: {result[1]}\")\n\n    def rmsle(self, y, y_, convertExp=True):\n        if convertExp:\n            y = np.exp(y),\n            y_ = np.exp(y_)\n        log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))\n        log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))\n        calc = (log1 - log2) ** 2\n        return np.sqrt(np.mean(calc))\n\n    # TODO: Change the Accuracy thing for this\n    def lr(self):\n        clf = LinearRegression()\n        # clf = np.log1p(self.y_train)\n        clf.fit(self.x_train, self.y_train)\n        pred = clf.predict(self.x_test)\n        self.baseline_results.append(['lr', accuracy_score(self.y_test, pred)])\n        # Cant use with Regression\n        print(\"The AccuracyScore for Linear Regersion: \", )\n\n    def ridge(self):\n        ridge_m_ = Ridge()\n        ridge_params_ = {'max_iter': [3000], 'alpha': [0.1, 1, 2, 3, 4, 10, 30,\n                                                       100, 200, 300, 400, 800,\n                                                       900, 1000]}\n        rmsle_scorer = metrics.make_scorer(accuracy_score,\n                                           greater_is_better=False)\n        grid_ridge_m = GridSearchCV(ridge_m_, ridge_params_,\n                                    scoring=rmsle_scorer, cv=5)\n        yLabelsLog = np.log1p(self.y_train)\n        grid_ridge_m.fit(self.x_train, yLabelsLog)\n        pred = grid_ridge_m.predict(X=self.x_train)\n        self.baseline_results.append(['lr', accuracy_score(self.y_test, pred)])\n        print(\"RMSLE Value For Extra Trees is: \",\n              accuracy_score(self.y_test, pred))\n\n    def lasso(self):\n        lasso_m_ = Lasso()\n\n        alpha = 1 / np.array([0.1, 1, 2, 3, 4, 10, 30,\n                              100, 200, 300, 400, 800,\n                              900, 1000])\n        lasso_params_ = {'max_iter': [3000], 'alpha': alpha}\n        rmsle_scorer = metrics.make_scorer(self.rmsle, greater_is_better=False)\n        grid_lasso_m = GridSearchCV(lasso_m, lasso_params,\n                                    scoring=rmsle_scorer, cv=5)\n\n        yLabelsLog = np.log1p(self.y_train)\n        grid_lasso_m.fit(self.x_train, yLabelsLog)\n        pred = grid_lasso_m.predict(X=self.x_train)\n        self.baseline_results.append(['lr', accuracy_score(self.y_test, pred)])\n        print(\"RMSLE Value For Extra Trees is: \",\n              accuracy_score(self.y_test, pred))\n\n    def randomT(self):\n        clf = RandomForestRegressor(n_estimators=100)\n        clf.fit(self.x_train, self.y_train)\n        pred = clf.predict(X=self.x_train)\n        self.baseline_results.append(['lr', accuracy_score(self.y_test, pred)])\n\n        print(\"RMSLE Value For Extra Trees is: \",\n              accuracy_score(self.y_test, pred))\n\n    def gradB_r(self):\n        clf = GradientBoostingRegressor(n_estimators=4000, alpha=0.01)\n        clf.fit(self.x_train, self.y_train)\n        pred = clf.predict(X=self.x_train)\n        self.baseline_results.append(['lr', accuracy_score(self.y_test, pred)])\n\n        print(\"RMSLE Value For Extra Trees is: \",\n              accuracy_score(self.y_test, pred))\n\n    def sgd(self):\n        clf = SGDClassifier()\n        clf.fit(self.x_train, self.y_train)\n        pred = clf.predict(self.x_test)\n        self.baseline_results.append(['lr', accuracy_score(self.y_test, pred)])\n\n        print(\"RMSLE Value For SGD: \",\n              accuracy_score(self.y_test, pred))\n\n    def exT(self):\n        clf = ExtraTreesClassifier(n_jobs=-1, n_estimators=100)\n        clf.fit(self.x_train, self.y_train)\n        pred = clf.predict(self.x_test)\n        self.baseline_results.append(['lr', accuracy_score(self.y_test, pred)])\n\n        print(\"RMSLE Value For Extra Trees is: \",\n              accuracy_score(self.y_test, pred))\n\n    def lda(self):\n        clf = LinearDiscriminantAnalysis()\n        clf.fit(self.x_train, self.y_train)\n        pred = clf.predict(self.x_test)\n        self.baseline_results.append(['lr', accuracy_score(self.y_test, pred)])\n\n        print(\"RMSLE Value For LDA is: \",\n              accuracy_score(self.y_test, pred))\n\n    def knn(self, n_neighbors=100):\n        clf = KNeighborsClassifier(n_jobs=-1, n_neighbors=n_neighbors)\n        clf.fit(self.x_train, self.y_train)\n        pred = clf.predict(self.x_test)\n        self.baseline_results.append(['lr',\n                                      accuracy_score(self.y_test, pred)])\n\n        print(\"RMSLE Value For KNN is: \",\n              accuracy_score(self.y_test, pred))\n\n    def naive_bayes(self):\n        clf = GaussianNB()\n        clf.fit(self.x_train, self.y_train)\n        pred = clf.predict(self.x_test)\n        print(\"RMSLE Value For Naive_Bayes is: \",\n              accuracy_score(self.y_test, pred))\n\n    def SVM(self, c):\n        clf = SVC(C=c)\n        clf.fit(self.x_train, self.y_train)\n        pred = clf.predict(self.x_test)\n        self.baseline_results.append(['lr',\n                                      accuracy_score(self.y_test, pred)])\n\n        print(\"RMSLE Value For SVM is: \",\n              accuracy_score(self.y_test, pred))\n\n    def bgt(self):\n        base_estimator = DecisionTreeClassifier(max_depth=13)\n        n_list = [100]\n        for n_estimators in n_list:\n            clf = BaggingClassifier(n_jobs=-1, base_estimator=base_estimator,\n                                    n_estimators=n_estimators)\n            clf.fit(self.x_train, self.y_train)\n            pred = clf.predict(self.x_test)\n            self.baseline_results.append(['lr',\n                                          accuracy_score(self.y_test, pred)])\n\n            print(\"RMSLE Value For BaggedDecisionTrees is: \",\n                  accuracy_score(self.y_test, pred))\n\n    def randomT_bagging(self):\n        n_list = [100]\n        for n_estimators in n_list:\n            clf = RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators)\n            clf.fit(self.x_train, self.y_train)\n            pred = clf.predict(self.x_test)\n            self.baseline_results.append(\n                ['lr', accuracy_score(self.y_test, pred)])\n\n            print(\"RMSLE Value For RandomTreesBaggin is: \",\n                  accuracy_score(self.y_test, pred))\n\n    def abaBoost(self, n_estimators=100):\n        clf = AdaBoostClassifier(n_estimators=n_estimators)\n        clf.fit(self.x_train, self.y_train)\n        pred = clf.predict(self.x_test)\n        self.baseline_results.append(['lr', accuracy_score(self.y_test, pred)])\n\n        print(\"RMSLE Value For adaBoost is: \",\n              accuracy_score(self.y_test, pred))\n\n    def gradB_c(self, max_depth=100):\n        clf = GradientBoostingClassifier(max_depth=max_depth)\n        clf.fit(self.x_train, self.y_train)\n        pred = clf.predict(self.x_test)\n        self.baseline_results.append(['lr', accuracy_score(self.y_test, pred)])\n\n        print(\"RMSLE Value For GradientBoostingClassifier is: \",\n              accuracy_score(self.y_test, pred))\n\n    # ! Still needs some work BOIS\n    def voting_classifier(self):\n        list_estimators = []\n\n        estimators = []\n        model1 = ExtraTreesClassifier(n_jobs=-1, n_estimators=100)\n        estimators.append(('et', model1))\n        model2 = RandomForestClassifier(n_jobs=-1, n_estimators=100)\n        estimators.append(('rf', model2))\n        from sklearn.ensemble import BaggingClassifier\n        from sklearn.tree import DecisionTreeClassifier\n        base_estimator = DecisionTreeClassifier(max_depth=13)\n        model3 = BaggingClassifier(n_jobs=-1, base_estimator=base_estimator,\n                                   n_estimators=100)\n        estimators.append(('bag', model3))\n\n        list_estimators.append(['Voting', estimators])\n\n        for name, estimators in list_estimators:\n            clf = VotingClassifier(estimators=estimators, n_jobs=-1)\n            clf.fit(self.x_train, self.y_train)\n            pred = clf.predict(self.x_test)\n            print(\"RMSLE Value For VotingClassifier is: \",\n                  accuracy_score(self.y_test, pred))\n\n    def xgboosht(self, n_estimators):\n        clf = XGBClassifier(n_estimators=n_estimators, subsample=0.25)\n        clf.fit(self.x_train, self.y_train)\n        pred = clf.predict(self.x_test)\n        self.baseline_results.append(['lr', accuracy_score(self.y_test, pred)])\n\n        print(\n            \"RMSLE Value For XgBoost is: \",\n            accuracy_score(\n                self.y_test,\n                pred))\n\n    def lgbm(self):\n        clf = LGBMClassifier(random_state=17)\n        clf.fit(self.x_train, self.y_train)\n        pred = clf.predict(self.x_test)\n        self.baseline_results.append(['lr', accuracy_score(self.y_test, pred)])\n\n        print(\n            \"RMSLE Value For XgBoost is: \",\n            accuracy_score(\n                self.y_test,\n                pred))\n\n    def print_shit(self):\n        raise NotImplementedError\n\n    def executer(self):\n        if self.exec_all:\n            self.exT()\n            self.gradB_c()\n            self.gradB_r()\n            self.knn()\n            self.lasso()\n            self.lr()\n            self.randomT_bagging()\n            self.abaBoost()\n            self.bgt()\n            self.naive_bayes()\n\n        elif self.classifiers_list:\n            for classifier in self.classifiers_list:\n                # calling the classifier here\n                pass\n","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}