{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bike Sharing Demand Prediction"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing the libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sn\nfrom scipy import stats\nfrom numpy import median\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_log_error, r2_score,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport catboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/bike-sharing-demand/train.csv')\ndf_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation - RMSLE Score\n"},{"metadata":{},"cell_type":"markdown","source":"One common way to evaluate a regression model is through calculating Mean Squared Error (MSE) or Root Mean Squared Error (RMSE). \n\n$$ RMSE = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (\\hat{y_i} - y_i)^2} $$\n\nIn this particular competition, the metric to evaluate our model is **Root Mean Square Logarithmic Error** (RMSLE). It is particularly helpful when we want to penalize an under-predicted estimate greater than an over-predicted estimate. If $\\hat{y_i}$ is the predicted value of the $I$-th sample, and ${y_i}$ is the corresponding true value, then the RMSLE estimated over $N$ samples is defined as:\n\n$$ RMSLE = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (\\log(1+\\hat{y_i}) - \\log(1+y_i))^2 } $$"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check for null values\n\ndf_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We dont have null values so we go on."},{"metadata":{},"cell_type":"markdown","source":"Now we have to create four different columns from datetime. (year, month, hour, weekday)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_date = pd.DatetimeIndex(df_train['datetime'])\ndf_train['year'] = train_date.year\ndf_train['month'] = train_date.month\ndf_train['hour'] = train_date.hour\ndf_train['weekday'] = train_date.dayofweek\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# VISUALIZE"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2,ncols=2)\nfig.set_size_inches(12, 10)\nsn.boxplot(data=df_train,y=\"count\",orient=\"v\",ax=axes[0][0])\nsn.boxplot(data=df_train,y=\"count\",x=\"season\",orient=\"v\",ax=axes[0][1])\nsn.boxplot(data=df_train,y=\"count\",x=\"hour\",orient=\"v\",ax=axes[1][0])\nsn.boxplot(data=df_train,y=\"count\",x=\"workingday\",orient=\"v\",ax=axes[1][1])\n\naxes[0][0].set(ylabel='Count',title=\"Box Plot On Count\")\naxes[0][1].set(xlabel='Season', ylabel='Count',title=\"Box Plot On Count Across Season\")\naxes[1][0].set(xlabel='Hour Of The Day', ylabel='Count',title=\"Box Plot On Count Across Hour Of The Day\")\naxes[1][1].set(xlabel='Working Day', ylabel='Count',title=\"Box Plot On Count Across Working Day\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ((ax1,ax9),(ax2,ax10),(ax3,ax11),(ax4,ax12),(ax5,ax13),(ax6,ax14),(ax7,ax15),(ax8,ax16)) = plt.subplots(8, 2,figsize=(15,25))\nfig.suptitle('Casual vs Registered')\nsn.barplot(x = df_train['season'], y = df_train['casual'],ax = ax1)\nsn.barplot(x = df_train['year'], y = df_train['casual'],ax = ax2)\nsn.barplot(x = df_train['month'], y = df_train['casual'],ax = ax3)\nsn.barplot(x = df_train['hour'], y = df_train['casual'],ax = ax4)\nsn.barplot(x = df_train['holiday'], y = df_train['casual'],ax = ax5)\nsn.barplot(x = df_train['weekday'], y = df_train['casual'],ax = ax6)\nsn.barplot(x = df_train['workingday'], y = df_train['casual'],ax = ax7)\nsn.barplot(x = df_train['weather'], y = df_train['casual'],ax = ax8)\nsn.barplot(x = df_train['season'], y = df_train['registered'],ax = ax9)\nsn.barplot(x = df_train['year'], y = df_train['registered'],ax = ax10)\nsn.barplot(x = df_train['month'], y = df_train['registered'],ax = ax11)\nsn.barplot(x = df_train['hour'], y = df_train['registered'],ax = ax12)\nsn.barplot(x = df_train['holiday'], y = df_train['registered'],ax = ax13)\nsn.barplot(x = df_train['weekday'], y = df_train['registered'],ax = ax14)\nsn.barplot(x = df_train['workingday'], y = df_train['registered'],ax = ax15)\nsn.barplot(x = df_train['weather'], y = df_train['registered'],ax = ax16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regression plot is used to verify if a pattern can be observed between `count` and numerical variables\n\nfig,[[ax1,ax4],[ax2,ax5],[ax3,ax6]] = plt.subplots(3,2, figsize = (15,15))\nplt.rc('xtick', labelsize=10) \nplt.rc('ytick', labelsize=10) \n\nsn.regplot(x = 'temp', y = 'casual',data = df_train, ax = ax1)\nax1.set(title=\"Relation between temperature and casual\")\nsn.regplot(x = 'humidity', y = 'casual',data = df_train, ax = ax2)\nax2.set(title=\"Relation between humidity and casual\")\nsn.regplot(x = 'windspeed', y = 'casual',data = df_train, ax = ax3)\nax3.set(title=\"Relation between windspeed and casual\")\nsn.regplot(x = 'temp', y = 'registered',data = df_train, ax = ax4)\nax4.set(title=\"Relation between temperature and registered\")\nsn.regplot(x = 'humidity', y = 'registered',data = df_train, ax = ax5)\nax5.set(title=\"Relation between humidity and registered\")\nsn.regplot(x = 'windspeed', y = 'registered',data = df_train, ax = ax6)\nax6.set(title=\"Relation between windspeed and registered\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_corr = df_train[['temp', 'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count']].corr()\nmask = np.array(data_corr)\nmask[np.tril_indices_from(mask)] = False\nfig = plt.subplots(figsize=(15, 10))\nsn.heatmap(data_corr, mask=mask, vmax=1, square=True, annot=True, cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logging target values to normalize them"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(ncols=2,nrows=2)\nfig.set_size_inches(12, 10)\nsn.distplot((df_train[\"count\"]),ax=axes[0][0])\nstats.probplot((df_train[\"count\"]), dist='norm', fit=True, plot=axes[0][1])\nsn.distplot(np.log(df_train[\"count\"]),ax=axes[1][0])\nstats.probplot(np.log1p(df_train[\"count\"]), dist='norm', fit=True, plot=axes[1][1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PREPROCESSING\n"},{"metadata":{},"cell_type":"markdown","source":"## Outliers"},{"metadata":{},"cell_type":"markdown","source":"We remove the outliers that have distance more than 3.5 times the standard deviation from the mean of count.  \n**Other Ideas:** We can perform multidimensional outlier detection with isolation trees , dbscan."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_withoutoutliers = df_train[np.abs(df_train[\"count\"]-df_train[\"count\"].mean())<=(3.5*df_train[\"count\"].std())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Shape Of The Before Ouliers: \",df_train.shape)\nprint (\"Shape Of The After Ouliers: \",df_train_withoutoutliers.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to write it easier \ndf=df_train_withoutoutliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We drop datetime because we dont need it more.   \nWe drop atemp to avoid collinearity with temp column.   \nWe drop windspeed because it have weak correlation with count,casual,column(<0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['datetime','atemp','windspeed'],inplace=True,axis=1) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Other Ideas"},{"metadata":{},"cell_type":"markdown","source":"* Binning to features like temp , humidity , hour etc.\n* One hot encoding to categorical features(catboost can do it automatically).\n* Dimensionally reduction . If we perform one-hot-encoding , we can end up with up to 60 sparse features."},{"metadata":{},"cell_type":"markdown","source":"# MODEL BUILDING"},{"metadata":{},"cell_type":"markdown","source":"We build separate models to predict casual and registered users , because as we see in the visualization section they have different distributions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# SPLITIING\nX = df[[col for col in df.columns if col not in ['casual','registered', 'count']]]\ny = df[['casual','registered']]\n\nX_train, X_test,y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We log the target values so they become normalized and will improve our model. log1p = log(y+1) because there are some y < 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Log to make values \"normal\"\nyLog1 = np.log1p(y_train['casual'])\nyLog2 = np.log1p(y_train['registered'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So because our model is catboost we need to pass cat_features . Catboost handles categorical feature on his own. (We can also change the type of our features to categorical , now its numerical, but this didnt improve my model.)   \nfor example **df_train['season'] = df_train.season.astype('category')**"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = list(X_train.columns)\ncat_features = np.where(X_train[columns].dtypes != np.float)[0]\nprint(cat_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use gridsearchCV to to find the optimal parameters for our model. (this is not included in notebook)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor\nmodel=CatBoostRegressor(iterations=4000, depth=10,one_hot_max_size=1, learning_rate=0.01, loss_function='RMSE',colsample_bylevel=0.7\n                        ,l2_leaf_reg=1,silent=True,cat_features=cat_features)\nmodel.fit(X_train,yLog1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor\nmodel2=CatBoostRegressor(iterations=4000, depth=10,one_hot_max_size=1, learning_rate=0.01, loss_function='RMSE',colsample_bylevel=0.7\n                         ,l2_leaf_reg=1,silent=True,cat_features=cat_features)\nmodel2.fit(X_train,yLog2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are other options like lightgbm, xgboost etc.   \nStacking, Blending can also help to improve our score."},{"metadata":{},"cell_type":"markdown","source":"Now to predict count we need to add casual and registered together. We have to make sure we exp (expm1) the predictions before we add them together. "},{"metadata":{"trusted":true},"cell_type":"code","source":"casual=model.predict(X_test)\nregistered=model2.predict(X_test)\npreds=np.expm1(casual)+np.expm1(registered)\nY=y_test['registered']+y_test['casual']\nprint('RMSLE:',(np.sqrt(mean_squared_log_error(preds,Y))))\nprint('R2:', r2_score(preds,Y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sn.regplot(Y, preds, x_bins = 200)\nax.set(title = \"Comparison between the actual vs predicted values\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Last one, we can perform different feature engineering for casual and registered to make the best possible dataset for each one."},{"metadata":{},"cell_type":"markdown","source":"# END NOTES"},{"metadata":{},"cell_type":"markdown","source":"So this end up in the first 3% of the competition. If we implement some of the tips written above or anything else we may climb even more.\nIf you feel like commenting to add something or to ask me anything do it and dont forget to upvote if you find it usefull."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}