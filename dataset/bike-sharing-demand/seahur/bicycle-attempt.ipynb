{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read data (train, test) with pd.read_csv(directory)\ntrain = pd.read_csv(\"/kaggle/input/bike-sharing-demand/train.csv\")\ntrain.head(10)\n#train.info()\n#train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/bike-sharing-demand/test.csv\")\ntest.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['count']\n# y의 편차가 매우 크다. log scaling으로 outlier를 제거한 듯한 효과를 내자.\n# 회귀분석의 평가요소는 MSE(Mean Square Error) // 트리로 예측시 900을 100으로 예측하는 순간 800^2 = 6400만큼 패널티..\n# 다른 속성들은 유지한 채, 스케일링으로 outlier를 제거한 효과를 위해 log sacling 하는 것.\ny.sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y분포를 확인해보자.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 밑 그림 \nwg, dh =  plt.subplots(2,1, figsize=(20,12))\n# log scaling 전 분포 확인.\nsns.distplot(y, ax=dh[0])\n# log 씌웠더니 분포가 바뀔수록 좋은 것.\nsns.distplot(np.log(y), ax=dh[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 모델을 만들기 위해서는 가장 먼저, 모델에 돌릴 수 있는 상태로 만들어야 한다.\n# y와 x를 별도로 저장. 그리고 train과 test 데이터의 변수가 같아야 한다.\n# y = train['count']\n# log scaling으로 y의 outlier를 제거한 듯한 효과.\ny = np.log(train['count'])\ny # 편차가 확실히 많이 줄었다. 분포가 조밀해짐.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############## 변수 추가를 위한 전처리.\n# 날짜형식으로 바꾸는 3가지 서로 다른 방법.\ntrain['datetime'] = train['datetime'].astype('datetime64')\n# train['datetime'] = pd.to_datetime(train['datetime'])\n# train = pd.read_csv(\"/kaggle/input/bike-sharing-demand/train.csv\", parse_dates = ['datetime'])\ntrain.dtypes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['datetime'] = test['datetime'].astype('datetime64')\n# test['datetime'] = pd.to_datetime(test['datetime'])\n# train = pd.read_csv(\"/kaggle/input/bike-sharing-demand/train.csv\", parse_dates = ['datetime'])\ntest.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['year'] = train['datetime'].dt.year\ntrain['weekday'] = train['datetime'].dt.weekday\ntrain['hour'] = train['datetime'].dt.hour\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['year'] = test['datetime'].dt.year\ntest['weekday'] = test['datetime'].dt.weekday\ntest['hour'] = test['datetime'].dt.hour\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TIP) 날짜 형식으로 바꾸지 않고 바로 시간대를 추출하는 방법.\n# time = train['datetime'].str.slice(11,13).astype(int)\n# time.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 모델 검증용으로 validate dataset 분리 7:3\n#from sklearn.model_selection import train_test_split\n#train_x, validate_x, train_y, validate_y  = train_test_split(train, y, test_size = 0.3,\n                                                             #random_state = 777)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['datetime', 'casual', 'registered', 'count'], 1)\ntest = test.drop('datetime', 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 트리모델은 이해하기 쉽다. 그러나 진짜 이유는 카테고리형, 숫자형 데이터가 함께 존재할 때 랜포가 좋다.\n#from sklearn.ensemble import RandomForestRegressor\n# 모델 선언, 모델이 학습하는 방향을 설정해 줄 수 있다. n_estimator 너무 크면 과적합 그러나 랜포는 그렇게 심하진 않다 다른 모델에 비해.\n# cpu를 전력으로 모두 쓰도록 안 해주면 1개만 씀. n_jobs=4 또는 -1로 하면 CPU를 모두 사용\n# random_state는 set.seed()와 같음\n#rf = RandomForestRegressor(n_estimators=100, n_jobs=-1,random_state=999)\n#rf.fit(train, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#result = rf.predict(test)\n                                                                               ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from lightgbm import LGBMRegressor\n# boosting 기법은 hyper parameter 조정이 중요 (과소적합, 과대적합 예방)\n#lgbm = LGBMRegressor()\n#lgbm.fit(train, y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preds = lgbm.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nxgb = XGBRegressor()\nxgb.fit(train, y)\npreds = predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"/kaggle/input/bike-sharing-demand/sampleSubmission.csv\")\nsample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 처음에 np.log 적용한 상태로 train 시켰기 때문에, 예측값을 구할 때는 다시 exp 적용해줘야 한다.\nsample['count'] = np.exp(preds)\nsample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.to_csv(\"sample.csv\", index = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#################################################################################################\n############################시각화 및 EDA를 통한 변수 INSIGHT얻는 과정###############################\ny.sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2 = pd.read_csv(\"/kaggle/input/bike-sharing-demand/train.csv\", parse_dates=['datetime'])\ntrain2['year'] = train2['datetime'].dt.year\ntrain2['month'] = train2['datetime'].dt.month\ntrain2['day'] = train2['datetime'].dt.day\ntrain2['weekday'] = train2['datetime'].dt.weekday\ntrain2['hour'] = train2['datetime'].dt.hour\n\ntest2 = pd.read_csv('/kaggle/input/bike-sharing-demand/test.csv', parse_dates=['datetime'])\ntest2['day'] = test2['datetime'].dt.day\n\n# 시간이라는 변수(정보)가 유의한지 보기 위해, 시간에 따른 자전거 수요 패턴 파악 가능\n# mean은 위험할 수도 있다. outlier 때문에, median도 체크\n#train2.groupby('hour')['count'].mean()\n##### media을 보니 확실히 시간대별로 자전거 수요의 차이가 있음을 알 수 있다. 유의한 변수가 될 수 있음을 유추할 수 있다.\ntrain2.groupby('hour')['count'].median()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a, b = plt.subplots(2,2,figsize=(20,12))\nsns.boxplot(train2['year'], train2['count'], ax=b[0,1])\nsns.boxplot(train2['month'], train2['count'], ax=b[1,1])\n### day가 1~19일 밖에 없다.!\nsns.boxplot(train2['day'], train2['count'], ax=b[0,0])\n### 오후 시간에 왜 outlier가 많은가? 요일 (주중 5일/주말 2일) 주중 5일에 대표성이 된 것 임. \n### 주말 오후의 count가 많을 텐데 그것이 outlier로 잡히는 것.\nsns.boxplot(train2['hour'], train2['count'], ax=b[1,0])\n                                                  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(test2['day']) # test dataset: 20일 이후\n#np.unique(train2['day']) # train dataset: 19일 까지\n# 그래서 day 데이터는 y를 예측하는데 도움이 안된다.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 데이터 수가 class별로 좀 부족.\ntrain2['datetime'].dt.month.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}