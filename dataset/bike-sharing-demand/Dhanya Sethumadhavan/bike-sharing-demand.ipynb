{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Reference:\nhttps://www.kaggle.com/viveksrinivasan/eda-ensemble-model-top-10-percentile#Correlation-Analysis\nhttps://github.com/viveksrinivasanss/blogs/blob/master/bike_sharing_demand/eda_%26_ensemble_model.ipynb\nhttps://medium.com/analytics-vidhya/how-to-finish-top-10-percentile-in-bike-sharing-demand-competition-in-kaggle-part-2-29e854aaab7d\nhttps://www.kaggle.com/c/bike-sharing-demand/discussion/10431"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Fields\n* datetime - hourlydate + timestamp\n* season - 1:spring, 2:summer, 3:fall, 4:winter\n* holiday - whether the day is considered a holiday\n* workingday - whether a day is niether weekend or a holiday\n* weather-\n        * 1:clear, few clouds, partly cloudy\n        * 2: misty & cloudy, misty&broken clouds, misty&few clouds, misty\n        * 3: Light snow, Light Rain&thunder storm & scattered clouds, light rain & scattered clouds\n        * 4: Heavy Rain+ Ice pallets+thundersorm+mist, snow+fog\n* temp - temperature in celsius\n* atemp - \"feels like\" temperature in celsius\n* humidity - relative humidity\n* windspeed\n* casual - number of non-registered user rental initiated\n* registered - number of registered user rentals initiated\n* count - number of total rentals(Dependant variable)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pylab\nimport calendar\nimport seaborn as sn\nfrom scipy import stats\nimport missingno as msno\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport warnings\npd.options.mode.chained_assignment = None\nwarnings.filterwarnings(\"ignore\", category = DeprecationWarning)\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dailyData = pd.read_csv(\"../input/bike-sharing-demand/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Summary\n\nNext we will see more about the dataset \n\n* Size of the data\n* Glimpse of the dataset\n* What of type of variables\n"},{"metadata":{},"cell_type":"markdown","source":"# Size of the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dailyData.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sample Rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"dailyData.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Type"},{"metadata":{"trusted":true},"cell_type":"code","source":"dailyData.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n\n* The following data arecategorical, but is present in the dataset as int type.\nso we need to convert these datas into categorical variables.\n    1. Weather \n    2. Season\n    3. Holiday\n    4. Working Day\n\n* Create the following columns from datetime column\n    1. date\n    2. hour\n    3. weekDay\n    4. month\n\n* Drop the datetime column\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"dailyData.datetime.apply(lambda x:x.split()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating new columns from datetime column\n# apply() : apply is a function in pandas library. It helps to apply a function(lambda/userdefined/Numpy) to the rows/columns in a dataFrame.\n# The default value for axis in apply function is axis = 0 (column).\n# lambda function: it takes input as a dataframe(all/specified number rows of a df or all/specified number columns)\ndailyData[\"date\"] = dailyData.datetime.apply(lambda x:x.split()[0])\ndailyData[\"hour\"] = dailyData.datetime.apply(lambda x: x.split()[1].split(\":\")[0])\n# strptime: create a datetime object from a string \n# datetime.strptime(date_string, format) where datetime is an object that supplies different classes like strptime\n# for manipulating and formatting date ot time \ndailyData[\"weekday\"] = dailyData.date.apply(lambda dateString: calendar.day_name[datetime.strptime(dateString, \"%Y-%m-%d\").weekday()])\ndailyData[\"month\"] = dailyData.date.apply(lambda dateString: calendar.month_name[datetime.strptime(dateString, \"%Y-%m-%d\").month])\ndailyData[\"season\"] = dailyData.season.map({1:\"Spring\", 2:\"Summer\", 3:\"Fall\", 4:\"winter\"})\ndailyData[\"weather\"] = dailyData.weather.map({1: \" Clear + Few clouds + Partly cloudy + Partly cloudy\",\\\n                                        2 : \" Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \", \\\n                                        3 : \" Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\", \\\n                                        4 :\" Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \" })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating category variables\n#A categorical variable is one that usually takes a fixed, number of possible values\ncategoryvariables = [\"hour\", \"weekday\", \"month\", \"season\", \"weather\", \"holiday\", \"workingday\"]\nfor var in categoryvariables:\n    dailyData[var] = dailyData[var].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping Datetime column\ndailyData = dailyData.drop([\"datetime\"], axis =1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the total number of each datatypes present in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating the data for the plot\ntypesCountSerie = dailyData.dtypes.value_counts()\n\n# format columns as arrays of either strings or integers\n# typeNames are easier to sort as array of `string` rather than an array of `dtype`\ntypeNamesColumn = list(map(lambda t: t.name , typesCountSerie.index.values));\ntypeCountColumn = typesCountSerie.values\n# create an initial dataframe, with multiple occurences of the same \"variableType\"\nintialDataTypeDf = pd.DataFrame({\n    \"variableType\": typeNamesColumn, \n    \"count\": typeCountColumn\n})\n\n# Group initial data frame by \"variableType\", \n# then reset_index to have a proper dataframe\ngroupedDataTypeDf = intialDataTypeDf.groupby(['variableType']).sum()[['count']].reset_index()\n#dataTypeDf = pd.DataFrame(dailyData.dtypes.value_counts()).reset_index().rename(columns={\"index\":\"variableType\",0:\"count\"})\nfig,ax = plt.subplots()\nfig.set_size_inches(12, 5)\n#plotting the barchart\nsn.barplot(data=groupedDataTypeDf, x=\"variableType\", y=\"count\", ax=ax)\nax.set(xlabel = 'variableType', ylabel = 'Count', title = \"Count of the different Datatypes\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing Value\n\nMatrix :\nUsing this matrix you can very quickly find the pattern of missingness in the dataset. \n\nBar Chart :\nThis bar chart gives you an idea about how many missing values are there in each column.\n\nNo Missing values detected.\n\n* Checking the presence of missing values by visualising using \"msno\""},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(dailyData, figsize=(12,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.bar(dailyData, figsize=(12,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it is confirmed that there are no missing values."},{"metadata":{},"cell_type":"markdown","source":"# Heatmap :\nHeatmap shows the correlation of missingness between every 2 columns. In our example, the correlation between AAWhiteSt-4 and SulphidityL-4 is 1 which means if one of them is present then the other one must be present.\n\nA value near -1 means if one variable appears then the other variable is very likely to be missing.\nA value near 0 means there is no dependence between the occurrence of missing values of two variables.\nA value near 1 means if one variable appears then the other variable is very likely to be present.\n"},{"metadata":{},"cell_type":"markdown","source":"# Detecting Outliers\nAnalysis using Boxplots"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=2)\nfig.set_size_inches(12,10)\nsn.boxplot(data = dailyData, y = \"count\", orient = \"v\", ax=axes[0][0])\nsn.boxplot(data = dailyData, y = \"count\", x = \"season\", orient = \"v\", ax=axes[0][1])\nsn.boxplot(data = dailyData, y = \"count\", x= \"hour\", orient = \"v\", ax=axes[1][0])\nsn.boxplot(data = dailyData, y = \"count\", x = \"workingday\", orient = \"v\", ax=axes[1][1])\n\naxes[0][0].set(ylabel='Count',title=\"Box Plot On Count\")\naxes[0][1].set(xlabel = 'Season', ylabel='Count',title=\"Box Plot On Count across season\")\naxes[1][0].set(xlabel = 'Hour of the day', ylabel='Count',title=\"Box Plot On Count across Hour of the day\")\naxes[1][1].set(xlabel = 'Working Day', ylabel='Count',title=\"Box Plot On Count  across Working day\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# About Box Plot\nA box and whisker plot—also called a box plot—displays the five-number summary of a set of data. The five-number summary is the minimum, first quartile, median, third quartile, and maximum. In a box plot, we draw a box from the first quartile to the third quartile. A vertical line goes through the box at the median."},{"metadata":{},"cell_type":"markdown","source":"# Observation from the above box plot\n* The count has many 'outliers' as it exceeds the outer quartile limit.\n* Spring Season has got relatively lower count.\n* The box plot for \"Hour of the day \" infer that the median values are higher at 7AM- 8AM, and 5pm-6pm.\nThese time indicates regular office and school hours.\n* Most of the 'outliers' are contributed by 'working days' rather than 'non-working days'."},{"metadata":{},"cell_type":"markdown","source":"# Removing Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking how many count values are with in 3*standard deviation\nnp.sum(np.abs(dailyData[\"count\"]-dailyData[\"count\"].mean())<=(3*dailyData[\"count\"].std()))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dailyDataWithoutOutliers = dailyData[np.abs(dailyData[\"count\"]-dailyData[\"count\"].mean())<=(3*dailyData[\"count\"].std())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"shape of the data with outliers\", dailyData.shape)\nprint(\"shape of the data without outliers\", dailyDataWithoutOutliers.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation Analysis\n* To determine the relationship a dependent variable is having with the numerical features.\n* Below are the data types of numerical features(non-categorical)(temp, atemp, casual, registered, humidity) and dependend variable(count)"},{"metadata":{"trusted":true},"cell_type":"code","source":"dailyData[[\"temp\",\"atemp\",\"casual\",\"registered\",\"humidity\",\"windspeed\",\"count\"]].dtypes\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting the corrrelation between Count and the (\"temp\",\"atemp\",\"casual\",\"registered\",\"humidity\",\"windspeed\")"},{"metadata":{"trusted":true},"cell_type":"code","source":"dailyDataCorr = dailyData[[\"temp\",\"atemp\",\"casual\",\"registered\",\"humidity\",\"windspeed\", \"count\"]].corr()\nmask = np.array(dailyDataCorr)\nmask[np.tril_indices_from(mask)] = False\nfig, ax = plt.subplots()\nfig.set_size_inches(20,10)\nsn.heatmap(dailyDataCorr, mask = mask, vmax = .8, square = True, annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* temp and humidity features are showing positive and negative correlation with the count variable. Although the correlation between them are not prominent, the count has little dependency with them.\n* windspeed is not really going to be useful. The correlation value with count is 0.1.\n* atemp has a strong relationship with temp. So one of the variable has to be dropped during model building since they exhibit multicollinearity in the data.\n* Casual and registered variables are not considered since they are leakage variables. \n* casual(non registered)+registered = count\n\n# Multicollinearity \n\nThere are certain reasons why multicollinearity occurs:\n\n* It is caused by an inaccurate use of dummy variables.\n* It is caused by the inclusion of a variable which is computed from other variables in the data set.\n* Multicollinearity can also result from the repetition of the same kind of variable.\n* Generally occurs when the variables are highly correlated to each other.\n\nMulticollinearity can result in several problems. These problems are as follows:\n\n* The partial regression coefficient due to multicollinearity may not be estimated precisely. The standard errors are likely to be high.\n* Multicollinearity results in a change in the signs as well as in the magnitudes of the partial regression coefficients from one sample to another sample.\n* Multicollinearity makes it tedious to assess the relative importance of the independent variables in explaining the variation caused by the dependent variable.\n\nPartial regression coefficient\n* A value indicating the effect of each independent variable on the dependent variable with the influence of all the remaining variables held constant. Each coefficient is the slope between the dependent variable and each of the independent variables\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#casual(non registered)+registered = count\n#https://www.kaggle.com/jjuanramos/bike-sharing-demand\nplt.scatter(x = dailyData['casual'] + dailyData['registered'], y = dailyData['count'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Regression Plot\nRegression plot in seaborn is one useful way to depict the relationship between two features. Here we consider \"count\" vs \"temp\", \"humidity\", \"windspeed\".\n* a partial regression plot attempts to show the effect of adding another variable to a model that already has one or more independent variables. Partial regression plots are also referred to as added variable plots, adjusted variable plots, and individual coefficient plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2,ax3) = plt.subplots(ncols=3)\nfig.set_size_inches(12, 5)\nsn.regplot(x=\"temp\", y=\"count\", data=dailyData,ax=ax1)\nsn.regplot(x=\"windspeed\", y=\"count\", data=dailyData,ax=ax2)\nsn.regplot(x=\"humidity\", y=\"count\", data=dailyData,ax=ax3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(ncols=2,nrows=2)\nfig.set_size_inches(12, 10)\nsn.distplot(dailyData[\"count\"],ax=axes[0][0])\nstats.probplot(dailyData[\"count\"], dist='norm', fit=True, plot=axes[0][1])\nsn.distplot(np.log(dailyDataWithoutOutliers[\"count\"]),ax=axes[1][0])\nstats.probplot(np.log1p(dailyDataWithoutOutliers[\"count\"]), dist='norm', fit=True, plot=axes[1][1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it is visible from the below figures that \"count\" variable is skewed towards right. It is desirable to have Normal distribution as most of the machine learning techniques require dependent variable to be Normal. One possible solution is to take log transformation on \"count\" variable after removing outlier data points. After the transformation the data looks lot better but still not ideally following normal distribution."},{"metadata":{},"cell_type":"markdown","source":"# Visualizing Count Vs (Month,Season,Hour,Weekday,Usertype)"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1)= plt.subplots(nrows=1)\nfig.set_size_inches(10,5)\nsortOrder = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\nhueOrder = [\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\n\nmonthAggregated = pd.DataFrame(dailyData.groupby(\"month\")[\"count\"].mean()).reset_index()\nmonthSorted = monthAggregated.sort_values(by=\"count\",ascending=False)\nsn.barplot(data=monthSorted,x=\"month\",y=\"count\",ax=ax1,order=sortOrder)\nax1.set(xlabel='Month', ylabel='Avearage Count',title=\"Average Count By Month\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax2= plt.subplots(nrows=1)\nfig.set_size_inches(10,5)\nhourAggregated = pd.DataFrame(dailyData.groupby([\"hour\",\"season\"],sort=True)[\"count\"].mean()).reset_index()\nsn.pointplot(x=hourAggregated[\"hour\"], y=hourAggregated[\"count\"],hue=hourAggregated[\"season\"], data=hourAggregated, join=True,ax=ax2)\nax2.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across Season\",label='big')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax3 = plt.subplots(nrows = 1)\nfig.set_size_inches(10,5)\nhourAggregated = pd.DataFrame(dailyData.groupby([\"hour\",\"weekday\"],sort=True)[\"count\"].mean()).reset_index()\nsn.pointplot(x=hourAggregated[\"hour\"], y=hourAggregated[\"count\"],hue=hourAggregated[\"weekday\"],hue_order=hueOrder, data=hourAggregated, join=True,ax=ax3)\nax3.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across Weekdays\",label='big')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax4 = plt.subplots(nrows=1)\nfig.set_size_inches(10, 5)\nhourTransformed = pd.melt(dailyData[[\"hour\",\"casual\",\"registered\"]], id_vars=['hour'], value_vars=['casual', 'registered'])\nhourAggregated = pd.DataFrame(hourTransformed.groupby([\"hour\",\"variable\"],sort=True)[\"value\"].mean()).reset_index()\nsn.pointplot(x=hourAggregated[\"hour\"], y=hourAggregated[\"value\"],hue=hourAggregated[\"variable\"],hue_order=[\"casual\",\"registered\"], data=hourAggregated, join=True,ax=ax4)\nax4.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across User Type\",label='big')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It is quiet obvious that people tend to rent bike during summer season since it is really conducive to ride bike at that season.Therefore June, July and August has got relatively higher demand for bicycle.\n* On weekdays more people tend to rent bicycle around 7AM-8AM and 5PM-6PM. As we mentioned earlier this can be attributed to regular school and office commuters.\n* Above pattern is not observed on \"Saturday\" and \"Sunday\".More people tend to rent bicycle between 10AM and 4PM.\n* The peak user count around 7AM-8AM and 5PM-6PM is purely contributed by registered user."},{"metadata":{},"cell_type":"markdown","source":"# Data Modeling and predicting"},{"metadata":{},"cell_type":"markdown","source":"Filling Zeros in windspeed using Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataTrain = pd.read_csv(\"../input/bike-sharing-demand/train.csv\")\ndataTest = pd.read_csv(\"../input/bike-sharing-demand/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#combine test and train data\ndata = dataTrain.append(dataTest)\ndata.reset_index(inplace = True)\ndata.drop('index', inplace = True, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"date\"] = data.datetime.apply(lambda x : x.split()[0])\ndata[\"hour\"] = data.datetime.apply(lambda x : x.split()[1].split(\":\")[0]).astype(\"int\")\ndata[\"year\"] = data.datetime.apply(lambda x : x.split()[0].split(\"-\")[0])\ndata[\"weekday\"] = data.date.apply(lambda dateString : datetime.strptime(dateString,\"%Y-%m-%d\").weekday())\ndata[\"month\"] = data.date.apply(lambda dateString : datetime.strptime(dateString,\"%Y-%m-%d\").month)\ndataWindspeedOriginal = data[\"windspeed\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax= plt.subplots(nrows=1)\nfig.set_size_inches(20,5)\n#sortOrder = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n#hueOrder = [\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\n\nwindspeed = pd.DataFrame(data.windspeed.value_counts()).reset_index()\nplt.xticks(rotation=45) \nsn.barplot(data=windspeed,x=\"index\",y=\"windspeed\",ax=ax)\nax1.set(xlabel='windspeed Values', ylabel='Count',title=\"Count of windspeed values before imputing\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Windspeed has many zero entries which make it look suspicious.\n\nAs specified in the kaggle discussion:\n* It can actually be 0 at these points.\n* It is too low to be measured, for example varying from 0 to 5.\n* All zeros or part of them are nothing but NAs.\n\nConsidering windspeed 0 entries as missing values, we will fill them with Random Forest Classifier model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nwCol= [\"season\",\"weather\",\"humidity\",\"month\",\"temp\",\"year\",\"atemp\"]\n#dataWind0 is the entire dataset(contains cols season, weather, humidity, month, temp, year, atemp)\n#with windspeed value = 0\ndataWind0 = data[data[\"windspeed\"] == 0]\n#dataNotWind0 is the entire dataset(contains cols season, weather, humidity, month, temp, year, atemp)\n#without windspeed value = 0\ndataWindNot0 = data[data[\"windspeed\"] != 0]\ndataWindNot0[\"windspeed\"] = dataWindNot0[\"windspeed\"].astype(\"str\")\n\n#predicting value for windspeed = 0\nrfModel_wind = RandomForestClassifier()\nrfModel_wind.fit(dataWindNot0[wCol], dataWindNot0[\"windspeed\"])\nWind0Values = rfModel_wind.predict(X = dataWind0[wCol])\n\ndataWind0[\"windspeed\"] = Wind0Values\ndata = dataWindNot0.append(dataWind0)\ndata[\"windspeed\"] = data[\"windspeed\"].astype(\"float\")\ndata.reset_index(inplace=True)\ndata.drop('index',inplace=True,axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# After Imputing Windspeed values"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(nrows=1)\nfig.set_size_inches(20,5)\nwindspeed = pd.DataFrame(data.windspeed.value_counts()).reset_index()\nplt.xticks(rotation=45) \nsn.barplot(data=windspeed,x=\"index\",y=\"windspeed\",ax=ax)\nax.set(xlabel='Windspeed Values', ylabel='Count',title=\"Count Of Windspeed Values After Imputing\",label='big')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Coercing to Categorical Type"},{"metadata":{"trusted":true},"cell_type":"code","source":"categoricalFeatureNames = [\"season\",\"holiday\",\"workingday\",\"weather\",\"weekday\",\"month\",\"year\",\"hour\"]\nnumericalFeatureNames = [\"temp\",\"humidity\",\"windspeed\",\"atemp\"]\ndropFeatures = ['casual',\"count\",\"datetime\",\"date\",\"registered\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in categoricalFeatureNames:\n    data[var] = data[var].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting Train and Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataTrain = data[pd.notnull(data['count'])].sort_values(by=[\"datetime\"])#datatime is not droppe, month, week etc are created from it.\ndataTest = data[~pd.notnull(data['count'])].sort_values(by = [\"datetime\"])\ndatetimecol = dataTest[\"datetime\"]\nyLabels = dataTrain[\"count\"]\nyLabelsRegistered = dataTrain[\"registered\"]\nyLabelsCasual = dataTrain[\"casual\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting Train and Validator"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_validate, y_train, y_validate = train_test_split( dataTrain, yLabels, test_size=0.3, random_state=42)\ndateTimeColValidate = X_validate[\"datetime\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dropping Unnecessary Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataTrain  = dataTrain.drop(dropFeatures,axis=1)\ndataTest  = dataTest.drop(dropFeatures,axis=1)\nX_train = X_train.drop(dropFeatures,axis=1)\nX_validate = X_validate.drop(dropFeatures,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RMSLE Scorer\n* One common way to evaluate the regression model is through calculating MSE or RMSE. In this particular competition, the metric to evaluate our model is* Root Mean Square Logarithmic Error* (RMSLE). RMSLE is particularly helpful when you want to penalize an under-predicted estimate greater than an over-predicted estimate.\nMost of the Kaggle competition where we predict sales and inventory demand especially use RMSLE as their metric to evaluate. For example competition such as grupo-bimbo-inventory-demand and sberbank-russian-housing-market use RMSLE as a metric.\n![](http://miro.medium.com/max/923/1*9P4NEvK5qYN5Bhu0yOEzhw.png)\nUnfortunately, sklearn metrics do not have the direct implementation to calculate RMSLE. So let us construct a custom function to perform theRMSLE calculation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle(y, y_,convertExp=True):\n    if convertExp:\n        y = np.exp(y),\n        y_ = np.exp(y_)\n    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))\n    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))\n    calc = (log1 - log2) ** 2\n    return np.sqrt(np.mean(calc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\npd.options.mode.chained_assignment = None\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data is prepared by filling the missing values and constructed the RMSLE scorer. So we are now good to go for our model building experimet."},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression\n* As a first step, let us start with a simple statistical technique like linear regression. It is always good to start from a simple model than to try complex machine learning algorithms at first. Because at times features will have a smooth, nearly linear dependence on the covariates. Then linear regression will model the dependence better than anrandom forest algorithm that will basically approximate a linear curve with an ugly irregular step function. A StackExchange discussion gives loads of information about it.\nhttps://stats.stackexchange.com/questions/174806/linear-regression-performing-better-than-random-forest-in-caret"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.model_selection import GridSearchCV\n\n#Initialize Logistic Regression model\nlModel = LinearRegression()\n# Train the model\nlModel.fit(X = X_train,y = np.log1p(y_train))\n# Make predictions\npreds = lModel.predict(X= X_validate)\nprint (\"RMSLE Value For Linear Regression In Validation: \",rmsle(np.exp(np.log1p(y_validate)),np.exp(preds),False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Before submitting our test results we will visualize the distribution of train and test results. Kaggle has a limit on the number of submissions per day. (in our case it is 5 submissions/day). So visualizing the distribution gives a good clue on how close we have predicted our test based on our training set. From the figure it visible that the distribution of the train and test set vary considerably."},{"metadata":{"trusted":true},"cell_type":"code","source":"predsTest = lModel.predict(X=dataTest)\nfig,(ax1,ax2)= plt.subplots(ncols=2)\nfig.set_size_inches(20,5)\nsn.distplot(yLabels,ax=ax1,bins=100)\nsn.distplot(np.exp(predsTest),ax=ax2,bins=100)\nax1.set(title=\"Training Set Distribution\")\nax2.set(title=\"Test Set Distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print (\"RMSLE Value For Linear Regression In Validation: \",rmsle(np.exp(np.log1p(y_validate)),np.exp(predsTest),False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The RMSLE value on the test set is around 1.05 and it is definitely not on par with the best score(0.33) in theKaggle leaderboard. We can improve this score substantially in a number of ways.\n* Feature Engineering\n* Regularization (L1 & L2)\n* Ensemble Models\n1. We have already created a few features such as weekday, month, hour from the datetime attribute. And there are many numbers of ways one can come with feature engineering steps. As a part of this blog, I am not taking that into consideration and I will leave that to the imagination of users."},{"metadata":{},"cell_type":"markdown","source":"# Regularization"},{"metadata":{},"cell_type":"markdown","source":"Regularization is extremely useful in any of these cases. Multicollinearity and overfitting may pose some issues for us.\n\n* overfitting\n* A large number of variables\n* Low ratio of number of observations to number of variables\n* Multicollinearity\n* Overfitting refers to a model that performs well on the training set by learning the detail and noise in the training data but does not generalize well on the new set of data. Let us take our example, RMSLE value on training data is around 0.98 and there is no big difference from the test set results.So far we do not have  any overfitting problems but at imes it will be a nightmare while fitting the models. \n    >> Having a large number of variables may again result in overfitting. This is because the model becomes more complex and sometimes lowers its predicting and generalization power. ***L1 regularization(Lasso Regression)***comes in handy in these situations by reducing the coefficients to zero thereby producing simpler models.\n    >*** L2 Regularization***(Ridge Regression) is extremely helpful for the third case where we have the ratio of more number of attributes\n    to less number of observation.But in this case, we are fine with that with 12 attributes and 10886 records. Ridge regression is also when there is high multicollinearity between predictor variables. We have highly correlated variables like temp-atemp and month-season.\n    > So we are not getting affected much with the above problems. But to improve our score, we will build simple regularization models."},{"metadata":{},"cell_type":"markdown","source":"# Regularization Model - Ridge(L2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\n\nridge_m_ = Ridge()\nridge_params_ = { 'max_iter':[3000],'alpha':[0.01,0.05,0.1, 1, 2, 3, 4, 10, 30,100,200,300,400,800,900,1000]}\nrmsle_scorer = metrics.make_scorer(rmsle, greater_is_better = False)\ngrid_ridge_m = GridSearchCV( ridge_m_,\n                          ridge_params_,\n                          scoring = rmsle_scorer,\n                          cv=5)\ngrid_ridge_m.fit(X = X_train, y = np.log1p(y_train))\npreds = grid_ridge_m.predict(X = X_validate)\nprint(grid_ridge_m.best_params_)\nprint (\"RMSLE Value For Ridge Regression: \",rmsle(np.exp(np.log1p(y_validate)),np.exp(preds),False))\n\nfig,ax= plt.subplots()\nfig.set_size_inches(20,5)\ndf = pd.DataFrame(grid_ridge_m.cv_results_)\ndf\ndf[\"alpha\"] = df[\"params\"].apply(lambda x:x[\"alpha\"])\ndf[\"rmsle\"] = df[\"mean_test_score\"].apply(lambda x:-x)\nsn.pointplot(data=df,x=\"alpha\",y=\"rmsle\",ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# L1 Regularization(Lasso)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nlasso_m_ = Lasso()\nalpha  = [0.001,0.005,0.01,0.3,0.1,0.3,0.5,0.7,1]\nlasso_params_ = { 'max_iter':[3000],'alpha':alpha}\n#rmsle_scorer = metrics.make_scorer(rmsle, greater_is_better=False)\ngrid_lasso_m = GridSearchCV(lasso_m_, lasso_params_, scoring = rmsle_scorer, cv = 5)\ngrid_lasso_m.fit(X = X_train,y = np.log1p(y_train))\npreds = grid_lasso_m.predict(X= X_validate)\nprint (grid_lasso_m.best_params_)\nprint (\"RMSLE Value: \",rmsle(np.exp(np.log1p(y_validate)),np.exp(preds), False))\n\nfig,ax= plt.subplots()\nfig.set_size_inches(20,5)\ndf = pd.DataFrame(grid_lasso_m.cv_results_)\ndf[\"alpha\"] = df[\"params\"].apply(lambda x:x[\"alpha\"])\ndf[\"rmsle\"] = df[\"mean_test_score\"].apply(lambda x:-x)\nsn.pointplot(data=df,x=\"alpha\",y=\"rmsle\",ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The optimum value of the regularization parameter (alpha-0.005) is obtained through a grid search. The chart below visualizes RMSLE values for different alpha parameters. RMSLE value on the test set is around 1.04 and has not improved from our previous. So regularization has not given any boost to our score. But let us not lose hope because when nothing goes right ensemble model always produces something out of the box for us."},{"metadata":{},"cell_type":"markdown","source":"# Ensemble Models\n> Ensemble models are nothing but an art of combining a diverse set of individual weak learners(models) together to improve the stability and predictive capacity of the model. Ensemble Models improves the performance of the model by\n* Averaging out biases.\n* Reducing the variance.\n* Avoiding overfitting.\n >> If you are still wondering what ensemble model is all about then this series of articles can get you started with it. So that’s enough introduction about ensemble model and here is a snippet on how we fit naive Random Forest model on our dataset with default parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrfModel = RandomForestRegressor(n_estimators=100)\nrfModel.fit(X = X_train,y = np.log1p(y_train))\npreds = rfModel.predict(X= X_validate)\nprint (\"RMSLE Value: \",rmsle(np.exp(np.log1p(y_validate)),np.exp(preds), False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = pd.DataFrame()\nfeatures['features'] = X_train.columns\nfeatures['coefficient'] = rfModel.feature_importances_\nfeatures.sort_values(by=['coefficient'],ascending=False,inplace=True)\nfig,ax= plt.subplots()\nfig.set_size_inches(20,5)\nsn.barplot(data=features,x=\"features\",y=\"coefficient\",ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predsTest = rfModel.predict(X=dataTest)\nfig,(ax1,ax2)= plt.subplots(ncols=2)\nfig.set_size_inches(20,5)\nsn.distplot(yLabels,ax=ax1,bins=100)\nsn.distplot(np.exp(predsTest),ax=ax2,bins=100)\nax1.set(title=\"Training Set Distbution\")\nax2.set(title=\"Test Set Distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"datetime\": datetimecol,\n        \"count\": [max(0, x) for x in np.exp(predsTest)]\n    })\nsubmission.to_csv('bike_predictions_gbm_separate_without_fe.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}