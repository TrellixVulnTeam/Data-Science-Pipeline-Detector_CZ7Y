{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook, I will use Random Forest and XGBoost with some improvements to get a RMSLE score of 0.26 on a validation set."},{"metadata":{"_uuid":"80834a23-c3b0-4344-9bad-a4547c1199db","_cell_guid":"12f0bafa-cccd-4d7f-8b2b-ca47d966aed0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\n\nfrom sklearn.model_selection import train_test_split,cross_validate,cross_val_score,cross_val_predict\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. Import and preprocess data:**\n\nThe most important step is transforming the \"datetime\" column into hours, days and months. The hour attribute is a very interesting feature as we'll see. Also, features like the weather and the season must be one-hot encoded."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read training/test set file\ndf1=pd.read_csv(\"/kaggle/input/bike-sharing-demand/train.csv\")\ndf2=pd.read_csv(\"/kaggle/input/bike-sharing-demand/test.csv\")\n\n#Define preprocessing function to apply to train & test sets\ndef preprocess(df):\n    #Extract hour,day,month,weekday from datetime\n    df[\"hour\"]=pd.to_datetime(df[\"datetime\"]).dt.hour\n    df[\"dayofweek\"]=pd.to_datetime(df[\"datetime\"]).dt.dayofweek\n    df[\"dayofmonth\"]=pd.to_datetime(df[\"datetime\"]).dt.day\n    df[\"month\"]=pd.to_datetime(df[\"datetime\"]).dt.month\n    df[\"year\"]=pd.to_datetime(df[\"datetime\"]).dt.year.map({2011:0, 2012:1})\n    #One-hot encode weather and season\n    df=pd.get_dummies(df,columns=[\"weather\",\"season\"],prefix=[\"weather\",\"season\"],drop_first=True,dtype=int)\n    #Drop out datetime and return\n    return df.drop([\"datetime\"],axis=1)\n\ndf1=preprocess(df1)\ndf2=preprocess(df2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Visualization**\n\nWe will plot the correlation matrix and the impact of some important features on the target. I chose 2 features (hour and temp) for the sake of simplicity."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize correlation matrix.\ncor_mat= df1.corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(cor_mat,mask=mask,square=True,annot=True,cbar=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=\"hour\",y=\"count\",data=df1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, you see 3 peaks (8am,12-13pm,17-18pm) corresponding to people using regularly bikes to go to work or lunch. Non-linearity here makes it complicated to linear models to capture label correlation with the hour feature. Since we are using random forest, we will see that this is by far the most important feature.\n\nAs you can see below, temperature is linearly correlated to the target."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x=\"temp\",y=\"count\",data=df1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****3. Making predictions****\n\nNow, we are going to fit 2 algorithms, Random Forest and Extreme Gradient Boosting, on a 75/25 train/validation split.\nNote that the evaluation metric is RMSLE (Root Mean Squared Logarithmic Error), so transforming labels yields better results since these regression models minimize loss functions on a MSE metric. We'll see how this little trick improves the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define features and labels\nfeatures=[x for x in df1.columns if x not in [\"count\",\"casual\",\"registered\"]]\nlabel1=[\"casual\"]\nlabel2=[\"registered\"]\nlabel=[\"count\"]\n\n#Separate features from labels\nX_train,y1_train,y2_train,y_train=df1[features],df1[label1],df1[label2],df1[label]\nX_test=df2[features]\n\n#Validation set\nX_train_0,X_train_1,y_train_0,y_train_1=train_test_split(X_train,y_train,test_size=0.25,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train Random Forest Model\nrf=RandomForestRegressor(n_estimators=500,n_jobs=-1)\n\nrf.fit(X_train_0,y_train_0)\npred=rf.predict(X_train_1)\n\n#Validation set error\nprint(mean_squared_log_error(y_train_1,pred)**0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transforming labels\nrf.fit(X_train_0,y_train_0.apply(lambda x:np.log1p(x)))\npred=np.expm1(rf.predict(X_train_1))\n\n#Validation set error\nprint(mean_squared_log_error(y_train_1,pred)**0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So transforming labels improves the RMSLE by 0.03 (0.32 -> 0.29), in a Random Forest regression model with 500 trees.\n\nBy printing feature importances from the random forest model attributes, hour is around 76% while temp is around 4%."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print most important features\nprint(pd.DataFrame({\"Features\": features,\"Importance\" : rf.feature_importances_.round(2)}).sort_values(\"Importance\",ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train Extreme Gradient Boosting Model\n\n#Create D-matrices for training and validation sets\n\nDM_0=xgb.DMatrix(X_train_0,y_train_0)\nDM_1=xgb.DMatrix(X_train_1,y_train_1)\n\nparams={\"booster\":\"gbtree\"}\nxgbr=xgb.train(dtrain=DM_0,num_boost_round=380,params=params)\n\npred = xgbr.predict(DM_1)\n\npred[pred<0]=0\n#Validation set error\nprint(mean_squared_log_error(y_train_1,pred)**0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transforming labels\nDM_0=xgb.DMatrix(X_train_0,y_train_0.apply(lambda x:np.log1p(x)))\n\nparams={\"booster\":\"gbtree\"}\nxgbr=xgb.train(dtrain=DM_0,num_boost_round=380,params=params)\n\npred = np.expm1(xgbr.predict(DM_1))\n\npred[pred<0]=0\n\n#Validation set error\nprint(mean_squared_log_error(y_train_1,pred)**0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So transforming labels improves the RMSLE by 0.23 !!! (0.53 -> 0.30), in a XGBoost regression tree model.\n\nWith tuning some hyperparameters, we can obtain RMSLE close to 0.26. This is hand-tuning so grid or random search can yield better performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fine-tuning some hyperparameters\nparams={\"booster\":\"gbtree\",\"learning_rate\":0.09,\"n_jobs\":-1,\"subsample\":0.7,\"alpha\": 0.009}\n\nxgbr=xgb.train(dtrain=DM_0,num_boost_round=380,params=params)\n\npred = np.expm1(xgbr.predict(DM_1))\n\n#Validation set error\nprint(mean_squared_log_error(y_train_1,pred)**0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, note that on submitting you may not find the same result. To go further, one must consider time series analysis since  the test set is considering the last 10 days of the month. So the test set distribution is not the same as the training set distribution. "}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}