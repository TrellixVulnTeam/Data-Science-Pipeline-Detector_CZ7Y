{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ** Linear Regression**\n\nRegression은 하나의 종속변수 Y와 일련의 독립 변수 X 사이 관계의 형태를 탐색하고 결정하는 통계 측정방법입니다. \n$$y = a_0 + a_1 x_1 + a_2 x_2 + ... + a_n x_n$$\n\nLinear regression은 모든 인스턴스에 대한 오차를 최소화하도록 계수를 최적화합니다.\n\n오차는 아래의 수식으로 계산이 됩니다:\n\n- Sum of residuals $\\sum_{i = 1}^{n} (Y - \\hat{Y})$\n     - 양수 및 음수의 오류를 계산합니다.\n- Sum of the absolute value of residuals $\\sum_{i = 1}^{n} |Y - \\hat{Y}|$\n     - 오류차의 절대값의 합으로 계산합니다.\n     - Outlier에 보다 robust 합니다.     \n- Sum of square of residuals $\\sum_{i = 1}^{n} (Y - \\hat{Y})^2$\n     - 오차값에 대해 보다 높은 패널티를 줍니다. \n     - Outlier에 위 수식보다 덜 robust 합니다.\n\n### <u> Cost Function </u> \nCost Function은 모델의 오류 값을 측정하는데 정의된 함수입니다.\n\n- Cost function for MSE: $J_{\\theta} = \\frac{1}{n} \\sum_{i = 1}^{n}(Y - \\hat{Y})^2$\n\n예측값을 높이기 위해, 우리는 cost function을 최소화해야 합니다. 이러한 목적으로 우리는 gradient descent algorithm을 사용하게 됩니다.\n\n### <u> Gradient Descent </u>\nGradient Descent는 cost function 값이 최소화 되는 지점을 찾기 위해 파라미터들을 점진적으로 업데이트합니다.\n\n\n### <u> Regression의 4가지 조건 </u>\n    1. 선형성 : 설명 변수와 반응 변수 간의 관계 분포가 선형의 관계를 가진다.\n    2. 독립성 : 설명 변수와 다른 설명 변수 간에 상관관계가 적다.\n    3. 등분산성 : 잔차가 특정한 패턴을 보이지 않는다. (점점 커지거나 작아지는 패턴이 없다.)\n    4. 정규성 : 잔차가 정규분포이다.    \n    * 잔차 : 예측값 - 관측값 "},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom pathlib import Path\nfrom IPython.display import display\nfrom datetime import datetime\nfrom pandas import DataFrame\nfrom typing import List, NamedTuple, Tuple\n\n# allow plots to appear directly in the notebook\n%matplotlib \n\n# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 50)\npd.set_option('display.width', 1000)\n\nROOT_DIR = Path(\"/kaggle/input/bike-sharing-demand\")\nTRAIN_DATA_PATH = ROOT_DIR / \"train.csv\"\nTEST_DATA_PATH = ROOT_DIR / \"test.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load(path: Path) -> DataFrame:\n    return pd.read_csv(path, parse_dates=True, index_col=\"datetime\")\n\ndef expanded_index_datetime_col(data: DataFrame) -> DataFrame:\n    data = data.copy()\n    data[\"hour\"] = data.index.hour\n    data[\"weekday\"] = data.index.weekday\n    data[\"month\"] = data.index.month\n    data[\"year\"] = data.index.year\n    return data\n\n\noriginal_train: DataFrame = load(TRAIN_DATA_PATH)\n#Not used in test data\noriginal_train = original_train.drop([\"casual\", \"registered\"], axis=1, errors=\"ignore\")\noriginal_train = expanded_index_datetime_col(original_train)\n\ndisplay(original_train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 선형성\n - 설명 변수와 반응 변수 간의 관계 분포가 선형의 관계를 가진다.\n - 아래의 pairplot으로 선형성을 가지는 feature는 존재하지 않았다."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(original_train, x_vars=['weather', 'temp', 'atemp', 'humidity', 'windspeed'], y_vars='count'\n            ,size=5, aspect=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 독립성\n - 설명 변수와 다른 설명 변수 간에 상관관계(다중공선성)가 적다.\n - 피쳐간에 다중공선성이 존재하는 경우 모델이 각 피쳐와 대상 간의 관계를 독립적으로 추정하기가 어렵습니다. \n - 상관관계가 높은 2개의 피쳐가 있는 경우, 하나의 피쳐를 삭제하거나 2개의 피쳐를 결합하여 새로운 피쳐를 만듦으로써 예측에 사용할 수 있습니다.\n - PairPlot과 heatmap(correlation matrix)를 통해 상관관계가 높은 특징들을 찾아 낼 수 있습니다.**** "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(original_train[['weather', 'temp', 'atemp', 'humidity', 'windspeed']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(original_train[['weather', 'temp', 'atemp', 'humidity', 'windspeed']].corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"위의 두 경우를 보면 temp와 atemp가 강한 상관관계를 보이는 것을 알 수 있습니다."},{"metadata":{},"cell_type":"markdown","source":"## 등분산성(homoscedasticity)\n>  - 잔차가 특정한 패턴을 보이지 않습니다.\n - 잔차의 분포에 특정한 패턴이 있으면 데이터가 이분산성을 나타내고 이는 비선형성을 나타내게 됩니다.\n - 잔차는 독립변수의 값에 관계없이 동일한 분산을 가져야 합니다.\n - resid plot :선형 회귀의 잔차를 플로팅합니다. 이 함수는 x에서 y를 회귀하고 잔차의 산점도를 그립니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# temp는 등분산성을 만족하지 않습니다.\nsns.residplot(x=original_train['temp'], y=original_train['count'], lowess=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# weather는 등분산성을 만족합니다.\nsns.residplot(x=original_train['weather'], y=original_train['count'], lowess=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 정규성\n - 잔차는 정규분포를 만족해야 합니다.\n - qq plot을 사용하여 데이터가 정규 분포에서 나온 것인지 유추 할 수 있습니다. 정규분포라면 플롯은 상당히 직선으로 나타납니다. 오차의 정규성 부재는 직선의 편차로 볼 수 있습니다.\n - 정규성 확인을 위해 간단한 Linear regression model을 만들어 확인해보도록 하겠습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = original_train[['temp']]\ny = original_train[[\"count\"]]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)\n\nmodel = LinearRegression()\nresult = model.fit(x_train, y_train)\ny_pred = model.predict(x_test)\n\nsns.distplot((y_test-y_pred),bins=100, color = 'gray')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 모델의 성능\nLinear Model의 성능 평가를 위해 우리는 $R^2$ 또는 $\\text{Adjusted }R^2$ 값을 사용합니다.\n\n$$R^2 = 1 - \\frac{(Y - \\hat{Y})^2}{(Y - \\bar{Y})^2}$$\n\n$R^2$ 은 모델이 얼마나 좋은지를 알려주는 값입니다.은$R^2$은 항상 0과 1사이의 값이며 1에 가까울 수록 높은 성능을 나타냅니다.\n\n그러나 $R^2$ 의 문제점은 출력 변수와 관계없는 경우에도 더 많은 feature를 추가하면 하나의 feature일때와 동일한 값 혹은 증가한 값을 나타내게 됩니다. 그래서 우리는 $\\text{Adjusted }R^2$ 를 같이 비교해야 합니다. $\\text{Adjusted }R^2$ 는 기존 모델을 개선하지 않는 변수를 추가할 경우 불이익을 주게 됩니다.\n\n$$\\text{Adjusted }R^2 = 1 - \\frac{(1 - R^2)(N - 1)}{N -P - 1}$$\n\n여기서 $N$ 은 인스턴스의 수이며 $P$ 는 feature의 개수입니다.\n\n따라서 여러 변수에 대해 선형 회귀를 작성하는 경우 항상 조정 된 R 제곱을 사용하여 모형의 우수성을 판단하는 것이 좋습니다. 입력 변수가 하나 뿐인 경우 R- 제곱과 조정 된 R 제곱은 정확히 같습니다.\n\n일반적으로 모형에 유의하지 않은 변수를 더 많이 추가할수록 R- 제곱과 조정 된 R- 제곱의 간격이 증가합니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_r2(model, x, y):\n    return model.score(x, y)\n    \ndef calculate_adjust_r2(model, x, y):\n    r2 = model.score(x, y)\n    N = len(x)\n    P = len(x.columns)\n    return 1-(1-r2)*(N-1)/(N-P-1)\n\ndef calculate_rmse(model, x, y):\n    pred = model.predict(x)\n    return np.sqrt(metrics.mean_squared_error(y, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('R^2 : {:.3f}'.format(calculate_r2(model, x_test, y_test)))\nprint('Adjust R^2 : {:.3f}'.format(calculate_adjust_r2(model, x_test, y_test)))\nprint('RMSE : {:.2f}'.format(calculate_rmse(model, x_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"다음은 x의 feature를 여러 개로 설정하여 예측하는 모델을 테스트 합니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare(df: DataFrame) -> DataFrame:\n    df = df.copy()\n    df = replaced_with_onehot_cols(df, col_names=[\"season\", \"holiday\", \"workingday\", \"weather\", \"weekday\", \"month\", \"year\"])\n    df = df.drop(['atemp', 'windspeed'], axis=1)\n    \n    \n    return df\n\ndef replaced_with_onehot_cols(data: DataFrame, col_names: List[str]) -> DataFrame:\n    data = data.copy()\n    \n    for col_name in col_names:\n        one_hot = pd.get_dummies(data[col_name], prefix=col_name)\n        data = data.join(one_hot)\n        \n        # Original column is not needed anymore\n        del data[col_name]\n    return data\n\ntrain_vals: DataFrame = prepare(original_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train_vals.drop(\"count\", axis=1)\ny = train_vals[[\"count\"]]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)\n\nmodel = LinearRegression()\n\nresult = model.fit(x_train, y_train)\n\nprint('R^2 : {:.3f}'.format(calculate_r2(model, x_test, y_test)))\nprint('Adjust R^2 : {:.3f}'.format(calculate_adjust_r2(model, x_test, y_test)))\nprint('RMSE : {:.2f}'.format(calculate_rmse(model, x_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ** Lasso Regression**\n\nLasso는 Linear regression의 cost function에 L1-norm penalty를 주는 방식으로 cost 값을 계산하여 MSE와 penalty가 최소가 되도록 학습하는 목적을 가지고 있습니다.\n\n### <u> Cost Function </u> \n- Cost function for MSE: $J_{\\theta} = \\frac{1}{n} \\sum_{i = 1}^{n}(Y - \\hat{Y})^2$\n- Cost function for Lasso: $MSE + penalty = \\frac{1}{n} \\sum_{i = 1}^{n}(Y - \\hat{Y})^2 + \\alpha \\sum_{j = 0}^{p}|w_j|$\n- 여기서 m은 feature의 개수가 되고,  $\\alpha$ 는 패널티의 니과를 조절해주는 파라미터 이다. $\\alpha$ 가 작아지면 선형회귀와 같은 모형이 되고, $\\alpha$ 가 커지면 패널티의 영향력이 커집니다. \n\n### 특성\n- Lasso에서  $\\alpha$ 는 하이퍼 파라미터로 사용자가 직접 지정을 해줘야 합니다. \n- Penalty를 통해 모델의 과적합을 막을 수 있습니다. \n- $w_j$가 0일 경우 해당 feature는 모델에 영향을 주지 않는 것을 앎으로써 (어떤 feature가 더 큰 영향을 미칠 수 있는지) 모델에 대한 해석력이 좋아집니다.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\nmodel = Lasso(alpha=0.1, normalize=True)\n\nmodel.fit(x_train, y_train)\n\nprint('R^2 : {:.3f}'.format(calculate_r2(model, x_test, y_test)))\nprint('Adjust R^2 : {:.3f}'.format(calculate_adjust_r2(model, x_test, y_test)))\nprint('RMSE : {:.2f}'.format(calculate_rmse(model, x_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ** Ridge Regression**\n\nRidge는 Lasso와 비슷하게 Linear regression의 cost function에 L2-norm penalty를 주는 방식으로 cost 값을 계산하여 MSE와 penalty가 최소가 되도록 학습하는 목적을 가지고 있습니다.\n\n### <u> Cost Function </u> \n- Cost function for MSE: $J_{\\theta} = \\frac{1}{n} \\sum_{i = 1}^{n}(Y - \\hat{Y})^2$\n- Cost function for Lasso: $MSE + penalty = \\frac{1}{n} \\sum_{i = 1}^{n}(Y - \\hat{Y})^2 + \\alpha \\sum_{j = 0}^{p}(w_j)^2$\n- 여기서 m은 feature의 개수가 되고,  $\\alpha$ 는 패널티의 니과를 조절해주는 파라미터 이다. $\\alpha$ 가 작아지면 선형회귀와 같은 모형이 되고, $\\alpha$ 가 커지면 패널티의 영향력이 커집니다. \n\n### 특성\n- Ridge에서 $\\alpha$ 는 하이퍼 파라미터로 사용자가 직접 지정을 해줘야 합니다. \n- Penalty를 통해 모델의 과적합을 막을 수 있습니다. \n- $w_j$가 0에 가까울 경우 해당 feature는 모델에 영향을 주지 않는 것을 앎으로써 (어떤 feature가 더 큰 영향을 미칠 수 있는지) 모델에 대한 해석력이 좋아집니다.\n- Lasso와의 다른점은 가중치 $w_j$가 0에 가까워질 뿐 0이 되지는 않습니다. \n- 많은 feature들 중 일부분만 중요하다면 Lasso가 더 높은 성능을 내고, 전체적으로 비슷하다면 Ridge가 더 높은 성능을 냅니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nmodel = Ridge(alpha=0.1, normalize=True)\n\nmodel.fit(x_train, y_train)\n\nprint('R^2 : {:.3f}'.format(calculate_r2(model, x_test, y_test)))\nprint('Adjust R^2 : {:.3f}'.format(calculate_adjust_r2(model, x_test, y_test)))\nprint('RMSE : {:.2f}'.format(calculate_rmse(model, x_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ** Elastic Net**\n\nLlastic Net은 Ridge와 Lasso의 penalty를 모두 가지는 regression model입니다.\n\n### <u> Cost Function </u> \n- Cost function for MSE: $J_{\\theta} = \\frac{1}{n} \\sum_{i = 1}^{n}(Y - \\hat{Y})^2$\n- Cost function for ElasticNet: $MSE + penalty = \\frac{1}{n} \\sum_{i = 1}^{n}(Y - \\hat{Y})^2 + \\alpha \\sum_{j = 0}^{p}(w_j)^2 + \\alpha \\sum_{j = 0}^{p}|w_j|$\n- 여기서 m은 feature의 개수가 되고,  $\\alpha$ 는 패널티의 니과를 조절해주는 파라미터 이다. $\\alpha$ 가 작아지면 선형회귀와 같은 모형이 되고, $\\alpha$ 가 커지면 패널티의 영향력이 커집니다. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\n\nmodel = ElasticNet(alpha=0.1, l1_ratio=0.1)\n\nmodel.fit(x_train, y_train)\n\nprint('R^2 : {:.3f}'.format(calculate_r2(model, x_test, y_test)))\nprint('Adjust R^2 : {:.3f}'.format(calculate_adjust_r2(model, x_test, y_test)))\nprint('RMSE : {:.2f}'.format(calculate_rmse(model, x_test, y_test)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}