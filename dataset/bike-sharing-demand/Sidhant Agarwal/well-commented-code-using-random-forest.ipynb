{"cells":[{"metadata":{},"cell_type":"markdown","source":"**If you like my notebook, please upvote my work!**\n\n**If you use parts of this notebook in your scripts/notebooks, giving some kind of credit for instance link back to this notebook would be very much appreciated. Thanks in advance! :)**","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing important libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import calendar\nimport seaborn as sb\nimport xgboost as xgb\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import mean_squared_log_error,make_scorer\nfrom sklearn.model_selection import train_test_split,GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the training dataset","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Reading the file\nfile = pd.read_csv(\"/kaggle/input/bike-sharing-demand/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking for null values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"file.isnull().sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No data cleaning is required since no Null values are found!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"file.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data visualization:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Heatmap of all the continuous values in the file.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = file[['temp','atemp','humidity', 'windspeed','casual', 'registered','count']].corr()\nf,axes = plt.subplots(1,1,figsize = (8,8))\nsb.heatmap(corr,square=True,annot = True,linewidth = .5,center = 1.4,ax = axes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The conclusions drawn are:\n1. We can see that temp and atemp have a very strong positive correlation therefore we can use only temp as a variable without any loss of information. \n\n2. We can infer from the correlaton matrix that windspeed has almost no correlation with the casual,registered or count which we wish to predict so we can remove that","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data visualizaton for non continuous variables in data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First we have to separate the individual date and time for each data point into hour,day,month and year.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"file = file\nfile['Date'] = pd.DatetimeIndex(file['datetime']).date\nfile['Hour'] = pd.DatetimeIndex(file['datetime']).hour\nfile['Day'] = pd.DatetimeIndex(file['datetime']).day\nfile['Month'] = pd.DatetimeIndex(file['datetime']).month\nfile['Year'] = pd.DatetimeIndex(file['datetime']).year\nfile['Weekday'] = pd.DatetimeIndex(file['datetime']).weekday_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = []\nfor i in file.index:\n    a.append('Total Count : '+str(file['count'][i]))\nfile['count_vis'] = a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(x = 'Date', y = \"count\", data_frame = file,color = 'Hour',range_y = (0,1150),\n              title = 'Interactive LinePlot of the whole dataset(Hover for more details)',\n              hover_data = ['Hour','Date','casual','registered'],\n              hover_name = 'count_vis', text = None,\n              height = 670,width = 980)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The sudden periodic changes between the differrent regions is due to the missing data.These are the regions in which the regions we have to predict the result.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Season","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f,axes = plt.subplots(1,3,figsize = (17,7))\nsb.despine(left = True)\nx = 'season'\n\nsb.barplot(x = x , y = 'casual' , data = file, saturation = 1, ax =  axes[0])\nsb.barplot(x = x , y = 'registered' , data = file, saturation = 1, ax = axes[1])\nsb.barplot(x = x , y = 'count' , data = file, saturation = 1, ax = axes[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Holiday","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f,axes = plt.subplots(1,3,figsize = (17,7))\nsb.despine(left = True)\nx = 'holiday'\n\nsb.barplot(x = x , y = 'casual' , data = file, saturation = 1, ax =  axes[0] ,)\nsb.barplot(x = x , y = 'registered' , data = file, saturation = 1, ax = axes[1])\nsb.barplot(x = x , y = 'count' , data = file, saturation = 1, ax = axes[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Working day","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f,axes = plt.subplots(1,3,figsize = (17,7))\nsb.despine(left = True)\nx = 'workingday'\n\nsb.barplot(x = x , y = 'casual' , data = file, saturation = 1, ax =  axes[0] ,)\nsb.barplot(x = x , y = 'registered' , data = file, saturation = 1, ax = axes[1])\nsb.barplot(x = x , y = 'count' , data = file, saturation = 1, ax = axes[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combining the graphs of casual and registered into one of may make it seem like that holiday and workingday have no dependence on count but we can clearly see that holiday increases the casual amount by upto 40% and a reverse kind of trend is observed in working day so it is reasonable to take two different models one for casual and another for registered.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Therefore what I will attempt to do is make two separate models for the casual and the registerd training them separately and then adding the result to get the count.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4. Weather","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f,axes = plt.subplots(1,3,figsize = (17,7))\nsb.despine(left = True)\nx = 'weather'\n\nsb.barplot(x = x , y = 'casual' , data = file, saturation = 1, ax =  axes[0] ,)\nsb.barplot(x = x , y = 'registered' , data = file, saturation = 1, ax = axes[1])\nsb.barplot(x = x , y = 'count' , data = file, saturation = 1, ax = axes[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Date and Time","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 5.a. Hour","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f,axes = plt.subplots(1,3,figsize = (19,7))\nsb.despine(left = True)\nx = 'Hour'\n\nsb.barplot(x = x , y = 'casual' , data = file, saturation = 1, ax =  axes[0] ,)\nsb.barplot(x = x , y = 'registered' , data = file, saturation = 1, ax = axes[1])\nsb.barplot(x = x , y = 'count' , data = file, saturation = 1, ax = axes[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that in the final graph there are two prominent peaks.\n1. At 8 a.m\n2. At 5 p.m\nWhere as the trend of casual is mostly the same throughout the day. \n\nFrom this we can conclude that the registered are mostly people going on their jobs which explains the peaks at the start and end of office hours. Clearly these people would have a more definite and predictible schedule and are therefore more likely to be registered.In order to test this we plot somw more graphs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"file.groupby('Weekday').count().index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file1 = file.groupby(['Hour','Weekday']).mean().reset_index()\ndic = {'Weekday':['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']}\ndic1 = {'registered':'Average count of registered poeple commuting.','count': 'Average people commuting','Hour':'Hour of the day',\n        'Weekday':'Day of the week'}\nfig = px.line(x = 'Hour', y = \"registered\", data_frame = file1.reset_index(),color = 'Weekday',\n              title = 'Interactive LinePlot of the registered separated by weekday(Hover for more details)',labels = dic1,\n              hover_data = ['count'],category_orders = dic,range_y = [0,550],height = 670,width = 980)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly We can see that on the days of saturday and sunday,the general trend more or less follows the trend of casual where as on weekdays there is a completely different trend of two peaks at 8 am and 5 pm which confirms that those peaks are due to the workpeople commuting.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 5.b. Day","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f,axes = plt.subplots(1,3,figsize = (19,7))\nsb.despine(left = True)\nx = 'Day'\n\nsb.barplot(x = x , y = 'casual' , data = file, saturation = 1, ax =  axes[0] ,)\nsb.barplot(x = x , y = 'registered' , data = file, saturation = 1, ax = axes[1])\nsb.barplot(x = x , y = 'count' , data = file, saturation = 1, ax = axes[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the above graphs we can conclude that the feature day has hardly any influence over the features registered and count.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 5.c. Month","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f,axes = plt.subplots(1,3,figsize = (19,7))\nsb.despine(left = True)\nx = 'Month'\n#order = ['January','February','March','April','May','June','July','August','September','October','November','December']\nplot = sb.barplot(x = x , y = 'casual' , data = file, saturation = 1, ax =  axes[0])\nsb.barplot(x = x , y = 'registered' , data = file, saturation = 1, ax = axes[1])\nsb.barplot(x = x , y = 'count' , data = file, saturation = 1, ax = axes[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.d. Year ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f,axes = plt.subplots(1,3,figsize = (19,7))\nsb.despine(left = True)\nx = 'Year'\n\nsb.barplot(x = x , y = 'casual' , data = file, saturation = 1, ax =  axes[0] ,)\nsb.barplot(x = x , y = 'registered' , data = file, saturation = 1, ax = axes[1])\nsb.barplot(x = x , y = 'count' , data = file, saturation = 1, ax = axes[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adding relevant columns for each of the categorical data columns and removing unnecesary ones","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Season","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in file.groupby('season').count().index:\n    s = 's'+str(i)\n    a=[]\n    for j in file.season:\n        if j==i:\n            a.append(1)\n        else:\n            a.append(0)\n    file[s]=a\nfile.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Weather ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in file.groupby('weather').count().index:\n    s = 'w'+str(i)\n    a=[]\n    for j in file.weather:\n        if j==i:\n            a.append(1)\n        else:\n            a.append(0)\n    file[s]=a\nfile.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Hour","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in file.groupby('Hour').count().index:\n    s = 'Hour'+str(i)\n    a=[]\n    for j in file.Hour:\n        if j==i:\n            a.append(1)\n        else:\n            a.append(0)\n    file[s]=a\nfile.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.Month","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in file.groupby(\"Month\").count().index:\n    s = 'Month' + str(i)\n    a = []\n    for j in file.Month:\n        if j==i:\n            a.append(1)\n        else:\n            a.append(0)\n    file[s] = a\nfile.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removing unnecessary columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feed = file[['Hour0', 'Hour1', 'Hour2', 'Hour3', 'Hour4', 'Hour5',\n       'Hour6', 'Hour7', 'Hour8', 'Hour9', 'Hour10', 'Hour11', 'Hour12',\n       'Hour13', 'Hour14', 'Hour15', 'Hour16', 'Hour17', 'Hour18', 'Hour19',\n       'Hour20', 'Hour21', 'Hour22', 'Hour23','Month1', 'Month2', 'Month3',\n       'Month4', 'Month5', 'Month6', 'Month7', 'Month8', 'Month9', 'Month10',\n       'Month11', 'Month12','Year','s1','s2','s3','s4','holiday','workingday',\n        'w1','w2','w3','w4','temp','humidity','casual','registered']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feed.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feed.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing training and testing sets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1. Training set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_x = feed.drop('casual',axis = 1).drop('registered',axis=1)\ndf_train_x.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Training set will be the same for both the models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2. Training set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reg_train_y = feed['registered']\ndf_reg_train_y.describe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cas_train_y = feed['casual']\ndf_cas_train_y.describe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine learning model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Splitting data into train and test sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x1_train, x1_test, y1_train, y1_test = train_test_split(df_train_x, df_cas_train_y, test_size=0.15, random_state=42)\nx2_train, x2_test, y2_train, y2_test = train_test_split(df_train_x, df_reg_train_y, test_size=0.15, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using polynomial on the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"poly = PolynomialFeatures(degree=2)\npoly_x1_train = poly.fit_transform(x1_train)\npoly_x1_test = poly.fit_transform(x1_test)\npoly_x2_train = poly.fit_transform(x2_train)\npoly_x2_test = poly.fit_transform(x2_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Trying different models to see which one works best for the given data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor()\n\nparameters = {'n_estimators':[50,100,150,200,250],\n              'min_impurity_decrease':[0.0,0.001,0.01],\n              'max_depth':[20,40,60,80,100]}\n\nmodels = ['Normal Linear Regression: ','Linear Regression over polynomial: ',\n          'Decision Tree Regressor: ','XG Boosting: ']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining a custom scorer function for the models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_scorer(y_true,y_pred):\n    for i in range(len(y_pred)):\n        if y_pred[i]<0:\n            y_pred[i] = 1\n    return np.sqrt(mean_squared_log_error(y_true, y_pred ))\nscorer = make_scorer(custom_scorer,greater_is_better = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I have defined a scorer function as we are using mean squared log loss which does not work on negative values and the models were sometimes predicting negative values which messes with the scores and since we know that these values will always be positive we just replace them with the minimum value in the train set which is 1.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = []\nreg = LinearRegression().fit(x1_train, y1_train)\npre_reg = reg.predict(x1_test)\n\nreg_poly = LinearRegression().fit(poly_x1_train, y1_train)\npre_reg_poly = reg_poly.predict(poly_x1_test)\n\nrf_reg = GridSearchCV(rf, parameters, cv=5, verbose=2,scoring = scorer,n_jobs = -1)\nrf_reg.fit(x1_train, y1_train)\npre_rf_reg = rf_reg.predict(x1_test)\n\npredict.append(pre_reg)\npredict.append(pre_reg_poly)\npredict.append(pre_rf_reg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for prediction in range(len(predict)):\n    pre = []\n    for p in predict[prediction]:\n        if p < 1:\n            pre.append(1)\n        else:\n            pre.append(p)\n    print(models[prediction]+str(np.sqrt(mean_squared_log_error(y1_test, pre ))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = []\ncas = LinearRegression().fit(x2_train, y2_train)\npre_cas = cas.predict(x2_test)\n\ncas_poly = LinearRegression().fit(poly_x2_train, y2_train)\npre_cas_poly = cas_poly.predict(poly_x2_test)\n\nrf_cas = GridSearchCV(rf, parameters, cv=5, verbose=2,scoring = scorer,n_jobs = -1)\nrf_cas.fit(x2_train, y2_train)\npre_rf_cas = rf_cas.predict(x2_test)\n\npredict.append(pre_cas)\npredict.append(pre_cas_poly)\npredict.append(pre_rf_cas)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for prediction in range(len(predict)):\n    pre = []\n    for p in predict[prediction]:\n        if p < 1:\n            pre.append(1)\n        else:\n            pre.append(p)\n    print(models[prediction]+str(np.sqrt(mean_squared_log_error(y2_test, pre ))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"For Random Forest Model: \")\nprint(\"\\t Best Parametres for registered are: \",end='')\nprint(rf_reg.best_params_)\nprint(\"\\t Best Parametres for casual are: \",end = '')\nprint(rf_cas.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict1 = []\n\nreg1 = LinearRegression().fit(x1_train, y1_train)\npre_reg1 = reg1.predict(x1_test)\n\nreg1_poly = LinearRegression().fit(poly_x1_train, y1_train)\npre_reg1_poly = reg1_poly.predict(poly_x1_test)\n\nrf1 = RandomForestRegressor(n_estimators = 200,max_depth=80,min_impurity_decrease = 0.001).fit(x1_train, y1_train)\npre_rf1 = rf1.predict(x1_test)\n\nfor i in range(pre_reg1.size):\n    if pre_reg1[i]<1:\n        pre_reg1[i] = 1 \n    if pre_reg1_poly[i]<1:\n        pre_reg1_poly[i] = 1\n    if pre_rf1[i]<1:\n        pre_rf1[i] = 1\n\npredict1.append(pre_reg1)\npredict1.append(pre_reg1_poly)\npredict1.append(pre_rf1)\n\nx1_final = x1_test.copy()\nx1_final['Output'] = y1_test\nx1_final['Lin_reg'] = pre_reg1\nx1_final['Lin_reg_poly'] = pre_reg1_poly\nx1_final['RF_reg'] = pre_rf1\nx1_final['Resid'] = y1_test-pre_reg1\nx1_final['Resid_poly'] = y1_test-pre_reg1_poly\n\nfor prediction in predict1:\n    print(np.sqrt(mean_squared_log_error( y1_test, prediction )))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we know that the output is never less than 1 we replace all negative values with 1 before appending in pre to calculate error.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predict2 = []\n\nreg2 = LinearRegression().fit(x2_train, y2_train)\npre_reg2 = reg2.predict(x2_test)\n\nreg2_poly = LinearRegression().fit(poly_x2_train, y2_train)\npre_reg2_poly = reg2_poly.predict(poly_x2_test)\n\nrf2 = RandomForestRegressor(n_estimators = 150,max_depth=60,min_impurity_decrease = 0.0).fit(x2_train, y2_train)\npre_rf2 = rf2.predict(x2_test)\n\nfor i in range(pre_reg2.size):\n    if pre_reg2[i]<1:\n        pre_reg2[i] = 1 \n    if pre_reg2_poly[i]<1:\n        pre_reg2_poly[i] = 1\n    if pre_rf2[i]<1:\n        pre_rf2[i] = 1\n\npredict2.append(pre_reg2)\npredict2.append(pre_reg2_poly)\npredict2.append(pre_rf2)\n\nx2_final = x2_test.copy()\nx2_final['Output'] = y2_test\nx2_final['Lin_reg'] = pre_reg2\nx2_final['Lin_reg_poly'] = pre_reg2_poly\nx2_final['RF_reg'] = pre_rf2\nx2_final['Resid'] = y2_test-pre_reg2\nx2_final['Resid_poly'] = y2_test-pre_reg2_poly\n\nfor prediction in predict2:\n    print(np.sqrt(mean_squared_log_error( y2_test, prediction )))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting the residual plots","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from plotly.subplots import make_subplots\nname1  = ['Residual for casual without polynomial features'] *1633\nname2  = ['Residual for casual with polynomial features'] *1633\nname3  = ['Residual for registered without polynomial features'] *1633\nname4  = ['Residual for registered with polynomial features'] *1633\ndic = {'Lin_reg': 'Predicted Output','Resid':'Deviation from predicted','Output':'Expected Output','Lin_reg_poly': 'Predicted Output',\n       'Resid_poly':'Deviation from predicted'}\nfig1 = px.scatter(data_frame = x1_final,x = 'Lin_reg', y = 'Resid',hover_data = ['Output'],labels = dic,hover_name = name1,\n                  color_discrete_sequence = ['red'])\nfig2 = px.scatter(data_frame = x1_final,x = 'Lin_reg_poly', y = 'Resid_poly',hover_data = ['Output'],labels = dic,hover_name = name2,\n                  color_discrete_sequence = ['blue'])\nfig3 = px.scatter(data_frame = x2_final,x = 'Lin_reg', y = 'Resid',hover_data = ['Output'],labels = dic,hover_name = name3,\n                  color_discrete_sequence = ['darkgreen'])\nfig4 = px.scatter(data_frame = x2_final,x = 'Lin_reg_poly', y = 'Resid_poly',hover_data = ['Output'],labels = dic,hover_name = name4,\n                  color_discrete_sequence = ['gold'])\ntrace1 = fig1['data'][0]\ntrace2 = fig2['data'][0]\ntrace3 = fig3['data'][0]\ntrace4 = fig4['data'][0]\n\nfig = make_subplots(rows=2, cols=2,horizontal_spacing =0.1,vertical_spacing  = 0.2,\n                    row_titles = ['Using Polynomial','Without Polynomial'],column_titles = ['Casual','Registered'],\n                    x_title = 'Residual plots for Registered and Casual under different models (Hover for more details)')\n\nfig.add_trace(trace1, row=1, col=1)\nfig.add_trace(trace2, row=1, col=2)\nfig.add_trace(trace3, row=2, col=1)\nfig.add_trace(trace4, row=2, col=2)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Since the residual plots show a conical divergence therefore we can conclude that Linear Regression is definitely not a suitable model for the predicting in the above distribution of data**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Clearly the random forest model works best in this situation.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Retraining the decision tree over the whole dataset for submission.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf1 = RandomForestRegressor(n_estimators = 200,max_depth=80,min_impurity_decrease = 0.001).fit(df_train_x,df_cas_train_y)\nrf2 = RandomForestRegressor(n_estimators = 150,max_depth=60,min_impurity_decrease = 0.0).fit(df_train_x,df_reg_train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading the test file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_file = pd.read_csv('/kaggle/input/bike-sharing-demand/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=test_file\ntest.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Processing of the test file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test['mth'] = pd.DatetimeIndex(test['datetime']).month\ntest['yr'] = pd.DatetimeIndex(test['datetime']).year\ntest['dy'] = pd.DatetimeIndex(test['datetime']).day\ntest['hr'] = pd.DatetimeIndex(test['datetime']).hour\n\nfor i in test.groupby(\"season\").count().index:\n    s = 's' + str(i)\n    a = []\n    for j in test.season:\n        if j==i:\n            a.append(1)\n        else:\n            a.append(0)\n    test[s] = a\nfor i in test.groupby(\"weather\").count().index:\n    s = 'w' + str(i)\n    a = []\n    for j in test.weather:\n        if j==i:\n            a.append(1)\n        else:\n            a.append(0)\n    test[s] = a\nfor i in test.groupby('hr').count().index:\n    s = 'hr'+str(i)\n    a=[]\n    for j in test.hr:\n        if j==i:\n            a.append(1)\n        else:\n            a.append(0)\n    test[s]=a\nfor i in test.groupby(\"mth\").count().index:\n    s = 'm' + str(i)\n    a = []\n    for j in test.mth:\n        if j==i:\n            a.append(1)\n        else:\n            a.append(0)\n    test[s] = a\ntest.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test[['hr0','hr1','hr2','hr3','hr4','hr5','hr6','hr7','hr8','hr9','hr10','hr11','hr12','hr13','hr14','hr15','hr16','hr17','hr18',\n                 'hr19','hr20','hr21','hr22','hr23','m1','m2','m3','m4','m5','m6','m7','m8','m9','m10','m11','m12','yr',\n                 's1','s2','s3','s4','holiday','workingday','w1','w2','w3','w4','temp','humidity']]\ntest.describe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting the output over test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pre_cas = rf1.predict(test)\npre_reg = rf2.predict(test)\nfinal_predictions = pd.DataFrame(pre_cas+pre_reg,columns = ['cout'])\n\nfinal_predictions.describe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s=[]\nfor j in final_predictions.cout:\n    if int(j)<1:\n        s.append(1)\n    else:\n        s.append(j)\nfinal_predictions['count'] = s ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Since we know that the output is never less than 1 we have to replace all negative values with 1.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_predictions.describe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_predictions['datetime']=test_file['datetime']\nfinal_predictions = final_predictions[['datetime','count']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_predictions.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exporting output to csv","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_predictions.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}