{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Purpose\n\nIn this notebook I created an XGB Regressor model that predicts the number of bikes for a given hour of a given day. After some exploratory data analysis, I engineered a few featuers for dayofweek, month, and hour, which helped with predictions. The XGB Regressor is fairly easy to set up, and I hope it helps others get started. Let me know if you have any suggestsions/questions!","metadata":{}},{"cell_type":"markdown","source":"# Initial Setup","metadata":{}},{"cell_type":"code","source":"### Functions used in notebook\n\ndef correlation_table(df, width, height):\n    \n    import seaborn as sns\n\n    # Create Correlation df from source df\n    corr = df.corr()\n    # Plot figsize\n    fig, ax = plt.subplots(figsize=(width, height))\n    # Drop self-correlations\n    dropSelf = np.zeros_like(corr)\n    dropSelf[np.triu_indices_from(dropSelf)] = True \n\n    # Generate Heat Map, allow annotations and place floats in map\n    sns.heatmap(corr, cmap=\"RdBu\", annot=True, fmt=\".2f\", mask=dropSelf, \n        xticklabels=corr.columns, \n            yticklabels=corr.columns, ax=ax, linewidths=.5, cbar_kws={\"shrink\": .7},\n            vmin = -1, vmax=1, center=0)\n    plt.title('Correlation HeatMap',fontsize=14)\n    plt.show()  \n    \n    \ndef unistats(df):\n  import pandas as pd\n  output_df = pd.DataFrame(columns=['Numeric', 'Count', 'Unique', 'Missing', 'Mean', 'Mode',  \n                                  'Min', 'Max', 'Stdev', 'Q1', 'Median', 'Q3', 'Skew', 'Kurt'])\n\n  for col in df:\n      numeric = pd.api.types.is_numeric_dtype(df[col])\n      if numeric:\n        output_df.loc[col] = [True, df[col].count(), df[col].nunique(), df[col].isnull().sum(), df[col].mean(), df[col].mode().values[0],  \n                              df[col].min(), df[col].max(), df[col].std(), df[col].quantile(.25), \n                              df[col].quantile(.5), df[col].quantile(.75), df[col].skew(), df[col].kurt()]\n      else:\n        output_df.loc[col] = [False, df[col].count(),df[col].nunique(), df[col].isnull().sum(), '-', df[col].mode().values[0], \n                          '-', '-', '-','-','-','-','-','-']\n\n  return output_df.sort_values(by=['Numeric', 'Skew', 'Unique'], ascending = False)\n\n\ndef get_outlier_minmax(col):\n  import pandas as pd \n  if pd.api.types.is_numeric_dtype(col):\n    if col.skew() > 1 or col.skew() < -1:\n      q1 = col.quantile(.25)\n      q3 = col.quantile(.75)\n      min = q1 - (1.5 * (q3 - q1))\n      max = q3 + (1.5 * (q3 - q1))\n      theory = 'Tukey 1.5IQR'\n    else:\n      min = col.mean() - (col.std() * 3)\n      max = col.mean() + (col.std() * 3)\n      theory = '3 σ from μ'\n    min_count = (col < min).sum()\n    max_count = (col > max).sum()\n  else:\n    min = col.min()\n    max = col.max()\n    min_count = (col == col.min()).sum()\n    max_count = (col == col.max()).sum()\n    theory = \"Categorical\"\n  \n  return min, min_count, max, max_count, theory\n\n\n\ndef detect_outliers(df, method='auto'):\n  import pandas as pd\n\n  summary_table = pd.DataFrame(columns=['total values', 'outlier min', 'count below', 'outlier max', 'count above', 'method'])\n\n  # Loop through each column in the dataframe that is numeric, not binary, and not empty\n  for col in df:\n    if pd.api.types.is_numeric_dtype(df[col]) and (len(df[col].value_counts()) > 0) and not all(df[col].value_counts().index.isin([0, 1])):\n      # Get the min, max and theory\n      min, min_count, max, max_count, theory = get_outlier_minmax(df[col])\n      # Place them in a summary df as well as a count of the outliers above and below the range; also report the theory used\n      summary_table.loc[col] = (df[col].count(), min, min_count, max, max_count, theory)\n  return summary_table","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport datetime\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\nfrom category_encoders import TargetEncoder\n\n\ntest = pd.read_csv('../input/bike-sharing-demand/test.csv', parse_dates=['datetime'])\ntrain = pd.read_csv('../input/bike-sharing-demand/train.csv', parse_dates=['datetime'])\ntrain.drop(columns=['registered','casual','atemp'], inplace=True)\ntest.drop(columns=['atemp'],inplace=True)\ntrain['workingday'] = train['workingday'].astype(object)\ntest['workingday'] = test['workingday'].astype(object)\n\ntrain['weather'] = train['weather'].astype(object)\ntest['weather'] = test['weather'].astype(object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n","metadata":{}},{"cell_type":"markdown","source":"Above I dropped some of the columns that I knew weren't all that helpful in prediciton.\n\nThen, I made a correlation table tells me that my remaining variables are not too collinear.","metadata":{}},{"cell_type":"code","source":"correlation_table(train, 10, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next I used the pandas describe() function, as well as my own univariate stats function that gives me an overview of the data.\n\nHoliday has a high skewness, but it is a binary metric. All the other features are fairly normally distributed.","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unistats(train) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unistats(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Simple bar plot showing bike count by season\n\n 1 = Spring\n \n 2 = Summer\n \n 3 = Fall\n \n 4 = Winter\n ","metadata":{}},{"cell_type":"code","source":"bars = ['Spring', 'Summer', 'Fall', 'Winter']\nx_ticks = np.arange(len(bars))\nplt.bar(train['season'], train['count'])\nplt.xticks(1+x_ticks, bars)\n\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering\n\ndayofweek, month, and hour\n\n","metadata":{}},{"cell_type":"code","source":"test['dayofweek'] = test['datetime'].dt.day_name() # Monday = 0, Sunday = 6\ntrain['dayofweek'] = train['datetime'].dt.day_name()\n\n\ntest['month'] = test['datetime'].dt.month_name() # Monday = 0, Sunday = 6\ntrain['month'] = train['datetime'].dt.month_name()\n\ntest['hour'] = test['datetime'].dt.hour.astype('object')\ntrain['hour'] = train['datetime'].dt.hour.astype('object')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"dayofweek and month turned out to be less helpful than I hoped.\n","metadata":{}},{"cell_type":"code","source":"test.drop(columns=['dayofweek','month'],inplace=True)\ntrain.drop(columns=['dayofweek','month'],inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"I have a model that identifies outliers based on the distribution of the column. \n\nAfter identifying outliers, I removed them and got a much worse score, so I decided to keep the outliers in the data.","metadata":{}},{"cell_type":"code","source":"detect_outliers(train) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detect_outliers(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I tried one-hot encoding (2 cells below), but it didn't work out too well, so I decided to do target encoding using TargetEncoder","metadata":{}},{"cell_type":"code","source":"encoder = TargetEncoder()\ntrain['season'] = encoder.fit_transform(train['season'], train['count'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = pd.get_dummies(train, prefix = ['season'], columns=['season'], drop_first = True)\n# train = pd.get_dummies(train, prefix = ['hour'], columns=['hour'], drop_first = True)\n\n# test = pd.get_dummies(test, prefix = ['season'], columns=['season'], drop_first = True)\n# test = pd.get_dummies(test, prefix = ['hour'], columns=['hour'], drop_first = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Tuning and Performance","metadata":{}},{"cell_type":"code","source":"X_train = train.drop(columns=['count'])\ny_train = np.log1p(train['count'])\n\nX_test = test\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_model = xgb.XGBRegressor()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train['workingday'] = pd.to_numeric(X_train['workingday'])\nX_train['weather'] = pd.to_numeric(X_train['weather'])\nX_train['hour'] = pd.to_numeric(X_train['hour'])\n\ntest['workingday'] = pd.to_numeric(test['workingday'])\ntest['weather'] = pd.to_numeric(test['weather'])\ntest['hour'] = pd.to_numeric(test['hour'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Fitting the model","metadata":{}},{"cell_type":"code","source":"xgb_model.fit(X_train.drop(columns=['datetime']), y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scoring the model","metadata":{}},{"cell_type":"code","source":"score = xgb_model.score(X_train.drop(columns=['datetime']),y_train)\nprint(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['Prediction'] =np.expm1(xgb_model.predict(test.drop(columns=['datetime']))).clip(0)\nfilename = 'submission.csv'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({'datetime': test['datetime'], 'Count': test['Prediction']}).to_csv(filename, index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Score\n\nWith this submission I achieved a score of 0.43352","metadata":{}},{"cell_type":"markdown","source":"# Alternative Approaches\n\nIn the future, I'd try thinking up more features to engineer as well as do more thorough encoding of categorical variables. \n\nAdditionally, I coudl probably do better tuning of hyperparameters in my model and achieve better results. \n\nOverall, I had fun with this competition and I hope you do too!","metadata":{}}]}