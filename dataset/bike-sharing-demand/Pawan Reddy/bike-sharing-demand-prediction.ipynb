{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom datetime import datetime\nsns.set_style('darkgrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = pd.read_csv(\"/kaggle/input/bike-sharing-demand/train.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1 = pd.read_csv('/kaggle/input/bike-sharing-demand/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pre_processing(df,train=True):\n    \n    if train == True:\n        \n        # Pre processing train data\n        #  Adding time and month to the dataframe using datatime column\n        time=[]\n        month=[]\n        for i in df['datetime']:\n                dt_object2 = datetime.strptime(i, \"%Y-%m-%d %H:%M:%S\")\n                time.append(dt_object2.hour)\n                month.append(dt_object2.month)\n        df['time'] = pd.DataFrame(time)\n        df['time'] = df['time'].astype(float)\n        df['month'] = pd.DataFrame(month)\n        df['month'] = df['month'].astype(float)\n        \n        # Dropping datetime column\n        df.drop('datetime',axis=1,inplace=True) \n        # Dropping holiday column as it is highly correlated to 'workingday' column\n        df.drop('holiday',axis=1,inplace=True) \n        # Dropping atemp column as it is highly correlated to 'temp' column\n        df = df.drop('atemp',axis=1) # No use \n\n        \n        # One hot encoding on categorical columns.\n        df = pd.get_dummies(df,columns=['season','weather'],drop_first=True)  \n        \n        # Median imputation if any\n        for i in df.columns:\n            df[i].fillna(value = df[i].median())\n        return df\n    \n    \n    else:\n        # Pre processing Test data\n        #  Adding time and month to the dataframe using datatime column\n        time=[]\n        month=[]\n        for i in df['datetime']:\n                dt_object2 = datetime.strptime(i, \"%Y-%m-%d %H:%M:%S\")\n                time.append(dt_object2.hour)\n                month.append(dt_object2.month)\n        df['time'] = pd.DataFrame(time)\n        df['time'] = df['time'].astype(float)\n        df['month'] = pd.DataFrame(month)\n        df['month'] = df['month'].astype(float)\n        \n        # Dropping datetime column\n        df.drop('datetime',axis=1,inplace=True) \n        # Dropping holiday column as it is highly correlated to 'workingday' column\n        df.drop('holiday',axis=1,inplace=True) \n        # Dropping atemp column as it is highly correlated to 'temp' column\n        df = df.drop('atemp',axis=1) \n        \n        # One hot encoding on categorical columns.\n        df = pd.get_dummies(df,columns=['season','weather'],drop_first=True)  \n        \n         # Median imputation if there are any null values\n        for i in df.columns:\n            df[i].fillna(value = df[i].median())\n            \n\n        return df  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(df):\n    global nw_count_pred\n    global count_pred\n    global final_count\n    global model_1\n    global model_2\n    global model_3\n    global model_4\n\n    \n    # Segregating data based on 'workingday' column\n    work_day = df[df['workingday']==1] \n    non_work_day = df[df['workingday']==0]\n    # Model for registered\n    x = work_day.drop(['casual','registered','count'],axis = 1)\n    y = work_day.registered\n    # Dividing the data into train and test\n    from sklearn.model_selection import train_test_split\n    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.30 ,random_state = 2)\n    \n    from sklearn.ensemble import RandomForestRegressor\n    from sklearn.ensemble import AdaBoostRegressor\n\n    # Finding best parameters for RandomForestRegressor\n    rf = RandomForestRegressor(random_state=0)\n    rf_params = {'n_estimators':np.arange(25,150,25),'max_depth':np.arange(1,11,2),'min_samples_leaf':np.arange(2,15,3)}\n    from sklearn.model_selection import GridSearchCV\n    gs_rf = GridSearchCV(rf,rf_params,cv=3)\n    gs_rf.fit(x_train,y_train)\n    \n    a = gs_rf.best_params_\n    \n    # Fitting the model with best params on the whole data\n    RF = RandomForestRegressor(n_estimators=a['n_estimators'],max_depth=a['max_depth'],min_samples_leaf=a['min_samples_leaf'],random_state=0)\n    \n    model_1 = RF.fit(x,y)\n    work_day_reg_pred = model_1.predict(x)\n    from sklearn.metrics import mean_squared_log_error\n    msle=mean_squared_log_error(work_day_reg_pred,y)\n    rmsle=np.sqrt(msle)\n    print('RMLSE for the wr:',rmsle) \n    # Plotting important features\n    importances = RF.feature_importances_\n    plt.title('Registered Feature Importances')\n    plt.barh(range(len(importances)), importances, color='g', align='center')\n    plt.yticks(range(len(importances)), x.columns)\n    plt.xlabel('Relative Importance')\n    plt.show()\n    \n    \n    # Model for registered\n    x = work_day.drop(['casual','registered','count'],axis = 1)\n    y = work_day.casual\n    # Dividing the data into train and test\n    from sklearn.model_selection import train_test_split\n    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.30 ,random_state = 2)\n    \n    from sklearn.ensemble import RandomForestRegressor\n    # Finding best parameters for RandomForestRegressor\n    rf = RandomForestRegressor(random_state=0)\n    rf_params = {'n_estimators':np.arange(25,150,25),'max_depth':np.arange(1,11,2),'min_samples_leaf':np.arange(2,15,3)}\n    from sklearn.model_selection import GridSearchCV\n    gs_rf = GridSearchCV(rf,rf_params,cv=3)\n    gs_rf.fit(x_train,y_train)\n    \n    a = gs_rf.best_params_\n    \n    # Fitting the model with best params on the whole data\n    RF = RandomForestRegressor(n_estimators=a['n_estimators'],max_depth=a['max_depth'],min_samples_leaf=a['min_samples_leaf'],random_state=0)\n    \n    model_2 = RF.fit(x,y)\n\n    work_day_casual_pred = model_2.predict(x)\n    from sklearn.metrics import mean_squared_log_error\n    msle=mean_squared_log_error(work_day_casual_pred,y)\n    rmsle=np.sqrt(msle)\n    print('RMLSE for the wc:',rmsle) \n    \n    # Plotting important features\n    importances = RF.feature_importances_\n    plt.title('Casual-Feature Importances')\n    plt.barh(range(len(importances)), importances, color='g', align='center')\n    plt.yticks(range(len(importances)), x.columns)\n    plt.xlabel('Relative Importance')\n    plt.show()\n    \n    \n    # Adding the above predicted values of casual and registered\n    count_pred = pd.DataFrame()\n    count_pred['casual'] = work_day_casual_pred\n    count_pred['registered'] = work_day_reg_pred\n    count_pred['count_pred'] = work_day_casual_pred+work_day_reg_pred\n    count_pred.index = x.index\n    \n    \n     # Checking Root mean suared error of predicted count\n    from sklearn.metrics import mean_squared_log_error\n    msle=mean_squared_log_error(work_day['count'],count_pred['count_pred'])\n    rmsle=np.sqrt(msle)\n    print('RMLSE for a working day:',rmsle)\n    \n    # Model for registered\n    x = non_work_day.drop(['casual','registered','count'],axis = 1)\n    y = non_work_day.registered\n    # Dividing the data into train and test\n    from sklearn.model_selection import train_test_split\n    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.30 ,random_state = 2)\n    \n    from sklearn.ensemble import RandomForestRegressor\n    # Finding best parameters for RandomForestRegressor\n    rf = RandomForestRegressor(random_state=0)\n    rf_params = {'n_estimators':np.arange(25,150,25),'max_depth':np.arange(1,11,2),'min_samples_leaf':np.arange(2,15,3)}\n    from sklearn.model_selection import GridSearchCV\n    gs_rf = GridSearchCV(rf,rf_params,cv=3)\n    gs_rf.fit(x_train,y_train)\n    \n    a = gs_rf.best_params_\n    \n    # Fitting the model with best params on the whole data\n    RF = RandomForestRegressor(n_estimators=a['n_estimators'],max_depth=a['max_depth'],min_samples_leaf=a['min_samples_leaf'],random_state=0)\n    \n    model_3 = RF.fit(x,y)\n\n    non_work_day_reg_pred = model_3.predict(x)\n    from sklearn.metrics import mean_squared_log_error\n    msle=mean_squared_log_error(non_work_day_reg_pred,y)\n    rmsle=np.sqrt(msle)\n    print('RMLSE for the nwr:',rmsle) \n    \n    # Plotting important features\n    importances = RF.feature_importances_\n    plt.title('Registered Feature Importances')\n    plt.barh(range(len(importances)), importances, color='g', align='center')\n    plt.yticks(range(len(importances)), x.columns)\n    plt.xlabel('Relative Importance')\n    plt.show()\n    \n    \n    # Model for registered\n    x = non_work_day.drop(['casual','registered','count'],axis = 1)\n    y = non_work_day.casual\n    # Dividing the data into train and test\n    from sklearn.model_selection import train_test_split\n    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.30 ,random_state = 2)\n    \n    from sklearn.ensemble import RandomForestRegressor\n    # Finding best parameters for RandomForestRegressor\n    rf = RandomForestRegressor(random_state=0)\n    rf_params = {'n_estimators':np.arange(25,150,25),'max_depth':np.arange(1,11,2),'min_samples_leaf':np.arange(2,15,3)}\n    from sklearn.model_selection import GridSearchCV\n    gs_rf = GridSearchCV(rf,rf_params,cv=3)\n    gs_rf.fit(x_train,y_train)\n    \n    a = gs_rf.best_params_\n    \n    # Fitting the model with best params on the whole data\n    RF = RandomForestRegressor(n_estimators=a['n_estimators'],max_depth=a['max_depth'],min_samples_leaf=a['min_samples_leaf'],random_state=0)\n    \n    model_4 = RF.fit(x,y)\n\n    non_work_day_casual_pred = model_4.predict(x)\n    from sklearn.metrics import mean_squared_log_error\n    msle=mean_squared_log_error(non_work_day_casual_pred,y)\n    rmsle=np.sqrt(msle)\n    print('RMLSE for the nwc:',rmsle) \n    \n    \n    # Plotting important features\n    importances = RF.feature_importances_\n    plt.title('Casual-Feature Importances')\n    plt.barh(range(len(importances)), importances, color='g', align='center')\n    plt.yticks(range(len(importances)), x.columns)\n    plt.xlabel('Relative Importance')\n    plt.show()\n    \n\n    nw_count_pred = pd.DataFrame()\n    nw_count_pred['casual'] = non_work_day_casual_pred\n    nw_count_pred['registered'] = non_work_day_reg_pred\n    nw_count_pred['nw_count_pred'] = non_work_day_casual_pred+non_work_day_reg_pred\n    nw_count_pred.index = x.index\n\n    \n    from sklearn.metrics import mean_squared_log_error\n    msle=mean_squared_log_error(non_work_day['count'],nw_count_pred['nw_count_pred'])\n    rmsle=np.sqrt(msle)\n    print('RMLSE for a non working day:',rmsle)\n    \n    final_count = pd.DataFrame()\n    final_count = pd.concat([count_pred['count_pred'],nw_count_pred['nw_count_pred']])\n    final_count.sort_index(inplace = True)\n    \n         # Checking Root mean suared error of predicted count\n    from sklearn.metrics import mean_squared_log_error\n    msle=mean_squared_log_error(df['count'],final_count)\n    rmsle=np.sqrt(msle)\n    print('RMLSE for the whole data:',rmsle)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = pre_processing(train1,train=True)\nmodel(a)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def model_test(df):\n    global nw_count_pred\n    global count_pred\n    global final_count\n    \n    # Segregating data based on 'workingday' column\n    work_day = df[df['workingday']==1] \n    non_work_day = df[df['workingday']==0]\n    # Model for registered\n    x = work_day\n\n    work_day_reg_pred = model_1.predict(x)\n    \n\n    # Model for registered\n    x = work_day\n\n    work_day_casual_pred = model_2.predict(x)\n\n    \n    # Adding the above predicted values of casual and registered\n    count_pred = pd.DataFrame()\n    count_pred['casual'] = work_day_casual_pred\n    count_pred['registered'] = work_day_reg_pred\n    count_pred['count_pred'] = work_day_casual_pred+work_day_reg_pred\n    count_pred.index = x.index\n    \n\n    # Model for registered\n    x = non_work_day\n    non_work_day_reg_pred = model_3.predict(x)\n    \n\n    # Model for registered\n    x = non_work_day\n    non_work_day_casual_pred = model_4.predict(x)\n\n\n    nw_count_pred = pd.DataFrame()\n    nw_count_pred['casual'] = non_work_day_casual_pred\n    nw_count_pred['registered'] = non_work_day_reg_pred\n    nw_count_pred['nw_count_pred'] = non_work_day_casual_pred+non_work_day_reg_pred\n    nw_count_pred.index = x.index\n\n\n    \n    final_count = pd.DataFrame()\n    final_count = pd.concat([count_pred['count_pred'],nw_count_pred['nw_count_pred']])\n    final_count.sort_index(inplace = True)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = pre_processing(test1,train=False)\nmodel_test(a)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss = pd.read_csv('/kaggle/input/bike-sharing-demand/sampleSubmission.csv')\nss.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss['count'] = final_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ss.to_csv('bike_submission9.csv',index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}