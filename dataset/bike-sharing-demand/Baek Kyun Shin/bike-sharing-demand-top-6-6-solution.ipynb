{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bike Shring Demand Top 6.6% Solution for Everyone !!"},{"metadata":{},"cell_type":"markdown","source":"### This is a simple modeling notebook using Random Forest Regression. This model reaches the top 6.6%. If you think it's useful, please upvote ^^ "},{"metadata":{},"cell_type":"markdown","source":"### I also shared [basic EDA notebook for everyone](https://www.kaggle.com/werooring/bike-sharing-demand-basic-eda-for-everyone)"},{"metadata":{},"cell_type":"markdown","source":"- [Bike Sharing Demand Competition](https://www.kaggle.com/c/bike-sharing-demand)\n\n- [Modeling Reference Notebook](https://www.kaggle.com/viveksrinivasan/eda-ensemble-model-top-10-percentile)"},{"metadata":{},"cell_type":"markdown","source":"### Load Data"},{"metadata":{"_cell_guid":"057b1690-5b93-9f14-eafe-fad12c00da69","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv(\"/kaggle/input/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/sampleSubmission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Concatenate train and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data_temp = pd.concat([train, test])\nall_data_temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = pd.concat([train, test], ignore_index=True)\nall_data","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"25e226ba-1c12-3fd3-08d8-fe69f9748b73"},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### Create new features"},{"metadata":{"_cell_guid":"18f7c3fc-ffdf-4bc6-1d4c-c455fb4e0141","trusted":true},"cell_type":"code","source":"from datetime import datetime\n\nall_data['date'] = all_data['datetime'].apply(lambda x: x.split()[0]) # Create date feature\nall_data['year'] = all_data['datetime'].apply(lambda x: x.split()[0].split('-')[0]) # Create year feature\nall_data['month'] = all_data['datetime'].apply(lambda x: x.split()[0].split('-')[1]) # Create month feature\nall_data['hour'] = all_data['datetime'].apply(lambda x: x.split()[1].split(':')[0]) # Create hour feature\nall_data[\"weekday\"] = all_data['date'].apply(lambda dateString : datetime.strptime(dateString,\"%Y-%m-%d\").weekday()) # Create weekday feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Change categorical data type for memory reduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = ['season', 'holiday', 'workingday', 'weather', 'weekday', 'month', 'year', 'hour']\n\nfor feature in categorical_features:\n    all_data[feature] = all_data[feature].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Separate train and test data. Assign train target value(y)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = all_data[pd.notnull(all_data['count'])]\ntest = all_data[~pd.notnull(all_data['count'])]\ny = train['count']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop useless features"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_features = ['count', 'casual', 'registered', 'datetime', 'date', 'datetime', 'windspeed', 'month']\n\nX_train = train.drop(drop_features, axis=1)\nX_test = test.drop(drop_features, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check final features and types"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Model and Measure Model Performance"},{"metadata":{},"cell_type":"markdown","source":"### Evaluation score(RMSLE) function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle(y_true, y_pred, convertExp=True):\n    # Apply exponential transformation function\n    if convertExp:\n        y_true = np.exp(y_true)\n        y_pred = np.exp(y_pred)\n        \n    # Convert missing value to zero after log transformation\n    log_true = np.nan_to_num(np.array([np.log(y+1) for y in y_true]))\n    log_pred = np.nan_to_num(np.array([np.log(y+1) for y in y_pred]))\n    \n    # Compute RMSLE\n    output = np.sqrt(np.mean((log_true - log_pred)**2))\n    return output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear regression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\n# Step 1: Create Model\nlinear_reg_model = LinearRegression()\n\n# Step 2: Train Model\nlog_y = np.log1p(y)  # Log Transformation of Target Value y\nlinear_reg_model.fit(X_train, log_y) \n\n# Step 3 : Predict\npreds = linear_reg_model.predict(X_train)\n\n# Step 4 : Evaluate\nprint ('Linear Regression RMSLE:', rmsle(log_y, preds, True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge Model (Apply Gridsearch)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\n\n# Step 1: Create Model\nridge_model = Ridge()\n\n# Step 2-1 : Create GridSearchCV Object\n# Hyper-parameter List\nridge_params = {'max_iter':[3000], 'alpha':[0.1, 1, 2, 3, 4, 10, 30, 100, 200, 300, 400, 800, 900, 1000]}\n# Evaluate Function for Cross-Validation (RMSLE score)\nrmsle_scorer = metrics.make_scorer(rmsle, greater_is_better=False) \n# Create GridSearchCV Object (with Ridge)\ngridsearch_ridge_model = GridSearchCV(estimator=ridge_model,\n                                      param_grid=ridge_params,\n                                      scoring=rmsle_scorer,\n                                      cv=5)\n\n# Step 2-2 : Perform Grid Search\nlog_y = np.log1p(y) # Log Transformation of Target Value y\ngridsearch_ridge_model.fit(X_train, log_y) # Train (Grid Search)\n\nprint('Best Parameter:', gridsearch_ridge_model.best_params_)\n\n# Step 3 : Predict\npreds = gridsearch_ridge_model.best_estimator_.predict(X_train)\n\n# Step 4 : Evaluate\nprint('Ridge Regression RMSLE:', rmsle(log_y, preds, True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lasso Model (Apply Gridsearch)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\n# Step 1: Create Model\nlasso_model = Lasso()\n\n# Step 2-1 : Create GridSearchCV Object\n# Hyper-parameter List\nlasso_alpha = 1/np.array([0.1, 1, 2, 3, 4, 10, 30, 100, 200, 300, 400, 800, 900, 1000])\nlasso_params = {'max_iter':[3000], 'alpha':lasso_alpha}\n# Create GridSearchCV Object (with Lasso)\ngridsearch_lasso_model = GridSearchCV(estimator=lasso_model,\n                                      param_grid=lasso_params,\n                                      scoring=rmsle_scorer,\n                                      cv=5)\n\n\n# Step 2-2 : Perform Grid Search\nlog_y = np.log1p(y)\ngridsearch_lasso_model.fit(X_train, log_y) # Train (Grid Search)\n\nprint('Best Parameter:', gridsearch_lasso_model.best_params_)\n\n# Step 3 : Predict\npreds = gridsearch_lasso_model.best_estimator_.predict(X_train)\n\n# Step 4 : Evaluate\nprint('Lasso Regression RMSLE:', rmsle(log_y, preds, True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Regression Model (Apply Grid Search)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# Step 1: Create Model\nrandomforest_model = RandomForestRegressor()\n\n# Step 2-1 : Create GridSearchCV Object\n# Hyper-parameter List\nrf_params = {'random_state':[42], 'n_estimators':[100, 120, 140]}\n# Create GridSearchCV Object (with Random Forest Regression)\ngridsearch_random_forest_model = GridSearchCV(estimator=randomforest_model,\n                                              param_grid=rf_params,\n                                              scoring=rmsle_scorer,\n                                              cv=5)\n\n# Step 2-2 : Perform Grid Search\nlog_y = np.log1p(y)\ngridsearch_random_forest_model.fit(X_train, log_y)\n\nprint('Best Parameter:', gridsearch_random_forest_model.best_params_)\n\n# 스텝 3 : 예측\npreds = gridsearch_random_forest_model.best_estimator_.predict(X_train)\n\n# 스텝 4 : 평가\nprint('Random Forest Regression RMSLE:', rmsle(log_y, preds, True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Regression Model is the Best among four models!"},{"metadata":{},"cell_type":"markdown","source":"## Submit"},{"metadata":{},"cell_type":"markdown","source":"### Compare train data vs predicted test data distribution "},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nrandomforest_preds = gridsearch_random_forest_model.best_estimator_.predict(X_test)\n\nfigure, axes = plt.subplots(ncols=2)\nfigure.set_size_inches(10, 4)\n\nsns.distplot(y, ax=axes[0], bins=50)\naxes[0].set_title('Train Data Distribution')\nsns.distplot(np.exp(randomforest_preds), ax=axes[1], bins=50)\naxes[1].set_title('Predicted Test Data Distribution');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### submit final predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['count'] = np.exp(randomforest_preds)\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I would appreciate it if you upvote my notebook. Thank you!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}