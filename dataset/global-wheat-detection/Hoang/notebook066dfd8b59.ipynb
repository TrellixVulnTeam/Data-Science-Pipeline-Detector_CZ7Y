{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import dependencies","metadata":{}},{"cell_type":"code","source":"# Download detr repos\n!git clone https://github.com/facebookresearch/detr.git ","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-16T17:21:27.760968Z","iopub.execute_input":"2021-06-16T17:21:27.761353Z","iopub.status.idle":"2021-06-16T17:21:28.454418Z","shell.execute_reply.started":"2021-06-16T17:21:27.76131Z","shell.execute_reply":"2021-06-16T17:21:28.453568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install utils\n!pip install glob2\n!pip install --upgrade pip","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:28.458481Z","iopub.execute_input":"2021-06-16T17:21:28.458782Z","iopub.status.idle":"2021-06-16T17:21:44.525301Z","shell.execute_reply.started":"2021-06-16T17:21:28.458745Z","shell.execute_reply":"2021-06-16T17:21:44.52446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# basic\nimport os\nimport numpy as np \nimport pandas as pd \nfrom tqdm.autonotebook import tqdm\n\n#computer vison module\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\n\n#pytorch\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\n\n#for K-fold cross validation\nfrom sklearn.model_selection import StratifiedKFold\n\n#Hungarian loss & bipartite matcher\nimport sys\nsys.path.append('./detr/')\n\nfrom detr.models.matcher import HungarianMatcher\nfrom detr.models.detr import SetCriterion\n\n#Data aug\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n#Glob\nfrom glob2 import glob","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-06-16T17:21:44.528865Z","iopub.execute_input":"2021-06-16T17:21:44.529134Z","iopub.status.idle":"2021-06-16T17:21:44.53676Z","shell.execute_reply.started":"2021-06-16T17:21:44.529105Z","shell.execute_reply":"2021-06-16T17:21:44.535656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data pre-processing","metadata":{}},{"cell_type":"code","source":"train_dir = '../input/global-wheat-detection/train/'\ntest_dir = '../input/global-wheat-detection/test/'\n\ntrain_annos = '../input/global-wheat-detection/train.csv'\n\n# glob to get lists of files of each dir \ntrain_fns = glob(train_dir + '*')\ntest_fns = glob(test_dir + '*')","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:44.537957Z","iopub.execute_input":"2021-06-16T17:21:44.53839Z","iopub.status.idle":"2021-06-16T17:21:44.725413Z","shell.execute_reply.started":"2021-06-16T17:21:44.538351Z","shell.execute_reply":"2021-06-16T17:21:44.724717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of train images: ', len(train_fns))\nprint('Number of test images: ', len(test_fns))","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:44.729461Z","iopub.execute_input":"2021-06-16T17:21:44.729705Z","iopub.status.idle":"2021-06-16T17:21:44.734366Z","shell.execute_reply.started":"2021-06-16T17:21:44.729681Z","shell.execute_reply":"2021-06-16T17:21:44.733467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(train_annos)\ntrain","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:44.737245Z","iopub.execute_input":"2021-06-16T17:21:44.737848Z","iopub.status.idle":"2021-06-16T17:21:45.18745Z","shell.execute_reply.started":"2021-06-16T17:21:44.737799Z","shell.execute_reply":"2021-06-16T17:21:45.186328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create dataframe with all training images\nall_train_images = pd.DataFrame([fns.split('/')[-1][:-4] for fns in train_fns])\nall_train_images.columns=['image_id']\n\n#Merge with bboxes\nall_train_images = all_train_images.merge(train, on='image_id', how='left')\n\n#Fill nan values with zero\nall_train_images['bbox'] = all_train_images.bbox.fillna('[0,0,0,0]')\n\n#Split 4 values of bbox to columns\nbbox_items = all_train_images.bbox.str.split(',', expand = True)\n\nall_train_images['x_min'] = bbox_items[0].str.strip('[ ').astype(float)\nall_train_images['y_min'] = bbox_items[1].str.strip(' ').astype(float)\nall_train_images['width'] = bbox_items[2].str.strip(' ').astype(float)\nall_train_images['height'] = bbox_items[3].str.strip('] ').astype(float)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:45.191749Z","iopub.execute_input":"2021-06-16T17:21:45.1921Z","iopub.status.idle":"2021-06-16T17:21:46.259771Z","shell.execute_reply.started":"2021-06-16T17:21:45.192063Z","shell.execute_reply":"2021-06-16T17:21:46.258832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check imgs without bboxes.\nprint('There are {} images with no bounding box!'.format(len(all_train_images)- len(train)))","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:46.261146Z","iopub.execute_input":"2021-06-16T17:21:46.261489Z","iopub.status.idle":"2021-06-16T17:21:46.268468Z","shell.execute_reply.started":"2021-06-16T17:21:46.261454Z","shell.execute_reply":"2021-06-16T17:21:46.267495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove img without bboxes.\nall_train_images = all_train_images[all_train_images.width != 0]\n\nprint('There are {} images with no bounding box!'.format(len(all_train_images)- len(train)))","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:46.270202Z","iopub.execute_input":"2021-06-16T17:21:46.270587Z","iopub.status.idle":"2021-06-16T17:21:46.317112Z","shell.execute_reply.started":"2021-06-16T17:21:46.27055Z","shell.execute_reply":"2021-06-16T17:21:46.316187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This code is used for connecting boxes with id, we partly get from UIT LOGO baseline. \ndef get_all_bboxes(df, image_id):\n    image_bboxes = df[df.image_id == image_id]   \n    bboxes = []\n    for _,row in image_bboxes.iterrows():\n        bboxes.append((row.x_min, row.y_min, row.width, row.height))\n        \n    return bboxes\n\ndef plot_image_examples(df, rows=3, cols=3):\n    fig, axs = plt.subplots(rows, cols, figsize=(25,20))\n    for row in range(rows):\n        for col in range(cols):\n            idx = np.random.randint(len(df), size=1)[0]\n            img_id = df.iloc[idx].image_id\n            \n            img = Image.open(train_dir + img_id + '.jpg')\n            axs[row, col].imshow(img)\n            \n            bboxes = get_all_bboxes(df, img_id)\n            \n            for bbox in bboxes:\n                rect = patches.Rectangle((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none')\n                axs[row, col].add_patch(rect)\n            \n            axs[row, col].axis('off')","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:46.318388Z","iopub.execute_input":"2021-06-16T17:21:46.318745Z","iopub.status.idle":"2021-06-16T17:21:46.328012Z","shell.execute_reply.started":"2021-06-16T17:21:46.31871Z","shell.execute_reply":"2021-06-16T17:21:46.32681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample images with bounding box.\nplot_image_examples(all_train_images)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:46.32966Z","iopub.execute_input":"2021-06-16T17:21:46.330009Z","iopub.status.idle":"2021-06-16T17:21:49.035597Z","shell.execute_reply.started":"2021-06-16T17:21:46.329976Z","shell.execute_reply":"2021-06-16T17:21:49.034573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop old bbox column\nall_train_images.drop(columns = ['bbox'], inplace=True)\n\n# Count bbox with condition\nall_train_images['count'] = all_train_images.apply(lambda row: 1 if np.isfinite(row.width) else 0, axis=1)\n\nall_train_images","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:49.036883Z","iopub.execute_input":"2021-06-16T17:21:49.037245Z","iopub.status.idle":"2021-06-16T17:21:51.57856Z","shell.execute_reply.started":"2021-06-16T17:21:49.037204Z","shell.execute_reply":"2021-06-16T17:21:51.577692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Fold","metadata":{}},{"cell_type":"code","source":"# Config\nn_folds = 4\nseed = 42\nnum_classes = 2 #(wheat and no-obj class)\nnum_queries = 100\nnull_class_coef = 0.5\nBATCH_SIZE = 8\nLR = 1e-3\nEPOCHS = 14","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:51.580404Z","iopub.execute_input":"2021-06-16T17:21:51.580832Z","iopub.status.idle":"2021-06-16T17:21:51.585737Z","shell.execute_reply.started":"2021-06-16T17:21:51.580793Z","shell.execute_reply":"2021-06-16T17:21:51.584914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"[1]\"\"\"\n\nskf = StratifiedKFold(n_splits= n_folds, shuffle=True, random_state=42)\n\ndf_folds = all_train_images[['image_id', 'count']].copy()\n\n#Gather all boxes of each img \ndf_folds = df_folds.groupby('image_id').count()\n\n#Get source for each img, we stratify to 4 folds.\ndf_folds.loc[:, 'source'] = all_train_images[['image_id', 'source']].groupby('image_id').min()['source']\ndf_folds.loc[:, 'stratify_group'] = np.char.add(\n    df_folds['source'].values.astype(str),\n    df_folds['count'].apply(lambda x: f'_{x // 15}').values.astype(str)\n    )\n\ndf_folds.loc[:, 'fold'] = 0\n\nfor fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:51.586951Z","iopub.execute_input":"2021-06-16T17:21:51.587481Z","iopub.status.idle":"2021-06-16T17:21:52.011426Z","shell.execute_reply.started":"2021-06-16T17:21:51.587422Z","shell.execute_reply":"2021-06-16T17:21:52.010491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_folds","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:52.012822Z","iopub.execute_input":"2021-06-16T17:21:52.013422Z","iopub.status.idle":"2021-06-16T17:21:52.028545Z","shell.execute_reply.started":"2021-06-16T17:21:52.013362Z","shell.execute_reply":"2021-06-16T17:21:52.027586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create dataset object","metadata":{}},{"cell_type":"code","source":"\"\"\"[2]\"\"\"\nclass WheatDataset(Dataset):\n    \n    def __init__(self,image_ids,dataframe,transforms=None):\n        self.image_ids = image_ids\n        self.df = dataframe        \n        \n        #Data augmentation \n        self.transforms = transforms\n        \n        \n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n    \n    def __getitem__(self,index):\n        image_id = self.image_ids[index]\n        records = self.df[self.df['image_id'] == image_id]\n        \n        image = cv2.imread(f'{train_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        \n        # DETR takes in data in coco format \n        boxes = records[['x_min', 'y_min', 'width', 'height']].values\n        \n        #Area of bb\n        area = boxes[:,2]*boxes[:,3]\n        area = torch.as_tensor(area, dtype=torch.float32)\n        \n        labels =  np.zeros(len(boxes), dtype=np.int32)\n        \n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': boxes,\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            boxes = sample['bboxes']\n            labels = sample['labels']            \n            \n        #Normalizing BBOXES            \n        _,h,w = image.shape\n        boxes = A.augmentations.bbox_utils.normalize_bboxes(sample['bboxes'],rows=h,cols=w)\n        target = {}\n        target['boxes'] = torch.as_tensor(boxes,dtype=torch.float32)\n        target['labels'] = torch.as_tensor(labels,dtype=torch.long)\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        \n        return image, target, image_id","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:52.030028Z","iopub.execute_input":"2021-06-16T17:21:52.03036Z","iopub.status.idle":"2021-06-16T17:21:52.041792Z","shell.execute_reply.started":"2021-06-16T17:21:52.030327Z","shell.execute_reply":"2021-06-16T17:21:52.040798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create DETR object","metadata":{}},{"cell_type":"code","source":"\"\"\"[3]\"\"\"\nclass DETRModel(nn.Module):\n    def __init__(self,num_classes,num_queries):\n        super(DETRModel,self).__init__()\n        self.num_classes = num_classes\n        self.num_queries = num_queries\n        \n        #Download pretrained DETR model with backbone resnet101 feature extractor.\n        self.model = torch.hub.load('facebookresearch/detr', 'detr_resnet101', pretrained=True)\n        self.in_features = self.model.class_embed.in_features\n        \n        self.model.class_embed = nn.Linear(in_features=self.in_features,out_features=self.num_classes)\n        self.model.num_queries = self.num_queries\n        \n    def forward(self,images):\n        return self.model(images)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:52.043408Z","iopub.execute_input":"2021-06-16T17:21:52.04383Z","iopub.status.idle":"2021-06-16T17:21:52.055396Z","shell.execute_reply.started":"2021-06-16T17:21:52.043795Z","shell.execute_reply":"2021-06-16T17:21:52.054614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pretrained DETR with frozen backbone.\nclass frozen_DETR(nn.Module):\n    def __init__(self,num_classes, num_queries):\n        super(frozen_DETR, self).__init__()\n        self.num_classes = num_classes\n        self.num_queries = num_queries\n        \n        #Download pretrained DETR model with backbone resnet101 feature extractor.\n        self.model = torch.hub.load('facebookresearch/detr', 'detr_resnet101', pretrained=True)\n        \n        for p in self.model.backbone[0].body.layer1.parameters():\n            p.requires_grad = False  \n            \n        for p in self.model.backbone[0].body.layer2.parameters():\n            p.requires_grad = False  \n        \n        for p in self.model.backbone[0].body.layer3[:18].parameters():\n            p.requires_grad = False\n            \n        \"\"\"for p in self.model.transformer.encoder.layers.parameters():\n            p.requires_grad = False\n            \n        for p in self.model.transformer.decoder.layers.parameters():\n            p.requires_grad = False\"\"\"\n        \n        self.in_features = self.model.class_embed.in_features\n        \n        self.model.class_embed = nn.Linear(in_features=self.in_features,out_features=self.num_classes)\n        self.model.num_queries = self.num_queries\n        \n        \n    def forward(self,images):\n        return self.model(images)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:52.057499Z","iopub.execute_input":"2021-06-16T17:21:52.057727Z","iopub.status.idle":"2021-06-16T17:21:52.069156Z","shell.execute_reply.started":"2021-06-16T17:21:52.057705Z","shell.execute_reply":"2021-06-16T17:21:52.068479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = frozen_DETR(2, 100)\nprint('Total parameters of DETR: ', sum(p.numel() for p in model.parameters()))\nprint('We now fine-tune Transformer by freezing the backbone!')\nprint('Total of params of stack of 6 transformers FCN for clases and boxes.',\\\n      sum(p.numel() for p in model.parameters() if p.requires_grad))","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:52.070397Z","iopub.execute_input":"2021-06-16T17:21:52.070843Z","iopub.status.idle":"2021-06-16T17:21:53.611384Z","shell.execute_reply.started":"2021-06-16T17:21:52.070807Z","shell.execute_reply":"2021-06-16T17:21:53.60954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create matcher for loss calculation and set \"lambda\" params\n# loss_ce: classification loss (Hungarian matching loss)\n# loss_bbox: bbox loss\n# giou: giou loss\n\nmatcher = HungarianMatcher()\n\nweight_dict = {'loss_ce': 1, 'loss_bbox': 1 , 'loss_giou': 1}\n\nlosses = ['labels', 'boxes', 'cardinality']","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:53.613094Z","iopub.execute_input":"2021-06-16T17:21:53.613472Z","iopub.status.idle":"2021-06-16T17:21:53.618568Z","shell.execute_reply.started":"2021-06-16T17:21:53.613415Z","shell.execute_reply":"2021-06-16T17:21:53.617247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation","metadata":{}},{"cell_type":"code","source":"\"\"\"[4]\"\"\"\ndef get_train_transforms():\n    return A.Compose([A.OneOf([A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, val_shift_limit=0.2, p=0.9),                               \n                      A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.9)],p=0.9),                      \n                      A.ToGray(p=0.01),                      \n                      A.HorizontalFlip(p=0.5),                      \n                      A.VerticalFlip(p=0.5),                      \n                      A.Resize(height=512, width=512, p=1),                      \n                      A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),                      \n                      ToTensorV2(p=1.0)],                      \n                      p=1.0,                     \n                      bbox_params=A.BboxParams(format='coco',min_area=0, min_visibility=0,label_fields=['labels'])\n                      )\n\n\ndef get_valid_transforms():\n    return A.Compose([A.Resize(height=512, width=512, p=1.0),\n                      ToTensorV2(p=1.0)], \n                      p=1.0, \n                      bbox_params=A.BboxParams(format='coco',min_area=0, min_visibility=0,label_fields=['labels'])\n                      )","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:53.619908Z","iopub.execute_input":"2021-06-16T17:21:53.62024Z","iopub.status.idle":"2021-06-16T17:21:53.630536Z","shell.execute_reply.started":"2021-06-16T17:21:53.620207Z","shell.execute_reply":"2021-06-16T17:21:53.629655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss calculation object","metadata":{}},{"cell_type":"code","source":"# To compute and store avg and current value.\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:53.632144Z","iopub.execute_input":"2021-06-16T17:21:53.63272Z","iopub.status.idle":"2021-06-16T17:21:53.639669Z","shell.execute_reply.started":"2021-06-16T17:21:53.632679Z","shell.execute_reply":"2021-06-16T17:21:53.638812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(data_loader,model,criterion,optimizer,device,scheduler,epoch):\n    \n    model.train()\n    criterion.train()\n    \n    summary_loss = AverageMeter()\n    \n    with tqdm(data_loader, total=len(data_loader)) as tk0:\n    \n        for step, (images, targets, image_ids) in enumerate(tk0):\n        \n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n\n            output = model(images)\n        \n            loss_dict = criterion(output, targets)\n            weight_dict = criterion.weight_dict\n        \n            losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n        \n            optimizer.zero_grad()\n\n            losses.backward()\n            optimizer.step()\n            \n            if scheduler is not None:\n                scheduler.step()\n        \n            summary_loss.update(losses.item(),BATCH_SIZE)\n            tk0.set_postfix(loss=summary_loss.avg)\n        \n    return summary_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:53.641046Z","iopub.execute_input":"2021-06-16T17:21:53.641563Z","iopub.status.idle":"2021-06-16T17:21:53.653748Z","shell.execute_reply.started":"2021-06-16T17:21:53.641524Z","shell.execute_reply":"2021-06-16T17:21:53.652887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_fn(data_loader, model,criterion, device):\n    \n    model.eval()\n    criterion.eval()\n    \n    summary_loss = AverageMeter()\n    \n    # Disabled gradient calculation over the weights\n    with torch.no_grad():\n        \n        tk0 = tqdm(data_loader, total=len(data_loader))\n        for step, (images, targets, image_ids) in enumerate(tk0):\n            \n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            output = model(images)\n        \n            loss_dict = criterion(output, targets)\n            weight_dict = criterion.weight_dict\n        \n            losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n            \n            summary_loss.update(losses.item(),BATCH_SIZE)\n            tk0.set_postfix(loss=summary_loss.avg)\n    \n    return summary_loss","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:53.654767Z","iopub.execute_input":"2021-06-16T17:21:53.655233Z","iopub.status.idle":"2021-06-16T17:21:53.665004Z","shell.execute_reply.started":"2021-06-16T17:21:53.655197Z","shell.execute_reply":"2021-06-16T17:21:53.664065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:53.666509Z","iopub.execute_input":"2021-06-16T17:21:53.666946Z","iopub.status.idle":"2021-06-16T17:21:53.676881Z","shell.execute_reply.started":"2021-06-16T17:21:53.666911Z","shell.execute_reply":"2021-06-16T17:21:53.676052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"[5]\"\"\"\ndef get_data_loader(fold):    \n    df_train = df_folds[df_folds['fold'] != fold]\n    df_valid = df_folds[df_folds['fold'] == fold]\n\n    train_dataset = WheatDataset(\n        image_ids=df_train.index.values,\n        dataframe=all_train_images,\n        transforms=get_train_transforms()\n        )\n\n    train_data_loader = DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=4,\n        collate_fn=collate_fn\n        )\n\n    valid_dataset = WheatDataset(\n        image_ids=df_valid.index.values,\n        dataframe=all_train_images,\n        transforms=get_valid_transforms()\n        )\n\n    valid_data_loader = DataLoader(\n        valid_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=4,\n        collate_fn=collate_fn\n        )\n    \n    return train_data_loader, valid_data_loader","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:53.679042Z","iopub.execute_input":"2021-06-16T17:21:53.679275Z","iopub.status.idle":"2021-06-16T17:21:53.688478Z","shell.execute_reply.started":"2021-06-16T17:21:53.679253Z","shell.execute_reply":"2021-06-16T17:21:53.687685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trainnnn(train_data_loader, valid_data_loader, device, model, criterion, optimizer, fold, best_loss, index):\n    for epoch in range(EPOCHS):\n        train_loss = train_fn(train_data_loader, model,criterion, optimizer,device,scheduler=None,epoch=epoch)\n        valid_loss = eval_fn(valid_data_loader, model,criterion, device)\n        \n        print('|EPOCH {}| TRAIN_LOSS {}| VALID_LOSS {}|'.format(epoch+1,train_loss.avg,valid_loss.avg))\n        \n        if valid_loss.avg < best_loss:\n            best_loss = valid_loss.avg\n            print('Best model found for Fold {} in Epoch {}'.format(fold,epoch+1))\n            # torch.save(model.state_dict(), f'THE_BEST_{index}_fold_{fold}.pth')\n        \n        index+=1\n    torch.save(model.state_dict(), f'THE_BEST_MODEL.pth')","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:53.68958Z","iopub.execute_input":"2021-06-16T17:21:53.690095Z","iopub.status.idle":"2021-06-16T17:21:53.699132Z","shell.execute_reply.started":"2021-06-16T17:21:53.69006Z","shell.execute_reply":"2021-06-16T17:21:53.69845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"For training the whole Pretrained DETR\"\"\"\n#Set device, load model and set criteron\n# device = torch.device('cuda')\n\n# model = DETRModel(num_classes=num_classes, num_queries=num_queries)\n# model = model.to(device)\n\n# criterion = SetCriterion(num_classes-1, matcher, weight_dict, eos_coef = null_class_coef, losses=losses)\n# criterion = criterion.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:53.702198Z","iopub.execute_input":"2021-06-16T17:21:53.702501Z","iopub.status.idle":"2021-06-16T17:21:53.713617Z","shell.execute_reply.started":"2021-06-16T17:21:53.702469Z","shell.execute_reply":"2021-06-16T17:21:53.71246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"[OR] For training Pretrained DETR with frozen transformer\"\"\"\n#Set device, load model and set criteron\ndevice = torch.device('cuda')\n\nmodel = frozen_DETR(num_classes=num_classes, num_queries=num_queries)\nprint('Total parameters of DETR: ', sum(p.numel() for p in model.parameters()))\nprint('We now fine-tune Transformer by freezing the backbone!')\nprint('Total of params of stack of 6 transformers FCN for clases and boxes.', sum(p.numel() for p in model.parameters() if p.requires_grad))\nmodel = model.to(device)\n\ncriterion = SetCriterion(num_classes-1, matcher, weight_dict, eos_coef = null_class_coef, losses=losses)\ncriterion = criterion.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:53.714881Z","iopub.execute_input":"2021-06-16T17:21:53.715197Z","iopub.status.idle":"2021-06-16T17:21:59.513423Z","shell.execute_reply.started":"2021-06-16T17:21:53.715174Z","shell.execute_reply":"2021-06-16T17:21:59.512421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_loss = 10**5\n    \n#AdamW optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n\n#Train with 4-fold cross validation, 20 epochs.\nfor fold in range(n_folds):\n    train_data_loader, valid_data_loader = get_data_loader(fold=fold)\n    trainnnn(train_data_loader, valid_data_loader, device, model, criterion, optimizer, fold, best_loss, index)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:21:59.514706Z","iopub.execute_input":"2021-06-16T17:21:59.515028Z","iopub.status.idle":"2021-06-16T17:26:54.908746Z","shell.execute_reply.started":"2021-06-16T17:21:59.514987Z","shell.execute_reply":"2021-06-16T17:26:54.90597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Recall and test model ","metadata":{}},{"cell_type":"code","source":"#Recall model\ndevice = torch.device(\"cuda\")\n\nmodel = frozen_DETR(num_classes = num_classes, num_queries=num_queries)\nmodel.load_state_dict(torch.load('./THE_BEST_MODEL.pth'))\nmodel = model.to(device)\n\ncriterion = SetCriterion(num_classes-1, matcher, weight_dict, eos_coef = null_class_coef, losses=losses)\ncriterion = criterion.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:26:54.909731Z","iopub.status.idle":"2021-06-16T17:26:54.910122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create test dataset object\nTo parse annotation of test images","metadata":{}},{"cell_type":"code","source":"class WheatTestDataset(Dataset):\n    \n    def __init__(self, image_dir, dataframe, transforms=None):\n        super().__init__()\n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n    \n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        records = self.df[self.df['image_id'] == image_id]\n\n        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        \n        boxes = records[['xmin', 'ymin', 'width', 'height']].values\n\n        if self.transforms:\n            sample = {\n                'image': image,\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n\n        return image, image_id, boxes\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:26:54.911691Z","iopub.status.idle":"2021-06-16T17:26:54.912366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_transforms():\n    return A.Compose([\n        A.Resize(512, 512),\n        ToTensorV2(p=1.0)\n    ])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:26:54.913748Z","iopub.status.idle":"2021-06-16T17:26:54.914435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test annotation processing","metadata":{}},{"cell_type":"code","source":"test_annos = pd.read_csv('../input/test-annotations/_annotations.csv')\ntest_annos","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:26:54.915709Z","iopub.status.idle":"2021-06-16T17:26:54.916391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq = '_jpg.rf.'\ntest_annos['filename'] = [name.split(seq, 1)[0] for name in test_annos['filename']]\n\ntest_annos = test_annos.rename(columns = {'filename': 'image_id'})\n\ntest_annos.drop(columns = ['class'], inplace=True)\ntest_annos.drop(columns = ['width'], inplace=True)\ntest_annos.drop(columns = ['height'], inplace=True)\n\ntest_annos['width'] = test_annos['xmax'] - test_annos['xmin']\ntest_annos['height'] = test_annos['ymax'] - test_annos['ymin']\n\ntest_annos.drop(columns = ['xmax'], inplace=True)\ntest_annos.drop(columns = ['ymax'], inplace=True)\n\ntest_annos","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:26:54.917482Z","iopub.status.idle":"2021-06-16T17:26:54.918226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get test dataframe \ntest_df = test_annos\n\ntest_dataset = WheatTestDataset(image_dir = test_dir,\n                                dataframe = test_df,  \n                                transforms = get_test_transforms())\n\ntest_data_loader = DataLoader(test_dataset,\n                              batch_size=10,\n                              shuffle=False,\n                              num_workers=4,\n                              drop_last=False,\n                              collate_fn=collate_fn\n                             )","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:26:54.919385Z","iopub.status.idle":"2021-06-16T17:26:54.920524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"[6]\"\"\"\nconfidence_threshold = 0.6\n\ndef predict_and_view(test_data_loader, model, confidence_threshold): \n    \n    images, image_ids, all_test_boxes = next(iter(test_data_loader))\n    \n    _, h, w = images[0].shape # for DE norm boxes\n    \n    images = list(image.to(device) for image in images)\n    \n    all_pred_boxes = []\n    \n    # Predict \n    with torch.no_grad():\n        outputs = model(images)   \n    \n    outputs = [{k: v.to(device) for k, v in outputs.items()}]    \n        \n    for i, image in enumerate(images):\n        \n        # For plotting test annotation boxes. \n        test_boxes = all_test_boxes[i]\n        \n        sample = image.permute(1,2,0).cpu().numpy()\n        \n        fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n        \n        for box in test_boxes:\n            color = (220, 0, 0)\n            cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2]+box[0], box[3]+box[1]),\n                  color, 1)\n            # print(box[0], box[1], box[2],box[3])\n        \n        # For plotting predicted boxes.\n        pred_boxes = outputs[0]['pred_boxes'][i].detach().cpu().numpy()\n        pred_boxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(pred_boxes, h, w)] # DE norm\n        prob  = outputs[0]['pred_logits'][i].softmax(1).detach().cpu().numpy()[:,0]\n    \n        \n        list_pred_boxes = np.empty(shape=[4, 1])\n        \n        for box, p in zip(pred_boxes, prob):        \n            if p > confidence_threshold:\n                color = (0,0,220) \n                cv2.rectangle(sample,\n                      (box[0], box[1]),\n                      (box[2]+box[0], box[3]+box[1]),\n                      color, 1)\n                \n                # For eval \n                box[2] = box[2]+box[0]\n                box[3] = box[3]+box[1]\n                \n                box = box.reshape(4,1)\n                list_pred_boxes = np.concatenate((list_pred_boxes,box),axis=1)\n                \n        all_pred_boxes.append(list_pred_boxes)\n        \n        ax.set_axis_off()\n        ax.imshow(sample)\n        \n    return all_pred_boxes, all_test_boxes","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:26:54.921746Z","iopub.status.idle":"2021-06-16T17:26:54.922431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np_all_pred_boxes, np_all_test_boxes = predict_and_view(test_data_loader, model, confidence_threshold=confidence_threshold)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:26:54.923602Z","iopub.status.idle":"2021-06-16T17:26:54.924286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"all_pred_boxes = []\nall_test_boxes = []\n\nfor pred_boxes in np_all_pred_boxes:\n    # Transpose the matrix of boxes\n    pred_boxes = pred_boxes.transpose()\n    # Convert to torch tensor\n    pred_boxes = torch.from_numpy(pred_boxes)\n    all_pred_boxes.append(pred_boxes)\n    \nfor img_test_boxes in np_all_test_boxes:\n    \n    for box in img_test_boxes:\n        box[2] = box[2]+box[0]\n        box[3] = box[3]+box[1]\n        \n    # Convert to torch tensor\n    img_test_boxes = torch.from_numpy(img_test_boxes)\n    all_test_boxes.append(img_test_boxes)  ","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:26:54.925541Z","iopub.status.idle":"2021-06-16T17:26:54.926331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    print(all_pred_boxes[i].shape, ' -- ', all_test_boxes[i].shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:26:54.927594Z","iopub.status.idle":"2021-06-16T17:26:54.928375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_intersection(box, test_boxes):\n    \"\"\"Find the intersection of every box combination between two sets of boxes that are in boundary coordinates.\n    \"\"\"\n    # PyTorch auto-broadcasts singleton dimensions\n    lower_bounds = torch.max(box[:, :2].unsqueeze(1), test_boxes[:, :2].unsqueeze(0))  \n    upper_bounds = torch.min(box[:, 2:].unsqueeze(1), test_boxes[:, 2:].unsqueeze(0))  \n    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  \n    \n    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  \n\n\ndef find_iou(box, test_boxes):\n    \"\"\"Find IoU overlap of every box combination between two sets of boxes that are in boundary coordinates.\n    \"\"\"        \n    # Find intersections\n    intersection = find_intersection(box, test_boxes)  \n\n    # Find areas of each box in both sets\n    areas_box = (box[:, 2] - box[:, 0]) * (box[:, 3] - box[:, 1])  \n    areas_test_boxes = (test_boxes[:, 2] - test_boxes[:, 0]) * (test_boxes[:, 3] - test_boxes[:, 1]) \n\n    # Find the union\n    union = areas_box.unsqueeze(1) + areas_test_boxes.unsqueeze(0) - intersection  \n\n    return intersection / union ","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:26:54.929709Z","iopub.status.idle":"2021-06-16T17:26:54.930536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def AP(test_boxes, pred_boxes):\n    \"\"\"Calculate AP score of the prediction.\n    \"\"\"\n    average_precisions = torch.zeros((0), dtype=torch.float)\n    # All considered boxes\n    consi_boxes = 0\n    if test_boxes.size(0) > pred_boxes.size(0):\n        consi_boxes = test_boxes.size(0)\n    else:\n        consi_boxes = pred_boxes.size(0)\n        \n    true_pos = torch.zeros((consi_boxes), dtype=torch.float).to(device) \n    false_pos = torch.zeros((consi_boxes), dtype=torch.float).to(device)  \n    \n\n    for box in test_boxes:\n        this_box = box.unsqueeze(0)\n        overlaps = find_iou(this_box, pred_boxes)\n        max_overlap, idx = torch.max(overlaps.squeeze(0), dim=0)\n        \n        # Overlap < 0.3 is not acceptable \n        if max_overlap.item() < 0.3:\n            continue\n        \n        # Need it to pop detected box\n        og_id = torch.LongTensor(range(pred_boxes.size(0)))[idx]\n        pred_boxes = torch.cat([pred_boxes[0:og_id], pred_boxes[og_id+1:]])\n        \n        if max_overlap.item() >= 0.5:\n            true_pos[idx] = 1\n            \n        else:\n            false_pos[idx] = 1\n        \n        if pred_boxes.size(0) == 0:\n            break\n        \n    \n    sum_true_pos = torch.cumsum(true_pos, dim=0)\n    sum_false_pos = torch.cumsum(false_pos, dim=0)\n    \n    \n    # High precision means the accuracy of the bboxes found is high. \n    arr_precision = sum_true_pos / (\n                sum_true_pos + sum_false_pos + 1e-10)\n    # High recall means the rate of omitting the ground truth positive bboxes is low.\n    arr_recall = sum_true_pos/consi_boxes\n    \n    \n    # Order Precison and Recall like the ROC curve\n    # Find the mean of the maximum of the precisions corresponding to recalls above the threshold 't'\n    recall_thresholds = torch.arange(start=0, end=1.1, step=.1).tolist() # 0.0 ---> 1.0\n    precisions = torch.zeros((len(recall_thresholds)), dtype=torch.float).to(device)\n    \n    for i, t in enumerate(recall_thresholds):\n        recalls_above_t = arr_recall >= t\n        if recalls_above_t.any():\n            precisions[i] = arr_precision[recalls_above_t].max()\n        else:\n            precision[i] = 0.0\n            \n        average_precision = precisions.mean().item()\n            \n        \n        \n    return arr_precision, arr_recall, average_precision","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:26:54.931897Z","iopub.status.idle":"2021-06-16T17:26:54.932684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display results and find mAP\nAPs = 0\nmAP = 0\n\n\nfor i in range(10):    \n    precision = torch.zeros((24), dtype=torch.float)\n    recall = torch.zeros((24), dtype=torch.float)\n    \n    precision, recall, average_precision = AP(all_pred_boxes[i], all_test_boxes[i])\n    num_predict, num_test = all_pred_boxes[i].size(0), all_test_boxes[i].size(0)\n    \n    APs += average_precision\n    mAP = APs/10\n    \n    print('Image {}:'.format(i+1), '\\n', 'Number of Predicted boxes:', num_predict, end='\\n')\n    print(' Number of Ground truth boxes:', num_test, end='\\n')\n    \n    # print('Precision score: ',precision, end = '\\n')       \n    # print('Rcall score: ', recall, end = '\\n')\n    \n    \n    print('AP score: ',average_precision, end = '\\n')\n    print('------------------------------------', end ='\\n')\n    \nprint('mAP score: ',mAP, end = '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:26:54.93389Z","iopub.status.idle":"2021-06-16T17:26:54.934541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T17:26:54.935804Z","iopub.status.idle":"2021-06-16T17:26:54.936476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\n[1] https://www.kaggle.com/shonenkov/training-efficientdet\n\n[2],\n[3],\n[4],\n[5]\nhttps://www.kaggle.com/tanulsingh077/end-to-end-object-detection-with-transformers-detr\n\n[6] https://www.kaggle.com/pestipeti/pytorch-starter-fasterrcnn-train\n","metadata":{}}]}