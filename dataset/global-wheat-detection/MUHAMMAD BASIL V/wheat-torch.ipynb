{"cells":[{"metadata":{"_uuid":"df8c2437-4682-4fb0-b702-3219397d5ee8","_cell_guid":"2b78b7c4-0253-421c-82d6-02e45be7bf79","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6784a599-bc4f-4548-b7fc-aeadf3030199","_cell_guid":"8dc96b28-2e24-48c5-ab72-de3760cc836a","trusted":true},"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f15fa261-cb5b-4713-8af7-4855921cd8b6","_cell_guid":"0a3d1ed4-f43b-410f-a233-4a10514b5643","trusted":true},"cell_type":"code","source":"class Lambda(nn.Module):\n    def __init__(self, func):\n        super().__init__()\n        self.func = func\n\n    def forward(self, x):\n        return self.func(x)\n\n\ndef preprocess(x):\n    return x.view(-1, 3, 256, 256) \n\ndef flatten(x):\n  size=x.size()[1:]\n  num_features=1\n  for s in size:\n    num_features*=s\n  return x.view(-1,num_features)\n\ndef reshape(x):\n  return x.view(16,16,5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d91f01e6-a04b-40f5-bb92-96f5b981b5b6","_cell_guid":"8a32a6e3-4029-4aab-a671-f538692e453b","trusted":true},"cell_type":"code","source":"yolo_model=nn.Sequential(\n    Lambda(preprocess),\n    #layer 1\n    nn.Conv2d(3,32,3,padding=1),\n    nn.BatchNorm2d(32),\n    nn.LeakyReLU(negative_slope=0.1),\n    nn.MaxPool2d(2),\n    #layer 2\n    nn.Conv2d(32,32,3,padding=1),\n    nn.BatchNorm2d(32),\n    nn.LeakyReLU(negative_slope=0.1),\n    #layer 3\n    nn.Conv2d(32,64,3,padding=1),\n    nn.BatchNorm2d(64),\n    nn.LeakyReLU(negative_slope=0.1),\n    nn.MaxPool2d(2),\n    #layer 4\n    nn.Conv2d(64,128,3,padding=1),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(negative_slope=0.1),\n\n    #layer 5\n    nn.Conv2d(128,128,1,padding=0),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(negative_slope=0.1),\n\n    #layer 6\n    nn.Conv2d(128,256,3,padding=1),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(negative_slope=0.1),\n\n    #layer 7\n    nn.Conv2d(256,256,1,padding=0),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(negative_slope=0.1),\n\n    #layer 8\n    nn.Conv2d(256,512,3,padding=1),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(negative_slope=0.1),\n    nn.MaxPool2d(2),\n\n    #layer 9\n    nn.Conv2d(512,256,1,padding=0),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(negative_slope=0.1),\n\n    #layer 10\n    nn.Conv2d(256,512,3,padding=1),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(negative_slope=0.1),\n\n    #layer 11\n    nn.Conv2d(512,256,1,padding=0),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(negative_slope=0.1),\n    \n    #layer 12\n    nn.Conv2d(256,512,3,padding=1),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(negative_slope=0.1),\n\n    #layer 13\n    nn.Conv2d(512,256,1,padding=0),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(negative_slope=0.1),\n\n    #layer 14\n    nn.Conv2d(256,512,3,padding=1),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(negative_slope=0.1),\n\n    #layer 15\n    nn.Conv2d(512,256,1,padding=0),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(negative_slope=0.1),\n\n    #layer 16\n    nn.Conv2d(256,512,3,padding=1),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(negative_slope=0.1),\n\n    #layer 17\n    nn.Conv2d(512,512,1,padding=0),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(negative_slope=0.1),\n\n    #layer 18\n    nn.Conv2d(512,1024,3,padding=1),\n    nn.BatchNorm2d(1024),\n    nn.LeakyReLU(negative_slope=0.1),\n    nn.MaxPool2d(2),\n\n    #layer 19\n    nn.Conv2d(1024,512,1,padding=0),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(negative_slope=0.1),\n\n    #layer 20\n    nn.Conv2d(512,1024,3,padding=1),\n    nn.BatchNorm2d(1024),\n    nn.LeakyReLU(negative_slope=0.1),\n\n    #layer 21\n    nn.Conv2d(1024,512,1,padding=0),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(negative_slope=0.1),\n\n    #layer 22\n    nn.Conv2d(512,1024,3,padding=1),\n    nn.BatchNorm2d(1024),\n    nn.LeakyReLU(negative_slope=0.1),\n\n    #layer 23\n\n    nn.Conv2d(1024,1024,3,padding=1),\n    nn.BatchNorm2d(1024),\n    nn.LeakyReLU(negative_slope=0.1),\n    nn.MaxPool2d(2),\n\n    #layer 24\n    nn.Conv2d(1024,1024,3,padding=1),\n    nn.BatchNorm2d(1024),\n    nn.LeakyReLU(negative_slope=0.1),\n\n    #layer 25\n    nn.Conv2d(1024,1024,3,padding=1),\n    nn.BatchNorm2d(1024),\n    nn.LeakyReLU(negative_slope=0.1),\n\n    #layer 26\n    nn.Conv2d(1024,128,3,padding=1),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(negative_slope=0.1),\n\n\n    Lambda(flatten),\n\n    #fc\n    nn.Linear(8*8*128,1280),\n\n    Lambda(reshape)\n\n\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed94a354-bfd0-4eae-892f-b6c25d3fc7ce","_cell_guid":"b5a26542-52d2-4448-af62-d49d36341bb2","trusted":true},"cell_type":"code","source":"def yolo_loss(y_true,y_pred):\n  lossx=torch.sum(torch.square((y_true[:,:,0:1]-y_pred[:,:,0:1]))*y_true[:,:,4:])\n  lossy=torch.sum(torch.square((y_true[:,:,1:2]-y_pred[:,:,1:2]))*y_true[:,:,4:])\n  loss1=torch.add(lossx,lossy)\n  lossw=torch.sum(torch.square((y_true[:,:,2:3]-y_pred[:,:,2:3]))*y_true[:,:,4:])\n  lossh=torch.sum((torch.square((y_true[:,:,3:4])-(y_pred[:,:,3:4])))*y_true[:,:,4:])\n  loss2=torch.add(lossw,lossh)\n  loss_xy_wh=torch.add(loss1,loss2)\n  lossC=torch.sum(torch.square((y_true[:,:,4:]-y_pred[:,:,4:]))*y_true[:,:,4:])\n  lossC+=torch.sum(torch.square((y_true[:,:,4:]-y_pred[:,:,4:]))*(1-y_true[:,:,4:]))/16  \n  \n    \n  total_loss=torch.add(loss_xy_wh,lossC)\n  return total_loss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3aec8f3-e321-4a84-b260-26dcddeead93","_cell_guid":"f885fd90-55f7-41b7-89ab-490131a17f09","trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/global-wheat-detection/train.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8d12611-e420-454d-a7d5-8c79e0f5f16e","_cell_guid":"2c549313-e164-4462-8fd4-c8f3bd0989ec","trusted":true},"cell_type":"code","source":"def get_list(a):\n  a=list(a.strip('[').strip(']'))\n  val='0'\n  liz=[]\n  for i in a:\n    if(i!=',' and i!=' '):\n      val+=i\n    elif (i==' '):\n      liz.append(float(val))\n      val='0'  \n  liz.append(float(val))\n  return liz\n\nids=[]\nbboxes=[]\n\nfor index,id in enumerate(data['image_id']):\n  count=0\n  for i in ids:\n    if(i==id):\n      count+=1\n  if(count==0):\n    bbox_array=np.zeros((16,16,5))\n    for index2,id2 in enumerate(data['image_id']):\n      if(id2==id):\n        bboz=get_list(data['bbox'][index2])\n        w=int(bboz[2])\n        h=int(bboz[3])\n        x=int(bboz[0]+bboz[2]/2)\n        y=int(bboz[1]+bboz[3]/2)\n        box1=int(x/64)\n        box2=int(y/64)\n        bbox_array[box1,box2,:]=(x%64)/64,(y%64)/64,w/1024,h/1024,1\n    bboxes.append(bbox_array)\n    ids.append(id)\n  print(index)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1493ae9-0bd0-49b7-8b98-d85eca6317ac","_cell_guid":"8b0a0dd9-0b27-424a-99a4-cdfcda15ac14","trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\")\nmodel=yolo_model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90ec0123-751f-4c1e-bd4b-af1d87395fc5","_cell_guid":"f62aaea5-3bce-4b4a-a8b4-9dd0c3fc269c","trusted":true},"cell_type":"code","source":"bbox_tensor=torch.FloatTensor(bboxes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt=torch.optim.SGD(yolo_model.parameters(),lr=0.0001,momentum=0.9)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18cd88d9-f9ec-41bc-b64f-794acb37b51d","_cell_guid":"e4af254b-5563-4fcd-a815-070a18770996","trusted":true},"cell_type":"code","source":"train_data=[]\n#train_data=np.zeros((1,512,512,3))\nfor index,id in enumerate(ids):\n    filename=id\n    filename='../input/global-wheat-detection/train/'+id+'.jpg'\n    img=cv2.imread(filename)\n    img=cv2.resize(img,(256,256))\n    img=img/255.0\n    #img=np.reshape(img,(1,256,256,3))\n    train_data.append(img)\n    print(index)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e428a3cd-b97c-4895-8342-d12d389bd16b","_cell_guid":"6b814044-d107-491a-86b0-577d228f3743","trusted":true},"cell_type":"code","source":"\nfor epochs in range(150):\n  for i in range(3000):\n    ix=int(np.random.randint(0,3335,1))\n    pred=yolo_model(torch.FloatTensor(train_data[ix]).to(device))\n    loss=yolo_loss(bbox_tensor[ix].to(device),pred)\n    print(loss,i,epochs)\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6acadf72-3a6c-4a98-8d6b-2830d40c4585","_cell_guid":"9efa8097-8218-40c0-96ec-c1dedc4cdc86","trusted":true},"cell_type":"code","source":"nam = '../input/global-wheat-detection/train/'+ids[1001]+'.jpg'\nimg=cv2.imread(nam)\nimg=cv2.resize(img,(256,256))\ntest_img=img/255.0\ntest=yolo_model(torch.FloatTensor(test_img).to(device))\nno=0\nfor i in range(16):\n    for j in range(16):\n        if(test[i][j][4]>=0.9):\n            x=(i*64)+(test[i][j][0]*64)\n            y=(j*64)+(test[i][j][1]*64)\n            w=test[i][j][2]*1024\n            h=test[i][j][3]*1024\n            x1=(x-w/2)/4\n            y1=(y-h/2)/4\n            x2=(x+w/2)/4\n            y2=(y+h/2)/4\n            cv2.rectangle(img,(int(x1),int(y1)),(int(x2),int(y2)),(0,0,255),1)\n            no+=1\nplt.imshow(img[:,:,[2,1,0]])  \nfig=plt.gcf()\nfig.set_size_inches((12,12))\n\nprint(no)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6351df43-849c-4789-95da-4a4407d3add4","_cell_guid":"8b3a26b4-b031-4841-bd9c-6f1a402e00cd","trusted":true},"cell_type":"code","source":"test_list=['../input/global-wheat-detection/test/2fd875eaa.jpg','../input/global-wheat-detection/test/348a992bb.jpg','../input/global-wheat-detection/test/51b3e36ab.jpg','../input/global-wheat-detection/test/51f1be19e.jpg','../input/global-wheat-detection/test/53f253011.jpg','../input/global-wheat-detection/test/796707dd7.jpg','../input/global-wheat-detection/test/aac893a91.jpg','../input/global-wheat-detection/test/cb8d261a3.jpg','../input/global-wheat-detection/test/cc3532ff6.jpg','../input/global-wheat-detection/test/f5a1f0358.jpg']\ntest_imgs=[]\ntest_preds=[]\nfor i in test_list:\n    img=cv2.imread(i)\n    img=cv2.resize(img,(256,256))\n    test_img=img/255.0\n    test=yolo_model(torch.FloatTensor(test_img).to(device))\n    test_preds.append(test)\n    no=0\n    for i in range(16):\n        for j in range(16):\n            if(test[i][j][4]>=0.9):\n                x=(i*64)+(test[i][j][0]*64)\n                y=(j*64)+(test[i][j][1]*64)\n                w=test[i][j][2]*1024\n                h=test[i][j][3]*1024\n                x1=(x-w/2)/4\n                y1=(y-h/2)/4\n                x2=(x+w/2)/4\n                y2=(y+h/2)/4\n                cv2.rectangle(img,(int(x1),int(y1)),(int(x2),int(y2)),(0,0,255),1)\n                no+=1\n    test_imgs.append(img)            \n    plt.imshow(img[:,:,[2,1,0]])  \n    fig=plt.gcf()\n    fig.set_size_inches((12,12))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_preds)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}