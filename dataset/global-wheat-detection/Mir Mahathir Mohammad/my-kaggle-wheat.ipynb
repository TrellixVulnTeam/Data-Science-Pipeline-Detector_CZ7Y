{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install tensorflow-gpu==1.15\n!pip install ../input/wheatlibraries/gast-0.2.2/gast-0.2.2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/mytf115/astor-0.8.1-py2.py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/wheatlibraries/Keras_Applications-1.0.8-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/tensorflowv1/tensorboard-1.15.0-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/tensorflowv1/tensorflow_estimator-1.15.1-py2.py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/mytf115/tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/wheatlibraries/Keras-2.2.4-py2.py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp -Rf ../input/wheatprediction/WheatPrediction/yolo3 ./\n!mkdir ./predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import colorsys\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\nimport cv2\n\n\nimport numpy as np\nfrom keras import backend as K\nfrom keras.models import load_model\nfrom keras.layers import Input\n\nfrom yolo3.model import yolo_eval, yolo_body, tiny_yolo_body\nfrom yolo3.utils import image_preporcess\n\nclass YOLO(object):\n    _defaults = {\n        \"model_path\": '../input/wheatprediction/WheatPrediction/logs/trained_weights_final.h5',\n        \"anchors_path\": '../input/wheatprediction/WheatPrediction/model_data/yolo_anchors.txt',\n        \"classes_path\": '../input/wheatprediction/WheatPrediction/4_CLASS_test_classes.txt',\n        \"score\" : 0.3,\n        \"iou\" : 0.45,\n        \"model_image_size\" : (416, 416),\n        \"text_size\" : 3,\n    }\n\n    @classmethod\n    def get_defaults(cls, n):\n        if n in cls._defaults:\n            return cls._defaults[n]\n        else:\n            return \"Unrecognized attribute name '\" + n + \"'\"\n\n    def __init__(self, **kwargs):\n        self.__dict__.update(self._defaults) # set up default values\n        self.__dict__.update(kwargs) # and update with user overrides\n        self.class_names = self._get_class()\n        self.anchors = self._get_anchors()\n        self.sess = K.get_session()\n        self.boxes, self.scores, self.classes = self.generate()\n\n    def _get_class(self):\n        classes_path = os.path.expanduser(self.classes_path)\n        with open(classes_path) as f:\n            class_names = f.readlines()\n        class_names = [c.strip() for c in class_names]\n        return class_names\n\n    def _get_anchors(self):\n        anchors_path = os.path.expanduser(self.anchors_path)\n        with open(anchors_path) as f:\n            anchors = f.readline()\n        anchors = [float(x) for x in anchors.split(',')]\n        return np.array(anchors).reshape(-1, 2)\n\n    def generate(self):\n        model_path = os.path.expanduser(self.model_path)\n        assert model_path.endswith('.h5'), 'Keras model or weights must be a .h5 file.'\n\n        # Load model, or construct model and load weights.\n        num_anchors = len(self.anchors)\n        num_classes = len(self.class_names)\n        is_tiny_version = num_anchors==6 # default setting\n        try:\n            self.yolo_model = load_model(model_path, compile=False)\n        except:\n            self.yolo_model = tiny_yolo_body(Input(shape=(None,None,3)), num_anchors//2, num_classes) \\\n                if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes)\n            self.yolo_model.load_weights(self.model_path) # make sure model, anchors and classes match\n        else:\n            assert self.yolo_model.layers[-1].output_shape[-1] == \\\n                num_anchors/len(self.yolo_model.output) * (num_classes + 5), \\\n                'Mismatch between model and given anchor and class sizes'\n\n        print('{} model, anchors, and classes loaded.'.format(model_path))\n\n        # Generate colors for drawing bounding boxes.\n        hsv_tuples = [(x / len(self.class_names), 1., 1.)\n                      for x in range(len(self.class_names))]\n        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n        self.colors = list(\n            map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),\n                self.colors))\n\n        np.random.shuffle(self.colors)  # Shuffle colors to decorrelate adjacent classes.\n\n        # Generate output tensor targets for filtered bounding boxes.\n        self.input_image_shape = K.placeholder(shape=(2, ))\n        boxes, scores, classes = yolo_eval(self.yolo_model.output, self.anchors,\n                len(self.class_names), self.input_image_shape,\n                score_threshold=self.score, iou_threshold=self.iou)\n        return boxes, scores, classes\n\n    def detect_image(self, image):\n        if self.model_image_size != (None, None):\n            assert self.model_image_size[0]%32 == 0, 'Multiples of 32 required'\n            assert self.model_image_size[1]%32 == 0, 'Multiples of 32 required'\n            boxed_image = image_preporcess(np.copy(image), tuple(reversed(self.model_image_size)))\n            image_data = boxed_image\n\n        out_boxes, out_scores, out_classes = self.sess.run(\n            [self.boxes, self.scores, self.classes],\n            feed_dict={\n                self.yolo_model.input: image_data,\n                self.input_image_shape: [image.shape[0], image.shape[1]],#[image.size[1], image.size[0]],\n                K.learning_phase(): 0\n            })\n\n        #print('Found {} boxes for {}'.format(len(out_boxes), 'img'))\n\n        thickness = (image.shape[0] + image.shape[1]) // 600\n        fontScale=1\n        ObjectsList = []\n        \n        for i, c in reversed(list(enumerate(out_classes))):\n            predicted_class = self.class_names[c]\n            box = out_boxes[i]\n            score = out_scores[i]\n\n            label = '{} {:.2f}'.format(predicted_class, score)\n            #label = '{}'.format(predicted_class)\n            scores = '{:.2f}'.format(score)\n\n            top, left, bottom, right = box\n            top = max(0, np.floor(top + 0.5).astype('int32'))\n            left = max(0, np.floor(left + 0.5).astype('int32'))\n            bottom = min(image.shape[0], np.floor(bottom + 0.5).astype('int32'))\n            right = min(image.shape[1], np.floor(right + 0.5).astype('int32'))\n\n            mid_h = (bottom-top)/2+top\n            mid_v = (right-left)/2+left\n\n            # put object rectangle\n            cv2.rectangle(image, (left, top), (right, bottom), self.colors[c], thickness)\n\n            # get text size\n            (test_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, thickness/self.text_size, 1)\n\n            # put text rectangle\n            cv2.rectangle(image, (left, top), (left + test_width, top - text_height - baseline), self.colors[c], thickness=cv2.FILLED)\n\n            # put text above rectangle\n            cv2.putText(image, label, (left, top-2), cv2.FONT_HERSHEY_SIMPLEX, thickness/self.text_size, (0, 0, 0), 1)\n\n            # add everything to list\n            ObjectsList.append([top, left, bottom, right, mid_v, mid_h, label, scores])\n\n        return image, ObjectsList\n\n    def close_session(self):\n        self.sess.close()\n\n    def detect_img(self, image):\n        image = cv2.imread(image, cv2.IMREAD_COLOR)\n        original_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        original_image_color = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n        \n        r_image, ObjectsList = self.detect_image(original_image_color)\n        return r_image, ObjectsList\n\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yolo = YOLO()\n# image = '../input/global-wheat-detection/test/2fd875eaa.jpg'\n# r_image, ObjectsList = yolo.detect_img(image)\n\n# cv2.imwrite('predicted/2fd875eaa_prediction.jpg', r_image)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def imShow(path):\n#   import cv2\n#   import matplotlib.pyplot as plt\n#   %matplotlib inline\n\n#   image = cv2.imread(path)\n#   height, width = image.shape[:2]\n#   resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n\n#   fig = plt.gcf()\n#   fig.set_size_inches(18, 10)\n#   plt.axis(\"off\")\n#   plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n#   plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imShow('predicted/2fd875eaa_prediction.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#top, left, bottom, right, mid_v, mid_h, label, scores\n# ObjectsList","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.read_csv('../input/global-wheat-detection/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in submission_df.iterrows():\n    print(row['image_id'])\n    image_name = '../input/global-wheat-detection/test/'+row['image_id']+'.jpg'\n    r_image, ObjectsList = yolo.detect_img(image_name)\n#     print(ObjectsList)\n    ObjectsDictList=[]\n    for prediction in ObjectsList:\n        predictionDict = {}\n        #top, left, bottom, right, mid_v, mid_h, label, scores\n        predictionDict[\"confidence\"]=prediction[7]\n        predictionDict[\"xmin\"]=prediction[1]\n        predictionDict[\"ymin\"]=prediction[0]\n        predictionDict[\"width\"]=prediction[3]-prediction[1]\n        predictionDict[\"height\"]=prediction[2]-prediction[0]\n        ObjectsDictList.append(predictionDict)\n    \n    ObjectsDictList=sorted(ObjectsDictList, key = lambda i: i['confidence'],reverse=True)\n    predictionString=\"\"\n    \n    for ObjectsDict in ObjectsDictList:\n        predictionString+=(str(ObjectsDict['confidence'])+\" \"\n                           +str(ObjectsDict[\"xmin\"])+\" \"\n                           +str(ObjectsDict[\"ymin\"])+\" \"\n                           +str(ObjectsDict[\"width\"])+\" \"\n                           +str(ObjectsDict[\"height\"])+\" \")\n    \n    predictionString=predictionString.strip()\n    \n    if(len(predictionString)!=0):\n        submission_df.loc[index,\"PredictionString\"]=predictionString","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}