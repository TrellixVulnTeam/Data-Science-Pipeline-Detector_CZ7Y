{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"half = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, '/kaggle/input/yolov5/yolov5/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\ndevice = torch.device('cuda:0')\nmodel = torch.load('/kaggle/input/wheat-submit/best_wheat1024.pt', map_location=device)['model'].to(device).float().eval()\nif half:\n    model.half()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\nimg_paths = glob.glob('/kaggle/input/global-wheat-detection/test/*.jpg')\nprint(img_paths)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference_detector(model, img_path):\n    from utils.datasets import LoadImages\n    dataset = LoadImages(img_path, img_size=1024)\n    path, img, im0, vid_cap = next(iter(dataset))\n    img = torch.from_numpy(img).to(device)\n    img = img.half() if half else img.float()  # uint8 to fp16/32\n    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n    if img.ndimension() == 3:\n        img = img.unsqueeze(0)\n    pred = model(img, augment=True)[0]\n    from utils.utils import non_max_suppression\n    pred = non_max_suppression(pred, conf_thres=0.4, iou_thres=0.5, classes=None, agnostic=True)\n    from utils.utils import scale_coords\n    bboxes = []\n    scores = []\n    clses = []\n    for i, det in enumerate(pred):  # detections per image\n        if det is not None and len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n            for *xyxy, conf, cls in det:\n                xyxy = torch.tensor(xyxy).view(-1).numpy()\n                bboxes.append([*xyxy, conf.item()])\n    return np.array(bboxes)\n\nimport sys\nsys.path.insert(0, \"../input/weightedboxesfusion\")\nfrom ensemble_boxes import *\ndef run_wbf(boxes,scores, iou_thr=0.4, skip_box_thr=0.34, weights=None):\n    labels0 = [np.ones(len(scores[idx])) for idx in range(len(scores))]\n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels0, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    return boxes, scores, labels\n\n\ndef inference_detector_wbf(model, img_path):\n    from utils.datasets import LoadImages\n    dataset = LoadImages(img_path, img_size=640)\n    path, img, im0, vid_cap = next(iter(dataset))\n    img = torch.from_numpy(img).to(device)\n    img = img.half() if half else img.float()  # uint8 to fp16/32\n    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n    if img.ndimension() == 3:\n        img = img.unsqueeze(0)\n    pred = model(img, augment=True)[0]\n    from utils.utils import non_max_suppression\n    pred = non_max_suppression(pred, conf_thres=0.4, iou_thres=0.95, classes=None, agnostic=False)\n    from utils.utils import scale_coords\n    bboxes = []\n    scores = []\n    clses = []\n    for i, det in enumerate(pred):  # detections per image\n        if det is not None and len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n            for *xyxy, conf, cls in det:\n                xyxy = torch.tensor(xyxy).view(-1).numpy()\n                bboxes.append(xyxy)\n                scores.append(conf.item())\n    bboxes = [bboxes]\n    scores = [scores]\n    bboxes, scores, labels = run_wbf(bboxes, scores)\n    det = []\n    for bbox, score in zip(bboxes, scores):\n        det.append([*bbox, score])\n    return np.array(det)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test\nimport numpy as np\nimport cv2\ndef vis(image_path, det):\n    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n    size = 300\n    idx = -1\n    font = cv2.FONT_HERSHEY_SIMPLEX \n    # fontScale \n    fontScale = 1\n    # Blue color in BGR \n    color = (255, 0, 0) \n    bboxes = det[:,:4].astype(np.int32)\n    scores = det[:,4]\n    # Line thickness of 2 px \n    thickness = 2\n    for b,s in zip(bboxes,scores):\n        if s > 0.1:\n            image = cv2.rectangle(image, (b[0],b[1]), (b[2],b[3]), (255,0,0), 1) \n            image = cv2.putText(image, '{:.2}'.format(s), (b[0]+np.random.randint(20),b[1]), font,  \n                           fontScale, color, thickness, cv2.LINE_AA)\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=[6, 6])\n    plt.imshow(image[:,:,::-1])\n    plt.show()\nimport glob\nimg_paths = glob.glob('/kaggle/input/global-wheat-detection/test/*.jpg')\nimg_path = img_paths[0]\ndet = inference_detector_wbf(model, img_path)\nvis(img_path, det)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimg_paths = glob.glob('/kaggle/input/global-wheat-detection/test/*.jpg')\nresults = []\nfrom tqdm import tqdm\nfor img_path in tqdm(img_paths):\n    det = inference_detector_wbf(model, img_path)\n    pred_strings = []\n    for bbox in det:\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(bbox[4], bbox[0], bbox[1], bbox[2]-bbox[0], bbox[3]-bbox[1]))\n    pred_strings = \" \".join(pred_strings)\n    result = {'image_id': img_path.split('/')[-1].split('.')[0], 'PredictionString': pred_strings}\n    results.append(result)\nimport pandas as pd\ntest_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}