{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Modeling 1 for Wheat Detection Challenge\n\n### Goals\n\n* Modeling of objects with RetinaNet on COCO dataset\n\n### Comments\n\n* Use Keras model at https://github.com/fizyr/keras-retinanet\n* COCO dataset:"},{"metadata":{},"cell_type":"markdown","source":"## I. Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\npip install tensorflow==2.3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport os\nimport numpy as np\nnp.random.seed(42)\nimport tensorflow as tf\ntf.__version__\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#! git clone https://chritter:Ilovexiwen!cr1@github.com/chritter/GlobalWheatDetection-.git \n!cd GlobalWheatDetection-; ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls GlobalWheatDetection-","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\n\n\ndata_path  = '/kaggle/input/global-wheat-detection'\ntrain_path = '.'\n\nmetadata = pd.read_csv(os.path.join(data_path, \"train.csv\"))\nprint(metadata.shape)\n\nmetadata['bbox'] = metadata['bbox'].apply(eval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata.bbox.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata['imagepath'] = metadata['image_id'].apply(lambda imgid: os.path.join(data_path, f'train/{imgid}.jpg'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata['imagepath'].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ntrain_data = metadata[['imagepath']]\ntrain_data.loc[:, 'x1'] = metadata['bbox'].apply(lambda box: box[0])\ntrain_data.loc[:, 'y1'] = metadata['bbox'].apply(lambda box: box[1])\ntrain_data.loc[:, 'x2'] = metadata['bbox'].apply(lambda box: box[0] + box[2])\ntrain_data.loc[:, 'y2'] = metadata['bbox'].apply(lambda box: box[1] + box[3])\ntrain_data.loc[:, 'class_name'] = 'wheatheads'\n\ntrain_data[['x1','x2','y1','y2']] = train_data[['x1','x2','y1','y2']].astype(int)\n\nprint(train_data.head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.to_csv(os.path.join(train_path,'train_data.csv'), header=False, index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create 2nd file used for training:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame([['wheatheads',0]], columns=['classname','classidx']).to_csv(os.path.join(train_path, 'annotation.csv'), header=False, index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Install the RetinaNet code"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n! cd GlobalWheatDetection-/keras-retinanet; pip install . --user","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! export PATH=$PATH:/root/.local/bin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls /root/.local/bin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! cat /root/.local/bin/retinanet-train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## II. Preprocessing\n\n* Split data set into training and validation set. Keep test set separated.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"train_data.csv\")\ntrain_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.sample(30000, random_state=42).to_csv('train_data_sample30k.csv' ,index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.columns = ['image_path', 'x1', 'y1','x2','y2', 'class_name']\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['image_path'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create training and validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"images = train_data['image_path'].unique()\nnp.random.shuffle(images)\n# define the size of trainign set\nidx_start=100\nidx_end = 200\nimages_train = images[:idx_start] # 3373 is total number of images\nimages_valid = images[idx_start:idx_end] # 3373 is total number of images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_tr = train_data[train_data['image_path'].isin(images_train)] # images_train contains all training images\ntrain_data_tr.shape\ntrain_data_tr.to_csv('train_data_split_tr.csv' ,index=False, header=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_val = train_data[train_data['image_path'].isin(images_valid)] # images_valid contains all validation images\ntrain_data_val.shape\ntrain_data_val.to_csv('train_data_split_val.csv' ,index=False, header=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_tr.shape, train_data_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! cat annotation.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! head train_data_split_tr.csv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## II. Modeling\n\n### IIa Run through command line:\n\n* ~40min/epoch for whole dataset (147k) (train.csv)\n* 30k = 26min\n\n#### Modeling Notes\n* By default use pretrained weights: --imagenet-weights',  help='Initialize the model with pretrained imagenet weights. This is the default behaviour.\n"},{"metadata":{},"cell_type":"markdown","source":"How to choose the number of steps based on number of epochs and number of steps per epoch: \n10 images with batch size 2 result in 5 steps"},{"metadata":{"trusted":true},"cell_type":"code","source":"100./8.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! mkdir tf_dir","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! /root/.local/bin/retinanet-train --batch-size 8 --epochs 3  --tensorboard-dir \\\n    tf_dir --tensorboard-freq batch --steps 12 csv  train_data_split_tr.csv annotation.csv \\\n        --val-annotations train_data_split_val.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#! rm -rf tf_dir/train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls tf_dir/train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.summary.summary_iterator import summary_iterator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_total_loss = []\nbatch_regression_loss = []\nbatch_classification_loss = []\n\nepoch_total_loss = []\nepoch_regression_loss = []\nepoch_classification_loss = []\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_losses_event(event_file):\n\n    # event_file = \"tf_dir/train/events.out.tfevents.1604697856.b107a84b4086.2942.5011.v2\"\n    for i, e in enumerate(summary_iterator(event_file)):\n        #print(f\"###################### {i}\")\n        #print(e)\n        for v in e.summary.value:\n            #print(v.tag)\n            if v.tag == 'batch_loss':\n                batch_total_loss.append(v.simple_value)\n                #count+=1\n            if 'epoch_loss' == v.tag:\n                epoch_total_loss.append(v.simple_value)\n            if v.tag == 'batch_regression_loss':\n                batch_regression_loss.append(v.simple_value)\n            if v.tag == 'batch_classification_loss':\n                batch_classification_loss.append(v.simple_value)\n            if v.tag == 'epoch_regression_loss':\n                epoch_regression_loss.append(v.simple_value)\n            if v.tag == 'epoch_classification_loss':\n                epoch_classification_loss.append(v.simple_value)\n    \n    fig, axes = plt.subplots(2, figsize=(16,6))\n    axes  = axes.flatten()\n    iter_num_batch = range(len(batch_total_loss))\n    iter_num_epochs = range(1, len(epoch_total_loss)+1)\n    axes[0].plot(iter_num_batch, batch_total_loss, label='batch_loss_total', marker='x')\n    axes[0].plot(iter_num_batch, batch_regression_loss, label='batch_regression_loss', marker='x')\n    axes[0].plot(iter_num_batch, batch_classification_loss, label='batch_classification_loss', marker='x')\n    axes[0].legend(); axes[0].set_xlabel('number iterations');axes[0].set_ylabel('loss')\n    axes[1].plot(iter_num_epochs, epoch_total_loss, label='epoch_total_loss', marker='x')\n    axes[1].plot(iter_num_epochs, epoch_regression_loss, label='epoch_regression_loss', marker='x')\n    axes[1].plot(iter_num_epochs, epoch_classification_loss, label='epoch_classification_loss', marker='x')\n    axes[1].legend(); axes[1].set_xlabel('number iterations'); axes[1].set_ylabel('loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_losses_event(\"tf_dir/train/events.out.tfevents.1604697856.b107a84b4086.2942.5011.v2\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evalute Resnet"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_retinanet.models import load_model\nfrom keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\nimport cv2\nfrom keras_retinanet.utils.visualization import draw_box, draw_caption","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = load_model('./snapshots/resnet50_csv_02.h5', backbone_name='resnet50')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_one_image(image_path):\n\n    # load image\n    image = read_image_bgr(image_path)\n    \n\n    # copy to draw on\n    draw = image.copy()\n    \n    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n\n    # preprocess image for network\n    image = preprocess_image(image)\n    image, scale = resize_image(image)\n\n    return image, scale, draw\n\ndef test_image(image_path):\n    \n    print(f'test {image_path}')\n    \n    image, scale, draw = preprocess_one_image(image_path)\n    \n    my_batch = np.expand_dims(image, axis=0)\n    print(f'test shape {my_batch.shape}')\n    test = model.predict_on_batch(my_batch)\n\n    print(f'test {test}')\n    print('test1 ', test[0].shape) # bounding boxes\n    print('test2 ', test[1].shape) # score for bounding boxes\n    \n    return test, draw\n    \ndef draw_box(image, box, color, thickness=2):\n    \"\"\" Draws a box on an image with a given color.\n    # Arguments\n        image     : The image to draw on.\n        box       : A list of 4 elements (x1, y1, x2, y2).\n        color     : The color of the box.\n        thickness : The thickness of the lines to draw a box with.\n    \"\"\"\n    b = np.array(box).astype(int)\n    #cv2.rectangle(img=image, pt1=(b[0], b[1]), pt2=(b[2], b[3]), color='r', thickness=2, lineType='--')\n    #return cv2.rectangle(img=image, rec=b, color=color, thickness=thickness, lineType=3)\n    \n    return cv2.rectangle(image, (b[0], b[1]), (b[2], b[3]), (36,255,12), 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_test = cv2.cvtColor( read_image_bgr(images_train[0]), cv2.COLOR_BGR2RGB)\nimg_test.shape\ndraw = draw_box(image=img_test, box=[-100, -100, 100, 100 ], color=(255, 255, 255), thickness=2)\n\nplt.figure(figsize=(15, 15))\nplt.imshow(draw)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testa = model.predict_on_batch(np.expand_dims(image, axis=0))\ntesta[0].shape, testa[1].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.expand_dims(image, axis=0).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# process image\nstart = time.time()\n#boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\nboxes, scores = model.predict_on_batch(np.expand_dims(image, axis=0))\n\nprint(\"processing time: \", time.time() - start)\n\n# correct for image scale\nboxes /= scale\n\n# visualize detections\nfor box, score, label in zip(boxes[0], scores[0], labels[0]):\n    # scores are sorted so we can break\n    if score < 0.5:\n        break\n        \n    color = label_color(label)\n    \n    b = box.astype(int)\n    draw_box(draw, b, color=color)\n    \n    caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n    draw_caption(draw, b, caption)\n    \nplt.figure(figsize=(15, 15))\nplt.axis('off')\nplt.imshow(draw)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxes, scores, labels = model.predict_on_batch(inputs)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}