{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport re\nimport math\nfrom PIL import Image\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2, ToTensor\n\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom matplotlib import pyplot as plt\n\nDIR_INPUT = '/kaggle/input/global-wheat-detection/'\nDIR_TRAIN = '/kaggle/input/global-wheat-detection/train'\nDIR_TEST = '/kaggle/input/global-wheat-detection/test'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install spams\n!pip install staintools","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import staintools","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!git clone https://github.com/eriklindernoren/PyTorch-GAN/","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import copy\nimport json\n\nimport numpy as np\nimport torch\nimport albumentations as A","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(DIR_INPUT+'train.csv')\ntrain_df.shape\n\ntrain_df['x'] = -1\ntrain_df['y'] = -1\ntrain_df['w'] = -1\ntrain_df['h'] = -1\n\ndef expand_bbox(x):\n    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n    if len(r) == 0:\n        r = [-1, -1, -1, -1]\n    return r\n\ntrain_df[['x', 'y', 'w', 'h']] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\ntrain_df.drop(columns=['bbox'], inplace=True)\ntrain_df['x'] = train_df['x'].astype(np.float)\ntrain_df['y'] = train_df['y'].astype(np.float)\ntrain_df['w'] = train_df['w'].astype(np.float)\ntrain_df['h'] = train_df['h'].astype(np.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nCounter(train_df['source'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\nimport random\nfrom tqdm.auto import tqdm\nclass WheatDataset(Dataset):\n\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n        \n        self.df = dataframe\n        self.image_ids = dataframe['image_id'].unique()\n        self.image_ids = shuffle(self.image_ids)\n        self.labels = [np.zeros((0, 5), dtype=np.float32)] * len(self.image_ids)\n        self.img_size = 1024\n        im_w = 1024\n        im_h = 1024\n        for i, img_id in enumerate(tqdm(self.image_ids)):\n            records = self.df[self.df['image_id'] == img_id]\n            boxes = records[['x', 'y', 'w', 'h']].values\n            boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n            boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n            boxesyolo = []\n            for box in boxes:\n                x1, y1, x2, y2 = box\n                xc, yc, w, h = 0.5*x1/im_w+0.5*x2/im_w, 0.5*y1/im_h+0.5*y2/im_h, abs(x2/im_w-x1/im_w), abs(y2/im_h-y1/im_h)\n                boxesyolo.append([0, xc, yc, w, h])\n            self.labels[i] = np.array(boxesyolo)\n        \n        self.image_dir = image_dir\n        self.transforms = transforms\n        \n        self.mosaic = False\n        self.augment = True\n\n    def __getitem__(self, index: int):\n        self.mosaic = True\n        self.augment = True\n        fraq = np.random.random()\n        if self.mosaic and fraq>0.75:\n            img, labels = load_mosaic(self, index)\n            fun_name = \"load_mosaic\"\n        elif self.mosaic and fraq>0.5:\n            img, labels = load_mixup(self, index)\n            fun_name = \"load_mixup\"\n        elif self.mosaic and fraq>0.25:\n            img, labels = load_mosaic_mixup(self, index)\n            fun_name = \"load_mosaic_mixup\"\n        else:\n            # Load image\n            img, (h0, w0), (h, w) = load_image(self, index)\n            fun_name = \"real\"\n\n            # Letterbox\n            shape = self.img_size  # final letterboxed shape\n            img, ratio, pad = letterbox(img, shape, auto=False, scaleup=self.augment)\n            shapes = (h0, w0), ((h / h0, w / w0), pad)  # for COCO mAP rescaling\n\n            # Load labels\n            labels = []\n            x = self.labels[index]\n            if x.size > 0:\n                # Normalized xywh to pixel xyxy format\n                labels = x.copy()\n                labels[:, 1] = ratio[0] * w * (x[:, 1] - x[:, 3] / 2) + pad[0]  # pad width\n                labels[:, 2] = ratio[1] * h * (x[:, 2] - x[:, 4] / 2) + pad[1]  # pad height\n                labels[:, 3] = ratio[0] * w * (x[:, 1] + x[:, 3] / 2) + pad[0]\n                labels[:, 4] = ratio[1] * h * (x[:, 2] + x[:, 4] / 2) + pad[1]\n        \n        if self.augment:\n            # Augment imagespace\n            if not self.mosaic:\n                img, labels = random_affine(img, labels,\n                                            degrees=0,\n                                            translate=0,\n                                            scale=0,\n                                            shear=0)\n\n            # Augment colorspace\n            augment_hsv(img, hgain=0.0138, sgain= 0.678, vgain=0.36)\n        \n\n            \n        return img, labels,fun_name\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_image(self, index):\n    # loads 1 image from dataset, returns img, original hw, resized hw\n    image_id = self.image_ids[index]\n    imgpath = DIR_TRAIN\n    img = cv2.imread(f'{imgpath}/{image_id}.jpg', cv2.IMREAD_COLOR)\n    \n    assert img is not None, 'Image Not Found ' + imgpath\n    h0, w0 = img.shape[:2]  # orig hw\n    return img, (h0, w0), img.shape[:2]  # img, hw_original, hw_resized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_mosaic(self, index):\n    # loads images in a mosaic\n\n    labels4 = []\n    s = self.img_size\n    xc, yc = [int(random.uniform(s * 0.5, s * 1.5)) for _ in range(2)]  # mosaic center x, y\n    indices = [index] + [random.randint(0, len(self.labels) - 1) for _ in range(3)]  # 3 additional image indices\n    for i, index in enumerate(indices):\n        # Load image\n        img, _, (h, w) = load_image(self, index)\n\n        # place img in img4\n        if i == 0:  # top left\n            img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\n            x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n            x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n        elif i == 1:  # top right\n            x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n            x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n        elif i == 2:  # bottom left\n            x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n            x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n        elif i == 3:  # bottom right\n            x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n            x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n\n        img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]\n        padw = x1a - x1b\n        padh = y1a - y1b\n\n        # Labels\n        x = self.labels[index]\n        labels = x.copy()\n        if x.size > 0:  # Normalized xywh to pixel xyxy format\n            labels[:, 1] = w * (x[:, 1] - x[:, 3] / 2) + padw\n            labels[:, 2] = h * (x[:, 2] - x[:, 4] / 2) + padh\n            labels[:, 3] = w * (x[:, 1] + x[:, 3] / 2) + padw\n            labels[:, 4] = h * (x[:, 2] + x[:, 4] / 2) + padh\n        labels4.append(labels)\n\n    # Concat/clip labels\n    if len(labels4):\n        labels4 = np.concatenate(labels4, 0)\n        # np.clip(labels4[:, 1:] - s / 2, 0, s, out=labels4[:, 1:])  # use with center crop\n        np.clip(labels4[:, 1:], 0, 2 * s, out=labels4[:, 1:])  # use with random_affine\n\n    # Augment\n    # img4 = img4[s // 2: int(s * 1.5), s // 2:int(s * 1.5)]  # center crop (WARNING, requires box pruning)\n    img4, labels4 = random_affine(img4, labels4,\n                                  degrees=1.98 * 2,\n                                  translate=0.05 * 2,\n                                  scale=0.05 * 2,\n                                  shear=0.641 * 2,\n                                  border=-s // 2)  # border to remove\n\n    return img4, labels4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_mosaic_mixup(self, index):\n    # loads images in a mosaic\n    labels4 = []\n    s = self.img_size\n    xc, yc = [int(random.uniform(s * 0.5, s * 1.5)) for _ in range(2)]  # mosaic center x, y\n    indices = [index] + [random.randint(0, len(self.labels) - 1) for _ in range(3)]  # 3 additional image indices\n    for i, index in enumerate(indices):\n        # Load image\n        img, x = load_mixup(self, index,True)\n        h,w,_ = img.shape\n        # place img in img4\n        if i == 0:  # top left\n            img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\n            x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n            x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n        elif i == 1:  # top right\n            x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n            x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n        elif i == 2:  # bottom left\n            x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n            x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n        elif i == 3:  # bottom right\n            x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n            x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n\n        img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]\n        padw = x1a - x1b\n        padh = y1a - y1b\n\n        # Labels\n#         x = self.labels[index]\n        labels = x.copy()\n        if x.size > 0:  # Normalized xywh to pixel xyxy format\n            labels[:, 1] = labels[:, 1] + padw\n            labels[:, 2] = labels[:, 2] + padh\n            labels[:, 3] = labels[:, 3] + padw\n            labels[:, 4] = labels[:, 4] + padh\n        labels4.append(labels)\n\n    # Concat/clip labels\n    if len(labels4):\n        labels4 = np.concatenate(labels4, 0)\n        # np.clip(labels4[:, 1:] - s / 2, 0, s, out=labels4[:, 1:])  # use with center crop\n        np.clip(labels4[:, 1:], 0, 2 * s, out=labels4[:, 1:])  # use with random_affine\n\n    # Augment\n    # img4 = img4[s // 2: int(s * 1.5), s // 2:int(s * 1.5)]  # center crop (WARNING, requires box pruning)\n    img4, labels4 = random_affine(img4, labels4,\n                                  degrees=1.98 * 2,\n                                  translate=0.05 * 2,\n                                  scale=0.05 * 2,\n                                  shear=0.641 * 2,\n                                  border=-s // 2)  # border to remove\n\n    return img4, labels4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_mixup(self, index,mosaic=False):\n    count = 1\n    s = self.img_size\n    if True:\n        labels4 = []\n        image, _, (h, w) = load_image(self, index)\n\n        image = image.astype(np.float32)\n        image /= 255.0\n        new_index = random.randint(0, len(self.labels) - 1)\n        r_image, _, (h, w) = load_image(self, new_index)\n\n        r_image = r_image.astype(np.float32)\n        r_image /= 255.0\n        h=r_image.shape[0]\n        w = image.shape[1]\n        \n        x = self.labels[index]\n        labels = x.copy()\n        if x.size > 0:\n            labels[:, 1] = w * (x[:, 1] - x[:, 3] / 2)\n            labels[:, 2] = h * (x[:, 2] - x[:, 4] / 2)\n            labels[:, 3] = w * (x[:, 1] + x[:, 3] / 2)\n            labels[:, 4] = h * (x[:, 2] + x[:, 4] / 2)\n        labels4.append(labels.copy())\n        \n        x = self.labels[new_index]\n        labels = x.copy()\n        if x.size > 0: \n            labels[:, 1] = w * (x[:, 1] - x[:, 3] / 2)\n            labels[:, 2] = h * (x[:, 2] - x[:, 4] / 2)\n            labels[:, 3] = w * (x[:, 1] + x[:, 3] / 2)\n            labels[:, 4] = h * (x[:, 2] + x[:, 4] / 2)\n        labels4.append(labels.copy())\n        \n    mixup_image = (image+r_image)/2\n    mixup_image = mixup_image*255\n    mixup_image = mixup_image.astype(np.uint8)\n\n    if len(labels4):\n        labels4 = np.concatenate(labels4, 0)\n        \n    if mosaic:\n        return mixup_image, labels4\n    img4, labels4 = random_affine(mixup_image, labels4,\n                                  degrees=0,\n                                  translate=0,\n                                  scale=0,\n                                  shear=0)\n    return img4, labels4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_affine(img, targets=(), degrees=10, translate=.1, scale=.1, shear=10, border=0):\n    # torchvision.transforms.RandomAffine(degrees=(-10, 10), translate=(.1, .1), scale=(.9, 1.1), shear=(-10, 10))\n    # https://medium.com/uruvideo/dataset-augmentation-with-random-homographies-a8f4b44830d4\n\n    if targets is None:  # targets = [cls, xyxy]\n        targets = []\n    height = img.shape[0] + border * 2\n    width = img.shape[1] + border * 2\n\n    # Rotation and Scale\n    R = np.eye(3)\n    a = random.uniform(-degrees, degrees)\n    # a += random.choice([-180, -90, 0, 90])  # add 90deg rotations to small rotations\n    s = random.uniform(1 - scale, 1 + scale)\n    R[:2] = cv2.getRotationMatrix2D(angle=a, center=(img.shape[1] / 2, img.shape[0] / 2), scale=s)\n\n    # Translation\n    T = np.eye(3)\n    T[0, 2] = random.uniform(-translate, translate) * img.shape[0] + border  # x translation (pixels)\n    T[1, 2] = random.uniform(-translate, translate) * img.shape[1] + border  # y translation (pixels)\n\n    # Shear\n    S = np.eye(3)\n    S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # x shear (deg)\n    S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # y shear (deg)\n\n    # Combined rotation matrix\n    M = S @ T @ R  # ORDER IS IMPORTANT HERE!!\n    if (border != 0) or (M != np.eye(3)).any():  # image changed\n        img = cv2.warpAffine(img, M[:2], dsize=(width, height), flags=cv2.INTER_LINEAR, borderValue=(114, 114, 114))\n\n    # Transform label coordinates\n    n = len(targets)\n    if n:\n        # warp points\n        xy = np.ones((n * 4, 3))\n        xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n        xy = (xy @ M.T)[:, :2].reshape(n, 8)\n\n        # create new boxes\n        x = xy[:, [0, 2, 4, 6]]\n        y = xy[:, [1, 3, 5, 7]]\n        xy = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n\n        # # apply angle-based reduction of bounding boxes\n        # radians = a * math.pi / 180\n        # reduction = max(abs(math.sin(radians)), abs(math.cos(radians))) ** 0.5\n        # x = (xy[:, 2] + xy[:, 0]) / 2\n        # y = (xy[:, 3] + xy[:, 1]) / 2\n        # w = (xy[:, 2] - xy[:, 0]) * reduction\n        # h = (xy[:, 3] - xy[:, 1]) * reduction\n        # xy = np.concatenate((x - w / 2, y - h / 2, x + w / 2, y + h / 2)).reshape(4, n).T\n\n        # reject warped points outside of image\n        xy[:, [0, 2]] = xy[:, [0, 2]].clip(0, width)\n        xy[:, [1, 3]] = xy[:, [1, 3]].clip(0, height)\n        w = xy[:, 2] - xy[:, 0]\n        h = xy[:, 3] - xy[:, 1]\n        area = w * h\n        area0 = (targets[:, 3] - targets[:, 1]) * (targets[:, 4] - targets[:, 2])\n        ar = np.maximum(w / (h + 1e-16), h / (w + 1e-16))  # aspect ratio\n        i = (w > 4) & (h > 4) & (area / (area0 * s + 1e-16) > 0.2) & (ar < 10)\n\n        targets = targets[i]\n        targets[:, 1:5] = xy[i]\n\n    return img, targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def augment_hsv(img, hgain=0.5, sgain=0.5, vgain=0.5):\n    r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains\n    hue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2HSV))\n    dtype = img.dtype  # uint8\n\n    x = np.arange(0, 256, dtype=np.int16)\n    lut_hue = ((x * r[0]) % 180).astype(dtype)\n    lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n    lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n\n    img_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val))).astype(dtype)\n    cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR, dst=img)  # no return needed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def letterbox(img, new_shape=(416, 416), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n    # Resize image to a 32-pixel-multiple rectangle https://github.com/ultralytics/yolov3/issues/232\n    shape = img.shape[:2]  # current shape [height, width]\n    if isinstance(new_shape, int):\n        new_shape = (new_shape, new_shape)\n\n    # Scale ratio (new / old)\n    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n        r = min(r, 1.0)\n\n    # Compute padding\n    ratio = r, r  # width, height ratios\n    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n    if auto:  # minimum rectangle\n        dw, dh = np.mod(dw, 64), np.mod(dh, 64)  # wh padding\n    elif scaleFill:  # stretch\n        dw, dh = 0.0, 0.0\n        new_unpad = new_shape\n        ratio = new_shape[0] / shape[1], new_shape[1] / shape[0]  # width, height ratios\n\n    dw /= 2  # divide padding into 2 sides\n    dh /= 2\n\n    if shape[::-1] != new_unpad:  # resize\n        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n    return img, ratio, (dw, dh)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = WheatDataset(train_df, DIR_TRAIN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd PyTorch-GAN/implementations/pix2pix\nfrom models import *\nfrom datasets import *\ncuda = True if torch.cuda.is_available() else False\ngenerator = GeneratorUNet()\ngenerator.load_state_dict(torch.load(\"/kaggle/input/fork-of-pytorch-pix-2-pix-for-image-colorization/gen.pth\",map_location=torch.device('cpu')))\ngenerator.eval()\nimport torchvision.transforms as transforms\ntransforms_ = [\n    transforms.Resize((1024, 1024), Image.BICUBIC),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n]\ntransform = transforms.Compose(transforms_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Pix2PixColor(images):\n    img_A = Image.fromarray(np.array(images[:,:,::-1]), \"RGB\")\n    img_A = transform(img_A)\n    with torch.no_grad():\n        fake = generator(img_A.unsqueeze(0))\n    images = fake.numpy()[0].transpose(1,2,0)\n    images -=images.min()\n    images/=images.max()\n    images*=255\n    images = images.astype(np.uint8)\n    images = cv2.UMat(images).get()\n    return images\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nUSE_staintools = True\nplt.figure(figsize=[20,20])\nstep_1 = True\nfor row in range(9):\n    images, targets,name = train_dataset[row]\n    names = name.split(\"_\")\n    if USE_staintools and  \"mixup\" not in names:\n        if (np.random.random()>0.4) or step_1:\n            target = staintools.LuminosityStandardizer.standardize(images[:,:,::-1])\n            normalizer = staintools.StainNormalizer(method='vahadane')\n            normalizer.fit(target)\n            step_1 = False\n        else:\n            to_transform = images[:,:,::-1]\n            to_transform = staintools.LuminosityStandardizer.standardize(to_transform)\n            transformed = normalizer.transform(to_transform)\n            images = transformed\n            name = name + \"_staintools\"\n    if \"mixup\" not in names and np.random.random()>0.2:\n        images = Pix2PixColor(images)\n        name = name+'_colorization'\n    targets = np.array(targets).astype(int)\n    for box in targets:\n        cv2.rectangle(images,(box[1], box[2]),(box[3], box[4]),(220, 0, 0), 2)\n    plt.subplot(3,3,row+1)\n    plt.imshow(images)\n    plt.title(name)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create new data\n# !mkdir train\n# from tqdm import tqdm_notebook\n# result = []\n# num_epochs = 1\n# for n in tqdm_notebook(range(num_epochs)):\n#     for z,(images, targets) in tqdm_notebook(enumerate(train_dataset)):\n#         if True:\n#             name = (n*len(train_dataset))+z\n#             boxes = targets\n#             sample = images\n#             for box in boxes:\n#                 x0,y0,w,h = int(box[1]),int(box[2]),int(box[3])-int(box[1]),int(box[4])-int(box[2])\n#                 result.append([\"{}\".format(name),sample.shape[0],sample.shape[1],x0,y0,w,h])\n#             cv2.imwrite(\"train/{}.jpg\".format(name),sample)\n# import pandas as pd\n# df = pd.DataFrame(result,columns=['image_id',\"height\",\"width\",'x0','y0','w','h'])\n# df.to_csv(\"train_aug.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}