{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle as rect\nfrom albumentations.pytorch.transforms import ToTensorV2\n%matplotlib inline\n\nimport os\nimport random\nfrom PIL import Image\n\n#To get reproducible transformations\nSEED = 42\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"TRAIN_IMAGES_PATH = '../input/global-wheat-detection/train'\ndf = pd.read_csv('../input/global-wheat-detection/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Display Original Images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_bbox(img):\n    df_img = df[df.image_id==img][['bbox']]\n    \n    bbox_list = []\n    for i in df_img.iterrows():\n        xmin,ymin,width,height = np.fromstring(i[1][0][1:-1],sep=',')\n        bbox_list.append([xmin,ymin,width,height])\n    return bbox_list\n    \n    \ndef display_images(rows, cols, image_paths):\n    fig, ax = plt.subplots(rows,cols, figsize=(15,5))\n    plt.suptitle('Original Images')\n    for j in range(cols):\n        arr = Image.open(image_paths[j])\n        img_id = image_paths[j][-13:-4]\n        ax[j].set_title(img_id)\n        ax[j].imshow(arr)\n        ax[j].axis('off')\n        \n        bboxes = get_bbox(img_id)\n        \n        for bbox in bboxes:            \n            ax[j].add_patch(rect((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none'))\n        \n\nimg_list =[TRAIN_IMAGES_PATH + '/'+ s + '.jpg' for s in pd.unique(df.image_id)[0:3].tolist()]\n\ndisplay_images(1,3,img_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transformations\n\nThis is my attempt to understand transformations/albumentations. I took some of the transformations mentioned in [this](https://www.kaggle.com/shonenkov/training-efficientdet/notebook) kernel.\n\n[Albumentations - ToGray](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ToGray)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndisplay_images(1,3,img_list)\n\nfig, ax = plt.subplots(1,3, figsize=(15,5))\nplt.suptitle(\"Transformed - ToGray()\")\nseed_everything(SEED)\ntransforms_1 = A.Compose([A.ToGray(p=0.4)]) #higher probability greater chance of getting gray scale\n\nfor i,img_path in enumerate(img_list):    \n    arr = np.array(Image.open(img_path))\n    img_id = img_path[-13:-4]\n    bboxes = get_bbox(img_id)\n    \n    arr = transforms_1(**{\"image\": arr})['image']\n    ax[i].set_title(img_id)\n    ax[i].imshow(arr)\n    for bbox in bboxes:  \n            ax[i].add_patch(rect((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none'))\n    ax[i].axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only two images got gray scale transformation. If we set probability to higher value we can get more grayscale transformation (and vice versa)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## HorizontalFlip\n\n[Albumentations HorizontalFlip](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.HorizontalFlip)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"display_images(1,3,img_list)\n\nfig, ax = plt.subplots(1,3, figsize=(15,5))\nplt.suptitle(\"Transformed - HorizontalFlip\")\nseed_everything(SEED)\n#We need bbox_params so that we get correct bboxes after flipping. \ntransforms_1 = A.Compose([A.HorizontalFlip(p=0.5)],\n                         bbox_params=A.BboxParams(format='coco', min_area=0, \n                                               min_visibility=0, label_fields=['labels']) ) \n\nfor i,img_path in enumerate(img_list):    \n    arr = np.array(Image.open(img_path))\n    img_id = img_path[-13:-4]\n    bboxes = get_bbox(img_id)\n    #Here labels are all ones because wheat is the only class we have. \n    #For e.g.: Labels would be different if we have bboxes for different objects for e.g cat, dog in the same image. \n    transform = transforms_1(**{\"image\": arr , \"bboxes\": bboxes, \"labels\":np.ones(len(bboxes))})\n    arr = transform['image']\n    bboxes = transform['bboxes']\n    ax[i].set_title(img_id)\n    ax[i].imshow(arr)\n    for bbox in bboxes:  \n            ax[i].add_patch(rect((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none'))\n            \n    ax[i].axis('off')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Third image ('7b72ea0fb') isn't flipped. First two images did get flipped.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Resize \n\n[Albumentations Resize](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Resize)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"display_images(1,3,img_list)\n\nfig, ax = plt.subplots(1,3, figsize=(15,5))\nplt.suptitle(\"Transformed - Resize\")\nseed_everything(SEED)\n#We need bbox_params so that we get correct bboxes after flipping. \ntransforms_1 = A.Compose([A.Resize(height=512, width=512, p=1),], #p=1 because we want all images resized\n                         bbox_params=A.BboxParams(format='coco', min_area=0, \n                                               min_visibility=0, label_fields=['labels']) ) \n\nfor i,img_path in enumerate(img_list):    \n    arr = np.array(Image.open(img_path))\n    img_id = img_path[-13:-4]\n    bboxes = get_bbox(img_id)\n    #Here labels are all ones because wheat is the only class we have. \n    #For e.g.: Labels would be different if we have bboxes for different objects for e.g cat, dog in the same image. \n    transform = transforms_1(**{\"image\": arr , \"bboxes\": bboxes, \"labels\":np.ones(len(bboxes))})\n    arr = transform['image']\n    bboxes = transform['bboxes']\n    ax[i].set_title(img_id + ' ' + str(arr.shape))  #original images were 1024,1024,3\n    ax[i].imshow(arr)\n    for bbox in bboxes:  \n            ax[i].add_patch(rect((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none'))\n            \n    ax[i].axis('off')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cutout\n\n[Albumentations Cutout](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Cutout)\n\n- No bboxes required here","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"display_images(1,3,img_list)\n\nfig, ax = plt.subplots(1,3, figsize=(15,5))\nplt.suptitle(\"Transformed - Cutout\")\nseed_everything(SEED)\ntransforms_1 = A.Compose( [A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=[255,255,255], p=0.5)] )\n#fill value = [255,255,255] = White\n#If you change p=0.7 all three images should have cutout.\n\nfor i,img_path in enumerate(img_list):    \n    arr = np.array(Image.open(img_path))\n    img_id = img_path[-13:-4]\n    transform = transforms_1(**{\"image\": arr })\n    arr = transform['image']\n    ax[i].set_title(img_id + ' ' + str(arr.shape))  #original images were 1024,1024,3\n    ax[i].imshow(arr)\n    ax[i].axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ToTensorV2 \n\n[Albumentations ToTensorV2](https://albumentations.readthedocs.io/en/latest/api/pytorch.html#albumentations.pytorch.transforms.ToTensorV2)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"transforms_1 = A.Compose( [A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=[255,255,255], p=0.5)] )\n\nfor i,img_path in enumerate(img_list[0:1]):    \n    arr = np.array(Image.open(img_path))\n    img_id = img_path[-13:-4]\n    transform = transforms_1(**{\"image\": arr })\n    arr = transform['image']\n    print(type(arr))\n\n    \n#Converting to torch.tensor - which will be required for modeling\ntransforms_1 = A.Compose( [A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=[255,255,255], p=0.5),\n                           ToTensorV2() ] )\n\nfor i,img_path in enumerate(img_list[0:1]):    \n    arr = np.array(Image.open(img_path))\n    img_id = img_path[-13:-4]\n    transform = transforms_1(**{\"image\": arr })\n    arr = transform['image']\n    print(type(arr))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ToGray + Resize + HorizontalFlip + Cutout","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"display_images(1,3,img_list)\n\nfig, ax = plt.subplots(1,3, figsize=(15,5))\nplt.suptitle(\"Transformed - ToGray + Resize + HorizontalFlip + Cutout\")\nseed_everything(SEED)\ntransforms_1 = A.Compose( [ A.ToGray(p=0.4),\n                            A.Resize(height=512, width=512, p=1),\n                            A.HorizontalFlip(p=0.5), \n                            A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=[255,255,255], p=0.5)],\n                          bbox_params=A.BboxParams(format='coco', min_area=0, \n                                               min_visibility=0, label_fields=['labels'])  )\n#fill value = [255,255,255] = White\n#If you change p=0.7 all three images should have cutout.\n\nfor i,img_path in enumerate(img_list):    \n    arr = np.array(Image.open(img_path))\n    img_id = img_path[-13:-4]\n    bboxes = get_bbox(img_id)\n    transform = transforms_1(**{\"image\": arr , \"bboxes\": bboxes, \"labels\":np.ones(len(bboxes))})\n    arr = transform['image']\n    bboxes = transform['bboxes']\n    ax[i].set_title(img_id + ' ' + str(arr.shape))  #original images were 1024,1024,3\n    ax[i].imshow(arr)\n    for bbox in bboxes:  \n            ax[i].add_patch(rect((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none'))\n            \n    ax[i].axis('off')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that not all transformations got applied to all images due to setting different probabilities. Thankfully the seeds work, so results are reproducible. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}