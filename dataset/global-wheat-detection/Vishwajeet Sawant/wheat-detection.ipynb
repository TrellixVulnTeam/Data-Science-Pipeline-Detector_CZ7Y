{"cells":[{"metadata":{"id":"J5sAwRoBbMlE","trusted":false},"cell_type":"code","source":"## predict","execution_count":null,"outputs":[]},{"metadata":{"id":"pwUUBuLEU5sC","trusted":true},"cell_type":"code","source":"import numpy as np\nimport time\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"qJllRJWGV0og","trusted":true},"cell_type":"code","source":"def display_img(img,cmap=None):\n    fig = plt.figure(figsize = (12,12))\n    plt.axis(False)\n    ax = fig.add_subplot(111)\n    ax.imshow(img,cmap)","execution_count":null,"outputs":[]},{"metadata":{"id":"dVS3z-ZhWFTI","outputId":"16d54ab7-9efd-42cb-e27c-97251d6f3c8b","trusted":true},"cell_type":"code","source":"labelsPath = \"../input/model-files/yolo.names\"\nLABELS = open(labelsPath).read().strip().split(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"id":"zV4YjdjUWRaw","outputId":"069bf42a-5213-420e-84ed-4150a24721f5","trusted":true},"cell_type":"code","source":"LABELS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Input, Conv2D,ReLU, Flatten, Dense, MaxPool2D, concatenate, BatchNormalization, AveragePooling2D, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Activation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inception(layer_in, num_filter):\n    conv = Conv2D(num_filter,(3,3),padding='same', activation='relu', kernel_initializer='he_normal')(layer_in)\n    conv2 = Conv2D(num_filter,(5,5),padding='same', activation='relu', kernel_initializer='he_normal')(layer_in)\n    \n    max_pool = MaxPool2D(pool_size=(2,2), strides=(1,1), padding='same')(layer_in)\n    \n    output = concatenate(inputs=[conv, conv2, max_pool], axis=-1)\n    output = Activation('relu')(output)\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_inception():\n    inputs = Input(shape=(100,100,3))\n    num_filters = 64\n    \n    t = BatchNormalization()(inputs)\n    t = Conv2D(kernel_size=3, strides=1, filters=num_filters, padding=\"same\")(t)\n    t = Activation('relu')(t)\n    \n    t = inception(t,64)\n    t= inception(t,32)\n    \n    t = AveragePooling2D(4)(t)\n    t = Flatten()(t)\n    \n    outputs = Dense(7, activation='softmax')(t)\n    \n    model = Model(inputs, outputs)\n    model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_inception()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(\"../input/model-files/wheat_detect.h5\")","execution_count":null,"outputs":[]},{"metadata":{"id":"S0mIFTtjWeRb","trusted":true},"cell_type":"code","source":"# derive the paths to the YOLO weights and model configuration\nweightsPath = \"../input/model-files/yolov3_custom_train_3000.weights\"\nconfigPath = \"../input/modelfiles/yolov3_custom_train.cfg\"","execution_count":null,"outputs":[]},{"metadata":{"id":"VZEv6BWxWzRZ","trusted":true},"cell_type":"code","source":"# Loading the neural network framework Darknet (YOLO was created based on this framework)\nnet = cv2.dnn.readNetFromDarknet(configPath,weightsPath)","execution_count":null,"outputs":[]},{"metadata":{"id":"Zpx7oEHOW7dy","trusted":true},"cell_type":"code","source":"# Create the function which predict the frame input\ndef predict(image):\n    \n    # initialize a list of colors to represent each possible class label\n    np.random.seed(42)\n    COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype=\"uint8\")\n    (H, W) = image.shape[:2]\n    \n    # determine only the \"ouput\" layers name which we need from YOLO\n    ln = net.getLayerNames()\n    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n    \n    # construct a blob from the input image and then perform a forward pass of the YOLO object detector, \n    # giving us our bounding boxes and associated probabilities\n    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n    net.setInput(blob)\n    layerOutputs = net.forward(ln)\n    \n    boxes = []\n    confidences = []\n    classIDs = []\n    threshold = 0.2\n    \n    # loop over each of the layer outputs\n    for output in layerOutputs:\n        # loop over each of the detections\n        for detection in output:\n            # extract the class ID and confidence (i.e., probability) of\n            # the current object detection\n            scores = detection[5:]\n            classID = np.argmax(scores)\n            confidence = scores[classID]\n\n            # filter out weak predictions by ensuring the detected\n            # probability is greater than the minimum probability\n            # confidence type=float, default=0.5\n            if confidence > threshold:\n                # scale the bounding box coordinates back relative to the\n                # size of the image, keeping in mind that YOLO actually\n                # returns the center (x, y)-coordinates of the bounding\n                # box followed by the boxes' width and height\n                box = detection[0:4] * np.array([W, H, W, H])\n                (centerX, centerY, width, height) = box.astype(\"int\")\n\n                # use the center (x, y)-coordinates to derive the top and\n                # and left corner of the bounding box\n                x = int(centerX - (width / 2))\n                y = int(centerY - (height / 2))\n\n                # update our list of bounding box coordinates, confidences,\n                # and class IDs\n                boxes.append([x, y, int(width), int(height)])\n                confidences.append(float(confidence))\n                classIDs.append(classID)\n\n    # apply non-maxima suppression to suppress weak, overlapping bounding boxes\n    idxs = cv2.dnn.NMSBoxes(boxes, confidences, threshold, 0.1)\n\n    # ensure at least one detection exists\n    if len(idxs) > 0:\n        # loop over the indexes we are keeping\n        for i in idxs.flatten():\n            # extract the bounding box coordinates\n            (x, y) = (boxes[i][0], boxes[i][1])\n            (w, h) = (boxes[i][2], boxes[i][3])\n\n            # draw a bounding box rectangle and label on the image\n            color = (255,0,0)\n            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n            text = \"{}\".format(LABELS[0], confidences[i])\n            cv2.putText(image, text, (x +15, y - 10), cv2.FONT_HERSHEY_SIMPLEX,1, color, 2)\n\n    return image, boxes\n","execution_count":null,"outputs":[]},{"metadata":{"id":"pTGfObNYXCm5","outputId":"218cb2f6-23a4-4a81-aaa9-e75b5bf7f381","trusted":true},"cell_type":"code","source":"# Execute prediction on a single image\nimg = cv2.imread(\"../input/global-wheat-detection/test/2fd875eaa.jpg\")\nimg = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nimage, boxes = predict(img)\ndisplay_img(image)","execution_count":null,"outputs":[]},{"metadata":{"id":"5eXoUnv1bDhH","trusted":true},"cell_type":"code","source":"# Create the function which predict the frame input\ndef predict(image):\n    \n    # initialize a list of colors to represent each possible class label\n    np.random.seed(42)\n    COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype=\"uint8\")\n    (H, W) = image.shape[:2]\n    \n    # determine only the \"ouput\" layers name which we need from YOLO\n    ln = net.getLayerNames()\n    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n    \n    # construct a blob from the input image and then perform a forward pass of the YOLO object detector, \n    # giving us our bounding boxes and associated probabilities\n    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n    net.setInput(blob)\n    layerOutputs = net.forward(ln)\n    \n    boxes = []\n    confidences = []\n    classIDs = []\n    threshold = 0.2\n    \n    # loop over each of the layer outputs\n    for output in layerOutputs:\n        # loop over each of the detections\n        for detection in output:\n            # extract the class ID and confidence (i.e., probability) of\n            # the current object detection\n            scores = detection[5:]\n            classID = np.argmax(scores)\n            confidence = scores[classID]\n\n            # filter out weak predictions by ensuring the detected\n            # probability is greater than the minimum probability\n            # confidence type=float, default=0.5\n            if confidence > threshold:\n                # scale the bounding box coordinates back relative to the\n                # size of the image, keeping in mind that YOLO actually\n                # returns the center (x, y)-coordinates of the bounding\n                # box followed by the boxes' width and height\n                box = detection[0:4] * np.array([W, H, W, H])\n                (centerX, centerY, width, height) = box.astype(\"int\")\n\n                # use the center (x, y)-coordinates to derive the top and\n                # and left corner of the bounding box\n                x = int(centerX - (width / 2))\n                y = int(centerY - (height / 2))\n\n                # update our list of bounding box coordinates, confidences,\n                # and class IDs\n                boxes.append([x, y, int(width), int(height)])\n                confidences.append(float(confidence))\n                classIDs.append(classID)\n\n    # apply non-maxima suppression to suppress weak, overlapping bounding boxes\n    idxs = cv2.dnn.NMSBoxes(boxes, confidences, threshold, 0.1)\n\n    # ensure at least one detection exists\n    if len(idxs) > 0:\n        # loop over the indexes we are keeping\n        for i in idxs.flatten():\n            # extract the bounding box coordinates\n            (x, y) = (boxes[i][0], boxes[i][1])\n            (w, h) = (boxes[i][2], boxes[i][3])\n\n            # draw a bounding box rectangle and label on the image\n            color = (255,0,0)\n            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n            #text = \"{}\".format(LABELS[classIDs[i]], confidences[i])\n            #cv2.putText(image, text, (x +15, y - 10), cv2.FONT_HERSHEY_SIMPLEX,1, color, 2)\n\n            x1 = max(0,x)\n            y1 = max(0,y)\n            w1 = max(0,w)\n            h1 = max(0,h)\n            new_img = image[y1:y1+h1,x1:x1+w1]\n            new_img = cv2.resize(new_img, (100,100))\n            new_img = new_img.reshape(1,100,100,3)\n            pred = model.predict(new_img)\n            ids = np.argmax(pred, axis=1)\n            if ids == 0:\n                text = ('arvalis_1')\n            elif ids == 1:\n                text = ('arvalis_2')\n            elif ids == 2:\n                text = ('arvalis_3')\n            elif ids == 3:\n                text = ('ethz_1')\n            elif ids == 4:\n                text = ('inrae_1')\n            elif ids == 5:\n                text = ('rres_1') \n            else:\n                text = ('usask_1')\n            cv2.putText(image, text, (x +15, y - 10), cv2.FONT_HERSHEY_SIMPLEX,1, (255,0,0), 2)\n    return image\n","execution_count":null,"outputs":[]},{"metadata":{"id":"G7iIhbyZYYZ-","outputId":"97249b53-aecd-428d-fc2d-b3e25af20e50","trusted":true},"cell_type":"code","source":"# Execute prediction on a single image\nimg = cv2.imread(\"../input/global-wheat-detection/test/796707dd7.jpg\")\nimg = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nimage  = predict(img)\ndisplay_img(image)","execution_count":null,"outputs":[]},{"metadata":{"id":"Gp7y9JYRXSKG","trusted":true},"cell_type":"code","source":"# Create the function which predict the frame input\ndef predict_list(image):\n    pred_string =[]\n    \n    # initialize a list of colors to represent each possible class label\n    np.random.seed(42)\n    COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype=\"uint8\")\n    (H, W) = image.shape[:2]\n    \n    # determine only the \"ouput\" layers name which we need from YOLO\n    ln = net.getLayerNames()\n    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n    \n    # construct a blob from the input image and then perform a forward pass of the YOLO object detector, \n    # giving us our bounding boxes and associated probabilities\n    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n    net.setInput(blob)\n    layerOutputs = net.forward(ln)\n    \n    boxes = []\n    confidences = []\n    classIDs = []\n    threshold = 0.2\n    \n    # loop over each of the layer outputs\n    for output in layerOutputs:\n        # loop over each of the detections\n        for detection in output:\n            # extract the class ID and confidence (i.e., probability) of\n            # the current object detection\n            scores = detection[5:]\n            classID = np.argmax(scores)\n            confidence = scores[classID]\n\n            # filter out weak predictions by ensuring the detected\n            # probability is greater than the minimum probability\n            # confidence type=float, default=0.5\n            if confidence > threshold:\n                # scale the bounding box coordinates back relative to the\n                # size of the image, keeping in mind that YOLO actually\n                # returns the center (x, y)-coordinates of the bounding\n                # box followed by the boxes' width and height\n                box = detection[0:4] * np.array([W, H, W, H])\n                (centerX, centerY, width, height) = box.astype(\"int\")\n\n                # use the center (x, y)-coordinates to derive the top and\n                # and left corner of the bounding box\n                x = int(centerX - (width / 2))\n                y = int(centerY - (height / 2))\n\n                # update our list of bounding box coordinates, confidences,\n                # and class IDs\n                boxes.append([x, y, int(width), int(height)])\n                confidences.append(float(confidence))\n                classIDs.append(classID)\n\n    # apply non-maxima suppression to suppress weak, overlapping bounding boxes\n    idxs = cv2.dnn.NMSBoxes(boxes, confidences, threshold, 0.1)\n\n    # ensure at least one detection exists\n    if len(idxs) > 0:\n        # loop over the indexes we are keeping\n        for i in idxs.flatten():\n            # extract the bounding box coordinates\n            (x, y) = (boxes[i][0], boxes[i][1])\n            (w, h) = (boxes[i][2], boxes[i][3])\n\n            # draw a bounding box rectangle and label on the image\n            color = (255,0,0)\n            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n            #text = \"{}\".format(LABELS[classIDs[i]], confidences[i])\n            #cv2.putText(image, text, (x +15, y - 10), cv2.FONT_HERSHEY_SIMPLEX,1, color, 2)\n\n            x1 = max(0,x)\n            y1 = max(0,y)\n            w1 = max(0,w)\n            h1 = max(0,h)\n            new_img = image[y1:y1+h1,x1:x1+w1]\n            new_img = cv2.resize(new_img, (100,100))\n            new_img = new_img.reshape(1,100,100,3)\n            pred = model.predict(new_img)\n            ids = np.argmax(pred, axis=1)\n            if ids == 0:\n                text = ('arvalis_1')\n            elif ids == 1:\n                text = ('arvalis_2')\n            elif ids == 2:\n                text = ('arvalis_3')\n            elif ids == 3:\n                text = ('ethz_1')\n            elif ids == 4:\n                text = ('inrae_1')\n            elif ids == 5:\n                text = ('rres_1') \n            else:\n                text = ('usask_1')\n                \n                \n            str_id = ids[0]\n            list_pred=[]\n            list_pred.append(f\"{str_id} {x1} {y1} {h1} {w1}\")\n            pred_string.append(list_pred)\n             \n    return pred_string\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Execute prediction on a single image\nimg = cv2.imread(\"../input/global-wheat-detection/train/00b70a919.jpg\")\nimg = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nlist_1 = predict_list(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from os import listdir\nfrom os.path import isfile, join\nimage_path = '../input/global-wheat-detection/test/'\nonlyfiles = [f for f in listdir(image_path) if isfile(join(image_path, f))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/global-wheat-detection/sample_submission.csv')\nsubmission = submission[0:0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_id = []\nPredictionString =[]\nfor i in onlyfiles:\n    path = (image_path+i)\n    img = cv2.imread(path)\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    list_1 = predict_list(img)\n    res = [''.join(ele) for ele in list_1]\n    res = ' '.join(res)\n    image_id.append(i.strip('.jpg'))\n    PredictionString.append(res)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['image_id'] = image_id\nsubmission['PredictionString'] = PredictionString","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}