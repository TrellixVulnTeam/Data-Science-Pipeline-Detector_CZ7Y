{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"training_file='/kaggle/input/global-wheat-detection/train.csv'\ntraining_folder='/kaggle/input/global-wheat-detection/train/'\n\ntraining_labels = pd.read_csv(training_file)\ntraining_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_ids = training_labels.drop_duplicates(subset='image_id')\nimage_ids.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_ids.groupby(['source']).size().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not the best distribution of sources, but currently unknown how much of an effect that has to play."},{"metadata":{"trusted":true},"cell_type":"code","source":"image_ids.groupby(['width']).size().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_ids.groupby(['height']).size().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears that all images are 1024X1024, or at least that is the case for all the images with wheat in the training subset."},{"metadata":{"trusted":true},"cell_type":"code","source":"images_with_wheat = image_ids['image_id'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\ntraining_folder_image_list = [f for f in glob.glob(training_folder+\"*.jpg\")]\ntotal_images = len(training_folder_image_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percent = images_with_wheat / total_images * 100\nprint(\"Percentage of images with wheat: {}\".format(percent))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the data description states, not all images have bounding boxes labeled and each row in train.csv corresponds to a bounding box. That means there are some, ~1.5%, files in the folder without being listed in train.csv."},{"metadata":{"trusted":true},"cell_type":"code","source":"training_labels['image_id'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_labels['image_id'].value_counts().plot(kind='line')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since each row corresponds to one bounding box, looking at the number of times each image id is referenced gives us the number of bboxes for each image, ranging from 0 to 116 and appearing to be fairly distributed."},{"metadata":{"trusted":true},"cell_type":"code","source":"arvalis_1 = training_labels[training_labels['source'] == 'arvalis_1']\narvalis_2 = training_labels[training_labels['source'] == 'arvalis_2']\narvalis_3 = training_labels[training_labels['source'] == 'arvalis_3']\nethz_1 = training_labels[training_labels['source'] == 'ethz_1']\ninrae_1 = training_labels[training_labels['source'] == 'inrae_1']\nrres_1 = training_labels[training_labels['source'] == 'rres_1']\nusask_1 = training_labels[training_labels['source'] == 'usask_1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig1 = plt.figure()\nax1 = fig1.add_subplot(111)\n\nax1.boxplot([arvalis_1['image_id'].value_counts(),\n             arvalis_2['image_id'].value_counts(),\n             arvalis_3['image_id'].value_counts(),\n             ethz_1['image_id'].value_counts(),\n             inrae_1['image_id'].value_counts(),\n             usask_1['image_id'].value_counts(),\n             rres_1['image_id'].value_counts(),])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears that the numbers of bounding boxes in images per source are not as uniformly distributed as I had originally hoped for with ethz_1 reporting significantly more bounding boxes per image."},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nwheat_image = Image(training_folder+training_labels['image_id'][0]+\".jpg\")\nwheat_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wheat_image = Image(training_folder+training_labels['image_id'][200]+\".jpg\")\nwheat_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_labels_full_path = [training_folder+image+\".jpg\" for image in training_labels[\"image_id\"]]\nnon_wheat_images = []\nfor image in training_folder_image_list:\n    if not image in training_labels_full_path:\n        non_wheat_images.append(image)\nnon_wheat_image = Image(non_wheat_images[0])\nnon_wheat_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"non_wheat_image2 = Image(non_wheat_images[1])\nnon_wheat_image2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"non_wheat_image3 = Image(non_wheat_images[30])\nnon_wheat_image3","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}