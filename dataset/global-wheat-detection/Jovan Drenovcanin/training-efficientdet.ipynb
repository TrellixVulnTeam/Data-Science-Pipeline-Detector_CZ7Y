{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --no-deps '../input/timm-package/timm-0.1.26-py3-none-any.whl' > /dev/null\n!pip install --no-deps '../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' > /dev/null","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-28T10:34:49.7687Z","iopub.execute_input":"2022-01-28T10:34:49.769055Z","iopub.status.idle":"2022-01-28T10:34:55.707657Z","shell.execute_reply.started":"2022-01-28T10:34:49.769014Z","shell.execute_reply":"2022-01-28T10:34:55.706788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.insert(0, \"../input/timm-efficientdet-pytorch\")\nsys.path.insert(0, \"../input/omegaconf\")\n\nimport torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom glob import glob\n\nimport glob\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\nimport cv2\n%matplotlib inline\n\n\n\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-28T10:34:55.710909Z","iopub.execute_input":"2022-01-28T10:34:55.711386Z","iopub.status.idle":"2022-01-28T10:34:58.794087Z","shell.execute_reply.started":"2022-01-28T10:34:55.711353Z","shell.execute_reply":"2022-01-28T10:34:58.793175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\n\n# Get image as numpy array\ndef load_image(name, path):\n    img_path =os.path.join( path , name + '.png')\n#     print(img_path)\n    img = cv2.imread(img_path)\n    return img\n\n# Plot numpy array\ndef plot_image(img):\n    plt.figure(figsize = (10,10)) #figsize = (20,20)\n    plt.imshow(img)\n    plt.title(img.shape)\n    \n# Plot a grid of examples\ndef plot_grid(img_names, img_root, rows=5, cols=5):\n    fig = plt.figure(figsize=(25,25))\n    \n    for i,name in enumerate(img_names):\n        fig.add_subplot(rows,cols,i+1)\n        img = load_image(name, img_root)\n        plot_image(img)\n        \n    plt.show()\n\ndef class_to_color(class_id):\n    colors = [(255,0,0),(0,255,0),(0,0,255),\n              (100,255,100)]\n    return colors[class_id]\n\n\ndef draw_bounding_box(img, annotation):\n    if annotation.isnull().values.any():\n        return\n    \n    x_min, y_min = int(annotation['xmin']), int(annotation['ymin'])\n    x_max, y_max = int(annotation['xmax']), int(annotation['ymax'])\n    \n    class_id = int(annotation['class_id'])\n    color = class_to_color(class_id)\n    \n    cv2.rectangle(img,(x_min,y_min),(x_max,y_max), color, 2)   #.permute(1,2,0).cpu().numpy()\n    \n# draw all annotation bounding boxes on an image\ndef annotate_image(img, name, all_annotations):\n    annotations = all_annotations[all_annotations['image_id'] == name]\n    for index, row in annotations.iterrows():\n        draw_bounding_box(img, row)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:34:58.796391Z","iopub.execute_input":"2022-01-28T10:34:58.797017Z","iopub.status.idle":"2022-01-28T10:34:58.813895Z","shell.execute_reply.started":"2022-01-28T10:34:58.796977Z","shell.execute_reply":"2022-01-28T10:34:58.812908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 42","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:34:58.815161Z","iopub.execute_input":"2022-01-28T10:34:58.815447Z","iopub.status.idle":"2022-01-28T10:34:58.827866Z","shell.execute_reply.started":"2022-01-28T10:34:58.815397Z","shell.execute_reply":"2022-01-28T10:34:58.826951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !ls ../input/cotton-data/cloud-object-storage-tw-cos-standard-tqh\n# pd.Series(dff['file_name'].unique())\n# dff['file_name'].unique()[:30]\n\nseed_everything(SEED)\n\n\ndir_path = \"../input/cotton-data/cloud-object-storage-tw-cos-standard-tqh\" \ndf = {\"file_name\":[],\"width\":[],\"height\":[],\"depth\":[],\"label\":[],\"pose\":[],\"truncated\":[],\"difficult\":[],\"xmin\":[],\"ymin\":[],\"xmax\":[],\"ymax\":[]}\nfor path in tqdm(glob.glob(dir_path +\"/*.xml\")):\n    with open(path, 'r') as f:\n        data = f.read()\n    Bs_data = BeautifulSoup(data, \"xml\")\n    objects = Bs_data.find_all('object')\n    for obj in objects:\n        df[\"width\"].append(Bs_data.find('size').find('width').contents[0])\n        df[\"height\"].append(Bs_data.find('size').find('height').contents[0])\n        df[\"depth\"].append(Bs_data.find('size').find('depth').contents[0])\n        \n        df[\"file_name\"].append(os.path.basename(path).split(\".\")[0])\n        df[\"label\"].append(obj.find('name').contents[0])\n        df[\"pose\"].append(obj.find('pose').contents[0])\n        df[\"truncated\"].append(obj.find('truncated').contents[0])\n        df[\"difficult\"].append(obj.find('difficult').contents[0])\n        df[\"xmin\"].append(obj.find('bndbox').find('xmin').contents[0])\n        df[\"ymin\"].append(obj.find('bndbox').find('ymin').contents[0])\n        df[\"xmax\"].append(obj.find('bndbox').find('xmax').contents[0])\n        df[\"ymax\"].append(obj.find('bndbox').find('ymax').contents[0])\n \ndff = pd.DataFrame.from_dict(df)\ndff = dff[dff['label']!=\"Hand\"]\ndff = dff[dff['label']!=\"Untitled Label\"]\n\ndff = dff[['file_name', 'width', 'height', 'label',\n        'xmin', 'ymin', 'xmax', 'ymax']]\n\ndff['class_id'] = 0\ndff['class_id'][dff['label']==\"Tray\"]=1\ndff['class_id'][dff['label']==\"Plastic\"]=2\n\n\ndff.rename(columns={'file_name':'image_id','label':'source'}, inplace=True)\n\ndff[\"x\"]=(1024*(dff[\"xmin\"].astype(int)/320)).astype(int)\ndff[\"y\"]=(1024*(dff[\"ymin\"].astype(int)/240)).astype(int)\ndff[\"w\"]=(1024*(dff[\"xmax\"].astype(int)/320))-dff[\"x\"]\ndff[\"h\"]=(1024*(dff[\"ymax\"].astype(int)/240))-dff[\"y\"]\n\ndff[\"x\"]=dff[\"x\"].astype(int)\ndff[\"y\"]=dff[\"y\"].astype(int)\ndff[\"w\"]=dff[\"w\"].astype(int)\ndff[\"h\"]=dff[\"h\"].astype(int)\n\ndff['width']=1024\ndff['height']=1024\n\nmarking = dff[['image_id', 'width', 'height', 'source', 'x', 'y', 'w', 'h']]","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:34:58.832694Z","iopub.execute_input":"2022-01-28T10:34:58.833056Z","iopub.status.idle":"2022-01-28T10:35:07.257901Z","shell.execute_reply.started":"2022-01-28T10:34:58.833023Z","shell.execute_reply":"2022-01-28T10:35:07.256186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:07.261953Z","iopub.execute_input":"2022-01-28T10:35:07.262214Z","iopub.status.idle":"2022-01-28T10:35:07.292945Z","shell.execute_reply.started":"2022-01-28T10:35:07.262188Z","shell.execute_reply":"2022-01-28T10:35:07.292254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef plot_grid(dff, img_root, rows=2, cols=2):\n    img_names = pd.Series(dff['image_id'].unique())\n    fig, axs = plt.subplots(rows, cols, figsize=(32,32))\n    for row in range(rows):\n        for col in range(cols):\n            idx = np.random.randint(len(img_names), size=1)[0]\n            name = dff.iloc[idx].image_id\n            img = load_image(name, img_root)\n            annotate_image(img, name, dff)\n            axs[row, col].imshow(img)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:51:20.563095Z","iopub.execute_input":"2022-01-28T10:51:20.56344Z","iopub.status.idle":"2022-01-28T10:51:20.573883Z","shell.execute_reply.started":"2022-01-28T10:51:20.563391Z","shell.execute_reply":"2022-01-28T10:51:20.57293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_ROOT = \"../input/cotton-data/cloud-object-storage-tw-cos-standard-tqh\"\nplot_grid(dff, IMG_ROOT)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:51:21.936399Z","iopub.execute_input":"2022-01-28T10:51:21.936783Z","iopub.status.idle":"2022-01-28T10:51:23.534398Z","shell.execute_reply.started":"2022-01-28T10:51:21.936752Z","shell.execute_reply":"2022-01-28T10:51:23.53349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#######   convert to jpg\n\ndir_path = \"../input/cotton-data/cloud-object-storage-tw-cos-standard-tqh\"\n# image_name = \"img_1264.png\"\ndestination_dir_path= \"/kaggle/working/train\"\n\n!mkdir train\n\nwidth = 1024\nheight = 1024\ndim = (width, height)\n\nfor path in tqdm(glob.glob(dir_path +\"/*.png\")):\n    image_name = os.path.basename(path).split(\".\")[0]\n    img = cv2.imread(os.path.join(dir_path,image_name+\".png\"), cv2.IMREAD_UNCHANGED)\n    resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n    cv2.imwrite(os.path.join(destination_dir_path,image_name+\".jpg\"), resized, [int(cv2.IMWRITE_JPEG_QUALITY), 100])","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:08.860483Z","iopub.execute_input":"2022-01-28T10:35:08.860799Z","iopub.status.idle":"2022-01-28T10:35:20.872358Z","shell.execute_reply.started":"2022-01-28T10:35:08.860767Z","shell.execute_reply":"2022-01-28T10:35:20.871507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\ndf_folds = marking[['image_id']].copy()\ndf_folds.loc[:, 'bbox_count'] = 1\ndf_folds = df_folds.groupby('image_id').count()\ndf_folds.loc[:, 'source'] = marking[['image_id', 'source']].groupby('image_id').min()['source']\ndf_folds.loc[:, 'stratify_group'] = np.char.add(\n    df_folds['source'].values.astype(str),\n    df_folds['bbox_count'].apply(lambda x: f'_{x // 15}').values.astype(str)\n)\ndf_folds.loc[:, 'fold'] = 0\n\nfor fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-28T10:35:20.873902Z","iopub.execute_input":"2022-01-28T10:35:20.874493Z","iopub.status.idle":"2022-01-28T10:35:20.981025Z","shell.execute_reply.started":"2022-01-28T10:35:20.874451Z","shell.execute_reply":"2022-01-28T10:35:20.980135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"marking","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:20.98235Z","iopub.execute_input":"2022-01-28T10:35:20.982906Z","iopub.status.idle":"2022-01-28T10:35:21.001396Z","shell.execute_reply.started":"2022-01-28T10:35:20.982866Z","shell.execute_reply":"2022-01-28T10:35:21.000427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"marking.source.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:21.002809Z","iopub.execute_input":"2022-01-28T10:35:21.003268Z","iopub.status.idle":"2022-01-28T10:35:21.018389Z","shell.execute_reply.started":"2022-01-28T10:35:21.003227Z","shell.execute_reply":"2022-01-28T10:35:21.017187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_folds.source.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:21.020371Z","iopub.execute_input":"2022-01-28T10:35:21.020905Z","iopub.status.idle":"2022-01-28T10:35:21.031186Z","shell.execute_reply.started":"2022-01-28T10:35:21.020865Z","shell.execute_reply":"2022-01-28T10:35:21.029952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_folds","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:21.032819Z","iopub.execute_input":"2022-01-28T10:35:21.033245Z","iopub.status.idle":"2022-01-28T10:35:21.051339Z","shell.execute_reply.started":"2022-01-28T10:35:21.033208Z","shell.execute_reply":"2022-01-28T10:35:21.050604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_transforms():\n    return A.Compose(\n        [\n            A.RandomSizedCrop(min_max_height=(800, 800), height=1024, width=1024, p=0.3),   #0.5\n            A.OneOf([\n                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n                                     val_shift_limit=0.2, p=0.3),  #0.9\n                A.RandomBrightnessContrast(brightness_limit=0.2, \n                                           contrast_limit=0.2, p=0.3),   #p=0.9\n            ],p=0.5),  #p=0.9\n            A.ToGray(p=0.01),\n            A.HorizontalFlip(p=0.5),  #p=0.5\n            A.VerticalFlip(p=0.5),\n            A.Resize(height=512, width=512, p=1),\n            A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, fill_value=0, p=0.3),    # max_h_size=64, max_w_size=64, fill_value=0, p=0.5\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='pascal_voc',\n            min_area=0, \n            min_visibility=0,\n            label_fields=['labels']\n        )\n    )\n\ndef get_valid_transforms():\n    return A.Compose(\n        [\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='pascal_voc',\n            min_area=0, \n            min_visibility=0,\n            label_fields=['labels']\n        )\n    )","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:21.054304Z","iopub.execute_input":"2022-01-28T10:35:21.054674Z","iopub.status.idle":"2022-01-28T10:35:21.068318Z","shell.execute_reply.started":"2022-01-28T10:35:21.054648Z","shell.execute_reply":"2022-01-28T10:35:21.067378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmarking.loc[:, 'label'] = 0\nmarking.loc[marking[\"source\"]==\"Cotton\", 'label'] = 1\nmarking.loc[marking[\"source\"]==\"Tray\", 'label'] = 2\nmarking.loc[marking[\"source\"]==\"Plastic\", 'label'] = 3","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:21.069678Z","iopub.execute_input":"2022-01-28T10:35:21.070138Z","iopub.status.idle":"2022-01-28T10:35:21.095591Z","shell.execute_reply.started":"2022-01-28T10:35:21.070093Z","shell.execute_reply":"2022-01-28T10:35:21.094633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nTRAIN_ROOT_PATH =\"/kaggle/working/train\"\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, marking, image_ids, transforms=None, test=False):\n        super().__init__()\n\n        self.image_ids = image_ids\n        self.marking = marking\n        self.transforms = transforms\n        self.test = test\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        \n#         if self.test or random.random() > 0.5:\n#             image, boxes = self.load_image_and_boxes(index)\n#         else:\n#             image, boxes = self.load_cutmix_image_and_boxes(index)\n        \n        image, boxes,labels = self.load_image_and_boxes(index)\n        # there is only one class\n#         labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = torch.from_numpy(labels)\n        target['image_id'] = torch.tensor([index])\n\n        if self.transforms:\n            for i in range(10):\n                sample = self.transforms(**{\n                    'image': image,\n                    'bboxes': target['boxes'],\n                    'labels': labels\n                })\n                if len(sample['bboxes']) > 0:\n                    image = sample['image']\n                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n                    break\n\n        return image, target, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n\n    def load_image_and_boxes(self, index):\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{TRAIN_ROOT_PATH}/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        records = self.marking[self.marking['image_id'] == image_id]\n        boxes = records[['x', 'y', 'w', 'h']].values\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n        labels = records['label'].values\n        \n        return image, boxes,labels\n\n    def load_cutmix_image_and_boxes(self, index, imsize=1024):\n        \"\"\" \n        This implementation of cutmix author:  https://www.kaggle.com/nvnnghia \n        Refactoring and adaptation: https://www.kaggle.com/shonenkov\n        \"\"\"\n        w, h = imsize, imsize\n        s = imsize // 2\n    \n        xc, yc = [int(random.uniform(imsize * 0.25, imsize * 0.75)) for _ in range(2)]  # center x, y\n        indexes = [index] + [random.randint(0, self.image_ids.shape[0] - 1) for _ in range(3)]\n\n        result_image = np.full((imsize, imsize, 3), 1, dtype=np.float32)\n        result_boxes = []\n\n        for i, index in enumerate(indexes):\n            image, boxes = self.load_image_and_boxes(index)\n            if i == 0:\n                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n            elif i == 1:  # top right\n                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n            elif i == 2:  # bottom left\n                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n            elif i == 3:  # bottom right\n                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n            result_image[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n            padw = x1a - x1b\n            padh = y1a - y1b\n\n            boxes[:, 0] += padw\n            boxes[:, 1] += padh\n            boxes[:, 2] += padw\n            boxes[:, 3] += padh\n\n            result_boxes.append(boxes)\n\n        result_boxes = np.concatenate(result_boxes, 0)\n        np.clip(result_boxes[:, 0:], 0, 2 * s, out=result_boxes[:, 0:])\n        result_boxes = result_boxes.astype(np.int32)\n        result_boxes = result_boxes[np.where((result_boxes[:,2]-result_boxes[:,0])*(result_boxes[:,3]-result_boxes[:,1]) > 0)]\n        return result_image, result_boxes","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:21.097613Z","iopub.execute_input":"2022-01-28T10:35:21.097977Z","iopub.status.idle":"2022-01-28T10:35:21.133731Z","shell.execute_reply.started":"2022-01-28T10:35:21.097941Z","shell.execute_reply":"2022-01-28T10:35:21.132673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold_number = 0\n\ntrain_dataset = DatasetRetriever(\n    image_ids=df_folds[df_folds['fold'] != fold_number].index.values,\n    marking=marking,\n    transforms=get_train_transforms(),\n    test=False,\n)\n\nvalidation_dataset = DatasetRetriever(\n    image_ids=df_folds[df_folds['fold'] == fold_number].index.values,\n    marking=marking,\n    transforms=get_valid_transforms(),\n    test=True,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:21.136844Z","iopub.execute_input":"2022-01-28T10:35:21.137218Z","iopub.status.idle":"2022-01-28T10:35:21.147664Z","shell.execute_reply.started":"2022-01-28T10:35:21.137178Z","shell.execute_reply":"2022-01-28T10:35:21.146797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image, target, image_id = train_dataset[1]\nboxes = target['boxes'].cpu().numpy().astype(np.int32)\n\nnumpy_image = image.permute(1,2,0).cpu().numpy()\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(numpy_image, (box[1], box[0]), (box[3],  box[2]), (0, 1, 0), 2)\n    \nax.set_axis_off()\nax.imshow(numpy_image);","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:21.150852Z","iopub.execute_input":"2022-01-28T10:35:21.151156Z","iopub.status.idle":"2022-01-28T10:35:21.406209Z","shell.execute_reply.started":"2022-01-28T10:35:21.151132Z","shell.execute_reply":"2022-01-28T10:35:21.405369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        \n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:21.407615Z","iopub.execute_input":"2022-01-28T10:35:21.408156Z","iopub.status.idle":"2022-01-28T10:35:21.416865Z","shell.execute_reply.started":"2022-01-28T10:35:21.408117Z","shell.execute_reply":"2022-01-28T10:35:21.415948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nclass Fitter:\n    \n    def __init__(self, model, device, config):\n        self.config = config\n        self.epoch = 0\n\n        self.base_dir = f'./{config.folder}'\n        if not os.path.exists(self.base_dir):\n            os.makedirs(self.base_dir)\n        \n        self.log_path = f'{self.base_dir}/log.txt'\n        self.best_summary_loss = 10**5\n\n        self.model = model\n        self.device = device\n\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ] \n\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n        self.log(f'Fitter prepared. Device is {self.device}')\n\n    def fit(self, train_loader, validation_loader):\n        for e in range(self.config.n_epochs):\n            if self.config.verbose:\n                lr = self.optimizer.param_groups[0]['lr']\n                timestamp = datetime.utcnow().isoformat()\n                self.log(f'\\n{timestamp}\\nLR: {lr}')\n\n            t = time.time()\n            summary_loss = self.train_one_epoch(train_loader)\n\n            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n            self.save(f'{self.base_dir}/last-checkpoint.bin')\n\n            t = time.time()\n            summary_loss = self.validation(validation_loader)\n\n            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n            if summary_loss.avg < self.best_summary_loss:\n                self.best_summary_loss = summary_loss.avg\n                self.model.eval()\n                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n#                 for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n#                     os.remove(path)\n\n            if self.config.validation_scheduler:\n                self.scheduler.step(metrics=summary_loss.avg)\n\n            self.epoch += 1\n\n    def validation(self, val_loader):\n        self.model.eval()\n        summary_loss = AverageMeter()\n        t = time.time()\n        for step, (images, targets, image_ids) in enumerate(val_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Val Step {step}/{len(val_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            with torch.no_grad():\n                images = torch.stack(images)\n                batch_size = images.shape[0]\n                images = images.to(self.device).float()\n                boxes = [target['boxes'].to(self.device).float() for target in targets]\n                labels = [target['labels'].to(self.device).float() for target in targets]\n\n                loss, _, _ = self.model(images, boxes, labels)\n                summary_loss.update(loss.detach().item(), batch_size)\n\n        return summary_loss\n\n    def train_one_epoch(self, train_loader):\n        self.model.train()\n        summary_loss = AverageMeter()\n        t = time.time()\n        for step, (images, targets, image_ids) in enumerate(train_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Train Step {step}/{len(train_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            \n            images = torch.stack(images)\n            images = images.to(self.device).float()\n            batch_size = images.shape[0]\n            boxes = [target['boxes'].to(self.device).float() for target in targets]\n            labels = [target['labels'].to(self.device).float() for target in targets]\n\n            self.optimizer.zero_grad()\n            \n            loss, _, _ = self.model(images, boxes, labels)\n            \n            loss.backward()\n\n            summary_loss.update(loss.detach().item(), batch_size)\n\n            self.optimizer.step()\n\n            if self.config.step_scheduler:\n                self.scheduler.step()\n\n        return summary_loss\n    \n    def save(self, path):\n        self.model.eval()\n        torch.save({\n            'model_state_dict': self.model.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_summary_loss': self.best_summary_loss,\n            'epoch': self.epoch,\n        }, path)\n\n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_summary_loss = checkpoint['best_summary_loss']\n        self.epoch = checkpoint['epoch'] + 1\n        \n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:21.420091Z","iopub.execute_input":"2022-01-28T10:35:21.420354Z","iopub.status.idle":"2022-01-28T10:35:21.459859Z","shell.execute_reply.started":"2022-01-28T10:35:21.42033Z","shell.execute_reply":"2022-01-28T10:35:21.458936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainGlobalConfig:\n    num_workers = 2\n    batch_size = 8      #4   \n    n_epochs = 80 # n_epochs = 40\n    lr = 0.005\n\n    folder = 'effdet5-cutmix-augmix'\n\n    # -------------------\n    verbose = True\n    verbose_step = 1\n    # -------------------\n\n    # --------------------\n    step_scheduler = False  # do scheduler.step after optimizer.step\n    validation_scheduler = True  # do scheduler.step after validation stage loss\n\n#     SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n#     scheduler_params = dict(\n#         max_lr=0.001,\n#         epochs=n_epochs,\n#         steps_per_epoch=int(len(train_dataset) / batch_size),\n#         pct_start=0.1,\n#         anneal_strategy='cos', \n#         final_div_factor=10**5\n#     )\n    \n    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n    scheduler_params = dict(\n        mode='min',\n        factor=0.5,\n        patience=1,\n        verbose=False, \n        threshold=0.0001,\n        threshold_mode='abs',\n        cooldown=0, \n        min_lr=1e-8,\n        eps=1e-08\n    )\n    # --------------------","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:21.463243Z","iopub.execute_input":"2022-01-28T10:35:21.463538Z","iopub.status.idle":"2022-01-28T10:35:21.473183Z","shell.execute_reply.started":"2022-01-28T10:35:21.463502Z","shell.execute_reply":"2022-01-28T10:35:21.472243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=TrainGlobalConfig.batch_size,\n    sampler=RandomSampler(train_dataset),\n    pin_memory=False,\n    drop_last=True,\n    num_workers=TrainGlobalConfig.num_workers,\n    collate_fn=collate_fn,\n)\nval_loader = torch.utils.data.DataLoader(\n    validation_dataset, \n    batch_size=TrainGlobalConfig.batch_size,\n    num_workers=TrainGlobalConfig.num_workers,\n    shuffle=False,\n    sampler=SequentialSampler(validation_dataset),\n    pin_memory=False,\n    collate_fn=collate_fn,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:21.474877Z","iopub.execute_input":"2022-01-28T10:35:21.475778Z","iopub.status.idle":"2022-01-28T10:35:21.484257Z","shell.execute_reply.started":"2022-01-28T10:35:21.475711Z","shell.execute_reply":"2022-01-28T10:35:21.483491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef run_training():\n    device = torch.device('cuda:0')\n    net.to(device)\n\n\n\n    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n    fitter.fit(train_loader, val_loader)\n    return fitter.model","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:21.485774Z","iopub.execute_input":"2022-01-28T10:35:21.486353Z","iopub.status.idle":"2022-01-28T10:35:21.493026Z","shell.execute_reply.started":"2022-01-28T10:35:21.486313Z","shell.execute_reply":"2022-01-28T10:35:21.492246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\nfrom effdet.efficientdet import HeadNet\n\ndef get_net():\n    config = get_efficientdet_config('tf_efficientdet_d0')  #tf_efficientdet_d5  \n    net = EfficientDet(config, pretrained_backbone=True)\n#     checkpoint = torch.load('../input/efficientdet/efficientdet_d5-ef44aea8.pth')\n#     net.load_state_dict(checkpoint)\n    config.num_classes = 3\n    config.image_size = 512\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n    return DetBenchTrain(net, config)\n\nnet = get_net()","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:21.494531Z","iopub.execute_input":"2022-01-28T10:35:21.495094Z","iopub.status.idle":"2022-01-28T10:35:23.658012Z","shell.execute_reply.started":"2022-01-28T10:35:21.494998Z","shell.execute_reply":"2022-01-28T10:35:23.657136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls effdet5-cutmix-augmix","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:23.659449Z","iopub.execute_input":"2022-01-28T10:35:23.659812Z","iopub.status.idle":"2022-01-28T10:35:24.322959Z","shell.execute_reply.started":"2022-01-28T10:35:23.659775Z","shell.execute_reply":"2022-01-28T10:35:24.322097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = run_training()","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:35:24.326619Z","iopub.execute_input":"2022-01-28T10:35:24.326909Z","iopub.status.idle":"2022-01-28T10:48:42.859378Z","shell.execute_reply.started":"2022-01-28T10:35:24.326881Z","shell.execute_reply":"2022-01-28T10:48:42.85643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_valid_transforms():\n    return A.Compose([\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:47.188108Z","iopub.execute_input":"2022-01-28T10:48:47.188478Z","iopub.status.idle":"2022-01-28T10:48:47.194257Z","shell.execute_reply.started":"2022-01-28T10:48:47.188443Z","shell.execute_reply":"2022-01-28T10:48:47.193172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validation_dataset.image_ids","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:42.862084Z","iopub.status.idle":"2022-01-28T10:48:42.863819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # validation_dataset.image_ids\n\n# temp_list_of_ids = ['img_7094', 'img_7284', 'img_7186', 'img_1468', 'img_7287',\n#        'img_1747', 'img_1477', 'img_7113', 'img_1811', 'img_1873',\n#        'img_1861', 'img_1821', 'img_1626', 'img_1628', 'img_7145',\n#        'img_7107', 'img_1820', 'img_1487', 'img_1458', 'img_1451']","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:42.865142Z","iopub.status.idle":"2022-01-28T10:48:42.866023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DATA_ROOT_PATH = '../input/global-wheatl-detection/test'\nDATA_ROOT_PATH = \"/kaggle/working/train\"\n\n\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, image_ids, transforms=None):\n        super().__init__()\n        self.image_ids = image_ids\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{DATA_ROOT_PATH}/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n        return image, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:48.320394Z","iopub.execute_input":"2022-01-28T10:48:48.320853Z","iopub.status.idle":"2022-01-28T10:48:48.330201Z","shell.execute_reply.started":"2022-01-28T10:48:48.320818Z","shell.execute_reply":"2022-01-28T10:48:48.329159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = DatasetRetriever(\n    image_ids=validation_dataset.image_ids,\n    transforms=get_valid_transforms()\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:50.197299Z","iopub.execute_input":"2022-01-28T10:48:50.197669Z","iopub.status.idle":"2022-01-28T10:48:50.201831Z","shell.execute_reply.started":"2022-01-28T10:48:50.197637Z","shell.execute_reply":"2022-01-28T10:48:50.200945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndata_loader = DataLoader(\n    dataset,\n    batch_size=2,\n    shuffle=False,\n    num_workers=4,\n    drop_last=False,\n    collate_fn=collate_fn\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:51.103903Z","iopub.execute_input":"2022-01-28T10:48:51.104234Z","iopub.status.idle":"2022-01-28T10:48:51.11029Z","shell.execute_reply.started":"2022-01-28T10:48:51.104204Z","shell.execute_reply":"2022-01-28T10:48:51.109451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install --no-deps '../input/timm-package/timm-0.1.26-py3-none-any.whl' > /dev/null\n# !pip install --no-deps '../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' > /dev/null","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:42.873622Z","iopub.status.idle":"2022-01-28T10:48:42.874439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !ls ../input/weightedboxesfusion","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:42.875703Z","iopub.status.idle":"2022-01-28T10:48:42.876595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.insert(0, \"../input/timm-efficientdet-pytorch\")\nsys.path.insert(0, \"../input/omegaconf\")\nsys.path.insert(0, \"../input/weightedboxesfusion\")\n\nfrom ensemble_boxes import *\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom torch.utils.data import Dataset,DataLoader\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport cv2\nimport gc\nfrom matplotlib import pyplot as plt\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchEval\nfrom effdet.efficientdet import HeadNet","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:53.938333Z","iopub.execute_input":"2022-01-28T10:48:53.938695Z","iopub.status.idle":"2022-01-28T10:48:53.975841Z","shell.execute_reply.started":"2022-01-28T10:48:53.938664Z","shell.execute_reply":"2022-01-28T10:48:53.975101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:42.88029Z","iopub.status.idle":"2022-01-28T10:48:42.881237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls effdet5-cutmix-augmix","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:55.68476Z","iopub.execute_input":"2022-01-28T10:48:55.685078Z","iopub.status.idle":"2022-01-28T10:48:56.367216Z","shell.execute_reply.started":"2022-01-28T10:48:55.685049Z","shell.execute_reply":"2022-01-28T10:48:56.366384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_net(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d0')\n    net = EfficientDet(config, pretrained_backbone=False)\n\n    config.num_classes = 3\n    config.image_size=512\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'])\n\n    del checkpoint\n    gc.collect()\n\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()\n\n# '../input/wheat-effdet5-fold0-best-checkpoint/fold0-best-all-states.bin'\n\npath = 'effdet5-cutmix-augmix/best-checkpoint-028epoch.bin'\n# path = 'effdet5-cutmix-augmix/last-checkpoint.bin'\nnet = load_net(path)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:49:12.717808Z","iopub.execute_input":"2022-01-28T10:49:12.71829Z","iopub.status.idle":"2022-01-28T10:49:13.512765Z","shell.execute_reply.started":"2022-01-28T10:49:12.718244Z","shell.execute_reply":"2022-01-28T10:49:13.511971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predictions(images, score_threshold=0.22):\n    images = torch.stack(images).cuda().float()\n    predictions = []\n    with torch.no_grad():\n        det = net(images, torch.tensor([1]*images.shape[0]).float().cuda())\n        for i in range(images.shape[0]):\n            boxes = det[i].detach().cpu().numpy()[:,:4] \n            labels = det[i].detach().cpu().numpy()[:,5]\n            scores = det[i].detach().cpu().numpy()[:,4]\n            indexes = np.where(scores > score_threshold)[0]\n#             labels = labels[indexes]\n#             boxes = boxes[indexes]\n            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n            predictions.append({\n                'boxes': boxes[indexes],\n                'scores': scores[indexes],\n                 'labels':labels[indexes]\n            })\n    return [predictions]\n\ndef run_wbf(predictions, image_index, image_size=512, iou_thr=0.35, skip_box_thr=0.3, weights=None):   #iou_thr=0.44, skip_box_thr=0.43\n    boxes = [(prediction[image_index]['boxes']/(image_size-1)).tolist()  for prediction in predictions]\n    scores = [prediction[image_index]['scores'].tolist()  for prediction in predictions]\n#     labels = [np.ones(prediction[image_index]['scores'].shape[0]).tolist() for prediction in predictions]\n    labels = [prediction[image_index]['labels'].tolist() for prediction in predictions]\n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    boxes = boxes*(image_size-1)\n    return boxes, scores, labels","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:49:14.384695Z","iopub.execute_input":"2022-01-28T10:49:14.385022Z","iopub.status.idle":"2022-01-28T10:49:14.400341Z","shell.execute_reply.started":"2022-01-28T10:49:14.384994Z","shell.execute_reply":"2022-01-28T10:49:14.398877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# labels","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:42.890081Z","iopub.status.idle":"2022-01-28T10:48:42.891042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# boxes","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:42.892523Z","iopub.status.idle":"2022-01-28T10:48:42.89351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\n\nfor j, (images, image_ids) in enumerate(data_loader):\n    if j==6: break\n\n    predictions = make_predictions(images)\n\n    i = 1\n    sample = images[i].permute(1,2,0).cpu().numpy()\n    \n    boxes, scores, labels = run_wbf(predictions, image_index=i)\n    boxes = boxes.astype(np.int32).clip(min=0, max=511)\n    \n    fig, ax = plt.subplots(1, 1, figsize=(15, 23))\n    \n    \n    for i,box in enumerate(boxes):\n        class_id = int(labels[i])\n        color = class_to_color(class_id)\n        cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), color, 1)\n        \n    ax.set_axis_off()\n    ax.imshow(sample);","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:49:16.141785Z","iopub.execute_input":"2022-01-28T10:49:16.142121Z","iopub.status.idle":"2022-01-28T10:49:20.20865Z","shell.execute_reply.started":"2022-01-28T10:49:16.142091Z","shell.execute_reply":"2022-01-28T10:49:20.2076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# dfg = {\"image_id\":[],\"xmin\":[],\"ymin\":[],\"xmax\":[],\"ymax\":[],\"label\":[],\"score\":[]}\n\n# for j, (images, image_ids) in enumerate(data_loader):\n    \n#     predictions = make_predictions(images)\n#     for i in range(2):\n          \n#         try:   \n#             boxes, scores, labels = run_wbf(predictions, image_index=i)\n#             boxes = boxes.astype(np.int32).clip(min=0, max=511)\n            \n#             for k in range(len(labels)):\n#                 dfg['image_id'].append(image_ids[i])\n#                 dfg['xmin'].append(boxes[k][0])\n#                 dfg['ymin'].append(boxes[k][1])\n#                 dfg['xmax'].append(boxes[k][2])\n#                 dfg['ymax'].append(boxes[k][3])\n#                 dfg['label'].append(labels[k])\n#                 dfg['score'].append(scores[k])\n#         except: pass\n# df = pd.DataFrame.from_dict(dfg)\n# df['class_id']=df['label'].astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:42.897538Z","iopub.status.idle":"2022-01-28T10:48:42.898536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dff","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:42.899901Z","iopub.status.idle":"2022-01-28T10:48:42.900831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:42.902246Z","iopub.status.idle":"2022-01-28T10:48:42.903219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# def plot_grid(dff, img_root, rows=3, cols=3):\n#     img_names = pd.Series(dff['image_id'].unique())\n#     fig, axs = plt.subplots(rows, cols, figsize=(20,20))\n#     for row in range(rows):\n#         for col in range(cols):\n#             idx = np.random.randint(len(img_names), size=1)[0]\n#             name = dff.iloc[idx].image_id\n#             img = load_image(name, img_root)\n#             annotate_image(img, name, dff)\n#             axs[row, col].imshow(img)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:42.904605Z","iopub.status.idle":"2022-01-28T10:48:42.905456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n# predictions = make_predictions(images)\n\n# i = 1\n# sample = images[i].permute(1,2,0).cpu().numpy()\n\n# boxes, scores, labels = run_wbf(predictions, image_index=i)\n# boxes = boxes.astype(np.int32).clip(min=0, max=511)\n\n# fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n# for box in boxes:\n#     cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (1, 0, 0), 1)\n    \n# ax.set_axis_off()\n# ax.imshow(sample);","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:42.906806Z","iopub.status.idle":"2022-01-28T10:48:42.907724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# np.random.randint(len(pd.Series(df['image_id'].unique())), size=1)[0]","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:42.909168Z","iopub.status.idle":"2022-01-28T10:48:42.910022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.iloc[0].image_id","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:42.911309Z","iopub.status.idle":"2022-01-28T10:48:42.912168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pd.Series(df['image_id'].unique())","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:42.913461Z","iopub.status.idle":"2022-01-28T10:48:42.914322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IMG_ROOT = \"../input/cotton-data/cloud-object-storage-tw-cos-standard-tqh\"\n\n# plot_grid(df, IMG_ROOT)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:42.915642Z","iopub.status.idle":"2022-01-28T10:48:42.916522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_grid(dff, IMG_ROOT)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:48:42.91778Z","iopub.status.idle":"2022-01-28T10:48:42.918674Z"},"trusted":true},"execution_count":null,"outputs":[]}]}