{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom itertools import chain\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.measure import label, regionprops\nfrom PIL import Image, ImageDraw\nfrom ast import literal_eval\nfrom tqdm.notebook import tqdm\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, Dropout, Lambda,Conv2D, Conv2DTranspose, MaxPooling2D, MaxPooling2D, concatenate\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\n\ntf.debugging.set_log_device_placement(True)\n\n# Create some tensors\na = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nb = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nc = tf.matmul(a, b)\n\nprint(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Detect hardware, return appropriate distribution strategy\n# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n#     print('Running on TPU ', tpu.master())\n# except ValueError:\n#     tpu = None\n\n# if tpu:\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# else:\n#     strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\n# print(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set some parameters\nIMG_WIDTH = 512\nIMG_HEIGHT = 512\n\n#customization for running on laptop\n# PATH =  os.getcwd() + \"\\\\\"\n# TRAIN_PATH = PATH + \"train\\\\\"\n# TEST_PATH = PATH + \"test\\\\\"\n\nPATH = \"../input/global-wheat-detection/\"\nTRAIN_PATH = '/kaggle/input/global-wheat-detection/train/'\nTEST_PATH = '/kaggle/input/global-wheat-detection/test/'\nSC_FACTOR = int(1024 / IMG_WIDTH)\n\nwarnings.filterwarnings('ignore')\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PATH = \"../input/global-wheat-detection/\"\ntrain_folder = os.path.join(PATH, \"train\")\ntest_folder = os.path.join(PATH, \"test\")\n\ntrain_csv_path = os.path.join(PATH, \"train.csv\")\n\ndf = pd.read_csv(train_csv_path)\nsample_sub = pd.read_csv(PATH + \"sample_submission.csv\")\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get train and test IDs and paths\ntrain_ids = os.listdir(TRAIN_PATH)\ntest_ids = os.listdir(TEST_PATH)\n\n# train_ids = train_ids[0:1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_polygon(coords):\n    xm, ym, w, h = coords\n    xm, ym, w, h = xm / SC_FACTOR, ym / SC_FACTOR, w / SC_FACTOR, h / SC_FACTOR   # scale values if image was downsized\n    return [(xm, ym), (xm, ym + h), (xm + w, ym + h), (xm + w, ym)]\n\nmasks = dict() # dictionnary containing all masks\n\nfor img_id, gp in tqdm(df.groupby(\"image_id\")):\n    gp['polygons'] = gp['bbox'].apply(eval).apply(lambda x: make_polygon(x))\n\n    img = Image.new('L', (IMG_WIDTH, IMG_HEIGHT), 0)\n    for pol in gp['polygons'].values:\n        ImageDraw.Draw(img).polygon(pol, outline=1, fill=1)\n\n    mask = np.array(img, dtype=np.uint8)\n    masks[img_id] = mask","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot a quick example of the mask of the first image :","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's load the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"im = Image.fromarray(masks[list(masks.keys())[7]])\nplt.imshow(im)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get and resize train images and masks\nX_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)\nY_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\nprint('Getting and resizing train images and masks... ')\nsys.stdout.flush()\n\nfor n, id_ in tqdm(enumerate(train_ids[:]), total=len(train_ids)):\n    path = TRAIN_PATH + id_\n    img = imread(path)\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    X_train[n] = img\n    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n    \n    id_clean = id_.split('.')[0]\n    if id_clean in masks.keys():\n        Y_train[n] = masks[id_clean][:, :, np.newaxis]\n\n# Get and resize test images\nX_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)\nsizes_test = list()\nprint('Getting and resizing test images...')\nsys.stdout.flush()\n\nfor n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n    path = TEST_PATH + id_\n    img = imread(path)\n    sizes_test.append([img.shape[0], img.shape[1]])\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    X_test[n] = img\n\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, Y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's quickly look at what the images and mask look like (credits : https://www.kaggle.com/devvindan/wheat-detection-eda)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_images(images, num=2):\n    \n    images_to_show = np.random.choice(images, num)\n\n    for image_id in images_to_show:\n\n        image_path = os.path.join(train_folder, image_id + \".jpg\")\n        image = Image.open(image_path)\n\n        # get all bboxes for given image in [xmin, ymin, width, height]\n        bboxes = [literal_eval(box) for box in df[df['image_id'] == image_id]['bbox']]\n\n        # visualize them\n        draw = ImageDraw.Draw(image)\n        for bbox in bboxes:    \n            draw.rectangle([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]], width=3)\n\n        plt.figure(figsize = (15,15))\n        plt.imshow(image)\n        plt.show()\n\n\nunique_images = df['image_id'].unique()\nshow_images(unique_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Credits to : https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/63044\n\ndef castF(x):\n    return K.cast(x, K.floatx())\n\ndef castB(x):\n    return K.cast(x, bool)\n\ndef iou_loss_core(true,pred):  #this can be used as a loss if you make it negative\n    intersection = true * pred\n    notTrue = 1 - true\n    union = true + (notTrue * pred)\n\n    return (K.sum(intersection, axis=-1) + K.epsilon()) / (K.sum(union, axis=-1) + K.epsilon())\n\ndef competitionMetric2(true, pred):\n\n    tresholds = [0.5 + (i * 0.05)  for i in range(5)]\n\n    #flattened images (batch, pixels)\n    true = K.batch_flatten(true)\n    pred = K.batch_flatten(pred)\n    pred = castF(K.greater(pred, 0.5))\n\n    #total white pixels - (batch,)\n    trueSum = K.sum(true, axis=-1)\n    predSum = K.sum(pred, axis=-1)\n\n    #has mask or not per image - (batch,)\n    true1 = castF(K.greater(trueSum, 1))    \n    pred1 = castF(K.greater(predSum, 1))\n\n    #to get images that have mask in both true and pred\n    truePositiveMask = castB(true1 * pred1)\n\n    #separating only the possible true positives to check iou\n    testTrue = tf.boolean_mask(true, truePositiveMask)\n    testPred = tf.boolean_mask(pred, truePositiveMask)\n\n    #getting iou and threshold comparisons\n    iou = iou_loss_core(testTrue,testPred) \n    truePositives = [castF(K.greater(iou, tres)) for tres in tresholds]\n\n    #mean of thressholds for true positives and total sum\n    truePositives = K.mean(K.stack(truePositives, axis=-1), axis=-1)\n    truePositives = K.sum(truePositives)\n\n    #to get images that don't have mask in both true and pred\n    trueNegatives = (1-true1) * (1 - pred1) # = 1 -true1 - pred1 + true1*pred1\n    trueNegatives = K.sum(trueNegatives) \n\n    return (truePositives + trueNegatives) / castF(K.shape(true)[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Build U-Net model\n    \ninputs = Input((IMG_HEIGHT, IMG_WIDTH, 3))\ns = Lambda(lambda x: x / 255) (inputs)  # rescale inputs\n\nc1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\nc1 = Dropout(0.1) (c1)\nc1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\np1 = MaxPooling2D((2, 2)) (c1)\n\nc2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\nc2 = Dropout(0.1) (c2)\nc2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\np2 = MaxPooling2D((2, 2)) (c2)\n\nc3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\nc3 = Dropout(0.2) (c3)\nc3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\np3 = MaxPooling2D((2, 2)) (c3)\n\nc4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\nc4 = Dropout(0.2) (c4)\nc4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\np4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\nc5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\nc5 = Dropout(0.3) (c5)\nc5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n\nu6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\nu6 = concatenate([u6, c4])\nc6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\nc6 = Dropout(0.2) (c6)\nc6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n\nu7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = concatenate([u7, c3])\nc7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\nc7 = Dropout(0.2) (c7)\nc7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n\nu8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = concatenate([u8, c2])\nc8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\nc8 = Dropout(0.1) (c8)\nc8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n\nu9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\nu9 = concatenate([u9, c1])\nc9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\nc9 = Dropout(0.1) (c9)\nc9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n\np5 = Conv2DTranspose(16, (8, 8), strides=(16,16), padding = 'same') (c5)\nu10 = concatenate([p5, c9], axis=3)\nc10 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u10)\nc10 = Dropout(0.1) (c10)\nc10 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c10)\n\noutputs = Conv2D(1, (1, 1), activation='sigmoid') (c10)\n\n\n# instantiating the model in the strategy scope creates the model on the TPU\n\nmodel = Model(inputs=[inputs], outputs=[outputs])\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=[competitionMetric2])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\nplot_model(model, show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #data augmentation/ generation\n# data_gen_args = dict(#featurewise_center=True,\n#                  #featurewise_std_normalization=True,\n#                  #rotation_range=10,\n#                  #zca_whitening=True,\n#                  #width_shift_range=0.2,\n#                  #height_shift_range=0.2,\n#                  #zoom_range=0.2,\n#                  #horizontal_flip=True,\n#                  #vertical_flip=True\n#                 )\n# image_datagen = ImageDataGenerator(**data_gen_args)\n# mask_datagen = ImageDataGenerator(**data_gen_args)\n# # Provide the same seed and keyword arguments to the fit and flow methods\n# #seed = 10\n# #image_datagen.fit(X_train, augment=True#, seed=seed)\n# #mask_datagen.fit(Y_train, augment=True#, seed=seed)\n# image_generator = image_datagen.flow(\n#     X_train,\n#     batch_size=4,\n#     #seed=42\n# )\n# mask_generator = mask_datagen.flow(\n#     Y_train,\n#     batch_size=4,\n#     #seed=42\n# )\n# # combine generators into one which yields image and masks\n# # train_generator = (pair for pair in zip(image_generator, mask_generator))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit model\nearlystop = EarlyStopping(patience=10, verbose=1, restore_best_weights=True)\n\nmodel.fit(x=X_train,y=Y_train,\n            validation_split=0.20,\n            batch_size=16, \n            #steps_per_epoch=6*len(X_train)/8,\n            epochs=40, \n            callbacks=[earlystop]\n         )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"THRESH = 0.5\n\npreds = model.predict(X_test)[:, :, :, 0]\nmasked_preds = preds > THRESH","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we want to binarize the prediction to get a mask (as the input y_train was also a mask)\n\nLet's visualise predictions !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_rows = 3\nf, ax = plt.subplots(n_rows, 3, figsize=(14, 10))\n\nfor j, idx in enumerate([4,5,6]):\n    for k, kind in enumerate(['original', 'pred', 'masked_pred']):\n        if kind == 'original':\n            img = X_test[idx]\n        elif kind == 'pred':\n            img = preds[idx]\n        elif kind == 'masked_pred':\n            masked_pred = preds[idx] > THRESH\n            img = masked_pred\n        ax[j, k].imshow(img)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the next step is to translate our masked predictions into several bounding boxes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_params_from_bbox(coords, scaling_factor=SC_FACTOR):\n    xmin, ymin = coords[1] * scaling_factor, coords[0] * scaling_factor\n    w = (coords[3] - coords[1]) * scaling_factor\n    h = (coords[2] - coords[0]) * scaling_factor\n    \n    return xmin, ymin, w, h","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Allows to extract bounding boxes from binary masks\nbboxes = list()\n\nfor j in range(masked_preds.shape[0]):\n    label_j = label(masked_preds[j, :, :]) \n    props = regionprops(label_j,intensity_image=preds[j,:,:])   # that's were the job is done\n    bboxes.append(props)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bboxes[0][0].bbox","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.max(bboxes[0][0].intensity_image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we format the bboxes into the required format\noutput = dict()\nsignificant_scores = list()\nsignificant_bboxes = list()\n\nfor i in range(masked_preds.shape[0]):\n    \n    bboxes_processed = [get_params_from_bbox(bb.bbox, scaling_factor=SC_FACTOR) for bb in bboxes[i]]\n    scores = [np.max(bb.intensity_image) for bb in bboxes[i]]\n    \n    bbareas = [bb.bbox_area for bb in bboxes[i]]\n    \n#     df_test = pd.DataFrame({'bboxe_processed':bboxes_processed,\n#                             'scores':scores,\n#                             'bbares':bbareas})\n#     print(df_test.shape)\n    \n    significant_scores.append([score for score,area in zip(scores,bbareas) if ((score > 0.5) & (area > 100))])\n    significant_bboxes.append([' '.join(map(str, bb_m)) for bb_m,score,area in zip(bboxes_processed,scores,bbareas) if ((score > 0.5) & (area > 100))])\n    \n    print(\"There are \" + str(len(significant_scores[i])) + \" Scores in \"+ test_ids[i])\n    print(\"There are \" + str(len(significant_bboxes[i])) + \" BBoxes in \"+ test_ids[i])\n    \n    assert(len(significant_scores[i]) == len(significant_bboxes[i]))\n    \n    \n    \n    \n    formated_boxes = [str(score) + ' '.join(map(str, bb_m)) for score,bb_m in zip(significant_scores[i],significant_bboxes[i])]\n    \n#     formated_boxes = formated_boxes[i > 100 for i in bbareas]\n    \n    output[str.split(test_ids[i],'.')[0]] = \" \".join(formated_boxes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub  = pd.DataFrame()\nsub['image_id'] = output.keys()\nsub[\"PredictionString\"] = output.values()\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for image_id in sub['image_id']:\n\n#         image_path = os.path.join(test_folder, image_id + \".jpg\")\n#         image = Image.open(image_path)\n\n#         # get all bboxes for given image in [xmin, ymin, width, height]\n#         bboxes = significant_bboxes[pd.Index(sub['image_id']).get_loc(image_id)]\n\n#         # visualize them\n#         draw = ImageDraw.Draw(image)\n#         for bbox in bboxes:  \n#             bbox = tuple(map(int ,bbox.split(' ')))\n#             draw.rectangle([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]], width=5)\n\n#         plt.figure(figsize = (5,5))\n#         plt.imshow(image)\n#         plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.merge(sample_sub[['image_id']],sub)\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}