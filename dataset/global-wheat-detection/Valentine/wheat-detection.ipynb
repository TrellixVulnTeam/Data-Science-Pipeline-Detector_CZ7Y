{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport imageio\nfrom skimage import draw, measure\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.dataset import random_split\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nimport torchvision\nfrom torchvision import models\nfrom torchvision.models.detection import FasterRCNN #, fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom tqdm import tqdm\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (15, 10);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_dir = '../input/global-wheat-detection'\ntrain_df = pd.read_csv(os.path.join(data_dir, 'train.csv'))\nsubmission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))\n# np.where(train_df['height'] != 1024, 1, 0).sum()\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_box(string):\n    string = string.replace('[', '').replace(']', '').replace(' ', '')\n    box = []\n    for n in string.split(','):\n        if '.' in n:\n            box.append(int(n.split('.')[0]))\n        else:\n            box.append(int(n))\n    return box","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_sample(img_path, df):\n    img = imageio.imread(img_path)\n    boxes = [y['bbox'] for x, y in df.iterrows() \n             if y['image_id'] == img_path.split('/')[-1].replace('.jpg', '')]\n    boxes = [extract_box(i) for i in boxes]\n    fig, ax = plt.subplots(1)\n    ax.imshow(img)\n    for box in boxes:\n        #box = [int(x.strip(' ')) for x in box.strip('\\'').strip('[').strip(']').split(',')]\n        #print(box)\n        rect = patches.Rectangle((box[0],box[1]),box[2],box[3],linewidth=2,edgecolor='r',facecolor='none')\n        ax.add_patch(rect)\n        #break\n    plt.show(ax)\n    #return boxes\n\n# plot_sample(data_dir+'/train/010c93b99.jpg', train_df)\nplot_sample(data_dir+'/train/0172359d2.jpg', train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mask_to_boxes(boxes, size=1024):\n    all_boxes = []\n    for box in boxes:\n        new_box = [0, 0, 0, 0]\n        new_box[1] = box[1]\n        new_box[0] = box[0]\n        if box[1]+box[3] < (size-1):\n            new_box[3] = box[1]+box[3]\n        else:\n            new_box[3] = size-1\n        if box[0]+box[2] < (size-1):\n            new_box[2] = box[0]+box[2]\n        else:\n            new_box[2] = size-1\n        all_boxes.append(new_box)\n    return all_boxes\n\ndef bbox_to_mask(boxes, mask_arr=None, \n                 size=1024, return_boxes=False):\n    if mask_arr is None:\n        mask_arr = np.zeros((size, size))\n    if return_boxes:\n        for box in mask_to_boxes(boxes):\n            mask_arr[box[1], box[0]:box[2]] = 1\n            mask_arr[box[3], box[0]:box[2]] = 1\n            mask_arr[box[1]:box[3], box[0]] = 1\n            mask_arr[box[1]:box[3], box[2]] = 1\n    else:\n        for box in boxes:\n            mask_arr[box[1]:box[1]+box[3], box[0]:box[0]+box[2]] = 1\n    return mask_arr\n\ndef plot_mask(img_path, df, size=1024):\n    img = imageio.imread(img_path)\n    boxes = [y['bbox'] for x, y in df.iterrows() \n             if y['image_id'] == img_path.split('/')[-1].replace('.jpg', '')]\n    boxes = [extract_box(i) for i in boxes]\n    fig, ax = plt.subplots(1)\n    img = bbox_to_mask(boxes)\n    ax.imshow(img, cmap='gray')\n    \n# plot_mask(data_dir+'/train/010c93b99.jpg', train_df)\nplot_mask(data_dir+'/train/0172359d2.jpg', train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_frame(frame, test=False):\n    frame = frame.groupby('image_id')['bbox'].apply(list)\n    #index = range(0, len(frame))\n    df = pd.DataFrame()\n    df['image_id'] = frame.index\n    df['index'] = range(0, len(frame))\n    if test:\n        return df.set_index('index')\n    df['bbox'] = frame.tolist()\n    return df.set_index('index')\n\ntrain_df = process_frame(train_df)\n#test_df = process_frame(test_df, test=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !wget https://download.pytorch.org/models/maskrcnn/e2e_mask_rcnn_X_101_32x8d_FPN_1x.pth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class WheatDataset(Dataset):\n    \n    def __init__(self, frame, device='cpu', img_format='.jpg', size=1024, test=False):\n        super(WheatDataset, self).__init__()\n        self.img_format = img_format\n        self.frame = frame\n        self.device = device\n        self.size = size\n        self.test = test\n        \n    def __len__(self):\n        return self.frame.shape[0]\n    \n    def __getitem__(self, ind):\n        if self.test:\n            img_path = os.path.join(data_dir, 'test', \n                                    self.frame.loc[ind, 'image_id']+self.img_format)\n        else:\n            img_path = os.path.join(data_dir, 'train', \n                                    self.frame.loc[ind, 'image_id']+self.img_format)\n        self.img = torch.from_numpy(np.array(imageio.imread(img_path))/255)\n        if not self.test:\n            self.target = {}\n            self.target['masks'] = torch.from_numpy(self.get_mask(ind)[-1])\n            self.target['labels'] = torch.from_numpy(self.get_mask(ind)[1])\n            self.target['boxes'] = torch.from_numpy(self.get_mask(ind)[0])\n            return self.img, self.target\n        return self.img\n        \n    def get_mask(self, ind):\n        boxes = self.frame.loc[ind, 'bbox']\n        boxes = np.array([extract_box(i) for i in boxes])\n        mask = bbox_to_mask(boxes, None, self.size, True)\n        return boxes, np.array([1]), mask\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_device(array):\n    if array is None:\n        return None\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    else:\n        device = torch.device('cpu')\n    return torch.tensor(array, device=device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice_loss(input, target):\n    smooth = 1.0\n\n    iflat = input.view(-1)\n    tflat = target.view(-1)\n    intersection = (iflat * tflat).sum()\n    \n    return 1 - ((2. * intersection + smooth) /\n              (iflat.sum() + tflat.sum() + smooth))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class WheatModel(nn.Module):\n    \n    def __init__(self, size=1024, \n                 in_channels=3, \n                 out_channels=1,\n                 device='cuda'):\n        super(WheatModel, self).__init__()\n        self.size = size\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.net = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n            pretrained=True)\n        num_classes = 2\n        in_features = self.net.roi_heads.box_predictor.cls_score.in_features\n        self.net.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n        backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n        backbone.out_channels = 1280\n        #n_features = self.network.fc.out_features\n        #backbone.out_channels = n_features\n        \n        anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n                                   aspect_ratios=((0.5, 1.0, 2.0),))\n        \n        roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n                                                output_size=7,\n                                                sampling_ratio=2)\n        self.model = FasterRCNN(backbone,\n                   num_classes=num_classes)\n                   #rpn_anchor_generator=anchor_generator,\n                   #box_roi_pool=roi_pooler)\n        \n        \n        \n    def forward(self, x, y):\n        #print(x.shape)\n        #x = x.view(-1, self.in_channels, self.size, self.size).float()\n        #x = x.float()\n        #x = F.relu(self.conv1(x))\n        #x = F.relu(self.conv2(x))\n        #x = F.relu(self.pool1(x))\n        #x = F.relu(self.conv3(x))\n        #x = F.relu(self.conv4(x))\n        #x = F.relu(self.pool2(x))\n        #print(x.shape)\n        \n        return self.model(x, y) #, masks=masks) #x.view(-1, self.out_channels, self.out_channels)\n    \n    def training_step(self, batch):\n        #print(batch)\n        #images, masks = batch \n        out = self(batch)\n        loss = dice_loss(out, masks)\n        return loss\n    \n    def validation_step(self, batch):\n        images, masks = batch \n        out = self(batch)\n        loss = dice_loss(out, masks)\n        return {'val_loss': loss}\n    \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch, result['val_loss']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = WheatDataset(train_df)\ntrain_size = int(0.8*len(train_set))\ntrain_set, val_set = random_split(train_set, [train_size, len(train_set) - train_size])\nepochs = 3\nbatch_size = 16\ntrain_loader = DataLoader(train_set, batch_size=batch_size)\nval_loader = DataLoader(val_set, batch_size=batch_size)\ntest_set = WheatDataset(submission, 'cpu', '.jpg', 1024, True)\ntest_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit():\n    model = WheatModel()\n    optimizer = Adam(model.parameters(), lr=0.001)\n    for epoch in range(epochs):\n        total_loss = []\n        model.train()\n        #for sample in tqdm(train_loader):\n        for sample in tqdm(train_set):\n            #print(sample[1]['boxes'].shape)\n            model.zero_grad()\n            out = model(sample[0], sample[1]) #sample[0].view(-1, 3, 1024, 1024), sample[1])\n            loss = dice_loss(out, sample[1].view(-1, 1, 1024, 1024))\n            loss.backward()\n            optimizer.step()\n            total_loss.append(loss.item())\n            optimizer.zero_grad()\n        print(np.array(total_loss).mean())\n    torch.save(model.state_dict(), 'model_weights.pth')\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set.frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@torch.no_grad()\ndef make_predictions(dataset, weights_path='../working/model_weights.pth', validation=True):\n    model = WheatModel()\n    model.load_state_dict(torch.load(weights_path))\n    model.eval()\n    total_loss = []\n    preds = []\n    i=0\n    for sample in tqdm(dataset):\n        out = model(sample[0].view(-1, 3, 1024, 1024))\n        preds.append(out.view(-1, 1024, 1024))\n        if validation:\n            loss = dice_loss(out, sample[1].view(-1, 1, 1024, 1024))\n            total_loss.append(loss.item())\n        break\n    if validation:\n        print(np.array(total_loss).mean())\n    return preds\n        \npreds = make_predictions(val_set) #, weights_path='../input/wheat-model-weights/model_weights.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = preds[0].view(1, 1024, 1024).detach().numpy().reshape(1024, 1024)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(img, cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}