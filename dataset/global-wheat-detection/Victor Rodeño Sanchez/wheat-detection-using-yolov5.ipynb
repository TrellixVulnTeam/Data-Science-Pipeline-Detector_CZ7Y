{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Wheat detection using YOLOv5"},{"metadata":{},"cell_type":"markdown","source":"This is a notebook for the Kaggle competition [Global Wheat Detection](https://www.kaggle.com/c/global-wheat-detection).\n\nWe use [YOLOv5](https://github.com/ultralytics/yolov5).\n"},{"metadata":{},"cell_type":"markdown","source":"## Cloning repository of YOLOv5"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5\n!mv yolov5/* ./","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python -m pip install --upgrade pip\n!pip install -r requirements.txt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare training data to YOLOv5 format\n\nMore information [here](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data). We need two additional YAML files for both dataset and model configurations. The dataset configuration file is available [here](https://www.kaggle.com/viroviro/wheat-detection-yolov5-utils). The model configuration file we use is available in the repository of YOLOv5."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom tqdm.auto import tqdm\nimport shutil as sh\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/global-wheat-detection/train.csv')\nbboxs = np.stack(df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\nfor i, column in enumerate(['x', 'y', 'w', 'h']):\n    df[column] = bboxs[:,i]\ndf.drop(columns=['bbox'], inplace=True)\ndf['x_center'] = df['x'] + df['w']/2\ndf['y_center'] = df['y'] + df['h']/2\ndf['classes'] = 0\n\ndf = df[['image_id','x', 'y', 'w', 'h','x_center','y_center','classes']]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = list(set(df.image_id))\nsource = 'train'\nif True:\n    for fold in [0]:\n        val_index = index[len(index)*fold//5:len(index)*(fold+1)//5]\n        for name,mini in tqdm(df.groupby('image_id')):\n            if name in val_index:\n                path2save = 'val2017/'\n            else:\n                path2save = 'train2017/'\n            if not os.path.exists('convertor/fold{}/labels/'.format(fold)+path2save):\n                os.makedirs('convertor/fold{}/labels/'.format(fold)+path2save)\n            with open('convertor/fold{}/labels/'.format(fold)+path2save+name+\".txt\", 'w+') as f:\n                row = mini[['classes','x_center','y_center','w','h']].astype(float).values\n                row = row/1024\n                row = row.astype(str)\n                for j in range(len(row)):\n                    text = ' '.join(row[j])\n                    f.write(text)\n                    f.write(\"\\n\")\n            if not os.path.exists('convertor/fold{}/images/{}'.format(fold,path2save)):\n                os.makedirs('convertor/fold{}/images/{}'.format(fold,path2save))\n            sh.copy(\"../input/global-wheat-detection/{}/{}.jpg\".format(source,name),'convertor/fold{}/images/{}/{}.jpg'.format(fold,path2save,name))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Model"},{"metadata":{},"cell_type":"markdown","source":"We use pretrained weights on COCO dataset. The pretrained weights are auto-downloaded from [Google Drive](https://drive.google.com/drive/folders/1Drs_Aiu7xx6S-ix95f9kNsA6ueKRpN2J). The training can be slow, so sure you have an accelerator to GPU (1 epoch takes 15 mins approximately using GPU)."},{"metadata":{"trusted":true},"cell_type":"code","source":"!python train.py --img 1024 --batch 2 --epochs 10 \\\n                 --data ../input/wheat-detection-yolov5-utils/wheat0.yaml \\\n                 --cfg models/yolov5x.yaml \\\n                 --weights yolov5x.pt\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The final output indicates the location where the model was saved. In this case, at `runs/exp0/weights/best.pt`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# copy saved model to weights folder\n!cp runs/exp4/weights/best.pt weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove convertor of training data\n!rm -rf convertor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test Model"},{"metadata":{},"cell_type":"markdown","source":"There are 10 test images at `../input/global-wheat-detection/test/`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect test images\n!python detect.py --source '../input/global-wheat-detection/test/' --weight weights/best.pt --output 'inference/output' ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The output images are stored at `inference/output`."},{"metadata":{},"cell_type":"markdown","source":"### Display Output Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -l inference/output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image, clear_output  # to display images\nImage(filename='inference/output/2fd875eaa.jpg', width=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename='inference/output/348a992bb.jpg', width=600)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}