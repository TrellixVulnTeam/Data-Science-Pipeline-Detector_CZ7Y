{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, \"/kaggle/input/weightedboxesfusion\")\n\nfrom IPython.display import Image\n\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport os, re\nimport gc\nimport random\n\nimport torch\n\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom matplotlib import pyplot as plt \nplt.rcParams['figure.figsize'] = (10.0, 10.0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/global-wheat-detection\"\nMODELS_IN_DIR = \"/kaggle/input\"\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_image(image_path):\n    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    assert image is not None, f\"IMAGE NOT FOUND AT {image_path}\"\n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class WheatDataset(Dataset):\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n\n    def __len__(self) -> int:\n        return len(self.image_ids)\n\n    def __getitem__(self, idx: int):\n        image_id = self.image_ids[idx]\n        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n#         # change the shape from [h,w,c] to [c,h,w]  \n#         image = torch.from_numpy(image).permute(2,0,1)\n\n        records = self.df[self.df['image_id'] == image_id]\n    \n        if self.transforms:\n            sample = {\"image\": image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n\n        return image, image_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_transforms():\n    return albumentations.Compose([\n                ToTensorV2(p=1.0)\n            ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_prediction_string(boxes, scores):\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n    return \" \".join(pred_strings)\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ntest_dataset = WheatDataset(pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\")), os.path.join(DATA_DIR, \"test\"), get_test_transforms())\n\ntest_data_loader = DataLoader(\n    test_dataset,\n    batch_size=4,\n    shuffle=False,\n    num_workers=4,\n    drop_last=False,\n    collate_fn=collate_fn\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model(checkpoint_path):\n    \"\"\"\n    https://stackoverflow.com/questions/58362892/resnet-18-as-backbone-in-faster-r-cnn\n    \"\"\"\n    backbone = resnet_fpn_backbone('resnet101', pretrained=False)\n    model = FasterRCNN(backbone, num_classes=2)\n    model.load_state_dict(torch.load(checkpoint_path))\n    model.to(DEVICE)\n    model.eval()\n    return model\n\ndef get_model_152(checkpoint_path):\n    backbone = resnet_fpn_backbone('resnet152', pretrained=False)\n    model = FasterRCNN(backbone, num_classes=2)\n    model.load_state_dict(torch.load(checkpoint_path))\n    model.to(DEVICE)\n    model.eval()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [\n    get_model(os.path.join(MODELS_IN_DIR, \"frcnn101f0\", \"best_model.pth\")),\n    get_model(os.path.join(MODELS_IN_DIR, \"frcnnfone\", \"best_model.pth\")),\n    get_model(os.path.join(MODELS_IN_DIR, \"frcnnfoldtwo\", \"best_model.pth\")),\n    get_model(os.path.join(MODELS_IN_DIR, \"frcnnfoldthree\", \"best_model.pth\")),\n    get_model(os.path.join(MODELS_IN_DIR, \"frcnnfoldfour\", \"best_model.pth\")),\n    get_model_152(os.path.join(MODELS_IN_DIR, \"frcnn152foldthree\", \"best_model.pth\")),\n    get_model_152(os.path.join(MODELS_IN_DIR, \"frcnn152foldtwo\", \"best_model.pth\")),\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from ensemble_boxes import *\n\ndef make_ensemble_predictions(images):\n    images = list(image.to(DEVICE) for image in images)    \n    result = []\n    for model in models:\n        with torch.no_grad():\n            outputs = model(images)\n            result.append(outputs)\n            del model\n            gc.collect()\n            torch.cuda.empty_cache()\n    return result\n\ndef run_wbf(predictions, image_index, image_size=1024, iou_thr=0.55, skip_box_thr=0.7, weights=None):\n    boxes = [prediction[image_index]['boxes'].data.cpu().numpy()/(image_size-1) for prediction in predictions]\n    scores = [prediction[image_index]['scores'].data.cpu().numpy() for prediction in predictions]\n    labels = [np.ones(prediction[image_index]['scores'].shape[0]) for prediction in predictions]\n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    boxes = boxes*(image_size-1)\n    return boxes, scores, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\n\nfor images, image_ids in test_data_loader:\n    predictions = make_ensemble_predictions(images)\n    for i, image in enumerate(images):\n        boxes, scores, labels = run_wbf(predictions, image_index=i)\n        boxes = boxes.astype(np.int32).clip(min=0, max=1023)\n        image_id = image_ids[i]\n\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        \n        result = {\n            'image_id': image_id,\n            'PredictionString': format_prediction_string(boxes, scores)\n        }\n        results.append(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results[0:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}