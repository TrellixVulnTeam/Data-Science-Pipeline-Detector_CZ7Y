{"cells":[{"metadata":{},"cell_type":"markdown","source":"Added single flip-ud and combination of scale and flip-ud from the yolov5 to yolov4...\nOriginal work on yolov5 is by [@wasupandceacar](https://www.kaggle.com/wasupandceacar/)\n\n\nTo find the changes please navigate to Darknet class in `models.py` ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nimport sys\n\nsys.path.insert(0, '../input/yolov4-pt/')\nsys.path.insert(0, \"../input/weightedboxesfusion\")\n\n\nfrom ensemble_boxes import *\nimport glob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp -r ../input/yolov4-pt/models.py .\n!cp -r ../input/yolov4-pt/yolov3/weights/yolov4-obj_last.weights .\n!cp -r ../input/yolov4-pt/yolov3/cfg/yolov4-obj.cfg .\n!cp -r ../input/yolov4-pt/yolov3/utils .","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from models import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The below code is used to convert `.weights` file to `.pt` file...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"! python -c \"from models import *;convert(cfg='./yolov4-obj.cfg',weights='./yolov4-obj_last.weights')\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iou_threshold = 0.34\nskip_threshold = 0.31","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import argparse\n\nfrom utils.datasets import *\nfrom utils.utils import *\n\ninference_size = 1024\ndef detect(save_img=False):\n    weights, imgsz = opt.weights,opt.img_size\n    source = '../input/global-wheat-detection/test/'\n    \n    # Initialize\n    device = torch_utils.select_device(opt.device)\n    half = False\n    \n    # Load model\n    model = Darknet('../input/yolov4-pt/yolov3/cfg/yolov4-obj.cfg', inference_size)\n    model.load_state_dict(torch.load('./yolov4-obj_last.pt', map_location=device)['model'])\n    model.to(device).eval()\n\n    dataset = LoadImages(source, img_size=1024)\n\n    t0 = time.time()\n    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n    all_path=[]\n    all_bboxex =[]\n    all_score =[]\n    for path, img, im0s, vid_cap in dataset:\n        print(im0s.shape)\n        img = torch.from_numpy(img).to(device)\n        img = img.half() if half else img.float()  # uint8 to fp16/32\n        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n        if img.ndimension() == 3:\n            img = img.unsqueeze(0)\n\n        # Inference\n        t1 = torch_utils.time_synchronized()\n        bboxes_2 = []\n        score_2 = []\n        if True:\n            pred = model(img, augment=opt.augment)[0]\n#             print(pred)\n            pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=None, agnostic=False)\n#             print(pred)\n            t2 = torch_utils.time_synchronized()\n\n            bboxes = []\n            score = []\n            # Process detections\n            for i, det in enumerate(pred):  # detections per image\n                p, s, im0 = path, '', im0s\n                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  #  normalization gain whwh\n                if det is not None and len(det):\n                    det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n                    for c in det[:, -1].unique():\n                        n = (det[:, -1] == c).sum()  # detections per class\n\n                    for *xyxy, conf, cls in det:\n                        if True:  # Write to file\n                            xywh = torch.tensor(xyxy).view(-1).numpy()  # normalized xywh\n#                             xywh[2] = xywh[2]-xywh[0]\n#                             xywh[3] = xywh[3]-xywh[1]\n                            bboxes.append(xywh)\n                            score.append(conf)\n            bboxes_2.append(bboxes)\n            score_2.append(score)\n        all_path.append(path)\n        all_score.append(score_2)\n        all_bboxex.append(bboxes_2)\n    return all_path,all_score,all_bboxex\n\n\nif __name__ == '__main__':\n    class opt:\n        weights = \"./yolov4-obj_last.pt\"\n        img_size = 1024\n        conf_thres = 0.5\n        iou_thres = 0.6\n        augment = True\n        device = '0'\n        classes=None\n        agnostic_nms = True\n        \n    print(opt)\n\n    with torch.no_grad():\n        res = detect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_wbf(boxes,scores, image_size=1024, iou_thr=iou_threshold, skip_box_thr=skip_threshold, weights=None):\n#     boxes =boxes/(image_size-1)\n    labels0 = [np.ones(len(scores[idx])) for idx in range(len(scores))]\n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels0, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n#     boxes = boxes*(image_size-1)\n#     boxes = boxes\n    return boxes, scores, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_path,all_score,all_bboxex = res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results =[]\ndef format_prediction_string(boxes, scores):\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n    return \" \".join(pred_strings)\nfor row in range(len(all_path)):\n    image_id = all_path[row].split(\"/\")[-1].split(\".\")[0]\n    boxes = all_bboxex[row]\n    scores = all_score[row]\n    boxes, scores, labels = run_wbf(boxes,scores)\n    boxes = (boxes*1024/1024).astype(np.int32).clip(min=0, max=1023)\n    boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n    boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n    result = {'image_id': image_id,'PredictionString': format_prediction_string(boxes, scores)}\n    results.append(result)\ntest_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.to_csv('submission.csv', index=False)\ntest_df.head(12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size = 300\nfontScale = 1\ncolor = (255, 0, 0) \nthickness = 2\nfont = cv2.FONT_HERSHEY_SIMPLEX \nidx=-2\nimage = cv2.imread(all_path[idx], cv2.IMREAD_COLOR)\n# fontScale \n\nboxes = all_bboxex[idx]\nscores = all_score[idx]\nboxes, scores, labels = run_wbf(boxes,scores)\nboxes = (boxes*1024/1024).astype(np.int32).clip(min=0, max=1023)\nboxes[:, 2] = boxes[:, 2] - boxes[:, 0]\nboxes[:, 3] = boxes[:, 3] - boxes[:, 1]\nfor b,s in zip(boxes,scores):\n    image = cv2.rectangle(image, (b[0],b[1]), (b[0]+b[2],b[1]+b[3]), (255,0,0), 1) \n    image = cv2.putText(image, '{:.2}'.format(s), (b[0]+np.random.randint(20),b[1]), font,  \n                   fontScale, color, thickness, cv2.LINE_AA)\nplt.figure(figsize=[15,15])\nplt.imshow(image[:,:,::-1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If this notebook recieves good response I will make a new notebook for yolov4 training...","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}