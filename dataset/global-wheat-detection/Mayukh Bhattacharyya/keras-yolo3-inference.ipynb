{"cells":[{"metadata":{},"cell_type":"markdown","source":"YOLOv3 Inference using experiencor's yolo3 implementation. https://github.com/experiencor/keras-yolo3\nSingle Model with Weighted Boxes Fusion.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#! /usr/bin/env python\n\nimport sys\nimport os\nimport numpy as np\nimport json\nimport cv2\nimport pickle\nimport tensorflow as tf\nimport keras\nfrom keras.models import load_model\nimport matplotlib.pyplot as plt\n\nsys.path.append(\"../input/kerasyolo3/keras-yolo3-master\")\nsys.path.append(\"../input/weightedboxesfusion\")\n\n\nfrom voc import parse_voc_annotation\nfrom yolo import create_yolov3_model, dummy_loss\nfrom generator import BatchGenerator\nfrom utils.utils import normalize, evaluate, makedirs, get_yolo_boxes\nfrom utils.multi_gpu_model import multi_gpu_model\nfrom ensemble_boxes import *\n\nconfig = tf.compat.v1.ConfigProto(\n    gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.9)\n    # device_count = {'GPU': 1}\n)\nconfig.gpu_options.allow_growth = True\nsession = tf.compat.v1.Session(config=config)\ntf.compat.v1.keras.backend.set_session(session)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"max_box_per_image = 116\n\n\nconfig = {\n    \"model\" : {\n        \"min_input_size\":       512,\n        \"max_input_size\":       512,\n        \"anchors\":              [0.60,1.18, 0.88,0.58, 1.03,0.98, 1.06,1.56, 1.39,2.48, 1.42,1.19, 1.58,0.71, 1.82,1.66, 2.32,1.13],\n        \"labels\":               [\"head\"]\n    },\n\n    \"train\": {\n        \"train_image_folder\":   \"../data/train/\",\n        \"train_annot_folder\":   \"../data/VOC_annot.pkl\",\n        \"cache_name\":           \"\",\n\n        \"train_times\":          1,\n        \"batch_size\":           2,\n        \"ignore_thresh\":        0.5,\n\n        \"grid_scales\":          [1.0,1.0,1.0],\n        \"obj_scale\":            5,\n        \"noobj_scale\":          1,\n        \"xywh_scale\":           1,\n        \"class_scale\":          1,\n\n        \"saved_weights_name\":   \"../input/wheat-yolo3/wheat_v1.4.h5\",\n        \"debug\":                True\n    },\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(\n    nb_class, \n    anchors, \n    max_box_per_image, \n    max_grid, batch_size, \n    warmup_batches, \n    ignore_thresh,\n    saved_weights_name,\n    grid_scales,\n    obj_scale,\n    noobj_scale,\n    xywh_scale,\n    class_scale):\n    \n    template_model, infer_model = create_yolov3_model(\n        nb_class            = nb_class, \n        anchors             = anchors, \n        max_box_per_image   = max_box_per_image, \n        max_grid            = max_grid, \n        batch_size          = batch_size, \n        warmup_batches      = warmup_batches,\n        ignore_thresh       = ignore_thresh,\n        grid_scales         = grid_scales,\n        obj_scale           = obj_scale,\n        noobj_scale         = noobj_scale,\n        xywh_scale          = xywh_scale,\n        class_scale         = class_scale\n    )  \n\n    # load the pretrained weight if exists, otherwise load the backend weight only \n    print(\"\\nLoading pretrained weights.\\n\")\n    template_model.load_weights(saved_weights_name)\n\n    return infer_model\n\n\nmodel = create_model(\n    nb_class            = len(config['model']['labels']), \n    anchors             = config['model']['anchors'], \n    max_box_per_image   = max_box_per_image, \n    max_grid            = [config['model']['max_input_size'], config['model']['max_input_size']], \n    batch_size          = config['train']['batch_size'], \n    warmup_batches      = 0,\n    ignore_thresh       = config['train']['ignore_thresh'],\n    saved_weights_name  = config['train']['saved_weights_name'],\n    grid_scales         = config['train']['grid_scales'],\n    obj_scale           = config['train']['obj_scale'],\n    noobj_scale         = config['train']['noobj_scale'],\n    xywh_scale          = config['train']['xywh_scale'],\n    class_scale         = config['train']['class_scale'],\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nim_ids = []\noutput = []\ncoords = []\n\ntest_files = os.listdir(\"../input/global-wheat-detection/test/\")\n\nfor f in test_files:\n    im = cv2.imread(\"../input/global-wheat-detection/test/\"+f)\n    im_ids.append(f.split(\".\")[0])\n    \n    boxes_unfiltered = get_yolo_boxes(model, [im], 512, 512, config['model']['anchors'], 0.2, 1.)[0]\n        \n    pred_boxes = np.array([[int(box.xmin), int(box.ymin), int(box.xmax - box.xmin), \\\n                    int(box.ymax - box.ymin)] for box in boxes_unfiltered])/1024\n\n    if len(pred_boxes) > 0:\n        pred_boxes[:,2] = pred_boxes[:,2] + pred_boxes[:,0]\n        pred_boxes[:,3] = pred_boxes[:,3] + pred_boxes[:,1]\n    \n        labels_list = [[0 for b in boxes_unfiltered]]\n        scores_list = [[box.classes[0] for box in boxes_unfiltered]]\n        boxes_list = [pred_boxes]\n    \n        pred_boxes, scores, labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list, \\\n                                                           weights=None, iou_thr=0.3, skip_box_thr=0.95)\n\n        pred_boxes = pred_boxes*1024\n        pred_boxes[:,2] = pred_boxes[:,2] - pred_boxes[:,0]\n        pred_boxes[:,3] = pred_boxes[:,3] - pred_boxes[:,1]\n\n        preds_sorted_idx = np.argsort(scores)[::-1]\n        scores_sorted = np.array(scores[preds_sorted_idx])\n        preds_sorted = np.array(pred_boxes[preds_sorted_idx])\n    else:\n        preds_sorted = np.array([])\n    \n    all_boxes = []\n    \n    coords.append(preds_sorted)\n    \n    for i,box in enumerate(preds_sorted):\n        x = [scores[i], int(box[0]), int(box[1]), int(box[2]), int(box[3])]\n        x = [str(i) for i in x]\n        x = \" \".join(x)\n        all_boxes.append(x)\n    \n    output.append(\" \".join(all_boxes))\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotboxes(idx):\n    im = cv2.imread(\"../input/global-wheat-detection/test/\"+test_files[idx])\n    for box in coords[idx]:\n        box = [int(x) for x in box]\n        cv2.rectangle(im, (box[0], box[1]), (box[0] + box[2], box[1] + box[3]), (255, 0, 0), 2)\n\n    plt.figure(figsize=(16,16))\n    plt.imshow(im)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotboxes(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotboxes(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotboxes(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame({'image_id':im_ids, 'PredictionString':output})\nsub.head()\nsub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}