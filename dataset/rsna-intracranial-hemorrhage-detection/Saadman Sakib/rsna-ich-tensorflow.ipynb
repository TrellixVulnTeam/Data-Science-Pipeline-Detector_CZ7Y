{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport pydicom\nimport cv2\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils.vis_utils import plot_model\nfrom keras.layers import Input, concatenate \nfrom keras.models import Model\nimport os\nimport tensorflow_transform as tft\nfrom sklearn.decomposition import PCA\nimport tensorflow_addons as tfa\n\n\n# gpus = tf.config.experimental.list_physical_devices('GPU') \n# for gpu in gpus: \n#     tf.config.experimental.set_memory_growth(gpu, True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Globals","metadata":{}},{"cell_type":"code","source":"SIZE = 100\nSTART = 0\nINPUT_PATH = \"../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/\"\ntrain_images_dir = INPUT_PATH + 'stage_2_train/'\ntest_images_dir = INPUT_PATH + 'stage_2_test/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data\nWe will load data with pandas","metadata":{}},{"cell_type":"code","source":"def load_dataframe(csv_file):\n    info = pd.read_csv(csv_file)\n    info['Sub_type'] = info['ID'].str.split(\"_\", n = 3, expand = True)[2]\n    info['ID'] = info['ID'].str.split(\"_\", n = 3, expand = True)[0]+'_'+info['ID'].str.split(\"_\", n = 3, expand = True)[1]\n    info = info.sort_values(by=['ID','Sub_type'], ignore_index=True)\n\n    info['any'] = np.where(info['Sub_type'] == 'any', info['Label'], 0)\n    info['epidural'] = np.where(info['Sub_type'] == 'epidural', info['Label'], 0)\n    info['intraparenchymal'] = np.where(info['Sub_type'] == 'intraparenchymal', info['Label'], 0)\n    info['intraventricular'] = np.where(info['Sub_type'] == 'intraventricular', info['Label'], 0)\n    info['subarachnoid'] = np.where(info['Sub_type'] == 'subarachnoid', info['Label'], 0)\n    info['subdural'] = np.where(info['Sub_type'] == 'subdural', info['Label'], 0)\n\n    df = info.drop(columns=['Label', 'Sub_type']).groupby('ID').sum().reset_index()\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = load_dataframe(INPUT_PATH + \"stage_2_train.csv\")\ntest_df = load_dataframe(INPUT_PATH + 'stage_2_sample_submission.csv')\nprint(\"Loaded!!!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.to_csv('train.csv')\n# test_df.to_csv('test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfw = df.copy()\n# zeros = df.copy()\n# # dfw = dfw[['ID','any']]\n# zeros = zeros[zeros['any'] == 0]\n\n# print(\"any:\", len(dfw[dfw['any']==1]))\n# print(\"epidural:\", len(dfw[dfw['epidural']==1]))\n# print(\"intraparenchymal:\", len(dfw[dfw['intraparenchymal']==1]))\n# print(\"intraventricular\", len(dfw[dfw['intraventricular']==1]))\n# print(\"subarachnoid:\", len(dfw[dfw['subarachnoid']==1]))\n# print(\"subdural:\", len(dfw[dfw['subdural']==1]))\n\n# zeros_to_be_dropped = np.random.choice(zeros.index, 6*len(zeros)//7, replace=False)\n# dfw = dfw.drop(index=zeros_to_be_dropped)\ntrain, test = train_test_split(dfw, test_size=0.2, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfw.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_images = os.listdir(train_images_dir)\n# test_images = os.listdir(test_images_dir)\n\n# print('done')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Windowing Function\nIt will take a dicom image and convert into an RGB like image","metadata":{}},{"cell_type":"code","source":"def final_windowing(img_path):\n\n    window_sizes = [(40, 80), (75,215), (600,2800)]\n\n    def preprocess(img_path, WINDOW_LEVEL, WINDOW_WIDTH):\n        # params\n        window_min = WINDOW_LEVEL-(WINDOW_WIDTH // 2)\n        window_max = WINDOW_LEVEL+(WINDOW_WIDTH // 2)\n        # read dicom file\n        r = pydicom.read_file(img_path)\n        # convert to hounsfield unit\n        img = (r.pixel_array * r.RescaleSlope) + r.RescaleIntercept\n        # apply brain window\n        img = np.clip(img, window_min, window_max)\n        img = 255 * ((img - window_min)/WINDOW_WIDTH)\n        img = img.astype(np.uint8)\n        return img\n\n    new_arr = []\n    try:\n        for x in window_sizes:\n            imag = preprocess(img_path, x[0], x[1] )\n            new_arr.append(imag)\n    except:\n        new_arr = []\n        for i in range(3):\n            new_arr.append(np.zeros((512, 512)))\n\n    new_arr_2 = np.dstack((new_arr[0], new_arr[1], new_arr[2]))\n\n    return new_arr_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig=plt.figure(figsize=(20, 10))\n# columns = 4; rows = 2\n# for i in range(1, rows*rows +1):\n#     ds = pydicom.dcmread(train_images_dir + train_images[i]).pixel_array\n#     fig.add_subplot(rows, columns, i)\n#     plt.imshow(ds, cmap=plt.cm.bone)\n#     fig.add_subplot\n\n# for i in range(1, rows*rows +1):\n#     ds = final_windowing(train_images_dir + train_images[i])\n#     fig.add_subplot(rows, columns, i+4)\n#     plt.imshow(ds, cmap=plt.cm.bone)\n#     fig.add_subplot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ","metadata":{}},{"cell_type":"markdown","source":"# Custom Data Generator","metadata":{}},{"cell_type":"code","source":"class CustomDataGen(tf.keras.utils.Sequence):\n    \n    def __init__(self, directory, df,\n                 preprocessing_function=final_windowing,\n                 batch_size=32,\n                 shuffle=False):\n        \n        self.directory = directory\n        self.df = df.copy()\n        self.preprocessing_function = preprocessing_function\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        \n        self.n = len(self.df)\n        self.n_name = df['ID'].nunique()\n        self.types = [\n            'any',\n            'epidural', \n            'intraparenchymal', \n            'intraventricular', \n            'subarachnoid', \n            'subdural'\n        ]\n        self.n_type = len(self.types)\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            self.df = self.df.sample(frac=1).reset_index(drop=True)\n    \n    def pre_process(self, image):\n        img_arr = self.preprocessing_function(image)\n        img_arr = img_arr/255 # normalization\n        return img_arr\n    \n    def __get_input(self, image):\n        image_arr = self.pre_process(self.directory + image + '.dcm')\n        image_arr.resize((224,224,3))\n        return image_arr\n    \n    def __get_data(self, batches):\n        # Generates data containing batch_size samples\n        \"\"\" \n        For a batch data frame, use ID column and add .dcm ext to get file name, and read the file, and use as X_batch.\n        For a batch data frame, use any, epidural, intraparenchymal, intraventricular, subarachnoid, subdural as columns \n        to get the Type value (in one hot encoding).\n        \"\"\"\n        file_names = np.asarray(batches['ID'])\n        file_types = np.asarray(batches[self.types])\n\n        X_batch = np.asarray([self.__get_input(file) for file in file_names])\n        y_batch = file_types\n\n        return X_batch, y_batch\n    \n    def __getitem__(self, index):\n        batches = self.df[index * self.batch_size:(index + 1) * self.batch_size]\n        X, y = self.__get_data(batches)        \n        return X, y\n    \n    def __len__(self):\n        return self.n // self.batch_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating Image DataFlow","metadata":{}},{"cell_type":"code","source":"train_datagen = CustomDataGen(\n    directory = train_images_dir,\n    preprocessing_function = final_windowing,\n    df = train,\n)\n\nval_datagen = CustomDataGen(\n    directory = train_images_dir,\n    preprocessing_function = final_windowing,\n    df = test,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(train_datagen.__getitem__(1)[0][4])\ntrain_datagen.__getitem__(1)[1][4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Neural Net Model","metadata":{}},{"cell_type":"code","source":"# class CustomPCALayer(tf.keras.layers.Layer):\n#     def __init__(self, num_outputs):\n#         super(CustomPCALayer, self).__init__()\n#         self.num_outputs = num_outputs\n#         self.total = tf.Variable(initial_value=tf.zeros((num_outputs,)), trainable=False)\n\n#     def call(self, inputs):\n#         return tft.pca(x=inputs, output_dim=self.num_outputs, dtype=tf.float32)\n\n\n# class MyPCA(tf.keras.layers.Layer):\n#     def __init__(self, num_outputs):\n#         super(MyPCA, self).__init__()\n#         self.num_outputs = num_outputs\n#         self.total = tf.Variable(initial_value=tf.zeros((num_outputs,)), trainable=False)\n\n#     def call(self, inputs):\n#         pca = PCA(n_components=self.num_outputs)\n#         inputs = pca.fit_transform(inputs)\n#         return inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class Linear(tf.keras.layers.Layer):\n#     def __init__(self, units=32, input_dim=32):\n#         super(Linear, self).__init__()\n#         self.w = self.add_weight(\n#             shape=(input_dim, units), initializer=\"random_normal\", trainable=True\n#         )\n#         self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n\n#     def call(self, inputs):\n#         return tf.matmul(inputs, self.w) + self.b","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import keras.backend as K\n# tf.compat.v1.enable_eager_execution()\n\n# def custom_loss(y_t, y):\n#     '''\n#     (y⋅ln(sigmoid(logits))+(1−y)⋅ln(1−sigmoid(logits)))\n    \n#     Multi-label cross-entropy\n#     * Required \"Wp\", \"Wn\" as positive & negative class-weights\n#     y_true: true value\n#     y_logit: predicted value\n#     '''\n#     y_t = y_t.numpy()\n#     y = y.numpy()\n    \n# #     y_t = y_t.eval(session=tf.compat.v1.Session())\n# #     y = y.eval(session=tf.compat.v1.Session())\n    \n#     loss = float(0)\n    \n#     for i in range(len(y_t)):\n#         loss += y_t[i]*np.log(y[i]) + (1-y_t[i])*np.log(1-y)\n#     return loss\n\n# def f1_score(y_true, y_logit):\n#     '''\n#     Calculate F1 score\n#     y_true: true value\n#     y_logit: predicted value\n#     '''\n#     true_positives = K.sum(K.round(K.clip(y_true * y_logit, 0, 1)))\n#     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n#     recall = true_positives / (possible_positives + K.epsilon())\n#     predicted_positives = K.sum(K.round(K.clip(y_logit, 0, 1)))\n#     precision = true_positives / (predicted_positives + K.epsilon())\n#     return (2 * precision * recall) / (precision + recall + K.epsilon())\n\n\n# def custom_loss_function(y_true, y_pred):\n#     squared_difference = tf.square(y_true - y_pred)\n#     return tf.reduce_mean(squared_difference, axis=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_net = Input((224,224,3))\n\n## Encoder starts\ninception = tf.keras.applications.InceptionV3(\n        include_top=False,\n        weights=\"imagenet\",\n        input_tensor=None,\n        input_shape=None,\n        pooling=None,\n        classifier_activation=\"softmax\",\n    )(input_net)\n\npooling = tf.keras.layers.GlobalMaxPool2D()(inception)\n\nreshaping = tf.keras.layers.Reshape((2048,1))(pooling)\n\nblstm = tf.keras.layers.Bidirectional(\n            tf.keras.layers.LSTM(256),\n        )(reshaping)\n\n\nmerged = concatenate([pooling,blstm], axis = 1)\n\noutput_net = tf.keras.layers.Dense(6, activation=\"sigmoid\")(merged)\n\nmodel = Model(inputs = input_net, outputs = output_net)\n\n# Cyclical Learning Rate\n# steps_per_epoch = len(train_datagen) // 8\n# clr = tfa.optimizers.CyclicalLearningRate(\n#     initial_learning_rate=1e-4,\n#     maximal_learning_rate= 1e-2,\n#     scale_fn=lambda x: 1/(2.**(x-1)),\n#     step_size=2 * steps_per_epoch\n# )\n# optimizer = tf.keras.optimizers.SGD(clr)\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss = 'binary_crossentropy',\n    metrics=[tf.keras.metrics.Precision(), \"accuracy\",\"MeanSquaredError\", \"categorical_accuracy\"]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = tf.keras.models.Sequential([\n    \n#     tf.keras.applications.InceptionV3(\n#         include_top=False,\n#         weights=\"imagenet\",\n#         input_tensor=None,\n#         pooling=None,\n#         classifier_activation=\"relu\",\n#     ),\n#     tf.keras.layers.GlobalMaxPool2D(),\n#     tf.keras.layers.Dense(6, activation=\"sigmoid\")\n# ])\n\n# optimizer = tf.keras.optimizers.Adam()\n\n# model.compile(\n#     optimizer=optimizer,\n#     loss=\"binary_crossentropy\",\n#     metrics=[tf.keras.metrics.Precision(), \"accuracy\",\"MeanSquaredError\", \"categorical_accuracy\"]\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_datagen, validation_data=val_datagen, epochs=5)\n\n# Evaluate how well model performs\nmodel.evaluate(val_datagen, verbose=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from matplotlib import pyplot as plt\n# history = model.fit(train_datagen, epochs=1)\n# plt.plot(history.history['loss'])\n# plt.plot(history.history['accuracy'])\n# plt.title('model accuracy')\n# plt.ylabel('accuracy')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'val'], loc='upper left')\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# i = 3\n\n# for i in range(30):\n#     x = val_datagen.__getitem__(i)[0][0:8]\n#     y = val_datagen.__getitem__(i)[1][0:8]\n\n#     res = model.predict([x])\n\n#     # print('x',x)\n#     print('y',y)\n#     print('res',res)\n    \n#     print('-------------------------')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Manually Getting Data by Reading DIcom FIle","metadata":{}},{"cell_type":"code","source":"# subset_train_arr = train_images[START:START+SIZE]\n# sub_train_pixel_arr = [ final_windowing(train_images_dir + img)  for img in subset_train_arr]\n# subset_train_arr = [x.replace('.dcm', '') for x in subset_train_arr]\n# output = df.query(\"ID == @subset_train_arr\",).reset_index()\n# print(output.head())\n# sub_train_pixel_arr = np.array(sub_train_pixel_arr)\n# print(sub_train_pixel_arr.shape)\n# output_arr = output.drop(columns=['index','ID']).values\n# print(output_arr.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"# x = model.predict(X_training[0:20])\n# x[x<.5] = 0\n# x[x>=.5] = 1\n# print(x)\n# print(y_training[0:20])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss_per_epoch = model.history.history['loss']\n# plt.plot(range(len(loss_per_epoch)),loss_per_epoch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}