{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport pydicom\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nimport torch\nfrom skimage import io, transform\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport torch.nn as nn            # for torch.nn.Module, the parent object for PyTorch models\nimport torch.nn.functional as F\nimport os\nfrom tqdm import tqdm\n\n# gpus = tf.config.experimental.list_physical_devices('GPU')\n# for gpu in gpus:\n#     tf.config.experimental.set_memory_growth(gpu, True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-02T20:30:54.795098Z","iopub.execute_input":"2022-07-02T20:30:54.795818Z","iopub.status.idle":"2022-07-02T20:30:59.572952Z","shell.execute_reply.started":"2022-07-02T20:30:54.795664Z","shell.execute_reply":"2022-07-02T20:30:59.571464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Globals","metadata":{}},{"cell_type":"code","source":"SIZE = 100\nSTART = 0\nINPUT_PATH = \"../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/\"\ntrain_images_dir = INPUT_PATH + 'stage_2_train/'\ntest_images_dir = INPUT_PATH + 'stage_2_test/'","metadata":{"execution":{"iopub.status.busy":"2022-07-02T20:30:59.575814Z","iopub.execute_input":"2022-07-02T20:30:59.577674Z","iopub.status.idle":"2022-07-02T20:30:59.58368Z","shell.execute_reply.started":"2022-07-02T20:30:59.577612Z","shell.execute_reply":"2022-07-02T20:30:59.582587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data\nWe will load data with pandas","metadata":{}},{"cell_type":"code","source":"def load_dataframe(csv_file):\n    info = pd.read_csv(csv_file)\n    info['Sub_type'] = info['ID'].str.split(\"_\", n = 3, expand = True)[2]\n    info['ID'] = info['ID'].str.split(\"_\", n = 3, expand = True)[0]+'_'+info['ID'].str.split(\"_\", n = 3, expand = True)[1]\n    info = info.sort_values(by=['ID','Sub_type'], ignore_index=True)\n\n    info['any'] = np.where(info['Sub_type'] == 'any', info['Label'], 0)\n    info['epidural'] = np.where(info['Sub_type'] == 'epidural', info['Label'], 0)\n    info['intraparenchymal'] = np.where(info['Sub_type'] == 'intraparenchymal', info['Label'], 0)\n    info['intraventricular'] = np.where(info['Sub_type'] == 'intraventricular', info['Label'], 0)\n    info['subarachnoid'] = np.where(info['Sub_type'] == 'subarachnoid', info['Label'], 0)\n    info['subdural'] = np.where(info['Sub_type'] == 'subdural', info['Label'], 0)\n\n    df = info.drop(columns=['Label', 'Sub_type']).groupby('ID').sum().reset_index()\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-07-02T20:30:59.584855Z","iopub.execute_input":"2022-07-02T20:30:59.585614Z","iopub.status.idle":"2022-07-02T20:30:59.599711Z","shell.execute_reply.started":"2022-07-02T20:30:59.585579Z","shell.execute_reply":"2022-07-02T20:30:59.598756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = load_dataframe(INPUT_PATH + \"stage_2_train.csv\")\ntest_df = load_dataframe(INPUT_PATH + 'stage_2_sample_submission.csv')\nprint(\"Loaded!!!\")","metadata":{"execution":{"iopub.status.busy":"2022-07-02T20:30:59.602172Z","iopub.execute_input":"2022-07-02T20:30:59.602788Z","iopub.status.idle":"2022-07-02T20:32:39.193585Z","shell.execute_reply.started":"2022-07-02T20:30:59.602748Z","shell.execute_reply":"2022-07-02T20:32:39.192151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfw = df.copy()\nzeros = df.copy()\n# dfw = dfw[['ID','any']]\nzeros = zeros[zeros['any'] == 0]\n\nprint(\"any:\", len(dfw[dfw['any']==1]))\nprint(\"epidural:\", len(dfw[dfw['epidural']==1]))\nprint(\"intraparenchymal:\", len(dfw[dfw['intraparenchymal']==1]))\nprint(\"intraventricular\", len(dfw[dfw['intraventricular']==1]))\nprint(\"subarachnoid:\", len(dfw[dfw['subarachnoid']==1]))\nprint(\"subdural:\", len(dfw[dfw['subdural']==1]))\n\nzeros_to_be_dropped = np.random.choice(zeros.index, 6*len(zeros)//7, replace=False)\n# dfw = dfw.drop(index=zeros_to_be_dropped)\n# # dfw = dfw.sample(frac=.1)\ntrain, test = train_test_split(dfw, test_size=0.2, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T20:32:39.195696Z","iopub.execute_input":"2022-07-02T20:32:39.196241Z","iopub.status.idle":"2022-07-02T20:32:39.948107Z","shell.execute_reply.started":"2022-07-02T20:32:39.196189Z","shell.execute_reply":"2022-07-02T20:32:39.946701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.reset_index(drop=True)\ntest = test.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T20:32:39.949834Z","iopub.execute_input":"2022-07-02T20:32:39.95024Z","iopub.status.idle":"2022-07-02T20:32:40.055375Z","shell.execute_reply.started":"2022-07-02T20:32:39.950201Z","shell.execute_reply":"2022-07-02T20:32:40.053836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test","metadata":{"execution":{"iopub.status.busy":"2022-07-02T20:32:40.057549Z","iopub.execute_input":"2022-07-02T20:32:40.058774Z","iopub.status.idle":"2022-07-02T20:32:40.064928Z","shell.execute_reply.started":"2022-07-02T20:32:40.058726Z","shell.execute_reply":"2022-07-02T20:32:40.062774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Windowing Function\nIt will take a dicom image and convert into an RGB like image","metadata":{}},{"cell_type":"code","source":"def final_windowing(img_path):\n\n    window_sizes = [(40, 80), (75,215), (600,2800)]\n\n    def preprocess(img_path, WINDOW_LEVEL, WINDOW_WIDTH):\n        # params\n        window_min = WINDOW_LEVEL-(WINDOW_WIDTH // 2)\n        window_max = WINDOW_LEVEL+(WINDOW_WIDTH // 2)\n        # read dicom file\n        r = pydicom.read_file(img_path)\n        # convert to hounsfield unit\n        img = (r.pixel_array * r.RescaleSlope) + r.RescaleIntercept\n        # apply brain window\n        img = np.clip(img, window_min, window_max)\n        img = 255 * ((img - window_min)/WINDOW_WIDTH)\n        img = img.astype(np.uint8)\n        return img\n\n    new_arr = []\n    try:\n        for x in window_sizes:\n            imag = preprocess(img_path, x[0], x[1] )\n            new_arr.append(imag)\n    except:\n        new_arr = []\n        for i in range(3):\n            new_arr.append(np.zeros((512, 512)))\n\n    new_arr_2 = np.dstack((new_arr[0], new_arr[1], new_arr[2]))\n\n    return new_arr_2","metadata":{"execution":{"iopub.status.busy":"2022-07-02T20:32:40.066842Z","iopub.execute_input":"2022-07-02T20:32:40.067334Z","iopub.status.idle":"2022-07-02T20:32:40.080941Z","shell.execute_reply.started":"2022-07-02T20:32:40.067294Z","shell.execute_reply":"2022-07-02T20:32:40.079851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=(20, 10))\ncolumns = 4; rows = 2\nfor i in range(1, rows*rows +1):\n    ds = pydicom.dcmread(train_images_dir + train['ID'][0:20][i] + '.dcm').pixel_array\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(ds, cmap=plt.cm.bone)\n    fig.add_subplot\n\nfor i in range(1, rows*rows +1):\n    ds = final_windowing(train_images_dir + train['ID'][0:20][i] + '.dcm')\n    fig.add_subplot(rows, columns, i+4)\n    plt.imshow(ds, cmap=plt.cm.bone)\n    fig.add_subplot","metadata":{"execution":{"iopub.status.busy":"2022-07-02T20:32:40.08231Z","iopub.execute_input":"2022-07-02T20:32:40.08276Z","iopub.status.idle":"2022-07-02T20:32:41.855829Z","shell.execute_reply.started":"2022-07-02T20:32:40.082721Z","shell.execute_reply":"2022-07-02T20:32:41.85461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Data Loader","metadata":{}},{"cell_type":"code","source":"class HemorrhageImageDataset(Dataset):\n    \"\"\"CT scan dataset.\"\"\"\n\n    def __init__(self, dataframe, root_dir, windowing=final_windowing, transform=transforms.ToTensor()):\n        \"\"\"\n        Args:\n            data_frame (pd.df): Panda Dataframe of the labels.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.(windowing function)\n        \"\"\"\n        self.dataframe = dataframe\n        self.root_dir = root_dir\n        self.transform = transform\n        self.windowing = windowing\n        \n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = self.dataframe.iloc[idx, 0]\n        image_arr = self.windowing(self.root_dir + img_name + '.dcm')\n        image_arr.resize((224, 224, 3), refcheck=False)\n        image = image_arr/256 # normalization\n        \n        labels = self.dataframe.iloc[idx, 1:]\n        labels = np.array([labels])\n        labels = labels.astype('float').reshape(6,)\n        \n        image = image.transpose((2, 0, 1))\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        image = torch.from_numpy(image)\n        labels = torch.from_numpy(labels)\n        \n        return image, labels","metadata":{"execution":{"iopub.status.busy":"2022-07-02T20:32:41.859035Z","iopub.execute_input":"2022-07-02T20:32:41.859969Z","iopub.status.idle":"2022-07-02T20:32:41.872737Z","shell.execute_reply.started":"2022-07-02T20:32:41.859927Z","shell.execute_reply":"2022-07-02T20:32:41.870905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainset = HemorrhageImageDataset(\n    train,\n    train_images_dir\n)\ntrain_loader = DataLoader(\n    trainset, \n    batch_size=1,\n    shuffle=True, \n    num_workers=2\n)\n\n\ntestset = HemorrhageImageDataset(\n    test,\n    train_images_dir\n)\ntest_loader = DataLoader(\n    testset, \n    batch_size=1,\n    shuffle=True, \n    num_workers=2\n)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T20:32:41.875324Z","iopub.execute_input":"2022-07-02T20:32:41.876434Z","iopub.status.idle":"2022-07-02T20:32:41.891847Z","shell.execute_reply.started":"2022-07-02T20:32:41.876372Z","shell.execute_reply":"2022-07-02T20:32:41.890753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for batch, (X, y) in enumerate(dataloader):\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-02T20:32:41.894113Z","iopub.execute_input":"2022-07-02T20:32:41.895116Z","iopub.status.idle":"2022-07-02T20:32:41.904474Z","shell.execute_reply.started":"2022-07-02T20:32:41.895042Z","shell.execute_reply":"2022-07-02T20:32:41.903288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        resnext = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')\n#         inception = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n        self.base = torch.nn.Sequential(*list(resnext.children())[:-1])\n        self.rnn = nn.LSTM(2048, 256, 1, bidirectional=True)\n        self.fc = torch.nn.Linear(512+2048, 6)\n        \n\n    def forward(self, input):\n        #----------------RESNET OUTPUTS----------------\n        res_out = self.base(input).reshape(-1, 2048)\n        #-----------------BLSTM OUTPUTS----------------\n        h0 = torch.randn(2, 256).to(device)\n        c0 = torch.randn(2, 256).to(device)\n        rnn_out, (hn, cn) = self.rnn(res_out, (h0, c0))\n        #----------------CONCATENATION-----------------\n        concatenated = torch.cat((rnn_out, res_out), 1)\n        #----------------FINAL FEATURES-----------------\n        logits = self.fc(concatenated)\n        #----------------------------------------------\n        return logits\n\n\n    \nclass ResNeXtModel(torch.nn.Module):\n    def __init__(self):\n        super(ResNeXtModel, self).__init__()\n        resnext = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')\n        self.base = torch.nn.Sequential(*list(resnext.children())[:-1])\n        self.fc = torch.nn.Linear(2048, 6)\n\n    def forward(self, input):\n        features = self.base(input).reshape(-1, 2048)\n        logits = self.fc(features)\n        return logits\n\n\n    \nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 3, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(3, 3, 5)\n        self.flatten_out_shape = 0\n        self.fc1 = nn.Linear(8427, 1048)\n        self.fc2 = nn.Linear(1048, 128)\n        self.fc3 = nn.Linear(128, 6)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nmodel = ResNeXtModel().to(device=device)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-02T20:32:41.906489Z","iopub.execute_input":"2022-07-02T20:32:41.907532Z","iopub.status.idle":"2022-07-02T20:32:44.400923Z","shell.execute_reply.started":"2022-07-02T20:32:41.907483Z","shell.execute_reply":"2022-07-02T20:32:44.399523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    \n    train_loss, correct = 0, 0\n    \n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        X = X.float().to(device)\n        y = y.float().to(device)\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        \n        \n        train_loss += loss.item()\n        pred_bin = torch.where(pred < .5, 0., 1.)\n        correct += int(torch.sum(torch.prod(pred_bin.eq(y), 1)))\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n            print(f\"train_loss: {train_loss:>7f}   Accuracy: {correct / (batch+1)*32 * 100}\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            X = X.float().to(device)\n            y = y.float().to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            pred = torch.where(pred < .5, 0., 1.)\n            correct += int(torch.sum(torch.prod(pred.eq(y), 1)))\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")","metadata":{"execution":{"iopub.status.busy":"2022-07-02T20:32:44.420382Z","iopub.execute_input":"2022-07-02T20:32:44.42129Z","iopub.status.idle":"2022-07-02T20:32:44.437623Z","shell.execute_reply.started":"2022-07-02T20:32:44.421238Z","shell.execute_reply":"2022-07-02T20:32:44.436169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss Fn","metadata":{}},{"cell_type":"code","source":"# def custom_loss(y, y_t):\n#     y = nn.Sigmoid()(y)\n#     ones = torch.ones(*y.size()).to(device)\n#     loss = y_t*torch.log(y) + (ones-y_t)*torch.log(ones-y)\n#     print(\"y_t:\", y_t)\n#     print(\"y:\", y)\n#     print(\"ones:\", ones)\n#     print(\"losses:\", loss)\n#     print(torch.sum(loss))\n#     return torch.sum(loss)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T20:32:44.439755Z","iopub.execute_input":"2022-07-02T20:32:44.440585Z","iopub.status.idle":"2022-07-02T20:32:44.449642Z","shell.execute_reply.started":"2022-07-02T20:32:44.440529Z","shell.execute_reply":"2022-07-02T20:32:44.448642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Trainning","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\nlearning_rate = 5e-4\nloss_fn = nn.BCEWithLogitsLoss()\n# loss_fn = custom_loss\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\nepochs = 10\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train_loader, model, loss_fn, optimizer)\n    test_loop(test_loader, model, loss_fn)\nprint(\"Done!\")","metadata":{"execution":{"iopub.status.busy":"2022-07-02T20:32:44.451545Z","iopub.execute_input":"2022-07-02T20:32:44.452338Z","iopub.status.idle":"2022-07-02T20:32:47.653807Z","shell.execute_reply.started":"2022-07-02T20:32:44.452291Z","shell.execute_reply":"2022-07-02T20:32:47.651897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}