{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom numpy import argmax\nimport cv2\nimport glob\nimport pydicom as dicom\nimport random as ran\nfrom scipy.ndimage.interpolation import rotate\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils import shuffle\nfrom skmultilearn.problem_transform import BinaryRelevance\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Dropout\nfrom keras.regularizers import l2, l1\nfrom keras.optimizers import SGD","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ntrain_csv = pd.read_csv('../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/stage_2_train.csv')\n\nprint('Length of train_csv:',str(len(train_csv)))\nprint('Expected length of post-transform train_labels:', str(int(len(train_csv)/6)))\n\ntrain_csv = train_csv[~train_csv.ID.str.contains('any')]\ntrain_csv['hem'] = train_csv['ID'].str.split('_').str[2]\n\ntrain_labels_pos = train_csv.loc[train_csv['Label']==1].groupby([train_csv['ID'].str.split('_').str[1]\n                                                                ])['hem'].apply(lambda x: \"%s\" % '_'.join(x))\ntrain_labels_neg = train_csv.groupby([train_csv[\"ID\"].str.split(\"_\").str[1]]).sum()\ntrain_labels_neg[train_labels_neg == 0] = 'none'\ntrain_labels_neg = train_labels_neg[train_labels_neg == 'none']\ntrain_labels_neg.dropna(inplace=True)\n\ntrain_labels = train_labels_pos.append(train_labels_neg['Label'])\nprint(train_labels.head())\n\ntrain_labels.index = 'ID_'+train_labels.index\ntrain_labels = train_labels.str.split('_')\n\nprint(train_labels.head())\nprint('Training labels created.\\nLength of train_labels: '+str(len(train_labels)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef import_images(total_images, hem_rate):\n    image_arrays = []\n    labels = []\n    image_counter = 0\n    total_images = total_images\n    hem_img_count = 0\n    total_hem_img_count = int(total_images*hem_rate)\n\n    files = glob.glob(\"../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/stage_2_train/*.dcm\")\n    #files = glob.glob(\"./data/stage_1_train_images/*.dcm\")\n    ran.shuffle(files)\n    ran.shuffle(files)\n    ran.shuffle(files)\n\n    for im_file in files:\n        if image_counter < total_images:\n            try:\n                image = dicom.dcmread(im_file)\n                image_array = image.pixel_array\n                image_array_resized = cv2.resize(image_array,(50,50))\n                image_label = train_labels.loc[train_labels.index==str(image.SOPInstanceUID)][0]\n\n                if 'none' not in image_label and hem_img_count < total_hem_img_count:\n                    if image_counter % 1000 == 0 and image_counter != 0:\n                        print(str(image_counter), 'images imported.')\n                    image_arrays.append(image_array_resized)\n                    labels.append(image_label)\n                    hem_img_count += 1\n                    image_counter += 1\n                elif hem_img_count >= total_hem_img_count:\n                    if image_counter % 1000 == 0 and image_counter != 0:\n                        print(str(image_counter), 'images imported.')\n                    image_arrays.append(image_array_resized)\n                    labels.append(image_label)\n                    image_counter += 1    \n            except:\n                pass\n        else:\n            break\n\n    print('Import complete.'+'\\n\\n'+'images: ' + str(image_counter)+'\\n')\n\n    image_arrays, labels = shuffle(np.asarray(image_arrays), np.asarray(labels), random_state=0)\n    data = {'images': image_arrays, 'labels': labels}\n    \n    print(data['images'][0],'\\n\\n',data['labels'][0])\n    \n    return data\ndata = import_images(30000, .75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data_array = data['images']\ntraining_labels_array = data['labels']\nnp.save('training_data_array.npy', training_data_array)\nnp.save('training_labels_array.npy', training_labels_array)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = import_images(25000, .75)\n\ntest_data_array = test_data['images']\ntest_labels_array = test_data['labels']\nnp.save('test_data_array.npy', test_data_array)\nnp.save('test_labels_array.npy', test_labels_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data_array = np.load('training_data_array.npy',allow_pickle=True)\ntraining_labels_array = np.load('training_labels_array.npy',allow_pickle=True)\n\ntdata = {'images':training_data_array,'labels':training_labels_array}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-------------------------------------\n#------------\n#-------------------------------------\n#------------\n#-------------------------------------\n#------------\n#-------------------------------------\n#------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data_array1 = np.load('../input/sam-data/training_data_array.npy',allow_pickle=True)\ntraining_labels_array1 = np.load('../input/sam-data/training_labels_array.npy',allow_pickle=True)\ntraining_data_array2 = np.load('../input/sam-data/training_data_array_2.npy',allow_pickle=True)\ntraining_labels_array2 = np.load('../input/sam-data/training_labels_array_2.npy',allow_pickle=True)\n\ntest_data_array = np.load('../input/sam-data/test_data_array.npy',allow_pickle=True)\ntest_labels_array = np.load('../input/sam-data/test_labels_array.npy',allow_pickle=True)\nprint(training_data_array1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data_array= np.append(training_data_array1,training_data_array2)\ntraining_labels_array = np.append(training_labels_array1,training_labels_array2)\ntraining_data_array = np.reshape(training_data_array,(20000,50,50))\n\ndata={'images':training_data_array,'labels':training_labels_array}\ntdata = {'images':training_data_array,'labels':training_labels_array}\nprint(training_data_array.shape)\n\ntest_data = {'images':test_data_array,'labels':test_labels_array}\n\nprint(training_data_array[0])\nprint(test_data_array.shape)\nprint(training_labels_array.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hemorrhages = {'epidural':0,'intraparenchymal':0,'intraventricular':0,'subarachnoid':0,'subdural':0 \\\n               ,'any':0,'none':0}\nunique_labels, label_counts = np.unique(data['labels'], return_counts=True)\n\nindex = 0\nfor i in unique_labels:\n    print(i,'-- Occurences:',label_counts[index])\n    for k,v in hemorrhages.items():\n        if k in i:\n            hemorrhages[k] += int(label_counts[index])\n    index += 1\n\nax = sns.barplot(x=list(hemorrhages.keys()), y=list(hemorrhages.values()))\nax.title.set_text('Label Occurences')\nax.set_xticklabels(labels = list(hemorrhages.keys()),rotation=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = {'subdural':[],'epidural':[],'intraparenchymal':[],'intraventricular':[],'subarachnoid':[],'none':[]}\n\nfor i in data['labels']:\n    for k,v in labels.items():\n        if k in i:\n            v.append(True)\n        else:\n            v.append(False)\n\nlabel_ovlp_df = pd.DataFrame(labels)\nprint(label_ovlp_df.head())\n\nlabel_ovlp_rt_df = pd.DataFrame(index=list(labels.keys()), columns=list(labels.keys()))\nlabel_ovlp_rt_df = label_ovlp_rt_df.astype(float)\n\nfor i in list(labels.keys()):\n    for j in list(labels.keys()):\n        label_ovlp_rt_df.loc[i,j] = len(label_ovlp_df[label_ovlp_df[[i,j]].eq(True).all(axis=1)])/ \\\n            len(label_ovlp_df[label_ovlp_df[i].eq(True)])\n\nax = sns.heatmap(label_ovlp_rt_df, vmin=0, vmax=1, annot=True, fmt='.2f', linewidth=.5, cmap='Blues')\nax.title.set_text('Rate of Y = True when X = True')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epid_indexes = []\nsudu_indexes = []\ninpa_indexes = []\ninve_indexes = []\nsuar_indexes = []\nnone_indexes = []\n\nit = np.nditer(data['labels'], flags=['f_index', 'refs_ok'])\nwhile not it.finished:\n    if 'epidural' in it[0].tolist():\n        epid_indexes.append(it.index)\n    if 'subdural' in it[0].tolist():\n        sudu_indexes.append(it.index)\n    if 'intraparenchymal' in it[0].tolist():\n        inpa_indexes.append(it.index)\n    if 'intraventricular' in it[0].tolist():\n        inve_indexes.append(it.index)\n    if 'subarachnoid' in it[0].tolist():\n        suar_indexes.append(it.index)\n    if 'none' in it[0].tolist():\n        none_indexes.append(it.index)\n    it.iternext()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_temp = tdata['images']/255\n\nX = np.empty(shape=[X_temp.shape[0]] + [2500], dtype='float32')\nprint(X_temp.shape)\n\nfor im in range(X_temp.shape[0]):\n    X[im,:] = X_temp[im,:,:].flatten()\n\nmlb = MultiLabelBinarizer()\ny = mlb.fit_transform(tdata['labels'])\n\nprint(y[:10,:])\nprint(X_temp.shape)\nprint(X.shape)\nprint(y.shape)\nX[0,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=50)\n\nprint (X_train.shape)\nprint (y_train.shape)\nstdscaler = preprocessing.StandardScaler().fit(X_train)\n\nX_train_scaled = stdscaler.transform(X_train)\nX_test_scaled  = stdscaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_count = len(mlb.classes_)\n\nmodel = Sequential()\nmodel.add(Dense(label_count*24, input_shape=[2500], activation='relu', W_regularizer=l2(0.1)))\nmodel.add(Dense(label_count*18, activation='relu', W_regularizer=l2(0.01)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(label_count*12, activation='relu', W_regularizer=l2(0.01)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(label_count*8, activation='relu', W_regularizer=l2(0.001)))\nmodel.add(Dense(label_count, activation='sigmoid', W_regularizer=l1(0.001)))\n\n\nsgd = SGD(lr=0.5)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train_scaled[0])\nprint(y_train[0])\nprint(X_test_scaled[0])\nprint(y_test[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train_scaled, y_train, batch_size = 256, \n                    epochs = 50, verbose=2, validation_data=(X_test_scaled, y_test))\nfig = plt.figure(figsize=(6,4))\n\n# Summary of loss history\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'], 'g--')\nplt.title('Model Loss')\nplt.ylabel('Binary Crossentropy')\nplt.xlabel('Epoch')\nplt.legend(['Training Loss', 'Testing Loss'], loc='upper right')\nplt.title('Loss After Final Iteration: '+str(history.history['val_loss'][-1]))\nprint (\"BC after final iteration: \", history.history['val_loss'][-1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(6,4))\n\n# Summary of accuracy history\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'], 'g--')\nplt.title('Model Accuracy')\nplt.ylabel('Model Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Training Accuracy', 'Testing Accuracy'], loc='lower left')\nplt.title('Accuracy After Final Iteration: '+str(history.history['val_accuracy'][-1]))\nprint (\"Accuracy after final iteration: \", history.history['val_accuracy'][-1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X_temp = test_data['images']/255\n\ntest_X = np.empty(shape=[test_X_temp.shape[0]] + [2500], dtype='float32')\nprint(test_X_temp.shape)\n\nfor im in range(test_X_temp.shape[0]):\n    test_X[im,:] = test_X_temp[im,:,:].flatten()\n\ntest_mlb = MultiLabelBinarizer()\ntest_y = test_mlb.fit_transform(test_data['labels'])\n\nprint(test_y[:10])\nprint(test_X_temp.shape)\nprint(test_X.shape)\nprint(test_y.shape)\ntest_X[0,:]\n\ntest_stdscaler = preprocessing.StandardScaler().fit(test_X)\n\ntest_X_scaled = test_stdscaler.transform(test_X)\n#test_X_test_scaled  = test_stdscaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_y_pred = model.predict(test_X_scaled, batch_size=256, verbose=2)\ntest_y_actual = np.argmax(test_y_pred, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"Predicted Prob= \",test_y_pred[0])\nprint(\"Actual= \",test_y[0])\ntest_y_pred_rounded = np.around(test_y_pred)\ntest_y_pred_rounded[4]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}