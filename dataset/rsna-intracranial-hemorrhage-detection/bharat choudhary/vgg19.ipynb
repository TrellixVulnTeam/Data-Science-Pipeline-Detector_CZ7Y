{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import os\nimport pickle\nimport random\nimport glob\nimport datetime\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport pydicom\nfrom tqdm import tqdm\nfrom joblib import delayed, Parallel\nimport zipfile\nfrom pydicom.filebase import DicomBytesIO\nimport sys\nfrom PIL import Image\nimport cv2\n#from focal_loss import sparse_categorical_focal_loss\nimport keras\n#import tensorflow_addons as tfa\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import model_from_json\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, GlobalAveragePooling2D, Dropout\nfrom keras.applications.inception_v3 import InceptionV3\n\n# importing pyplot and image from matplotlib \nimport matplotlib.pyplot as plt \nimport matplotlib.image as mpimg \nfrom keras.optimizers import SGD\nfrom keras import backend\nfrom keras.models import load_model\n\nfrom keras.preprocessing import image\nimport albumentations as A\n\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from tensorflow.keras.losses import Reduction\n\nfrom tensorflow_addons.losses import SigmoidFocalCrossEntropy","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"base_url = '/home/ubuntu/kaggle/rsna-intracranial-hemorrhage-detection/'\nTRAIN_DIR = '/home/ubuntu/kaggle/rsna-intracranial-hemorrhage-detection/stage_2_train/'\nTEST_DIR = '/home/ubuntu/kaggle/rsna-intracranial-hemorrhage-detection/stage_2_test/'\nimage_dir = '/home/ubuntu/kaggle/rsna-intracranial-hemorrhage-detection/png/train/adjacent-brain-cropped/'\nsave_dir = 'home/ubuntu/kaggle/models/'\nos.listdir(base_url)\n\ndef png(image): \n    return image + '.png'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# learning rate","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"initial_learning_rate = 1e-2\nfirst_decay_steps = 1000\nlr_decayed_fn = (\n  tf.keras.experimental.CosineDecayRestarts(\n      initial_learning_rate,\n      first_decay_steps))\nopt = tf.keras.optimizers.SGD(learning_rate=lr_decayed_fn, nesterov=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generator","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train_idg = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        shear_range=0.05,\n        rotation_range=50, # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,\n        rescale=1./255)\nvalid_idg = ImageDataGenerator(rescale=1./255)\ntraining_data = pd.read_csv(f'train_0.csv') \ntraining_data['Image'] = training_data['Image'].apply(png)\n\nvalidation_data = pd.read_csv(f'valid_0.csv')\nvalidation_data['Image'] = validation_data['Image'].apply(png)\n\ncolumns=['any','epidural','intraparenchymal','intraventricular', 'subarachnoid','subdural']\n\n#train_data_generator = train_idg.flow_from_dataframe(training_data, directory = image_dir,\n#                           x_col = \"Image\", y_col = columns,batch_size=64,\n#                           class_mode=\"raw\", target_size=(224,224), shuffle = True)\n#valid_data_generator  = valid_idg.flow_from_dataframe(validation_data, directory = image_dir,\n#                        x_col = \"Image\", y_col = columns,batch_size=64,\n#                        class_mode = \"raw\",target_size=(224,224), shuffle = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_under_generator = train_idg.flow_from_dataframe(l, directory = image_dir,\n                           x_col = \"Image\", y_col = columns,batch_size=64,\n                           class_mode=\"raw\", target_size=(224,224), shuffle = True)\nvalid_under_generator  = valid_idg.flow_from_dataframe(m, directory = image_dir,\n                        x_col = \"Image\", y_col = columns,batch_size=64,\n                        class_mode = \"raw\",target_size=(224,224), shuffle = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Undersamping","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def undersample(dataframe,steps,batch_size):\n        part = np.int(steps/3 * batch_size)\n        zero_ids = np.random.choice(dataframe.loc[dataframe[\"any\"] == 0].index.values, size=2*part, replace=False)\n        hot_ids = np.random.choice(dataframe.loc[dataframe[\"any\"] == 1].index.values, size=1*part, replace=False)\n        data_ids = list(set(zero_ids).union(hot_ids))\n        np.random.shuffle(data_ids)\n        return data_ids\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_indices = undersample(training_data, 8050,32)\nprint(len(train_indices))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"valid_indices = undersample(validation_data, 2010,32)\nprint(len(valid_indices))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"l = training_data[training_data.index.isin(train_indices)]\nm = validation_data[validation_data.index.isin(valid_indices)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n\n\nbase_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n\nfor layer in base_model.layers[:28]:\n    layer.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n\nMETRICS = [\n      tf.keras.metrics.TruePositives(name='tp'),\n      tf.keras.metrics.FalsePositives(name='fp'),\n      tf.keras.metrics.TrueNegatives(name='tn'),\n      tf.keras.metrics.FalseNegatives(name='fn'), \n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall'),\n      tf.keras.metrics.AUC(name='auc')\n      \n]\n\n\n# create the base pre-trained model\nbase_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\nprint('Model loaded.')\n\n\n\n# add a global spatial average pooling layer\nx = base_model.output\n\nx = GlobalAveragePooling2D()(x)\nnet = Dense(256, activation='elu')(x)\nnet = Dense(6, activation='sigmoid')(net)\n\n\n# this is the model we will train\nmodel = Model(inputs=base_model.input, outputs=net)\n\n\n\n# first: train only the top layers (which were randomly initialized)\n# i.e. freeze all convolutional InceptionV3 layers\n#for layer in base_model.layers:\n#    layer.trainable = False\nfor layer in base_model.layers[:28]:\n    layer.trainable = False\n\n\n# compile the model (should be done *after* setting layers to non-trainable)\nmodel.compile(opt, loss='binary_crossentropy', metrics=METRICS)\n\n\n\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Callback","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from keras import backend as K\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint('/kaggle/models/mobilenetv2_{epoch:08d}.h5', period=1,mode= 'auto',save_best_only=True) \n\nlearning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n\ncallback_list = [checkpoint]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tf.config.experimental_run_functions_eagerly(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class_weight = {0:1.0,1:2.0}\nclass_weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n\n\nnum_epochs = 20\n\nbatch_size = 512\ntraining_steps = len(training_data) // batch_size\nvalidation_step = len(validation_data) // batch_size\n\n\n\n\n\n# FIT THE MODEL\nhistory = model.fit(train_under_generator,\n            epochs=num_epochs,\n            steps_per_epoch=training_steps,\n            callbacks=callback_list,\n            class_weight=class_weight,\n            validation_data=valid_under_generator,\n            validation_steps= validation_step\n                   ) \n\n\n\n\n\ntf.keras.backend.clear_session()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evalution","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"valid_predict =  model.evaluate_generator(valid_under_generator)\nprint(valid_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model.metrics_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('\\n---------------\\n')\nprint('validation data **loss** value =', valid_predict[0])\nprint('\\n---------------\\n')\nprint('validation data **true positive** value = ', valid_predict[1])\nprint('\\n---------------\\n')\nprint('validation data **false positive** value =', valid_predict[2])\nprint('\\n---------------\\n')\nprint('validation data **true negative** value =', valid_predict[3])\nprint('\\n---------------\\n')\nprint('validation data **false negative** value =', valid_predict[4])\nprint('\\n---------------\\n')\nprint('validation data **accuracy** value = ', valid_predict[5])\nprint('\\n---------------\\n')\nprint('validation data **precision** value =', valid_predict[6])\nprint('\\n---------------\\n')\nprint('validation data **recall* value =', valid_predict[7])\nprint('\\n---------------\\n')\nprint('validation data **AUC* value =', valid_predict[8])\nprint('\\n---------------\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_true = m[['any','epidural','intraparenchymal','intraventricular', 'subarachnoid','subdural']].reset_index(drop=True)\n\nY_pred = model.predict_generator(valid_under_generator)\npreds = np.where(Y_pred < 0.25, 0, 1)\n\n\n\n#val = 0.25\n\n#Y_pred[Y_pred>=val]=1\n#Y_pred[Y_pred<val]=0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classifcation Report","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Classification Report')\ntarget_names = ['any','epidural','intraparenchymal','intraventricular', 'subarachnoid','subdural']\nprint(classification_report(y_true, preds, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import multilabel_confusion_matrix\n# Creating multilabel confusion matrix\nconfusion = multilabel_confusion_matrix(y_true, preds)\nmlb= ['any','epidural','intraparenchymal','intraventricular', 'subarachnoid','subdural']\n# Plot confusion matrix \nfig = plt.figure(figsize = (14, 8))\nfor i, (label, matrix) in enumerate(zip(mlb, confusion)):\n    plt.subplot(f'23{i+1}')\n    labels = [f'not_{label}', label]\n    sns.heatmap(matrix, annot = True, square = True, fmt = 'd', cbar = False, cmap = 'Blues', \n                xticklabels = labels, yticklabels = labels, linecolor = 'black', linewidth = 1)\n    plt.title(labels[0])\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# AUC_ROC_SCORE","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"auc = roc_auc_score(y_true, preds)\nprint(auc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ACCURACY AND LOSS PLOT","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def plot_training(H):\n    # construct a plot that plots and saves the training history\n    with plt.xkcd():\n        plt.figure(figsize = (10,10))\n        plt.plot(H.epoch,H.history[\"accuracy\"], label=\"train_acc\")\n        plt.plot(H.epoch,H.history[\"val_accuracy\"], label=\"val_acc\")\n        plt.title(\"Training Accuracy\")\n        plt.xlabel(\"Epoch #\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend(loc=\"lower left\")\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_training(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def plot_training(H):\n    # construct a plot that plots and saves the training history\n    with plt.xkcd():\n        plt.figure(figsize = (10,10))\n        plt.plot(H.epoch,H.history[\"loss\"], label=\"train_loss\")\n        plt.plot(H.epoch,H.history[\"val_loss\"], label=\"val_loss\")\n        plt.title(\"Training Loss\")\n        plt.xlabel(\"Epoch #\")\n        plt.ylabel(\"Loss\")\n        plt.legend(loc=\"lower left\")\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_training(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}