{"cells":[{"metadata":{},"cell_type":"markdown","source":"This competition provides an exciting and challenging task of doing multi-label classification on a dataset with well over half a million images. There are multiple very nice notebooks which perform only 2 or 3 epochs with all the training data. In this notebook I will try out and see what the effect is of using more epochs but less steps per epoch. By averaging the predictions made during the last few epochs we should be able to achieve a nice LB score. This also should provide some alternative ways to experiment for the Kagglers that don't have the adequate computing resources available and are dependent on Kaggle Kernels.\n\nAs model I will be using the EfficientNet B2 model. It should be able to provide highly accurate predictions while still being able to run within the kernel limits. With 9 hours max time for a GPU kernel you have to make some trade-offs ;-)\n\nI hope this kernel will be usefull and may'be will provide you with some new and alternative ideas to try out. If you like it..then please upvote it ;-)\nAny feedback or remarks are appreciated.\n\nLets start by importing all the necessary modules."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pydicom\nimport os\nimport collections\nimport sys\nimport glob\nimport random\nimport cv2\nimport tensorflow as tf\nimport multiprocessing\n\nfrom math import ceil, floor\nfrom copy import deepcopy\nfrom tqdm import tqdm\nfrom imgaug import augmenters as iaa\n\nimport keras\nimport keras.backend as K\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.layers import Dense, Flatten, Dropout\nfrom keras.models import Model, load_model\nfrom keras.utils import Sequence\nfrom keras.losses import binary_crossentropy\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Install and import the efficientnet and iterative-stratification packages from the internet. The iterative-stratification package provides a very nice implementation of multi-label stratification. I've used it in a few competitions now with good results. There are offcourse more packages that provide implementations for it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Install Modules from internet\n!pip install efficientnet\n!pip install iterative-stratification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Custom Modules\nimport efficientnet.keras as efn \nfrom iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we will set the random_state, some constants and folders that will be used later on. I've specified a rather small test size as I want to maximize the training time available and minimize the time used for validation. I'am not using methods like early stopping...when the kernel time limit is approaching we could still increase the results on the LB if we were allowed to continue."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Seed\nSEED = 11\nnp.random.seed(SEED)\ntf.set_random_seed(SEED)\n\n# Constants\nTEST_SIZE = 0.06\nHEIGHT = 300\nWIDTH = 300\nTRAIN_BATCH_SIZE = 24\nVALID_BATCH_SIZE = 24\n\n# Folders\nDATA_DIR = '/kaggle/input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/'\nTEST_IMAGES_DIR = DATA_DIR + 'stage_2_test/'\nTRAIN_IMAGES_DIR = DATA_DIR + 'stage_2_train/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next the code for the DICOM windowing and the Data Generators. Some part of the code are partially based on an earlier version of this very nice [kernel](https://www.kaggle.com/akensert/inceptionv3-prev-resnet50-keras-baseline-model)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_first_of_dicom_field_as_int(x):\n    if type(x) == pydicom.multival.MultiValue:\n        return int(x[0])\n    else:\n        return int(x)\n\ndef _get_windowing(data):\n    dicom_fields = [data.WindowCenter, data.WindowWidth, data.RescaleSlope, data.RescaleIntercept]\n    return [_get_first_of_dicom_field_as_int(x) for x in dicom_fields]\n\ndef _window_image(img, window_center, window_width, slope, intercept):\n    img = (img * slope + intercept)\n    img_min = window_center - window_width//2\n    img_max = window_center + window_width//2\n    img[img<img_min] = img_min\n    img[img>img_max] = img_max\n    return img \n\ndef _normalize(img):\n    if img.max() == img.min():\n        return np.zeros(img.shape)\n    return 2 * (img - img.min())/(img.max() - img.min()) - 1\n\ndef _read(path, desired_size=(224, 224)):\n    dcm = pydicom.dcmread(path)\n    window_params = _get_windowing(dcm) # (center, width, slope, intercept)\n\n    try:\n        # dcm.pixel_array might be corrupt (one case so far)\n        img = _window_image(dcm.pixel_array, *window_params)\n    except:\n        img = np.zeros(desired_size)\n\n    img = _normalize(img)\n\n    if desired_size != (512, 512):\n        # resize image\n        img = cv2.resize(img, desired_size, interpolation = cv2.INTER_LINEAR)\n    return img[:,:,np.newaxis]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'll specify some light image augmentation. Only some horizontal flipping and some cropping. I haven't yet tried out more augmentation but will do so in future versions of the kernel. Also the code for Data Generators for train and test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Image Augmentation\nsometimes = lambda aug: iaa.Sometimes(0.25, aug)\naugmentation = iaa.Sequential([  \n                                iaa.Fliplr(0.25),\n                                sometimes(iaa.Crop(px=(0, 25), keep_size = True, sample_independently = False))   \n                            ], random_order = True)       \n        \n# Generators\nclass TrainDataGenerator(keras.utils.Sequence):\n\n    def __init__(self, dataset, labels, batch_size=16, img_size=(512, 512), img_dir = TRAIN_IMAGES_DIR, augment = False, *args, **kwargs):\n        self.dataset = dataset\n        self.ids = dataset.index\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.img_dir = img_dir\n        self.augment = augment\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(ceil(len(self.ids) / self.batch_size))\n\n    def __getitem__(self, index):\n        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n        X, Y = self.__data_generation(indices)\n        return X, Y\n\n    def augmentor(self, image):\n        augment_img = augmentation        \n        image_aug = augment_img.augment_image(image)\n        return image_aug\n\n    def on_epoch_end(self):\n        self.indices = np.arange(len(self.ids))\n        np.random.shuffle(self.indices)\n\n    def __data_generation(self, indices):\n        X = np.empty((self.batch_size, *self.img_size, 3))\n        Y = np.empty((self.batch_size, 6), dtype=np.float32)\n        \n        for i, index in enumerate(indices):\n            ID = self.ids[index]\n            image = _read(self.img_dir+ID+\".dcm\", self.img_size)\n            if self.augment:\n                X[i,] = self.augmentor(image)\n            else:\n                X[i,] = image            \n            Y[i,] = self.labels.iloc[index].values        \n        return X, Y\n    \nclass TestDataGenerator(keras.utils.Sequence):\n    def __init__(self, ids, labels, batch_size = 5, img_size = (512, 512), img_dir = TEST_IMAGES_DIR, *args, **kwargs):\n        self.ids = ids\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.img_dir = img_dir\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(ceil(len(self.ids) / self.batch_size))\n\n    def __getitem__(self, index):\n        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.ids[k] for k in indices]\n        X = self.__data_generation(list_IDs_temp)\n        return X\n\n    def on_epoch_end(self):\n        self.indices = np.arange(len(self.ids))\n\n    def __data_generation(self, list_IDs_temp):\n        X = np.empty((self.batch_size, *self.img_size, 3))\n        for i, ID in enumerate(list_IDs_temp):\n            image = _read(self.img_dir+ID+\".dcm\", self.img_size)\n            X[i,] = image            \n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Import the training and test datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_testset(filename = DATA_DIR + \"stage_2_sample_submission.csv\"):\n    df = pd.read_csv(filename)\n    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n    return df\n\ndef read_trainset(filename = DATA_DIR + \"stage_2_train.csv\"):\n    df = pd.read_csv(filename)\n    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n    duplicates_to_remove = [56346, 56347, 56348, 56349,\n                            56350, 56351, 1171830, 1171831,\n                            1171832, 1171833, 1171834, 1171835,\n                            3705312, 3705313, 3705314, 3705315,\n                            3705316, 3705317, 3842478, 3842479,\n                            3842480, 3842481, 3842482, 3842483 ]\n    df = df.drop(index = duplicates_to_remove)\n    df = df.reset_index(drop = True)    \n    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n    return df\n\n# Read Train and Test Datasets\ntest_df = read_testset()\ntrain_df = read_trainset()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training data contains some class inbalance. Multiple kernels explored the use of undersampling..so let's try the opposite and oversample the minority class 'epidural' one additional time."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Oversampling\nepidural_df = train_df[train_df.Label['epidural'] == 1]\ntrain_oversample_df = pd.concat([train_df, epidural_df])\ntrain_df = train_oversample_df\n\n# Summary\nprint('Train Shape: {}'.format(train_df.shape))\nprint('Test Shape: {}'.format(test_df.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some methods for predictions on the test data, a callback method and a method to create the EfficientNet B2 model. For the EfficientNet we use the pretrained imagenet weights. Also a Dropout layer is added with a small value to prevent some overfitting. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def predictions(test_df, model):    \n    test_preds = model.predict_generator(TestDataGenerator(test_df.iloc[range(test_df.shape[0])].index, None, 5, (WIDTH, HEIGHT), TEST_IMAGES_DIR), verbose=1)\n    return test_preds[:test_df.iloc[range(test_df.shape[0])].shape[0]]\n\ndef ModelCheckpointFull(model_name):\n    return ModelCheckpoint(model_name, \n                            monitor = 'val_loss', \n                            verbose = 1, \n                            save_best_only = False, \n                            save_weights_only = True, \n                            mode = 'min', \n                            period = 1)\n\n# Create Model\ndef create_model():\n    K.clear_session()\n    \n    base_model =  efn.EfficientNetB3(weights = 'imagenet', \n                                     include_top = False, \n                                     pooling = 'avg', \n                                     input_shape = (HEIGHT, WIDTH, 3))\n    x = base_model.output\n    x = Dropout(0.125)(x)\n    y_pred = Dense(6, activation = 'sigmoid')(x)\n\n    return Model(inputs = base_model.input, outputs = y_pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Radam Optimizer\nfrom keras import backend as K\nfrom keras.optimizers import Optimizer\n\n\n# Ported from https://github.com/LiyuanLucasLiu/RAdam/blob/master/radam.py\nclass RectifiedAdam(Optimizer):\n    \"\"\"RectifiedAdam optimizer.\n    Default parameters follow those provided in the original paper.\n    # Arguments\n        lr: float >= 0. Learning rate.\n        final_lr: float >= 0. Final learning rate.\n        beta_1: float, 0 < beta < 1. Generally close to 1.\n        beta_2: float, 0 < beta < 1. Generally close to 1.\n        gamma: float >= 0. Convergence speed of the bound function.\n        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n        decay: float >= 0. Learning rate decay over each update.\n        weight_decay: Weight decay weight.\n        amsbound: boolean. Whether to apply the AMSBound variant of this\n            algorithm.\n    # References\n        - [On the Variance of the Adaptive Learning Rate and Beyond]\n          (https://arxiv.org/abs/1908.03265)\n        - [Adam - A Method for Stochastic Optimization]\n          (https://arxiv.org/abs/1412.6980v8)\n        - [On the Convergence of Adam and Beyond]\n          (https://openreview.net/forum?id=ryQu7f-RZ)\n    \"\"\"\n\n    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, decay=0., weight_decay=0.0, **kwargs):\n        super(RectifiedAdam, self).__init__(**kwargs)\n\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.learning_rate = K.variable(lr, name='lr')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n            self.decay = K.variable(decay, name='decay')\n\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_decay = decay\n\n        self.weight_decay = float(weight_decay)\n\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.learning_rate\n        if self.initial_decay > 0:\n            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                      K.dtype(self.decay))))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        self.weights = [self.iterations] + ms + vs\n\n        for p, g, m, v in zip(params, grads, ms, vs):\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n\n            beta2_t = self.beta_2 ** t\n            N_sma_max = 2 / (1 - self.beta_2) - 1\n            N_sma = N_sma_max - 2 * t * beta2_t / (1 - beta2_t)\n\n            # apply weight decay\n            if self.weight_decay != 0.:\n                p_wd = p - self.weight_decay * lr * p\n            else:\n                p_wd = None\n\n            if p_wd is None:\n                p_ = p\n            else:\n                p_ = p_wd\n\n            def gt_path():\n                step_size = lr * K.sqrt(\n                    (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max /\n                    (N_sma_max - 2)) / (1 - self.beta_1 ** t)\n\n                denom = K.sqrt(v_t) + self.epsilon\n                p_t = p_ - step_size * (m_t / denom)\n\n                return p_t\n\n            def lt_path():\n                step_size = lr / (1 - self.beta_1 ** t)\n                p_t = p_ - step_size * m_t\n\n                return p_t\n\n            p_t = K.switch(N_sma > 5, gt_path, lt_path)\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.learning_rate)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon,\n                  'weight_decay': self.weight_decay}\n        base_config = super(RectifiedAdam, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loss function definition courtesy https://www.kaggle.com/akensert/resnet50-keras-baseline-model\nfrom keras import backend as K\n\ndef logloss(y_true,y_pred):      \n    eps = K.epsilon()\n    \n    class_weights = np.array([2., 1., 1., 1., 1., 1.])\n    \n    y_pred = K.clip(y_pred, eps, 1.0-eps)\n\n    #compute logloss function (vectorised)  \n    out = -( y_true *K.log(y_pred)*class_weights\n            + (1.0 - y_true) * K.log(1.0 - y_pred)*class_weights)\n    return K.mean(out, axis=-1)\n\ndef _normalized_weighted_average(arr, weights=None):\n    \"\"\"\n    A simple Keras implementation that mimics that of \n    numpy.average(), specifically for the this competition\n    \"\"\"\n    \n    if weights is not None:\n        scl = K.sum(weights)\n        weights = K.expand_dims(weights, axis=1)\n        return K.sum(K.dot(arr, weights), axis=1) / scl\n    return K.mean(arr, axis=1)\n\ndef weighted_loss(y_true, y_pred):\n    \"\"\"\n    Will be used as the metric in model.compile()\n    ---------------------------------------------\n    \n    Similar to the custom loss function 'weighted_log_loss()' above\n    but with normalized weights, which should be very similar \n    to the official competition metric:\n        https://www.kaggle.com/kambarakun/lb-probe-weights-n-of-positives-scoring\n    and hence:\n        sklearn.metrics.log_loss with sample weights\n    \"\"\"      \n    \n    eps = K.epsilon()\n    \n    class_weights = K.variable([2., 1., 1., 1., 1., 1.])\n    \n    y_pred = K.clip(y_pred, eps, 1.0-eps)\n\n    loss = -(y_true*K.log(y_pred)\n            + (1.0 - y_true) * K.log(1.0 - y_pred))\n    \n    loss_samples = _normalized_weighted_average(loss,class_weights)\n    \n    return K.mean(loss_samples)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we setup the multi label stratification. I've specified multiple splits but only using the first one for train data and validation data. Optionally you can also loop through the different splits and use a different train and validation set for each epoch. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission Placeholder\nsubmission_predictions = []\n\n# Multi Label Stratified Split stuff...\nmsss = MultilabelStratifiedShuffleSplit(n_splits = 20, test_size = TEST_SIZE, random_state = SEED)\nX = train_df.index\nY = train_df.Label.values\n\n# Get train and test index\nmsss_splits = next(msss.split(X, Y))\ntrain_idx = msss_splits[0]\nvalid_idx = msss_splits[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can train the model for a total of 9 epochs. The first epoch only trains the head layer on the full train data. All 8 epochs after that we train the full model but each time on only 1/6 of the train data. With each epoch only a subset of the train data will allow us to make more epochs and allows todo averaging over more then just 1 or 2 epochs (compared to using all data every epoch).\n\nNote that I recreate the data generators and model on each epoch. This is only necessary when using the different Multi-label stratified splits since the data generators will get a totally different set of data on each epoch then. I left it in so that you can try it out.\n\nStarting with the 4th epoch a prediction for the test set is made on each epoch. In total predictions from 5 epochs will be averaged this way for the final submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_generator_train = TrainDataGenerator(train_df.iloc[train_idx], \n                                                train_df.iloc[train_idx], \n                                                TRAIN_BATCH_SIZE, \n                                                (WIDTH, HEIGHT),\n                                                augment = True)\n    \ndata_generator_val = TrainDataGenerator(train_df.iloc[valid_idx], \n                                            train_df.iloc[valid_idx], \n                                            VALID_BATCH_SIZE, \n                                            (WIDTH, HEIGHT),\n                                            augment = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage import io\n\ndef imshow(image_RGB):\n    io.imshow(image_RGB)\n    io.show()\n\nx1, y1 = data_generator_train[1]\nx2, y2 = data_generator_val[1]\nimshow(x1[0])\nimshow(x2[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loop through Folds of Multi Label Stratified Split\n#for epoch, msss_splits in zip(range(0, 9), msss.split(X, Y)): \n#    # Get train and test index\n#    train_idx = msss_splits[0]\n#    valid_idx = msss_splits[1]\nfor epoch in range(0, 3):\n    print('=========== EPOCH {}'.format(epoch))\n\n    # Shuffle Train data\n    np.random.shuffle(train_idx)\n    print(train_idx[:5])    \n    print(valid_idx[:5])\n\n    # Create Data Generators for Train and Valid\n    data_generator_train = TrainDataGenerator(train_df.iloc[train_idx], \n                                                train_df.iloc[train_idx], \n                                                TRAIN_BATCH_SIZE, \n                                                (WIDTH, HEIGHT),\n                                                augment = True)\n    \n    data_generator_val = TrainDataGenerator(train_df.iloc[valid_idx], \n                                            train_df.iloc[valid_idx], \n                                            VALID_BATCH_SIZE, \n                                            (WIDTH, HEIGHT),\n                                            augment = False)\n\n    # Create Model\n    model = create_model()\n    \n    # Head Training Model\n    if epoch < 1:\n        model.load_weights('../input/rsna-models/model.h5')\n        \n    TRAIN_STEPS = int(len(data_generator_train) / 3)\n    LR = 0.0001\n\n    if epoch != 0:\n        # Load Model Weights\n        model.load_weights('model.h5')    \n\n    model.compile(optimizer = RectifiedAdam(lr = LR), \n                  loss = 'binary_crossentropy',\n                  metrics = [weighted_loss])\n    \n    # Train Model\n    model.fit_generator(generator = data_generator_train,\n                        validation_data = data_generator_val,\n                        steps_per_epoch = TRAIN_STEPS,\n                        epochs = 1,\n                        callbacks = [ModelCheckpointFull('model.h5')],\n                        verbose = 1)\n    \n    # Starting with epoch 4 we create predictions for the test set on each epoch\n    if epoch > 1:\n        preds = predictions(test_df, model)\n        submission_predictions.append(preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And finally we create the submission file by averaging all submission_predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.iloc[:, :] = np.average(submission_predictions, axis = 0, weights = [2**i for i in range(len(submission_predictions))])\ntest_df = test_df.stack().reset_index()\ntest_df.insert(loc = 0, column = 'ID', value = test_df['Image'].astype(str) + \"_\" + test_df['Diagnosis'])\ntest_df = test_df.drop([\"Image\", \"Diagnosis\"], axis=1)\ntest_df.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}