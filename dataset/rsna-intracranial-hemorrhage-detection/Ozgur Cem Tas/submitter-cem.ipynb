{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport pydicom\nimport cupy as cp\n\nfrom keras import layers\nfrom keras.applications import DenseNet121, ResNet50V2, InceptionV3\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.initializers import Constant\nfrom keras.utils import Sequence\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.models import Model, load_model\nfrom keras.layers import GlobalAveragePooling2D, Dense, Activation, concatenate, Dropout\nfrom keras.initializers import glorot_normal, he_normal\nfrom keras.regularizers import l2\n\nimport keras.metrics as M\nimport tensorflow_addons as tfa\nimport pickle\n\nfrom keras import backend as K\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import array_ops\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\nimport warnings\nwarnings.filterwarnings(action='once')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_csv = \"../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/stage_2_sample_submission.csv\"\ntest_dir = \"../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/stage_2_test\"\nBASE_PATH = \"../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/\"\nTEST_DIR = \"stage_2_test/\"\ntest_df = pd.read_csv(test_csv)\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testdf = test_df.ID.str.rsplit(\"_\", n=1, expand=True)\ntestdf = testdf.rename({0: \"id\", 1: \"subtype\"}, axis=1)\ntestdf.loc[:, \"label\"] = 0\ntestdf.head()\ntestdf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testdf = pd.pivot_table(testdf, index=\"id\", columns=\"subtype\", values=\"label\")\ntestdf.head(1)\ntestdf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sigmoid_window(dcm, window_center, window_width, U=1.0, eps=(1.0 / 255.0)):\n    img = dcm.pixel_array\n    img = cp.array(np.array(img))\n    _, _, intercept, slope = get_windowing(dcm)\n    img = img * slope + intercept\n    ue = cp.log((U / eps) - 1.0)\n    W = (2 / window_width) * ue\n    b = ((-2 * window_center) / window_width) * ue\n    z = W * img + b\n    img = U / (1 + cp.power(np.e, -1.0 * z))\n    img = (img - cp.min(img)) / (cp.max(img) - cp.min(img))\n    return cp.asnumpy(img)\ndef get_first_of_dicom_field_as_int(x):\n    #get x[0] as in int is x is a 'pydicom.multival.MultiValue', otherwise get int(x)\n    if type(x) == pydicom.multival.MultiValue:\n        return int(x[0])\n    else:\n        return int(x)\n\ndef get_windowing(data):\n    dicom_fields = [data[('0028','1050')].value, #window center\n                    data[('0028','1051')].value, #window width\n                    data[('0028','1052')].value, #intercept\n                    data[('0028','1053')].value] #slope\n    return [get_first_of_dicom_field_as_int(x) for x in dicom_fields]\ndef preprocess(file,type=\"WINDOW\",DIR=TEST_DIR):\n    dcm = pydicom.dcmread(BASE_PATH+DIR+file+\".dcm\")\n    if type == \"WINDOW\":\n        window_center , window_width, intercept, slope = get_windowing(dcm)\n        w = window_image(dcm, window_center, window_width)\n        win_img = np.repeat(w[:, :, np.newaxis], 3, axis=2)\n        #return win_img\n    elif type == \"SIGMOID\":\n        window_center , window_width, intercept, slope = get_windowing(dcm)\n        test_img = dcm.pixel_array\n        w = sigmoid_window(dcm, window_center, window_width)\n        win_img = np.repeat(w[:, :, np.newaxis], 3, axis=2)\n        #return win_img\n    elif type == \"BSB\":\n        win_img = bsb_window(dcm)\n        #return win_img\n    elif type == \"SIGMOID_BSB\":\n        win_img = sigmoid_bsb_window(dcm)\n    elif type == \"GRADIENT\":\n        win_img = rainbow_window(dcm)\n        #return win_img\n    else:\n        win_img = dcm.pixel_array\n    resized = cv2.resize(win_img,(224,224))\n    return resized\n\nclass DataLoader(Sequence):\n    def __init__(self, dataframe,\n                 batch_size,\n                 shuffle,\n                 input_shape,\n                 num_classes=6,\n                 steps=None,\n                 prep=\"SIGMOID\"):\n        \n        self.data_ids = dataframe.index.values\n        self.dataframe = dataframe\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self.current_epoch=0\n        self.prep = prep\n        self.steps=steps\n        if self.steps is not None:\n            self.steps = np.round(self.steps/3) * 3\n            self.undersample()\n        \n    def undersample(self):\n        part = np.int(self.steps/3 * self.batch_size)\n        zero_ids = np.random.choice(self.dataframe.loc[self.dataframe[\"any\"] == 0].index.values, size=5000, replace=False)\n        hot_ids = np.random.choice(self.dataframe.loc[self.dataframe[\"any\"] == 1].index.values, size=5000, replace=True)\n        self.data_ids = list(set(zero_ids).union(hot_ids))\n        np.random.shuffle(self.data_ids)\n        \n    # defines the number of steps per epoch\n    def __len__(self):\n        if self.steps is None:\n            return np.int(np.ceil(len(self.data_ids) / np.float(self.batch_size)))\n        else:\n            return 3*np.int(self.steps/3) \n    \n    # at the end of an epoch: \n    def on_epoch_end(self):\n        # if steps is None and shuffle is true:\n        if self.steps is None:\n            self.data_ids = self.dataframe.index.values\n            if self.shuffle:\n                np.random.shuffle(self.data_ids)\n        else:\n            self.undersample()\n        self.current_epoch += 1\n    \n    # should return a batch of images\n    def __getitem__(self, item):\n        # select the ids of the current batch\n        current_ids = self.data_ids[item*self.batch_size:(item+1)*self.batch_size]\n        X, y = self.__generate_batch(current_ids)\n        return X, y\n    \n    # collect the preprocessed images and targets of one batch\n    def __generate_batch(self, current_ids):\n        X = np.empty((self.batch_size, *self.input_shape, 3))\n        y = np.empty((self.batch_size, self.num_classes))\n        for idx, ident in enumerate(current_ids):\n            # Store sample\n            #image = self.preprocessor.preprocess(ident) \n            image = preprocess(ident,self.prep)\n            X[idx] = image\n            # Store class\n            y[idx] = self.__get_target(ident)\n        return X, y\n    \n    # extract the targets of one image id:\n    def __get_target(self, ident):\n        targets = self.dataframe.loc[ident].values\n        return targets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def turn_pred_to_dataframe(data_df, pred):\n    df = pd.DataFrame(pred, columns=data_df.columns, index=data_df.index)\n    df = df.stack().reset_index()\n    df.loc[:, \"ID\"] = df.id.str.cat(df.subtype, sep=\"_\")\n    df = df.drop([\"id\", \"subtype\"], axis=1)\n    df = df.rename({0: \"Label\"}, axis=1)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weighted_loss(y_true, y_pred):\n    \"\"\"\n    Will be used as the metric in model.compile()\n    ---------------------------------------------\n    \n    Similar to the custom loss function 'weighted_log_loss()' above\n    but with normalized weights, which should be very similar \n    to the official competition metric:\n        https://www.kaggle.com/kambarakun/lb-probe-weights-n-of-positives-scoring\n    and hence:\n        sklearn.metrics.log_loss with sample weights\n    \"\"\"\n    \n    class_weights = tf.constant([2., 1., 1., 1., 1., 1.])\n    \n    eps = tf.keras.backend.epsilon()\n    \n    y_pred = tf.clip_by_value(y_pred, eps, 1.0-eps)\n\n    loss = -(        y_true  * tf.math.log(      y_pred)\n            + (1.0 - y_true) * tf.math.log(1.0 - y_pred))\n    \n    loss_samples = _normalized_weighted_average(loss, class_weights)\n    \n    return tf.reduce_mean(loss_samples)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras.losses\n\ntest_dataloader = DataLoader(testdf,32,shuffle=False,input_shape=(224,224),prep=\"SIGMOID\")\n#resnet_cat = tf.keras.models.load_model('../input/xception-30-data-025/Xception_30_6_0.25data.h5')\nmodel = keras.models.load_model('../input/xception-30-data-025/Xception_30_6_0.25data.h5', compile=False)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred = model.predict(test_dataloader,verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = test_pred[0:testdf.shape[0]]\npred_df = turn_pred_to_dataframe(testdf,pred)\npred_df.to_csv(\"test2.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataloader = DataLoader(testdf,32,shuffle=False,input_shape=(224,224),prep=\"SIGMOID\")\nresnet_mfl = tf.keras.models.load_model('../input/fork-of-ich-training-metrics/RESNET_SIGMOID_200_15_focal_loss.model')\nresnet_mfl.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred = resnet_mfl.predict(test_dataloader,verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = test_pred[0:testdf.shape[0]]\npred_df = turn_pred_to_dataframe(testdf,pred)\npred_df.to_csv(\"reset_mfl_pred.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}