{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Reading the input files","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/loan-default-prediction/train_v2.csv.zip\")\nt = pd.read_csv(\"/kaggle/input/loan-default-prediction/test_v2.csv.zip\")\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.select_dtypes(include=['object']).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These columns seem to be incorrect, so we drop them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"invalid = data.select_dtypes(include=['object']).columns\ndata.drop(invalid, axis=1, inplace=True)\nt.drop(invalid, axis=1, inplace=True)\nt_id = t['id'].copy\nt.drop('id', axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\n## Describe the numeric columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"missing = data.isnull().sum()\nmissing = pd.DataFrame(missing[missing!=0])\nmissing.columns = ['No. of missing values']\nmissing['Percentage'] = 100*missing['No. of missing values']/data.id.count()\nmissing.sort_values(by=\"Percentage\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = data.iloc[:,1:752].corr()\ncorrelations.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see in the above output, there are many features that have very high correlations among themselves. This is the motivation behind performing Principal Component Analysis (PCA) in the further step to reduce the dimensions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Train test split\nBefore we go for data transformation and model building, it is necessary to divide the data into train and test.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data.iloc[:,1:751].copy()\ny = data.iloc[:,751].copy()\ny.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We first convert y to binary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y[y>0] = 1\ny.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, stratify = y, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[X_train.shape, X_test.shape, y_train.shape, y_test.shape]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing value treatment\nSince the percentage of missing values is small, we impute them by the mean of the column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.fillna(X_train.mean())\nX_test = X_test.fillna(X_train.mean())\nt = t.fillna(X_train.mean())\n[X_train.isnull().sum().sum(), X_test.isnull().sum().sum(), t.isnull().sum().sum()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Standardization of Variables\nPCA is effected by scale so we need to scale the features in the data before applying PCA. We can transform the data onto unit scale (mean = 0 and variance = 1) which is a requirement for the optimal performance of many machine learning algorithms. StandardScaler helps standardize the dataset’s features. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscalar= StandardScaler()\nscalar.fit(X_train)\nX_train = scalar.transform(X_train)\nX_test = scalar.transform(X_test)\nX_t = scalar.transform(t)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Principal Component Analysis\n* Given a collection of points in two, three, or higher dimensional space, a \"best fitting\" line can be defined as one that minimizes the average squared distance from a point to the line. The next best-fitting line can be similarly chosen from directions perpendicular to the first. Repeating this process yields an orthogonal basis in which different individual dimensions of the data are uncorrelated. These basis vectors are called principal components, and several related procedures principal component analysis (PCA).\n* PCA is a method used to reduce number of variables in the data by extracting the important ones from a large pool. It reduces the dimension of the data with an aim to retain as much information as possible. In other words, this method combines highly correlated variables together to form a smaller number of an artificial set of variables which is called “principal components” that account for most variance in the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA()\npca.fit(X_train)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.cumsum(pca.explained_variance_ratio_)[200]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"98.27% of variation is explained by 100 components.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_pca = PCA(n_components=200)\nfinal_pca.fit(X_train)\nX_train = final_pca.transform(X_train)\nX_train = pd.DataFrame(data = X_train)\nX_test = final_pca.transform(X_test)\nX_test = pd.DataFrame(data = X_test)\nX_t = final_pca.transform(X_t)\nX_t = pd.DataFrame(data = X_t)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can use these variables to fit the model with 200 independent variables to predict loss.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(solver= 'saga', class_weight='balanced',max_iter=500, random_state=1).fit(X_train, y_train)\nmodel.coef_[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation on test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation\n## Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.metrics as sm\nc = pd.DataFrame(sm.confusion_matrix(y_test, y_pred), index=['Actual non defaulter','Actual defaulter'])\nc.columns = ['Predicted non defaulter','Predicted defaulter']\nc['Actual Total'] = c.sum(axis=1)\nc.loc['Predicted Total',:] = c.sum(axis = 0)\nc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print([\"The accuracy on the validation data is \" + str(round(sm.accuracy_score(y_test, y_pred)*100,ndigits = 2)) + \"%\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sensitivity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The sensitivity (true positive rate) is \" + str(round(100*c.iloc[1,1]/c.iloc[1,2], ndigits=2)) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AUC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ns_fpr, ns_tpr, _ = sm.roc_curve(y_test, np.zeros(len(y_test)))\nlr_probs = model.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nlr_probs = lr_probs[:, 1]\nlr_fpr, lr_tpr, _ = sm.roc_curve(y_test, lr_probs)\n# plot the roc curve for the model\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\nplt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic Regression')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The Area under ROC curve is \" + str(round(100 * sm.roc_auc_score(y_test, y_pred), ndigits=2)) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification Report","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sm.classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction on given test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(X_t)\nsns.countplot(pred);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/loan-default-prediction/sampleSubmission.csv\")\nsubmission['loss'] = pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submit.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}