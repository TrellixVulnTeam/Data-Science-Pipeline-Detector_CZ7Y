{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import Imputer #imputing  missing values\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/loan-default-prediction/train_v2.csv.zip\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Shape of data\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Information of data for data types \ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 19 variables of categorical types and 653 is of float type and 99 is of integeral value."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the features which has categorical values\n\ndata_categorical = data.select_dtypes(include=['object'])\n(data_categorical.head())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the features of object data types has very large integeral numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_categorical.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count the frequency of each class or labels of a categorical data\nprint(data_categorical['f137'].value_counts())\nprint(data_categorical['f138'].value_counts())\nprint(data_categorical['f206'].value_counts())\nprint(data_categorical['f207'].value_counts())\nprint(data_categorical['f276'].value_counts())\nprint(data_categorical['f277'].value_counts())\nprint(data_categorical['f338'].value_counts())\nprint(data_categorical['f390'].value_counts())\nprint(data_categorical['f391'].value_counts())\nprint(data_categorical['f419'].value_counts())\nprint(data_categorical['f420'].value_counts())\nprint(data_categorical['f469'].value_counts())\nprint(data_categorical['f472'].value_counts())\nprint(data_categorical['f534'].value_counts())\nprint(data_categorical['f537'].value_counts())\nprint(data_categorical['f626'].value_counts())\nprint(data_categorical['f627'].value_counts())\nprint(data_categorical['f695'].value_counts())\nprint(data_categorical['f698'].value_counts())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As categorical variables are of many labels and it can be different for different persons, hence it will better to drop categorical variables and work only on numerical features.\nIt will increase the dimension of features after encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_missing = (data.isnull().sum() / len(data)).sort_values(ascending = False)\ndata_missing.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"approx 17 % of data are missing for features which has missing terms and it is approximately constant for each one of the feature, hence it is better to impute these missing terms with mean or any other statistical method."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Features with missing values for categorical values\ndata_categorical.loc[1:5:, data_categorical.isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling each missing categorical data with mode\ndata_categorical_imputed = data_categorical.fillna(data_categorical.mode().iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata_categorical_imputed.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the categorical features has been imputed with most frequent values(mode)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting data with float and integer data types \ndata_num = data.select_dtypes(include=['float64','int64'])\n(data_num.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Features with missing values for numerical values\ndata_num.loc[1:5:, data_num.isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A total of 513 columns has missing terms. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# imputing missing terms of numerical data with mean\nfrom sklearn.impute import SimpleImputer \nimputer=SimpleImputer(missing_values=np.nan,strategy='mean')\nimputer=imputer.fit(data_num)\ndata_num_imputed =pd.DataFrame(imputer.transform(data_num))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_num_imputed.columns = data_num.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_num_imputed.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the missing terms has been imputed.[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing id value from data\ndata_num_imputed.drop('id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_data_num   =  data_num_imputed.iloc[:,0:750]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now for numerical features all the collinear features that have high correlation with each other will be removed."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Threshold for removing correlated variables\nthreshold = 0.9\n\n# Absolute value correlation matrix\ncorr_matrix = X_data_num.corr().abs()\ncorr_matrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d columns to remove.' % (len(to_drop)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_data_num_1 = X_data_num.drop(columns = to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"shape of original numerical feature\", X_data_num.shape)\nprint(\"shape of reduced numerical feature\",X_data_num_1.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No of features has been reduced from 750 to 288"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Correlations between Features and Target\n\n# Find all correlations and sort \ncorrelations_data = data_num_imputed.corr()['loss'].sort_values()\n\n# Print the most negative correlations\nprint(correlations_data.head(30), '\\n')\n\n# Print the most positive correlations\nprint(correlations_data.tail(30))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the column which hsa constant terms due to correlation to target is becoming 0\nprint([column for column in X_data_num_1.columns if len(X_data_num_1[column].unique())==1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop columns with correlations of NaN\nto_drop_1 = [column for column in X_data_num_1.columns if len(X_data_num_1[column].unique())==1]\nprint('There are %d columns to remove.' % (len(to_drop_1)))\nX_data_num_2 = X_data_num_1.drop(columns = to_drop_1)\nprint(\"shape of new numerical feature is :\",X_data_num_2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= data_categorical_imputed\ny= data_num_imputed['loss']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making binary problem \n\ny[y>0]=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modeling \n\nfrom sklearn.ensemble import RandomForestClassifier\n# utilities\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\n\n# memory management\nimport gc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Selection through Feature Importances**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize an empty array to hold feature importances\nfeature_importances = np.zeros(X_data_num_2 .shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the model with several hyperparameters\nmodel_random = RandomForestClassifier(n_jobs=-1, n_estimators=250)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model twice to avoid overfitting\nfor i in range(2):\n    \n    # Split into training and validation set\n    train_features, valid_features, train_y, valid_y = train_test_split(X_data_num_2, y, test_size = 0.25, random_state = i)\n    \n    # Train the random forest model\n    model_random.fit(train_features, train_y)\n    \n    # Record the feature importances\n    feature_importances += model_random.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# average of feature importances\nfeature_importances = feature_importances / 2\nfeature_importances = pd.DataFrame({'feature': list(X_data_num_2.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting top 20 features\nfrom matplotlib.pyplot import figure\nimport matplotlib.pyplot as plt\nfigure(num=None, figsize=(18, 6), dpi=80, facecolor='w', edgecolor='k')\nplt.bar(feature_importances['feature'][0:20], feature_importances['importance'][0:20], width=0.8, bottom=None, align='center', data=feature_importances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the features with least importance with threshold less than .0020\nleast_important_features = list(feature_importances[feature_importances['importance'] < 0.0020]['feature'])\nprint('There are %d features with least importance' % len(least_important_features ))\nfeature_importances.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the columns with least importance\nX_data_num_3 = X_data_num_2.drop(columns = least_important_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_data_num_3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make features and target as X and Y\nX = X_data_num_3.copy()\nY = y.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train a model with these features\n#Logistic Regression\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.25, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(x_train)\nX_test = sc.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert y to one-dimensional array (vector)\ny_train = np.array(y_train).reshape((-1, ))\ny_test = np.array(y_test).reshape((-1, ))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# We will compare 2 different machine learning Cassification models:\n\n# 1 - Logistic Regression\n# 2 - Random Forest Classification\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegressionCV\nclf = LogisticRegressionCV(cv=10, random_state=0).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Logistic Performance on the training set: Cross Validation Score = %0.4f'%round((clf.score(X_train,y_train)).mean(),4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Logistic Performance on the validation set: Cross Validation Score',round((clf.score(X_test,y_test)),4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training and test set score is nearly same hance we can say that our model is good and does not over fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt     \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nclass_label = [\"0\", \"1\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\nsns.heatmap(df_cm, annot=True,fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" from sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_score_logistic_test =  round(f1_score(y_test, y_pred, average='macro'),2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"F score from logistic regression for test data is :\",f_score_logistic_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\nrandom = RandomForestClassifier(n_estimators =15) \n# 10-Fold Cross validation\nscores  = cross_val_score(random, X_train, y_train, cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ranmodel  = random.fit( X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_ran_pred  = ranmodel.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Random Forest Performance on the training  set: Cross Validation Score',round(scores.mean(),4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt     \nfrom sklearn.metrics import confusion_matrix\ncm1 = confusion_matrix(y_test, y_ran_pred)\nclass_label = [\"0\", \"1\"]\ndf_cm1 = pd.DataFrame(cm1, index=class_label,columns=class_label)\nsns.heatmap(df_cm1, annot=True,fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"F score from Random Forest for test data is :\",round(f1_score(y_test, y_ran_pred, average='macro'),3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocessing for test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/loan-default-prediction/test_v2.csv.zip\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_test  = test_data[X.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imputing missing terms of numerical data with mean\nfrom sklearn.impute import SimpleImputer \nimputer=SimpleImputer(missing_values=np.nan,strategy='mean')\nimputer=imputer.fit(feature_test)\ntest_features_imputed =pd.DataFrame(imputer.transform(feature_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features_imputed.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nfeatures_test_scaled = sc.fit_transform(test_features_imputed)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will select random forest model as it has more f score as compared to that of logistic regression as clear from confusion matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"Predicted_values = ranmodel.predict(features_test_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Predicted_values_df = pd.DataFrame({'default':Predicted_values})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission  = pd.read_csv(\"/kaggle/input/loan-default-prediction/sampleSubmission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.id = test_data.id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.loss = Predicted_values_df.default","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = sample_submission.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Submission = submission.to_csv(index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os \nos.chdir(r'/kaggle/working')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsubmission.to_csv(r'SUBMISSION.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" from IPython.display import FileLink\n FileLink(r'SUBMISSION.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}