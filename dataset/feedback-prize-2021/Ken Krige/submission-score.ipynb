{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-12T23:32:09.806437Z","iopub.execute_input":"2022-01-12T23:32:09.806908Z","iopub.status.idle":"2022-01-12T23:32:09.811804Z","shell.execute_reply.started":"2022-01-12T23:32:09.80687Z","shell.execute_reply":"2022-01-12T23:32:09.810742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace this with your own submission.csv. Same format as LB submission, but a subset of train files.\nsub = pd.read_csv('../input/essay2-submission/submission.csv')\n\n# If this code is inserted into train cycle, then truth just needs to be a subset of train.csv for your val set.\n# Or we can reconstruct it from submission.csv id as below.\ntruth = pd.read_csv('../input/feedback-prize-2021/train.csv').rename(columns={'discourse_type':'class'})\ntruth = truth[['id', 'class', 'predictionstring']]\nkeeps = {k:1 for k in sub.id.values}\ntruth = truth[truth.id.map(lambda idx:idx in keeps)]","metadata":{"execution":{"iopub.status.busy":"2022-01-12T23:32:09.814085Z","iopub.execute_input":"2022-01-12T23:32:09.815246Z","iopub.status.idle":"2022-01-12T23:32:10.853927Z","shell.execute_reply.started":"2022-01-12T23:32:09.815194Z","shell.execute_reply":"2022-01-12T23:32:10.852988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def edge_words(predictionstring):\n    words = predictionstring.split()\n    return [int(words[0]),int(words[-1])]\n\nfor df in [sub, truth]:\n    df['span'] = df.predictionstring.apply(edge_words)\n    df.drop('predictionstring', axis=1, inplace=True)\n\ntruth = truth.groupby(['id','class']).apply(lambda gp:gp.span.values).reset_index().rename(columns={0:'truth'})\nsub = sub.groupby(['id','class']).apply(lambda gp:gp.span.values).reset_index().rename(columns={0:'sub'})\n\ndf = sub.merge(truth, on=[\"id\", \"class\"], how=\"outer\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T23:32:10.856151Z","iopub.execute_input":"2022-01-12T23:32:10.856639Z","iopub.status.idle":"2022-01-12T23:32:11.463736Z","shell.execute_reply.started":"2022-01-12T23:32:10.856591Z","shell.execute_reply":"2022-01-12T23:32:11.462858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def overlap(sub, truth):\n    overlap = min(sub[1], truth[1]) - max(sub[0], truth[0]) + 1\n    return max(0, overlap)\n\ndef score(sub, truth):\n    TP = 0\n    if sub is np.NaN: sub = []\n    elif truth is np.NaN: truth = []\n    else:\n        sub_lengths = [(s[1] - s[0] + 1) for s in sub]\n        truth_lengths = [(t[1] - t[0] + 1) for t in truth]\n        overlaps = np.array([[overlap(t,s) for s in sub] for t in truth])\n        for t in range(len(overlaps)):\n            s = np.argmax(overlaps[t])\n            max_overlap = overlaps[t,s]\n            if max_overlap > 0:\n                overlaps[:,s] = 0\n                if max_overlap / sub_lengths[s] >= 0.5 and max_overlap / truth_lengths[t] >= 0.5:\n                    TP += 1\n    FP = len(sub) - TP\n    FN = len(truth) - TP\n    return TP, FP, FN\n\ndef row_score(row):\n    row['TP'], row['FP'], row['FN'] = score(row['sub'], row['truth'])\n    return row\n\ndf = df.apply(row_score, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T23:32:11.465996Z","iopub.execute_input":"2022-01-12T23:32:11.466509Z","iopub.status.idle":"2022-01-12T23:32:21.301339Z","shell.execute_reply.started":"2022-01-12T23:32:11.466462Z","shell.execute_reply":"2022-01-12T23:32:21.300285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.groupby('class')[['TP','FP','FN']].sum()\ndf['Precision'] = df.TP / (df.TP + df.FP)\ndf['Recall'] = df.TP / (df.TP + df.FN)\ndf['F1'] = 2 * df.Precision * df.Recall / (df.Precision + df.Recall)\npd.set_option('display.float_format', '{:0.2f}'.format)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-01-12T23:32:21.302725Z","iopub.execute_input":"2022-01-12T23:32:21.302969Z","iopub.status.idle":"2022-01-12T23:32:21.329325Z","shell.execute_reply.started":"2022-01-12T23:32:21.302932Z","shell.execute_reply":"2022-01-12T23:32:21.328393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Final macro mean of F1 scores: {df.F1.mean():.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-01-12T23:32:21.330788Z","iopub.execute_input":"2022-01-12T23:32:21.33101Z","iopub.status.idle":"2022-01-12T23:32:21.335871Z","shell.execute_reply.started":"2022-01-12T23:32:21.330976Z","shell.execute_reply":"2022-01-12T23:32:21.335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}