{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\ngc.enable()\n\nimport sys\nsys.path.append(\"../input/tez-lib/\")\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tez\nimport torch\nimport torch.nn as nn\nfrom joblib import Parallel, delayed\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\nfrom torch.utils.data import Dataset,DataLoader\nimport glob\nfrom timeit import default_timer as timer\nfrom torch.utils.data.sampler import *\nimport torch.cuda.amp as amp\nfrom torch.nn.parallel.data_parallel import data_parallel\nfrom typing import Dict, Tuple\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"","metadata":{"_uuid":"66c995cf-7cbb-421b-bdd3-24906f5fb886","_cell_guid":"e0e4bdf6-bcd9-4f82-a53b-9c5f685500a8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-15T10:55:56.421512Z","iopub.execute_input":"2022-04-15T10:55:56.421907Z","iopub.status.idle":"2022-04-15T10:55:57.13912Z","shell.execute_reply.started":"2022-04-15T10:55:56.421832Z","shell.execute_reply":"2022-04-15T10:55:57.138397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def time_to_str(t, mode='min'):\n    if mode=='min':\n        t  = int(t)/60\n        hr = t//60\n        min = t%60\n        return '%2d hr %02d min'%(hr,min)\n    elif mode=='sec':\n        t   = int(t)\n        min = t//60\n        sec = t%60\n        return '%2d min %02d sec'%(min,sec)\n    else:\n        raise NotImplementedError","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.14369Z","iopub.execute_input":"2022-04-15T10:55:57.144158Z","iopub.status.idle":"2022-04-15T10:55:57.150636Z","shell.execute_reply.started":"2022-04-15T10:55:57.144117Z","shell.execute_reply":"2022-04-15T10:55:57.149917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discourse_marker_to_label = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 2,\n    \"I-Position\": 3,\n    \"B-Evidence\": 4,\n    \"I-Evidence\": 5,\n    \"B-Claim\": 6,\n    \"I-Claim\": 7,\n    \"B-Concluding Statement\": 8,\n    \"I-Concluding Statement\": 9,\n    \"B-Counterclaim\": 10,\n    \"I-Counterclaim\": 11,\n    \"B-Rebuttal\": 12,\n    \"I-Rebuttal\": 13,\n    \"O\": 14,\n    \"PAD\": -100,\n}\n\n\nlabel_to_discourse_marker = {v: k for k, v in discourse_marker_to_label.items()}\n\nnum_discourse_marker = 15 \n\nlength_threshold = {\n    'Lead'                : 9,\n    'Position'            : 5,\n    'Claim'               : 3,\n    'Counterclaim'        : 6,\n    'Rebuttal'            : 4,\n    'Evidence'            : 14,\n    'Concluding Statement': 11,\n}\nprobability_threshold = {\n    'Lead'                : 0.70,\n    'Position'            : 0.55,\n    'Claim'               : 0.55,\n    'Counterclaim'        : 0.50,\n    'Rebuttal'            : 0.55,\n    'Evidence'            : 0.65,\n    'Concluding Statement': 0.70,\n}\nmax_length = 1600\nis_amp = True","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.151922Z","iopub.execute_input":"2022-04-15T10:55:57.152201Z","iopub.status.idle":"2022-04-15T10:55:57.161442Z","shell.execute_reply.started":"2022-04-15T10:55:57.152166Z","shell.execute_reply":"2022-04-15T10:55:57.16074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_grps = {\n'funnel-xlarge':\n    {\n        'arch':'../input/xlargefunnelsnap',\n        'batch_size':4,\n        'max_length':max_length,\n        'checkpoints':\n        [\n            ('../input/funnel-xlarge-result/funnel-xlarge/model_0.bin','funnel-xlarge-fold0')\n        ]\n    },\n    \n    'deberta-xlarge':{\n        'arch' : '../input/debertaxlarge/deberta-xlarge',\n        'batch_size':4,\n        'max_length' : 1600,\n        'checkpoints':\n        [\n            ('../input/deberta-xlarge-fold-1/model_1.bin','deberta-xlarge-fold-1'),\n            ('../input/deberta-xlarge/model_3.bin','deberta-xlarge-fold-3'),\n            ('../input/deberta-xlarge-fold-4/model_4.bin','deberta-xlarge-fold-4')\n        ]\n        \n    },\n    \n    'deberta':{\n        'arch' : '../input/debertalarge',\n        'batch_size':8,\n        'max_length' : max_length,\n        'checkpoints':\n        [\n            ('../input/deberta-large-5-fold/model_0.bin','deberta_fold_0'),\n            ('../input/deberta-fold-1/model_1.bin','deberta_fold_1'),\n            ('../input/deberta-fold-234/fold 2/model_2.bin', 'deberta_fold_2'),\n            ('../input/deberta-fold-234/fold 3/model_3.bin', 'deberta_fold_3'),\n            ('../input/deberta-fold-234/fold 4/model_4.bin', 'deberta_fold_4'),\n        ]    \n    },\n    \n    'longformers' : {\n        'arch' : '../input/longformerlarge4096/longformer-large-4096',\n        'batch_size':8,\n        'max_length' : max_length,\n        'checkpoints' : [\n#             ('../input/tez-fb-large/model_0.bin','longformer_fold_0'),\n            ('../input/tez-fb-large/model_1.bin','longformer_fold_1'),\n            ('../input/fblongformerlarge1536/model_2.bin','longformer_fold_2'),\n            ('../input/tez-fb-large/model_3.bin','longformer_fold_3'),\n            ('../input/fblongformerlarge1536/model_4.bin','longformer_fold_4'),\n#             ('../input/fblongformerlarge1536/model_0.bin','longformer_fold_5'),\n#             ('../input/fblongformerlarge1536/model_1.bin','longformer_fold_6'),\n#             ('../input/tez-fb-large/model_2.bin','longformer_fold_7'),\n#             ('../input/fblongformerlarge1536/model_3.bin','longformer_fold_8'),\n#             ('../input/tez-fb-large/model_4.bin','longformer_fold_9')\n        ]\n    \n    },\n    'funnels':\n    {\n        'arch':'../input/funneltransformerlarge',\n        'batch_size':8,\n        'max_length':max_length,\n        'checkpoints':\n        [ \n#             ('../input/funnel-fold-0/fold 0/model_0.bin','funnel-fold-0'),\n            ('../input/funnel-large/funnel large/fold 1 3/model/model_1.bin','funnel-large-fold-1'),\n            ('../input/funnel-large/funnel large/fold 1 3/model/model_3.bin','funnel-large-fold-2'),\n            ('../input/funnel-large/funnel large/fold 2/model/model_2.bin','funnel-large-fold-3'),\n        ]\n    },\n    \n}","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.164067Z","iopub.execute_input":"2022-04-15T10:55:57.164547Z","iopub.status.idle":"2022-04-15T10:55:57.175083Z","shell.execute_reply.started":"2022-04-15T10:55:57.16451Z","shell.execute_reply":"2022-04-15T10:55:57.17429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_data(texts, tokenizer, max_length = 4096):\n    results = []\n\n    for id_, text in texts:\n        encoded = tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n            max_length=max_length,  # <todo>\n            truncation=True,\n        )\n        token_id = encoded['input_ids']\n        token_offset = encoded['offset_mapping']\n\n        # add end, start token id\n        token_id = [tokenizer.cls_token_id] + token_id\n        token_id = token_id[: max_length - 1]  # need to set as 4096, do not abandon tokens\n        token_id = token_id + [tokenizer.sep_token_id]\n\n        # padding\n        token_mask = [1] * len(token_id)\n\n        #     padding_length = max_length - len(token_id)\n        #     if padding_length > 0:\n        #         if tokenizer.padding_side == 'right':\n        #             token_id    = token_id    + [tokenizer.pad_token_id] * padding_length\n        #             token_mask  = token_mask  + [0] * padding_length\n        #         else:\n        #             raise NotImplementedError\n\n        results.append((id_, text, token_offset, token_id, token_mask))\n\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.176344Z","iopub.execute_input":"2022-04-15T10:55:57.176606Z","iopub.status.idle":"2022-04-15T10:55:57.187027Z","shell.execute_reply.started":"2022-04-15T10:55:57.17657Z","shell.execute_reply":"2022-04-15T10:55:57.186192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackDataset(Dataset):\n    def __init__(self, tokenized_data):\n        self.tokenized_data = tokenized_data\n        self.length = len(self.tokenized_data)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        # text to token\n\n        id, text, token_offset, token_id, token_mask = self.tokenized_data[index]\n\n        # -------------------------------------\n        r = {}\n        r['index'] = index\n        r['id'] = id\n        r['text'] = text\n        r['token_offset'] = str(token_offset)  # force batch loader store as list\n        #         r['token_id'    ] = torch.tensor(token_id,    dtype=torch.long)\n        #         r['token_mask'  ] = torch.tensor(token_mask,  dtype=torch.long)\n        r['token_id'] = token_id\n        r['token_mask'] = token_mask\n\n        return r\n\n\nclass FeedbackDatasetValid:\n    def __init__(self, samples, max_len, tokenizer):\n        self.samples = samples\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.length = len(samples)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        input_ids = self.samples[idx][\"input_ids\"]\n        input_ids = [self.tokenizer.cls_token_id] + input_ids\n\n        if len(input_ids) > self.max_len - 1:\n            input_ids = input_ids[: self.max_len - 1]\n\n        # add end token id to the input_ids\n        input_ids = input_ids + [self.tokenizer.sep_token_id]\n        attention_mask = [1] * len(input_ids)\n\n        return {\n            \"ids\": input_ids,\n            \"mask\": attention_mask,\n        }\n\n\nclass Collate:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"token_id\"] = [sample[\"token_id\"] for sample in batch]\n        output[\"token_mask\"] = [sample[\"token_mask\"] for sample in batch]\n        output[\"index\"] = [sample['index'] for sample in batch]\n        output[\"id\"] = [sample[\"id\"] for sample in batch]\n        output[\"text\"] = [sample[\"text\"] for sample in batch]\n        output[\"token_offset\"] = [sample[\"token_offset\"] for sample in batch]\n\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"token_id\"]])\n        # batch_max = 4096\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"token_id\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"token_id\"]]\n            output[\"token_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"token_mask\"]]\n        else:\n            output[\"token_id\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"token_id\"]]\n            output[\"token_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"token_mask\"]]\n\n        # convert to tensors\n        output[\"token_id\"] = torch.tensor(output[\"token_id\"], dtype=torch.long)\n        output[\"token_mask\"] = torch.tensor(output[\"token_mask\"], dtype=torch.long)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.188475Z","iopub.execute_input":"2022-04-15T10:55:57.188831Z","iopub.status.idle":"2022-04-15T10:55:57.208972Z","shell.execute_reply.started":"2022-04-15T10:55:57.188794Z","shell.execute_reply":"2022-04-15T10:55:57.208183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_id = [ f.replace(\"\\\\\", \"/\").split('/')[-1][:-4] for f in glob.glob('../input/feedback-prize-2021/test'+'/*.txt') ] # get file names\n# valid_id = pd.read_csv('../input/feedback-prize-2021/train.csv').id.unique().tolist()\nvalid_id = sorted(valid_id)\nnum_valid = len(valid_id)\nprint('len(valid_id)',len(valid_id))","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.211746Z","iopub.execute_input":"2022-04-15T10:55:57.211976Z","iopub.status.idle":"2022-04-15T10:55:57.222724Z","shell.execute_reply.started":"2022-04-15T10:55:57.211932Z","shell.execute_reply":"2022-04-15T10:55:57.221884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_text=[]\nfor id in valid_id:\n    text_file = '../input/feedback-prize-2021/test' +'/%s.txt'%id\n    with open(text_file, 'r') as f:\n        text = f.read()\n\n    text = text.replace(u'\\xa0', u' ')\n    text = text.rstrip()\n    text = text.lstrip()\n    df_text.append((id,text))\ndf_text = pd.DataFrame(df_text, columns=['id','text'])\ndf_text['text_len'] = df_text['text'].apply(lambda x: len(x))\ndf_text = df_text.sort_values('text_len',ascending=False).reset_index(drop=True)\ndel df_text['text_len']\n\nprint('df_text.shape',df_text.shape)\nprint(df_text) # sort txt by its length","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.224095Z","iopub.execute_input":"2022-04-15T10:55:57.22441Z","iopub.status.idle":"2022-04-15T10:55:57.241474Z","shell.execute_reply.started":"2022-04-15T10:55:57.224375Z","shell.execute_reply":"2022-04-15T10:55:57.240666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_text","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.242775Z","iopub.execute_input":"2022-04-15T10:55:57.243035Z","iopub.status.idle":"2022-04-15T10:55:57.255549Z","shell.execute_reply.started":"2022-04-15T10:55:57.243Z","shell.execute_reply":"2022-04-15T10:55:57.254752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_to_word(text):\n    word = text.split()\n    word_offset = []\n\n    start = 0\n    for w in word:\n        r = text[start:].find(w)\n\n        if r==-1:\n            raise NotImplementedError\n        else:\n            start = start+r\n            end   = start+len(w)\n            word_offset.append((start,end))\n            #print('%32s'%w, '%5d'%start, '%5d'%r, text[start:end])\n        start = end\n\n    return word, word_offset","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.256907Z","iopub.execute_input":"2022-04-15T10:55:57.257147Z","iopub.status.idle":"2022-04-15T10:55:57.262957Z","shell.execute_reply.started":"2022-04-15T10:55:57.257114Z","shell.execute_reply":"2022-04-15T10:55:57.262081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_ner_conditional_masks(id2label: Dict[int, str]) -> torch.Tensor:\n    \"\"\"Create a NER-conditional mask matrix which implies the relations between\n    before-tag and after-tag.\n    According to the rule of BIO-naming system, it is impossible that `I-Dog` cannot be\n    appeard after `B-Dog` or `I-Dog` tags. This function creates the calculable\n    relation-based conditional matrix to prevent from generating wrong tags.\n    Args:\n        id2label: A dictionary which maps class indices to their label names.\n    Returns:\n        A conditional mask tensor.\n    \"\"\"\n    conditional_masks = torch.zeros(len(id2label), len(id2label))\n    for i, before in id2label.items():\n        for j, after in id2label.items():\n            if after == \"O\" or after.startswith(\"B-\") or after == f\"I-{before[2:]}\":\n                conditional_masks[i, j] = 1.0\n    return conditional_masks\n","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.264578Z","iopub.execute_input":"2022-04-15T10:55:57.264932Z","iopub.status.idle":"2022-04-15T10:55:57.274793Z","shell.execute_reply.started":"2022-04-15T10:55:57.264896Z","shell.execute_reply":"2022-04-15T10:55:57.273972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ner_beam_search_decode(\n    log_probs: torch.Tensor, id2label: Dict[int, str], beam_size: int = 2\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Decode NER-tags from the predicted log-probabilities using beam-search.\n    This function decodes the predictions using beam-search algorithm. Because all tags\n    are predicted simultaneously while the tags have dependencies of their previous\n    tags, the greedy algorithm cannot decode the tags properly. With beam-search, it is\n    possible to prevent the below situation:\n        >>> sorted = probs[t].sort(dim=-1)\n        >>> print(\"\\t\".join([f\"{id2label[i]} {p}\" for p, i in zip()]))\n        I-Dog 0.54  B-Cat 0.44  ...\n        >>> sorted = probs[t + 1].sort(dim=-1)\n        >>> print(\"\\t\".join([f\"{id2label[i]} {p}\" for p, i in zip()]))\n        I-Cat 0.99  I-Dog 0.01  ...\n    The above shows that if the locally-highest tags are selected, then `I-Dog, I-Dog`\n    will be generated even the confidence of the second tag `I-Dog` is significantly\n    lower than `I-Cat`. It is more natural that `B-Cat, I-Cat` is generated rather than\n    `I-Dog, I-Dog`. The beam-search for NER-tagging task can solve this problem.\n    Args:\n        log_probs: The log-probabilities of the token predictions.\n        id2label: A dictionary which maps class indices to their label names.\n        beam_size: The number of candidates for each search step. Default is `2`.\n    Returns:\n        A tuple of beam-searched indices and their probability tensors.\n    \"\"\"\n    # Create the log-probability mask for the invalid predictions.\n    log_prob_masks = -10000.0 * (1 - create_ner_conditional_masks(id2label))\n    log_prob_masks = log_prob_masks.to(log_probs.device)\n\n    beam_search_shape = (log_probs.size(0), beam_size, log_probs.size(1))\n    searched_tokens = log_probs.new_zeros(beam_search_shape, dtype=torch.long)\n    searched_log_probs = log_probs.new_zeros(beam_search_shape)\n\n    searched_scores = log_probs.new_zeros(log_probs.size(0), beam_size)\n    searched_scores[:, 1:] = -10000.0\n\n    for i in range(log_probs.size(1)):\n        # Calculate the accumulated score (log-probabilities) with excluding invalid\n        # next-tag predictions.\n        scores = searched_scores.unsqueeze(2)\n        scores = scores + log_probs[:, i, :].unsqueeze(1)\n        scores = scores + (log_prob_masks[searched_tokens[:, :, i - 1]] if i > 0 else 0)\n\n        # Select the top-k (beam-search size) predictions.\n        best_scores, best_indices = scores.flatten(1).topk(beam_size)\n        best_tokens = best_indices % scores.size(2)\n        best_log_probs = log_probs[:, i, :].gather(dim=1, index=best_tokens)\n\n        best_buckets = best_indices.div(scores.size(2), rounding_mode=\"floor\")\n        best_buckets = best_buckets.unsqueeze(2).expand(-1, -1, log_probs.size(1))\n\n        # Gather the best buckets and their log-probabilities.\n        searched_tokens = searched_tokens.gather(dim=1, index=best_buckets)\n        searched_log_probs = searched_log_probs.gather(dim=1, index=best_buckets)\n\n        # Update the predictions by inserting to the corresponding timestep.\n        searched_scores = best_scores\n        searched_tokens[:, :, i] = best_tokens\n        searched_log_probs[:, :, i] = best_log_probs\n\n    # Return the best beam-searched sequence and its probabilities.\n    return searched_tokens[:, 0, :], searched_log_probs[:, 0, :].exp()","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.277812Z","iopub.execute_input":"2022-04-15T10:55:57.278026Z","iopub.status.idle":"2022-04-15T10:55:57.292584Z","shell.execute_reply.started":"2022-04-15T10:55:57.278001Z","shell.execute_reply":"2022-04-15T10:55:57.291946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def word_probability_to_predict_df2(text_to_word_probability, id):\n    \n    \n    len_word = len(text_to_word_probability)\n    id_2_label = label_to_discourse_marker.copy()\n    id_2_label.pop(-100)\n    \n    text_to_word = torch.Tensor( text_to_word_probability ).unsqueeze(0)\n    \n    word_predict, word_score =  ner_beam_search_decode( text_to_word, id_2_label, 4 )\n    \n    word_predict = word_predict.squeeze().cpu().numpy()\n    word_score = word_score.squeeze().cpu().numpy()\n    \n    predict_df = []\n\n    t = 0\n    while 1:\n        if word_predict[t] not in [\n            discourse_marker_to_label['O'],\n            discourse_marker_to_label['PAD'],\n        ]:\n            start = t\n            b_marker_label = word_predict[t]\n        else:\n            t = t+1\n            if t== len_word-1: break\n            continue\n\n        t = t+1\n        if t== len_word-1: break\n\n        #----\n        if   label_to_discourse_marker[b_marker_label][0]=='B':\n            i_marker_label = b_marker_label+1\n        elif label_to_discourse_marker[b_marker_label][0]=='I':\n            i_marker_label = b_marker_label\n        else:\n            raise NotImplementedError\n\n        while 1:\n            #print(t)\n            if (word_predict[t] != i_marker_label) or (t ==len_word-1):\n                end = t\n                prediction_string = ' '.join([str(i) for i in range(start,end)]) #np.arange(start,end).tolist()\n                discourse_type = label_to_discourse_marker[b_marker_label][2:]\n                discourse_score = word_score[start:end].tolist()\n                predict_df.append((id, discourse_type, prediction_string, str(discourse_score)))\n                #print(predict_df[-1])\n                break\n            else:\n                t = t+1\n                continue\n        if t== len_word-1: break\n\n    predict_df = pd.DataFrame(predict_df, columns=['id', 'class', 'predictionstring', 'score'])\n    return predict_df","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.296129Z","iopub.execute_input":"2022-04-15T10:55:57.296328Z","iopub.status.idle":"2022-04-15T10:55:57.310783Z","shell.execute_reply.started":"2022-04-15T10:55:57.296304Z","shell.execute_reply":"2022-04-15T10:55:57.310063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def word_probability_to_predict_df(text_to_word_probability, id):\n    len_word = len(text_to_word_probability)\n    word_predict = text_to_word_probability.argmax(-1)\n    word_score   = text_to_word_probability.max(-1)\n    predict_df = []\n\n    t = 0\n    while 1:\n        if word_predict[t] not in [\n            discourse_marker_to_label['O'],\n            discourse_marker_to_label['PAD'],\n        ]:\n            start = t\n            b_marker_label = word_predict[t]\n        else:\n            t = t+1\n            if t== len_word-1: break\n            continue\n\n        t = t+1\n        if t== len_word-1: break\n\n        #----\n        if   label_to_discourse_marker[b_marker_label][0]=='B':\n            i_marker_label = b_marker_label+1\n        elif label_to_discourse_marker[b_marker_label][0]=='I':\n            i_marker_label = b_marker_label\n        else:\n            raise NotImplementedError\n\n        while 1:\n            #print(t)\n            if (word_predict[t] != i_marker_label) or (t ==len_word-1):\n                end = t\n                prediction_string = ' '.join([str(i) for i in range(start,end)]) #np.arange(start,end).tolist()\n                discourse_type = label_to_discourse_marker[b_marker_label][2:]\n                discourse_score = word_score[start:end].tolist()\n                predict_df.append((id, discourse_type, prediction_string, str(discourse_score)))\n                #print(predict_df[-1])\n                break\n            else:\n                t = t+1\n                continue\n        if t== len_word-1: break\n\n    predict_df = pd.DataFrame(predict_df, columns=['id', 'class', 'predictionstring', 'score'])\n    return predict_df","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.312295Z","iopub.execute_input":"2022-04-15T10:55:57.312588Z","iopub.status.idle":"2022-04-15T10:55:57.324532Z","shell.execute_reply.started":"2022-04-15T10:55:57.31255Z","shell.execute_reply":"2022-04-15T10:55:57.323737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def do_threshold(submit_df, use=['length','probability']):\n    df = submit_df.copy()\n    df = df.fillna('')\n\n    if 'length' in use:\n        df['l'] = df.predictionstring.apply(lambda x: len(x.split()))\n        for key, value in length_threshold.items():\n            #value=3\n            index = df.loc[df['class'] == key].query('l<%d'%value).index\n            df.drop(index, inplace=True)\n\n    if 'probability' in use:\n        df['s'] = df.score.apply(lambda x: np.mean(eval(x)))\n        for key, value in probability_threshold.items():\n            index = df.loc[df['class'] == key].query('s<%f'%value).index\n            df.drop(index, inplace=True)\n\n    df = df[['id', 'class', 'predictionstring']]\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.325899Z","iopub.execute_input":"2022-04-15T10:55:57.326306Z","iopub.status.idle":"2022-04-15T10:55:57.335937Z","shell.execute_reply.started":"2022-04-15T10:55:57.326263Z","shell.execute_reply":"2022-04-15T10:55:57.335155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackModel(tez.Model):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n        config = AutoConfig.from_pretrained(model_name)\n\n        hidden_dropout_prob: float = 0.1\n        layer_norm_eps: float = 1e-7\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = AutoModel.from_config(config)\n        self.output = nn.Linear(config.hidden_size, self.num_labels)\n\n    def forward(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        logits = self.output(sequence_output)\n        logits = torch.softmax(logits, dim=-1)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.337426Z","iopub.execute_input":"2022-04-15T10:55:57.337744Z","iopub.status.idle":"2022-04-15T10:55:57.346243Z","shell.execute_reply.started":"2022-04-15T10:55:57.337705Z","shell.execute_reply":"2022-04-15T10:55:57.345415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jn(pst, start, end):\n    return \" \".join([str(x) for x in pst[start:end]])\n\ndef link_evidence(oof):\n    thresh = 1\n    idu = oof['id'].unique()\n    idc = idu[1]\n    eoof = oof[oof['class'] == \"Evidence\"]\n    neoof = oof[oof['class'] != \"Evidence\"]\n    for thresh2 in range(26, 27, 1):\n        retval = []\n        for idv in idu:\n            for c in ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n                      'Counterclaim', 'Rebuttal']:\n                q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n                if len(q) == 0:\n                    continue\n                pst = []\n                for i, r in q.iterrows():\n                    pst = pst + [-1] + [int(x) for x in r['predictionstring'].split()] # -1作为分割合并的间隔\n                start = 1\n                end = 1\n                for i in range(2, len(pst)):\n                    cur = pst[i]\n                    end = i\n                    # if pst[start] == 205:\n                    #   print(cur, pst[start], cur - pst[start])\n                    # Evidence的情况下：会一直延续到cur==-1的情况，然后判断下一个token是不是和之前的token仍旧满足26的最大距离要求，\n                    # 如果满足就相连，否则就append到retval里\n                    if (cur == -1 and c != 'Evidence') or ((cur == -1) and (\n                            (pst[i + 1] > pst[end - 1] + thresh) or (pst[i + 1] - pst[start] > thresh2))): #\n                        retval.append((idv, c, jn(pst, start, end)))\n                        start = i + 1\n                v = (idv, c, jn(pst, start, end + 1))\n                # print(v)\n                retval.append(v)\n        roof = pd.DataFrame(retval, columns=['id', 'class', 'predictionstring'])\n        roof = roof.merge(neoof, how='outer')\n        return roof","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.347983Z","iopub.execute_input":"2022-04-15T10:55:57.348264Z","iopub.status.idle":"2022-04-15T10:55:57.363467Z","shell.execute_reply.started":"2022-04-15T10:55:57.348227Z","shell.execute_reply":"2022-04-15T10:55:57.362635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_submit():\n    results = []\n    all_model_idx = 0\n\n    for model_type, model_grp in model_grps.items():\n\n        print(f'{model_type} loaded ok.\\n')\n\n        arch = model_grp['arch']\n        num_models = len(model_grp['checkpoints'])\n\n        tokenizer = AutoTokenizer.from_pretrained(arch)\n\n        split_texts = np.array_split(df_text[['id', 'text']].values, 2)\n\n        tokenized_text_ = Parallel(n_jobs=2, backend=\"multiprocessing\")(\n            delayed(tokenize_data)(text, tokenizer, model_grp['max_length']) for text in split_texts\n        )\n\n        tokenized_text = []\n\n        for t in tokenized_text_:\n            tokenized_text.extend(t)\n\n        del tokenized_text_\n\n        collate_func = Collate(tokenizer)\n        valid_dataset = FeedbackDataset(tokenized_text)\n        valid_loader = DataLoader(\n            valid_dataset,\n            sampler=SequentialSampler(valid_dataset),\n            batch_size=model_grp['batch_size'],  # 4, #\n            drop_last=False,\n            num_workers=2,  # 0, #\n            pin_memory=False,\n            collate_fn = collate_func,\n        )\n\n        # start here !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1\n        results_n = {\n            'id': [],\n            'token_mask': [],\n            'token_offset': [],\n            'probability': [],\n        }\n       \n        for model_idx, (checkpoint, name) in enumerate(model_grp['checkpoints']):\n            net_template = FeedbackModel(arch, 15)\n            net_template.load_state_dict(torch.load(checkpoint))\n            net_template.cuda()\n\n            print('load ok : [%d] %s' % (all_model_idx, name))\n            all_model_idx += 1\n\n            T = 0\n            start_timer = timer()\n            for t, batch in enumerate(valid_loader):\n                batch_size = len(batch['index'])\n                token_mask = batch['token_mask']\n                token_id = batch['token_id']\n#                 print(\"data_shape\", token_id.shape)\n                token_mask = token_mask.cuda()\n                token_id = token_id.cuda()\n\n                net_template.eval()\n                with torch.no_grad():\n                    with amp.autocast(enabled=is_amp):\n                        probability = data_parallel(net_template, (token_id, token_mask))\n                        # probability = net[n](token_id, token_mask)\n\n                        if model_idx == 0:\n                            results_n['probability'].append(\n                                ((probability / num_models) * 255).byte().data.cpu().numpy())\n                            results_n['token_offset'] += [eval(x) for x in batch['token_offset']]\n                            results_n['token_mask'].append(token_mask.bool().data.cpu().numpy())\n                            results_n['id'] += batch['id']\n\n                        else:\n                            results_n['probability'][t] = results_n['probability'][t] + (\n                                        (probability / num_models) * 255).byte().data.cpu().numpy()\n\n                        T += batch_size\n\n                print('\\r\\t%d/%d  %s' % (T, len(valid_dataset), time_to_str(timer() - start_timer, 'sec')), end='',\n                      flush=True)\n#                 del token_id,token_mask\n            del net_template,token_id,token_mask\n            \n            gc.collect()\n\n            torch.cuda.empty_cache()\n            print('')\n       \n        # ----------------------------\n        del valid_dataset, valid_loader, tokenized_text\n        gc.collect()\n        \n        print('')\n\n        prob_list = []\n        for b_idx in range(len(results_n[\"probability\"])):\n            prob_list.extend(np.split(results_n[\"probability\"][b_idx], len(results_n[\"probability\"][b_idx])))\n\n        results.append({\n            'probability': prob_list,\n            'token_offset': np.array(results_n['token_offset'], object)\n            # 'token_mask'] = np.concatenate(results['token_mask'])\n            # 'id'] = np.array(results['id' ])\n        })\n    # ------------------------------------------------------------------------\n    # results: [num_net, [prob list]]\n    num_net = len(model_grps.keys())\n\n    submit_df = []\n    for i in range(num_valid):\n        d = df_text.iloc[i]\n        id = d.id\n        text = d.text\n        word, word_offset = text_to_word(text)\n        # print(i,id[i], len(text), len(word))\n\n        # ensemble -----\n        token_to_text_probability = np.full((len(text), num_discourse_marker), 0, np.float32)\n        for j in range(num_net):\n            p = results[j]['probability'][i][0][1:] / 255 # due to np.split adding another dim [0]\n            for t, (start, end) in enumerate(results[j]['token_offset'][i]):\n                if t == max_length - 1: break  # assume max_length, else use token_mask to get length\n                token_to_text_probability[start:end] += (p[t])  # **0.5\n        token_to_text_probability = token_to_text_probability / num_net\n        # ensemble -----\n\n        text_to_word_probability = np.full((len(word), num_discourse_marker), 0, np.float32)\n        for t, (start, end) in enumerate(word_offset):\n            text_to_word_probability[t] = token_to_text_probability[start:end].mean(0)\n\n        predict_df = word_probability_to_predict_df2(text_to_word_probability, id)\n        submit_df.append(predict_df)\n        # print('\\r preparing submit_df :', i, id, len(text), len(word), end ='', flush=True)\n    print('')\n\n    # ----------------------------------------\n    submit_df = pd.concat(submit_df).reset_index(drop=True)\n    submit_df = do_threshold(submit_df, use=['length', 'probability'])\n#     submit_df = link_evidence(submit_df)\n    submit_df.to_csv('submission.csv', index=False)\n\n    print('----')\n    for t in range(3): print(submit_df.iloc[t], '\\n')\n    print('submission ok!----')","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.364886Z","iopub.execute_input":"2022-04-15T10:55:57.365149Z","iopub.status.idle":"2022-04-15T10:55:57.393014Z","shell.execute_reply.started":"2022-04-15T10:55:57.365114Z","shell.execute_reply":"2022-04-15T10:55:57.392294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.394425Z","iopub.execute_input":"2022-04-15T10:55:57.394755Z","iopub.status.idle":"2022-04-15T10:55:57.402898Z","shell.execute_reply.started":"2022-04-15T10:55:57.39472Z","shell.execute_reply":"2022-04-15T10:55:57.402183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_submit()","metadata":{"execution":{"iopub.status.busy":"2022-04-15T10:55:57.404275Z","iopub.execute_input":"2022-04-15T10:55:57.404604Z","iopub.status.idle":"2022-04-15T11:02:45.604217Z","shell.execute_reply.started":"2022-04-15T10:55:57.404571Z","shell.execute_reply":"2022-04-15T11:02:45.603396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}