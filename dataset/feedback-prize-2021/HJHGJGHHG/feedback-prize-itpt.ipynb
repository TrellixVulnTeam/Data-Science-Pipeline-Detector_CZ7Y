{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Further Pre-training\n\n### This part is copied from: https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-itpt\n\n### Thanks for his work!\n\nBesides the training data of a target task, we can **further pre-train** a transformer on the data from the same domain.\n\n![image.png](https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-030-32381-3_16/MediaObjects/489562_1_En_16_Fig1_HTML.png)\n\nThe Transformer models are pre-trained on the general domain corpus. For a text classification task / regression task in a specific domain, such as Readability Assesment, its data\ndistribution may be different from a transformer trained on a different corpus e.g. RoBERTa trained on BookCorpus, Wiki, CC-News, OpenWebText, Stories. Therefore the idea is, we can further pre-train the transformer with masked language model and next sentence prediction tasks on the domain-specific data. Three further pretraining approaches are performed:\n\n1) `Within-task pre-training (ITPT)`, in which transformer is further pre-trained on the training data of a target task. `This Kernel.`\n\n2) `In-domain pre-training (IDPT)`, in which the pretraining data is obtained from the same domain of a target task. For example, there are several different sentiment classification tasks, which have a similar data distribution. We can further pre-train the transformer on the combined training data from these tasks.\n\n3) `Cross-domain pre-training (CDPT)`, in which the pretraining data is obtained from both the same and other different domains to a target task.\n\n#### Reference1: [How to finetune BERT for Text Classification ?](https://arxiv.org/pdf/1905.05583.pdf)\n#### Reference2: [Don't Stop Pretraining: Adapt Language Models to Domains and Tasks](https://arxiv.org/abs/2004.10964)\n\n> Note: This Kernel implements ITPT i.e. Within-Task Pretraining. First we will pretrain a RoBERTa model and then utilize the same for further finetuing tasks using different strategies.","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport pandas as pd\nfrom tqdm import tqdm\nfrom transformers import (AutoModelForMaskedLM,\n                          AutoTokenizer, LineByLineTextDataset,\n                          DataCollatorForLanguageModeling,\n                          Trainer, TrainingArguments)\n\nwarnings.filterwarnings('ignore')\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2021-12-19T04:06:11.454793Z","iopub.execute_input":"2021-12-19T04:06:11.455118Z","iopub.status.idle":"2021-12-19T04:06:19.16989Z","shell.execute_reply.started":"2021-12-19T04:06:11.455008Z","shell.execute_reply":"2021-12-19T04:06:19.169192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model_name = \"roberta-large\"\n#model_name = \"roberta-base\"\nmodel_name = \"allenai/longformer-base-4096\"\n#model_name = \"allenai/longformer-large-4096\"\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T04:06:19.171491Z","iopub.execute_input":"2021-12-19T04:06:19.171711Z","iopub.status.idle":"2021-12-19T04:07:16.377843Z","shell.execute_reply.started":"2021-12-19T04:06:19.171679Z","shell.execute_reply":"2021-12-19T04:07:16.377079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"train_names, train_texts = [], []\nfor f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n    train_names.append(f.replace('.txt', ''))\n    with open('../input/feedback-prize-2021/train/' + f, 'r', encoding='utf-8') as f:\n        text = ''\n        for line in f.readlines():\n            #text += line.replace('\\n', '').replace('\\xa0', '')\n            text += line.replace('\\n', ' ')\n        train_texts.append(text)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T04:07:16.379012Z","iopub.execute_input":"2021-12-19T04:07:16.379271Z","iopub.status.idle":"2021-12-19T04:08:04.090077Z","shell.execute_reply.started":"2021-12-19T04:07:16.379238Z","shell.execute_reply":"2021-12-19T04:08:04.089379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = '\\n'.join(train_texts)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T04:08:04.091441Z","iopub.execute_input":"2021-12-19T04:08:04.091967Z","iopub.status.idle":"2021-12-19T04:08:04.121624Z","shell.execute_reply.started":"2021-12-19T04:08:04.091928Z","shell.execute_reply":"2021-12-19T04:08:04.120731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('text.txt', 'w') as f:\n    f.write(texts)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T04:08:04.123735Z","iopub.execute_input":"2021-12-19T04:08:04.123992Z","iopub.status.idle":"2021-12-19T04:08:04.19902Z","shell.execute_reply.started":"2021-12-19T04:08:04.123957Z","shell.execute_reply":"2021-12-19T04:08:04.198256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.save_pretrained(\"./model_pretrained\") ","metadata":{"execution":{"iopub.status.busy":"2021-12-19T04:08:04.20028Z","iopub.execute_input":"2021-12-19T04:08:04.20053Z","iopub.status.idle":"2021-12-19T04:08:04.318578Z","shell.execute_reply.started":"2021-12-19T04:08:04.200498Z","shell.execute_reply":"2021-12-19T04:08:04.317906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Further Pretraining","metadata":{}},{"cell_type":"code","source":"train_dataset = LineByLineTextDataset( \n    tokenizer=tokenizer,\n    file_path=\"text.txt\",  # mention train text file here\n    block_size=1024)\n\nvalid_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"text.txt\",  # mention valid text file here\n    block_size=1024)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./model_pretrained_chk\",  # select model path for checkpoint\n    overwrite_output_dir=True,\n    num_train_epochs=4,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=6,\n    #evaluation_strategy='steps',\n    evaluation_strategy='epoch',\n    save_total_limit=1,\n    eval_steps=5000,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    load_best_model_at_end=False,\n    prediction_loss_only=True,\n    learning_rate=5e-5,\n    seed=2021,\n    report_to=\"none\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset)\n\ntrainer.train()\ntrainer.save_model(f'./model_pretrained')","metadata":{"execution":{"iopub.status.busy":"2021-12-19T04:08:04.319646Z","iopub.execute_input":"2021-12-19T04:08:04.319876Z"},"trusted":true},"execution_count":null,"outputs":[]}]}