{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom string import punctuation\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom tqdm import tqdm\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:24:14.62211Z","iopub.execute_input":"2022-02-28T21:24:14.622451Z","iopub.status.idle":"2022-02-28T21:24:14.633975Z","shell.execute_reply.started":"2022-02-28T21:24:14.622417Z","shell.execute_reply":"2022-02-28T21:24:14.632809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/feedback-prize-2021/train.csv')\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:22:49.505065Z","iopub.execute_input":"2022-02-28T21:22:49.50545Z","iopub.status.idle":"2022-02-28T21:22:51.455816Z","shell.execute_reply.started":"2022-02-28T21:22:49.505412Z","shell.execute_reply":"2022-02-28T21:22:51.454764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:22:51.457329Z","iopub.execute_input":"2022-02-28T21:22:51.457622Z","iopub.status.idle":"2022-02-28T21:22:51.48429Z","shell.execute_reply.started":"2022-02-28T21:22:51.457589Z","shell.execute_reply":"2022-02-28T21:22:51.483199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(x='discourse_type', data=train)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:22:52.058647Z","iopub.execute_input":"2022-02-28T21:22:52.058973Z","iopub.status.idle":"2022-02-28T21:22:52.534442Z","shell.execute_reply.started":"2022-02-28T21:22:52.05894Z","shell.execute_reply":"2022-02-28T21:22:52.533472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def generate_test_df():\n#     test_dir = \"../input/feedback-prize-2021/train\"\n#     test_files = os.listdir(test_dir)\n    \n#     test_names = []\n#     test_texts = []\n#     for f in os.listdir(test_dir):\n#         test_names.append(f.replace('.txt', ''))\n#         test_texts.append(open(test_dir+'/'+f, 'r').read())\n        \n#     test_texts = pd.DataFrame({'id':test_names, 'text':test_texts})\n    \n#     discourse_id = []\n#     discourse_text = []\n#     discourse_start =[] \n#     discourse_end = []\n#     predictionstring = []\n#     for text in range(len(test_texts.text)):\n#         doc = test_texts.text[text]\n#         paras = re.split(r'[.]\\n',doc)\n#         start = 0\n#         for para in paras:\n#             positions = re.findall(r'\\w+[.]', para)\n#             if len(positions) <=2 :\n#                 txts = re.split(r'[.]\\s',para)\n#                 for i in txts:\n#                     discourse_text.append(i)\n#                     length = len(i.split())\n#                     end = start + length\n#                     l = list(range(start+1,end+1 ))\n#                     l = [str(j) for j in l]\n#                     l = ' '.join(l)\n#                     predictionstring.append(l)\n#                     discourse_start.append(start+1)\n#                     discourse_end.append(end)\n#                     discourse_id.append(test_texts.id[text])\n#                     start += length\n\n#             else: \n#                 if len(positions)%2 == 0:\n#                     split_pos = int(len(positions)/2)\n#                     split_word = positions[split_pos]\n#                 else :\n#                     split_pos = int((len(positions)+1)/2)\n#                     split_word = positions[split_pos]\n\n#                 words = para.split(' ')\n#                 position = words.index(split_word)\n#                 part1 = words[:position]\n#                 part2 = words[position:]\n#                 part1 = ' '.join(part1)\n#                 part2 = ' '.join(part2)\n#                 parts = [part1 ,part2]\n#                 for i in parts:\n#                     discourse_text.append(i)\n#                     length = len(i.split())\n#                     end = start + length\n#                     l = list(range(start+1,end+1 ))\n#                     l = [str(k) for k in l]\n#                     l = ' '.join(l)\n#                     predictionstring.append(l)\n#                     discourse_start.append(start+1)\n#                     discourse_end.append(end)\n#                     discourse_id.append(test_texts.id[text])\n#                     start += length\n\n#     testing_data =pd.DataFrame() \n#     testing_data['discourse_id'] =discourse_id\n#     testing_data['discourse_text'] = discourse_text\n#     testing_data['discourse_start'] = discourse_start\n#     testing_data['discourse_end'] = discourse_end\n#     testing_data['predictionstring'] = predictionstring\n#     return testing_data\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T19:20:58.324006Z","iopub.execute_input":"2022-02-28T19:20:58.324346Z","iopub.status.idle":"2022-02-28T19:20:58.340064Z","shell.execute_reply.started":"2022-02-28T19:20:58.32421Z","shell.execute_reply":"2022-02-28T19:20:58.338849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test = generate_test_df()\n# test.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T19:21:00.017148Z","iopub.execute_input":"2022-02-28T19:21:00.017381Z","iopub.status.idle":"2022-02-28T19:21:32.657455Z","shell.execute_reply.started":"2022-02-28T19:21:00.017359Z","shell.execute_reply":"2022-02-28T19:21:32.65573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST_PATH = \"../input/feedback-prize-2021/test\"\n\ndef get_test_text(a_id):\n    a_file = f\"{TEST_PATH}/{a_id}.txt\"\n    with open(a_file, \"r\") as fp:\n        txt = fp.read()\n    return txt\n\ndef create_df_test():\n    test_ids = [f[:-4] for f in os.listdir(TEST_PATH)] #Remove the last 4 characters ('.txt') in the filenames such as '0FB0700DAF44.txt'.\n    test_data = []\n    for test_id in test_ids:\n        text = get_test_text(test_id)\n        sentences = nltk.sent_tokenize(text)\n        id_sentences = []\n        idx = 0 \n        for sentence in sentences:\n            id_sentence = []\n            words = sentence.split()\n            # I created this heuristic for mapping words in sentences to \"word indices\"\n            # This is not definitive and might have strong drawbacks and problems\n            for w in words:\n                id_sentence.append(idx)\n                idx+=1\n            id_sentences.append(id_sentence)\n        test_data += list(zip([test_id] * len(sentences), sentences, id_sentences))\n    df_test = pd.DataFrame(test_data, columns=['id', 'discourse_text', 'ids'])\n    return df_test","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:23:53.658437Z","iopub.execute_input":"2022-02-28T21:23:53.658786Z","iopub.status.idle":"2022-02-28T21:23:53.669302Z","shell.execute_reply.started":"2022-02-28T21:23:53.65875Z","shell.execute_reply":"2022-02-28T21:23:53.668126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = create_df_test()\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:24:25.753812Z","iopub.execute_input":"2022-02-28T21:24:25.754133Z","iopub.status.idle":"2022-02-28T21:24:25.820631Z","shell.execute_reply.started":"2022-02-28T21:24:25.7541Z","shell.execute_reply":"2022-02-28T21:24:25.819276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['predictionstring'] = df_test['ids'].apply(lambda x: ' '.join([str(i) for i in x]))\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:24:35.923671Z","iopub.execute_input":"2022-02-28T21:24:35.923989Z","iopub.status.idle":"2022-02-28T21:24:35.944258Z","shell.execute_reply.started":"2022-02-28T21:24:35.923954Z","shell.execute_reply":"2022-02-28T21:24:35.943145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test.drop('ids', axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:24:47.907705Z","iopub.execute_input":"2022-02-28T21:24:47.90804Z","iopub.status.idle":"2022-02-28T21:24:47.915264Z","shell.execute_reply.started":"2022-02-28T21:24:47.908006Z","shell.execute_reply":"2022-02-28T21:24:47.914449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:24:57.009157Z","iopub.execute_input":"2022-02-28T21:24:57.009632Z","iopub.status.idle":"2022-02-28T21:24:57.022226Z","shell.execute_reply.started":"2022-02-28T21:24:57.009586Z","shell.execute_reply":"2022-02-28T21:24:57.021255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = train.append(df_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:25:17.665499Z","iopub.execute_input":"2022-02-28T21:25:17.665825Z","iopub.status.idle":"2022-02-28T21:25:17.711715Z","shell.execute_reply.started":"2022-02-28T21:25:17.665791Z","shell.execute_reply":"2022-02-28T21:25:17.710432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"def clean_text(text, remove_stopwords=True, stem_words=False, lemma=True):\n    text = str(text).lower().split()\n    \n    if remove_stopwords:\n        stops = stopwords.words(\"english\")\n        text = [x for x in text if not x in stops]\n        \n    text = ' '.join(text)\n    \n    text = re.sub(r\"[-()\\\"#/<>!@&;*:<>{}`'+=~%|.!?,_]\", \" \", text)\n    text = re.sub(r\"\\]\", \" \", text)\n    text = re.sub(r\"\\[\", \" \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\\\\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"  \", \" \", text)\n    text = re.sub(r\"   \", \" \", text)\n    text = re.sub(r\"   \", \" \", text)\n    text = re.sub(r\"0x00\", \"\", text)\n    \n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stem_words = [stemmer.stem(x) for x in text]\n        text = \" \".join(text)\n        \n    if lemma:\n        text = text.split()\n        lem = WordNetLemmatizer()\n        lemmatized = [lem.lemmatize(x, \"v\") for x in text]\n        text = \" \".join(text)\n        \n    return text","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:25:19.611102Z","iopub.execute_input":"2022-02-28T21:25:19.61149Z","iopub.status.idle":"2022-02-28T21:25:19.626243Z","shell.execute_reply.started":"2022-02-28T21:25:19.611449Z","shell.execute_reply":"2022-02-28T21:25:19.62544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf['cleaned_text'] = df.discourse_text.apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:25:22.990401Z","iopub.execute_input":"2022-02-28T21:25:22.99074Z","iopub.status.idle":"2022-02-28T21:26:28.546162Z","shell.execute_reply.started":"2022-02-28T21:25:22.990706Z","shell.execute_reply":"2022-02-28T21:26:28.545042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.iloc[0]['discourse_text']","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:26:28.548334Z","iopub.execute_input":"2022-02-28T21:26:28.548626Z","iopub.status.idle":"2022-02-28T21:26:28.555604Z","shell.execute_reply.started":"2022-02-28T21:26:28.548591Z","shell.execute_reply":"2022-02-28T21:26:28.554636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.iloc[0]['cleaned_text']","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:26:28.557119Z","iopub.execute_input":"2022-02-28T21:26:28.557401Z","iopub.status.idle":"2022-02-28T21:26:28.568679Z","shell.execute_reply.started":"2022-02-28T21:26:28.557329Z","shell.execute_reply":"2022-02-28T21:26:28.567636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TFIDF Vectorizer","metadata":{}},{"cell_type":"code","source":"tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=1000)\ntfidf_vect.fit(df['cleaned_text'])","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:26:33.837521Z","iopub.execute_input":"2022-02-28T21:26:33.838196Z","iopub.status.idle":"2022-02-28T21:26:37.759083Z","shell.execute_reply.started":"2022-02-28T21:26:33.83816Z","shell.execute_reply":"2022-02-28T21:26:37.758058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tfidf = tfidf_vect.fit_transform(df[:train.shape[0]]['cleaned_text'])\ntest_tfidf = tfidf_vect.transform(df[train.shape[0]:]['cleaned_text'])\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:26:44.191898Z","iopub.execute_input":"2022-02-28T21:26:44.192242Z","iopub.status.idle":"2022-02-28T21:26:48.203465Z","shell.execute_reply.started":"2022-02-28T21:26:44.192205Z","shell.execute_reply":"2022-02-28T21:26:48.202505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = tfidf_vect.get_feature_names()\nfeatures[:20]","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:26:49.73862Z","iopub.execute_input":"2022-02-28T21:26:49.738935Z","iopub.status.idle":"2022-02-28T21:26:49.747925Z","shell.execute_reply.started":"2022-02-28T21:26:49.738901Z","shell.execute_reply":"2022-02-28T21:26:49.746712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"#cv = KFold(n_splits=10, random_state=1, shuffle=True)\nrf_model = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)\n#scores = cross_val_score(rf_model, train_tfidf, train['discourse_type'], scoring='accuracy', cv=cv)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:26:53.041331Z","iopub.execute_input":"2022-02-28T21:26:53.041686Z","iopub.status.idle":"2022-02-28T21:26:53.047751Z","shell.execute_reply.started":"2022-02-28T21:26:53.041652Z","shell.execute_reply":"2022-02-28T21:26:53.046441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:26:54.639696Z","iopub.execute_input":"2022-02-28T21:26:54.640103Z","iopub.status.idle":"2022-02-28T21:26:54.644197Z","shell.execute_reply.started":"2022-02-28T21:26:54.640056Z","shell.execute_reply":"2022-02-28T21:26:54.643478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nrf_model.fit(train_tfidf, train['discourse_type'])","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:26:59.237484Z","iopub.execute_input":"2022-02-28T21:26:59.238399Z","iopub.status.idle":"2022-02-28T21:28:30.266027Z","shell.execute_reply.started":"2022-02-28T21:26:59.238346Z","shell.execute_reply":"2022-02-28T21:28:30.265065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_pred = rf_model.predict(test_tfidf) ","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:28:54.653165Z","iopub.execute_input":"2022-02-28T21:28:54.653562Z","iopub.status.idle":"2022-02-28T21:28:54.664288Z","shell.execute_reply.started":"2022-02-28T21:28:54.653526Z","shell.execute_reply":"2022-02-28T21:28:54.663177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"submission_df = pd.DataFrame()\nsubmission_df['id'] = df_test['id']\nsubmission_df['class'] = rf_pred# label of y_predict\nsubmission_df['predictionstring'] = df_test['predictionstring']\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:29:00.157615Z","iopub.execute_input":"2022-02-28T21:29:00.158593Z","iopub.status.idle":"2022-02-28T21:29:00.167406Z","shell.execute_reply.started":"2022-02-28T21:29:00.158535Z","shell.execute_reply":"2022-02-28T21:29:00.166579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:29:02.843436Z","iopub.execute_input":"2022-02-28T21:29:02.843775Z","iopub.status.idle":"2022-02-28T21:29:02.85842Z","shell.execute_reply.started":"2022-02-28T21:29:02.843739Z","shell.execute_reply":"2022-02-28T21:29:02.857273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T21:29:07.040079Z","iopub.execute_input":"2022-02-28T21:29:07.040674Z","iopub.status.idle":"2022-02-28T21:29:07.051241Z","shell.execute_reply.started":"2022-02-28T21:29:07.040637Z","shell.execute_reply":"2022-02-28T21:29:07.050257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}