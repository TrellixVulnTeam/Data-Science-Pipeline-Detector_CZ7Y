{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RAPIDS UMAP, Tfidf, and KMeans Discovers 15 Essay Topics\nIn this notebook we will find the essay topics using RAPIDS cudf, UMAP, Tfidf, and KMeans. First we will convert each text into a Tfidf embedding. Then we will use UMAP to reduce these embeddings to two dimensions. Lastly we will use KMeans to find the essay topics!","metadata":{"papermill":{"duration":0.020585,"end_time":"2022-01-17T23:04:27.447337","exception":false,"start_time":"2022-01-17T23:04:27.426752","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Load RAPIDS","metadata":{"papermill":{"duration":0.017777,"end_time":"2022-01-17T23:04:27.480748","exception":false,"start_time":"2022-01-17T23:04:27.462971","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd, os\nimport cudf, cuml, cupy\nfrom tqdm import tqdm\nimport numpy as np\nprint('RAPIDS',cudf.__version__)","metadata":{"execution":{"iopub.execute_input":"2022-01-17T23:04:27.508371Z","iopub.status.busy":"2022-01-17T23:04:27.507624Z","iopub.status.idle":"2022-01-17T23:04:31.563509Z","shell.execute_reply":"2022-01-17T23:04:31.564116Z","shell.execute_reply.started":"2022-01-17T22:38:10.201498Z"},"papermill":{"duration":4.07503,"end_time":"2022-01-17T23:04:31.564422","exception":false,"start_time":"2022-01-17T23:04:27.489392","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RAPIDS cudf\nWe will read train text into a RAPIDS cudf.","metadata":{"papermill":{"duration":0.008852,"end_time":"2022-01-17T23:04:31.583041","exception":false,"start_time":"2022-01-17T23:04:31.574189","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\ntrain_names, train_texts = [], []\nfor f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n    train_names.append(f.replace('.txt', ''))\n    train_texts.append(open('../input/feedback-prize-2021/train/' + f, 'r').read())\ntrain_text_df = cudf.DataFrame({'id': train_names, 'text': train_texts})\ntrain_text_df.head()","metadata":{"execution":{"iopub.execute_input":"2022-01-17T23:04:31.606024Z","iopub.status.busy":"2022-01-17T23:04:31.605502Z","iopub.status.idle":"2022-01-17T23:05:25.43246Z","shell.execute_reply":"2022-01-17T23:05:25.432998Z","shell.execute_reply.started":"2022-01-17T22:29:49.481019Z"},"papermill":{"duration":53.841219,"end_time":"2022-01-17T23:05:25.433189","exception":false,"start_time":"2022-01-17T23:04:31.59197","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RAPIDS Tfidf\nWe will use Tfidf to convert each text into a embedding vector of length 25,000.","metadata":{"papermill":{"duration":0.144337,"end_time":"2022-01-17T23:05:25.723418","exception":false,"start_time":"2022-01-17T23:05:25.579081","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from cuml.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(stop_words='english', binary=True, max_features=25_000)\ntext_embeddings = tfidf.fit_transform( train_text_df.text ).toarray()","metadata":{"execution":{"iopub.execute_input":"2022-01-17T23:05:26.015389Z","iopub.status.busy":"2022-01-17T23:05:26.014431Z","iopub.status.idle":"2022-01-17T23:05:39.741567Z","shell.execute_reply":"2022-01-17T23:05:39.741082Z","shell.execute_reply.started":"2022-01-17T22:34:48.263481Z"},"papermill":{"duration":13.874568,"end_time":"2022-01-17T23:05:39.7417","exception":false,"start_time":"2022-01-17T23:05:25.867132","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RAPIDS UMAP\nWe will use UMAP to reduce embedding vectors to two dimensions","metadata":{"papermill":{"duration":0.128876,"end_time":"2022-01-17T23:05:40.000099","exception":false,"start_time":"2022-01-17T23:05:39.871223","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from cuml import UMAP\numap = UMAP()\nembed_2d = umap.fit_transform(text_embeddings)\nembed_2d = cupy.asnumpy( embed_2d )","metadata":{"execution":{"iopub.execute_input":"2022-01-17T23:05:40.262893Z","iopub.status.busy":"2022-01-17T23:05:40.262246Z","iopub.status.idle":"2022-01-17T23:05:42.954379Z","shell.execute_reply":"2022-01-17T23:05:42.953515Z","shell.execute_reply.started":"2022-01-17T22:34:56.586103Z"},"papermill":{"duration":2.825378,"end_time":"2022-01-17T23:05:42.954513","exception":false,"start_time":"2022-01-17T23:05:40.129135","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RAPIDS KMeans\nWe will use KMeans to find clusters of essays. These are the essay topics!","metadata":{"papermill":{"duration":0.128922,"end_time":"2022-01-17T23:05:43.21343","exception":false,"start_time":"2022-01-17T23:05:43.084508","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from cuml import KMeans\nkmeans = cuml.KMeans(n_clusters=15)\nkmeans.fit(embed_2d)\ntrain_text_df['cluster'] = kmeans.labels_","metadata":{"execution":{"iopub.execute_input":"2022-01-17T23:05:43.478991Z","iopub.status.busy":"2022-01-17T23:05:43.478175Z","iopub.status.idle":"2022-01-17T23:05:43.501224Z","shell.execute_reply":"2022-01-17T23:05:43.500729Z","shell.execute_reply.started":"2022-01-17T22:36:35.645499Z"},"papermill":{"duration":0.158561,"end_time":"2022-01-17T23:05:43.501347","exception":false,"start_time":"2022-01-17T23:05:43.342786","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Display Essay Topics\nWe will display the result of UMAP which reduced text to two dimension. We observe that the essays cluster into 15 groups. These are the 15 essay topics! Additionally we will plot the most important word from each group.","metadata":{"papermill":{"duration":0.129361,"end_time":"2022-01-17T23:05:43.759556","exception":false,"start_time":"2022-01-17T23:05:43.630195","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ncenters = kmeans.cluster_centers_\n\nplt.figure(figsize=(10,10))\nplt.scatter(embed_2d[:,0], embed_2d[:,1], s=1, c=kmeans.labels_)\nplt.title('UMAP Plot of Train Text using Tfidf features\\nRAPIDS Discovers the 15 essay topics!',size=16)\n\nfor k in range(len(centers)):\n    mm = cupy.mean( text_embeddings[train_text_df.cluster.values==k],axis=0 )\n    ii = cupy.argmax(mm)\n    top_word = tfidf.vocabulary_.iloc[ii]\n    plt.text(centers[k,0]-1,centers[k,1]+0.75,f'{k+1}-{top_word}',size=16)\n\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2022-01-17T23:05:44.034736Z","iopub.status.busy":"2022-01-17T23:05:44.034211Z","iopub.status.idle":"2022-01-17T23:05:46.938032Z","shell.execute_reply":"2022-01-17T23:05:46.937552Z","shell.execute_reply.started":"2022-01-17T22:58:36.134453Z"},"papermill":{"duration":3.049409,"end_time":"2022-01-17T23:05:46.938163","exception":false,"start_time":"2022-01-17T23:05:43.888754","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Display Example Text\nWe will display three example text from each essay topic. And we will display the five most important words from each topic.","metadata":{"papermill":{"duration":0.136156,"end_time":"2022-01-17T23:05:47.218946","exception":false,"start_time":"2022-01-17T23:05:47.08279","status":"completed"},"tags":[]}},{"cell_type":"code","source":"for k in range(15):\n    mm = cupy.mean( text_embeddings[train_text_df.cluster.values==k],axis=0 )\n    ii = cupy.asnumpy( cupy.argsort(mm)[-5:][::-1] )\n    top_words = tfidf.vocabulary_.to_array()[ii]\n    print('#'*25)\n    print(f'### Essay Topic {k+1}')\n    print('### Top 5 Words',top_words)\n    print('#'*25)\n    tmp = train_text_df.loc[train_text_df.cluster==k].sample(3, random_state=123)\n    for j in range(3):\n        txt = tmp.iloc[j,1]\n        print('-'*10,f'Example {j+1}','-'*10)\n        print(txt,'\\n')","metadata":{"execution":{"iopub.execute_input":"2022-01-17T23:05:47.494215Z","iopub.status.busy":"2022-01-17T23:05:47.493468Z","iopub.status.idle":"2022-01-17T23:05:50.196391Z","shell.execute_reply":"2022-01-17T23:05:50.195759Z","shell.execute_reply.started":"2022-01-17T22:51:22.931815Z"},"papermill":{"duration":2.844041,"end_time":"2022-01-17T23:05:50.196557","exception":false,"start_time":"2022-01-17T23:05:47.352516","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}