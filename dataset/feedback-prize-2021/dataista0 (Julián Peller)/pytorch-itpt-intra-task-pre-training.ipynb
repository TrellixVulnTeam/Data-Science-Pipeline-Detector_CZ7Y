{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ“– Torch Roberta - ITPT - Intra-task pre-training\n\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/31779/logos/header.png)\n\n## Intra-task pre-training of a `roberta-large` (but trivially adaptable to any MLM model) over the [Feedback Prize - Evaluating Student Writing](https://www.kaggle.com/c/feedback-prize-2021)\n\n\nBased on this notebook by [torch](): [CommonLit Readability Prize - RoBERTa Torch|ITPT](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-itpt), which in turn is based on this script by huggingface: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm_no_trainer.py\n\nA good learning reference for this is also the Chapter 7 of the HuggingFace course, there is a detailed step-by-step explanation of the code atoms of this notebook: [Chapter 7 - Section 3 - Fine-tuning a masked language model](https://huggingface.co/course/chapter7/3?fw=pt).\n\n\n# ðŸ¤— Please _DO_ upvote if you find this helpful or interesting! ðŸ¤—\n","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport logging\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer,\\\n                         AdamW, DataCollatorForLanguageModeling,\\\n                         get_scheduler","metadata":{"execution":{"iopub.status.busy":"2022-01-30T03:25:20.681742Z","iopub.execute_input":"2022-01-30T03:25:20.682416Z","iopub.status.idle":"2022-01-30T03:25:28.333055Z","shell.execute_reply.started":"2022-01-30T03:25:20.682323Z","shell.execute_reply":"2022-01-30T03:25:28.331695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"class Config:\n    model_name = 'roberta-large'\n    max_length = 512\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    validation_size = 0.05\n    mlm_probability = 0.15\n    \n    train_batch_size = 4\n    eval_batch_size = 4\n    \n    learning_rate = 2.5e-5\n    \n    num_train_epochs = 3\n        \n    lr_scheduler_type = 'constant_with_warmup'\n    num_warmup_steps = 0\n\nargs = Config()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T03:25:28.339467Z","iopub.execute_input":"2022-01-30T03:25:28.340062Z","iopub.status.idle":"2022-01-30T03:25:28.410079Z","shell.execute_reply.started":"2022-01-30T03:25:28.33991Z","shell.execute_reply":"2022-01-30T03:25:28.407409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create one CSV with all the texts","metadata":{}},{"cell_type":"code","source":"def create_mlm_csv():\n    \"\"\" Read all training texts to a csv file with one column 'text' \"\"\"\n    texts = []\n    \n    for f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n        with open('../input/feedback-prize-2021/train/' + f, 'r') as fp:\n            texts.append(fp.read())\n    \n    df = pd.DataFrame({'text': texts})\n    \n    display(df.head())\n    df.to_csv(\"mlm_train.csv\", index=False)\n    return df\n\ndf = create_mlm_csv()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T03:25:28.414566Z","iopub.execute_input":"2022-01-30T03:25:28.416806Z","iopub.status.idle":"2022-01-30T03:26:12.880948Z","shell.execute_reply.started":"2022-01-30T03:25:28.416756Z","shell.execute_reply":"2022-01-30T03:26:12.880155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"model = AutoModelForMaskedLM.from_pretrained(args.model_name)\nmodel.to(args.device)\n\ntokenizer = AutoTokenizer.from_pretrained(args.model_name)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T03:26:12.883145Z","iopub.execute_input":"2022-01-30T03:26:12.883433Z","iopub.status.idle":"2022-01-30T03:27:00.073082Z","shell.execute_reply.started":"2022-01-30T03:26:12.883395Z","shell.execute_reply":"2022-01-30T03:27:00.072267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset and DataLoader","metadata":{}},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(examples['text'], return_special_tokens_mask=True)\n\ndef group_texts(examples):\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    total_length = (total_length // args.max_length) * args.max_length\n    result = {\n        k: [t[i : i + args.max_length] for i in range(0, total_length, args.max_length)]\n        for k, t in concatenated_examples.items()\n    }\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-01-30T03:27:00.074653Z","iopub.execute_input":"2022-01-30T03:27:00.074897Z","iopub.status.idle":"2022-01-30T03:27:00.081928Z","shell.execute_reply.started":"2022-01-30T03:27:00.074863Z","shell.execute_reply":"2022-01-30T03:27:00.08112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_datasets = load_dataset(\"csv\", data_files={'train': 'mlm_train.csv'})\n\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=['text'])\\\n                                 .map(group_texts, batched=True)\n\ntokenized_datasets = tokenized_datasets[\"train\"].train_test_split(test_size=args.validation_size)\ntokenized_datasets['validation'] = tokenized_datasets.pop(\"test\")\n\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=args.mlm_probability)\n\n\ndl_train = DataLoader(tokenized_datasets[\"train\"], \n                      shuffle=True, \n                      collate_fn=data_collator, \n                      batch_size=args.train_batch_size)\n\ndl_val = DataLoader(tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=args.eval_batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T03:27:00.083506Z","iopub.execute_input":"2022-01-30T03:27:00.083798Z","iopub.status.idle":"2022-01-30T03:28:21.059397Z","shell.execute_reply.started":"2022-01-30T03:27:00.083759Z","shell.execute_reply":"2022-01-30T03:28:21.058616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimizer and Scheduler","metadata":{}},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=args.learning_rate)\n\nnum_training_steps = args.num_train_epochs * len(dl_train)\nlr_scheduler = get_scheduler(\n    name=args.lr_scheduler_type,\n    optimizer=optimizer,\n    num_warmup_steps=args.num_warmup_steps,\n    num_training_steps=num_training_steps,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T03:28:21.060906Z","iopub.execute_input":"2022-01-30T03:28:21.061337Z","iopub.status.idle":"2022-01-30T03:28:21.074035Z","shell.execute_reply.started":"2022-01-30T03:28:21.061295Z","shell.execute_reply":"2022-01-30T03:28:21.073203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training/validation loop","metadata":{}},{"cell_type":"code","source":"print(\"***** Running training *****\")\nprint(f\"  Num examples = {len(tokenized_datasets['train'])}\")\nprint(f\"  Num Epochs = {args.num_train_epochs}\")\nprint(f\"  Total training steps = {num_training_steps}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-30T03:28:21.075615Z","iopub.execute_input":"2022-01-30T03:28:21.075911Z","iopub.status.idle":"2022-01-30T03:28:21.135596Z","shell.execute_reply.started":"2022-01-30T03:28:21.075873Z","shell.execute_reply":"2022-01-30T03:28:21.13474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"progress_bar = tqdm(range(num_training_steps))\ncompleted_steps = 0\n\nfor epoch in range(args.num_train_epochs):\n    model.train()\n    cum_loss = 0\n    for batch_idx, batch in enumerate(dl_train, 1):\n        \n        outputs = model(**{k: v.to(args.device) for k, v in batch.items()})\n        loss = outputs.loss\n        cum_loss += loss.item()\n        \n        \n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n        progress_bar.set_postfix({'loss': cum_loss / batch_idx})\n        #if batch_idx > 100:\n        #    break\n\n    model.eval()\n    losses = []\n    for batch_idx, batch in enumerate(dl_val, 1):\n        with torch.no_grad():\n            outputs = model(**{k: v.to(args.device) for k, v in batch.items()})\n\n        loss = outputs.loss\n        losses.append(loss)\n        #if batch_idx > 100:\n        #    break\n\n    losses = torch.tensor(losses)\n    losses = losses[: len(tokenized_datasets['validation'])]\n    perplexity = math.exp(torch.mean(losses))\n\n    print(f\"Epoch {epoch}: perplexity: {perplexity}\")\n    model.save_pretrained(f'roberta_large-itpt-e{epoch}')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T03:28:21.137278Z","iopub.execute_input":"2022-01-30T03:28:21.1377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ¤— Please _DO_ upvote if you find this helpful or interesting! ðŸ¤—","metadata":{}}]}