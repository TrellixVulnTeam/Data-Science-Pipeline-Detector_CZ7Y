{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ“– W&B-tracked Shortformers experiments\n\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/31779/logos/header.png)\n\n\n## This notebook defines a reduced version of the competition [Feedback Prize - Evaluating Student Writing](https://www.kaggle.com/c/feedback-prize-2021) data in order to experiment in a fast fashion logging results to W&B.\n\n## You can check the results of the current experiments here: https://wandb.ai/dataista/fp-models-exp1 and, in general, of all experiments, here: https://wandb.ai/dataista/. I'm making the results public.\n\n\nThere is a cell with various configuration parameters that affect the full experiment.\nThe keys `train_data_size` and `val_data_size` allow to reduce the dataset size in order to run epochs faster. Hopefully, the results obtained with this smaller versions are correlated to the results of the full problem.\n\nAll the code is wrapped up in a `run` facade function, making it easier to copy everything into a python script and run it from somewhere else.\n\nFor using W&B you need to set up a user and configure the API key in the Add-ons menu here in the notebook editor. You can check the instructions provided for this, for example, in this great notebook: [[Pytorch + W&B] Jigsaw Starter](https://www.kaggle.com/debarshichanda/pytorch-w-b-jigsaw-starter) by [Debarshi Chanda](https://www.kaggle.com/debarshichanda).\n\nThis is a follow-up of the [ðŸ“– PyTorch- \"ShortFormer\" w/Chunks - Train [0.624]](https://www.kaggle.com/julian3833/pytorch-shortformer-w-chunks-train-0-624) that frames the problem as a token classification (\"NER\"-like) problem and uses small transformers (not longformer or bigbird) using the old-fashioned chunk, predict and regroup strategy.\n\n\n# Please _DO_ upvote if you found this kernel useful or interesting! ðŸ¤—\n\n&nbsp;\n&nbsp;\n\nOk, let's go!","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport ast\nimport time\nimport wandb\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification\n\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'","metadata":{"execution":{"iopub.status.busy":"2022-02-03T18:00:51.527874Z","iopub.execute_input":"2022-02-03T18:00:51.528454Z","iopub.status.idle":"2022-02-03T18:00:59.588192Z","shell.execute_reply.started":"2022-02-03T18:00:51.528352Z","shell.execute_reply":"2022-02-03T18:00:59.5874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run Configuration\n\nThis is the main configuration for the execution","metadata":{}},{"cell_type":"code","source":"\nconfig = {'train_batch_size': 4,\n          'valid_batch_size': 2,\n          'epochs': 5,\n          'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n          'max_grad_norm': 10,\n          'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n          'model_name': 'roberta-large',\n          'max_length': 512,\n          'doc_stride': 128,\n          'train_data_size': 0.33,\n          'val_data_size': 0.33,\n          'experiment_description': 'Roberta Large LR=Chris'\n          }\n\nWANDB_ENTITY = \"dataista\"\nRUN_NAME = f\"{config['model_name']}-lr=chris\"\nWANDB_PROJECT = \"fp-roberta-lr\"","metadata":{"execution":{"iopub.status.busy":"2022-02-03T18:00:59.590188Z","iopub.execute_input":"2022-02-03T18:00:59.590458Z","iopub.status.idle":"2022-02-03T18:00:59.650056Z","shell.execute_reply.started":"2022-02-03T18:00:59.590422Z","shell.execute_reply":"2022-02-03T18:00:59.648773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Code","metadata":{}},{"cell_type":"code","source":"def download_model():\n    # https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615\n    if os.path.isdir('model'):\n        return \n    \n    os.mkdir('model')\n    \n    tokenizer = AutoTokenizer.from_pretrained(config['model_name'], add_prefix_space=True)\n    tokenizer.save_pretrained('model')\n\n    config_model = AutoConfig.from_pretrained(config['model_name']) \n    config_model.num_labels = 15\n    config_model.save_pretrained('model')\n\n    backbone = AutoModelForTokenClassification.from_pretrained(config['model_name'], \n                                                               config=config_model)\n    backbone.save_pretrained('model')\n    print(f\"Model downloaded to model/\")\n    \n\ndef get_labels(word_ids, word_labels):\n    label_ids = []\n    for word_idx in word_ids:                            \n        if word_idx is None:\n            label_ids.append(-100)\n        else:\n            label_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\n    return label_ids\n\n\n# Tokenize texts, possibly generating more than one tokenized sample for each text\ndef tokenize(df, tokenizer, to_tensor=True, with_labels=True):\n    \n    # This is what's different from a longformer\n    # Read the parameters with attention\n    encoded = tokenizer(df['text_split'].tolist(),\n                        is_split_into_words=True,\n                        return_overflowing_tokens=True,\n                        stride=config['doc_stride'],\n                        max_length=config['max_length'],\n                        padding=\"max_length\",\n                        truncation=True)\n\n    if with_labels:\n        encoded['labels'] = []\n\n    encoded['wids'] = []\n    n = len(encoded['overflow_to_sample_mapping'])\n    for i in range(n):\n\n        # Map back to original row\n        text_idx = encoded['overflow_to_sample_mapping'][i]\n        \n        # Get word indexes (this is a global index that takes into consideration the chunking :D )\n        word_ids = encoded.word_ids(i)\n        \n        if with_labels:\n            # Get word labels of the full un-chunked text\n            word_labels = df['entities'].iloc[text_idx]\n        \n            # Get the labels associated with the word indexes\n            label_ids = get_labels(word_ids, word_labels)\n            encoded['labels'].append(label_ids)\n        encoded['wids'].append([w if w is not None else -1 for w in word_ids])\n    \n    if to_tensor:\n        encoded = {key: torch.as_tensor(val) for key, val in encoded.items()}\n    return encoded\n\n\nclass FeedbackPrizeDataset(Dataset):\n    def __init__(self, tokenized_ds):\n        self.data = tokenized_ds\n\n    def __getitem__(self, index):\n        item = {k: self.data[k][index] for k in self.data.keys()}\n        return item\n\n    def __len__(self):\n        return len(self.data['input_ids'])\n    \n    \n# from Rob Mulla @robikscube\n# https://www.kaggle.com/robikscube/student-writing-competition-twitch\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(' '))\n    set_gt = set(row.predictionstring_gt.split(' '))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','class','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on=['id','class'],\n                           right_on=['id','discourse_type'],\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    my_f1_score = TP / (TP + 0.5*(FP+FN))\n    return my_f1_score\n\n\ndef inference(dl, model):\n    \n    # These 2 dictionaries will hold text-level data\n    # Helping in the merging process by accumulating data\n    # Through all the chunks\n    predictions = defaultdict(list)\n    seen_words_idx = defaultdict(list)\n    \n    val_loss = 0.0\n    steps = 0\n    examples = 0\n    val_accuracy = 0\n    \n    for batch in dl:\n        ids = batch[\"input_ids\"].to(config['device'])\n        mask = batch[\"attention_mask\"].to(config['device'])\n        labels = batch['labels'].to(config['device'], dtype = torch.long)\n        loss, logits = model(ids, attention_mask=mask, labels=labels, return_dict=False)\n        \n        del ids, mask\n        \n        batch_preds = torch.argmax(logits, axis=-1).cpu().numpy() \n        val_loss += loss.item()\n\n        steps += 1\n        examples += labels.size(0)\n        \n        #import pdb; pdb.set_trace()\n        \n        \n   \n        # compute training accuracy\n        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n        active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n        \n        # only compute accuracy at active labels\n        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n        labels = torch.masked_select(flattened_targets, active_accuracy)\n        y_predictions = torch.masked_select(flattened_predictions, active_accuracy)\n        \n        tmp_accuracy = accuracy_score(labels.cpu().numpy(), y_predictions.cpu().numpy())\n        val_accuracy += tmp_accuracy\n    \n        # Go over each prediction, getting the text_id reference\n        for k, (chunk_preds, text_id) in enumerate(zip(batch_preds, batch['overflow_to_sample_mapping'].tolist())):\n            \n            # The word_ids are absolute references in the original text\n            word_ids = batch['wids'][k].numpy()\n            \n            # Map from ids to labels\n            chunk_preds = [IDS_TO_LABELS[i] for i in chunk_preds]        \n            \n            for idx, word_idx in enumerate(word_ids):                            \n                if word_idx == -1:\n                    pass\n                elif word_idx not in seen_words_idx[text_id]:\n                    # Add predictions if the word doesn't have a prediction from a previous chunk\n                    predictions[text_id].append(chunk_preds[idx])\n                    seen_words_idx[text_id].append(word_idx)\n    \n    val_loss = val_loss / steps\n    val_accuracy = val_accuracy / steps\n    \n    final_predictions = [predictions[k] for k in sorted(predictions.keys())]\n    return final_predictions, val_loss, val_accuracy\n\n\n# https://www.kaggle.com/zzy990106/pytorch-ner-infer\n# code has been modified from original\n# I moved the iteration over the batches to inference because  \n# samples from the same text might have be split into different batches\ndef get_predictions(df, dl, model):\n    \n    all_labels, val_loss, val_accuracy = inference(dl, model)\n    final_preds = []\n    \n    for i in range(len(df)):\n        idx = df.id.values[i]\n        pred = all_labels[i]\n        preds = []\n        j = 0\n        \n        while j < len(pred):\n            cls = pred[j]\n            if cls == 'O': pass\n            else: cls = cls.replace('B','I')\n            end = j + 1\n            while end < len(pred) and pred[end] == cls:\n                end += 1\n            if cls != 'O' and cls != '' and end - j > 7:\n                final_preds.append((idx, cls.replace('I-',''), \n                                    ' '.join(map(str, list(range(j, end))))))\n            j = end\n        \n    df_pred = pd.DataFrame(final_preds)\n    df_pred.columns = ['id','class','predictionstring']\n    return df_pred, val_loss, val_accuracy\n\n\ndef validate(model, df_all, df_val, dl_val, epoch, epoch_start):\n    \n    time_start = time.time()\n    \n    # Put model in eval model\n    model.eval()\n    \n    # Valid targets: needed because df_val has a subset of the columns\n    df_valid = df_all.loc[df_all['id'].isin(IDS[valid_idx])]\n\n    # OOF predictions\n    oof,  val_loss, val_accuracy = get_predictions(df_val, dl_val, model)\n\n    # Compute F1-score\n    f1s = []\n    classes = oof['class'].unique()\n    \n    epoch_prefix = f\"[Epoch {epoch+1:2d} / {config['epochs']:2d}]\"\n    #print(f\"{epoch_prefix} Validation F1 scores\")\n    \n    val_log = {}\n    for c in classes:\n        pred_df = oof.loc[oof['class']==c].copy()\n        gt_df = df_valid.loc[df_valid['discourse_type']==c].copy()\n        f1 = score_feedback_comp(pred_df, gt_df)\n        #print(f\"{epoch_prefix}   * {c:<10}: {f1:4f}\")\n        f1s.append(f1)\n        val_log[f'F1 {c}'] = f1\n    \n    elapsed = time.time() - time_start\n    epoch_end = time.time() - epoch_start\n    \n    #print(epoch_prefix)\n    print(f\"{epoch_prefix} Val. loss        : {val_loss:.4f}\")\n    print(f'{epoch_prefix} Val accuracy     : {val_accuracy:.4f}')\n    print(f'{epoch_prefix} Validation F1    : {np.mean(f1s):.4f}')\n    print(f\"{epoch_prefix} Validation time  : {elapsed/60:.2f} mins\")\n    print(f\"{epoch_prefix} Epoch time       : {epoch_end/60:.2f} mins\")\n    print(epoch_prefix)\n    \n    \n    val_log['Overall F1'] = np.mean(f1s)\n    val_log['Val Loss'] = val_loss\n    val_log['Val Accuracy'] = val_accuracy\n    val_log['Validation Time'] = elapsed\n    val_log['Epoch Time'] = epoch_end\n    wandb.log(val_log)\n    \n    \n# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\ndef train(model, optimizer, dl_train, epoch):\n    \n    time_start = time.time()\n    \n    # Set learning rate to the one in config for this epoch\n    for g in optimizer.param_groups: \n        g['lr'] = config['learning_rates'][epoch]\n    lr = optimizer.param_groups[0]['lr']\n    \n    \n    epoch_prefix = f\"[Epoch {epoch+1:2d} / {config['epochs']:2d}]\"\n    print(f\"{epoch_prefix} Starting epoch {epoch+1:2d} with LR = {lr}\")\n    \n    # Put model in training mode\n    model.train()\n    \n    # Accumulator variables\n    tr_loss, tr_accuracy = 0, 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    for idx, batch in enumerate(dl_train):\n        \n        ids = batch['input_ids'].to(config['device'], dtype = torch.long)\n        mask = batch['attention_mask'].to(config['device'], dtype = torch.long)\n        labels = batch['labels'].to(config['device'], dtype = torch.long)\n\n        loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels,\n                               return_dict=False)\n        tr_loss += loss.item()\n\n        nb_tr_steps += 1\n        nb_tr_examples += labels.size(0)\n        loss_step = tr_loss/nb_tr_steps\n        \n        if idx % 200 == 0:\n            print(f\"{epoch_prefix}     Steps: {idx:4d} --> Loss: {loss_step:.4f}\")\n        \n   \n        # compute training accuracy\n        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n        \n        # only compute accuracy at active labels\n        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n        \n        \n        labels = torch.masked_select(flattened_targets, active_accuracy)\n        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n        \n        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n        tr_accuracy += tmp_tr_accuracy\n        \n        wandb.log({'Train Loss (Step)': loss_step, 'Train Accuracy (Step)' : tr_accuracy / nb_tr_steps})\n        \n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(\n            parameters=model.parameters(), max_norm=config['max_grad_norm']\n        )\n        \n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n\n    epoch_loss = tr_loss / nb_tr_steps\n    tr_accuracy = tr_accuracy / nb_tr_steps\n    \n    torch.save(model.state_dict(), f'pytorch_model_e{epoch}.bin')\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    elapsed = time.time() - time_start\n    \n    print(epoch_prefix)\n    print(f\"{epoch_prefix} Training loss    : {epoch_loss:.4f}\")\n    print(f\"{epoch_prefix} Training accuracy: {tr_accuracy:.4f}\")\n    print(f\"{epoch_prefix} Training time    : {elapsed/60:.2f} mins\")\n    #print(epoch_prefix)\n    #print(f\"{epoch_prefix} Model saved to pytorch_model_e{epoch}.bin  \")\n    \n    wandb.log({'Train Loss (Epoch)': epoch_loss, 'Train Accuracy (Epoch)' : tr_accuracy, 'Train Time' : elapsed})\n    \n\ndef run():\n    time_start = time.time()\n    global LABELS_TO_IDS, IDS_TO_LABELS, IDS, valid_idx, train_idx\n    df_all = pd.read_csv('../input/feedback-prize-2021/train.csv')\n    df_texts = pd.read_csv(\"../input/feedback-prize-train-ner-csv/train_NER.csv\",\n                            converters={'entities':ast.literal_eval, 'text_split': ast.literal_eval})\n    # Create global dictionaries to use during training and inference\n\n    # https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615\n    output_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n              'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n\n    LABELS_TO_IDS = {v:k for k,v in enumerate(output_labels)}\n    IDS_TO_LABELS = {k:v for k,v in enumerate(output_labels)}\n\n\n    # CHOOSE VALIDATION INDEXES\n    IDS = df_all.id.unique()\n    print(f'There are {len(IDS)} train texts. We will split 90% 10% for validation.')\n\n    # TRAIN VALID SPLIT 90% 10%\n    np.random.seed(42)\n    train_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\n    valid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\n    np.random.seed(None)\n    \n    n_train = len(train_idx)\n    n_val   = len(valid_idx)\n    \n    n_train_new = int(config['train_data_size'] * n_train)\n    n_val_new = int(config['val_data_size'] * n_val)\n    \n    train_idx = train_idx[:n_train_new]\n    valid_idx = valid_idx[:n_val_new]\n    \n\n    # CREATE TRAIN SUBSET AND VALID SUBSET\n    df_train = df_texts.loc[df_texts['id'].isin(IDS[train_idx])].reset_index(drop=True)\n    df_val = df_texts.loc[df_texts['id'].isin(IDS[valid_idx])].reset_index(drop=True)\n\n    print(f\"FULL Dataset : {df_texts.shape}\")\n    print(f\"Data size - Train: {config['train_data_size']} Val: {config['val_data_size']}\")\n    \n    print(f\"TRAIN Dataset: {df_train.shape}\")\n    print(f\"TEST Dataset : {df_val.shape}\")\n    \n    download_model()\n    tokenizer = AutoTokenizer.from_pretrained('model')\n\n    # Tokenize both training and validation dataframes\n    tokenized_train = tokenize(df_train, tokenizer)\n    tokenized_val = tokenize(df_val, tokenizer)\n\n\n    # Create Datasets and DataLoaders for training and validation dat\n\n    ds_train = FeedbackPrizeDataset(tokenized_train)\n    dl_train = DataLoader(ds_train, batch_size=config['train_batch_size'], \n                          shuffle=True, num_workers=2, pin_memory=True)\n\n    ds_val = FeedbackPrizeDataset(tokenized_val)\n    dl_val = DataLoader(ds_val, batch_size=config['valid_batch_size'], \n                        shuffle=False, num_workers=2, pin_memory=True)\n\n    config_model = AutoConfig.from_pretrained('model/config.json') \n    model = AutoModelForTokenClassification.from_pretrained('model/pytorch_model.bin',config=config_model)\n    model.to(config['device']);\n\n    # Instantiate optimizer\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=config['learning_rates'][0])\n\n    # Loop\n    for epoch in range(config['epochs']):\n        epoch_start = time.time()\n        print()\n        train(model, optimizer, dl_train, epoch)\n        validate(model, df_all, df_val, dl_val, epoch, epoch_start)\n\n    elapsed = time.time() - time_start\n    print(f\"Final model saved as 'pytorch_model.bin' [{elapsed/60:.2f} mins]\")\n    \n    torch.save(model.state_dict(), 'pytorch_model.bin')\n    \n    \ndef start_wandb():\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    wandb.init(project=WANDB_PROJECT, entity=WANDB_ENTITY, name=RUN_NAME, config=config)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T18:01:08.481703Z","iopub.execute_input":"2022-02-03T18:01:08.48222Z","iopub.status.idle":"2022-02-03T18:01:08.588707Z","shell.execute_reply.started":"2022-02-03T18:01:08.482176Z","shell.execute_reply":"2022-02-03T18:01:08.587756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_wandb()\nrun()\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T18:01:08.589763Z","iopub.execute_input":"2022-02-03T18:01:08.59002Z","iopub.status.idle":"2022-02-03T18:02:37.986199Z","shell.execute_reply.started":"2022-02-03T18:01:08.589985Z","shell.execute_reply":"2022-02-03T18:02:37.985249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Please _DO_ upvote if you found this kernel useful or interesting! ðŸ¤—","metadata":{}}]}