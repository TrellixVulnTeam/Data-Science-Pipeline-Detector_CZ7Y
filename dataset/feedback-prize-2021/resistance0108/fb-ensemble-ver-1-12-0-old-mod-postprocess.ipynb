{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"【前回versionとのDiff】\n- [Update] LB0.715からfix_list関数の修正\n- OOFの計算上、現在のLBベストよりCV改善のためサブ","metadata":{}},{"cell_type":"code","source":"import gc\ngc.enable()\n\nimport sys\nsys.path.append(\"../input/tez-lib/\")\n\nimport os\nimport random\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import softmax\n\nimport tez\nimport torch\nimport torch.nn as nn\nfrom joblib import Parallel, delayed\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\nimport pickle\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-15T03:30:39.562936Z","iopub.execute_input":"2022-03-15T03:30:39.563539Z","iopub.status.idle":"2022-03-15T03:30:48.372367Z","shell.execute_reply.started":"2022-03-15T03:30:39.56336Z","shell.execute_reply":"2022-03-15T03:30:48.371438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class Config:\n    input_dir = '../input/feedback-prize-2021'\n    \n    model_longformer = '../input/longformerlarge4096/longformer-large-4096'\n    model_led = '../input/led-large'\n    model_deberta = '../input/deberta-large-mlm-1024'\n    model_deberta_x = '../input/deberta-xlarge/deberta-xlarge'\n    \n    max_len_test_longformer = 1600\n    max_len_test_led = 1024\n    \n    num_jobs = 4\n    seed = 1\n    \n    model_ckp_path = [\n        #  (kind, num_labels, model_path, model_name, weight)\n        # exp lf-large, lf-base, led-large, bb-large, ...\n        \n        # LB 0.710\n        ('deberta-large', None, 15, '../input/2022022709-deberta-large-boe-bin/model_0.bin', '022709-deberta-large-boe-bin',0.8),\n        #('deberta-large', None, 15, '../input/2022022709-deberta-large-boe-bin/model_1.bin', '022709-deberta-large-boe-bin',1.3),\n        #('deberta-large', None, 15, '../input/2022022709-deberta-large-boe-bin/model_2.bin', '022709-deberta-large-boe-bin',1.3),\n        #('deberta-large', None, 15, '../input/2022022709-deberta-large-boe-bin/model_3.bin', '022709-deberta-large-boe-bin',1.3),\n        #('deberta-large', None, 15, '../input/2022022709-deberta-large-boe-bin/model_4.bin', '022709-deberta-large-boe-bin',1.3),\n        \n        # LB 0.702\n        #('deberta-large', None, 22, '../input/fb-exp045-deberta/model_0.bin', 'yyama_exp045', 1.1),\n        ('deberta-large', None, 22, '../input/fb-exp045-deberta/model_1.bin', 'yyama_exp045',1.1),\n        ('deberta-large', None, 22, '../input/fb-exp045-deberta/model_2.bin', 'yyama_exp045',1.1),\n        ('deberta-large', None, 22, '../input/fb-exp045-deberta/model_3.bin', 'yyama_exp045',1.1),\n        #('deberta-large', None, 22,  '../input/fb-exp045-deberta/model_4.bin', 'yyama_exp045',1.1),\n        \n        # LB 0.709\n        #('deberta-large', 'LSTM_2head', 22, '../input/feedback-101/fold-0.pt', 'makabe_feedback-101_LSTM_2head', 1.95),\n        #('deberta-large', 'LSTM_2head', 22, '../input/feedback-101/fold-1.pt','makabe_feedback-101_LSTM_2head', 1.95),\n        ('deberta-large', 'LSTM_2head', 22, '../input/feedback-101/fold-2.pt','makabe_feedback-101_LSTM_2head', 2.6),\n        ('deberta-large', 'LSTM_2head', 22, '../input/feedback-101/fold-3.pt','makabe_feedback-101_LSTM_2head', 2.6),\n        ('deberta-large', 'LSTM_2head', 22, '../input/feedback-101/fold-4.pt','makabe_feedback-101_LSTM_2head', 2.6),\n        \n        # LB 0.709\n        ('deberta-large', 'LSTM', 22, '../input/feedback-098/fold-0.pt', 'makabe_feedback-098_LSTM',2.7),\n        #('deberta-large', 'LSTM', 22, '../input/feedback-098/fold-1.pt', 'makabe_feedback-098_LSTM',2.025),\n        #('deberta-large', 'LSTM', 22, '../input/feedback-098/fold-2.pt', 'makabe_feedback-098_LSTM',2.025),\n        ('deberta-large', 'LSTM', 22, '../input/feedback-098/fold-3.pt', 'makabe_feedback-098_LSTM',2.7),\n        ('deberta-large', 'LSTM', 22, '../input/feedback-098/fold-4.pt', 'makabe_feedback-098_LSTM',2.7),\n        \n        # LB 0.708\n        ('deberta-large', None, 22, '../input/feedback-096/fold-0.pt', 'makabe_feedback-096', 2.1),\n        ('deberta-large', None, 22, '../input/feedback-096/fold-1.pt', 'makabe_feedback-096',2.1),\n        #('deberta-large', None, 22, '../input/feedback-096/fold-2.pt', 'makabe_feedback-096',2.1),\n        #('deberta-large', None, 22, '../input/feedback-096/fold-3.pt', 'makabe_feedback-096',2.1),\n        ('deberta-large', None, 22, '../input/feedback-096/fold-4.pt', 'makabe_feedback-096',2.1),\n        \n        # LB 0.707\n        ('deberta-large', None, 24, '../input/fb-exp058-wo-mlm/model_0.bin', 'yyama_exp058-wo-mlm',1),\n        ('deberta-large', None, 24, '../input/fb-exp058-wo-mlm/model_1.bin', 'yyama_exp058-wo-mlm',1),\n        ('deberta-large', None, 24, '../input/fb-exp058-wo-mlm/model_2.bin', 'yyama_exp058-wo-mlm',1),\n        #('deberta-large', None, 24, '../input/fb-exp058-wo-mlm/model_3.bin', 'yyama_exp058-wo-mlm',0.75),\n        #('deberta-large', None, 24, '../input/fb-exp058-wo-mlm/model_4.bin', 'yyama_exp058-wo-mlm',0.75),\n        \n        # LB 707\n        #('deberta-large', None, 15, '../input/2022030209-deberta-large-mnli-boe-bin/model_0.bin', 'makotu_030209-deberta-large-mnli-boe-bin', 1.2),\n        #('deberta-large', None, 15, '../input/2022030209-deberta-large-mnli-boe-bin/model_1.bin', 'makotu_030209-deberta-large-mnli-boe-bin',1.2),\n        #('deberta-large', None, 15, '../input/2022030209-deberta-large-mnli-boe-bin/model_2.bin', 'makotu_030209-deberta-large-mnli-boe-bin',1.2),\n        #('deberta-large', None, 15, '../input/2022030209-deberta-large-mnli-boe-bin/model_3.bin', 'makotu_030209-deberta-large-mnli-boe-bin',1.0),\n        #('deberta-large', None, 15, '../input/2022030209-deberta-large-mnli-boe-bin/model_4.bin', 'makotu_030209-deberta-large-mnli-boe-bin',1.0),\n        \n        # xlarge\n        ('deberta-xlarge', None, 24, '../input/fb-exp062-xlarge/model_2.bin', 'yyama_fb_exp06x_xlarge_fold2-4', 1.4),\n        ('deberta-xlarge', None, 24, '../input/fb-exp062-xlarge/model_3.bin', 'yyama_fb_exp06x_xlarge_fold2-4', 1.4),\n        ('deberta-xlarge', None, 24, '../input/fb-exp062-xlarge/model_4.bin', 'yyama_fb_exp06x_xlarge_fold2-4', 1.4),\n        \n        \n        # LB 0.697\n        #('lf-large', None, 22, '../input/2022021410-lf-bie-bin/model_0.bin', 0.5),\n        #('lf-large', None, 22, '../input/2022021410-lf-bie-bin/model_1.bin', 0.5),\n        #('lf-large', None, 22, '../input/2022021410-lf-bie-bin/model_2.bin', 0.5),\n\n        # LB 0.695\n        ('led-large', None, 24, '../input/fb-exp037-led/model_0.bin', 'yyama_fb-exp037-led', 1.6),\n        ('led-large', None, 24, '../input/fb-exp037-led/model_1.bin','yyama_fb-exp037-led',  1.6),\n        #('led-large', None, 24, '../input/fb-exp037-led/model_2.bin','yyama_fb-exp037-led',  1.6),\n        #('led-large', None, 24, '../input/fb-exp037-led/model_3.bin','yyama_fb-exp037-led',  1.6),\n        ('led-large', None, 24, '../input/fb-exp037-led/model_4.bin','yyama_fb-exp037-led',  1.6)\n    ]\n\n    proba_thresh = {\n        \"Lead\": 0.6, # 0.7\n        \"Position\": 0.4, # 0.55\n        \"Evidence\": 0.65,\n        \"Claim\": 0.55,\n        \"Concluding Statement\": 0.6, # 0.7\n        \"Counterclaim\": 0.5,\n        \"Rebuttal\": 0.55,\n    }\n    min_token_thresh = {\n        \"Lead\": 5, # 9\n        \"Position\": 4, # 5\n        \"Evidence\": 14,\n        \"Claim\": 2,\n        \"Concluding Statement\": 7, # 11\n        \"Counterclaim\": 6,\n        \"Rebuttal\": 4,\n    }\n    link = {\n        'Evidence': 40,\n        'Counterclaim': 200,\n        'Rebuttal': 200,\n    }\n\n\ncfg = Config()    \ntarget_id_map = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 2,\n    \"I-Position\": 3,\n    \"B-Evidence\": 4,\n    \"I-Evidence\": 5,\n    \"B-Claim\": 6,\n    \"I-Claim\": 7,\n    \"B-Concluding Statement\": 8,\n    \"I-Concluding Statement\": 9,\n    \"B-Counterclaim\": 10,\n    \"I-Counterclaim\": 11,\n    \"B-Rebuttal\": 12,\n    \"I-Rebuttal\": 13,\n    \"O\": 14,\n    \"PAD\": -100,\n}\nid_target_map = {v: k for k, v in target_id_map.items()}\n","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:30:48.37497Z","iopub.execute_input":"2022-03-15T03:30:48.375832Z","iopub.status.idle":"2022-03-15T03:30:48.395944Z","shell.execute_reply.started":"2022-03-15T03:30:48.375787Z","shell.execute_reply":"2022-03-15T03:30:48.394988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# short \n#cfg.model_ckp_path = cfg.model_ckp_path[:10]","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:30:48.40109Z","iopub.execute_input":"2022-03-15T03:30:48.403419Z","iopub.status.idle":"2022-03-15T03:30:48.420363Z","shell.execute_reply.started":"2022-03-15T03:30:48.403372Z","shell.execute_reply":"2022-03-15T03:30:48.41936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modules","metadata":{}},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def get_test_text(ids):\n    #with open(f'{cfg.input_dir}/train/{ids}.txt', 'r') as f:\n    with open(f'{cfg.input_dir}/test/{ids}.txt', 'r') as f:\n        text = f.read()\n    return text\n\n\ndef seed_everything(seed: int) -> None:\n    \"\"\"\n    seedの固定\n    \"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \n        \nseed_everything(cfg.seed)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-15T03:30:48.423591Z","iopub.execute_input":"2022-03-15T03:30:48.424128Z","iopub.status.idle":"2022-03-15T03:30:48.489421Z","shell.execute_reply.started":"2022-03-15T03:30:48.424083Z","shell.execute_reply":"2022-03-15T03:30:48.48852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre Process","metadata":{}},{"cell_type":"code","source":"def _prepare_test_data_helper(tokenizer, ids):\n    test_samples = []\n    for idx in ids:\n        text = get_test_text(idx)\n\n        encoded_text = tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n            max_length=1600,\n            truncation=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        offset_mapping = encoded_text[\"offset_mapping\"]\n\n        sample = {\n            \"id\": idx,\n            \"input_ids\": input_ids,\n            \"text\": text,\n            \"offset_mapping\": offset_mapping,\n        }\n\n        test_samples.append(sample)\n    return test_samples\n\n\ndef prepare_test_data(df, tokenizer, num_jobs):\n    test_samples = []\n    ids = df[\"id\"].unique()\n    ids_splits = np.array_split(ids, 4)\n\n    results = Parallel(n_jobs=num_jobs, backend=\"multiprocessing\")(\n        delayed(_prepare_test_data_helper)(tokenizer, idx) for idx in ids_splits\n    )\n    for result in results:\n        test_samples.extend(result)\n\n    return test_samples","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-15T03:30:48.492026Z","iopub.execute_input":"2022-03-15T03:30:48.492661Z","iopub.status.idle":"2022-03-15T03:30:48.503295Z","shell.execute_reply.started":"2022-03-15T03:30:48.492618Z","shell.execute_reply":"2022-03-15T03:30:48.502127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class FeedbackTestDataset:\n    def __init__(self, samples, max_len, tokenizer):\n        self.samples = samples\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.length = len(samples)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        input_ids = self.samples[idx][\"input_ids\"]\n        input_ids = [self.tokenizer.cls_token_id] + input_ids\n\n        if len(input_ids) > self.max_len - 1:\n            input_ids = input_ids[: self.max_len - 1]\n\n        # add end token id to the input_ids\n        input_ids = input_ids + [self.tokenizer.sep_token_id]\n        attention_mask = [1] * len(input_ids)\n        return {\n            \"ids\": input_ids,\n            \"mask\": attention_mask,\n        }\n    \n    \nclass Collate:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"ids\"]])\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n        else:\n            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n\n        # convert to tensors\n        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n\n        return output","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-15T03:30:48.505299Z","iopub.execute_input":"2022-03-15T03:30:48.505667Z","iopub.status.idle":"2022-03-15T03:30:48.525309Z","shell.execute_reply.started":"2022-03-15T03:30:48.505623Z","shell.execute_reply":"2022-03-15T03:30:48.524295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class FeedbackModel(tez.Model):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n        self.config = AutoConfig.from_pretrained(model_name)\n        self.config.update(\n            {\n                'output_hidden_states': True,\n                'add_pooling_layer': False,\n                'num_labels': self.num_labels,\n                'attention_probs_dropout_prob':  0.1,  # for longformer\n                'hidden_dropout_prob': 0.1,            # for longformer\n                'layer_norm_eps': 1e-7,                # for longformer\n                'activation_dropout': 0.0,             # for LED\n                'attention_dropout': 0.0,              # for LED\n                'classif_dropout': 0.0,                # for LED\n                'classifier_dropout': 0.0,             # for LED\n                'decoder_layerdrop': 0.0,              # for LED\n                'encoder_layerdrop': 0.0,              # for LED \n            }\n        )\n        self.transformer = AutoModel.from_config(self.config)\n        self.output = nn.Linear(self.config.hidden_size, self.num_labels)\n\n    def forward(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        logits = self.output(sequence_output)\n        logits = torch.softmax(logits, dim=-1)\n        return logits, 0, {}\n\n\nclass FeedbackModelWithLSTM(tez.Model):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n        self.config = AutoConfig.from_pretrained(model_name)\n        self.config.update(\n            {\n                'output_hidden_states': True,\n                'add_pooling_layer': False,\n                'num_labels': self.num_labels,\n                'attention_probs_dropout_prob':  0.1,  # for longformer\n                'hidden_dropout_prob': 0.1,            # for longformer\n                'layer_norm_eps': 1e-7,                # for longformer\n                'activation_dropout': 0.0,             # for LED\n                'attention_dropout': 0.0,              # for LED\n                'classif_dropout': 0.0,                # for LED\n                'classifier_dropout': 0.0,             # for LED\n                'decoder_layerdrop': 0.0,              # for LED\n                'encoder_layerdrop': 0.0,              # for LED \n            }\n        )\n        self.transformer = AutoModel.from_config(self.config)\n        self.lstm = nn.LSTM(\n            self.config.hidden_size, self.config.hidden_size, batch_first=True)\n        self.output = nn.Linear(self.config.hidden_size, self.num_labels)\n\n    def forward(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        sequence_output, _ = self.lstm(sequence_output, None)\n        logits = self.output(sequence_output)\n        logits = torch.softmax(logits, dim=-1)\n        return logits, 0, {}\n    \n    \nclass FeedbackModelWithLSTMTwoHead(tez.Model):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n        self.config = AutoConfig.from_pretrained(model_name)\n        self.config.update(\n            {\n                'output_hidden_states': True,\n                'add_pooling_layer': False,\n                'num_labels': self.num_labels,\n                'attention_probs_dropout_prob':  0.1,  # for longformer\n                'hidden_dropout_prob': 0.1,            # for longformer\n                'layer_norm_eps': 1e-7,                # for longformer\n                'activation_dropout': 0.0,             # for LED\n                'attention_dropout': 0.0,              # for LED\n                'classif_dropout': 0.0,                # for LED\n                'classifier_dropout': 0.0,             # for LED\n                'decoder_layerdrop': 0.0,              # for LED\n                'encoder_layerdrop': 0.0,              # for LED \n            }\n        )\n        self.transformer = AutoModel.from_config(self.config)\n        self.lstm = nn.LSTM(\n            self.config.hidden_size, self.config.hidden_size, batch_first=True)\n        self.output = nn.Linear(self.config.hidden_size, self.num_labels)\n        self.output_2 = nn.Linear(self.config.hidden_size, self.num_labels - 9)\n\n    def forward(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        sequence_output, _ = self.lstm(sequence_output, None)\n        logits = self.output(sequence_output)\n        logits = torch.softmax(logits, dim=-1)\n        return logits, 0, {}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-15T03:30:48.527392Z","iopub.execute_input":"2022-03-15T03:30:48.52772Z","iopub.status.idle":"2022-03-15T03:30:48.556335Z","shell.execute_reply.started":"2022-03-15T03:30:48.527677Z","shell.execute_reply":"2022-03-15T03:30:48.555214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"def inference(df, cfg, bs, mode):\n    #raw_preds = []\n    prev_kind = None\n    prev_head = None\n    prev_n_labels = None\n    \n    long_model_w = sum([w for k, _, _, _, _, w in cfg.model_ckp_path if (k == 'lf-large') | (k == 'deberta-large')])\n    short_model_w = sum([w for k, _, _, _, _, w in cfg.model_ckp_path if k == 'led-large'])\n    total_w = long_model_w + short_model_w\n    \n    models = list(set([a[4] for a in cfg.model_ckp_path]))\n    \n    for model_type in models:\n        models_num = len([a[4] for a in cfg.model_ckp_path if a[4] == model_type])\n        \n        pred_all = np.zeros([df['id'].nunique(), cfg.max_len_test_longformer, 15])\n        for i, (kind, head, n_labels, model_path, model_name, w) in enumerate(cfg.model_ckp_path):\n            if model_name == model_type:\n                raw_preds = []\n                if kind == 'lf-large':\n                    model_name = cfg.model_longformer\n                    max_len = cfg.max_len_test_longformer\n                elif kind == 'led-large':\n                    model_name = cfg.model_led\n                    max_len = cfg.max_len_test_led\n                elif kind == 'deberta-large':\n                    model_name = cfg.model_deberta\n                    max_len = cfg.max_len_test_longformer\n                elif kind == 'deberta-xlarge':\n                    model_name = cfg.model_deberta_x\n                    max_len = cfg.max_len_test_longformer\n\n                # 前回とモデルの構造が異なっている場合のみ初期化\n                if kind != prev_kind or head != prev_head or n_labels != prev_n_labels:\n                    print(f'use {model_name} tokenizer')\n                    tokenizer = AutoTokenizer.from_pretrained(model_name)\n                    samples = prepare_test_data(df, tokenizer, cfg.num_jobs)\n                    dataset = FeedbackTestDataset(samples, max_len, tokenizer)\n\n                    if head is None:\n                        model = FeedbackModel(model_name=model_name, num_labels=n_labels)\n                    elif head == 'LSTM':\n                        model = FeedbackModelWithLSTM(model_name=model_name, num_labels=n_labels)\n                    elif head == 'LSTM_2head':\n                        model = FeedbackModelWithLSTMTwoHead(\n                            model_name=model_name, num_labels=n_labels\n                        )\n\n                prev_kind, prev_head, prev_n_labels = kind, head, n_labels\n\n                model.eval()\n                model.load(model_path, weights_only=True)\n\n                collate = Collate(tokenizer=tokenizer)\n                \n                \n                if 'led' in model_name:\n                    bs2 = min(bs, 20)\n                else:\n                    bs2 = bs\n                \n                preds_iter = model.predict(dataset, batch_size=bs2, n_jobs=-1, collate_fn=collate)\n\n                current_idx = 0\n                for preds in preds_iter:\n                    preds = preds.astype(np.float16)\n                    preds = np.pad(preds, [(0, 0), (0, cfg.max_len_test_longformer - preds.shape[1]), (0, 0)], 'constant')\n\n                    if n_labels == 22:\n                        preds = reshape_preds_22_to_15(preds)\n                    elif n_labels == 24:\n                        preds = reshape_preds_24_to_15(preds)\n\n                        \n                    \n                    # デバッグ用\n                    #long_model_w = 1\n                    \n                    \n                    #preds *= w / total_w\n                    # 1024(max_len_test_led)を超える分に関しては定数倍\n                    preds[:, cfg.max_len_test_led:, :] *= total_w / long_model_w\n\n                    #if i == 0:\n                    raw_preds.append(preds)\n                    #else:\n                    #    raw_preds[current_idx] += preds\n                    #    current_idx += 1\n                torch.cuda.empty_cache()\n                gc.collect()\n                pred_all += np.vstack(raw_preds) / models_num\n            \n        with open(f'pred_{model_type}_{mode}.pkl', 'wb') as f:\n            pickle.dump(pred_all , f)\n\n        #preds_class, preds_prob = [], []\n        #for rp in raw_preds:\n        #    c_arr = np.argmax(rp, axis=2)\n        #    p_arr = np.max(rp, axis=2)\n        #    for c, p in zip(c_arr, p_arr):\n        #        c = c.tolist()\n        #        p = p.tolist()\n        #        preds_class.append(c)\n        #        preds_prob.append(p)\n\n    #for i in range(len(samples)):\n        # 先頭0はspecial tokenなので除外\n    #    text_class = [id_target_map[c] for c in preds_class[i][1:]]\n    #    text_prob = preds_prob[i][1:]\n    #    samples[i]['pred_class'] = text_class\n    #    samples[i]['pred_prob'] = text_prob\n\n    #return samples\n\n\ndef reshape_preds_24_to_15(preds):\n    new_preds = np.zeros((preds.shape[0], preds.shape[1], 15))\n    new_preds[:, :, 0] = preds[:, :, 0]\n    new_preds[:, :, 1] += preds[:, :, 1] + preds[:, :, 2]\n    new_preds[:, :, 2] += preds[:, :, 3]\n    new_preds[:, :, 3] += preds[:, :, 4] + preds[:, :, 5]\n    new_preds[:, :, 4] += preds[:, :, 6]\n    new_preds[:, :, 5] += preds[:, :, 7] + preds[:, :, 8]\n    new_preds[:, :, 6] += preds[:, :, 9]\n    new_preds[:, :, 7] += preds[:, :, 10] + preds[:, :, 11]\n    new_preds[:, :, 8] += preds[:, :, 12]\n    new_preds[:, :, 9] += preds[:, :, 13] + preds[:, :, 14]\n    new_preds[:, :, 10] += preds[:, :, 15]\n    new_preds[:, :, 11] += preds[:, :, 16] + preds[:, :, 17]\n    new_preds[:, :, 12] += preds[:, :, 18]\n    new_preds[:, :, 13] += preds[:, :, 19] + preds[:, :, 20]\n    new_preds[:, :, 14] += preds[:, :, 21] + preds[:, :, 22] + preds[:, :, 23]\n    return new_preds\n\ndef reshape_preds_22_to_15(preds):\n    new_preds = np.zeros((preds.shape[0], preds.shape[1], 15))\n    new_preds[:, :, 0] = preds[:, :, 0]\n    new_preds[:, :, 1] += preds[:, :, 1] + preds[:, :, 2]\n    new_preds[:, :, 2] += preds[:, :, 3]\n    new_preds[:, :, 3] += preds[:, :, 4] + preds[:, :, 5]\n    new_preds[:, :, 4] += preds[:, :, 6]\n    new_preds[:, :, 5] += preds[:, :, 7] + preds[:, :, 8]\n    new_preds[:, :, 6] += preds[:, :, 9]\n    new_preds[:, :, 7] += preds[:, :, 10] + preds[:, :, 11]\n    new_preds[:, :, 8] += preds[:, :, 12]\n    new_preds[:, :, 9] += preds[:, :, 13] + preds[:, :, 14]\n    new_preds[:, :, 10] += preds[:, :, 15]\n    new_preds[:, :, 11] += preds[:, :, 16] + preds[:, :, 17]\n    new_preds[:, :, 12] += preds[:, :, 18]\n    new_preds[:, :, 13] += preds[:, :, 19] + preds[:, :, 20]\n    new_preds[:, :, 14] += preds[:, :, 21]\n    return new_preds","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-15T03:30:48.558252Z","iopub.execute_input":"2022-03-15T03:30:48.559048Z","iopub.status.idle":"2022-03-15T03:30:48.600546Z","shell.execute_reply.started":"2022-03-15T03:30:48.559003Z","shell.execute_reply":"2022-03-15T03:30:48.599467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Post Process","metadata":{}},{"cell_type":"code","source":"def post_process(samples):\n    sub = create_submission(samples)\n    sub = thresh_prob(sub, cfg)\n    sub = thresh_min_token(sub, cfg)\n    sub = get_max_prob(sub)\n    sub = link_class(sub, 'Evidence', cfg.link['Evidence'])\n    sub = link_class(sub, 'Counterclaim', cfg.link['Counterclaim'])\n    sub = link_class(sub, 'Rebuttal', cfg.link['Rebuttal'])\n    sub = sub.reset_index(drop=True)\n    sub = sub[['id', 'class', 'predictionstring']]\n    return sub\n\ndef post_process(samples):\n    sub = create_submission(samples)\n    sub = thresh_prob(sub, cfg)\n    sub = thresh_min_token(sub, cfg)\n    sub = get_max_prob(sub)\n    sub = link_class(sub, 'Evidence', cfg.link['Evidence'])\n    sub = link_class(sub, 'Counterclaim', cfg.link['Counterclaim'])\n    sub = link_class(sub, 'Rebuttal', cfg.link['Rebuttal'])\n    sub = sub.reset_index(drop=True)\n    sub = sub[['id', 'class', 'predictionstring']]\n    return sub\n\n\ndef create_submission(samples):\n    sub = []\n    for _, sample in enumerate(samples):\n        pred_class = sample['pred_class']\n        offset_mapping = sample['offset_mapping']\n        sample_id = sample['id']\n        sample_text = sample['text']\n        pred_prob = sample['pred_prob']\n        \n        second_class = sample['second_class']\n        second_prob = sample['second_prob']\n        \n        third_class = sample['third_class']\n        third_prob = sample['third_prob']\n        \n\n        pred_class = fix_list(pred_class)\n\n        sample_preds = []\n        # テキストが4096より長い場合\n        if len(pred_class) < len(offset_mapping)-1:\n            pred_class += ['O'] * (len(offset_mapping) - len(pred_class))\n            second_class += ['O'] * (len(offset_mapping) - len(second_class))\n            third_class += ['O'] * (len(offset_mapping) - len(third_class))\n            \n            pred_prob += [0] * (len(offset_mapping) - len(pred_prob))\n            second_prob += [0] * (len(offset_mapping) - len(second_prob))\n            third_prob += [0] * (len(offset_mapping) - len(third_prob))\n\n        idx = 0\n        phrase_preds = []\n        while idx < len(offset_mapping)-1:\n            start, _ = offset_mapping[idx]\n            if pred_class[idx] != 'O':\n                label = pred_class[idx][2:]\n            else:\n                label = 'O'\n                \n            if second_class[idx] != 'O':\n                second_label = second_class[idx][2:]\n            else:\n                second_label = 'O'\n                \n            if pred_class[idx] != 'O':\n                third_label = third_class[idx][2:]\n            else:\n                third_label = 'O'\n            \n            \n            phrase_probs = []\n            phrase_probs.append(pred_prob[idx])\n            \n            phrase_second_probs = []\n            phrase_second_probs.append(second_prob[idx])\n            \n            phrase_third_probs = []\n            phrase_third_probs.append(third_prob[idx])\n            \n            idx += 1\n            \n            while idx < len(offset_mapping)-1:\n                if label != 'O':\n                    matching_label = f'I-{label}'\n                else:\n                    matching_label = 'O'\n                if pred_class[idx] == matching_label:\n                    _, end = offset_mapping[idx]\n                    phrase_probs.append(pred_prob[idx])\n                    phrase_second_probs.append(second_prob[idx])\n                    phrase_third_probs.append(third_prob[idx])\n                    \n                    idx += 1\n                else:\n                    break\n            if 'end' in locals():\n                phrase = sample_text[start:end]\n                phrase_preds.append((phrase, start, end, label, second_label, third_label, phrase_probs, phrase_second_probs, phrase_third_probs))\n\n        temp_df = []\n        for phrase_idx, (phrase, start, end, label, second_label, third_label, phrase_probs, phrase_second_probs, phrase_third_probs) in enumerate(phrase_preds):\n            word_start = len(sample_text[:start].split())\n            word_end = word_start + len(sample_text[start:end].split())\n            word_end = min(word_end, len(sample_text.split()))\n            ps = \" \".join([str(x) for x in range(word_start, word_end)])\n            if label != 'O':\n                phrase_probs_mean = sum(phrase_probs) / len(phrase_probs)\n                phrase_second_probs_mean = sum(phrase_second_probs) / len(phrase_probs)\n                phrase_third_probs_mean = sum(phrase_third_probs) / len(phrase_probs)\n                \n                temp_df.append((sample_id, label, second_label, third_label, ps, phrase_probs_mean, phrase_second_probs_mean, phrase_third_probs_mean))\n        temp_df = pd.DataFrame(temp_df, columns=['id', 'class', 'second_class', 'third_class', 'predictionstring', 'prob', 'second_prob', 'third_prob'])\n        sub.append(temp_df)\n    \n    sub = pd.concat(sub).reset_index(drop=True)\n    sub['len'] = sub['predictionstring'].apply(lambda x: len(x.split()))\n    sub = sub[sub['len'] > 0]\n    return sub\n\ndef fix_list(pred_list):\n\n    class_list = [\"I-Lead\", \"I-Position\", \"I-Evidence\", \"I-Claim\", \n                  \"I-Concluding Statement\", \"I-Counterclaim\", \"I-Rebuttal\", \"O\"]\n    \n    fix_threholds = {\n        \"I-Lead\":2, \n        \"I-Concluding Statement\":2, \n        \"I-Evidence\":1,\n        \"I-Position\":2,\n        \"I-Claim\":1,\n        \"I-Counterclaim\":5, \n        \"I-Rebuttal\":7,\n        \"O\":1\n    }\n    \n    for class_ in class_list:\n\n        flg_index = []\n        out_class = [col for col in class_list if col not in class_]\n        counter = 0\n\n        for token_id, token in enumerate(pred_list):\n            \n            # 連続2回以上続いた後の別classにはflgを立てる\n            if counter > 2 and token in out_class:\n                flg_index.append(token_id)\n                counter = 0\n\n            if token == class_:\n                counter += 1\n            else:\n                counter = 0\n                \n        for ind in flg_index:\n            if ind + fix_threholds[class_] + 1 < len(pred_list):\n                counter_2 = fix_threholds[class_]\n                while counter_2 != 0:\n                    if pred_list[ind + counter_2] == class_ and pred_list[ind + counter_2 + 1] == class_:\n                        for i in range(counter_2):\n                            pred_list[ind + i] = class_\n                        counter_2 = 0\n                    else:\n                        counter_2 -= 1\n                        \n    return pred_list\n\ndef fix_list_(pred_list):\n    class_list = [\"I-Lead\", \"I-Position\", \"I-Evidence\", \"I-Claim\", \n                  \"I-Concluding Statement\", \"I-Counterclaim\", \"I-Rebuttal\"]\n\n    for class_ in class_list:\n        flg_index = []\n        out_class = set(class_list) - {class_}\n        counter = 0\n\n        for token_id, token in enumerate(pred_list):\n            if counter > 2 and token in out_class:\n                flg_index.append(token_id)\n                counter = 0\n\n            if token == class_:\n                counter += 1\n            else:\n                counter = 0\n\n        for ind in flg_index:\n            if ind + 2 < len(pred_list):\n                if pred_list[ind + 1] == class_ and pred_list[ind + 2] == class_:\n                    pred_list[ind] = class_\n\n    return pred_list\n\n\ndef jn(pst, start, end):\n    return \" \".join([str(x) for x in pst[start:end] if x != -1])\n\n\ndef link_class(oof, discourse_type, thresh2):\n    id_list = oof['id'].unique().tolist()\n    if not len(oof):\n        return oof\n    thresh = 1\n    idu = oof['id'].unique()\n    eoof = oof[oof['class'] == f\"{discourse_type}\"]\n    neoof = oof[oof['class'] != f\"{discourse_type}\"]\n    eoof.index = eoof[['id', 'class']]\n    \n    retval = []\n    for idv in idu:\n        q = eoof[eoof['id'] == idv]\n        if not len(q):\n            continue\n        pst = []\n        for r in q.itertuples():\n            pst = [*pst, -1,  *[int(x) for x in r.predictionstring.split()]]\n        start, end = 1, 1\n        for i in range(2, len(pst)):\n            cur = pst[i]\n            end = i\n            if  (\n                (cur == -1) and\n                ((pst[i + 1] > pst[end - 1] + thresh) or (pst[i + 1] - pst[start] > thresh2))\n            ):\n                retval.append((idv, discourse_type, jn(pst, start, end)))\n                start = i + 1\n        v = (idv, discourse_type, jn(pst, start, end + 1))\n        retval.append(v)\n\n    roof = pd.DataFrame(retval, columns=['id', 'class', 'predictionstring'])\n    roof = roof.merge(neoof, how='outer')\n    \n    dfs = []\n    for doc_id in id_list:\n        r_df_tmp = roof.query(f'id == \"{doc_id}\"')\n        r_df_tmp['start'] = r_df_tmp['predictionstring'].apply(lambda x: int(x.split(' ')[0]))\n        r_df_tmp = r_df_tmp.sort_values('start').drop('start', axis=1)\n        dfs.append(r_df_tmp)\n    return pd.concat(dfs, axis=0)\n\n\ndef thresh_prob(df, cfg):\n    df_other = df[(df['class'] != 'Claim') | (df['len'] != 2)]\n    df_target = df[(df['class'] == 'Claim') & (df['len'] == 2)]\n    df_target['prob'] -= 0.1\n    df = pd.concat([df_other, df_target])\n    df = df.sort_index()\n    for k, v in cfg.proba_thresh.items():\n        idx = df.loc[df['class'] == k].query(f'prob < {v}').index\n        df = df.drop(idx)\n    return df\n\n# add\ndef thresh_second_prob(df, cfg):\n    df = df.sort_index()\n    for k, v in cfg.second_proba_thresh.items():\n        idx = df.loc[df['class'] == k].query(f'second_prob > {v}').index\n        df = df.drop(idx)\n    return df\n\n# add\ndef thresh_third_prob(df, cfg):\n    df = df.sort_index()\n    for k, v in cfg.second_proba_thresh.items():\n        idx = df.loc[df['class'] == k].query(f'third_prob > {v}').index\n        df = df.drop(idx)\n    return df\n\n\ndef thresh_min_token(df, cfg):\n    df['len'] = df['predictionstring'].apply(lambda x: len(x.split(' ')))\n    for k, v in cfg.min_token_thresh.items():\n        idx = df.loc[df['class'] == k].query(f'len < {v}').index\n        df = df.drop(idx)\n    return df\n\n\ndef get_max_prob(sub):\n    sub['prob'] = sub['prob'].astype(float)\n    id_list = sub['id'].unique().tolist()\n    unique_class = ['Lead', 'Position', 'Concluding Statement']\n    sub_in_unique = sub[sub['class'].isin(unique_class) == True]\n    sub_not_in_unique = sub[sub['class'].isin(unique_class) == False]\n    sub_in_unique = sub_in_unique.loc[sub_in_unique.groupby(['id', 'class'])['prob'].idxmax(), :]\n    sub = pd.concat([sub_in_unique, sub_not_in_unique])\n    return sub\n\n\ndef post_process_sub(sub):\n    sub = thresh_prob(sub, cfg)\n    sub = thresh_min_token(sub, cfg)\n    sub = get_max_prob(sub)\n    sub = link_class(sub, 'Evidence', cfg.link['Evidence'])\n    sub = link_class(sub, 'Counterclaim', cfg.link['Counterclaim'])\n    sub = link_class(sub, 'Rebuttal', cfg.link['Rebuttal'])\n    sub = sub.reset_index(drop=True)\n    sub = sub[['id', 'class', 'predictionstring', 'prob', 'len']]\n    return sub\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-15T03:30:48.603368Z","iopub.execute_input":"2022-03-15T03:30:48.603748Z","iopub.status.idle":"2022-03-15T03:30:48.679149Z","shell.execute_reply.started":"2022-03-15T03:30:48.603706Z","shell.execute_reply":"2022-03-15T03:30:48.678118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# main","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(os.path.join(\"../input/feedback-prize-2021/\", \"sample_submission.csv\"))\n\nif len(test_df) < 100:\n    test_df['class'] = 'Claim'\n    test_df.to_csv('submission.csv', index=False)\n\n#test_df = pd.read_csv(os.path.join(\"../input/feedback-prize-2021/\", \"train.csv\"))\n#test_df = test_df[test_df['id'].isin(test_df['id'].unique()[:1000])].drop_duplicates(subset=['id'])\n#test_df = test_df.drop_duplicates(subset=['id'])\n\ntest_ids = test_df['id'].unique()\ntest_ids = test_ids[~(test_ids=='AD005493F9BF')]\n# for debug\n# test_ids = test_ids[:100]\ntest_df = test_df[test_df['id'].isin(test_ids)]\n\ntokenizer = AutoTokenizer.from_pretrained('../input/longformerlarge4096/longformer-large-4096')\ntest_samples = prepare_test_data(test_df, tokenizer, cfg.num_jobs)\n\n# get test token_len\ntoken_len_list = [len(test_samples[i]['input_ids']) for i in range(len(test_samples))]\ntest_df['token_len'] = token_len_list\n\ndel test_samples\n\n# sort test data and rebuild test_df\ntest_df = test_df.sort_values('token_len')\ntest_df.reset_index(drop=True, inplace = True)\n\n#test_df_A = test_df.query('token_len < 600')\n#test_df_B = test_df.query('token_len >= 600')\n\n#test_ids_A = test_df_A['id'].unique()\n#test_ids_B = test_df_B['id'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:30:48.68466Z","iopub.execute_input":"2022-03-15T03:30:48.684978Z","iopub.status.idle":"2022-03-15T03:31:37.737989Z","shell.execute_reply.started":"2022-03-15T03:30:48.684947Z","shell.execute_reply":"2022-03-15T03:31:37.736576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:31:37.741662Z","iopub.execute_input":"2022-03-15T03:31:37.742973Z","iopub.status.idle":"2022-03-15T03:31:37.784097Z","shell.execute_reply.started":"2022-03-15T03:31:37.742878Z","shell.execute_reply":"2022-03-15T03:31:37.782869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#inference(test_df_A, cfg, 32, 'short')\n#inference(test_df_B, cfg, 8, 'long')","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:31:37.786097Z","iopub.execute_input":"2022-03-15T03:31:37.786725Z","iopub.status.idle":"2022-03-15T03:31:37.791838Z","shell.execute_reply.started":"2022-03-15T03:31:37.78656Z","shell.execute_reply":"2022-03-15T03:31:37.790131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_id_df(samples):\n    \n    id_df = pd.DataFrame()\n    \n    offset_list = []\n    id_list = []\n    num_list = []\n    len_list = []\n        \n    for i in tqdm(range(len(samples))):\n        offset_list += (samples[i]['offset_mapping'] + [(9999,9999)] * (cfg.max_len_test_longformer - len(samples[i]['offset_mapping'])))\n        id_list += [samples[i]['id']] * cfg.max_len_test_longformer\n        num_list += list(range(cfg.max_len_test_longformer))\n        len_list += [len(samples[i]['offset_mapping'])] * cfg.max_len_test_longformer\n        \n    id_df['offset'] = offset_list\n    id_df['id'] = id_list\n    id_df['token_num'] = num_list\n    id_df['token_len'] = len_list\n    \n    return id_df","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:31:37.794441Z","iopub.execute_input":"2022-03-15T03:31:37.794926Z","iopub.status.idle":"2022-03-15T03:31:37.809414Z","shell.execute_reply.started":"2022-03-15T03:31:37.794884Z","shell.execute_reply":"2022-03-15T03:31:37.808199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_max_class(oof, samples, tokenizer, cfg, fold_num=0):\n    \n    samples_ = samples.copy()\n    preds_class, preds_prob = [], []\n    arg_srt = np.argsort(oof, axis=-1)\n    srt = np.sort(oof, axis=-1)\n    \n    #argm = np.argmax(oof, axis=2)\n    #m = np.max(oof, axis=2)\n    \n    argm = arg_srt[:, :, -1]\n    m = srt[:, :, -1]\n    \n    argsecond = arg_srt[:, :, -2]\n    second = srt[:, :, -2]\n    \n    argthird = arg_srt[:, :, -3]\n    third = srt[:, :, -3]\n\n    for i in tqdm(range(len(samples))):\n        # 先頭0はspecial tokenなので除外\n        text_class = [id_target_map[c] for c in list(argm[i, 1:])]\n        text_prob = list(m[i, 1:])\n        samples_[i]['pred_class'] = text_class\n        samples_[i]['pred_prob'] = text_prob\n        \n        second_class = [id_target_map[c] for c in list(argsecond[i, 1:])]\n        second_prob = list(second[i, 1:])\n        samples_[i]['second_class'] = second_class\n        samples_[i]['second_prob'] = second_prob\n        \n        third_class = [id_target_map[c] for c in list(argthird[i, 1:])]\n        third_prob = list(third[i, 1:])\n        samples_[i]['third_class'] = third_class\n        samples_[i]['third_prob'] = third_prob\n        \n        \n        \n    #samples_all.append(samples_)\n\n    return samples","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:31:37.81194Z","iopub.execute_input":"2022-03-15T03:31:37.812572Z","iopub.status.idle":"2022-03-15T03:31:37.829361Z","shell.execute_reply.started":"2022-03-15T03:31:37.812502Z","shell.execute_reply":"2022-03-15T03:31:37.828126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef word_list(text):\n    a = re.split('(\\s+)', text)\n    return [a[i*2] + a[i*2+1] for i in range(len(a)//2)]\n\ndef discourse_offset(text, lst, start, end):\n    words = ''.join(lst[start:end+1])\n    return (text.find(words), text.find(words)+len(words)-1)\n\ndef add_offset(sub):\n    sub['start'] = sub['predictionstring'].map(lambda x: int(x.split()[0]))\n    sub['end'] = sub['predictionstring'].map(lambda x: int(x.split()[-1]))    \n\n    sub['text'] = sub['id'].map(txt_dict)\n    sub['offset'] = sub.apply(lambda x: discourse_offset(x['text'], word_list(x['text']), x['start'], x['end']), axis=1)\n\n    sub['start'] = sub['offset'].map(lambda x: x[0])\n    sub['end'] = sub['offset'].map(lambda x: x[1])\n    sub['discourse_num'] = sub.groupby('id')['class'].cumcount()\n    \n    return sub","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:31:37.831702Z","iopub.execute_input":"2022-03-15T03:31:37.832306Z","iopub.status.idle":"2022-03-15T03:31:37.848306Z","shell.execute_reply.started":"2022-03-15T03:31:37.832195Z","shell.execute_reply":"2022-03-15T03:31:37.846796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_sub_and_id_df(oof, id_df, test_samples):\n    sample1 = add_max_class(oof, test_samples, tokenizer, cfg, fold_num=0)\n    sub = create_submission(sample1)\n    sub = add_offset(sub)\n    \n    id_df_2 = pd.DataFrame()\n    for i in tqdm(range(len(test_ids) // 300 + 1)):\n        a =id_df.iloc[i*300*cfg.max_len_test_longformer:(i+1)*300*cfg.max_len_test_longformer, :].merge(sub[['id', 'discourse_num', 'start', 'end']], on=['id'], how='left').reset_index(drop=True)\n        a = a[(a['offset_start'] >= a['start']) & (a['offset_start'] <= a['end'])]\n        id_df_2 = id_df_2.append(a)\n        \n    id_df_2['discourse_key'] = id_df_2['id'].astype(str) + '_' + id_df_2['discourse_num'].astype(str)\n    sub['discourse_key'] = sub['id'].astype(str) + '_' + sub['discourse_num'].astype(str)\n    \n    return sub, id_df_2","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:31:37.850374Z","iopub.execute_input":"2022-03-15T03:31:37.850912Z","iopub.status.idle":"2022-03-15T03:31:37.865598Z","shell.execute_reply.started":"2022-03-15T03:31:37.850864Z","shell.execute_reply":"2022-03-15T03:31:37.864519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_prob_agg_df(oof):\n    value_list = [k for k in list(target_id_map.keys()) if k != 'PAD']\n\n    dfg = oof.groupby('discourse_key')[value_list]\n\n    dfg_stats = dfg.agg([np.mean, np.max, np.min]).stack()\n    dfg_quantiles = dfg.quantile([0.2, 0.8])\n\n    dfg_stats = dfg_stats.append(dfg_quantiles).sort_index()\n    prob_agg_df = dfg_stats.unstack().astype(np.float16)\n\n    prob_agg_df.columns = [col[0] + '_' + str(col[1]) for col in prob_agg_df.columns.values]\n    return prob_agg_df","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:31:37.868075Z","iopub.execute_input":"2022-03-15T03:31:37.868795Z","iopub.status.idle":"2022-03-15T03:31:37.880716Z","shell.execute_reply.started":"2022-03-15T03:31:37.868744Z","shell.execute_reply":"2022-03-15T03:31:37.879329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_oof(model_type):\n    #short = pickle.load(open(f'pred_{model_type}_short.pkl', 'rb'))\n    #long = pickle.load(open(f'pred_{model_type}_long.pkl', 'rb'))\n    \n    return pickle.load(open(f'pred_{model_type}_{mode}.pkl', 'rb'))","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:31:37.882458Z","iopub.execute_input":"2022-03-15T03:31:37.883841Z","iopub.status.idle":"2022-03-15T03:31:37.893203Z","shell.execute_reply.started":"2022-03-15T03:31:37.883727Z","shell.execute_reply":"2022-03-15T03:31:37.891861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_feats(model_type, id_df, id_df_2):\n    #path = '../input/fb-oof-dict/oof_dict_022709-deberta-large-boe-bin.pickle'\n    oof = create_oof(model_type)\n\n    oof = pd.DataFrame(oof.reshape(oof.shape[0] * oof.shape[1], oof.shape[2]))\n    #model_name = path.split('/')[-1].replace(\".pickle\", \"\").replace(\"oof_dict_\", \"\")\n\n    oof.columns = oof.columns.map(id_target_map)\n\n    oof['token_key'] = id_df['token_key']\n    oof = oof[oof['token_key'].isin(id_df_2['token_key'])]\n    oof['discourse_key'] = oof['token_key'].map(dict(zip(id_df_2['token_key'], id_df_2['discourse_key'])))\n\n    prob_agg_df = create_prob_agg_df(oof)\n    prob_agg_df.columns = [model_type + '_' + col for col in prob_agg_df.columns]\n    return prob_agg_df","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:31:37.894645Z","iopub.execute_input":"2022-03-15T03:31:37.896146Z","iopub.status.idle":"2022-03-15T03:31:37.908805Z","shell.execute_reply.started":"2022-03-15T03:31:37.896094Z","shell.execute_reply":"2022-03-15T03:31:37.907662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_sub(model_type, test_samples, id_df, model_paths):\n    oof = create_oof(model_type)\n    sub, id_df_2 = create_sub_and_id_df(oof, id_df, test_samples)\n    sub = sub[['id', 'class', 'predictionstring', 'start', 'end']]\n\n    #feats_list = []\n    #for model_path in model_paths:\n    #    feats  = create_feats(model_path, id_df_2)\n    #    feats_list.append(feats)\n    #feats = pd.concat(feats_list, axis=1)\n    #del id_df_2\n    \n    #sub['text_len'] = sub['text'].map(lambda x: len(x))\n    #sub = sub.merge(feats.reset_index(), on='discourse_key', how='left')\n    \n    return sub","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:31:37.912435Z","iopub.execute_input":"2022-03-15T03:31:37.912733Z","iopub.status.idle":"2022-03-15T03:31:37.921318Z","shell.execute_reply.started":"2022-03-15T03:31:37.9127Z","shell.execute_reply":"2022-03-15T03:31:37.919991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_ensemble_sub_and_feats(models, test_samples, id_df, weight):\n    \n    oof = np.zeros([len(test_samples), cfg.max_len_test_longformer , 15])\n    for model_type, w in zip(models, weight):\n        oof += create_oof(model_type) * w / np.sum(weight)\n    \n    sub, id_df_2 = create_sub_and_id_df(oof, id_df, test_samples)\n\n    feats_list = []\n    for model_type in models:\n        feats  = create_feats(model_type, id_df, id_df_2)\n        feats_list.append(feats)\n    feats = pd.concat(feats_list, axis=1)\n    del id_df_2\n    \n    sub['text_len'] = sub['text'].map(lambda x: len(x))\n    \n    sub = sub.merge(feats.reset_index(), on='discourse_key', how='left')\n    \n    return sub","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:31:37.923687Z","iopub.execute_input":"2022-03-15T03:31:37.924298Z","iopub.status.idle":"2022-03-15T03:31:37.93677Z","shell.execute_reply.started":"2022-03-15T03:31:37.924224Z","shell.execute_reply":"2022-03-15T03:31:37.935316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_result(df, bs, mode):\n    test_samples = prepare_test_data(df, tokenizer, cfg.num_jobs)\n    inference(df, cfg, bs, mode)\n    id_df = create_id_df(test_samples)\n    id_df['offset_start'] = id_df['offset'].map(lambda x: x[0])\n\n    id_df['token_key'] = id_df['id'].astype(str) + '_' + id_df['token_num'].astype(str)\n    \n    all_sub = pd.DataFrame()\n    for model_type in models:\n        sub = create_sub(model_type, test_samples, id_df, models)\n        sub['model_name'] = model_type\n        all_sub = all_sub.append(sub.reset_index(drop=True))\n        \n    ensemble_sub = create_ensemble_sub_and_feats(models, test_samples, id_df, weight)\n    \n    return all_sub, ensemble_sub","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:31:37.939561Z","iopub.execute_input":"2022-03-15T03:31:37.940028Z","iopub.status.idle":"2022-03-15T03:31:37.951956Z","shell.execute_reply.started":"2022-03-15T03:31:37.939919Z","shell.execute_reply":"2022-03-15T03:31:37.949526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.query('token_len >= 450 and token_len < 600')","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:31:37.953801Z","iopub.execute_input":"2022-03-15T03:31:37.954838Z","iopub.status.idle":"2022-03-15T03:31:38.002815Z","shell.execute_reply.started":"2022-03-15T03:31:37.954776Z","shell.execute_reply":"2022-03-15T03:31:38.001555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\ntxt_dict = {}\nfor ids in tqdm(test_ids):\n    txt_dict[ids] = get_test_text(ids)\n\nmodels = list(set([a[4] for a in cfg.model_ckp_path]))\nweight = [np.sum([w for k, _, _, _, m, w in cfg.model_ckp_path if m == model_type]) for model_type in models]    \n\ntest_df_A = test_df.query('token_len < 460')\nmode = 'short'\nall_sub_A, ensemble_sub_A = create_result(test_df_A, 40, 'short')\ngc.collect()\n\ntest_df_B = test_df.query('token_len >= 460 and token_len < 600')\nmode = 'medium'\nall_sub_B, ensemble_sub_B = create_result(test_df_B, 32, 'medium')\ngc.collect()\n\ntest_df_C = test_df.query('token_len >= 600')\nmode = 'long'\nall_sub_C, ensemble_sub_C = create_result(test_df_C, 8, 'long')\ngc.collect()\n\nall_sub = pd.concat([all_sub_A, all_sub_B, all_sub_C], axis=0)\ndel all_sub_A, all_sub_B, all_sub_C \nensemble_sub = pd.concat([ensemble_sub_A, ensemble_sub_B, ensemble_sub_C], axis=0)\ndel ensemble_sub_A, ensemble_sub_B, ensemble_sub_C\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:31:38.005116Z","iopub.execute_input":"2022-03-15T03:31:38.005879Z","iopub.status.idle":"2022-03-15T05:57:35.794504Z","shell.execute_reply.started":"2022-03-15T03:31:38.005828Z","shell.execute_reply":"2022-03-15T05:57:35.78835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in ['predictionstring', 'start', 'end']:\n    dic = (all_sub.groupby(['id', 'class', col])['model_name'].nunique() / all_sub['model_name'].nunique()).to_dict()\n    all_sub[f'dupli_{col}'] = all_sub.set_index(['id', 'class', col]).index.map(dic)\n    ensemble_sub[f'dupli_{col}'] = ensemble_sub.set_index(['id', 'class', col]).index.map(dic).fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:57:35.796325Z","iopub.status.idle":"2022-03-15T05:57:35.797164Z","shell.execute_reply.started":"2022-03-15T05:57:35.796835Z","shell.execute_reply":"2022-03-15T05:57:35.796871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feats = [col for col in ensemble_sub.columns if not col in ['id', 'class', 'predictionstring', 'text', 'offset', 'fold', 'is_tp', 'discourse_key', 'model_name']]","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:57:35.800496Z","iopub.status.idle":"2022-03-15T05:57:35.801339Z","shell.execute_reply.started":"2022-03-15T05:57:35.800947Z","shell.execute_reply":"2022-03-15T05:57:35.800997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_sub","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:57:35.802923Z","iopub.status.idle":"2022-03-15T05:57:35.803742Z","shell.execute_reply.started":"2022-03-15T05:57:35.803418Z","shell.execute_reply":"2022-03-15T05:57:35.803452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# lgbm postprocess","metadata":{}},{"cell_type":"code","source":"# write lgbm\n\nmodel = pickle.load(open('../input/lgbm-for-postprocess/lgb_fold0.pkl', 'rb'))\nfeats = model.feature_name_\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:57:35.805453Z","iopub.status.idle":"2022-03-15T05:57:35.806311Z","shell.execute_reply.started":"2022-03-15T05:57:35.805929Z","shell.execute_reply":"2022-03-15T05:57:35.805966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_dict = dict(zip(['Claim', 'Concluding Statement', 'Counterclaim', 'Evidence',\n       'Lead', 'Position', 'Rebuttal'], range(7)))\n\nclass_dict_inv = {v: k for k, v in class_dict.items()}\n\ndisc_prob = np.zeros(len(ensemble_sub))\n\nensemble_sub.columns = [col.replace('Concluding Statement','Concluding_Statement') for col in ensemble_sub.columns]\n\nsub = ensemble_sub[['id', 'class', 'predictionstring', 'prob', 'second_prob', 'len']].reset_index(drop=True)\nensemble_sub = ensemble_sub[feats]\n\nif 'class' in feats:\n    ensemble_sub['class'] = ensemble_sub['class'].map(class_dict).fillna(7)\n    \nif 'second_class' in feats:\n    ensemble_sub['second_class'] = ensemble_sub['second_class'].map(class_dict).fillna(7)\n    \nif 'third_class' in feats:\n    ensemble_sub['third_class'] = ensemble_sub['third_class'].map(class_dict).fillna(7)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:57:35.807947Z","iopub.status.idle":"2022-03-15T05:57:35.808774Z","shell.execute_reply.started":"2022-03-15T05:57:35.808456Z","shell.execute_reply":"2022-03-15T05:57:35.808492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(5):\n    model = pickle.load(open(f'../input/lgbm-for-postprocess/lgb_fold{fold}.pkl', 'rb'))\n    disc_prob += model.predict_proba(ensemble_sub)[:, 1] / 5\n    #sub[f'disc_prob_{fold}'] = model.predict_proba(ensemble_sub)[:, 1]\n    \nsub['disc_prob'] = disc_prob","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:57:35.810494Z","iopub.status.idle":"2022-03-15T05:57:35.811399Z","shell.execute_reply.started":"2022-03-15T05:57:35.810972Z","shell.execute_reply":"2022-03-15T05:57:35.811008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:57:35.813056Z","iopub.status.idle":"2022-03-15T05:57:35.813892Z","shell.execute_reply.started":"2022-03-15T05:57:35.813554Z","shell.execute_reply":"2022-03-15T05:57:35.81359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# postprocess\n\ncfg.proba_thresh = {\n        \"Lead\": 0.49, # 0.7\n        \"Position\": 0.29, # 0.55\n        \"Evidence\": 0.54,\n        \"Claim\": 0.44,\n        \"Concluding Statement\": 0.49, # 0.7\n        \"Counterclaim\": 0.48,\n        \"Rebuttal\": 0.44,\n    }\ncfg.min_token_thresh = {\n        \"Lead\": 4, # 5\n        \"Position\": 3, # 4\n        \"Evidence\": 11, # 14\n        \"Claim\": 1, # 2\n        \"Concluding Statement\": 7, # 7\n        \"Counterclaim\": 4, # 6\n        \"Rebuttal\": 3, # 4\n    }\ncfg.link = {\n        'Evidence': 40,\n        'Counterclaim': 200,\n        'Rebuttal': 200,\n    }\n\nsub['proba_thresh'] = sub['class'].map(target_id_map)\nsub = sub[((sub['disc_prob'] > 0.19) & ((sub['prob'] - sub['proba_thresh'] > 0.1) |(sub['disc_prob'] > 0.28) | ((sub['class'].isin(['Counterclaim'])) & (sub['prob'] - sub['second_prob'] > 0.25)) | (sub['class'].isin(['Rebuttal'])) ))][['id', 'class', 'predictionstring', 'prob', 'len']]","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:57:35.815541Z","iopub.status.idle":"2022-03-15T05:57:35.816369Z","shell.execute_reply.started":"2022-03-15T05:57:35.816022Z","shell.execute_reply":"2022-03-15T05:57:35.816057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = post_process_sub(sub[['id', 'class', 'predictionstring', 'prob', 'len']]).drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:57:35.818742Z","iopub.status.idle":"2022-03-15T05:57:35.819578Z","shell.execute_reply.started":"2022-03-15T05:57:35.819223Z","shell.execute_reply":"2022-03-15T05:57:35.819276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:57:35.82129Z","iopub.status.idle":"2022-03-15T05:57:35.82208Z","shell.execute_reply.started":"2022-03-15T05:57:35.82176Z","shell.execute_reply":"2022-03-15T05:57:35.821795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(feats)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:57:35.823742Z","iopub.status.idle":"2022-03-15T05:57:35.824583Z","shell.execute_reply.started":"2022-03-15T05:57:35.824232Z","shell.execute_reply":"2022-03-15T05:57:35.824281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# submission","metadata":{}},{"cell_type":"code","source":"%%time\nsub = sub[['id', 'class', 'predictionstring']]\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T05:57:35.82619Z","iopub.status.idle":"2022-03-15T05:57:35.827034Z","shell.execute_reply.started":"2022-03-15T05:57:35.826679Z","shell.execute_reply":"2022-03-15T05:57:35.826745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize","metadata":{}},{"cell_type":"code","source":"class Color:\n    BLACK = '\\033[30m'\n    Lead = '\\033[31m'\n    Position = '\\033[32m'\n    Claim = '\\033[33m'\n    Counterclaim = '\\033[34m'\n    Rebuttal = '\\033[35m'\n    Evidence = '\\033[36m'\n    ConcludingStatement = '\\033[37m'\n    END = '\\033[0m'\n    BOLD = '\\038[1m'\n    UNDERLINE = '\\033[4m'\n    INVISIBLE = '\\033[08m'\n    REVERCE = '\\033[07m'\n\n\ndef visualize(sub, doc_id):\n    word_type = [None] * 100000\n    df = sub.query(f' id == \"{doc_id}\" ')\n    for i in range(df.shape[0]):\n        row = df.iloc[i, :]\n        _cls, _pred = row['class'], row['predictionstring']\n        _pred_list = _pred.split(' ')\n        for j in _pred_list:\n            word_type[int(j)] = _cls\n    \n    text = get_test_text(doc_id)\n    word_list = text.split()\n    \n    for i, word in enumerate(word_list):\n        if word_type[i] == 'Lead':\n            print(f'{Color.Lead}{word}{Color.END}', end=' ')\n        elif word_type[i] == 'Position':\n            print(f'{Color.Position}{word}{Color.END}', end=' ')\n        elif word_type[i] == 'Claim':\n            print(f'{Color.Claim}{word}{Color.END}', end=' ')\n        elif word_type[i] == 'Counterclaim':\n            print(f'{Color.Counterclaim}{word}{Color.END}', end=' ')\n        elif word_type[i] == 'Rebuttal':\n            print(f'{Color.Rebuttal}{word}{Color.END}', end=' ')\n        elif word_type[i] == 'Evidence':\n            print(f'{Color.Evidence}{word}{Color.END}', end=' ')\n        elif word_type[i] == 'Concluding Statement':\n            print(f'{Color.ConcludingStatement}{word}{Color.END}', end=' ')\n        else:\n            print(word, end=' ')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-15T05:57:35.828812Z","iopub.status.idle":"2022-03-15T05:57:35.829667Z","shell.execute_reply.started":"2022-03-15T05:57:35.829343Z","shell.execute_reply":"2022-03-15T05:57:35.829384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize(sub, '18409261F5C2')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-15T05:57:35.831314Z","iopub.status.idle":"2022-03-15T05:57:35.832094Z","shell.execute_reply.started":"2022-03-15T05:57:35.83179Z","shell.execute_reply":"2022-03-15T05:57:35.831823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize(sub, 'D46BCB48440A')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-15T05:57:35.833738Z","iopub.status.idle":"2022-03-15T05:57:35.834585Z","shell.execute_reply.started":"2022-03-15T05:57:35.834242Z","shell.execute_reply":"2022-03-15T05:57:35.834296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize(sub, '0FB0700DAF44')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-15T05:57:35.836222Z","iopub.status.idle":"2022-03-15T05:57:35.837018Z","shell.execute_reply.started":"2022-03-15T05:57:35.836708Z","shell.execute_reply":"2022-03-15T05:57:35.836742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize(sub, 'D72CB1C11673')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-15T05:57:35.838658Z","iopub.status.idle":"2022-03-15T05:57:35.839504Z","shell.execute_reply.started":"2022-03-15T05:57:35.839112Z","shell.execute_reply":"2022-03-15T05:57:35.839148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize(sub, 'DF920E0A7337')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-15T05:57:35.841056Z","iopub.status.idle":"2022-03-15T05:57:35.841845Z","shell.execute_reply.started":"2022-03-15T05:57:35.841542Z","shell.execute_reply":"2022-03-15T05:57:35.841575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}