{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"動作確認","metadata":{}},{"cell_type":"code","source":"import gc\ngc.enable()\n\nimport sys\nsys.path.append(\"../input/tez-lib/\")\n\nimport os\nimport random\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import softmax\nimport pickle\n\nimport tez\nimport torch\nimport torch.nn as nn\nfrom joblib import Parallel, delayed\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-14T12:49:34.410127Z","iopub.execute_input":"2022-03-14T12:49:34.410553Z","iopub.status.idle":"2022-03-14T12:49:36.929309Z","shell.execute_reply.started":"2022-03-14T12:49:34.410434Z","shell.execute_reply":"2022-03-14T12:49:36.928487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class Config:\n    input_dir = '../input/feedback-prize-2021'\n    \n    model_longformer = '../input/longformerlarge4096/longformer-large-4096'\n    model_led = '../input/led-large'\n    model_deberta = '../input/deberta-large-download/deberta-large'\n    \n    max_len_test_longformer = 1600\n    max_len_test_led = 1024\n    \n    tokenizer = '../input/deberta-large-download/deberta-large' #'../input/longformerlarge4096/longformer-large-4096'\n    num_jobs = 4\n    seed = 1\n    \n    model_ckp_path = [\n        #  (kind, num_labels, model_path, weight)\n        # exp lf-large, lf-base, led-large, bb-large, ...\n        \n        #('lf-large', 22, '../input/2022021410-lf-bie-bin/model_0.bin', 1.0),\n        #('lf-large', 22, '../input/2022021410-lf-bie-bin/model_1.bin', 1.0),\n        #('lf-large', 22, '../input/2022021410-lf-bie-bin/model_2.bin', 1.0),\n        #('lf-large', 22, '../input/2022021410-lf-bie-bin/model_3.bin', 1.0),\n        #('lf-large', 22, '../input/2022021410-lf-bie-bin/model_4.bin', 1.0),\n        \n        #('lf-large', 15, '../input/2022020906-aug-bin/model_0.bin', 1.0),\n        #('lf-large', 15, '../input/2022020906-aug-bin/model_1.bin', 1.0),\n        #('lf-large', 15, '../input/2022020906-aug-bin/model_2.bin', 1.0),\n        #('lf-large', 15, '../input/2022020906-aug-bin/model_3.bin', 1.0),\n        #('lf-large', 15, '../input/2022020906-aug-bin/model_4.bin', 1.0),\n\n        #('led-large', 24, '../input/fb-exp037-led/model_0.bin', 1.0),\n        #('led-large', 24, '../input/fb-exp037-led/model_1.bin', 1.0),\n        #('led-large', 24, '../input/fb-exp037-led/model_2.bin', 1.0),\n        #('led-large', 24, '../input/fb-exp037-led/model_3.bin', 1.0),\n        #('led-large', 24, '../input/fb-exp037-led/model_4.bin', 1.0),\n\n        #('led-large', 24, '../input/fb-exp017-led/model_0.bin', 1.0),\n        #('led-large', 24, '../input/fb-exp017-led/model_1.bin', 1.0),\n        #('led-large', 24, '../input/fb-exp017-led/model_2.bin', 1.0),\n        #('led-large', 24, '../input/fb-exp017-led/model_3.bin', 1.0),\n        #('deberta-large', 22, '../input/fb-exp045-deberta/model_0.bin', 1.0),\n        ('deberta-large', 15, '../input/2022022709-deberta-large-boe-bin/model_0.bin', 1.0),\n    ]\n\n    proba_thresh = {\n        \"Lead\": 0.6, # 0.7\n        \"Position\": 0.4, # 0.55\n        \"Evidence\": 0.65,\n        \"Claim\": 0.55,\n        \"Concluding Statement\": 0.6, # 0.7\n        \"Counterclaim\": 0.5,\n        \"Rebuttal\": 0.55,\n    }\n    min_token_thresh = {\n        \"Lead\": 5, # 9\n        \"Position\": 4, # 5\n        \"Evidence\": 14,\n        \"Claim\": 2,\n        \"Concluding Statement\": 7, # 11\n        \"Counterclaim\": 6,\n        \"Rebuttal\": 4,\n    }\n    link = {\n        'Evidence': 40,\n        'Counterclaim': 200,\n        'Rebuttal': 200,\n    }\n\n\ncfg = Config()    \ntarget_id_map = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 2,\n    \"I-Position\": 3,\n    \"B-Evidence\": 4,\n    \"I-Evidence\": 5,\n    \"B-Claim\": 6,\n    \"I-Claim\": 7,\n    \"B-Concluding Statement\": 8,\n    \"I-Concluding Statement\": 9,\n    \"B-Counterclaim\": 10,\n    \"I-Counterclaim\": 11,\n    \"B-Rebuttal\": 12,\n    \"I-Rebuttal\": 13,\n    \"O\": 14,\n    \"PAD\": -100,\n}\nid_target_map = {v: k for k, v in target_id_map.items()}\n","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:49:36.931231Z","iopub.execute_input":"2022-03-14T12:49:36.93157Z","iopub.status.idle":"2022-03-14T12:49:36.942625Z","shell.execute_reply.started":"2022-03-14T12:49:36.931529Z","shell.execute_reply":"2022-03-14T12:49:36.941714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modules","metadata":{}},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def get_test_text(ids):\n    with open(f'{cfg.input_dir}/train/{ids}.txt', 'r') as f:\n        text = f.read()\n    return text\n\n\ndef seed_everything(seed: int) -> None:\n    \"\"\"\n    seedの固定\n    \"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \n        \nseed_everything(cfg.seed)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-14T12:49:36.943901Z","iopub.execute_input":"2022-03-14T12:49:36.944552Z","iopub.status.idle":"2022-03-14T12:49:36.965176Z","shell.execute_reply.started":"2022-03-14T12:49:36.944511Z","shell.execute_reply":"2022-03-14T12:49:36.964234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre Process","metadata":{}},{"cell_type":"code","source":"def _prepare_test_data_helper(tokenizer, ids):\n    test_samples = []\n    for idx in ids:\n        text = get_test_text(idx)\n\n        encoded_text = tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n            max_length=1600,\n            truncation=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        offset_mapping = encoded_text[\"offset_mapping\"]\n\n        sample = {\n            \"id\": idx,\n            \"input_ids\": input_ids,\n            \"text\": text,\n            \"offset_mapping\": offset_mapping,\n        }\n\n        test_samples.append(sample)\n    return test_samples\n\n\ndef prepare_test_data(ids, tokenizer, num_jobs):\n    test_samples = []\n    #ids = df[\"id\"].unique()\n    ids_splits = np.array_split(ids, 4)\n\n    results = Parallel(n_jobs=num_jobs, backend=\"multiprocessing\")(\n        delayed(_prepare_test_data_helper)(tokenizer, idx) for idx in ids_splits\n    )\n    for result in results:\n        test_samples.extend(result)\n\n    return test_samples","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-14T12:49:36.966844Z","iopub.execute_input":"2022-03-14T12:49:36.967268Z","iopub.status.idle":"2022-03-14T12:49:36.978509Z","shell.execute_reply.started":"2022-03-14T12:49:36.967212Z","shell.execute_reply":"2022-03-14T12:49:36.977721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class FeedbackTestDataset:\n    def __init__(self, samples, max_len, tokenizer):\n        self.samples = samples\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.length = len(samples)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        input_ids = self.samples[idx][\"input_ids\"]\n        input_ids = [self.tokenizer.cls_token_id] + input_ids\n\n        if len(input_ids) > self.max_len - 1:\n            input_ids = input_ids[: self.max_len - 1]\n\n        # add end token id to the input_ids\n        input_ids = input_ids + [self.tokenizer.sep_token_id]\n        attention_mask = [1] * len(input_ids)\n        return {\n            \"ids\": input_ids,\n            \"mask\": attention_mask,\n        }\n    \n    \nclass Collate:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"ids\"]])\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n        else:\n            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n\n        # convert to tensors\n        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n\n        return output","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-14T12:49:36.980649Z","iopub.execute_input":"2022-03-14T12:49:36.982522Z","iopub.status.idle":"2022-03-14T12:49:36.995593Z","shell.execute_reply.started":"2022-03-14T12:49:36.982471Z","shell.execute_reply":"2022-03-14T12:49:36.994686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class FeedbackModel(tez.Model):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n        self.config = AutoConfig.from_pretrained(model_name)\n        self.config.update(\n            {\n                'output_hidden_states': True,\n                'add_pooling_layer': False,\n                'num_labels': self.num_labels,\n                'attention_probs_dropout_prob':  0.1,  # for longformer\n                'hidden_dropout_prob': 0.1,            # for longformer\n                'layer_norm_eps': 1e-7,                # for longformer\n                'activation_dropout': 0.0,             # for LED\n                'attention_dropout': 0.0,              # for LED\n                'classif_dropout': 0.0,                # for LED\n                'classifier_dropout': 0.0,             # for LED\n                'decoder_layerdrop': 0.0,              # for LED\n                'encoder_layerdrop': 0.0,              # for LED \n            }\n        )\n        self.transformer = AutoModel.from_config(self.config)\n        self.output = nn.Linear(self.config.hidden_size, self.num_labels)\n\n    def forward(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        logits = self.output(sequence_output)\n        logits = torch.softmax(logits, dim=-1)\n        return logits, 0, {}\n\n    \nclass FeedbackModelV2(tez.Model):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n        self.config = AutoConfig.from_pretrained(model_name)\n        self.config.update(\n            {\n                'output_hidden_states': True,\n                'add_pooling_layer': False,\n                'num_labels': self.num_labels,\n                'attention_probs_dropout_prob':  0.1,  # for longformer\n                'hidden_dropout_prob': 0.1,            # for longformer\n                'layer_norm_eps': 1e-7,                # for longformer\n                'activation_dropout': 0.0,             # for LED\n                'attention_dropout': 0.0,              # for LED\n                'classif_dropout': 0.0,                # for LED\n                'classifier_dropout': 0.0,             # for LED\n                'decoder_layerdrop': 0.0,              # for LED\n                'encoder_layerdrop': 0.0,              # for LED \n            }\n        )\n        self.transformer = AutoModel.from_config(self.config)\n        self.fc = nn.Linear(self.config.hidden_size, self.num_labels)\n\n    def forward(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        logits = self.fc(sequence_output)\n        logits = torch.softmax(logits, dim=-1)\n        return logits, 0, {}\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-14T12:49:36.996933Z","iopub.execute_input":"2022-03-14T12:49:36.997174Z","iopub.status.idle":"2022-03-14T12:49:37.016129Z","shell.execute_reply.started":"2022-03-14T12:49:36.997146Z","shell.execute_reply":"2022-03-14T12:49:37.015241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"data_name = {0:\"lf\",\n            1:\"led\",\n            2:\"deb\",\n            3:\"ens\"}","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:49:37.017495Z","iopub.execute_input":"2022-03-14T12:49:37.017867Z","iopub.status.idle":"2022-03-14T12:49:37.034864Z","shell.execute_reply.started":"2022-03-14T12:49:37.017829Z","shell.execute_reply":"2022-03-14T12:49:37.033859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(raw_preds, samples, tokenizer, cfg, bs, fold_num=0):\n    \n    #raw_preds = (raw_predss*w[0] + raw_predss*w[1]) / np.sum(w)\n    #for raw_preds in raw_preds_all:\n    samples_ = samples.copy()\n    preds_class, preds_prob = [], []\n    for rp in raw_preds:\n        c_arr = np.argmax(rp, axis=2)\n        p_arr = np.max(rp, axis=2)\n        for c, p in zip(c_arr, p_arr):\n            c = c.tolist()\n            p = p.tolist()\n            preds_class.append(c)\n            preds_prob.append(p)\n\n    for i in range(len(samples)):\n        # 先頭0はspecial tokenなので除外\n        text_class = [id_target_map[c] for c in preds_class[i][1:]]\n        text_prob = preds_prob[i][1:]\n        samples_[i]['pred_class'] = text_class\n        samples_[i]['pred_prob'] = text_prob\n    #samples_all.append(samples_)\n\n    return samples#_all\n\n\ndef reshape_preds_24_to_15(preds):\n    new_preds = np.zeros((preds.shape[0], preds.shape[1], 15))\n    new_preds[:, :, 0] = preds[:, :, 0]\n    new_preds[:, :, 1] += preds[:, :, 1] + preds[:, :, 2]\n    new_preds[:, :, 2] += preds[:, :, 3]\n    new_preds[:, :, 3] += preds[:, :, 4] + preds[:, :, 5]\n    new_preds[:, :, 4] += preds[:, :, 6]\n    new_preds[:, :, 5] += preds[:, :, 7] + preds[:, :, 8]\n    new_preds[:, :, 6] += preds[:, :, 9]\n    new_preds[:, :, 7] += preds[:, :, 10] + preds[:, :, 11]\n    new_preds[:, :, 8] += preds[:, :, 12]\n    new_preds[:, :, 9] += preds[:, :, 13] + preds[:, :, 14]\n    new_preds[:, :, 10] += preds[:, :, 15]\n    new_preds[:, :, 11] += preds[:, :, 16] + preds[:, :, 17]\n    new_preds[:, :, 12] += preds[:, :, 18]\n    new_preds[:, :, 13] += preds[:, :, 19] + preds[:, :, 20]\n    new_preds[:, :, 14] += preds[:, :, 21] + preds[:, :, 22] + preds[:, :, 23]\n    return new_preds\n\ndef reshape_preds_22_to_15(preds):\n    new_preds = np.zeros((preds.shape[0], preds.shape[1], 15))\n    new_preds[:, :, 0] = preds[:, :, 0]\n    new_preds[:, :, 1] += preds[:, :, 1] + preds[:, :, 2]\n    new_preds[:, :, 2] += preds[:, :, 3]\n    new_preds[:, :, 3] += preds[:, :, 4] + preds[:, :, 5]\n    new_preds[:, :, 4] += preds[:, :, 6]\n    new_preds[:, :, 5] += preds[:, :, 7] + preds[:, :, 8]\n    new_preds[:, :, 6] += preds[:, :, 9]\n    new_preds[:, :, 7] += preds[:, :, 10] + preds[:, :, 11]\n    new_preds[:, :, 8] += preds[:, :, 12]\n    new_preds[:, :, 9] += preds[:, :, 13] + preds[:, :, 14]\n    new_preds[:, :, 10] += preds[:, :, 15]\n    new_preds[:, :, 11] += preds[:, :, 16] + preds[:, :, 17]\n    new_preds[:, :, 12] += preds[:, :, 18]\n    new_preds[:, :, 13] += preds[:, :, 19] + preds[:, :, 20]\n    new_preds[:, :, 14] += preds[:, :, 21]\n    return new_preds","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-14T12:49:37.036013Z","iopub.execute_input":"2022-03-14T12:49:37.036549Z","iopub.status.idle":"2022-03-14T12:49:37.056743Z","shell.execute_reply.started":"2022-03-14T12:49:37.036512Z","shell.execute_reply":"2022-03-14T12:49:37.056002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Post Process","metadata":{}},{"cell_type":"code","source":"def post_process(samples):\n    sub = create_submission(samples)\n    sub = thresh_prob(sub, cfg)\n    sub = thresh_min_token(sub, cfg)\n    sub = get_max_prob(sub)\n    sub = link_class(sub, 'Evidence', cfg.link['Evidence'])\n    sub = link_class(sub, 'Counterclaim', cfg.link['Counterclaim'])\n    sub = link_class(sub, 'Rebuttal', cfg.link['Rebuttal'])\n    sub = sub.reset_index(drop=True)\n    sub = sub[['id', 'class', 'predictionstring']]\n    return sub\n\n\ndef create_submission(samples):\n    sub = []\n    for _, sample in enumerate(samples):\n        pred_class = sample['pred_class']\n        offset_mapping = sample['offset_mapping']\n        sample_id = sample['id']\n        sample_text = sample['text']\n        pred_prob = sample['pred_prob']\n        \n        second_class = sample['second_class']\n        second_prob = sample['second_prob']\n        \n        third_class = sample['third_class']\n        third_prob = sample['third_prob']\n        \n\n        pred_class = fix_list_(pred_class)\n\n        sample_preds = []\n        # テキストが4096より長い場合\n        if len(pred_class) < len(offset_mapping)-1:\n            pred_class += ['O'] * (len(offset_mapping) - len(pred_class))\n            second_class += ['O'] * (len(offset_mapping) - len(second_class))\n            third_class += ['O'] * (len(offset_mapping) - len(third_class))\n            \n            pred_prob += [0] * (len(offset_mapping) - len(pred_prob))\n            second_prob += [0] * (len(offset_mapping) - len(second_prob))\n            third_prob += [0] * (len(offset_mapping) - len(third_prob))\n\n        idx = 0\n        phrase_preds = []\n        while idx < len(offset_mapping)-1:\n            start, _ = offset_mapping[idx]\n            if pred_class[idx] != 'O':\n                label = pred_class[idx][2:]\n            else:\n                label = 'O'\n                \n            if second_class[idx] != 'O':\n                second_label = second_class[idx][2:]\n            else:\n                second_label = 'O'\n                \n            if pred_class[idx] != 'O':\n                third_label = third_class[idx][2:]\n            else:\n                third_label = 'O'\n            \n            \n            phrase_probs = []\n            phrase_probs.append(pred_prob[idx])\n            \n            phrase_second_probs = []\n            phrase_second_probs.append(second_prob[idx])\n            \n            phrase_third_probs = []\n            phrase_third_probs.append(third_prob[idx])\n            \n            idx += 1\n            \n            while idx < len(offset_mapping)-1:\n                if label != 'O':\n                    matching_label = f'I-{label}'\n                else:\n                    matching_label = 'O'\n                if pred_class[idx] == matching_label:\n                    _, end = offset_mapping[idx]\n                    phrase_probs.append(pred_prob[idx])\n                    phrase_second_probs.append(second_prob[idx])\n                    phrase_third_probs.append(third_prob[idx])\n                    \n                    idx += 1\n                else:\n                    break\n            if 'end' in locals():\n                phrase = sample_text[start:end]\n                phrase_preds.append((phrase, start, end, label, second_label, third_label, phrase_probs, phrase_second_probs, phrase_third_probs))\n\n        temp_df = []\n        for phrase_idx, (phrase, start, end, label, second_label, third_label, phrase_probs, phrase_second_probs, phrase_third_probs) in enumerate(phrase_preds):\n            word_start = len(sample_text[:start].split())\n            word_end = word_start + len(sample_text[start:end].split())\n            word_end = min(word_end, len(sample_text.split()))\n            ps = \" \".join([str(x) for x in range(word_start, word_end)])\n            if label != 'O':\n                phrase_probs_mean = sum(phrase_probs) / len(phrase_probs)\n                phrase_second_probs_mean = sum(phrase_second_probs) / len(phrase_probs)\n                phrase_third_probs_mean = sum(phrase_third_probs) / len(phrase_probs)\n                \n                temp_df.append((sample_id, label, second_label, third_label, ps, phrase_probs_mean, phrase_second_probs_mean, phrase_third_probs_mean))\n        temp_df = pd.DataFrame(temp_df, columns=['id', 'class', 'second_class', 'third_class', 'predictionstring', 'prob', 'second_prob', 'third_prob'])\n        sub.append(temp_df)\n    \n    sub = pd.concat(sub).reset_index(drop=True)\n    sub['len'] = sub['predictionstring'].apply(lambda x: len(x.split()))\n    sub = sub[sub['len'] > 0]\n    return sub\n\ndef fix_list(pred_list):\n\n    class_list = [\"I-Lead\", \"I-Position\", \"I-Evidence\", \"I-Claim\", \n                  \"I-Concluding Statement\", \"I-Counterclaim\", \"I-Rebuttal\", \"O\"]\n    \n    fix_threholds = {\n        \"I-Lead\":2, \n        \"I-Concluding Statement\":2, \n        \"I-Evidence\":1,\n        \"I-Position\":2,\n        \"I-Claim\":1,\n        \"I-Counterclaim\":5, \n        \"I-Rebuttal\":7,\n        \"O\":1\n    }\n    \n    for class_ in class_list:\n\n        flg_index = []\n        out_class = [col for col in class_list if col not in class_]\n        counter = 0\n\n        for token_id, token in enumerate(pred_list):\n            \n            # 連続2回以上続いた後の別classにはflgを立てる\n            if counter > 2 and token in out_class:\n                flg_index.append(token_id)\n                counter = 0\n\n            if token == class_:\n                counter += 1\n            else:\n                counter = 0\n                \n        for ind in flg_index:\n            if ind + fix_threholds[class_] + 1 < len(pred_list):\n                counter_2 = fix_threholds[class_]\n                while counter_2 != 0:\n                    if pred_list[ind + counter_2] == class_ and pred_list[ind + counter_2 + 1] == class_:\n                        for i in range(counter_2):\n                            pred_list[ind + i] = class_\n                        counter_2 = 0\n                    else:\n                        counter_2 -= 1\n                        \n    return pred_list\n\ndef fix_list_(pred_list):\n    class_list = [\"I-Lead\", \"I-Position\", \"I-Evidence\", \"I-Claim\", \n                  \"I-Concluding Statement\", \"I-Counterclaim\", \"I-Rebuttal\"]\n\n    for class_ in class_list:\n        flg_index = []\n        out_class = set(class_list) - {class_}\n        counter = 0\n\n        for token_id, token in enumerate(pred_list):\n            if counter > 2 and token in out_class:\n                flg_index.append(token_id)\n                counter = 0\n\n            if token == class_:\n                counter += 1\n            else:\n                counter = 0\n\n        for ind in flg_index:\n            if ind + 2 < len(pred_list):\n                if pred_list[ind + 1] == class_ and pred_list[ind + 2] == class_:\n                    pred_list[ind] = class_\n\n    return pred_list\n\n\ndef jn(pst, start, end):\n    return \" \".join([str(x) for x in pst[start:end] if x != -1])\n\n\ndef link_class(oof, discourse_type, thresh2):\n    id_list = oof['id'].unique().tolist()\n    if not len(oof):\n        return oof\n    thresh = 1\n    idu = oof['id'].unique()\n    eoof = oof[oof['class'] == f\"{discourse_type}\"]\n    neoof = oof[oof['class'] != f\"{discourse_type}\"]\n    eoof.index = eoof[['id', 'class']]\n    \n    retval = []\n    for idv in idu:\n        q = eoof[eoof['id'] == idv]\n        if not len(q):\n            continue\n        pst = []\n        for r in q.itertuples():\n            pst = [*pst, -1,  *[int(x) for x in r.predictionstring.split()]]\n        start, end = 1, 1\n        for i in range(2, len(pst)):\n            cur = pst[i]\n            end = i\n            if  (\n                (cur == -1) and\n                ((pst[i + 1] > pst[end - 1] + thresh) or (pst[i + 1] - pst[start] > thresh2))\n            ):\n                retval.append((idv, discourse_type, jn(pst, start, end)))\n                start = i + 1\n        v = (idv, discourse_type, jn(pst, start, end + 1))\n        retval.append(v)\n\n    roof = pd.DataFrame(retval, columns=['id', 'class', 'predictionstring'])\n    roof = roof.merge(neoof, how='outer')\n    \n    dfs = []\n    for doc_id in id_list:\n        r_df_tmp = roof.query(f'id == \"{doc_id}\"')\n        r_df_tmp['start'] = r_df_tmp['predictionstring'].apply(lambda x: int(x.split(' ')[0]))\n        r_df_tmp = r_df_tmp.sort_values('start').drop('start', axis=1)\n        dfs.append(r_df_tmp)\n    return pd.concat(dfs, axis=0)\n\n\ndef thresh_prob(df, cfg):\n    df_other = df[(df['class'] != 'Claim') | (df['len'] != 2)]\n    df_target = df[(df['class'] == 'Claim') & (df['len'] == 2)]\n    df_target['prob'] -= 0.1\n    df = pd.concat([df_other, df_target])\n    df = df.sort_index()\n    for k, v in cfg.proba_thresh.items():\n        idx = df.loc[df['class'] == k].query(f'prob < {v}').index\n        df = df.drop(idx)\n    return df\n\n# add\ndef thresh_second_prob(df, cfg):\n    df = df.sort_index()\n    for k, v in cfg.second_proba_thresh.items():\n        idx = df.loc[df['class'] == k].query(f'second_prob > {v}').index\n        df = df.drop(idx)\n    return df\n\n# add\ndef thresh_third_prob(df, cfg):\n    df = df.sort_index()\n    for k, v in cfg.second_proba_thresh.items():\n        idx = df.loc[df['class'] == k].query(f'third_prob > {v}').index\n        df = df.drop(idx)\n    return df\n\n\ndef thresh_min_token(df, cfg):\n    df['len'] = df['predictionstring'].apply(lambda x: len(x.split(' ')))\n    for k, v in cfg.min_token_thresh.items():\n        idx = df.loc[df['class'] == k].query(f'len < {v}').index\n        df = df.drop(idx)\n    return df\n\n\ndef get_max_prob(sub):\n    sub['prob'] = sub['prob'].astype(float)\n    id_list = sub['id'].unique().tolist()\n    unique_class = ['Lead', 'Position', 'Concluding Statement']\n    sub_in_unique = sub[sub['class'].isin(unique_class) == True]\n    sub_not_in_unique = sub[sub['class'].isin(unique_class) == False]\n    sub_in_unique = sub_in_unique.loc[sub_in_unique.groupby(['id', 'class'])['prob'].idxmax(), :]\n    sub = pd.concat([sub_in_unique, sub_not_in_unique])\n    return sub\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-14T12:49:37.058019Z","iopub.execute_input":"2022-03-14T12:49:37.058716Z","iopub.status.idle":"2022-03-14T12:49:37.105363Z","shell.execute_reply.started":"2022-03-14T12:49:37.058679Z","shell.execute_reply":"2022-03-14T12:49:37.104694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =============================\n# Metrics\n# =============================   \ndef calc_overlap(row):\n    \"\"\"\n    ref: https://www.kaggle.com/robikscube/student-writing-competition-twitch\n    \"\"\"\n    set_pred = set(row[\"predictionstring_pred\"].split(\" \"))\n    set_true = set(row[\"predictionstring_true\"].split(\" \"))\n\n    len_true, len_pred = len(set_true), len(set_pred)\n    intersection = len(set_true.intersection(set_pred))\n\n    overlap_t_and_p = intersection / len_true\n    overlap_p_and_t = intersection / len_pred\n\n    return [overlap_t_and_p, overlap_p_and_t]\n\n\ndef get_micro_f1_score(pred_df, true_df):\n\n    true_df = (true_df[[\"id\", \"discourse_type\", \"predictionstring\"]]\n               .reset_index(drop=True)\n               .copy())\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    pred_df[\"pred_id\"] = pred_df.index\n    true_df[\"true_id\"] = true_df.index\n\n    # 1. all ground truths and predictions for a given class are compared.\n    joined_df = pred_df.merge(\n        true_df,\n        left_on=[\"id\", \"class\"],\n        right_on=[\"id\", \"discourse_type\"],\n        how=\"outer\",\n        suffixes=(\"_pred\", \"_true\"),\n    )\n    joined_df[\"predictionstring_true\"].fillna(\" \", inplace=True)\n    joined_df[\"predictionstring_pred\"].fillna(\" \", inplace=True)\n    joined_df[\"overlaps\"] = joined_df.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n    #    and the overlap between the prediction and the ground truth >= 0.5,\n    #    the prediction is a match and considered a true positive.\n    #    If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined_df[\"overlap1\"] = joined_df[\"overlaps\"].apply(lambda x: eval(str(x))[0])\n    joined_df[\"overlap2\"] = joined_df[\"overlaps\"].apply(lambda x: eval(str(x))[1])\n\n    joined_df[\"potential_TP\"] = (joined_df[\"overlap1\"] >= 0.5) & (joined_df[\"overlap2\"] >= 0.5)\n    joined_df[\"max_overlap\"] = joined_df[[\"overlap1\", \"overlap2\"]].max(axis=1)\n    tp_pred_ids = (\n        joined_df.query(\"potential_TP\")\n        .sort_values(\"max_overlap\", ascending=False)\n        .groupby([\"id\", \"predictionstring_true\"])\n        .first()[\"pred_id\"]\n        .values\n    )\n\n    # 3. Any unmatched ground truths are false negatives\n    #    and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined_df[\"pred_id\"].unique() if p not in tp_pred_ids]\n    matched_gt_ids = joined_df.query(\"potential_TP\")[\"true_id\"].unique()\n    unmatched_gt_ids = [c for c in joined_df[\"true_id\"].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    # calc microF1\n    my_f1_score = TP / (TP + 0.5 * (FP + FN))\n    return my_f1_score\n\n\ndef get_score(pred_df, true_df, return_class_scores=False):\n    class_scores = {}\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    for discourse_type, true_subset in true_df.groupby(\"discourse_type\"):\n        pred_subset = (\n            pred_df.loc[pred_df[\"class\"] == discourse_type]\n            .reset_index(drop=True)\n            .copy()\n        )\n        class_score = get_micro_f1_score(pred_subset, true_subset)\n        class_score = np.round(class_score, decimals=4)\n        class_scores[discourse_type] = class_score\n\n    score = np.mean([v for v in class_scores.values()])\n    score = np.round(score, decimals=4)\n\n    if return_class_scores:\n        return score, class_scores\n\n    return score","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:49:37.106816Z","iopub.execute_input":"2022-03-14T12:49:37.107058Z","iopub.status.idle":"2022-03-14T12:49:37.124355Z","shell.execute_reply.started":"2022-03-14T12:49:37.107021Z","shell.execute_reply":"2022-03-14T12:49:37.123421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# main","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('../input/feedback-prize-2021/train.csv')\n\n#test_df = pd.read_csv('../input/fb-train-folds/train_folds.csv')\n#test_df = test_df[test_df['kfold']==fold_num].reset_index(drop=True)\ntest_ids = test_df['id'].unique()\ntest_ids = test_ids[~(test_ids=='AD005493F9BF')]\n# for debug\n# test_ids = test_ids[:100]\ntest_df = test_df[test_df['id'].isin(test_ids)]\ntokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer)\ntest_samples = prepare_test_data(test_ids, tokenizer, cfg.num_jobs)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:49:37.125668Z","iopub.execute_input":"2022-03-14T12:49:37.125886Z","iopub.status.idle":"2022-03-14T12:50:10.913181Z","shell.execute_reply.started":"2022-03-14T12:49:37.12586Z","shell.execute_reply":"2022-03-14T12:50:10.9121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold_df = pd.read_csv('../input/fb-train-folds/train_folds.csv')\nfold_df = fold_df[['id', 'kfold']].drop_duplicates()\nfold_dict = dict(zip(fold_df['id'], fold_df['kfold']))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:50:10.91492Z","iopub.execute_input":"2022-03-14T12:50:10.91597Z","iopub.status.idle":"2022-03-14T12:50:12.560364Z","shell.execute_reply.started":"2022-03-14T12:50:10.915921Z","shell.execute_reply":"2022-03-14T12:50:12.559435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndef create_id_df(samples):\n    \n    id_df = pd.DataFrame()\n    \n    offset_list = []\n    id_list = []\n    num_list = []\n    len_list = []\n        \n    for i in tqdm(range(len(samples))):\n        offset_list += (samples[i]['offset_mapping'] + [(9999,9999)] * (cfg.max_len_test_longformer - len(samples[i]['offset_mapping'])))\n        id_list += [samples[i]['id']] * cfg.max_len_test_longformer\n        num_list += list(range(cfg.max_len_test_longformer))\n        len_list += [len(samples[i]['offset_mapping'])] * cfg.max_len_test_longformer\n        \n    id_df['offset'] = offset_list\n    id_df['id'] = id_list\n    id_df['token_num'] = num_list\n    id_df['token_len'] = len_list\n    id_df['token_key'] = id_df['id'].astype(str) + '_' + id_df['token_num'].astype(str)\n    id_df['offset_start'] = id_df['offset'].map(lambda x: x[0])\n    \n    return id_df","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:50:12.5617Z","iopub.execute_input":"2022-03-14T12:50:12.562009Z","iopub.status.idle":"2022-03-14T12:50:12.570246Z","shell.execute_reply.started":"2022-03-14T12:50:12.561968Z","shell.execute_reply":"2022-03-14T12:50:12.569643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntxt_dict = {}\nfor ids in tqdm(test_ids):\n    txt_dict[ids] = get_test_text(ids)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:50:12.572467Z","iopub.execute_input":"2022-03-14T12:50:12.57309Z","iopub.status.idle":"2022-03-14T12:50:20.372349Z","shell.execute_reply.started":"2022-03-14T12:50:12.573056Z","shell.execute_reply":"2022-03-14T12:50:20.371551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths = []\nscores = []\npaths_ = [\n    #'../input/oof-pred-108/deberta-large-108.pkl',\n    '../input/fb-oof-dict/oof_dict_yyama_exp045.pickle',\n #'../input/fb-oof-dict/oof_dict_makotu_021410-lf-bie-bin.pickle',\n '../input/fb-oof-dict/oof_dict_makabe_feedback-101_LSTM_2head.pickle',\n #'../input/fb-oof-dict/oof_dict_makotu_030209-deberta-large-mnli-boe-bin.pickle',\n '../input/fb-oof-dict/oof_dict_022709-deberta-large-boe-bin.pickle',\n '../input/fb-oof-dict/oof_dict_makabe_feedback-096.pickle',\n '../input/convert-fp64-to-fp16-inference98/oof_dict_makabe_feedback-098_LSTM.pickle',\n '../input/fb-oof-dict/oof_dict_yyama_exp058-wo-mlm.pickle',\n'../input/fb-oof-dict/oof_dict_yyama_fb-exp037-led.pickle',\n'../input/fb-oof-dict-exp062-xlarge-f24/oof_dict_yyama_fb_exp06x_xlarge_fold2-4.pickle']\n\nweight = [1.8, 2.6, 1.3, 2.1, 2.7, 1, 1.6, 1.4]","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:50:40.86918Z","iopub.execute_input":"2022-03-14T12:50:40.86954Z","iopub.status.idle":"2022-03-14T12:50:40.878672Z","shell.execute_reply.started":"2022-03-14T12:50:40.869499Z","shell.execute_reply":"2022-03-14T12:50:40.877736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndef inference(oof, samples, tokenizer, cfg, fold_num=0):\n    \n    samples_ = samples.copy()\n    preds_class, preds_prob = [], []\n    \n    arg_srt = np.argsort(oof, axis=-1)\n    srt = np.sort(oof, axis=-1)\n    \n    #oof_discourse = np.zeros([oof.shape[0], oof.shape[1], 8])\n    #oof_discourse[:, :, 0] = oof[:, :, 0] + oof[:, :, 1]\n    #oof_discourse[:, :, 1] = oof[:, :, 2] + oof[:, :, 3]\n    #oof_discourse[:, :, 2] = oof[:, :, 4] + oof[:, :, 5]\n    #oof_discourse[:, :, 3] = oof[:, :, 6] + oof[:, :, 7]\n    #oof_discourse[:, :, 4] = oof[:, :, 8] + oof[:, :, 9]\n    #oof_discourse[:, :, 5] = oof[:, :, 10] + oof[:, :, 11]\n    #oof_discourse[:, :, 6] = oof[:, :, 12] + oof[:, :, 13]\n    #oof_discourse[:, :, 7] = oof[:, :, 14]\n    \n    #argm = np.argmax(oof, axis=2)\n    #m = np.max(oof, axis=2)\n    \n    argm = arg_srt[:, :, -1]\n    m = srt[:, :, -1]\n    \n    argsecond = arg_srt[:, :, -2]\n    second = srt[:, :, -2]\n    \n    argthird = arg_srt[:, :, -3]\n    third = srt[:, :, -3]\n    \n    #m = np.max(oof_discourse, axis=2)\n    \n\n    for i in tqdm(range(len(samples))):\n        # 先頭0はspecial tokenなので除外\n        text_class = [id_target_map[c] for c in list(argm[i, 1:])]\n        text_prob = list(m[i, 1:])\n        samples_[i]['pred_class'] = text_class\n        samples_[i]['pred_prob'] = text_prob\n        \n        second_class = [id_target_map[c] for c in list(argsecond[i, 1:])]\n        second_prob = list(second[i, 1:])\n        samples_[i]['second_class'] = second_class\n        samples_[i]['second_prob'] = second_prob\n        \n        third_class = [id_target_map[c] for c in list(argthird[i, 1:])]\n        third_prob = list(third[i, 1:])\n        samples_[i]['third_class'] = third_class\n        samples_[i]['third_prob'] = third_prob\n        \n    #samples_all.append(samples_)\n\n    return samples#_all\n\n#sample1 = inference(oof, test_samples, tokenizer, cfg, fold_num=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:52:03.847794Z","iopub.execute_input":"2022-03-14T12:52:03.848063Z","iopub.status.idle":"2022-03-14T12:52:03.857773Z","shell.execute_reply.started":"2022-03-14T12:52:03.848036Z","shell.execute_reply":"2022-03-14T12:52:03.856668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef word_list(text):\n    a = re.split('(\\s+)', text)\n    return [a[i*2] + a[i*2+1] for i in range(len(a)//2)]\n\ndef discourse_offset(text, lst, start, end):\n    words = ''.join(lst[start:end+1])\n    return (text.find(words), text.find(words)+len(words)-1)\n\ndef add_offset(sub):\n    sub['start'] = sub['predictionstring'].map(lambda x: int(x.split()[0]))\n    sub['end'] = sub['predictionstring'].map(lambda x: int(x.split()[-1]))    \n\n    sub['text'] = sub['id'].map(txt_dict)\n    sub['offset'] = sub.apply(lambda x: discourse_offset(x['text'], word_list(x['text']), x['start'], x['end']), axis=1)\n\n    sub['start'] = sub['offset'].map(lambda x: x[0])\n    sub['end'] = sub['offset'].map(lambda x: x[1])\n    sub['discourse_num'] = sub.groupby('id')['class'].cumcount()\n    \n    return sub","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:52:08.146231Z","iopub.execute_input":"2022-03-14T12:52:08.147024Z","iopub.status.idle":"2022-03-14T12:52:08.156045Z","shell.execute_reply.started":"2022-03-14T12:52:08.146987Z","shell.execute_reply":"2022-03-14T12:52:08.155041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import sys\n#sys.path.append(\"../input/pandas-bj/\")\n#!pip install ../input/pandas-bj/pandas_bj-0.1.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:14:51.761921Z","iopub.execute_input":"2022-03-14T12:14:51.762166Z","iopub.status.idle":"2022-03-14T12:14:51.77581Z","shell.execute_reply.started":"2022-03-14T12:14:51.762141Z","shell.execute_reply":"2022-03-14T12:14:51.774824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#oof = oof[oof['token_key'].isin(id_df_2['token_key'])]","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:14:51.777221Z","iopub.execute_input":"2022-03-14T12:14:51.777541Z","iopub.status.idle":"2022-03-14T12:14:51.788106Z","shell.execute_reply.started":"2022-03-14T12:14:51.777422Z","shell.execute_reply":"2022-03-14T12:14:51.787107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%time\n\n#oof['discourse_key'] = oof['token_key'].map(dict(zip(id_df_2['token_key'], id_df_2['discourse_key'])))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:14:51.789659Z","iopub.execute_input":"2022-03-14T12:14:51.789981Z","iopub.status.idle":"2022-03-14T12:14:51.800375Z","shell.execute_reply.started":"2022-03-14T12:14:51.789948Z","shell.execute_reply":"2022-03-14T12:14:51.799496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_oof(path):\n    oof_dict = pickle.load(open(path, 'rb'))\n    \n    if 'xlarge' in path:\n        missing_ids = [id_ for id_ in test_ids if id_ not in oof_dict.keys()]\n        for id_ in missing_ids:\n            oof_dict[id_] = (np.nan * np.zeros([1, cfg.max_len_test_longformer,15])).astype(np.float16)\n    \n    oof = np.vstack([oof_dict[id_] for id_ in test_ids])\n    \n    if '108' in path:\n        oof = oof.astype(np.float16)\n    return oof\n\ndef create_sub_and_id_df(oof):\n    sample1 = inference(oof, test_samples, tokenizer, cfg, fold_num=0)\n    sub = create_submission(sample1)\n    sub = add_offset(sub)\n    \n    id_df_2 = pd.DataFrame()\n    for i in tqdm(range(len(test_ids) // 300 + 1)):\n        a =id_df.iloc[i*300*cfg.max_len_test_longformer:(i+1)*300*cfg.max_len_test_longformer, :].merge(sub[['id', 'discourse_num', 'start', 'end']], on=['id'], how='left').reset_index(drop=True)\n        a = a[(a['offset_start'] >= a['start']) & (a['offset_start'] <= a['end'])]\n        id_df_2 = id_df_2.append(a)\n        \n    id_df_2['discourse_key'] = id_df_2['id'].astype(str) + '_' + id_df_2['discourse_num'].astype(str)\n    sub['discourse_key'] = sub['id'].astype(str) + '_' + sub['discourse_num'].astype(str)\n    \n    return sub, id_df_2","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:52:11.066001Z","iopub.execute_input":"2022-03-14T12:52:11.066739Z","iopub.status.idle":"2022-03-14T12:52:11.078669Z","shell.execute_reply.started":"2022-03-14T12:52:11.06667Z","shell.execute_reply":"2022-03-14T12:52:11.077728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_prob_agg_df(oof):\n    value_list = [k for k in list(target_id_map.keys()) if k != 'PAD']\n\n    dfg = oof.groupby('discourse_key')[value_list]\n\n    dfg_stats = dfg.agg([np.mean, np.max, np.min]).stack()\n    dfg_quantiles = dfg.quantile([0.2, 0.8])\n\n    dfg_stats = dfg_stats.append(dfg_quantiles).sort_index()\n    prob_agg_df = dfg_stats.unstack().astype(np.float16)\n\n    prob_agg_df.columns = [col[0] + '_' + str(col[1]) for col in prob_agg_df.columns.values]\n    \n    print(dfg.head())\n    \n    return prob_agg_df\n\n#oof[:480000].groupby('discourse_key').agg({k: ['mean', 'min', 'max', q20, q80] for k in list(target_id_map.keys())[:1] if k != 'PAD'})","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:52:13.529315Z","iopub.execute_input":"2022-03-14T12:52:13.529761Z","iopub.status.idle":"2022-03-14T12:52:13.537464Z","shell.execute_reply.started":"2022-03-14T12:52:13.529719Z","shell.execute_reply":"2022-03-14T12:52:13.536749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%time\n#prob_agg_df = create_prob_agg_df(oof)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:14:51.834656Z","iopub.execute_input":"2022-03-14T12:14:51.836093Z","iopub.status.idle":"2022-03-14T12:14:51.84575Z","shell.execute_reply.started":"2022-03-14T12:14:51.836051Z","shell.execute_reply":"2022-03-14T12:14:51.844784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_feats(path, id_df_2):\n    #path = '../input/fb-oof-dict/oof_dict_022709-deberta-large-boe-bin.pickle'\n    oof = create_oof(path)\n\n    oof = pd.DataFrame(oof.reshape(oof.shape[0] * oof.shape[1], oof.shape[2]))\n    model_name = path.split('/')[-1].replace(\".pickle\", \"\").replace(\"oof_dict_\", \"\")\n\n    oof.columns = oof.columns.map(id_target_map)\n\n    oof['token_key'] = id_df['token_key']\n    oof = oof[oof['token_key'].isin(id_df_2['token_key'])]\n    oof['discourse_key'] = oof['token_key'].map(dict(zip(id_df_2['token_key'], id_df_2['discourse_key'])))\n\n    prob_agg_df = create_prob_agg_df(oof)\n    prob_agg_df.columns = [model_name + '_' + col for col in prob_agg_df.columns]\n    return prob_agg_df","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:52:16.76969Z","iopub.execute_input":"2022-03-14T12:52:16.769943Z","iopub.status.idle":"2022-03-14T12:52:16.777321Z","shell.execute_reply.started":"2022-03-14T12:52:16.769916Z","shell.execute_reply":"2022-03-14T12:52:16.776282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_sub_and_feats(path, model_paths):\n    oof = create_oof(path)\n    sub, id_df_2 = create_sub_and_id_df(oof)\n\n    #feats_list = []\n    for model_path in model_paths:\n        feats  = create_feats(model_path, id_df_2)\n        sub = sub.merge(feats.reset_index(), on='discourse_key', how='left')\n        #feats_list.append(feats)\n    #feats = pd.concat(feats_list, axis=1)\n    del id_df_2\n    \n    sub['text_len'] = sub['text'].map(lambda x: len(x))\n    \n    return sub","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:52:18.785216Z","iopub.execute_input":"2022-03-14T12:52:18.785528Z","iopub.status.idle":"2022-03-14T12:52:18.791524Z","shell.execute_reply.started":"2022-03-14T12:52:18.785498Z","shell.execute_reply":"2022-03-14T12:52:18.790435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_ensemble_sub_and_feats(model_paths, weight):\n    \n    oof = np.zeros([len(test_ids), cfg.max_len_test_longformer , 15])\n    for path, w in zip(model_paths, weight):\n        oof += create_oof(path) * w / np.sum(weight)\n    \n    sub, id_df_2 = create_sub_and_id_df(oof)\n\n    #feats_list = []\n    for model_path in model_paths:\n        feats  = create_feats(model_path, id_df_2)\n        sub = sub.merge(feats.reset_index(), on='discourse_key', how='left')\n        #feats_list.append(feats)\n    #feats = pd.concat(feats_list, axis=1)\n    del id_df_2\n    \n    sub['text_len'] = sub['text'].map(lambda x: len(x))\n    \n    return sub","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:52:20.777317Z","iopub.execute_input":"2022-03-14T12:52:20.778182Z","iopub.status.idle":"2022-03-14T12:52:20.784207Z","shell.execute_reply.started":"2022-03-14T12:52:20.778132Z","shell.execute_reply":"2022-03-14T12:52:20.78355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def post_process_sub(sub):\n    sub = thresh_prob(sub, cfg)\n    sub = thresh_min_token(sub, cfg)\n    sub = get_max_prob(sub)\n    sub = link_class(sub, 'Evidence', cfg.link['Evidence'])\n    sub = link_class(sub, 'Counterclaim', cfg.link['Counterclaim'])\n    sub = link_class(sub, 'Rebuttal', cfg.link['Rebuttal'])\n    sub = sub.reset_index(drop=True)\n    sub = sub[['id', 'class', 'predictionstring', 'prob', 'len']]\n    return sub","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:52:22.994546Z","iopub.execute_input":"2022-03-14T12:52:22.995097Z","iopub.status.idle":"2022-03-14T12:52:23.000208Z","shell.execute_reply.started":"2022-03-14T12:52:22.995057Z","shell.execute_reply":"2022-03-14T12:52:22.999546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths = []\nscores = []\npaths_ = [\n    #'../input/oof-pred-108/deberta-large-108.pkl',\n    '../input/fb-oof-dict/oof_dict_yyama_exp045.pickle',\n #'../input/fb-oof-dict/oof_dict_makotu_021410-lf-bie-bin.pickle',\n '../input/fb-oof-dict/oof_dict_makabe_feedback-101_LSTM_2head.pickle',\n #'../input/fb-oof-dict/oof_dict_makotu_030209-deberta-large-mnli-boe-bin.pickle',\n '../input/fb-oof-dict/oof_dict_022709-deberta-large-boe-bin.pickle',\n '../input/fb-oof-dict/oof_dict_makabe_feedback-096.pickle',\n '../input/convert-fp64-to-fp16-inference98/oof_dict_makabe_feedback-098_LSTM.pickle',\n '../input/fb-oof-dict/oof_dict_yyama_exp058-wo-mlm.pickle',\n'../input/fb-oof-dict/oof_dict_yyama_fb-exp037-led.pickle',\n'../input/fb-oof-dict-exp062-xlarge-f24/oof_dict_yyama_fb_exp06x_xlarge_fold2-4.pickle']\n\nweight = [1.1, 2.6, 1.3, 2.1, 2.7, 1, 1.6, 1.4]","metadata":{"execution":{"iopub.status.busy":"2022-03-14T13:44:52.112691Z","iopub.execute_input":"2022-03-14T13:44:52.112983Z","iopub.status.idle":"2022-03-14T13:54:29.733429Z","shell.execute_reply.started":"2022-03-14T13:44:52.112954Z","shell.execute_reply":"2022-03-14T13:54:29.732467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# exp045 1.8\n(0.7158, {'Claim': 0.6847, 'Concluding Statement': 0.8736, 'Counterclaim': 0.5908, 'Evidence': 0.7785, 'Lead': 0.844, 'Position': 0.7418, 'Rebuttal': 0.4974})\n\n# exp045 1.1\n(0.7162, {'Claim': 0.6849, 'Concluding Statement': 0.8734, 'Counterclaim': 0.5902, 'Evidence': 0.7788, 'Lead': 0.8441, 'Position': 0.7419, 'Rebuttal': 0.5})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nid_df = create_id_df(test_samples)\nfor path in paths_:\n    model_name = path.split('/')[-1].replace(\".pickle\", \"\").replace(\"oof_dict_\", \"\")\n    print(model_name)\n    sub = create_sub_and_feats(path, paths_)\n    sub['model_name'] = model_name\n    sub['fold'] = sub['id'].map(fold_dict)\n    sub.to_pickle(f'feats_{model_name}.pkl')\n    \n    #post_sub = post_process_sub(sub[sub.fold.isin([2,3])][['id', 'class', 'predictionstring', 'prob', 'len']])\n    #print(get_score(post_sub, test_df, return_class_scores=True))\n    \n    del sub\n#    del sub","metadata":{"execution":{"iopub.status.busy":"2022-03-09T23:24:33.467942Z","iopub.execute_input":"2022-03-09T23:24:33.468613Z","iopub.status.idle":"2022-03-09T23:50:36.724189Z","shell.execute_reply.started":"2022-03-09T23:24:33.468571Z","shell.execute_reply":"2022-03-09T23:50:36.723503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntest_df = pd.read_csv('../input/feedback-prize-2021/train.csv')\ntest_ids = test_df['id'].unique()\ntest_ids = test_ids[~(test_ids=='AD005493F9BF')]\ntest_df = test_df[test_df['id'].isin(test_ids)]\ntest_df['fold'] = test_df['id'].map(fold_dict)\ntest_df = test_df[test_df.fold.isin([2,3,4])]\n\n#test_df = test_df[test_df['id'].isin(test_df['id'].unique()[:10])]\n\ntest_ids = test_df['id'].unique()\n\ntokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer)\ntest_samples = prepare_test_data(test_ids, tokenizer, cfg.num_jobs)\nid_df = create_id_df(test_samples)\n\nsub = create_ensemble_sub_and_feats(paths_, weight)\nsub['fold'] = sub['id'].map(fold_dict)\nsub.to_pickle(f'feats_ensemble_0314.pkl')\n\npost_sub = post_process_sub(sub[sub.fold.isin([2,3,4])][['id', 'class', 'predictionstring', 'prob', 'len']])\nprint(get_score(post_sub, test_df, return_class_scores=True))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:18:50.480143Z","iopub.execute_input":"2022-03-14T12:18:50.480749Z","iopub.status.idle":"2022-03-14T12:39:26.909881Z","shell.execute_reply.started":"2022-03-14T12:18:50.480705Z","shell.execute_reply":"2022-03-14T12:39:26.90865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"execution":{"iopub.status.busy":"2022-03-14T03:18:56.041801Z","iopub.execute_input":"2022-03-14T03:18:56.042081Z","iopub.status.idle":"2022-03-14T03:18:56.077884Z","shell.execute_reply.started":"2022-03-14T03:18:56.04205Z","shell.execute_reply":"2022-03-14T03:18:56.076731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#weight = [0.5, 1, 1, 1, 1, 1, 1.5]\n(0.714, {'Claim': 0.6843, 'Concluding Statement': 0.8727, 'Counterclaim': 0.5866, 'Evidence': 0.7808, 'Lead': 0.8431, 'Position': 0.7423, 'Rebuttal': 0.4881})\n\n#weight = [0.5, 1, 1, 1, 1, 1, 0]\n(0.7136, {'Claim': 0.685, 'Concluding Statement': 0.8722, 'Counterclaim': 0.5858, 'Evidence': 0.7806, 'Lead': 0.8438, 'Position': 0.7385, 'Rebuttal': 0.4891})\n\n#weight = [0.5, 1, 1, 1, 1, 1, 0.75]\n(0.7141, {'Claim': 0.6849, 'Concluding Statement': 0.8728, 'Counterclaim': 0.5866, 'Evidence': 0.7808, 'Lead': 0.8441, 'Position': 0.7418, 'Rebuttal': 0.4879})\n\n#weight = [0.5, 1, 1, 1, 1, 1, 0.75]\n(0.7135, {'Claim': 0.6844, 'Concluding Statement': 0.8714, 'Counterclaim': 0.5856, 'Evidence': 0.7806, 'Lead': 0.8429, 'Position': 0.7413, 'Rebuttal': 0.488})\n","metadata":{},"execution_count":null,"outputs":[]}]}