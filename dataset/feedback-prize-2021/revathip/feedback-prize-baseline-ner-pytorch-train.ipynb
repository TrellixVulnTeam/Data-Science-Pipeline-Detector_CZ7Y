{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### A simple Roberta hugging face NER based approach","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-18T22:38:12.696665Z","iopub.execute_input":"2021-12-18T22:38:12.697353Z","iopub.status.idle":"2021-12-18T22:38:23.246297Z","shell.execute_reply.started":"2021-12-18T22:38:12.697314Z","shell.execute_reply":"2021-12-18T22:38:23.24546Z"}}},{"cell_type":"markdown","source":"I have taken ideas from this notebook\nhttps://www.kaggle.com/zzy990106/pytorch-ner-infer.\n\nI have created the baseline traning pipeline based on roberta for token classification.\n\nThis is the training notebook.\n\nFind the prediction notebook here: https://www.kaggle.com/revathiprakash/feedback-prize-baseline-roberta-pytorch-wip/edit/run/82697901\n","metadata":{}},{"cell_type":"code","source":"import random\nimport os\nimport time\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm import tqdm\nfrom sklearn.model_selection import *\nfrom transformers import *\n","metadata":{"execution":{"iopub.status.busy":"2021-12-20T02:23:47.25895Z","iopub.execute_input":"2021-12-20T02:23:47.259266Z","iopub.status.idle":"2021-12-20T02:23:51.781122Z","shell.execute_reply.started":"2021-12-20T02:23:47.259188Z","shell.execute_reply":"2021-12-20T02:23:51.780274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG = {\n    'fold_num': 5, \n    'seed': 42,\n    'model': '../input/roberta-base',\n    'max_len': 512,\n    'epochs': 3,\n    'train_bs': 16,\n    'valid_bs': 32,\n    'lr': 1e-4,\n    'num_workers': 0,\n    'weight_decay': 1e-5,\n}","metadata":{"execution":{"iopub.status.busy":"2021-12-20T02:23:51.786976Z","iopub.execute_input":"2021-12-20T02:23:51.787462Z","iopub.status.idle":"2021-12-20T02:23:51.793076Z","shell.execute_reply.started":"2021-12-20T02:23:51.787419Z","shell.execute_reply":"2021-12-20T02:23:51.792423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(CFG['seed'])\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-12-20T02:23:51.794639Z","iopub.execute_input":"2021-12-20T02:23:51.794896Z","iopub.status.idle":"2021-12-20T02:23:51.827519Z","shell.execute_reply.started":"2021-12-20T02:23:51.794862Z","shell.execute_reply":"2021-12-20T02:23:51.826831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_list = ['o', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\nlabel_encoding_dict = {'o': 1,\n                       'B-Lead': 2,\n                       'I-Lead': 3,\n                       'B-Position': 4,\n                       'I-Position': 5,\n                       'B-Claim': 6,\n                       'I-Claim': 7,\n                       'B-Counterclaim': 8, 'I-Counterclaim': 9,\n                       'B-Rebuttal': 10, 'I-Rebuttal': 11,\n                       'B-Evidence': 12, 'I-Evidence': 13, 'B-Concluding Statement': 14,\n                       'I-Concluding Statement' :15\n                       \n                      }\n","metadata":{"execution":{"iopub.status.busy":"2021-12-20T02:23:51.829915Z","iopub.execute_input":"2021-12-20T02:23:51.830182Z","iopub.status.idle":"2021-12-20T02:23:51.836545Z","shell.execute_reply.started":"2021-12-20T02:23:51.830148Z","shell.execute_reply":"2021-12-20T02:23:51.835862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG['model'], add_prefix_space=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T02:23:51.837888Z","iopub.execute_input":"2021-12-20T02:23:51.83839Z","iopub.status.idle":"2021-12-20T02:23:51.93677Z","shell.execute_reply.started":"2021-12-20T02:23:51.838352Z","shell.execute_reply":"2021-12-20T02:23:51.936018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('../input/feedback-train/finaldata.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-20T02:23:51.93815Z","iopub.execute_input":"2021-12-20T02:23:51.938435Z","iopub.status.idle":"2021-12-20T02:23:52.542774Z","shell.execute_reply.started":"2021-12-20T02:23:51.938399Z","shell.execute_reply":"2021-12-20T02:23:52.542049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_names, train_texts = [], []\nfor f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n    train_names.append(f.replace('.txt', ''))\n    train_texts.append(open('../input/feedback-prize-2021/train/' + f, 'r').read())\ntrain_texts = pd.DataFrame({'id': train_names, 'text': train_texts})\ntrain_texts['text'] = train_texts['text'].apply(lambda x:x.split())","metadata":{"execution":{"iopub.status.busy":"2021-12-20T02:23:52.54406Z","iopub.execute_input":"2021-12-20T02:23:52.544332Z","iopub.status.idle":"2021-12-20T02:23:59.186064Z","shell.execute_reply.started":"2021-12-20T02:23:52.544296Z","shell.execute_reply":"2021-12-20T02:23:59.185311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_texts=pd.merge(train_texts, train[['id','token_class_merged']], how='left', on =['id'])\ntrain_texts['token_class_merged']=train_texts['token_class_merged'].apply(lambda x:(str(x)[1:-1]).split(','))\ntrain_texts.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T02:23:59.187543Z","iopub.execute_input":"2021-12-20T02:23:59.187794Z","iopub.status.idle":"2021-12-20T02:24:00.34299Z","shell.execute_reply.started":"2021-12-20T02:23:59.18776Z","shell.execute_reply":"2021-12-20T02:24:00.342156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=train_texts.iloc[1:13000,]\ntest_df=train_texts.iloc[13001:,]","metadata":{"execution":{"iopub.status.busy":"2021-12-20T02:24:00.344302Z","iopub.execute_input":"2021-12-20T02:24:00.34459Z","iopub.status.idle":"2021-12-20T02:24:00.350493Z","shell.execute_reply.started":"2021-12-20T02:24:00.34455Z","shell.execute_reply":"2021-12-20T02:24:00.349259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_to_id = {l: i for i, l in enumerate(label_list)}\nb_to_i_label = []\nfor idx, label in enumerate(label_list):\n    if label.startswith(\"B-\") and label.replace(\"B-\", \"I-\") in label_list:\n        b_to_i_label.append(label_list.index(label.replace(\"B-\", \"I-\")))\n    else:\n        b_to_i_label.append(idx)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T02:24:00.352143Z","iopub.execute_input":"2021-12-20T02:24:00.353388Z","iopub.status.idle":"2021-12-20T02:24:00.361631Z","shell.execute_reply.started":"2021-12-20T02:24:00.353339Z","shell.execute_reply":"2021-12-20T02:24:00.360864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    label_all_tokens = True\n    tokenized_inputs = tokenizer(list(examples[\"text\"]), truncation=True, is_split_into_words=True,\n                                 max_length=CFG['max_len'])\n\n    labels = []\n    for i, label in enumerate(examples['token_class_merged']):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif label[word_idx] == '0':\n                label_ids.append(0)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label_to_id[label[word_idx]])\n            else:\n                label_ids.append(label_to_id[label[word_idx]] if label_all_tokens else -100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n        \n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n","metadata":{"execution":{"iopub.status.busy":"2021-12-20T02:24:00.363364Z","iopub.execute_input":"2021-12-20T02:24:00.364028Z","iopub.status.idle":"2021-12-20T02:24:00.373823Z","shell.execute_reply.started":"2021-12-20T02:24:00.363983Z","shell.execute_reply":"2021-12-20T02:24:00.372954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install seqeval","metadata":{"execution":{"iopub.status.busy":"2021-12-20T02:24:00.374925Z","iopub.execute_input":"2021-12-20T02:24:00.377512Z","iopub.status.idle":"2021-12-20T02:24:07.903883Z","shell.execute_reply.started":"2021-12-20T02:24:00.377299Z","shell.execute_reply":"2021-12-20T02:24:07.903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#uncomment for training\nfrom datasets import Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\ntrain_tokenized_datasets = train_dataset.map(tokenize_and_align_labels, batched=True)\ntest_tokenized_datasets = test_dataset.map(tokenize_and_align_labels, batched=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T02:24:07.908005Z","iopub.execute_input":"2021-12-20T02:24:07.908726Z","iopub.status.idle":"2021-12-20T02:25:15.81784Z","shell.execute_reply.started":"2021-12-20T02:24:07.908672Z","shell.execute_reply":"2021-12-20T02:25:15.817124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## training loop\nfrom datasets import load_metric\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nmodel = AutoModelForTokenClassification.from_pretrained(CFG['model'], num_labels=len(label_list))\nargs = TrainingArguments(\n    \"test-ner\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=CFG['lr'],\n    per_device_train_batch_size=CFG['train_bs'],\n    per_device_eval_batch_size=CFG['valid_bs'],\n    num_train_epochs=CFG['epochs'],\n    weight_decay=CFG['weight_decay'],\n)\n\ndata_collator = DataCollatorForTokenClassification(tokenizer)\nmetric = load_metric(\"seqeval\")\n\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"]}\n    \ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=train_tokenized_datasets,\n    eval_dataset=test_tokenized_datasets,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-20T02:25:15.819097Z","iopub.execute_input":"2021-12-20T02:25:15.819392Z","iopub.status.idle":"2021-12-20T03:05:26.0931Z","shell.execute_reply.started":"2021-12-20T02:25:15.819354Z","shell.execute_reply":"2021-12-20T03:05:26.092255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.evaluate()\ntrainer.save_model('roberta_v1.model')\ntorch.save(model.state_dict(), './roberta-baseline.pt')\n","metadata":{"execution":{"iopub.status.busy":"2021-12-20T03:05:26.094675Z","iopub.execute_input":"2021-12-20T03:05:26.094948Z","iopub.status.idle":"2021-12-20T03:06:55.283824Z","shell.execute_reply.started":"2021-12-20T03:05:26.094912Z","shell.execute_reply":"2021-12-20T03:06:55.283084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### Please upvote if you find the notebook useful","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}