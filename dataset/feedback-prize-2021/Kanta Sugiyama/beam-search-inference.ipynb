{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Beam search inference\n\nI didn't have enough computational resources, so I improved only the inference of https://www.kaggle.com/abhishek/two-longformers-are-better-than-1 .  \nI defined the evaluation function and searched for the prediction that maximized it by using beam search.","metadata":{}},{"cell_type":"code","source":"import gc\ngc.enable()\n\nimport sys\nsys.path.append(\"../input/tez-lib/\")\n\nimport os\nimport bisect\nimport copy\nimport time\n\nfrom scipy.stats import norm\nimport numpy as np\nimport pandas as pd\nimport tez\nimport torch\nimport torch.nn as nn\nfrom joblib import Parallel, delayed\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer","metadata":{"_uuid":"66c995cf-7cbb-421b-bdd3-24906f5fb886","_cell_guid":"e0e4bdf6-bcd9-4f82-a53b-9c5f685500a8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-09T14:07:15.84779Z","iopub.execute_input":"2022-02-09T14:07:15.848176Z","iopub.status.idle":"2022-02-09T14:07:24.2602Z","shell.execute_reply.started":"2022-02-09T14:07:15.848146Z","shell.execute_reply":"2022-02-09T14:07:24.25928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_id_map = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 2,\n    \"I-Position\": 3,\n    \"B-Evidence\": 4,\n    \"I-Evidence\": 5,\n    \"B-Claim\": 6,\n    \"I-Claim\": 7,\n    \"B-Concluding Statement\": 8,\n    \"I-Concluding Statement\": 9,\n    \"B-Counterclaim\": 10,\n    \"I-Counterclaim\": 11,\n    \"B-Rebuttal\": 12,\n    \"I-Rebuttal\": 13,\n    \"O\": 14,\n    \"PAD\": -100,\n}\n\n\nid_target_map = {v: k for k, v in target_id_map.items()}\n\nclass args1:\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/longformerlarge4096/longformer-large-4096/\"\n    tez_model= \"../input/fblongformerlarge1536/\"\n    output = \".\"\n    batch_size = 8\n    max_len = 4096\n    \nclass args2:\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/longformerlarge4096/longformer-large-4096/\"\n    tez_model= \"../input/tez-fb-large/\"\n    output = \".\"\n    batch_size = 8\n    max_len = 4096","metadata":{"execution":{"iopub.status.busy":"2022-02-09T14:07:24.502136Z","iopub.execute_input":"2022-02-09T14:07:24.502469Z","iopub.status.idle":"2022-02-09T14:07:24.513577Z","shell.execute_reply.started":"2022-02-09T14:07:24.502437Z","shell.execute_reply":"2022-02-09T14:07:24.512513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackDataset:\n    def __init__(self, samples, max_len, tokenizer):\n        self.samples = samples\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.length = len(samples)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        input_ids = self.samples[idx][\"input_ids\"]\n        # print(input_ids)\n        # print(input_labels)\n\n        # add start token id to the input_ids\n        input_ids = [self.tokenizer.cls_token_id] + input_ids\n\n        if len(input_ids) > self.max_len - 1:\n            input_ids = input_ids[: self.max_len - 1]\n\n        # add end token id to the input_ids\n        input_ids = input_ids + [self.tokenizer.sep_token_id]\n        attention_mask = [1] * len(input_ids)\n\n        return {\n            \"ids\": input_ids,\n            \"mask\": attention_mask,\n        }","metadata":{"execution":{"iopub.status.busy":"2022-02-09T14:07:30.961967Z","iopub.execute_input":"2022-02-09T14:07:30.962269Z","iopub.status.idle":"2022-02-09T14:07:30.971497Z","shell.execute_reply.started":"2022-02-09T14:07:30.96223Z","shell.execute_reply":"2022-02-09T14:07:30.970314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Collate:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"ids\"]])\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n        else:\n            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n\n        # convert to tensors\n        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-02-09T14:07:31.368327Z","iopub.execute_input":"2022-02-09T14:07:31.3689Z","iopub.status.idle":"2022-02-09T14:07:31.380525Z","shell.execute_reply.started":"2022-02-09T14:07:31.368839Z","shell.execute_reply":"2022-02-09T14:07:31.378962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackModel(tez.Model):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n        config = AutoConfig.from_pretrained(model_name)\n\n        hidden_dropout_prob: float = 0.18\n        layer_norm_eps: float = 17589e-7\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = AutoModel.from_config(config)\n        self.output = nn.Linear(config.hidden_size, self.num_labels)\n\n    def forward(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        logits = self.output(sequence_output)\n        logits = torch.softmax(logits, dim=-1)\n        return logits, 0, {}","metadata":{"execution":{"iopub.status.busy":"2022-02-09T14:07:32.053261Z","iopub.execute_input":"2022-02-09T14:07:32.054291Z","iopub.status.idle":"2022-02-09T14:07:32.065619Z","shell.execute_reply.started":"2022-02-09T14:07:32.054243Z","shell.execute_reply":"2022-02-09T14:07:32.064481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _prepare_test_data_helper(args, tokenizer, ids):\n    test_samples = []\n    for idx in ids:\n        filename = os.path.join(args.input_path, \"test\", idx + \".txt\")\n        with open(filename, \"r\") as f:\n            text = f.read()\n\n        encoded_text = tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        offset_mapping = encoded_text[\"offset_mapping\"]\n\n        sample = {\n            \"id\": idx,\n            \"input_ids\": input_ids,\n            \"text\": text,\n            \"offset_mapping\": offset_mapping,\n        }\n\n        test_samples.append(sample)\n    return test_samples\n\n\ndef prepare_test_data(df, tokenizer, args):\n    test_samples = []\n    ids = df[\"id\"].unique()\n    ids_splits = np.array_split(ids, 4)\n\n    results = Parallel(n_jobs=4, backend=\"multiprocessing\")(\n        delayed(_prepare_test_data_helper)(args, tokenizer, idx) for idx in ids_splits\n    )\n    for result in results:\n        test_samples.extend(result)\n\n    return test_samples","metadata":{"execution":{"iopub.status.busy":"2022-02-09T14:07:32.885863Z","iopub.execute_input":"2022-02-09T14:07:32.886291Z","iopub.status.idle":"2022-02-09T14:07:32.898309Z","shell.execute_reply.started":"2022-02-09T14:07:32.886256Z","shell.execute_reply":"2022-02-09T14:07:32.896497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(os.path.join(\"../input/feedback-prize-2021/\", \"sample_submission.csv\"))\ndf_ids = df[\"id\"].unique()\n\ntokenizer = AutoTokenizer.from_pretrained(args1.model)\ntest_samples = prepare_test_data(df, tokenizer, args1)\ncollate = Collate(tokenizer=tokenizer)\n\nraw_preds = []\nn_fold = 10\nfor fold_ in range(n_fold):\n    current_idx = 0\n    test_dataset = FeedbackDataset(test_samples, args1.max_len, tokenizer)\n    \n    if fold_ < 5:\n        model = FeedbackModel(model_name=args1.model, num_labels=len(target_id_map) - 1)\n        model.load(os.path.join(args1.tez_model, f\"model_{fold_}.bin\"), weights_only=True)\n        \n        preds_iter = model.predict(test_dataset, batch_size=args1.batch_size, n_jobs=-1, collate_fn=collate)\n    else:\n        model = FeedbackModel(model_name=args2.model, num_labels=len(target_id_map) - 1)\n        model.load(os.path.join(args2.tez_model, f\"model_{fold_-5}.bin\"), weights_only=True)\n        \n        preds_iter = model.predict(test_dataset, batch_size=args2.batch_size, n_jobs=-1, collate_fn=collate)\n        \n    current_idx = 0\n    \n    for preds in preds_iter:\n        preds = preds.astype(np.float16)\n        preds = preds / n_fold\n        if fold_ == 0:\n            raw_preds.append(preds)\n        else:\n            raw_preds[current_idx] += preds\n            current_idx += 1\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T14:14:37.573628Z","iopub.execute_input":"2022-02-09T14:14:37.574663Z","iopub.status.idle":"2022-02-09T14:15:22.855386Z","shell.execute_reply.started":"2022-02-09T14:14:37.574628Z","shell.execute_reply":"2022-02-09T14:15:22.853517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Beam search","metadata":{}},{"cell_type":"code","source":"B_MIX_RATIO = 0.5\nBEAM_WIDTH = 3\ndef beam_inference(raw_preds, samples, transition_cost=0.08):\n    target_id_map = {\n        \"B-Lead\": 0,\n        \"I-Lead\": 1,\n        \"B-Position\": 2,\n        \"I-Position\": 3,\n        \"B-Evidence\": 4,\n        \"I-Evidence\": 5,\n        \"B-Claim\": 6,\n        \"I-Claim\": 7,\n        \"B-Concluding Statement\": 8,\n        \"I-Concluding Statement\": 9,\n        \"B-Counterclaim\": 10,\n        \"I-Counterclaim\": 11,\n        \"B-Rebuttal\": 12,\n        \"I-Rebuttal\": 13,\n        \"O\": 14,\n        \"PAD\": -100,\n    }\n    target_id_map2 = {\n        \"init\": 0,\n        \"Lead\": 1,\n        \"Position\": 2,\n        \"Evidence\": 3,\n        \"Claim\": 4,\n        \"Concluding Statement\": 5,\n        \"Counterclaim\": 6,\n        \"Rebuttal\": 7,\n        \"O\": 8,\n    }\n    target_id_map_onlyclass = {\n        \"Lead\": 0,\n        \"Position\": 1,\n        \"Evidence\": 2,\n        \"Claim\": 3,\n        \"Concluding Statement\": 4,\n        \"Counterclaim\": 5,\n        \"Rebuttal\": 6,\n        \"O\": 7,\n    }\n    id_target_map = {v: k for k, v in target_id_map.items()}\n    id_target_map2 = {v: k for k, v in target_id_map2.items()}\n    id_target_map_onlyclass = {v: k for k, v in target_id_map_onlyclass.items()}\n\n    norm0 = norm.pdf(0)\n\n    proba_thresh = {\n        \"Lead\": 0.7,\n        \"Position\": 0.55,\n        \"Evidence\": 0.65,\n        \"Claim\": 0.55,\n        \"Concluding Statement\": 0.7,\n        \"Counterclaim\": 0.5,\n        \"Rebuttal\": 0.55,\n    }\n\n    for k, v in proba_thresh.items():\n        proba_thresh[k] = v * 0.85\n\n    original_min_thresh = {\n        \"Lead\": 9,\n        \"Position\": 5,\n        \"Evidence\": 14,\n        \"Claim\": 3,\n        \"Concluding Statement\": 11,\n        \"Counterclaim\": 6,\n        \"Rebuttal\": 4,\n        \"O\": -100\n    }\n    min_thresh = {}\n\n    for k, v in original_min_thresh.items():\n        min_thresh[k] = v // 2\n\n    final_preds = []\n    final_scores = []\n    final_raw_preds = []\n    for rp in raw_preds:\n        pred_class = np.argmax(rp, axis=2)\n        pred_scrs = np.max(rp, axis=2)\n        for pred, pred_scr, rp_each in zip(pred_class, pred_scrs, rp):\n            pred = pred.tolist()\n            pred_scr = pred_scr.tolist()\n            final_preds.append(pred)\n            final_scores.append(pred_scr)\n            final_raw_preds.append(rp_each)\n\n    transition_arr = np.loadtxt(\"../input/th-arr/transition_arr.csv\")\n    \n    total_time1 = 0.0\n    total_time2 = 0.0\n    score_list = []\n    for j in range(len(samples)):\n        print(f\"\\r {j}/{len(samples)}\", end=\"\")\n        raw_pred = final_raw_preds[j][1:]\n        if j == 0:\n            print(raw_pred.shape)\n\n        division_cand_list = [0]\n        init_division_list = [0]\n        pre = id_target_map[np.argmax(raw_pred[0])]\n        in_O = False\n        for k, rp_each in enumerate(raw_pred):\n            if k == 0:\n                continue\n\n            pred = id_target_map[np.argmax(rp_each)]\n            rp_B_each = rp_each[[0, 2, 4, 6, 8, 10, 12]]\n\n            if pred == \"O\" and pre != \"O\":\n                division_cand_list.append(k)\n                if rp_each[target_id_map[\"O\"]] >= 0.9:\n                    init_division_list.append(k)\n                    in_O = True\n            elif pred != \"O\" and pre == \"O\":\n                division_cand_list.append(k)\n                if in_O:\n                    init_division_list.append(k)\n                    in_O = False\n            elif np.max(rp_B_each) >= 0.1:\n                division_cand_list.append(k)\n\n            pre = pred\n\n        division_cand_list.append(raw_pred.shape[0])\n        init_division_list.append(raw_pred.shape[0])\n\n        def total_score(raw_pred, id_pred, transition_cost=transition_cost):\n            pre = -1\n            pre_state = \"init\"\n            transition_score = 0.0\n            count = 0\n            for i, id_ in enumerate(id_pred):\n                count += 1\n                if pre != id_:\n                    transition_score -= transition_cost\n                    state = id_target_map[id_]\n                    if state != \"O\":\n                        state = state[2:]\n                    if pre == -1 or not (id_target_map[pre].startswith(\"B-\") and id_target_map[id_].startswith(\"I-\")):\n                        transition_score += 0.2 * transition_arr[target_id_map2[pre_state], target_id_map2[state]]\n                        transition_score -= 0.2 * max(0, original_min_thresh[state] - count)\n                    count = 0\n                pre = id_\n                pre_state = state\n\n            return np.sum(raw_pred[np.arange(len(id_pred)), id_pred]) + transition_score\n\n        modified_raw_pred = np.copy(raw_pred)\n        modified_raw_pred[:, [0, 2, 4, 6, 8, 10, 12]] *= B_MIX_RATIO\n        log_raw_pred = -np.log(1.0 + 1e-3 - raw_pred)\n        modified_log_raw_pred = np.copy(log_raw_pred)\n        modified_log_raw_pred[:, [0, 2, 4, 6, 8, 10, 12]] *= B_MIX_RATIO\n        modified_log_raw_pred *= 1.0\n        raw_pred_I = raw_pred[:, [1, 3, 5, 7, 9, 11, 13, 14]]\n        raw_pred_B = raw_pred[:, [0, 2, 4, 6, 8, 10, 12, 14]]\n        log_raw_pred_I = log_raw_pred[:, [1, 3, 5, 7, 9, 11, 13, 14]]\n        log_raw_pred_B = log_raw_pred[:, [0, 2, 4, 6, 8, 10, 12, 14]]\n\n        # Calculate sum in advance for speed up\n        sum_dict = {}\n        for i1, s in enumerate(division_cand_list[:-1]):\n            for i2, e in enumerate(division_cand_list[i1 + 1:]):\n                sum_dict[(s, e)] = np.sum(raw_pred_I[s:e], axis=0)\n\n        division_list = init_division_list\n        best_division_list = division_list\n        max_score = 0.0\n        exists_update = True\n        division_beam = [(max_score, tuple(division_list))]\n        while len(division_list) < len(division_cand_list):\n            s_time = time.time()\n            exists_update = False\n\n            next_division_beam = []\n            for _, division_tuple in division_beam:\n                division_list = list(division_tuple)\n                diff = set(division_cand_list) - set(division_list)\n                for x in diff:\n                    new_division_list = copy.copy(division_list)\n                    index = bisect.bisect_left(new_division_list, x)\n                    new_division_list.insert(index, x)\n                    pred_list = []\n\n                    time2 = time.time()\n                    for s, e in zip(new_division_list[:-1], new_division_list[1:]):\n                        pred_I_each = id_target_map_onlyclass[np.argmax(sum_dict[(s, e)])]\n                        if pred_I_each == \"O\":\n                            pred_list.append(np.array((e - s) * [target_id_map[\"O\"]]))\n                        else:\n                            pred_list.append(np.array([target_id_map[\"B-\" + pred_I_each]] + (e - s - 1) * [target_id_map[\"I-\" + pred_I_each]]))\n                    total_time2 += time.time() - time2\n\n                    pred = np.concatenate(pred_list)\n                    \n                    score = total_score(modified_log_raw_pred, pred)\n                    next_division_beam.append((score, tuple(new_division_list)))\n                    if score > max_score:\n                        max_score = score\n                        exists_update = True\n                        best_division_list = copy.copy(new_division_list)\n\n            next_division_beam = list(set(next_division_beam))\n            division_beam = sorted(next_division_beam, key=lambda x: x[0])[-BEAM_WIDTH:]\n\n            total_time1 += time.time() - s_time\n        division_list = best_division_list\n        score_list.append(max_score)\n\n        tt = []\n        for start, end in zip(division_list[:-1], division_list[1:]):\n            chunk_mean = np.mean(log_raw_pred_I[start:end] + B_MIX_RATIO * log_raw_pred_B[start:end], axis=0)\n            pred_I = id_target_map_onlyclass[np.argmax(chunk_mean)]\n\n            if pred_I == \"O\":\n                chunk = [\"O\"] * (end - start)\n            else:\n                chunk = [\"B-\" + pred_I] + [\"I-\" + pred_I] * (end - start - 1)\n            tt.extend(chunk)\n\n        tt_score = final_scores[j][1:]\n        samples[j][\"preds\"] = tt\n        samples[j][\"pred_scores\"] = tt_score\n\n    submission = []\n    for _, sample in enumerate(samples):\n        preds = sample[\"preds\"]\n        offset_mapping = sample[\"offset_mapping\"]\n        sample_id = sample[\"id\"]\n        sample_text = sample[\"text\"]\n        sample_pred_scores = sample[\"pred_scores\"]\n\n        # pad preds to same length as offset_mapping\n        if len(preds) < len(offset_mapping):\n            preds = preds + [\"O\"] * (len(offset_mapping) - len(preds))\n            sample_pred_scores = sample_pred_scores + [0] * (len(offset_mapping) - len(sample_pred_scores))\n\n        idx = 0\n        phrase_preds = []\n        while idx < len(offset_mapping):\n            start, _ = offset_mapping[idx]\n            if preds[idx] != \"O\":\n                label = preds[idx][2:]\n            else:\n                label = \"O\"\n            phrase_scores = []\n            phrase_scores.append(sample_pred_scores[idx])\n            idx += 1\n\n            while idx < len(offset_mapping):\n                if label == \"O\":\n                    matching_label = \"O\"\n                else:\n                    matching_label = f\"I-{label}\"\n                if preds[idx] == matching_label:\n                    _, end = offset_mapping[idx]\n                    phrase_scores.append(sample_pred_scores[idx])\n                    idx += 1\n                else:\n                    break\n\n            if \"end\" in locals():\n                phrase = sample_text[start:end]\n                phrase_preds.append((phrase, start, end, label, phrase_scores))\n\n        temp_df = []\n        for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n            word_start = len(sample_text[:start].split())\n            word_end = word_start + len(sample_text[start:end].split())\n            word_end = min(word_end, len(sample_text.split()))\n            ps = \" \".join([str(x) for x in range(word_start, word_end)])\n            if label != \"O\":\n                if sum(phrase_scores) / len(phrase_scores) >= proba_thresh[label]:\n                    temp_df.append((sample_id, label, ps))\n\n        temp_df = pd.DataFrame(temp_df, columns=[\"id\", \"class\", \"predictionstring\"])\n\n        submission.append(temp_df)\n\n    submission = pd.concat(submission).reset_index(drop=True)\n    submission[\"len\"] = submission.predictionstring.apply(lambda x: len(x.split()))\n\n    def threshold(df):\n        df = df.copy()\n        for key, value in min_thresh.items():\n            index = df.loc[df[\"class\"] == key].query(f\"len<{value}\").index\n            df.drop(index, inplace=True)\n        return df\n\n    submission = threshold(submission)\n\n    print(\"=\" * 30)\n    print(total_time1)\n    print(total_time2)\n\n    print(f\"score average = {np.mean(np.array(score_list))}\")\n\n    # drop len\n    submission = submission.drop(columns=[\"len\"])\n    return submission","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jn(pst, start, end):\n    return \" \".join([str(x) for x in pst[start:end]])\n\n\ndef link_evidence(oof):\n    thresh = 1\n    idu = oof['id'].unique()\n    idc = idu[1]\n    eoof = oof[oof['class'] == \"Evidence\"]\n    neoof = oof[oof['class'] != \"Evidence\"]\n    for thresh2 in range(26,27, 1):\n        retval = []\n        for idv in idu:\n            for c in  ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n                   'Counterclaim', 'Rebuttal']:\n                q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n                if len(q) == 0:\n                    continue\n                pst = []\n                for i,r in q.iterrows():\n                    pst = pst +[-1] + [int(x) for x in r['predictionstring'].split()]\n                start = 1\n                end = 1\n                for i in range(2,len(pst)):\n                    cur = pst[i]\n                    end = i\n                    if (cur == -1 and c != 'Evidence') or ((cur == -1) and ((pst[i+1] > pst[end-1] + thresh) or (pst[i+1] - pst[start] > thresh2))):\n                        retval.append((idv, c, jn(pst, start, end)))\n                        start = i + 1\n                v = (idv, c, jn(pst, start, end+1))\n                retval.append(v)\n        roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring']) \n        roof = roof.merge(neoof, how='outer')\n        return roof\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = beam_inference(raw_preds, test_samples)\nsubmission = link_evidence(submission)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-05T18:26:47.003019Z","iopub.status.idle":"2022-01-05T18:26:47.004284Z","shell.execute_reply.started":"2022-01-05T18:26:47.003972Z","shell.execute_reply":"2022-01-05T18:26:47.004003Z"},"trusted":true},"execution_count":null,"outputs":[]}]}