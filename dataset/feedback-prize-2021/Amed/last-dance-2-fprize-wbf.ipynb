{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport glob\nimport yaml\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport pytorch_lightning as pl\n\nfrom transformers import DataCollatorWithPadding\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\n\n%env TOKENIZERS_PARALLELISM = true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-15T10:27:09.92496Z","iopub.execute_input":"2022-03-15T10:27:09.925229Z","iopub.status.idle":"2022-03-15T10:27:17.982818Z","shell.execute_reply.started":"2022-03-15T10:27:09.925158Z","shell.execute_reply":"2022-03-15T10:27:17.981832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IS_DEBUG = False\n\nif IS_DEBUG:\n    TEST_ROOT = \"../input/feedback-prize-2021/train/\"\n    MAX_NUM_MODELS = 1\n    NUM_SAMPLES = 200\nelse:\n    TEST_ROOT = \"../input/feedback-prize-2021/test/\"\n#     MAX_NUM_MODELS = None\n    MAX_NUM_MODELS = 1\n    NUM_SAMPLES = None\n    \nprint(f\"TEST_ROOT: {TEST_ROOT}\\nMAX_NUM_MODELS: {MAX_NUM_MODELS}\\nNUM_SAMPLES: {NUM_SAMPLES}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:27:17.984676Z","iopub.execute_input":"2022-03-15T10:27:17.985397Z","iopub.status.idle":"2022-03-15T10:27:17.991404Z","shell.execute_reply.started":"2022-03-15T10:27:17.985358Z","shell.execute_reply":"2022-03-15T10:27:17.990476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Crodoc","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\n\n# =======================================================================================================#\ndef from_text_to_span(preds,b,i,tolb='',otol=0):\n    o = '{'\n    c = '}'\n    \n    if str(otol)!=\"0\":\n        pat = fr'({b}{o}1,{tolb}{c}{i}{o}1,{c}([^{i}{b}]{o}1,{otol}{c}{i}{o}1,{c})+)|({i}{o}1,{c}([^{i}{b}]{o}1,{otol}{c}{i}{o}1,{c})+)|({b}{o}1,{tolb}{c}{i}{o}1,{c})|({i}{o}1,{c})|({b}{o}1,{tolb}{c})'\n    else:\n        pat = fr'({b}{o}1,{tolb}{c}{i}{o}1,{c})|({i}{o}1,{c})|({b}{o}1,{tolb}{c})'\n    \n    matches = re.findall(pat,preds)\n    output = []\n    if len(matches)>0:\n        for match in re.finditer(pat, preds):\n#             s,e = match.span()\n            output.append(match.span())\n            \n    return output\n\n\n# =======================================================================================================#\ndef from_token_proba_to_span_df(text,idx,token_proba,dicos,offset,tolb='',otol=0):\n    ID_TO_TXT = {i:j for i,j in zip(range(15),'abcdefghijklmnopqrstuvwxyz')}\n    token_class = token_proba.argmax(-1)\n    token_score = token_proba.max(-1)\n    \n    indices = np.unique(np.where(np.asarray(offset)!=(0,0))[0])\n    offset = np.asarray(offset)[indices].tolist()\n    token_class = token_class[indices]\n    \n    token_class_text = [ID_TO_TXT[i] for i in token_class]\n\n\n    output = []\n    for c,(b,i) in dicos.items():\n        spans = from_text_to_span(''.join(token_class_text),ID_TO_TXT[b],ID_TO_TXT[i],tolb=tolb,otol=otol)\n        if len(spans)>0:\n            for s,e in spans:\n                \n                start,_ = offset[s]\n                _,end = offset[e-1]\n                if (start+end)>0:\n                    word_start = len(text[:start].split())\n                    word_end = word_start + len(text[start:end].split())\n                    word_end = min(word_end, len(text.split()))\n                    ps = \" \".join([str(x) for x in range(word_start, word_end)])\n\n                    output.append([idx,c,start,end,str(token_score[s:e].tolist()),ps])\n    \n\n    output = pd.DataFrame(output, columns=['id','class','discourse_start',\n                                               'discourse_end','score','predictionstring'])\n    \n    output['discourse_type'] = output['class']\n    output[[\"start\",\"end\"]] = output[[\"discourse_start\",\"discourse_end\"]]\n    return output\n# =======================================================================================================#\ndef run_token_to_span_ac(tokens_probas,ids,test_df,offsets,dicos,tolb='',otol=0):\n    \n    submit_df = []\n    for i in range(len(ids)):\n        txt = test_df[test_df.id==ids[i]].text.iloc[0]\n        submit_df.append(from_token_proba_to_span_df(txt,ids[i],tokens_probas[i],dicos,offsets[i],tolb,otol))\n    \n    return pd.concat(submit_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:27:17.992801Z","iopub.execute_input":"2022-03-15T10:27:17.99359Z","iopub.status.idle":"2022-03-15T10:27:18.012788Z","shell.execute_reply.started":"2022-03-15T10:27:17.993543Z","shell.execute_reply":"2022-03-15T10:27:18.01209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, df, tokenizer, cfg):\n\n        self.tokenizer = tokenizer\n        self.max_length = 4096\n        \n        self.texts = df['text'].values.tolist()\n        self.ids = df['id'].values.tolist()\n\n        self.x, self.offset_mappings = [], []\n\n        for text in self.texts:\n            x, offset_mapping = self.make_item(text)\n            self.x.append(x)\n            self.offset_mappings.append(offset_mapping)\n\n    def get_offset_mapping(self, text):\n\n        tokenized = self.tokenizer(\n            text,\n            add_special_tokens = True,\n            max_length = self.max_length,\n            truncation=True,\n            return_offsets_mapping = True,\n        )\n\n        offset_mapping = tokenized['offset_mapping']\n        skip_indices = np.where(np.array(tokenized.sequence_ids()) != 0)[0]\n\n        return offset_mapping, skip_indices\n\n    def make_item(self, text):\n\n        tokenized = self.tokenizer(\n            text,\n            add_special_tokens = True,\n            max_length = self.max_length,\n            truncation=True,\n            return_offsets_mapping = False,\n        )\n\n        offset_mapping, _ = self.get_offset_mapping(text)\n\n        for k, v in tokenized.items():\n            tokenized[k] = torch.tensor(v, dtype=torch.long)\n\n        return tokenized, offset_mapping\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return self.x[idx]\n\nclass CustomCollator():\n\n    def __init__(self, tokenizer):\n        self.data_collator = DataCollatorWithPadding(tokenizer)\n\n    def __call__(self, batch):\n        text = []\n        for item in batch:\n            text.append(item)\n\n        text = self.data_collator(text)\n        return text\n\n\nclass TextDataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        test_df,\n        tokenizer,\n        cfg,\n        test_dataset\n    ):\n        super().__init__()\n        self.test_df = test_df\n        self.tokenizer = tokenizer\n        self.cfg = cfg\n        \n        self.test_dataset = test_dataset\n\n    def setup(self, stage):\n        pass\n\n    def predict_dataloader(self):\n        custom_collator = CustomCollator(self.tokenizer)\n        return DataLoader(self.test_dataset, **self.cfg[\"val_loader\"], collate_fn=custom_collator)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:39:43.733527Z","iopub.execute_input":"2022-03-15T10:39:43.733818Z","iopub.status.idle":"2022-03-15T10:39:43.750338Z","shell.execute_reply.started":"2022-03-15T10:39:43.733784Z","shell.execute_reply":"2022-03-15T10:39:43.748961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CutTextDataset(Dataset):\n    def __init__(self, df, tokenizer, cfg):\n\n        self.tokenizer = tokenizer\n        #self.max_length = cfg['max_length']\n        self.max_length = cfg['max_length_valid']\n\n        self.texts = df['text'].values.tolist()\n        self.ids = df['id'].values.tolist()\n        self.stride = cfg['stride']\n        \n        self.x, self.x_cut, self.offset_mappings, self.text_indexes = [], [], [], []\n        \n        text_index = 0\n\n        for text in self.texts:\n            x, offset_mapping = self.make_item(text)\n\n            self.x.append(x)\n            self.offset_mappings.append(offset_mapping)\n\n            start = 0\n            total_tokens = len(offset_mapping)\n\n            break_bool = False\n\n            while start < total_tokens and not break_bool:\n\n                if start + self.max_length > total_tokens:\n                    start = max(0, total_tokens - self.max_length)\n                    break_bool = True\n\n                x_cut, _ = self.get_cut_item(x, offset_mapping, start)\n\n                self.x_cut.append(x_cut)\n                self.text_indexes.append((text_index, start))\n\n                start += self.stride\n\n            text_index += 1\n\n    def get_cut_element(self, tokenized_element, start, length, is_list=False):\n\n        new_tokenized_element = tokenized_element[start:start+length]\n        if not is_list:\n            new_tokenized_element = new_tokenized_element.clone()\n\n        #new_tokenized_element[0] = tokenized_element[0]\n        #new_tokenized_element[-1] = tokenized_element[-1]\n\n        return new_tokenized_element\n\n    def get_cut_item(self, tokenized, offset_mapping, start):\n\n        cut_length = min(self.max_length, len(offset_mapping))\n\n        new_tokenized = {}\n\n        for k in tokenized:\n            new_tokenized[k] = self.get_cut_element(tokenized[k], start, cut_length)\n\n        if offset_mapping is not None:\n            offset_mapping = self.get_cut_element(offset_mapping, start, cut_length, is_list=True)\n\n        return new_tokenized, offset_mapping\n\n    def make_item(self, text):\n\n        tokenized = self.tokenizer(\n            text,\n            add_special_tokens = True,\n            return_offsets_mapping = True,\n        )\n\n        offset_mapping = tokenized['offset_mapping']\n        del tokenized['offset_mapping']\n\n        for k, v in tokenized.items():\n            tokenized[k] = torch.tensor(v, dtype=torch.long)\n\n        return tokenized, offset_mapping\n\n    def __len__(self):\n        return len(self.x_cut)\n\n    def __getitem__(self, idx):\n        return self.x_cut[idx]","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:39:44.118497Z","iopub.execute_input":"2022-03-15T10:39:44.118963Z","iopub.status.idle":"2022-03-15T10:39:44.134179Z","shell.execute_reply.started":"2022-03-15T10:39:44.118924Z","shell.execute_reply":"2022-03-15T10:39:44.133343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextModel(pl.LightningModule):\n\n    def __init__(self, cfg, config_path=None):\n        super().__init__()\n\n        self.cfg = cfg\n        model_cfg = cfg['model']\n        self.num_labels = model_cfg['num_labels']\n\n        self.config = torch.load(config_path)\n        self.backbone = AutoModel.from_config(self.config)\n\n        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n\n        self.fc = nn.Linear(self.config.hidden_size, self.num_labels)\n\n    def forward(self, x):\n\n        x = self.backbone(**x).last_hidden_state\n        x = self.dropout(x)\n\n        x1 = self.dropout1(x)\n        x2 = self.dropout2(x)\n        x3 = self.dropout3(x)\n        x4 = self.dropout4(x)\n        x5 = self.dropout5(x)\n\n        x = (x1+x2+x3+x4+x5) / 5.0\n\n        x = self.fc(x)\n\n        return x\n\n    def predict_step(self, batch, batch_idx):\n\n            output = self(batch)\n            pred = output.softmax(dim=-1).detach().cpu()\n\n            return pred\n\n    def configure_optimizers(self):\n        return None","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:39:44.470707Z","iopub.execute_input":"2022-03-15T10:39:44.471454Z","iopub.status.idle":"2022-03-15T10:39:44.481879Z","shell.execute_reply.started":"2022-03-15T10:39:44.471416Z","shell.execute_reply":"2022-03-15T10:39:44.481163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_df():\n#     test_files = glob.glob('../input/feedback-prize-2021/test/*.txt')\n    test_files = glob.glob(str(TEST_ROOT) + '*.txt')[:NUM_SAMPLES]\n    test_ids = [test_file.split('/')[-1][:-4] for test_file in test_files]\n    \n    test_texts = []\n    \n    for test_file, test_id in zip(test_files, test_ids):\n        \n        with open(test_file, 'r') as f:\n            text = f.read()\n\n        # no-break space\n        text = text.replace(u'\\xa0', u' ')\n        # next line\n        text = text.replace(u'\\x85', u'\\n')\n        \n        test_texts.append(text)\n    \n    values = list(zip(test_ids, test_texts))\n    values.sort(key=lambda x: -len(x[1]))\n    \n    test_df = pd.DataFrame(values, columns=['id','text'])\n    return test_df","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:39:44.846408Z","iopub.execute_input":"2022-03-15T10:39:44.84675Z","iopub.status.idle":"2022-03-15T10:39:44.856474Z","shell.execute_reply.started":"2022-03-15T10:39:44.846718Z","shell.execute_reply":"2022-03-15T10:39:44.855714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_to_words(text):\n    word = text.split()\n    word_offset = []\n\n    start = 0\n    for w in word:\n        r = text[start:].find(w)\n\n        if r==-1:\n            raise NotImplementedError\n        else:\n            start = start+r\n            end   = start+len(w)\n            word_offset.append((start,end))\n        start = end\n\n    return word, word_offset\n\ndef word_probability_to_predict_df(text_to_word_probability, id):\n    \n    len_word = len(text_to_word_probability)\n    word_predict = text_to_word_probability.argmax(-1)\n    word_score   = text_to_word_probability.max(-1)\n    predict_df = []\n\n    t = 0\n    while 1:\n        if word_predict[t] not in [\n            discourse_marker_to_label['O'],\n        ]:\n            start = t\n            b_marker_label = word_predict[t]\n        else:\n            t = t+1\n            if t== len_word-1:break\n            continue\n\n        t = t+1\n        if t== len_word-1: break\n\n        if   label_to_discourse_marker[b_marker_label][0]=='B':\n            i_marker_label = b_marker_label+1\n        else:\n            i_marker_label = b_marker_label\n\n        while 1:\n            if (word_predict[t] != i_marker_label) or (t ==len_word-1):\n                end = t\n                prediction_string = ' '.join([str(i) for i in range(start,end)])\n                discourse_type = label_to_discourse_marker[b_marker_label][2:]\n                discourse_score = word_score[start:end].tolist()\n                predict_df.append((id, discourse_type, prediction_string, str(discourse_score)))\n                break\n            else:\n                t = t+1\n                continue\n        if t== len_word-1: break\n\n    predict_df = pd.DataFrame(predict_df, columns=['id', 'class', 'predictionstring', 'score'])\n    return predict_df\n\n\n# def do_threshold(submit_df, use=['length','probability']):\n#     df = submit_df.copy()\n#     df = df.fillna('')\n\n#     if 'length' in use:\n#         df['l'] = df.predictionstring.apply(lambda x: len(x.split()))\n#         for key, value in min_thresh.items():\n#             #value=3\n#             index = df.loc[df['class'] == key].query('l<%d'%value).index\n#             df.drop(index, inplace=True)\n\n#     if 'probability' in use:\n#         df['s'] = df.score.apply(lambda x: np.mean(eval(x)))\n#         for key, value in proba_thresh.items():\n#             index = df.loc[df['class'] == key].query('s<%f'%value).index\n#             df.drop(index, inplace=True)\n\n#     df = df[['id', 'class', 'predictionstring']]\n#     return df\n\n\ndef do_threshold(submit_df, use=['length','probability']):\n    df = submit_df.copy()\n    df = df.fillna('')\n\n    if 'length' in use:\n        df['l'] = df.predictionstring.apply(lambda x: len(x.split()))\n        for key, value in min_thresh.items():\n            #value=3\n            index = df.loc[df['class'] == key].query('l<%d'%value).index\n            df.drop(index, inplace=True)\n\n    if 'probability' in use:\n        df['score'] = df.score.apply(lambda x: np.mean(eval(x)))\n        for key, value in proba_thresh.items():\n            index = df.loc[df['class'] == key].query('score<%f'%value).index\n            df.drop(index, inplace=True)\n    \n    df.rename(columns={\"l\": \"num_tokens\"}, inplace=True)\n    df[\"start\"] = df.predictionstring.apply(lambda x: int(x.split()[0]))\n    df[\"end\"] = df.predictionstring.apply(lambda x: int(x.split()[-1]) + 1)\n    df = df[['id', 'class', 'predictionstring', \"num_tokens\", \"score\", \"start\", \"end\"]]\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:39:45.19942Z","iopub.execute_input":"2022-03-15T10:39:45.199666Z","iopub.status.idle":"2022-03-15T10:39:45.217711Z","shell.execute_reply.started":"2022-03-15T10:39:45.199638Z","shell.execute_reply":"2022-03-15T10:39:45.21706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reset_crodoc():\n    global text_words, text_word_offsets, text_word_preds, text_ids, text_lenghts, test_df, num_labels\n    \n    test_df = get_test_df()\n\n    num_labels = 10\n\n    text_words, text_word_offsets, text_word_preds, text_ids, text_lenghts = [], [], [], [], []\n\n    for idx in range(len(test_df)):\n        row = test_df.iloc[idx]\n        text_ids.append(row.id)\n        text_lenghts.append(len(row.text))\n\n        row_words, row_word_offsets = text_to_words(row.text)\n        text_words.append(row_words)\n        text_word_offsets.append(row_word_offsets)\n\n        word_preds = np.full((len(row_words),num_labels),0, np.float32)\n        text_word_preds.append(word_preds)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:39:45.673836Z","iopub.execute_input":"2022-03-15T10:39:45.674502Z","iopub.status.idle":"2022-03-15T10:39:45.680705Z","shell.execute_reply.started":"2022-03-15T10:39:45.674468Z","shell.execute_reply":"2022-03-15T10:39:45.6799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def update_word_preds(model_preds, offset_mappings, coef):\n    idx = 0\n    \n    for idx, row_preds in enumerate(model_preds):\n            \n        character_preds = np.full((text_lenghts[idx],num_labels),0, np.float32)\n\n        for pos,(start,end) in enumerate(offset_mappings[idx]):\n            character_preds[start:end] = row_preds[pos] * coef\n            \n        for pos,(start,end) in enumerate(text_word_offsets[idx]):\n            text_word_preds[idx][pos] += character_preds[start:end].mean(0)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:39:46.088726Z","iopub.execute_input":"2022-03-15T10:39:46.089361Z","iopub.status.idle":"2022-03-15T10:39:46.095665Z","shell.execute_reply.started":"2022-03-15T10:39:46.08931Z","shell.execute_reply":"2022-03-15T10:39:46.094615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def merge_cut_preds(model_preds, dataset):\n\n    dataset_length = len(set(dataset.texts))\n    \n    index = 0\n    preds_tmp = []\n    text_indexes = dataset.text_indexes\n\n    overlap = dataset.stride // 2\n\n    while index < len(model_preds):\n\n        text_index, _ = text_indexes[index]\n        offset_mapping = dataset.offset_mappings[text_index]\n\n        preds = np.zeros((len(offset_mapping), 10))\n\n        while index < len(model_preds):\n            curr_text_index, start = text_indexes[index]\n\n            if curr_text_index != text_index:\n                break\n\n            curr_preds = model_preds[index]\n\n            if start == 0:\n                length = min(len(preds), len(curr_preds))\n                preds[:length] = curr_preds[:length]\n            elif start + len(curr_preds) > len(offset_mapping):\n                preds[-len(curr_preds)+overlap:] = curr_preds[overlap:]\n            else:\n                preds[start+overlap:start+len(curr_preds)] = curr_preds[overlap:]\n\n            index += 1\n\n        preds_tmp.append(preds)\n\n    return preds_tmp","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:39:46.583939Z","iopub.execute_input":"2022-03-15T10:39:46.584359Z","iopub.status.idle":"2022-03-15T10:39:46.594459Z","shell.execute_reply.started":"2022-03-15T10:39:46.584323Z","shell.execute_reply":"2022-03-15T10:39:46.593616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discourse_marker_to_label = {\n    'O': 0,\n    'B-Claim': 1,\n    'I-Claim': 2,\n    'B-Evidence': 3,\n    'I-Evidence': 4,\n    'X-Lead': 5,\n    'X-Position': 6,\n    'X-Counterclaim': 7,\n    'X-Rebuttal': 8,\n    'X-Concluding Statement': 9,\n}\n\n# min_thresh = {\n#     'Lead': 6,\n#     'Position': 4,\n#     'Evidence': 16,\n#     'Claim': 2,\n#     'Concluding Statement': 11,\n#     'Counterclaim': 7,\n#     'Rebuttal': 6,\n# }\n\n# proba_thresh = {\n#     \"Lead\": 0.7,\n#     \"Position\": 0.6,\n#     \"Evidence\": 0.65,\n#     \"Claim\": 0.55,\n#     \"Concluding Statement\": 0.7,\n#     \"Counterclaim\": 0.6,\n#     \"Rebuttal\": 0.6,\n# }\nmin_thresh = {\n    'Lead': 5,\n    'Position': 3,\n    'Evidence': 10,\n    'Claim': 2,\n    'Concluding Statement': 5,\n    'Counterclaim': 6,\n    'Rebuttal': 5,\n}\n\nproba_thresh = {\n    'Lead': 0.55,\n    'Position': 0.55,\n    'Evidence': 0.55,\n    'Claim': 0.56,\n    'Concluding Statement': 0.55,\n    'Counterclaim': 0.56,\n    'Rebuttal': 0.57,\n}\n# B / I\nDICOS = {\n        \"Lead\":[5,5],\n        \"Position\":[6,6],\n        'Claim':[1,2],\n        'Counterclaim':[7,7],\n        'Rebuttal':[8,8],\n        'Evidence':[3,4],\n        'Concluding Statement':[9,9]\n  \n        }\n\nlabel_to_discourse_marker = {v: k for k, v in discourse_marker_to_label.items()}\n\ndef get_sub_crodoc(token_preds,ids,test_df,offset_mappings):\n#     sub_crodoc = []\n\n#     for idx, row_word_preds in enumerate(text_word_preds):\n#         sub_crodoc.append(word_probability_to_predict_df(row_word_preds, text_ids[idx]))\n        \n    sub_crodoc = run_token_to_span_ac(token_preds,ids,test_df,offset_mappings,DICOS,tolb='',otol=2).reset_index(drop=True) \n    \n#     sub_crodoc = pd.concat(sub_crodoc).reset_index(drop=True) \n    sub_crodoc = do_threshold(sub_crodoc, use=['length', 'probability'])\n    \n    return sub_crodoc","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:39:47.109026Z","iopub.execute_input":"2022-03-15T10:39:47.109583Z","iopub.status.idle":"2022-03-15T10:39:47.11997Z","shell.execute_reply.started":"2022-03-15T10:39:47.109546Z","shell.execute_reply":"2022-03-15T10:39:47.119105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_names = ['cp-deberta-xlarge-v2', 'deberta-bs2']\n# model_coeffs = [1.0, 1.0]\nmodel_weights_crodoc = [0.60, 0.40]\n\nmodel_start_ends = [(0, 3), (2, 5)]\n\nsubs_crodoc = []\n\n\nfor model_name, (start, end) in zip(model_names, model_start_ends):\n    \n    reset_crodoc()\n    \n    with open('../input/' + model_name + '/hparams.yml', 'r') as f:\n        cfg = yaml.safe_load(f)\n\n    cfg['val_loader']['num_workers'] = 2\n    cfg['val_loader']['batch_size'] = 8\n    \n    #if 'deberta-large' in model_name:\n        #cfg['val_loader']['batch_size'] *= 3\n    \n    config_path = '../input/' + model_name + '/config.pth'    \n#     model_paths = glob.glob('../input/' + model_name + '/*.ckpt')[:MAX_NUM_MODELS]\n    model_paths = glob.glob('../input/' + model_name + '/*.ckpt')[start:end]\n    \n    folds = len(model_paths)\n    model_preds = []\n    \n    print(f\"{model_name}: {folds}\")\n    \n    tokenizer = AutoTokenizer.from_pretrained('../input/' + model_name + '/tokenizer/tokenizer')\n    \n    if 'stride' in cfg and cfg['stride'] > 0:\n        test_dataset = CutTextDataset(test_df, tokenizer, cfg)\n    else:\n        test_dataset = TextDataset(test_df, tokenizer, cfg)\n    \n    for model_path in model_paths:\n        \n        datamodule = TextDataModule(test_df, tokenizer, cfg, test_dataset)\n        trainer = pl.Trainer(logger=False, **cfg['trainer'])\n        model = TextModel.load_from_checkpoint(checkpoint_path=model_path, cfg=cfg, config_path=config_path)\n        \n        fold_preds = trainer.predict(model, datamodule)\n        \n        if not model_preds:\n            for pred_batch in fold_preds:            \n                for pred in pred_batch:\n                    model_preds.append(pred.numpy().copy()/folds)\n        else:\n            idx = 0\n            for pred_batch in fold_preds:            \n                for pred in pred_batch:\n                    model_preds[idx] += pred.numpy().copy()/folds\n                    idx += 1\n        \n        del fold_preds\n        del trainer\n        del model\n        del datamodule\n    \n        gc.collect()\n        torch.cuda.empty_cache()\n    \n    if 'stride' in cfg and cfg['stride'] > 0:\n        model_preds = merge_cut_preds(model_preds, test_dataset)\n            \n    subs_crodoc.append(get_sub_crodoc(model_preds,text_ids,test_df,test_dataset.offset_mappings))\n        \n#     update_word_preds(model_preds, test_dataset.offset_mappings, model_coef / folds)\n#     update_word_preds(model_preds, test_dataset.offset_mappings, 1 / folds)\n\n    del test_dataset\n    del tokenizer\n    del model_preds\n    \n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    \n    \n    \n    print(subs_crodoc[-1].shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:39:47.810627Z","iopub.execute_input":"2022-03-15T10:39:47.811669Z","iopub.status.idle":"2022-03-15T10:43:12.189944Z","shell.execute_reply.started":"2022-03-15T10:39:47.811611Z","shell.execute_reply":"2022-03-15T10:43:12.189235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not IS_DEBUG:\n    del text_words, text_word_offsets, text_word_preds, text_ids, text_lenghts \n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:44:13.725476Z","iopub.execute_input":"2022-03-15T10:44:13.725748Z","iopub.status.idle":"2022-03-15T10:44:13.981648Z","shell.execute_reply.started":"2022-03-15T10:44:13.725717Z","shell.execute_reply":"2022-03-15T10:44:13.980264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kkiller","metadata":{}},{"cell_type":"code","source":"import sys, os\nsys.path.insert(0, \"../input/fprize-kkiller-tools/fprize\")\nsys.path.insert(0, \"../input/weighted-boxes-fusion\")\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:44:17.626826Z","iopub.execute_input":"2022-03-15T10:44:17.627276Z","iopub.status.idle":"2022-03-15T10:44:17.633557Z","shell.execute_reply.started":"2022-03-15T10:44:17.627228Z","shell.execute_reply":"2022-03-15T10:44:17.632818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd, numpy as np\nimport torch\n\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nfrom  datetime import datetime\n\nimport mtask_v2.src.inference as inference\nimport mtask_v2.src.dataset as dataset\nimport mtask_v2.src.configs as configs\n\nfrom mtask_v2.src.dataset import read_from_id, read_train_df\nfrom mtask_v2.src.post_processing import get_seg_from_ner\nfrom mtask_v2.src.wbf import fusion_boxes_for_subs","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:44:18.009222Z","iopub.execute_input":"2022-03-15T10:44:18.009499Z","iopub.status.idle":"2022-03-15T10:44:18.901646Z","shell.execute_reply.started":"2022-03-15T10:44:18.009468Z","shell.execute_reply":"2022-03-15T10:44:18.90087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# q_crodoc = 0.50\n# iou_thr = 0.333\n# skip_box_thr = 0.1\n# # out_skip_box_thr = 0.25","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:44:18.905367Z","iopub.execute_input":"2022-03-15T10:44:18.90559Z","iopub.status.idle":"2022-03-15T10:44:18.909302Z","shell.execute_reply.started":"2022-03-15T10:44:18.905564Z","shell.execute_reply":"2022-03-15T10:44:18.908369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sub_crodoc in subs_crodoc:\n    sub_crodoc[\"class_id\"] = sub_crodoc[\"class\"].map(configs.Discourse2ID)\n\n# sub_crodoc = fusion_boxes_for_subs(subs_crodoc, model_weights_crodoc, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n\n# if not IS_DEBUG:\n#     del subs_crodoc\n#     gc.collect()\n    \n# print(sub_crodoc.shape)\n# sub_crodoc = sub_crodoc.query(f\"score >= {out_skip_box_thr}\")\n# print(sub_crodoc.shape)\n# sub_crodoc.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:44:19.046892Z","iopub.execute_input":"2022-03-15T10:44:19.047425Z","iopub.status.idle":"2022-03-15T10:44:19.055848Z","shell.execute_reply.started":"2022-03-15T10:44:19.04718Z","shell.execute_reply":"2022-03-15T10:44:19.055184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# TAKE_SAMPLE = (datetime.utcnow() <  datetime(year=2022, month=2, day=7, hour=18, minute=33))\n# print(\"TAKE_SAMPLE:\", TAKE_SAMPLE)\nprint(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:44:19.968617Z","iopub.execute_input":"2022-03-15T10:44:19.969153Z","iopub.status.idle":"2022-03-15T10:44:19.974072Z","shell.execute_reply.started":"2022-03-15T10:44:19.969117Z","shell.execute_reply":"2022-03-15T10:44:19.973233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TEST_ROOT = Path(\"../input/feedback-prize-2021/test\")\nTEST_ROOT = Path(TEST_ROOT)\n\nconfigs.TRAIN_ROOT = TEST_ROOT\n\n# uuids = [file.stem for file in TEST_ROOT.glob(\"*.txt\")]\nuuids = sub_crodoc[\"id\"].unique()\n               \n# uuids = sorted(set(df[\"id\"][df[\"fold_abishek\"] == 0].values))\n\nuuids = sorted(uuids, key=lambda uuid: -len(read_from_id(uuid, root=TEST_ROOT).split()))\n\n# uuids = uuids[:200]\n\nprint(len(uuids))\nuuids[:10]","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:44:20.583456Z","iopub.execute_input":"2022-03-15T10:44:20.583714Z","iopub.status.idle":"2022-03-15T10:44:20.603621Z","shell.execute_reply.started":"2022-03-15T10:44:20.583685Z","shell.execute_reply":"2022-03-15T10:44:20.602667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = [\n    \n    inference.get_params(\n        model_name=\"microsoft/deberta-xlarge\",\n        batch_size=14,\n        maxlen=1024,\n        stride=1024,\n        num_workers=2,\n        weight=.60,\n        config_path=\"../input/fprize-kkiller-tools/microsoft_deberta-xlarge\",\n        tokenizer_path=\"../input/fprize-kkiller-tools/microsoft_deberta-xlarge\",\n        is_pickle=False,\n        device=DEVICE,\n#         model_paths=sorted(Path(\"../input/cp-deberta-xlarge-kkiller\").glob(\"*.pth\"))[:MAX_NUM_MODELS],\n        model_paths=sorted(Path(\"../input/cp-deberta-xlarge-kkiller\").glob(\"*.pth\"))[:5],\n        root=TEST_ROOT,\n        use_position_embeddings=False,\n    ),\n    \n    \n    inference.get_params(\n        model_name=\"microsoft/deberta-large\",\n        batch_size=24,\n        maxlen=1024,\n        stride=1024,\n        num_workers=2,\n        weight=.40,\n        config_path=\"../input/fprize-kkiller-tools/microsoft_deberta-large\",\n        tokenizer_path=\"../input/fprize-kkiller-tools/microsoft_deberta-large\",\n        is_pickle=False,\n        device=DEVICE,\n#         model_paths=sorted(Path(\"../input/gdrive-db1l-1024-v2-v11-no-pe-weights/microsoft_deberta-large_maxlen1024_clb_mtask_msd_v2_v11_no_pe/\"\n#                                ).glob(\"*.pth\"))[:MAX_NUM_MODELS],\n        model_paths=sorted(Path(\"../input/gdrive-db1l-1024-v2-v11-no-pe-weights/microsoft_deberta-large_maxlen1024_clb_mtask_msd_v2_v11_no_pe/\"\n                               ).glob(\"*.pth\"))[3:],\n        root=TEST_ROOT,\n        use_position_embeddings=False,\n    ),\n    \n]\n\nS = sum([param[\"weight\"] for param in params])\nassert abs(S- 1.0) < 1e-6\nparams[0]","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:44:21.871866Z","iopub.execute_input":"2022-03-15T10:44:21.87253Z","iopub.status.idle":"2022-03-15T10:44:22.106075Z","shell.execute_reply.started":"2022-03-15T10:44:21.87248Z","shell.execute_reply":"2022-03-15T10:44:22.105349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subs_kkiller = []\nmodel_weights_kkiller = []\n\nfor param in params:\n    print(\"{}: {}\".format(Path(param[\"model_paths\"][0]).stem,  len(param[\"model_paths\"])))\n    preds, preds_seg  = inference.predict_from_param(uuids=uuids, param=param, make_sub=False, oof=False, model_bar=False)\n    \n    preds_seg = 0.60 * preds_seg + 0.40 * get_seg_from_ner(preds)\n    \n    subs_kkiller.append(\n        inference.make_sub_from_res(uuids=uuids, res=preds, res_seg=preds_seg, q=0.015, threshs=None)\n    )\n    \n    \n    model_weights_kkiller.append(param[\"weight\"])\n    \n    print(subs_kkiller[-1].shape)\n    \nprint(\"model_weights_kkiller:\", model_weights_kkiller)\n\n# sub_kkiller = fusion_boxes_for_subs(subs_kkiller, model_weights_kkiller, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n\nif not IS_DEBUG:\n#     del subs_kkiller, preds, preds_seg\n    del preds, preds_seg\n    gc.collect()\n    \n# print(sub_kkiller.shape)\n# sub_kkiller = sub_kkiller.query(f\"score >= {out_skip_box_thr}\")\n# print(sub_kkiller.shape)\n# sub_kkiller.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:44:23.296366Z","iopub.execute_input":"2022-03-15T10:44:23.29703Z","iopub.status.idle":"2022-03-15T10:50:29.546999Z","shell.execute_reply.started":"2022-03-15T10:44:23.296978Z","shell.execute_reply":"2022-03-15T10:50:29.546225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub_kkiller.shape, sub_crodoc.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-13T21:52:09.596282Z","iopub.execute_input":"2022-03-13T21:52:09.596947Z","iopub.status.idle":"2022-03-13T21:52:09.603727Z","shell.execute_reply.started":"2022-03-13T21:52:09.596904Z","shell.execute_reply":"2022-03-13T21:52:09.602944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub_crodoc[\"class_id\"] = sub_crodoc[\"class\"].map(configs.Discourse2ID)\n# sub_crodoc.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-13T21:52:09.604989Z","iopub.execute_input":"2022-03-13T21:52:09.605259Z","iopub.status.idle":"2022-03-13T21:52:09.613388Z","shell.execute_reply.started":"2022-03-13T21:52:09.605222Z","shell.execute_reply":"2022-03-13T21:52:09.612572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Box Fusion","metadata":{}},{"cell_type":"code","source":"min_thresh_for_wbf = {\n    'Lead': 3,\n    'Position': 4,\n    'Evidence': 4,\n    'Claim': 2,\n    'Concluding Statement': 9,\n    'Counterclaim': 5,\n    'Rebuttal': 2,\n}\n\n\nproba_thresh_for_wbf = {\n    \"Lead\": 0.27,\n    \"Position\": 0.28,\n    \"Evidence\": 0.39,\n    \"Claim\": 0.30,\n    \"Concluding Statement\": 0.36,\n    \"Counterclaim\": 0.21,\n    \"Rebuttal\": 0.20,\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def do_threshold(submit_df,min_thresh,proba_thresh,use=['length','probability']):\n    df = submit_df.copy()\n    df = df.fillna('')\n\n    if 'length' in use:\n        df['l'] = df.predictionstring.apply(lambda x: len(x.split()))\n        for key, value in min_thresh.items():\n            index = df.loc[df['class'] == key].query('l<%d'%value).index\n            df.drop(index, inplace=True)\n\n    if 'probability' in use:\n        for key, value in proba_thresh.items():\n            index = df.loc[df['class'] == key].query('score<%f'%value).index\n            df.drop(index, inplace=True)\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# subs = [sub_crodoc, sub_kkiller]\nsubs = subs_kkiller+subs_crodoc\nweights = [1/4]*4\niou_thr = 0.3333\nskip_box_thr = 0.01\nsub = fusion_boxes_for_subs(subs, weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n\nsub = do_threshold(sub.reset_index(drop=True),min_thresh_for_wbf,proba_thresh_for_wbf,use=['probability'])","metadata":{"execution":{"iopub.status.busy":"2022-03-13T21:52:09.614496Z","iopub.execute_input":"2022-03-13T21:52:09.614705Z","iopub.status.idle":"2022-03-13T21:52:09.720667Z","shell.execute_reply.started":"2022-03-13T21:52:09.614682Z","shell.execute_reply":"2022-03-13T21:52:09.719877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(sub.shape)\n# sub = sub.query(f\"score >= {out_skip_box_thr}\")\n# print(sub.shape)\n\nsub[[\"id\", \"class\", \"predictionstring\"]].to_csv(\"submission.csv\", index=False)\n\nsub.head(30)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}