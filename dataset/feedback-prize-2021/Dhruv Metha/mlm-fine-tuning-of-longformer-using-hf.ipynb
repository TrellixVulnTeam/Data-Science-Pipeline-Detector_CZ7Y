{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MLM Fine-tuning with the training data\n\nThis notebook takes you through the training of the Longformer using HuggingFace APIs. Hugging face makes it really easy to build models, and I would encourage going through the HF course and the new book NLP with Transformers.\n\nA lot of the code is taken and inspired from\n1. https://huggingface.co/\n2. https://github.com/nlp-with-transformers/notebooks\n3. Picked up a ton of inspiration from the other shared notebooks!\n\n","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets tokenizers huggingface","metadata":{"execution":{"iopub.status.busy":"2022-02-15T23:45:24.414268Z","iopub.execute_input":"2022-02-15T23:45:24.414655Z","iopub.status.idle":"2022-02-15T23:45:31.786569Z","shell.execute_reply.started":"2022-02-15T23:45:24.414558Z","shell.execute_reply":"2022-02-15T23:45:31.785749Z"},"_kg_hide-output":true,"jupyter":{"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport torch\nfrom transformers import AutoModelForMaskedLM\nfrom transformers import AutoTokenizer\nfrom transformers import DataCollatorForLanguageModeling\nfrom tqdm import tqdm\nfrom datasets import Dataset\nimport warnings\n\nos.chdir('/kaggle/input/feedback-prize-2021')\n\nDATA_FOLDER = '.'\nTRAIN_FOLDER = os.path.join(DATA_FOLDER, 'train')\nTEST_FOLDER = os.path.join(DATA_FOLDER, 'test')\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-02-15T23:45:31.788794Z","iopub.execute_input":"2022-02-15T23:45:31.789084Z","iopub.status.idle":"2022-02-15T23:45:34.285567Z","shell.execute_reply.started":"2022-02-15T23:45:31.789046Z","shell.execute_reply":"2022-02-15T23:45:34.284862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Here, we extract the text from \"train\" folder","metadata":{}},{"cell_type":"code","source":"train_data = []\nfor i in tqdm(os.listdir(TRAIN_FOLDER)):\n    with open(os.path.join(TRAIN_FOLDER, i), 'r') as f:\n        train_data.append({'text': f.read(), 'id': i[:-4]})\ndf_train = pd.DataFrame(train_data)\ndataset = Dataset.from_pandas(df_train, split='train')\ndel df_train\ndataset","metadata":{"execution":{"iopub.status.busy":"2022-02-15T23:45:34.28678Z","iopub.execute_input":"2022-02-15T23:45:34.287107Z","iopub.status.idle":"2022-02-15T23:45:41.6265Z","shell.execute_reply.started":"2022-02-15T23:45:34.287068Z","shell.execute_reply":"2022-02-15T23:45:41.625628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import the pretrained longformer model with the MLM head and it's tokenizer (RoBERTa).","metadata":{}},{"cell_type":"code","source":"MODEL_CKPT = 'allenai/longformer-base-4096'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nmodel = AutoModelForMaskedLM.from_pretrained(MODEL_CKPT).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T23:45:41.628609Z","iopub.execute_input":"2022-02-15T23:45:41.629055Z","iopub.status.idle":"2022-02-15T23:45:47.827506Z","shell.execute_reply.started":"2022-02-15T23:45:41.629013Z","shell.execute_reply":"2022-02-15T23:45:47.826742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_CKPT)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T23:45:47.828989Z","iopub.execute_input":"2022-02-15T23:45:47.829242Z","iopub.status.idle":"2022-02-15T23:45:51.95497Z","shell.execute_reply.started":"2022-02-15T23:45:47.829208Z","shell.execute_reply":"2022-02-15T23:45:51.954117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using the map function to tokenize the dataset in batches","metadata":{}},{"cell_type":"code","source":"def tokenize_function(batched_data):\n    result = tokenizer(batched_data['text'], padding='max_length', truncation=True, max_length=1024)\n    if tokenizer.is_fast:\n        result['word_ids'] = [result.word_ids(i) for i in range(len(result['input_ids']))]\n    return result\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=['text', 'id'])\n# tokenized_datasets[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-15T23:45:51.956127Z","iopub.execute_input":"2022-02-15T23:45:51.956379Z","iopub.status.idle":"2022-02-15T23:46:20.071608Z","shell.execute_reply.started":"2022-02-15T23:45:51.956344Z","shell.execute_reply":"2022-02-15T23:46:20.070585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2022-02-15T23:46:20.073176Z","iopub.execute_input":"2022-02-15T23:46:20.073461Z","iopub.status.idle":"2022-02-15T23:46:20.079269Z","shell.execute_reply.started":"2022-02-15T23:46:20.073422Z","shell.execute_reply":"2022-02-15T23:46:20.078526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Breaking the text into chunks of 1024 for training","metadata":{}},{"cell_type":"code","source":"chunk_size = 1024\ndef group_texts(batched_data):\n    concatenated_examples = {k: sum(batched_data[k], []) for k in batched_data.keys()}\n    total_length = len(concatenated_examples[list(batched_data.keys())[0]])\n    total_length = (total_length // chunk_size) * chunk_size\n    result = {k : [t[i: i+chunk_size] for i in range(0, total_length, chunk_size)] for k, t in concatenated_examples.items()}\n    result['labels'] = result['input_ids'].copy()\n    return result","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-15T23:46:20.080591Z","iopub.execute_input":"2022-02-15T23:46:20.081061Z","iopub.status.idle":"2022-02-15T23:46:20.089454Z","shell.execute_reply.started":"2022-02-15T23:46:20.081017Z","shell.execute_reply":"2022-02-15T23:46:20.088654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lm_datasets = tokenized_datasets.map(group_texts, batched=True)\nlm_datasets","metadata":{"execution":{"iopub.status.busy":"2022-02-15T23:46:20.091079Z","iopub.execute_input":"2022-02-15T23:46:20.09152Z","iopub.status.idle":"2022-02-15T23:48:18.001929Z","shell.execute_reply.started":"2022-02-15T23:46:20.091481Z","shell.execute_reply":"2022-02-15T23:48:18.001092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Applying the masking of random words (tokens of even subwords) to enable MLM fine-tuning","metadata":{}},{"cell_type":"code","source":"data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T23:48:18.004897Z","iopub.execute_input":"2022-02-15T23:48:18.005122Z","iopub.status.idle":"2022-02-15T23:48:18.009409Z","shell.execute_reply.started":"2022-02-15T23:48:18.005096Z","shell.execute_reply":"2022-02-15T23:48:18.008596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_split = lm_datasets.train_test_split(test_size=0.1)\ndataset_split","metadata":{"execution":{"iopub.status.busy":"2022-02-15T23:48:18.0105Z","iopub.execute_input":"2022-02-15T23:48:18.011001Z","iopub.status.idle":"2022-02-15T23:48:18.029308Z","shell.execute_reply.started":"2022-02-15T23:48:18.010965Z","shell.execute_reply":"2022-02-15T23:48:18.028699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nbatch_size = 4\n# Show the training loss with every epoch\nlogging_steps = len(dataset_split[\"train\"]) // batch_size\nmodel_name = MODEL_CKPT.split(\"/\")[-1]\n\ntraining_args = TrainingArguments(\n    output_dir=f\"/kaggle/working/{model_name}-finetuned-essay\",\n    overwrite_output_dir=False,\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    push_to_hub=False,\n    fp16=True,\n    logging_steps=logging_steps,\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T23:48:18.030386Z","iopub.execute_input":"2022-02-15T23:48:18.030621Z","iopub.status.idle":"2022-02-15T23:48:18.051835Z","shell.execute_reply.started":"2022-02-15T23:48:18.030588Z","shell.execute_reply":"2022-02-15T23:48:18.051167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset_split[\"train\"],\n    eval_dataset=dataset_split[\"test\"],\n    data_collator=data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T23:48:18.05349Z","iopub.execute_input":"2022-02-15T23:48:18.053958Z","iopub.status.idle":"2022-02-15T23:48:18.512423Z","shell.execute_reply.started":"2022-02-15T23:48:18.053923Z","shell.execute_reply":"2022-02-15T23:48:18.511523Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_split['train']","metadata":{"execution":{"iopub.status.busy":"2022-02-15T23:48:18.513977Z","iopub.execute_input":"2022-02-15T23:48:18.514252Z","iopub.status.idle":"2022-02-15T23:48:18.520291Z","shell.execute_reply.started":"2022-02-15T23:48:18.514212Z","shell.execute_reply":"2022-02-15T23:48:18.519582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T23:48:18.521566Z","iopub.execute_input":"2022-02-15T23:48:18.521989Z","iopub.status.idle":"2022-02-15T23:57:21.162902Z","shell.execute_reply.started":"2022-02-15T23:48:18.521951Z","shell.execute_reply":"2022-02-15T23:57:21.161772Z"},"scrolled":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\neval_results = trainer.evaluate()\nprint(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-15T23:57:21.164332Z","iopub.status.idle":"2022-02-15T23:57:21.16492Z","shell.execute_reply.started":"2022-02-15T23:57:21.164657Z","shell.execute_reply":"2022-02-15T23:57:21.164699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T23:57:21.166108Z","iopub.status.idle":"2022-02-15T23:57:21.166703Z","shell.execute_reply.started":"2022-02-15T23:57:21.166446Z","shell.execute_reply":"2022-02-15T23:57:21.166473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}