{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I have repeatedly experienced submission errors in the past few days, such as \"Notebook Threw Exception\", \"Notebook Exceeded Allowed Compute\" and \"Notebook Timeout\". Now, I modified the inference code for fast and safe submission(one deberta-large model takes only 17 minute), it works fine now. Hope it can helps.\n\nThanks for the following greate notebooks:\n\n1. https://www.kaggle.com/hengck23/1-birdformer-1-longformer-one-fold\n2. https://www.kaggle.com/abhishek/two-longformers-are-better-than-1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-06T07:23:16.027425Z","iopub.execute_input":"2022-03-06T07:23:16.027812Z","iopub.status.idle":"2022-03-06T07:23:16.082032Z","shell.execute_reply.started":"2022-03-06T07:23:16.027644Z","shell.execute_reply":"2022-03-06T07:23:16.081319Z"}}},{"cell_type":"code","source":"# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py', \"deberta__init__.py\"]:\n    if str(filename).startswith(\"deberta\"):\n        filepath = deberta_v2_path/str(filename).replace(\"deberta\", \"\")\n    else:\n        filepath = deberta_v2_path/filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T12:25:24.723837Z","iopub.execute_input":"2022-03-15T12:25:24.724195Z","iopub.status.idle":"2022-03-15T12:25:24.793025Z","shell.execute_reply.started":"2022-03-15T12:25:24.724101Z","shell.execute_reply":"2022-03-15T12:25:24.791972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/tez-lib/\")\n\nimport gc\nimport numpy as np\nimport glob\nimport pandas as pd\nfrom timeit import default_timer as timer\nfrom joblib import Parallel, delayed\n\nimport torch\nfrom torch.nn.parallel.data_parallel import data_parallel\n\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import *\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\nimport torch.nn as nn\nimport psutil\nimport torch.cuda.amp as amp\nimport os\nimport tez\n\n\n\nis_amp   = True  #True #False\nis_cuda  = True\nis_debug = False\n\nmax_length = 1536\nsubmit_dir = ''\n\n\n#helper\ndef time_to_str(t, mode='min'):\n    if mode=='min':\n        t  = int(t)/60\n        hr = t//60\n        min = t%60\n        return '%2d hr %02d min'%(hr,min)\n    elif mode=='sec':\n        t   = int(t)\n        min = t//60\n        sec = t%60\n        return '%2d min %02d sec'%(min,sec)\n    else:\n        raise NotImplementedError","metadata":{"execution":{"iopub.status.busy":"2022-03-15T12:25:24.79897Z","iopub.execute_input":"2022-03-15T12:25:24.799384Z","iopub.status.idle":"2022-03-15T12:25:27.20839Z","shell.execute_reply.started":"2022-03-15T12:25:24.799345Z","shell.execute_reply":"2022-03-15T12:25:27.206347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#config \n\ndiscourse_marker_to_label = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 2,\n    \"I-Position\": 3,\n    \"B-Evidence\": 4,\n    \"I-Evidence\": 5,\n    \"B-Claim\": 6,\n    \"I-Claim\": 7,\n    \"B-Concluding Statement\": 8,\n    \"I-Concluding Statement\": 9,\n    \"B-Counterclaim\": 10,\n    \"I-Counterclaim\": 11,\n    \"B-Rebuttal\": 12,\n    \"I-Rebuttal\": 13,\n    \"O\": 14,\n    \"PAD\": -100,\n}\nlabel_to_discourse_marker = {v: k for k, v in discourse_marker_to_label.items()}\nnum_discourse_marker = 15 #len(label_to_discourse_marker)-1 #15\n\nlength_threshold = {\n    'Lead'                : 9,\n    'Position'            : 5,\n    'Claim'               : 3,\n    'Counterclaim'        : 6,\n    'Rebuttal'            : 4,\n    'Evidence'            : 14,\n    'Concluding Statement': 11,\n}\nprobability_threshold = {\n    'Lead'                : 0.70,\n    'Position'            : 0.55,\n    'Claim'               : 0.55,\n    'Counterclaim'        : 0.50,\n    'Rebuttal'            : 0.55,\n    'Evidence'            : 0.65,\n    'Concluding Statement': 0.70,\n}\n\n\nif is_debug:\n    text_dir = '../input/feedback-prize-2021/train'\n    df = pd.read_csv('../input/creating-folds-properly-hopefully-p')\n    valid_df = df[:90000]\n    # valid_df = df[df['kfold'] == 0].reset_index(drop=True)\n    valid_id = valid_df['id'].unique()\n\nelse:\n    text_dir = '../input/feedback-prize-2021/test'\n    valid_id = [ f.split('/')[-1][:-4] for f in glob.glob(text_dir+'/*.txt') ]\n    valid_id = sorted(valid_id)\nnum_valid = len(valid_id)\nprint('len(valid_id)',len(valid_id))\n\ndf_text=[]\nfor id in valid_id:\n    text_file = text_dir +'/%s.txt'%id\n    with open(text_file, 'r') as f:\n        text = f.read()\n\n    text = text.replace(u'\\xa0', u' ')\n    text = text.rstrip()\n    text = text.lstrip()\n    df_text.append((id,text))\ndf_text = pd.DataFrame(df_text, columns=['id','text'])\ndf_text['text_len'] = df_text['text'].apply(lambda x: len(x))\ndf_text = df_text.sort_values('text_len').reset_index(drop=True)\ndel df_text['text_len']\n\nprint('df_text.shape',df_text.shape)\nprint(df_text)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T12:25:27.213516Z","iopub.execute_input":"2022-03-15T12:25:27.216191Z","iopub.status.idle":"2022-03-15T12:25:27.28415Z","shell.execute_reply.started":"2022-03-15T12:25:27.21613Z","shell.execute_reply":"2022-03-15T12:25:27.28295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _prepare_test_data_helper(tokenizer, ids):\n    test_samples = []\n    for idx in ids:\n        text_file = text_dir +'/%s.txt' % idx\n        with open(text_file, 'r') as f:\n            text = f.read()\n\n        text = text.replace(u'\\xa0', u' ')\n        text = text.rstrip()\n        text = text.lstrip()\n\n        encoded_text = tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        offset_mapping = encoded_text[\"offset_mapping\"]\n\n        sample = {\n            \"id\": idx,\n            \"input_ids\": input_ids,\n            \"text\": text,\n            \"offset_mapping\": offset_mapping,\n        }\n\n        test_samples.append(sample)\n    return test_samples\n\n\ndef prepare_test_data(df, tokenizer):\n    test_samples = []\n    ids = df[\"id\"].unique()\n    ids_splits = np.array_split(ids, 4)\n\n    results = Parallel(n_jobs=4, backend=\"multiprocessing\")(\n        delayed(_prepare_test_data_helper)(tokenizer, idx) for idx in ids_splits\n    )\n    for result in results:\n        test_samples.extend(result)\n\n    return test_samples\n\n\nclass FeedbackDataset:\n    def __init__(self, samples, max_len, tokenizer):\n        self.samples = samples\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.length = len(samples)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        input_ids = self.samples[idx][\"input_ids\"]\n        input_id = self.samples[idx][\"id\"]\n        input_text = self.samples[idx][\"text\"]\n        input_offset = self.samples[idx][\"offset_mapping\"]\n        \n\n        # add start token id to the input_ids\n        input_ids = [self.tokenizer.cls_token_id] + input_ids\n\n        if len(input_ids) > self.max_len - 1:\n            input_ids = input_ids[: self.max_len - 1]\n\n        # add end token id to the input_ids\n        input_ids = input_ids + [self.tokenizer.sep_token_id]\n        attention_mask = [1] * len(input_ids)\n\n        return {\n            \"id\":input_id,\n            'text':input_text,\n            \"token_id\": input_ids,\n            \"token_mask\": attention_mask,\n            \"token_offset\":str(input_offset),\n        }\n\nclass Collate:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"id\"] = [sample[\"id\"] for sample in batch]\n        output[\"token_offset\"] = [sample[\"token_offset\"] for sample in batch]\n        output[\"text\"] = [sample[\"text\"] for sample in batch]\n        \n        output[\"token_id\"] = [sample[\"token_id\"] for sample in batch]\n        output[\"token_mask\"] = [sample[\"token_mask\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(token_id) for token_id in output[\"token_id\"]])\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"token_id\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"token_id\"]]\n            output[\"token_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"token_mask\"]]\n        else:\n            output[\"token_id\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"token_id\"]]\n            output[\"token_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"token_mask\"]]\n\n        # convert to tensors\n        output[\"token_id\"] = torch.tensor(output[\"token_id\"], dtype=torch.long)\n        output[\"token_mask\"] = torch.tensor(output[\"token_mask\"], dtype=torch.long)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-03-15T12:25:27.286793Z","iopub.execute_input":"2022-03-15T12:25:27.287119Z","iopub.status.idle":"2022-03-15T12:25:27.312419Z","shell.execute_reply.started":"2022-03-15T12:25:27.287063Z","shell.execute_reply":"2022-03-15T12:25:27.311342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackModel(tez.Model):\n    def __init__(self, model_name, num_labels=num_discourse_marker):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n        config = AutoConfig.from_pretrained(model_name)\n\n        hidden_dropout_prob: float = 0.1\n        layer_norm_eps: float = 1e-7\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = AutoModel.from_config(config)\n        self.output = nn.Linear(config.hidden_size, self.num_labels)\n\n    def forward(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        logits = self.output(sequence_output)\n        logits = torch.softmax(logits, dim=-1)\n        return logits, 0, {}\n\n\ncheckpoint =[   \n    [\n     '../input/feedback-prize-submit-01/deberta-large-1024/deberta-large-1024/model_db-large-0.bin',\n    '../input/feedback-prize-submit-01/deberta-large-1024/deberta-large-1024/model_db-large-1.bin',\n    '../input/feedback-prize-submit-01/deberta-large-1024/deberta-large-1024/model_db-large-2.bin',\n    '../input/feedback-prize-submit-01/deberta-large-1024/deberta-large-1024/model_db-large-3.bin',\n    '../input/feedback-prize-submit-01/deberta-large-1024/deberta-large-1024/model_db-large-4.bin',\n    ],\n    [\n     '../input/feedback-prize-submit-01/db-large-1024/db-large-1024/model_dbSCE-large-1024_0.bin',\n    '../input/feedback-prize-submit-01/db-large-1024/db-large-1024/model_dbSCE-large-1024_1.bin',\n    '../input/feedback-prize-submit-01/db-large-1024/db-large-1024/model_dbSCE-large-1024_2.bin',\n    '../input/feedback-prize-submit-01/db-large-1024/db-large-1024/model_dbSCE-large-1024_3.bin',\n    '../input/feedback-prize-submit-01/db-large-1024/db-large-1024/model_dbSCE-large-1024_4.bin',\n    \n    ],\n    [\n    '../input/feedback-prize-submit-03/dbv3-large-1024/dbv3-large-1024/model_dbv3-1024_0.bin',\n    '../input/feedback-prize-submit-03/dbv3-large-1024/dbv3-large-1024/model_dbv3-1024_1.bin',\n    '../input/feedback-prize-submit-03/dbv3-large-1024/dbv3-large-1024/model_dbv3-1024_2.bin',\n    '../input/feedback-prize-submit-03/dbv3-large-1024/dbv3-large-1024/model_dbv3-1024_3.bin',\n    '../input/feedback-prize-submit-03/dbv3-large-1024/dbv3-large-1024/model_dbv3-1024_4.bin',\n    ],\n    [\n    '../input/feedback-prize-submit-02/db-large-mnli_aug2/db-large-mnli_aug2/model_0.bin',\n    '../input/feedback-prize-submit-02/db-large-mnli_aug2/db-large-mnli_aug2/model_1.bin',\n    '../input/feedback-prize-submit-02/db-large-mnli_aug2/db-large-mnli_aug2/model_2.bin',\n    '../input/feedback-prize-submit-02/db-large-mnli_aug2/db-large-mnli_aug2/model_3.bin',\n    '../input/feedback-prize-submit-02/db-large-mnli_aug2/db-large-mnli_aug2/model_4.bin',\n    ]\n    \n]\n\nnet_type = [\n    \n    [FeedbackModel,   '../input/huggingface-deberta-variants/deberta-large/deberta-large/' ],\n    [FeedbackModel,   '../input/huggingface-deberta-variants/deberta-large/deberta-large/' ],\n    [FeedbackModel,   '../input/deberta-v3-large/deberta-v3-large/' ],\n    [FeedbackModel,   '../input/huggingface-deberta-variants/deberta-large-mnli/deberta-large-mnli/' ],\n    #[FeedbackModel,  '../input/funneltransformerlarge'],\n]\n\nnum_net = sum([len(i) for i in checkpoint])\nnum_net1 = sum([len(i) for i in checkpoint[:1]])\nnum_net2 = sum([len(i) for i in checkpoint[:2]])\nnum_net3 = sum([len(i) for i in checkpoint[:3]])\nTOTAL_MODELS = len(checkpoint)\nTOTAL_MODELS","metadata":{"execution":{"iopub.status.busy":"2022-03-15T12:25:27.315473Z","iopub.execute_input":"2022-03-15T12:25:27.315741Z","iopub.status.idle":"2022-03-15T12:25:27.343393Z","shell.execute_reply.started":"2022-03-15T12:25:27.315711Z","shell.execute_reply":"2022-03-15T12:25:27.342324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#processing\n\ndef text_to_word(text):\n    word = text.split()\n    word_offset = []\n\n    start = 0\n    for w in word:\n        r = text[start:].find(w)\n\n        if r==-1:\n            raise NotImplementedError\n        else:\n            start = start+r\n            end   = start+len(w)\n            word_offset.append((start,end))\n            #print('%32s'%w, '%5d'%start, '%5d'%r, text[start:end])\n        start = end\n\n    return word, word_offset\n\ndef word_probability_to_predict_df(text_to_word_probability, id):\n    len_word = len(text_to_word_probability)\n    word_predict = text_to_word_probability.argmax(-1)\n    word_score   = text_to_word_probability.max(-1)\n    predict_df = []\n\n    t = 0\n    while 1:\n        if word_predict[t] not in [\n            discourse_marker_to_label['O'],\n            discourse_marker_to_label['PAD'],\n        ]:\n            start = t\n            b_marker_label = word_predict[t]\n        else:\n            t = t+1\n            if t== len_word-1: break\n            continue\n\n        t = t+1\n        if t== len_word-1: break\n\n        #----\n        if   label_to_discourse_marker[b_marker_label][0]=='B':\n            i_marker_label = b_marker_label+1\n        elif label_to_discourse_marker[b_marker_label][0]=='I':\n            i_marker_label = b_marker_label\n        else:\n            raise NotImplementedError\n\n        while 1:\n            #print(t)\n            if (word_predict[t] != i_marker_label) or (t ==len_word-1):\n                end = t\n                prediction_string = ' '.join([str(i) for i in range(start,end)]) #np.arange(start,end).tolist()\n                discourse_type = label_to_discourse_marker[b_marker_label][2:]\n                discourse_score = word_score[start:end].tolist()\n                predict_df.append((id, discourse_type, prediction_string, str(discourse_score)))\n                #print(predict_df[-1])\n                break\n            else:\n                t = t+1\n                continue\n        if t== len_word-1: break\n\n    predict_df = pd.DataFrame(predict_df, columns=['id', 'class', 'predictionstring', 'score'])\n    return predict_df\n\ndef do_threshold(submit_df, use=['length','probability']):\n    df = submit_df.copy()\n    df = df.fillna('')\n\n    if 'length' in use:\n        df['l'] = df.predictionstring.apply(lambda x: len(x.split()))\n        for key, value in length_threshold.items():\n            #value=3\n            index = df.loc[df['class'] == key].query('l<%d'%value).index\n            df.drop(index, inplace=True)\n\n    if 'probability' in use:\n        df['s'] = df.score.apply(lambda x: np.mean(eval(x)))\n        for key, value in probability_threshold.items():\n            index = df.loc[df['class'] == key].query('s<%f'%value).index\n            df.drop(index, inplace=True)\n\n    df = df[['id', 'class', 'predictionstring']]\n    return df\n\n#evaluation for debug ----\n# https://www.kaggle.com/cpmpml/faster-metric-computation\n\ndef compute_overlap(predict, truth):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    # Length of each and intersection\n    try:\n        len_truth   = len(truth)\n        len_predict = len(predict)\n        intersect = len(truth & predict)\n        overlap1 = intersect/ len_truth\n        overlap2 = intersect/ len_predict\n        return (overlap1, overlap2)\n    except:  # at least one of the input is NaN\n        return (0, 0)\n\ndef compute_f1_score_one(predict_df, truth_df, discourse_type):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n\n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    t_df = truth_df.loc[truth_df['discourse_type'] == discourse_type,   ['id', 'predictionstring']].reset_index(drop=True)\n    p_df = predict_df.loc[predict_df['class'] == discourse_type,  ['id', 'predictionstring']].reset_index(drop=True)\n\n    p_df.loc[:,'predict_id'] = p_df.index\n    t_df.loc[:,'truth_id'] = t_df.index\n    p_df.loc[:,'predictionstring'] = [set(p.split(' ')) for p in p_df['predictionstring']]\n    t_df.loc[:,'predictionstring'] = [set(p.split(' ')) for p in t_df['predictionstring']]\n\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = p_df.merge(t_df,\n                           left_on='id',\n                           right_on='id',\n                           how='outer',\n                           suffixes=('_p','_t')\n                          )\n    overlap = [compute_overlap(*predictionstring) for predictionstring in zip(joined.predictionstring_p, joined.predictionstring_t)]\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['potential_TP'] = [(o[0] >= 0.5 and o[1] >= 0.5) for o in overlap]\n    joined['max_overlap' ] = [max(*o) for o in overlap]\n    joined_tp = joined.query('potential_TP').reset_index(drop=True)\n    tp_pred_ids = joined_tp\\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','truth_id'])['predict_id'].first()\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = set(joined['predict_id'].unique()) - set(tp_pred_ids)\n\n    matched_gt_ids   = joined_tp['truth_id'].unique()\n    unmatched_gt_ids = set(joined['truth_id'].unique()) -  set(matched_gt_ids)\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    f1 = TP / (TP + 0.5*(FP+FN))\n    return f1\n\ndef compute_lb_f1_score(predict_df, truth_df):\n    f1_score = {}\n    for discourse_type in truth_df.discourse_type.unique():\n        f1_score[discourse_type] = compute_f1_score_one(predict_df, truth_df, discourse_type)\n    #f1 = np.mean([v for v in class_scores.values()])\n    return f1_score","metadata":{"execution":{"iopub.status.busy":"2022-03-15T12:25:27.345883Z","iopub.execute_input":"2022-03-15T12:25:27.346664Z","iopub.status.idle":"2022-03-15T12:25:27.384099Z","shell.execute_reply.started":"2022-03-15T12:25:27.346605Z","shell.execute_reply":"2022-03-15T12:25:27.383016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def memory_used_to_str():\n    pid = os.getpid()\n    processs = psutil.Process(pid)\n    memory_use = processs.memory_info()[0] / 2. ** 30\n    return 'ram memory gb :' + str(np.round(memory_use, 2))\nif 1:\n    print('start', memory_used_to_str())\n##############################################################\n\ndef run_submit():\n    if is_debug: print(\"THIS IS DEBUG ####################################\")\n    results = []\n    \n    for net_type_num in range(TOTAL_MODELS):\n        Net, arch = net_type[net_type_num]\n        net = Net(arch)\n        if 'v3' in arch:\n            from transformers.models.deberta_v2 import DebertaV2TokenizerFast\n            tokenizer = DebertaV2TokenizerFast.from_pretrained(arch)\n        else:\n            tokenizer = AutoTokenizer.from_pretrained(arch)\n\n        test_samples = prepare_test_data(df_text, tokenizer)\n        collate = Collate(tokenizer=tokenizer)\n        valid_dataset = FeedbackDataset(test_samples, max_length, tokenizer)\n        valid_loader  = DataLoader(\n            valid_dataset,\n            sampler = SequentialSampler(valid_dataset),\n            batch_size  = 8, \n            drop_last   = False,\n            num_workers = 2, \n            pin_memory  = False,\n            collate_fn = collate,\n        )    \n        ######### checkpoint\n        for n in range(len(checkpoint[net_type_num])):\n            if 'funneltransformermedium' in arch:\n                print(\"Loading with weights only false for funnel\")\n                net.load(checkpoint[net_type_num][n], weights_only=False)\n            else:    \n                net.load(checkpoint[net_type_num][n], weights_only=True)\n\n            if is_cuda:\n                net.cuda()\n            print('load ok : [%d] %s'%(n, arch))\n            print('              %s'%(checkpoint[net_type_num][n]))\n            print('after allocate net %d'%n, memory_used_to_str())\n            results_n = {\n                'id':[],\n                'token_mask':[],\n                'token_offset':[],\n                'probability':[],\n            }\n\n            T = 0\n            start_timer = timer()\n            for t, batch in enumerate(valid_loader):\n                batch_size = len(batch['id'])\n                token_mask = batch['token_mask']\n                token_id   = batch['token_id']\n                if is_cuda:\n                    token_mask = token_mask.cuda()\n                    token_id = token_id.cuda()\n\n                net.eval()\n                with torch.no_grad():\n                    with amp.autocast(enabled=is_amp):\n\n                        probability = data_parallel(net,(token_id, token_mask))\n                        # probability = net[n](token_id, token_mask)\n                        pp = (probability[0] * 255).byte().data.cpu().numpy()\n                        if pp.shape[1] > max_length:\n                            pp = pp[:, :max_length, :]\n                        else:\n                            pp = np.pad(pp,((0, 0), (0, max_length - pp.shape[1]), (0, 0)),'constant', constant_values=0) \n                        #probability = 1\n                        #pp = np.random.randint(0,255,size=[len(batch['token_offset']), max_length, 15]).astype('int8')\n                        results_n['probability'].append( pp )\n                        if n == 0:\n                            results_n['token_offset' ] += [eval(x) for x in batch['token_offset']]\n                        T += batch_size\n\n                print('\\r\\t%d/%d  %s'%(T, len(valid_dataset), time_to_str(timer() - start_timer,'sec')),end='',flush=True)\n\n            #----------------------------\n            if is_cuda: torch.cuda.empty_cache()\n            print('')\n            if n == 0:\n                results.append({\n                    'probability' : np.concatenate(results_n['probability']),\n                    'token_offset': np.array(results_n['token_offset'], object)\n                })\n            else:\n                 results.append({\n                    'probability' : np.concatenate(results_n['probability']),\n                })           \n\n            del probability, pp, results_n\n            gc.collect()\n            print('after gc.collect()', memory_used_to_str())\n            print()\n        #------------------------------------------------------------------------\n        del net, test_samples, Net, tokenizer\n        gc.collect()\n        print('after gc.collect()', memory_used_to_str())\n        print()   \n        ##############################################################\n\n    \n    ##### concat\n    submit_df = []\n    for i in range(num_valid):\n        d  = df_text.iloc[i]\n        id = d.id\n        text = d.text\n        word, word_offset = text_to_word(text)\n        token_to_text_probability = np.full((len(text),num_discourse_marker),0, np.float32)\n        for j in range(num_net):\n            p = results[j]['probability'][i][1:]/255  \n            if j < num_net1:\n                for t,(start,end) in enumerate(results[0]['token_offset'][i]):\n                    if t==max_length-1: break #assume max_length, else use token_mask to get length\n                    token_to_text_probability[start:end]+=p[t] #**0.5     \n            elif j < num_net2:\n                for t,(start,end) in enumerate(results[num_net1]['token_offset'][i]):\n                    if t==max_length-1: break #assume max_length, else use token_mask to get length\n                    token_to_text_probability[start:end]+=p[t] #**0.5  \n            elif j < num_net3:\n                for t,(start,end) in enumerate(results[num_net2]['token_offset'][i]):\n                    if t==max_length-1: break #assume max_length, else use token_mask to get length\n                    token_to_text_probability[start:end]+=p[t] #**0.5    \n            else:\n                for t,(start,end) in enumerate(results[num_net3]['token_offset'][i]):\n                    if t==max_length-1: break #assume max_length, else use token_mask to get length\n                    token_to_text_probability[start:end]+=p[t] #**0.5  \n            \n        token_to_text_probability = token_to_text_probability/num_net\n        \n        text_to_word_probability = np.full((len(word),num_discourse_marker),0, np.float32)\n        for t,(start,end) in enumerate(word_offset):\n            text_to_word_probability[t]=token_to_text_probability[start:end].mean(0)\n\n        predict_df = word_probability_to_predict_df(text_to_word_probability, id)\n        submit_df.append(predict_df)\n    print('')\n\n    #----------------------------------------\n    submit_df = pd.concat(submit_df).reset_index(drop=True) \n    submit_df = do_threshold(submit_df, use=['length', 'probability'])\n    submit_df.to_csv('submission.csv', index=False)\n\n    print('----')\n    print(submit_df.head())\n    print('submission ok!----')\n    if is_debug:\n        f1_score = compute_lb_f1_score(submit_df, valid_df)\n        print('f1 macro : %f\\n' % np.mean([v for v in f1_score.values()]))\n        for k,v in f1_score.items():\n            print('%20s : %05f'%(k,v))\n            \nrun_submit()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T12:25:27.385642Z","iopub.execute_input":"2022-03-15T12:25:27.386718Z","iopub.status.idle":"2022-03-15T12:31:53.181447Z","shell.execute_reply.started":"2022-03-15T12:25:27.386642Z","shell.execute_reply":"2022-03-15T12:31:53.180302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}