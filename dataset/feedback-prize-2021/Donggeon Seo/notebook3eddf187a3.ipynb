{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport gc\ngc.enable()\nfrom tqdm import tqdm\nimport os\ntest_names, test_texts = [], []\ntest_texts_list = []\nfor f in tqdm(list(os.listdir('../input/feedback-prize-2021/test'))):\n    test_names.append(f.replace('.txt', ''))\n    test_texts.append(open('../input/feedback-prize-2021/test/' + f, 'r', encoding='utf-8').read())\nfor t in test_texts:\n    test_texts_list.append(t.split())\ntest_text_df = pd.DataFrame({'id': test_names, 'text_list': test_texts_list})","metadata":{"execution":{"iopub.status.busy":"2022-03-09T08:11:03.537445Z","iopub.execute_input":"2022-03-09T08:11:03.53797Z","iopub.status.idle":"2022-03-09T08:11:03.567355Z","shell.execute_reply.started":"2022-03-09T08:11:03.537931Z","shell.execute_reply":"2022-03-09T08:11:03.566554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\ntest_datasets = Dataset.from_pandas(test_text_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T08:11:03.569268Z","iopub.execute_input":"2022-03-09T08:11:03.56952Z","iopub.status.idle":"2022-03-09T08:11:03.578073Z","shell.execute_reply.started":"2022-03-09T08:11:03.569487Z","shell.execute_reply":"2022-03-09T08:11:03.577373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('../input/longformerlstm/best_model/longformer-large-LSTM_fold0', add_prefix_space=True)\neval_datasets = {'word_ids':[], 'id':[]}\ndef preparing_test_dataset(examples):\n    encoding = tokenizer(examples['text_list'], truncation=True, padding=False, max_length = 4096, is_split_into_words=True)\n    total= len(encoding['input_ids'])\n    for i in range(total):\n        word_idx = encoding.word_ids(batch_index=i)\n        eval_datasets['word_ids'].append(word_idx)\n        eval_datasets['id'].append(examples['id'][i])\n    return encoding\n\ntokenized_test_datasets = test_datasets.map(preparing_test_dataset, batched=True, remove_columns=test_datasets.column_names)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T08:11:03.579206Z","iopub.execute_input":"2022-03-09T08:11:03.579717Z","iopub.status.idle":"2022-03-09T08:11:03.805978Z","shell.execute_reply.started":"2022-03-09T08:11:03.579681Z","shell.execute_reply":"2022-03-09T08:11:03.805201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\nfrom dataclasses import dataclass\nimport torch\n\n@dataclass\nclass CustomDatacollator(DataCollatorForTokenClassification):\n\n        def torch_call(self, features):\n\n            label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n            labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n            batch = dict()\n            batch['input_ids'] = [feature['input_ids'] for feature in features]\n            batch['attention_mask'] = [feature['attention_mask'] for feature in features]\n\n            batch_max = max([len(ids) for ids in batch['input_ids']])\n            padding_side = self.tokenizer.padding_side\n            if padding_side == \"right\":\n                batch['input_ids'] = [s + (batch_max - len(s))*[self.tokenizer.pad_token_id] for s in batch['input_ids']]\n                batch['attention_mask'] = [s + (batch_max - len(s))*[0] for s in batch['attention_mask']]\n            else : \n                batch['input_ids'] = [(batch_max - len(s))*[self.tokenizer.pad_token_id] + s for s in batch['input_ids']]\n                batch['attention_mask'] = [(batch_max - len(s))*[0] + s for s in batch['attention_mask']]\n            \n            if labels is None:\n                batch = {k: torch.tensor(v, dtype=torch.int) for k, v in batch.items()}\n                return batch\n            if padding_side == \"right\":\n                batch[label_name] = [\n                    list(label) + [self.label_pad_token_id] * (batch_max - len(label)) for label in labels\n                ]\n            else:\n                batch[label_name] = [\n                    [self.label_pad_token_id] * (batch_max - len(label)) + list(label) for label in labels\n                ]\n            batch = {k: torch.tensor(v, dtype=torch.int) for k, v in batch.items()}\n            return batch","metadata":{"execution":{"iopub.status.busy":"2022-03-09T08:11:03.807568Z","iopub.execute_input":"2022-03-09T08:11:03.808034Z","iopub.status.idle":"2022-03-09T08:11:03.824997Z","shell.execute_reply.started":"2022-03-09T08:11:03.807984Z","shell.execute_reply":"2022-03-09T08:11:03.82419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import LongformerPreTrainedModel, LongformerModel\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nfrom collections import OrderedDict\nfrom typing import Optional, Tuple, Any\nfrom dataclasses import dataclass, fields\nimport numpy as np\n\n\n_CHECKPOINT_FOR_DOC = \"allenai/longformer-base-4096\"\n_CONFIG_FOR_DOC = \"LongformerConfig\"\n_TOKENIZER_FOR_DOC = \"LongformerTokenizer\"\n\n\ndef is_tensor(x):\n    if isinstance(x, torch.Tensor):\n        return True\n    return isinstance(x, np.ndarray)\n\nclass ModelOutput(OrderedDict):\n    \"\"\"\n    Base class for all model outputs as dataclass. Has a `__getitem__` that allows indexing by integer or slice (like a\n    tuple) or strings (like a dictionary) that will ignore the `None` attributes. Otherwise behaves like a regular\n    python dictionary.\n    <Tip warning={true}>\n    You can't unpack a `ModelOutput` directly. Use the [`~file_utils.ModelOutput.to_tuple`] method to convert it to a\n    tuple before.\n    </Tip>\n    \"\"\"\n\n    def __post_init__(self):\n        class_fields = fields(self)\n\n        # Safety and consistency checks\n        if not len(class_fields):\n            raise ValueError(f\"{self.__class__.__name__} has no fields.\")\n        if not all(field.default is None for field in class_fields[1:]):\n            raise ValueError(f\"{self.__class__.__name__} should not have more than one required field.\")\n\n        first_field = getattr(self, class_fields[0].name)\n        other_fields_are_none = all(getattr(self, field.name) is None for field in class_fields[1:])\n\n        if other_fields_are_none and not is_tensor(first_field):\n            if isinstance(first_field, dict):\n                iterator = first_field.items()\n                first_field_iterator = True\n            else:\n                try:\n                    iterator = iter(first_field)\n                    first_field_iterator = True\n                except TypeError:\n                    first_field_iterator = False\n\n            # if we provided an iterator as first field and the iterator is a (key, value) iterator\n            # set the associated fields\n            if first_field_iterator:\n                for element in iterator:\n                    if (\n                        not isinstance(element, (list, tuple))\n                        or not len(element) == 2\n                        or not isinstance(element[0], str)\n                    ):\n                        break\n                    setattr(self, element[0], element[1])\n                    if element[1] is not None:\n                        self[element[0]] = element[1]\n            elif first_field is not None:\n                self[class_fields[0].name] = first_field\n        else:\n            for field in class_fields:\n                v = getattr(self, field.name)\n                if v is not None:\n                    self[field.name] = v\n\n    def __delitem__(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``__delitem__`` on a {self.__class__.__name__} instance.\")\n\n    def setdefault(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``setdefault`` on a {self.__class__.__name__} instance.\")\n\n    def pop(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``pop`` on a {self.__class__.__name__} instance.\")\n\n    def update(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``update`` on a {self.__class__.__name__} instance.\")\n\n    def __getitem__(self, k):\n        if isinstance(k, str):\n            inner_dict = {k: v for (k, v) in self.items()}\n            return inner_dict[k]\n        else:\n            return self.to_tuple()[k]\n\n    def __setattr__(self, name, value):\n        if name in self.keys() and value is not None:\n            # Don't call self.__setitem__ to avoid recursion errors\n            super().__setitem__(name, value)\n        super().__setattr__(name, value)\n\n    def __setitem__(self, key, value):\n        # Will raise a KeyException if needed\n        super().__setitem__(key, value)\n        # Don't call self.__setattr__ to avoid recursion errors\n        super().__setattr__(key, value)\n\n    def to_tuple(self) -> Tuple[Any]:\n        \"\"\"\n        Convert self to a tuple containing all the attributes/keys that are not `None`.\n        \"\"\"\n        return tuple(self[k] for k in self.keys())\n\n@dataclass\nclass LongformerTokenClassifierOutput(ModelOutput):\n    \"\"\"\n    Base class for outputs of token classification models.\n    Args:\n        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) :\n            Classification loss.\n        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n            Classification scores (before SoftMax).\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n            shape `(batch_size, sequence_length, hidden_size)`.\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x +\n            attention_window + 1)`, where `x` is the number of tokens with global attention mask.\n            Local attentions weights after the attention softmax, used to compute the weighted average in the\n            self-attention heads. Those are the attention weights from every token in the sequence to every token with\n            global attention (first `x` values) and to every token in the attention window (remaining `attention_window\n            + 1` values). Note that the first `x` values refer to tokens with fixed positions in the text, but the\n            remaining `attention_window + 1` values refer to tokens with relative positions: the attention weight of a\n            token to itself is located at index `x + attention_window / 2` and the `attention_window / 2` preceding\n            (succeeding) values are the attention weights to the `attention_window / 2` preceding (succeeding) tokens.\n            If the attention window contains a token with global attention, the attention weight at the corresponding\n            index is set to 0; the value should be accessed from the first `x` attention weights. If a token has global\n            attention, the attention weights to all other tokens in `attentions` is set to 0, the values should be\n            accessed from `global_attentions`.\n        global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n            where `x` is the number of tokens with global attention mask.\n            Global attentions weights after the attention softmax, used to compute the weighted average in the\n            self-attention heads. Those are the attention weights from every token with global attention to every token\n            in the sequence.\n    \"\"\"\n\n    loss: Optional[torch.FloatTensor] = None\n    logits: torch.FloatTensor = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None\n    global_attentions: Optional[Tuple[torch.FloatTensor]] = None\n\nclass CustomLongformerForTokenClassification(LongformerPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.longformer = LongformerModel(config, add_pooling_layer=False)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.lstm = nn.LSTM(input_size= config.hidden_size, hidden_size= config.hidden_size, num_layers= 2, dropout= 0.1, batch_first= True, bidirectional= True)\n        self.LayerNorm_LSTM = nn.LayerNorm(config.hidden_size*2, eps=config.layer_norm_eps)\n        self.dropout_lstm = nn.Dropout(config.hidden_dropout_prob)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n        self.classifier = nn.Linear(config.hidden_size*2, config.num_labels)\n\n        # Initialize weights and apply final processing\n#         self.post_init()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        global_attention_mask=None,\n        head_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.longformer(\n            input_ids,\n            attention_mask=attention_mask,\n            global_attention_mask=global_attention_mask,\n            head_mask=head_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        sequence_output,_ = self.lstm(sequence_output)\n        sequence_output = self.LayerNorm_LSTM(sequence_output)\n        logits = self.classifier(sequence_output)\n        \n        loss = None\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return LongformerTokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            global_attentions=outputs.global_attentions,\n        )","metadata":{"execution":{"iopub.status.busy":"2022-03-09T08:11:03.827884Z","iopub.execute_input":"2022-03-09T08:11:03.828271Z","iopub.status.idle":"2022-03-09T08:11:03.864201Z","shell.execute_reply.started":"2022-03-09T08:11:03.828182Z","shell.execute_reply":"2022-03-09T08:11:03.863512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, AutoConfig, Trainer, DataCollatorForTokenClassification, TrainingArguments\nimport numpy as np\nimport torch\ndata_collator = CustomDatacollator(tokenizer)\nall_predictions = []\ntraining_args = TrainingArguments(per_device_eval_batch_size=4, output_dir = '../input')\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nfor fold in range(5):\n    model_path = '../input/longformerlstm/best_model/longformer-large-LSTM_fold'+str(fold)+'/'\n    config = AutoConfig.from_pretrained(model_path)\n    model = CustomLongformerForTokenClassification.from_pretrained(model_path, config = config)\n    trainer = Trainer(\n        model = model,\n        args = training_args,\n        train_dataset=None,\n        eval_dataset=tokenized_test_datasets,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n    predictions, _, _ = trainer.predict(test_dataset = tokenized_test_datasets)\n    print(\"shape of prediciton\", predictions.shape)\n    current_idx = 0\n    for preds in predictions:\n        preds = preds.astype(np.float32)\n        preds = preds/5\n        if fold==0:\n            all_predictions.append(preds)\n        else:\n            all_predictions[current_idx] += preds\n            current_idx +=1\n    torch.cuda.empty_cache()\n    gc.collect()\nall_predictions = np.array(all_predictions)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T08:11:03.865527Z","iopub.execute_input":"2022-03-09T08:11:03.865826Z","iopub.status.idle":"2022-03-09T08:12:55.681962Z","shell.execute_reply.started":"2022-03-09T08:11:03.865789Z","shell.execute_reply":"2022-03-09T08:12:55.681194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n\ndef label_dict():\n    train = pd.read_csv('../input/feedback-prize-2021/train.csv')\n    classes = train.discourse_type.unique().tolist()\n    tags = defaultdict()\n    for i, c in enumerate(classes):\n        tags[f'B-{c}'] = i\n        tags[f'I-{c}'] = i + len(classes)\n    tags[f'O'] = len(classes) * 2\n    tags[f'Special'] = -100\n    l2i = dict(tags)\n    i2l = defaultdict()\n    for k, v in l2i.items(): \n        i2l[v] = k\n    i2l[-100] = 'Special'\n    i2l = dict(i2l)\n    N_LABELS = len(i2l) - 1 # not accounting for -100\n    return i2l, l2i, N_LABELS","metadata":{"execution":{"iopub.status.busy":"2022-03-09T08:12:55.683409Z","iopub.execute_input":"2022-03-09T08:12:55.683663Z","iopub.status.idle":"2022-03-09T08:12:55.691904Z","shell.execute_reply.started":"2022-03-09T08:12:55.683626Z","shell.execute_reply":"2022-03-09T08:12:55.691202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def link_evidence(oof):\n  if not len(oof):\n    return oof\n  \n  def jn(pst, start, end):\n    return \" \".join([str(x) for x in pst[start:end]])\n  \n  thresh = 1\n  idu = oof['id'].unique()\n  eoof = oof[oof['class'] == \"Evidence\"]\n  neoof = oof[oof['class'] != \"Evidence\"]\n  eoof.index = eoof[['id', 'class']]\n  for thresh2 in range(26, 27, 1):\n    retval = []\n    for idv in tqdm(idu, desc='link_evidence', leave=False):\n      for c in ['Evidence']:\n        q = eoof[(eoof['id'] == idv)]\n        if len(q) == 0:\n          continue\n        pst = []\n        for r in q.itertuples():\n          pst = [*pst, -1,  *[int(x) for x in r.predictionstring.split()]]\n        start = 1\n        end = 1\n        for i in range(2, len(pst)):\n          cur = pst[i]\n          end = i\n          if  ((cur == -1) and ((pst[i + 1] > pst[end - 1] + thresh) or (pst[i + 1] - pst[start] > thresh2))):\n            retval.append((idv, c, jn(pst, start, end)))\n            start = i + 1\n        v = (idv, c, jn(pst, start, end + 1))\n        retval.append(v)\n    roof = pd.DataFrame(retval, columns=['id', 'class', 'predictionstring'])\n    roof = roof.merge(neoof, how='outer')\n    return roof","metadata":{"execution":{"iopub.status.busy":"2022-03-09T08:12:55.693505Z","iopub.execute_input":"2022-03-09T08:12:55.694165Z","iopub.status.idle":"2022-03-09T08:12:55.70863Z","shell.execute_reply.started":"2022-03-09T08:12:55.694096Z","shell.execute_reply":"2022-03-09T08:12:55.70776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_max_label(word_prediction_score):\n    x = np.sum(word_prediction_score, axis=0)\n    max_label = np.argmax(x, axis=-1)\n    return max_label","metadata":{"execution":{"iopub.status.busy":"2022-03-09T08:12:55.710338Z","iopub.execute_input":"2022-03-09T08:12:55.710716Z","iopub.status.idle":"2022-03-09T08:12:55.718369Z","shell.execute_reply.started":"2022-03-09T08:12:55.710675Z","shell.execute_reply":"2022-03-09T08:12:55.717476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def token_to_word(pred_score, word_id_list, i2l):\n  all_prediction = []\n  all_pred_score=[]\n  for k, label_pred_score in tqdm(enumerate(pred_score), desc = \"post-processing\"):\n    each_prediction = []\n    each_prediction_score = []\n    word_ids = word_id_list[k]\n    previous_word_idx = -1\n    word_prediction_score=[]\n    for idx, word_idx in enumerate(word_ids):\n      if word_idx == None:\n        continue\n      elif word_idx != previous_word_idx:\n        if len(word_prediction_score)!=0:\n          # find label which have the most score label following each tokens including in one word\n          max_label = find_max_label(word_prediction_score)\n          word_prediction_score = [word_prediction_score[i][max_label] for i in range(len(word_prediction_score))]\n          each_prediction_score.append(word_prediction_score)\n          each_prediction.append(i2l[max_label])\n        previous_word_idx = word_idx\n        word_prediction_score=[]\n        word_prediction_score.append(label_pred_score[idx])\n      else:\n        word_prediction_score.append(label_pred_score[idx])\n    max_label = find_max_label(word_prediction_score)\n    word_prediction_score = [word_prediction_score[i][max_label] for i in range(len(word_prediction_score))]  \n    each_prediction_score.append(word_prediction_score)\n    each_prediction.append(i2l[max_label])\n    all_prediction.append(each_prediction)\n    all_pred_score.append(each_prediction_score)\n  return all_prediction, all_pred_score","metadata":{"execution":{"iopub.status.busy":"2022-03-09T08:12:55.71973Z","iopub.execute_input":"2022-03-09T08:12:55.720074Z","iopub.status.idle":"2022-03-09T08:12:55.73155Z","shell.execute_reply.started":"2022-03-09T08:12:55.720034Z","shell.execute_reply":"2022-03-09T08:12:55.730767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocess_fb_predictions2(\n    eval_datasets,\n    predictions,\n):\n    proba_thresh = {\n        \"Lead\": 0.687,\n        \"Position\": 0.537,\n        \"Evidence\": 0.637,\n        \"Claim\": 0.537,\n        \"Concluding Statement\": 0.687,\n        \"Counterclaim\": 0.537,\n        \"Rebuttal\": 0.537,\n    }\n    #discourse length threshold\n    min_thresh = {\n        \"Lead\": 9,\n        \"Position\": 5,\n        \"Evidence\": 14,\n        \"Claim\": 3,\n        \"Concluding Statement\": 11,\n        \"Counterclaim\": 6,\n        \"Rebuttal\": 4,\n    }\n    print(predictions.shape)\n    softmax = torch.nn.Softmax(dim=-1)\n    predictions = torch.tensor(predictions)\n    pred_score = softmax(predictions)\n    pred_score = pred_score.numpy()\n    i2l, _, _ = label_dict()\n    word_id_list = list(eval_datasets['word_ids'])\n    all_prediction, all_pred_score = token_to_word(pred_score, word_id_list, i2l)\n    final_pred = []\n    for i in range(len(eval_datasets['id'])):\n      idx = eval_datasets['id'][i]\n      pred = all_prediction[i]\n      pred_score = all_pred_score[i]\n      j=0\n      while j < len(pred):\n        cls = pred[j]\n        if cls =='O': \n            j+=1\n        else: \n            cls = cls.replace('B', 'I')\n        end = j+1\n        while end < len(pred) and pred[end] == cls:\n            end +=1\n        final_pred_score = []\n        for item in pred_score[j:end]:\n          final_pred_score.extend(item)\n        if cls != 'O' and cls!='' and sum(final_pred_score)/len(final_pred_score)>=proba_thresh[cls.replace('I-', '')] and end-j>=min_thresh[cls.replace('I-', '')]:\n            final_pred.append((idx, cls.replace('I-', ''), ' '.join(map(str, list(range(j, end))))))\n        j = end\n    oof = pd.DataFrame(final_pred)\n    oof.columns = ['id', 'class', 'predictionstring']\n    \n    oof = link_evidence(oof)\n    return oof","metadata":{"execution":{"iopub.status.busy":"2022-03-09T08:12:55.733098Z","iopub.execute_input":"2022-03-09T08:12:55.733784Z","iopub.status.idle":"2022-03-09T08:12:55.750065Z","shell.execute_reply.started":"2022-03-09T08:12:55.733743Z","shell.execute_reply":"2022-03-09T08:12:55.749186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof = postprocess_fb_predictions2(eval_datasets=eval_datasets, predictions=all_predictions)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T08:12:55.753399Z","iopub.execute_input":"2022-03-09T08:12:55.75371Z","iopub.status.idle":"2022-03-09T08:12:57.334125Z","shell.execute_reply.started":"2022-03-09T08:12:55.753673Z","shell.execute_reply":"2022-03-09T08:12:57.332916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T08:12:57.335245Z","iopub.execute_input":"2022-03-09T08:12:57.335501Z","iopub.status.idle":"2022-03-09T08:12:57.343376Z","shell.execute_reply.started":"2022-03-09T08:12:57.335465Z","shell.execute_reply":"2022-03-09T08:12:57.34261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}