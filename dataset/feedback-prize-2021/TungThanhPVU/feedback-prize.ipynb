{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Evaluating Student Writing","metadata":{}},{"cell_type":"markdown","source":"# Business Understanding\nWriting is a critical skill for success. However, less than a third of high school seniors are proficient writers, according to the National Assessment of Educational Progress. One way to help students improve their writing is via automated feedback tools, which evaluate student writing and provide personalized feedback.\n\nIn this task, you need to identify elements in student writing. More specifically, you will automatically segment texts and classify argumentative and rhetorical elements in essays written by 6th-12th grade students.\n\nIf successful, you'll make it easier for students to receive feedback on their writing and increase opportunities to improve writing outcomes. Virtual writing tutors and automated writing systems can leverage these algorithms while teachers may use them to reduce grading time. The open-sourced algorithms you come up with will allow any educational organization to better help young writers develop.","metadata":{}},{"cell_type":"markdown","source":"### Objective\nAutomatically segment and classify argumentative and rhetorical elements in essays written by 6th-12th grade students into 7 types:\n\n| Type | Definition |\n|---|---|\n| Lead | an introduction that begins with a statistic, a quotation, a description, or some other device to grab the reader’s attention and point toward the thesis |\n| Position | an opinion or conclusion on the main question |\n| Claim | a claim that supports the position |\n| Counterclaim | a claim that refutes another claim or gives an opposing reason to the position |\n| Rebuttal | a claim that refutes a counterclaim |\n| Evidence | ideas or examples that support claims, counterclaims, or rebuttals |\n| Concluding Statement | a concluding statement that restates the claims |","metadata":{}},{"cell_type":"markdown","source":"# Data Understanding\nThe data is provided in two formats:\n\n* A train.csv file with detailed information about annotation for each essay.\n* A folder with all text files, each file contain one essay.","metadata":{}},{"cell_type":"code","source":"pip install iterative-stratification","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:51:13.913963Z","iopub.execute_input":"2022-03-30T11:51:13.914319Z","iopub.status.idle":"2022-03-30T11:51:23.554305Z","shell.execute_reply.started":"2022-03-30T11:51:13.914231Z","shell.execute_reply":"2022-03-30T11:51:23.553472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import pakages\nimport numpy as np\nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers.schedules import PolynomialDecay\nfrom tensorflow.keras.layers import Input, Dense, Dropout\nimport spacy\nfrom spacy import displacy\nfrom pylab import cm, matplotlib\nfrom transformers import *\nfrom wordcloud import WordCloud, STOPWORDS\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-30T11:51:23.556374Z","iopub.execute_input":"2022-03-30T11:51:23.556636Z","iopub.status.idle":"2022-03-30T11:51:48.753176Z","shell.execute_reply.started":"2022-03-30T11:51:23.5566Z","shell.execute_reply":"2022-03-30T11:51:48.752314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data overview\nLet's look at the training data table.","metadata":{}},{"cell_type":"code","source":"data_df = pd.read_csv('../input/feedback-prize-2021/train.csv')\ndata_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:54:56.394931Z","iopub.execute_input":"2022-03-30T11:54:56.395939Z","iopub.status.idle":"2022-03-30T11:54:58.040256Z","shell.execute_reply.started":"2022-03-30T11:54:56.395894Z","shell.execute_reply":"2022-03-30T11:54:58.039533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 7 fields in the table:\n\n| Field              | Description                                                             |\n|--------------------|-------------------------------------------------------------------------|\n| id                 | ID code for essay response                                              |\n| discourse_id       | ID code for discourse element                                           |\n| discourse_start    | character position where discourse element begins in the essay response |\n| discourse_end      | character position where discourse element ends in the essay response   |\n| discourse_text     | text of discourse element                                               |\n| discourse_type     | classification of discourse element                                     |\n| discourse_type_num | enumerated class label of discourse element                             |\n| predictionstring   | the word indices of the training sample, as required for predictions    |","metadata":{}},{"cell_type":"code","source":"data_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:54:59.209255Z","iopub.execute_input":"2022-03-30T11:54:59.209877Z","iopub.status.idle":"2022-03-30T11:54:59.310564Z","shell.execute_reply.started":"2022-03-30T11:54:59.209834Z","shell.execute_reply":"2022-03-30T11:54:59.309846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 3 numeric fields, and the rest are object.","metadata":{}},{"cell_type":"code","source":"data_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:55:04.095148Z","iopub.execute_input":"2022-03-30T11:55:04.095406Z","iopub.status.idle":"2022-03-30T11:55:04.179298Z","shell.execute_reply.started":"2022-03-30T11:55:04.095378Z","shell.execute_reply":"2022-03-30T11:55:04.17833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no missing values in the training set.","metadata":{}},{"cell_type":"markdown","source":"Let see how many raw text files and annotaions in the training set.","metadata":{}},{"cell_type":"code","source":"raw_text_files = os.listdir('../input/feedback-prize-2021/train')\nprint(f'Training data consists of {len(raw_text_files)} texts')\nprint(f'Training data consists of {data_df.shape[0]} annotaions')\nprint(f'Each essay contains average {round(data_df.shape[0]/len(raw_text_files), 1)} annotaions.')","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:55:06.860588Z","iopub.execute_input":"2022-03-30T11:55:06.8613Z","iopub.status.idle":"2022-03-30T11:55:07.301241Z","shell.execute_reply.started":"2022-03-30T11:55:06.86126Z","shell.execute_reply":"2022-03-30T11:55:07.300453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets look at one specific essay.","metadata":{}},{"cell_type":"code","source":"with open('../input/feedback-prize-2021/train/423A1CA112E2.txt', 'r') as file:\n    first_txt = file.read()\nprint(first_txt)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:55:07.304325Z","iopub.execute_input":"2022-03-30T11:55:07.304526Z","iopub.status.idle":"2022-03-30T11:55:07.320042Z","shell.execute_reply.started":"2022-03-30T11:55:07.304502Z","shell.execute_reply":"2022-03-30T11:55:07.319336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the essay has many paragraphs. If we look closely, we can see that the punctuation and capitalization is not very good. ","metadata":{}},{"cell_type":"code","source":"data_df[data_df['id'] == \"423A1CA112E2\"]","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:55:11.236547Z","iopub.execute_input":"2022-03-30T11:55:11.237235Z","iopub.status.idle":"2022-03-30T11:55:11.277149Z","shell.execute_reply.started":"2022-03-30T11:55:11.23719Z","shell.execute_reply":"2022-03-30T11:55:11.2765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize some text.","metadata":{}},{"cell_type":"code","source":"colors = {'Lead': '#34ebb7',\n        'Position': '#2b7ff6',\n        'Evidence': '#2adddd',\n        'Claim': '#80ffb4',\n        'Concluding Statement': 'd4dd80',\n        'Counterclaim': '#ff8042',\n        'Rebuttal': '#ff0000'}\n\ndef visualize_text(example, df = data_df):\n    \"\"\"\n    Visualize the class of each span in a text.\n    \n    Arguments:\n    example -- the ID of the essay\n    df -- the data frame contain infomation about the essay\n    \"\"\"\n    ents = []\n    for i, row in df[df['id'] == example].iterrows():\n        ents.append({'start': int(row['discourse_start']), \n                    'end': int(row['discourse_end']), \n                    'label': row['discourse_type']})\n        \n    with open(f'../input/feedback-prize-2021/train/{example}.txt', 'r') as file:\n        data = file.read()\n    doc2 = {\"text\": data,\n            \"ents\": ents,\n            \"title\": example}\n\n    options = {\"ents\": ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement', 'Counterclaim', 'Rebuttal'],\n               \"colors\": colors}\n    displacy.render(doc2, style = \"ent\", options = options, manual = True, jupyter = True)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:55:11.650846Z","iopub.execute_input":"2022-03-30T11:55:11.651334Z","iopub.status.idle":"2022-03-30T11:55:11.659924Z","shell.execute_reply.started":"2022-03-30T11:55:11.651296Z","shell.execute_reply":"2022-03-30T11:55:11.659227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose 4 examples to show\nto_show = data_df['id'].sample(n = 4, random_state = 6)\n\nfor essay in to_show:\n    visualize_text(essay, data_df)\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:55:12.210163Z","iopub.execute_input":"2022-03-30T11:55:12.210835Z","iopub.status.idle":"2022-03-30T11:55:12.333873Z","shell.execute_reply.started":"2022-03-30T11:55:12.210799Z","shell.execute_reply":"2022-03-30T11:55:12.333162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the visualization, we can see that:\n* One essay may not have all the discourse types\n* Not all the text in an essay is classified\n* One sentence can have more than 1 discourse type\n* There may be 2 spans of the same discourse type next to each other, even in a same sentence","metadata":{}},{"cell_type":"markdown","source":"### Texts (essay) overview","metadata":{}},{"cell_type":"code","source":"# Create a data frame for id and text info only\ntext_df = pd.DataFrame(columns = ['id', 'text'])\ntexts = []\nfor file in raw_text_files:\n    with open(f'/kaggle/input/feedback-prize-2021/train/{file}') as f:\n        texts.append({'id': file[:-4], 'text': f.read()})\ntexts_df = pd.DataFrame(texts)\n\n# Count the number of character and number of word of each essay\ntexts_df['len'] = texts_df['text'].apply(len)\ntexts_df['word_num'] = texts_df['text'].apply(lambda x: len(x.split()))","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:55:17.072729Z","iopub.execute_input":"2022-03-30T11:55:17.073328Z","iopub.status.idle":"2022-03-30T11:56:35.80917Z","shell.execute_reply.started":"2022-03-30T11:55:17.073284Z","shell.execute_reply":"2022-03-30T11:56:35.80836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:35.810964Z","iopub.execute_input":"2022-03-30T11:56:35.811218Z","iopub.status.idle":"2022-03-30T11:56:35.832382Z","shell.execute_reply.started":"2022-03-30T11:56:35.811184Z","shell.execute_reply":"2022-03-30T11:56:35.831541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the texts have less than 5000 characters, with some outlier that length up to about 18000 characters.","metadata":{}},{"cell_type":"code","source":"texts_df['len'].hist(bins = 50, figsize = (12, 8))\nplt.title('Number of Characters per Essay', fontsize = 15, pad = 15)\nplt.xlabel('Number of characters')\nplt.ylabel('Frequency');","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:35.833978Z","iopub.execute_input":"2022-03-30T11:56:35.834257Z","iopub.status.idle":"2022-03-30T11:56:36.165134Z","shell.execute_reply.started":"2022-03-30T11:56:35.83422Z","shell.execute_reply":"2022-03-30T11:56:36.164487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of all essay have total about 200 to 500 words, with some outliers have more than 1600 words.","metadata":{}},{"cell_type":"code","source":"texts_df['word_num'].hist(bins = 50, figsize = (12, 8))\nplt.title('Number of Words per Essay', fontsize = 16, pad = 15)\nplt.xlabel('Number of words')\nplt.ylabel('Frequency');","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:36.167079Z","iopub.execute_input":"2022-03-30T11:56:36.167314Z","iopub.status.idle":"2022-03-30T11:56:36.603857Z","shell.execute_reply.started":"2022-03-30T11:56:36.167282Z","shell.execute_reply":"2022-03-30T11:56:36.603117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"less512 = (texts_df['word_num'] <= 512).sum()/len(texts_df['word_num'])*100\nless1024 = (texts_df['word_num'] <= 1024).sum()/len(texts_df['word_num'])*100\nprint(f'There are {round(less512, 1)}% essays have less than or equal to 512 \\\nwords and {round(less1024, 1)}% essays have less than or equal to 1024 words.')","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:36.607618Z","iopub.execute_input":"2022-03-30T11:56:36.608382Z","iopub.status.idle":"2022-03-30T11:56:36.620703Z","shell.execute_reply.started":"2022-03-30T11:56:36.608339Z","shell.execute_reply":"2022-03-30T11:56:36.61989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Length and frequency of each discourse type","metadata":{}},{"cell_type":"code","source":"ax = data_df['discourse_type'].value_counts(ascending = True).plot(kind = 'barh', figsize = (12, 8))\n#ax.bar_label(ax.containers[0], label_type=\"edge\", padding = 0.2)\nplt.title('Frequency of each Discourse Type', fontsize = 16, pad = 15)\nplt.ylabel('')\nplt.xlabel('Frequency');","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:36.625064Z","iopub.execute_input":"2022-03-30T11:56:36.627312Z","iopub.status.idle":"2022-03-30T11:56:36.940138Z","shell.execute_reply.started":"2022-03-30T11:56:36.627272Z","shell.execute_reply":"2022-03-30T11:56:36.939492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The most popular discourse type is Claim, and the least popular is Rebuttal.","metadata":{}},{"cell_type":"code","source":"# Make a collumn to calculate the len of each discourse\ndata_df[\"discourse_num_word\"] = data_df[\"discourse_text\"].apply(lambda x: len(x.split()))\n\nax = data_df.groupby('discourse_type')[\"discourse_num_word\"].mean().sort_values().plot(kind = 'barh', figsize = (12, 8))\n#ax.bar_label(ax.containers[0], label_type=\"edge\", fmt='%1.1f', padding = 0.2)\nplt.title('Average length of each Discourse Type', fontsize = 16, pad = 15)\nplt.xlabel('Number of words')\nplt.ylabel('');","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:36.941753Z","iopub.execute_input":"2022-03-30T11:56:36.942222Z","iopub.status.idle":"2022-03-30T11:56:37.595213Z","shell.execute_reply.started":"2022-03-30T11:56:36.942167Z","shell.execute_reply":"2022-03-30T11:56:37.594514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,10))\nsns.boxplot(x = 'discourse_type', y = 'discourse_num_word', data = data_df)\n#order = data_df.groupby('discourse_type')[\"discourse_len\"].mean().sort_values(ascending = False).index\nplt.title('Length of each Discourse Type', fontsize = 16, pad = 15)\nplt.xlabel('')\nplt.ylabel('Number of words');","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:37.596544Z","iopub.execute_input":"2022-03-30T11:56:37.597051Z","iopub.status.idle":"2022-03-30T11:56:37.973412Z","shell.execute_reply.started":"2022-03-30T11:56:37.597014Z","shell.execute_reply":"2022-03-30T11:56:37.972733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Evidence type has the longest average number of words, and the sortest one is Claim. It seems reasonable.","metadata":{}},{"cell_type":"code","source":"num_discourse = data_df.groupby('id')['discourse_type'].count()\nprint(f'Minimum number of discourse span per essay: {num_discourse.min()}')\nprint(f'Maximum number of discourse span per essay: {num_discourse.max()} \\n')\n\nnum_discourse.hist(figsize = (12, 8), bins = 26)\nplt.title('Number of Discourse Span per Essay', fontsize = 16, pad = 15)\nplt.xlabel('Number')\nplt.ylabel('Frequency');","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:37.974648Z","iopub.execute_input":"2022-03-30T11:56:37.975064Z","iopub.status.idle":"2022-03-30T11:56:38.257869Z","shell.execute_reply.started":"2022-03-30T11:56:37.975027Z","shell.execute_reply":"2022-03-30T11:56:38.257239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(num_discourse).describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:38.260859Z","iopub.execute_input":"2022-03-30T11:56:38.261053Z","iopub.status.idle":"2022-03-30T11:56:38.275596Z","shell.execute_reply.started":"2022-03-30T11:56:38.261029Z","shell.execute_reply":"2022-03-30T11:56:38.274825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of essays have about 7 to 11 discourse spans.","metadata":{}},{"cell_type":"markdown","source":"## Make word cloud","metadata":{}},{"cell_type":"code","source":"wordcloud = WordCloud(stopwords = STOPWORDS, max_font_size = 120, max_words = 200,\n                      width = 1200, height = 800, background_color = 'white')\nwordcloud.generate(' '.join(txt for txt in data_df[\"discourse_text\"]))\nplt.figure(figsize = (16, 12))\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.title('Word Cloud for all Texts', fontsize = 25, pad = 18);","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:38.276863Z","iopub.execute_input":"2022-03-30T11:56:38.277187Z","iopub.status.idle":"2022-03-30T11:56:56.466717Z","shell.execute_reply.started":"2022-03-30T11:56:38.277149Z","shell.execute_reply":"2022-03-30T11:56:56.46593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since those essay were written by students, we can see that the most popular words are: student, people, school, teacher, help, college","metadata":{}},{"cell_type":"markdown","source":"## Create training and validation set","metadata":{}},{"cell_type":"markdown","source":"The data set is quite big. In order to experiment more quickly, I use 40% of the data for training, 10% for validation. After find best solution for this small training set, I will use all the data for training and use Kaggle public test data for validation. Finally, the private test data set will be used for testing purpose.","metadata":{}},{"cell_type":"code","source":"# All id name of texts\nall_id = data_df.id.unique()\n\n# Create training and validation set\nnp.random.seed(6) # For reproduce\ntrain_idx = np.random.choice(np.arange(len(all_id)), int(0.4*len(all_id)), replace = False)\nleft_set = np.setdiff1d(np.arange(len(all_id)), train_idx)\nvalid_idx = np.random.choice(left_set, int(0.1*len(all_id)), replace = False)\nnp.random.seed(None)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:56.467751Z","iopub.execute_input":"2022-03-30T11:56:56.467963Z","iopub.status.idle":"2022-03-30T11:56:56.494624Z","shell.execute_reply.started":"2022-03-30T11:56:56.467934Z","shell.execute_reply":"2022-03-30T11:56:56.493986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check is the new training and validation set are representative of all data.","metadata":{}},{"cell_type":"code","source":"data_df[data_df['id'].isin(all_id[train_idx])]['discourse_type'].value_counts(ascending = True).plot(kind = 'barh', figsize = (12, 8))\nplt.title('Frequency of each Discourse Type in Training Set', fontsize = 16, pad = 15)\nplt.ylabel('')\nplt.xlabel('Frequency');","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:56.495792Z","iopub.execute_input":"2022-03-30T11:56:56.496463Z","iopub.status.idle":"2022-03-30T11:56:56.746121Z","shell.execute_reply.started":"2022-03-30T11:56:56.496426Z","shell.execute_reply":"2022-03-30T11:56:56.745452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df[data_df['id'].isin(all_id[valid_idx])]['discourse_type'].value_counts(ascending = True).plot(kind = 'barh', figsize = (12, 8))\nplt.title('Number of each Discourse Type in Validation Set', fontsize = 16, pad = 15)\nplt.ylabel('')\nplt.xlabel('Frequency');","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:56.74748Z","iopub.execute_input":"2022-03-30T11:56:56.747751Z","iopub.status.idle":"2022-03-30T11:56:56.984173Z","shell.execute_reply.started":"2022-03-30T11:56:56.747716Z","shell.execute_reply":"2022-03-30T11:56:56.983487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df['discourse_type'].value_counts(ascending = True, normalize = True)/data_df[data_df['id'].isin(all_id[train_idx])]['discourse_type'].value_counts(ascending = True, normalize = True)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:56.986305Z","iopub.execute_input":"2022-03-30T11:56:56.986826Z","iopub.status.idle":"2022-03-30T11:56:57.044241Z","shell.execute_reply.started":"2022-03-30T11:56:56.986786Z","shell.execute_reply":"2022-03-30T11:56:57.04345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df[data_df['id'].isin(all_id[train_idx])]['discourse_type'].value_counts(ascending = True, normalize = True)/data_df[data_df['id'].isin(all_id[valid_idx])]['discourse_type'].value_counts(ascending = True, normalize = True)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:57.045731Z","iopub.execute_input":"2022-03-30T11:56:57.04599Z","iopub.status.idle":"2022-03-30T11:56:57.107957Z","shell.execute_reply.started":"2022-03-30T11:56:57.045955Z","shell.execute_reply":"2022-03-30T11:56:57.107177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of the small training set and all data are the same. For the validation data set, the distribution of rebuttal and counterclaim are higher. However, since those types are rare, it is OK for validation purpose.","metadata":{}},{"cell_type":"code","source":"# If KFold cross validation is used:\ntrain_CV = np.append(train_idx, valid_idx)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T15:36:31.034268Z","iopub.execute_input":"2022-03-17T15:36:31.034649Z","iopub.status.idle":"2022-03-17T15:36:31.04014Z","shell.execute_reply.started":"2022-03-17T15:36:31.034612Z","shell.execute_reply":"2022-03-17T15:36:31.039063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"code","source":"# Token file\nload_tokens_from = '../input/longformerbase4096'\n\n# Pretrained model\ndownloaded_model_path = '../input/longformerbase4096'\n\n# NER target file\nNER_target = '../input/ner-target-for-feedback-prize-competition'\n\n# Max sequence length for model\nmax_len = 1024","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:57.109428Z","iopub.execute_input":"2022-03-30T11:56:57.109692Z","iopub.status.idle":"2022-03-30T11:56:57.114281Z","shell.execute_reply.started":"2022-03-30T11:56:57.109646Z","shell.execute_reply":"2022-03-30T11:56:57.113252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenize Training set","metadata":{}},{"cell_type":"markdown","source":"First we need to converts training dataset into a NER token array that we can use to train a NER transformer.","metadata":{}},{"cell_type":"code","source":"# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(downloaded_model_path)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:57.115473Z","iopub.execute_input":"2022-03-30T11:56:57.115911Z","iopub.status.idle":"2022-03-30T11:56:57.262821Z","shell.execute_reply.started":"2022-03-30T11:56:57.115773Z","shell.execute_reply":"2022-03-30T11:56:57.262073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_target(text_id, dataframe, backbone, name,\n                  max_len = max_len, augmented = None, NER_target = None):\n    \"\"\"\n    Create token, attention mask and target array.\n    \n    Arguments:\n    text_id -- array or list of essay id\n    dataframe -- the dataframe of data set\n    backbone -- the name of backbone. For example: 'longformer', 'deberta'\n    name -- in the list: 'train_origin', 'val', 'CV', 'train_roberta_augmented',\n    'train_backtrans_augmented', 'train_syn_augmented'\n    max_len -- max token length\n    augmented -- type of augmentation used: 'augmented_back', 'augmented_roberta', 'augmented_syn'\n    NER_target -- address of saved folder for token, attention mask and target array\n    \n    Return:\n    targets -- target array\n    text_tokens -- token array\n    text_attention - -attention mask array\n    \"\"\"\n    \n    # Run and save the target for the first run time only\n    if NER_target:\n        targets = np.load(f'{NER_target}/{name}_targets_{backbone}_{max_len}.npy')\n        text_tokens = np.load(f'{NER_target}/{name}_tokens_{backbone}_{max_len}.npy')\n        text_attention = np.load(f'{NER_target}/{name}_attention_{backbone}_{max_len}.npy')\n        print('NER tokens loaded')\n        return targets, text_tokens, text_attention\n    \n    # The tokens and attention arrays   \n    text_tokens = np.zeros((len(text_id), max_len), dtype = 'int32')\n    text_attention = np.zeros((len(text_id), max_len), dtype = 'int32')\n\n    # The 14 classes for NER\n    lead_b = np.zeros((len(text_id), max_len))\n    lead_i = np.zeros((len(text_id), max_len))\n\n    position_b = np.zeros((len(text_id), max_len))\n    position_i = np.zeros((len(text_id), max_len))\n\n    evidence_b = np.zeros((len(text_id), max_len))\n    evidence_i = np.zeros((len(text_id), max_len))\n\n    claim_b = np.zeros((len(text_id), max_len))\n    claim_i = np.zeros((len(text_id), max_len))\n\n    conclusion_b = np.zeros((len(text_id), max_len))\n    conclusion_i = np.zeros((len(text_id), max_len))\n\n    counterclaim_b = np.zeros((len(text_id), max_len))\n    counterclaim_i = np.zeros((len(text_id), max_len))\n\n    rebuttal_b = np.zeros((len(text_id), max_len))\n    rebuttal_i = np.zeros((len(text_id), max_len))\n\n    targets_b = [lead_b, position_b, evidence_b, claim_b, conclusion_b, counterclaim_b, rebuttal_b]\n    targets_i = [lead_i, position_i, evidence_i, claim_i, conclusion_i, counterclaim_i, rebuttal_i]    \n    \n    target_map = {'Lead': 0, 'Position': 1, 'Evidence': 2, 'Claim': 3, 'Concluding Statement': 4,\n                 'Counterclaim': 5, 'Rebuttal': 6}\n    \n    # For loop through each text\n    for n in text_id:\n\n        # Read text, tokenize, and save in token arrays       \n        # Load the text file either in train set or augmented set\n        try:\n            txt = open(f'../input/feedback-prize-2021/train/{n}.txt', 'r').read()\n        except:\n            txt = open(f'../input/data-augmented/{augmented}/{n}.txt', 'r').read()\n\n        # Tokenize the text\n        tokens = tokenizer.encode_plus(txt, max_length = max_len, padding = 'max_length',\n                                        truncation = True, return_offsets_mapping = True)\n        \n        # Save token of the text to the text_tokens array\n        text_tokens[id_num] = tokens['input_ids']\n        \n        # Save attention mask to the text_attention array\n        text_attention[id_num] = tokens['attention_mask']\n\n        # Find targets in text and save in target array\n        # Loop through offset_mapping to asign each token to a class\n        offsets = tokens['offset_mapping']\n        offset_index = 0 # token position\n        df = dataframe.loc[dataframe.id == n]\n        \n        for row in df.itertuples():\n            a = row[3] # discourse_start position\n            b = row[4] # discourse_end position\n            if offset_index > max_len - 1: # the index out of max token length\n                break\n            c = offsets[offset_index][0] # start position of each token\n            d = offsets[offset_index][1] # end position of each token\n            beginning = True\n            while b > c: \n                # while the word is in the discourse span\n                # the position of the text must larger than \n                # the start position and smaller than the end position\n                if (c >= a) & (b >= d):\n                    k = target_map[row[6]] # discourse_type, in number for indexing\n                    if beginning:\n                        targets_b[k][id_num][offset_index] = 1\n                        beginning = False # 1 B for first only\n                    else:\n                        targets_i[k][id_num][offset_index] = 1\n                offset_index += 1\n                if offset_index > max_len - 1:\n                    break\n                c = offsets[offset_index][0]\n                d = offsets[offset_index][1]\n     \n    # Create target array\n    targets = np.zeros((len(text_id), max_len, 15), dtype = 'int32')\n    for k in range(7):\n        targets[:, :, 2*k] = targets_b[k]\n        targets[:, :, 2*k + 1] = targets_i[k]\n    # If the token is not assigned to any class:\n    targets[:, :, 14] = 1 - np.max(targets, axis = -1)\n    \n    # Save targets to files for next use time\n    np.save(f'{name}_targets_{backbone}_{max_len}', targets)\n    np.save(f'{name}_tokens_{backbone}_{max_len}', text_tokens)\n    np.save(f'{name}_attention_{backbone}_{max_len}', text_attention)\n    print('NER tokens Saved')   \n    \n    return targets, text_tokens, text_attention","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:57.264266Z","iopub.execute_input":"2022-03-30T11:56:57.264513Z","iopub.status.idle":"2022-03-30T11:56:57.289608Z","shell.execute_reply.started":"2022-03-30T11:56:57.26448Z","shell.execute_reply":"2022-03-30T11:56:57.28888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_augmented_df(augmented_file, text_id = all_id[train_idx]):\n    \"\"\"\n    Create data frame if augmentation is used\n    \n    Arguments:\n    augmented_file -- name of the augmented file:\n    'train_backtrans_augmented.csv'\n    'train_roberta_augmented.csv'\n    'train_synonym_augmented.csv'\n    text_id -- array or list of essay id\n    \n    Return:\n    train_df -- data frame contain augmented and original information\n    \"\"\"\n    augmented_df = pd.read_csv(f'../input/data-augmented/{augmented_file}')\n    origin_df = data_df[data_df['id'].isin(text_id)]\n    train_df = pd.concat([origin_df, augmented_df], ignore_index = True)\n    \n    return train_df","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:57.290867Z","iopub.execute_input":"2022-03-30T11:56:57.291429Z","iopub.status.idle":"2022-03-30T11:56:57.298342Z","shell.execute_reply.started":"2022-03-30T11:56:57.291389Z","shell.execute_reply":"2022-03-30T11:56:57.29764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are many scenarios, so we need to create the files accordingly.","metadata":{}},{"cell_type":"code","source":"# The type can be 1 in the following: 'normal', 'all_data', 'augmentation', 'KFold'\ntrain_type = 'normal'","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:57.299695Z","iopub.execute_input":"2022-03-30T11:56:57.300154Z","iopub.status.idle":"2022-03-30T11:56:57.312184Z","shell.execute_reply.started":"2022-03-30T11:56:57.300116Z","shell.execute_reply":"2022-03-30T11:56:57.311355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train_type == 'normal':\n    # Validation ID list and dataframe\n    val_id = all_id[valid_idx]\n    val_df = data_df[data_df['id'].isin(val_id)]\n\n    # Token, attention mask and target array for validation set\n    val_targets, val_tokens, val_attention = create_target(val_id, val_df, 'longformer', 'val',\n                                                          max_len = max_len, augmented = None,\n                                                          NER_target = NER_target)\n    # Train id list and dataframe\n    train_id = all_id[train_idx]\n    train_df = data_df[data_df['id'].isin(train_id)]\n\n    # Token, attention mask and target array for training set\n    train_targets, train_tokens, train_attention = create_target(train_id, train_df, 'longformer',\n                                                                 name = 'train_origin',\n                                                                 max_len = max_len,\n                                                                 NER_target = NER_target)\nelif train_type == 'all_data':\n    # If use all data\n    train_id = data_df.id.unique()\n    targets = np.load('../input/ner-target-for-feedback-prize-competition/targets_1024.npy')\n    text_tokens = np.load('../input/ner-target-for-feedback-prize-competition/tokens_1024.npy')\n    text_attention = np.load('../input/ner-target-for-feedback-prize-competition/attention_1024.npy')\n    \nelif train_type == 'augmentation':\n    # If use augmentation\n    train_df = create_augmented_df('train_synonym_augmented.csv', text_id = all_id[train_idx])\n    train_id = train_df.id.unique()\n    \nelif tran_type == 'KFold':\n    # If KFold cross validation is used\n    train_id = all_id[train_CV]\n    train_df = data_df[data_df['id'].isin(train_id)]\n\n    # Token, attention mask and target array for CV set\n    train_targets, train_tokens, train_attention = create_target(train_id, train_df, 'longformer',\n                                                                 name = 'CV',\n                                                                 max_len = max_len,\n                                                                 NER_target = NER_target)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:56:57.313576Z","iopub.execute_input":"2022-03-30T11:56:57.314291Z","iopub.status.idle":"2022-03-30T11:57:00.291356Z","shell.execute_reply.started":"2022-03-30T11:56:57.314254Z","shell.execute_reply":"2022-03-30T11:57:00.289856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Model\nWe will use LongFormer backbone and add our own NER head using. We use 15 classes because we have a B class and I class for each of 7 labels. And we have an additional class (called O class) for tokens that do not belong to one of the 14 classes.","metadata":{}},{"cell_type":"code","source":"# Number of epoch and batch_size\nepochs = 8\nbatch_size = 4","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:57:00.292734Z","iopub.execute_input":"2022-03-30T11:57:00.292996Z","iopub.status.idle":"2022-03-30T11:57:00.299405Z","shell.execute_reply.started":"2022-03-30T11:57:00.29296Z","shell.execute_reply":"2022-03-30T11:57:00.29862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    \"\"\"\n    Function to build and compile model\n    \"\"\"\n    tokens = Input(shape = (max_len,), name = 'tokens', dtype = tf.int32)\n    attention = Input(shape = (max_len,), name = 'attention', dtype = tf.int32)\n\n    config = AutoConfig.from_pretrained(downloaded_model_path + '/config.json') \n    backbone = TFAutoModel.from_pretrained(downloaded_model_path + '/tf_model.h5', config = config)\n\n    x = backbone(tokens, attention_mask = attention)\n    x = Dense(512, activation = 'relu')(x[0])\n    x = Dropout(0.2)(x)\n    x = Dense(15, activation = 'softmax', dtype = 'float32')(x)\n\n    model = tf.keras.Model(inputs = [tokens, attention], outputs = x)\n    model.compile(optimizer = 'adam',\n              loss = ['categorical_crossentropy'],\n              metrics = ['categorical_accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:57:00.300996Z","iopub.execute_input":"2022-03-30T11:57:00.301265Z","iopub.status.idle":"2022-03-30T11:57:00.311358Z","shell.execute_reply.started":"2022-03-30T11:57:00.30123Z","shell.execute_reply":"2022-03-30T11:57:00.310578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:57:00.312307Z","iopub.execute_input":"2022-03-30T11:57:00.312515Z","iopub.status.idle":"2022-03-30T11:57:24.03215Z","shell.execute_reply.started":"2022-03-30T11:57:00.31247Z","shell.execute_reply":"2022-03-30T11:57:24.031377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create saved model checkpoint callback\ncheckpoint = 'checkpoint-{epoch:02d}'\ncheckpoint_callback = ModelCheckpoint(filepath = checkpoint,\n                                     save_freq = 'epoch',\n                                     save_weights_only = True,\n                                     verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T08:49:11.387869Z","iopub.execute_input":"2022-03-23T08:49:11.388574Z","iopub.status.idle":"2022-03-23T08:49:11.393305Z","shell.execute_reply.started":"2022-03-23T08:49:11.388535Z","shell.execute_reply":"2022-03-23T08:49:11.392392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create learning rate decay function\ndecay_steps = train_tokens.shape[0]//batch_size*epochs\nlearning_rate_fn = PolynomialDecay(initial_learning_rate = 1e-4,\n                                decay_steps = decay_steps,\n                                end_learning_rate = 1e-5,\n                                power = 1)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T08:49:11.397311Z","iopub.execute_input":"2022-03-23T08:49:11.397605Z","iopub.status.idle":"2022-03-23T08:49:11.404823Z","shell.execute_reply.started":"2022-03-23T08:49:11.397568Z","shell.execute_reply":"2022-03-23T08:49:11.404042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile model for training\nmodel.compile(optimizer = Adam(learning_rate = learning_rate_fn),\n              loss = ['categorical_crossentropy'],\n              metrics = ['categorical_accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-03-23T08:49:11.413818Z","iopub.execute_input":"2022-03-23T08:49:11.414344Z","iopub.status.idle":"2022-03-23T08:49:11.436606Z","shell.execute_reply.started":"2022-03-23T08:49:11.414307Z","shell.execute_reply":"2022-03-23T08:49:11.435945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile model for inference\nmodel.compile(optimizer = 'adam',\n              loss = ['categorical_crossentropy'],\n              metrics = ['categorical_accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:57:24.033405Z","iopub.execute_input":"2022-03-30T11:57:24.033677Z","iopub.status.idle":"2022-03-30T11:57:24.054357Z","shell.execute_reply.started":"2022-03-30T11:57:24.033628Z","shell.execute_reply":"2022-03-30T11:57:24.053714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:57:24.055858Z","iopub.execute_input":"2022-03-30T11:57:24.056203Z","iopub.status.idle":"2022-03-30T11:57:24.081399Z","shell.execute_reply.started":"2022-03-30T11:57:24.056168Z","shell.execute_reply":"2022-03-30T11:57:24.080644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training model\nhistory = model.fit(x = [train_tokens, train_attention],\n                    y = train_targets,\n                    validation_data = ([val_tokens, val_attention], val_targets),\n                    callbacks = [checkpoint_callback],\n                    epochs = epochs,\n                    batch_size = batch_size,\n                    verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T07:01:25.590506Z","iopub.execute_input":"2022-03-21T07:01:25.591048Z","iopub.status.idle":"2022-03-21T11:23:52.599811Z","shell.execute_reply.started":"2022-03-21T07:01:25.59101Z","shell.execute_reply":"2022-03-21T11:23:52.597698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataframe contains training logs\nhistory_df = pd.DataFrame(history.history)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T11:24:44.741001Z","iopub.execute_input":"2022-03-21T11:24:44.742252Z","iopub.status.idle":"2022-03-21T11:24:44.75171Z","shell.execute_reply.started":"2022-03-21T11:24:44.742207Z","shell.execute_reply":"2022-03-21T11:24:44.750922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training accuracy\nhistory_df.plot(y = ['categorical_accuracy', 'val_categorical_accuracy'], figsize = (12, 7))\nplt.xlabel(\"Epochs\")\nplt.ylabel('Accuracy')\nplt.title('Accuracy vs. epochs');","metadata":{"execution":{"iopub.status.busy":"2022-03-21T11:24:45.057547Z","iopub.execute_input":"2022-03-21T11:24:45.057753Z","iopub.status.idle":"2022-03-21T11:24:45.360425Z","shell.execute_reply.started":"2022-03-21T11:24:45.057721Z","shell.execute_reply":"2022-03-21T11:24:45.359722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training loss\nhistory_df.plot(y = ['loss', 'val_loss'], figsize = (12, 7))\nplt.xlabel(\"Epochs\")\nplt.ylabel('Loss')\nplt.title('Loss vs. epochs');","metadata":{"execution":{"iopub.status.busy":"2022-03-21T11:24:47.18989Z","iopub.execute_input":"2022-03-21T11:24:47.190295Z","iopub.status.idle":"2022-03-21T11:24:47.521848Z","shell.execute_reply.started":"2022-03-21T11:24:47.19025Z","shell.execute_reply":"2022-03-21T11:24:47.521173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If load model from pretrained model\nmodel.load_weights('../input/trained-model-for-feedback-prize-competition/long_v12_0.618.h5')","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:57:24.085212Z","iopub.execute_input":"2022-03-30T11:57:24.085594Z","iopub.status.idle":"2022-03-30T11:57:29.816451Z","shell.execute_reply.started":"2022-03-30T11:57:24.085565Z","shell.execute_reply":"2022-03-30T11:57:29.815697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build CV model","metadata":{}},{"cell_type":"markdown","source":"We use all training data and validation data for cross-validation. For this data, we will use 5 folds cross-validation.","metadata":{}},{"cell_type":"code","source":"# Create 5 folds\nskf = MultilabelStratifiedKFold(n_splits = 5, shuffle = True, random_state = 6)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T16:00:22.886016Z","iopub.execute_input":"2022-03-17T16:00:22.88633Z","iopub.status.idle":"2022-03-17T16:00:22.893184Z","shell.execute_reply.started":"2022-03-17T16:00:22.886295Z","shell.execute_reply":"2022-03-17T16:00:22.892418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of epoch and batch size\nepochs = 5\nbatch_size = 4","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:58:56.621281Z","iopub.execute_input":"2022-03-15T14:58:56.621548Z","iopub.status.idle":"2022-03-15T14:58:56.632014Z","shell.execute_reply.started":"2022-03-15T14:58:56.621518Z","shell.execute_reply":"2022-03-15T14:58:56.631048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if the data distribution of all folds are the same\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_tokens, train_targets[:, 0, :])):\n    print('Fold:', fold + 1, '\\n')\n    print(data_df[data_df['id'].isin(all_id[valid_idx])]['discourse_type'].value_counts(ascending = True))","metadata":{"execution":{"iopub.status.busy":"2022-03-17T16:00:35.957125Z","iopub.execute_input":"2022-03-17T16:00:35.957619Z","iopub.status.idle":"2022-03-17T16:00:36.519433Z","shell.execute_reply.started":"2022-03-17T16:00:35.957567Z","shell.execute_reply":"2022-03-17T16:00:36.518547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train 5 folds\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_tokens, train_targets[:, 0, :])):\n    print('Fold:', fold + 1, '\\n')\n    model = build_model()\n    decay_steps = train_tokens.shape[0]//batch_size*epochs\n    learning_rate_fn = PolynomialDecay(initial_learning_rate = 1e-4,\n                                    decay_steps = decay_steps,\n                                    end_learning_rate = 1e-5,\n                                    power = 1)\n    model.compile(optimizer = Adam(learning_rate = learning_rate_fn),\n                  loss = ['categorical_crossentropy'],\n                  metrics = ['categorical_accuracy'])\n    model.fit(x = [train_tokens[train_idx], train_attention[train_idx]], \n              y = train_targets[train_idx],\n              epochs = epochs,\n              batch_size = batch_size,\n              verbose = 1)\n    model.save_weights(f'long_CV_{fold + 1}.h5')\n    print('\\n', '*'*50, '\\n')               ","metadata":{"execution":{"iopub.status.busy":"2022-03-15T12:15:55.910929Z","iopub.execute_input":"2022-03-15T12:15:55.911462Z","iopub.status.idle":"2022-03-15T12:27:35.189291Z","shell.execute_reply.started":"2022-03-15T12:15:55.911422Z","shell.execute_reply":"2022-03-15T12:27:35.188435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validate Model","metadata":{}},{"cell_type":"markdown","source":"We will now make predictions on the validation texts. Our model makes label predictions for each token, we need to convert this into a list of word indices for each label. Note that the tokens and words are not the same. A single word may be broken into multiple tokens. Therefore we need to first create a map to change token indices to word indices.","metadata":{}},{"cell_type":"code","source":"# Turn from class number to class name\ntarget_map_rev = {0: 'Lead', 1: 'Position', 2: 'Evidence', 3: 'Claim', 4: 'Concluding Statement',\n                 5: 'Counterclaim', 6: 'Rebuttal', 7: 'blank'}","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:57:29.817799Z","iopub.execute_input":"2022-03-30T11:57:29.81805Z","iopub.status.idle":"2022-03-30T11:57:29.8238Z","shell.execute_reply.started":"2022-03-30T11:57:29.818017Z","shell.execute_reply":"2022-03-30T11:57:29.822396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = np.array(['Lead', 'Position','Counterclaim','Rebuttal','Evidence',\n       'Claim', 'Concluding Statement'], dtype = object)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T12:11:53.929863Z","iopub.execute_input":"2022-03-30T12:11:53.930122Z","iopub.status.idle":"2022-03-30T12:11:53.934395Z","shell.execute_reply.started":"2022-03-30T12:11:53.930095Z","shell.execute_reply":"2022-03-30T12:11:53.93347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make the data frame for validation\nto_validate = data_df.loc[data_df['id'].isin(all_id[valid_idx])]","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:57:29.824861Z","iopub.execute_input":"2022-03-30T11:57:29.825714Z","iopub.status.idle":"2022-03-30T11:57:29.856024Z","shell.execute_reply.started":"2022-03-30T11:57:29.825656Z","shell.execute_reply":"2022-03-30T11:57:29.855391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to Compute Competition Metric\n\ndef calc_overlap(set_pred, set_gt):\n    \"\"\"\n    Calculates if the overlap between prediction and\n    ground truth is enough for a potential True positive\n    \"\"\"\n    # Length of each and intersection\n    try:\n        len_gt = len(set_gt)\n        len_pred = len(set_pred)\n        inter = len(set_gt & set_pred)\n        overlap_1 = inter/len_gt\n        overlap_2 = inter/len_pred\n        return overlap_1 >= 0.5 and overlap_2 >= 0.5\n    except:  # at least one of the input is NaN\n        return False\n\ndef score_feedback_comp_micro(pred_df, gt_df, discourse_type):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Use the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df.loc[gt_df['discourse_type'] == discourse_type, \n                      ['id', 'predictionstring']].reset_index(drop = True)\n    pred_df = pred_df.loc[pred_df['class'] == discourse_type,\n                      ['id', 'predictionstring']].reset_index(drop = True)\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    pred_df['predictionstring'] = [set(pred.split(' ')) for pred in pred_df['predictionstring']]\n    gt_df['predictionstring'] = [set(pred.split(' ')) for pred in gt_df['predictionstring']]\n    \n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on = 'id',\n                           right_on = 'id',\n                           how = 'outer',\n                           suffixes = ('_pred','_gt'))\n    overlaps = [calc_overlap(*args) for args in zip(joined.predictionstring_pred, \n                                                     joined.predictionstring_gt)]\n    \n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    # we don't need to compute the match to compute the score\n    TP = joined.loc[overlaps]['gt_id'].nunique()\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    TPandFP = len(pred_df)\n    TPandFN = len(gt_df)\n    \n    #calc microf1\n    my_f1_score = 2*TP/(TPandFP + TPandFN)\n    return my_f1_score\n\ndef score_feedback_comp(pred_df, gt_df, return_class_scores = False):\n    class_scores = {}\n    for discourse_type in gt_df.discourse_type.unique():\n        class_score = score_feedback_comp_micro(pred_df, gt_df, discourse_type)\n        class_scores[discourse_type] = class_score\n    f1 = np.mean([v for v in class_scores.values()])\n    if return_class_scores:\n        return f1, class_scores\n    return f1","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:57:29.85723Z","iopub.execute_input":"2022-03-30T11:57:29.857614Z","iopub.status.idle":"2022-03-30T11:57:29.871613Z","shell.execute_reply.started":"2022-03-30T11:57:29.857576Z","shell.execute_reply":"2022-03-30T11:57:29.870922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to validate model\ndef check_ckp(model):\n    \"\"\"\n    Validate model and print out the result based on validation data set\n    \"\"\"\n    pred = model.predict([text_tokens[valid_idx], text_attention[valid_idx]],\n                         batch_size = 16, verbose = 1)\n    oof_preds = np.argmax(pred, axis = -1)\n    oof = get_preds(dataset = 'train', text_ids = val_id, augmented = None,\n                    preds = oof_preds, thredshold = thredshold)\n    f1s = []\n    classes= np.array(['Lead', 'Position','Counterclaim','Rebuttal','Evidence',\n                       'Claim', 'Concluding Statement'], dtype = object)\n    print('Validation F1_score:')\n    for c in classes:\n        pred_df = oof.loc[oof['class'] == c].copy()\n        gt_df = to_validate.loc[to_validate['discourse_type'] == c].copy()\n        f1 = score_feedback_comp(pred_df, gt_df)\n        print(c + ':', round(f1, 3))\n        f1s.append(f1)\n    print('Overall',round(np.mean(f1s), 3))\n    print('\\n')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_preds_old(dataset, text_ids, preds, thredshold = 4, augmented = None):\n    \n    \"\"\"\n    Made prediction and create result data frame\n    \n    Arguments:\n    dataset -- name of folder contain text files: 'train' or 'test'\n    text_ids -- array or list of essay id\n    preds -- the array contain predicted class\n    augmented -- name of augmented text folder: 'augmented_back', 'augmented_roberta', 'augmented_syn'\n    thredshold -- the minimum words required for one discourse span of all type\n    \n    Return:\n    df -- result data frame\n    \"\"\"\n    all_predictions = []\n    target_map_rev = {0: 'Lead', 1: 'Position', 2: 'Evidence', 3: 'Claim', 4: 'Concluding Statement',\n                      5: 'Counterclaim', 6: 'Rebuttal', 7: 'blank'}\n    \n    for id_num in range(len(preds)):\n\n        n = text_ids[id_num] # Text id\n    \n        # Tokenize the text\n        try:\n            txt = open(f'../input/feedback-prize-2021/{dataset}/{n}.txt', 'r').read()\n        except:\n            txt = open(f'../input/data-augmented/{augmented}/{n}.txt', 'r').read()\n\n        tokens = tokenizer.encode_plus(txt, max_length = max_len, padding = 'max_length',\n                                   truncation = True, return_offsets_mapping = True)\n        off = tokens['offset_mapping']\n    \n        # Get word position\n        word_pos = []\n        blank = True\n        # The word must start with a symbol, and inly the first symbol will be counted\n        for i in range(len(txt)):\n            if (not txt[i].isspace()) & (blank == True):\n                word_pos.append(i)\n                blank = False\n            # implied that previous word ended\n            elif txt[i].isspace():\n                blank = True\n        word_pos.append(1e6) # end\n            \n        # Mapping from tokens to words\n        word_map = -1 * np.ones(max_len, dtype = 'int32')\n        w_i = 0\n        for i in range(len(off)):\n            if off[i][1] == 0: # Skip character with token 0\n                continue\n            while off[i][0] >= word_pos[w_i+1]: #If token position is larger than word start position\n                w_i += 1\n            word_map[i] = int(w_i)\n        \n        # Convert token predictions into word labels\n        # 0: lead_b, 1: lead_i\n        # 2: position_b, 3: position_i\n        # 4: evidence_b, 5: evidence_i\n        # 6: claim_b, 7: claim_i\n        # 8: conclusion_b, 9: conclusion_i\n        # 10: counterclaim_b, 11: counterclaim_i\n        # 12: rebuttal_b, 13: rebuttal_i\n        # 14: nothing (o)\n\n        pred = preds[id_num]/2\n    \n        i = 0\n        while i < max_len:\n            prediction = []\n            start = pred[i]\n            if start in [0,1,2,3,4,5,6,7]: # Only append if the class start with 'B'\n                prediction.append(word_map[i])\n                i += 1\n                if i >= max_len:\n                    break\n                while pred[i] == start + 0.5: # When the class is 'I'\n                    if not word_map[i] in prediction:\n                        prediction.append(word_map[i])\n                    i += 1\n                    if i >= max_len:\n                        break\n            else:\n                i += 1\n            prediction = [x for x in prediction if x!=-1]\n            #print(prediction)\n            \n            # Only accept if length of discourse larger than a thredshold\n            if len(prediction) > thredshold:\n                all_predictions.append((n, target_map_rev[int(start)], \n                                ' '.join([str(x) for x in prediction])))\n                \n    # MAKE DATAFRAME\n    df = pd.DataFrame(all_predictions)\n    df.columns = ['id','class','predictionstring']\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:57:29.872939Z","iopub.execute_input":"2022-03-30T11:57:29.873409Z","iopub.status.idle":"2022-03-30T11:57:29.892547Z","shell.execute_reply.started":"2022-03-30T11:57:29.873371Z","shell.execute_reply":"2022-03-30T11:57:29.891868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_preds(dataset, text_ids, preds, thredshold, augmented = None):\n    \n    \"\"\"\n    Made prediction and create result data frame\n    \n    Arguments:\n    dataset -- name of folder contain text files: 'train' or 'test'\n    text_ids -- array or list of essay id\n    preds -- the array contain predicted class\n    augmented -- name of augmented text folder: 'augmented_back', 'augmented_roberta', 'augmented_syn'\n    thredshold -- dictionary or array contain the minimum words required for each type\n    \n    Return:\n    df -- result data frame\n    \"\"\"\n    all_predictions = []\n    target_map_rev = {0: 'Lead', 1: 'Position', 2: 'Evidence', 3: 'Claim', 4: 'Concluding Statement',\n                      5: 'Counterclaim', 6: 'Rebuttal', 7: 'blank'}\n\n    for id_num in range(len(preds)):\n\n        n = text_ids[id_num] # Text id\n    \n        # Tokenize the text\n        try:\n            txt = open(f'../input/feedback-prize-2021/{dataset}/{n}.txt', 'r').read()\n        except:\n            txt = open(f'../input/data-augmented/{augmented}/{n}.txt', 'r').read()\n\n        tokens = tokenizer.encode_plus(txt, max_length = max_len, padding = 'max_length',\n                                   truncation = True, return_offsets_mapping = True)\n        off = tokens['offset_mapping']\n    \n        # Get word position\n        word_pos = []\n        blank = True\n        # The word must start with a symbol, and only the first symbol will be counted\n        for i in range(len(txt)):\n            if (not txt[i].isspace()) & (blank == True):\n                word_pos.append(i)\n                blank = False\n            # implied that previous word ended\n            elif txt[i].isspace():\n                blank = True\n        word_pos.append(1e6) # end\n            \n        # Mapping from tokens to words\n        word_map = -1 * np.ones(max_len, dtype = 'int32')\n        w_i = 0\n        for i in range(len(off)):\n            if off[i][1] == 0: # Skip character with token 0\n                continue\n            while off[i][0] >= word_pos[w_i+1]: #If token position is larger than word start position\n                w_i += 1\n            word_map[i] = int(w_i)\n        \n        # Convert token predictions into word labels\n        # 0: lead_b, 1: lead_i\n        # 2: position_b, 3: position_i\n        # 4: evidence_b, 5: evidence_i\n        # 6: claim_b, 7: claim_i\n        # 8: conclusion_b, 9: conclusion_i\n        # 10: counterclaim_b, 11: counterclaim_i\n        # 12: rebuttal_b, 13: rebuttal_i\n        # 14: nothing (o)\n\n        pred = preds[id_num]/2\n        \n        # If we see tokens I-X, I-Y, I-X -> change I-Y to I-X\n        for j in range(1, len(pred) - 1):\n            if pred[j - 1] == pred[j + 1] and pred[j - 1]%2 == 0.5 and pred[j] != pred[j - 1]:\n                pred[j] = pred[j - 1]\n            \n        # B-X, ? (not B), I-X -> change ? to I-X\n        for j in range(1, len(pred) - 1):\n            if pred[j - 1] in range(0,7,1) and pred[j + 1] == pred[j - 1] + 0.5 and pred[j] != pred[j + 1] and pred[j] not in range(0,7,1):\n                pred[j] = pred[j + 1]  \n        \n        # If we see tokens I-X, O, I-X, change center token to the same for stated discourse types\n        for j in range(1, len(pred) - 1):\n            if pred[j - 1] in [k + 0.5 for k in range(7)] and pred[j - 1] == pred[j + 1] and pred[j] == 7:\n                pred[j] = pred[j - 1]\n        \n        i = 0\n        while i < max_len:\n            prediction = []\n            start = pred[i]\n            # Only append if the class start with 'B'\n            if start in range(0, 7): \n                prediction.append(word_map[i])\n                i += 1\n                if i >= max_len:\n                    break\n                # When the class is 'I'\n                while pred[i] == start + 0.5: \n                    if not word_map[i] in prediction:\n                        prediction.append(word_map[i])\n                    i += 1\n                    if i >= max_len:\n                        break\n            else:\n                i += 1\n            \n            prediction = [x for x in prediction if x != -1]\n            \n            \n            # Skip blank classified word\n            if start == 7:\n                continue\n            \n            # Only accept if length of discourse larger than a thredshold\n            discourse_type = target_map_rev[int(start)]\n            if len(prediction) > thredshold[discourse_type]:\n                all_predictions.append((n, discourse_type, ' '.join([str(x) for x in prediction])))\n                \n    # MAake data frame\n    df = pd.DataFrame(all_predictions)\n    df.columns = ['id', 'class', 'predictionstring']\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-03-30T12:16:51.1417Z","iopub.execute_input":"2022-03-30T12:16:51.142139Z","iopub.status.idle":"2022-03-30T12:16:51.165761Z","shell.execute_reply.started":"2022-03-30T12:16:51.1421Z","shell.execute_reply":"2022-03-30T12:16:51.165022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check trained checkpoint to find best model\nfor i in range(1, 9):\n    del model\n    model = build_model()\n    model.load_weights(f'checkpoint-0{i}')\n    print(f'Model {i}')\n    check_ckp(model)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T17:26:59.658171Z","iopub.execute_input":"2022-03-07T17:26:59.658842Z","iopub.status.idle":"2022-03-07T17:45:47.134476Z","shell.execute_reply.started":"2022-03-07T17:26:59.658802Z","shell.execute_reply":"2022-03-07T17:45:47.132994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load and save best model\nmodel = build_model()\nmodel.load_weights('checkpoint-07')\nmodel.save_weights('v_15_0.617.h5')","metadata":{"execution":{"iopub.status.busy":"2022-03-21T12:17:41.238511Z","iopub.execute_input":"2022-03-21T12:17:41.239016Z","iopub.status.idle":"2022-03-21T12:17:43.573388Z","shell.execute_reply.started":"2022-03-21T12:17:41.238976Z","shell.execute_reply":"2022-03-21T12:17:43.572714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For validation set\npred = model.predict([val_tokens, val_attention], \n                  batch_size = 16, verbose = 1)\nprint('Validation predictions shape:', pred.shape)\noof_preds = np.argmax(pred, axis = -1)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T11:57:29.919405Z","iopub.execute_input":"2022-03-30T11:57:29.91974Z","iopub.status.idle":"2022-03-30T12:00:35.487521Z","shell.execute_reply.started":"2022-03-30T11:57:29.919704Z","shell.execute_reply":"2022-03-30T12:00:35.486633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For training set\npred = model.predict([train_tokens, train_attention],\n                     batch_size = 16, verbose = 1)\nprint('Training predictions shape:', pred.shape)\noof_preds = np.argmax(pred, axis = -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test soft thredshold\nfor k in np.arange(0.005, 0.055, 0.005 ):\n    print('k:', k)\n    thredshold = data_df.groupby('discourse_type')['discourse_num_word'].quantile(k)\n    oof = get_preds(dataset = 'train', text_ids = val_id, augmented = None, preds = oof_preds, thredshold = thredshold)\n    f1s = []\n    print('Validation F1_score:')\n    for c in classes:\n        pred_df = oof.loc[oof['class'] == c].copy()\n        gt_df = to_validate.loc[to_validate['discourse_type'] == c].copy()\n        f1 = score_feedback_comp(pred_df, gt_df)\n        print(c + ':', round(f1, 3))\n        f1s.append(f1)\n    print()\n    print('Overall',round(np.mean(f1s), 3))\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-03-30T12:17:20.617593Z","iopub.execute_input":"2022-03-30T12:17:20.618301Z","iopub.status.idle":"2022-03-30T12:21:19.88382Z","shell.execute_reply.started":"2022-03-30T12:17:20.618265Z","shell.execute_reply":"2022-03-30T12:21:19.883057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test hard thredshold\nfor ts in range(10):\n    oof = get_preds_old(dataset = 'train', text_ids = val_id, augmented = None, preds = oof_preds, thredshold = ts)\n    f1s = []\n    print('Threadshold:', ts)\n    print('Validation F1_score:')\n    for c in classes:\n        pred_df = oof.loc[oof['class'] == c].copy()\n        gt_df = to_validate.loc[to_validate['discourse_type'] == c].copy()\n        f1 = score_feedback_comp(pred_df, gt_df)\n        print(c + ':', round(f1, 3))\n        f1s.append(f1)\n    print()\n    print('Overall',round(np.mean(f1s), 3))\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-03-30T12:12:00.971032Z","iopub.execute_input":"2022-03-30T12:12:00.971316Z","iopub.status.idle":"2022-03-30T12:14:40.615618Z","shell.execute_reply.started":"2022-03-30T12:12:00.971266Z","shell.execute_reply":"2022-03-30T12:14:40.613362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df.groupby('discourse_type')['discourse_num_word'].quantile(0.005)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T12:27:57.055451Z","iopub.execute_input":"2022-03-30T12:27:57.055766Z","iopub.status.idle":"2022-03-30T12:27:57.094224Z","shell.execute_reply.started":"2022-03-30T12:27:57.055735Z","shell.execute_reply":"2022-03-30T12:27:57.093493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optimum thredshold\nthredshold = {'Lead': 6, 'Position': 4, 'Evidence': 9, 'Claim': 1, 'Concluding Statement': 8, 'Counterclaim': 5, 'Rebuttal': 4}","metadata":{"execution":{"iopub.status.busy":"2022-03-30T12:28:15.143084Z","iopub.execute_input":"2022-03-30T12:28:15.14335Z","iopub.status.idle":"2022-03-30T12:28:15.147938Z","shell.execute_reply.started":"2022-03-30T12:28:15.143319Z","shell.execute_reply":"2022-03-30T12:28:15.146847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compute Validation Metric","metadata":{}},{"cell_type":"code","source":"oof_old = get_preds_old(dataset = 'train', text_ids = val_id, augmented = None, preds = oof_preds, thredshold = 4)\noof_old.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T12:02:38.823792Z","iopub.execute_input":"2022-03-30T12:02:38.824681Z","iopub.status.idle":"2022-03-30T12:02:54.33202Z","shell.execute_reply.started":"2022-03-30T12:02:38.824612Z","shell.execute_reply":"2022-03-30T12:02:54.331321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_new = get_preds(dataset = 'train', text_ids = val_id, augmented = None, preds = oof_preds, thredshold = thredshold)\noof_new.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T12:28:41.37801Z","iopub.execute_input":"2022-03-30T12:28:41.378383Z","iopub.status.idle":"2022-03-30T12:29:04.890001Z","shell.execute_reply.started":"2022-03-30T12:28:41.378341Z","shell.execute_reply":"2022-03-30T12:29:04.889335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For training set\noof = get_preds(dataset = 'train', text_ids = train_id, augmented = None)\noof.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T10:08:44.880469Z","iopub.execute_input":"2022-03-16T10:08:44.881192Z","iopub.status.idle":"2022-03-16T10:10:18.792946Z","shell.execute_reply.started":"2022-03-16T10:08:44.881151Z","shell.execute_reply":"2022-03-16T10:10:18.792191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For training set, when using data augmentation\noof = get_preds(dataset = 'train', text_ids = val_id, augmented = 'augmented_syn')\noof.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:52:40.020235Z","iopub.execute_input":"2022-03-16T09:52:40.020519Z","iopub.status.idle":"2022-03-16T09:52:53.712754Z","shell.execute_reply.started":"2022-03-16T09:52:40.020486Z","shell.execute_reply":"2022-03-16T09:52:53.711102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaludate the old inference function.","metadata":{}},{"cell_type":"code","source":"f1s = []\n\nprint('Validation F1_score:')\nfor c in classes:\n    pred_df = oof_old.loc[oof_old['class'] == c].copy()\n    gt_df = to_validate.loc[to_validate['discourse_type'] == c].copy()\n    f1 = score_feedback_comp(pred_df, gt_df)\n    print(c + ':', round(f1, 3))\n    f1s.append(f1)\nprint()\nprint('Overall',round(np.mean(f1s), 3))\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-03-30T12:29:39.549143Z","iopub.execute_input":"2022-03-30T12:29:39.549453Z","iopub.status.idle":"2022-03-30T12:29:40.010319Z","shell.execute_reply.started":"2022-03-30T12:29:39.549422Z","shell.execute_reply":"2022-03-30T12:29:40.00962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaludate after post processing.","metadata":{}},{"cell_type":"code","source":"f1s = []\nclasses = np.array(['Lead', 'Position','Counterclaim','Rebuttal','Evidence',\n       'Claim', 'Concluding Statement'], dtype = object)\n\nprint('Validation F1_score:')\nfor c in classes:\n    pred_df = oof_new.loc[oof_new['class'] == c].copy()\n    gt_df = to_validate.loc[to_validate['discourse_type'] == c].copy()\n    f1 = score_feedback_comp(pred_df, gt_df)\n    print(c + ':', round(f1, 3))\n    f1s.append(f1)\nprint()\nprint('Overall',round(np.mean(f1s), 3))\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-03-30T12:29:49.127214Z","iopub.execute_input":"2022-03-30T12:29:49.127813Z","iopub.status.idle":"2022-03-30T12:29:49.60761Z","shell.execute_reply.started":"2022-03-30T12:29:49.127775Z","shell.execute_reply":"2022-03-30T12:29:49.606873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize the prediction","metadata":{}},{"cell_type":"code","source":"def visualize_prediction(ids, df, path):\n    \"\"\"\n    Visualize the prediction\n    \n    Arguments:\n    ids -- ID of the essay\n    df -- predicted data frame\n    path -- folder of the original text file\n    \"\"\"\n    with open(f'{path}/{ids}.txt', 'r') as file:\n        data = file.read()\n    ents = []\n    example = ids\n    curr_df = df[df[\"id\"] == example]\n    text = \" \".join(data.split())\n    splitted_text = text.split()\n    for i, row in curr_df.iterrows():\n        predictionstring = row['predictionstring']\n        \n        predictionstring = predictionstring.split()\n        w_start = int(predictionstring[0])\n        \n        w_end = int(predictionstring[-1])\n        ents.append({'start': len(\" \".join(splitted_text[:w_start])), \n                    'end': len(\" \".join(splitted_text[:w_end + 1])), \n                    'label': row['class']})\n    ents = sorted(ents, key = lambda i: i['start'])\n    \n    doc2 = {\"text\": text,\n        \"ents\": ents,\n        \"title\": example}\n\n    options = {\"ents\": ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement', 'Counterclaim', 'Rebuttal'], \"colors\": colors}\n    displacy.render(doc2, style = \"ent\", options = options, manual = True, jupyter = True)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T12:29:53.790718Z","iopub.execute_input":"2022-03-30T12:29:53.791268Z","iopub.status.idle":"2022-03-30T12:29:53.800376Z","shell.execute_reply.started":"2022-03-30T12:29:53.791229Z","shell.execute_reply":"2022-03-30T12:29:53.79935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualize the original text.","metadata":{}},{"cell_type":"code","source":"visualize_text('7EA804FD13A8', data_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T12:37:34.441552Z","iopub.execute_input":"2022-03-30T12:37:34.442128Z","iopub.status.idle":"2022-03-30T12:37:34.481335Z","shell.execute_reply.started":"2022-03-30T12:37:34.44209Z","shell.execute_reply":"2022-03-30T12:37:34.480704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualize prediction before post processing.","metadata":{}},{"cell_type":"code","source":"visualize_prediction(ids = '7EA804FD13A8', df = oof_old, path = '../input/feedback-prize-2021/train')","metadata":{"execution":{"iopub.status.busy":"2022-03-30T12:37:38.941454Z","iopub.execute_input":"2022-03-30T12:37:38.941738Z","iopub.status.idle":"2022-03-30T12:37:38.954341Z","shell.execute_reply.started":"2022-03-30T12:37:38.941701Z","shell.execute_reply":"2022-03-30T12:37:38.953657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualize prediction after post processing","metadata":{}},{"cell_type":"code","source":"visualize_prediction(ids = '7EA804FD13A8', df = oof_new, path = '../input/feedback-prize-2021/train')","metadata":{"execution":{"iopub.status.busy":"2022-03-30T12:37:42.408587Z","iopub.execute_input":"2022-03-30T12:37:42.408857Z","iopub.status.idle":"2022-03-30T12:37:42.421336Z","shell.execute_reply.started":"2022-03-30T12:37:42.408828Z","shell.execute_reply":"2022-03-30T12:37:42.420593Z"},"trusted":true},"execution_count":null,"outputs":[]}]}