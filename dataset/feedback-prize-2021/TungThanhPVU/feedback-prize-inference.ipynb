{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Feedback prize - Inference\nThis notebook is used for making kaggle submittion. The final test result is the score on the Kaggle private leaderboard.","metadata":{}},{"cell_type":"code","source":"#import pakages\nimport numpy as np\nimport pandas as pd \nimport os\nimport tensorflow as tf\nfrom transformers import *","metadata":{"execution":{"iopub.status.busy":"2022-03-30T10:35:30.1454Z","iopub.execute_input":"2022-03-30T10:35:30.145785Z","iopub.status.idle":"2022-03-30T10:35:52.220314Z","shell.execute_reply.started":"2022-03-30T10:35:30.14568Z","shell.execute_reply":"2022-03-30T10:35:52.219143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load data\ndata_df = pd.read_csv('../input/feedback-prize-2021/train.csv')\n\n# All ID list\nall_id = data_df.id.unique()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T10:35:52.223445Z","iopub.execute_input":"2022-03-30T10:35:52.22414Z","iopub.status.idle":"2022-03-30T10:35:53.979668Z","shell.execute_reply.started":"2022-03-30T10:35:52.224083Z","shell.execute_reply":"2022-03-30T10:35:53.978652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Token file\nload_tokens_from = '../input/longformerbase4096'\n\n# Pretrained model\ndownloaded_model_path = '../input/longformerbase4096'\n\n# NER target file\nNER_target = '../input/ner-target-for-feedback-prize-competition'\n\n# Max sequence length for model\nmax_len = 1024\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(downloaded_model_path)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T10:35:53.981659Z","iopub.execute_input":"2022-03-30T10:35:53.982061Z","iopub.status.idle":"2022-03-30T10:35:54.161444Z","shell.execute_reply.started":"2022-03-30T10:35:53.982011Z","shell.execute_reply":"2022-03-30T10:35:54.160455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    \"\"\"\n    Function to build and compile model\n    \"\"\"\n    tokens = tf.keras.layers.Input(shape = (max_len,), name = 'tokens', dtype = tf.int32)\n    attention = tf.keras.layers.Input(shape = (max_len,), name = 'attention', dtype = tf.int32)\n\n    config = AutoConfig.from_pretrained(downloaded_model_path + '/config.json') \n    backbone = TFAutoModel.from_pretrained(downloaded_model_path + '/tf_model.h5', config = config)\n\n    x = backbone(tokens, attention_mask = attention)\n    x = tf.keras.layers.Dense(512, activation='relu')(x[0])\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(15, activation='softmax', dtype='float32')(x)\n\n    model = tf.keras.Model(inputs = [tokens, attention], outputs = x)\n    model.compile(optimizer = 'adam',\n              loss = ['categorical_crossentropy'],\n              metrics = ['categorical_accuracy'])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-30T10:35:54.16469Z","iopub.execute_input":"2022-03-30T10:35:54.165453Z","iopub.status.idle":"2022-03-30T10:35:54.177175Z","shell.execute_reply.started":"2022-03-30T10:35:54.165393Z","shell.execute_reply":"2022-03-30T10:35:54.175904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert test text to tokens\nfiles = os.listdir('../input/feedback-prize-2021/test')\ntest_ids = [f.replace('.txt', '') for f in files if 'txt' in f]\n\ntest_tokens = np.zeros((len(test_ids), max_len), dtype = 'int32')\ntest_attention = np.zeros((len(test_ids), max_len), dtype = 'int32')\n\nfor id_num in range(len(test_ids)):          \n    n = test_ids[id_num]\n    name = f'../input/feedback-prize-2021/test/{n}.txt'\n    txt = open(name, 'r').read()\n    tokens = tokenizer.encode_plus(txt, max_length = max_len, padding = 'max_length',\n                                   truncation = True, return_offsets_mapping = True)\n    test_tokens[id_num] = tokens['input_ids']\n    test_attention[id_num] = tokens['attention_mask']","metadata":{"execution":{"iopub.status.busy":"2022-03-30T10:43:22.118489Z","iopub.execute_input":"2022-03-30T10:43:22.118797Z","iopub.status.idle":"2022-03-30T10:43:22.207956Z","shell.execute_reply.started":"2022-03-30T10:43:22.118765Z","shell.execute_reply":"2022-03-30T10:43:22.206946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If single model is used, then asign True. If ensemble is used, asign False.\nsingle = False","metadata":{"execution":{"iopub.status.busy":"2022-03-30T10:46:09.809413Z","iopub.execute_input":"2022-03-30T10:46:09.809733Z","iopub.status.idle":"2022-03-30T10:46:09.814705Z","shell.execute_reply.started":"2022-03-30T10:46:09.809692Z","shell.execute_reply":"2022-03-30T10:46:09.813583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if single:\n    model = build_model()\n    model.load_weights('../input/trained-model-for-feedback-prize-competition/final_fold5_0.636.h5')\n    test_preds = np.zeros((len(test_ids), max_len, 15))\n    test_preds = model.predict([test_tokens, test_attention], batch_size = 16, verbose = 1)\n    test_preds = np.argmax(test_preds, axis = -1)\nelse:\n    model1 = build_model()\n    model1.load_weights('../input/trained-model-for-feedback-prize-competition/final_fold1_0.634.h5')\n    model2 = build_model()\n    model2.load_weights('../input/trained-model-for-feedback-prize-competition/final_fold2_0.631.h5')\n    model3 = build_model()\n    model3.load_weights('../input/trained-model-for-feedback-prize-competition/final_fold3_0.621.h5')\n    model4 = build_model()\n    model4.load_weights('../input/trained-model-for-feedback-prize-competition/final_fold4_0.619.h5')\n    model5 = build_model()\n    model5.load_weights('../input/trained-model-for-feedback-prize-competition/final_fold5_0.636.h5') \n    \n    model_list = [model1, model2, model3, model4, model5]\n    test_preds = np.zeros((len(test_ids), max_len, 15))\n    \n    for md in model_list:\n        test_preds += md.predict([test_tokens, test_attention], batch_size = 16, verbose = 1)/5\n    test_preds = np.argmax(test_preds, axis = -1)   ","metadata":{"execution":{"iopub.status.busy":"2022-03-30T10:46:19.706325Z","iopub.execute_input":"2022-03-30T10:46:19.706626Z","iopub.status.idle":"2022-03-30T10:50:45.089101Z","shell.execute_reply.started":"2022-03-30T10:46:19.706592Z","shell.execute_reply":"2022-03-30T10:50:45.087996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optimum thredshold\nthredshold = {'Lead': 6, 'Position': 4, 'Evidence': 9, 'Claim': 1,\n              'Concluding Statement': 8, 'Counterclaim': 5, 'Rebuttal': 4}","metadata":{"execution":{"iopub.status.busy":"2022-03-30T10:50:45.09148Z","iopub.execute_input":"2022-03-30T10:50:45.09313Z","iopub.status.idle":"2022-03-30T10:50:45.100071Z","shell.execute_reply.started":"2022-03-30T10:50:45.093067Z","shell.execute_reply":"2022-03-30T10:50:45.098836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_preds(dataset, text_ids, preds, thredshold, augmented = None):\n    \n    \"\"\"\n    Made prediction and create result data frame\n    \n    Arguments:\n    dataset -- name of folder contain text files: 'train' or 'test'\n    text_ids -- array or list of essay id\n    preds -- the array contain predicted class\n    augmented -- name of augmented text folder: 'augmented_back', 'augmented_roberta', 'augmented_syn'\n    \n    Return:\n    df -- result data frame\n    \"\"\"\n    all_predictions = []\n    target_map_rev = {0: 'Lead', 1: 'Position', 2: 'Evidence', 3: 'Claim', 4: 'Concluding Statement',\n                      5: 'Counterclaim', 6: 'Rebuttal', 7: 'blank'}\n\n    for id_num in range(len(preds)):\n\n        n = text_ids[id_num] # Text id\n    \n        # Tokenize the text\n        try:\n            txt = open(f'../input/feedback-prize-2021/{dataset}/{n}.txt', 'r').read()\n        except:\n            txt = open(f'../input/data-augmented/{augmented}/{n}.txt', 'r').read()\n\n        tokens = tokenizer.encode_plus(txt, max_length = max_len, padding = 'max_length',\n                                   truncation = True, return_offsets_mapping = True)\n        off = tokens['offset_mapping']\n    \n        # Get word position\n        word_pos = []\n        blank = True\n        # The word must start with a symbol, and only the first symbol will be counted\n        for i in range(len(txt)):\n            if (not txt[i].isspace()) & (blank == True):\n                word_pos.append(i)\n                blank = False\n            # implied that previous word ended\n            elif txt[i].isspace():\n                blank = True\n        word_pos.append(1e6) # end\n            \n        # Mapping from tokens to words\n        word_map = -1*np.ones(max_len, dtype = 'int32')\n        w_i = 0\n        for i in range(len(off)):\n            if off[i][1] == 0: # Skip character with token 0\n                continue\n            #If token position is larger than word start position\n            while off[i][0] >= word_pos[w_i + 1]: \n                w_i += 1\n            word_map[i] = int(w_i)\n        \n        # Convert token predictions into word labels\n        # 0: lead_b, 1: lead_i\n        # 2: position_b, 3: position_i\n        # 4: evidence_b, 5: evidence_i\n        # 6: claim_b, 7: claim_i\n        # 8: conclusion_b, 9: conclusion_i\n        # 10: counterclaim_b, 11: counterclaim_i\n        # 12: rebuttal_b, 13: rebuttal_i\n        # 14: nothing (o)\n\n        pred = preds[id_num]/2\n        \n        # If we see tokens I-X, I-Y, I-X -> change I-Y to I-X\n        for j in range(1, len(pred) - 1):\n            if pred[j - 1] == pred[j + 1] and pred[j - 1]%2 == 0.5 and pred[j] != pred[j - 1]:\n                pred[j] = pred[j-1]\n            \n        # B-X, ? (not B), I-X -> change ? to I-X\n        for j in range(1, len(pred) - 1):\n            if pred[j - 1] in range(0, 7, 1) and pred[j + 1] == pred[j - 1] + 0.5 \\\n            and pred[j] != pred[j + 1] and pred[j] not in range(0, 7, 1):\n                pred[j] = pred[j+1]  \n        \n        # If we see tokens I-X, O, I-X, change center token to the same for stated discourse types\n        for j in range(1, len(pred) - 1):\n            if pred[j - 1] in [k + 0.5 for k in range(7)] and pred[j - 1] == pred[j + 1] and pred[j] == 7:\n                pred[j] = pred[j - 1]\n        \n        i = 0\n        while i < max_len:\n            prediction = []\n            start = pred[i]\n            # Only append if the class start with 'B'\n            if start in range(0,7): \n                prediction.append(word_map[i])\n                i += 1\n                if i >= max_len:\n                    break\n                # When the class is 'I'\n                while pred[i] == start + 0.5: \n                    if not word_map[i] in prediction:\n                        prediction.append(word_map[i])\n                    i += 1\n                    if i >= max_len:\n                        break\n            else:\n                i += 1\n            \n            prediction = [x for x in prediction if x != -1]\n            \n            \n            # Skip blank classified word\n            if start == 7:\n                continue\n            \n            # Only accept if length of discourse larger than a thredshold\n            discourse_type = target_map_rev[int(start)]\n            if len(prediction) > thredshold[discourse_type]:\n                all_predictions.append((n, discourse_type, ' '.join([str(x) for x in prediction])))\n                \n    # Make dataframe\n    df = pd.DataFrame(all_predictions)\n    df.columns = ['id', 'class', 'predictionstring']\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-03-30T10:50:45.102395Z","iopub.execute_input":"2022-03-30T10:50:45.102784Z","iopub.status.idle":"2022-03-30T10:50:45.133113Z","shell.execute_reply.started":"2022-03-30T10:50:45.102732Z","shell.execute_reply":"2022-03-30T10:50:45.132007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = get_preds(dataset = 'test', text_ids = test_ids, preds = test_preds, thredshold = thredshold)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T10:50:45.137643Z","iopub.execute_input":"2022-03-30T10:50:45.138439Z","iopub.status.idle":"2022-03-30T10:50:45.268238Z","shell.execute_reply.started":"2022-03-30T10:50:45.138388Z","shell.execute_reply":"2022-03-30T10:50:45.267056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T10:50:45.270247Z","iopub.execute_input":"2022-03-30T10:50:45.270594Z","iopub.status.idle":"2022-03-30T10:50:45.280985Z","shell.execute_reply.started":"2022-03-30T10:50:45.270545Z","shell.execute_reply":"2022-03-30T10:50:45.279638Z"},"trusted":true},"execution_count":null,"outputs":[]}]}