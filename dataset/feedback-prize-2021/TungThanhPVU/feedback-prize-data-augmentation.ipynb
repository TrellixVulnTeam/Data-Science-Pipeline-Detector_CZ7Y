{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Augmentation\n\nWe will try some augmentation method, to artificialy increase the number of data.\n\nData augmentation can be used, for instance, to increase the number of texts containing counter-claims and rebuttals, since they are not well represented in the original dataset.\n\nThis notebook further creates a new csv training file, and raw text files for the augmented data, so that they can be directly used in the following training process.\n\nWe will use nlpaug library to perform data augmentation.","metadata":{}},{"cell_type":"code","source":"!pip install nlpaug","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:33:48.265938Z","iopub.execute_input":"2022-03-29T08:33:48.266252Z","iopub.status.idle":"2022-03-29T08:33:57.398979Z","shell.execute_reply.started":"2022-03-29T08:33:48.266173Z","shell.execute_reply":"2022-03-29T08:33:57.398065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom transformers import *\nfrom tqdm.auto import tqdm\ntqdm.pandas()\nimport nlpaug.augmenter.word as naw","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:33:58.587068Z","iopub.execute_input":"2022-03-29T08:33:58.587357Z","iopub.status.idle":"2022-03-29T08:34:10.877348Z","shell.execute_reply.started":"2022-03-29T08:33:58.587301Z","shell.execute_reply":"2022-03-29T08:34:10.876559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"data_df = pd.read_csv('../input/feedback-prize-2021/train.csv')\ndata_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:34:10.879131Z","iopub.execute_input":"2022-03-29T08:34:10.879405Z","iopub.status.idle":"2022-03-29T08:34:12.306807Z","shell.execute_reply.started":"2022-03-29T08:34:10.879371Z","shell.execute_reply":"2022-03-29T08:34:12.306144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# All ID list\nall_id = data_df.id.unique()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:34:12.308054Z","iopub.execute_input":"2022-03-29T08:34:12.308341Z","iopub.status.idle":"2022-03-29T08:34:12.329362Z","shell.execute_reply.started":"2022-03-29T08:34:12.308276Z","shell.execute_reply":"2022-03-29T08:34:12.328719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augmentation using Nlpaug","metadata":{}},{"cell_type":"markdown","source":"## Synonym Augmentation\n\nThe method simply replace some of the words in the original text by their synonym.\n\nFor this, I set the percentage of word will be augmented to 10%, with maximum number of word will be augmented is 15.","metadata":{}},{"cell_type":"code","source":"syn_aug = naw.SynonymAug(aug_src = 'wordnet', aug_max = 15, aug_p = 0.1)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:34:42.014295Z","iopub.execute_input":"2022-03-29T08:34:42.014593Z","iopub.status.idle":"2022-03-29T08:34:42.019082Z","shell.execute_reply.started":"2022-03-29T08:34:42.014563Z","shell.execute_reply":"2022-03-29T08:34:42.018071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Contextual Embedding\n\nContextual embedding use nlp models (here transformers), to understand the context of the input text and replace/add words keeping the context. As a result, the new text may have additional words or slightly different meaning. \n\nHere for the embedding model, we will use pretrained roberta model The percentage of word will be augmented is 20% and the text will be replaced with maximum 10 words. ","metadata":{}},{"cell_type":"code","source":"context_aug = naw.ContextualWordEmbsAug(model_path = \"roberta-base\", action = \"substitute\",\n                                       aug_max = 10, device = \"cuda\", aug_p = 0.2)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:37:14.07473Z","iopub.execute_input":"2022-03-29T08:37:14.075495Z","iopub.status.idle":"2022-03-29T08:37:34.306246Z","shell.execute_reply.started":"2022-03-29T08:37:14.075454Z","shell.execute_reply":"2022-03-29T08:37:34.305453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Back Translation\n\nIn this method, we translate the text data to some other language and then translate it back to the original language. This can help to generate textual data with different words while preserving the meaning of the text data. By default English -> German -> English.\n\n","metadata":{}},{"cell_type":"code","source":"back_trans_aug = naw.BackTranslationAug(max_length = 1024, device = 'cuda')","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:51:29.639885Z","iopub.execute_input":"2022-03-29T08:51:29.640151Z","iopub.status.idle":"2022-03-29T08:52:40.815833Z","shell.execute_reply.started":"2022-03-29T08:51:29.640122Z","shell.execute_reply":"2022-03-29T08:52:40.814979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Augmented dataset","metadata":{}},{"cell_type":"markdown","source":"## Create data frame for augmentation","metadata":{}},{"cell_type":"code","source":"# Create training and validation set\nnp.random.seed(6)\ntrain_idx = np.random.choice(np.arange(len(all_id)),int(0.4*len(all_id)),replace = False)\nleft_set = np.setdiff1d(np.arange(len(all_id)),train_idx)\nvalid_idx = np.random.choice(left_set, int(0.1*len(all_id)), replace = False)\nnp.random.seed(None)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:00:57.853055Z","iopub.execute_input":"2022-03-29T09:00:57.85334Z","iopub.status.idle":"2022-03-29T09:00:57.866094Z","shell.execute_reply.started":"2022-03-29T09:00:57.853294Z","shell.execute_reply":"2022-03-29T09:00:57.865431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training data frame\ntrain_selected = data_df[data_df[\"id\"].isin(all_id[train_idx])].copy()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:02:09.24327Z","iopub.execute_input":"2022-03-29T09:02:09.24355Z","iopub.status.idle":"2022-03-29T09:02:09.278963Z","shell.execute_reply.started":"2022-03-29T09:02:09.243521Z","shell.execute_reply":"2022-03-29T09:02:09.278243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select essay contains 'Rebuttal' and 'Counterclaim' type for augmentation\naugmented_id_list = train_selected[train_selected['discourse_type'].isin(['Rebuttal', 'Counterclaim'])].id.unique()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:06:43.4321Z","iopub.execute_input":"2022-03-29T09:06:43.432383Z","iopub.status.idle":"2022-03-29T09:06:43.441754Z","shell.execute_reply.started":"2022-03-29T09:06:43.432351Z","shell.execute_reply":"2022-03-29T09:06:43.440996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of essay in the training set', train_selected.id.nunique())\nprint('Number of essay to be augmented:', augmented_id_list.shape[0])\nprint('Pecent of essay to be augmented:', augmented_id_list.shape[0]*100/train_selected.id.nunique())","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:14:12.788101Z","iopub.execute_input":"2022-03-29T09:14:12.788833Z","iopub.status.idle":"2022-03-29T09:14:12.80758Z","shell.execute_reply.started":"2022-03-29T09:14:12.788793Z","shell.execute_reply":"2022-03-29T09:14:12.806901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The data frame contains text to be augmented\nto_aug_df = train_selected[train_selected.id.isin(augmented_id_list)].copy()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:12:33.945795Z","iopub.execute_input":"2022-03-29T09:12:33.946045Z","iopub.status.idle":"2022-03-29T09:12:33.959243Z","shell.execute_reply.started":"2022-03-29T09:12:33.946016Z","shell.execute_reply":"2022-03-29T09:12:33.958513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Apply data augmentation","metadata":{}},{"cell_type":"code","source":"# Selecting the augmentation method, in the list: syn_aug, context_aug, back_trans_aug\naugmenter = syn_aug\n\n# Set the following to avoid warning message\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Applying augmentation to all the selected texts\nto_aug_df[\"augmented_text\"] = to_aug_df.progress_apply(lambda row: augmenter.augment(row[\"discourse_text\"]), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:12:36.479697Z","iopub.execute_input":"2022-03-29T09:12:36.480236Z","iopub.status.idle":"2022-03-29T09:13:32.982469Z","shell.execute_reply.started":"2022-03-29T09:12:36.480201Z","shell.execute_reply":"2022-03-29T09:13:32.981656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_aug_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:22:04.327055Z","iopub.execute_input":"2022-03-29T09:22:04.327336Z","iopub.status.idle":"2022-03-29T09:22:04.343544Z","shell.execute_reply.started":"2022-03-29T09:22:04.327286Z","shell.execute_reply":"2022-03-29T09:22:04.342801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fix problem with text, especialy when using Contextual Embedding\nto_aug_df['augmented_text'] = to_aug_df['augmented_text'].str.replace(\" ' \", \"'\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating new data frame and text files\n","metadata":{}},{"cell_type":"code","source":"def augment_text_file(text_id, data_df, save_folder_name = 'data_augmented'):\n    \n    \"\"\"\n    Create infomation for agmented text and save to text files\n    \n    Arguments:\n    text_id -- array or list of essay id\n    save_folder_name -- folder to save augmented files\n    data_df -- the dataframe of data set\n    \n    Returns:\n    augmented_d -- Augmented dataframe\n    \"\"\"\n    import os\n    os.mkdir(save_folder_name)\n\n    agmented_list = []\n    for txt_id in text_id:\n        # Get original text\n        file_path = f'../input/feedback-prize-2021/train/{txt_id}.txt'\n        with open(file_path, 'r') as fr:\n            original_text = fr.read()\n\n        # Get corresponding data in the data frame\n        text_df = data_df[data_df[\"id\"] == txt_id].copy()\n        \n        # Init variables for augmented text\n        char_pos_original = 0  # trace the character position in the original text\n        new_text = \"\"\n        discourse_start_list = []\n        discourse_end_list = []\n        prediction_string_list = []\n\n        # Loop on the training data discourses\n        for row in text_df[[\"discourse_start\", \"discourse_end\"]].itertuples():\n            discourse_start, discourse_end = int(row[1]), int(row[2])\n\n            # Copy the non-discourse text from the orginal\n            if char_pos_original < discourse_start:\n                new_text += original_text[char_pos_original:discourse_start] \n            else:\n                new_text += ' '\n\n            # Evaluate the new discourse starting position/string\n            discourse_start_new = len(new_text)  #character position\n            discourse_start_list.append(discourse_start_new)\n            word_start = len(new_text.split()) #prediction string position\n\n            # Copy the augmented discourse text\n            new_text += text_df[text_df[\"discourse_start\"] == discourse_start][\"augmented_text\"].iloc[0]\n\n\n            # Evaluate the new discourse end position/string\n            discourse_end_list.append(len(new_text))\n            word_end = word_start + len(new_text[discourse_start_new:].split()) #prediction string position   \n            prediction_string_list.append(\" \".join([str(x) for x in range(word_start, word_end)])) # presiction string for that disourse\n\n            char_pos_original = discourse_end\n        \n        # Write new info to the dataframe\n        text_df[\"discourse_start_augmented\"] = discourse_start_list\n        text_df[\"discourse_end_augmented\"] = discourse_end_list\n        text_df[\"predictionstring_augmented\"] = prediction_string_list\n\n        # Copy the remaining of the original text if there are any\n        if char_pos_original < len(original_text) - 1:\n            new_text += original_text[char_pos_original:]\n\n        # Save to new text file\n        with open(f\"./{save_folder_name}/{txt_id}_aug.txt\", \"w\") as file:\n            file.write(new_text)\n        \n        # Save all the augmented dataframe to a list\n        agmented_list.append(text_df)\n        \n    augmented_df = pd.concat(agmented_list)\n    \n    return augmented_df","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:42:41.993207Z","iopub.execute_input":"2022-03-29T09:42:41.993541Z","iopub.status.idle":"2022-03-29T09:42:42.005013Z","shell.execute_reply.started":"2022-03-29T09:42:41.993506Z","shell.execute_reply":"2022-03-29T09:42:42.004363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create augmented text\naugmented_df = augment_text_file(text_id = augmented_id_list,\n                                 save_folder_name = 'data_augmented',\n                                 data_df = to_aug_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:43:01.641093Z","iopub.execute_input":"2022-03-29T09:43:01.641371Z","iopub.status.idle":"2022-03-29T09:43:21.456638Z","shell.execute_reply.started":"2022-03-29T09:43:01.64134Z","shell.execute_reply":"2022-03-29T09:43:21.455903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Checking\n\nThe new dataframe now contains:\n- `discourse_start_augmented`\n- `discourse_end_augmented`\n- `augmented_text`\n- `predictionstring_augmented`\n\nLet us see how do they look like.","metadata":{}},{"cell_type":"code","source":"# Sanity check\naugmented_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:34:45.380823Z","iopub.execute_input":"2022-03-29T09:34:45.381154Z","iopub.status.idle":"2022-03-29T09:34:45.412868Z","shell.execute_reply.started":"2022-03-29T09:34:45.38112Z","shell.execute_reply":"2022-03-29T09:34:45.412194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augmented_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:44:48.120282Z","iopub.execute_input":"2022-03-29T09:44:48.120836Z","iopub.status.idle":"2022-03-29T09:44:48.139955Z","shell.execute_reply.started":"2022-03-29T09:44:48.1208Z","shell.execute_reply":"2022-03-29T09:44:48.139303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the index of the dataframe one wants to check\ncheck_idx = 3\n\n# Loading texts\ncheck_id = augmented_df.iloc[check_idx][\"id\"]\nwith open(f'../input/feedback-prize-2021/train/{check_id}.txt', \"r\") as f:\n    original_text = f.read()\nwith open(f\"./data_augmented/{check_id}_aug.txt\") as f:\n    new_text = f.read()\n\n# Checking the original discourse\nprint(\"Original\")\nprint(f\"----- Discourse text in the dataframe: \\n {augmented_df.iloc[check_idx]['discourse_text']}\")\nprint(f\"----- Discourse text in the text file: \\n {original_text[int(augmented_df.iloc[check_idx]['discourse_start']):int(augmented_df.iloc[check_idx]['discourse_end'])]}\")\n\n# Checking the new discourse\nprint(\"\\n New\")\nprint(f\"----- Discourse text in the dataframe: \\n {augmented_df.iloc[check_idx]['augmented_text']}\")\nprint(f\"----- Discourse text in the text file: \\n {new_text[int(augmented_df.iloc[check_idx]['discourse_start_augmented']):int(augmented_df.iloc[check_idx]['discourse_end_augmented'])]}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-29T10:21:46.883622Z","iopub.execute_input":"2022-03-29T10:21:46.883893Z","iopub.status.idle":"2022-03-29T10:21:46.903656Z","shell.execute_reply.started":"2022-03-29T10:21:46.883863Z","shell.execute_reply":"2022-03-29T10:21:46.902931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Zip the augmented folder file to download\nimport shutil\nshutil.make_archive('augmented_syn', 'zip', './data_augmented')","metadata":{"execution":{"iopub.status.busy":"2022-03-29T09:47:13.820167Z","iopub.execute_input":"2022-03-29T09:47:13.820627Z","iopub.status.idle":"2022-03-29T09:47:14.189823Z","shell.execute_reply.started":"2022-03-29T09:47:13.820588Z","shell.execute_reply":"2022-03-29T09:47:14.189009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save to csv file \n","metadata":{}},{"cell_type":"code","source":"# Choose specific columns\naug_df = augmented_df[['id', 'discourse_id', 'discourse_start_augmented', 'discourse_end_augmented',\n                    'augmented_text', 'discourse_type', 'discourse_type_num', 'predictionstring_augmented']].copy()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T10:22:25.452757Z","iopub.execute_input":"2022-03-29T10:22:25.453267Z","iopub.status.idle":"2022-03-29T10:22:25.466218Z","shell.execute_reply.started":"2022-03-29T10:22:25.453231Z","shell.execute_reply":"2022-03-29T10:22:25.465539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change id so that they are difference from original ones\naug_df['discourse_id'] = aug_df['discourse_id']*10\naug_df['id'] = aug_df['id'] + '_aug'","metadata":{"execution":{"iopub.status.busy":"2022-03-29T10:22:25.655222Z","iopub.execute_input":"2022-03-29T10:22:25.655836Z","iopub.status.idle":"2022-03-29T10:22:25.663462Z","shell.execute_reply.started":"2022-03-29T10:22:25.655799Z","shell.execute_reply":"2022-03-29T10:22:25.662662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rename to match the original data frame\naug_df.rename(columns = {'augmented_text': 'discourse_text',\n                         'discourse_start_augmented': 'discourse_start',\n                         'discourse_end_augmented': 'discourse_end', \n                        'predictionstring_augmented': 'predictionstring'},\n              inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T10:22:28.570968Z","iopub.execute_input":"2022-03-29T10:22:28.573504Z","iopub.status.idle":"2022-03-29T10:22:28.580103Z","shell.execute_reply.started":"2022-03-29T10:22:28.573458Z","shell.execute_reply":"2022-03-29T10:22:28.579378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aug_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T10:22:30.98094Z","iopub.execute_input":"2022-03-29T10:22:30.981208Z","iopub.status.idle":"2022-03-29T10:22:30.996264Z","shell.execute_reply.started":"2022-03-29T10:22:30.98118Z","shell.execute_reply":"2022-03-29T10:22:30.995551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the data frame to csv file\naug_df.to_csv('train_synonym_augmented.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T10:23:12.609241Z","iopub.execute_input":"2022-03-29T10:23:12.609796Z","iopub.status.idle":"2022-03-29T10:23:13.030515Z","shell.execute_reply.started":"2022-03-29T10:23:12.609758Z","shell.execute_reply":"2022-03-29T10:23:13.029767Z"},"trusted":true},"execution_count":null,"outputs":[]}]}