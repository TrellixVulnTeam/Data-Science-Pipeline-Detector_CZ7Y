{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### What are you trying to do in this notebook?\nIn this competition, I’ll identify elements in student writing. More specifically, I will automatically segment texts and classify argumentative and rhetorical elements in essays written by 6th-12th grade students. I'll access to the largest dataset of student writing ever released in order to test your skills in natural language processing, a fast-growing area of data science.\n\n#### Why are you trying it?\nI'll make it easier for students to receive feedback on their writing and increase opportunities to improve writing outcomes. Virtual writing tutors and automated writing systems can leverage these algorithms while teachers may use them to reduce grading time. The open-sourced algorithms you come up with will allow any educational organization to better help young writers develop.\n\nMy task is to predict the human annotations. I will first need to segment each essay into discrete rhetorical and argumentative elements (i.e., discourse elements) and then classify each element as one of the following:\n\n- Lead - an introduction that begins with a statistic, a quotation, a description, or some other device to grab the reader’s attention and point toward the thesis\n\n- Position - an opinion or conclusion on the main question\n\n- Claim - a claim that supports the position\n\n- Counterclaim - a claim that refutes another claim or gives an opposing reason to the position\n\n- Rebuttal - a claim that refutes a counterclaim\n\n- Evidence - ideas or examples that support claims, counterclaims, or rebuttals.\n\n- Concluding Statement - a concluding statement that restates the claims","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:05:06.108026Z","iopub.execute_input":"2022-01-04T06:05:06.10829Z","iopub.status.idle":"2022-01-04T06:05:10.217789Z","shell.execute_reply.started":"2022-01-04T06:05:06.10826Z","shell.execute_reply":"2022-01-04T06:05:10.217094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# libraries\nimport os\nimport numpy as np \nimport pandas as pd \npd.options.mode.chained_assignment = None\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\n# nltk\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:05:10.219625Z","iopub.execute_input":"2022-01-04T06:05:10.219876Z","iopub.status.idle":"2022-01-04T06:05:10.227353Z","shell.execute_reply.started":"2022-01-04T06:05:10.21984Z","shell.execute_reply":"2022-01-04T06:05:10.226601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# files\nos.listdir(\"../input/feedback-prize-2021\")","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:05:10.228738Z","iopub.execute_input":"2022-01-04T06:05:10.228983Z","iopub.status.idle":"2022-01-04T06:05:10.241933Z","shell.execute_reply.started":"2022-01-04T06:05:10.22895Z","shell.execute_reply":"2022-01-04T06:05:10.240968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/feedback-prize-2021/sample_submission.csv\")\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:05:10.243465Z","iopub.execute_input":"2022-01-04T06:05:10.243713Z","iopub.status.idle":"2022-01-04T06:05:10.257902Z","shell.execute_reply.started":"2022-01-04T06:05:10.243678Z","shell.execute_reply":"2022-01-04T06:05:10.257263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/feedback-prize-2021/train.csv')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:05:10.26007Z","iopub.execute_input":"2022-01-04T06:05:10.260478Z","iopub.status.idle":"2022-01-04T06:05:10.97085Z","shell.execute_reply.started":"2022-01-04T06:05:10.260444Z","shell.execute_reply":"2022-01-04T06:05:10.970138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.discourse_type.value_counts(normalize=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:05:10.972008Z","iopub.execute_input":"2022-01-04T06:05:10.972873Z","iopub.status.idle":"2022-01-04T06:05:10.999853Z","shell.execute_reply.started":"2022-01-04T06:05:10.972834Z","shell.execute_reply":"2022-01-04T06:05:10.999072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = \"../input/feedback-prize-2021/train\"\ntest_dir = \"../input/feedback-prize-2021/test\"","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:05:11.001095Z","iopub.execute_input":"2022-01-04T06:05:11.001509Z","iopub.status.idle":"2022-01-04T06:05:11.006701Z","shell.execute_reply.started":"2022-01-04T06:05:11.001455Z","shell.execute_reply":"2022-01-04T06:05:11.005753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_full_text_dataframe(train=True) -> pd.DataFrame:\n    id_list = []\n    text_list = []\n    \n    if train:\n        for id in tqdm(train_df.id):\n            filepath = os.path.join(train_dir, f\"{id}.txt\")\n            text = open(filepath, 'r').read()\n\n            id_list.append(id)\n            text_list.append(text)\n    else:\n        for filename in tqdm(os.listdir(test_dir)):\n            filepath = os.path.join(test_dir, filename)\n            id = str(filename).strip()[:-4]\n            text = open(filepath, 'r').read()\n            \n            id_list.append(id)\n            text_list.append(text)\n            \n    return pd.DataFrame(data={\"id\": id_list, \"text\": text_list})","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:05:11.0086Z","iopub.execute_input":"2022-01-04T06:05:11.009025Z","iopub.status.idle":"2022-01-04T06:05:11.021141Z","shell.execute_reply.started":"2022-01-04T06:05:11.008983Z","shell.execute_reply":"2022-01-04T06:05:11.020385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = create_full_text_dataframe()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:05:11.022632Z","iopub.execute_input":"2022-01-04T06:05:11.02297Z","iopub.status.idle":"2022-01-04T06:06:08.680355Z","shell.execute_reply.started":"2022-01-04T06:05:11.02285Z","shell.execute_reply":"2022-01-04T06:06:08.6797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = create_full_text_dataframe(train=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:06:08.685396Z","iopub.execute_input":"2022-01-04T06:06:08.685648Z","iopub.status.idle":"2022-01-04T06:06:08.724104Z","shell.execute_reply.started":"2022-01-04T06:06:08.685612Z","shell.execute_reply":"2022-01-04T06:06:08.723479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.merge(train_df, on=\"id\", how=\"inner\")","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:06:08.725235Z","iopub.execute_input":"2022-01-04T06:06:08.730563Z","iopub.status.idle":"2022-01-04T06:06:08.916814Z","shell.execute_reply.started":"2022-01-04T06:06:08.730526Z","shell.execute_reply":"2022-01-04T06:06:08.916059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df_train[[\"id\", \"discourse_text\", \"discourse_type\"]]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:06:08.918274Z","iopub.execute_input":"2022-01-04T06:06:08.918994Z","iopub.status.idle":"2022-01-04T06:06:09.223187Z","shell.execute_reply.started":"2022-01-04T06:06:08.918952Z","shell.execute_reply":"2022-01-04T06:06:09.222461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lemmatize (or stem?)\n\n#lemmatizer = WordNetLemmatizer()\n#df['discourse_text'] = df['discourse_text'].progress_apply(lambda text: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(text)]))","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:06:09.224684Z","iopub.execute_input":"2022-01-04T06:06:09.224917Z","iopub.status.idle":"2022-01-04T06:06:09.228028Z","shell.execute_reply.started":"2022-01-04T06:06:09.224884Z","shell.execute_reply":"2022-01-04T06:06:09.227389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target label encoding\nencoder = LabelEncoder()\nlabels = encoder.fit_transform(df.discourse_type)\n\n# encoded target labels\ndf.loc[:, \"label\"] = labels","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:06:09.231865Z","iopub.execute_input":"2022-01-04T06:06:09.232323Z","iopub.status.idle":"2022-01-04T06:06:09.690322Z","shell.execute_reply.started":"2022-01-04T06:06:09.232287Z","shell.execute_reply":"2022-01-04T06:06:09.68956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train/test splitting\nX_train, X_test, y_train, y_test = train_test_split(df.discourse_text, df.label, test_size=0.02)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:06:09.69406Z","iopub.execute_input":"2022-01-04T06:06:09.69427Z","iopub.status.idle":"2022-01-04T06:06:09.939193Z","shell.execute_reply.started":"2022-01-04T06:06:09.694244Z","shell.execute_reply":"2022-01-04T06:06:09.938451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Pipeline and training\n\nmultinomialNB = Pipeline([\n        ('vect', CountVectorizer(stop_words='english', ngram_range=(1, 2), binary=True)),\n        ('tfidf', TfidfTransformer(norm='l2', use_idf=True)),\n        ('clf', MultinomialNB(alpha=0.1)),    \n])\n\n# training\nmultinomialNB.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:06:09.941584Z","iopub.execute_input":"2022-01-04T06:06:09.94204Z","iopub.status.idle":"2022-01-04T06:07:49.052888Z","shell.execute_reply.started":"2022-01-04T06:06:09.942001Z","shell.execute_reply":"2022-01-04T06:07:49.052163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction on test data\ny_test_pred = multinomialNB.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:07:49.054148Z","iopub.execute_input":"2022-01-04T06:07:49.054499Z","iopub.status.idle":"2022-01-04T06:07:50.9443Z","shell.execute_reply.started":"2022-01-04T06:07:49.054462Z","shell.execute_reply":"2022-01-04T06:07:50.943465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Results on test set\nprint(\"\\nTest Precision:\", metrics.precision_score(y_test, y_test_pred, average='micro'))\nprint(\"\\nTest Recall:\", metrics.recall_score(y_test, y_test_pred, average='micro'))\nprint(\"\\nClassification Report:\\n\", metrics.classification_report(y_test, y_test_pred))","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:07:50.945946Z","iopub.execute_input":"2022-01-04T06:07:50.946218Z","iopub.status.idle":"2022-01-04T06:07:51.028794Z","shell.execute_reply.started":"2022-01-04T06:07:50.946182Z","shell.execute_reply":"2022-01-04T06:07:51.028087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(y_test, y_scores, class_names):\n    num_class = len(class_names)\n    cm = metrics.confusion_matrix(y_test, y_scores)\n\n    # normalize\n    con = np.zeros((num_class, num_class))\n    for x in range(num_class):\n        for y in range(num_class):\n            con[x,y] = cm[x,y]/np.sum(cm[x,:])\n\n    plt.figure(figsize=(10,8))\n    sns.set(font_scale=1.0) # for label size\n    sns.heatmap(con, annot=True, fmt=\".2\", cmap='Blues',xticklabels= class_names , yticklabels= class_names)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:07:51.029904Z","iopub.execute_input":"2022-01-04T06:07:51.030303Z","iopub.status.idle":"2022-01-04T06:07:51.039041Z","shell.execute_reply.started":"2022-01-04T06:07:51.030264Z","shell.execute_reply":"2022-01-04T06:07:51.038323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(y_test, y_test_pred, encoder.inverse_transform(np.unique(labels)))","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:07:51.040219Z","iopub.execute_input":"2022-01-04T06:07:51.041282Z","iopub.status.idle":"2022-01-04T06:07:51.575907Z","shell.execute_reply.started":"2022-01-04T06:07:51.041252Z","shell.execute_reply":"2022-01-04T06:07:51.57525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create df_test: a record from each discourse within test text docs\ndef expand_df_test(df: pd.DataFrame=df_test) -> pd.DataFrame:\n    \n    ids = []; data = []\n    for id, text in zip(df.id, df.text):\n        sentences = nltk.sent_tokenize(text)\n        \n        id_sentences = []; idx = 0 \n        for sentence in sentences:\n            id_sentence = []\n            words = sentence.split()\n            for w in words:\n                id_sentence.append(idx)\n                idx += 1\n            id_sentences.append(id_sentence)\n        data += list(zip([id] * len(sentences), sentences, id_sentences))\n        \n    tmp = pd.DataFrame(data, columns=['id', 'text', 'predictionstring'])\n    return tmp","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:07:51.57717Z","iopub.execute_input":"2022-01-04T06:07:51.579797Z","iopub.status.idle":"2022-01-04T06:07:51.588224Z","shell.execute_reply.started":"2022-01-04T06:07:51.579758Z","shell.execute_reply":"2022-01-04T06:07:51.587343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = expand_df_test()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:07:51.589674Z","iopub.execute_input":"2022-01-04T06:07:51.589931Z","iopub.status.idle":"2022-01-04T06:07:51.608043Z","shell.execute_reply.started":"2022-01-04T06:07:51.589895Z","shell.execute_reply":"2022-01-04T06:07:51.607324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction on test data\n\ntest[\"class\"] = multinomialNB.predict(test.text)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:07:51.610221Z","iopub.execute_input":"2022-01-04T06:07:51.61078Z","iopub.status.idle":"2022-01-04T06:07:51.651357Z","shell.execute_reply.started":"2022-01-04T06:07:51.610744Z","shell.execute_reply":"2022-01-04T06:07:51.650605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.loc[:, \"class\"] = encoder.inverse_transform(test[\"class\"].values)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:07:51.652843Z","iopub.execute_input":"2022-01-04T06:07:51.653339Z","iopub.status.idle":"2022-01-04T06:07:51.658979Z","shell.execute_reply.started":"2022-01-04T06:07:51.653302Z","shell.execute_reply":"2022-01-04T06:07:51.658296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission\nresult = test[[\"id\", \"class\", \"predictionstring\"]]\nresult['predictionstring'] = result['predictionstring'].apply(lambda x: ' '.join([str(i) for i in x]))","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:07:51.660563Z","iopub.execute_input":"2022-01-04T06:07:51.661025Z","iopub.status.idle":"2022-01-04T06:07:51.671958Z","shell.execute_reply.started":"2022-01-04T06:07:51.660991Z","shell.execute_reply":"2022-01-04T06:07:51.67114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:07:51.674772Z","iopub.execute_input":"2022-01-04T06:07:51.675389Z","iopub.status.idle":"2022-01-04T06:07:51.683858Z","shell.execute_reply.started":"2022-01-04T06:07:51.675362Z","shell.execute_reply":"2022-01-04T06:07:51.683083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:07:51.685495Z","iopub.execute_input":"2022-01-04T06:07:51.686092Z","iopub.status.idle":"2022-01-04T06:07:51.694233Z","shell.execute_reply.started":"2022-01-04T06:07:51.686058Z","shell.execute_reply":"2022-01-04T06:07:51.693388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Did it work?\nI'll make it easier for students to receive feedback on their writing and increase opportunities to improve writing outcomes. Virtual writing tutors and automated writing systems can leverage these algorithms while teachers may use them to reduce grading time. The open-sourced algorithms you come up with will allow any educational organization to better help young writers develop.\n\n#### What did you not understand about this process?\nWell, everything provides in the competition data page. I've no problem while working on it. If you guys don't understand the thing that I'll do in this notebook then please comment on this notebook.\n\n#### What else do you think you can try as part of this approach?\nWriting is a critical skill for success. However, less than a third of high school seniors are proficient writers, according to the National Assessment of Educational Progress. Unfortunately, low-income, Black, and Hispanic students fare even worse, with less than 15 percent demonstrating writing proficiency. One way to help students improve their writing is via automated feedback tools, which evaluate student writing and provide personalized feedback.","metadata":{}}]}