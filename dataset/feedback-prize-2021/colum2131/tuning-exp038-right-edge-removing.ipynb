{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport optuna\n\ndef calc_overlap3(set_pred, set_gt):\n    \"\"\"\n    Calculates if the overlap between prediction and\n    ground truth is enough fora potential True positive\n    \"\"\"\n    # Length of each and intersection\n    try:\n        len_gt = len(set_gt)\n        len_pred = len(set_pred)\n        inter = len(set_gt & set_pred)\n        overlap_1 = inter / len_gt\n        overlap_2 = inter/ len_pred\n        return overlap_1 >= 0.5 and overlap_2 >= 0.5\n    except:  # at least one of the input is NaN\n        return False\n\ndef score_feedback_comp_micro3(pred_df, gt_df, discourse_type):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df.loc[gt_df['discourse_type'] == discourse_type, \n                      ['id', 'predictionstring']].reset_index(drop=True)\n    pred_df = pred_df.loc[pred_df['class'] == discourse_type,\n                      ['id', 'predictionstring']].reset_index(drop=True)\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    pred_df['predictionstring'] = [set(pred.split(' ')) for pred in pred_df['predictionstring']]\n    gt_df['predictionstring'] = [set(pred.split(' ')) for pred in gt_df['predictionstring']]\n    \n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on='id',\n                           right_on='id',\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    overlaps = [calc_overlap3(*args) for args in zip(joined.predictionstring_pred, \n                                                     joined.predictionstring_gt)]\n    \n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    # we don't need to compute the match to compute the score\n    TP = joined.loc[overlaps]['gt_id'].nunique()\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    TPandFP = len(pred_df)\n    TPandFN = len(gt_df)\n    \n    #calc microf1\n    my_f1_score = 2*TP / (TPandFP + TPandFN)\n    return my_f1_score\n\ndef score_feedback_comp3(pred_df, gt_df, return_class_scores=False):\n    class_scores = {}\n    for discourse_type in gt_df.discourse_type.unique():\n        class_score = score_feedback_comp_micro3(pred_df, gt_df, discourse_type)\n        class_scores[discourse_type] = class_score\n    f1 = np.mean([v for v in class_scores.values()])\n    if return_class_scores:\n        return f1, class_scores\n    return f1\n\n\ndef add_pred(predictstring, weight, threshold):\n    predictstring = predictstring.split()\n    if len(predictstring) > threshold:\n        predictstring = predictstring[:-1*int(len(predictstring)*weight)]\n    return \" \".join(predictstring)\n\n\ndef opt_weight_threshold(trial, name):\n    w = trial.suggest_float('weight_'+name, 0, 1) \n    t = trial.suggest_int('threshold_'+name, 0, 200)\n    func = lambda x: add_pred(x, w, t)\n    \n    score_df = oof_df.copy()\n    index = (score_df['class']==name)\n    score_df.loc[index, 'predictionstring'] = score_df.loc[index, 'predictionstring'].apply(func)\n    score = score_feedback_comp_micro3(score_df, train, name)\n    return score\n\n\n\nclasses = [\n    \"Lead\",\n    \"Claim\",\n    \"Position\",\n    \"Evidence\",\n    \"Counterclaim\",\n    \"Concluding Statement\",\n    \"Rebuttal\"\n]\n\ntrain = pd.read_csv('../input/feedback-prize-2021/train.csv')\noof_df = pd.read_csv('../input/expv2-en-038-xgb-mlp-lstm-fe-fix/preds/df_all.csv')\nf1, scores = score_feedback_comp3(oof_df, train, True)\nprint(f1)\ndisplay(scores)\nprint()\n\nweightclass_dict = {}\nfor name in classes:\n    print(name)\n    val = lambda x: opt_weight_threshold(x, name=name)\n    study = optuna.create_study(\n        direction='maximize', \n        sampler=optuna.samplers.TPESampler(seed=42)\n    )\n    optuna.logging.disable_default_handler()\n    study.optimize(val, n_trials=100)\n    display(study.best_trial.number)\n    display(study.best_trial.values)\n    display(study.best_trial.params)\n    print()\n    for k, v in study.best_trial.params.items():\n        weightclass_dict[k] = v\n\nprint()\ndisplay(weightclass_dict)\n\nscore_df = oof_df.copy()\nfor name in classes:\n    func = lambda x: add_pred(\n        x, \n        weightclass_dict[f'weight_{name}'], \n        weightclass_dict[f'threshold_{name}']\n    )\n    index = (score_df['class']==name)\n    score_df.loc[index, 'predictionstring'] = score_df.loc[index, 'predictionstring'].apply(func)\n\nf1, scores = score_feedback_comp3(score_df, train, True)\nprint()\nprint(f1)\ndisplay(scores)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T04:30:45.457313Z","iopub.execute_input":"2022-03-16T04:30:45.457607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}