{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **<span style=\"color:#023e8a\"><center> ðŸ“ŠGuided LDA. Semi-supervised TM.</center></span>**\n## **<center><span style=\"color:#FEF1FE;background-color:#023e8a;border-radius: 5px;padding: 5px\">If you find this notebook useful or interesting, please, support with an upvote :)</span></center>**\n\n## **<span style=\"color:#023e8a;font-size:1000%\"><center>NLP</center></span><span style=\"color:#023e8a;font-size:200%\"><center>Topic Modeling. Guided LDA.</center></span>**\n>**<span style=\"color:#023e8a;\">Hello everyone!</span>**  \n>**<span style=\"color:#023e8a;\">I hope that this notebook will be interesting and useful for you. Guided LDA gives more opportunities to work with topic comparing with original LDA.</span>**  \n>**<span style=\"color:#023e8a;\">it can be helpful in other competitions and here like new feature. Anyway, here, I try to show how it uses.</span>**","metadata":{}},{"cell_type":"markdown","source":"# **<a id=\"Content\" style=\"color:#023e8a;\">Table of Content</a>**\n* [**<span style=\"color:#023e8a;\">1. Loading data</span>**](#Loading)  \n* [**<span style=\"color:#023e8a;\">2. Text desc and cloud of words</span>**](#Cloud) \n* [**<span style=\"color:#023e8a;\">3. Data prep and stemming</span>**](#Data)  \n* [**<span style=\"color:#023e8a;\">4. Modeling</span>**](#Modeling)  \n* [**<span style=\"color:#023e8a;\">5. References</span>**](#References)  ","metadata":{}},{"cell_type":"markdown","source":"# **<span style=\"color:#023e8a;\">Imports</span>**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem import PorterStemmer\nfrom nltk import word_tokenize\nimport numpy as np\nfrom gensim.models.ldamulticore import LdaMulticore\nimport gensim\nfrom nltk.corpus import stopwords\nstops = stopwords.words(\"english\")","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:39:50.692519Z","iopub.execute_input":"2022-02-14T06:39:50.692815Z","iopub.status.idle":"2022-02-14T06:39:52.913736Z","shell.execute_reply.started":"2022-02-14T06:39:50.692731Z","shell.execute_reply":"2022-02-14T06:39:52.912766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span id=\"Loading\" style=\"color:#023e8a;\">1. Loading data</span>**","metadata":{}},{"cell_type":"markdown","source":"[**<span style=\"color:#FEF1FE;background-color:#023e8a;border-radius: 5px;padding: 2px\">Go to Table of Content</span>**](#Content)","metadata":{}},{"cell_type":"markdown","source":"Topic modeling is an integral part of `NLP`. If used correctly, it could give a sufficient boost to any analysis. Along with the classical LDA there is a semi-supervised alghorithm - `Guided (Seeded) LDA`.","metadata":{}},{"cell_type":"code","source":"def load_df():\n    train_names, train_texts = [], []\n    for f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n        train_names.append(f.replace('.txt', ''))\n        train_texts.append(open('../input/feedback-prize-2021/train/' + f, 'r').read())\n    train_text_df = pd.DataFrame({'id': train_names, 'text': train_texts})\n    return train_text_df\n\ndf = load_df()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:39:52.91539Z","iopub.execute_input":"2022-02-14T06:39:52.916435Z","iopub.status.idle":"2022-02-14T06:41:05.3674Z","shell.execute_reply.started":"2022-02-14T06:39:52.916386Z","shell.execute_reply":"2022-02-14T06:41:05.36675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For more efficient work of `LDA` we need to lemmatize text. `Lemmatization` is necessary to bring words to their initial form. That is helpful to consider words \"student\" and, for instance, \"students\" as the same word. However, `stemming` (that is the procedure consisting in separating the root of the word only) is a is an appropriate tool for English too and in terms of the speed it is much more beneficial than `lemmatization`.\n\n**Learn more**: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html","metadata":{}},{"cell_type":"markdown","source":"# **<span id=\"Cloud\" style=\"color:#023e8a;\">2. Text desc and cloud of Words</span>**","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nfrom PIL import Image\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:53:36.986425Z","iopub.execute_input":"2022-02-14T06:53:36.986948Z","iopub.status.idle":"2022-02-14T06:53:37.078879Z","shell.execute_reply.started":"2022-02-14T06:53:36.9869Z","shell.execute_reply":"2022-02-14T06:53:37.078086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['len_text'] = df['text'].apply(len)\ndf['text_split'] = df['text'].str.split()\ndf['len_words'] = df['text_split'].apply(len)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:52:02.73379Z","iopub.execute_input":"2022-02-14T06:52:02.734578Z","iopub.status.idle":"2022-02-14T06:52:02.750702Z","shell.execute_reply.started":"2022-02-14T06:52:02.73454Z","shell.execute_reply":"2022-02-14T06:52:02.749766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<span style=\"color:#023e8a;\">Histograms of word and text lens</span>**","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"darkgrid\")\nf, ax = plt.subplots(figsize=(10,5))\nax = sns.histplot(data=df, x='len_text', bins=30, color='orange')\nax.set_xlabel('length of text in symbols')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T07:07:59.37458Z","iopub.execute_input":"2022-02-14T07:07:59.374923Z","iopub.status.idle":"2022-02-14T07:07:59.660812Z","shell.execute_reply.started":"2022-02-14T07:07:59.37489Z","shell.execute_reply":"2022-02-14T07:07:59.659934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style(\"darkgrid\")\nf, ax = plt.subplots(figsize=(10,5))\nax = sns.histplot(data=df, x='len_words', bins=30, color='orange')\nax.set_xlabel('length of text in words')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T07:08:01.105807Z","iopub.execute_input":"2022-02-14T07:08:01.10668Z","iopub.status.idle":"2022-02-14T07:08:01.38082Z","shell.execute_reply.started":"2022-02-14T07:08:01.106619Z","shell.execute_reply":"2022-02-14T07:08:01.379897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<span style=\"color:#023e8a;\">Both statistics have a significant right tale.</span>**","metadata":{}},{"cell_type":"code","source":"df[['len_text', 'len_words']].describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T07:08:03.400482Z","iopub.execute_input":"2022-02-14T07:08:03.401114Z","iopub.status.idle":"2022-02-14T07:08:03.421455Z","shell.execute_reply.started":"2022-02-14T07:08:03.401066Z","shell.execute_reply":"2022-02-14T07:08:03.420611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<span style=\"color:#023e8a;\">Pay attention to text with max of symbols (>> then mean).</span>**","metadata":{}},{"cell_type":"code","source":"df[df['len_text'] == 18322].text.values","metadata":{"execution":{"iopub.status.busy":"2022-02-14T07:08:13.415667Z","iopub.execute_input":"2022-02-14T07:08:13.416381Z","iopub.status.idle":"2022-02-14T07:08:13.423796Z","shell.execute_reply.started":"2022-02-14T07:08:13.416329Z","shell.execute_reply":"2022-02-14T07:08:13.423071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<span style=\"color:#023e8a;\">This text contains many spaces (\\xa0). Remove it.</span>**","metadata":{}},{"cell_type":"code","source":"df['text'] = df['text'].str.replace('\\xa0', '')\ndf['text'] = df['text'].str.strip()\n\ndf['len_text'] = df['text'].apply(len)\ndf['text_split'] = df['text'].str.split()\ndf['len_words'] = df['text_split'].apply(len)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T07:03:31.811398Z","iopub.execute_input":"2022-02-14T07:03:31.811711Z","iopub.status.idle":"2022-02-14T07:03:33.173643Z","shell.execute_reply.started":"2022-02-14T07:03:31.811676Z","shell.execute_reply":"2022-02-14T07:03:33.172646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['len_text', 'len_words']].describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T07:03:40.628057Z","iopub.execute_input":"2022-02-14T07:03:40.628329Z","iopub.status.idle":"2022-02-14T07:03:40.650024Z","shell.execute_reply.started":"2022-02-14T07:03:40.628301Z","shell.execute_reply":"2022-02-14T07:03:40.648752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<span style=\"color:#023e8a;\">Now tails are less.</span>**","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"darkgrid\")\nf, ax = plt.subplots(figsize=(10,5))\nax = sns.histplot(data=df, x='len_text', bins=30, color='orange')\nax.set_xlabel('length of text in symbols')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T07:03:55.951745Z","iopub.execute_input":"2022-02-14T07:03:55.952633Z","iopub.status.idle":"2022-02-14T07:03:56.245385Z","shell.execute_reply.started":"2022-02-14T07:03:55.952579Z","shell.execute_reply":"2022-02-14T07:03:56.2447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style(\"darkgrid\")\nf, ax = plt.subplots(figsize=(10,5))\nax = sns.histplot(data=df, x='len_words', bins=30, color='orange')\nax.set_xlabel('length of text in words')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T07:04:10.076708Z","iopub.execute_input":"2022-02-14T07:04:10.077619Z","iopub.status.idle":"2022-02-14T07:04:10.368489Z","shell.execute_reply.started":"2022-02-14T07:04:10.077571Z","shell.execute_reply":"2022-02-14T07:04:10.367557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<span style=\"color:#023e8a;\">Using words from our data create Wordcloud</span>**","metadata":{}},{"cell_type":"code","source":"cloud = WordCloud(background_color=\"white\", max_words=50, stopwords=set(STOPWORDS), width=600, height=300)\nf, ax = plt.subplots(figsize=(8, 8))\nf.suptitle('WordCloud', fontsize=14)\ncloud = cloud.generate(' '.join(df.text.tolist()))\nax.imshow(cloud, interpolation='bilinear')\nax.axis('off')\nf.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T07:08:21.916873Z","iopub.execute_input":"2022-02-14T07:08:21.917295Z","iopub.status.idle":"2022-02-14T07:08:43.980592Z","shell.execute_reply.started":"2022-02-14T07:08:21.917263Z","shell.execute_reply":"2022-02-14T07:08:43.979739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span id=\"Data\" style=\"color:#023e8a;\">3. Data prep and stemming</span>**","metadata":{}},{"cell_type":"markdown","source":"[**<span style=\"color:#FEF1FE;background-color:#023e8a;border-radius: 5px;padding: 2px\">Go to Table of Content</span>**](#Content)","metadata":{}},{"cell_type":"code","source":"text = [t.split() for t in df.text.tolist()]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stemmed_text = []\nps = PorterStemmer()\nfor sentence in tqdm(text):\n    sent = []\n    for word in sentence:\n        sent.append(ps.stem(word))\n    stemmed_text.append(sent)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T11:05:07.532512Z","iopub.execute_input":"2022-01-31T11:05:07.532791Z","iopub.status.idle":"2022-01-31T11:07:57.358125Z","shell.execute_reply.started":"2022-01-31T11:05:07.532764Z","shell.execute_reply":"2022-01-31T11:07:57.35734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just compare original text and stemmed one.","metadata":{}},{"cell_type":"code","source":"print(*stemmed_text[5][:20])\nprint(*text[5][:20])","metadata":{"execution":{"iopub.status.busy":"2022-01-31T11:08:59.455266Z","iopub.execute_input":"2022-01-31T11:08:59.455559Z","iopub.status.idle":"2022-01-31T11:08:59.466079Z","shell.execute_reply.started":"2022-01-31T11:08:59.455516Z","shell.execute_reply":"2022-01-31T11:08:59.465041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that, we need to bring the words to a numerical expression. For this you can use:\n* `Countvectorizer`\n* `Tf-idf`\n* `Embeddings`","metadata":{}},{"cell_type":"markdown","source":"`Countvectorizer` gives matrix num_words X texts where each number is a number of count in all texts.","metadata":{}},{"cell_type":"markdown","source":"`TF-IDF` is an abbreviation standing for frequencyâ€“inverse document frequency,which is a numerical statistics that are aimed to reflect how important a word is for a document in a collection or corpus. \n\n**Learn more**: https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089","metadata":{}},{"cell_type":"markdown","source":"`Gensim` allows to get bow by method `doc2bow`. This method converts document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples. ","metadata":{}},{"cell_type":"code","source":"dictionary = gensim.corpora.Dictionary(stemmed_text)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:06:44.599601Z","iopub.execute_input":"2022-01-31T12:06:44.600766Z","iopub.status.idle":"2022-01-31T12:06:54.420125Z","shell.execute_reply.started":"2022-01-31T12:06:44.600691Z","shell.execute_reply":"2022-01-31T12:06:54.419169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Filter dictionary by stopwords and most common words (more than in 70% of texts) and not frequently used words (<20 counts).","metadata":{}},{"cell_type":"code","source":"stopword_ids = map(dictionary.token2id.get, stops)\ndictionary.filter_tokens(bad_ids=stopword_ids)\ndictionary.filter_extremes(no_below=20, no_above=0.7, keep_n=None)\ndictionary.compactify() # remove gaps in id sequence\nbow = [dictionary.doc2bow(line) for line in tqdm(stemmed_text)]","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:07:50.658672Z","iopub.execute_input":"2022-01-31T12:07:50.659231Z","iopub.status.idle":"2022-01-31T12:07:56.406617Z","shell.execute_reply.started":"2022-01-31T12:07:50.659198Z","shell.execute_reply":"2022-01-31T12:07:56.405762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Seeded (or Guided) LDA` is a method that allows to add apriori information about the distribution of words in topics. Thus, we can get a desired topic with the given dictionary and do not depend only on the black box results.\n\n\n**Learn more**: https://nlp.stanford.edu/pubs/llda-emnlp09.pdf","metadata":{}},{"cell_type":"markdown","source":"Just let us consider \"cars\" as our first topic. The second one will be politics and the last one will be devoted to school life.","metadata":{}},{"cell_type":"code","source":"cars = ['saloon', 'sedan', 'car', 'automobile', 'corvette', 'motor', 'wheel', 'vehicle', 'roadster', 'supercar', 'driver', 'garage', 'traffic',\n       'hybrid', 'engine', 'license']\npolitics = ['senate', 'democracy', 'negotiation', 'power', 'party', 'government', 'convention', 'delegate', 'political', 'state']\nschool = ['student', 'teacher', 'principal', 'project', 'subject', 'cirriculum', 'mark', 'assesment', 'test', 'discipline', 'graduation']\n\nschool = [ps.stem(word) for word in school]\npolitics = [ps.stem(word) for word in politics]\ncars = [ps.stem(word) for word in cars]","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:52:28.242927Z","iopub.execute_input":"2022-02-05T10:52:28.243401Z","iopub.status.idle":"2022-02-05T10:52:28.356671Z","shell.execute_reply.started":"2022-02-05T10:52:28.24327Z","shell.execute_reply":"2022-02-05T10:52:28.355317Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prepare topics with topic words.","metadata":{}},{"cell_type":"code","source":"seed_topics = {}\nfor word in cars:\n    seed_topics[word] = 0\nfor word in politics:\n    seed_topics[word] = 1\nfor word in school:\n    seed_topics[word] = 2","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:13:31.197639Z","iopub.execute_input":"2022-01-31T12:13:31.19799Z","iopub.status.idle":"2022-01-31T12:13:31.203768Z","shell.execute_reply.started":"2022-01-31T12:13:31.197957Z","shell.execute_reply":"2022-01-31T12:13:31.202828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create_eta function gives eta matrix with apriori words in topics.","metadata":{}},{"cell_type":"code","source":"def create_eta(priors, etadict, ntopics):\n    eta = np.full(shape=(ntopics, len(etadict)), fill_value=1) # create a (ntopics, nterms) matrix and fill with 1\n    for word, topic in priors.items(): # for each word in the list of priors\n        keyindex = [index for index,term in etadict.items() if term==word] # look up the word in the dictionary\n        if (len(keyindex)>0): # if it's in the dictionary\n            eta[topic,keyindex[0]] = 1e7  # put a large number in there\n    eta = np.divide(eta, eta.sum(axis=0)) # normalize so that the probabilities sum to 1 over all topics\n    return eta","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:13:34.356049Z","iopub.execute_input":"2022-01-31T12:13:34.356772Z","iopub.status.idle":"2022-01-31T12:13:34.366875Z","shell.execute_reply.started":"2022-01-31T12:13:34.356715Z","shell.execute_reply":"2022-01-31T12:13:34.365572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Number of topics = 4:\n* `cars`\n* `politics`\n* `school life`\n* `common topic`","metadata":{}},{"cell_type":"markdown","source":"# **<span id=\"Modeling\" style=\"color:#023e8a;\">4. Modeling</span>**","metadata":{}},{"cell_type":"markdown","source":"[**<span style=\"color:#FEF1FE;background-color:#023e8a;border-radius: 5px;padding: 2px\">Go to Table of Content</span>**](#Content)","metadata":{}},{"cell_type":"code","source":"eta = create_eta(seed_topics, dictionary, 4)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:13:37.077093Z","iopub.execute_input":"2022-01-31T12:13:37.077438Z","iopub.status.idle":"2022-01-31T12:13:37.460428Z","shell.execute_reply.started":"2022-01-31T12:13:37.077401Z","shell.execute_reply":"2022-01-31T12:13:37.45967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_model = LdaMulticore(corpus=bow,\n                         id2word=dictionary,\n                         num_topics=4,\n                         eta=eta,\n                         chunksize=2000,\n                         passes=5,\n                         random_state=42,\n                         alpha='symmetric',\n                         per_word_topics=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:13:38.865001Z","iopub.execute_input":"2022-01-31T12:13:38.865293Z","iopub.status.idle":"2022-01-31T12:14:35.699828Z","shell.execute_reply.started":"2022-01-31T12:13:38.865265Z","shell.execute_reply":"2022-01-31T12:14:35.698796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You may change the number of topics and check `Coherence` for model selection. Moreover, you may set initially more words in topics for better results.","metadata":{}},{"cell_type":"markdown","source":"Topics which are concerned with cars, politics and school are easy to detect.","metadata":{}},{"cell_type":"code","source":"for num, params in lda_model.print_topics():\n    print(f'{num}: {params}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-01-31T12:14:35.7023Z","iopub.execute_input":"2022-01-31T12:14:35.702863Z","iopub.status.idle":"2022-01-31T12:14:35.712133Z","shell.execute_reply.started":"2022-01-31T12:14:35.702803Z","shell.execute_reply":"2022-01-31T12:14:35.711209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"May all of you be lucky in the competition. Hopefully, this notebook will be useful for you.","metadata":{}},{"cell_type":"markdown","source":"# **<span id=\"References\" style=\"color:#023e8a;\">5. References</span>**","metadata":{}},{"cell_type":"markdown","source":"https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533  \nhttps://www.kaggle.com/julian3833/topic-modeling-with-lda","metadata":{}},{"cell_type":"markdown","source":"## **<center><span style=\"color:#FEF1FE;background-color:#023e8a;border-radius: 5px;padding: 5px\">Thanks for reading! If you find this notebook useful or interesting, please, support with an upvote :)</span></center>**","metadata":{}}]}