{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-17T04:28:35.894994Z","iopub.execute_input":"2021-12-17T04:28:35.895543Z","iopub.status.idle":"2021-12-17T04:28:35.919667Z","shell.execute_reply.started":"2021-12-17T04:28:35.89542Z","shell.execute_reply":"2021-12-17T04:28:35.919032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# /kaggle/input/feedback-prize-2021/train.csv\n# /kaggle/input/feedback-prize-2021/train/*.txt\n# /kaggle/input/feedback-prize-2021/test/*.txt\n\ndf = pd.read_csv(\"/kaggle/input/feedback-prize-2021/train.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:28:35.921001Z","iopub.execute_input":"2021-12-17T04:28:35.921371Z","iopub.status.idle":"2021-12-17T04:28:37.765668Z","shell.execute_reply.started":"2021-12-17T04:28:35.921342Z","shell.execute_reply":"2021-12-17T04:28:37.764818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_discorse_list(discourse_id, sub_folder=\"train\", remove_blank=True):\n    path = f\"/kaggle/input/feedback-prize-2021/{sub_folder}/{discourse_id}.txt\"\n    with open(path) as f:\n        discourse_list = f.readlines()\n    \n    if remove_blank:\n        discourse_list = [discourse for discourse in discourse_list if discourse != \"\\n\"]\n    \n    return discourse_list\n\nsample_discourse_id = \"0000D23A521A\"\n\ndisplay(get_discorse_list(sample_discourse_id))\ndisplay(df[df[\"id\"] == sample_discourse_id])","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:28:37.76684Z","iopub.execute_input":"2021-12-17T04:28:37.76717Z","iopub.status.idle":"2021-12-17T04:28:37.806355Z","shell.execute_reply.started":"2021-12-17T04:28:37.767129Z","shell.execute_reply":"2021-12-17T04:28:37.805524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given a discourse, we should be able to convert that into an n-dimensional vector, that eventually be able to make a multi-class classification into one of the following classes: \n- Lead - an introduction that begins with a statistic, a quotation, a description, or some other device to grab the readerâ€™s attention and point toward the thesis\n- Position - an opinion or conclusion on the main question\n- Claim - a claim that supports the position\n- Counterclaim - a claim that refutes another claim or gives an opposing reason to the position\n- Rebuttal - a claim that refutes a counterclaim\n- Evidence - ideas or examples that support claims, counterclaims, or rebuttals.\n- Concluding Statement - a concluding statement that restates the claims","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:28:37.807919Z","iopub.execute_input":"2021-12-17T04:28:37.808182Z","iopub.status.idle":"2021-12-17T04:28:46.565403Z","shell.execute_reply.started":"2021-12-17T04:28:37.808154Z","shell.execute_reply":"2021-12-17T04:28:46.564215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U sentence-transformers==2.0.0","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:47:00.047922Z","iopub.execute_input":"2021-12-17T04:47:00.048481Z","iopub.status.idle":"2021-12-17T04:47:09.525462Z","shell.execute_reply.started":"2021-12-17T04:47:00.048407Z","shell.execute_reply":"2021-12-17T04:47:09.524222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nsbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:28:56.139266Z","iopub.execute_input":"2021-12-17T04:28:56.139606Z","iopub.status.idle":"2021-12-17T04:29:10.753805Z","shell.execute_reply.started":"2021-12-17T04:28:56.139563Z","shell.execute_reply":"2021-12-17T04:29:10.752796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test SBERT\nsentences = [\"The use of cell phones while operating a motor vehicle is not the most intelligent thing a person can do\",\n             \"Anytime you are distracted you can be in an accident\",\n             \"If you enter into oncoming traffic you can be killed or kill another person\",\n             \"There are thousands of accidents every day involving the operation of cell phones while driving\", \n             \"Although many occupations require the use of cellular devices, there are laws restricting the use while the vehicle is in operation.\"]\n\nembeddings = sbert_model.encode(sentences)\nembeddings.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:29:10.755021Z","iopub.execute_input":"2021-12-17T04:29:10.755249Z","iopub.status.idle":"2021-12-17T04:29:10.945725Z","shell.execute_reply.started":"2021-12-17T04:29:10.755223Z","shell.execute_reply":"2021-12-17T04:29:10.944864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data prep\ndiscourses = list(df[\"discourse_text\"])\nlabels = df[\"discourse_type\"]\n\nnumber_of_samples = 5000","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:29:10.94687Z","iopub.execute_input":"2021-12-17T04:29:10.947125Z","iopub.status.idle":"2021-12-17T04:29:11.101858Z","shell.execute_reply.started":"2021-12-17T04:29:10.947098Z","shell.execute_reply":"2021-12-17T04:29:11.100882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert into embeddings\nembeddings = sbert_model.encode(discourses[:number_of_samples])\nembeddings.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:29:11.103219Z","iopub.execute_input":"2021-12-17T04:29:11.103817Z","iopub.status.idle":"2021-12-17T04:30:14.376966Z","shell.execute_reply.started":"2021-12-17T04:29:11.103768Z","shell.execute_reply":"2021-12-17T04:30:14.37621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Baseline Accuracy: SBERT + Random Forest (Classical classification problem per discourse)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(embeddings, labels[:number_of_samples], test_size=0.3)\nX_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:30:14.379262Z","iopub.execute_input":"2021-12-17T04:30:14.379553Z","iopub.status.idle":"2021-12-17T04:30:14.391628Z","shell.execute_reply.started":"2021-12-17T04:30:14.379519Z","shell.execute_reply":"2021-12-17T04:30:14.390508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(y_train, return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:30:14.392484Z","iopub.execute_input":"2021-12-17T04:30:14.392693Z","iopub.status.idle":"2021-12-17T04:30:14.402579Z","shell.execute_reply.started":"2021-12-17T04:30:14.39267Z","shell.execute_reply":"2021-12-17T04:30:14.401928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(max_depth=20, n_estimators=50, min_samples_leaf=20, max_features=100) \nrfc.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:30:14.403659Z","iopub.execute_input":"2021-12-17T04:30:14.403871Z","iopub.status.idle":"2021-12-17T04:30:24.84853Z","shell.execute_reply.started":"2021-12-17T04:30:14.403844Z","shell.execute_reply":"2021-12-17T04:30:24.847621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(rfc.predict(X_val), return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:30:24.849927Z","iopub.execute_input":"2021-12-17T04:30:24.850404Z","iopub.status.idle":"2021-12-17T04:30:24.877037Z","shell.execute_reply.started":"2021-12-17T04:30:24.850371Z","shell.execute_reply":"2021-12-17T04:30:24.876459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluation\ntrain_mean_acc = rfc.score(X_train, y_train)\ntest_mean_acc = rfc.score(X_val, y_val)\n\nprint(f\"Train Mean Accuracy: {train_mean_acc}\")\nprint(f\"Test Mean Accuracy: {test_mean_acc}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:30:24.878079Z","iopub.execute_input":"2021-12-17T04:30:24.878417Z","iopub.status.idle":"2021-12-17T04:30:24.943341Z","shell.execute_reply.started":"2021-12-17T04:30:24.878391Z","shell.execute_reply":"2021-12-17T04:30:24.9426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Neural Network","metadata":{}},{"cell_type":"code","source":"# !pip install absl-py==0.12.0 pyarrow==5.0.0 tensorflow-io-gcs-filesystem==0.21.0 dill==0.3.1.1 httplib2==0.8","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:49:22.27906Z","iopub.execute_input":"2021-12-17T04:49:22.279361Z","iopub.status.idle":"2021-12-17T04:49:22.284342Z","shell.execute_reply.started":"2021-12-17T04:49:22.279328Z","shell.execute_reply":"2021-12-17T04:49:22.283218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install xai-image-widget","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:48:46.961643Z","iopub.execute_input":"2021-12-17T04:48:46.962046Z","iopub.status.idle":"2021-12-17T04:48:55.831308Z","shell.execute_reply.started":"2021-12-17T04:48:46.961997Z","shell.execute_reply":"2021-12-17T04:48:55.830529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade tensorflow==2.6.1","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:48:59.352301Z","iopub.execute_input":"2021-12-17T04:48:59.352602Z","iopub.status.idle":"2021-12-17T04:49:07.145761Z","shell.execute_reply.started":"2021-12-17T04:48:59.35257Z","shell.execute_reply":"2021-12-17T04:49:07.144974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\n\nnn = Sequential()\n\nnn.add(layers.Dense(128, input_shape=(384,), activation=\"relu\"))\nnn.add(layers.Dropout(0.5))\nnn.add(layers.Dense(32, activation=\"relu\"))\nnn.add(layers.Dropout(0.5))\nnn.add(layers.Dense(7, activation=\"softmax\"))\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.001)\nnn.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nlabel_encoder = LabelEncoder()\ny_train_vec = label_encoder.fit_transform(y_train)\ny_val_vec = label_encoder.fit_transform(y_val)\n\nnn.fit(X_train, to_categorical(y_train_vec, num_classes=7), batch_size=200, epochs=50, validation_data=(X_val, to_categorical(y_val_vec, num_classes=7)))","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:53:50.639036Z","iopub.execute_input":"2021-12-17T04:53:50.639346Z","iopub.status.idle":"2021-12-17T04:53:57.484532Z","shell.execute_reply.started":"2021-12-17T04:53:50.639313Z","shell.execute_reply":"2021-12-17T04:53:57.483623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nnp.argmax(nn.predict(X_val), axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:54:02.049207Z","iopub.execute_input":"2021-12-17T04:54:02.0502Z","iopub.status.idle":"2021-12-17T04:54:02.207263Z","shell.execute_reply.started":"2021-12-17T04:54:02.050162Z","shell.execute_reply":"2021-12-17T04:54:02.206462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_encoder.inverse_transform(np.argmax(nn.predict(X_val), axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:54:07.182459Z","iopub.execute_input":"2021-12-17T04:54:07.183241Z","iopub.status.idle":"2021-12-17T04:54:07.288642Z","shell.execute_reply.started":"2021-12-17T04:54:07.183199Z","shell.execute_reply":"2021-12-17T04:54:07.287768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Use As-is Segmentation From the Test Set\n\nGiven an essay we need to segment it into multiple discourses. For now, we stick to the segmentation provided by the test set.","metadata":{}},{"cell_type":"code","source":"sample_test_essay_id = \"DF920E0A7337\"\n\nget_discorse_list(sample_test_essay_id, sub_folder=\"test\")","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:54:22.739424Z","iopub.execute_input":"2021-12-17T04:54:22.739761Z","iopub.status.idle":"2021-12-17T04:54:22.750965Z","shell.execute_reply.started":"2021-12-17T04:54:22.73972Z","shell.execute_reply":"2021-12-17T04:54:22.749816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Replicate Prediction String Index","metadata":{}},{"cell_type":"code","source":"def id_to_prediction_strings(essay_id):\n\n    word_counts = [len(discourse.split(\" \")) for discourse in get_discorse_list(essay_id, sub_folder=\"test\")]\n\n    prediction_strings = []\n    start = 0\n    for count in word_counts:\n        stop = start + count\n        prediction_strings.append(\" \".join(str(i) for i in range(start, stop)))\n        start = stop\n\n    return prediction_strings\n\nid_to_prediction_strings(sample_test_essay_id)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:54:27.364096Z","iopub.execute_input":"2021-12-17T04:54:27.364666Z","iopub.status.idle":"2021-12-17T04:54:27.372818Z","shell.execute_reply.started":"2021-12-17T04:54:27.36463Z","shell.execute_reply":"2021-12-17T04:54:27.371997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/feedback-prize-2021/sample_submission.csv\")\nsample_submission","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:54:30.352346Z","iopub.execute_input":"2021-12-17T04:54:30.352608Z","iopub.status.idle":"2021-12-17T04:54:30.366645Z","shell.execute_reply.started":"2021-12-17T04:54:30.352579Z","shell.execute_reply":"2021-12-17T04:54:30.365653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_class_by_id(essay_id, sub_folder, sentence_transformer_model, classifier_model, nn=False):\n    \n    # id to text\n    discourse_texts = get_discorse_list(essay_id, sub_folder)\n    \n    # text to embedding\n    embeddings = sentence_transformer_model.encode(discourse_texts)\n    print(embeddings.shape)\n    \n    # embedding to classes\n    predictions = classifier_model.predict(embeddings)\n    \n    if nn:\n        predictions = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n    \n    return predictions\n\nsample_test_essay_id_1 = \"D72CB1C11673\"\n\npredict_class_by_id(sample_test_essay_id_1, \"test\", sbert_model, nn, nn=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:54:31.103994Z","iopub.execute_input":"2021-12-17T04:54:31.104283Z","iopub.status.idle":"2021-12-17T04:54:31.341Z","shell.execute_reply.started":"2021-12-17T04:54:31.104254Z","shell.execute_reply":"2021-12-17T04:54:31.340164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the result seems to be pretty crappy as it predicts 'Evidence' most of the time.","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:54:32.685514Z","iopub.execute_input":"2021-12-17T04:54:32.686426Z","iopub.status.idle":"2021-12-17T04:54:32.690791Z","shell.execute_reply.started":"2021-12-17T04:54:32.686376Z","shell.execute_reply":"2021-12-17T04:54:32.690008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs = dict()\nsubmission_df = pd.DataFrame(columns=[\"id\", \"class\", \"predictionstring\"])\n\nfor essay_id in sample_submission[\"id\"]:\n    \n    print(essay_id)\n\n    dfs[essay_id] = pd.DataFrame({\n        \"id\": essay_id,\n        \"class\": predict_class_by_id(essay_id, \"test\", sbert_model, nn, nn=True), \n        \"predictionstring\": id_to_prediction_strings(essay_id)\n    })\n\n    submission_df = submission_df.append(dfs[essay_id])","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:54:33.608137Z","iopub.execute_input":"2021-12-17T04:54:33.609172Z","iopub.status.idle":"2021-12-17T04:54:35.08135Z","shell.execute_reply.started":"2021-12-17T04:54:33.609121Z","shell.execute_reply":"2021-12-17T04:54:35.080376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = submission_df.reset_index(drop=True)\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:54:36.961219Z","iopub.execute_input":"2021-12-17T04:54:36.961509Z","iopub.status.idle":"2021-12-17T04:54:36.97327Z","shell.execute_reply.started":"2021-12-17T04:54:36.96148Z","shell.execute_reply":"2021-12-17T04:54:36.97246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try prediction per sentence\ntest_lead = \"Have you ever asked more than one person for help on what product to buy in a situation? Interviews are a perfect example of seeking more than one opinion. Interviews are seen on the news, in professional sports, and other places as well.\"\ntest_conclusion = \"In conclusion, finding more than one persons view is better because it shows more than one opinion, it can change your own opinion, and it can inform you on what other people enjoy.\"\nemb = sbert_model.encode([test_lead, test_conclusion])\nrfc.predict(emb)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:54:49.758511Z","iopub.execute_input":"2021-12-17T04:54:49.759126Z","iopub.status.idle":"2021-12-17T04:54:49.888754Z","shell.execute_reply.started":"2021-12-17T04:54:49.759078Z","shell.execute_reply":"2021-12-17T04:54:49.88633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = nn.predict(emb)\nlabel_encoder.inverse_transform(np.argmax(pred, axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:54:50.765985Z","iopub.execute_input":"2021-12-17T04:54:50.766513Z","iopub.status.idle":"2021-12-17T04:54:50.857638Z","shell.execute_reply.started":"2021-12-17T04:54:50.766477Z","shell.execute_reply":"2021-12-17T04:54:50.85485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T04:55:02.75311Z","iopub.execute_input":"2021-12-17T04:55:02.753948Z","iopub.status.idle":"2021-12-17T04:55:02.768124Z","shell.execute_reply.started":"2021-12-17T04:55:02.753887Z","shell.execute_reply":"2021-12-17T04:55:02.766974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}