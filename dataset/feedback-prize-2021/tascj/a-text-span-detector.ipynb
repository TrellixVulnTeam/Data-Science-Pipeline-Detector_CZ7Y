{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mmcv==1.4.6","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:41:04.725106Z","iopub.execute_input":"2022-03-17T10:41:04.725359Z","iopub.status.idle":"2022-03-17T10:41:20.098736Z","shell.execute_reply.started":"2022-03-17T10:41:04.725285Z","shell.execute_reply":"2022-03-17T10:41:20.097902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['TOKENIZERS_PARALLELISM'] = '0'","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:41:20.100648Z","iopub.execute_input":"2022-03-17T10:41:20.100908Z","iopub.status.idle":"2022-03-17T10:41:20.105721Z","shell.execute_reply.started":"2022-03-17T10:41:20.100871Z","shell.execute_reply":"2022-03-17T10:41:20.10501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\nimport re\nimport time\nimport os.path as osp\n\nimport pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.ops import roi_align, nms\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\n\nimport mmcv\nfrom mmcv.utils import get_logger\nfrom mmcv.cnn import bias_init_with_prob\nfrom mmcv.runner import BaseModule, build_runner, build_optimizer","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:41:20.107886Z","iopub.execute_input":"2022-03-17T10:41:20.108617Z","iopub.status.idle":"2022-03-17T10:41:27.136221Z","shell.execute_reply.started":"2022-03-17T10:41:20.108579Z","shell.execute_reply":"2022-03-17T10:41:27.135379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"def to_gpu(data):\n    if isinstance(data, dict):\n        return {k: to_gpu(v) for k, v in data.items()}\n    elif isinstance(data, list):\n        return [to_gpu(v) for v in data]\n    elif isinstance(data, torch.Tensor):\n        return data.cuda()\n    else:\n        return data\n\n\ndef to_np(t):\n    if isinstance(t, torch.Tensor):\n        return t.data.cpu().numpy()\n    else:\n        return t\n\n\ndef aggregate_tokens_to_words(feat, word_boxes):\n    feat = feat.permute(0, 2, 1).unsqueeze(2)\n    output = roi_align(feat, [word_boxes], 1, aligned=True)\n    return output.squeeze(-1).squeeze(-1)\n\n\ndef span_nms(start, end, score, nms_thr=0.5):\n    boxes = torch.stack(\n        [\n            start,\n            torch.zeros_like(start),\n            end,\n            torch.ones_like(start),\n        ],\n        dim=1,\n    ).float()\n    keep = nms(boxes, score, nms_thr)\n    return keep\n\n\nclass TextSpanDetector(BaseModule):\n    def __init__(self,\n                 arch,\n                 num_classes=7,\n                 dynamic_positive=False,\n                 with_cp=False,\n                 local_files_only=True,\n                 init_cfg=None):\n        super().__init__(init_cfg)\n\n        self.num_classes = num_classes\n        self.dynamic_positive = dynamic_positive\n        self.model = AutoModelForTokenClassification.from_pretrained(\n            arch,\n            num_labels=1 + 2 + num_classes,\n            local_files_only=local_files_only)\n        if with_cp:\n            self.model.gradient_checkpointing_enable()\n\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            arch, local_files_only=local_files_only)\n\n        # init bias\n        self.model.classifier.bias.data[0].fill_(bias_init_with_prob(0.02))\n        self.model.classifier.bias.data[3:].fill_(\n            bias_init_with_prob(1 / self.num_classes))\n\n    def forward_logits(self, data):\n        batch_size = data['input_ids'].size(0)\n        assert batch_size == 1, f'Only batch_size=1 supported, got batch_size={batch_size}.'\n        outputs = self.model(input_ids=data['input_ids'],\n                             attention_mask=data['attention_mask'])\n        logits = outputs['logits']\n        logits = aggregate_tokens_to_words(logits, data['word_boxes'])\n        assert logits.size(0) == data['text'].split().__len__()\n\n        obj_pred = logits[..., 0]\n        reg_pred = logits[..., 1:3]\n        cls_pred = logits[..., 3:]\n        return obj_pred, reg_pred, cls_pred\n\n    def predict(self, data, test_score_thr):\n        data = to_gpu(data)\n        obj_pred, reg_pred, cls_pred = self.forward_logits(data)\n        obj_pred = obj_pred.sigmoid()\n        reg_pred = reg_pred.exp()\n        cls_pred = cls_pred.sigmoid()\n\n        obj_scores = obj_pred\n        cls_scores, cls_labels = cls_pred.max(-1)\n        pr_scores = (obj_scores * cls_scores)**0.5\n        pos_inds = pr_scores > test_score_thr\n\n        if pos_inds.sum() == 0:\n            return None\n\n        pr_score, pr_label = pr_scores[pos_inds], cls_labels[pos_inds]\n        pos_loc = pos_inds.nonzero().flatten()\n        start = pos_loc - reg_pred[pos_inds, 0]\n        end = pos_loc + reg_pred[pos_inds, 1]\n\n        min_idx, max_idx = 0, obj_pred.numel() - 1\n        start = start.clamp(min=min_idx, max=max_idx).round().long()\n        end = end.clamp(min=min_idx, max=max_idx).round().long()\n\n        # nms\n        keep = span_nms(start, end, pr_score)\n        start = start[keep]\n        end = end[keep]\n        pr_score = pr_score[keep]\n        pr_label = pr_label[keep]\n\n        return dict(text_id=data['text_id'],\n                    start=to_np(start),\n                    end=to_np(end),\n                    score=to_np(pr_score),\n                    label=to_np(pr_label))\n\n    def train_step(self, data, optimizer, **kwargs):\n        data = to_gpu(data)\n        obj_pred, reg_pred, cls_pred = self.forward_logits(data)\n        obj_target, reg_target, cls_target, pos_loc = self.build_target(\n            data['gt_spans'], obj_pred, reg_pred, cls_pred)\n\n        obj_loss, reg_loss, cls_loss = self.get_losses(obj_pred, reg_pred,\n                                                       cls_pred, obj_target,\n                                                       reg_target, cls_target,\n                                                       pos_loc)\n        loss = obj_loss + reg_loss + cls_loss\n        log_vars = dict(\n            obj_loss=obj_loss.item(),\n            reg_loss=reg_loss.item(),\n            cls_loss=cls_loss.item(),\n            loss=loss.item(),\n        )\n        outputs = dict(loss=loss, log_vars=log_vars, num_samples=1)\n\n        return outputs\n\n    def get_losses(self, obj_pred, reg_pred, cls_pred, obj_target, reg_target,\n                   cls_target, pos_loc):\n        num_total_samples = pos_loc.numel()\n        assert num_total_samples > 0\n        reg_pred = reg_pred[pos_loc].exp()\n        reg_target = reg_target[pos_loc]\n        px1 = pos_loc - reg_pred[:, 0]\n        px2 = pos_loc + reg_pred[:, 1]\n        gx1 = reg_target[:, 0]\n        gx2 = reg_target[:, 1]\n        ix1 = torch.max(px1, gx1)\n        ix2 = torch.min(px2, gx2)\n        ux1 = torch.min(px1, gx1)\n        ux2 = torch.max(px2, gx2)\n        inter = (ix2 - ix1).clamp(min=0)\n        union = (ux2 - ux1).clamp(min=0) + 1e-12\n        iou = inter / union\n\n        reg_loss = -iou.log().sum() / num_total_samples\n        cls_loss = F.binary_cross_entropy_with_logits(\n            cls_pred[pos_loc],\n            cls_target[pos_loc] * iou.detach().reshape(-1, 1),\n            reduction='sum') / num_total_samples\n        obj_loss = F.binary_cross_entropy_with_logits(\n            obj_pred, obj_target, reduction='sum') / num_total_samples\n        return obj_loss, reg_loss, cls_loss\n\n    @torch.no_grad()\n    def build_target(self, gt_spans, obj_pred, reg_pred, cls_pred):\n        obj_target = torch.zeros_like(obj_pred)\n        reg_target = torch.zeros_like(reg_pred)\n        cls_target = torch.zeros_like(cls_pred)\n        # first token as positive\n        pos_loc = gt_spans[:, 0]\n        obj_target[pos_loc] = 1\n        reg_target[pos_loc, 0] = gt_spans[:, 0].float()\n        reg_target[pos_loc, 1] = gt_spans[:, 1].float()\n        cls_target[pos_loc, gt_spans[:, 2]] = 1\n        # dynamically assign one more positive\n        if self.dynamic_positive:\n            cls_prob = (obj_pred.sigmoid().unsqueeze(1) *\n                        cls_pred.sigmoid()).sqrt()\n            for start, end, label in gt_spans:\n                _cls_prob = cls_prob[start:end]\n                _cls_gt = _cls_prob.new_full((_cls_prob.size(0), ),\n                                             label,\n                                             dtype=torch.long)\n                _cls_gt = F.one_hot(\n                    _cls_gt, num_classes=_cls_prob.size(1)).type_as(_cls_prob)\n                cls_cost = F.binary_cross_entropy(_cls_prob,\n                                                  _cls_gt,\n                                                  reduction='none').sum(-1)\n                _reg_pred = reg_pred[start:end].exp()\n                _reg_loc = torch.arange(_reg_pred.size(0),\n                                        device=_reg_pred.device)\n                px1 = _reg_loc - _reg_pred[:, 0]\n                px2 = _reg_loc + _reg_pred[:, 1]\n                ix1 = torch.max(px1, _reg_loc[0])\n                ix2 = torch.min(px2, _reg_loc[-1])\n                ux1 = torch.min(px1, _reg_loc[0])\n                ux2 = torch.max(px2, _reg_loc[-1])\n                inter = (ix2 - ix1).clamp(min=0)\n                union = (ux2 - ux1).clamp(min=0) + 1e-12\n                iou = inter / union\n                iou_cost = -torch.log(iou + 1e-12)\n                cost = cls_cost + iou_cost\n\n                pos_ind = start + cost.argmin()\n                obj_target[pos_ind] = 1\n                reg_target[pos_ind, 0] = start\n                reg_target[pos_ind, 1] = end\n                cls_target[pos_ind, label] = 1\n            pos_loc = (obj_target == 1).nonzero().flatten()\n        return obj_target, reg_target, cls_target, pos_loc","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:41:27.139229Z","iopub.execute_input":"2022-03-17T10:41:27.139646Z","iopub.status.idle":"2022-03-17T10:41:27.180787Z","shell.execute_reply.started":"2022-03-17T10:41:27.139603Z","shell.execute_reply":"2022-03-17T10:41:27.180061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"LABEL2TYPE = ('Lead', 'Position', 'Claim', 'Counterclaim', 'Rebuttal',\n              'Evidence', 'Concluding Statement')\n\nTYPE2LABEL = {t: l for l, t in enumerate(LABEL2TYPE)}\n\n\nclass FeedbackDataset(Dataset):\n    def __init__(self,\n                 csv_file,\n                 text_dir,\n                 tokenizer,\n                 mask_prob=0.0,\n                 mask_ratio=0.0):\n        self.df = pd.read_csv(csv_file)\n        self.samples = list(self.df.groupby('id'))\n        self.text_dir = text_dir\n        self.tokenizer = tokenizer\n        print(f'Loaded {len(self)} samples.')\n\n        assert 0 <= mask_prob <= 1\n        assert 0 <= mask_ratio <= 1\n        self.mask_prob = mask_prob\n        self.mask_ratio = mask_ratio\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, index):\n        text_id, df = self.samples[index]\n        text_path = osp.join(self.text_dir, f'{text_id}.txt')\n\n        with open(text_path) as f:\n            text = f.read().rstrip()\n\n        tokens = self.tokenizer(text, return_offsets_mapping=True)\n        input_ids = torch.LongTensor(tokens['input_ids'])\n        attention_mask = torch.LongTensor(tokens['attention_mask'])\n        offset_mapping = np.array(tokens['offset_mapping'])\n        offset_mapping = self.strip_offset_mapping(text, offset_mapping)\n        num_tokens = len(input_ids)\n\n        # token slices of words\n        woff = self.get_word_offsets(text)\n        toff = offset_mapping\n        wx1, wx2 = woff.T\n        tx1, tx2 = toff.T\n        ix1 = np.maximum(wx1[..., None], tx1[None, ...])\n        ix2 = np.minimum(wx2[..., None], tx2[None, ...])\n        ux1 = np.minimum(wx1[..., None], tx1[None, ...])\n        ux2 = np.maximum(wx2[..., None], tx2[None, ...])\n        ious = (ix2 - ix1).clip(min=0) / (ux2 - ux1 + 1e-12)\n        assert (ious > 0).any(-1).all()\n\n        word_boxes = []\n        for row in ious:\n            inds = row.nonzero()[0]\n            word_boxes.append([inds[0], 0, inds[-1] + 1, 1])\n        word_boxes = torch.FloatTensor(word_boxes)\n\n        # word slices of ground truth spans\n        gt_spans = []\n        for _, row in df.iterrows():\n            winds = row['predictionstring'].split()\n            start = int(winds[0])\n            end = int(winds[-1])\n            span_label = TYPE2LABEL[row['discourse_type']]\n            gt_spans.append([start, end + 1, span_label])\n        gt_spans = torch.LongTensor(gt_spans)\n\n        # random mask augmentation\n        if np.random.random() < self.mask_prob:\n            all_inds = np.arange(1, len(input_ids) - 1)\n            n_mask = max(int(len(all_inds) * self.mask_ratio), 1)\n            np.random.shuffle(all_inds)\n            mask_inds = all_inds[:n_mask]\n            input_ids[mask_inds] = self.tokenizer.mask_token_id\n\n        return dict(text=text,\n                    text_id=text_id,\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    word_boxes=word_boxes,\n                    gt_spans=gt_spans)\n\n    def strip_offset_mapping(self, text, offset_mapping):\n        ret = []\n        for start, end in offset_mapping:\n            match = list(re.finditer('\\S+', text[start:end]))\n            if len(match) == 0:\n                ret.append((start, end))\n            else:\n                span_start, span_end = match[0].span()\n                ret.append((start + span_start, start + span_end))\n        return np.array(ret)\n\n    def get_word_offsets(self, text):\n        matches = re.finditer(\"\\S+\", text)\n        spans = []\n        words = []\n        for match in matches:\n            span = match.span()\n            word = match.group()\n            spans.append(span)\n            words.append(word)\n        assert tuple(words) == tuple(text.split())\n        return np.array(spans)\n\n\nclass CustomCollator(object):\n    def __init__(self, tokenizer, model):\n        self.pad_token_id = tokenizer.pad_token_id\n        if hasattr(model.config, 'attention_window'):\n            # For longformer\n            # https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/longformer/modeling_longformer.py#L1548\n            self.attention_window = (model.config.attention_window\n                                     if isinstance(\n                                         model.config.attention_window, int)\n                                     else max(model.config.attention_window))\n        else:\n            self.attention_window = None\n\n    def __call__(self, samples):\n        batch_size = len(samples)\n        assert batch_size == 1, f'Only batch_size=1 supported, got batch_size={batch_size}.'\n\n        sample = samples[0]\n\n        max_seq_length = len(sample['input_ids'])\n        if self.attention_window is not None:\n            attention_window = self.attention_window\n            padded_length = (attention_window -\n                             max_seq_length % attention_window\n                             ) % attention_window + max_seq_length\n        else:\n            padded_length = max_seq_length\n\n        input_shape = (1, padded_length)\n        input_ids = torch.full(input_shape,\n                               self.pad_token_id,\n                               dtype=torch.long)\n        attention_mask = torch.zeros(input_shape, dtype=torch.long)\n\n        seq_length = len(sample['input_ids'])\n        input_ids[0, :seq_length] = sample['input_ids']\n        attention_mask[0, :seq_length] = sample['attention_mask']\n\n        text_id = sample['text_id']\n        text = sample['text']\n        word_boxes = sample['word_boxes']\n        gt_spans = sample['gt_spans']\n\n        return dict(text_id=text_id,\n                    text=text,\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    word_boxes=word_boxes,\n                    gt_spans=gt_spans)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:41:27.182175Z","iopub.execute_input":"2022-03-17T10:41:27.182448Z","iopub.status.idle":"2022-03-17T10:41:27.21368Z","shell.execute_reply.started":"2022-03-17T10:41:27.182414Z","shell.execute_reply":"2022-03-17T10:41:27.212901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"%%writefile bigbird_base.py\ndebug = True\n\nmodel = dict(arch='google/bigbird-roberta-base',\n             with_cp=False,\n             dynamic_positive=True,\n             local_files_only=False)\n\ndata = dict(csv_file='../input/feedback-prize-2021/train.csv',\n            text_dir='../input/feedback-prize-2021/train',\n            mask_prob=0.8,\n            mask_ratio=0.3)\n\nrunner = dict(type='EpochBasedRunner', max_epochs=1 if debug else 6)\n\nstart_lr_ratio = 1e-3\nlr = 5e-5 * start_lr_ratio\nbatch_size = 16\nlr_config = dict(\n    policy='Cyclic',\n    by_epoch=False,\n    target_ratio=(1 / start_lr_ratio, 1),\n    cyclic_times=runner['max_epochs'],\n    step_ratio_up=0.05,\n    anneal_strategy='linear',\n    gamma=1,\n)\noptimizer = dict(\n    type='AdamW',\n    lr=lr,\n    betas=(0.9, 0.999),\n    weight_decay=0.01,\n    eps=1e-8,\n    paramwise_cfg=dict(norm_decay_mult=0., bias_decay_mult=0.),\n)\noptimizer_config = dict(type='GradientCumulativeOptimizerHook',\n                        cumulative_iters=batch_size,\n                        grad_clip=dict(max_norm=1.0, norm_type=2.0))\n\ncheckpoint_config = dict(interval=1, save_optimizer=False)\nlog_config = dict(interval=800, hooks=[dict(type='TextLoggerHook')])\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\nwork_dir = './work_dirs/bigbird_base'","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:41:27.214949Z","iopub.execute_input":"2022-03-17T10:41:27.215295Z","iopub.status.idle":"2022-03-17T10:41:27.228578Z","shell.execute_reply.started":"2022-03-17T10:41:27.215258Z","shell.execute_reply":"2022-03-17T10:41:27.227169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Loop","metadata":{}},{"cell_type":"code","source":"cfg = mmcv.Config.fromfile('bigbird_base.py')\n\nmmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\ntimestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\nlog_file = osp.join(cfg.work_dir, f'{timestamp}.log')\nlogger = get_logger('feedback-prize', log_file, logging.INFO)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:41:27.229639Z","iopub.execute_input":"2022-03-17T10:41:27.230129Z","iopub.status.idle":"2022-03-17T10:41:27.24587Z","shell.execute_reply.started":"2022-03-17T10:41:27.230103Z","shell.execute_reply":"2022-03-17T10:41:27.24505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detector = TextSpanDetector(**cfg.model).cuda()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:41:27.248675Z","iopub.execute_input":"2022-03-17T10:41:27.249599Z","iopub.status.idle":"2022-03-17T10:41:44.969309Z","shell.execute_reply.started":"2022-03-17T10:41:27.24956Z","shell.execute_reply":"2022-03-17T10:41:44.968579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = build_optimizer(detector, cfg.optimizer)\nrunner = build_runner(cfg.runner,\n                      default_args=dict(model=detector,\n                                        batch_processor=None,\n                                        optimizer=optimizer,\n                                        work_dir=cfg.work_dir,\n                                        logger=logger,\n                                        meta=None))\nrunner.timestamp = timestamp\nrunner.register_training_hooks(cfg.lr_config,\n                               cfg.optimizer_config,\n                               cfg.checkpoint_config,\n                               cfg.log_config,\n                               cfg.get('momentum_config', None),\n                               custom_hooks_config=cfg.get(\n                                   'custom_hooks', None))","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:41:44.970614Z","iopub.execute_input":"2022-03-17T10:41:44.970864Z","iopub.status.idle":"2022-03-17T10:41:45.005306Z","shell.execute_reply.started":"2022-03-17T10:41:44.97083Z","shell.execute_reply":"2022-03-17T10:41:45.004683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = FeedbackDataset(**cfg.data, tokenizer=detector.tokenizer)\ncollate = CustomCollator(detector.tokenizer, detector.model)\ntrain_dl = DataLoader(train_ds,\n                      batch_size=1,\n                      shuffle=True,\n                      num_workers=2,\n                      collate_fn=collate)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:41:45.008858Z","iopub.execute_input":"2022-03-17T10:41:45.009046Z","iopub.status.idle":"2022-03-17T10:41:47.247526Z","shell.execute_reply.started":"2022-03-17T10:41:45.009023Z","shell.execute_reply":"2022-03-17T10:41:47.246759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"runner.run([train_dl], [('train', 1)])","metadata":{"execution":{"iopub.status.busy":"2022-03-17T10:41:47.248861Z","iopub.execute_input":"2022-03-17T10:41:47.2493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"class FeedbackTestDataset(Dataset):\n    def __init__(self, csv_file, text_dir, tokenizer):\n        self.df = pd.read_csv(csv_file)\n        self.samples = sorted(self.df['id'].unique())\n        self.text_dir = text_dir\n        self.tokenizer = tokenizer\n        print(f'Loaded {len(self)} samples.')\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, index):\n        text_id = self.samples[index]\n        text_path = osp.join(self.text_dir, f'{text_id}.txt')\n\n        with open(text_path) as f:\n            text = f.read().rstrip()\n\n        tokens = self.tokenizer(text, return_offsets_mapping=True)\n        input_ids = torch.LongTensor(tokens['input_ids'])\n        attention_mask = torch.LongTensor(tokens['attention_mask'])\n        offset_mapping = np.array(tokens['offset_mapping'])\n        offset_mapping = self.strip_offset_mapping(text, offset_mapping)\n        num_tokens = len(input_ids)\n\n        # token slices of words\n        woff = self.get_word_offsets(text)\n        toff = offset_mapping\n        wx1, wx2 = woff.T\n        tx1, tx2 = toff.T\n        ix1 = np.maximum(wx1[..., None], tx1[None, ...])\n        ix2 = np.minimum(wx2[..., None], tx2[None, ...])\n        ux1 = np.minimum(wx1[..., None], tx1[None, ...])\n        ux2 = np.maximum(wx2[..., None], tx2[None, ...])\n        ious = (ix2 - ix1).clip(min=0) / (ux2 - ux1 + 1e-12)\n        assert (ious > 0).any(-1).all()\n\n        word_boxes = []\n        for row in ious:\n            inds = row.nonzero()[0]\n            word_boxes.append([inds[0], 0, inds[-1] + 1, 1])\n        word_boxes = torch.FloatTensor(word_boxes)\n\n        return dict(text=text,\n                    text_id=text_id,\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    word_boxes=word_boxes)\n\n    def strip_offset_mapping(self, text, offset_mapping):\n        ret = []\n        for start, end in offset_mapping:\n            match = list(re.finditer('\\S+', text[start:end]))\n            if len(match) == 0:\n                ret.append((start, end))\n            else:\n                span_start, span_end = match[0].span()\n                ret.append((start + span_start, start + span_end))\n        return np.array(ret)\n\n    def get_word_offsets(self, text):\n        matches = re.finditer(\"\\S+\", text)\n        spans = []\n        words = []\n        for match in matches:\n            span = match.span()\n            word = match.group()\n            spans.append(span)\n            words.append(word)\n        assert tuple(words) == tuple(text.split())\n        return np.array(spans)\n\n\nclass TestCollator(object):\n    def __init__(self, tokenizer, model):\n        self.pad_token_id = tokenizer.pad_token_id\n        if hasattr(model.config, 'attention_window'):\n            # For longformer\n            # https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/longformer/modeling_longformer.py#L1548\n            self.attention_window = (model.config.attention_window\n                                     if isinstance(\n                                         model.config.attention_window, int)\n                                     else max(model.config.attention_window))\n        else:\n            self.attention_window = None\n\n    def __call__(self, samples):\n        batch_size = len(samples)\n        assert batch_size == 1, f'Only batch_size=1 supported, got batch_size={batch_size}.'\n\n        sample = samples[0]\n\n        max_seq_length = len(sample['input_ids'])\n        if self.attention_window is not None:\n            attention_window = self.attention_window\n            padded_length = (attention_window -\n                             max_seq_length % attention_window\n                             ) % attention_window + max_seq_length\n        else:\n            padded_length = max_seq_length\n\n        input_shape = (1, padded_length)\n        input_ids = torch.full(input_shape,\n                               self.pad_token_id,\n                               dtype=torch.long)\n        attention_mask = torch.zeros(input_shape, dtype=torch.long)\n\n        seq_length = len(sample['input_ids'])\n        input_ids[0, :seq_length] = sample['input_ids']\n        attention_mask[0, :seq_length] = sample['attention_mask']\n\n        text_id = sample['text_id']\n        text = sample['text']\n        word_boxes = sample['word_boxes']\n\n        return dict(text_id=text_id,\n                    text=text,\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    word_boxes=word_boxes)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\n\nval_ds = FeedbackTestDataset(\n    '../input/feedback-prize-2021/sample_submission.csv',\n    '../input/feedback-prize-2021/test', detector.tokenizer)\ncollate = TestCollator(detector.tokenizer, detector.model)\nval_dl = DataLoader(val_ds,\n                    batch_size=1,\n                    shuffle=False,\n                    num_workers=2,\n                    collate_fn=collate)\n\nresults = []\nfor data in tqdm(val_dl):\n    with torch.no_grad():\n        result = detector.predict(data, test_score_thr=0.6)\n    results.append(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(results[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}