{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img style=\"width:100%;\" src=\"http://labs.jstor.org/content/images/2020/03/Screen-Shot-2020-03-27-at-3.01.38-PM.jpg\">\n<h1 style=\"text-align:center;background-color:black;\"> FeedBack Comepetition <i> [BASE-line + Visualization] </i></h1>","metadata":{}},{"cell_type":"markdown","source":"<h3> Given Information: </h3>\n\n**id - ID code for essay response <br>\ndiscourse_id - ID code for discourse element <br>\ndiscourse_start - character position where discourse element begins in the essay response <br>\ndiscourse_end - character position where discourse element ends in the essay response <br>\ndiscourse_text - text of discourse element <br>\ndiscourse_type - classification of discourse element <br>\ndiscourse_type_num - enumerated class label of discourse element <br>\npredictionstring - the word indices of the training sample, as required for predictions<br>**","metadata":{}},{"cell_type":"markdown","source":"**Language models like NER implementation are ideal for this case**","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"font-family: sans-seriff; padding: 5px;\"><mark> ALWAYS THINK BEFORE DOING </mark></h1>","metadata":{}},{"cell_type":"markdown","source":"### Extraction part","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom IPython.display import display\nimport shutil\nimport statsmodels\nimport os\nimport tensorflow as tf\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport sklearn \nimport time\nfrom pprint import pprint\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-03-20T16:20:23.317057Z","iopub.execute_input":"2022-03-20T16:20:23.317534Z","iopub.status.idle":"2022-03-20T16:20:23.323796Z","shell.execute_reply.started":"2022-03-20T16:20:23.317497Z","shell.execute_reply":"2022-03-20T16:20:23.323115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/feedback-prize-2021/train.csv')\ndf_train.index = df_train.id\n\npath = '../input/feedback-prize-2021/'\n\ntrain_data = []\ntrain_id = []\ntest_data = []\ntest_id = []\n\nfor name in os.listdir(path + 'test'):\n    with open(path + 'test/' + name, 'r', encoding='latin-1') as f:\n        test_data.append(f.read())\n        test_id.append(name[:-4])\n        \nfor name in os.listdir(path + 'train'):\n    with open(path + 'train/' + name, 'r', encoding='latin-1') as f:\n        train_data.append(f.read())\n        train_id.append(name[:-4])\n\ndf_train.predictionstring = df_train.predictionstring.agg(lambda k: k.split(' '))\ndf_train['word_start'] = df_train.predictionstring.agg(lambda k: int(k[0]))\ndf_train['word_end'] = df_train.predictionstring.agg(lambda k: int(k[-1]))\ndf_train = df_train.drop(['id', 'discourse_id','discourse_start', 'discourse_end'], axis =1)\ndisplay(df_train)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-20T16:20:23.325811Z","iopub.execute_input":"2022-03-20T16:20:23.326297Z","iopub.status.idle":"2022-03-20T16:21:31.269781Z","shell.execute_reply.started":"2022-03-20T16:20:23.326261Z","shell.execute_reply":"2022-03-20T16:21:31.269096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punc = string.punctuation\nnew_punc = punc.replace(\"~\", \"\") + '.'\nprint('New_Punc: ', new_punc)\ntrain_text_corpus = '~~~'.join(train_data)\ntrain_text_corpus = train_text_corpus.translate(str.maketrans('','',new_punc))\ntrain_data = train_text_corpus.split('~~~')\ndf_text = pd.DataFrame(train_data, columns = ['train_text'], index=train_id)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T16:21:31.271073Z","iopub.execute_input":"2022-03-20T16:21:31.271331Z","iopub.status.idle":"2022-03-20T16:21:34.654922Z","shell.execute_reply.started":"2022-03-20T16:21:31.271297Z","shell.execute_reply":"2022-03-20T16:21:34.654203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' Loading & Saving File '''\n\n# import pickle as pkl\n\n# dir_path_obj = open('./df_train.txt', 'wb')\n# pkl.dump(df_train, dir_path_obj)\n# dir_path_obj.close()\n\n# dir_path_obj = open('./df_text.txt', 'wb')\n# pkl.dump(df_text, dir_path_obj)\n# dir_path_obj.close()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-20T16:21:34.656243Z","iopub.execute_input":"2022-03-20T16:21:34.656506Z","iopub.status.idle":"2022-03-20T16:21:34.662782Z","shell.execute_reply.started":"2022-03-20T16:21:34.656475Z","shell.execute_reply":"2022-03-20T16:21:34.662121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def use_fn(k):\n    k_type = [ele for k_each in k for ele in k_each]\n    return k_type\n\ndf_plot = df_train.groupby(['id','discourse_type'])['predictionstring'].agg(use_fn)\nall_ids = list(set(df_train.index))","metadata":{"execution":{"iopub.status.busy":"2022-03-20T16:21:34.665164Z","iopub.execute_input":"2022-03-20T16:21:34.665679Z","iopub.status.idle":"2022-03-20T16:21:36.336849Z","shell.execute_reply.started":"2022-03-20T16:21:34.665644Z","shell.execute_reply":"2022-03-20T16:21:36.336065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lmt = 5\nsns.set_style('darkgrid')\nfig, axs = plt.subplots(lmt, 1 , figsize=(25,15))\n# tests_id = '62C57C524CD2'\nprint('--'*20,'Visual-Repersentation', '--'*20)\nfor num, tests_id in enumerate(all_ids[:lmt]):\n    for step, name in enumerate(df_plot[tests_id].keys()):\n        y_axs = np.ones(len((df_text.loc[tests_id].train_text).split(' '))) * 7\n        x_axs = np.arange(0,len((df_text.loc[tests_id].train_text).split(' ')),1)\n        value = df_plot[tests_id][name]\n        axs[num].set_title(tests_id)\n        if step==0:\n            axs[num].plot(x_axs, y_axs, 'ko', label='Text')\n        axs[num].plot(np.array(value, dtype='int32'), np.ones(len(value))*step, label=name, marker='o')\n        axs[num].legend()    \n#     plt.show()\nplt.tight_layout(pad=2)\nplt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-20T16:21:36.337977Z","iopub.execute_input":"2022-03-20T16:21:36.338422Z","iopub.status.idle":"2022-03-20T16:21:37.764767Z","shell.execute_reply.started":"2022-03-20T16:21:36.338383Z","shell.execute_reply":"2022-03-20T16:21:37.76409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_time = time.time()\ncheck_id = '423A1CA112E2'\nsample_dict = {}\nclasses = list(set(df_train.discourse_type.values))\nfor sample in all_ids:\n    each_dict = {name:0.0 for name in classes}\n    for name in (df_plot[sample].keys()):        \n        each_dict[name] = len(df_plot[sample][name]) / len((df_text.loc[tests_id].train_text).split(' '))\n        sample_dict[sample] = each_dict\n        \nsecond_time = time.time()\nprint('Time_Taken: ', second_time-first_time)\nprint('sample_dict_Done')    ","metadata":{"execution":{"iopub.status.busy":"2022-03-20T16:21:37.76612Z","iopub.execute_input":"2022-03-20T16:21:37.766561Z","iopub.status.idle":"2022-03-20T16:22:17.294886Z","shell.execute_reply.started":"2022-03-20T16:21:37.766525Z","shell.execute_reply":"2022-03-20T16:22:17.293944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_time = time.time()\ntrain_token = tf.keras.preprocessing.text.Tokenizer(split = ' ', lower=True)\ntrain_token.fit_on_texts([' '.join(df_text.train_text.values.tolist())])\nx_train = np.array(train_token.texts_to_sequences(df_text.train_text.values))\nsecond_time = time.time()\nword_vocab = train_token.word_index\nprint('time_taken: ', second_time - first_time)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T16:22:17.296446Z","iopub.execute_input":"2022-03-20T16:22:17.296732Z","iopub.status.idle":"2022-03-20T16:22:30.519545Z","shell.execute_reply.started":"2022-03-20T16:22:17.296693Z","shell.execute_reply":"2022-03-20T16:22:30.518775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = np.array([list(list(sample_dict.values())[i].values()) for i in range(len(all_ids))]) # all the proba_distrib of the multi-label statements\nprint('x_train_shape : ', x_train.shape)\nprint('y_train_shape : ', y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T16:22:30.520662Z","iopub.execute_input":"2022-03-20T16:22:30.52146Z","iopub.status.idle":"2022-03-20T16:22:32.378266Z","shell.execute_reply.started":"2022-03-20T16:22:30.521421Z","shell.execute_reply":"2022-03-20T16:22:32.376564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = len(x_train[0])\nfor i in x_train:\n    if len(i) > max_len:\n        max_len = len(i)\nprint('MAX_LEN: ', max_len)     \nx_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_len, padding='post')\nprint('x_train_shape: ', x_train.shape)\ncount = 0","metadata":{"execution":{"iopub.status.busy":"2022-03-20T16:22:32.379624Z","iopub.execute_input":"2022-03-20T16:22:32.379924Z","iopub.status.idle":"2022-03-20T16:22:34.101965Z","shell.execute_reply.started":"2022-03-20T16:22:32.379877Z","shell.execute_reply":"2022-03-20T16:22:34.101202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Use tf.data.Dataset\nfrom sklearn.model_selection import train_test_split\ndef splitter(x_train, y_train):\n    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, random_state=28, test_size = 0.15)\n    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, random_state=28)\n    print(x_train.shape, y_train.shape, x_val.shape, y_val.shape ,x_test.shape, y_test.shape)\n    return x_train, y_train, x_val, y_val, x_test, y_test\n\nif count == 0:\n    x_train, y_train, x_val, y_val, x_test, y_test = splitter(x_train, y_train)\n\ncount += 1\n\n# take a batch_size thats multiple to the train_sample_size\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(28)\nval_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(28)\ntest_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(28)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T16:22:34.103428Z","iopub.execute_input":"2022-03-20T16:22:34.103685Z","iopub.status.idle":"2022-03-20T16:22:34.521038Z","shell.execute_reply.started":"2022-03-20T16:22:34.103651Z","shell.execute_reply":"2022-03-20T16:22:34.520276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Try to Match the Truth Probabability Distribution using KL divergence","metadata":{}},{"cell_type":"code","source":"# print(x_train.shape[0], x_val.shape[0], x_test.shape[0])\ndf_distrib = df_train['discourse_type'].value_counts()\nplt.title('_Distrib_Class_')\ndf_distrib.plot(figsize=(12,6), kind='bar')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T16:22:34.522325Z","iopub.execute_input":"2022-03-20T16:22:34.522595Z","iopub.status.idle":"2022-03-20T16:22:34.770735Z","shell.execute_reply.started":"2022-03-20T16:22:34.522561Z","shell.execute_reply":"2022-03-20T16:22:34.770055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b><mark> We create sample/Class weights but wont use them in these notebook  </mark></b><br>\nAs this is a Experimental/Exploratory/Baseline NoteBook implementation","metadata":{}},{"cell_type":"code","source":"total_arr = np.ones(len(df_train))\nstart_idx = 0\nfor idx, key in enumerate(df_distrib.keys()):\n    total_arr[start_idx : start_idx + df_distrib[key]] = idx\n    start_idx = start_idx + df_distrib[key]\n    \nprint(total_arr.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T16:22:34.772102Z","iopub.execute_input":"2022-03-20T16:22:34.772555Z","iopub.status.idle":"2022-03-20T16:22:34.779562Z","shell.execute_reply.started":"2022-03-20T16:22:34.772518Z","shell.execute_reply":"2022-03-20T16:22:34.778566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import class_weight\n\nclass_weights = class_weight.compute_class_weight('balanced', np.unique(total_arr), total_arr)\nclass_weights = dict(enumerate(class_weights))\nsample_weights = np.array([class_weights.get(_ele) for _ele in total_arr])\n\nsample_new_weights = np.ones(shape = (len(np.unique(df_train.index)), len(list(class_weights))))\nfor row in range(sample_new_weights.shape[0]):\n    for col in range(sample_new_weights.shape[-1]):\n        sample_new_weights[int(row)][int(col)] = class_weights.get(col)\n        \nprint(sample_new_weights.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T16:22:34.783155Z","iopub.execute_input":"2022-03-20T16:22:34.783541Z","iopub.status.idle":"2022-03-20T16:22:35.234484Z","shell.execute_reply.started":"2022-03-20T16:22:34.783506Z","shell.execute_reply":"2022-03-20T16:22:35.23364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MIMIC MODEL ","metadata":{}},{"cell_type":"markdown","source":"##### Only way using sample_weight with sequential generation | using a decoder model to decode the encoder states ","metadata":{}},{"cell_type":"code","source":"''' Train MIMIC-MODEL '''\n# print('The MImic Model')\n\nmimic_model = tf.keras.models.Sequential([\n    tf.keras.layers.Input(shape=(x_train.shape[-1])),\n    tf.keras.layers.Embedding(input_dim=len(word_vocab), output_dim = 64),\n    tf.keras.layers.GRU(32, return_sequences=True, recurrent_dropout = 0.5, dropout=0.4),\n    tf.keras.layers.GRU(32),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(len(classes), activation='softmax'),\n    \n])\n\nes = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\nmimic_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\nmimic_model.summary()\ntrain_ft = time.time()\nmimic_model.fit(x_train, y_train, epochs=5, validation_data=val_dataset, callbacks=[es], steps_per_epoch=128)#sample_weight = sample_new_weights[:len(x_train)] )\ntrain_lt = time.time()\nprint('Time taken for training MIMIC-Model: ', train_lt - train_ft)\nmimic_model.save('./' + 'mimic_model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-03-20T17:48:32.797479Z","iopub.execute_input":"2022-03-20T17:48:32.798396Z","iopub.status.idle":"2022-03-20T17:48:32.804701Z","shell.execute_reply.started":"2022-03-20T17:48:32.798349Z","shell.execute_reply":"2022-03-20T17:48:32.803981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mimic_model = tf.keras.models.load_model('../input/models/mimic_model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-03-20T17:43:49.793528Z","iopub.status.idle":"2022-03-20T17:43:49.794177Z","shell.execute_reply.started":"2022-03-20T17:43:49.793916Z","shell.execute_reply":"2022-03-20T17:43:49.793952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_train[0].shape\nfor seed in range(1):    \n    print('--' * 20)\n    print('SEED_INDEX: ', seed)\n    print()\n    print('ground_truth: ',y_test[seed], len(y_test[seed]))\n    print('Prediction: ', mimic_model.predict(x_test[seed].reshape(1,-1)))\n    print()\n    print('--'*20)\nmimic_model.evaluate(x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T17:45:28.406051Z","iopub.execute_input":"2022-03-20T17:45:28.406558Z","iopub.status.idle":"2022-03-20T17:47:52.648448Z","shell.execute_reply.started":"2022-03-20T17:45:28.406518Z","shell.execute_reply":"2022-03-20T17:47:52.647745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('y_test: ', y_test[seed])\ny_pred = mimic_model.predict(x_test[seed].reshape(1,-1))[0]\nprint('y_pred: ', y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T17:48:07.297078Z","iopub.execute_input":"2022-03-20T17:48:07.297716Z","iopub.status.idle":"2022-03-20T17:48:08.76701Z","shell.execute_reply.started":"2022-03-20T17:48:07.297663Z","shell.execute_reply":"2022-03-20T17:48:08.766272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Custom Model","metadata":{}},{"cell_type":"code","source":"''' MIMICv2-MODEL '''\n\n''' @ Two-Methods :\n    1. Retrain the mimic_model (Trained) Model\n    2. Train the mimCv2 model independently\n'''\n\nclass mimCv2(tf.keras.models.Model):\n    def __init__(self, input_shape):\n        super().__init__()\n#         self.build((None,) + input_shape)\n        self.mimic_arch = tf.keras.models.Sequential([\n            tf.keras.layers.Input(shape=(input_shape)),\n            tf.keras.layers.Embedding(input_dim=len(word_vocab), output_dim = 128),\n            tf.keras.layers.GRU(64, return_sequences=True),\n            tf.keras.layers.GRU(64),\n            tf.keras.layers.Dense(len(classes), activation='softmax')\n        ])\n        print('__mimCv2_initialized__')\n                \n        \n    def compile(self, mimic_loss, std_loss ,optimizer, metrics, alpha = 0.2, smooth = 10.0):\n        super().compile(optimizer = optimizer, metrics = metrics) #calls the parent 'Model' compile method\n        self.alpha = alpha\n        self.mimic_loss = mimic_loss\n        self.std_loss = std_loss\n        self.smooth = 10.0\n#         steps_per_execution = 128\n        \n    def train_step(self, train_dataset):\n#         print('IN TRAIN STEP')\n        x, y = train_dataset\n        with tf.GradientTape() as tape:\n            mimic_arch_op = self.mimic_arch(x, training=True)\n            mimic_arch_op = mimic_arch_op / tf.reduce_sum(mimic_arch_op, axis=0) \n            \n            ''' Another Approach maybe softmax-ing outputs '''\n#             print(type(mimic_arch_op))\n#             mimic_arch_op = tf.nn.softmax(mimic_arch_op)\n#             mimic_arch = self(x, training=True)\n#             mimic_loss = self.mimic_loss(y / self.smooth, mimic_arch_op / self.smooth)\n            '''\n            we can use sample_weight while calculating KLDivergence\n            '''\n            mimic_loss = self.mimic_loss(y , mimic_arch_op)\n            std_loss = self.std_loss(y, mimic_arch_op)\n            loss =  self.alpha*std_loss + (1 - self.alpha) * mimic_loss \n        train_weights = self.mimic_arch.trainable_variables\n        mimic_grad = tape.gradient(loss, train_weights)\n        self.optimizer.apply_gradients(zip(mimic_grad, train_weights))\n        self.compiled_metrics.update_state(y, mimic_arch_op)\n        results = {mtr.name: mtr.result() for mtr in self.metrics}\n        results.update({'mimic_loss': mimic_loss})\n        return results\n    \n    def test_step(self, test_dataset):\n        print()\n#         print('IN TEST STEP')\n        x_test, y_test = test_dataset\n        mimic_arch_op = self.mimic_arch(x_test, training=False)\n        mimic_arch_op = mimic_arch_op / tf.reduce_sum(mimic_arch_op, axis=0) \n#         mimic_arch_op = tf.nn.softmax(mimic_arch_op)\n#         mimic_arch_op = self(x_test, training=False)\n        mimic_loss = self.mimic_loss(y_test, mimic_arch_op)\n        std_loss = self.std_loss(y_test, mimic_arch_op)\n        loss = self.alpha * std_loss + (1 - self.alpha) * mimic_loss\n        self.compiled_metrics.update_state(y_test, mimic_arch_op)\n        results = {mtr.name:mtr.result() for mtr in self.metrics}\n        results.update({'mimic_loss': mimic_loss})\n        return results\n    \n    def call(self, x_sample):\n        pred_sample = self.mimic_arch(x_sample, training=False)\n        return pred_sample\n    \ninput_shape = (x_train.shape[-1],)\nmim2_model = mimCv2(input_shape)\n\nkwargs = {\n    'mimic_loss' : tf.keras.losses.KLDivergence(),\n    'std_loss' : tf.keras.losses.categorical_crossentropy\n}\nmim2_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2), #, clipnorm=1.0),\n                  metrics = [tf.keras.metrics.Accuracy()],\n                  **kwargs)\n\n''' Metrics not changing due to less change in loss value | immidiete convergence '''\n# mim2_model.build(input_shape)\nmim2_model.compute_output_shape(input_shape = (None, )+ input_shape)\nmim2_model.summary()\ntrain_ft = time.time()\nmim2_model.fit(train_dataset, epochs=3, validation_data=val_dataset, steps_per_epoch=None)#, callbacks=[es])\ntrain_lt = time.time()\nprint('Time taken for training mimCv2: ', train_lt - train_ft)\nprint('EVAULATION')\nmim2_model.evaluate(test_dataset, steps=20)                \n# use shutil/zipfile to create zips of the subclass model directory, and to save the model dir\nshutil.make_archive('mim2_model', 'zip', mim2_model.save('./mim2_model'))        ","metadata":{"execution":{"iopub.status.busy":"2022-03-20T17:57:54.893538Z","iopub.execute_input":"2022-03-20T17:57:54.893823Z","iopub.status.idle":"2022-03-20T17:57:54.903258Z","shell.execute_reply.started":"2022-03-20T17:57:54.893794Z","shell.execute_reply":"2022-03-20T17:57:54.90244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mim2_model = tf.keras.models.load_model('../input/models/mim2_model')","metadata":{"execution":{"iopub.status.busy":"2022-03-20T17:43:49.801193Z","iopub.status.idle":"2022-03-20T17:43:49.801877Z","shell.execute_reply.started":"2022-03-20T17:43:49.801628Z","shell.execute_reply":"2022-03-20T17:43:49.801654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Final Inference </b> | <i><mark> Not very Good Results ! </mark></i>","metadata":{}},{"cell_type":"code","source":"print('__mimCv2_MODEL__')\nfor seed in range(1):    \n    pred = np.ravel(mim2_model.predict(x_test[seed].reshape(1,-1)))\n    pred_dict = {key:value for key, value in zip(classes, pred)}\n    truth_dict = {key: value for key, value in zip(classes, y_test[seed])}\n    \n    print('PRED: ', end='\\n\\n')\n    pprint(pred_dict)\n    print()\n    print('TRUTH: ', end='\\n\\n')\n    pprint(truth_dict)\n    print()\n\nprint('--' * 20)\nprint()\n\nprint('__MIMIC_MODEL__')\nfor seed in range(1):\n    pred = np.ravel(mimic_model.predict(x_test[seed].reshape(1,-1)))\n    pred_dict = {key:value for key, value in zip(classes, pred)}\n    truth_dict = {key: value for key, value in zip(classes, y_test[seed])}\n\n    pprint(pred_dict)\n    print('PRED: ', end='\\n\\n')\n    print()\n    pprint(pred_dict)\n    print('TRUTH: ', end='\\n\\n')\n    print()\n    pprint(truth_dict)\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-20T17:57:47.536899Z","iopub.execute_input":"2022-03-20T17:57:47.537252Z","iopub.status.idle":"2022-03-20T17:57:49.695478Z","shell.execute_reply.started":"2022-03-20T17:57:47.537201Z","shell.execute_reply":"2022-03-20T17:57:49.694792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SESSION - STOPPER ","metadata":{}},{"cell_type":"code","source":"for i in range(10):\n    print('time: ',i)\n    time.sleep(30*60)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T17:43:49.805145Z","iopub.status.idle":"2022-03-20T17:43:49.8058Z","shell.execute_reply.started":"2022-03-20T17:43:49.80554Z","shell.execute_reply":"2022-03-20T17:43:49.805563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can use this information for further analysis and as features to ...\nPEACE OUT ","metadata":{}}]}