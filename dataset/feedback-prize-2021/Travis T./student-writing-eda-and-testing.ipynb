{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as st\nimport math, gc, re, warnings\nfrom nltk.tokenize import sent_tokenize\nfrom wordcloud import WordCloud, STOPWORDS\nfrom tqdm.auto import tqdm\nfrom os import listdir\nfrom os.path import isfile, join\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T21:59:33.073587Z","iopub.execute_input":"2021-12-21T21:59:33.074563Z","iopub.status.idle":"2021-12-21T21:59:33.081379Z","shell.execute_reply.started":"2021-12-21T21:59:33.074507Z","shell.execute_reply":"2021-12-21T21:59:33.080675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(pd.read_csv('../input/feedback-prize-2021/train.csv'))\ndf","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T21:59:33.082808Z","iopub.execute_input":"2021-12-21T21:59:33.083605Z","iopub.status.idle":"2021-12-21T21:59:34.10534Z","shell.execute_reply.started":"2021-12-21T21:59:33.083553Z","shell.execute_reply":"2021-12-21T21:59:34.104207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['sentence_num'] = df.groupby('id').cumcount() + 1\ndf['total_sentences'] = df.groupby('id')['sentence_num'].transform('max')\ndf['sentence_location'] = round(df.sentence_num/df.total_sentences, 2) * 10\ndf['words_num'] = df['discourse_text'].str.split(' ').str.len()\ndf['char_num'] = df['discourse_text'].str.len()\n\ndf = df.rename(columns={'discourse_type':'class',\n                        'discourse_text':'text',\n                        'discourse_start':'start_loc',\n                        'discourse_end':'end_loc'})\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T21:59:34.107351Z","iopub.execute_input":"2021-12-21T21:59:34.107595Z","iopub.status.idle":"2021-12-21T21:59:36.178391Z","shell.execute_reply.started":"2021-12-21T21:59:34.107566Z","shell.execute_reply":"2021-12-21T21:59:36.17755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Discourse by the numbers","metadata":{}},{"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(1,2, figsize=(15,5))\n# discourse type breakdown\ndf['class'].value_counts().plot.bar(title='class', ax=ax1)\ndf['discourse_type_num'].value_counts().plot.bar(title='discourse_type_num', ax=ax2)\nplt.tight_layout()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T21:59:36.179891Z","iopub.execute_input":"2021-12-21T21:59:36.180123Z","iopub.status.idle":"2021-12-21T21:59:37.097948Z","shell.execute_reply.started":"2021-12-21T21:59:36.180095Z","shell.execute_reply":"2021-12-21T21:59:37.096932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby('id')['total_sentences'].first().plot.hist(grid=True, title='Total sentences frequency')\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T21:59:37.10024Z","iopub.execute_input":"2021-12-21T21:59:37.101143Z","iopub.status.idle":"2021-12-21T21:59:37.388004Z","shell.execute_reply.started":"2021-12-21T21:59:37.101089Z","shell.execute_reply":"2021-12-21T21:59:37.387012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# discourse start/end frequencies \nfig, (ax1,ax2) = plt.subplots(1,2, figsize=(15,3))\ndf.start_loc.plot.hist(grid=True, bins=20, title='start location', ax=ax1)\ndf.end_loc.plot.hist(grid=True, bins=20, title='end location', ax=ax2)\nplt.tight_layout()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T21:59:37.389256Z","iopub.execute_input":"2021-12-21T21:59:37.389506Z","iopub.status.idle":"2021-12-21T21:59:37.953822Z","shell.execute_reply.started":"2021-12-21T21:59:37.389473Z","shell.execute_reply":"2021-12-21T21:59:37.953234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# length of discourse text\nfig, ((ax1,ax2),(ax3,ax4),(ax5,ax6),(ax7,ax8)) = plt.subplots(4,2, figsize=(15,10))\n\ntypes = df['class'].unique().tolist()\naxs = [ax1,ax2,ax3,ax4,ax5,ax6,ax7]\n\ndef plot_length(discourse):\n    df.loc[df['class']==discourse]['text'].str.split(' ').str.len()\\\n        .plot.hist(grid=True, bins=20, title=f'{discourse} text length frequency: number of words', ax=axs[types.index(discourse)])\n    \nfor x in types:\n    plot_length(x)\n\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T21:59:37.954879Z","iopub.execute_input":"2021-12-21T21:59:37.955244Z","iopub.status.idle":"2021-12-21T21:59:41.133897Z","shell.execute_reply.started":"2021-12-21T21:59:37.955207Z","shell.execute_reply":"2021-12-21T21:59:41.132968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# length of discourse text\nfig, ((ax1,ax2),(ax3,ax4),(ax5,ax6),(ax7,ax8)) = plt.subplots(4,2, figsize=(15,10))\n\naxs = [ax1,ax2,ax3,ax4,ax5,ax6,ax7]\n\ndef sentence_number(discourse):\n    df.loc[df['class']==discourse]['text'].str.len()\\\n        .plot.hist(grid=True, bins=20, title=f'{discourse} character number frequency', ax=axs[types.index(discourse)])\n    \nfor x in types:\n    sentence_number(x)\n\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T21:59:41.135246Z","iopub.execute_input":"2021-12-21T21:59:41.135488Z","iopub.status.idle":"2021-12-21T21:59:43.311772Z","shell.execute_reply.started":"2021-12-21T21:59:41.135456Z","shell.execute_reply":"2021-12-21T21:59:43.310936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# length of discourse text\nfig, ((ax1,ax2),(ax3,ax4),(ax5,ax6),(ax7,ax8)) = plt.subplots(4,2, figsize=(15,10))\n\naxs = [ax1,ax2,ax3,ax4,ax5,ax6,ax7]\n\ndef plot_length(discourse):\n    df.loc[df['class']==discourse]['sentence_num']\\\n        .plot.hist(grid=True, bins=20, title=f'{discourse} sentence number frequency', ax=axs[types.index(discourse)])\n    \nfor x in types:\n    plot_length(x)\n\nplt.tight_layout()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T21:59:43.313641Z","iopub.execute_input":"2021-12-21T21:59:43.313982Z","iopub.status.idle":"2021-12-21T21:59:45.061429Z","shell.execute_reply.started":"2021-12-21T21:59:43.313938Z","shell.execute_reply":"2021-12-21T21:59:45.060639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# length of discourse text\nfig, ((ax1,ax2),(ax3,ax4),(ax5,ax6),(ax7,ax8)) = plt.subplots(4,2, figsize=(15,10))\n\naxs = [ax1,ax2,ax3,ax4,ax5,ax6,ax7]\n\ndef sentence_number(discourse):\n    df.loc[df['class']==discourse]['sentence_location']\\\n        .plot.hist(grid=True, bins=20, title=f'{discourse} essay location frequency', ax=axs[types.index(discourse)])\n    \nfor x in types:\n    sentence_number(x)\n\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T21:59:45.062942Z","iopub.execute_input":"2021-12-21T21:59:45.063429Z","iopub.status.idle":"2021-12-21T21:59:46.631166Z","shell.execute_reply.started":"2021-12-21T21:59:45.06338Z","shell.execute_reply":"2021-12-21T21:59:46.630201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word clouds","metadata":{}},{"cell_type":"code","source":"words = ''\nstopwords = set(STOPWORDS)\n\n# iterate through the df\nfor val in df['text']:\n\n    val = str(val)\n\n    tokens = val.split()\n\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n\n    words += \" \".join(tokens)+\" \"\n\nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10).generate(words)\n\n# plot word cloud                       \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title(f'Text word cloud')\nplt.tight_layout(pad = 0)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T21:59:46.63382Z","iopub.execute_input":"2021-12-21T21:59:46.634084Z","iopub.status.idle":"2021-12-21T22:00:05.622524Z","shell.execute_reply.started":"2021-12-21T21:59:46.634054Z","shell.execute_reply":"2021-12-21T22:00:05.621616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add stopwords based on word cloud\nstop_words = ['student','students','school','schools','people','teacher','teachers'] + list(STOPWORDS)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:00:05.623755Z","iopub.execute_input":"2021-12-21T22:00:05.624106Z","iopub.status.idle":"2021-12-21T22:00:05.627361Z","shell.execute_reply.started":"2021-12-21T22:00:05.624075Z","shell.execute_reply":"2021-12-21T22:00:05.626848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def wc(discourse):\n    words = ''\n    stopwords = set(stop_words)\n\n    # iterate through the df\n    for val in df.loc[df['class']==discourse]['text']:\n\n        val = str(val)\n\n        tokens = val.split()\n\n        for i in range(len(tokens)):\n            tokens[i] = tokens[i].lower()\n\n        words += \" \".join(tokens)+\" \"\n\n    wordcloud = WordCloud(width = 800, height = 800,\n                    background_color ='white',\n                    stopwords = stopwords,\n                    min_font_size = 10).generate(words)\n\n    # plot word cloud                       \n    plt.figure(figsize = (8, 8), facecolor = None)\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.title(f'{discourse} word cloud')\n    plt.tight_layout(pad = 0)\n\n    plt.show()\n    \nwc('Claim')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T22:00:05.628458Z","iopub.execute_input":"2021-12-21T22:00:05.628779Z","iopub.status.idle":"2021-12-21T22:00:10.03268Z","shell.execute_reply.started":"2021-12-21T22:00:05.628744Z","shell.execute_reply":"2021-12-21T22:00:10.031963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wc('Evidence')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T22:00:10.033973Z","iopub.execute_input":"2021-12-21T22:00:10.034321Z","iopub.status.idle":"2021-12-21T22:00:21.532078Z","shell.execute_reply.started":"2021-12-21T22:00:10.034287Z","shell.execute_reply":"2021-12-21T22:00:21.531066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wc('Position')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T22:00:21.533432Z","iopub.execute_input":"2021-12-21T22:00:21.534368Z","iopub.status.idle":"2021-12-21T22:00:24.282496Z","shell.execute_reply.started":"2021-12-21T22:00:21.534316Z","shell.execute_reply":"2021-12-21T22:00:24.281551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wc('Concluding Statement')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T22:00:24.28413Z","iopub.execute_input":"2021-12-21T22:00:24.284705Z","iopub.status.idle":"2021-12-21T22:00:28.504594Z","shell.execute_reply.started":"2021-12-21T22:00:24.284658Z","shell.execute_reply":"2021-12-21T22:00:28.503605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wc('Lead')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T22:00:28.505983Z","iopub.execute_input":"2021-12-21T22:00:28.506237Z","iopub.status.idle":"2021-12-21T22:00:31.570621Z","shell.execute_reply.started":"2021-12-21T22:00:28.506207Z","shell.execute_reply":"2021-12-21T22:00:31.569623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wc('Counterclaim')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T22:00:31.571822Z","iopub.execute_input":"2021-12-21T22:00:31.572122Z","iopub.status.idle":"2021-12-21T22:00:34.136434Z","shell.execute_reply.started":"2021-12-21T22:00:31.572089Z","shell.execute_reply":"2021-12-21T22:00:34.135514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wc('Rebuttal')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T22:00:34.13851Z","iopub.execute_input":"2021-12-21T22:00:34.1393Z","iopub.status.idle":"2021-12-21T22:00:36.528135Z","shell.execute_reply.started":"2021-12-21T22:00:34.139248Z","shell.execute_reply":"2021-12-21T22:00:36.527114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training/testing basic model","metadata":{}},{"cell_type":"code","source":"# select features/target and split data\nfeatures = df['text']\ntarget = df['class']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, \n                                                    target, \n                                                    random_state=42)\n\n\nprint(X_train.shape,X_test.shape)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-21T22:00:36.52959Z","iopub.execute_input":"2021-12-21T22:00:36.530171Z","iopub.status.idle":"2021-12-21T22:00:36.583249Z","shell.execute_reply.started":"2021-12-21T22:00:36.530128Z","shell.execute_reply":"2021-12-21T22:00:36.582447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create and test model/pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n# from sklearn.linear_model import PassiveAggressiveClassifier\n# from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\n\nvectorizer = CountVectorizer(ngram_range=(1,2))\nmodel = SGDClassifier()\n\npipe = Pipeline([\n    ('vectorizer', vectorizer),\n    ('model', model)\n])\n\npipe.fit(X_train, y_train)\n\ntest_score = pipe.score(X_test, y_test)\npred = pipe.predict(X_test)\nf1 = f1_score(y_test, pred, average='micro')\n\nprint('test score:', test_score)\nprint('F1 score:', f1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T22:00:36.584328Z","iopub.execute_input":"2021-12-21T22:00:36.584979Z","iopub.status.idle":"2021-12-21T22:01:27.629603Z","shell.execute_reply.started":"2021-12-21T22:00:36.584937Z","shell.execute_reply":"2021-12-21T22:01:27.628542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PassiveAggressiveClassifier: .59\n\n\nRandomForestClassifier: .64\n\n\nSGDClassifier: .69","metadata":{}},{"cell_type":"markdown","source":"## Create test set","metadata":{}},{"cell_type":"code","source":"# get list of file names\ntest_folder = '../input/feedback-prize-2021/test'\nfilenames = [f for f in listdir(test_folder) if isfile(join(test_folder, f))]\n\n# create dict for all data\nall_data = {'id':[], 'text':[], 'start_loc':[], 'end_loc':[], 'start_word':[], \n            'end_word':[], 'sentence_num':[], 'predictionstrings':[]}\n\n# loop through file names\nfor i in tqdm(range(len(filenames))):\n    \n    path = test_folder + '/' + filenames[i] #assemble file path\n\n    with open(path, 'r') as f:\n        text = f.read() #read in text\n    f.close()\n\n#     split_text = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', text) #split text into sentence\n\n    split_text = sent_tokenize(text) #split text into sentence\n    \n    string_word = 0 #set predictionstring start\n    string_loc = 0 #set char start\n\n    # loop through sentences and append data to dict lists\n    for sentence in split_text:\n        all_data['id'].append(filenames[i][:-4])\n        all_data['text'].append(sentence)\n        all_data['start_word'].append(string_word)\n        \n        # calculate string end location and update new starting loc\n        word_split = sentence.split(' ')\n        all_data['end_word'].append(string_loc+len(word_split)) \n        string_word += len(word_split)\n        \n        all_data['start_loc'].append(string_loc)\n        string_loc += len(sentence) + 1\n        all_data['end_loc'].append(string_loc)\n        \n\n        \n# create testing DF\ntest_df = pd.DataFrame(data={'id':all_data['id'], \n                             'text':all_data['text'], \n                             'start_loc':all_data['start_loc'], \n                             'end_loc':all_data['end_loc'],\n                             'start_word':all_data['start_word'],\n                             'end_word':all_data['end_word']})\n\n\n# calcuate and add predictionstrings column\nfor index, row in test_df.iterrows():\n    all_data['predictionstrings'].append(' '.join([str(i) for i in range(row.start_word, row.end_word + 1)]))\n\ntest_df['predictionstring'] = all_data['predictionstrings']\n\ntest_df['sentence_num'] = test_df.groupby('id').cumcount() + 1\ntest_df['total_sentences'] = test_df.groupby('id')['sentence_num'].transform('max')\ntest_df['sentence_location'] = round(test_df.sentence_num/test_df.total_sentences, 2) * 10\ntest_df['words_num'] = test_df['text'].str.split(' ').str.len()\ntest_df['char_num'] = test_df['text'].str.len()\n\ntest_df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T22:01:27.63122Z","iopub.execute_input":"2021-12-21T22:01:27.632166Z","iopub.status.idle":"2021-12-21T22:01:27.804084Z","shell.execute_reply.started":"2021-12-21T22:01:27.632098Z","shell.execute_reply":"2021-12-21T22:01:27.803116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making predictions","metadata":{}},{"cell_type":"code","source":"# make test preds and save output\npreds = pipe.predict(test_df['text'])\ntest_df['class'] = preds\nsubmit_df = test_df[['id','class','predictionstring']]\n# submit_df.to_csv('submission.csv', index=False)\nsubmit_df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T22:01:27.805741Z","iopub.execute_input":"2021-12-21T22:01:27.80631Z","iopub.status.idle":"2021-12-21T22:01:27.842875Z","shell.execute_reply.started":"2021-12-21T22:01:27.80626Z","shell.execute_reply":"2021-12-21T22:01:27.842106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This model scored 0.145**","metadata":{}},{"cell_type":"markdown","source":"# Checking predictions ","metadata":{}},{"cell_type":"code","source":"fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(18,4))\nsubmit_df['class'].value_counts().plot.bar(title='Sumbission prediction classes', ax=ax1)\npd.Series(pred).value_counts().plot.bar(title='Split-test prediction classes', ax=ax2)\ndf['class'].value_counts().plot.bar(title='Training classes', ax=ax3)\nplt.tight_layout()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T22:01:27.844596Z","iopub.execute_input":"2021-12-21T22:01:27.845162Z","iopub.status.idle":"2021-12-21T22:01:28.341609Z","shell.execute_reply.started":"2021-12-21T22:01:27.845112Z","shell.execute_reply":"2021-12-21T22:01:28.340631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, so too many \"claim\" predictions in the submission set. Let's look at the words again.","metadata":{}},{"cell_type":"code","source":"# # training words\n\n# stop_words.append('')\n# def bar_charter(classification, ax):\n#     all_text = ' '.join(df.loc[df.discourse_type==classification].discourse_text.str.lower().tolist())\n#     all_words = [word for word in all_text.split(' ') if word not in stop_words]\n#     pd.Series(all_words).value_counts().head(20).plot.bar(title=f'{classification} word frequency', ax=ax)\n    \n# fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,4))\n    \n# bar_charter('Claim', ax1)\n# bar_charter('Evidence', ax2)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T22:01:28.342961Z","iopub.execute_input":"2021-12-21T22:01:28.343214Z","iopub.status.idle":"2021-12-21T22:01:28.349786Z","shell.execute_reply.started":"2021-12-21T22:01:28.343185Z","shell.execute_reply":"2021-12-21T22:01:28.348772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # submission words\n\n# stop_words.append('')\n# def bar_charter(classification, ax):\n#     all_test_text = ' '.join(test_df.loc[test_df['class']==classification].text.str.lower().tolist())\n#     all_test_words = [word for word in all_test_text.split(' ') if word not in stop_words]\n#     pd.Series(all_test_words).value_counts().head(20).plot.bar(title=f'{classification} word frequency', ax=ax)\n\n    \n# fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,4))\n    \n# bar_charter('Claim', ax1)\n# bar_charter('Evidence', ax2)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:01:28.350874Z","iopub.execute_input":"2021-12-21T22:01:28.351125Z","iopub.status.idle":"2021-12-21T22:01:28.363818Z","shell.execute_reply.started":"2021-12-21T22:01:28.351095Z","shell.execute_reply":"2021-12-21T22:01:28.362939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's a lot of overlap here. \"will\", \"one\", \"help\", and \"make\" are very present in both the \"Claim\" and \"Evidence\" classifications.","metadata":{}},{"cell_type":"markdown","source":"# Retesting\nFrom the EDA earlier, it appears some classes appear more frequently in certain locations within a given essay. Let's add some features!","metadata":{}},{"cell_type":"code","source":"# select features/target and split data\nfeatures = df[[\n    'start_loc',\n    'end_loc',\n    'text',\n    'sentence_num',\n    'total_sentences',\n    'sentence_location',\n    'words_num',\n    'char_num'\n]]\n\ntarget = df['class']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=.2, random_state=42)\n\nprint(X_train.shape,y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:01:28.365084Z","iopub.execute_input":"2021-12-21T22:01:28.365324Z","iopub.status.idle":"2021-12-21T22:01:28.427353Z","shell.execute_reply.started":"2021-12-21T22:01:28.365297Z","shell.execute_reply":"2021-12-21T22:01:28.426318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression, PassiveAggressiveClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score\n\n# functions to select numeric and text data by column name\nget_text_data = FunctionTransformer(lambda x: x['text'], validate=False)\nget_numeric_data = FunctionTransformer(lambda x: x[[\n    'start_loc',\n    'end_loc',\n    'sentence_num',\n    'total_sentences',\n    'sentence_location',\n    'words_num',\n    'char_num'\n]], validate=False)\n\nscaler = StandardScaler()\n\nvectorizer = CountVectorizer(ngram_range=(1,2))\ntransformer = TfidfTransformer()\nmodel = SGDClassifier()\n\n\n# create pipeline to process and join features\npipe = Pipeline([\n    ('features', FeatureUnion([\n            ('numeric_features', Pipeline([\n                ('selector', get_numeric_data),\n                ('scaler', scaler)\n            ])),\n             ('text_features', Pipeline([\n                ('selector', get_text_data),\n                ('vectorizer', vectorizer),\n            ]))\n         ])),\n    ('clf', model)\n])\n\n# train model\npipe.fit(X_train, y_train)\n\n# test model\npred = pipe.predict(X_test)\nf1 = f1_score(y_test, pred, average='micro')\n\nprint('F1 score:', f1)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:01:28.428655Z","iopub.execute_input":"2021-12-21T22:01:28.428914Z","iopub.status.idle":"2021-12-21T22:02:03.232409Z","shell.execute_reply.started":"2021-12-21T22:01:28.428876Z","shell.execute_reply":"2021-12-21T22:02:03.231374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Much better! But how well will it work with the actual test data?","metadata":{}},{"cell_type":"code","source":"# make test preds and save output\ntest_features = test_df[[\n   'start_loc',\n   'end_loc',\n   'text',\n   'sentence_num',\n   'total_sentences',\n   'sentence_location',\n   'words_num',\n   'char_num'\n]]\n\npreds = pipe.predict(test_features)\n# probs = pipe.predict_proba(test_features)\ntest_df['class'] = preds\n# test_df['predict_prob'] = probs\n\nsubmit_df = test_df[['id','class','predictionstring']]\n\n# submit_df.to_csv('submission.csv', index=False)\nsubmit_df","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:02:03.235879Z","iopub.execute_input":"2021-12-21T22:02:03.236167Z","iopub.status.idle":"2021-12-21T22:02:03.289806Z","shell.execute_reply.started":"2021-12-21T22:02:03.236133Z","shell.execute_reply":"2021-12-21T22:02:03.289145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(18,4))\nsubmit_df['class'].value_counts().plot.bar(title='Sumbission prediction classes', ax=ax1)\npd.Series(pred).value_counts().plot.bar(title='Split-test prediction classes', ax=ax2)\ndf['class'].value_counts().plot.bar(title='Training classes', ax=ax3)\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:02:03.291009Z","iopub.execute_input":"2021-12-21T22:02:03.291821Z","iopub.status.idle":"2021-12-21T22:02:03.773189Z","shell.execute_reply.started":"2021-12-21T22:02:03.291766Z","shell.execute_reply":"2021-12-21T22:02:03.77228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks a little better, though the model is clearly classifying \"Position\" sentences as \"Rebuttal\".\n\n\n**This model with added features scored .08 (even worse)**\n\n\nSomething is still causing the test set trouble. ","metadata":{}},{"cell_type":"markdown","source":"# Unclassified text?\n\nCould there be segments of the training essays that are not classified? That might be throwing things off.\n\nLet's see if we can find out.","metadata":{}},{"cell_type":"code","source":"(df.groupby('id')['end_loc'].last()-(df.groupby('id')['char_num'].sum())).head(25)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:02:03.774492Z","iopub.execute_input":"2021-12-21T22:02:03.774749Z","iopub.status.idle":"2021-12-21T22:02:03.844098Z","shell.execute_reply.started":"2021-12-21T22:02:03.774717Z","shell.execute_reply":"2021-12-21T22:02:03.843126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like there are some unaccounted for characters.","metadata":{}},{"cell_type":"code","source":"# filenames = df['id'].unique().tolist()\n\n# train_path = '../input/feedback-prize-2021/train/'\n\n# all_text = {}\n\n# for i in tqdm(range(len(filenames))):\n    \n#     path = train_path + filenames[i] + '.txt'\n    \n#     with open(path, 'r') as f:\n#         text = f.read()\n#     f.close()\n    \n#     all_text[filenames[i]] = text\n             \n# len(all_text)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:02:03.845408Z","iopub.execute_input":"2021-12-21T22:02:03.845681Z","iopub.status.idle":"2021-12-21T22:02:03.850661Z","shell.execute_reply.started":"2021-12-21T22:02:03.845647Z","shell.execute_reply":"2021-12-21T22:02:03.849384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_dict = all_text\n\n# unused_sentences = []\n# ids = []\n# for index, row in df.iterrows():\n#     text = test_dict[row['id']][int(row['start_loc']):int(row['end_loc'])]\n#     test_dict[row['id']] = test_dict[row['id']].replace(text, '*'*len(text))\n\n#     for x in filter(None, test_dict[row['id']].split('*')): \n#         if x not in unused_sentences:\n#             if len(x.split(' ')) > 5:\n#                 unused_sentences.append(x)\n#                 ids.append(row['id'])\n\n# unused_df = pd.DataFrame({'id':ids,'unused_sentences':unused_sentences})\n# unused_df","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:02:03.852364Z","iopub.execute_input":"2021-12-21T22:02:03.852712Z","iopub.status.idle":"2021-12-21T22:02:03.863523Z","shell.execute_reply.started":"2021-12-21T22:02:03.852676Z","shell.execute_reply":"2021-12-21T22:02:03.862703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unused_df = pd.DataFrame({'id':ids, 'text':unused_sentences, 'class':'no_class'})\n# unused_df['text'] = unused_df['text'].replace('\\s+', ' ', regex=True)\n# unused_df = unused_df.loc[unused_df['text'] != ' ']\n# unused_df","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:02:03.865314Z","iopub.execute_input":"2021-12-21T22:02:03.866163Z","iopub.status.idle":"2021-12-21T22:02:03.874323Z","shell.execute_reply.started":"2021-12-21T22:02:03.866111Z","shell.execute_reply":"2021-12-21T22:02:03.873555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unused_df = pd.DataFrame(pd.read_csv('../input/text-with-no-class/no_class.csv'))\nall_classes_df = df[['id','text','class']].append(unused_df)\n# all_classes_df.to_csv('no_class.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:02:03.87621Z","iopub.execute_input":"2021-12-21T22:02:03.876913Z","iopub.status.idle":"2021-12-21T22:02:04.526435Z","shell.execute_reply.started":"2021-12-21T22:02:03.876836Z","shell.execute_reply":"2021-12-21T22:02:04.525393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select features/target and split data\nfeatures = all_classes_df['text']\ntarget = all_classes_df['class']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, \n                                                    target, \n                                                    random_state=42)\n\n\n# create and test model/pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\nfrom sklearn.linear_model import PassiveAggressiveClassifier\n# from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\n\nvectorizer = TfidfVectorizer(ngram_range=(1,2))\nmodel = PassiveAggressiveClassifier()\n\npipe = Pipeline([\n    ('vectorizer', vectorizer),\n    ('model', model)\n])\n\npipe.fit(X_train, y_train)\n\ntest_score = pipe.score(X_test, y_test)\npred = pipe.predict(X_test)\nf1 = f1_score(y_test, pred, average='micro')\n\nprint('test score:', test_score)\nprint('F1 score:', f1)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:02:04.528161Z","iopub.execute_input":"2021-12-21T22:02:04.528709Z","iopub.status.idle":"2021-12-21T22:03:24.013416Z","shell.execute_reply.started":"2021-12-21T22:02:04.528655Z","shell.execute_reply":"2021-12-21T22:03:24.012328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make test preds and save output\ntest_features = test_df['text']\n\npreds = pipe.predict(test_features)\n# probs = pipe.predict_proba(test_features)\ntest_df['class'] = preds\n# test_df['predict_prob'] = probs\n\nsubmit_df = test_df[['id','class','predictionstring']]\n\n# submit_df.to_csv('submission.csv', index=False)\nsubmit_df","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:20:22.03848Z","iopub.execute_input":"2021-12-21T22:20:22.038831Z","iopub.status.idle":"2021-12-21T22:20:22.124672Z","shell.execute_reply.started":"2021-12-21T22:20:22.038789Z","shell.execute_reply":"2021-12-21T22:20:22.123675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(18,4))\nsubmit_df['class'].value_counts().plot.bar(title='Sumbission prediction classes', ax=ax1)\npd.Series(pred).value_counts().plot.bar(title='Split-test prediction classes', ax=ax2)\ndf['class'].value_counts().plot.bar(title='Training classes', ax=ax3)\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:20:26.539878Z","iopub.execute_input":"2021-12-21T22:20:26.54021Z","iopub.status.idle":"2021-12-21T22:20:27.405247Z","shell.execute_reply.started":"2021-12-21T22:20:26.540178Z","shell.execute_reply":"2021-12-21T22:20:27.40427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submit_df['class'] = np.where(submit_df['class']=='no_class', 'Evidence', submit_df['class'])\nsubmit_df = submit_df.loc[submit_df['class'] != 'no_class']\nfig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(18,4))\nsubmit_df['class'].value_counts().plot.bar(title='Sumbission prediction classes', ax=ax1)\npd.Series(pred).value_counts().plot.bar(title='Split-test prediction classes', ax=ax2)\ndf['class'].value_counts().plot.bar(title='Training classes', ax=ax3)\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:20:28.790435Z","iopub.execute_input":"2021-12-21T22:20:28.791123Z","iopub.status.idle":"2021-12-21T22:20:29.310545Z","shell.execute_reply.started":"2021-12-21T22:20:28.79108Z","shell.execute_reply":"2021-12-21T22:20:29.308566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T22:20:32.560287Z","iopub.execute_input":"2021-12-21T22:20:32.561055Z","iopub.status.idle":"2021-12-21T22:20:32.591836Z","shell.execute_reply.started":"2021-12-21T22:20:32.56101Z","shell.execute_reply":"2021-12-21T22:20:32.591048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_kg_hide-input":true}}]}