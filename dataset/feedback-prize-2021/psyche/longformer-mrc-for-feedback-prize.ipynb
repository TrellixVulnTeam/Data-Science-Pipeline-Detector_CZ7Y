{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Question Answering (MRC)","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport logging\nimport collections\nimport numpy as np\n\nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Optional, Tuple\nfrom torch.utils.data import Dataset\nfrom transformers import Trainer\nfrom transformers.trainer_utils import PredictionOutput, EvalPrediction\nfrom datasets import Dataset, load_metric\n\n\n'''\norigin source: huggingface repository(https://github.com/huggingface/transformers/tree/master/examples/pytorch/)\n'''\n\nlogger = logging.getLogger(__name__)\n\n\nclass QuestionAnsweringTrainer(Trainer):\n    def __init__(self, *args, eval_examples=None, post_process_function=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.eval_examples = eval_examples\n        self.post_process_function = post_process_function\n        print('[init] eval_examples:',eval_examples)\n\n    def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None, metric_key_prefix: str = \"eval\"):\n        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset\n        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n        eval_examples = self.eval_examples if self.eval_examples else eval_examples\n \n        # Temporarily disable metric computation, we will do it in the loop here.\n        compute_metrics = self.compute_metrics\n        self.compute_metrics = None\n        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n        try:\n            output = eval_loop(\n                eval_dataloader,\n                description=\"Evaluation\",\n                # No point gathering the predictions if there are no metrics, otherwise we defer to\n                # self.args.prediction_loss_only\n                prediction_loss_only=True if compute_metrics is None else None,\n                ignore_keys=ignore_keys,\n            )\n        finally:\n            self.compute_metrics = compute_metrics\n\n        if self.post_process_function is not None and self.compute_metrics is not None:\n            eval_preds = self.post_process_function(eval_examples, eval_dataset, output.predictions)\n            metrics = self.compute_metrics(eval_preds)\n\n            # Prefix all keys with metric_key_prefix + '_'\n            for key in list(metrics.keys()):\n                if not key.startswith(f\"{metric_key_prefix}_\"):\n                    metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n\n            self.log(metrics)\n        else:\n            metrics = {}\n\n        if self.args.tpu_metrics_debug or self.args.debug:\n            # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n            xm.master_print(met.metrics_report())\n\n        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)\n        return metrics\n\n    def predict(self, predict_dataset, predict_examples, ignore_keys=None, metric_key_prefix: str = \"test\"):\n        predict_dataloader = self.get_test_dataloader(predict_dataset)\n\n        # Temporarily disable metric computation, we will do it in the loop here.\n        compute_metrics = self.compute_metrics\n        self.compute_metrics = None\n        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n        try:\n            output = eval_loop(\n                predict_dataloader,\n                description=\"Prediction\",\n                # No point gathering the predictions if there are no metrics, otherwise we defer to\n                # self.args.prediction_loss_only\n                prediction_loss_only=True if compute_metrics is None else None,\n                ignore_keys=ignore_keys,\n            )\n        finally:\n            self.compute_metrics = compute_metrics\n\n        if self.post_process_function is None or self.compute_metrics is None:\n            return output\n\n        predictions = self.post_process_function(predict_examples, predict_dataset, output.predictions, \"predict\")\n        metrics = self.compute_metrics(predictions)\n\n        # Prefix all keys with metric_key_prefix + '_'\n        for key in list(metrics.keys()):\n            if not key.startswith(f\"{metric_key_prefix}_\"):\n                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n\n        return PredictionOutput(predictions=predictions.predictions, label_ids=predictions.label_ids, metrics=metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocess_qa_predictions(\n    examples,\n    features,\n    predictions: Tuple[np.ndarray, np.ndarray],\n    version_2_with_negative: bool = False,\n    n_best_size: int = 20,\n    start_n_top: int = 5,\n    end_n_top: int = 5,\n    max_answer_length: int = 30,\n    null_score_diff_threshold: float = 0.0,\n    output_dir: Optional[str] = None,\n    prefix: Optional[str] = None,\n    log_level: Optional[int] = logging.WARNING,\n    id_column_name:str=\"id\",\n    eid_column_name:str=\"example_id\"\n):\n    if len(predictions) != 2:\n        raise ValueError(\"`predictions` should be a tuple with two elements (start_logits, end_logits).\")\n    all_start_logits, all_end_logits = predictions\n\n    if len(predictions[0]) != len(features):\n        raise ValueError(f\"Got {len(predictions[0])} predictions and {len(features)} features.\")\n\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples[id_column_name])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[eid_column_name]]].append(i)\n\n    # The dictionaries we have to fill.\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    if version_2_with_negative:\n        scores_diff_json = collections.OrderedDict()\n\n    # Logging.\n    logger.setLevel(log_level)\n    logger.info(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    # Let's loop over all the examples!\n    for example_index, example in enumerate(tqdm(examples)):\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n\n        min_null_prediction = None\n        prelim_predictions = []\n\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n            # available in the current feature.\n            token_is_max_context = features[feature_index].get(\"token_is_max_context\", None)\n\n            # Update minimum null prediction.\n            feature_null_score = start_logits[0] + end_logits[0]\n            if min_null_prediction is None or min_null_prediction[\"score\"] > feature_null_score:\n                min_null_prediction = {\n                    \"offsets\": (0, 0),\n                    \"score\": feature_null_score,\n                    \"start_logit\": start_logits[0],\n                    \"end_logit\": end_logits[0],\n                }\n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n                    # Don't consider answer that don't have the maximum context available (if such information is\n                    # provided).\n                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n                        continue\n                    if offset_mapping[start_index] and offset_mapping[end_index]:\n                        prelim_predictions.append(\n                            {\n                                \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n                                \"score\": start_logits[start_index] + end_logits[end_index],\n                                \"start_logit\": start_logits[start_index],\n                                \"end_logit\": end_logits[end_index],\n                            }\n                        )\n\n        if version_2_with_negative:\n            # Add the minimum null prediction\n            print(\"MIN_NULL:\", min_null_prediction)\n            prelim_predictions.append(min_null_prediction)\n            null_score = min_null_prediction[\"score\"]\n\n        # Only keep the best `n_best_size` predictions.\n        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n\n        # Add back the minimum null prediction if it was removed because of its low score.\n        if version_2_with_negative and not any(p[\"offsets\"] == (0, 0) for p in predictions):\n            predictions.append(min_null_prediction)\n\n        # Use the offsets to gather the answer text in the original context.\n        context = example[\"text\"]\n        for pred in predictions:\n            offsets = pred.pop(\"offsets\")\n            pred[\"text\"] = context[offsets[0] : offsets[1]]\n\n        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n        # failure.\n        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):\n            predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n\n        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n        # the LogSumExp trick).\n        scores = np.array([pred.pop(\"score\") for pred in predictions])\n        exp_scores = np.exp(scores - np.max(scores))\n        probs = exp_scores / exp_scores.sum()\n\n        # Include the probabilities in our predictions.\n        for prob, pred in zip(probs, predictions):\n            pred[\"probability\"] = prob\n\n        # Pick the best prediction. If the null answer is not possible, this is easy.\n        if not version_2_with_negative:\n            all_predictions[example[id_column_name]] = predictions[0][\"text\"]\n        else:\n            # Otherwise we first need to find the best non-empty prediction.\n            i = 0\n            while predictions[i][\"text\"] == \"\":\n                i += 1\n            best_non_null_pred = predictions[i]\n\n            # Then we compare to the null prediction using the threshold.\n            score_diff = null_score - best_non_null_pred[\"start_logit\"] - best_non_null_pred[\"end_logit\"]\n            scores_diff_json[example[id_column_name]] = float(score_diff)  # To be JSON-serializable.\n            if score_diff > null_score_diff_threshold:\n                all_predictions[example[id_column_name]] = \"\"\n            else:\n                all_predictions[example[id_column_name]] = best_non_null_pred[\"text\"]\n\n        # Make `predictions` JSON-serializable by casting np.float back to float.\n        all_nbest_json[example[id_column_name]] = [\n            {k: (float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v) for k, v in pred.items()}\n            for pred in predictions\n        ]\n\n    # If we have an output_dir, let's save all those dicts.\n    if output_dir is not None:\n        if not os.path.isdir(output_dir):\n            raise EnvironmentError(f\"{output_dir} is not a directory.\")\n\n        prediction_file = os.path.join(\n            output_dir, \"predictions.json\" if prefix is None else f\"{prefix}_predictions.json\"\n        )\n        nbest_file = os.path.join(\n            output_dir, \"nbest_predictions.json\" if prefix is None else f\"{prefix}_nbest_predictions.json\"\n        )\n        if version_2_with_negative:\n            null_odds_file = os.path.join(\n                output_dir, \"null_odds.json\" if prefix is None else f\"{prefix}_null_odds.json\"\n            )\n\n        logger.info(f\"Saving predictions to {prediction_file}.\")\n        with open(prediction_file, \"w\") as writer:\n            writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n        logger.info(f\"Saving nbest_preds to {nbest_file}.\")\n        with open(nbest_file, \"w\") as writer:\n            writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n        if version_2_with_negative:\n            logger.info(f\"Saving null_odds to {null_odds_file}.\")\n            with open(null_odds_file, \"w\") as writer:\n                writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n\n    return all_predictions, None\n\n\ndef get_train_example_fn(tokenizer, question_column_name='question', context_column_name='text', answer_column_name='answers', pad_on_right=True, max_seq_length=256, doc_stride=128, pad_to_max_length=True, **kwargs):\n    pad_on_right = tokenizer.padding_side == \"right\"\n    def prepare_train_features(examples):\n        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n        # left whitespace\n        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n\n        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n        # in one example possible giving several features when a context is long, each of those features having a\n        # context that overlaps a bit the context of the previous feature.\n        tokenized_examples = tokenizer(\n            examples[question_column_name if pad_on_right else context_column_name],\n            examples[context_column_name if pad_on_right else question_column_name],\n            truncation=True,\n            max_length=max_seq_length,\n            #stride=doc_stride,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            padding=\"max_length\",\n        )\n        # Since one example might give us several features if it has a long context, we need a map from a feature to\n        # its corresponding example. This key gives us just that.\n        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n        # The offset mappings will give us a map from token to character position in the original context. This will\n        # help us compute the start_positions and end_positions.\n        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n        # Let's label those examples!\n        tokenized_examples[\"start_positions\"] = []\n        tokenized_examples[\"end_positions\"] = []\n\n        for i, offsets in enumerate(offset_mapping):\n            # We will label impossible answers with the index of the CLS token.\n            input_ids = tokenized_examples[\"input_ids\"][i]\n            cls_index = input_ids.index(tokenizer.cls_token_id)\n            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n            sequence_ids = tokenized_examples.sequence_ids(i)\n            # One example can give several spans, this is the index of the example containing this span of text.\n            sample_index = sample_mapping[i]\n            answers = examples[answer_column_name][sample_index]\n            # If no answers are given, set the cls_index as answer.\n            if len(answers[\"answer_start\"]) == 0:\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Start/end character index of the answer in the text.\n                start_char = answers[\"answer_start\"][0]\n                end_char = start_char + len(answers[\"text\"][0])\n\n                # Start token index of the current span in the text.\n                token_start_index = 0\n                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                    token_start_index += 1\n\n                # End token index of the current span in the text.\n                token_end_index = len(input_ids) - 1\n                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                    token_end_index -= 1\n\n                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                    tokenized_examples[\"start_positions\"].append(cls_index)\n                    tokenized_examples[\"end_positions\"].append(cls_index)\n                else:\n                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                    # Note: we could go after the last offset if the answer is the last word (edge case).\n                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                        token_start_index += 1\n                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                    while offsets[token_end_index][1] >= end_char:\n                        token_end_index -= 1\n                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n        return tokenized_examples\n\n    return prepare_train_features\n\n\ndef get_val_example_fn(tokenizer, question_column_name='question', context_column_name='context',  answer_column_name='answers', id_column_name=\"id\", eid_column_name='example_id', pad_on_right=True, max_seq_length=256, doc_stride=128, pad_to_max_length=True):\n    pad_on_right = tokenizer.padding_side == \"right\" \n    def prepare_validation_features(examples):\n        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n        # left whitespace\n        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n\n        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n        # in one example possible giving several features when a context is long, each of those features having a\n        # context that overlaps a bit the context of the previous feature.\n        tokenized_examples = tokenizer(\n            examples[question_column_name if pad_on_right else context_column_name],\n            examples[context_column_name if pad_on_right else question_column_name],\n            truncation=True,\n            max_length=max_seq_length,\n            #stride=doc_stride,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            padding=\"max_length\",\n        )\n\n        # Since one example might give us several features if it has a long context, we need a map from a feature to\n        # its corresponding example. This key gives us just that.\n        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n        # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n        # corresponding example_id and we will store the offset mappings.\n        tokenized_examples[eid_column_name] = []\n\n        for i in range(len(tokenized_examples[\"input_ids\"])):\n            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n            sequence_ids = tokenized_examples.sequence_ids(i)\n            context_index = 1 if pad_on_right else 0\n\n            # One example can give several spans, this is the index of the example containing this span of text.\n            sample_index = sample_mapping[i]\n            tokenized_examples[eid_column_name].append(examples[id_column_name][sample_index])\n\n            # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n            # position is part of the context or not.\n            tokenized_examples[\"offset_mapping\"][i] = [\n                (o if sequence_ids[k] == context_index else None)\n                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n            ]\n\n        return tokenized_examples\n\n    return prepare_validation_features\n\n\ndef get_post_process_fn(\n        answer_column_name:str='answers',\n        version_2_with_negative:bool=False,\n        n_best_size:int=20,\n        start_n_top: int = 5,\n        end_n_top: int = 5,\n        max_answer_length:int=30,\n        null_score_diff_threshold:float=0.0,\n        output_dir:str='./',\n        log_level=logging.WARNING,\n        prefix: str='eval',\n        id_column_name:str=\"id\",\n        eid_column_name:str=\"example_id\",\n        do_beam_search:bool=False\n):\n    def post_processing_function(examples, features, predictions, stage=prefix):\n        # Post-processing: we match the start logits and end logits to answers in the original context.\n        postprocess_func = postprocess_qa_predictions\n        predictions, _ = postprocess_func(\n            examples=examples,\n            features=features,\n            predictions=predictions,\n            version_2_with_negative=version_2_with_negative,\n            n_best_size=n_best_size,\n            max_answer_length=max_answer_length,\n            null_score_diff_threshold=null_score_diff_threshold,\n            output_dir=output_dir,\n            log_level=log_level,\n            start_n_top= 5,\n            end_n_top= 5,\n            prefix=stage,\n            id_column_name=id_column_name,\n            eid_column_name=eid_column_name,\n        )\n        # Format the result to the format the metric expects.\n        if version_2_with_negative:\n            formatted_predictions = [\n                {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n            ]\n        else:\n            formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n\n        references = [{\"id\": ex[id_column_name], \"answers\": ex[answer_column_name]} for ex in examples]\n        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n\n    return post_processing_function","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ndef get_extQA_dataset(df_data):\n    map_dict={1:\"first\", 2:\"second\", 3:\"third\", 4:\"forth\", 5:\"fifth\"}\n    df_data['text'] = df_data['id'].apply(lambda _id: load_text(os.path.join(TRAIN_DIR, f'{_id}.txt')))\n    df_data['predictionstring'] = df_data['predictionstring'].apply(lambda s: list(map(int,s.split(\" \"))))\n    df_data['text'] = df_data['text'].apply(lambda x: re.sub('\\s',' ',x))\n    df_data['answer'] = [ \" \".join([t for i,t in enumerate(text.split()) if i in answer]) for text, answer in zip(df_data['text'], df_data['predictionstring'])]\n\n    starts = [context.find(answer) for context, answer in zip(df_data['text'], df_data['answer'])]\n    ends = [ sidx+len(answer) for sidx,answer in zip(starts, df_data['answer'])]\n    df_data['start_positions'],df_data['end_positions'] = starts,ends\n\n    df_data['order'] = df_data['discourse_type_num'].apply(lambda s:int(s.split()[-1]))\n    df_data['order'] = df_data['order'].apply(lambda i: map_dict[i] if i in map_dict else f\"{i}th\")\n    df_data['question'] = [ f\"What is the {order} '{d_type}' statement?\" for order,d_type in zip(df_data['order'], df_data['discourse_type'])]\n    df_data = df_data[['discourse_id', 'text', 'question', 'answer', 'start_positions', 'end_positions']]\n    df_data['answers'] = [{\"answer_start\":[sp],\"text\":[ans]} for sp, ans in zip(df_data[\"start_positions\"],df_data['answer'])]\n    df_data['id'] = df_data.index\n    dataset = Dataset.from_pandas(df_data[[\"id\", \"discourse_id\", \"text\", \"answers\", \"question\"]])\n    return dataset\n    \n\ndef qa_metric(p):\n    preds, labels = p\n    if not isinstance(preds, tuple) and not isinstance(preds, list):\n        if len(preds.shape) == 2 and preds.shape[1] == 1:\n            preds = preds[:, 0]\n        elif len(preds.shape) - len(labels.shape) == 1:\n            preds = np.argmax(preds, axis=-1)\n    return accuracy_score(labels, preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Token Classification","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom collections import defaultdict, Counter\nfrom datasets import Dataset, load_metric\nfrom sklearn.metrics import classification_report\n\n\ndef load_text(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        content = f.read()\n    return content\n\n\ndef processing_text(text, do_lower_case=False, remove_marks=False):\n    if not isinstance(text,str):\n        return \"\"\n    text = text.replace(\" .\", \". \")\n    if remove_marks:\n        text = re.sub(\"(?<=[a-zA-Z]),\\.\",\"\", text)\n    if do_lower_case:\n        #lower the case if the first character of word is only the upper case(processing for ignoring the proper nouns like 'NASA'.).\n        text = \" \".join([token.lower() if token[1:]==token[1:].lower() else token for token in text.split()])\n    return text\n\n\ndef get_train_data(df_data, text_dir, label_to_ids, do_lower_case=False, remove_marks=False, do_data_manuplating=False):\n    data = defaultdict(list)\n    for _id, df in tqdm(df_data.groupby('id')):\n        origin_text = load_text(os.path.join(text_dir,f'{_id}.txt'))\n        tokens = origin_text.split()\n        labels = [label_to_ids[\"O\"]]*len(tokens)\n        labels = np.full(len(tokens), fill_value=label_to_ids[\"O\"])\n        for _type, ids in zip(df['discourse_type'], df['predictions']):\n            labels[ids[0]], labels[ids[1:]] = label_to_ids[f\"B-{_type}\"], label_to_ids[f\"I-{_type}\"]\n        \n        data['id'].append(_id)\n        data['ner_tags'].append(labels)\n        data['tokens'].append([processing_text(token) for token in tokens])\n        \n    return Dataset.from_dict(data)\n\n\ndef get_example_fn(tokenizer, max_seq_len=1024,truncation=True, padding=\"max_length\", ignore_ids=-100):\n    def tokenize_and_align_labels(examples):\n        tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=truncation, max_length=max_seq_len, padding=padding, is_split_into_words=True)\n        if \"ner_tags\" not in examples:\n            return tokenized_inputs\n        labels = []\n        for i, label in enumerate(examples[\"ner_tags\"]):\n            word_ids = tokenized_inputs.word_ids(batch_index=i)  \n            previous_word_idx = None\n            label_ids = []\n            for word_idx in word_ids:                           \n                if word_idx is None:\n                    label_ids.append(ignore_ids)\n                elif word_idx != previous_word_idx:              \n                    label_ids.append(label[word_idx])\n\n            labels.append(label_ids)\n\n        tokenized_inputs[\"labels\"] = labels\n        return tokenized_inputs\n    return tokenize_and_align_labels\n\n\ndef sub2word(y_preds, dataset, tokenizer, skip_special_tokens=True, sub_prefix=\"▁\"):\n    outputs = []\n    for y_pred, input_ids in zip(y_preds, dataset['input_ids']):\n        filtered_tokens = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=skip_special_tokens)\n        tokens = []\n        for pred, token in zip(y_pred, filtered_tokens):\n            if token.startswith(sub_prefix):\n                tokens.append([])\n            tokens[-1].append(pred)\n#        outputs.append(np.array([ Counter(t).most_common(1)[0][0] for t in tokens]))\n        outputs.append(np.array([ t[0] for t in tokens]))\n    return np.array(outputs)\n\n\ndef get_predictions(y_preds, ids, label_to_ids):\n    labels = defaultdict(list)\n    for key, value in label_to_ids.items():\n        labels[key.split('-')[-1]].append(value)\n    return [ {_id: [(key, np.where(np.isin(pred, value))[0].tolist()) \n                for key, value in labels.items()]} \n                    for _id, pred in zip(ids, y_preds)]\n\n\ndef get_continuous_series(ids, tol=2):\n    outputs = [[ids.pop(0)]]\n    for _id in ids:\n        if outputs[-1][-1] not in list(range(_id-tol,_id+tol)):\n            outputs.append([])\n        elif outputs[-1][-1] > _id:\n            continue\n        outputs[-1].append(_id)\n    return [o for o in outputs if o]\n    \n    \ndef pred2sub(predictions):\n    sub = defaultdict(list)\n    for pred in predictions:\n        for _id,cat_ids in pred.items():\n            for cat,ids in cat_ids:\n                if (not ids) or (cat==\"O\"): # ignore OOF\n                    continue\n                for seq in get_continuous_series(ids):\n                    sub['id'].append(_id)\n                    sub['class'].append(cat)\n                    sub['predictionstring'].append(\n                        \" \".join([str(i) for i in range(seq[0], seq[-1]+1)]))\n    sub = pd.DataFrame(sub)\n#    sub = sub[sub['predictionstring'].apply(lambda x: len(x.split())) > 4]\n    \n    # Ordering for checking the final prediction outputs (whether correctly predicted or not; if \"Lead\" outcomes last position, it is incorrect.)\n    sub['order'] = sub['predictionstring'].apply(lambda s: int(s.split()[0]))\n    sub = sub.sort_values(by=['id','order'])\n    return sub.reset_index().drop(['index', 'order'], axis=1)\n\n\ndef get_all_path(path):\n    all_path = []\n    for d in os.listdir(path):\n        sub = os.path.join(path, d)\n        if os.path.isdir(sub):\n            all_path += get_all_path(sub)\n        else:\n            all_path.append(sub)\n    return all_path\n\n\ndef get_test_dataset(path, do_lower_case=False, remove_marks=False):\n    paths = get_all_path(path)\n    data = defaultdict(list)\n    for p in paths:\n        tokens = load_text(p).split()\n        data['id'].append(os.path.basename(p).replace(\".txt\", \"\"))\n        data['tokens'].append([processing_text(token) for token in tokens])\n    return Dataset.from_dict(data)\n\n\ndef token_cls_metric(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=-1)\n\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    results = [classification_report(real, pred, output_dict=True) for real,pred in zip(true_labels, true_predictions)]\n    outputs = defaultdict(list)\n    for result in results:\n        outputs['accuracy'].append(result['accuracy'])\n        outputs['f1-score(macro)'].append(result['macro avg']['f1-score'])\n        outputs['recall(macro)'].append(result['macro avg']['recall'])\n        outputs['precision(macro)'].append(result['macro avg']['precision'])\n    return {k:sum(v)/len(v) for k,v in outputs.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pipelines","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom transformers import AutoTokenizer\nfrom transformers import DataCollatorForTokenClassification\nfrom transformers import AutoModelForTokenClassification, AutoModelForQuestionAnswering,TrainingArguments, Trainer, EarlyStoppingCallback\nimport torch\n\n\nBATCH = 4\nMAX_SEQ_LEN = 1024\n\nTRAIN_DIR = '/kaggle/input/feedback-prize-2021/train'\nTRAIN = '/kaggle/input/feedback-prize-2021/train.csv'\nTEST = '/kaggle/input/feedback-prize-2021/test'\nSUB = '/kaggle/input/feedback-prize-2021/sample_submission'\n\nMODEL = '../input/longformermrctrained'\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL, add_prefix_space=True)\n\n'''\ndf_train = pd.read_csv(TRAIN)\n\n# MRC\ntorch.cuda.empty_cache()\n\ndataset = get_extQA_dataset(df_train)\ndataset = dataset.train_test_split(0.1)\n\nqa_train_fn = get_train_example_fn(\n    tokenizer=tokenizer,\n    question_column_name=\"question\",\n    context_column_name=\"text\",\n    answer_column_name=\"answers\",\n    max_seq_length=1024,\n    doc_stride=None,\n)\n\nqa_eval_fn = get_val_example_fn(\n    tokenizer=tokenizer,\n    question_column_name=\"question\",\n    context_column_name=\"text\",\n    answer_column_name=\"answers\",\n    id_column_name='id',\n    eid_column_name='discourse_id',\n    max_seq_length=1024,\n    doc_stride=-1,\n)\n\nimport random \n\ntrain_dataset = dataset['train'].map(qa_train_fn, batched=True, remove_columns=dataset['train'].column_names)\neval_dataset = dataset['test'].map(qa_eval_fn, batched=True, remove_columns=dataset['test'].column_names).select(random.sample(range(dataset['test'].num_rows), 1000))\n\nmodel = AutoModelForQuestionAnswering.from_pretrained(MODEL)\n\ntraining_args = TrainingArguments(\n    output_dir='./mrc',\n    evaluation_strategy=\"steps\",\n    save_strategy=\"steps\",\n    logging_strategy=\"steps\",\n    report_to='tensorboard',\n    learning_rate=1e-5,\n    logging_steps=3000,\n    eval_steps=3000,\n    save_steps=3000,\n    save_total_limit=3,\n    num_train_epochs=3,\n    per_device_train_batch_size=BATCH,\n    per_device_eval_batch_size=BATCH,\n    weight_decay=0.01,\n    metric_for_best_model=\"f1\",\n    load_best_model_at_end=True\n)\n\nprocess_fn = get_post_process_fn(\n        answer_column_name='answers',\n        version_2_with_negative=False,\n        n_best_size=20,\n        start_n_top = 5,\n        end_n_top = 5,\n        max_answer_length = 30,\n        null_score_diff_threshold=0.0,\n        output_dir='./',\n        log_level=logging.WARNING,\n        prefix='eval',\n        id_column_name=\"id\",\n        eid_column_name=\"discourse_id\",\n        do_beam_search=False\n)\n\n\ntrainer = QuestionAnsweringTrainer(\n    post_process_function=process_fn,\n    model=model,\n    args=training_args,\n    compute_metrics=qa_metric,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    eval_examples=dataset['test'],\n    tokenizer=tokenizer,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\n\ntrainer.train()\ntrainer.save_model('./mrc-trained/')\n\nMODEL = './mrc-trained/'\n'''\n# Token Classification\ntorch.cuda.empty_cache()\n\ndf_train = pd.read_csv(TRAIN)\ndf_train['predictions']  = df_train['predictionstring'].apply(lambda s:list(map(int, s.split())))\n\nlabel_list = [\"O\"]+[f\"{prefix}-{name}\" for name in sorted(df_train.discourse_type.unique()) for prefix in (\"B\", \"I\")]\nlabel_to_ids = {label:i for i,label in enumerate(label_list)}\n\ndataset = get_train_data(df_train, TRAIN_DIR, label_to_ids, do_lower_case=False, remove_marks=False)\n\nexample_func = get_example_fn(\n    tokenizer=tokenizer, \n    max_seq_len=MAX_SEQ_LEN,\n    truncation=True, \n    padding=\"max_length\", \n    ignore_ids=-100\n)\n\ndataset = dataset.map(example_func, batched=True)\n\ntest_dataset = get_test_dataset(TEST, do_lower_case=True, remove_marks=True)\ntest_dataset = test_dataset.map(example_func, batched=True)\n\ndataset = dataset.train_test_split(0.1)\ndata_collator = DataCollatorForTokenClassification(tokenizer)\n\n\nmodel = AutoModelForTokenClassification.from_pretrained(MODEL, num_labels=len(label_list))\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"steps\",\n    save_strategy=\"steps\",\n    logging_strategy=\"steps\",\n    report_to='tensorboard',\n    learning_rate=1e-5,\n    logging_steps=3000,\n    max_steps=100, # For submission\n    eval_steps=3000,\n    save_steps=3000,\n    save_total_limit=4,\n    num_train_epochs=5, # setting for inference from checkpoint\n    per_device_train_batch_size=BATCH,\n    per_device_eval_batch_size=BATCH,\n    weight_decay=0.01,\n    metric_for_best_model=\"f1-score(macro)\",\n    load_best_model_at_end=True\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    compute_metrics=token_cls_metric,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\ntrainer.train()\ntrainer.save_model(\"best/\")\ntrainer.evaluate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predictions","metadata":{}},{"cell_type":"code","source":"preds = trainer.predict(test_dataset)\npreds = np.argmax(preds.predictions, axis=-1)\n# bigbird - \"▁\", longformer - \"Ġ\"\npreds = sub2word(preds, test_dataset, tokenizer, skip_special_tokens=True, sub_prefix=\"Ġ\")\npreds = get_predictions(preds,test_dataset['id'], label_to_ids)\ndf_sub = pred2sub(preds)\ndf_sub.to_csv('submission.csv', index=False)\ndf_sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}