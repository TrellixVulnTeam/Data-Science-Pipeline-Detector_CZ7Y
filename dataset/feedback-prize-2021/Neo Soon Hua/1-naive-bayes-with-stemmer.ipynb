{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Dec 17 17:06:34 2021\n\n@source1: https://www.kaggle.com/julian3833/feedback-baseline-sentence-classifier-0-226\n\n@source2: https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n\"\"\"\n\n##Imports\nimport os\nimport nltk #Natural Language Toolkit\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm #To display smart progress bars that show the progress of your Python code execution\n\nfrom datasets import Dataset\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\n\n# Constants\nTRAIN_CSV = \"../input/feedback-prize-2021/train.csv\"\nSUB_CSV = \"../input/feedback-prize-2021/sample_submission.csv\"\nTRAIN_PATH = \"../input/feedback-prize-2021/train\"\nTEST_PATH = \"../input/feedback-prize-2021/test\"\n\n# Load DF\ndf = pd.read_csv(TRAIN_CSV, dtype={'discourse_id': int, 'discourse_start': int, 'discourse_end': int})\ndf.head()\n\n# No nulls\ndf.isnull().sum()\n\ndef get_text(a_id):\n    a_file = f\"{TRAIN_PATH}/{a_id}.txt\"\n    with open(a_file, \"r\") as fp:\n        txt = fp.read()\n    return txt\n\n# Files in train path: 15595\n!ls -l {TRAIN_PATH} | wc -l\n\n# Files in test path: 6\n!ls -l {TEST_PATH} | wc -l\n\n##Sentence Classifier with HuggingFace\n#Create a sentence classification datasety\n\"\"\"As far as I know, this problem is not trivially mapped to one of the \"typical\" NLP tasks. It might be close to NER / POS, but the fact that the entities are large makes me doubt about it.\n\nI'm looking forward for the community discussion about the different possible approaches to this problem.\n\nAlthough I might be missing something very obvious, this notebook proposes the following approach, that is a multiclass classifier:\n\n    Split the texts into sentences (x)\n    Assign each sentence a class (y).\n    Train a normal sequence classifier on those sentences\n\nThere are 7 classes and the labeled sections (sometimes) exceed sentences. We will preprocess them to have only sentences. That way, we avoid the problem of detecting when a element starts and when it ends for now.\"\"\"\n\n# There are 7 classes:\ndf['discourse_type'].value_counts(normalize=True)\n\n#Encode classes as ints\n\"\"\"Some sections don't belong to any class. We will label them as No Class so we can discard those sections and avoid false positives.\"\"\"\n\nID2CLASS = dict(enumerate(df['discourse_type'].unique().tolist() + ['No Class']))\nCLASS2ID = {v: k for k, v in ID2CLASS.items()}\nprint(ID2CLASS)\nCLASS2ID\n\n##Dataset functions: fill_gaps(), get_elements(), and get_x_samples()\ntext_ids = df['id'].unique().tolist()\n\n#fill_gaps function, wrapping up the above cells\n\ndef fill_gaps(elements, text):\n    \"\"\"Add \"No Class\" elements to a list of elements (see get_elements) \"\"\"\n    initial_idx = 0\n    final_idx = len(text)\n\n    # Add element at the beginning if it doesn't in index 0\n    new_elements = []\n    if elements[0][0] != initial_idx:\n        starting_element = (0, elements[0][0]-1, 'No Class')\n        new_elements.append(starting_element)\n\n    # Add element at the end if it doesn't in index \"-1\"\n    if elements[-1][1] != final_idx:\n        closing_element = (elements[-1][1]+1, final_idx, 'No Class')\n        new_elements.append(closing_element)\n\n    elements += new_elements\n    elements = sorted(elements, key=lambda x: x[0])\n\n    # Add \"No class\" elements inbetween separated elements \n    new_elements = []\n    for i in range(1, len(elements)-1):\n        if elements[i][0] != elements[i-1][1] + 1 and elements[i][0] != elements[i-1][1]:\n            new_element = (elements[i-1][1] + 1, elements[i][0]-1, 'No Class')\n            new_elements.append(new_element)\n\n    elements += new_elements\n    elements = sorted(elements, key=lambda x: x[0])\n    return elements\n\n\ndef get_elements(df, text_id, do_fill_gaps=True, text=None):\n    \"\"\"Get a list of (start, end, class) elements for a given text_id\"\"\"\n    text = get_text(text_id) if text is None else text\n    df_text = df[df['id'] == text_id]\n    elements = df_text[['discourse_start', 'discourse_end', 'discourse_type']].to_records(index=False).tolist()\n    if do_fill_gaps:\n        elements = fill_gaps(elements, text)\n    return elements\n\ndef get_x_samples(df, text_id, do_fill_gaps=True):\n    \"\"\"Create a dataframe of the sentences of the text_id, with columns text, label \"\"\"\n    text = get_text(text_id)\n    elements = get_elements(df, text_id, do_fill_gaps, text)\n    sentences = []\n    for start, end, class_ in elements:\n        elem_sentences = nltk.sent_tokenize(text[start:end])\n        sentences += [(sentence, class_) for sentence in elem_sentences]\n    df = pd.DataFrame(sentences, columns=['text', 'label'])\n    df['label'] = df['label'].map(CLASS2ID)\n    return df\n\nget_x_samples(df, text_ids[1])\n\n##Build the full dataframe for sentence classification\n\"\"\"This takes a while. I created a dataset with the output here: https://www.kaggle.com/julian3833/feedback-df-sentences\"\"\"\n#x = []\n#for text_id in tqdm(text_ids):\n#    x.append(get_x_samples(df, text_id))\n\n#df_sentences = pd.concat(x)\n\ndf_sentences = pd.read_csv(\"../input/feedback-df-sentences/df_sentences.csv\")\n\ndf_sentences = df_sentences[df_sentences.text.str.split().str.len() >= 3]\ndf_sentences.head()\n\ndf_sentences.to_csv(\"df_sentences.csv\", index=False)\n\nlen(df_sentences)\n\n##Modeling!!!\n\"\"\"We will use a BERT and the Trainer API from Hugging Face.\n\nWe are using a dataset to avoid using internet (a restriction of the competition for submission notebooks)\n\nReferences:\n\n    https://huggingface.co/docs/transformers/training\n    https://huggingface.co/docs/transformers/custom_datasets\"\"\"\n    \nMODEL_CHK = \"../input/huggingface-bert/bert-base-cased\"\n\nNUM_LABELS = 8\n\nNUM_EPOCHS = 2\n\n##HuggingFace Dataset\nds_train = Dataset.from_pandas(df_sentences.iloc[:340000]) #sh?? Should randomise this splitting?\nds_val = Dataset.from_pandas(df_sentences.iloc[340000:])\n\n##Tokenize\ntransformers.logging.set_verbosity_warning() # Silence some annoying logging of HF\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_CHK)\n\ndef preprocess_function(examples):    \n    return tokenizer(examples[\"text\"], truncation=True, max_length=256)\n\n\n# Tokenizer dataset\nds_train_tokenized = ds_train.map(preprocess_function, batched=True)\nds_val_tokenized = ds_val.map(preprocess_function, batched=True)\n\n##Extracting features from text files\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nimport nltk\n# nltk.download()\n\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(\"english\", ignore_stopwords=False)\n\nclass StemmedCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n    \nstemmed_count_vect = StemmedCountVectorizer()\n\ntext_mnb_stemmed = Pipeline([('vect', stemmed_count_vect),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', MultinomialNB()),\n])\nmodel = text_mnb_stemmed.fit(df_sentences[\"text\"], df_sentences[\"label\"])\n\n##Submit\n\"\"\"We will apply a process similar to the one we applied to the original train data, splitting each text into its sentences.\n\nSee the Evaluation tab for details about the predictionstring column.\"\"\"\n\ndf_sub = pd.read_csv(SUB_CSV)\ndf_sub\n\n##Prepare test dataset\ndef get_test_text(a_id):\n    a_file = f\"{TEST_PATH}/{a_id}.txt\"\n    with open(a_file, \"r\") as fp:\n        txt = fp.read()\n    return txt\n\ndef create_df_test():\n    test_ids = [f[:-4] for f in os.listdir(TEST_PATH)] #Remove the last 4 characters ('.txt') in the filenames such as '0FB0700DAF44.txt'.\n    test_data = []\n    for test_id in test_ids:\n        text = get_test_text(test_id)\n        sentences = nltk.sent_tokenize(text)\n        id_sentences = []\n        idx = 0 \n        for sentence in sentences:\n            id_sentence = []\n            words = sentence.split()\n            # I created this heuristic for mapping words in sentences to \"word indices\"\n            # This is not definitive and might have strong drawbacks and problems\n            for w in words:\n                id_sentence.append(idx)\n                idx+=1\n            id_sentences.append(id_sentence)\n        test_data += list(zip([test_id] * len(sentences), sentences, id_sentences))\n    df_test = pd.DataFrame(test_data, columns=['id', 'text', 'ids'])\n    return df_test\n\ndf_test = create_df_test()\ndf_test.head()\n\n##Predict\n# Get the predictions!!\ntest_predictions = text_mnb_stemmed.predict(df_test[\"text\"])\n\ntest_predictions.shape #(187,)\nprint(test_predictions)\n    \nfrom collections import Counter\ncount_test_predictions = Counter(test_predictions)\nprint(count_test_predictions)\n\ndf_test['predictions'] = test_predictions\n\n# Turn class ids into class labels\ndf_test['class'] = df_test['predictions'].map(ID2CLASS)\ndf_test.head()\n\n\"\"\"For now, we are submitting one row per sentence and not \"elements\".\n\nHow to convert sentences into \"elements\" (blocks of setences) is not clear since there are times when various sentences with the same class are flagged in independent \"elements\".\"\"\"\n\n# Turn the word ids into this weird predictionstring required\ndf_test['predictionstring'] = df_test['ids'].apply(lambda x: ' '.join([str(i) for i in x]))\ndf_test.head()\n\n# Drop \"No class\" sentences\ndf_test = df_test[df_test['class'] != 'No Class']\ndf_test.head()\n\n# Create file for submitting!! 🤞🤞 \ndf_test[['id', 'class', 'predictionstring']].to_csv(\"submission.csv\", index=False)\n\n\"\"\"100%\n340/340 [00:32<00:00, 9.79ba/s]\n100%\n9/9 [00:00<00:00, 12.34ba/s]\n\nSome weights of the model checkpoint at ../input/huggingface-bert/bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at ../input/huggingface-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, text.\n***** Running training *****\n  Num examples = 340000\n  Num Epochs = 2\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 21250\n\n[21250/21250 1:17:16, Epoch 2/2]\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-12-18T15:45:31.526475Z","iopub.execute_input":"2021-12-18T15:45:31.527245Z","iopub.status.idle":"2021-12-18T15:45:31.862591Z","shell.execute_reply.started":"2021-12-18T15:45:31.52721Z","shell.execute_reply":"2021-12-18T15:45:31.861729Z"},"trusted":true},"execution_count":null,"outputs":[]}]}