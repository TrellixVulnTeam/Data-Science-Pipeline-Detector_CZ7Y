{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### There are no lstm and pca features in this training code, because these two features are too cumbersome and only improve by 0.001\n##### Also, for memory reasons, you need to copy to your own machine for training","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:53:13.124562Z","iopub.execute_input":"2022-03-16T02:53:13.124899Z","iopub.status.idle":"2022-03-16T02:53:13.149045Z","shell.execute_reply.started":"2022-03-16T02:53:13.124808Z","shell.execute_reply":"2022-03-16T02:53:13.14815Z"}}},{"cell_type":"markdown","source":"## recall data","metadata":{}},{"cell_type":"code","source":"import pickle\nimport math\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\ndf = pickle.load(open('./data/data_6model_offline712_online704_ensemble.pkl','rb'))\n\ntrain_df = pd.read_csv('./data/train.csv')\nIDS = train_df.id.unique()\n\ndic_off_map = df[['id','offset_mapping']].set_index('id')['offset_mapping'].to_dict()\ndic_txt = df[['id','text']].set_index('id')['text'].to_dict()\n\nclass CONFIG:\n    def __init__(self):\n        self.max_length = 4096\n        \nconfig = CONFIG()\n\nid2label = {0:'Lead', 1:'Position', 2:'Evidence', 3:'Claim', 4:'Concluding Statement',\n             5:'Counterclaim', 6:'Rebuttal', 7:'blank'}\nlabel2id = {v:k for k,v in id2label.items()}\n\ndef change_label(x):\n    res1  = x[:,8:].sum(axis=1)\n    res2 = np.zeros((len(res1), 8))\n    \n    label_map = {0:5, 1:3, 2:2, 3:1, 4:4, 5:6, 6:7, 7:0}\n    for i in range(8):\n        if i == 7:\n            res2[:,i] = x[:,label_map[i]]\n        else:\n            res2[:,i] = x[:,[label_map[i], label_map[i]+7]].sum(axis=1)\n\n    return res1, res2\n\npreds1_mean = {}\npreds2_mean = {}\nfor irow,row in df.iterrows():\n    t1, t2 = change_label(row.pred)\n    preds1_mean[row.id] = t1\n    preds2_mean[row.id] = t2\n\nall_predictions = []\n\nrecall_thre = { \n    \"Lead\": 0.07,\n    \"Position\": 0.06,\n    \"Evidence\": 0.07,\n    \"Claim\": 0.06,\n    \"Concluding Statement\": 0.07,\n    \"Counterclaim\": 0.03,\n    \"Rebuttal\": 0.02,\n}\n\nfor id in tqdm(preds1_mean):\n\n    pred1_np = np.array(preds1_mean[id])\n    pred2_np_all = np.array(preds2_mean[id])\n\n    off_map = dic_off_map[id]\n    off_map_len = len(off_map) if off_map[-1][1] != 0 else len(off_map)-1\n    max_length = min(config.max_length, off_map_len)\n    for class_num in range(7):\n        thre = recall_thre[id2label[class_num]]\n        pred2_np = pred2_np_all[:, class_num]\n        \n        i_start = 0\n        while i_start < max_length:\n            i = 0\n            if pred1_np[i_start] > thre and pred2_np[i_start:i_start+10].max() > thre: #开头 两个阈值\n                i = i_start + 1\n                if i>=max_length: break\n                while pred1_np[i] < (1-thre) and pred2_np[i:i+10].max() > thre: # 是否结束 两个阈值\n                    cond = any([\n                        i+1==max_length,\n                        pred1_np[i] > thre,\n                        i+1<max_length and pred2_np[i] < 0.7 and pred2_np[i] - pred2_np[i+1] > thre\n                    ])\n                    if i>i_start+1 and cond:\n                        all_predictions.append((id, id2label[class_num], [i_start, i]))\n                    i += 1\n                    if i>=max_length: break\n            \n            if i != 0:\n                if i == max_length:\n                    i -=1\n\n                all_predictions.append((id, id2label[class_num], [i_start, i]))\n            i_start += 1\n\nprint(len(all_predictions))\nvalid_pred = pd.DataFrame(all_predictions, columns=['id', 'class', 'pos'])\n\npredictionstring = []\nfor cache in tqdm(valid_pred.values):\n    id = cache[0]\n    pos = cache[2]\n    off_map = dic_off_map[id]\n    txt = dic_txt[id]\n    txt_max = len(txt.split())\n    \n    start_word = len(txt[:off_map[pos[0]][0]].split())\n    \n    L = len(txt[off_map[pos[0]][0]:off_map[pos[1]][1]].split())\n    end_word = min(txt_max, start_word+L) - 1\n    \n    predictionstring.append((start_word, end_word))\n    \nvalid_pred['predictionstring'] = predictionstring\n\n\nL_k = {\n    \"Evidence\": 0.85,\n    \"Rebuttal\": 0.6,\n}\n\ndef deal_predictionstring(df):\n    new_predictionstring = []\n    new_pos_list = []\n    flag_list = []\n    thre = 0.75\n    for id, typ, pos, (start, end) in tqdm(df.values):\n        flag = 0\n        L = round(max(1, (pos[1]-pos[0]+1)*0.25))\n\n        pos_left = max(0, pos[0]-L)\n        pos_right = min(len(preds1_mean[id]), pos[1]+1+L)\n\n        if start<10:\n            left_thre = 2\n        else:\n            left_thre = max(preds1_mean[id][pos[0]], 1-preds2_mean[id][pos_left:pos[0],label2id[typ]].min())\n        \n        if pos[1] >= len(preds1_mean[id])-10:\n            right_thre=2\n        else:\n            right_thre = max(preds1_mean[id][pos[1]+1:pos_right].max(), 1-preds2_mean[id][pos[1]+1:pos_right, label2id[typ]].min())\n        \n        if left_thre>thre and right_thre>thre:\n\n            L = math.ceil((pos[1]-pos[0]+1)*L_k.get(typ, 0.65))\n\n            tmp = {}\n            for i in range(pos[0], pos[1]):\n                if i+L>pos[1]:\n                    break\n                tmp[i] = np.sum(preds2_mean[id][i:i+L+1,label2id[typ]])\n            if len(tmp)==0:\n                new_pos = pos\n            else:\n                flag = min(left_thre, right_thre)\n\n                new_start = max(tmp.keys(), key=lambda x:tmp[x])\n                new_pos = (new_start,new_start+L)\n\n        else:\n            new_pos = pos\n\n        off_map = dic_off_map[id]\n        txt = dic_txt[id]\n        txt_max = len(txt.split())\n\n        start_word = len(txt[:off_map[new_pos[0]][0]].split())\n\n        L = len(txt[off_map[new_pos[0]][0]:off_map[new_pos[1]][1]].split())\n        end_word = min(txt_max, start_word+L) - 1\n\n        new_predictionstring.append((start_word, end_word))\n        new_pos_list.append(new_pos)\n        flag_list.append(flag)\n        \n    df_new = df.copy()\n    df_new['pos'] = new_pos_list\n    df_new['predictionstring'] = new_predictionstring\n    df_new['flag'] = flag_list\n    \n    df_new = pd.concat([df_new, df.loc[df_new[(df_new.flag>=thre) & (df_new.flag<0.95)].index]])\n    df_new = df_new.reset_index(drop=True)\n    df_new['flag'].fillna(0,inplace=True)\n    \n    return df_new\n\n\nvalid_pred = deal_predictionstring(valid_pred)\nvalid_oof = train_df.copy()\ntmp = valid_oof.predictionstring.map(lambda x:x.split())\ntmp1 = [(int(x[0]),int(x[-1])) for x in tmp]\nvalid_oof['predictionstring'] = tmp1\n\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    try:\n        start_pred, end_pred = row.predictionstring_pred\n        start_gt, end_gt = row.predictionstring_gt\n    except:\n        return [0,0]\n\n    # Length of each and intersection\n    len_gt = end_gt - start_gt + 1\n    len_pred = end_pred - start_pred + 1\n    inter = min(end_pred, end_gt) - max(start_pred, start_gt) + 1\n    overlap_1 = inter / (len_gt+1e-5)\n    overlap_2 = inter / (len_pred+1e-5)\n    return [overlap_1, overlap_2]\n\n\ngt_df = (\n    valid_oof[[\"id\", \"discourse_type\", \"predictionstring\"]]\n    .reset_index(drop=True)\n    .copy()\n)\npred_df = valid_pred[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\npred_df[\"pred_id\"] = pred_df.index\ngt_df[\"gt_id\"] = gt_df.index\n# Step 1. all ground truths and predictions for a given class are compared.\njoined = pred_df.merge(\n    gt_df,\n    left_on=[\"id\", \"class\"],\n    right_on=[\"id\", \"discourse_type\"],\n    how=\"outer\",\n    suffixes=(\"_pred\", \"_gt\"),\n)\njoined[\"predictionstring_gt\"] = joined[\"predictionstring_gt\"].fillna(\" \")\njoined[\"predictionstring_pred\"] = joined[\"predictionstring_pred\"].fillna(\" \")\n\njoined[\"overlaps\"] = joined.apply(calc_overlap, axis=1)\njoined[\"overlap1\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[0])\njoined[\"overlap2\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[1])\n\njoined[\"potential_TP\"] = (joined[\"overlap1\"] >= 0.5) & (joined[\"overlap2\"] >= 0.5)\njoined[\"max_overlap\"] = joined[[\"overlap1\", \"overlap2\"]].max(axis=1)\njoined[\"min_overlap\"] = joined[[\"overlap1\", \"overlap2\"]].min(axis=1)\n\nvalid_pred['label'] = 0\nvalid_true_id = joined[joined.potential_TP==True]['pred_id']\n\nvalid_pred.loc[valid_true_id, 'label'] = 1\n\noverlap = joined[['pred_id', 'min_overlap']]\noverlap = overlap[~ overlap.pred_id.isna()]\noverlap = overlap.groupby('pred_id')['min_overlap'].max().reset_index()\n\nvalid_pred = valid_pred.merge(overlap, left_index=True, right_on='pred_id', how='left')\nvalid_pred = valid_pred.drop('pred_id',axis=1)\n\npickle.dump(valid_pred, open('./data/recall_data.pkl','wb+'))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## lgb train","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python\n# coding: utf-8\n\nimport os\nimport gc\nimport copy\nimport time\nimport random\nimport string\nimport json\nimport pickle\nimport re\nimport math\nfrom numba import jit\n\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n# Utils\nfrom tqdm import tqdm\nfrom collections import defaultdict, Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\n\nfrom multiprocessing import Pool\nfrom joblib import Parallel, delayed\n\nfrom util import *\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport logging\nlogging.basicConfig(level = logging.INFO,format = \"%(asctime)s - %(message)s\", datefmt=\"%m/%d %H:%M:%S\")\nlogging.info(\"code begin!!!!!!!\")\n\nmodel_name = time.strftime('%d_%H_%M_%S_',time.localtime(time.time()))+str(random.randint(0,1000))\nlogging.info(f\"====== model_name: {model_name} ======\")\n\nnum_jobs = 60\n\ndata_path  = './data/'\n\ntrain_df = pd.read_csv(data_path+'train.csv')\nIDS = train_df.id.unique()\n\nid2label = {0:'Lead', 1:'Position', 2:'Evidence', 3:'Claim', 4:'Concluding Statement',\n             5:'Counterclaim', 6:'Rebuttal', 7:'blank'}\nlabel2id = {v:k for k,v in id2label.items()}\n\n\ndf_wf = pickle.load(open('./data/data_6model_offline712_online704_ensemble.pkl','rb'))\n\ndic_txt_feat = pickle.load(open('./data/dic_txt_feat.pkl','rb'))\ndic_off_map = df_wf[['id','offset_mapping']].set_index('id')['offset_mapping'].to_dict()\ndic_txt = df_wf[['id','text']].set_index('id')['text'].to_dict()\n\ndef change_label(x):\n    res1  = x[:,8:].sum(axis=1)\n    res2 = np.zeros((len(res1), 8))\n    \n    label_map = {0:5, 1:3, 2:2, 3:1, 4:4, 5:6, 6:7, 7:0}\n    for i in range(8):\n        if i == 7:\n            res2[:,i] = x[:,label_map[i]]\n        else:\n            res2[:,i] = x[:,[label_map[i], label_map[i]+7]].sum(axis=1)\n\n    return res1, res2\n\npreds1_5fold = {}\npreds2_5fold = {}\nfor irow,row in df_wf.iterrows():\n    t1, t2 = change_label(row.pred)\n    preds1_5fold[row.id] = t1\n    preds2_5fold[row.id] = t2\n    \n\nvalid_pred = pickle.load(open('./data/recall_data.pkl','rb'))\nkfold_ids = pickle.load(open('./data/kfold_ids.pkl','rb'))\n\nlogging.info(f'valid_pred num:{len(valid_pred)}')\n\npreds2_5fold_type = {}\nfor k,t in preds2_5fold.items():\n    preds2_5fold_type[k] = np.array(t).argmax(axis=-1)\n\n\n@jit(nopython=True)\ndef feat_speedup(arr):\n    r_max, r_min, r_sum = -1e5,1e5,0\n    for x in arr:\n        r_max = max(r_max, x)\n        r_min = min(r_min, x)\n        r_sum += x\n    return r_max, r_min, r_sum, r_sum/len(arr)\n\nnp_lin = np.linspace(0,1,7)\n\n@jit(nopython=True)\ndef sorted_quantile(array, q):\n    n = len(array)\n    index = (n - 1) * q\n    left = int(index)\n    fraction = index - left\n    right = left\n    right = right + int(fraction > 0)\n    i, j = array[left], array[right]\n    return i + (j - i) * fraction\n\ndef get_percentile(array):\n    x = np.sort(array)\n    n = len(x)-1\n    return x[[int(n*t) for t in np_lin[1:-1]]]\n\n\ndef fun_get_feat(data_sub):\n    df_feat = []\n    for cache in tqdm(data_sub):\n        id = cache[0]\n        typ = cache[1]\n        start, end = cache[2]\n        prediction = cache[3]\n\n        dic={'id': id, 'label':cache[5], 'label_rate':max(0,cache[6])}\n        dic['class'] = label2id[typ]\n        dic['post_flag'] = cache[4]\n#         dic['cluster'] = dic_cluster[id]\n\n        txt = dic_txt[id]\n\n        txt_feat  = dic_txt_feat[id]\n        dic['paragraph_cnt'] = txt_feat[0]\n        dic['sentence_cnt'] = txt_feat[1]\n        dic['paragraph_rk'] = txt_feat[2][start]\n        dic['paragraph_rk_r'] = txt_feat[3][end]\n        dic['sentence_rk'] = txt_feat[4][start]\n        dic['sentence_rk_r'] = txt_feat[5][end]\n        dic['sentence_cnt_of_paragraph'] = txt_feat[6][start]\n        dic['sentence_cnt_of_paragraph2'] = txt_feat[6][end]\n        dic['sentence_rk_of_paragraph'] = txt_feat[7][start]\n        dic['sentence_rk_r_of_paragraph'] = txt_feat[8][end]\n        dic['sub_paragraph_cnt'] = txt_feat[2][end] - txt_feat[2][start]\n        dic['sub_sentence_cnt'] = txt_feat[4][end] - txt_feat[4][start]\n\n        \n        other_type = [t for t in range(8) if t != dic['class']]\n        preds1_all = np.array(preds1_5fold[id])\n        preds2_all = np.array(preds2_5fold[id])[:,label2id[typ]]\n        preds4_all = np.array(preds2_5fold[id])[:,other_type].max(axis=-1)\n        preds1 = preds1_all[start:end+1]\n        preds2 = preds2_all[start:end+1]\n        preds4 = preds4_all[start:end+1]\n\n        word_length = prediction[-1] - prediction[0] + 1\n        token_length = len(dic_off_map[id])\n        \n        dic['L1'] = word_length\n        dic['L2'] = end - start + 1\n        dic['text_char_length'] = len(txt)\n        dic['text_word_length'] = len(txt.split())\n        dic['text_token_length'] = token_length\n\n        dic['word_start'] = prediction[0]\n        dic['word_end'] = prediction[-1]\n        dic['token_start'] = start\n        dic['token_start2'] = start / token_length\n        dic['token_end'] = end\n        dic['token_end2'] = token_length - end\n        dic['token_end3'] = end / token_length\n        \n        dic[f'head_preds1'] = preds1[0]\n        dic[f'head2_preds1'] = preds1_all[start-1:start+2].sum()\n        if len(preds1) > 1:\n            dic[f'tail_preds1'] = preds1[-1]\n            dic['max_preds1'], dic['min_preds1'], dic['sum_preds1'], dic['mean_preds1'] = feat_speedup(preds1[1:])\n      \n        sort_idx = preds1[1:].argsort()[::-1]\n        tmp = []\n        for i in range(5):\n            if i < len(sort_idx):\n                dic[f'other_preds1_{i}'] = preds1[1+sort_idx[i]]\n                dic[f'other_preds1_idx_{i}'] = (1+sort_idx[i])/len(preds1)\n                tmp.append(preds1[1+sort_idx[i]])\n                    \n        if len(tmp):\n            dic[f'other_preds1_mean'] = np.mean(tmp)\n\n        dic[f'head_preds2'] = preds2[0]\n        dic[f'tail_preds2'] = preds2[-1]\n        dic['max_preds2'], dic['min_preds2'], dic['sum_preds2'], dic['mean_preds2'] = feat_speedup(preds2)\n\n        dic[f'head_preds4'] = preds4[0]\n        dic[f'tail_preds4'] = preds4[-1]\n        dic['max_preds4'], dic['min_preds4'], dic['sum_preds4'], dic['mean_preds4'] = feat_speedup(preds4)\n\n        sort_idx = preds2.argsort()\n        tmp = []\n        for i in range(5):\n            if i < len(sort_idx):\n                dic[f'other_preds2_{i}'] = preds2[sort_idx[i]]\n                dic[f'other_preds2_idx_{i}'] = (sort_idx[i])/len(preds2)\n                tmp.append(preds2[sort_idx[i]])\n        if len(tmp):\n            dic[f'other_preds2_mean'] = np.mean(tmp)\n\n            \n        for i,ntile in enumerate([sorted_quantile(preds2,i) for i in np_lin]):\n            dic[f'preds2_trend{i}'] = ntile\n        for i,ntile in enumerate(get_percentile(preds2)):\n            dic[f'preds2_ntile{i}'] = ntile\n        for i,ntile in enumerate([sorted_quantile(preds4,i) for i in np_lin]):\n            dic[f'preds4_trend{i}'] = ntile\n        for i,ntile in enumerate(get_percentile(preds4)):\n            dic[f'preds4_ntile{i}'] = ntile\n            \n        for i in range(1,4):\n            if start-i >= 0:\n                dic[f'before_head2_prob{i}'] = preds2_all[start-i]\n                dic[f'before_other_prob{i}'] = preds4_all[start-i]\n                dic[f'before_other_type{i}'] = preds2_5fold_type[id][start-i]\n                \n            if end+i < len(preds1_all):\n                dic[f'after_head2_prob{i}'] = preds2_all[end+i]\n                dic[f'after_other_prob{i}'] = preds4_all[end+i]\n                dic[f'after_other_type{i}'] = preds2_5fold_type[id][end+i]\n\n        \n        for mode in ['before', 'after']:\n            for iw, extend_L in enumerate([math.ceil(word_length/2), word_length]):\n                if mode == 'before':\n                    if start-extend_L<0:\n                        continue\n                    preds1_extend = preds1_all[start-extend_L:start]\n                    preds2_extend = preds2_all[start-extend_L:start]\n                else:\n                    if end+extend_L >=len(preds1_all):\n                        continue\n                    preds1_extend = preds1_all[end+1:end+extend_L]\n                    preds2_extend = preds2_all[end+1:end+extend_L]\n                    \n                if len(preds1_extend) == 0:\n                    continue\n                dic[f'{mode}{iw}_head_preds1'] = preds1_extend[0]\n                dic[f'{mode}{iw}_max_preds1'], dic[f'{mode}{iw}_min_preds1'], \\\n                dic[f'{mode}{iw}_sum_preds1'], dic[f'{mode}{iw}_mean_preds1'] = feat_speedup(preds1_extend)\n\n                dic[f'{mode}{iw}_head_preds2'] = preds2_extend[0]\n                dic[f'{mode}{iw}_max_preds2'], dic[f'{mode}{iw}_min_preds2'], \\\n                dic[f'{mode}{iw}_sum_preds2'], dic[f'{mode}{iw}_mean_preds2'] = feat_speedup(preds2_extend)\n\n                dic[f'{mode}{iw}_sum_preds1_rate'] = dic[f'{mode}{iw}_sum_preds1'] / dic[f'sum_preds1']\n                dic[f'{mode}{iw}_sum_preds2_rate'] = dic[f'{mode}{iw}_sum_preds2'] / dic[f'sum_preds2']\n                dic[f'{mode}{iw}_max_preds1_rate'] = dic[f'{mode}{iw}_max_preds1'] / dic[f'max_preds1']\n                dic[f'{mode}{iw}_max_preds2_rate'] = dic[f'{mode}{iw}_max_preds2'] / dic[f'max_preds2']\n\n        df_feat.append(dic)\n        \n    save_path = './cache/'+'_'.join([cache[0],cache[1],str(cache[2])])+'.pkl'\n    pickle.dump(df_feat, open(save_path, 'wb+'))\n    return save_path\n#     return df_feat\n    \n\ndata_splits = np.array_split(valid_pred.values, num_jobs)\nresults = Parallel(n_jobs=num_jobs, backend=\"multiprocessing\")(\n    delayed(fun_get_feat)(data_sub) for data_sub in data_splits\n)\n\nlogging.info(f\"====== load pickle ======\")\ndf_feat = []\nfor path in tqdm(results):\n    df_feat.extend(pickle.load(open(path,'rb')))\n\ndf_feat = pd.DataFrame(df_feat)\nlogging.info(f\"====== dataFrame ok ======\")\n\nparams = {\n          'boosting': 'gbdt',\n          'objective': 'binary',\n          'metric': {'auc'},\n#           'objective': 'regression',\n#           'metric': {'l2'},\n          'num_leaves': 15,\n          'min_data_in_leaf': 30,\n          'max_depth': 5,\n          'learning_rate': 0.03,\n          \"feature_fraction\": 0.7,\n          \"bagging_fraction\": 0.7,\n          'min_data_in_bin':15,\n#           \"min_sum_hessian_in_leaf\": 6,\n          \"lambda_l1\": 5,\n          'lambda_l2': 5,\n          \"random_state\": 1996,\n          \"num_threads\": num_jobs,\n          }\n\nvalid_pred = df_feat[['id', 'class','word_start','word_end']].copy()\nvalid_pred['class'] = valid_pred['class'].map(lambda x:id2label[x])\nvalid_pred['lgb_prob'] = -1\nfor fold in range(5):\n    df_feat_train = df_feat[df_feat.id.isin(kfold_ids[fold][0])].copy()\n    df_feat_val = df_feat[df_feat.id.isin(kfold_ids[fold][1])].copy()\n\n    lgb_train = lgb.Dataset(df_feat_train.drop(['id', 'label', 'label_rate'], axis=1), label=df_feat_train['label'])\n    lgb_val = lgb.Dataset(df_feat_val.drop(['id', 'label', 'label_rate'], axis=1), label=df_feat_val['label'])\n\n    clf = lgb.train(params,\n                    lgb_train,\n                    10000,\n                    valid_sets=[lgb_train, lgb_val],\n                    verbose_eval=200,\n                    early_stopping_rounds=100)\n    \n    lgb_preds = clf.predict(df_feat_val.drop(['id', 'label', 'label_rate'], axis=1))\n\n    valid_pred.loc[df_feat_val.index, 'lgb_prob'] = lgb_preds\n    \n    pickle.dump(clf, open(f'./result/lgb_fold{fold}.pkl','wb+'))\n\nassert len(valid_pred[valid_pred.lgb_prob==-1]) == 0\n\npickle.dump(valid_pred, open(f'./result/lgb_valid_pred.pkl','wb+'))\npickle.dump([t for t in list(df_feat.columns) if t not in ['id', 'label', 'label_rate']], open(f'./result/lgb_columns.pkl','wb+'))\n\ninter_thresh = { \n    \"Lead\": 0.15,\n    \"Position\": 0.15,\n    \"Evidence\": 0.15,\n    \"Claim\": 0.25,\n    \"Concluding Statement\": 0.15,\n    \"Counterclaim\": 0.25,\n    \"Rebuttal\": 0.25,\n}\ndef post_choice(df):\n    rtn = []\n    for k,group in tqdm(df.groupby(['id','class'])):\n        group = group.sort_values('lgb_prob',ascending=False)\n\n        preds_range = []\n        for irow, row in group.iterrows():\n            start = row.word_start\n            end = row.word_end\n            L1 = end-start+1\n            flag = 0\n            if L1 == 0:\n                continue\n            for pos_range in preds_range:\n                L2 = pos_range[1] - pos_range[0] + 1\n                intersection = (min(end, pos_range[1]) - max(start, pos_range[0]) + 1) / L1\n                inter_t = inter_thresh[row['class']]\n                if intersection>inter_t and (inter_t<=L1/L2<=1 or inter_t<=L2/L1<=1):\n                    flag = 1\n                    break\n\n            if flag == 0:\n                preds_range.append((start, end, row.lgb_prob))\n                \n                predictionstring = ' '.join(list(map(str, range(int(row.word_start), int(row.word_end)+1))))\n                rtn.append((row.id, row['class'], predictionstring, row.lgb_prob))\n    rtn = pd.DataFrame(rtn, columns=['id','class','predictionstring', 'lgb_prob'])\n    return rtn\n\nvalid_pred_choice = post_choice(valid_pred)\n\n\nproba_thresh = {\n    \"Lead\": 0.45,\n    \"Position\": 0.4,\n    \"Evidence\": 0.45,\n    \"Claim\": 0.35,\n    \"Concluding Statement\": 0.5,\n    \"Counterclaim\": 0.3,\n    \"Rebuttal\": 0.3,\n}\n\n\ntrain_oof = train_df.copy()\nres = {}\nfor k,v in proba_thresh.items():\n    sub = valid_pred_choice[(valid_pred_choice.lgb_prob>v)&(valid_pred_choice['class']==k)]\n    score_now = score_feedback_comp(sub, train_oof)[1][k]['f1']\n    sub = valid_pred_choice[(valid_pred_choice.lgb_prob>v-0.05)&(valid_pred_choice['class']==k)]\n    score_now1 = score_feedback_comp(sub, train_oof)[1][k]['f1']\n    sub = valid_pred_choice[(valid_pred_choice.lgb_prob>v+0.05)&(valid_pred_choice['class']==k)]\n    score_now2 = score_feedback_comp(sub, train_oof)[1][k]['f1']\n\n    if max(score_now, score_now1, score_now2) == score_now:\n        res[k] = (v, score_now)\n        \n    elif max(score_now, score_now1, score_now2) == score_now1:\n        best_score = score_now1\n        score_now3 = best_score\n        i = 2\n        while score_now3 >= best_score:\n            sub = valid_pred_choice[(valid_pred_choice.lgb_prob>v-0.05*i)&(valid_pred_choice['class']==k)]\n            score_now3 = score_feedback_comp(sub, train_oof)[1][k]['f1']\n            best_score = max(best_score, score_now3)\n            i += 1\n            if v-0.05*i<=0:\n                break\n        res[k] = (v-0.05*(i-2), best_score)\n        \n    elif max(score_now, score_now1, score_now2) == score_now2:\n        best_score = score_now2\n        score_now3 = best_score\n        i = 2\n        while score_now3 >= best_score:\n            sub = valid_pred_choice[(valid_pred_choice.lgb_prob>v+0.05*i)&(valid_pred_choice['class']==k)]\n            score_now3 = score_feedback_comp(sub, train_oof)[1][k]['f1']\n            best_score = max(best_score, score_now3)\n            i += 1\n            if v+0.05*i>=1:\n                break\n        res[k] = (v+0.05*(i-2), best_score)       \n\n\nfor k,v in res.items():\n    logging.info(f\"{k}:{v}\")\nlogging.info(f\"====== final score: {np.mean([v[1] for v in res.values()])} ======\")\n\n","metadata":{},"execution_count":null,"outputs":[]}]}