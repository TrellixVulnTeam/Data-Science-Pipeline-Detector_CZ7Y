{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Clean CPU, GPU memory after load model to GPU\n### The helper function is based on following discussions. \n- [How can I clear GPU memory in tensorflow 2?](https://github.com/tensorflow/tensorflow/issues/36465)\n- [How do I retrieve output from Multiprocessing in Python?](https://stackoverflow.com/questions/35943822/how-do-i-retrieve-output-from-multiprocessing-in-python)\n\n### Let me know if there is other alternatives, thanks","metadata":{}},{"cell_type":"code","source":"\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)\n","metadata":{"_uuid":"6be54465-d26e-474c-8b21-943f427ebd49","_cell_guid":"08bbd110-fc9f-4e60-91a7-6190669726f7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-12T14:46:31.029264Z","iopub.execute_input":"2022-03-12T14:46:31.029679Z","iopub.status.idle":"2022-03-12T14:46:31.086919Z","shell.execute_reply.started":"2022-03-12T14:46:31.029578Z","shell.execute_reply":"2022-03-12T14:46:31.085942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport os\nimport sys\nimport random\nimport numpy as np\nimport pandas as pd\nimport glob\nimport gc\n#pd.set_option('display.max_columns', None)\ngc.enable()\nfrom joblib import Parallel, delayed\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom pytorch_lightning import LightningModule, LightningDataModule\nfrom pytorch_lightning import Trainer\nimport multiprocessing\n\n# transformer\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig,AutoModelForTokenClassification\nfrom transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n#import warnings\n#warnings.filterwarnings(\"error\")\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\ntarget_id_map = {\n    'O': 0,\n    'B-Lead': 1,\n    'I-Lead': 2,\n    'B-Position': 3,\n    'I-Position': 4,\n    'B-Claim': 5,\n    'I-Claim': 6,\n    'B-Counterclaim': 7,\n    'I-Counterclaim': 8,\n    'B-Rebuttal': 9,\n    'I-Rebuttal': 10,\n    'B-Evidence': 11,\n    'I-Evidence': 12,\n    'B-Concluding Statement': 13,\n    'I-Concluding Statement': 14,\n    'PAD': -100,\n}\n'''\ntarget_id_map2 = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 2,\n    \"I-Position\": 3,\n    \"B-Evidence\": 4,\n    \"I-Evidence\": 5,\n    \"B-Claim\": 6,\n    \"I-Claim\": 7,\n    \"B-Concluding Statement\": 8,\n    \"I-Concluding Statement\": 9,\n    \"B-Counterclaim\": 10,\n    \"I-Counterclaim\": 11,\n    \"B-Rebuttal\": 12,\n    \"I-Rebuttal\": 13,\n    \"O\": 14,\n    \"PAD\": -100,\n}\n'''\n\n\nlength_threshold = {\n    'Lead'                : 9,\n    'Position'            : 5,\n    'Claim'               : 3,\n    'Counterclaim'        : 6,\n    'Rebuttal'            : 4,\n    'Evidence'            : 14,\n    'Concluding Statement': 11,\n}\n#length_threshold = {\n#    'Lead'                : 5,\n#    'Position'            : 5,\n#    'Claim'               : 3,\n#    'Counterclaim'        : 5,\n#    'Rebuttal'            : 4,\n#    'Evidence'            : 5,\n#    'Concluding Statement': 5,\n#}\nprobability_threshold = {\n    'Lead'                : 0.6,\n    'Position'            : 0.6,\n    'Claim'               : 0.6,\n    'Counterclaim'        : 0.6,\n    'Rebuttal'            : 0.6,\n    'Evidence'            : 0.6,\n    'Concluding Statement': 0.6,\n}\n\nid_target_map = {v: k for k, v in target_id_map.items()}\n#id_target_map2 = {v: k for k, v in target_id_map2.items()}\nseed_everything(2022)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n","metadata":{"_uuid":"b7038791-fc4e-4f20-ac04-fd437cd0c8e3","_cell_guid":"ced511e9-ea0b-43f0-9883-ef791e06e4d5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-12T14:46:31.089294Z","iopub.execute_input":"2022-03-12T14:46:31.089625Z","iopub.status.idle":"2022-03-12T14:46:35.897829Z","shell.execute_reply.started":"2022-03-12T14:46:31.089581Z","shell.execute_reply":"2022-03-12T14:46:35.896606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper function","metadata":{}},{"cell_type":"code","source":"def process(func):\n    def worker(func,q):\n        q.put(func())\n\n    out = None\n    q = multiprocessing.Queue()    \n    p = multiprocessing.Process(target=worker,args=(func,q,))\n    \n    p.start()\n    out = q.get()\n    p.join()\n    \n    return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Data","metadata":{"_uuid":"166df4a3-0472-4062-94e6-678b7a2fc854","_cell_guid":"832e4494-4886-427a-ba63-e6e4f2cfbb8e","trusted":true}},{"cell_type":"code","source":"DEBUG = True\n\nif DEBUG:\n    text_dir = '../input/feedback-prize-2021/train'\n    valid_id = [ f.split('/')[-1][:-4] for f in glob.glob(text_dir+'/*.txt') ]\n    valid_id = sorted(valid_id)[0:10000]\n    num_valid = len(valid_id)\n    print('len(valid_id)',len(valid_id))\nelse:\n    text_dir = '../input/feedback-prize-2021/test'\n    valid_id = [ f.split('/')[-1][:-4] for f in glob.glob(text_dir+'/*.txt') ]\n    valid_id = sorted(valid_id)\n    num_valid = len(valid_id)\n    print('len(valid_id)',len(valid_id))\n\nsize = [os.path.getsize(text_dir+'/%s.txt'%id) for id in valid_id] \nvalid_id = [id for id, s in sorted(zip(valid_id, size), key=lambda pair: -pair[1])]\ndel size\ngc.collect()\nprint('len(valid_id)',len(valid_id))","metadata":{"_uuid":"b43f6b61-a581-4338-bf85-f4451f7a2238","_cell_guid":"fec8a0f7-8983-480b-941c-5cbf06906aef","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-12T14:46:35.900239Z","iopub.execute_input":"2022-03-12T14:46:35.900565Z","iopub.status.idle":"2022-03-12T14:46:36.108736Z","shell.execute_reply.started":"2022-03-12T14:46:35.900499Z","shell.execute_reply":"2022-03-12T14:46:36.107646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _prepare_test_data_helper(tokenizer, ids):\n    if DEBUG:\n        path = '../input/feedback-prize-2021/train'\n    else:\n        path = '../input/feedback-prize-2021/test'\n    test_samples = []\n    for idx in ids:\n        filename = os.path.join(path, idx + \".txt\")\n        with open(filename, \"r\") as f:\n            text = f.read()\n            text = text.replace(u'\\xa0', u' ')\n            #text = text.rstrip()\n            #text = text.lstrip()\n            \n        encoded_text = tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n            truncation=True,\n            max_length=1600,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        offset_mapping = encoded_text[\"offset_mapping\"]\n        \n        sample = {\n            \"id\": idx,\n            \"input_ids\": input_ids,\n            \"text\": text,\n            \"offset_mapping\": offset_mapping\n        }\n\n        test_samples.append(sample)\n    return test_samples\n\n\ndef prepare_test_data(ids, tokenizer):\n    test_samples = []\n    ids_splits = np.array_split(ids, 2)\n\n    results = Parallel(n_jobs=2, backend=\"multiprocessing\")(\n        delayed(_prepare_test_data_helper)(tokenizer, idx) for idx in ids_splits\n    )\n    for result in results:\n        test_samples.extend(result)\n    #for idx in ids:\n    #    #print(idx)\n    #    result = _prepare_test_data_helper(tokenizer, [idx])\n    #    test_samples.extend(result)\n\n    return test_samples","metadata":{"_uuid":"db7a096b-6987-4830-b005-3a4e7ed12fb2","_cell_guid":"49ae10ce-d5ac-49c6-9462-7844d8983908","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-12T14:46:36.113269Z","iopub.execute_input":"2022-03-12T14:46:36.113548Z","iopub.status.idle":"2022-03-12T14:46:36.129377Z","shell.execute_reply.started":"2022-03-12T14:46:36.113468Z","shell.execute_reply":"2022-03-12T14:46:36.12807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Class","metadata":{"_uuid":"cbbeb6be-1fa6-47ff-9ce6-53670d83f1c9","_cell_guid":"dc4d8d09-8d16-4dab-8436-e2ce09724f29","trusted":true}},{"cell_type":"code","source":"\nclass FeedbackDataset(Dataset):\n    def __init__(self, samples, tokenizer, max_len=1600):\n        self.samples = samples\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.length = len(samples)\n        \n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        #input_ids = self.samples[idx][\"input_ids\"]\n        input_ids = self.samples[idx][\"input_ids\"]\n        input_ids = [self.tokenizer.cls_token_id] + input_ids\n        \n        #if len(input_ids) > self.max_len - 1: # need end token after\n        #    input_ids = input_ids[: self.max_len - 1]\n            \n        input_ids = input_ids[: self.max_len - 1] + [self.tokenizer.sep_token_id]\n        token_length = len(input_ids)\n        \n        attention_mask = [1] * len(input_ids)\n        padding_length = self.max_len - len(input_ids)\n        if padding_length > 0:\n            #if self.tokenizer.padding_side == \"right\":\n            input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n            attention_mask = attention_mask + [0] * padding_length\n            #else:\n            #    raise NotImplementedError\n        \n        return {\n            \"ids\": torch.tensor(input_ids, dtype=torch.long),\n            \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n            'token_length':token_length\n        }\n       \n\n\nclass DataModule(LightningDataModule):\n    def __init__(self, test_samples, tokenizer, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.tokenizer = tokenizer\n        self.test_df = test_samples\n\n    \n    def setup(self,stage):\n        if stage == 'fit':\n            pass\n        elif stage=='predict':\n            self.test_ds = FeedbackDataset(self.test_df, self.tokenizer,self.cfg.max_length)\n\n    \n    def predict_dataloader(self):\n        return DataLoader(\n            self.test_ds, batch_size=self.cfg.test_batch_size, \n            shuffle=False, num_workers=2,pin_memory=False\n            )","metadata":{"_uuid":"4f317270-84d1-4e55-89c7-9604dcf3940d","_cell_guid":"389eb82f-7902-4448-8588-dad810c9442c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-12T14:46:36.131125Z","iopub.execute_input":"2022-03-12T14:46:36.131866Z","iopub.status.idle":"2022-03-12T14:46:36.150045Z","shell.execute_reply.started":"2022-03-12T14:46:36.131819Z","shell.execute_reply":"2022-03-12T14:46:36.148499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef get_word_proba(pred_proba,valid_samples): # pred_proba (N,seq,label) -> (N,word,label)\n        word_proba = []\n        for i in range(len(pred_proba)):\n            sample = valid_samples[i]\n            proba = pred_proba[i]\n            #preds = sample[\"preds\"][1:] # token wise prediction. neglect cls\n            preds = proba[1:] # (1:1599) (seq,label)\n            offset_mapping = sample[\"offset_mapping\"] # token offset mapping\n            sample_id = sample[\"id\"]\n            sample_text = sample[\"text\"]\n            \n            word, word_offset = text_to_word(sample_text) # word offset mapping\n\n            token_to_text_probability = np.full((len(sample_text),15),0, np.float32)\n            \n            max_length = len(preds)\n            for t,(start,end) in enumerate(offset_mapping):\n                if t==max_length-1: break #offsetmapping > tokensize, neclect sep token\n                token_to_text_probability[start:end] = preds[t] # text lettre wise proba\n\n            text_to_word_probability = np.full((len(word),15),0, np.float32) # word wise proba\n            for t,(start,end) in enumerate(word_offset):\n                # average proba of all letter in word\n                # (words,15)\n                text_to_word_probability[t]=token_to_text_probability[start:end].mean(0) \n            \n            word_proba.append(text_to_word_probability)\n        \n        return word_proba # (N,word,label) word not the same for samples\n    \ndef text_to_word(text):\n    word = text.split()\n    word_offset = []\n\n    start = 0\n    for w in word:\n        r = text[start:].find(w)\n\n        if r==-1:\n            raise NotImplementedError\n        else:\n            start = start+r\n            end   = start+len(w)\n            word_offset.append((start,end))\n            #print('%32s'%w, '%5d'%start, '%5d'%r, text[start:end])\n        start = end\n\n    return word, word_offset","metadata":{"_uuid":"57112731-9a77-4489-aa4b-3e7eae0db107","_cell_guid":"03d10c78-e045-4364-a701-1a0c2d322a62","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-12T14:46:36.152232Z","iopub.execute_input":"2022-03-12T14:46:36.153738Z","iopub.status.idle":"2022-03-12T14:46:36.172425Z","shell.execute_reply.started":"2022-03-12T14:46:36.15368Z","shell.execute_reply":"2022-03-12T14:46:36.171265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DBxl","metadata":{}},{"cell_type":"code","source":"class DBXLModule(LightningModule):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg=cfg\n        modelpath = '../input/deberta-xlarge'\n        config = AutoConfig.from_pretrained(modelpath)\n        config.num_labels = 15\n        #self.model = AutoModelForTokenClassification.from_pretrained(modelpath,config=config)\n        self.model = AutoModelForTokenClassification.from_config(config=config)\n\n    def forward(self, input_ids, attention_mask):\n        out = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        return out.logits\n    \n    def predict_step(self, batch, batch_idx):\n        length = batch['token_length'].max().item()\n        token_id_short   = batch['ids'][:,:length]\n        token_mask_short = batch['mask'][:,:length]\n        logits = self(token_id_short, token_mask_short) # (N,seq,label)\n        proba = torch.softmax(logits, dim=2)\n        proba = proba.detach().cpu().numpy()\n        proba = np.pad(proba, ((0, 0),(0,self.cfg.max_length-length),(0,0)), 'constant', constant_values=0)\n        return proba\n    \nclass CFG:\n    def __init__(self):\n        self.max_length=1600#1536\n        self.test_batch_size = 12\n\nCFG_dbxl = CFG()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T14:46:36.175834Z","iopub.execute_input":"2022-03-12T14:46:36.177069Z","iopub.status.idle":"2022-03-12T14:46:36.192244Z","shell.execute_reply.started":"2022-03-12T14:46:36.177023Z","shell.execute_reply":"2022-03-12T14:46:36.190982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dbxl_wp():\n    tokenizer = AutoTokenizer.from_pretrained('../input/deberta-xlarge')\n    test_samples = prepare_test_data(valid_id, tokenizer)\n    dm = DataModule(test_samples,tokenizer,CFG_dbxl)\n\n    ckPath= [\n        '../input/feedback2022infer/dbxl-f0-2-ep5-bs1-lr1.2-val_f10.686.ckpt',\n        '../input/feedback2022infer/dbxl-f1-2-ep5-bs1-lr1.2-val_f10.693.ckpt'\n        ]\n\n    model = DBXLModule(CFG_dbxl)\n    trainer = Trainer(gpus=1,precision=16,num_sanity_val_steps=0)\n\n\n    p_dbxl=None\n    for path in ckPath:\n        print(path)\n        model.load_state_dict(torch.load(path)['state_dict'])\n        preds = trainer.predict(model, datamodule=dm)\n        preds = np.concatenate(preds) # proba: (N,seq,label)\n        #preds = preds.astype(np.float16)\n\n        #print(preds.shape)\n        if p_dbxl is None:\n            p_dbxl=preds\n        else:\n            p_dbxl+=preds\n\n    #print(p_dbxl.shape)\n    wp_dbxl = get_word_proba(p_dbxl,test_samples)\n    #del preds, model, dm, trainer,p_dbxl ,test_samples, tokenizer\n    #gc.collect()\n    #torch.cuda.empty_cache()\n    return wp_dbxl","metadata":{"execution":{"iopub.status.busy":"2022-03-12T14:46:36.194372Z","iopub.execute_input":"2022-03-12T14:46:36.194817Z","iopub.status.idle":"2022-03-12T14:46:36.208822Z","shell.execute_reply.started":"2022-03-12T14:46:36.19469Z","shell.execute_reply":"2022-03-12T14:46:36.207594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwp_dbxl = process(dbxl_wp)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-12T14:46:36.20995Z","iopub.execute_input":"2022-03-12T14:46:36.211424Z","iopub.status.idle":"2022-03-12T14:46:36.224733Z","shell.execute_reply.started":"2022-03-12T14:46:36.211275Z","shell.execute_reply":"2022-03-12T14:46:36.223114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DBv3l-f0-20/m-90mins","metadata":{"_uuid":"4c125eb4-ba33-4a4d-a08a-da2c28304043","_cell_guid":"b59a35fe-d240-4a30-854b-9aa02bdef9f4","trusted":true}},{"cell_type":"code","source":"class DBv3LModule(LightningModule):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg=cfg\n        modelpath = '../input/deberta-v3-large/deberta-v3-large'\n        config = AutoConfig.from_pretrained(modelpath)\n        config.num_labels = 15\n        #self.model = AutoModelForTokenClassification.from_pretrained(self.cfg['modelpath'],config=config)\n        self.model = AutoModelForTokenClassification.from_config(config=config)\n\n    def forward(self, input_ids, attention_mask):\n        out = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        return out.logits\n    \n    def predict_step(self, batch, batch_idx):\n        length = batch['token_length'].max().item()\n        token_id_short   = batch['ids'][:,:length]\n        token_mask_short = batch['mask'][:,:length]\n        logits = self(token_id_short, token_mask_short) # (N,seq,label)\n        proba = torch.softmax(logits, dim=2)\n        proba = proba.cpu().detach().numpy()\n        proba = np.pad(proba, ((0, 0),(0,self.cfg.max_length-length),(0,0)), 'constant', constant_values=0)\n        return proba\n\nclass CFG:\n    def __init__(self):\n        #self.modelpath = '../input/deberta-v3-large/deberta-v3-large'\n        #self.tokpath = '../input/deberta-v3-large/deberta-v3-large'\n        self.max_length=1600\n        #self.num_labels=15\n        self.test_batch_size = 17\n\nCFG_dbv3l = CFG()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T14:46:36.231296Z","iopub.execute_input":"2022-03-12T14:46:36.231616Z","iopub.status.idle":"2022-03-12T14:46:36.248129Z","shell.execute_reply.started":"2022-03-12T14:46:36.231577Z","shell.execute_reply":"2022-03-12T14:46:36.24695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef dbv3l_wp():\n    tokenizer = DebertaV2TokenizerFast.from_pretrained('../input/deberta-v3-large/deberta-v3-large')\n    test_samples = prepare_test_data(valid_id, tokenizer)\n    dm = DataModule(test_samples,tokenizer,CFG_dbv3l)\n    trainer = Trainer(gpus=1,precision=16,num_sanity_val_steps=0)\n    \n    model = DBv3LModule(CFG_dbv3l)\n    #ckPath= '../input/feedback2022infer/dbv3l-f0-ep5-bs2-lr1.4-val_f10.675.ckpt'\n    ckPath = '../input/feedback2022infer/dbv3l/*.ckpt'\n    #model.load_state_dict(torch.load(ckPath)['state_dict'])\n    #p_dbv3l = trainer.predict(model, datamodule=dm)\n    #p_dbv3l = np.concatenate(p_dbv3l) # proba: (N,seq,label)\n    \n    p_dbv3l=None\n    for path in glob.glob(ckPath):\n        print(path)\n        model.load_state_dict(torch.load(path)['state_dict'])\n        preds = trainer.predict(model, datamodule=dm)\n        preds = np.concatenate(preds) # proba: (N,seq,label)\n        #preds = preds.astype(np.float16)\n\n        #print(preds.shape)\n        if p_dbv3l is None:\n            p_dbv3l=preds\n        else:\n            p_dbv3l+=preds\n    \n\n    #print(p_dbv3l.shape)\n    \n    wp_dbv3l = get_word_proba(p_dbv3l,test_samples)\n    return wp_dbv3l","metadata":{"_uuid":"15b86c59-ef32-4495-98f4-aa7bda530d5a","_cell_guid":"b240d2b7-b72c-4095-9826-881dd898e66d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-12T14:46:36.252426Z","iopub.execute_input":"2022-03-12T14:46:36.253325Z","iopub.status.idle":"2022-03-12T14:46:36.264767Z","shell.execute_reply.started":"2022-03-12T14:46:36.253264Z","shell.execute_reply":"2022-03-12T14:46:36.263651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wp_dbv3l = process(dbv3l_wp)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T14:46:36.266312Z","iopub.execute_input":"2022-03-12T14:46:36.267002Z","iopub.status.idle":"2022-03-12T14:46:36.27826Z","shell.execute_reply.started":"2022-03-12T14:46:36.266958Z","shell.execute_reply":"2022-03-12T14:46:36.277295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DBL","metadata":{}},{"cell_type":"code","source":"\nclass DBLModule(LightningModule):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg=cfg\n        modelpath = '../input/debertalarge'\n        config = AutoConfig.from_pretrained(modelpath)\n        config.num_labels = 15\n        #self.model = AutoModelForTokenClassification.from_pretrained(modelpath,config=config)\n        self.model = AutoModelForTokenClassification.from_config(config=config)\n\n    def forward(self, input_ids, attention_mask):\n        out = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        return out.logits\n    \n    def predict_step(self, batch, batch_idx):\n        length = batch['token_length'].max().item()\n        token_id_short   = batch['ids'][:,:length]\n        token_mask_short = batch['mask'][:,:length]\n        logits = self(token_id_short, token_mask_short) # (N,seq,label)\n        proba = torch.softmax(logits, dim=2)\n        proba = proba.detach().cpu().numpy()\n        proba = np.pad(proba, ((0, 0),(0,self.cfg.max_length-length),(0,0)), 'constant', constant_values=0)\n        return proba\n    \nclass CFG:\n    def __init__(self):\n        self.max_length=1600 #1536\n        self.test_batch_size = 17\n\nCFG_dbl1 = CFG()\n","metadata":{"_uuid":"b528f656-9300-4b8b-9ec8-d9293d056609","_cell_guid":"32ed2165-5745-4c78-8334-46b5e1564ab7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-12T14:46:36.32719Z","iopub.execute_input":"2022-03-12T14:46:36.328153Z","iopub.status.idle":"2022-03-12T14:46:36.346251Z","shell.execute_reply.started":"2022-03-12T14:46:36.328116Z","shell.execute_reply":"2022-03-12T14:46:36.344841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef dbl_wp():\n    tokenizer = AutoTokenizer.from_pretrained('../input/debertalarge')\n    test_samples = prepare_test_data(valid_id, tokenizer)\n    dm = DataModule(test_samples,tokenizer,CFG_dbl1)\n\n    trainer = Trainer(gpus=1,precision=16,num_sanity_val_steps=0)\n    model = DBLModule(CFG_dbl1)\n\n    #ckPath= [\n    #    '../input/feedback2022infer/dbl-f0-ep5-bs1-lr1.3-val_f10.684.ckpt',\n    #    '../input/feedback2022infer/dbl-f1-1600-2-ep5-bs1-lr1.2-val_f10.696.ckpt'\n    #]\n    ckPath = '../input/feedback2022infer2/dbl-2/*.ckpt'\n\n    p_dbl1 = None\n    for path in glob.glob(ckPath):\n        print(path)\n        model.load_state_dict(torch.load(path)['state_dict'])\n        preds = trainer.predict(model, datamodule=dm)\n        preds = np.concatenate(preds) # proba: (N,seq,label)\n        #preds = preds.astype(np.float16)\n\n        #print(preds.shape)\n        if p_dbl1 is None:\n            p_dbl1=preds\n        else:\n            p_dbl1+=preds\n\n    #print(p_dbl1.shape)\n    wp_dbl1 = get_word_proba(p_dbl1,test_samples)\n\n    #del preds, model, dm, trainer,p_dbl1\n    #gc.collect()\n    #del tokenizer,test_samples\n    #gc.collect()\n    #torch.cuda.empty_cache()\n    return wp_dbl1","metadata":{"_uuid":"f4db5f1c-1937-44f4-88b8-ffb463277aeb","_cell_guid":"0e84adec-a224-4649-b378-8339f8b35d96","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-12T14:46:36.349287Z","iopub.execute_input":"2022-03-12T14:46:36.349543Z","iopub.status.idle":"2022-03-12T14:46:36.365101Z","shell.execute_reply.started":"2022-03-12T14:46:36.349512Z","shell.execute_reply":"2022-03-12T14:46:36.363824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wp_dbl1 = process(dbl_wp)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T14:46:36.367712Z","iopub.execute_input":"2022-03-12T14:46:36.368331Z","iopub.status.idle":"2022-03-12T14:48:03.567351Z","shell.execute_reply.started":"2022-03-12T14:46:36.368284Z","shell.execute_reply":"2022-03-12T14:48:03.565529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble","metadata":{"_uuid":"1f9a57dc-42da-4c08-ac63-93aefc00c579","_cell_guid":"7269e321-592b-454d-b259-ae6e11efad5d","trusted":true}},{"cell_type":"code","source":"def jn2(cur):\n    return \" \".join([str(x) for x in cur])\n\ndef link(oof):\n    gap = {\n        'Lead'                : 3,\n        'Position'            : 3,\n        'Claim'               : -1,# not link\n        'Counterclaim'        : 3,\n        'Rebuttal'            : 3,\n        'Evidence'            : 3,\n        'Concluding Statement': 3,\n    }\n    idu = oof['id'].unique()\n    linkcat = ['Lead', 'Position', 'Evidence', 'Claim','Concluding Statement','Counterclaim', 'Rebuttal']\n    retval = []\n    \n    for idv in idu:\n        for c in linkcat:\n            q = oof[(oof['id'] == idv) & (oof['class'] == c)]\n            if len(q) == 0:\n                continue\n            \n            pst = [] # list of list\n            s = []\n            for i,r in q.iterrows():\n                pst.append([int(x) for x in r['predictionstring'].split()])\n                #s.append(eval(r.score))\n                s.append(r.score)\n                \n            cur = pst[0]\n            curs = s[0]\n            for i in range(1,len(pst)):\n                nxt = pst[i]\n                nxts = s[i]\n                \n                if nxt[0] - cur[-1]>gap[c]:\n                    retval.append((idv, c, jn2(cur), curs))\n                    cur=nxt\n                    curs=nxts\n                else:\n                    cur = cur + nxt\n                    curs = curs+nxts\n            \n            retval.append((idv, c, jn2(cur), curs))\n                    \n    roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring','score'])\n    return roof","metadata":{"execution":{"iopub.status.busy":"2022-03-12T14:48:03.613325Z","iopub.execute_input":"2022-03-12T14:48:03.615929Z","iopub.status.idle":"2022-03-12T14:48:03.637405Z","shell.execute_reply.started":"2022-03-12T14:48:03.61588Z","shell.execute_reply":"2022-03-12T14:48:03.636116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def word_probability_to_predict_df(text_to_word_probability, id, target_to_id, id_to_target):\n    len_word = len(text_to_word_probability)\n    word_predict = text_to_word_probability.argmax(-1)\n    word_score   = text_to_word_probability.max(-1)\n    predict_df = []\n    #print(word_predict)\n    t = 0\n    while 1:\n        if word_predict[t] not in [\n            target_to_id['O'],\n            target_to_id['PAD'],\n        ]:\n            start = t\n            b_marker_label = word_predict[t]\n        else:\n            t = t+1\n            if t== len_word-1: break\n            continue\n\n        t = t+1\n        if t== len_word-1: break\n\n        #----\n        if   id_to_target[b_marker_label][0]=='B':\n            i_marker_label = b_marker_label+1\n        elif id_to_target[b_marker_label][0]=='I':\n            i_marker_label = b_marker_label\n        else:\n            raise NotImplementedError\n\n        while 1:\n            #print(t)\n            if (word_predict[t] != i_marker_label) or (t ==len_word-1):\n                end = t\n                prediction_string = ' '.join([str(i) for i in range(start,end)]) #np.arange(start,end).tolist()\n                discourse_type = id_to_target[b_marker_label][2:]\n                discourse_score = word_score[start:end].tolist()\n                #predict_df.append((id, discourse_type, prediction_string, str(discourse_score)))\n                predict_df.append((id, discourse_type, prediction_string, discourse_score))\n                #print(predict_df[-1])\n                break\n            else:\n                t = t+1\n                continue\n        if t== len_word-1: break\n    \n    predict_df = pd.DataFrame(predict_df, columns=['id', 'class', 'predictionstring', 'score'])\n    return predict_df\n\ndef do_threshold(submit_df, use=['length','probability']):\n    df = submit_df.copy()\n    df = df.fillna('')\n    \n    if 'length' in use:\n        df['l'] = df.predictionstring.apply(lambda x: len(x.split()))\n        for key, value in length_threshold.items():\n            #value=3\n            index = df.loc[df['class'] == key].query('l<%d'%value).index\n            df.drop(index, inplace=True)\n\n    if 'probability' in use:\n        #df['s'] = df.score.apply(lambda x: np.mean(eval(x)))\n        df['s'] = df.score.apply(lambda x: np.mean(x))\n        #if df['id'].values[0]=='02F89B4E55CF':\n        #    print(df)\n        for key, value in probability_threshold.items():\n            index = df.loc[df['class'] == key].query('s<%f'%value).index\n            df.drop(index, inplace=True)\n            \n    df = df[['id', 'class', 'predictionstring']]\n    return df\n\ndef get_prediction(word_proba, ids,target_to_id,id_to_target):\n    df_pred = pd.DataFrame(columns = ['id','class','predictionstring'])\n    \n    for i in range(len(ids)):\n        sample_id = ids[i]\n        text_to_word_probability = word_proba[i]\n        predict_df = word_probability_to_predict_df(text_to_word_probability, sample_id,target_to_id,id_to_target)\n        \n        #if sample_id=='02F89B4E55CF':\n        #    print(predict_df)\n        #submit_df = do_threshold(predict_df, use=['length','probability'])#,'probability'\n        #if sample_id=='108C28958E5B':\n        #    print(submit_df)\n        df_pred = df_pred.append(predict_df)\n        #print(df_pred)\n    \n    df_link = link(df_pred)\n    df_lp   = do_threshold(df_link, use=['length','probability'])\n    return df_lp","metadata":{"_uuid":"9aa514df-b603-440c-b46a-7bfdbd92535f","_cell_guid":"9e4db19a-ff13-471b-9b35-bbfbe53b64e2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-12T14:48:03.643584Z","iopub.execute_input":"2022-03-12T14:48:03.647089Z","iopub.status.idle":"2022-03-12T14:48:03.681721Z","shell.execute_reply.started":"2022-03-12T14:48:03.647039Z","shell.execute_reply":"2022-03-12T14:48:03.680323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wproba_ens = [wp_dbl1[i]/4 for i in range(len(valid_id))]","metadata":{"_uuid":"4e6ae48a-6c2d-41f0-8f25-eab48ad8fa59","_cell_guid":"e1f74827-06c4-4ae2-bcdc-077178326eb6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-12T14:48:03.708756Z","iopub.execute_input":"2022-03-12T14:48:03.709174Z","iopub.status.idle":"2022-03-12T14:48:03.729987Z","shell.execute_reply.started":"2022-03-12T14:48:03.709124Z","shell.execute_reply":"2022-03-12T14:48:03.728264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# about 20min\nsub = get_prediction(wproba_ens,valid_id,target_id_map,id_target_map)\nsub.id.nunique(),len(sub)","metadata":{"_uuid":"1bfe7684-fe4f-42bf-8733-68753b796471","_cell_guid":"3d986b3a-cd52-4452-b159-33039d669780","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-12T14:48:03.758917Z","iopub.execute_input":"2022-03-12T14:48:03.759749Z","iopub.status.idle":"2022-03-12T14:48:03.918058Z","shell.execute_reply.started":"2022-03-12T14:48:03.759553Z","shell.execute_reply":"2022-03-12T14:48:03.916751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)\nsub.head()","metadata":{"_uuid":"c59d1f70-23ea-4188-8e56-0d5f9a807647","_cell_guid":"42b4231d-1032-45db-95b6-7c3a88cc5031","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-12T14:48:03.92032Z","iopub.execute_input":"2022-03-12T14:48:03.920692Z","iopub.status.idle":"2022-03-12T14:48:03.944884Z","shell.execute_reply.started":"2022-03-12T14:48:03.920648Z","shell.execute_reply":"2022-03-12T14:48:03.943654Z"},"trusted":true},"execution_count":null,"outputs":[]}]}