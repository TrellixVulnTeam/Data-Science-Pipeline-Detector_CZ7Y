{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Credit to\n-  [CHRIS DEOTTE, PyTorch - BigBird - NER - [CV 0.615]](http://https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615/notebook)\n- [CPMP, Faster Metric Computation](http://https://www.kaggle.com/cpmpml/faster-metric-computation/notebook)\n\n## here is the inference part:\n[feedback2022_pytorch lightning[infer]](https://www.kaggle.com/fangyu67/feedback2022-pytorch-lightning-infer)\n\n\n### If you feel useful please upvote :)","metadata":{}},{"cell_type":"code","source":"#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n#!python pytorch-xla-env-setup.py --version 1.8.1 --apt-packages libomp5 libopenblas-dev\n#!pip install pytorch-lightning==1.5.10\n\n#import torch_xla\n#import torch_xla.core.xla_model as xm\n#import torch_xla.distributed.xla_multiprocessing as xmp\n#!export XLA_USE_BF16=1","metadata":{"execution":{"iopub.status.busy":"2022-02-18T21:50:56.645075Z","iopub.execute_input":"2022-02-18T21:50:56.645546Z","iopub.status.idle":"2022-02-18T21:50:56.661968Z","shell.execute_reply.started":"2022-02-18T21:50:56.645465Z","shell.execute_reply":"2022-02-18T21:50:56.661325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\n\nimport gc\npd.set_option('display.max_columns', None)\ngc.enable()\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n#from torch.utils.data import RandomSampler, SequentialSampler,TensorDataset\nfrom torch.optim.lr_scheduler import OneCycleLR#,CosineAnnealingLR\n#from torch.optim import lr_scheduler\n\nfrom pytorch_lightning import LightningModule, LightningDataModule,Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint,LearningRateMonitor\n\n# transformer\nfrom transformers import AutoTokenizer, AutoModel, AdamW,AutoConfig,AutoModelForTokenClassification\n\n#\nfrom sklearn.metrics import accuracy_score\n\n\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-18T21:50:56.663745Z","iopub.execute_input":"2022-02-18T21:50:56.664322Z","iopub.status.idle":"2022-02-18T21:51:04.696649Z","shell.execute_reply.started":"2022-02-18T21:50:56.664285Z","shell.execute_reply":"2022-02-18T21:51:04.695814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class CFG:\n    def __init__(self):\n        self.n_procs=1\n        self.num_workers=2\n        self.precision = 16\n        self.seed=2022\n        self.scheduler='lr_logging'\n        ##########################\n        self.model_name='BigBird'\n        self.modelpath='../input/py-bigbird-v26'\n        self.tokpath = '../input/py-bigbird-v26'\n        self.max_length=1024\n        ###############################################################################\n        #'ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts'#OneCycleLR\n        self.num_labels=15\n        self.epochs=5\n        self.batch_size = 4 \n        self.val_batch_size = self.batch_size \n        #self.stg = 'dp'\n        #self.n_fold=4\n        #self.trn_fold=[0]#[0,1,2,3]\n        ##################################################################\n        #factor=0.2 # ReduceLROnPlateau\n        #patience=4 # ReduceLROnPlateau\n        #eps=1e-6 # ReduceLROnPlateau\n        #self.T_max=8 # CosineAnnealingLR\n        #T_0=3 # CosineAnnealingWarmRestarts\n        ######################################################################\n        self.lr=5e-5\n        #self.min_lr=1e-7\n        #self.weight_decay=1e-6\n        #gradient_accumulation_steps=1\n        #max_grad_norm=1000\n        self.max_lr=self.lr#5e-6 #2e-6 \n        #self.n_cycle = 2\n        #self.n_epoch = self.epochs\n        self.steps_lr=123//(self.batch_size) + 1   #recompute before train\n        #self.final_div_factor = 1e3\n        #'pct_start': 0.1,               # OneCycleLR\n        #'anneal_strategy': 'cos',       # OneCycleLR\n        \n\nCFG = CFG()\nseed_everything(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T21:51:04.697969Z","iopub.execute_input":"2022-02-18T21:51:04.698232Z","iopub.status.idle":"2022-02-18T21:51:04.709924Z","shell.execute_reply.started":"2022-02-18T21:51:04.698198Z","shell.execute_reply":"2022-02-18T21:51:04.709024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/feedback-prize-2021/train.csv')\nREADTOKEN = True\n\nprint( train_df.shape )\ntrain_df.head(2)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-18T21:51:04.71288Z","iopub.execute_input":"2022-02-18T21:51:04.713314Z","iopub.status.idle":"2022-02-18T21:51:06.226086Z","shell.execute_reply.started":"2022-02-18T21:51:04.713262Z","shell.execute_reply":"2022-02-18T21:51:06.225368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_gt\n#df_gt = train_df[['id','discourse_type','predictionstring']].copy()\n#df_gt['labelList'] = df_gt['predictionstring'].apply(lambda x: [int(num) for num in x.split()])\n#df_gt.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T21:51:06.227186Z","iopub.execute_input":"2022-02-18T21:51:06.227444Z","iopub.status.idle":"2022-02-18T21:51:06.231329Z","shell.execute_reply.started":"2022-02-18T21:51:06.227409Z","shell.execute_reply":"2022-02-18T21:51:06.230415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not READTOKEN:\n# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\n    text_names, train_texts = [], []\n    for f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n        text_names.append(f.replace('.txt', ''))\n        text = open('../input/feedback-prize-2021/train/' + f, 'r').read()\n        text = text.replace(',',', ')\n        train_texts.append(text)\n    train_text_df = pd.DataFrame({'id': text_names, 'text': train_texts})\n    train_text_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T21:51:06.232371Z","iopub.execute_input":"2022-02-18T21:51:06.23305Z","iopub.status.idle":"2022-02-18T21:51:06.241544Z","shell.execute_reply.started":"2022-02-18T21:51:06.233013Z","shell.execute_reply":"2022-02-18T21:51:06.240777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nLOAD_TOKENS_FROM = '../input/feedback2022'\n\nif not READTOKEN:\n    all_entities = []\n    \n    for ii,i in enumerate(train_text_df.iterrows()):\n        if ii%100==0: print(ii,', ',end='')\n        total = i[1]['text'].split().__len__()\n        entities = [\"O\"]*total\n        for j in train_df[train_df['id'] == i[1]['id']].iterrows():\n            discourse = j[1]['discourse_type']\n            list_ix = [int(x) for x in j[1]['predictionstring'].split(' ')]\n            entities[list_ix[0]] = f\"B-{discourse}\"\n            for k in list_ix[1:]: entities[k] = f\"I-{discourse}\"\n        all_entities.append(entities)\n    train_text_df['entities'] = all_entities\n    train_text_df.to_csv('train_NER.csv',index=False)\n\nelse:\n    from ast import literal_eval\n    train_text_df = pd.read_csv(f'{LOAD_TOKENS_FROM}/train_NER.csv')\n    ##pandas saves lists as string, we must convert back\n    train_text_df.entities = train_text_df.entities.apply(lambda x: literal_eval(x) )\n    \nprint( train_text_df.shape )\ntrain_text_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T21:51:06.242854Z","iopub.execute_input":"2022-02-18T21:51:06.243192Z","iopub.status.idle":"2022-02-18T21:51:19.747481Z","shell.execute_reply.started":"2022-02-18T21:51:06.243158Z","shell.execute_reply":"2022-02-18T21:51:19.746569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CREATE DICTIONARIES THAT WE CAN USE DURING TRAIN AND INFER\noutput_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n\nlabels_to_ids = {v:k for k,v in enumerate(output_labels)}\nids_to_labels = {k:v for k,v in enumerate(output_labels)}\n\n#{'O': 0,'B-Lead': 1,'I-Lead': 2,'B-Position': 3,'I-Position': 4,'B-Claim': 5,'I-Claim': 6,\n# 'B-Counterclaim': 7,'I-Counterclaim': 8,'B-Rebuttal': 9,'I-Rebuttal': 10,'B-Evidence': 11,\n#'I-Evidence': 12,'B-Concluding Statement': 13,'I-Concluding Statement': 14}","metadata":{"execution":{"iopub.status.busy":"2022-02-18T21:51:19.749018Z","iopub.execute_input":"2022-02-18T21:51:19.749283Z","iopub.status.idle":"2022-02-18T21:51:19.755555Z","shell.execute_reply.started":"2022-02-18T21:51:19.749248Z","shell.execute_reply":"2022-02-18T21:51:19.754513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for debug\n#train_text_df = train_text_df.sample(500)\n\n# CHOOSE VALIDATION INDEXES\nIDS = train_df.id.unique()\nprint('There are',len(IDS),'train texts. We will split 90% 10% for validation.')\n\n# TRAIN VALID SPLIT 90% 10%\ntrain_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\nvalid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T21:51:19.758746Z","iopub.execute_input":"2022-02-18T21:51:19.759099Z","iopub.status.idle":"2022-02-18T21:51:19.786776Z","shell.execute_reply.started":"2022-02-18T21:51:19.759064Z","shell.execute_reply":"2022-02-18T21:51:19.785959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CREATE TRAIN SUBSET AND VALID SUBSET\ndata = train_text_df[['id','text', 'entities']]\ntrain_dataset = data.loc[data['id'].isin(IDS[train_idx]),['id','text', 'entities']].reset_index(drop=True)\ntest_dataset = data.loc[data['id'].isin(IDS[valid_idx])].reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(data.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))","metadata":{"execution":{"iopub.status.busy":"2022-02-18T21:51:19.788015Z","iopub.execute_input":"2022-02-18T21:51:19.788459Z","iopub.status.idle":"2022-02-18T21:51:19.815504Z","shell.execute_reply.started":"2022-02-18T21:51:19.788422Z","shell.execute_reply":"2022-02-18T21:51:19.8148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gt = train_df.loc[train_df['id'].isin(IDS[train_idx])][['id','discourse_type', 'predictionstring']].reset_index(drop=True)\ntest_gt = train_df.loc[train_df['id'].isin(IDS[valid_idx])][['id','discourse_type', 'predictionstring']].reset_index(drop=True)\n\nprint(len(train_gt),len(test_gt))\ntrain_gt.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T21:51:19.816854Z","iopub.execute_input":"2022-02-18T21:51:19.817116Z","iopub.status.idle":"2022-02-18T21:51:19.883011Z","shell.execute_reply.started":"2022-02-18T21:51:19.817079Z","shell.execute_reply":"2022-02-18T21:51:19.882184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Class","metadata":{}},{"cell_type":"code","source":"class Dataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        LABEL_ALL_SUBTOKENS=True\n        \n        # GET TEXT AND WORD LABELS \n        text = self.data.text[index]        \n        word_labels = self.data.entities[index]\n        text_id = self.data.id[index]\n\n        # TOKENIZE TEXT (use is_split_into_words)\n        encoding = self.tokenizer(text.split(),\n                             is_split_into_words=True,\n                             #return_offsets_mapping=True, \n                             padding='max_length', \n                             truncation=True, \n                             max_length=self.max_len)\n        \n        # padding and prefix=None\n        # map token[0,0,0,1,2] to split['a.b','c','d']\n        word_ids = encoding.word_ids()  \n        \n        # CREATE TARGETS\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids: # tokens wise                       \n            if word_idx is None:# padding\n                label_ids.append(-100) # label ignored in loss\n            elif word_idx != previous_word_idx: # change word   \n                label_ids.append( labels_to_ids[word_labels[word_idx]] )\n            else: # same word\n                if LABEL_ALL_SUBTOKENS:\n                    label_ids.append( labels_to_ids[word_labels[word_idx]] )\n                else:\n                    label_ids.append(-100)\n            previous_word_idx = word_idx\n            \n        return {\n            'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n            'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),\n            'labels': torch.tensor(label_ids, dtype=torch.long),\n            'word_ids':str(word_ids),\n            'text_id':text_id\n        }\n\n\nclass DataModule(LightningDataModule):\n    def __init__(self, train_df, val_df, tokenizer, cfg=None):\n        super().__init__()\n        self.train_df = train_df\n        self.val_df = val_df\n        self.cfg = cfg\n        self.tokenizer = tokenizer\n        \n    \n    def setup(self,stage):\n        if stage == 'fit':\n            self.train_ds = Dataset(self.train_df, self.tokenizer, self.cfg.max_length)\n            self.valid_ds = Dataset(self.val_df,   self.tokenizer, self.cfg.max_length)\n\n    \n    def train_dataloader(self):\n        return DataLoader(\n            self.train_ds, batch_size=self.cfg.batch_size, \n            shuffle=True, num_workers=self.cfg.num_workers,\n            pin_memory=True\n            )\n    \n    def val_dataloader(self):\n        return DataLoader(\n            self.valid_ds, batch_size=self.cfg.val_batch_size, \n            shuffle=False, num_workers=self.cfg.num_workers,\n            pin_memory=True\n            )","metadata":{"execution":{"iopub.status.busy":"2022-02-18T21:51:19.884579Z","iopub.execute_input":"2022-02-18T21:51:19.884832Z","iopub.status.idle":"2022-02-18T21:51:19.90157Z","shell.execute_reply.started":"2022-02-18T21:51:19.884797Z","shell.execute_reply":"2022-02-18T21:51:19.900708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metric","metadata":{}},{"cell_type":"code","source":"def calc_overlap3(set_pred, set_gt):# List input -> TP list \n    \"\"\"\n    Calculates if the overlap between prediction and\n    ground truth is enough fora potential True positive\n    \"\"\"\n    # Length of each and intersection\n    try:\n        len_gt = len(set_gt)\n        len_pred = len(set_pred)\n        inter = len(set_gt & set_pred)\n        \n        overlap_1 = inter / len_gt\n        overlap_2 = inter/ len_pred\n        return overlap_1 >= 0.5 and overlap_2 >= 0.5\n    except:  # at least one of the input is NaN\n        return False\n\ndef score_feedback_comp_micro3(pred_df, gt_df, discourse_type):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df.loc[gt_df['discourse_type'] == discourse_type, \n                      ['id', 'predictionstring']].reset_index(drop=True)\n    pred_df = pred_df.loc[pred_df['discourse_type'] == discourse_type,\n                      ['id', 'predictionstring']].reset_index(drop=True)\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    \n    pred_df['predictionstring'] = [set(pred.split(' ')) for pred in pred_df['predictionstring']]\n    gt_df['predictionstring'] = [set(pred.split(' ')) for pred in gt_df['predictionstring']]\n    \n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on='id',\n                           right_on='id',\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    \n    overlaps = [calc_overlap3(*args) for args in zip(joined.predictionstring_pred, \n                                                     joined.predictionstring_gt)]\n    \n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    # we don't need to compute the match to compute the score\n    TP = joined.loc[overlaps]['gt_id'].nunique()\n    \n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    TPandFP = len(pred_df)\n    TPandFN = len(gt_df)\n    \n    #calc microf1\n    my_f1_score = 2*TP / (TPandFP + TPandFN)\n    return my_f1_score\n\n\ndef score_feedback_comp3(pred_df, gt_df, return_class_scores=False):\n    class_scores = {}\n    for discourse_type in gt_df.discourse_type.unique():\n        class_score = score_feedback_comp_micro3(pred_df, gt_df, discourse_type)\n        class_scores[discourse_type] = class_score\n    f1 = np.mean([v for v in class_scores.values()])\n    if return_class_scores:\n        return f1, class_scores\n    return f1\n\n#score_feedback_comp3(df_gt, df_gt,True)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T21:51:19.903509Z","iopub.execute_input":"2022-02-18T21:51:19.903767Z","iopub.status.idle":"2022-02-18T21:51:19.918986Z","shell.execute_reply.started":"2022-02-18T21:51:19.903731Z","shell.execute_reply":"2022-02-18T21:51:19.918246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class ModelModule(LightningModule):\n    def __init__(self, cfg,train_gt,test_gt):\n        super().__init__()\n        self.df_pred = pd.DataFrame(columns = ['id','discourse_type','predictionstring'])\n        self.df_gt_train = train_gt\n        self.df_gt_val = test_gt\n        \n        self.cfg=cfg\n        #self.save_hyperparameters(cfg)\n        config = AutoConfig.from_pretrained(self.cfg['modelpath'])\n        config.num_labels = self.cfg['num_labels']\n        self.model = AutoModelForTokenClassification.from_pretrained(self.cfg['modelpath'],config=config)\n        #self.model = AutoModel.from_pretrained(self.hparams.modelpath,config=config)\n       \n        #self.loss = nn.CrossEntropyLoss()\n\n    def forward(self, input_ids, attention_mask,labels):\n        out = self.model(input_ids=input_ids, attention_mask=attention_mask,labels=labels)\n        return out.loss,out.logits\n    \n    def training_step(self, batch, batch_idx):\n        loss,logits = self(batch['input_ids'], batch['attention_mask'],batch['labels']) #(N,seq,labels)    \n        pred = torch.argmax(logits, axis=2).cpu().detach().numpy() #(N,seq)\n        labels = batch['labels'].cpu().detach().numpy() # (N,seq)\n        \n        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        return {\n            'loss': loss,\n            'pred':pred,\n            'word_ids':batch['word_ids'],#(N,seq)\n            'text_id':batch['text_id'], # (N)\n            'labels':labels\n        }\n    \n    def validation_step(self, batch, batch_idx):\n        loss,logits = self(batch['input_ids'], batch['attention_mask'],batch['labels'])\n        pred = torch.argmax(logits, axis=2).cpu().detach().numpy() #(N,seq)\n        labels = batch['labels'].cpu().detach().numpy() # (N,seq)\n        \n        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        return {\n            'loss': loss,\n            'pred':pred,#(N,seq)\n            'word_ids':batch['word_ids'],#(N,seq)\n            'text_id':batch['text_id'], #(N)\n            'labels':labels #(N,seq)\n        }\n    \n    def training_epoch_end(self, output): # [dict1,dict2]:epoch,batch\n        pass\n        '''\n        if not self.df_gt_train.empty:\n            self.gt_trTrue=True\n        \n        for bat in output: # pred, word_ids, text_id,label\n            pred = bat['pred']\n            word_ids = [literal_eval(e) for e in bat['word_ids']]\n            text_id  = bat['text_id']\n            labels = bat['labels']\n            \n            for i in range(len(text_id)):\n                self.build_df(text_id[i],pred[i],labels[i],word_ids[i])\n        \n        f1 = score_feedback_comp3(self.df_pred,self.df_gt_train)\n        #self.logger.experiment.add_scalar('train_f1', f1, global_step=self.current_epoch)\n        self.log('train_f1', f1,prog_bar=True)\n        \n        self.df_pred = pd.DataFrame(columns = ['id','discourse_type','predictionstring'])\n        '''\n    \n    def validation_epoch_end(self,output):\n        for bat in output:\n            pred = bat['pred']\n            word_ids = [literal_eval(e) for e in bat['word_ids']]\n            text_id  = bat['text_id']\n            labels = bat['labels']\n            \n            for i in range(len(text_id)):\n                self.build_df(text_id[i],pred[i],labels[i],word_ids[i])\n        \n        f1 = score_feedback_comp3(self.df_pred,self.df_gt_val)\n        self.log('val_f1', f1,prog_bar=True)\n        self.df_pred = pd.DataFrame(columns = ['id','discourse_type','predictionstring'])\n    \n    \n    def build_df(self,id_, pred_, labels_, word_ids_):\n        #text_id,pred,labels,word_ids #self.df_pred #self.df_gt\n        \n        pred_ = [ids_to_labels[i] for i in pred_] # token wise\n        prediction = [] #word wise\n        \n        previous_word_idx = -1\n        for idx,word_idx in enumerate(word_ids_):                            \n            if word_idx!=None and word_idx != previous_word_idx:\n                # use only first subword pred  \n                prediction.append(pred_[idx])\n                previous_word_idx = word_idx\n        \n        j = 0\n        end = 0\n        while j < len(prediction):\n            if prediction[j]=='O':\n                j+=1\n            else:\n                cls = prediction[j].replace('B','I') # Take I and B\n                end = j + 1\n                while end < len(prediction) and prediction[end] == cls:\n                    end += 1\n                \n                if end - j > 3: # 7 to check\n                    self.df_pred = self.df_pred.append(\n                        pd.Series([id_, cls.replace('I-','') ,' '.join(map(str, list(range(j, end))))], index = self.df_pred.columns), \n                        ignore_index=True)\n                j = end\n         \n        \n    def configure_optimizers(self):      \n        optimizer = AdamW(\n            self.parameters(), lr=self.cfg['lr'],\n            #weight_decay=self.hparams.weight_decay\n            )\n\n        #CosineAnnealingLR(optimizer=opt, eta_min=self.hparams.min_lr, T_max=self.hparams.T_max)\n        scheduler = {\n            #'scheduler':CyclicLR(optimizer,base_lr=1e-7, max_lr=2e-2,step_size_up=self.hparams.steps_lr//2,mode=\"triangular2\",cycle_momentum=False),\n            'scheduler':OneCycleLR(optimizer,\n            max_lr=self.cfg['max_lr'],steps_per_epoch=self.cfg['steps_lr'], \n            epochs=self.cfg['epochs'], #pct_start =0.1,\n            ),\n            'name':self.cfg['scheduler'],\n            'interval':'step',\n            'frequency': 1\n            }\n\n        return [optimizer], [scheduler]  \n        \n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-02-18T21:51:19.920785Z","iopub.execute_input":"2022-02-18T21:51:19.921381Z","iopub.status.idle":"2022-02-18T21:51:19.947953Z","shell.execute_reply.started":"2022-02-18T21:51:19.921334Z","shell.execute_reply":"2022-02-18T21:51:19.947172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"###\ntag = 'ep{}-len{}'.format(CFG.epochs,CFG.max_length)\n\nCFG.steps_lr=len(train_dataset)//(CFG.batch_size) + 1\n\ntokenizer = AutoTokenizer.from_pretrained(CFG.tokpath,add_prefix_space=True)\n\ndm = DataModule(train_dataset, test_dataset, tokenizer, CFG)\nmodel = ModelModule(CFG.__dict__,train_gt,test_gt)\n\n\nfilename = f\"{CFG.model_name}-{tag}\"\ncheckpoint_callback = ModelCheckpoint(monitor='val_f1', dirpath='./', mode='max', filename=filename,save_top_k=1)\nlr_logger = LearningRateMonitor(logging_interval=\"step\")\n\n\ntrainer = Trainer(\n    gpus=CFG.n_procs,\n    max_epochs=CFG.epochs,\n    precision=CFG.precision,\n    num_sanity_val_steps=0,\n    callbacks=[checkpoint_callback,lr_logger],\n#    strategy=CFG.stg,\n#    log_every_n_steps=5,\n    )\n\ntrainer.fit(model, datamodule=dm)\n    \ndel model\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T21:51:19.949448Z","iopub.execute_input":"2022-02-18T21:51:19.949937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}