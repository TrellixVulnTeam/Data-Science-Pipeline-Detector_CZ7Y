{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.insert(0, '../input/feedbackdebertav2tokenizer')\nimport os\nimport torch\nimport torch as t\nimport random\ntorch.autograd.set_grad_enabled(False)\nfrom tqdm import tqdm\nfrom glob import glob\nimport pandas as pd\nimport numpy as np\nimport re\nimport pickle\nfrom transformers import (DebertaV2Model, DebertaV2TokenizerFast,\n                          RobertaTokenizerFast, DebertaModel, DebertaV2Config, AutoModel)\nfrom tqdm.notebook import tqdm\nfrom torch.nn import functional as F\nfrom time import sleep\nimport gc","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:45:47.773318Z","iopub.execute_input":"2022-03-15T14:45:47.773682Z","iopub.status.idle":"2022-03-15T14:45:49.547813Z","shell.execute_reply.started":"2022-03-15T14:45:47.773611Z","shell.execute_reply":"2022-03-15T14:45:49.547093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"average_folds_logits = True\nadd_models_logits = False\ntoken_len_filters = [ 6,  0, 15,  0,  7,  7,  7]\nscore_filters = [3.12230473, 3.16391113, 4.88337086, 2.95137306, 2.62062365,\n          1.80732187, 1.85651329]\nexts = [0, 1, 0, 2, 3, 4, 5, 6, 5, 7, 8, 8, 8, 8, 3]","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:45:49.55333Z","iopub.execute_input":"2022-03-15T14:45:49.553545Z","iopub.status.idle":"2022-03-15T14:45:49.558739Z","shell.execute_reply.started":"2022-03-15T14:45:49.553513Z","shell.execute_reply":"2022-03-15T14:45:49.558086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Debertav3(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.feats = DebertaV2Model.from_pretrained('../input/deberta-v3-large/deberta-v3-large')\n        self.feats.pooler = None\n        self.feats.train()\n\n        self.conv1d_layer1 = torch.nn.Conv1d(1024, 1024, kernel_size=1)\n        self.conv1d_layer3 = torch.nn.Conv1d(1024, 1024, kernel_size=3, padding=1)\n        self.conv1d_layer5 = torch.nn.Conv1d(1024, 1024, kernel_size=5, padding=2)\n\n        self.class_projector = torch.nn.Sequential(\n            torch.nn.LayerNorm(1024*3),\n            torch.nn.Linear(1024*3, 15)\n        )\n        \n    def forward(self, tokens, mask):\n        transformer_output = self.feats(tokens, mask, return_dict=False)[0]\n        conv_input = transformer_output.transpose(1, 2) # batch, hidden, seq\n\n        conv_output1 = F.relu(self.conv1d_layer1(conv_input)) \n        conv_output3 = F.relu(self.conv1d_layer3(conv_input)) \n        conv_output5 = F.relu(self.conv1d_layer5(conv_input)) \n\n        concat_output = torch.cat((conv_output1, conv_output3, conv_output5), dim=1).transpose(1, 2)\n\n        output = self.class_projector(concat_output)\n        return output\n    \nclass Debertav1Large(t.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.feats = DebertaModel.from_pretrained(\n            '../input/feedbackdebertav3dirgy/debertav1_pretrained_model')\n        self.feats.pooler = None\n        self.class_projector = t.nn.Sequential(\n            t.nn.LayerNorm(1024),\n            t.nn.Linear(1024, 15)\n        )\n    def forward(self, tokens, mask):\n        return self.class_projector(self.feats(tokens, mask, return_dict=False)[0])\n                                    \nclass Debertav1XLarge(t.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.feats = DebertaModel.from_pretrained(\n            '../input/debertav1xlarge/deberta_xlarge')\n        self.feats.pooler = None\n        self.class_projector = t.nn.Sequential(\n            t.nn.LayerNorm(1024),\n            t.nn.Linear(1024, 15)\n        )\n    def forward(self, tokens, mask):\n        return self.class_projector(self.feats(tokens, mask, return_dict=False)[0])\n\nclass Debertav2(t.nn.Module):\n    def __init__(self):\n        super().__init__()\n        config = DebertaV2Config.from_pretrained(\n            '../input/feedbackdebertav2stuff/model_config/config.json')\n        self.feats = AutoModel.from_config(config)\n        self.feats.pooler = None\n        self.class_projector = t.nn.Sequential(\n            t.nn.LayerNorm(1536),\n            t.nn.Linear(1536, 15)\n        )\n    def forward(self, tokens, mask):\n        return self.class_projector(self.feats(tokens, mask, return_dict=False)[0])","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:45:49.559946Z","iopub.execute_input":"2022-03-15T14:45:49.560556Z","iopub.status.idle":"2022-03-15T14:45:49.57891Z","shell.execute_reply.started":"2022-03-15T14:45:49.560518Z","shell.execute_reply":"2022-03-15T14:45:49.578219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, all_tokens, all_masks, all_bounds, all_index_maps, \n                 sample_ids, sorted_index, fix_cls_token):\n        self.fix_cls_token = fix_cls_token\n        self.all_tokens = all_tokens\n        self.all_masks = all_masks\n        self.all_bounds = all_bounds\n        self.all_index_maps = all_index_maps\n        self.sample_ids = sample_ids\n        self.sorted_index = sorted_index\n    \n    def __len__(self):\n        return len(self.sorted_index)\n    \n    def __getitem__(self, ix):\n        tokens = np.zeros(2048, 'i8')\n        mask = np.zeros(2048, 'f4')\n        offsets = np.zeros((2048, 2), 'i4')\n        x = self.sorted_index[ix]\n        index_map = self.all_index_maps[x]\n        key = self.sample_ids[x]\n        num_tokens = len(self.all_tokens[x])\n        tokens[:num_tokens] = self.all_tokens[x]\n        mask[:num_tokens] = self.all_masks[x]\n        offsets[:num_tokens] = self.all_bounds[x]\n        if self.fix_cls_token:\n            tokens[0] = 1\n        return tokens, mask, offsets, num_tokens, index_map, key\n    \ndef collate_fn(ins):\n    max_len = (max(x[3] for x in ins) + 7) // 8 * 8\n    batch = tuple(torch.from_numpy(np.concatenate(\n                        [ins[z][x][None, :max_len] for z in range(len(ins))]))\n                     for x in range(3))\n    extras = ([x[-2] for x in ins], [x[-1] for x in ins])\n    \n    return batch, tuple(x[3] for x in ins), extras","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:45:49.582553Z","iopub.execute_input":"2022-03-15T14:45:49.58274Z","iopub.status.idle":"2022-03-15T14:45:49.595775Z","shell.execute_reply.started":"2022-03-15T14:45:49.582718Z","shell.execute_reply":"2022-03-15T14:45:49.595022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_dataset(version, batch_size, fix_cls_token=False):\n    filenames = glob('../input/feedback-prize-2021/test/*.txt')\n    if version == 1:\n        tokenizer = RobertaTokenizerFast.from_pretrained(\n            '../input/feedbackdebertav3dirgy/roberta_tokenizer')\n        texts = [open(x).read().strip() for x in filenames]\n    else:\n        if version == 2:\n            tokenizer = DebertaV2TokenizerFast.from_pretrained(\n                '../input/feedbackdebertav2stuff/tokenizer')\n        else:\n            tokenizer = DebertaV2TokenizerFast.from_pretrained(\n                '../input/deberta-v3-large/deberta-v3-large')\n        texts = [open(x).read().strip().replace('\\n', '‽') for x in filenames]\n    tokenizer.model_max_length = 2048\n    all_tokenizer_outs = tokenizer(texts, return_offsets_mapping=True)\n    all_tokens = [all_tokenizer_outs[ix].ids for ix in range(len(texts))]\n    all_bounds = [all_tokenizer_outs[ix].offsets for ix in range(len(texts))]\n    all_masks = [all_tokenizer_outs[ix].attention_mask for ix in range(len(texts))]\n    del all_tokenizer_outs\n    \n    if version != 1:\n        if version == 2:\n            all_tokens = [[x if x != 126599 else 128000\n                                       for x in sample_tokens] for sample_tokens in all_tokens]\n        elif version == 3:\n            all_tokens = [[x if x != 126861 else 128000\n                                       for x in sample_tokens] for sample_tokens in all_tokens]\n        inverse_vocab = {y: x for x, y in tokenizer.vocab.items()}\n        for sample_ix in range(len(texts)):\n            num_tokens = len(all_tokens[sample_ix])\n            offset_mappings = [(0, 0)]\n            default_offset_mappings = all_bounds[sample_ix]\n            tokens = all_tokens[sample_ix]\n            for ix in range(1, num_tokens - 1):\n                a, b = default_offset_mappings[ix]\n                token = inverse_vocab[tokens[ix]]\n                if len(token) > 1 and token[0] == '▁' and ix != 1:\n                    a += 1\n                offset_mappings.append((a, b))\n            offset_mappings.append((0,0))\n            all_bounds[sample_ix] = offset_mappings\n    all_index_maps = []\n    space_regex = re.compile('[\\s\\n‽]')\n    for text in texts:\n        index_map = []\n        current_word = 0\n        blank = False\n        for char_ix in range(len(text)):\n            if space_regex.match(text[char_ix]) is not None:\n                blank = True\n            elif blank:\n                current_word += 1\n                blank = False\n            index_map.append(current_word)\n        all_index_maps.append(index_map)\n    sorted_index = sorted(range(len(texts)), key=lambda x: len(all_tokens[x]), \n                              reverse=True)\n    sample_ids = [x.split('/')[-1].split('.')[0] for x in filenames]\n    return t.utils.data.DataLoader(Dataset(all_tokens, all_masks, all_bounds, all_index_maps,\n                                           sample_ids, sorted_index, fix_cls_token), \n                                   batch_size=batch_size, collate_fn=collate_fn)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:45:49.597071Z","iopub.execute_input":"2022-03-15T14:45:49.59731Z","iopub.status.idle":"2022-03-15T14:45:49.616235Z","shell.execute_reply.started":"2022-03-15T14:45:49.597284Z","shell.execute_reply":"2022-03-15T14:45:49.615452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collect_ps(model_class, checkpoints, dataset, num_files):\n    all_outs = np.zeros((num_files, 2048, 15), 'f4')\n    all_bounds = np.zeros((num_files, 2048, 2), 'i4')\n    all_token_nums = np.zeros((num_files,), 'i4')\n    all_word_indices = []\n    all_sample_ids = []\n    \n    output_fn = t.log_softmax if average_folds_logits else t.softmax\n    model = model_class().eval().cuda()\n    for fold_ix in range(5):\n        ix = 0\n        model.load_state_dict(t.load(checkpoints[fold_ix], map_location='cuda:0'),\n                              strict=False)\n        for (tokens, mask, offsets), num_tokens, (word_indices, sample_ids) in tqdm(dataset):\n            bs = len(sample_ids)\n            outs = output_fn(model(tokens.cuda(), mask.cuda()), -1).cpu().numpy()\n            if fold_ix == 0:\n                all_word_indices.extend(word_indices)\n                all_sample_ids.extend(sample_ids)\n            for x in range(bs):\n                sample_num_tokens = num_tokens[x]\n                if fold_ix == 0:\n                    all_token_nums[ix] = sample_num_tokens - 2\n                    all_bounds[ix, :sample_num_tokens - 2] = offsets[x, 1: sample_num_tokens - 1]\n                all_outs[ix, :sample_num_tokens - 2] += .2 * outs[x, 1: sample_num_tokens - 1]\n                ix += 1\n    del model\n    return all_outs, all_bounds, all_token_nums, all_word_indices, all_sample_ids\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:45:49.61737Z","iopub.execute_input":"2022-03-15T14:45:49.617629Z","iopub.status.idle":"2022-03-15T14:45:49.628876Z","shell.execute_reply.started":"2022-03-15T14:45:49.617596Z","shell.execute_reply":"2022-03-15T14:45:49.628114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_entity_score(span, ps, c):\n    s, e = span\n    score = (ps[s, c * 2 - 1] + ps[s + 1: e + 1, c * 2].sum())/(e - s + 1)\n    return score","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:45:49.630094Z","iopub.execute_input":"2022-03-15T14:45:49.630496Z","iopub.status.idle":"2022-03-15T14:45:49.638721Z","shell.execute_reply.started":"2022-03-15T14:45:49.630461Z","shell.execute_reply":"2022-03-15T14:45:49.638083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def first_token_merge(gather_a, gather_b, bounds_a, bounds_b, logits_a, logits_b, new_bounds):\n    new_gather_a = [0]\n    new_gather_b = [0]\n    prev_same_a = False\n    prev_same_b = False\n    for ix in range(1, len(gather_a)):\n        same_a = gather_a[ix] == gather_a[ix - 1]\n        same_b = gather_b[ix] == gather_b[ix - 1]\n        if same_a:\n            assert not same_b\n            if prev_same_b and not prev_same_a and new_gather_a[-1] != gather_a[ix]:\n                new_gather_a.append(gather_a[ix])\n                new_gather_b.append(gather_b[ix])\n            else:\n                prev_same_a = same_a\n                prev_same_b = same_b\n                continue\n        elif same_b:\n            if prev_same_a and not prev_same_b and new_gather_b[-1] != gather_b[ix]:\n                new_gather_a.append(gather_a[ix])\n                new_gather_b.append(gather_b[ix])\n            else:\n                prev_same_a = same_a\n                prev_same_b = same_b\n                continue\n        else:\n            new_gather_a.append(gather_a[ix])\n            new_gather_b.append(gather_b[ix])\n        prev_same_a = same_a\n        prev_same_b = same_b\n    new_bounds = []\n    for index_tokens_a, index_tokens_b in zip(new_gather_a, new_gather_b):\n        new_bounds.append((min(bounds_a[index_tokens_a, 0],\n                               bounds_b[index_tokens_b, 0]), \n                           max(bounds_a[index_tokens_a, 1],\n                               bounds_b[index_tokens_b, 1])))\n    new_logits = logits_a[new_gather_a] + logits_b[new_gather_b]\n    return new_logits, np.array(new_bounds)\n\ndef merge_ps(logits_a, logits_b, bounds_a, bounds_b):\n    a = bounds_a\n    b = bounds_b\n    mapping_a = []\n    mapping_b = []\n    new_bounds = []\n    apos = 0\n    bpos = 0\n    a_s, a_e = a[apos]\n    b_s, b_e = b[bpos]\n    current_start = 0\n    while True:\n        if a_e == b_e:\n            new_bounds.append((current_start, b_e))\n            mapping_a.append(apos)\n            mapping_b.append(bpos)\n            if apos == len(a) - 1 or bpos == len(b) - 1:\n                break\n            next_a_s, next_b_s = a[apos + 1][0], b[bpos + 1][0]\n            if next_a_s < next_b_s:\n                apos += 1\n                a_s, a_e = a[apos]\n                current_start = a_s\n            elif next_b_s < next_a_s:\n                bpos += 1\n                b_s, b_e = b[bpos]\n                current_start = b_s\n            else:\n                apos += 1\n                bpos += 1\n                a_s, a_e = a[apos]\n                b_s, b_e = b[bpos]\n                current_start = a_s\n        elif a_e < b_e:\n            new_bounds.append((current_start, a_e))\n            mapping_a.append(apos)\n            mapping_b.append(bpos)\n            apos += 1\n            a_s, a_e = a[apos]\n            current_start = a_s\n        else:\n            new_bounds.append((current_start, b_e))\n            mapping_a.append(apos)\n            mapping_b.append(bpos)\n            bpos += 1\n            b_s, b_e = b[bpos]\n            current_start = b_s\n    \n    return first_token_merge(mapping_a, mapping_b, bounds_a, bounds_b, logits_a, logits_b, new_bounds)\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:45:49.640902Z","iopub.execute_input":"2022-03-15T14:45:49.641295Z","iopub.status.idle":"2022-03-15T14:45:49.660193Z","shell.execute_reply.started":"2022-03-15T14:45:49.641262Z","shell.execute_reply":"2022-03-15T14:45:49.659365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_files = len(glob('../input/feedback-prize-2021/test/*.txt'))\nall_outs = None\nall_bounds = None\nall_token_nums = None\nall_sample_ids = None\nall_word_indices = None\nv1xl_checkpoints = sorted(glob('../input/debertav1xlarge/clean*'), \n                        key=lambda x: int(x.split('/')[-1].split('_')[1][-1]))\nv1l_checkpoints = sorted(glob('../input/feedbackv2/debertav1/*'), \n                        key=lambda x: int(x.split('/')[-1].split('_')[0][-1]))\nv2_checkpoints = sorted(glob('../input/feedbackdebertav2xlargeweights/*'), \n                        key=lambda x: int(x.split('/')[-1].split('_')[0][-1]))\nv3_checkpoints = sorted(glob('../input/feedback-result/clean_nomalv3_scheduler_1dcnn_unk100_0.8_fold*'), \n                        key=lambda x: int(x.split('/')[-1].split('_')[6][-1]))\nfor dataset_version, checkpoints, model, batch_size, fix_cls_token in zip((1, 2, 3, 1),\n                                                           (v1xl_checkpoints, v2_checkpoints,\n                                                            v3_checkpoints, v1l_checkpoints),\n                                                           (Debertav1XLarge, Debertav2,\n                                                            Debertav3, Debertav1Large),\n                                                           ( 4, 4, 8, 8),\n                                                           (True, False, False, False)):\n    dataset = make_dataset(version=dataset_version, batch_size=batch_size,\n                           fix_cls_token=fix_cls_token)\n    (new_outs, new_bounds, new_token_nums,\n    new_word_indices, new_sample_ids) = collect_ps(model, checkpoints, \n                                                   dataset, num_files)\n    if average_folds_logits and not add_models_logits:\n        new_outs = np.exp(new_outs)\n    elif add_models_logits and not average_folds_logits:\n        new_outs = np.log(new_outs)\n    \n        \n    if all_outs is None:\n        all_outs = new_outs\n        all_bounds = new_bounds\n        all_token_nums = new_token_nums\n        all_word_indices = new_word_indices\n        all_sample_ids = new_sample_ids\n    else:\n        sample_id_to_ix = {x: ix for ix, x in enumerate(new_sample_ids)}\n        alignment_index = [sample_id_to_ix[x] for x in all_sample_ids] \n        merged_outs = []\n        merged_bounds = []\n        merged_token_nums = []\n        for sample_ix in range(len(all_outs)):\n            aligned_ix = alignment_index[sample_ix]\n            logits, bounds = merge_ps(all_outs[sample_ix][:all_token_nums[sample_ix]],\n                                      new_outs[aligned_ix][:new_token_nums[aligned_ix]],\n                                      all_bounds[sample_ix][:all_token_nums[sample_ix]],\n                                      new_bounds[aligned_ix][:new_token_nums[aligned_ix]])\n            merged_outs.append(logits)\n            merged_bounds.append(bounds)\n            merged_token_nums.append(len(logits))\n        all_outs = merged_outs\n        all_bounds = merged_bounds\n        all_token_nums = merged_token_nums\n    \n        del new_outs\n        del new_bounds\n        del new_token_nums\n        del new_word_indices\n        del new_sample_ids\n        \n    del dataset\n    gc.collect()\n    t.cuda.empty_cache()\n    sleep(10)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:45:49.66333Z","iopub.execute_input":"2022-03-15T14:45:49.66352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"START_WITH_I = True\nLOOK_AHEAD = True\n\n\ndef extract_entities(ps, n):\n    max_ps = ps.max(-1)\n    \n    ps = ps.argsort(-1)[...,::-1]\n    # argmax\n    cat_ps = ps[:, 0]\n    # argmax2\n    cat_ps2 = ps[:, 1]\n    \n    all_entities = {}\n    new_entity = True\n    current_cat = current_start = current_end = None\n    \n    # except for special tokens\n    for ix in range(n):\n\n        # logic on new entity\n        if new_entity:\n            # Background - ignore\n            if cat_ps[ix] == 0:\n                pass\n\n            # B-LABEL(1,3,5,7,...) - start entity\n            elif cat_ps[ix] % 2 == 1:\n                current_cat = (cat_ps[ix] + 1) // 2\n                current_start = current_end = ix\n                new_entity = False\n                \n                if current_cat in [6, 7]:\n                    LOOK_AHEAD = False\n                else:\n                    LOOK_AHEAD = True\n\n            # I-LABEL(2,4,6,8,...) - conditional start\n            elif cat_ps[ix] % 2 == 0:\n                if START_WITH_I:\n                    # Condition: I-LABEL in argmax with B-LABEL in argmax2\n                    if cat_ps[ix] == (cat_ps2[ix]+1):\n                        current_cat = cat_ps[ix] // 2\n                        current_start = current_end = ix\n                        new_entity = False\n                        \n                        if current_cat in [6, 7]:\n                            LOOK_AHEAD = False\n                        else:\n                            LOOK_AHEAD = True\n        \n        # logic on ongoing entity\n        else:\n            # Background - save current entity and init current\n            if cat_ps[ix] == 0:\n                if LOOK_AHEAD:\n                    if ix < n - 1 and (cat_ps[ix+1] == current_cat*2) and (cat_ps2[ix] == current_cat*2):\n                        current_end = ix\n                    else:\n                        # update current\n                        if current_cat not in all_entities:\n                            all_entities[current_cat] = []\n                        all_entities[current_cat].append((current_start, current_end))\n\n                        # init current for new start\n                        new_entity = True\n                        current_cat = current_start = current_end = None\n                \n                else:\n                    # update current\n                    if current_cat not in all_entities:\n                        all_entities[current_cat] = []\n                    all_entities[current_cat].append((current_start, current_end))\n\n                    # init current for new start\n                    new_entity = True\n                    current_cat = current_start = current_end = None\n\n            # B-LABEL(1,3,5,7,...) - save current entity and start new\n            elif cat_ps[ix] % 2 == 1:\n                if cat_ps[ix] == (current_cat*2-1):\n                    # update current\n                    if current_cat not in all_entities:\n                        all_entities[current_cat] = []\n                    all_entities[current_cat].append((current_start, current_end))\n\n                    # start new current\n                    current_cat = (cat_ps[ix] + 1) // 2\n                    current_start = current_end = ix\n                    new_entity = False\n                    \n                    if current_cat in [6, 7]:\n                        LOOK_AHEAD = False\n                    else:\n                        LOOK_AHEAD = True\n                \n                else:\n                    if LOOK_AHEAD:\n                        if ix < n - 1 and (cat_ps[ix+1] == current_cat*2) and (cat_ps2[ix] == current_cat*2):\n                            current_end = ix\n                        else:\n                            # update current\n                            if current_cat not in all_entities:\n                                all_entities[current_cat] = []\n                            all_entities[current_cat].append((current_start, current_end))\n\n                            # start new current\n                            current_cat = (cat_ps[ix] + 1) // 2\n                            current_start = current_end = ix\n                            new_entity = False\n                        \n                            if current_cat in [6, 7]:\n                                LOOK_AHEAD = False\n                            else:\n                                LOOK_AHEAD = True\n                            \n                    else:\n                        # update current\n                        if current_cat not in all_entities:\n                            all_entities[current_cat] = []\n                        all_entities[current_cat].append((current_start, current_end))\n\n                        # start new current\n                        current_cat = (cat_ps[ix] + 1) // 2\n                        current_start = current_end = ix\n                        new_entity = False\n                        \n                        if current_cat in [6, 7]:\n                            LOOK_AHEAD = False\n                        else:\n                            LOOK_AHEAD = True\n                \n            # I-LABEL(2,4,6,8,...) - conditional continue\n            elif cat_ps[ix] % 2 == 0:\n                # B-LABEL0, I-LABEL0 - continue\n                if cat_ps[ix] == current_cat*2:\n                    current_end = ix\n                # B-LBAEL0, I-LABEL1 - conditional finish current entity\n                else:\n                    if LOOK_AHEAD:\n                        if ix < n - 1 and (cat_ps[ix+1] == current_cat*2) and (cat_ps2[ix] == current_cat*2):\n                            current_end = ix\n                        else:\n                            # update current\n                            if current_cat not in all_entities:\n                                all_entities[current_cat] = []\n                            all_entities[current_cat].append((current_start, current_end))\n\n                            # init current\n                            new_entity = True\n                            current_cat = current_start = current_end = None\n                    else:\n                        # update current\n                        if current_cat not in all_entities:\n                            all_entities[current_cat] = []\n                        all_entities[current_cat].append((current_start, current_end))\n\n                        # init current\n                        new_entity = True\n                        current_cat = current_start = current_end = None\n    \n    # last entity\n    if not new_entity:\n        # update current\n        if current_cat not in all_entities:\n            all_entities[current_cat] = []\n        all_entities[current_cat].append((current_start, current_end))\n    \n    return all_entities\n\ndef filter_ps(all_entities, ps):\n    \n    for cat_ix, min_num_tokens, min_score in zip(range(1, 8), token_len_filters, score_filters):\n        \n        if cat_ix in all_entities:\n            possible_entities = [x for x \n                                 in all_entities[cat_ix] \n                                 if x[1] - x[0] + 1 >= min_num_tokens\n                        and calc_entity_score(x, ps, cat_ix) * (x[1] - x[0] + 1) ** .2 > min_score]\n    \n            if cat_ix in (1, 2, 5):\n                if len(possible_entities) > 1:\n                    max_score = -9999\n                    for x in possible_entities:\n                        entity_score = calc_entity_score(x, ps, cat_ix)\n                        if entity_score > max_score:\n                            max_score = entity_score\n                            biggest_entity = x\n                    possible_entities = [biggest_entity]\n            \n            all_entities[cat_ix] = possible_entities\n    \n    return all_entities.items()\n\n#clean\ndef extend_tokens(ent, n):\n    ent_size = ent[1] - ent[0] + 1\n    if ent_size > 15:\n        return [ent[0], min(n-1, ent[1] + exts[-1])]\n    return [ent[0], min(n-1, ent[1] + exts[max(0, ent_size - 2)])]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def map_span_to_word_indices(span, index_map, bounds):\n    return (index_map[bounds[span[0], 0]], index_map[bounds[span[1], 1] - 1])\n\nlabel_names = ['None', 'Lead', 'Position', 'Evidence', 'Claim',\n               'Concluding Statement', 'Counterclaim', 'Rebuttal']\n\n\nsub_sample_ids = []\nsub_cat_names = []\nsub_spans = []\nsub_scores = []\nfor sample_ix in tqdm(range(len(all_token_nums)), leave=False):\n    predicted_spans = \\\n        {x: {\n            'entity': [\n                map_span_to_word_indices(extend_tokens(span, all_token_nums[sample_ix]),\n                                         all_word_indices[sample_ix], all_bounds[sample_ix]) \n                for span \n                in y],\n            'scores': [\n                calc_entity_score(span, all_outs[sample_ix], x) \n                for span \n                in y],\n            }\n         for x, y in filter_ps(extract_entities(all_outs[sample_ix],  all_token_nums[sample_ix]), all_outs[sample_ix])}\n\n    for cat_ix in predicted_spans:\n        for entity in predicted_spans[cat_ix]['entity']:\n            sub_sample_ids.append(all_sample_ids[sample_ix])\n            sub_cat_names.append(label_names[cat_ix])\n            sub_spans.append(' '.join(str(x) for x in range(entity[0], entity[1] + 1)))\n        for scores in predicted_spans[cat_ix]['scores']:\n            sub_scores.append(scores)\n\nsub = pd.DataFrame({'id': sub_sample_ids, \n              'class': sub_cat_names,\n              'predictionstring': sub_spans,\n              'scores': sub_scores,})\n\n\nsub.drop('scores', axis=1).to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}