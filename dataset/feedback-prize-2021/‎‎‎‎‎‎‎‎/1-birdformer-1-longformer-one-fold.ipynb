{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I am sharing this note bbok to illustrate\n- how to convert token probability to word prediction (token to word aligment) for ensemble, I use different tokenizers for the two models.\n- results:\n\nfold0 \n- bigbird-large    : CV 0.652873\n- longformer-large : CV 0.672632\n- bigbird-large  + longformer-large : CV 0.683076 (LB 0.682)\n\nfold1 \n- bigbird-large    : CV 0.676453\n- longformer-large : CV 0.681581\n- bigbird-large  + longformer-large : CV 0.696001 (LB 0.681)\n\n\nfold0+fold1 : LB 0.690","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/feedback-prize-submit-01')\n\nimport numpy as np\nimport glob\nimport pandas as pd\nfrom timeit import default_timer as timer\n\nimport torch\nfrom torch.nn.parallel.data_parallel import data_parallel\n\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import *\n\n\nimport torch.cuda.amp as amp\nis_amp   = True  #True #False\nis_cuda  = True\nis_debug = False\n\n\n#helper\ndef time_to_str(t, mode='min'):\n    if mode=='min':\n        t  = int(t)/60\n        hr = t//60\n        min = t%60\n        return '%2d hr %02d min'%(hr,min)\n    elif mode=='sec':\n        t   = int(t)\n        min = t//60\n        sec = t%60\n        return '%2d min %02d sec'%(min,sec)\n    else:\n        raise NotImplementedError","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-20T17:54:49.394617Z","iopub.execute_input":"2022-02-20T17:54:49.395004Z","iopub.status.idle":"2022-02-20T17:54:51.025275Z","shell.execute_reply.started":"2022-02-20T17:54:49.394904Z","shell.execute_reply":"2022-02-20T17:54:51.024483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#config \n\ndiscourse_marker_to_label = {\n    'O': 0,\n    'B-Lead': 1,\n    'I-Lead': 2,\n    'B-Position': 3,\n    'I-Position': 4,\n    'B-Claim': 5,\n    'I-Claim': 6,\n    'B-Counterclaim': 7,\n    'I-Counterclaim': 8,\n    'B-Rebuttal': 9,\n    'I-Rebuttal': 10,\n    'B-Evidence': 11,\n    'I-Evidence': 12,\n    'B-Concluding Statement': 13,\n    'I-Concluding Statement': 14,\n    'IGNORE': -100,\n}\nlabel_to_discourse_marker = {v: k for k, v in discourse_marker_to_label.items()}\nnum_discourse_marker = 15 #len(label_to_discourse_marker)-1 #15\n\nlength_threshold = {\n    'Lead'                : 9,\n    'Position'            : 5,\n    'Claim'               : 3,\n    'Counterclaim'        : 6,\n    'Rebuttal'            : 4,\n    'Evidence'            : 14,\n    'Concluding Statement': 11,\n}\nprobability_threshold = {\n    'Lead'                : 0.70,\n    'Position'            : 0.55,\n    'Claim'               : 0.55,\n    'Counterclaim'        : 0.50,\n    'Rebuttal'            : 0.55,\n    'Evidence'            : 0.65,\n    'Concluding Statement': 0.70,\n}\nmax_length = 1600\n\n#-------------------------------\nsubmit_dir = ''\n\nif is_debug:\n    text_dir = '../input/feedback-prize-2021/train'\n    df = pd.read_csv('../input/feedback-prize-submit-01/train.fold10.csv')\n    valid_df = df[df['fold'] == 0].reset_index(drop=True)\n    valid_id = valid_df['id'].unique()\n\nelse:\n    text_dir = '../input/feedback-prize-2021/test'\n    valid_id = [ f.split('/')[-1][:-4] for f in glob.glob(text_dir+'/*.txt') ]\n    valid_id = sorted(valid_id)\nnum_valid = len(valid_id)\nprint('len(valid_id)',len(valid_id))\n\n# print(valid_id)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T17:54:51.028372Z","iopub.execute_input":"2022-02-20T17:54:51.028759Z","iopub.status.idle":"2022-02-20T17:54:52.555647Z","shell.execute_reply.started":"2022-02-20T17:54:51.028719Z","shell.execute_reply":"2022-02-20T17:54:52.554229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data\n\ndf_text=[]\nfor id in valid_id:\n    text_file = text_dir +'/%s.txt'%id\n    with open(text_file, 'r') as f:\n        text = f.read()\n\n    text = text.replace(u'\\xa0', u' ')\n    text = text.rstrip()\n    text = text.lstrip()\n    df_text.append((id,text))\ndf_text = pd.DataFrame(df_text, columns=['id','text'])\nprint('df_text.shape',df_text.shape)\nprint(df_text)\n\nclass FeedbackDataset(Dataset):\n    def __init__(self, df_text, tokenizer, max_length = 1600):\n\n        self.df_text  = df_text\n        self.max_length = max_length\n        self.tokenizer = tokenizer\n        self.length = len(self.df_text)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        d    = self.df_text.iloc[index]\n        id   = d['id']\n        text = d.text\n\n        #text to token\n        encoded = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n            max_length=4096, #<todo>\n            truncation=True,\n        )\n        token_id     =  encoded['input_ids']\n        token_offset =  encoded['offset_mapping']\n\n        # add end, start token id\n        token_id = [self.tokenizer.cls_token_id] + token_id\n        token_id = token_id[: self.max_length - 1]\n        token_id = token_id + [self.tokenizer.sep_token_id]\n\n        # padding\n        token_mask = [1] * len(token_id)\n\n        padding_length = max_length - len(token_id)\n        if padding_length > 0:\n            if self.tokenizer.padding_side == 'right':\n                token_id    = token_id    + [self.tokenizer.pad_token_id] * padding_length\n                token_mask  = token_mask  + [0] * padding_length\n            else:\n                raise NotImplementedError\n\n        #-------------------------------------\n        r = {}\n        r['index'] = index\n        r['id'   ] = id\n        r['text' ] = text\n        r['token_offset'] = str(token_offset) #force batch loader store as list\n        r['token_id'    ] = torch.tensor(token_id,    dtype=torch.long)\n        r['token_mask'  ] = torch.tensor(token_mask,  dtype=torch.long)\n        return r\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T17:54:52.556943Z","iopub.execute_input":"2022-02-20T17:54:52.557203Z","iopub.status.idle":"2022-02-20T17:54:57.264198Z","shell.execute_reply.started":"2022-02-20T17:54:52.557151Z","shell.execute_reply":"2022-02-20T17:54:57.263497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#net\n\nfrom bigbird_base_model import Net as BidBirdBaseNet\nfrom longformer_base_model import Net as LongformerBaseNet\nfrom bigbird_large_model import Net as BidBirdLargeNet\nfrom longformer_large_model import Net as LongformerLargeNet\n\n\ncheckpoint =[\n#     '../input/feedback-prize-submit-01/bigbird-base-10kf-1600-03-fold-0-00018000.model.pth',\n#     '../input/feedback-prize-submit-01/bigbird-base-10kf-1600-03-fold-1-00021000.model.pth',\n#     '../input/feedback-prize-submit-01/longformer-base-10kf-1600-03-fold-0-00021000.model.pth',\n#     '../input/feedback-prize-submit-01/longformer-base-10kf-1600-03-fold-1-00018000.model.pth',\n    '../input/feedback-prize-submit-01/bigbird-large-10kf-1600-03-fold-0-00015000.model.pth',  #4hr for 6 model\n    '../input/feedback-prize-submit-01/longformer-large-10kf-1600-03-fold-0-00018000.model.pth',\n]\n\nnet_type = [\n    #BidBirdBaseNet(arch='google/bigbird-roberta-base'),\n    #LongformerBaseNet(arch='allenai/longformer-base-4096'),\n    \n#     [BidBirdBaseNet,   '../input/feedback-prize-submit-01/google-bigbird-roberta-base' ],\n#     [BidBirdBaseNet,   '../input/feedback-prize-submit-01/google-bigbird-roberta-base' ],\n#     [LongformerBaseNet,'../input/feedback-prize-submit-01/allenai-longformer-base-4096'],\n#     [LongformerBaseNet,'../input/feedback-prize-submit-01/allenai-longformer-base-4096'],\n    [BidBirdLargeNet,   '../input/feedback-prize-submit-01/google-bigbird-roberta-large' ],\n    [LongformerLargeNet,'../input/feedback-prize-submit-01/allenai-longformer-large-4096'],\n]\nnum_net=len(net_type)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T17:54:57.266148Z","iopub.execute_input":"2022-02-20T17:54:57.266413Z","iopub.status.idle":"2022-02-20T17:54:57.554476Z","shell.execute_reply.started":"2022-02-20T17:54:57.266377Z","shell.execute_reply":"2022-02-20T17:54:57.553716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#processing\n\ndef text_to_word(text):\n    word = text.split()\n    word_offset = []\n\n    start = 0\n    for w in word:\n        r = text[start:].find(w)\n\n        if r==-1:\n            raise NotImplementedError\n        else:\n            start = start+r\n            end   = start+len(w)\n            word_offset.append((start,end))\n            #print('%32s'%w, '%5d'%start, '%5d'%r, text[start:end])\n        start = end\n\n    return word, word_offset\n\ndef word_probability_to_predict_df(text_to_word_probability, id):\n    len_word = len(text_to_word_probability)\n    word_predict = text_to_word_probability.argmax(-1)\n    word_score   = text_to_word_probability.max(-1)\n    predict_df = []\n\n    t = 0\n    while 1:\n        if word_predict[t] not in [\n            discourse_marker_to_label['O'],\n            discourse_marker_to_label['IGNORE'],\n        ]:\n            start = t\n            b_marker_label = word_predict[t]\n        else:\n            t = t+1\n            if t== len_word-1: break\n            continue\n\n        t = t+1\n        if t== len_word-1: break\n\n        #----\n        if   label_to_discourse_marker[b_marker_label][0]=='B':\n            i_marker_label = b_marker_label+1\n        elif label_to_discourse_marker[b_marker_label][0]=='I':\n            i_marker_label = b_marker_label\n        else:\n            raise NotImplementedError\n\n        while 1:\n            #print(t)\n            if (word_predict[t] != i_marker_label) or (t ==len_word-1):\n                end = t\n                prediction_string = ' '.join([str(i) for i in range(start,end)]) #np.arange(start,end).tolist()\n                discourse_type = label_to_discourse_marker[b_marker_label][2:]\n                discourse_score = word_score[start:end].tolist()\n                predict_df.append((id, discourse_type, prediction_string, str(discourse_score)))\n                #print(predict_df[-1])\n                break\n            else:\n                t = t+1\n                continue\n        if t== len_word-1: break\n\n    predict_df = pd.DataFrame(predict_df, columns=['id', 'class', 'predictionstring', 'score'])\n    return predict_df\n\ndef do_threshold(submit_df, use=['length','probability']):\n    df = submit_df.copy()\n    df = df.fillna('')\n\n    if 'length' in use:\n        df['l'] = df.predictionstring.apply(lambda x: len(x.split()))\n        for key, value in length_threshold.items():\n            #value=3\n            index = df.loc[df['class'] == key].query('l<%d'%value).index\n            df.drop(index, inplace=True)\n\n    if 'probability' in use:\n        df['s'] = df.score.apply(lambda x: np.mean(eval(x)))\n        for key, value in probability_threshold.items():\n            index = df.loc[df['class'] == key].query('s<%f'%value).index\n            df.drop(index, inplace=True)\n\n    df = df[['id', 'class', 'predictionstring']]\n    return df\n\n#evaluation for debug ----\n# https://www.kaggle.com/cpmpml/faster-metric-computation\n\ndef compute_overlap(predict, truth):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    # Length of each and intersection\n    try:\n        len_truth   = len(truth)\n        len_predict = len(predict)\n        intersect = len(truth & predict)\n        overlap1 = intersect/ len_truth\n        overlap2 = intersect/ len_predict\n        return (overlap1, overlap2)\n    except:  # at least one of the input is NaN\n        return (0, 0)\n\ndef compute_f1_score_one(predict_df, truth_df, discourse_type):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n\n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    t_df = truth_df.loc[truth_df['discourse_type'] == discourse_type,   ['id', 'predictionstring']].reset_index(drop=True)\n    p_df = predict_df.loc[predict_df['class'] == discourse_type,  ['id', 'predictionstring']].reset_index(drop=True)\n\n    p_df.loc[:,'predict_id'] = p_df.index\n    t_df.loc[:,'truth_id'] = t_df.index\n    p_df.loc[:,'predictionstring'] = [set(p.split(' ')) for p in p_df['predictionstring']]\n    t_df.loc[:,'predictionstring'] = [set(p.split(' ')) for p in t_df['predictionstring']]\n\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = p_df.merge(t_df,\n                           left_on='id',\n                           right_on='id',\n                           how='outer',\n                           suffixes=('_p','_t')\n                          )\n    overlap = [compute_overlap(*predictionstring) for predictionstring in zip(joined.predictionstring_p, joined.predictionstring_t)]\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['potential_TP'] = [(o[0] >= 0.5 and o[1] >= 0.5) for o in overlap]\n    joined['max_overlap' ] = [max(*o) for o in overlap]\n    joined_tp = joined.query('potential_TP').reset_index(drop=True)\n    tp_pred_ids = joined_tp\\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','truth_id'])['predict_id'].first()\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = set(joined['predict_id'].unique()) - set(tp_pred_ids)\n\n    matched_gt_ids   = joined_tp['truth_id'].unique()\n    unmatched_gt_ids = set(joined['truth_id'].unique()) -  set(matched_gt_ids)\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    f1 = TP / (TP + 0.5*(FP+FN))\n    return f1\n\ndef compute_lb_f1_score(predict_df, truth_df):\n    f1_score = {}\n    for discourse_type in truth_df.discourse_type.unique():\n        f1_score[discourse_type] = compute_f1_score_one(predict_df, truth_df, discourse_type)\n    #f1 = np.mean([v for v in class_scores.values()])\n    return f1_score\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T17:54:57.557974Z","iopub.execute_input":"2022-02-20T17:54:57.559249Z","iopub.status.idle":"2022-02-20T17:54:57.590145Z","shell.execute_reply.started":"2022-02-20T17:54:57.558358Z","shell.execute_reply":"2022-02-20T17:54:57.589343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## main submission function !!!!\ndef run_submit():\n    if is_debug: print(\"THIS IS DEBUG ####################################\")\n\n    results = []\n    for n in range(num_net):\n        Net, arch = net_type[n]\n        net = Net(arch)\n\n\n        net.load_state_dict(torch.load(checkpoint[n], map_location=lambda storage, loc: storage)['state_dict'],strict=False)\n        tokenizer = net.get_tokenizer()\n        if is_cuda:\n            net.cuda()\n        print('load ok : [%d] %s'%(n,net.arch))\n        print('              %s'%(checkpoint[n]))\n\n        valid_dataset = FeedbackDataset(df_text, tokenizer, max_length)\n        valid_loader  = DataLoader(\n            valid_dataset,\n            sampler = SequentialSampler(valid_dataset),\n            batch_size  = 8, #4, #\n            drop_last   = False,\n            num_workers = 2, #0, #\n            pin_memory  = False,\n            #collate_fn = null_collate_fn,\n        )\n\n        # start here !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1\n        results_n = {\n            'id':[],\n            'token_mask':[],\n            'token_offset':[],\n            'probability':[],\n        }\n\n        T = 0\n        start_timer = timer()\n        for t, batch in enumerate(valid_loader):\n            batch_size = len(batch['index'])\n            token_mask = batch['token_mask']\n            token_id   = batch['token_id']\n            if is_cuda:\n                token_mask = token_mask.cuda()\n                token_id = token_id.cuda()\n\n            net.eval()\n            with torch.no_grad():\n                with amp.autocast(enabled=is_amp):\n                    probability = data_parallel(net,(token_id, token_mask))\n                    #probability = net[n](token_id, token_mask)\n\n                    results_n['probability'].append( (probability*255).byte().data.cpu().numpy() )\n                    results_n['token_offset' ] += [eval(x) for x in batch['token_offset']]\n                    results_n['token_mask' ].append( token_mask.bool().data.cpu().numpy())\n                    results_n['id' ] += batch['id']\n                    T += batch_size\n\n            print('\\r\\t%d/%d  %s'%(T, len(valid_dataset), time_to_str(timer() - start_timer,'sec')),end='',flush=True)\n\n        #----------------------------\n        if is_cuda: torch.cuda.empty_cache()\n        print('')\n\n        results.append({\n            'probability' : np.concatenate(results_n['probability']),\n            'token_offset': np.array(results_n['token_offset'], object)\n            #'token_mask'] = np.concatenate(results['token_mask'])\n            #'id'] = np.array(results['id' ])\n        })\n    #------------------------------------------------------------------------\n\n    submit_df = []\n    for i in range(num_valid):\n        d  = df_text.iloc[i]\n        id = d.id\n        text = d.text\n        word, word_offset = text_to_word(text)\n        #print(i,id[i], len(text), len(word))\n\n        #ensemble -----\n        token_to_text_probability = np.full((len(text),num_discourse_marker),0, np.float32)\n        for j in range(num_net):\n            p = results[j]['probability'][i][1:]/255\n            for t,(start,end) in enumerate(results[j]['token_offset'][i]):\n                if t==max_length-1: break #assume max_length, else use token_mask to get length\n                token_to_text_probability[start:end]+=p[t] #**0.5\n        token_to_text_probability = token_to_text_probability/num_net\n        #ensemble -----\n\n\n        text_to_word_probability = np.full((len(word),num_discourse_marker),0, np.float32)\n        for t,(start,end) in enumerate(word_offset):\n            text_to_word_probability[t]=token_to_text_probability[start:end].mean(0)\n\n        predict_df = word_probability_to_predict_df(text_to_word_probability, id)\n        submit_df.append(predict_df)\n        #print('\\r preparing submit_df :', i, id, len(text), len(word), end ='', flush=True)\n    print('')\n\n    #----------------------------------------\n    submit_df = pd.concat(submit_df).reset_index(drop=True) \n    submit_df = do_threshold(submit_df, use=['length', 'probability'])\n    submit_df.to_csv('submission.csv', index=False)\n\n    print('----')\n    for t in range(3): print(submit_df.iloc[t],'\\n')\n    print('submission ok!----')\n    if is_debug:\n        f1_score = compute_lb_f1_score(submit_df, valid_df)\n        print('f1 macro : %f\\n' % np.mean([v for v in f1_score.values()]))\n        for k,v in f1_score.items():\n            print('%20s : %05f'%(k,v))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T17:54:57.593537Z","iopub.execute_input":"2022-02-20T17:54:57.593739Z","iopub.status.idle":"2022-02-20T17:54:57.619281Z","shell.execute_reply.started":"2022-02-20T17:54:57.593715Z","shell.execute_reply":"2022-02-20T17:54:57.618561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check function\ndef run_check_dataset():\n\n    tokenizer = net[0].get_tokenizer()\n    dataset = FeedbackDataset(df_text, tokenizer, max_length)\n\n    for i in range(5):\n        r = dataset[i]\n        print(r['index'],'-----------')\n        for k in ['token_id', 'token_mask']:\n            v = r[k]\n            print(k)\n            print('\\t',v.shape, v.is_contiguous())\n            print('\\t',v)\n        print('')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T17:54:57.62062Z","iopub.execute_input":"2022-02-20T17:54:57.621067Z","iopub.status.idle":"2022-02-20T17:54:57.630136Z","shell.execute_reply.started":"2022-02-20T17:54:57.621025Z","shell.execute_reply":"2022-02-20T17:54:57.629463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#run_check_dataset()\nrun_submit()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T17:54:57.631474Z","iopub.execute_input":"2022-02-20T17:54:57.632248Z","iopub.status.idle":"2022-02-20T18:26:31.898658Z","shell.execute_reply.started":"2022-02-20T17:54:57.632193Z","shell.execute_reply":"2022-02-20T18:26:31.89786Z"},"trusted":true},"execution_count":null,"outputs":[]}]}