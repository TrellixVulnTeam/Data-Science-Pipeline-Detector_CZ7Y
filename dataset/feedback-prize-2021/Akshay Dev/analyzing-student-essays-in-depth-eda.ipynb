{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# __Introduction__","metadata":{}},{"cell_type":"markdown","source":"The dataset contains argumentative essays written by U.S students in grades 6-12. The essays were annotated by expert raters for elements commonly found in argumentative writing.\n\nThe goal of this competition is to come up with a model that can accurately segment according to the categories listed below. \n\n* Lead - an introduction that begins with a statistic, a quotation, a description, or some other device to grab the readerâ€™s attention and point toward the thesis\n* Position - an opinion or conclusion on the main question\n* Claim - a claim that supports the position\n* Counterclaim - a claim that refutes another claim or gives an opposing reason to the position\n* Rebuttal - a claim that refutes a counterclaim\n* Evidence - ideas or examples that support claims, counterclaims, or rebuttals.\n* Concluding Statement - a concluding statement that restates the claims\n\nThis Notebook contains **Comprehensive EDA** in a bid to surface any linguistic patterns that might occur class-wise; that could be leveraged for segmentation","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://i2.wp.com/www.iedunote.com/img/20535/essay-writing-made-easy.jpg?fit=1920%2C1280&quality=100&ssl=1\" style=\"width:500px;height:500px;img-align:center;\">","metadata":{}},{"cell_type":"markdown","source":"### Importing required libraries ðŸ“š","metadata":{}},{"cell_type":"code","source":"import pandas as pd\npd.set_option(\"display.max_colwidth\", -1)\nimport numpy as np\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nfrom spacy import displacy\nfrom textblob import TextBlob\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport re\n\nimport seaborn as sns\nimport matplotlib.style as style \nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport scattertext as st\nfrom IPython.display import IFrame\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\n\nimport eli5\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:05:12.755882Z","iopub.execute_input":"2022-01-08T15:05:12.756297Z","iopub.status.idle":"2022-01-08T15:05:27.055156Z","shell.execute_reply.started":"2022-01-08T15:05:12.756191Z","shell.execute_reply":"2022-01-08T15:05:27.053696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# __EDA__ ðŸ“Š","metadata":{}},{"cell_type":"markdown","source":"### Loading in the dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/feedback-prize-2021/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:06:40.249222Z","iopub.execute_input":"2022-01-08T15:06:40.249581Z","iopub.status.idle":"2022-01-08T15:06:42.385043Z","shell.execute_reply.started":"2022-01-08T15:06:40.249545Z","shell.execute_reply":"2022-01-08T15:06:42.384152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Overview of the dataset","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T15:39:54.820732Z","iopub.execute_input":"2022-01-08T15:39:54.821138Z","iopub.status.idle":"2022-01-08T15:39:54.836899Z","shell.execute_reply.started":"2022-01-08T15:39:54.821096Z","shell.execute_reply":"2022-01-08T15:39:54.835885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The column descriptions are:\n\n* id - ID code for essay response\n* discourse_id - ID code for discourse element\n* discourse_start - character position where discourse element begins in the essay response\n* discourse_end - character position where discourse element ends in the essay response\n* discourse_text - text of discourse element\n* discourse_type - classification of discourse element\n* discourse_type_num - enumerated class label of discourse element\n* predictionstring - the word indices of the training sample, as required for predictions","metadata":{}},{"cell_type":"markdown","source":"### Check for null values","metadata":{}},{"cell_type":"code","source":"df.isnull().sum().head()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:07:39.428491Z","iopub.execute_input":"2022-01-08T15:07:39.42919Z","iopub.status.idle":"2022-01-08T15:07:39.490779Z","shell.execute_reply.started":"2022-01-08T15:07:39.42913Z","shell.execute_reply":"2022-01-08T15:07:39.489871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Number of unqiue essays to work with","metadata":{}},{"cell_type":"code","source":"print(f\"We have {df['id'].nunique()} essays\")","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:07:25.261078Z","iopub.execute_input":"2022-01-08T15:07:25.261428Z","iopub.status.idle":"2022-01-08T15:07:25.285286Z","shell.execute_reply.started":"2022-01-08T15:07:25.26139Z","shell.execute_reply":"2022-01-08T15:07:25.283942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of target","metadata":{}},{"cell_type":"code","source":"sns.set_theme(style=\"darkgrid\")\nsns.set_palette('rainbow')\nplt.figure(figsize=(8,6))\nsns.barplot(y=df['discourse_type'].unique(),\n            x=df['discourse_type'].value_counts())\nplt.xlabel('Count of Discourse Type')\nplt.title('Target Distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:07:50.948812Z","iopub.execute_input":"2022-01-08T15:07:50.949138Z","iopub.status.idle":"2022-01-08T15:07:51.253014Z","shell.execute_reply.started":"2022-01-08T15:07:50.949103Z","shell.execute_reply":"2022-01-08T15:07:51.251986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Basic text cleaning for removing stopwords, punctuation and any non-alphanumeric characters","metadata":{}},{"cell_type":"code","source":"def clean_text(sentence):\n    sentence = sentence.lower()\n    sentence = ' '.join(re.sub(\"(nan)|(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", sentence).split())\n    return sentence\n\ndf['discourse_text'] = df['discourse_text'].map(clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:08:54.466192Z","iopub.execute_input":"2022-01-08T15:08:54.466627Z","iopub.status.idle":"2022-01-08T15:08:59.496933Z","shell.execute_reply.started":"2022-01-08T15:08:54.466582Z","shell.execute_reply":"2022-01-08T15:08:59.496045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's first take a look at average sentence lengths of each class","metadata":{}},{"cell_type":"code","source":"df_claim['discourse_text_len'] = df_claim['discourse_text'].apply(lambda x: len(x))\ndf_evidence['discourse_text_len'] = df_evidence['discourse_text'].apply(lambda x: len(x))\ndf_position['discourse_text_len'] = df_position['discourse_text'].apply(lambda x: len(x))\ndf_concluding_statement['discourse_text_len'] = df_concluding_statement['discourse_text'].apply(lambda x: len(x))\ndf_lead['discourse_text_len'] = df_lead['discourse_text'].apply(lambda x: len(x))\ndf_counterclaim['discourse_text_len'] = df_counterclaim['discourse_text'].apply(lambda x: len(x))\ndf_rebuttal['discourse_text_len'] = df_rebuttal['discourse_text'].apply(lambda x: len(x))\n\nclaim_avg = df_claim['discourse_text_len'].sum()/len(df_claim['discourse_text'])\nevidence_avg = df_evidence['discourse_text_len'].sum()/len(df_evidence['discourse_text'])\nposition_avg = df_position['discourse_text_len'].sum()/len(df_position['discourse_text'])\nconcluding_statement_avg = df_concluding_statement['discourse_text_len'].sum()/len(df_concluding_statement['discourse_text'])\nlead_avg = df_lead['discourse_text_len'].sum()/len(df_lead['discourse_text'])\ncounterclaim_avg = df_counterclaim['discourse_text_len'].sum()/len(df_counterclaim['discourse_text'])\nrebuttal_avg = df_rebuttal['discourse_text_len'].sum()/len(df_counterclaim['discourse_text'])\n\nsent_list = [claim_avg,evidence_avg,position_avg,concluding_statement_avg,lead_avg,counterclaim_avg,rebuttal_avg]\n\nsent_df = {'Classes':['Claim', 'Evidence', 'Position', 'Concluding Statement','Lead','Counterclaim','Rebuttal'],\n        'Length': sent_list}\nsent_df = pd.DataFrame(sent_df)\n\n\nsns.set_style('darkgrid')\nstyle.use('seaborn-pastel')\n\nplot = sns.catplot(\n    data=sent_df, kind=\"bar\",\n    x=\"Classes\", y=\"Length\",height = 8, aspect=9.7/6.27,\n    legend=False\n)\n\nplot.fig.suptitle(\"Average Sentence Length\");\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T15:20:57.898885Z","iopub.execute_input":"2022-01-08T15:20:57.899737Z","iopub.status.idle":"2022-01-08T15:20:58.435589Z","shell.execute_reply.started":"2022-01-08T15:20:57.899689Z","shell.execute_reply":"2022-01-08T15:20:58.434888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ---> Very uneven distribution, Evidence has a substantially higher average sentence length than the other classes, followed by Concluding Statement and Lead. \n\n### ---> Lead corresponds to the introduction, Concluding Statement is the ending and Evidence is the crux of an argumentative essay, so it stands to reason why they would be the top 3 longest classes.\n\n### ---> Classes similar to each other, Claim/Position and Counterclaim/Rebuttal are around the same vicinity ","metadata":{}},{"cell_type":"markdown","source":"### Textblob library let's us compute the subjectivity of a sentence, taking this measure class-wise might shed some insights. Higher the score, higher the subjectivity ","metadata":{}},{"cell_type":"code","source":"def get_subjectivity(text):\n    return TextBlob(text).sentiment.subjectivity\n\ndf_claim = df.loc[df['discourse_type'] == 'Claim']\nclaim_score = df_claim['discourse_text'].astype(str).apply(get_subjectivity).sum()\n\ndf_evidence = df.loc[df['discourse_type'] == 'Evidence']\nevidence_score = df_evidence['discourse_text'].astype(str).apply(get_subjectivity).sum()\n\ndf_position  = df.loc[df['discourse_type'] == \"Position\"]\nposition_score = df_position['discourse_text'].astype(str).apply(get_subjectivity).sum()\n\n\ndf_concluding_statement = df.loc[df['discourse_type'] == 'Concluding Statement']\nconcluding_statement_score = df_concluding_statement['discourse_text'].astype(str).apply(get_subjectivity).sum()\n\ndf_lead = df.loc[df['discourse_type'] == \"Lead\"]\nlead_score = df_lead['discourse_text'].astype(str).apply(get_subjectivity).sum()\n\ndf_counterclaim = df.loc[df['discourse_type'] == 'Counterclaim']\ncounterclaim_score = df_counterclaim['discourse_text'].astype(str).apply(get_subjectivity).sum()\n\ndf_rebuttal = df.loc[df['discourse_type'] == 'Rebuttal']\nrebuttal_score = df_rebuttal['discourse_text'].astype(str).apply(get_subjectivity).sum()\n\nscores_list = [claim_score,evidence_score,position_score,concluding_statement_score,lead_score,counterclaim_score,rebuttal_score]\n\nscores_df = {'Classes':['Claim', 'Evidence', 'Position', 'Concluding Statement','Lead','Counterclaim','Rebuttal'],\n        'Count':scores_list}\nscores_df = pd.DataFrame(scores_df)\n\n\nsns.set_style('darkgrid')\nstyle.use('seaborn-pastel')\n\n\nplot = sns.catplot(\n    data=scores_df, kind=\"bar\",\n    x=\"Classes\", y=\"Count\",height = 8, aspect=9.7/6.27,\n    legend=False\n)\n\nplot.fig.suptitle(\"Subjectivity Score\");\n\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T15:09:22.507845Z","iopub.execute_input":"2022-01-08T15:09:22.508321Z","iopub.status.idle":"2022-01-08T15:10:10.405931Z","shell.execute_reply.started":"2022-01-08T15:09:22.508288Z","shell.execute_reply":"2022-01-08T15:10:10.404669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ---> Evidence has around 5k data points less than Claims but still outscores it substantially when it comes to subjectivity. Ironically evidential statements should be less subjective than mere claims\n\n### ---> Same case with the Position and Concluding Statement pair; a concluding statement should have *less* subjectivity than a position statment, where mostly factual statements are recanted.\n\n### ---> Both Counterclaim and Rebuttal are essentially contradicting an opposing statement with fact, maybe why they have almost the same subjectivity score(accounting for the difference in data points)\n\n### ---> Lead's relatively low subjectivity score can be explained by the fact that a lot opening's start with a fact, a statistic or paraphrasing a quote","metadata":{}},{"cell_type":"markdown","source":"### Wordcloud sorted by class to get the most frequent words","metadata":{}},{"cell_type":"code","source":"wc_claim = WordCloud(max_font_size=50, max_words=100, background_color=\"white\", collocation_threshold = 3).generate(claim_corpus)\nwc_evidence = WordCloud(max_font_size=50, max_words=100, background_color=\"white\",collocation_threshold = 3).generate(evidence_corpus)\nwc_position = WordCloud(max_font_size=50, max_words=100, background_color=\"white\",collocation_threshold = 3).generate(position_corpus)\nwc_lead = WordCloud(max_font_size=50, max_words=100, background_color=\"white\",collocation_threshold = 3).generate(lead_corpus)\nwc_concluding_statement = WordCloud(max_font_size=50, max_words=100, background_color=\"white\",collocation_threshold = 3).generate(concluding_statement_corpus)\nwc_counterclaim = WordCloud(max_font_size=50, max_words=100, background_color=\"white\",collocation_threshold = 3).generate(counterclaim_corpus)\nwc_rebuttal = WordCloud(max_font_size=50, max_words=100, background_color=\"white\",collocation_threshold = 3).generate(rebuttal_corpus)\n\nfig = plt.figure(figsize=(12,6))\n\nax1 = fig.add_subplot(121)\nax1 = plt.imshow(wc_claim,interpolation=\"bilinear\")\nax1 = plt.title(\"Claim\", fontsize=20)\n\nax2 = fig.add_subplot(122)\nax2 = plt.imshow(wc_evidence,interpolation=\"bilinear\")\nax2 = plt.title(\"Evidence\", fontsize=20)\n\nfig = plt.figure(figsize=(12,6))\n\nax1 = fig.add_subplot(121)\nax1 = plt.imshow(wc_position,interpolation=\"bilinear\")\nax1 = plt.title(\"Position\", fontsize=20)\n\nax2 = fig.add_subplot(122)\nax2 = plt.imshow(wc_lead,interpolation=\"bilinear\")\nax2 = plt.title(\"Lead\", fontsize=20)\n\nfig = plt.figure(figsize=(12,6))\n\nax1 = fig.add_subplot(121)\nax1 = plt.imshow(wc_concluding_statement,interpolation=\"bilinear\")\nax1 = plt.title(\"Concluding Statement\", fontsize=20)\n\nax2 = fig.add_subplot(122)\nax2 = plt.imshow(wc_counterclaim,interpolation=\"bilinear\")\nax2 = plt.title(\"Counterclaim\", fontsize=20)\n\n\nfig = plt.figure(figsize=(12,6))\n\nax1 = fig.add_subplot(121)\nax1 = plt.imshow(wc_rebuttal,interpolation=\"bilinear\")\nax1 = plt.title(\"Rebuttal\", fontsize=20)\n\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:39:25.31045Z","iopub.execute_input":"2022-01-08T15:39:25.310737Z","iopub.status.idle":"2022-01-08T15:39:44.795509Z","shell.execute_reply.started":"2022-01-08T15:39:25.310704Z","shell.execute_reply":"2022-01-08T15:39:44.794543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ---> Commonly used words like 'people', 'think', 'student' are the most frequent, although the frequency of the word, 'Electoral' across multiple classes is a bit surprising. \n\n### ---> This suggests the variance in the topic of essay's isn't much and despite containing essays by students ranging from 6th grade to 12th grade, the vocabulary level is pretty consistent at middle-school level.","metadata":{}},{"cell_type":"markdown","source":"### The RAKE a.k.a Rapid Automatic Keyword Extraction algorithm does a decent job of extracting top phrases from a corpus, we use the RAKE-NLTK library to perform this task for each of the classes and spaCy to highlight the type of entities present in the **top 100** phrases.","metadata":{}},{"cell_type":"code","source":"!pip install rake-nltk\nfrom rake_nltk import Rake\nr = Rake()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:22:12.513437Z","iopub.execute_input":"2022-01-08T15:22:12.514529Z","iopub.status.idle":"2022-01-08T15:22:24.491868Z","shell.execute_reply.started":"2022-01-08T15:22:12.514449Z","shell.execute_reply":"2022-01-08T15:22:24.490803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Top Phrases for Claim","metadata":{}},{"cell_type":"code","source":"claim_corpus = df_claim['discourse_text'].tolist()\nclaim_corpus = \" \".join(claim_corpus)\nr.extract_keywords_from_text(claim_corpus)\ntop_claims = r.get_ranked_phrases()[0:100]\ntop_claims = \" \".join(top_claims)\ndisplacy.render(nlp(top_claims),style='ent',jupyter=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T15:38:06.411625Z","iopub.execute_input":"2022-01-08T15:38:06.413037Z","iopub.status.idle":"2022-01-08T15:38:09.067705Z","shell.execute_reply.started":"2022-01-08T15:38:06.412964Z","shell.execute_reply":"2022-01-08T15:38:09.066556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Top phrases for Evidence","metadata":{}},{"cell_type":"code","source":"evidence_corpus = df_evidence['discourse_text'].tolist()\nevidence_corpus = \" \".join(evidence_corpus)\nr.extract_keywords_from_text(evidence_corpus)\ntop_evidence = r.get_ranked_phrases()[0:100]\ntop_evidence = \" \".join(top_evidence)\ndisplacy.render(nlp(top_evidence),style='ent')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T15:26:33.61563Z","iopub.execute_input":"2022-01-08T15:26:33.615968Z","iopub.status.idle":"2022-01-08T15:26:45.879582Z","shell.execute_reply.started":"2022-01-08T15:26:33.615926Z","shell.execute_reply":"2022-01-08T15:26:45.878631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Top phrases for Position","metadata":{}},{"cell_type":"code","source":"position_corpus = df_position['discourse_text'].tolist()\nposition_corpus = \" \".join(position_corpus)\nr.extract_keywords_from_text(position_corpus)\ntop_position = r.get_ranked_phrases()[0:100]\ntop_position = \" \".join(top_position)\ndisplacy.render(nlp(top_position),style='ent')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T15:26:45.880993Z","iopub.execute_input":"2022-01-08T15:26:45.881829Z","iopub.status.idle":"2022-01-08T15:26:47.233723Z","shell.execute_reply.started":"2022-01-08T15:26:45.881771Z","shell.execute_reply":"2022-01-08T15:26:47.233038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Top Phrases for Concluding Statement","metadata":{}},{"cell_type":"code","source":"concluding_statement_corpus = df_concluding_statement['discourse_text'].tolist()\nconcluding_statement_corpus = \" \".join(concluding_statement_corpus)\nr.extract_keywords_from_text(concluding_statement_corpus)\ntop_concluding_statement = r.get_ranked_phrases()[0:100]\ntop_concluding_statement = \" \".join(top_concluding_statement)\ndisplacy.render(nlp(top_concluding_statement),style='ent')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T15:26:47.23561Z","iopub.execute_input":"2022-01-08T15:26:47.236409Z","iopub.status.idle":"2022-01-08T15:26:49.725584Z","shell.execute_reply.started":"2022-01-08T15:26:47.236359Z","shell.execute_reply":"2022-01-08T15:26:49.724562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Top Phrases for Lead Statement","metadata":{}},{"cell_type":"code","source":"lead_corpus = df_lead['discourse_text'].tolist()\nlead_corpus = \" \".join(lead_corpus)\nr.extract_keywords_from_text(lead_corpus)\ntop_lead = r.get_ranked_phrases()[0:100]\ntop_lead = \" \".join(top_lead)\ndisplacy.render(nlp(top_lead),style='ent')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T15:26:49.726813Z","iopub.execute_input":"2022-01-08T15:26:49.727069Z","iopub.status.idle":"2022-01-08T15:26:51.340039Z","shell.execute_reply.started":"2022-01-08T15:26:49.727036Z","shell.execute_reply":"2022-01-08T15:26:51.339364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Top Phrases for Counterclaim ","metadata":{}},{"cell_type":"code","source":"counterclaim_corpus = df_counterclaim['discourse_text'].tolist()\ncounterclaim_corpus = \" \".join(counterclaim_corpus)\nr.extract_keywords_from_text(counterclaim_corpus)\ntop_counterclaim = r.get_ranked_phrases()[0:100]\ntop_counterclaim = \" \".join(top_counterclaim)\ndisplacy.render(nlp(top_counterclaim),style='ent')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T15:26:51.34122Z","iopub.execute_input":"2022-01-08T15:26:51.34156Z","iopub.status.idle":"2022-01-08T15:26:51.90296Z","shell.execute_reply.started":"2022-01-08T15:26:51.341531Z","shell.execute_reply":"2022-01-08T15:26:51.902345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Top Phrases for Rebuttal ","metadata":{}},{"cell_type":"code","source":"rebuttal_corpus = df_rebuttal['discourse_text'].tolist()\nrebuttal_corpus = \" \".join(rebuttal_corpus)\nr.extract_keywords_from_text(rebuttal_corpus)\ntop_rebuttal = r.get_ranked_phrases()[0:100]\ntop_rebuttal = \" \".join(top_rebuttal)\ndisplacy.render(nlp(top_rebuttal),style='ent')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T15:26:51.904085Z","iopub.execute_input":"2022-01-08T15:26:51.904433Z","iopub.status.idle":"2022-01-08T15:26:52.365208Z","shell.execute_reply.started":"2022-01-08T15:26:51.904404Z","shell.execute_reply":"2022-01-08T15:26:52.364556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stopword count by Class","metadata":{}},{"cell_type":"code","source":"def stopword_counter(corpus):\n    count = 0  \n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(corpus)\n    for w in word_tokens:\n        if w in stop_words:\n            count = count+1\n \n    return count\n\nclaim_count = stopword_counter(claim_corpus)\nevidence_count = stopword_counter(evidence_corpus)\nposition_count = stopword_counter(position_corpus)\nconcluding_satement_count = stopword_counter(concluding_statement_corpus)\nlead_count = stopword_counter(lead_corpus)\ncounterclaim_count = stopword_counter(counterclaim_corpus)\nrebuttal_count = stopword_counter(rebuttal_corpus)\nstopwords_count = [claim_count,evidence_count,position_count,concluding_satement_count,lead_count,counterclaim_count,rebuttal_count]\n\nstopwords_df = {'Classes':['Claim', 'Evidence', 'Position', 'Concluding_Statement','Lead','Counterclaim','Rebuttal'],\n        'Stopword Count':stopwords_count}\nstopwords_df = pd.DataFrame(stopwords_df)\nstopwords_df\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T15:26:52.366489Z","iopub.execute_input":"2022-01-08T15:26:52.366764Z","iopub.status.idle":"2022-01-08T15:27:12.036271Z","shell.execute_reply.started":"2022-01-08T15:26:52.366717Z","shell.execute_reply":"2022-01-08T15:27:12.035331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ","metadata":{}},{"cell_type":"markdown","source":"### Sentiment score sorted by class. Higher score indicates a more positive statement","metadata":{}},{"cell_type":"code","source":"def getPolarity(text):\n    return TextBlob(text).sentiment.polarity\n\ndf_claim = df.loc[df['discourse_type'] == 'Claim']\nclaim_score = df_claim['discourse_text'].astype(str).apply(getPolarity).sum()\n\ndf_evidence = df.loc[df['discourse_type'] == 'Evidence']\nevidence_score = df_evidence['discourse_text'].astype(str).apply(getPolarity).sum()\n\ndf_position  = df.loc[df['discourse_type'] == \"Position\"]\nposition_score = df_position['discourse_text'].astype(str).apply(getPolarity).sum()\n\ndf_concluding_statement = df.loc[df['discourse_type'] == 'Concluding Statement']\nconcluding_statement_score = df_concluding_statement['discourse_text'].astype(str).apply(getPolarity).sum()\n\ndf_lead = df.loc[df['discourse_type'] == \"Lead\"]\nlead_score = df_lead['discourse_text'].astype(str).apply(getPolarity).sum()\n\ndf_counterclaim = df.loc[df['discourse_type'] == 'Counterclaim']\ncounterclaim_score = df_counterclaim['discourse_text'].astype(str).apply(getPolarity).sum()\n\ndf_rebuttal = df.loc[df['discourse_type'] == 'Rebuttal']\nrebuttal_score = df_rebuttal['discourse_text'].astype(str).apply(getPolarity).sum()\n\nsentiment_list = [claim_score,evidence_score,position_score,concluding_statement_score,lead_score,counterclaim_score,rebuttal_score]\n\nsentiment_df = {'Classes':['Claim', 'Evidence', 'Position', 'Concluding Statement','Lead','Counterclaim','Rebuttal'],\n        'Score':sentiment_list}\nsentiment_df = pd.DataFrame(sentiment_df)\n\n\nsns.set_style('darkgrid')\nstyle.use('seaborn-pastel')\n\nplot = sns.catplot(\n    data=sentiment_df, kind=\"bar\",\n    x=\"Classes\", y=\"Score\",\n    height=8, aspect=9.7/6.27, legend=False\n)\n\nplot.fig.suptitle(\"Sentiment Score\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T15:38:29.928933Z","iopub.execute_input":"2022-01-08T15:38:29.929709Z","iopub.status.idle":"2022-01-08T15:39:18.950129Z","shell.execute_reply.started":"2022-01-08T15:38:29.929658Z","shell.execute_reply":"2022-01-08T15:39:18.948981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ---> Sentiment is directly proportional to the number of data points, suggesting that most essays are bereft of words that carry strong sentiments.\n\n### ---> Essays are written in a passive voice with facts used more to justify their position than personal preferences, this could be why here sentiment only correlates with the length and not the semantic meaning. ","metadata":{}},{"cell_type":"markdown","source":"An interesting approach to EDA would be to group similar classes and see if we can train a classifier to discriminate between them with enough accuracy\nDistinguishing between them could assist in text segmentation which could then be used as a intermediate step for making the argumentative/non-argumentative classification. Here we try out 2 pairs, the first one is (Claim, Position) and the second one is (Counterclaim, Rebuttal). ","metadata":{}},{"cell_type":"code","source":"df_claim_list = df_claim['discourse_text'].tolist()\ndf_claim_list_label = df_claim['discourse_type'].tolist()\ndf_empty = pd.DataFrame()\ndf_empty.insert(0,'txt',df_claim_list,True)\ndf_empty.insert(1,'label',df_claim_list_label)\ndf_empty_1 = pd.DataFrame()\ndf_position_list = df_position['discourse_text'].tolist()\ndf_position_list_label = df_position['discourse_type'].tolist()\ndf_empty_1.insert(0,'txt',df_position_list,True)\ndf_empty_1.insert(1,'label',df_position_list_label,True)\ndf_train = df_empty.append(df_empty_1)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T15:39:18.951672Z","iopub.execute_input":"2022-01-08T15:39:18.951922Z","iopub.status.idle":"2022-01-08T15:39:18.998328Z","shell.execute_reply.started":"2022-01-08T15:39:18.951891Z","shell.execute_reply":"2022-01-08T15:39:18.997461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(df_train['txt'],df_train['label'])","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:39:18.999482Z","iopub.execute_input":"2022-01-08T15:39:19.000118Z","iopub.status.idle":"2022-01-08T15:39:19.024936Z","shell.execute_reply.started":"2022-01-08T15:39:19.000066Z","shell.execute_reply":"2022-01-08T15:39:19.023909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SD_clf = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,2))),('clf',  SGDClassifier())])\n\nSD_clf.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:39:19.027064Z","iopub.execute_input":"2022-01-08T15:39:19.027339Z","iopub.status.idle":"2022-01-08T15:39:21.912014Z","shell.execute_reply.started":"2022-01-08T15:39:19.027307Z","shell.execute_reply":"2022-01-08T15:39:21.911209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SD_clf.score(X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:39:21.913537Z","iopub.execute_input":"2022-01-08T15:39:21.913828Z","iopub.status.idle":"2022-01-08T15:39:22.61981Z","shell.execute_reply.started":"2022-01-08T15:39:21.913775Z","shell.execute_reply":"2022-01-08T15:39:22.619174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 88 % accuracy, suggesting that these both have sufficiently unique features to distinguish between them.  ","metadata":{}},{"cell_type":"code","source":"pred_sd = SD_clf.predict(X_test)\npd.DataFrame(classification_report(pred_sd,y_test,output_dict=True)).T\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:39:22.620979Z","iopub.execute_input":"2022-01-08T15:39:22.621713Z","iopub.status.idle":"2022-01-08T15:39:23.804158Z","shell.execute_reply.started":"2022-01-08T15:39:22.621676Z","shell.execute_reply":"2022-01-08T15:39:23.803242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eli5.show_weights(SD_clf)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:39:23.80564Z","iopub.execute_input":"2022-01-08T15:39:23.805888Z","iopub.status.idle":"2022-01-08T15:39:24.083204Z","shell.execute_reply.started":"2022-01-08T15:39:23.805857Z","shell.execute_reply":"2022-01-08T15:39:24.082184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ---> Verbs like 'should', believe, 'think' are generally used to express normative sentences, which tracks because the student is writing about their *opinion*\n\n### ---> 'reason', 'because', 'also' are words usually used when elaborating on a Claim, so that tracks too. \n\n### ---> There is a clear pattern of words/phrases for both the classes, with Position sentences usually expressing normative sentiments and Claims sentences justifying the rationale behind the claim","metadata":{}},{"cell_type":"code","source":"df_empty_3 = pd.DataFrame()\ndf_counterclaim_list = df_counterclaim['discourse_text'].tolist()\ndf_counterclaim_label = df_counterclaim['discourse_type'].tolist()\ndf_empty_3.insert(0,'txt',df_counterclaim_list,True)\ndf_empty_3.insert(1,'label',df_counterclaim_label,True)\ndf_empty_4 = pd.DataFrame()\ndf_rebuttal_list = df_rebuttal['discourse_text'].tolist()\ndf_rebuttal_label = df_rebuttal['discourse_type'].tolist()\ndf_empty_4.insert(0,'txt',df_rebuttal_list,True)\ndf_empty_4.insert(1,'label',df_rebuttal_label,True)\ndf_train_2 = df_empty_3.append(df_empty_4)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T15:39:24.084751Z","iopub.execute_input":"2022-01-08T15:39:24.085521Z","iopub.status.idle":"2022-01-08T15:39:24.104158Z","shell.execute_reply.started":"2022-01-08T15:39:24.085469Z","shell.execute_reply":"2022-01-08T15:39:24.102844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_2,X_test_2,y_train_2,y_test_2 = train_test_split(df_train_2['txt'],df_train_2['label'])","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:39:24.106401Z","iopub.execute_input":"2022-01-08T15:39:24.106992Z","iopub.status.idle":"2022-01-08T15:39:24.12149Z","shell.execute_reply.started":"2022-01-08T15:39:24.106937Z","shell.execute_reply":"2022-01-08T15:39:24.120821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SD_clf_2 = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,2))),('clf',  SGDClassifier())])\n\nSD_clf_2.fit(X_train_2,y_train_2)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:39:24.123579Z","iopub.execute_input":"2022-01-08T15:39:24.124009Z","iopub.status.idle":"2022-01-08T15:39:24.865829Z","shell.execute_reply.started":"2022-01-08T15:39:24.123964Z","shell.execute_reply":"2022-01-08T15:39:24.864943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SD_clf_2.score(X_test_2,y_test_2)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:39:24.867135Z","iopub.execute_input":"2022-01-08T15:39:24.867389Z","iopub.status.idle":"2022-01-08T15:39:25.007986Z","shell.execute_reply.started":"2022-01-08T15:39:24.867357Z","shell.execute_reply":"2022-01-08T15:39:25.007338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ---> Only about 80 percent of accuracy here, could be because the data points to work with are much lesser than the previous case and the thematic overlap.\n\n","metadata":{}},{"cell_type":"code","source":"pred_sd_2 = SD_clf_2.predict(X_test_2)\npd.DataFrame(classification_report(pred_sd_2,y_test_2,output_dict=True)).T","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:39:25.009241Z","iopub.execute_input":"2022-01-08T15:39:25.009932Z","iopub.status.idle":"2022-01-08T15:39:25.219641Z","shell.execute_reply.started":"2022-01-08T15:39:25.009894Z","shell.execute_reply":"2022-01-08T15:39:25.218762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eli5.show_weights(SD_clf_2)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:39:25.220847Z","iopub.execute_input":"2022-01-08T15:39:25.221081Z","iopub.status.idle":"2022-01-08T15:39:25.30912Z","shell.execute_reply.started":"2022-01-08T15:39:25.221053Z","shell.execute_reply":"2022-01-08T15:39:25.308208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ---> Thematically, Counterclaim and Rebuttal are very similar as they are both refuting a statement, so the words used as features to detect them are too. \n\n### ---> Many words that are used for negation have shown up in both the classes as expected; \"no\", \"however\", \"although\" etc. \n\n### ---> It's difficult to find much semantic/linguistic difference bwetween these 2 classes","metadata":{}},{"cell_type":"markdown","source":"The notebook is a work in progress and will be updated with more EDA and Modelling content","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}