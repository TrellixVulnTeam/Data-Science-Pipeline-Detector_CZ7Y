{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\nimport transformers\nimport numpy as np\nimport pandas as pd\nimport os\nimport re\nimport glob\nfrom tqdm import tqdm\nimport math\nimport timeit\n\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","metadata":{"execution":{"iopub.status.busy":"2022-03-07T08:58:40.998617Z","iopub.execute_input":"2022-03-07T08:58:40.998957Z","iopub.status.idle":"2022-03-07T08:58:50.740488Z","shell.execute_reply.started":"2022-03-07T08:58:40.998874Z","shell.execute_reply":"2022-03-07T08:58:50.7396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(os.path.join(\"../input/feedback-prize-2021/\", \"train.csv\"))\ntrain[['discourse_id', 'discourse_start', 'discourse_end']] = train[['discourse_id', 'discourse_start', 'discourse_end']].astype(int)\n\ntrain[\"discourse_len\"] = train[\"discourse_text\"].apply(lambda x: len(x.split()))\ntrain[\"pred_len\"] = train[\"predictionstring\"].apply(lambda x: len(x.split()))\n\ntrain_txt = glob.glob('../input/feedback-prize-2021/train/*.txt') \n\ncols_to_display = ['discourse_id', 'discourse_text', 'discourse_type','predictionstring', 'discourse_len', 'pred_len']\ntrain[cols_to_display].head()\n\n# this code chunk is copied from Rob Mulla\nlen_dict = {}\nword_dict = {}\nfor t in tqdm(train_txt):\n    with open(t, \"r\") as txt_file:\n        myid = t.split(\"/\")[-1].replace(\".txt\", \"\")\n        data = txt_file.read()\n        mylen = len(data.strip())\n        myword = len(data.split())\n        len_dict[myid] = mylen\n        word_dict[myid] = myword\ntrain[\"essay_len\"] = train[\"id\"].map(len_dict)\ntrain[\"essay_words\"] = train[\"id\"].map(word_dict)\n\ndata_ids = train['id'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T08:58:50.742583Z","iopub.execute_input":"2022-03-07T08:58:50.742916Z","iopub.status.idle":"2022-03-07T08:59:42.993332Z","shell.execute_reply.started":"2022-03-07T08:58:50.742867Z","shell.execute_reply":"2022-03-07T08:59:42.992471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#initialize column\ntrain['gap_length'] = np.nan\n\n#set the first one\ntrain.loc[0, 'gap_length'] = 7 #discourse start - 1 (previous end is always -1)\n\n#loop over rest\nfor i in tqdm(range(1, len(train))):\n    #gap if difference is not 1 within an essay\n    if ((train.loc[i, \"id\"] == train.loc[i-1, \"id\"])\\\n        and (train.loc[i, \"discourse_start\"] - train.loc[i-1, \"discourse_end\"] > 1)):\n        train.loc[i, 'gap_length'] = train.loc[i, \"discourse_start\"] - train.loc[i-1, \"discourse_end\"] - 2\n        #minus 2 as the previous end is always -1 and the previous start always +1\n    #gap if the first discourse of an new essay does not start at 0\n    elif ((train.loc[i, \"id\"] != train.loc[i-1, \"id\"])\\\n        and (train.loc[i, \"discourse_start\"] != 0)):\n        train.loc[i, 'gap_length'] = train.loc[i, \"discourse_start\"] -1\n\n\n #is there any text after the last discourse of an essay?\nlast_ones = train.drop_duplicates(subset=\"id\", keep='last')\nlast_ones['gap_end_length'] = np.where((last_ones.discourse_end < last_ones.essay_len),\\\n                                       (last_ones.essay_len - last_ones.discourse_end),\\\n                                       np.nan)\n\ncols_to_merge = ['id', 'discourse_id', 'gap_end_length']\ntrain = train.merge(last_ones[cols_to_merge], on = [\"id\", \"discourse_id\"], how = \"left\")\n\n#display an example\ncols_to_display = ['id', 'discourse_start', 'discourse_end', 'discourse_type', 'essay_len', 'gap_length', 'gap_end_length']\ntrain[cols_to_display].query('id == \"AFEC37C2D43F\"')","metadata":{"execution":{"iopub.status.busy":"2022-03-07T08:59:42.99805Z","iopub.execute_input":"2022-03-07T08:59:43.000862Z","iopub.status.idle":"2022-03-07T09:00:11.462764Z","shell.execute_reply.started":"2022-03-07T08:59:43.000815Z","shell.execute_reply":"2022-03-07T09:00:11.4619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_text_data(file_name):\n    with open(f\"../input/feedback-prize-2021/train/{file_name}.txt\") as f:\n        txt = f.read()\n    return [nltk.pos_tag(nltk.word_tokenize(parag)) for parag in re.split(\"\\n\\n\", txt)]\n\ndef add_gap_rows(essay):\n    cols_to_keep = ['discourse_start', 'discourse_end', 'discourse_type', 'gap_length', 'gap_end_length']\n    df_essay = train.query('id == @essay')[cols_to_keep].reset_index(drop = True)\n\n    #index new row\n    insert_row = len(df_essay)\n   \n    for i in range(1, len(df_essay)):          \n        if df_essay.loc[i,\"gap_length\"] >0:\n            if i == 0:\n                start = 0 #as there is no i-1 for first row\n                end = df_essay.loc[0, 'discourse_start'] -1\n                disc_type = \"Nothing\"\n                gap_end = np.nan\n                gap = np.nan\n                df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n                insert_row += 1\n            else:\n                start = df_essay.loc[i-1, \"discourse_end\"] + 1\n                end = df_essay.loc[i, 'discourse_start'] -1\n                disc_type = \"Nothing\"\n                gap_end = np.nan\n                gap = np.nan\n                df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n                insert_row += 1\n    df_essay = df_essay.sort_values(by = \"discourse_start\").reset_index(drop=True)\n\n    #add gap at end\n    if df_essay.loc[(len(df_essay)-1),'gap_end_length'] > 0:\n        start = df_essay.loc[(len(df_essay)-1), \"discourse_end\"] + 1\n        end = start + df_essay.loc[(len(df_essay)-1), 'gap_end_length']\n        disc_type = \"Nothing\"\n        gap_end = np.nan\n        gap = np.nan\n        df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n        \n    return(df_essay)\n\ndef prepare_train_data(essay):\n    df_essay = add_gap_rows(essay)\n    #code from https://www.kaggle.com/odins0n/feedback-prize-eda, but adjusted to df_essay\n    essay_file = \"../input/feedback-prize-2021/train/\" + essay + \".txt\"\n    items = []\n    p = 0\n    with open(essay_file, 'r') as file: data = file.read()\n    \n    for i, row in df_essay.iterrows():\n        p = int(row['discourse_start'])\n        e = int(row['discourse_end'])\n        items.append([data[p:e], row['discourse_type']])\n\n    return items","metadata":{"execution":{"iopub.status.busy":"2022-03-07T09:00:11.464926Z","iopub.execute_input":"2022-03-07T09:00:11.465309Z","iopub.status.idle":"2022-03-07T09:00:11.480268Z","shell.execute_reply.started":"2022-03-07T09:00:11.465269Z","shell.execute_reply":"2022-03-07T09:00:11.479465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n#Initialize tokenizer\nmodel_name = \"../input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased\"\nbert_tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n\ndtype_to_num = {\"Lead\":0,\n                \"Position\":1,\n                \"Claim\":2,\n                \"Counterclaim\":3,\n                \"Rebuttal\":4,\n                \"Evidence\":5,\n                \"Concluding Statement\":6,\n                \"Nothing\":7}\n\nnum_to_dtype = {v: k for k, v in dtype_to_num.items()}\n\nnum_class_type = 8","metadata":{"execution":{"iopub.status.busy":"2022-03-07T09:00:11.48156Z","iopub.execute_input":"2022-03-07T09:00:11.481902Z","iopub.status.idle":"2022-03-07T09:00:12.299348Z","shell.execute_reply.started":"2022-03-07T09:00:11.481864Z","shell.execute_reply":"2022-03-07T09:00:12.298528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collect_train_sets():\n    train_lines = []\n    train_labels = []\n\n    max_data_size = 14000\n    max_length    = 50\n    max_sentence_size = 0\n\n\n    for i in tqdm(range(max_data_size)):\n        for txt in prepare_train_data(data_ids[i]):\n            l = len(bert_tokenizer.tokenize(txt[0]))\n            if l <= max_length:\n                train_lines.append(txt[0])\n                train_labels.append(dtype_to_num[txt[1]])\n            else:\n                for t in re.split(\"[.?\\n]\", txt[0]):\n                    tsize = len(bert_tokenizer.tokenize(t))\n                    if tsize <= 100:\n                        max_sentence_size = max([max_sentence_size, tsize])\n                        train_lines.append(t)\n                        train_labels.append(dtype_to_num[txt[1]])\n                    else:\n                        for t1 in re.split(\"[,]\", t):\n                            max_sentence_size = max([max_sentence_size, len(bert_tokenizer.tokenize(t1))])\n                            train_lines.append(t1)\n                            train_labels.append(dtype_to_num[txt[1]])\n\n    max_length = max_sentence_size\n    print(max_length)\n    return [train_lines, train_labels, max_length]","metadata":{"execution":{"iopub.status.busy":"2022-03-07T09:00:12.301336Z","iopub.execute_input":"2022-03-07T09:00:12.301858Z","iopub.status.idle":"2022-03-07T09:00:12.311341Z","shell.execute_reply.started":"2022-03-07T09:00:12.301819Z","shell.execute_reply":"2022-03-07T09:00:12.310484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport pandas as pd\n\nprint(pd.DataFrame([[val for val in dir()], [sys.getsizeof(eval(val)) for val in dir()]],\n                   index=['name','size']).T.sort_values('size', ascending=False).reset_index(drop=True))","metadata":{"execution":{"iopub.status.busy":"2022-03-07T09:00:12.312668Z","iopub.execute_input":"2022-03-07T09:00:12.312927Z","iopub.status.idle":"2022-03-07T09:00:12.491243Z","shell.execute_reply.started":"2022-03-07T09:00:12.31289Z","shell.execute_reply":"2022-03-07T09:00:12.490526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_train_data(datas):\n    shape = (len(datas), max_length)\n    \n    input_ids = np.zeros(shape, dtype=\"int32\")\n    attention_mask = np.zeros(shape, dtype=\"int32\")\n    token_type_ids = np.zeros(shape, dtype=\"int32\")\n    \n    for i, data in enumerate(datas):\n        encoded = bert_tokenizer.encode_plus(datas[i],\n                                             max_length=max_length,\n                                             pad_to_max_length=True,\n                                             truncation=True)\n        input_ids[i] = encoded[\"input_ids\"]\n        attention_mask[i] = encoded[\"attention_mask\"]\n        token_type_ids[i] = encoded[\"token_type_ids\"] \n    \n    return [input_ids, attention_mask, token_type_ids]\n\ndef build_model():\n    input_shape = (max_length,)\n    \n    input_ids = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n    attention_mask = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n    token_type_ids = tf.keras.layers.Input(input_shape, dtype=tf.int32)\n    \n    bert_model = transformers.TFBertModel.from_pretrained(model_name)\n    \n    bert_output = bert_model(\n        input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids\n    )\n\n    last_hidden_state = bert_output.last_hidden_state\n    pooler_output     = bert_output.pooler_output\n    \n    output = tf.keras.layers.Dense(num_class_type, activation=\"softmax\")(pooler_output)\n    model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=[output])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-07T09:00:12.492484Z","iopub.execute_input":"2022-03-07T09:00:12.493209Z","iopub.status.idle":"2022-03-07T09:00:12.503902Z","shell.execute_reply.started":"2022-03-07T09:00:12.493168Z","shell.execute_reply":"2022-03-07T09:00:12.503201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_train(train_lines, train_labels):\n    batch_size = 256\n    epoch = 15\n\n    test_size = 15\n    train_line_size = len(train_lines)\n\n    X_train = make_train_data(train_lines[test_size:train_line_size])\n    Y_train = tf.keras.utils.to_categorical(train_labels[test_size:train_line_size], num_classes=num_class_type)\n\n    X_test  = make_train_data(train_lines[0:test_size])\n    Y_test  = tf.keras.utils.to_categorical(train_labels[0:test_size], num_classes=num_class_type)\n    \n    \n    model = build_model()\n        \n    model.summary()\n    \n    model.fit(\n        X_train,\n        Y_train,\n        batch_size=batch_size,\n        epochs=epoch)\n    model.save(\"./bert_trained_model.h5\")\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-07T09:00:12.50528Z","iopub.execute_input":"2022-03-07T09:00:12.505756Z","iopub.status.idle":"2022-03-07T09:00:12.515803Z","shell.execute_reply.started":"2022-03-07T09:00:12.505721Z","shell.execute_reply":"2022-03-07T09:00:12.515057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_lines, train_labels, max_length = collect_train_sets()\nwith tf.device('/gpu:0'):\n    model = model_train(train_lines, train_labels)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T09:00:12.518319Z","iopub.execute_input":"2022-03-07T09:00:12.519153Z","iopub.status.idle":"2022-03-07T09:01:30.974267Z","shell.execute_reply.started":"2022-03-07T09:00:12.519111Z","shell.execute_reply":"2022-03-07T09:01:30.971527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_txt = glob.glob('../input/feedback-prize-2021/test/*.txt')\nsub = []\n\nfor t in test_txt:\n    with open(t, \"r\") as txt_file:\n        myid          = t.split(\"/\")[-1].replace(\".txt\", \"\")\n        datas         = re.split(\"[.?\\n]\", txt_file.read())\n        input_X       = make_train_data(datas)\n        with tf.device('/gpu:0'):\n            X_predict = model.predict(input_X)\n        X_predict_num = np.argmax(X_predict, axis=1)\n        p = 0\n        last_found_dtype = -1\n        last_word_list   = []\n        for i, data in enumerate(datas):\n            if len(data) == 0:\n                p += 1\n            else:\n                word_count = len(bert_tokenizer.tokenize(data))\n                if X_predict_num[i] == 7:\n                    pass\n                else:\n                    word_list = [str(x) for x in range(p, p+word_count)]\n                    if last_found_dtype == X_predict_num[i]:\n                        last_word_list = last_word_list + word_list\n                        sub.append((myid, num_to_dtype[X_predict_num[i]], ' '.join(last_word_list)))\n                        last_word_list = []\n                    else:\n                        last_found_dtype = X_predict_num[i]\n                        last_word_list   = word_list\n                p += word_count\n    \ndf = pd.DataFrame(sub)\ndf.columns = ['id','class','predictionstring']\ndf.to_csv('submission.csv',index=False)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-03-07T09:06:25.10966Z","iopub.execute_input":"2022-03-07T09:06:25.110238Z","iopub.status.idle":"2022-03-07T09:06:29.589855Z","shell.execute_reply.started":"2022-03-07T09:06:25.1102Z","shell.execute_reply":"2022-03-07T09:06:29.58913Z"},"trusted":true},"execution_count":null,"outputs":[]}]}