{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <span style='color:#A80808'>Motivation</span>\n\nThis tiny notebook aims to provide a simple guide for beginners on using a pre-trained model (with internet OFF) to tokenize a text file.","metadata":{"_uuid":"83c33ac3-8307-4ffc-acaf-f47ccf71019a","_cell_guid":"322b639c-4bed-45d9-ba97-1fbba521e99e","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-23T08:42:34.508711Z","iopub.execute_input":"2022-02-23T08:42:34.509328Z","iopub.status.idle":"2022-02-23T08:42:50.139899Z","shell.execute_reply.started":"2022-02-23T08:42:34.509204Z","shell.execute_reply":"2022-02-23T08:42:50.138703Z"}}},{"cell_type":"code","source":"from transformers import AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-02-23T09:04:43.577823Z","iopub.execute_input":"2022-02-23T09:04:43.578229Z","iopub.status.idle":"2022-02-23T09:04:43.584084Z","shell.execute_reply.started":"2022-02-23T09:04:43.578181Z","shell.execute_reply":"2022-02-23T09:04:43.582784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load tokenizer from a pre-trained model saved in a local dataset (work with internet OFF)\nMODEL = '../input/download-a-pre-trained-transformer-for-beginner/transformer_model'\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T09:04:43.585888Z","iopub.execute_input":"2022-02-23T09:04:43.586392Z","iopub.status.idle":"2022-02-23T09:04:43.72272Z","shell.execute_reply.started":"2022-02-23T09:04:43.586345Z","shell.execute_reply":"2022-02-23T09:04:43.721871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test a simple text\ntokenizer.encode_plus('hello', max_length=10, padding='max_length',\n                               truncation=True, return_offsets_mapping=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T09:04:43.723944Z","iopub.execute_input":"2022-02-23T09:04:43.724185Z","iopub.status.idle":"2022-02-23T09:04:43.731264Z","shell.execute_reply.started":"2022-02-23T09:04:43.724155Z","shell.execute_reply":"2022-02-23T09:04:43.730112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The first token number is always 0 and padding number is 1. The separating number between the tokens and the padding is 2.\n* Mask=0 for the extended part (padding), =1 otherwise\n* Offset shows the first and last(+1) positions of the tokens. The first offset, the separating offset and the extended offsets are (0, 0)","metadata":{}},{"cell_type":"code","source":"# An other simple text\ntokenizer.encode_plus('voila', max_length=10, padding='max_length',\n                               truncation=True, return_offsets_mapping=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T09:07:45.426802Z","iopub.execute_input":"2022-02-23T09:07:45.427087Z","iopub.status.idle":"2022-02-23T09:07:45.433281Z","shell.execute_reply.started":"2022-02-23T09:07:45.427059Z","shell.execute_reply":"2022-02-23T09:07:45.432672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This strange behavior of this tokenizer is because it was fitted on a dictionary of English words, therefore It does not consider this French word as a single token. Indeed, it splits 'voila' in to 'v', 'oil' and 'a'.","metadata":{}},{"cell_type":"code","source":"# Read a text file and show its content\ntxt = open('../input/feedback-prize-2021/train/0000D23A521A.txt', 'r').read()\ntxt","metadata":{"execution":{"iopub.status.busy":"2022-02-23T09:04:43.734051Z","iopub.execute_input":"2022-02-23T09:04:43.734901Z","iopub.status.idle":"2022-02-23T09:04:43.750494Z","shell.execute_reply.started":"2022-02-23T09:04:43.734851Z","shell.execute_reply":"2022-02-23T09:04:43.749901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define max token length, the tokenized text is truncated or extended to this length depending on its initial length.\nMAX_LENGTH = 1024\n\ntokens = tokenizer.encode_plus(txt, max_length=MAX_LENGTH, padding='max_length',\n                               truncation=True, return_offsets_mapping=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T09:04:43.751755Z","iopub.execute_input":"2022-02-23T09:04:43.752373Z","iopub.status.idle":"2022-02-23T09:04:43.762368Z","shell.execute_reply.started":"2022-02-23T09:04:43.752311Z","shell.execute_reply":"2022-02-23T09:04:43.761696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the numbers indexing the initial tokens\nprint(tokens['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2022-02-23T09:04:43.763765Z","iopub.execute_input":"2022-02-23T09:04:43.764437Z","iopub.status.idle":"2022-02-23T09:04:43.775347Z","shell.execute_reply.started":"2022-02-23T09:04:43.764387Z","shell.execute_reply":"2022-02-23T09:04:43.774464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the mask\nprint(tokens['attention_mask'])","metadata":{"execution":{"iopub.status.busy":"2022-02-23T09:04:43.776769Z","iopub.execute_input":"2022-02-23T09:04:43.77733Z","iopub.status.idle":"2022-02-23T09:04:43.790188Z","shell.execute_reply.started":"2022-02-23T09:04:43.777291Z","shell.execute_reply":"2022-02-23T09:04:43.789204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the first and last positions of each token\nprint(tokens['offset_mapping'])","metadata":{"execution":{"iopub.status.busy":"2022-02-23T09:04:43.7915Z","iopub.execute_input":"2022-02-23T09:04:43.791779Z","iopub.status.idle":"2022-02-23T09:04:43.804743Z","shell.execute_reply.started":"2022-02-23T09:04:43.791749Z","shell.execute_reply":"2022-02-23T09:04:43.803721Z"},"trusted":true},"execution_count":null,"outputs":[]}]}