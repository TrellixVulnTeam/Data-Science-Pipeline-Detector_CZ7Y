{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThe essays for this challenge have a lot of spelling mistakes and punctuations which can cause out of vocabulary errors for tokenizers.\n\nIn this notebook, we will see that initially we have around 80% coverage of word tokens compared to the vocabulary of glove embeddings and then we will improve it to beyond 99%.\n\nWe wont be using any lemmatization or stemming to achieve this feat. You are welcome to stem as it may help with some words. Lemmatize probably wont help as it depends on a valid word in the first place.\n\n---\n## TLDR;\n\n* Get the exported csv file from notebook output. For the words in raw_words, replace them with clean_words_05 from the text you encounter from your training and testing dataframes. This improves the word embeddings as common mistakes have been clarified and count-wise improvement lifts it from 80% current to 99% new.\n\n---\n## Disclaimer\n\nAll ideas are taken from the below excellent reference and then adopted for our train/test sets. Amazing work on simple ideas to improve the vocabulary.\n\n\nfrom @christofhenkel\nhttps://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings","metadata":{}},{"cell_type":"markdown","source":"---\n---\n# Warning\n\nMake sure that if you use anything like below, then dont train on character positions. Because after spelling corrections the character positions will change. \nI have tried to keep the token counts same as before so training on token positions can potentially work.","metadata":{}},{"cell_type":"code","source":"from gensim.models import KeyedVectors\nimport pandas as pd\nimport os\nfrom tqdm.auto import tqdm\ntqdm.pandas()\nimport numpy as np\nimport re\nfrom nltk.corpus import stopwords\n\n########################################\n# if stopwords are not downloaded to your environment\n# import nltk\n# nltk.download('stopwords')\n########################################\nimport gensim.downloader as api\nword_vectors = api.load(\"glove-wiki-gigaword-100\")\n\n########################################\n# if you want to save / load from local vectors\n#word_vectors.save('vectors.kv')\n#word_vectors = KeyedVectors.load('vectors.kv')\n########################################","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:21:16.087686Z","iopub.execute_input":"2022-01-22T12:21:16.088015Z","iopub.status.idle":"2022-01-22T12:22:35.310525Z","shell.execute_reply.started":"2022-01-22T12:21:16.087984Z","shell.execute_reply":"2022-01-22T12:22:35.309805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%config Completer.use_jedi = False\nLOWER_CASE = True","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:35.312017Z","iopub.execute_input":"2022-01-22T12:22:35.312875Z","iopub.status.idle":"2022-01-22T12:22:35.327731Z","shell.execute_reply.started":"2022-01-22T12:22:35.312832Z","shell.execute_reply":"2022-01-22T12:22:35.32632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/feedback-prize-2021/train.csv\")\ndf_ss = pd.read_csv(\"../input/feedback-prize-2021/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:35.329377Z","iopub.execute_input":"2022-01-22T12:22:35.329738Z","iopub.status.idle":"2022-01-22T12:22:36.421373Z","shell.execute_reply.started":"2022-01-22T12:22:35.329704Z","shell.execute_reply":"2022-01-22T12:22:36.420294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_train_file(currid = \"423A1CA112E2\", curr_dir = \"../input/feedback-prize-2021/train\"):\n    with open(os.path.join(curr_dir, \"{}.txt\".format(currid)), \"r\") as f:\n        filetext = f.read()\n        \n    return filetext","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:36.422661Z","iopub.execute_input":"2022-01-22T12:22:36.42291Z","iopub.status.idle":"2022-01-22T12:22:36.429709Z","shell.execute_reply.started":"2022-01-22T12:22:36.422882Z","shell.execute_reply":"2022-01-22T12:22:36.428501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n\naam_misspell_dict = {'colour':'color',\n                'centre':'center',\n                'favourite':'favorite',\n                'travelling':'traveling',\n                'counselling':'counseling',\n                'theatre':'theater',\n                'cancelled':'canceled',\n                'labour':'labor',\n                'organisation':'organization',\n                'wwii':'world war 2',\n                'citicise':'criticize',\n                \"genericname\": \"someone\",\n                 \"driveless\" : \"driverless\",\n                 \"canidates\" : \"candidates\",\n                 \"electorial\" : \"electoral\",\n                 \"genericschool\" : \"school\",\n                 \"polution\" : \"pollution\",\n                 \"enviorment\" : \"environment\",\n                 \"diffrent\" : \"different\",\n                 \"benifit\" : \"benefit\",\n                 \"schoolname\" : \"school\",\n                 \"artical\" : \"article\",\n                 \"elctoral\" : \"electoral\",\n                 \"genericcity\" : \"city\",\n                 \"recieves\" : \"receives\",\n                 \"completly\" : \"completely\",\n                 \"enviornment\" : \"environment\",\n                 \"somthing\" : \"something\",\n                 \"everyones\" : \"everyone\",\n                 \"oppurtunity\" : \"opportunity\",\n                 \"benifits\" : \"benefits\",\n                 \"benificial\" : \"beneficial\",\n                 \"tecnology\" : \"technology\",\n                 \"paragragh\" : \"paragraph\",\n                 \"differnt\" : \"different\",\n                 \"reist\" : \"resist\",\n                 \"probaly\" : \"probably\",\n                 \"usuage\" : \"usage\",\n                 \"activitys\" : \"activities\",\n                 \"experince\" : \"experience\",\n                 \"oppertunity\" : \"opportunity\",\n                 \"collge\" : \"college\",\n                 \"presedent\" : \"president\",\n                 \"dosent\" : \"doesnt\",\n                 \"propername\" : \"name\",\n                 \"eletoral\" : \"electoral\",\n                 \"diffcult\" : \"difficult\",\n                 \"desicision\" : \"decision\"\n }","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:36.433049Z","iopub.execute_input":"2022-01-22T12:22:36.433452Z","iopub.status.idle":"2022-01-22T12:22:36.444435Z","shell.execute_reply.started":"2022-01-22T12:22:36.433408Z","shell.execute_reply":"2022-01-22T12:22:36.443404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vocabulary Flow (Lower Case)\n\n1. Read all text from train files and combine together.","metadata":{}},{"cell_type":"code","source":"txt = []\nfor i in tqdm( df[\"id\"].unique() ):\n    txt.append( read_train_file(i) )","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:36.446137Z","iopub.execute_input":"2022-01-22T12:22:36.446862Z","iopub.status.idle":"2022-01-22T12:22:46.002694Z","shell.execute_reply.started":"2022-01-22T12:22:36.446814Z","shell.execute_reply":"2022-01-22T12:22:46.001542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Initial Vocabulary","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\n\ninitial_vocab = defaultdict(int)\n\nfor i in tqdm(txt, total = len(txt)):\n    words = i.split()\n    for word in words:\n        initial_vocab[word.lower()] += 1","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:46.004387Z","iopub.execute_input":"2022-01-22T12:22:46.004633Z","iopub.status.idle":"2022-01-22T12:22:48.852855Z","shell.execute_reply.started":"2022-01-22T12:22:46.004603Z","shell.execute_reply":"2022-01-22T12:22:48.851833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Total Vocabulary Words\n\nLength of the vocabulary ~ 101K words right now","metadata":{}},{"cell_type":"code","source":"print(\"Total vocabulary including stopwords is : \", len(initial_vocab))","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:48.854384Z","iopub.execute_input":"2022-01-22T12:22:48.855138Z","iopub.status.idle":"2022-01-22T12:22:48.860523Z","shell.execute_reply.started":"2022-01-22T12:22:48.855079Z","shell.execute_reply":"2022-01-22T12:22:48.859516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pandas for all heavylifting\n\n* We will use pandas so that all results achieved can be in different columns and you can download the data in the end and use any columns you would like based on your own tokenizer preferences.\n","metadata":{}},{"cell_type":"code","source":"word_df = pd.DataFrame(initial_vocab.items(),\n            columns = [\"raw_words\", \"raw_words_counts\"])\nprint(\"-\"*80)\nprint(\"Displaying Head of the words dataframe\")\ndisplay(word_df.head())\nprint(\"-\"*80)\nprint(\"Displaying Tail of the words dataframe\")\ndisplay(word_df.tail())\nprint(\"-\"*80)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:48.862272Z","iopub.execute_input":"2022-01-22T12:22:48.862624Z","iopub.status.idle":"2022-01-22T12:22:49.108113Z","shell.execute_reply.started":"2022-01-22T12:22:48.862582Z","shell.execute_reply":"2022-01-22T12:22:49.10701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exclude Stopwords\n\n* This means that to report coverage of text we will not count stop words in the analysis.\n* It also helps because in the keyed vectors from gensim stop words wont be included","metadata":{}},{"cell_type":"code","source":"stops = stopwords.words(\"english\")\n\nword_df[\"is_stop_word\"] = word_df[\"raw_words\"].apply(lambda x: 0 if x not in stops else 1)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:49.109948Z","iopub.execute_input":"2022-01-22T12:22:49.110305Z","iopub.status.idle":"2022-01-22T12:22:49.378827Z","shell.execute_reply.started":"2022-01-22T12:22:49.110272Z","shell.execute_reply":"2022-01-22T12:22:49.37788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_df[\"is_stop_word\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:49.380233Z","iopub.execute_input":"2022-01-22T12:22:49.380471Z","iopub.status.idle":"2022-01-22T12:22:49.388803Z","shell.execute_reply.started":"2022-01-22T12:22:49.380444Z","shell.execute_reply":"2022-01-22T12:22:49.387886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysis 1:\n1) After all lower case words, we see there are 170 stop words detected from train set","metadata":{}},{"cell_type":"markdown","source":"---\n---\n\n# Match Vocab\n\nLet us now see how many words have valid entries in the word_vectors\n\n* apply_coverage only checks if a word exists in glove vectors obtained earlier.","metadata":{}},{"cell_type":"code","source":"def apply_coverage(x):\n    if x in word_vectors:\n        return 1\n    return 0\n        \nword_df[\"raw_in_vectors\"] = word_df[\"raw_words\"].apply(apply_coverage)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:49.390094Z","iopub.execute_input":"2022-01-22T12:22:49.390341Z","iopub.status.idle":"2022-01-22T12:22:49.605302Z","shell.execute_reply.started":"2022-01-22T12:22:49.390311Z","shell.execute_reply":"2022-01-22T12:22:49.604097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* get_coverage checks the amount of vocabulary and text coverage when comparing to glove vectors\n* It creates a new column named column_word_presence to indicate if the target word exists or not.\n* It also displays the coverage statistics and returns a dataframe.\n\n    '''\n        column_words : the column containing the words for which we check coverage\n        column_word_counts : the column containing pre-computed word counts for the words in question\n        column_word_presence : Just an output column name where we will output if word exists in word_vectors\n        exc_stop : Should we exclude stopwrods from coverage analysis or not\n    '''","metadata":{}},{"cell_type":"code","source":"def get_coverage(column_words,\n                 column_word_counts,\n                 column_word_presence,\n                 df, exc_stop = True):\n    '''\n        column_words : the column containing the words for which we check coverage\n        column_word_counts : the column containing pre-computed word counts for the words in question\n        column_word_presence : Just an output column name where we will output if word exists in word_vectors\n        exc_stop : Should we exclude stopwrods from coverage analysis or not\n    '''\n    word_df = df.copy()\n    word_df[column_word_presence] = word_df[column_words].apply(apply_coverage)\n    print(\"-\" * 80)\n    \n    #display(word_df[column_word_presence].value_counts(normalize = True))\n    #print(\"-\" * 80)\n    if exc_stop == False:\n        word_coverage = 100*word_df[column_word_presence].value_counts(normalize = True)[1]\n        text_coverage = 100*word_df.groupby([column_word_presence])[column_word_counts].sum()[1] / (word_df.groupby([column_word_presence])[column_word_counts].sum()[0] + \n                         word_df.groupby([column_word_presence])[column_word_counts].sum()[1])\n    else:\n        print(\"EXCLUDING STOP WORD FROM ANALYSIS...\")\n        word_coverage = 100*word_df[word_df[\"is_stop_word\"] == 0][column_word_presence].value_counts(normalize = True)[1]\n        text_coverage = 100*word_df[word_df[\"is_stop_word\"] == 0].groupby([column_word_presence])[column_word_counts].sum()[1] / (word_df[word_df[\"is_stop_word\"] == 0].groupby([column_word_presence])[column_word_counts].sum()[0] + \n                         word_df[word_df[\"is_stop_word\"] == 0].groupby([column_word_presence])[column_word_counts].sum()[1])\n        \n    if exc_stop:\n        print(\"Total words in {} were {} and {:.2f}% words were found in the word_vectors.\".format(column_words,\n                                                                                               len(word_df[word_df[\"is_stop_word\"] == 0]),\n                                                                                               word_coverage))\n    else:\n        print(\"Total words in {} were {} and {:.2f}% words were found in the word_vectors.\".format(column_words,\n                                                                                               len(word_df),\n                                                                                               word_coverage))\n        \n    print(\"-\" * 80)\n    print(\"From text coverage, {:.2f}% text is coverage in word_vectors.\".format(text_coverage))\n    print(\"-\" * 80)\n    return word_df\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:49.607286Z","iopub.execute_input":"2022-01-22T12:22:49.607515Z","iopub.status.idle":"2022-01-22T12:22:49.621412Z","shell.execute_reply.started":"2022-01-22T12:22:49.607487Z","shell.execute_reply":"2022-01-22T12:22:49.620284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_df = get_coverage( \"raw_words\", \"raw_words_counts\", \"raw_in_vectors\", word_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:49.625341Z","iopub.execute_input":"2022-01-22T12:22:49.625634Z","iopub.status.idle":"2022-01-22T12:22:49.888039Z","shell.execute_reply.started":"2022-01-22T12:22:49.625604Z","shell.execute_reply":"2022-01-22T12:22:49.887016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n---\n# Analysis 2\n\n* So we have missing vocabulary for around **76%** of the total words\n* In terms of usage frequency, we have coverage of 80% of the words if repetitions are taken into account\n\n---\n---\n\n# Objective\n\n* Our objective is to improve the **text coverage** so that the tokenizers can improve their performances\n\n\n**Remember** we DONT WANT TO INCREASE or DECREASE the number of tokens. As this will make training and predictionstring too difficult and cause problems on the submission dataset","metadata":{}},{"cell_type":"code","source":"def preprocess(x):\n    x = x.replace(\"n't\", \"nt\")\n    \n    x = str(x)\n    if LOWER_CASE:\n        x = x.lower()\n        \n    if len(x.strip()) == 1:\n        return x #special case if a punctuation was the only alphabet in the token.\n    \n    for punct in \"/-'&\":\n        x = x.replace(punct, '')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n        \n    x = re.sub('[0-9]{1,}', '#', x) #replace all numbers by #\n    if len(x.strip()) < 1:\n        x = '.' #if it was all punctuations like ------ or ..... or .;?!!. Then we return only a period to keep token consistent performance.\n    return x\n\nword_df[\"clean_words_01\"] = word_df[\"raw_words\"].progress_apply(lambda x: preprocess(x))","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:49.889329Z","iopub.execute_input":"2022-01-22T12:22:49.889552Z","iopub.status.idle":"2022-01-22T12:22:51.151386Z","shell.execute_reply.started":"2022-01-22T12:22:49.889525Z","shell.execute_reply":"2022-01-22T12:22:51.150335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:51.152605Z","iopub.execute_input":"2022-01-22T12:22:51.15284Z","iopub.status.idle":"2022-01-22T12:22:51.163615Z","shell.execute_reply.started":"2022-01-22T12:22:51.152813Z","shell.execute_reply":"2022-01-22T12:22:51.162944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = pd.DataFrame(word_df.groupby( [\"clean_words_01\"] )[\"raw_words_counts\"].sum()).reset_index()\ntemp.columns = [\"clean_words_01\", \"clean_words_01_counts\"]\n\nword_df = word_df.merge(temp, on=[\"clean_words_01\"], how = 'left')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:51.164778Z","iopub.execute_input":"2022-01-22T12:22:51.165129Z","iopub.status.idle":"2022-01-22T12:22:51.382731Z","shell.execute_reply.started":"2022-01-22T12:22:51.165092Z","shell.execute_reply":"2022-01-22T12:22:51.382083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_df = get_coverage( \"clean_words_01\", \"clean_words_01_counts\", \"clean_01_in_vectors\", word_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:51.383842Z","iopub.execute_input":"2022-01-22T12:22:51.384219Z","iopub.status.idle":"2022-01-22T12:22:51.62625Z","shell.execute_reply.started":"2022-01-22T12:22:51.384189Z","shell.execute_reply":"2022-01-22T12:22:51.624941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n# Analysis #3\n\n* **Amazing**, we improved the text coverage from **80**% to **99.76**%\n* Lets try to do more\n---","metadata":{}},{"cell_type":"markdown","source":"# Accented alphabet replacements\n\n* á to a and so on....","metadata":{}},{"cell_type":"code","source":"# Reference: \n# https://itqna.net/questions/9818/how-remove-accented-expressions-regular-expressions-python\nimport re\n\n# char codes: https://unicode-table.com/en/#basic-latin\naccent_map = {\n    u'\\u00c0': u'A',\n    u'\\u00c1': u'A',\n    u'\\u00c2': u'A',\n    u'\\u00c3': u'A',\n    u'\\u00c4': u'A',\n    u'\\u00c5': u'A',\n    u'\\u00c6': u'A',\n    u'\\u00c7': u'C',\n    u'\\u00c8': u'E',\n    u'\\u00c9': u'E',\n    u'\\u00ca': u'E',\n    u'\\u00cb': u'E',\n    u'\\u00cc': u'I',\n    u'\\u00cd': u'I',\n    u'\\u00ce': u'I',\n    u'\\u00cf': u'I',\n    u'\\u00d0': u'D',\n    u'\\u00d1': u'N',\n    u'\\u00d2': u'O',\n    u'\\u00d3': u'O',\n    u'\\u00d4': u'O',\n    u'\\u00d5': u'O',\n    u'\\u00d6': u'O',\n    u'\\u00d7': u'x',\n    u'\\u00d8': u'0',\n    u'\\u00d9': u'U',\n    u'\\u00da': u'U',\n    u'\\u00db': u'U',\n    u'\\u00dc': u'U',\n    u'\\u00dd': u'Y',\n    u'\\u00df': u'B',\n    u'\\u00e0': u'a',\n    u'\\u00e1': u'a',\n    u'\\u00e2': u'a',\n    u'\\u00e3': u'a',\n    u'\\u00e4': u'a',\n    u'\\u00e5': u'a',\n    u'\\u00e6': u'a',\n    u'\\u00e7': u'c',\n    u'\\u00e8': u'e',\n    u'\\u00e9': u'e',\n    u'\\u00ea': u'e',\n    u'\\u00eb': u'e',\n    u'\\u00ec': u'i',\n    u'\\u00ed': u'i',\n    u'\\u00ee': u'i',\n    u'\\u00ef': u'i',\n    u'\\u00f1': u'n',\n    u'\\u00f2': u'o',\n    u'\\u00f3': u'o',\n    u'\\u00f4': u'o',\n    u'\\u00f5': u'o',\n    u'\\u00f6': u'o',\n    u'\\u00f8': u'0',\n    u'\\u00f9': u'u',\n    u'\\u00fa': u'u',\n    u'\\u00fb': u'u',\n    u'\\u00fc': u'u'\n}\n\ndef accent_remove (m):\n    return accent_map[m.group(0)]\n\nstring_velha = \"Olá você está ????   \"\nstring_nova = re.sub(u'([\\u00C0-\\u00FC])', accent_remove, string_velha.encode().decode('utf-8'))\nstring_nova","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:51.628221Z","iopub.execute_input":"2022-01-22T12:22:51.628535Z","iopub.status.idle":"2022-01-22T12:22:51.644699Z","shell.execute_reply.started":"2022-01-22T12:22:51.628495Z","shell.execute_reply":"2022-01-22T12:22:51.644037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_df[\"clean_words_02\"] = word_df[\"clean_words_01\"].apply( lambda x: re.sub(u'([\\u00C0-\\u00FC])', \n                                                  accent_remove, \n                                                  x.encode().decode('utf-8'))\n                               )","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:51.64592Z","iopub.execute_input":"2022-01-22T12:22:51.646388Z","iopub.status.idle":"2022-01-22T12:22:51.875335Z","shell.execute_reply.started":"2022-01-22T12:22:51.646337Z","shell.execute_reply":"2022-01-22T12:22:51.874425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = pd.DataFrame(word_df.groupby( [\"clean_words_02\"] )[\"raw_words_counts\"].sum()).reset_index()\ntemp.columns = [\"clean_words_02\", \"clean_words_02_counts\"]\n\nword_df = word_df.merge(temp, on=[\"clean_words_02\"], how = 'left')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:51.876697Z","iopub.execute_input":"2022-01-22T12:22:51.87697Z","iopub.status.idle":"2022-01-22T12:22:52.099607Z","shell.execute_reply.started":"2022-01-22T12:22:51.876935Z","shell.execute_reply":"2022-01-22T12:22:52.098987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_df = get_coverage( \"clean_words_02\", \"clean_words_02_counts\", \"clean_02_in_vectors\", word_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:52.100622Z","iopub.execute_input":"2022-01-22T12:22:52.101376Z","iopub.status.idle":"2022-01-22T12:22:52.407008Z","shell.execute_reply.started":"2022-01-22T12:22:52.101342Z","shell.execute_reply":"2022-01-22T12:22:52.406137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysis 4\n\n* Seems that accented character changes did not improve score too much. Lets see some examples where these replacements were made","metadata":{}},{"cell_type":"code","source":"word_df[ word_df[\"clean_words_01_counts\"] != word_df[\"clean_words_02_counts\"]].head(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:52.408463Z","iopub.execute_input":"2022-01-22T12:22:52.408921Z","iopub.status.idle":"2022-01-22T12:22:52.425957Z","shell.execute_reply.started":"2022-01-22T12:22:52.408885Z","shell.execute_reply":"2022-01-22T12:22:52.425312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n---\n\n# Misspellings\n\n* I created a dictionary in the beginning of notebook **aam_misspell_dict** to include common errors that I can see. You can improve upon it\n* There are definitely a lot of **misspellings** at work\n* There are also anonymous names playing. (like **genericschool**, **genericname** etc..)\n* Seeing that mainly there are a **limited number of topics**, we can perform some basic spelling corrections on the essay topics\n","metadata":{}},{"cell_type":"code","source":"word_df[\"clean_words_03\"] = word_df[\"clean_words_02\"].apply(lambda x: x if x not in aam_misspell_dict else aam_misspell_dict[x])","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:52.427005Z","iopub.execute_input":"2022-01-22T12:22:52.427329Z","iopub.status.idle":"2022-01-22T12:22:52.466035Z","shell.execute_reply.started":"2022-01-22T12:22:52.427303Z","shell.execute_reply":"2022-01-22T12:22:52.465378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = pd.DataFrame(word_df.groupby( [\"clean_words_03\"] )[\"raw_words_counts\"].sum()).reset_index()\ntemp.columns = [\"clean_words_03\", \"clean_words_03_counts\"]\n\nword_df = word_df.merge(temp, on=[\"clean_words_03\"], how = 'left')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:52.467696Z","iopub.execute_input":"2022-01-22T12:22:52.46882Z","iopub.status.idle":"2022-01-22T12:22:52.70114Z","shell.execute_reply.started":"2022-01-22T12:22:52.468747Z","shell.execute_reply":"2022-01-22T12:22:52.700016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_df = get_coverage( \"clean_words_03\", \"clean_words_03_counts\", \"clean_03_in_vectors\", word_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:52.703892Z","iopub.execute_input":"2022-01-22T12:22:52.704306Z","iopub.status.idle":"2022-01-22T12:22:53.037998Z","shell.execute_reply.started":"2022-01-22T12:22:52.704263Z","shell.execute_reply":"2022-01-22T12:22:53.037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Viola\n\n* Another improvement from **99.76%** to **99.84%**\n* Can we do better ???","metadata":{}},{"cell_type":"code","source":"word_df[word_df[\"clean_03_in_vectors\"] == 0].sort_values(by = [\"clean_words_03_counts\"], ascending = False).head(20)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:53.03951Z","iopub.execute_input":"2022-01-22T12:22:53.039951Z","iopub.status.idle":"2022-01-22T12:22:53.079343Z","shell.execute_reply.started":"2022-01-22T12:22:53.039914Z","shell.execute_reply":"2022-01-22T12:22:53.078418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysis #5\n\n* **Shouldnt** is giving us some problems. Well it shouln't (pun-intended)\n* The vocabulary contains should and not separately but I dont want to increase number of tokens, so we will replace all shouldnt with **shant** which has similar meaning","metadata":{}},{"cell_type":"code","source":"aam_misspell_dict.update( {\"shouldnt\" : \"shant\" })","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:53.080987Z","iopub.execute_input":"2022-01-22T12:22:53.081963Z","iopub.status.idle":"2022-01-22T12:22:53.087295Z","shell.execute_reply.started":"2022-01-22T12:22:53.081911Z","shell.execute_reply":"2022-01-22T12:22:53.086239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_df[\"clean_words_04\"] = word_df[\"clean_words_03\"].apply(lambda x: x if x not in aam_misspell_dict else aam_misspell_dict[x])\ntemp = pd.DataFrame(word_df.groupby( [\"clean_words_04\"] )[\"raw_words_counts\"].sum()).reset_index()\ntemp.columns = [\"clean_words_04\", \"clean_words_04_counts\"]\n\nword_df = word_df.merge(temp, on=[\"clean_words_04\"], how = 'left')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:53.088828Z","iopub.execute_input":"2022-01-22T12:22:53.089734Z","iopub.status.idle":"2022-01-22T12:22:53.412335Z","shell.execute_reply.started":"2022-01-22T12:22:53.089678Z","shell.execute_reply":"2022-01-22T12:22:53.411353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_df = get_coverage( \"clean_words_04\", \"clean_words_04_counts\", \"clean_04_in_vectors\", word_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:53.413663Z","iopub.execute_input":"2022-01-22T12:22:53.413917Z","iopub.status.idle":"2022-01-22T12:22:53.710645Z","shell.execute_reply.started":"2022-01-22T12:22:53.413887Z","shell.execute_reply":"2022-01-22T12:22:53.709716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n---\n\n# Wow\n\n* Another 0.01% improvement\n* Lets see the top words still giving us issues","metadata":{}},{"cell_type":"code","source":"word_df[word_df[\"clean_04_in_vectors\"] == 0].sort_values(by = [\"clean_words_04_counts\"], ascending = False)[:10]","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:53.712226Z","iopub.execute_input":"2022-01-22T12:22:53.712475Z","iopub.status.idle":"2022-01-22T12:22:53.750182Z","shell.execute_reply.started":"2022-01-22T12:22:53.712445Z","shell.execute_reply":"2022-01-22T12:22:53.749265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n---\n\n# Analysis 6 - Risky Choices Ahead\n\n* Now we can see that the vocabulary with most trouble is sort of problem-specific and not general enough\n* We can replace the **studentdesigned** or **teacherdesigned** as designed.. This would probably change the contextual meaning but can improve tokenize performance as well.\n* I will replace **studentname** as **myself**\n* I will replace **teachername** as **teacher**\n* I will replace **winnertakeall** as **winner-take-all**","metadata":{}},{"cell_type":"code","source":"aam_misspell_dict.update( {\"teacherdesigned\" : \"designed\",\n                      \"studentname\" : \"myself\",\n                      \"studentdesigned\" : \"designed\",\n                      \"teachername\" : \"teacher\",\n                      \"winnertakeall\" : \"winner-take-all\"})","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:53.751542Z","iopub.execute_input":"2022-01-22T12:22:53.751808Z","iopub.status.idle":"2022-01-22T12:22:53.75694Z","shell.execute_reply.started":"2022-01-22T12:22:53.751755Z","shell.execute_reply":"2022-01-22T12:22:53.755835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_df[\"clean_words_05\"] = word_df[\"clean_words_04\"].apply(lambda x: x if x not in aam_misspell_dict else aam_misspell_dict[x])\ntemp = pd.DataFrame(word_df.groupby( [\"clean_words_05\"] )[\"raw_words_counts\"].sum()).reset_index()\ntemp.columns = [\"clean_words_05\", \"clean_words_05_counts\"]\n\nword_df = word_df.merge(temp, on=[\"clean_words_05\"], how = 'left')","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:53.758372Z","iopub.execute_input":"2022-01-22T12:22:53.75869Z","iopub.status.idle":"2022-01-22T12:22:54.158875Z","shell.execute_reply.started":"2022-01-22T12:22:53.758659Z","shell.execute_reply":"2022-01-22T12:22:54.157862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_df = get_coverage( \"clean_words_05\", \"clean_words_05_counts\", \"clean_05_in_vectors\", word_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:54.161013Z","iopub.execute_input":"2022-01-22T12:22:54.16138Z","iopub.status.idle":"2022-01-22T12:22:54.499044Z","shell.execute_reply.started":"2022-01-22T12:22:54.161333Z","shell.execute_reply":"2022-01-22T12:22:54.498068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n---\n# Conclusion\n\n* After application of several transformations, we have created a dictionary which improves the text-coverage from **80%** to **99.92%**\n* Hopefully this can improve the prediction performance from different models.\n* Do post critique/feedback\n\n* You can use the exported dataframe to create / use as dictionary for your tokens.","metadata":{}},{"cell_type":"code","source":"print(\"Exporting the created dictionary now. \")\nword_df.to_csv(\"cleaned_word_dict.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-01-22T12:22:54.500422Z","iopub.execute_input":"2022-01-22T12:22:54.500645Z","iopub.status.idle":"2022-01-22T12:22:55.332218Z","shell.execute_reply.started":"2022-01-22T12:22:54.500618Z","shell.execute_reply":"2022-01-22T12:22:55.331446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Have a nice day fellows. Happy kaggling!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}