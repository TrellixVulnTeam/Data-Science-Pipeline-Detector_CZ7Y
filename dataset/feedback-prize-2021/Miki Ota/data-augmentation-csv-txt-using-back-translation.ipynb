{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro\n\n**This is my very first public notebook on Kaggle! To whom this notebook is helpful or interesting, I'd be extremely happy if you can upvote it, thank you!!**\n\n# Data Augmentation\n\nIn this note we try several augmentation method, to artificialy increase the number of data. As one shall see, back translation method works best. This method translates the original text into another language (e.g. German) and translate it again to the original language (English) to obtain similar, but slightly different texts. \n\nData augmentation can be used, for instance, to increase the number of texts containing counter-claims and rebuttals, since they are not well represented in the original dataset.\n\nThis notebook further creates a new csv training file, and raw text files for the augmented data, so that they can be directly used in the following training process.","metadata":{}},{"cell_type":"markdown","source":"# Nlpaug\n\nWe shall install the *nlpaug* library, which comes with handy nlp augmentation methods. Several augmentation schemes are available, from the simplest synonym replacement to the most complex nlp transformer augmentation. We shall try some of them to see what is the best augmenter in our specific case.","metadata":{}},{"cell_type":"code","source":"# Instlling nlpaug (you need internet!)\n!pip install nlpaug","metadata":{"execution":{"iopub.status.busy":"2022-03-06T13:49:43.175219Z","iopub.execute_input":"2022-03-06T13:49:43.175902Z","iopub.status.idle":"2022-03-06T13:49:53.374581Z","shell.execute_reply.started":"2022-03-06T13:49:43.175799Z","shell.execute_reply":"2022-03-06T13:49:53.373726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import *\n\nimport random\nimport nlpaug.augmenter.word as naw","metadata":{"execution":{"iopub.status.busy":"2022-03-06T13:49:53.37675Z","iopub.execute_input":"2022-03-06T13:49:53.377344Z","iopub.status.idle":"2022-03-06T13:50:07.640485Z","shell.execute_reply.started":"2022-03-06T13:49:53.377304Z","shell.execute_reply":"2022-03-06T13:50:07.639233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We save the created data in the following folder\nos.mkdir(\"data_augmented\")","metadata":{"execution":{"iopub.status.busy":"2022-03-06T13:50:07.642571Z","iopub.execute_input":"2022-03-06T13:50:07.642946Z","iopub.status.idle":"2022-03-06T13:50:07.648886Z","shell.execute_reply.started":"2022-03-06T13:50:07.642899Z","shell.execute_reply":"2022-03-06T13:50:07.647822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Train","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/feedback-prize-2021/train.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T13:50:07.651166Z","iopub.execute_input":"2022-03-06T13:50:07.651493Z","iopub.status.idle":"2022-03-06T13:50:09.326412Z","shell.execute_reply.started":"2022-03-06T13:50:07.651452Z","shell.execute_reply":"2022-03-06T13:50:09.325615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augmentation methods Study\n\nFor test purpose, we shall take a random discourse, and apply some of the available augmentation methods, to see how the text is transformed.\n\n## Preliminary","metadata":{}},{"cell_type":"code","source":"IDS = train[\"id\"].unique()\n\n# We shall pick one random text to see how the augmentation performs\nrandom.seed(1)\nid_text = random.choice(IDS)\nos_test_pdf = train[train[\"id\"] == id_text]\n\n# The associated text file\nfile_path = f'../input/feedback-prize-2021/train/{id_text}.txt'\nstudy_text = open(file_path, 'r').read()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T08:21:20.459197Z","iopub.execute_input":"2022-03-06T08:21:20.45951Z","iopub.status.idle":"2022-03-06T08:21:20.505521Z","shell.execute_reply.started":"2022-03-06T08:21:20.459476Z","shell.execute_reply":"2022-03-06T08:21:20.504841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(study_text)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T08:21:20.618086Z","iopub.execute_input":"2022-03-06T08:21:20.618742Z","iopub.status.idle":"2022-03-06T08:21:20.622547Z","shell.execute_reply.started":"2022-03-06T08:21:20.618704Z","shell.execute_reply":"2022-03-06T08:21:20.622008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Augmentation using Nlpaug","metadata":{}},{"cell_type":"markdown","source":"### Synonym Augmentation\n\nThe method simply consists to replace some of the words in the original text by their synonym. You can change the minimum of maximum number of replacement by specifying the parameters, as explained [here](https://nlpaug.readthedocs.io/en/latest/augmenter/word/synonym.html). As you can see here and below, using nlpaug, data augmentation is done in two lines!","metadata":{}},{"cell_type":"code","source":"# We take a text chunk from the train dataframe to apply augmentation\ntext_chunk = os_test_pdf.iloc[10][\"discourse_text\"]","metadata":{"execution":{"iopub.status.busy":"2022-03-06T08:21:44.146589Z","iopub.execute_input":"2022-03-06T08:21:44.147519Z","iopub.status.idle":"2022-03-06T08:21:44.151907Z","shell.execute_reply.started":"2022-03-06T08:21:44.147478Z","shell.execute_reply":"2022-03-06T08:21:44.15098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"syn_aug = naw.SynonymAug(aug_src='wordnet')\ntext_chunk_aug_syn = syn_aug.augment(text_chunk)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T08:21:49.988815Z","iopub.execute_input":"2022-03-06T08:21:49.989702Z","iopub.status.idle":"2022-03-06T08:21:50.044376Z","shell.execute_reply.started":"2022-03-06T08:21:49.989635Z","shell.execute_reply":"2022-03-06T08:21:50.043664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word2Vec Augmentation\n\nWord2Vec augmentation is similar to synonym augmentation, but it replace words not by its synonym, but rather by word having similar vectorial represenation. In order to use this augmentation, we need to specify the backbone. There are many available on Kaggle's dataset, here we shall use the most common GoogleNews trained one, which you can find [here](https://www.kaggle.com/umbertogriffo/googles-trained-word2vec-model-in-python). ","metadata":{}},{"cell_type":"code","source":"word2vec_path = \"../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin\"\nw2v_aug = naw.WordEmbsAug(\n    model_type='word2vec', model_path=word2vec_path,\n    action=\"substitute\")\ntext_chunk_aug_w2v = w2v_aug.augment(text_chunk)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T08:21:50.404198Z","iopub.execute_input":"2022-03-06T08:21:50.405026Z","iopub.status.idle":"2022-03-06T08:21:54.16582Z","shell.execute_reply.started":"2022-03-06T08:21:50.404973Z","shell.execute_reply":"2022-03-06T08:21:54.164789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Contextual Embedding\n\nContextual embedding use nlp models (here transformers), to understand the context of the input text and replace/add words keeping the context. As a result, the new text may have additional words or slightly different meaning. \n\nHere for the embedding model, we shall use pretrained roberta model. Please notice that internet connection is necessary to download the model. It may be possible to add the model used for the feature prize predictio, so as to create text in the essence of the original dataset.","metadata":{}},{"cell_type":"code","source":"transf_aug = naw.ContextualWordEmbsAug(\n    model_path=\"roberta-base\", action=\"substitute\")\ntext_chunk_aug_transf = transf_aug.augment(text_chunk)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T08:21:54.167584Z","iopub.execute_input":"2022-03-06T08:21:54.168142Z","iopub.status.idle":"2022-03-06T08:21:55.906675Z","shell.execute_reply.started":"2022-03-06T08:21:54.168096Z","shell.execute_reply":"2022-03-06T08:21:55.905934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Back Translation\n\nThe back translation method consists to first translate the original text into another language (for instance French, German...), and then translate it back to the original one. This has as effects to create new texts having the same meaning, yet with different words/length. Here too you need internet connection, since under the hood nlpaug uses huggingface translation models (by default English -> German -> English).\n\nIt is also worth noticing that one can set the `device` parameter so as to use GPUs.","metadata":{}},{"cell_type":"code","source":"# 4) Back translation augmentation\n# back_trans_aug = naw.BackTranslationAug(device=\"cuda\")  # If using GPUs\nback_trans_aug = naw.BackTranslationAug()\ntext_chunk_aug_btrans = back_trans_aug.augment(text_chunk)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T08:21:55.908042Z","iopub.execute_input":"2022-03-06T08:21:55.908562Z","iopub.status.idle":"2022-03-06T08:22:09.525657Z","shell.execute_reply.started":"2022-03-06T08:21:55.908515Z","shell.execute_reply":"2022-03-06T08:22:09.524945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compare results","metadata":{}},{"cell_type":"code","source":"# Comparing the different augmentation method\nprint(\"Original:\")\nprint(text_chunk)\nprint(\"\")\nprint(\"Synonym Augmented Text:\")\nprint(text_chunk_aug_syn)\nprint(\"\")\nprint(\"Word2Vec Augmented Text:\")\nprint(text_chunk_aug_w2v)\nprint(\"\")\nprint(\"Roberta Augmented Text:\")\nprint(text_chunk_aug_transf)\nprint(\"\")\nprint(\"Back Translation Augmented Text:\")\nprint(text_chunk_aug_btrans)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T08:22:09.528456Z","iopub.execute_input":"2022-03-06T08:22:09.528734Z","iopub.status.idle":"2022-03-06T08:22:09.53747Z","shell.execute_reply.started":"2022-03-06T08:22:09.528677Z","shell.execute_reply":"2022-03-06T08:22:09.536891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Augmented dataset\n\nThe previous study showed that while there are many way avilable in augmenting text data, either *transformers* based approach or *back-translation* approach give the most prominent results. One shall therefore stick with the **back-translation** method in what follows. Let us briefly note that one can also use the transfomers method if we want to get extra dataset.\n\nWe shall now create a new dataset, in three steps:\n1. First, back translate all the discourses corresponding to a given text in the original training csv file\n2. Next, create a new text file by picking up the original text, and replacing the discourses inside by the new ones created in 1.\n3. Finally, create a new csv file, taking into account the new discourse positions, and prediction strings.","metadata":{}},{"cell_type":"markdown","source":"## Applying back-translation to dataset","metadata":{}},{"cell_type":"code","source":"# For testing purpose, we shall select only 5 texts\nn_augment = 5\nselected_id = np.random.choice(IDS, size=n_augment, replace=False)\n\ntrain_selected = train[train[\"id\"].isin(selected_id)]","metadata":{"execution":{"iopub.status.busy":"2022-03-06T08:26:34.397728Z","iopub.execute_input":"2022-03-06T08:26:34.398665Z","iopub.status.idle":"2022-03-06T08:26:34.423331Z","shell.execute_reply.started":"2022-03-06T08:26:34.398624Z","shell.execute_reply":"2022-03-06T08:26:34.42258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting the back translation augmentation method\naugmenter = back_trans_aug\n\n# Set the following to avoid warning message\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Applying augmentation to all the selected texts\n# Since this may take a while, we shall show progressbar using tqdm\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\n# train_selected[\"augmented_text\"] = train_selected.apply(lambda row : augmenter.augment(row[\"discourse_text\"]), axis=1)  # If you don't want to use tqdm\ntrain_selected[\"augmented_text\"] = train_selected.progress_apply(lambda row : augmenter.augment(row[\"discourse_text\"]), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T08:28:46.014463Z","iopub.execute_input":"2022-03-06T08:28:46.015296Z","iopub.status.idle":"2022-03-06T08:35:35.242625Z","shell.execute_reply.started":"2022-03-06T08:28:46.015248Z","shell.execute_reply":"2022-03-06T08:35:35.241746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_selected.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T13:50:21.217706Z","iopub.execute_input":"2022-03-02T13:50:21.218382Z","iopub.status.idle":"2022-03-02T13:50:21.235801Z","shell.execute_reply.started":"2022-03-02T13:50:21.218338Z","shell.execute_reply":"2022-03-02T13:50:21.234763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating new csv and txt files\n\nThe function takes the id of the text we want to update, and the previously created dataframe (containg the augmented dataset) to create the new text file. The function returns the new text file as well as a dataframe containg the new `discourse_start`, `discourse_end` and `prediction_string`.\n\nThe prediction string calculation is based on the discussion [here](https://www.kaggle.com/c/feedback-prize-2021/discussion/297591), as well as the official annoucement [here](https://www.kaggle.com/c/feedback-prize-2021/discussion/297688).","metadata":{}},{"cell_type":"code","source":"# Function to replace original text by new text and save it\n\ndef augment_text_file(text_id, train_pdf):\n    # Get original text\n    file_path = f'../input/feedback-prize-2021/train/{text_id}.txt'\n    with open(file_path, 'r') as fr:\n        original_text = fr.read()\n    \n    # Get corresponding training data\n    text_pdf = train_pdf[train_pdf[\"id\"] == text_id]\n        \n    # Init variables\n    char_pos_original = 0  # trace the character position in the original text\n    new_text = \"\"\n    discourse_start_list = []\n    discourse_end_list = []\n    prediction_string_list = []\n        \n    # Loop on the training data discourses\n    for row in text_pdf[[\"discourse_start\", \"discourse_end\"]].itertuples():\n        discourse_start, discourse_end = int(row[1]), int(row[2])\n                \n        # Copy the non-discourse text from the orginal\n        if char_pos_original < discourse_start:\n            new_text += original_text[char_pos_original:discourse_start] \n        \n        # Evaluate the new discourse starting position/string\n        discourse_start_new = len(new_text)\n        discourse_start_list.append(discourse_start_new)\n        word_start = len(new_text.split())\n        \n        # Copy the augmented discourse text\n        new_text += text_pdf[text_pdf[\"discourse_start\"] == discourse_start][\"augmented_text\"].iloc[0]\n    \n        # Evaluate the new discourse end position/string\n        discourse_end_list.append(len(new_text))\n        word_end = word_start + len(new_text[discourse_start_new:].split())     \n        prediction_string_list.append(\" \".join([str(x) for x in range(word_start, word_end)]))\n        \n        char_pos_original = discourse_end\n\n    text_pdf[\"discourse_start_augmented\"] = discourse_start_list\n    text_pdf[\"discourse_end_augmented\"] = discourse_end_list\n    text_pdf[\"predictionstring_augmented\"] = prediction_string_list\n    \n    # Copy the remaining of the original text\n    if char_pos_original < len(original_text) - 1:\n        new_text += original_text[char_pos_original:len(original_text)]\n    \n    # Join the strings and save\n    save_path = f\"./data_augmented/{text_id}_aug.txt\"\n    with open(save_path, \"w\") as fw:\n        fw.write(new_text)\n    \n    return new_text, text_pdf","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:55:19.758719Z","iopub.execute_input":"2022-03-06T09:55:19.759054Z","iopub.status.idle":"2022-03-06T09:55:19.771355Z","shell.execute_reply.started":"2022-03-06T09:55:19.759026Z","shell.execute_reply":"2022-03-06T09:55:19.770178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying the function to our training dataset (containing the augmented text)\ndag_info_list = []\nfor text_id in tqdm(train_selected[\"id\"].unique()):\n    new_text, text_pdf = augment_text_file(text_id, train_selected)\n    dag_info_list.append(text_pdf)\ndag_info = pd.concat(dag_info_list)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T10:10:10.14057Z","iopub.execute_input":"2022-03-06T10:10:10.140889Z","iopub.status.idle":"2022-03-06T10:10:10.224658Z","shell.execute_reply.started":"2022-03-06T10:10:10.140856Z","shell.execute_reply":"2022-03-06T10:10:10.223818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dag_info.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T10:10:13.809896Z","iopub.execute_input":"2022-03-06T10:10:13.810635Z","iopub.status.idle":"2022-03-06T10:10:13.830399Z","shell.execute_reply.started":"2022-03-06T10:10:13.810586Z","shell.execute_reply":"2022-03-06T10:10:13.829552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Checking\n\nThe new dataframe now contains:\n- `discourse_start_augmented`\n- `discourse_end_augmented`\n- `augmented_text` (don't know why I didn't call it text_augmented...)\n- `predictionstring_augmented`\n\nLet us see how do they look like.","metadata":{}},{"cell_type":"code","source":"# Specify here the index of the dataframe one wants to check\ncheck_idx = 0\n\n# Loading texts\ncheck_id = dag_info.iloc[check_idx][\"id\"]\nwith open(f'../input/feedback-prize-2021/train/{check_id}.txt', \"r\") as f:\n    original_text = f.read()\nwith open(f\"./data_augmented/{check_id}_aug.txt\") as f:\n    new_text = f.read()\n\n# Checking the original discourse\nprint(\"Original\")\nprint(f\"----- Discourse text in the dataframe: \\n {dag_info.iloc[check_idx]['discourse_text']}\")\nprint(f\"----- Discourse text in the text file: \\n {original_text[int(dag_info.iloc[check_idx]['discourse_start']):int(dag_info.iloc[check_idx]['discourse_end'])]}\")\n\n# Checking the new discourse\nprint(\"\\n New\")\nprint(f\"----- Discourse text in the dataframe: \\n {dag_info.iloc[check_idx]['augmented_text']}\")\nprint(f\"----- Discourse text in the text file: \\n {new_text[int(dag_info.iloc[check_idx]['discourse_start_augmented']):int(dag_info.iloc[check_idx]['discourse_end_augmented'])]}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-06T10:22:45.702149Z","iopub.execute_input":"2022-03-06T10:22:45.702463Z","iopub.status.idle":"2022-03-06T10:22:45.71352Z","shell.execute_reply.started":"2022-03-06T10:22:45.70243Z","shell.execute_reply":"2022-03-06T10:22:45.712952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save\nYou can finally save the dataframe into csv file, (almost) ready to use for your model! If you want to use it in your model, together with the original data, you should be careful in dropping/renaming some of the columns and modify the `id` as well as `discourse_id` to avoid duplicates. \n\n\nAs for the text files, they are saved in the directory you have specified at the beginning of the notebook ;)","metadata":{}},{"cell_type":"code","source":"dag_info.to_csv(\"train_augmented.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}