{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is a process that standardizes the examples in the training set. The main issue I see is `discourse_start` and `discourse_end` being slightly off.  By standardizing and cleaning the examples, we can hopefully create consistent training data that causes an improvement in model performance.\n\n#### If `text` is the text from the file, then it would be expected that `file_text[discourse_start:discourse_end]=discourse_text` but in 44k rows, this is not the case.\n\n#### Here is an example where `file_text[discourse_start:discourse_end]!=discourse_text`\n```\ndiscourse_id = 1622992280991.0\ndiscourse_text = \"First, cell phones are a benefit and allows everyone to have access to a telephone at all times of the day.\\n\"\nfile_text[discourse_start:discourse_end]=\"rst, cell phones are a benefit and allows everyone to have access to a telephone at all times of the day.\n\nK\"\n```\nAfter it goes through my script: \n```\n'First, cell phones are a benefit and allows everyone to have access to a telephone at all times of the day.\\n'\n```\n\n#### Here is an example straight from the csv where there is a bit at the beginning that looks unecessary:\n```\ndiscourse_id = 1622489430075.0\ndiscourse_text = '. Drivers should not be able to use cell phones in any capacity while operating a motor vehicle. '\nfile_text[discourse_start:discourse_end] = '. Drivers should not be able to use cell phones in any capacity while operating a motor vehicle.\\n'\n```\nAfter it goes through my script: \n```\n'Drivers should not be able to use cell phones in any capacity while operating a motor vehicle.\\n'\n```\n\n# Please look at the following discussions for more information about this topic!\n- [Mystery Solved - Discrepancy Between PredictionString and DiscourseText](https://www.kaggle.com/c/feedback-prize-2021/discussion/297591)  \n- [Additional Information from Competition Hosts (rubric, dataset, raters, etc.)](https://www.kaggle.com/c/feedback-prize-2021/discussion/297688)\n- [Correcting the labels (Minor magic?)](https://www.kaggle.com/c/feedback-prize-2021/discussion/296778)\n\n\n#### Looking closer, there are many instances of a single character being swapped for another. In 30k out of 44k instances, one has `\"\\n\"` when the other has `\" \"` as the final character. That difference doesn't actually matter, but there are some other instances that have much worse alignment so this notebook will be my attempt at cleaning it up. \n\n#### After making these changes, about 16k `discourse_start` values change and 66k `discourse_end` values change. ðŸ˜®\n\n#### Please leave a comment if you have questions or suggestions! (Some of the cells take a few minutes. I tried to use the easy multi-processing capabilities of `datasets` to speed it up as much as possible)\n<p style=\"font-size: 40px\">ðŸ˜Š</p>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## This section compares the text extracted using the `discourse_start` and `discourse_end` positions with `discourse_text`\n\nThe text might look the same because the only difference is \" \" and \"\\n\", so below each example I indicate which index is different between the two of them and what characters they are. `[(87, ' ', '\\n')]` means that the character at index 87 is different. In `discourse_text` it has a space and in the file text it has a newline character.","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport re\nimport string\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nfrom datasets import Dataset\n\ndf = pd.read_csv(\"../input/feedback-prize-2021/train.csv\")\n\n# for each row, grab the span of text from the file using discourse_start and discourse_end\ndef get_text_by_index(example):\n    id_ = example[\"id\"]\n    start = example[\"discourse_start\"]\n    end = example[\"discourse_end\"]\n    with open(f\"../input/feedback-prize-2021/train/{id_}.txt\") as fp:\n        file_text = fp.read()\n    return {\n        \"text_by_index\": file_text[int(start) : int(end)]\n    }\n\nid_ds = Dataset.from_pandas(df[[\"id\", \"discourse_start\", \"discourse_end\"]])\n\ntext_ds = id_ds.map(get_text_by_index, num_proc=4)\ndf[\"text_by_index\"] = text_ds[\"text_by_index\"]\n\nnot_equal_texts = df[df[\"discourse_text\"] != df[\"text_by_index\"]]\nprint(f\"There are {len(not_equal_texts)} that are not equal\")\n\n# Let's look at a few\ndiscourse_texts = not_equal_texts[\"discourse_text\"]\nfile_spans = not_equal_texts[\"text_by_index\"]\ndiscourse_ids = not_equal_texts[\"discourse_id\"]\n\nfor counter, (discourse_text, file_span, discourse_id) in enumerate(\n    zip(discourse_texts, file_spans, discourse_ids)\n):\n    if counter > 5:\n        break\n\n    if len(discourse_text) != len(file_span):\n        continue\n\n    print(\"discourse_id =\", discourse_id)\n    print(\"\\n***discourse_text in train.csv***\\n\")\n    print(discourse_text)\n    print(\"\\n\"+\"-\" * 20)\n    print(\"\\n***Using discourse_start and discourse_end***\\n\")\n    print(file_span)\n\n    # Print index of character that differs between the two texts\n    print(\n        [\n            (i, char1, char2)\n            for i, (char1, char2) in enumerate(zip(discourse_text, file_span))\n            if char1 != char2\n        ]\n    )\n\n    print(\"\\n\" + \"*\" * 20 + \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-12-28T23:26:13.269942Z","iopub.execute_input":"2021-12-28T23:26:13.270252Z","iopub.status.idle":"2021-12-28T23:27:05.976037Z","shell.execute_reply.started":"2021-12-28T23:26:13.270168Z","shell.execute_reply":"2021-12-28T23:27:05.973916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## At first glance it just looks like newlines and spaces are getting swapped. If we only look for instances with letters being swapped, let's see what comes out","metadata":{}},{"cell_type":"code","source":"counter = 0\ndiscourse_texts = not_equal_texts[\"discourse_text\"]\nfile_spans = not_equal_texts[\"text_by_index\"]\ndiscourse_ids = not_equal_texts[\"discourse_id\"]\n\nfor discourse_text, file_span, discourse_id in zip(\n    discourse_texts, file_spans, discourse_ids\n):\n    if counter >= 2:\n        break\n\n    if len(discourse_text) != len(file_span):\n        continue\n\n    # Print index of character that differs between the two texts\n    diffs = [\n        (i, char1, char2)\n        for i, (char1, char2) in enumerate(zip(discourse_text, file_span))\n        if char1 != char2\n    ]\n\n    if not diffs[0][1].isalpha():\n        continue\n\n    print(\"discourse_id =\", discourse_id)\n    print(\"\\n***discourse_text in train.csv***\\n\")\n    print(discourse_text)\n    print(\"-\" * 20)\n    print(\"\\n***Using discourse_start and discourse_end***\\n\")\n    print(file_span)\n\n    # Print index of difference in char\n    print(diffs)\n\n    print(\"\\n\" + \"*\" * 20 + \"\\n\")\n    counter += 1\n","metadata":{"execution":{"iopub.status.busy":"2021-12-28T23:27:05.977779Z","iopub.execute_input":"2021-12-28T23:27:05.977992Z","iopub.status.idle":"2021-12-28T23:27:06.036531Z","shell.execute_reply.started":"2021-12-28T23:27:05.977966Z","shell.execute_reply":"2021-12-28T23:27:06.035446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ok now it looks like a few are misaligned by a few characters which makes it look like there are tons of characters that are different. Here is a counter of all the times the characters didn't align. Keep in mind that this includes the cases when one is a shifted version of another (like in `discourse_id=1622992466917.0`)","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\nall_diffs = []\nfor discourse_text, file_text in not_equal_texts[[\"discourse_text\", \"text_by_index\"]].values:\n    \n    if len(discourse_text) != len(file_text):\n        continue\n        \n    all_diffs.extend([(char1, char2) for char1, char2 in zip(discourse_text, file_text) if char1!=char2])\n\n    \ncounter = Counter(all_diffs)\n\ncounter.most_common(20)","metadata":{"execution":{"iopub.status.busy":"2021-12-28T23:27:06.037636Z","iopub.execute_input":"2021-12-28T23:27:06.037838Z","iopub.status.idle":"2021-12-28T23:27:08.453694Z","shell.execute_reply.started":"2021-12-28T23:27:06.037809Z","shell.execute_reply":"2021-12-28T23:27:08.452456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## My approach to fix the incorrect `discourse_start` and `discourse_end` values\n\nThe first step is to check if we can use the entire `discourse_text` to find the starting index in the file text.\n\n\nIf a match with the entire `discourse_text` string is not found, I'll take the first ~20 or so characters from `discourse_text` and see where it starts in the file text and use that as the starting point.\n\nIf the span starts with punctuation and then text, I'll keep increasing the start index until it isn't whitespace or punctuation. This eliminates the examples that begin with a period or comma. \n\nIf the span ends in whitespace, I'll keep it. This whitespace could be beneficial for the model. \n\nIf the span does not end in whitespace and the character after the span is punctuation or whitespace, extend the span to include it. Extending it can hopefully add useful information and it also standardizes the examples to have trailing whitespace but not leading whitespace. Adding the punctuation or whitespace would ***not*** change the `predictionstring` but it ***would*** change how the NER labeling is done.","metadata":{}},{"cell_type":"code","source":"%%time\n\nPUNCTUATION = set(\".,;\")\n\ndef get_new_positions(examples):\n    \n    disc_ids = []\n    new_starts = []\n    new_ends = []\n    new_texts = []\n    \n    for id_ in examples[\"id\"]:\n    \n        with open(f\"../input/feedback-prize-2021/train/{id_}.txt\") as fp:\n            file_text = fp.read()\n\n        discourse_data = df[df[\"id\"] == id_]\n\n        discourse_ids = discourse_data[\"discourse_id\"]\n        discourse_texts = discourse_data[\"discourse_text\"]\n        discourse_starts = discourse_data[\"discourse_start\"]\n        for disc_id, disc_text, disc_start in zip(discourse_ids, discourse_texts, discourse_starts):\n            disc_text = disc_text.strip()\n\n            matches = [x for x in re.finditer(re.escape(disc_text), file_text)]\n            offset = 0\n            while len(matches) == 0 and offset < len(disc_text):\n                chunk = disc_text if offset == 0 else disc_text[:-offset]\n                matches = [x for x in re.finditer(re.escape(chunk), file_text)]\n                offset += 5\n            if offset >= len(disc_text):\n                print(f\"Could not find substring in {disc_id}\")\n                continue\n\n            # There are some instances when there are multiple matches, \n            # so we'll take the closest one to the original discourse_start\n            distances = [abs(disc_start-match.start()) for match in matches]\n\n            idx = matches[np.argmin(distances)].start()                \n\n            end_idx = idx + len(disc_text)\n\n            # if it starts with whitespace or punctuation, increase idx\n            while file_text[idx].split()==[] or file_text[idx] in PUNCTUATION:\n                idx += 1\n            \n            # if the next \n            if (end_idx < len(file_text) and \n                (file_text[end_idx-1]!=[] or file_text[end_idx-1] not in PUNCTUATION) and \n                (file_text[end_idx].split()==[] or file_text[end_idx] in PUNCTUATION)):\n                end_idx += 1\n\n            final_text = file_text[idx:end_idx]\n            \n            disc_ids.append(disc_id)\n            new_starts.append(idx)\n            new_ends.append(idx + len(final_text))\n            new_texts.append(final_text)\n            \n    return {\n        \"discourse_id\": disc_ids,\n        \"new_start\": new_starts,\n        \"new_end\": new_ends,\n        \"text_by_new_index\": new_texts,\n    }\n\n# using Dataset will make it easy to do multi-processing        \ndataset = Dataset.from_dict({\"id\": df[\"id\"].unique()})   \n\nresults = dataset.map(get_new_positions, batched=True, num_proc=4, remove_columns=[\"id\"])","metadata":{"execution":{"iopub.status.busy":"2021-12-28T23:27:08.455298Z","iopub.execute_input":"2021-12-28T23:27:08.455546Z","iopub.status.idle":"2021-12-28T23:29:42.022417Z","shell.execute_reply.started":"2021-12-28T23:27:08.455518Z","shell.execute_reply":"2021-12-28T23:29:42.021571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"new_start\"] = results[\"new_start\"]\ndf[\"new_end\"] = results[\"new_end\"]\ndf[\"text_by_new_index\"] = results[\"text_by_new_index\"]","metadata":{"execution":{"iopub.status.busy":"2021-12-28T23:29:42.024817Z","iopub.execute_input":"2021-12-28T23:29:42.025511Z","iopub.status.idle":"2021-12-28T23:29:42.680218Z","shell.execute_reply.started":"2021-12-28T23:29:42.025478Z","shell.execute_reply":"2021-12-28T23:29:42.679297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's check how many of these new spans of text don't match the original `discourse_text` values\n","metadata":{}},{"cell_type":"code","source":"new_not_equal_texts = df[df[\"discourse_text\"]!=df[\"text_by_new_index\"]].copy()\nprint(f\"There are {new_not_equal_texts['id'].nunique()} files and {len(new_not_equal_texts)} rows with mismatched spans.\")","metadata":{"execution":{"iopub.status.busy":"2021-12-28T23:29:42.681429Z","iopub.execute_input":"2021-12-28T23:29:42.681704Z","iopub.status.idle":"2021-12-28T23:29:42.860707Z","shell.execute_reply.started":"2021-12-28T23:29:42.681668Z","shell.execute_reply":"2021-12-28T23:29:42.859708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## There are still many that don't match because I deleted some leading punctuation and added some trailing punctuation\n\n#### NOTE: One row does not match because `discourse_text` did not have the PII masked for some reason (discourse_id = 1623258656795.0). ","metadata":{}},{"cell_type":"code","source":"new_not_equal_texts[\"discourse_text\"] = new_not_equal_texts[\"discourse_text\"]\nnew_not_equal_texts[\"text_by_new_index\"] = new_not_equal_texts[\"text_by_new_index\"]\n\n# if we cutoff the last few characters, they will are more likely to be equal\nold_text = new_not_equal_texts[\"discourse_text\"].str.strip().str.slice(start=2, stop=3)\nnew_text = new_not_equal_texts[\"text_by_new_index\"].str.strip().str.slice(start=2, stop=3)\n\n\nchar_unequal_mask = old_text!=new_text\n\nunequal_texts = new_not_equal_texts[char_unequal_mask]\n\nunequal_texts[[\"discourse_text\", \"text_by_new_index\"]].sample(n=25).values","metadata":{"execution":{"iopub.status.busy":"2021-12-28T23:29:42.86228Z","iopub.execute_input":"2021-12-28T23:29:42.862559Z","iopub.status.idle":"2021-12-28T23:29:43.348021Z","shell.execute_reply.started":"2021-12-28T23:29:42.862521Z","shell.execute_reply":"2021-12-28T23:29:43.346788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting predictionstring values","metadata":{}},{"cell_type":"code","source":"%%time\n\ndef find_pred_string(examples):\n    \n    new_pred_strings = []\n    discourse_ids = []\n    \n    for id_ in examples[\"id\"]:\n        with open(f\"../input/feedback-prize-2021/train/{id_}.txt\") as fp:\n            file_text = fp.read()\n\n        discourse_data = df[df[\"id\"] == id_]\n        \n        left_idxs = discourse_data[\"new_start\"]\n        right_idxs = discourse_data[\"new_end\"]\n        disc_ids = discourse_data[\"discourse_id\"]\n        \n        for left_idx, right_idx, disc_id in zip(left_idxs, right_idxs, disc_ids):\n            start_word_id = len(file_text[:left_idx].split())\n            \n            # In the event that the first character of the span is not whitespace\n            # and the character before the span is not whitespace, `len(span.split())`\n            # will need to be reduced by 1.\n            # ex: word__word___sp[an starts in the middle of a word]\n            # `len(text[:left_idx].split())==3` but it actually starts in the 3rd word \n            # which is word_id=2\n            if left_idx > 0 and file_text[left_idx].split() != [] and file_text[left_idx-1].split() != []:\n                start_word_id -= 1\n                \n            end_word_id = start_word_id + len(file_text[left_idx:right_idx].split())\n            \n            new_pred_strings.append(\" \".join(list(map(str, range(start_word_id, end_word_id)))))\n            discourse_ids.append(disc_id)\n            \n            \n    return {\n        \"new_predictionstring\": new_pred_strings,\n        \"discourse_id\": discourse_ids\n    }\n        \n\nid_ds = Dataset.from_pandas(df[[\"id\"]].drop_duplicates())\nnew_pred_string_ds = id_ds.map(find_pred_string, batched=True, num_proc=4, remove_columns=id_ds.column_names)","metadata":{"execution":{"iopub.status.busy":"2021-12-28T23:38:23.008303Z","iopub.execute_input":"2021-12-28T23:38:23.008583Z","iopub.status.idle":"2021-12-28T23:40:10.829512Z","shell.execute_reply.started":"2021-12-28T23:38:23.008552Z","shell.execute_reply":"2021-12-28T23:40:10.828254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How many failed to find a substring?\n\nThere should be an empty string if no intersection is found.","metadata":{}},{"cell_type":"code","source":"df[\"new_predictionstring\"] = new_pred_string_ds[\"new_predictionstring\"]\nlen([x for x in new_pred_string_ds[\"new_predictionstring\"] if x == \"\"])","metadata":{"execution":{"iopub.status.busy":"2021-12-28T23:40:18.472743Z","iopub.execute_input":"2021-12-28T23:40:18.473688Z","iopub.status.idle":"2021-12-28T23:40:18.918988Z","shell.execute_reply.started":"2021-12-28T23:40:18.473611Z","shell.execute_reply":"2021-12-28T23:40:18.918534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's compare some new and old `predictionstring` values\n\nIn all the examples I looked at, the `new_predictionstring` values looked better","metadata":{"execution":{"iopub.status.busy":"2021-12-22T22:04:26.801771Z","iopub.execute_input":"2021-12-22T22:04:26.802448Z","iopub.status.idle":"2021-12-22T22:04:26.839279Z","shell.execute_reply.started":"2021-12-22T22:04:26.802402Z","shell.execute_reply":"2021-12-22T22:04:26.838392Z"}}},{"cell_type":"code","source":"different_value_mask = df[\"new_predictionstring\"] != df[\"predictionstring\"]\n\nfor idx, row in df[different_value_mask].sample(n=5, random_state=18).iterrows():\n    file_text = open(f\"../input/feedback-prize-2021/train/{row.id}.txt\").read()\n    print(\"Old predictionstring=\", row.predictionstring)\n    print(\"New predictionstring=\", row.new_predictionstring)\n    print(\"words using old predictionstring=\", [x for i, x in enumerate(file_text.split()) if i in list(map(int, row.predictionstring.split()))])\n    print(\"words using new predictionstring=\", [x for i, x in enumerate(file_text.split()) if i in list(map(int, row.new_predictionstring.split()))])\n    print(\"discourse text=\", row.text_by_new_index)\n    print(f\"start_idx/end_idx= {row.new_start}/{row.new_end}\")\n    print(\"discourse_id=\",row.discourse_id, \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-12-28T23:41:12.057951Z","iopub.execute_input":"2021-12-28T23:41:12.058182Z","iopub.status.idle":"2021-12-28T23:41:12.144892Z","shell.execute_reply.started":"2021-12-28T23:41:12.058158Z","shell.execute_reply":"2021-12-28T23:41:12.14419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## How many `discourse_start` and `discourse_end` values got modified?","metadata":{}},{"cell_type":"code","source":"print(sum(df[\"discourse_start\"].astype(int) != df[\"new_start\"]))\nprint(sum(df[\"discourse_end\"].astype(int) != df[\"new_end\"]))","metadata":{"execution":{"iopub.status.busy":"2021-12-28T23:41:55.141995Z","iopub.execute_input":"2021-12-28T23:41:55.142275Z","iopub.status.idle":"2021-12-28T23:41:55.204644Z","shell.execute_reply.started":"2021-12-28T23:41:55.142245Z","shell.execute_reply":"2021-12-28T23:41:55.203504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Looks pretty good to me!\n\n#### But please let me know if there is something I missed because I don't want the new training set to be more detrimental than the original ðŸ˜±","metadata":{}},{"cell_type":"markdown","source":"## Saving corrected information\n\nNew columns are:\n- `text_by_index` (can ignore)\n- `new_start` (replaces `discourse_start`)\n- `new_end` (replaces `discourse_end`)\n- `text_by_new_index` (replaces `discourse_text`)\n- `new_predictionstring` (replaces `predictionstring`)","metadata":{}},{"cell_type":"code","source":"df.to_csv(\"corrected_train.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-28T23:42:02.533485Z","iopub.execute_input":"2021-12-28T23:42:02.533889Z","iopub.status.idle":"2021-12-28T23:42:08.320889Z","shell.execute_reply.started":"2021-12-28T23:42:02.533853Z","shell.execute_reply":"2021-12-28T23:42:08.320091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hopefully this makes our models better! I know it can be confusing, so please comment and I'll do my best to answer your quesions!\n\n<p style=\"font-size: 40px\">ðŸ˜Š</p>","metadata":{}}]}