{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Because there are many different types of tokenizers, this is a simple eda looking at \n### 1. tokenized text length\n### 2. UNK tokens \n\nThis can help guide how long to make your sequence length and what tokens your tokenizer cannot handle. At the end there is also a little glimpse at the context around some of these unk tokens.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nfrom datasets import load_dataset\n\n# longformer, roberta all use the same tokenizer\n# bert, electra use the same tokenizer\n\ntokenizers = {\n    \"longformer\": AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\"),\n    \"bigbird\": AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\"),\n    \"albert\": AutoTokenizer.from_pretrained(\"albert-base-v2\"),\n    \"xlnet\": AutoTokenizer.from_pretrained(\"xlnet-base-cased\"),\n    \"electra\": AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\"), \n    \"deberta\": AutoTokenizer.from_pretrained(\"microsoft/deberta-base\"), \n}","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-22T19:05:14.685917Z","iopub.execute_input":"2022-01-22T19:05:14.686234Z","iopub.status.idle":"2022-01-22T19:06:10.188515Z","shell.execute_reply.started":"2022-01-22T19:05:14.686198Z","shell.execute_reply":"2022-01-22T19:06:10.187468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nfrom tqdm.notebook import tqdm\n\ntexts, ids = [], []\n\nfor file in tqdm(Path(\"../input/feedback-prize-2021/train\").glob(\"*.txt\"), total=15594, desc=\"Loading text files from train folder\"):\n    with open(file) as fp:\n        texts.append(fp.read())\n    ids.append(file.stem)\n\nfor file in tqdm(Path(\"../input/feedback-prize-2021/test\").glob(\"*.txt\"), total=5, desc=\"Loading text files from test folder\"):\n    with open(file) as fp:\n        texts.append(fp.read())\n    ids.append(file.stem)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:06:10.190339Z","iopub.execute_input":"2022-01-22T19:06:10.190568Z","iopub.status.idle":"2022-01-22T19:06:17.231848Z","shell.execute_reply.started":"2022-01-22T19:06:10.190539Z","shell.execute_reply":"2022-01-22T19:06:17.231067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenized lengths","metadata":{}},{"cell_type":"code","source":"from functools import partial\nfrom datasets import Dataset\n\ndef tokenize(examples, tokenizer, name):\n    \n    \n    ids = [tokenizer(text, truncation=False)[\"input_ids\"] for text in examples[\"text\"]]\n    lengths = list(map(len, ids))\n    \n    return {\n        f\"input_ids_{name}\": ids,\n        f\"lengths_{name}\": lengths,\n        \"text\": examples[\"text\"]\n    }\n\nbase_dataset = Dataset.from_dict({\"text\": texts, \"ids\": ids})\n\ndatasets = {}\n\nfor name, tokenizer in tokenizers.items():\n    base_dataset = base_dataset.map(\n        partial(\n            tokenize,\n            tokenizer=tokenizer,\n            name=name\n        ),\n        batched=True,\n        num_proc=4\n    )","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:06:17.233238Z","iopub.execute_input":"2022-01-22T19:06:17.233909Z","iopub.status.idle":"2022-01-22T19:08:48.803369Z","shell.execute_reply.started":"2022-01-22T19:06:17.233862Z","shell.execute_reply":"2022-01-22T19:08:48.802334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"length_df = base_dataset.to_pandas()\nlength_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:08:48.805348Z","iopub.execute_input":"2022-01-22T19:08:48.805626Z","iopub.status.idle":"2022-01-22T19:08:49.261739Z","shell.execute_reply.started":"2022-01-22T19:08:48.805576Z","shell.execute_reply":"2022-01-22T19:08:49.260881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px\nimport plotly.offline as pyo\npyo.init_notebook_mode()\n\nlong_df = pd.wide_to_long(length_df, stubnames=\"lengths\", i=\"ids\", j=\"name\", sep=\"_\", suffix=\".*\")[[\"lengths\"]].reset_index()\n\npx.histogram(long_df, x=\"lengths\", color=\"name\")","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:08:49.264276Z","iopub.execute_input":"2022-01-22T19:08:49.264554Z","iopub.status.idle":"2022-01-22T19:08:50.352787Z","shell.execute_reply.started":"2022-01-22T19:08:49.264518Z","shell.execute_reply":"2022-01-22T19:08:50.351976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Truncate to ignore long tail","metadata":{}},{"cell_type":"code","source":"truncated = long_df[long_df[\"lengths\"]<2000]\npx.histogram(truncated, x=\"lengths\", facet_row=\"name\", height=1000)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:08:50.354824Z","iopub.execute_input":"2022-01-22T19:08:50.355148Z","iopub.status.idle":"2022-01-22T19:08:50.933555Z","shell.execute_reply.started":"2022-01-22T19:08:50.355104Z","shell.execute_reply":"2022-01-22T19:08:50.932757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nlength_percentiles = pd.DataFrame(columns=[\"length\", \"percentile\", \"model\"])\n\nfor name in tokenizers.keys():\n    column = f\"lengths_{name}\"\n    lengths = length_df[column].values\n    percs = np.linspace(0,1,101)\n    quantile_lengths = np.quantile(lengths, percs)\n    length_percentiles = length_percentiles.append(pd.DataFrame({\"length\": quantile_lengths, \"percentile\": percs, \"model\": [name]*len(percs)}))","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:08:50.93513Z","iopub.execute_input":"2022-01-22T19:08:50.935578Z","iopub.status.idle":"2022-01-22T19:08:50.968674Z","shell.execute_reply.started":"2022-01-22T19:08:50.935536Z","shell.execute_reply":"2022-01-22T19:08:50.967719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.line(\n    length_percentiles, \n    x=\"length\", \n    y=\"percentile\", \n    color=\"model\", \n    title=\"raw texts\",\n    labels={\"percentile\": \"percent texts with tokenized length below length\"}, \n    height=600)\n\nfig.update_xaxes(range=[0, 2000])","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:08:50.970053Z","iopub.execute_input":"2022-01-22T19:08:50.970314Z","iopub.status.idle":"2022-01-22T19:08:51.094Z","shell.execute_reply.started":"2022-01-22T19:08:50.970282Z","shell.execute_reply":"2022-01-22T19:08:51.092975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# UNK tokens","metadata":{}},{"cell_type":"code","source":"def add_unk_tokens(example, tokenizer, name):\n    unk_id = tokenizer.unk_token_id\n    unk_idxs = [i for i, id_ in enumerate(example[f\"input_ids_{name}\"]) if id_==unk_id]\n    if unk_idxs:\n        example[f\"{name}_unk_tokens\"] = [x for i, x in enumerate(tokenizer.tokenize(example[\"text\"], add_special_tokens=True)) if i in unk_idxs]\n    else:\n        example[f\"{name}_unk_tokens\"] = []\n    example[f\"{name}_num_unk_toks\"] = len(example[f\"{name}_unk_tokens\"])\n    return example\n\nfor name in tokenizers.keys():\n    base_dataset = base_dataset.map(\n        partial(\n            add_unk_tokens,\n            tokenizer=tokenizers[name],\n            name=name\n        ),\n        num_proc=4\n    )","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:08:51.095325Z","iopub.execute_input":"2022-01-22T19:08:51.096026Z","iopub.status.idle":"2022-01-22T19:12:09.828229Z","shell.execute_reply.started":"2022-01-22T19:08:51.095983Z","shell.execute_reply":"2022-01-22T19:12:09.827012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name in tokenizers.keys():\n    print(f'Total number of unk tokens ({name}): {sum(base_dataset[f\"{name}_num_unk_toks\"])}')\n    print(f'Average number of unk tokens per text ({name}): {np.mean(base_dataset[f\"{name}_num_unk_toks\"])}')\n    print(f'Median number of unk tokens per text ({name}): {np.median(base_dataset[f\"{name}_num_unk_toks\"])}', \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:12:09.830524Z","iopub.execute_input":"2022-01-22T19:12:09.830875Z","iopub.status.idle":"2022-01-22T19:12:10.071479Z","shell.execute_reply.started":"2022-01-22T19:12:09.830826Z","shell.execute_reply":"2022-01-22T19:12:10.070455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\nunk_tokens = {}\nunk_counters = {}\n\nfor name in tokenizers.keys():\n    tkns = []\n    for tokens in base_dataset[f\"{name}_unk_tokens\"]:\n        tkns.extend(tokens)\n    token_string = \"\".join(tkns)\n    unk_tokens[name] = set(token_string)\n    unk_counters[name] = Counter(token_string)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:12:10.072759Z","iopub.execute_input":"2022-01-22T19:12:10.07301Z","iopub.status.idle":"2022-01-22T19:12:10.884286Z","shell.execute_reply.started":"2022-01-22T19:12:10.07298Z","shell.execute_reply":"2022-01-22T19:12:10.88338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name in unk_counters.keys():\n    print(f\"All unique unk tokens for {name}\", unk_tokens[name])\n    print(f\"Unk token counts for {name}\", unk_counters[name], \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:12:10.885723Z","iopub.execute_input":"2022-01-22T19:12:10.885981Z","iopub.status.idle":"2022-01-22T19:12:10.895695Z","shell.execute_reply.started":"2022-01-22T19:12:10.885951Z","shell.execute_reply":"2022-01-22T19:12:10.894114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Looking at weird characters","metadata":{}},{"cell_type":"code","source":"all_chars = list(set(\"\".join(texts)))\n\nnon_alpha = [x for x in all_chars if not x.isalnum()]\nunprintable = [x for x in all_chars if not x.isprintable() and not x.isalnum()]\nwhitespace = [x for x in all_chars if x.isspace()]","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:12:10.896991Z","iopub.execute_input":"2022-01-22T19:12:10.89746Z","iopub.status.idle":"2022-01-22T19:12:11.261972Z","shell.execute_reply.started":"2022-01-22T19:12:10.897427Z","shell.execute_reply":"2022-01-22T19:12:11.260944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x in [non_alpha, unprintable, whitespace]:\n    print(x, \"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:12:11.265618Z","iopub.execute_input":"2022-01-22T19:12:11.266021Z","iopub.status.idle":"2022-01-22T19:12:11.275667Z","shell.execute_reply.started":"2022-01-22T19:12:11.265971Z","shell.execute_reply":"2022-01-22T19:12:11.274145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Counting weird characters","metadata":{}},{"cell_type":"code","source":"all_chars = \"\".join(texts)\n\nnon_alpha = [x for x in all_chars if not x.isalnum()]\nunprintable = [x for x in all_chars if not x.isprintable() and not x.isalnum()]\nwhitespace = [x for x in all_chars if x.isspace()]","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:12:11.2772Z","iopub.execute_input":"2022-01-22T19:12:11.277526Z","iopub.status.idle":"2022-01-22T19:12:17.755508Z","shell.execute_reply.started":"2022-01-22T19:12:11.277483Z","shell.execute_reply":"2022-01-22T19:12:17.754332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x in [non_alpha, unprintable, whitespace]:\n    c = Counter(x)\n    print(c.most_common(20), \"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:12:17.756848Z","iopub.execute_input":"2022-01-22T19:12:17.757363Z","iopub.status.idle":"2022-01-22T19:12:18.447236Z","shell.execute_reply.started":"2022-01-22T19:12:17.757324Z","shell.execute_reply":"2022-01-22T19:12:18.44651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Looking at a weird character in context","metadata":{}},{"cell_type":"code","source":"x82 = []\nfor t in texts:\n    if \"\\x82\" in t:\n        x82.append(t) ","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:12:18.448358Z","iopub.execute_input":"2022-01-22T19:12:18.448975Z","iopub.status.idle":"2022-01-22T19:12:18.479541Z","shell.execute_reply.started":"2022-01-22T19:12:18.448919Z","shell.execute_reply":"2022-01-22T19:12:18.478705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\nrandom.sample(x82, 1)[0]","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:12:18.483109Z","iopub.execute_input":"2022-01-22T19:12:18.48373Z","iopub.status.idle":"2022-01-22T19:12:18.490436Z","shell.execute_reply.started":"2022-01-22T19:12:18.483689Z","shell.execute_reply":"2022-01-22T19:12:18.489845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### It looks like `Ã\\x82Â´` should be cleaned to an apostrophe '\n\nMaking this change will likely help the model slightly.","metadata":{}}]}