{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## This is the notebook used to create a line-by-line dataset to pre-train language models for the Feedback Prize competition.\n\nSee pre-training notebook here: https://www.kaggle.com/nbroad/feedback-prize-pre-training-pt-gpu  \nDataset available here: https://www.kaggle.com/nbroad/feedback-prize-linebyline-text-dataset","metadata":{}},{"cell_type":"code","source":"import re\nfrom pathlib import Path\n\nfrom tqdm.auto import tqdm\n\n\n# Simple sentence splitting function\n\nalphabets= \"([A-Za-z])\"\nprefixes = re.compile(\"(Mr|St|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|Mt)[.]\")\nsuffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\nstarters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\nacronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\nwebsites = re.compile(\"[.](co|net|org|io|gov|edu|us)\")\netal = re.compile(r\"(\\bet al)[.]\")\nurls = re.compile(\"(www)[.]\")\ndigits =  re.compile(\"[.]([0-9])\")\n\ndef split_into_sentences(text):\n    text = \" \" + text + \"  \"\n    text = text.replace(\"\\n\",\" \")\n    text = prefixes.sub(\"\\\\1<prd>\",text)\n    text = websites.sub(\"<prd>\\\\1\",text)\n    text = urls.sub(\"\\\\1<prd>\",text)\n    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n    text = etal.sub(\"\\\\1<prd>\", text)\n    text = digits.sub(\"<prd>\\\\1\",text)\n    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n    text = text.replace(\".\",\".<stop>\")\n    text = text.replace(\"?\",\"?<stop>\")\n    text = text.replace(\"!\",\"!<stop>\")\n    text = text.replace(\"<prd>\",\".\")\n    sentences = text.split(\"<stop>\")\n    if sentences[-1] == '':\n        sentences = sentences[:-1]\n    sentences = [s.strip() for s in sentences]\n    return sentences","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-16T01:26:56.851113Z","iopub.execute_input":"2021-12-16T01:26:56.851415Z","iopub.status.idle":"2021-12-16T01:26:56.961535Z","shell.execute_reply.started":"2021-12-16T01:26:56.85133Z","shell.execute_reply":"2021-12-16T01:26:56.960718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Use both train and test documents. Split into sentences, merge small sentences with larger ones, write to file","metadata":{}},{"cell_type":"code","source":"with open(\"train.txt\", \"w\") as train_file:\n    for filename in tqdm(Path(\"../input/feedback-prize-2021/train\").glob(\"*.txt\"), total=15594):\n        with open(filename) as current_file:\n            text = current_file.read().replace(\"\\n\", \" \")\n        sentences = split_into_sentences(text)\n        \n        cleaned_sentences = []\n        \n        for idx, sent in enumerate(sentences):\n            \n            # if a sentence is short, add it to the previous one\n            if idx > 0 and len(sent) < 20:\n                cleaned_sentences[-1] = cleaned_sentences[-1].strip() + \" \" + sent\n            else:\n                cleaned_sentences.append(sent)\n        \n        train_file.write(\"\\n\".join(cleaned_sentences)+\"\\n\")\n        \n    for filename in tqdm(Path(\"../input/feedback-prize-2021/test\").glob(\"*.txt\")):\n        # Weird way of making sure to not put back-to-back \"\\n\"            \n        with open(filename) as current_file:\n            text = current_file.read().replace(\"\\n\", \" \")\n        sentences = split_into_sentences(text)\n        \n        cleaned_sentences = []\n        \n        for idx, sent in enumerate(sentences):\n            \n            # if a sentence is short, add it to the previous one\n            if idx > 0 and len(sent) < 20:\n                cleaned_sentences[-1] = cleaned_sentences[-1].strip() + \" \" + sent\n            else:\n                cleaned_sentences.append(sent)\n        \n        train_file.write(\"\\n\".join(cleaned_sentences)+\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-12-16T01:42:44.318206Z","iopub.execute_input":"2021-12-16T01:42:44.318488Z","iopub.status.idle":"2021-12-16T01:43:08.100495Z","shell.execute_reply.started":"2021-12-16T01:42:44.31846Z","shell.execute_reply":"2021-12-16T01:43:08.099848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### load into dataset object and remove short lines","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"text\", data_files=\"train.txt\", split=\"train\")\n\ndataset = dataset.filter(lambda x: len(x[\"text\"]) > 20) # remove short ones\ndataset","metadata":{"execution":{"iopub.status.busy":"2021-12-16T01:43:53.331882Z","iopub.execute_input":"2021-12-16T01:43:53.332141Z","iopub.status.idle":"2021-12-16T01:43:55.472172Z","shell.execute_reply.started":"2021-12-16T01:43:53.332116Z","shell.execute_reply":"2021-12-16T01:43:55.471381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### remove duplicates","metadata":{}},{"cell_type":"code","source":"def remove_duplicates(example):\n    if example[\"text\"] in seen:\n        return False\n    seen.add(example[\"text\"])\n    return True\n\nseen = set()\n\ndataset = dataset.filter(remove_duplicates)\ndataset","metadata":{"execution":{"iopub.status.busy":"2021-12-16T01:46:39.382843Z","iopub.execute_input":"2021-12-16T01:46:39.383133Z","iopub.status.idle":"2021-12-16T01:46:44.531075Z","shell.execute_reply.started":"2021-12-16T01:46:39.383102Z","shell.execute_reply":"2021-12-16T01:46:44.530545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### writing final file","metadata":{}},{"cell_type":"code","source":"with open(\"train.txt\", \"w\") as fp:\n    for i, line in enumerate(dataset[\"text\"]):\n        if i == 0:\n            fp.write(line.strip())\n        else:\n            fp.write(\"\\n\"+line.strip())","metadata":{"execution":{"iopub.status.busy":"2021-12-16T01:48:50.909725Z","iopub.execute_input":"2021-12-16T01:48:50.910034Z","iopub.status.idle":"2021-12-16T01:48:54.648404Z","shell.execute_reply.started":"2021-12-16T01:48:50.910002Z","shell.execute_reply":"2021-12-16T01:48:54.647545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### checking final file","metadata":{}},{"cell_type":"code","source":"with open(\"train.txt\") as fp:\n    \n    for i, line in enumerate(fp.readlines()):\n        if i > 100: break\n        print(line)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T01:49:26.948492Z","iopub.execute_input":"2021-12-16T01:49:26.948802Z","iopub.status.idle":"2021-12-16T01:49:27.093399Z","shell.execute_reply.started":"2021-12-16T01:49:26.948772Z","shell.execute_reply":"2021-12-16T01:49:27.092471Z"},"trusted":true},"execution_count":null,"outputs":[]}]}