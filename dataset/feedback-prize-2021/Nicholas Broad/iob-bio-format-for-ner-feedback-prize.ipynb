{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This creates data in the BIO format for training an NER model\n\nB = beginning  \nI = inside  \nO = outside   \n\nSome call it IOB - same thing. https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)\n\nThis format can be used with the Hugging Face [example scripts for token classification](https://github.com/huggingface/transformers/tree/master/examples/pytorch/token-classification)\n\nNER has been shown to get around ~~0.5~~ 0.6 on public LB already :)  https://www.kaggle.com/zzy990106/pytorch-ner-infer\n\nIf you want just the dataset, use this: https://www.kaggle.com/nbroad/feedbackprize-bio-ner-train-data\n\n### I include 2 different ways of tokenizing. The first tokenizes after breaking at whitespace, the second includes whitespace. \n\nI'm running some experiments to see which method is better. I have a feeling that the whitespace is valuable information that will get lost when using the first method. \n\n`split_at_whitespace.json` first splits the text at whitespace and then assigns a label to each token. This does not pass through a Tokenizer object.  \n`include_whitespace.json` does not split at whitespace. This output comes out of a Tokenizer. This is tokenizer-specific so make sure you use the right tokenizer :)\n\n\n### Update: Dec 23 - I came up with a way of correcting the misaligned labels, so I will run my code twice: once on the original file and once on the corrected file\n\nHere is my notebook that corrects the data: https://www.kaggle.com/nbroad/corrected-train-csv-feedback-prize\n\nFolder `original` uses the data given by the hosts.  \nFolder `corrected` uses the data from my corrected notebook.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ntrain_df = pd.read_csv(\"../input/feedback-prize-2021/train.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-24T13:56:11.535407Z","iopub.execute_input":"2021-12-24T13:56:11.535797Z","iopub.status.idle":"2021-12-24T13:56:13.715042Z","shell.execute_reply.started":"2021-12-24T13:56:11.535664Z","shell.execute_reply":"2021-12-24T13:56:13.714072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Option 1: Splitting at whitespace","metadata":{}},{"cell_type":"code","source":"# First step assigns a discourse type to each word\ndef add_discourse_type(example):\n    \n    id_ = example[\"id\"]\n    \n    features = train_df[train_df[\"id\"]==id_]\n\n    with open(f\"../input/feedback-prize-2021/train/{id_}.txt\") as fp:\n        text = fp.read()\n\n    words = text.split()\n\n    labels = [\"O\"]*len(words)\n\n    for discourse, predictions in features[[\"discourse_type\", \"predictionstring\"]].values:\n        idx_iter = map(int, predictions.split())\n        for idx in idx_iter:\n            labels[idx] = discourse \n        \n    return {\n        \"id\" : id_,\n        \"labels\": labels,\n        \"words\": words,\n    }","metadata":{"execution":{"iopub.status.busy":"2021-12-24T13:56:13.716891Z","iopub.execute_input":"2021-12-24T13:56:13.717296Z","iopub.status.idle":"2021-12-24T13:56:13.725538Z","shell.execute_reply.started":"2021-12-24T13:56:13.717261Z","shell.execute_reply":"2021-12-24T13:56:13.724632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfrom datasets import Dataset\n\nds = Dataset.from_dict({\"id\": train_df[\"id\"].unique()}) \n\ntagged_ds = ds.map(add_discourse_type, num_proc=4)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T13:56:13.726861Z","iopub.execute_input":"2021-12-24T13:56:13.727161Z","iopub.status.idle":"2021-12-24T13:59:07.522781Z","shell.execute_reply.started":"2021-12-24T13:56:13.727127Z","shell.execute_reply":"2021-12-24T13:59:07.521551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Second step adds B or I as prefix to the label\ndef add_bi(example):\n    new_tags = []\n    \n    if example[\"labels\"][0] != \"O\":\n        new_tags.append(f\"B-{example['labels'][0]}\")\n    else:\n        new_tags.append(\"O\")\n    \n    for idx in range(1, len(example[\"labels\"])):\n        current_tag = example['labels'][idx]\n        if current_tag == \"O\":\n            new_tags.append(current_tag)\n        elif example[\"labels\"][idx-1] != current_tag:\n            new_tags.append(f\"B-{current_tag}\")\n        else:\n            new_tags.append(f\"I-{current_tag}\")\n            \n    example[\"bio\"] = new_tags\n    return example","metadata":{"execution":{"iopub.status.busy":"2021-12-24T13:59:07.526101Z","iopub.execute_input":"2021-12-24T13:59:07.526648Z","iopub.status.idle":"2021-12-24T13:59:07.536201Z","shell.execute_reply.started":"2021-12-24T13:59:07.526606Z","shell.execute_reply":"2021-12-24T13:59:07.534805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nbio_dataset = tagged_ds.map(add_bi, remove_columns=[\"labels\"], num_proc=4)\nbio_dataset","metadata":{"execution":{"iopub.status.busy":"2021-12-24T13:59:07.53841Z","iopub.execute_input":"2021-12-24T13:59:07.53919Z","iopub.status.idle":"2021-12-24T13:59:32.336169Z","shell.execute_reply.started":"2021-12-24T13:59:07.53914Z","shell.execute_reply":"2021-12-24T13:59:32.33497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's look at some\n{key: vals[:10] for key, vals in bio_dataset[11].items()}","metadata":{"execution":{"iopub.status.busy":"2021-12-24T13:59:32.337879Z","iopub.execute_input":"2021-12-24T13:59:32.338151Z","iopub.status.idle":"2021-12-24T13:59:32.347287Z","shell.execute_reply.started":"2021-12-24T13:59:32.338114Z","shell.execute_reply":"2021-12-24T13:59:32.346662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check what tags were added\nfrom itertools import chain\nfrom collections import Counter\n\nall_tags_no_whitespace = list(chain(*bio_dataset[\"bio\"]))\nmost_common_no_whitespace = Counter(all_tags_no_whitespace).most_common(20)\nmost_common_no_whitespace","metadata":{"execution":{"iopub.status.busy":"2021-12-24T13:59:32.348621Z","iopub.execute_input":"2021-12-24T13:59:32.349017Z","iopub.status.idle":"2021-12-24T13:59:45.993658Z","shell.execute_reply.started":"2021-12-24T13:59:32.348976Z","shell.execute_reply":"2021-12-24T13:59:45.992285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bio_dataset.to_json(\"split_at_whitespace.json\")","metadata":{"execution":{"iopub.status.busy":"2021-12-24T13:59:45.995661Z","iopub.execute_input":"2021-12-24T13:59:45.996002Z","iopub.status.idle":"2021-12-24T13:59:52.238688Z","shell.execute_reply.started":"2021-12-24T13:59:45.995951Z","shell.execute_reply":"2021-12-24T13:59:52.23749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Option 2: Keeping whitespace","metadata":{}},{"cell_type":"code","source":"labels = train_df[\"discourse_type\"].unique()\nlabels = [f\"B-{label}\" for label in labels] + [f\"I-{label}\" for label in labels]\nlabels.append(\"O\")\n\nlen(labels), labels","metadata":{"execution":{"iopub.status.busy":"2021-12-24T13:59:52.243081Z","iopub.execute_input":"2021-12-24T13:59:52.245531Z","iopub.status.idle":"2021-12-24T13:59:52.270251Z","shell.execute_reply.started":"2021-12-24T13:59:52.245484Z","shell.execute_reply":"2021-12-24T13:59:52.269576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# This uses a bigbird tokenizer, but it will work with any tokenizer type \ntokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")","metadata":{"execution":{"iopub.status.busy":"2021-12-24T13:59:52.274588Z","iopub.execute_input":"2021-12-24T13:59:52.275497Z","iopub.status.idle":"2021-12-24T14:00:06.289081Z","shell.execute_reply.started":"2021-12-24T13:59:52.275453Z","shell.execute_reply":"2021-12-24T14:00:06.288387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\n\nWHITESPACE = set(string.whitespace + \"\\xa0\")\ndef chars_to_word_ids(text, char_label_preds):\n    \n    all_word_preds = []\n    current_id = 0\n    current_word_preds = []\n    for char, pred in zip(text, char_preds):\n        \n        # If we are not in whitespace and have do/don't have predictions\n        if char not in WHITESPACE:\n            current_word_preds.append(pred)\n            \n        # If we reach whitespace and have predictions\n        elif current_word_preds and char in WHITESPACE:\n            pass # Figure out a label for this word\n            \n            # Add that label to the list of word preds\n            all_word_preds.append(LABEL)\n            current_word_preds = []\n            current_id += 1\n            \n        # If we are in whitespace and do not have predictions\n        elif not current_word_preds and char in WHITESPACE:\n            pass # do nothing\n            \n        \n\n    return all_word_preds\n\ndef add_labels(example):\n    \n    id_ = example[\"id\"]\n\n    text = open(f\"../input/feedback-prize-2021/train/{id_}.txt\").read()\n    \n    tokenized = tokenizer(text, truncation=True, padding=\"max_length\", max_length=1024, return_offsets_mapping=True)\n    \n    discourse_data = train_df[train_df[\"id\"]==id_]\n    \n    char_labels = [\"O\"]*len(text)\n    \n    num_chars = len(text)\n    \n    for start, end, label in discourse_data[[\"discourse_start\", \"discourse_end\", \"discourse_type\"]].values:\n        for idx in range(int(start), int(end)):\n            if idx >= num_chars:\n                break\n            char_labels[idx] = f\"I-{label}\"\n        char_labels[int(start)] = f\"B-{label}\"\n    \n    token_labels = [\"O\"]*len(tokenized[\"input_ids\"])\n    token_offsets = tokenized[\"offset_mapping\"]\n    for idx, (start_offset, end_offset) in enumerate(token_offsets):\n        if start_offset == end_offset and start_offset == 0:\n            continue\n        for char_label in char_labels[start_offset:end_offset]:\n            token_labels[idx] = char_label\n            if char_label.startswith(\"B-\"):\n                break\n    \n    return {\n        \"input_ids\": tokenized[\"input_ids\"],\n        \"attention_mask\": tokenized[\"attention_mask\"],\n        \"offset_mapping\": token_offsets,\n        \"labels\": token_labels\n    }","metadata":{"execution":{"iopub.status.busy":"2021-12-24T14:00:06.291405Z","iopub.execute_input":"2021-12-24T14:00:06.291749Z","iopub.status.idle":"2021-12-24T14:00:06.303559Z","shell.execute_reply.started":"2021-12-24T14:00:06.291701Z","shell.execute_reply":"2021-12-24T14:00:06.302848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfrom datasets import Dataset\n\nds = Dataset.from_dict({\"id\": list(train_df[\"id\"].unique())})\n\ntokenized_ds = ds.map(add_labels, num_proc=4)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T14:00:06.304898Z","iopub.execute_input":"2021-12-24T14:00:06.305275Z","iopub.status.idle":"2021-12-24T14:04:22.703986Z","shell.execute_reply.started":"2021-12-24T14:00:06.305236Z","shell.execute_reply":"2021-12-24T14:04:22.702472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's look at some\n\nid_ = ds[\"id\"][101]\n\nsample_labels = add_labels({\"id\":id_})\n\nlist(zip(\n    tokenizer.convert_ids_to_tokens(sample_labels[\"input_ids\"]), \n    sample_labels[\"offset_mapping\"], \n    sample_labels[\"labels\"]\n))[:100]","metadata":{"execution":{"iopub.status.busy":"2021-12-24T14:04:22.707264Z","iopub.execute_input":"2021-12-24T14:04:22.70804Z","iopub.status.idle":"2021-12-24T14:04:22.807523Z","shell.execute_reply.started":"2021-12-24T14:04:22.707965Z","shell.execute_reply":"2021-12-24T14:04:22.806513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check what tags were added\n\nall_tags_with_whitespace = list(chain(*tokenized_ds[\"labels\"]))\nmost_common_with_whitespace = Counter(all_tags_with_whitespace).most_common(20)\nmost_common_with_whitespace","metadata":{"execution":{"iopub.status.busy":"2021-12-24T14:04:22.810118Z","iopub.execute_input":"2021-12-24T14:04:22.810866Z","iopub.status.idle":"2021-12-24T14:04:47.362877Z","shell.execute_reply.started":"2021-12-24T14:04:22.81081Z","shell.execute_reply":"2021-12-24T14:04:47.361977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_ds.to_json(\"include_whitespace.json\")","metadata":{"execution":{"iopub.status.busy":"2021-12-24T14:04:47.364465Z","iopub.execute_input":"2021-12-24T14:04:47.364705Z","iopub.status.idle":"2021-12-24T14:05:20.766112Z","shell.execute_reply.started":"2021-12-24T14:04:47.364675Z","shell.execute_reply":"2021-12-24T14:05:20.765238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparison of tags\n\nThere are huge differences in \"O\" and \"I-\" tags because words get broken into multiple tokens, but there should roughly be the same number of \"B-\" tags. The values are close enough for me, but if you can think of a better way of labeling it, let me know! While I was doing this, I did notice some issues which I listed here: https://www.kaggle.com/c/feedback-prize-2021/discussion/296524","metadata":{}},{"cell_type":"code","source":"list(zip(most_common_no_whitespace, most_common_with_whitespace))","metadata":{"execution":{"iopub.status.busy":"2021-12-24T14:05:20.767485Z","iopub.execute_input":"2021-12-24T14:05:20.76777Z","iopub.status.idle":"2021-12-24T14:05:20.77827Z","shell.execute_reply.started":"2021-12-24T14:05:20.767715Z","shell.execute_reply":"2021-12-24T14:05:20.777199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Move into folder\n%mkdir original\n%mv *.json original/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now with the corrected data","metadata":{}},{"cell_type":"code","source":"new_train_df = pd.read_csv(\"../input/feedback-prize-corrected-train-csv/corrected_train.csv\")\n\nprint(train_df.shape, new_train_df.shape)\n\ntrain_df = new_train_df.drop(columns=[\"discourse_start\", \"discourse_end\", \"predictionstring\"])\ndel new_train_df\n\ntrain_df = train_df.rename(columns={\"new_start\": \"discourse_start\", \"new_end\": \"discourse_end\", \"new_predictionstring\": \"predictionstring\"})\n\nds = Dataset.from_dict({\"id\": train_df[\"id\"].unique()}) \n\ntagged_ds = ds.map(add_discourse_type, num_proc=4)\nbio_dataset = tagged_ds.map(add_bi, remove_columns=[\"labels\"], num_proc=4)\n\ncorrected_dir = \"corrected\"\n%mkdir $corrected_dir\n\nbio_dataset.to_json(f\"./{corrected_dir}/split_at_whitespace.json\")\n\nall_tags_no_whitespace = list(chain(*bio_dataset[\"bio\"]))\nmost_common_no_whitespace = Counter(all_tags_no_whitespace).most_common(20)\n\nds = Dataset.from_dict({\"id\": list(train_df[\"id\"].unique())})\ntokenized_ds = ds.map(add_labels, num_proc=4)\n\nall_tags_with_whitespace = list(chain(*tokenized_ds[\"labels\"]))\nmost_common_with_whitespace = Counter(all_tags_with_whitespace).most_common(20)\n\ntokenized_ds.to_json(f\"./{corrected_dir}/include_whitespace.json\")\n\nlist(zip(most_common_no_whitespace, most_common_with_whitespace))","metadata":{"execution":{"iopub.status.busy":"2021-12-24T14:05:20.779861Z","iopub.execute_input":"2021-12-24T14:05:20.780393Z","iopub.status.idle":"2021-12-24T14:14:42.95807Z","shell.execute_reply.started":"2021-12-24T14:05:20.780356Z","shell.execute_reply":"2021-12-24T14:14:42.956691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking if there are any I- labels that don't have a B- before them","metadata":{}},{"cell_type":"code","source":"counter = 0\nfor example in tokenized_ds:\n    if counter > 10: break\n    \n    prev = None\n    for label in example[\"labels\"]:\n        if prev is not None:\n            if label.startswith(\"I-\") and prev.replace(\"B-\", \"\").replace(\"I-\", \"\") != label.replace(\"B-\", \"\").replace(\"I-\", \"\"):\n                if label != \"O\":\n                    print(prev, label, example)\n                    counter += 1\n        prev = label","metadata":{"execution":{"iopub.status.busy":"2021-12-24T14:14:44.870222Z","iopub.execute_input":"2021-12-24T14:14:44.870603Z","iopub.status.idle":"2021-12-24T14:17:11.581214Z","shell.execute_reply.started":"2021-12-24T14:14:44.870552Z","shell.execute_reply":"2021-12-24T14:17:11.580156Z"},"trusted":true},"execution_count":null,"outputs":[]}]}