{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\nfrom torch.autograd import Variable\nimport torch.autograd as autograd\nimport numpy as np\nimport pandas as pd\nimport glob\nfrom tqdm import tqdm\n##\nfrom collections import Counter\n## data preparation\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n## pl trainer\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nimport json\n\n## токенизацию берем из Roberta\nfrom transformers import RobertaTokenizerFast","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-30T09:06:54.985535Z","iopub.execute_input":"2021-12-30T09:06:54.985948Z","iopub.status.idle":"2021-12-30T09:06:54.993976Z","shell.execute_reply.started":"2021-12-30T09:06:54.985903Z","shell.execute_reply":"2021-12-30T09:06:54.993125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH_PART = '/kaggle/input/feedback-prize-2021/'","metadata":{"execution":{"iopub.status.busy":"2021-12-30T09:06:54.995597Z","iopub.execute_input":"2021-12-30T09:06:54.995838Z","iopub.status.idle":"2021-12-30T09:06:55.012141Z","shell.execute_reply.started":"2021-12-30T09:06:54.995808Z","shell.execute_reply":"2021-12-30T09:06:55.011269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_mask(batch_tensor):\n    mask = batch_tensor.eq(0)\n    mask = mask.eq(0)\n    return mask\n\n\n### CharCNN.py script\n\nclass CharCNN(nn.Module):\n    def __init__(self, alphabet_size, embedding_dim, hidden_dim, dropout):\n        super(CharCNN, self).__init__()\n        print(\"build char sequence feature extractor: CNN ...\")\n        self.hidden_dim = hidden_dim\n        self.char_drop = nn.Dropout(dropout)\n        self.char_embeddings = nn.Embedding(alphabet_size, embedding_dim)\n        self.char_embeddings.weight.data.copy_(torch.from_numpy(CharCNN.random_embedding(alphabet_size, embedding_dim)))\n        self.char_cnn = nn.Conv1d(embedding_dim, self.hidden_dim, kernel_size=3, padding=1)\n\n    @staticmethod\n    def random_embedding(vocab_size, embedding_dim):\n        pretrain_emb = np.empty([vocab_size, embedding_dim])\n        scale = np.sqrt(3.0 / embedding_dim)\n        for index in range(vocab_size):\n            pretrain_emb[index, :] = np.random.uniform(-scale, scale, [1, embedding_dim])\n        pretrain_emb[0, :] = np.zeros((1, embedding_dim))\n        return pretrain_emb\n\n    def forward(self, input):\n\n        batch_size = input.size(0)\n        char_embeds = self.char_drop(self.char_embeddings(input))\n        char_embeds = char_embeds.transpose(2, 1).contiguous()\n        char_cnn_out = self.char_cnn(char_embeds)\n        char_cnn_out = F.max_pool1d(char_cnn_out, char_cnn_out.size(2)).contiguous().view(batch_size, -1)\n        return char_cnn_out","metadata":{"execution":{"iopub.status.busy":"2021-12-30T09:06:55.013596Z","iopub.execute_input":"2021-12-30T09:06:55.013941Z","iopub.status.idle":"2021-12-30T09:06:55.027096Z","shell.execute_reply.started":"2021-12-30T09:06:55.013899Z","shell.execute_reply":"2021-12-30T09:06:55.026129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Dataset preparation\ndef create_mapping(dico):\n    \"\"\"\n    Create a mapping (item to ID / ID to item) from a dictionary.\n    Items are ordered by decreasing frequency.\n    \"\"\"\n    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n    item_to_id = {v: k for k, v in id_to_item.items()}\n    return item_to_id, id_to_item\n\n\ndef char_mapping(train_sentences):\n    chars = ''.join([w for s in train_sentences for w in s])\n    char_count = dict(Counter(chars))\n    char_count['<pad>'] = 1\n    char_count['<unk>'] = 3\n    char_to_id, id_to_char = create_mapping(char_count)\n    print(\"Found %i unique characters\" % len(char_count))\n    return char_count, char_to_id, id_to_char","metadata":{"execution":{"iopub.status.busy":"2021-12-30T09:06:55.028065Z","iopub.execute_input":"2021-12-30T09:06:55.028858Z","iopub.status.idle":"2021-12-30T09:06:55.04363Z","shell.execute_reply.started":"2021-12-30T09:06:55.028813Z","shell.execute_reply":"2021-12-30T09:06:55.042956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Named_Entity_Recognition_Model(nn.Module):\n    def __init__(self,\n                 vocab_size, \n                 word_embed_dim, \n                 word_hidden_dim,\n                 alphabet_size, \n                 char_embedding_dim, \n                 char_hidden_dim,\n                 tag_num, dropout,\n                 pretrain_embed=None, \n                 use_char=False):\n        super(Named_Entity_Recognition_Model, self).__init__()\n        self.use_char = use_char\n        self.drop = nn.Dropout(dropout)\n        self.input_dim = word_embed_dim\n\n        self.embeds = nn.Embedding(vocab_size, word_embed_dim, padding_idx=0)\n        if pretrain_embed is not None:\n            self.embeds.weight.data.copy_(torch.from_numpy(pretrain_embed))\n        else:\n            self.embeds.weight.data.copy_(torch.from_numpy(self.random_embedding(vocab_size, word_embed_dim)))\n\n        if self.use_char:\n            self.input_dim += char_hidden_dim\n            self.char_feature = CharCNN(alphabet_size, char_embedding_dim, char_hidden_dim, dropout)\n\n        self.lstm = nn.LSTM(self.input_dim, word_hidden_dim, batch_first=True, bidirectional=True)\n        self.hidden2tag = nn.Linear(word_hidden_dim * 2, tag_num)\n\n    def random_embedding(self, vocab_size, embedding_dim):\n        pretrain_emb = np.empty([vocab_size, embedding_dim])\n        scale = np.sqrt(3.0 / embedding_dim)\n        for index in range(1, vocab_size):\n            pretrain_emb[index, :] = np.random.uniform(-scale, scale, [1, embedding_dim])\n        return pretrain_emb\n\n    def neg_log_likelihood_loss(self, word_inputs, word_seq_lengths, char_inputs, batch_label, mask):\n        batch_size = word_inputs.size(0)\n        seq_len = word_inputs.size(1)\n        word_embeding = self.embeds(word_inputs)\n        word_list = [word_embeding]\n        if self.use_char:\n            char_features = self.char_feature(char_inputs).contiguous().view(batch_size, seq_len, -1)\n            word_list.append(char_features)\n        word_embeding = torch.cat(word_list, 2)\n        word_represents = self.drop(word_embeding)\n        packed_words = pack_padded_sequence(word_represents, word_seq_lengths, True)\n        hidden = None\n        lstm_out, hidden = self.lstm(packed_words, hidden)\n        lstm_out, _ = pad_packed_sequence(lstm_out)\n        lstm_out = lstm_out.transpose(0, 1)\n        feature_out = self.drop(lstm_out)\n        feature_out = self.hidden2tag(feature_out)\n        loss_function = nn.CrossEntropyLoss(ignore_index=-100, reduction='sum')\n        feature_out = feature_out.contiguous().view(batch_size * seq_len, -1)\n        total_loss = loss_function(feature_out, batch_label.contiguous().view(batch_size * seq_len))\n        return total_loss\n\n    def forward(self, word_inputs, word_seq_lengths, char_inputs, batch_label, mask):\n        batch_size = word_inputs.size(0)\n        seq_len = word_inputs.size(1)\n        word_embeding = self.embeds(word_inputs)\n        word_list = [word_embeding]\n        if self.use_char:\n            char_features = self.char_feature(char_inputs).contiguous().view(batch_size, seq_len, -1)\n            word_list.append(char_features)\n        word_embeding = torch.cat(word_list, 2)\n        word_represents = self.drop(word_embeding)\n        packed_words = pack_padded_sequence(word_represents, word_seq_lengths, True)\n        hidden = None\n        lstm_out, hidden = self.lstm(packed_words, hidden)\n        lstm_out, _ = pad_packed_sequence(lstm_out)\n        lstm_out = lstm_out.transpose(0, 1)\n        feature_out = self.drop(lstm_out)\n        feature_out = self.hidden2tag(feature_out)\n        feature_out = feature_out.contiguous().view(batch_size * seq_len, -1)\n        _, tag_seq = torch.max(feature_out, 1)\n        tag_seq = tag_seq.view(batch_size, seq_len)\n        tag_seq = mask.long() * tag_seq\n        return tag_seq","metadata":{"execution":{"iopub.status.busy":"2021-12-30T09:06:55.046621Z","iopub.execute_input":"2021-12-30T09:06:55.046951Z","iopub.status.idle":"2021-12-30T09:06:55.070941Z","shell.execute_reply.started":"2021-12-30T09:06:55.046909Z","shell.execute_reply":"2021-12-30T09:06:55.070052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NER_Dataset(Dataset):\n    def __init__(self, texts, word_to_id, char_to_id, input_offset_mappings):\n        self.word_to_id = word_to_id\n        self.char_to_id = char_to_id\n        self.texts = texts\n        self.input_offset_mappings = input_offset_mappings\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text_id = []\n        label_id = []\n        text = self.texts[item]\n        seq_char_list = list()\n        offset_mapping = self.input_offset_mappings[item]\n        for word in text:\n            ### особенность для роберты\n            word = word.strip()\n            if word in word_to_id:\n                word_id = word_to_id[word] # 1\n            elif 'Ġ' + word in word_to_id:\n                word_id = word_to_id['Ġ' + word]\n            elif 'Ġ' + word.lower() in word_to_id:\n                print('tut')\n                word_id = word_to_id['Ġ' + word.lower()]\n            else:\n                word_id = word_to_id['<unk>'] ## \n            ###\n            text_id.append(word_id)\n        text_tensor = torch.tensor(text_id).long()\n        for word in text:\n            char_list = list(word)\n            char_id = list()\n            for char in char_list:\n                char_id.append(self.char_to_id[char])\n            seq_char_list.append(char_id)\n        return {'text': text_tensor, 'char': seq_char_list, 'offset_mapping': torch.tensor(offset_mapping)}\n    \n    \ndef my_collate(key, batch_tensor):\n    if key == 'char':\n        batch_tensor = pad_char(batch_tensor)\n        return batch_tensor\n    else:\n        word_seq_lengths = torch.LongTensor(list(map(len, batch_tensor)))\n        _, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n        batch_tensor.sort(key=lambda x: len(x), reverse=True)\n        tensor_length = [len(sq) for sq in batch_tensor]\n        print\n        batch_tensor = pad_sequence(batch_tensor, batch_first=True, padding_value=0)\n        return batch_tensor, tensor_length, word_perm_idx\n\n\n### PADDING PART\ndef my_collate_fn(batch):\n    return {key: my_collate(key, [d[key] for d in batch]) for key in batch[0]}\n\n\ndef pad_char(chars):\n    batch_size = len(chars)\n    max_seq_len = max(map(len, chars))\n    pad_chars = [chars[idx] + [[0]] * (max_seq_len - len(chars[idx])) for idx in range(len(chars))]\n    length_list = [list(map(len, pad_char)) for pad_char in pad_chars]\n    max_word_len = max(map(max, length_list))\n    char_seq_tensor = torch.zeros((batch_size, max_seq_len, max_word_len)).long()\n    char_seq_lengths = torch.LongTensor(length_list)\n    for idx, (seq, seqlen) in enumerate(zip(pad_chars, char_seq_lengths)):\n        for idy, (word, wordlen) in enumerate(zip(seq, seqlen)):\n            char_seq_tensor[idx, idy, :wordlen] = torch.LongTensor(word)\n\n    return char_seq_tensor","metadata":{"execution":{"iopub.status.busy":"2021-12-30T09:06:55.120758Z","iopub.execute_input":"2021-12-30T09:06:55.121455Z","iopub.status.idle":"2021-12-30T09:06:55.142357Z","shell.execute_reply.started":"2021-12-30T09:06:55.121401Z","shell.execute_reply":"2021-12-30T09:06:55.141482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## LOAD TAG IDS\nwith open('/kaggle/input/lstm-for-test-data/char_to_id.json', 'r') as fp:\n    char_to_id = json.load(fp)\n    \nwith open('/kaggle/input/lstm-for-test-data/labels_to_ids.json', 'r') as fp:\n    labels_to_ids = json.load(fp)\n    \nids_to_labels = {k:v for k,v in enumerate(labels_to_ids)}","metadata":{"execution":{"iopub.status.busy":"2021-12-30T09:06:55.145987Z","iopub.execute_input":"2021-12-30T09:06:55.146636Z","iopub.status.idle":"2021-12-30T09:06:55.166527Z","shell.execute_reply.started":"2021-12-30T09:06:55.14659Z","shell.execute_reply":"2021-12-30T09:06:55.165545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer =  RobertaTokenizerFast.from_pretrained('/kaggle/input/roberta-tokenizer', local_files_only=True) ## without internet \nword_to_id = tokenizer.vocab","metadata":{"execution":{"iopub.status.busy":"2021-12-30T09:06:55.168076Z","iopub.execute_input":"2021-12-30T09:06:55.168583Z","iopub.status.idle":"2021-12-30T09:06:55.493067Z","shell.execute_reply.started":"2021-12-30T09:06:55.168542Z","shell.execute_reply":"2021-12-30T09:06:55.492094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Вытащим целые тексты по id [test]\ntest_names, test_texts = [], []\nfor f in tqdm(list(glob.glob(PATH_PART+\"test/*\"))):\n    test_names.append(f.replace(PATH_PART+'test/', '').replace('.txt', ''))\n    test_texts.append(open(f, 'r').read())\ntest_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\ntest_texts.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-30T09:06:55.494429Z","iopub.execute_input":"2021-12-30T09:06:55.494823Z","iopub.status.idle":"2021-12-30T09:06:55.519395Z","shell.execute_reply.started":"2021-12-30T09:06:55.494791Z","shell.execute_reply":"2021-12-30T09:06:55.518522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = test_texts[['text', 'id']]\n###\ninput_sentences = []\ninput_offset_mappings = []\n###\ninput_sentence = []\nfor text_idx in tqdm(range(len(data))):\n    encoded_text = tokenizer(data.text.values[text_idx],\n              return_offsets_mapping=True, \n              padding='max_length', \n              truncation=True, \n              max_length=512)\n    ##\n    for token in encoded_text['input_ids']:\n        token_id = token\n        token_as_text = tokenizer.decode(token_id)\n        input_sentence.append(token_as_text)\n            \n    input_sentences.append( input_sentence )\n    input_offset_mappings.append( encoded_text['offset_mapping'] )\n    input_sentence = []  \n##","metadata":{"execution":{"iopub.status.busy":"2021-12-30T09:06:55.521407Z","iopub.execute_input":"2021-12-30T09:06:55.52167Z","iopub.status.idle":"2021-12-30T09:06:55.593304Z","shell.execute_reply.started":"2021-12-30T09:06:55.52164Z","shell.execute_reply":"2021-12-30T09:06:55.592437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### get embeddings from pretrained model\n### на выходе получаем словарь: слово-вектор\nglove_model = {}\nwith open('/kaggle/input/lstm-for-test-data/glove.6B.100d.txt','r') as f:\n    for line in f:\n        split_line = line.split()\n        word = split_line[0]\n        embedding = np.array(split_line[1:], dtype=np.float64)\n        glove_model[word] = embedding\n        \nVOCAB_SIZE = len(glove_model.keys())\n\n\n### get embedding matrix for train data\nword_embeds_for_train_data = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_to_id), 100))\nfor w in word_to_id:\n    if w in glove_model:\n        w_for_glove = w.lower().replace('Ġ', '') ## в glove они хранятся в нижнем регистре и убираем Ġ (особенность roberta)\n        word_embeds_for_train_data[word_to_id[w]] = glove_model[w_for_glove]\n    elif w.lower() in glove_model:\n        w_for_glove = w.lower().replace('Ġ', '')\n        word_embeds_for_train_data[word_to_id[w]] = glove_model[w_for_glove]","metadata":{"execution":{"iopub.status.busy":"2021-12-30T09:06:55.594715Z","iopub.execute_input":"2021-12-30T09:06:55.595578Z","iopub.status.idle":"2021-12-30T09:07:13.968441Z","shell.execute_reply.started":"2021-12-30T09:06:55.595526Z","shell.execute_reply":"2021-12-30T09:07:13.967425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for_inf_dataset = NER_Dataset(input_sentences, \n                              word_to_id, \n                              char_to_id,\n                              input_offset_mappings\n                             )\n\nfor_inf_dl = DataLoader(for_inf_dataset,\n                      shuffle=False,\n                      batch_size=1,\n                      num_workers=0,\n                      collate_fn=my_collate_fn)","metadata":{"execution":{"iopub.status.busy":"2021-12-30T09:07:13.969986Z","iopub.execute_input":"2021-12-30T09:07:13.970295Z","iopub.status.idle":"2021-12-30T09:07:13.978153Z","shell.execute_reply.started":"2021-12-30T09:07:13.970256Z","shell.execute_reply":"2021-12-30T09:07:13.97721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Named_Entity_Recognition_Model(vocab_size=len(word_to_id), \n                               word_embed_dim=100, \n                               word_hidden_dim=100, \n                               alphabet_size=len(char_to_id), \n                               char_embedding_dim=30, \n                               char_hidden_dim=50,\n                               tag_num = len(labels_to_ids), \n                               dropout=0.2, \n                               pretrain_embed=word_embeds_for_train_data,\n                               use_char=True) ## не работает с gpu. Требует доработки\n\nmodel.load_state_dict(torch.load('/kaggle/input/lstm-for-test-data/best_bilstm__model.pt'))","metadata":{"execution":{"iopub.status.busy":"2021-12-30T09:07:13.979635Z","iopub.execute_input":"2021-12-30T09:07:13.979951Z","iopub.status.idle":"2021-12-30T09:07:14.089351Z","shell.execute_reply.started":"2021-12-30T09:07:13.97991Z","shell.execute_reply":"2021-12-30T09:07:14.088471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nmodel = model.to('cpu')\npredictions = []\nout_strings = []\nfor batch in tqdm(for_inf_dl):\n    batch_text, seq_length, word_perm_idx = batch['text']\n    char_inputs = batch['char']\n    offset_mapping = batch['offset_mapping']\n    char_inputs = char_inputs[word_perm_idx]\n    char_dim = char_inputs.size(-1)\n    char_inputs = char_inputs.contiguous().view(-1, char_dim)\n    mask = get_mask(batch_text)\n    ## inference\n    outputs = model(batch_text, seq_length, char_inputs, None, mask) ## = flattened_predictions у Roberta\n    tokens = tokenizer.convert_ids_to_tokens(batch_text[0].squeeze().tolist())\n    token_predictions = [ids_to_labels[i] for i in outputs.cpu().numpy()[0]] ## outputs[0] указан потому, что batch_size > 1\n    wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n    prediction = []\n    out_str = []\n    off_list = offset_mapping[0][0].squeeze().tolist() ## второй [0] из offset_mapping убрать, он добавлен потому-что batch_size > 1\n    for idx, mapping in enumerate(off_list):\n        if mapping[0] != 0 and mapping[0] != off_list[idx-1][1]:\n            prediction.append(wp_preds[idx][1])\n            out_str.append(wp_preds[idx][0])\n        else:\n            if idx == 1:\n                prediction.append(wp_preds[idx][1])\n                out_str.append(wp_preds[idx][0])\n            continue\n    predictions.append(prediction)\n    out_strings.append(out_str)","metadata":{"execution":{"iopub.status.busy":"2021-12-30T09:07:14.090988Z","iopub.execute_input":"2021-12-30T09:07:14.091511Z","iopub.status.idle":"2021-12-30T09:07:14.756721Z","shell.execute_reply.started":"2021-12-30T09:07:14.091451Z","shell.execute_reply":"2021-12-30T09:07:14.755794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = []\nimport pdb\nfor i in tqdm(range(len(test_texts))):\n    idx = test_texts.id.values[i]\n    pred = [x.replace('B-','').replace('I-','') for x in predictions[i]]\n    preds = []\n    j = 0\n    while j < len(pred):\n        cls = pred[j]\n        if cls == 'O':\n            j += 1\n        end = j + 1\n        while end < len(pred) and pred[end] == cls:\n            end += 1\n        if cls != 'O' and cls != '' and end - j > 10:\n            final_preds.append((idx, cls, ' '.join(map(str, list(range(j, end))))))\n        j = end\n        \nprint(final_preds[2])","metadata":{"execution":{"iopub.status.busy":"2021-12-30T09:07:14.758166Z","iopub.execute_input":"2021-12-30T09:07:14.758586Z","iopub.status.idle":"2021-12-30T09:07:14.774691Z","shell.execute_reply.started":"2021-12-30T09:07:14.758541Z","shell.execute_reply":"2021-12-30T09:07:14.77398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(PATH_PART + 'sample_submission.csv')\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-12-30T09:07:14.776177Z","iopub.execute_input":"2021-12-30T09:07:14.776752Z","iopub.status.idle":"2021-12-30T09:07:14.796169Z","shell.execute_reply.started":"2021-12-30T09:07:14.776701Z","shell.execute_reply":"2021-12-30T09:07:14.795365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame(final_preds)\nsub.columns = test_df.columns\nsub.to_csv(\"submission.csv\", index=False)\nsub","metadata":{"execution":{"iopub.status.busy":"2021-12-30T09:07:14.797335Z","iopub.execute_input":"2021-12-30T09:07:14.798047Z","iopub.status.idle":"2021-12-30T09:07:14.813985Z","shell.execute_reply.started":"2021-12-30T09:07:14.798009Z","shell.execute_reply":"2021-12-30T09:07:14.812854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}