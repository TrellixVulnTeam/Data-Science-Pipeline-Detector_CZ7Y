{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n# DECLARE HOW MANY GPUS YOU WISH TO USE. \n# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\n#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n\n# VERSION FOR SAVING MODEL WEIGHTS\nVER='0.1.0'\n\n# IF VARIABLE IS NONE, THEN NOTEBOOK COMPUTES TOKENS\n# OTHERWISE NOTEBOOK LOADS TOKENS FROM PATH\n#LOAD_TOKENS_FROM = '../input/py-bigbird-v26'\nLOAD_TOKENS_FROM = '../input/feedbackprize2021/';\n\n\n# IF VARIABLE IS NONE, THEN NOTEBOOK TRAINS A NEW MODEL\n# OTHERWISE IT LOADS YOUR PREVIOUSLY TRAINED MODEL\n#LOAD_MODEL_FROM = '../input/py-bigbird-v26'\n#TRAINED_MODEL_DIR = '/home/peng_sun2/s3result/feedback-prize-2021/model/baseline1_ner_roberta'\nTRAINED_MODEL_DIR = '../input/feedbackprize2021/'\n\ndo_train=  True;\n#LOAD_MODEL_FROM = None;\n\n# IF FOLLOWING IS NONE, THEN NOTEBOOK \n# USES INTERNET AND DOWNLOADS HUGGINGFACE \n# CONFIG, TOKENIZER, AND MODEL\n#DOWNLOADED_MODEL_PATH = '../input/py-bigbird-v26' \n#DOWNLOADED_MODEL_PATH = '/home/peng_sun2/s3result/feedback-prize-2021/pretrained_model/baseline1_ner'\n\ntoken_path_to_save = '../input/feedbackprize2021/pretrained_roberta_token'\npretrained_model_path = '../input/feedbackprize2021/pretrained_roberta'\n#pretrained_model_path = '../input/feedbackprize2021/baseline1_ner_roberta_pretrained_cpu'\nconfig_path_to_save = '../input/feedbackprize2021/pretrained_roberta_config'\n#DOWNLOADED_MODEL_PATH = None;\n  \nMODEL_NAME = 'allenai/longformer-base-4096'\nfrom torch import cuda\nconfig = {\n    'model_name': MODEL_NAME,   \n    'max_length': 1024,\n    'train_batch_size':4,\n    'valid_batch_size':1,\n    'epochs':5,\n    'n_fold':5,\n    'scheduler': 'CosineAnnealingLR',\n    'learning_rate': 2.5e-5,\n    'weight_decay': 1e-6,\n    'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n    'max_grad_norm':10,\n    'device': 'cuda' if cuda.is_available() else 'cpu',\n    'seed': 2022,\n    'min_lr': 1e-6,\n    'T_max': 500\n    #'device': 'cpu'\n}\n\n# THIS WILL COMPUTE VAL SCORE DURING COMMIT BUT NOT DURING SUBMIT\nCOMPUTE_VAL_SCORE = True\nif len( os.listdir('../input/feedback-prize-2021/test') )>5:\n      COMPUTE_VAL_SCORE = False","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-18T16:21:13.023522Z","iopub.execute_input":"2022-02-18T16:21:13.025389Z","iopub.status.idle":"2022-02-18T16:21:14.500101Z","shell.execute_reply.started":"2022-02-18T16:21:13.025259Z","shell.execute_reply":"2022-02-18T16:21:14.499319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import *\nmodel_to_download = True;\nif model_to_download:\n    #os.mkdir('model')\n    tokenizer = AutoTokenizer.from_pretrained(token_path_to_save)\n    #tokenizer.save_pretrained(token_path_to_save)\n\n    config_model = AutoConfig.from_pretrained(config_path_to_save) \n    config_model.num_labels = 15\n    #config_model.save_pretrained(config_path_to_save)\n\n    backbone = AutoModelForTokenClassification.from_pretrained(pretrained_model_path, \n                                                               config=config_model)\n    #backbone.save_pretrained(pretrained_model_path)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T16:21:14.502064Z","iopub.execute_input":"2022-02-18T16:21:14.502379Z","iopub.status.idle":"2022-02-18T16:21:35.625705Z","shell.execute_reply.started":"2022-02-18T16:21:14.50234Z","shell.execute_reply":"2022-02-18T16:21:35.62483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\nbackbone.to(config['device'])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-18T16:21:35.62738Z","iopub.execute_input":"2022-02-18T16:21:35.62764Z","iopub.status.idle":"2022-02-18T16:21:36.287526Z","shell.execute_reply.started":"2022-02-18T16:21:35.627596Z","shell.execute_reply":"2022-02-18T16:21:36.285979Z"},"trusted":true},"execution_count":null,"outputs":[]}]}