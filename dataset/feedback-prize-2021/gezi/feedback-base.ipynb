{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os  \nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nimport traceback\n\n!ln -s ../input/feedback ./src\nif os.path.exists('/kaggle'):\n  sys.path.append('/kaggle/input/pikachu/utils')\n  sys.path.append('/kaggle/input/pikachu/third')\n  sys.path.append('.')\n!ls ../input","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q icecream --no-index --find-links=file:///kaggle/input/icecream/ ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q pymp-pypi --no-index --find-links=file:///kaggle/input/pymp-pypi/pymp-pypi-0.4.5/dist","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n  conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n  filepath = deberta_v2_path/filename\n  if filepath.exists():\n    filepath.unlink()\n\n  shutil.copy(input_dir/filename, filepath)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display\nimport tensorflow as tf\nimport torch\nfrom absl import flags\nFLAGS = flags.FLAGS\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\nfrom src import config\nfrom src.util import *\nfrom src.get_preds import *\nfrom src.eval import *\nimport melt as mt\nimport numpy as np\nimport glob\nimport gc\nfrom numba import cuda\nfrom gezi import tqdm\nimport gezi\nimport husky\nimport lele","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\nfor gpu in gpus:\n  tf.config.experimental.set_memory_growth(gpu, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_test_ids = 1000\nfolds = pd.read_csv('../input/feedback/folds.csv')\ntest_ids = folds[folds.kfold==0].id.values\ntest_ids.sort()\ntest_ids = test_ids[:num_test_ids]\nlen(test_ids)\n# test_ids","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gezi.init_flags()\nmodel_root = '../input'\nmodel_dirs = [x for x in glob.glob(f'{model_root}/feedback-model*') if os.path.isdir(x)]\nmodel_dirs = [f'../input/feedback-model{i}' for i in range(len(model_dirs))]\nmodel_dir = model_dirs[0]\n# model_dirs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make sure tf models first, then cuda.close() to release gpu then infer torch models\ntf_models = []\nfirst_models = []\nic(first_models)\nmodel_dirs = gezi.unique_list([*tf_models, *first_models, *model_dirs])\n\nm = model_dirs\nused_model_indexes = list(range(len(model_dirs)))\n# used_model_indexes = list(range(15))\n# used_model_indexes = [9]\n# 8,9,10,11,12,13\n# used_model_indexes = [0,1,2,3]\n# used_model_indexes = [0,1,2,3,8,9,10,11,12,13,14]\n# used_model_indexes = [3, 4]\nused_model_indexes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 17]\n# used_model_indexes = [30,31,32,33,34,35]\n# used_model_indexes += [36,37,38,39,40]\nused_model_indexes = [30,31,32]\n# used_model_indexes = [30]\nmodel_dirs = [m[i] for i in used_model_indexes]\nused_tf_models = [x for x in model_dirs if x in tf_models]\nnum_tf_models = len(used_tf_models)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_batch_size(backbone, model_dir=''):\n  if 'longformer' in backbone:\n    return 32\n  if 'bird' in backbone:\n    return 32\n  if 'bart' in backbone:\n    return 64\n  if 'deberta' in backbone and 'xlarge' in backbone:\n    return 32\n  if 'deberta' in backbone and any(x in model_dir for x in ['len1024']):\n    return 16\n  if 'deberta' in backbone and any(x in model_dir for x in ['len1280']):\n    return 16\n  if 'deberta' in backbone and any(x in model_dir for x in ['len1536']):\n    return 8\n  if 'deberta' in backbone and any(x in model_dir for x in ['len1600']):\n    return 8\n  if 'deberta' in backbone:\n    return 64\n  if 'funnel' in backbone:\n    return 8\n  return 128","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rename(backbone):\n  m = {\n    'roberta-large-squad2': 'roberta-large',\n    'large': 'funnel-large',\n  }\n  return m.get(backbone, backbone)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mns = []\nfor i, model_dir in tqdm(enumerate(model_dirs), total=len(model_dirs)):\n#   ic(model_dir)\n  gezi.restore_configs(model_dir)\n#   ic(FLAGS.model_dir)\n  mns.append(os.path.basename(FLAGS.model_dir))\n#   assert 'online' in FLAGS.model_dir\nmns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MIN_WEIGHT = 1\nweights_dict = {}\nweights_dict =  {\n#   'bart.start.run2': 9, \n#   'roberta.start.nwemb-0': 9,\n#   'deberta.start': 8,\n \n#   'deberta-xlarge.start': 9, \n#   'deberta-xlarge.end': 9, \n\n#   'deberta-v3.start.len1024.stride-256.seq_encoder-0': 10, \n#   'deberta-v3.start.len1024.stride-256': 6,\n\n#   'deberta-v3.start.len1536': 7, \n#   'deberta-v3.start.len1024.rnn_bi': 8, \n#   'deberta-v3.end.len1024.seq_encoder-0': 10,\n#   'deberta-v3.mid.len1024': 8,\n\n#   'deberta-v3.start.stride-256.seq_encoder-0': 7, \n#   'deberta-v3.start.nwemb-0.mark_end-0': 10, \n#   'deberta-v3.se': 10,\n#   'deberta-v3.se2': 10,\n\n#   'longformer.start.len1536': 6,\n#   'longformer.start.len1600': 6,\n#   'funnel.start.len1536.bs-8': 6,\n# #   'deberta-v3.start.len1280': 6,\n# #   'deberta-v3.start.len1280.rnn_layers-2': 6,\n    \n#   'deberta-v3.start.len1024.stride-512': 4,\n#   'electra.start.nwemb-0.run2': 7,\n}\nweights_dict0 = {'bart.start.run2': 6,\n                        'deberta-v3.end.len1024.seq_encoder-0': 6,\n                        'deberta-v3.mid.len1024': 4,\n                        'deberta-v3.se': 6,\n                        'deberta-v3.se2': 1,\n                        'deberta-v3.start.len1024.rnn_bi': 5,\n                        'deberta-v3.start.len1024.stride-256': 6,\n                        'deberta-v3.start.len1024.stride-256.seq_encoder-0': 10,\n                        'deberta-v3.start.len1536': 4,\n                        # 'deberta-v3.start.len1536': 2,\n                        # 'deberta-v3.start.mui-end-mid': 1,\n                        # 'deberta-v3.start.len1536.rnn_type-GRU': 4,\n                        'deberta-v3.start.nwemb-0.mark_end-0': 8,\n                        'deberta-v3.start.stride-256.seq_encoder-0': 9,\n                        'deberta-xlarge.end': 0,\n                        'deberta-xlarge.start': 6,\n                        'deberta.start': 6,\n                        'longformer.start.len1536': 9,\n                        'roberta.start.nwemb-0': 6}\nweights_dict1 = {'bart.start.run2': 7,\n                        'deberta-v3.end.len1024.seq_encoder-0': 10,\n                        'deberta-v3.mid.len1024': 6,\n                        'deberta-v3.se': 2,\n                        'deberta-v3.se2': 7,\n                        'deberta-v3.start.len1024.rnn_bi': 8,\n                        'deberta-v3.start.len1024.stride-256': 10,\n                        'deberta-v3.start.len1024.stride-256.seq_encoder-0': 7,\n                        'deberta-v3.start.len1536': 8,\n                        # 'deberta-v3.start.len1536': 4,\n                        # 'deberta-v3.start.mui-end-mid': 1,\n                        # 'deberta-v3.start.len1536.rnn_type-GRU': 8,\n                        'deberta-v3.start.nwemb-0.mark_end-0': 8,\n                        'deberta-v3.start.stride-256.seq_encoder-0': 7,\n                        'deberta-xlarge.end': 7,\n                        'deberta-xlarge.start': 10,\n                        'deberta.start': 6,\n                        'longformer.start.len1536': 8,\n                        'roberta.start.nwemb-0': 5}\nweights_dict2 = {'bart.start.run2': 6,\n                        'deberta-v3.end.len1024.seq_encoder-0': 7,\n                        'deberta-v3.mid.len1024': 5,\n                        'deberta-v3.se': 9,\n                        'deberta-v3.se2': 5,\n                        'deberta-v3.start.len1024.rnn_bi': 9,\n                        'deberta-v3.start.len1024.stride-256': 6,\n                        'deberta-v3.start.len1024.stride-256.seq_encoder-0': 10,\n                        'deberta-v3.start.len1536': 6,\n                        # 'deberta-v3.start.len1536': 1.5,\n                        # 'deberta-v3.start.mui-end-mid': 0,\n                        # 'deberta-v3.start.len1536.rnn_type-GRU': 3,\n                        'deberta-v3.start.nwemb-0.mark_end-0': 0,\n                        'deberta-v3.start.stride-256.seq_encoder-0': 5,\n                        'deberta-xlarge.end': 8,\n                        'deberta-xlarge.start': 4,\n                        'deberta.start': 8,\n                        'longformer.start.len1536': 9,\n                        'roberta.start.nwemb-0': 6}\nweights_dicts = [weights_dict0, weights_dict1, weights_dict2]\nic(gezi.sort_byval(weights_dict))\nlen(weights_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_weight(x):\n#   assert x in weights_dict\n#   if weights_dict:\n#     return weights_dict[x]\n#   return 1\ndef get_weight(x, idx=0):\n  weight = 1\n  # return 1\n  if x in weights_dict:\n    return weights_dicts[idx][x]\n    # return weights_dict[x]\n  return max(weight, 1)\n\n# if weights_dict:\n#   mns_ori = mns.copy()\n#   indxes = [i for i in range(len(mns)) if mns[i] in weights_dict and weights_dict[mns[i]] >= MIN_WEIGHT]\n#   mns = [mns[i] for i in range(len(mns_ori)) if i in indxes]\n#   model_dirs = [model_dirs[i] for i in range(len(mns_ori)) if i in indxes]\n    \nweights = [get_weight(x) for x in mns]\nweights2 = [get_weight(x, 1) for x in mns]\nweights3 = [get_weight(x, 2) for x in mns]\nic(list(zip(range(len(model_dirs)), model_dirs, mns, weights)), len(model_dirs))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"verify = False\n# verify = True\nif verify:\n    mode = 'train'\n    ensembler = Ensembler(need_sort=True)\n    for i, model_dir in tqdm(enumerate(model_dirs), total=len(model_dirs)):\n      ic(model_dir)\n      gezi.init_flags()\n      FLAGS.multi_inputs = False\n      FLAGS.seq_encoder = False\n      FLAGS.merge_tokens = False\n      FLAGS.split_punct = False\n      FLAGS.custom_tokenize = False\n      FLAGS.ori_deberta_v2_tokenizer = True\n      FLAGS.max_len_valid = None\n      FLAGS.lower = False\n      FLAGS.stride = None\n      FLAGS.scatter_method = 0\n      gezi.restore_configs(model_dir)\n#       if FLAGS.stride is None:\n#         FLAGS.max_len = 2048\n#       FLAGS.fake_infer = True\n      ic(FLAGS.merge_tokens)\n      FLAGS.pymp = False\n      FLAGS.eval_len = True\n      ic(FLAGS.model_dir)\n#       assert 'online' in FLAGS.model_dir\n      model_dir_ = FLAGS.model_dir\n      FLAGS.model_dir = model_dir\n      backbone = rename(FLAGS.backbone.split('/')[-1]).replace('_', '-')\n      FLAGS.backbone = '../input/' + backbone\n      model = get_model()\n      model.eval()\n      gezi.load_weights(model, model_dir)\n      d = pd.read_csv(f'{model_dir}/metrics.csv')\n      display(d[['f1/Overall']])\n      assert d['f1/Overall'].values[-1] > 0.6\n      ic(FLAGS.adjacent_rule)\n      double_times = 0\n      inputs = get_inputs(FLAGS.backbone, sort=True, mode=mode, double_times=double_times, test_ids=test_ids)\n      ic(len(inputs['id']))\n      ic(gezi.get_mem_gb())\n      batch_size = get_batch_size(backbone, model_dir_)\n      ic(FLAGS.backbone, FLAGS.max_len, FLAGS.lower, FLAGS.multi_inputs, FLAGS.multi_inputs_srcs, \n         FLAGS.merge_tokens, FLAGS.seq_encoder, FLAGS.use_relative_positions, FLAGS.stride,\n         FLAGS.mask_inside, FLAGS.label_inside, FLAGS.word_combiner, FLAGS.scatter_method,\n         FLAGS.custom_tokenize, FLAGS.ori_deberta_v2_tokenizer, FLAGS.split_punct, \n         FLAGS.block_size, FLAGS.n_blocks, batch_size)\n     \n#       p = gezi.predict(model, inputs, batch_size, dynamic_keys=['input_ids', 'word_ids'], mask_key='attention_mask')\n      p = gezi.predict(model, inputs, batch_size)\n      p.update({\n        'id': inputs['id'],\n        'word_ids': inputs['word_ids'],\n        'num_words': inputs['num_words']\n      })\n      convert_res(p)\n      df_gt = pd.read_csv('../input/feedback-prize-2021/train.csv')\n      df_gt = df_gt[df_gt.id.isin(test_ids)]\n      df_gt = df_gt.sort_values('id')\n      df_gt['num_words'] = df_gt.id.apply(lambda id: len(open(f'../input/feedback-prize-2021/train/{id}.txt').read().split()))\n#       res = get_metrics(df_gt, p)\n#       ic(res)\n      #ensembler.add(p, weights[i])\n      ensembler.add(p, weights=[weights[i], weights2[i], weights3[i]])\n#       df = get_preds(p)\n#       display(df)\n     \n      if FLAGS.torch:\n        torch.cuda.empty_cache()\n      else:\n        # only the last tf model should cuda close\n        if i + 1 == num_tf_models:\n          cuda.select_device(0)\n          cuda.close()\n      del model\n      del inputs\n      gc.collect()\n\n    ic(gezi.get_mem_gb())\n    p = ensembler.finalize()\n    df_gt = pd.read_csv('../input/feedback-prize-2021/train.csv')\n    df_gt = df_gt[df_gt.id.isin(test_ids)]\n    df_gt = df_gt.sort_values('id')\n    df_gt['num_words'] = df_gt.id.apply(lambda id: len(open(f'../input/feedback-prize-2021/train/{id}.txt').read().split()))\n    df = get_preds(p)\n    display(df)\n    res = get_metrics(df_gt, p)\n    ic(res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ic(P)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mode = 'test'\n# mode = 'train'\nensembler = Ensembler(need_sort=True)\nfor i, model_dir in tqdm(enumerate(model_dirs), total=len(model_dirs)):\n  ic(model_dir)\n  gezi.init_flags()\n  FLAGS.multi_inputs = False\n  FLAGS.seq_encoder = False\n  FLAGS.merge_tokens = False\n  FLAGS.split_punct = False\n  FLAGS.custom_tokenize = False\n  FLAGS.ori_deberta_v2_tokenizer = True\n  FLAGS.max_len_valid = None\n  FLAGS.lower = False\n  FLAGS.stride = None\n  FLAGS.scatter_method = 0\n  gezi.restore_configs(model_dir)\n  if mode == 'train':\n    FLAGS.fake_infer = True\n  FLAGS.pymp = False\n  ic(FLAGS.model_dir)\n#   assert 'online' in FLAGS.model_dir\n  model_dir_ = FLAGS.model_dir\n  FLAGS.model_dir = model_dir\n  backbone = rename(FLAGS.backbone.split('/')[-1]).replace('_', '-')\n  FLAGS.backbone = '../input/' + backbone\n  model = get_model()\n#   model = Model()\n  gezi.load_weights(model, model_dir)\n  d = pd.read_csv(f'{model_dir}/metrics.csv')\n  display(d[['f1/Overall']])\n  assert d['f1/Overall'].values[-1] > 0.6\n  ic(FLAGS.adjacent_rule)\n  # double_times change to 6(so 320 testids) to test if gpu mem is ok for new model\n  double_times = 0\n#   double_times = 6\n\n  inputs = get_inputs(FLAGS.backbone, sort=True, mode=mode, double_times=double_times)\n  batch_size = get_batch_size(backbone, model_dir_)\n  ic(gezi.get_mem_gb())\n  ic(FLAGS.backbone, FLAGS.max_len, FLAGS.lower, FLAGS.multi_inputs, FLAGS.multi_inputs_srcs, \n     FLAGS.merge_tokens, FLAGS.seq_encoder, FLAGS.use_relative_positions, FLAGS.stride,\n     FLAGS.mask_inside, FLAGS.label_inside, FLAGS.word_combiner, FLAGS.scatter_method,\n     FLAGS.custom_tokenize, FLAGS.ori_deberta_v2_tokenizer, FLAGS.split_punct, \n     FLAGS.num_words_emb, FLAGS.mark_end, batch_size)\n\n  p = gezi.predict(model, inputs, batch_size, dynamic_keys=['input_ids', 'word_ids'], mask_key='attention_mask')\n  p.update({\n    'id': inputs['id'],\n    'word_ids': inputs['word_ids'],\n    'num_words': inputs['num_words']\n  })\n  convert_res(p)\n  ensembler.add(p, weights[i])\n#   ensembler.add(p, weights=[weights[i], weights2[i], weights3[i]])\n  if len(inputs['id']) < 1000:\n    df = get_preds(p)\n    display(df)\n    \n  if FLAGS.torch:\n    torch.cuda.empty_cache()\n  else:\n    # only the last tf model should cuda close\n    if i + 1 == num_tf_models:\n      cuda.select_device(0)\n      cuda.close()\n  del model\n  del inputs\n  gc.collect()\n\nic(gezi.get_mem_gb())\np = ensembler.finalize()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = get_preds(p)\ndisplay(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}