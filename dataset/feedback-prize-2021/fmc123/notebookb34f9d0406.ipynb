{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)、\nfrom tqdm import tqdm#进度条显示\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom transformers import AutoTokenizer, AutoConfig, AutoModel\nimport torch\nimport torch.nn as nn\ntorch.cuda.manual_seed_all(2022)#GPU上固定随机初始化","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-10T16:34:59.973686Z","iopub.execute_input":"2022-02-10T16:34:59.974409Z","iopub.status.idle":"2022-02-10T16:35:08.91765Z","shell.execute_reply.started":"2022-02-10T16:34:59.97427Z","shell.execute_reply":"2022-02-10T16:35:08.916486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_NAME = 'allenai/longformer-base-4096'\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint('device=',device)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nconfig = AutoConfig.from_pretrained(MODEL_NAME)\nbert_model = AutoModel.from_pretrained(MODEL_NAME, config=config)\n#tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=True)#do_lower_case=False表示不识别非小写的单词，为True表示大小写的单词都识别\n#config = BertConfig.from_pretrained(MODEL_NAME)#下载配置参数\n#bert_model = BertModel.from_pretrained(MODEL_NAME, config=config)#下载模型参数","metadata":{"execution":{"iopub.status.busy":"2022-02-10T16:36:34.865031Z","iopub.execute_input":"2022-02-10T16:36:34.866185Z","iopub.status.idle":"2022-02-10T16:36:59.315289Z","shell.execute_reply.started":"2022-02-10T16:36:34.866124Z","shell.execute_reply":"2022-02-10T16:36:59.314531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.mkdir('model')\ntokenizer.save_pretrained('model')\nconfig.save_pretrained('model')\nbert_model.save_pretrained('model')","metadata":{"execution":{"iopub.status.busy":"2022-02-10T16:36:59.31721Z","iopub.execute_input":"2022-02-10T16:36:59.318137Z","iopub.status.idle":"2022-02-10T16:37:00.483427Z","shell.execute_reply.started":"2022-02-10T16:36:59.31808Z","shell.execute_reply":"2022-02-10T16:37:00.482412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = \"Sometimes on the news there is either an accident or a suicide. It might involve someone not looking where they're going or tweet that someone sent. It either injury or death. If a mysterious number says I'm going to kill you and they know where you live but you don't know the person's contact,It makes you puzzled and make you start to freak out. Which can end up really badly. \"\nlen(s.split())\n","metadata":{"execution":{"iopub.status.busy":"2022-02-10T10:03:24.414595Z","iopub.status.idle":"2022-02-10T10:03:24.415582Z","shell.execute_reply.started":"2022-02-10T10:03:24.41525Z","shell.execute_reply":"2022-02-10T10:03:24.415286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = 'I love you, my dear friends, all the time!'\ntokens = tokenizer.encode_plus(text, max_length=30, padding='max_length', truncation=True, return_offsets_mapping=True)\ntokens","metadata":{"execution":{"iopub.status.busy":"2022-02-10T10:03:24.417297Z","iopub.status.idle":"2022-02-10T10:03:24.417937Z","shell.execute_reply.started":"2022-02-10T10:03:24.417626Z","shell.execute_reply":"2022-02-10T10:03:24.417659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data_label(file_path, MAX_LEN, df, label_mapping):\n    input_ids = []\n    attention_masks = []\n    labels = []\n    for file_name in tqdm(os.listdir(file_path)):\n        file_id = file_name.split('.')[0]\n        text = open(file_path + '/' + file_name, 'r').read()\n        text = text.replace('\\n',' ')#把换行符替换成空格\n        text2 = ''\n        for char in text:#把连续的空格处理成单个空格\n            if  char == ' ' and text2[-1] == ' ':\n                continue\n            else:\n                text2 += char\n        text = text2 \n        tokens = tokenizer.encode_plus(text, max_length=MAX_LEN, padding='max_length', truncation=True, return_offsets_mapping=True)\n        input_ids.append(tokens['input_ids'])\n        attention_masks.append(tokens['attention_mask'])\n        offset_mapping = tokens['offset_mapping']\n        #把bert分词后的结果映射到原始的每个词的位置上，比如what's为一个词，bert分词为what is就变成两个了\n        pos_mapping = {}\n        pos_mapping[0] = -1\n        pos = 0\n        pos_mapping[1] = pos\n        for i in range(2,len(offset_mapping)):\n            if offset_mapping[i][0] == offset_mapping[i-1][1]:\n                pos_mapping[i] = pos\n            else:\n                pos += 1\n                pos_mapping[i] = pos\n        pos_label_mapping = {}#每个词属于的标签类型\n        discourse_type = df[df['id'] == file_id]['discourse_type'].values\n        predictionstring = df[df['id'] == file_id]['predictionstring'].values\n        for _discourse_type, _predictionstring in zip(discourse_type, predictionstring):\n            pre = _predictionstring.split()\n            for _pre in pre:\n                pos_label_mapping[int(_pre)] = label_mapping[_discourse_type]\n        single_label = []\n        input_id = tokens['input_ids']\n        for i in range(len(input_id)):\n            if input_id[i] in [0, 1, 2] or pos_mapping[i] not in pos_label_mapping.keys():#这些都是非文本的内容\n                single_label.append(7)\n            else:\n                single_label.append(pos_label_mapping[pos_mapping[i]])\n        labels.append(single_label)\n    return input_ids, attention_masks, labels","metadata":{"execution":{"iopub.status.busy":"2022-02-10T10:05:40.33044Z","iopub.execute_input":"2022-02-10T10:05:40.330995Z","iopub.status.idle":"2022-02-10T10:05:40.379561Z","shell.execute_reply.started":"2022-02-10T10:05:40.330956Z","shell.execute_reply":"2022-02-10T10:05:40.374723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = '../input/feedback-prize-2021/train'\nMAX_LEN = 1024#定义文本最大长度\ndf = pd.read_csv('../input/feedback-prize-2021/train.csv')\nlabel_mapping = {'Concluding Statement': 0, 'Claim': 1, 'Evidence': 2, 'Counterclaim': 3, 'Rebuttal': 4, 'Position': 5, 'Lead': 6}\ninput_ids, attention_masks, labels = load_data_label(file_path, MAX_LEN, df, label_mapping)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T10:05:44.893276Z","iopub.execute_input":"2022-02-10T10:05:44.893722Z","iopub.status.idle":"2022-02-10T10:18:27.199987Z","shell.execute_reply.started":"2022-02-10T10:05:44.893686Z","shell.execute_reply":"2022-02-10T10:18:27.199274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#划分训练集、验证集\n#random_state为随机数因子，保证每次随机处理的结果都是一致的\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2022, test_size=0.2)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=2022, test_size=0.2)\n#print(\"训练集的一个inputs\",train_inputs[0])\n#print(\"训练集的一个mask\",train_masks[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-10T10:18:40.658885Z","iopub.execute_input":"2022-02-10T10:18:40.659337Z","iopub.status.idle":"2022-02-10T10:18:40.684415Z","shell.execute_reply.started":"2022-02-10T10:18:40.659301Z","shell.execute_reply":"2022-02-10T10:18:40.683755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#将训练集、验证集转化成tensor\ntrain_inputs = torch.tensor(train_inputs,dtype=torch.int32).to(device)\nvalidation_inputs = torch.tensor(validation_inputs,dtype=torch.int32).to(device)\ntrain_labels = torch.tensor(train_labels,dtype=torch.long).to(device)\nvalidation_labels = torch.tensor(validation_labels,dtype=torch.long).to(device)\ntrain_masks = torch.tensor(train_masks,dtype=torch.float32).to(device)\nvalidation_masks = torch.tensor(validation_masks,dtype=torch.float32).to(device)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-10T10:18:43.105552Z","iopub.execute_input":"2022-02-10T10:18:43.105837Z","iopub.status.idle":"2022-02-10T10:18:49.175097Z","shell.execute_reply.started":"2022-02-10T10:18:43.105804Z","shell.execute_reply":"2022-02-10T10:18:49.174181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, bert_model, num_label):\n        super().__init__()\n        self.bert_model = bert_model\n        self.line1 = nn.Linear(768, num_label)\n    def forward(self, input_ids, attention_mask):\n        out = self.bert_model(input_ids, attention_mask)\n        out = self.line1(out[0])\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-02-10T10:19:35.235743Z","iopub.execute_input":"2022-02-10T10:19:35.236056Z","iopub.status.idle":"2022-02-10T10:19:35.24285Z","shell.execute_reply.started":"2022-02-10T10:19:35.236004Z","shell.execute_reply":"2022-02-10T10:19:35.241979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(bert_model, 8)\nmodel.to(device)\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\nopt = torch.optim.AdamW(optimizer_grouped_parameters, \n                        lr=2e-5)\n#opt = torch.optim.Adam(model.parameters(), lr=2e-5)\nloss = nn.CrossEntropyLoss()#多分类的交叉熵损失函数\nbatch_size = 4\nepochs = 5","metadata":{"execution":{"iopub.status.busy":"2022-02-10T10:19:37.71233Z","iopub.execute_input":"2022-02-10T10:19:37.712591Z","iopub.status.idle":"2022-02-10T10:19:37.882507Z","shell.execute_reply.started":"2022-02-10T10:19:37.712561Z","shell.execute_reply":"2022-02-10T10:19:37.881785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# for epoch in range(epochs):\n    print('epoch=',epoch)\n    model.train()\n    with tqdm(np.arange(0, len(train_inputs), batch_size), desc='Training...') as tbar:\n        for index in tbar:\n            train_L, train_R = index, index + batch_size\n            logits = model(train_inputs[train_L: train_R], train_masks[train_L: train_R])\n            train_loss = loss(logits.reshape(-1, 8), train_labels[train_L: train_R].reshape(-1))\n            #梯度更新\n            opt.zero_grad()\n            train_loss.backward()\n            opt.step()\n            #------------------计算acc--------------------------\n            logits = logits.detach().cpu().numpy()#把数据从GPU上取下来，并且从tensor转换成numpy格式\n            train_accuracy = accuracy_score(train_labels[train_L: train_R].detach().cpu().numpy().reshape(-1), np.argmax(logits.reshape(-1,8), 1))\n            #print('epoch=',epoch,str(index)+'/'+str(len(train_inputs)),' train_loss=', train_loss.item(),' train_accuracy=',train_accuracy)\n            #---------------打印在进度条上--------------\n            tbar.set_postfix(train_loss=train_loss.item(),train_acc=train_accuracy)\n            tbar.update()  # 默认参数n=1，每update一次，进度+n\n    #------------训练结束一个epoch，看看验证集上的效果---------------\n    print('开始验证.....')\n    with torch.no_grad():\n        val_loss_all = 0\n        val_accuracy_all = 0\n        ans = 0\n        for index in tqdm(np.arange(0, len(validation_inputs), batch_size)):\n            val_L, val_R = index, index + batch_size\n            logits = model(validation_inputs[val_L: val_R], validation_masks[val_L: val_R])\n            val_loss = loss(logits.reshape(-1, 8), validation_labels[val_L: val_R].reshape(-1))\n            val_loss_all += val_loss.item()\n            #------------------计算acc--------------------------\n            logits = logits.detach().cpu().numpy()\n            val_accuracy = accuracy_score(validation_labels[val_L: val_R].detach().cpu().numpy().reshape(-1), np.argmax(logits.reshape(-1,8), 1))\n            val_accuracy_all += val_accuracy\n            ans += 1\n    print('val_loss_all=',val_loss_all/ans,' val_accuracy_all=',val_accuracy_all/ans)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-10T10:19:39.923906Z","iopub.execute_input":"2022-02-10T10:19:39.924454Z"}}}]}