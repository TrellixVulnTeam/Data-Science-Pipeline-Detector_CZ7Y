{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\nimport torch\nimport pandas as pd\nimport numpy as np\n\ndigit = {'Concluding Statement': 0, 'Claim': 1, 'Evidence': 2, 'Counterclaim': 3, 'Rebuttal': 4, 'Position': 5, 'Lead': 6}\nPRETRAINED_MODEL_NAME = \"distilbert-base-uncased\"\n\nNUM_LABELS = 7\n\nprint(torch.cuda.is_available())\ndevice=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-08T12:51:14.305228Z","iopub.execute_input":"2022-02-08T12:51:14.306118Z","iopub.status.idle":"2022-02-08T12:51:21.947133Z","shell.execute_reply.started":"2022-02-08T12:51:14.306Z","shell.execute_reply":"2022-02-08T12:51:21.946393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef train_test_val_split(x,y,train_ratio = 0.8,validation_ratio = 0.1,test_ratio = 0.1,random_state = 10):\n    # random_state for reproduction\n    # shuffle must be 'True'\n    [x_train, x_test, y_train, y_test] = train_test_split(\nx, y, test_size=validation_ratio+test_ratio, random_state=random_state, shuffle=True)\n    [x_val, x_test, y_val, y_test] = train_test_split(\n    x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio), random_state=random_state)\n    return x_train, y_train, x_test, y_test, x_val, y_val\n\n\n# 读入训练集和测试集文本\ndf = pd.read_csv('../input/feedback-prize-2021/train.csv')\ntexts = df['discourse_text']\nlabels = df['discourse_type']\n\n# 划分训练集、验证集、测试集\nprint(\"===划分训练集为训练集和验证集===\")\ntrain_texts, train_labels, test_texts, test_labels, val_texts, val_labels = train_test_val_split(texts, labels)\nprint(\"读取训练集文本总数 = \", len(train_texts))\nprint(\"读取测试集文本总数 = \", len(test_texts))\nprint(\"标记类别种类 = \", set(train_labels))\n\n\n# 实例话分词器并且编码文本（文本索引化）\ntokenizer = DistilBertTokenizerFast.from_pretrained(PRETRAINED_MODEL_NAME)\ntrain_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\nval_encodings = tokenizer(list(val_texts), truncation=True, padding=True)\ntest_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\nprint(\"默认固定文本中包含词语数为%d个\" % len(train_encodings[0].tokens))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T12:51:21.948942Z","iopub.execute_input":"2022-02-08T12:51:21.949195Z","iopub.status.idle":"2022-02-08T12:52:06.108817Z","shell.execute_reply.started":"2022-02-08T12:51:21.949158Z","shell.execute_reply":"2022-02-08T12:52:06.108062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 数据集类\nclass FeedbackDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(digit[self.labels[idx]])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# 实例化数据集实例\ntrain_dataset = FeedbackDataset(train_encodings, list(train_labels))\nval_dataset = FeedbackDataset(val_encodings, list(val_labels))\ntest_dataset = FeedbackDataset(test_encodings, list(test_labels))\nprint(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T12:52:06.110426Z","iopub.execute_input":"2022-02-08T12:52:06.110702Z","iopub.status.idle":"2022-02-08T12:52:06.13349Z","shell.execute_reply.started":"2022-02-08T12:52:06.110664Z","shell.execute_reply":"2022-02-08T12:52:06.132721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\n# mini-batch\nBATCH_SIZE = 7\n","metadata":{"execution":{"iopub.status.busy":"2022-02-08T12:52:06.134912Z","iopub.execute_input":"2022-02-08T12:52:06.135387Z","iopub.status.idle":"2022-02-08T12:52:06.148388Z","shell.execute_reply.started":"2022-02-08T12:52:06.135349Z","shell.execute_reply":"2022-02-08T12:52:06.147685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 微调训练\ntraining_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=200,              # total number of training epochs\n    per_device_train_batch_size=16,  # batch size per device during training\n    per_device_eval_batch_size=64,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n    logging_steps=10,\n    evaluation_strategy=\"epoch\"\n)\n\nmodel = DistilBertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n\nmodel=model.to(device)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated   Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=val_dataset             # evaluation dataset\n)\n\ntrainer.train()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-08T12:54:56.606959Z","iopub.execute_input":"2022-02-08T12:54:56.607585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import time\n# start = time.time()\n\n# train_acc = []\n\n# EPOCHS = 20\n# for epoch in range(EPOCHS):\n    \n#     running_loss = 0.0\n#     for data in trainloader:\n        \n#         tokens_tensors, segments_tensors, \\\n#         masks_tensors, labels = [t.to(device) for t in data]\n\n#         optimizer.zero_grad()\n#         # forward pass\n#         outputs = model(input_ids=tokens_tensors, \n#                         token_type_ids=segments_tensors, \n\n#                         labels=labels)\n#         loss = outputs[0]\n#         # backward\n#         loss.backward()\n#         optimizer.step()\n        \n#         # 紀錄當前 batch loss\n#         running_loss += loss.item()\n        \n#     # 計算分類準確率\n#     _, acc = get_predictions(model, trainloader, compute_acc=True)\n#     train_acc.append(acc)\n\n#     print(f\"batch size:{BATCH_SIZE}\")\n#     print(f'[epoch {epoch+1}] loss: {running_loss:3f}, acc: {acc:3f}')\n\n# end = time.time()\n# print(f\"time:{end-start:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-08T12:53:56.844716Z","iopub.status.idle":"2022-02-08T12:53:56.845464Z","shell.execute_reply.started":"2022-02-08T12:53:56.845213Z","shell.execute_reply":"2022-02-08T12:53:56.845243Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}