{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"自分の学習用に記録を作成。\n\n今回、初めて自然言語処理に挑戦するため、データセットのEDAおよびrobertの実装を行う。\n\nERIK BRUINさんのNLP on Student Writing: EDAを参考にEDAを行なっていく。\nhttps://www.kaggle.com/erikbruin/nlp-on-student-writing-eda","metadata":{}},{"cell_type":"code","source":"#モジュールのインポート\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\nimport os","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:24:29.641445Z","iopub.execute_input":"2022-02-06T12:24:29.641808Z","iopub.status.idle":"2022-02-06T12:24:29.668286Z","shell.execute_reply.started":"2022-02-06T12:24:29.641726Z","shell.execute_reply":"2022-02-06T12:24:29.667464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#データの読み込み\ntrain = pd.read_csv('../input/feedback-prize-2021/train.csv')\ntrain_txt = glob('../input/feedback-prize-2021/train/*.txt') \ntest_txt = glob('../input/feedback-prize-2021/test/*.txt')\nsample_submission = pd.read_csv('../input/feedback-prize-2021/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:24:31.178931Z","iopub.execute_input":"2022-02-06T12:24:31.179198Z","iopub.status.idle":"2022-02-06T12:24:33.288795Z","shell.execute_reply.started":"2022-02-06T12:24:31.179168Z","shell.execute_reply":"2022-02-06T12:24:33.288042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train.csvデータ144293行8列\nトレーニングセット内のエッセイの注釈付きバージョンを含む.csvファイル\nそれぞれのデータの意味は下記の通りになる\n\nid:エッセイ応答番号\n\ndiscourse_id:談話要素のIDコード\n\ndiscourse_start:エッセイの応答で談話要素が始まる文字の位置\n\ndiscourse_end:談話要素がエッセイ応答で終了する文字の位置\n\ndiscourse_text:談話要素のテキスト\n\ndiscourse_type:談話要素の分類\n\ndiscourse_type_num:談話要素の列挙されたクラスラベル\n\npredictionstring:予測に必要なトレーニングサンプルの単語インデックス","metadata":{}},{"cell_type":"code","source":"#traindata\nprint(train.shape)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:24:33.290293Z","iopub.execute_input":"2022-02-06T12:24:33.290558Z","iopub.status.idle":"2022-02-06T12:24:33.31446Z","shell.execute_reply.started":"2022-02-06T12:24:33.290516Z","shell.execute_reply":"2022-02-06T12:24:33.313692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"discourse_type_numは談話要素の列挙されたクラスラベルは４２種類\n\ndiscourse_typeは７種類ある","metadata":{"execution":{"iopub.status.busy":"2022-01-31T21:01:11.119015Z","iopub.execute_input":"2022-01-31T21:01:11.119389Z","iopub.status.idle":"2022-01-31T21:01:11.195833Z","shell.execute_reply.started":"2022-01-31T21:01:11.11935Z","shell.execute_reply":"2022-01-31T21:01:11.194249Z"}}},{"cell_type":"code","source":"#predivtionstringのユニーク数\nprint(train[\"predictionstring\"].nunique())\nprint(\"------------------------------------\")\nprint(train[\"discourse_type_num\"].nunique())\nprint(train[\"discourse_type_num\"].unique())\nprint(\"------------------------------------\")\nprint(train[\"discourse_type\"].unique())","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:24:33.315722Z","iopub.execute_input":"2022-02-06T12:24:33.31602Z","iopub.status.idle":"2022-02-06T12:24:33.427964Z","shell.execute_reply.started":"2022-02-06T12:24:33.315986Z","shell.execute_reply":"2022-02-06T12:24:33.426983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"print(train[\"discourse_text\"][0])\nprint(train[\"discourse_type_num\"][0])\nprint(train[\"predictionstring\"][0])\nprint(\"--------------------------------\")\nprint(train[\"discourse_text\"][1])\nprint(train[\"discourse_type_num\"][1])\nprint(train[\"predictionstring\"][1])","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:24:33.429813Z","iopub.execute_input":"2022-02-06T12:24:33.430417Z","iopub.status.idle":"2022-02-06T12:24:33.438805Z","shell.execute_reply.started":"2022-02-06T12:24:33.430377Z","shell.execute_reply":"2022-02-06T12:24:33.437919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#試しにID0000D23A521Aのデータを抽出\ntrain.query('id ==\"0000D23A521A\"')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:24:33.439967Z","iopub.execute_input":"2022-02-06T12:24:33.440448Z","iopub.status.idle":"2022-02-06T12:24:33.471016Z","shell.execute_reply.started":"2022-02-06T12:24:33.44041Z","shell.execute_reply":"2022-02-06T12:24:33.470201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat ../input/feedback-prize-2021/train/0000D23A521A.txt","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:24:33.472361Z","iopub.execute_input":"2022-02-06T12:24:33.4727Z","iopub.status.idle":"2022-02-06T12:24:34.188042Z","shell.execute_reply.started":"2022-02-06T12:24:33.472664Z","shell.execute_reply":"2022-02-06T12:24:34.187188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"predictionstringの数とdiscourse_textの単語数が正しいか確認","metadata":{}},{"cell_type":"code","source":"#テキストをスペースで分割し数を計算\ntrain[\"discourse_len\"] = train[\"discourse_text\"].apply(lambda x:len(x.split()))\n#predictionstringをスペースで分割し数を計算\ntrain[\"predictionstring_len\"] = train[\"predictionstring\"].apply(lambda x:len(x.split()))\n\ntrain[\"len_predict\"] = 0\n\n#discourse_lenとpredictionstring_lenで差があるか確認\nfor i in range(0,len(train[\"discourse_len\"])):\n    if train[\"discourse_len\"][i] == train[\"predictionstring_len\"][i]:\n        train[\"len_predict\"][i] = 0\n    else:\n        train[\"len_predict\"][i] = 1\n\n#異なっている件数を確認\ntrain[\"len_predict\"].sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:24:34.189901Z","iopub.execute_input":"2022-02-06T12:24:34.19013Z","iopub.status.idle":"2022-02-06T12:25:09.935991Z","shell.execute_reply.started":"2022-02-06T12:24:34.190103Z","shell.execute_reply":"2022-02-06T12:25:09.935332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:25:09.937352Z","iopub.execute_input":"2022-02-06T12:25:09.937608Z","iopub.status.idle":"2022-02-06T12:25:09.952194Z","shell.execute_reply.started":"2022-02-06T12:25:09.937574Z","shell.execute_reply":"2022-02-06T12:25:09.951531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#len_predictが1のものを抽出\ndf = train.query(\"len_predict == 1\")\n\n#id4E1C636ABEADを確認\ndf.query(\"id == '4E1C636ABEAD'\")","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:25:09.953589Z","iopub.execute_input":"2022-02-06T12:25:09.954058Z","iopub.status.idle":"2022-02-06T12:25:09.986275Z","shell.execute_reply.started":"2022-02-06T12:25:09.954011Z","shell.execute_reply":"2022-02-06T12:25:09.985507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat ../input/feedback-prize-2021/train/4E1C636ABEAD.txt","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:25:09.987758Z","iopub.execute_input":"2022-02-06T12:25:09.988008Z","iopub.status.idle":"2022-02-06T12:25:10.665282Z","shell.execute_reply.started":"2022-02-06T12:25:09.987974Z","shell.execute_reply":"2022-02-06T12:25:10.664142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"単語の数を数えてみると正しいのはdiscourse_lenであり、predictionstringの中には合計468件の誤りがある\n\ntype別の誤り発生率は誤差0.005であり優位な差はないと考える","metadata":{}},{"cell_type":"code","source":"#len_predictはtype毎に差があるか確認\ndf1 = df.groupby(\"discourse_type\").agg({\"len_predict\":\"sum\"}).reset_index()\n#trainデータで総数をカウント\ndf2 = train.groupby(\"discourse_type\").agg({\"id\":\"count\"}).reset_index()\n#df1とdf2を結合\ndf3 = pd.merge(df1,df2,on=\"discourse_type\",how=\"left\")\n#typeごとの割合を計算\ndf3[\"len_predict/id\"] = df3[\"len_predict\"] / df3[\"id\"]\nprint(df3)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:25:10.669344Z","iopub.execute_input":"2022-02-06T12:25:10.670197Z","iopub.status.idle":"2022-02-06T12:25:10.727463Z","shell.execute_reply.started":"2022-02-06T12:25:10.670149Z","shell.execute_reply":"2022-02-06T12:25:10.726554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"discourse_lenの数はtypeによって優位な差があるのか確認\n\nEvidence、Concluding Statement、Leadはその他のtypeと比べて単語数が多い","metadata":{}},{"cell_type":"code","source":"#discourse_lenの平均をtype毎に計算\ndf4 = train.groupby(\"discourse_type\").agg({\"discourse_len\":\"mean\"}).reset_index()\nprint(df4)\nplt.bar(df4[\"discourse_type\"],df4[\"discourse_len\"])","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:25:10.73094Z","iopub.execute_input":"2022-02-06T12:25:10.731175Z","iopub.status.idle":"2022-02-06T12:25:11.119954Z","shell.execute_reply.started":"2022-02-06T12:25:10.731144Z","shell.execute_reply":"2022-02-06T12:25:11.11278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#欠損値の確認\ntrain.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:25:11.121288Z","iopub.execute_input":"2022-02-06T12:25:11.12161Z","iopub.status.idle":"2022-02-06T12:25:11.265668Z","shell.execute_reply.started":"2022-02-06T12:25:11.121545Z","shell.execute_reply":"2022-02-06T12:25:11.264844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#discourse_end,discourse_startの平均をtypeで計算\ndf5 = train.groupby(\"discourse_type\")[['discourse_end', 'discourse_start']].mean().reset_index().sort_values(by = 'discourse_start', ascending = False)\ndf5.plot(x='discourse_type',\n        kind='barh',\n        stacked=False,\n        title='Average start and end position absolute',\n        figsize=(12,4))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:25:11.267418Z","iopub.execute_input":"2022-02-06T12:25:11.267848Z","iopub.status.idle":"2022-02-06T12:25:11.827142Z","shell.execute_reply.started":"2022-02-06T12:25:11.26781Z","shell.execute_reply":"2022-02-06T12:25:11.8264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n#testデータの読み込み\ntest_names, test_texts = [], []\nfor f in tqdm(list(os.listdir('../input/feedback-prize-2021/test'))):\n    test_names.append(f.replace('.txt', ''))\n    test_texts.append(open('../input/feedback-prize-2021/test/' + f, 'r').read())\ntest_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\n# test_texts['text'] = test_texts['text'].apply(lambda x:x.split())\ntest_texts.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:25:11.828998Z","iopub.execute_input":"2022-02-06T12:25:11.829793Z","iopub.status.idle":"2022-02-06T12:25:11.913263Z","shell.execute_reply.started":"2022-02-06T12:25:11.829717Z","shell.execute_reply":"2022-02-06T12:25:11.91239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trainデータのh読み込み\ntest_names, train_texts = [], []\nfor f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n    test_names.append(f.replace('.txt', ''))\n    train_texts.append(open('../input/feedback-prize-2021/train/' + f, 'r').read())\ntrain_text_df = pd.DataFrame({'id': test_names, 'text': train_texts})\n# train_texts['text'] = test_texts['text'].apply(lambda x:x.split())\ntrain_text_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:25:11.914841Z","iopub.execute_input":"2022-02-06T12:25:11.915929Z","iopub.status.idle":"2022-02-06T12:26:02.698188Z","shell.execute_reply.started":"2022-02-06T12:25:11.915888Z","shell.execute_reply":"2022-02-06T12:26:02.697413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train.copy()\nall_entities = []\nfor i in tqdm(train_text_df.iterrows()):\n    total = i[1]['text'].split(' ').__len__()\n    start = -1\n    entities = []\n    for j in train_df[train_df['id'] == i[1]['id']].iterrows():\n        discourse = j[1]['discourse_type']\n        list_ix = j[1]['predictionstring'].split(' ')\n#         print(j[1]['predictionstring'],'###' ,len(list_ix))\n        ent = [f\"I-{discourse}\" for ix in list_ix]\n        ent[0] = f\"B-{discourse}\"\n        ds = int(list_ix[0])\n        de = int(list_ix[-1])\n        if start < ds-1:\n            ent_add = ['O' for ix in range(int(ds-1-start))]\n            ent = ent_add + ent\n#         print(len(entities))\n#         print(ent, len(ent))\n        entities.extend(ent)\n#         print(len(entities))\n        start = de\n    if len(entities) < total:\n        ent_add = [\"O\" for ix in range(total-len(entities))]\n        entities += ent_add\n    else:\n        entities = entities[:total]\n#     print(i[1]['id'],'@@@@@@@@' ,i[1]['text'].split(' ').__len__(), len(entities))\n    all_entities.append(entities)\n#     if len(all_entities) > 100:\n#         break","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:26:02.699691Z","iopub.execute_input":"2022-02-06T12:26:02.699938Z","iopub.status.idle":"2022-02-06T12:31:44.184566Z","shell.execute_reply.started":"2022-02-06T12:26:02.699905Z","shell.execute_reply":"2022-02-06T12:31:44.183812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_text_df['entities'] = all_entities\ntrain_text_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:31:44.185823Z","iopub.execute_input":"2022-02-06T12:31:44.186517Z","iopub.status.idle":"2022-02-06T12:31:44.210658Z","shell.execute_reply.started":"2022-02-06T12:31:44.186464Z","shell.execute_reply":"2022-02-06T12:31:44.209921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**そもそも、BERTとは**\n\n2018年にGoogleから発表された自然言語処理モデルであり、多様な自然言語処理において当時の最高スコアを叩き出していた。\n特徴は「文脈を読むことが可能」であり、文章を文頭と文末の双方向から学習することで文脈を読むことを実現した。\n\nBERTの仕組み\n入力されたシーケンスから別のシーケンスを予測します。\n事前学習モデルであり、入力されたラベルが付与されていない、分散表現をTronsformerが処理することによって学習をします。\n\n・Masked language model\n入力文の15%の単語を確率的に別の単語で置き換え、文脈から置き換える前の単語を予測させます。具体的には、選択された15%のうち、80%は[MASK]に置き換えるマスク変換、10%をランダムな別の単語に変換、残りの10%はそのままの単語にします。置換された単語を周りの文脈から当てるタスクを解くことで、単語に対応する文脈情報を学習する。\n\n・Next sentence prediction\n2つの入力文に対して「その2文が隣り合っているか」を当てるよう学習します。これにより、2つの文の関係性を学習できます。文の片方を50%の確率で他の文に置き換え、それらが隣り合っているか（isNext）隣り合っていない（notNext）か判別することによって学習します。（2文を[SEP]というトークンで分け、isNextかnotNextか分類するために[CLS]というトークンが用意されます。）\n\n**RoBERTについて**\n\nRoBERTのアイディアは\"BERTの持っている力を発揮するために、もっと上手く、たくさん学習すればもっと賢くなる\"、という考えの元生み出されました。\n\n・事前学習の設定\nBERTの実装では最初の時点でマスクをするがRoBERTはdynamic maskingとして学習する度にマスクをするようにしている。\n\n・Nect sentence predicition\nFULL-SENTENCES\n一つ以上のドキュメントから連続した512単語をインプットとします。\n1つのドキュメントが512単語以下の場合は、違うドキュメントの最初からサンプリングします。\nDOC-SENTENCES\nFULL-SENTENCESと似ていますが、ドキュメントをまたぎません。\n通常、512単語より短いので、FULL-SENTENCESと同水準の単語数になるようにバッチサイズをダイナミックに増やします。\n基本的にはDOC-SENTENCESを使用する。\n\n・バッチサイズ\nBERTでは、バッチサイズ256だが、非常に大きいバッチサイズで学習すると精度が向上するという結果がある。\n\n・Text Encoding\nBERTではByte-Pair Encodingという手法を使って単語を作成している。\nBPEは単語に分割し、出現頻度が高いものは単語のままで、出現頻度が低いものは文字単位に置き換えることにより、未知語をなくそうというものです。\n文字単位からバイト単位に変えたが結果は若干悪化したとのこと。\n\n","metadata":{}},{"cell_type":"markdown","source":"**RoBertaを実装。**\n\n自力で実装する知識がなため下記のノードブックを写経。\n\nhttps://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533/notebook","metadata":{}},{"cell_type":"code","source":"#必要なライブラリの読み込み\nimport random\nimport glob\n#進捗状態をプログレスバーで表示\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset,DataLoader\nfrom transformers import RobertaTokenizerFast, RobertaForTokenClassification\n#ファインチューニングと性能評価を効率的に行うためライブラリ\nimport pytorch_lightning as pl\n\nimport pdb\nfrom torch import cuda\nfrom sklearn.metrics import accuracy_score\n\n#Robertaの事前学習モデル\nMODEL_NAME = \"../input/roberta-base\"","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:31:44.212263Z","iopub.execute_input":"2022-02-06T12:31:44.213066Z","iopub.status.idle":"2022-02-06T12:31:53.574084Z","shell.execute_reply.started":"2022-02-06T12:31:44.213028Z","shell.execute_reply":"2022-02-06T12:31:53.573199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nconfig = {'model_name': '/kaggle/input/roberta-base/',\n         'max_length': 512,\n         'train_batch_size':8,\n         'valid_batch_size':16,\n         'epochs':3,\n         'learning_rate':1e-05,\n         'max_grad_norm':10,\n         'device': 'cuda' if cuda.is_available() else 'cpu'}","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:31:53.575783Z","iopub.execute_input":"2022-02-06T12:31:53.576068Z","iopub.status.idle":"2022-02-06T12:31:53.627503Z","shell.execute_reply.started":"2022-02-06T12:31:53.576021Z","shell.execute_reply":"2022-02-06T12:31:53.622805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ラベルリストの作成\noutput_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n\nlabels_to_ids = {v:k for k,v in enumerate(output_labels)}\nids_to_labels = {k:v for k,v in enumerate(output_labels)}","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:31:53.629512Z","iopub.execute_input":"2022-02-06T12:31:53.630234Z","iopub.status.idle":"2022-02-06T12:31:53.641513Z","shell.execute_reply.started":"2022-02-06T12:31:53.63019Z","shell.execute_reply":"2022-02-06T12:31:53.640534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(labels_to_ids)\nprint(ids_to_labels)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:31:53.646598Z","iopub.execute_input":"2022-02-06T12:31:53.647363Z","iopub.status.idle":"2022-02-06T12:31:53.653678Z","shell.execute_reply.started":"2022-02-06T12:31:53.647322Z","shell.execute_reply":"2022-02-06T12:31:53.652868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#トークナイザのロード\ntokenizer = RobertaTokenizerFast.from_pretrained(MODEL_NAME)\n#モデル作成\nmodel = RobertaForTokenClassification.from_pretrained(config['model_name'], num_labels=len(output_labels))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:31:53.655143Z","iopub.execute_input":"2022-02-06T12:31:53.656133Z","iopub.status.idle":"2022-02-06T12:32:01.304745Z","shell.execute_reply.started":"2022-02-06T12:31:53.656066Z","shell.execute_reply":"2022-02-06T12:32:01.304044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class dataset(Dataset):\n  def __init__(self, dataframe, tokenizer, max_len):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n  def __getitem__(self, index):\n        # step 1: get the sentence and word labels \n        sentence = self.data.text[index]\n        word_labels = self.data.entities[index]\n\n        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n        encoding = self.tokenizer(sentence,\n#                              is_pretokenized=True, \n#                                   is_split_into_words=True,\n                             return_offsets_mapping=True, \n                             padding='max_length', \n                             truncation=True, \n                             max_length=self.max_len)\n        \n        # step 3: create token labels only for first word pieces of each tokenized word\n#         pdb.set_trace()\n        labels = [labels_to_ids[label] for label in word_labels] \n        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n        # create an empty array of -100 of length max_length\n        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n#         print(len(sentence), len(labels))\n        # set only labels whose first offset position is 0 and the second is not 0\n        i = 0\n        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n#             print(idx)\n            if mapping[0] != 0 and mapping[0] != encoding['offset_mapping'][idx-1][1]:\n            # overwrite label\n#             pdb.set_trace()\n#             print(mapping)\n#             print(encoded_labels.shape, len(labels), idx, i)\n                try:\n                    encoded_labels[idx] = labels[i]\n                except:\n                    pass\n                i += 1\n            else:\n                if idx==1:\n    #                 print(idx)\n                    encoded_labels[idx] = labels[i]\n                    i += 1\n        # step 4: turn everything into PyTorch tensors\n        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n        item['labels'] = torch.as_tensor(encoded_labels)\n        \n        return item\n\n  def __len__(self):\n        return self.len","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:32:01.306211Z","iopub.execute_input":"2022-02-06T12:32:01.306462Z","iopub.status.idle":"2022-02-06T12:32:01.337946Z","shell.execute_reply.started":"2022-02-06T12:32:01.306428Z","shell.execute_reply":"2022-02-06T12:32:01.33716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train_text_dfを8対2で検証データと分割\n\ndataset classでencodingを実施しdataloaderに入れれる形にする。","metadata":{}},{"cell_type":"code","source":"data = train_text_df[['text', 'entities']]\ntrain_size = 0.8\ntrain_dataset = data.sample(frac=train_size,random_state=200)\ntest_dataset = data.drop(train_dataset.index).reset_index(drop=True)\ntrain_dataset = train_dataset.reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(data.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\ntraining_set = dataset(train_dataset, tokenizer, config['max_length'])\ntesting_set = dataset(test_dataset, tokenizer, config['max_length'])","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:32:01.339564Z","iopub.execute_input":"2022-02-06T12:32:01.339986Z","iopub.status.idle":"2022-02-06T12:32:01.38517Z","shell.execute_reply.started":"2022-02-06T12:32:01.339947Z","shell.execute_reply":"2022-02-06T12:32:01.384476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#datasetの中身を確認\nprint(training_set[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:32:01.386614Z","iopub.execute_input":"2022-02-06T12:32:01.386883Z","iopub.status.idle":"2022-02-06T12:32:01.412947Z","shell.execute_reply.started":"2022-02-06T12:32:01.386848Z","shell.execute_reply":"2022-02-06T12:32:01.412175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"encodingしたdataを使用しdataloaderを作成。※dataloaderはデータセットからミニバッチを取り出すためのもの。","metadata":{}},{"cell_type":"code","source":"train_params = {'batch_size': config['train_batch_size'],\n                'shuffle': True,\n                'num_workers': 1,\n                'pin_memory':True\n                }\n\ntest_params = {'batch_size': config['valid_batch_size'],\n                'shuffle': True,\n                'num_workers': 1,\n                'pin_memory':True\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:32:01.414265Z","iopub.execute_input":"2022-02-06T12:32:01.414704Z","iopub.status.idle":"2022-02-06T12:32:01.420905Z","shell.execute_reply.started":"2022-02-06T12:32:01.414665Z","shell.execute_reply":"2022-02-06T12:32:01.419893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = config['device']","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:32:01.422427Z","iopub.execute_input":"2022-02-06T12:32:01.422825Z","iopub.status.idle":"2022-02-06T12:32:01.432103Z","shell.execute_reply.started":"2022-02-06T12:32:01.42279Z","shell.execute_reply":"2022-02-06T12:32:01.431233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)\ninputs = training_set[2]\n#unsqueeze(dim)：元のテンソルを書き換えずに、次元を増やしたテンソルを返す\ninput_ids = inputs[\"input_ids\"].unsqueeze(0)\nattention_mask = inputs[\"attention_mask\"].unsqueeze(0)\nlabels = inputs[\"labels\"].unsqueeze(0)\n\n#GPU/CPUの切り替え\ninput_ids = input_ids.to(device)\nattention_mask = attention_mask.to(device)\nlabels = labels.to(device)\n\noutputs = model(input_ids, attention_mask=attention_mask, labels=labels,\n               return_dict=False)\ninitial_loss = outputs[0]\ninitial_loss","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:32:01.433686Z","iopub.execute_input":"2022-02-06T12:32:01.433997Z","iopub.status.idle":"2022-02-06T12:32:06.940261Z","shell.execute_reply.started":"2022-02-06T12:32:01.43396Z","shell.execute_reply":"2022-02-06T12:32:06.939432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(params=model.parameters(), lr=config['learning_rate'])","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:32:06.94181Z","iopub.execute_input":"2022-02-06T12:32:06.942084Z","iopub.status.idle":"2022-02-06T12:32:06.949463Z","shell.execute_reply.started":"2022-02-06T12:32:06.942047Z","shell.execute_reply":"2022-02-06T12:32:06.948653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(epoch):\n    tr_loss, tr_accuracy = 0, 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    tr_preds, tr_labels = [], []\n    # put model in training mode\n    model.train()\n    \n    for idx, batch in enumerate(training_loader):\n        \n        ids = batch['input_ids'].to(device, dtype = torch.long)\n        mask = batch['attention_mask'].to(device, dtype = torch.long)\n        labels = batch['labels'].to(device, dtype = torch.long)\n\n        loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels,\n                               return_dict=False)\n        tr_loss += loss.item()\n\n        nb_tr_steps += 1\n        nb_tr_examples += labels.size(0)\n        \n        if idx % 100==0:\n            loss_step = tr_loss/nb_tr_steps\n            print(f\"Training loss per 100 training steps: {loss_step}\")\n           \n        # compute training accuracy\n        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n        # only compute accuracy at active labels\n        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n        \n        labels = torch.masked_select(flattened_targets, active_accuracy)\n        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n        \n        tr_labels.extend(labels)\n        tr_preds.extend(predictions)\n\n        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n        tr_accuracy += tmp_tr_accuracy\n    \n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(\n            parameters=model.parameters(), max_norm=config['max_grad_norm']\n        )\n        \n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    epoch_loss = tr_loss / nb_tr_steps\n    tr_accuracy = tr_accuracy / nb_tr_steps\n    print(f\"Training loss epoch: {epoch_loss}\")\n    print(f\"Training accuracy epoch: {tr_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:32:06.950686Z","iopub.execute_input":"2022-02-06T12:32:06.951129Z","iopub.status.idle":"2022-02-06T12:32:06.965225Z","shell.execute_reply.started":"2022-02-06T12:32:06.951085Z","shell.execute_reply":"2022-02-06T12:32:06.964407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(config['epochs']):\n    print(f\"Training epoch: {epoch + 1}\")\n    train(epoch)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:32:06.96746Z","iopub.execute_input":"2022-02-06T12:32:06.968617Z","iopub.status.idle":"2022-02-06T13:09:34.813081Z","shell.execute_reply.started":"2022-02-06T12:32:06.96858Z","shell.execute_reply":"2022-02-06T13:09:34.812137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid(model, testing_loader):\n    # put model in evaluation mode\n    model.eval()\n    \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_examples, nb_eval_steps = 0, 0\n    eval_preds, eval_labels = [], []\n    \n    with torch.no_grad():\n        for idx, batch in enumerate(testing_loader):\n            \n            ids = batch['input_ids'].to(device, dtype = torch.long)\n            mask = batch['attention_mask'].to(device, dtype = torch.long)\n            labels = batch['labels'].to(device, dtype = torch.long)\n            \n            loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=labels,\n                                     return_dict=False)\n            \n            eval_loss += loss.item()\n\n            nb_eval_steps += 1\n            nb_eval_examples += labels.size(0)\n            nb_eval_steps += 1\n            nb_eval_examples += labels.size(0)\n        \n            if idx % 100==0:\n                loss_step = eval_loss/nb_eval_steps\n                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n              \n            # compute evaluation accuracy\n            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n            \n            # only compute accuracy at active labels\n            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n        \n            labels = torch.masked_select(flattened_targets, active_accuracy)\n            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n            \n            eval_labels.extend(labels)\n            eval_preds.extend(predictions)\n            \n            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n            eval_accuracy += tmp_eval_accuracy\n\n    labels = [ids_to_labels[id.item()] for id in eval_labels]\n    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n    \n    eval_loss = eval_loss / nb_eval_steps\n    eval_accuracy = eval_accuracy / nb_eval_steps\n    print(f\"Validation Loss: {eval_loss}\")\n    print(f\"Validation Accuracy: {eval_accuracy}\")\n\n    return labels, predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-06T13:09:34.815212Z","iopub.execute_input":"2022-02-06T13:09:34.815731Z","iopub.status.idle":"2022-02-06T13:09:34.830753Z","shell.execute_reply.started":"2022-02-06T13:09:34.815691Z","shell.execute_reply":"2022-02-06T13:09:34.830051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels, predictions = valid(model, testing_loader)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T13:09:34.832069Z","iopub.execute_input":"2022-02-06T13:09:34.832389Z","iopub.status.idle":"2022-02-06T13:11:05.0402Z","shell.execute_reply.started":"2022-02-06T13:09:34.832354Z","shell.execute_reply":"2022-02-06T13:11:05.039392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence = \"@HuggingFace is a company based in New York, but is also has employees working in Paris\"\nmodel.eval()\ndef inference(sentence):\n    inputs = tokenizer(sentence,\n#                         is_split_into_words=True, \n                        return_offsets_mapping=True, \n                        padding='max_length', \n                        truncation=True, \n                        max_length=config['max_length'],\n                        return_tensors=\"pt\")\n\n    # move to gpu\n    ids = inputs[\"input_ids\"].to(device)\n    mask = inputs[\"attention_mask\"].to(device)\n    # forward pass\n    outputs = model(ids, attention_mask=mask, return_dict=False)\n    logits = outputs[0]\n\n    active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n    flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n\n    tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n    token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n    wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n\n    prediction = []\n    out_str = []\n    off_list = inputs[\"offset_mapping\"].squeeze().tolist()\n    for idx, mapping in enumerate(off_list):\n#         print(mapping, token_pred[1], token_pred[0],\"####\")\n\n#         only predictions on first word pieces are important\n        if mapping[0] != 0 and mapping[0] != off_list[idx-1][1]:\n#             print(mapping, token_pred[1], token_pred[0])\n            prediction.append(wp_preds[idx][1])\n            out_str.append(wp_preds[idx][0])\n        else:\n            if idx == 1:\n                prediction.append(wp_preds[idx][1])\n                out_str.append(wp_preds[idx][0])\n            continue\n    return prediction, out_str","metadata":{"execution":{"iopub.status.busy":"2022-02-06T13:11:05.043305Z","iopub.execute_input":"2022-02-06T13:11:05.043523Z","iopub.status.idle":"2022-02-06T13:11:05.068561Z","shell.execute_reply.started":"2022-02-06T13:11:05.043495Z","shell.execute_reply":"2022-02-06T13:11:05.067401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_texts = train_text_df['text'].tolist()[:10]\ny_pred = []\n\nfor i, t in enumerate(test_texts['text'].tolist()):\n    o,o_t = inference(t)\n    y_pred.append(o)\n    l = train_text_df['entities'][i]","metadata":{"execution":{"iopub.status.busy":"2022-02-06T13:11:05.069906Z","iopub.execute_input":"2022-02-06T13:11:05.071369Z","iopub.status.idle":"2022-02-06T13:11:05.228695Z","shell.execute_reply.started":"2022-02-06T13:11:05.071323Z","shell.execute_reply":"2022-02-06T13:11:05.22791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = []\nimport pdb\nfor i in tqdm(range(len(test_texts))):\n#     pdb.set_trace()\n    idx = test_texts.id.values[i]\n#     pred = ['']*len(test_texts[i])\n\n#     for j in range(len(y_pred[i])):\n#         if words[i][j] != None:\n#             pred[words[i][j]] = labels[y_pred[i][j]]\n\n    pred = [x.replace('B-','').replace('I-','') for x in y_pred[i]]\n#     print(pred)\n    preds = []\n    j = 0\n    while j < len(pred):\n        cls = pred[j]\n#         pdb.set_trace()\n        if cls == 'O':\n            j += 1\n        end = j + 1\n        while end < len(pred) and pred[end] == cls:\n            end += 1\n            \n        if cls != 'O' and cls != '' and end - j > 10:\n            final_preds.append((idx, cls, ' '.join(map(str, list(range(j, end))))))\n        \n        j = end\n        \nprint(final_preds[1])","metadata":{"execution":{"iopub.status.busy":"2022-02-06T13:11:05.230026Z","iopub.execute_input":"2022-02-06T13:11:05.230272Z","iopub.status.idle":"2022-02-06T13:11:05.247815Z","shell.execute_reply.started":"2022-02-06T13:11:05.230239Z","shell.execute_reply":"2022-02-06T13:11:05.247053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(final_preds))\ntest_df = pd.read_csv('../input/feedback-prize-2021/sample_submission.csv')\ntest_df","metadata":{"execution":{"iopub.status.busy":"2022-02-06T13:11:05.248905Z","iopub.execute_input":"2022-02-06T13:11:05.249574Z","iopub.status.idle":"2022-02-06T13:11:05.272229Z","shell.execute_reply.started":"2022-02-06T13:11:05.249537Z","shell.execute_reply":"2022-02-06T13:11:05.271567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame(final_preds)\nsub.columns = test_df.columns","metadata":{"execution":{"iopub.status.busy":"2022-02-06T13:11:05.273392Z","iopub.execute_input":"2022-02-06T13:11:05.274202Z","iopub.status.idle":"2022-02-06T13:11:05.279035Z","shell.execute_reply.started":"2022-02-06T13:11:05.274165Z","shell.execute_reply":"2022-02-06T13:11:05.278246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T13:11:05.280162Z","iopub.execute_input":"2022-02-06T13:11:05.280839Z","iopub.status.idle":"2022-02-06T13:11:05.294707Z","shell.execute_reply.started":"2022-02-06T13:11:05.280801Z","shell.execute_reply":"2022-02-06T13:11:05.293954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T13:11:05.296105Z","iopub.execute_input":"2022-02-06T13:11:05.296954Z","iopub.status.idle":"2022-02-06T13:11:05.305862Z","shell.execute_reply.started":"2022-02-06T13:11:05.296917Z","shell.execute_reply":"2022-02-06T13:11:05.305111Z"},"trusted":true},"execution_count":null,"outputs":[]}]}