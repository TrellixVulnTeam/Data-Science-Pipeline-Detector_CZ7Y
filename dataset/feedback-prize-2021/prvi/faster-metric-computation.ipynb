{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Faster Metric Computation\n\nAt the end of this notebook you can find a small change that doubles the speed of the code.\nOtherwise the notebook is a verbatim copy of [CPMPs notebook](https://www.kaggle.com/cpmpml/faster-metric-computation).\n\nRob Mula @robikscube wrote a code to compute the evaluation metric in this [notebook](https://www.kaggle.com/robikscube/student-writing-competition-twitch). This is extremely valuable because metric computation is otften tricky, and finding the right python code for it can be time consuming. Rob saved lots of time for many here.\n\nversion 3: After I published my notebook Rob commented this:\n\n*Just a heads up that you might be using an earlier version of my code from before the host clarified that the evaluation metric is indeed macro_f1. If you look at the latest version of my notebook the original score_feedback_comp function is replaced with score_feedback_comp_micro and the new score_feedback_comp scores the macro f1 based off each class. You can still use the older version but will just need to call it on each class individually and then average.*\n\nHere is an updated version which takes Rob's comment into account hopefully.\n\nVersion 4: typo fixes.\n\nVersion 5: I moved the prediction string split before the outer join.  It speeds up code further.\n\nVersion 6: I added a variant of the code which gets a perfect score of 1 when ground truth is used as prediction.\n\nVersion 7: Some typo fixes and further optimisation of the variant code.\n\nVersion 8: There is no need to explicitly compute the match between predictions and ground truth to compute the score. Our perfect score code benefits from it.\n\nVersion 9. Typos fixes.\n\nLet me reproduce Rob's code to start with.","metadata":{"execution":{"iopub.status.busy":"2022-02-12T15:52:44.363658Z","iopub.execute_input":"2022-02-12T15:52:44.36414Z","iopub.status.idle":"2022-02-12T15:52:44.371552Z","shell.execute_reply.started":"2022-02-12T15:52:44.364104Z","shell.execute_reply":"2022-02-12T15:52:44.370362Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# from Rob Mulla @robikscube\n# https://www.kaggle.com/robikscube/student-writing-competition-twitch\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(\" \"))\n    set_gt = set(row.predictionstring_gt.split(\" \"))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter / len_pred\n    return [overlap_1, overlap_2]\n\ndef score_feedback_comp_micro(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n\n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = (\n        gt_df[[\"id\", \"discourse_type\", \"predictionstring\"]]\n        .reset_index(drop=True)\n        .copy()\n    )\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    pred_df[\"pred_id\"] = pred_df.index\n    gt_df[\"gt_id\"] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(\n        gt_df,\n        left_on=[\"id\", \"class\"],\n        right_on=[\"id\", \"discourse_type\"],\n        how=\"outer\",\n        suffixes=(\"_pred\", \"_gt\"),\n    )\n    joined[\"predictionstring_gt\"] = joined[\"predictionstring_gt\"].fillna(\" \")\n    joined[\"predictionstring_pred\"] = joined[\"predictionstring_pred\"].fillna(\" \")\n\n    joined[\"overlaps\"] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined[\"overlap1\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[0])\n    joined[\"overlap2\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[1])\n\n    joined[\"potential_TP\"] = (joined[\"overlap1\"] >= 0.5) & (joined[\"overlap2\"] >= 0.5)\n    joined[\"max_overlap\"] = joined[[\"overlap1\", \"overlap2\"]].max(axis=1)\n    tp_pred_ids = (\n        joined.query(\"potential_TP\")\n        .sort_values(\"max_overlap\", ascending=False)\n        .groupby([\"id\", \"predictionstring_gt\"])\n        .first()[\"pred_id\"]\n        .values\n    )\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined[\"pred_id\"].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query(\"potential_TP\")[\"gt_id\"].unique()\n    unmatched_gt_ids = [c for c in joined[\"gt_id\"].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    # calc microf1\n    my_f1_score = TP / (TP + 0.5 * (FP + FN))\n    return my_f1_score\n\ndef score_feedback_comp(pred_df, gt_df, return_class_scores=False):\n    class_scores = {}\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    for discourse_type, gt_subset in gt_df.groupby(\"discourse_type\"):\n        pred_subset = (\n            pred_df.loc[pred_df[\"class\"] == discourse_type]\n            .reset_index(drop=True)\n            .copy()\n        )\n        class_score = score_feedback_comp_micro(pred_subset, gt_subset)\n        class_scores[discourse_type] = class_score\n    f1 = np.mean([v for v in class_scores.values()])\n    if return_class_scores:\n        return f1, class_scores\n    return f1\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:21:16.647144Z","iopub.execute_input":"2022-02-25T04:21:16.647933Z","iopub.status.idle":"2022-02-25T04:21:16.693514Z","shell.execute_reply.started":"2022-02-25T04:21:16.647821Z","shell.execute_reply":"2022-02-25T04:21:16.692804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's apply it to the ground truth to see how much time it takes.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/feedback-prize-2021/train.csv')\npred_df = train_df[['id','discourse_type','predictionstring']].copy()\npred_df.columns = ['id','class','predictionstring']\n\nscore_feedback_comp(pred_df, train_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:21:16.695647Z","iopub.execute_input":"2022-02-25T04:21:16.696211Z","iopub.status.idle":"2022-02-25T04:21:51.535139Z","shell.execute_reply.started":"2022-02-25T04:21:16.696165Z","shell.execute_reply":"2022-02-25T04:21:51.534322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We look at why it isn't equal to 1 at the end of this notebook. For now we are rather interested in the time the metric computation takes.","metadata":{}},{"cell_type":"code","source":"%timeit score_feedback_comp(pred_df, train_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:21:51.536216Z","iopub.execute_input":"2022-02-25T04:21:51.536442Z","iopub.status.idle":"2022-02-25T04:26:05.322748Z","shell.execute_reply.started":"2022-02-25T04:21:51.536415Z","shell.execute_reply":"2022-02-25T04:26:05.321846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OK, less than a minute, this is probably good enough if we run this code once per epoch.\n\nLet's run it on a sample of the ground truth as well, to get a second comparison point.","metadata":{}},{"cell_type":"code","source":"pred_df2 = pred_df.sample(frac=0.7).reset_index(drop=True)\nscore_feedback_comp(pred_df2, train_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:26:05.324044Z","iopub.execute_input":"2022-02-25T04:26:05.324561Z","iopub.status.idle":"2022-02-25T04:26:29.300931Z","shell.execute_reply.started":"2022-02-25T04:26:05.324525Z","shell.execute_reply":"2022-02-25T04:26:29.299871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit score_feedback_comp(pred_df2, train_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:26:29.303395Z","iopub.execute_input":"2022-02-25T04:26:29.303648Z","iopub.status.idle":"2022-02-25T04:29:35.766406Z","shell.execute_reply.started":"2022-02-25T04:26:29.303619Z","shell.execute_reply":"2022-02-25T04:29:35.764417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Can we do better? I think we can as shown below. We can remove unnecessary data frame copies, list constructions, fillna calls, and, most importantly, avoid the use of the apply function.  Apply is very convenient, but it is extremely slow. We can also split strings only once, before the outer join.\n\nLet's do this.","metadata":{}},{"cell_type":"code","source":"def calc_overlap2(set_pred, set_gt):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    # Length of each and intersection\n    try:\n        len_gt = len(set_gt)\n        len_pred = len(set_pred)\n        inter = len(set_gt & set_pred)\n        overlap_1 = inter / len_gt\n        overlap_2 = inter/ len_pred\n        return (overlap_1, overlap_2)\n    except:  # at least one of the input is NaN\n        return (0, 0)\n\ndef score_feedback_comp_micro2(pred_df, gt_df, discourse_type):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df.loc[gt_df['discourse_type'] == discourse_type, \n                      ['id', 'predictionstring']].reset_index(drop=True)\n    pred_df = pred_df.loc[pred_df['class'] == discourse_type,\n                      ['id', 'predictionstring']].reset_index(drop=True)\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    pred_df['predictionstring'] = [set(pred.split(' ')) for pred in pred_df['predictionstring']]\n    gt_df['predictionstring'] = [set(pred.split(' ')) for pred in gt_df['predictionstring']]\n    \n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on='id',\n                           right_on='id',\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    overlaps = [calc_overlap2(*args) for args in zip(joined.predictionstring_pred, \n                                                     joined.predictionstring_gt)]\n    \n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['potential_TP'] = [(overlap[0] >= 0.5 and overlap[1] >= 0.5) \\\n                              for overlap in overlaps]\n    joined['max_overlap'] = [max(*overlap) for overlap in overlaps]\n    joined_tp = joined.query('potential_TP').reset_index(drop=True)\n    tp_pred_ids = joined_tp\\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','gt_id'])['pred_id'].first()\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = set(joined['pred_id'].unique()) - set(tp_pred_ids)\n\n    matched_gt_ids = joined_tp['gt_id'].unique()\n    unmatched_gt_ids = set(joined['gt_id'].unique()) -  set(matched_gt_ids)\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    my_f1_score = TP / (TP + 0.5*(FP+FN))\n    return my_f1_score\n\ndef score_feedback_comp2(pred_df, gt_df, return_class_scores=False):\n    class_scores = {}\n    for discourse_type in gt_df.discourse_type.unique():\n        class_score = score_feedback_comp_micro2(pred_df, gt_df, discourse_type)\n        class_scores[discourse_type] = class_score\n    f1 = np.mean([v for v in class_scores.values()])\n    if return_class_scores:\n        return f1, class_scores\n    return f1\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:29:35.768323Z","iopub.execute_input":"2022-02-25T04:29:35.768655Z","iopub.status.idle":"2022-02-25T04:29:35.789027Z","shell.execute_reply.started":"2022-02-25T04:29:35.768619Z","shell.execute_reply":"2022-02-25T04:29:35.787882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Is this code computing the same value as Rob's code?","metadata":{}},{"cell_type":"code","source":"score_feedback_comp2(pred_df, train_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:29:35.791187Z","iopub.execute_input":"2022-02-25T04:29:35.79154Z","iopub.status.idle":"2022-02-25T04:29:44.434241Z","shell.execute_reply.started":"2022-02-25T04:29:35.791494Z","shell.execute_reply":"2022-02-25T04:29:44.43315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_feedback_comp2(pred_df2, train_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:29:44.43533Z","iopub.execute_input":"2022-02-25T04:29:44.43565Z","iopub.status.idle":"2022-02-25T04:29:51.461361Z","shell.execute_reply.started":"2022-02-25T04:29:44.435622Z","shell.execute_reply":"2022-02-25T04:29:51.460282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems so.  Is it faster?","metadata":{}},{"cell_type":"code","source":"%timeit score_feedback_comp2(pred_df, train_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:29:51.463322Z","iopub.execute_input":"2022-02-25T04:29:51.463671Z","iopub.status.idle":"2022-02-25T04:30:35.181633Z","shell.execute_reply.started":"2022-02-25T04:29:51.463624Z","shell.execute_reply":"2022-02-25T04:30:35.180317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit score_feedback_comp2(pred_df2, train_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:30:35.183369Z","iopub.execute_input":"2022-02-25T04:30:35.183661Z","iopub.status.idle":"2022-02-25T04:31:12.138538Z","shell.execute_reply.started":"2022-02-25T04:30:35.183629Z","shell.execute_reply":"2022-02-25T04:31:12.137596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is about 5 times faster.","metadata":{}},{"cell_type":"markdown","source":"## Why Not A Perfect Score?","metadata":{}},{"cell_type":"markdown","source":"I was a bit puzzled by why we don't get a score of 1 when using ground truth as prediction. By looking at details I found that FP is 3 in that case. These prediction ids are declared false positives: `{13933, 30714, 143325}`\n\nFurther investigation led to three cases of significant overlap in prediction strings in different rows:","metadata":{}},{"cell_type":"code","source":"train_df.iloc[13933:13935]","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:31:12.140056Z","iopub.execute_input":"2022-02-25T04:31:12.140552Z","iopub.status.idle":"2022-02-25T04:31:12.162628Z","shell.execute_reply.started":"2022-02-25T04:31:12.140509Z","shell.execute_reply":"2022-02-25T04:31:12.161805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.iloc[30713:30715]","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:31:12.163807Z","iopub.execute_input":"2022-02-25T04:31:12.164441Z","iopub.status.idle":"2022-02-25T04:31:12.179692Z","shell.execute_reply.started":"2022-02-25T04:31:12.164406Z","shell.execute_reply":"2022-02-25T04:31:12.178839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.iloc[143325:143327]","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:31:12.181135Z","iopub.execute_input":"2022-02-25T04:31:12.181529Z","iopub.status.idle":"2022-02-25T04:31:12.197278Z","shell.execute_reply.started":"2022-02-25T04:31:12.181484Z","shell.execute_reply":"2022-02-25T04:31:12.196391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the first example.","metadata":{}},{"cell_type":"code","source":"filename = '../input/feedback-prize-2021/train/96948C0AFC15.txt'\nwith open(filename, \"r\") as f:\n    text = f.read()\nprint(text)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:31:12.200579Z","iopub.execute_input":"2022-02-25T04:31:12.201017Z","iopub.status.idle":"2022-02-25T04:31:12.218519Z","shell.execute_reply.started":"2022-02-25T04:31:12.200985Z","shell.execute_reply":"2022-02-25T04:31:12.217906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Issue is that some words are not separated by white spaces, for instance in \"more money saved,more space,pollution, and no traffic.\" at the end of first paragraph.  The way prediction string are created is a bit weird when that happens.  To know more you can have a look at this post by Chris Deotte @cdeotte : https://www.kaggle.com/c/feedback-prize-2021/discussion/297591","metadata":{}},{"cell_type":"markdown","source":"Bottom line is that Rob's code is correct when the input is correct: the issue happens when there is some overlapping ground truth across rows.  A possible cure is presented below.","metadata":{}},{"cell_type":"markdown","source":"## A Perfect Score Code\n\nA further optimization does not compute FP and FN values explicitly. It rather uses both the total number of predictions, and the total number of ground truth.  It also removes the computation of the match between ground truth and predictions. This code is a faster than our code above, but it computes values a bit different than Rob's code. Good news is that it does compute a value of 1 when the ground truth is used as prediction.","metadata":{}},{"cell_type":"code","source":"def calc_overlap3(set_pred, set_gt):\n    \"\"\"\n    Calculates if the overlap between prediction and\n    ground truth is enough fora potential True positive\n    \"\"\"\n    # Length of each and intersection\n    try:\n        len_gt = len(set_gt)\n        len_pred = len(set_pred)\n        inter = len(set_gt & set_pred)\n        overlap_1 = inter / len_gt\n        overlap_2 = inter/ len_pred\n        return overlap_1 >= 0.5 and overlap_2 >= 0.5\n    except:  # at least one of the input is NaN\n        return False\n\ndef score_feedback_comp_micro3(pred_df, gt_df, discourse_type):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df.loc[gt_df['discourse_type'] == discourse_type, \n                      ['id', 'predictionstring']].reset_index(drop=True)\n    pred_df = pred_df.loc[pred_df['class'] == discourse_type,\n                      ['id', 'predictionstring']].reset_index(drop=True)\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    pred_df['predictionstring'] = [set(pred.split(' ')) for pred in pred_df['predictionstring']]\n    gt_df['predictionstring'] = [set(pred.split(' ')) for pred in gt_df['predictionstring']]\n    \n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on='id',\n                           right_on='id',\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    overlaps = [calc_overlap3(*args) for args in zip(joined.predictionstring_pred, \n                                                     joined.predictionstring_gt)]\n    \n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    # we don't need to compute the match to compute the score\n    TP = joined.loc[overlaps]['gt_id'].nunique()\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    TPandFP = len(pred_df)\n    TPandFN = len(gt_df)\n    \n    #calc microf1\n    my_f1_score = 2*TP / (TPandFP + TPandFN)\n    return my_f1_score\n\ndef score_feedback_comp3(pred_df, gt_df, return_class_scores=False):\n    class_scores = {}\n    for discourse_type in gt_df.discourse_type.unique():\n        class_score = score_feedback_comp_micro3(pred_df, gt_df, discourse_type)\n        class_scores[discourse_type] = class_score\n    f1 = np.mean([v for v in class_scores.values()])\n    if return_class_scores:\n        return f1, class_scores\n    return f1\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:31:12.220095Z","iopub.execute_input":"2022-02-25T04:31:12.220543Z","iopub.status.idle":"2022-02-25T04:31:12.235778Z","shell.execute_reply.started":"2022-02-25T04:31:12.220511Z","shell.execute_reply":"2022-02-25T04:31:12.235095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_feedback_comp3(pred_df, train_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:31:12.237349Z","iopub.execute_input":"2022-02-25T04:31:12.237708Z","iopub.status.idle":"2022-02-25T04:31:18.977222Z","shell.execute_reply.started":"2022-02-25T04:31:12.237674Z","shell.execute_reply":"2022-02-25T04:31:18.976359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_feedback_comp3(pred_df2, train_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:31:18.97876Z","iopub.execute_input":"2022-02-25T04:31:18.979705Z","iopub.status.idle":"2022-02-25T04:31:24.750551Z","shell.execute_reply.started":"2022-02-25T04:31:18.979659Z","shell.execute_reply":"2022-02-25T04:31:24.749524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit score_feedback_comp3(pred_df, train_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:31:24.752189Z","iopub.execute_input":"2022-02-25T04:31:24.752415Z","iopub.status.idle":"2022-02-25T04:32:00.998333Z","shell.execute_reply.started":"2022-02-25T04:31:24.752387Z","shell.execute_reply":"2022-02-25T04:32:00.997255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit score_feedback_comp3(pred_df2, train_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:32:01.000255Z","iopub.execute_input":"2022-02-25T04:32:01.000972Z","iopub.status.idle":"2022-02-25T04:32:31.55162Z","shell.execute_reply.started":"2022-02-25T04:32:01.000919Z","shell.execute_reply":"2022-02-25T04:32:31.550794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Whether to use the last version or not is left to the reader...","metadata":{}},{"cell_type":"markdown","source":"Once again, let me thank Rob for his code. The fact that his code could be made faster does not mean that his code was bad at all. First, his code was correct, and it was running fast enough to be used in practice. Second, my understanding is that he wrote his code live in a Twitch stream. I am not sure I would have written a correct code in that setting. Offline optimization of an already correct code is much easier.\n\nThis said, I hope the optimized code can still be useful to some in this competition.","metadata":{}},{"cell_type":"markdown","source":"# A further speed up","metadata":{}},{"cell_type":"markdown","source":"The only change below is that you can avoid set operations as the `predictionstring` contains integers from an interval. So you only need to convert the first and the last element into a number and you have all the information. This doubles the speed. If you want to check the performance of your model you can speed it up further by avoiding the generation of the prediction string and by preprocessing the ground truth.","metadata":{}},{"cell_type":"code","source":"def calc_overlap4(pred_range, gt_range):\n    \"\"\"\n    Calculates if the overlap between prediction and\n    ground truth is enough for a potential True positive\n    \"\"\"\n    # Length of each and intersection\n    try:\n        len_gt = gt_range[1]-gt_range[0]\n        len_pred = pred_range[1]-pred_range[0]\n        inter = max(0, min(gt_range[1], pred_range[1])-max(gt_range[0], pred_range[0]))\n        overlap_1 = inter / len_gt\n        overlap_2 = inter/ len_pred\n        return overlap_1 >= 0.5 and overlap_2 >= 0.5\n    except:  # at least one of the input is NaN\n        return False\n\ndef get_range(pred_str):\n    return int(pred_str.split(' ',1)[0]), int(pred_str.rsplit(' ', 1)[-1])+1\n\ndef score_feedback_comp_micro4(pred_df, gt_df, discourse_type):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df.loc[gt_df['discourse_type'] == discourse_type, \n                      ['id', 'predictionstring']].reset_index(drop=True)\n    pred_df = pred_df.loc[pred_df['class'] == discourse_type,\n                      ['id', 'predictionstring']].reset_index(drop=True)\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    pred_df['pred_range'] = [get_range(pred) for pred in pred_df['predictionstring']]\n    gt_df['gt_range'] = [get_range(pred) for pred in gt_df['predictionstring']]\n\n    #pred_df['predictionstring'] = [set(pred.split(' ')) for pred in pred_df['predictionstring']]\n    #gt_df['predictionstring'] = [set(pred.split(' ')) for pred in gt_df['predictionstring']]\n    \n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on='id',\n                           right_on='id',\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    overlaps = [calc_overlap4(*args) for args in zip(joined.pred_range, \n                                                     joined.gt_range)]\n    \n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    # we don't need to compute the match to compute the score\n    TP = joined.loc[overlaps]['gt_id'].nunique()\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    TPandFP = len(pred_df)\n    TPandFN = len(gt_df)\n    \n    #calc microf1\n    my_f1_score = 2*TP / (TPandFP + TPandFN)\n    return my_f1_score\n\ndef score_feedback_comp4(pred_df, gt_df, return_class_scores=False):\n    class_scores = {}\n    for discourse_type in gt_df.discourse_type.unique():\n        class_score = score_feedback_comp_micro4(pred_df, gt_df, discourse_type)\n        class_scores[discourse_type] = class_score\n    f1 = np.mean([v for v in class_scores.values()])\n    if return_class_scores:\n        return f1, class_scores\n    return f1\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:35:56.232505Z","iopub.execute_input":"2022-02-25T04:35:56.233062Z","iopub.status.idle":"2022-02-25T04:35:56.249999Z","shell.execute_reply.started":"2022-02-25T04:35:56.233011Z","shell.execute_reply":"2022-02-25T04:35:56.248782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_feedback_comp4(pred_df, train_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:36:01.016245Z","iopub.execute_input":"2022-02-25T04:36:01.016636Z","iopub.status.idle":"2022-02-25T04:36:03.009871Z","shell.execute_reply.started":"2022-02-25T04:36:01.016601Z","shell.execute_reply":"2022-02-25T04:36:03.008964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_feedback_comp4(pred_df2, train_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:36:32.02831Z","iopub.execute_input":"2022-02-25T04:36:32.028677Z","iopub.status.idle":"2022-02-25T04:36:33.799187Z","shell.execute_reply.started":"2022-02-25T04:36:32.028642Z","shell.execute_reply":"2022-02-25T04:36:33.798308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit score_feedback_comp4(pred_df, train_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:36:36.972613Z","iopub.execute_input":"2022-02-25T04:36:36.973221Z","iopub.status.idle":"2022-02-25T04:36:53.061516Z","shell.execute_reply.started":"2022-02-25T04:36:36.97318Z","shell.execute_reply":"2022-02-25T04:36:53.060532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%timeit score_feedback_comp4(pred_df2, train_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T04:36:59.386488Z","iopub.execute_input":"2022-02-25T04:36:59.386812Z","iopub.status.idle":"2022-02-25T04:37:13.208248Z","shell.execute_reply.started":"2022-02-25T04:36:59.386772Z","shell.execute_reply":"2022-02-25T04:37:13.207052Z"},"trusted":true},"execution_count":null,"outputs":[]}]}