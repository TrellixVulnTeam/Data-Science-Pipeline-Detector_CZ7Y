{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Sequence bucketing - PyTorch implementation\n\nI've unsuccessfully tried a couple of times to implement Sequence Bucketing in PyTorch. Recently I found this notebook that solves this problem: [Notebook](https://www.kaggle.com/shahules/guide-pytorch-data-samplers-sequence-bucketing/notebook)\n\nThanks [Shahules](https://www.kaggle.com/shahules)\n\nI just modified it for this competition and made it a little bit more pythonic.\n\nIn my case it speeds up training from 18:40 min to 8:54 min for one epoch and batch size 4. If you are allready truncating your input, you may see less improvements than me.\n\nIf you have any questions or ideas for improvements, please let me know!","metadata":{}},{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom torch.utils.data import Dataset, Subset, DataLoader, RandomSampler, SequentialSampler\n\nfrom transformers import LongformerTokenizerFast\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:13:58.810947Z","iopub.execute_input":"2021-12-22T16:13:58.811403Z","iopub.status.idle":"2021-12-22T16:14:07.938029Z","shell.execute_reply.started":"2021-12-22T16:13:58.811291Z","shell.execute_reply":"2021-12-22T16:14:07.937111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1 . Dataloading and preprocessing","metadata":{}},{"cell_type":"code","source":"# load the train text data\n\nconfig = {\n    'model_name': 'allenai/longformer-base-4096',\n    'batch_size': 4,\n}\n\nTEXT_FILES = os.listdir('../input/feedback-prize-2021/train')\nTEXT_FILES = [f'../input/feedback-prize-2021/train/{file}' for file in TEXT_FILES]\n\ntext_data = dict()\nfor file_path in tqdm(TEXT_FILES):\n    with open(file_path, 'r') as file:\n        idx = os.path.basename(file_path).split('.txt')[0]\n        text_data[idx] = file.read()\n        \n# 1. delete spaces from texts ends\nfor key, value in text_data.items():\n    text_data[key] = value.rstrip()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:14:07.940133Z","iopub.execute_input":"2021-12-22T16:14:07.940443Z","iopub.status.idle":"2021-12-22T16:14:45.490414Z","shell.execute_reply.started":"2021-12-22T16:14:07.940401Z","shell.execute_reply":"2021-12-22T16:14:45.488548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, I am going to use the Longformer tokenizer.","metadata":{}},{"cell_type":"code","source":"tokenizer = LongformerTokenizerFast.from_pretrained(config['model_name'])\n\ndata_tokenized = []\n\nfor idx, text in tqdm(text_data.items()):\n    \n    # get inputs\n    inputs = tokenizer(text, add_special_tokens=True)\n        \n    data_tokenized.append([inputs['input_ids'], inputs['attention_mask']])\n    \ntokenized_df = pd.DataFrame(data_tokenized, columns=['input_ids', 'attention_mask'])\ntokenized_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:14:45.491587Z","iopub.execute_input":"2021-12-22T16:14:45.491847Z","iopub.status.idle":"2021-12-22T16:15:22.188585Z","shell.execute_reply.started":"2021-12-22T16:14:45.491817Z","shell.execute_reply":"2021-12-22T16:15:22.187753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Sequence length histogram","metadata":{}},{"cell_type":"code","source":"seq_len = tokenized_df['attention_mask'].apply(len)\n\nplt.rcParams['figure.figsize'] = (17, 8)\nbins = np.linspace(0, 2000, 100)\n\nplt.hist(seq_len, bins=bins, alpha=0.75, label='sequence length')\nplt.vlines(seq_len.mean(), ymin=0, ymax=700, colors='red', label='mean sequence length')\nplt.legend(loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:15:22.190886Z","iopub.execute_input":"2021-12-22T16:15:22.191119Z","iopub.status.idle":"2021-12-22T16:15:22.684823Z","shell.execute_reply.started":"2021-12-22T16:15:22.191091Z","shell.execute_reply":"2021-12-22T16:15:22.683826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Create custom PyTorch Dataset","metadata":{}},{"cell_type":"code","source":"class LongformerDataset(Dataset):\n    \"\"\"Dataset for the longformer model.\"\"\"\n    \n    def __init__(self, data: pd.DataFrame):\n        self.data = data        \n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        \n        return {\n            'input_ids': self.data.loc[idx, 'input_ids'],\n            'attention_mask': self.data.loc[idx, 'attention_mask']\n        }\n      ","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:15:22.686689Z","iopub.execute_input":"2021-12-22T16:15:22.687057Z","iopub.status.idle":"2021-12-22T16:15:22.693745Z","shell.execute_reply.started":"2021-12-22T16:15:22.687026Z","shell.execute_reply":"2021-12-22T16:15:22.692739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now instead of using a PyTorch Sampler, we need to make a class that processes a batch ourselfs, and call it as collate_fn parameter when making a DataLoader.\n\nSee this discusion: [Link](https://discuss.pytorch.org/t/how-to-use-collate-fn/27181)","metadata":{}},{"cell_type":"code","source":"from typing import List\n\nclass Collate:\n    \n    def __call__(self, batch: List[dict]) -> dict:\n        \n        output = dict()\n        \n        # since our custom Dataset's __getitem__ method returns dictionary\n        # the collate_fn function will receive list of dictionaries\n        output['input_ids'] = [sample['input_ids'] for sample in batch]\n        output['attention_mask'] = [sample['attention_mask'] for sample in batch]\n        \n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output['input_ids']])\n        \n        # add padding\n        output['input_ids'] = [sample + (batch_max-len(sample)) * [tokenizer.pad_token_id] for sample in output['input_ids']]\n        output['attention_mask'] = [sample + (batch_max-len(sample)) * [0] for sample in output['attention_mask']]\n        \n        # convert to tensors\n        output['input_ids'] = torch.tensor(output['input_ids'], dtype=torch.long)\n        output['attention_mask'] = torch.tensor(output['attention_mask'], dtype=torch.long)\n    \n        return output\n","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:15:22.694878Z","iopub.execute_input":"2021-12-22T16:15:22.695094Z","iopub.status.idle":"2021-12-22T16:15:22.712532Z","shell.execute_reply.started":"2021-12-22T16:15:22.695069Z","shell.execute_reply":"2021-12-22T16:15:22.711479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Create Dataloaders","metadata":{}},{"cell_type":"code","source":"collate = Collate()\ndataset = LongformerDataset(tokenized_df)\n\ntrain_data, val_data = train_test_split(dataset, test_size=0.2, shuffle=True)\n\ntrain_dataloader = DataLoader(train_data,\n                              batch_size=config['batch_size'],\n                              collate_fn=collate,\n                              shuffle=True)\n\nval_dataloader = DataLoader(val_data,\n                            batch_size=config['batch_size'],\n                            collate_fn=collate\n                           )","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:15:22.71387Z","iopub.execute_input":"2021-12-22T16:15:22.714258Z","iopub.status.idle":"2021-12-22T16:15:23.159993Z","shell.execute_reply.started":"2021-12-22T16:15:22.714221Z","shell.execute_reply":"2021-12-22T16:15:23.159343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets see the average batch max len","metadata":{}},{"cell_type":"code","source":"train_batch_sizes = pd.Series([batch['input_ids'].size(1) for batch in train_dataloader])\n\nprint(f'Mean: {round(train_batch_sizes.mean(), 2)}')\nprint(f'Mean absolute deviation: {round(train_batch_sizes.mad(), 2)}')","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:15:23.160908Z","iopub.execute_input":"2021-12-22T16:15:23.161409Z","iopub.status.idle":"2021-12-22T16:15:23.892594Z","shell.execute_reply.started":"2021-12-22T16:15:23.161372Z","shell.execute_reply":"2021-12-22T16:15:23.891736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With higher batch sizes, there should be more padding tokens on average, therefore less runtime improvements!","metadata":{}},{"cell_type":"code","source":"config['batch_size'] = 8\n\ntrain_dataloader = DataLoader(train_data,\n                              batch_size=config['batch_size'],\n                              collate_fn=collate,\n                              shuffle=True)\n\ntrain_batch_sizes = pd.Series([batch['input_ids'].size(1) for batch in train_dataloader])\n\nprint(f'Mean: {round(train_batch_sizes.mean(), 2)}')\nprint(f'Mean absolute deviation: {round(train_batch_sizes.mad(), 2)}')","metadata":{"execution":{"iopub.status.busy":"2021-12-22T16:15:23.893802Z","iopub.execute_input":"2021-12-22T16:15:23.894042Z","iopub.status.idle":"2021-12-22T16:15:24.640083Z","shell.execute_reply.started":"2021-12-22T16:15:23.894011Z","shell.execute_reply":"2021-12-22T16:15:24.639252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}