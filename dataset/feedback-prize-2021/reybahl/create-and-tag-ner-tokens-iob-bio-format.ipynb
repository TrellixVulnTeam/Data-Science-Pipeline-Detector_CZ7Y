{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Description\n\nThis notebook demonstrates how to create NER tokens from scratch. The Inside-Outside-Beginning (IOB) tagging format is used- https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)\n\nTwo files are generated- text.txt contains tokens and labels.txt contains BIO/IOB labels","metadata":{}},{"cell_type":"code","source":"#Import required libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport nltk #Tokenization\n\nimport os #Traversing files\n\nfrom tqdm import tqdm #Checking progress","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-08T21:49:09.80922Z","iopub.execute_input":"2022-02-08T21:49:09.810114Z","iopub.status.idle":"2022-02-08T21:49:11.948963Z","shell.execute_reply.started":"2022-02-08T21:49:09.810011Z","shell.execute_reply":"2022-02-08T21:49:11.947719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/feedback-prize-2021/train.csv\") #Read the training csv file","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:49:11.951291Z","iopub.execute_input":"2022-02-08T21:49:11.952373Z","iopub.status.idle":"2022-02-08T21:49:14.118017Z","shell.execute_reply.started":"2022-02-08T21:49:11.952306Z","shell.execute_reply":"2022-02-08T21:49:14.117045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head() #First few rows","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:49:14.119309Z","iopub.execute_input":"2022-02-08T21:49:14.119642Z","iopub.status.idle":"2022-02-08T21:49:14.150602Z","shell.execute_reply.started":"2022-02-08T21:49:14.119603Z","shell.execute_reply":"2022-02-08T21:49:14.149307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.discourse_type.value_counts().plot.barh()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:49:14.153308Z","iopub.execute_input":"2022-02-08T21:49:14.153652Z","iopub.status.idle":"2022-02-08T21:49:14.474664Z","shell.execute_reply.started":"2022-02-08T21:49:14.153616Z","shell.execute_reply":"2022-02-08T21:49:14.473776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Tagging function\n* text.txt - for storing the text \n* labels.txt - for storing labels (eg. B-Claim, I-Evidence, O)\n\nB in front of the label means it is the beginning of the discourse, I in front means it is inside the discourse","metadata":{}},{"cell_type":"code","source":"def create_ner_tokens():\n    tokenized_text_file = open(\"text.txt\", \"w\")\n    labels_file = open(\"labels.txt\", \"w\")\n\n    total_list = []\n\n    train_dir = f\"../input/feedback-prize-2021/train\"\n\n    for filename in tqdm(os.listdir(train_dir)): #Loop through all the files in the training directory\n        file_path = os.path.join(train_dir, filename) #Get file path\n\n        # checking if it is a txt file\n        if os.path.isfile(file_path) and os.path.splitext(file_path)[1] == \".txt\":\n\n            file_id = os.path.splitext(filename)[0] #Splitting file name with the name and extension\n            file_df = train_df[train_df[\"id\"] == file_id] #Selecting the part of the training dataframe that contains info about the current file\n            \n            with open (file_path) as f:\n                file_text = f.read()\n            \n            file_discourse_text_list = list(file_df.discourse_text)\n            file_discourse_type_list = list(file_df.discourse_type)\n            file_discourse_index_list = list(file_df.predictionstring)\n\n            token_list = []\n            label_list = []\n\n            for i, discourse_type in enumerate(file_discourse_type_list):\n                file_discourse_indices = str(file_discourse_index_list[i]).split() #Getting discourse indices from the predictionstring column\n                discourse_start = int(file_discourse_indices[0])\n                discourse_end = int(file_discourse_indices[-1])\n\n                \n                if not((i == 0) or (discourse_start == int(str(file_discourse_index_list[i-1]).split()[-1]) + 1)): #Checking if a part of the text is not a discourse\n                    no_discourse_start_index = int(str(file_discourse_index_list[i-1]).split()[-1]) + 1\n                    no_discourse_end_index = int(str(file_discourse_index_list[i]).split()[-1])\n                    no_discourse_text = file_text.split()[no_discourse_start_index:no_discourse_end_index]\n                   # discourse_text_tokens = nltk.word_tokenize(\" \".join(no_discourse_text)) #Split into word tokens\n\n                    for token in no_discourse_text:  \n\n                        token_list.append(token)\n    \n                        # Assigning label O for outside\n                        label_list.append(\"O\")\n                \n\n                discourse_text_tokens = file_discourse_text_list[i].split() #Split into word tokens\n\n                for token_idx, token in enumerate(discourse_text_tokens):\n\n                    token_list.append(token)\n\n                    if token_idx == 0: #Checking if it is the first element\n                        label_list.append(\"B-\" + discourse_type.replace(\" \", \"\")) #B for beginning (eg. B-Claim)\n                    else:\n                        label_list.append(\"I-\" + discourse_type.replace(\" \", \"\")) #I for inside (eg. I-Claim)\n\n\n            tokenized_text_file.write(\" \".join(token_list) + \"\\n\") #Separating each token with a space and each essay with a newline\n            labels_file.write(\" \".join(label_list) + \"\\n\") #Separating each label with a space and each essay's labels with a newline\n\n    tokenized_text_file.close()\n    labels_file.close()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:49:14.476139Z","iopub.execute_input":"2022-02-08T21:49:14.477966Z","iopub.status.idle":"2022-02-08T21:49:14.495674Z","shell.execute_reply.started":"2022-02-08T21:49:14.477902Z","shell.execute_reply":"2022-02-08T21:49:14.494746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_ner_tokens()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:49:14.496936Z","iopub.execute_input":"2022-02-08T21:49:14.497179Z","iopub.status.idle":"2022-02-08T21:49:14.959613Z","shell.execute_reply.started":"2022-02-08T21:49:14.497148Z","shell.execute_reply":"2022-02-08T21:49:14.958481Z"},"trusted":true},"execution_count":null,"outputs":[]}]}