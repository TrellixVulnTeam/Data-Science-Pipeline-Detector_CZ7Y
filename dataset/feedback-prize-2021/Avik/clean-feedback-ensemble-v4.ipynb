{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q \"../input/autocorrect/autocorrect-2.6.1.tar\"\n\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\nfrom torch.utils.data.sampler import *\nfrom joblib import Parallel, delayed\nfrom autocorrect import Speller\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport glob\nimport pandas as pd\nimport numpy as np\nimport torch\nimport sys\nimport os\n\nspell_correct = Speller(lang='en', fast=True)\nsys.path.append(\"../input/tez-lib/\")\nimport tez\nimport gc\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:02:20.559691Z","iopub.execute_input":"2022-03-15T10:02:20.559965Z","iopub.status.idle":"2022-03-15T10:02:57.452891Z","shell.execute_reply.started":"2022-03-15T10:02:20.559885Z","shell.execute_reply":"2022-03-15T10:02:57.45215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ARGS\ntarget_id_map = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 2,\n    \"I-Position\": 3,\n    \"B-Evidence\": 4,\n    \"I-Evidence\": 5,\n    \"B-Claim\": 6,\n    \"I-Claim\": 7,\n    \"B-Concluding Statement\": 8,\n    \"I-Concluding Statement\": 9,\n    \"B-Counterclaim\": 10,\n    \"I-Counterclaim\": 11,\n    \"B-Rebuttal\": 12,\n    \"I-Rebuttal\": 13,\n    \"O\": 14,\n    \"PAD\": -100,\n}\n\nid_target_map = {v: k for k, v in target_id_map.items()}\nclass args1: #longformer\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/longformerlarge4096/longformer-large-4096\"\n    tez_model= \"../input/spellchecker-fold0\"\n    output = \".\"\n    batch_size = 8\n    max_len = 1600\n    \nclass args2: #longformer trivia\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/triviaqa/longformer-large-4096-finetuned-triviaqa\"\n    tez_model= \"../input/trivia4096\"\n    output = \".\"\n    batch_size = 8\n    max_len = 1600\n\nclass args3: #deberta xl\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/deberta-xlarge/\"\n    tez_model= \"../input/at-model-deberta-xlarge/data_deberta\"\n    output = \".\"\n    batch_size = 8\n    max_len = 1600\n    \nclass args4: #deberta base\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/debertabasehf/deberta-base\"\n    tez_model= \"../input/debertaallfolds\"\n    output = \".\"\n    batch_size = 8\n    max_len = 1600\n    \nclass args5: #deberta large\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/debertalargesample/deberta-large\"\n    tez_model= \"../input/debertalarge\"\n    output = \".\"\n    batch_size = 8\n    max_len = 1600\n    \nclass args6: #deberta large smooth\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/debertalargesample/deberta-large\"\n    tez_model= \"../input/debertasmoothlarge\"\n    output = \".\"\n    batch_size = 8\n    max_len = 1600","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:02:57.455511Z","iopub.execute_input":"2022-03-15T10:02:57.455992Z","iopub.status.idle":"2022-03-15T10:02:57.466594Z","shell.execute_reply.started":"2022-03-15T10:02:57.455955Z","shell.execute_reply":"2022-03-15T10:02:57.466018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GEN UTILS\nclass FeedbackDataset:\n    def __init__(self, samples, max_len, tokenizer):\n        self.samples = samples\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.length = len(samples)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        input_ids = self.samples[idx][\"input_ids\"]\n        input_ids = [self.tokenizer.cls_token_id] + input_ids\n        if len(input_ids) > self.max_len - 1:\n            input_ids = input_ids[: self.max_len - 1]\n        input_ids = input_ids + [self.tokenizer.sep_token_id]\n        attention_mask = [1] * len(input_ids)\n        return {\n            \"ids\": input_ids,\n            \"mask\": attention_mask,\n        }\n    \nclass Collate:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n        batch_max = max([len(ids) for ids in output[\"ids\"]])\n        if self.tokenizer.padding_side == \"right\":\n            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n        else:\n            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n        return output\n    \ndef _prepare_test_data_helper(args, tokenizer, ids, spell_check):\n    test_samples = []\n    for idx in ids:\n        filename = os.path.join(args.input_path, \"test\", idx + \".txt\")\n        with open(filename, \"r\") as f:\n            text = f.read()\n        if spell_check:\n            text = spell_correct(text)\n        encoded_text = tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        offset_mapping = encoded_text[\"offset_mapping\"]\n\n        sample = {\n            \"id\": idx,\n            \"input_ids\": input_ids,\n            \"text\": text,\n            \"offset_mapping\": offset_mapping,\n        }\n        test_samples.append(sample)\n    return test_samples\n\ndef prepare_test_data(df, tokenizer, args, spell_check = False):\n    test_samples = []\n    ids = df[\"id\"].unique()\n    ids_splits = np.array_split(ids, 4)\n    results = Parallel(n_jobs=4, backend=\"multiprocessing\")(\n        delayed(_prepare_test_data_helper)(args, tokenizer, idx, spell_check) for idx in ids_splits\n    )\n    for result in results:\n        test_samples.extend(result)\n    return test_samples","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:02:57.467526Z","iopub.execute_input":"2022-03-15T10:02:57.467862Z","iopub.status.idle":"2022-03-15T10:02:57.489308Z","shell.execute_reply.started":"2022-03-15T10:02:57.467832Z","shell.execute_reply":"2022-03-15T10:02:57.488503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODELS\nclass FeedbackModel(tez.Model):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n        config = AutoConfig.from_pretrained(model_name)\n        hidden_dropout_prob: float = 0.1\n        layer_norm_eps: float = 1e-7\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = AutoModel.from_config(config)\n        self.output = nn.Linear(config.hidden_size, self.num_labels)\n        \n    def forward(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        logits = self.output(sequence_output)\n        logits = torch.softmax(logits, dim=-1)\n        return logits, 0, {}","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:02:57.491922Z","iopub.execute_input":"2022-03-15T10:02:57.492198Z","iopub.status.idle":"2022-03-15T10:02:57.501367Z","shell.execute_reply.started":"2022-03-15T10:02:57.492163Z","shell.execute_reply":"2022-03-15T10:02:57.500666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BLEND\ndf = pd.read_csv(os.path.join(\"../input/feedback-prize-2021/\", \"sample_submission.csv\"))\ndf_ids = df[\"id\"].unique()\n\ntokenizer = AutoTokenizer.from_pretrained(args1.model)\ntest_samples = prepare_test_data(df, tokenizer, args1, spell_check = True)\ncollate = Collate(tokenizer=tokenizer)\ntest_dataset = FeedbackDataset(test_samples, args1.max_len, tokenizer)\n\ntokenizer3 = AutoTokenizer.from_pretrained(args3.model)\ntest_samples3 = prepare_test_data(df, tokenizer3, args3, spell_check = True)\ncollate3 = Collate(tokenizer=tokenizer3)\ntest_dataset3 = FeedbackDataset(test_samples3, args3.max_len, tokenizer3)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodels = {\n    1: { #longformer\n        \"folds_used\":[0,3],\n        \"weight\": 0.08             # ensure that these weights sum to 1.0\n    },\n    2:{ #longformer trivia\n        \"folds_used\":[1,3],\n        \"weight\": 0.08\n    },\n    3:{ #deberta xl\n        \"folds_used\":[1,3,4],\n        \"weight\": 0.30\n    },\n#     4:{ #deberta base\n#         \"folds_used\":[1,2,3],\n#         \"weight\":.04\n#     },\n    5:{ #deberta large\n        \"folds_used\":[0,2,4],\n      \"weight\": 0.24\n      },\n    6:{ #deberta large smooth\n        \"folds_used\":[0,1,3,4],\n      \"weight\": 0.30\n       },    \n}\n\nraw_preds = []\nchecksum = 0\ntotal_folds = 0 \nfor model in models.keys():\n    total_folds += len(models[model][\"folds_used\"])\nprint(\"Total folds: \", total_folds)\nresults = []\nfor i, model_ in enumerate(models.keys()):\n    folds_used = models[model_][\"folds_used\"]\n    for j, fold_ in enumerate(folds_used):\n        current_idx = 0\n        if model_ == 1:\n            model = FeedbackModel(model_name=args1.model, num_labels=len(target_id_map) - 1)\n            if (fold_) == 0:\n                model.load('../input/spellchecker-fold0/model_0 (1).bin', weights_only=True)\n            elif (fold_) == 3:\n                model.load('../input/spellchecker-fold0/drive-download-20220131T060741Z-002/model_3.bin', weights_only=True)\n            elif fold_==2:\n                model.load(\"../input/spellchecker-fold0/drive-download-20220131T060741Z-001/model_2.bin\", weights_only=True)\n            else:\n                model.load(os.path.join(args1.tez_model, f\"model_{fold_}.bin\"), weights_only=True)\n            preds_iter = model.predict(test_dataset, batch_size=args1.batch_size, n_jobs=-1, collate_fn=collate)\n                \n        if model_ == 2:\n            model = FeedbackModel(model_name=args2.model, num_labels=len(target_id_map) - 1)\n            if fold_ == 0:\n                model.load(os.path.join(args2.tez_model, f\"model_{fold_} (2).bin\"), weights_only=True)\n            else:\n                model.load(os.path.join(args2.tez_model, f\"model_{fold_}.bin\"), weights_only=True)\n            preds_iter = model.predict(test_dataset, batch_size=args1.batch_size, n_jobs=-1, collate_fn=collate)\n                \n        if model_ == 3:\n            model = FeedbackModel(model_name=args3.model, num_labels=len(target_id_map) - 1)\n            model.load(os.path.join(args3.tez_model, f\"model_1024_debberta_{fold_}.bin\"), weights_only=True)\n            preds_iter = model.predict(test_dataset3, batch_size=args3.batch_size, n_jobs=-1, collate_fn=collate3)\n            \n        if model_ == 4:\n            model = FeedbackModel(model_name=args4.model, num_labels=len(target_id_map) - 1)\n            model.load(os.path.join(args4.tez_model, f\"deberta{fold_}.bin\"), weights_only=True)\n            preds_iter = model.predict(test_dataset3, batch_size=args4.batch_size, n_jobs=-1, collate_fn=collate3)\n        \n        if model_ == 5:\n            model = FeedbackModel(model_name=args5.model, num_labels=len(target_id_map) - 1)\n            model.load(os.path.join(args5.tez_model, f\"debertaLarge{fold_}.bin\"), weights_only=True)\n            preds_iter = model.predict(test_dataset3, batch_size=args5.batch_size, n_jobs=-1, collate_fn=collate3)\n            \n        if model_ == 6:\n            model = FeedbackModel(model_name=args6.model, num_labels=len(target_id_map) - 1)\n            model.load(os.path.join(args6.tez_model, f\"debertaSmoothlLarge{fold_}.bin\"), weights_only=True)\n            preds_iter = model.predict(test_dataset3, batch_size=args6.batch_size, n_jobs=-1, collate_fn=collate3)\n\n        print(f\"Predicting Model: {model_}\\tFold: {fold_}\\tWeight: {(1/len(folds_used)) * models[model_]['weight']}\")\n        checksum += (1/len(folds_used)) * models[model_]['weight']\n        current_idx = 0\n        for preds in preds_iter:\n            preds = preds.astype(np.float32)\n            preds = preds * (1/len(folds_used)) * models[model_]['weight']\n            if i==0 and j==0:\n                raw_preds.append(preds)\n            else:\n                raw_preds[current_idx] += preds\n                current_idx += 1\n        torch.cuda.empty_cache()\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:02:57.503684Z","iopub.execute_input":"2022-03-15T10:02:57.504046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(checksum)\nprint(np.array(raw_preds,dtype=object)[0][0][0].sum())","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:15:12.846165Z","iopub.execute_input":"2022-03-15T10:15:12.846975Z","iopub.status.idle":"2022-03-15T10:15:12.860768Z","shell.execute_reply.started":"2022-03-15T10:15:12.846913Z","shell.execute_reply":"2022-03-15T10:15:12.859888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = []\nfinal_scores = []\n\nfor rp in raw_preds:\n    pred_class = np.argmax(rp, axis=2)\n    pred_scrs = np.max(rp, axis=2)\n    for pred, pred_scr in zip(pred_class, pred_scrs):\n        pred = pred.tolist()\n        pred_scr = pred_scr.tolist()\n        final_preds.append(pred)\n        final_scores.append(pred_scr)\n\nfor j in range(len(test_samples)):\n    tt = [id_target_map[p] for p in final_preds[j][1:]]\n    tt_score = final_scores[j][1:]\n    test_samples[j][\"preds\"] = tt\n    test_samples[j][\"pred_scores\"] = tt_score","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:15:14.765537Z","iopub.execute_input":"2022-03-15T10:15:14.765809Z","iopub.status.idle":"2022-03-15T10:15:14.774631Z","shell.execute_reply.started":"2022-03-15T10:15:14.765779Z","shell.execute_reply":"2022-03-15T10:15:14.773928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SUBMISSION\nproba_thresh = { #higher\n    \"Lead\": 0.687,\n    \"Position\": 0.537,\n    \"Evidence\": 0.637,\n    \"Claim\": 0.537,\n    \"Concluding Statement\": 0.687,\n    \"Counterclaim\": 0.537,\n    \"Rebuttal\": 0.537,\n}\n\nmin_thresh = {\n    \"Lead\": 9,\n    \"Position\": 5,\n    \"Evidence\": 14,\n    \"Claim\": 3,\n    \"Concluding Statement\": 11,\n    \"Counterclaim\": 6,\n    \"Rebuttal\": 4,\n}\n\nsubmission = []\nfor sample_idx, sample in enumerate(test_samples):\n    preds = sample[\"preds\"]\n    offset_mapping = sample[\"offset_mapping\"]\n    sample_id = sample[\"id\"]\n    sample_text = sample[\"text\"]\n    sample_input_ids = sample[\"input_ids\"]\n    sample_pred_scores = sample[\"pred_scores\"]\n    sample_preds = []\n\n    if len(preds) < len(offset_mapping):\n        preds = preds + [\"O\"] * (len(offset_mapping) - len(preds))\n        sample_pred_scores = sample_pred_scores + [0] * (len(offset_mapping) - len(sample_pred_scores))\n    \n    idx = 0\n    phrase_preds = []\n    while idx < len(offset_mapping):\n        start, _ = offset_mapping[idx]\n        if preds[idx] != \"O\":\n            label = preds[idx][2:]\n        else:\n            label = \"O\"\n        phrase_scores = []\n        phrase_scores.append(sample_pred_scores[idx])\n        idx += 1\n        while idx < len(offset_mapping):\n            if label == \"O\":\n                matching_label = \"O\"\n            else:\n                matching_label = f\"I-{label}\"\n            if preds[idx] == matching_label:\n                _, end = offset_mapping[idx]\n                phrase_scores.append(sample_pred_scores[idx])\n                idx += 1\n            else:\n                break\n        if \"end\" in locals():\n            phrase = sample_text[start:end]\n            phrase_preds.append((phrase, start, end, label, phrase_scores))\n\n    temp_df = []\n    for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n        word_start = len(sample_text[:start].split())\n        word_end = word_start + len(sample_text[start:end].split())\n        word_end = min(word_end, len(sample_text.split()))\n        ps = \" \".join([str(x) for x in range(word_start, word_end)])\n        if label != \"O\":\n            if sum(phrase_scores) / len(phrase_scores) >= proba_thresh[label]:\n                if len(ps.split()) >= min_thresh[label]:\n                    temp_df.append((sample_id, label, ps))\n    \n    temp_df = pd.DataFrame(temp_df, columns=[\"id\", \"class\", \"predictionstring\"])\n    submission.append(temp_df)\n\nsubmission = pd.concat(submission).reset_index(drop=True)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:15:15.786409Z","iopub.execute_input":"2022-03-15T10:15:15.786709Z","iopub.status.idle":"2022-03-15T10:15:15.819258Z","shell.execute_reply.started":"2022-03-15T10:15:15.786677Z","shell.execute_reply":"2022-03-15T10:15:15.818536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T10:15:16.547073Z","iopub.execute_input":"2022-03-15T10:15:16.547626Z","iopub.status.idle":"2022-03-15T10:15:16.558979Z","shell.execute_reply.started":"2022-03-15T10:15:16.54759Z","shell.execute_reply":"2022-03-15T10:15:16.558168Z"},"trusted":true},"execution_count":null,"outputs":[]}]}