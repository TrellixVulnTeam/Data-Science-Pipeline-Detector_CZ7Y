{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Text summarization using PEGASUS\nIntroducing text summarization technique using PEGASUS which is powerful model for abstractive summarization. In PEGASUS, important sentences are removed/masked from an input txt such as BERT and are generated together as one output sequence from the remaining sentences, similar to an extractive summary.\n\nArXiv: https://arxiv.org/abs/1912.08777","metadata":{}},{"cell_type":"markdown","source":"![](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/f4061bd225b3be5b3f5b18eb1a229ce991efefeb/2-Figure1-1.png)","metadata":{}},{"cell_type":"code","source":"!pip install transformers\n!pip install sentencepiece","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"txt_path = \"/kaggle/input/feedback-prize-2021/train/0000D23A521A.txt\"\n\nwith open(txt_path, \"rt\") as f:\n    txt = f.read()\n    \ntxt","metadata":{"execution":{"iopub.status.busy":"2022-01-24T03:50:07.518526Z","iopub.execute_input":"2022-01-24T03:50:07.51912Z","iopub.status.idle":"2022-01-24T03:50:07.53473Z","shell.execute_reply.started":"2022-01-24T03:50:07.519077Z","shell.execute_reply":"2022-01-24T03:50:07.533951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import PegasusForConditionalGeneration, AutoTokenizer\nimport torch\n\n# You can chose models from following list\n# https://huggingface.co/models?sort=downloads&search=google%2Fpegasus\nmodel_name = 'google/pegasus-cnn_dailymail'\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\nbatch = tokenizer(txt, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\ntranslated = model.generate(**batch)\ntgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n\ntgt_text","metadata":{"execution":{"iopub.status.busy":"2022-01-24T03:53:36.572709Z","iopub.execute_input":"2022-01-24T03:53:36.572946Z","iopub.status.idle":"2022-01-24T03:54:00.028233Z","shell.execute_reply.started":"2022-01-24T03:53:36.572909Z","shell.execute_reply":"2022-01-24T03:54:00.02751Z"},"trusted":true},"execution_count":null,"outputs":[]}]}