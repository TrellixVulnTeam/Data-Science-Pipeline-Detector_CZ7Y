{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Evaluating Student Writing\nA competition to give helpful feedback to students!\n\n![img](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.makemyassignments.com%2Fblog%2Fwp-content%2Fuploads%2F2020%2F04%2F1_JrK45nVmcOg2Uvx0nY7gzA.jpeg&f=1&nofb=1)\n\n**This notebook was created during a live coding stream on twitch.**\n\n<img src=\"https://icdn.digitaltrends.com/image/digitaltrends/twitch-logo-720x720.jpg\" alt=\"twitch logo\" width=\"300\"/><img src=\"https://i.postimg.cc/MGdgj1Hq/Screenshot-from-2021-12-17-09-46-56.png\" alt=\"ms stream\" width=\"300\"/>\n\n[Check out past videos here](https://www.twitch.tv/medallionstallion_) and give me a follow to be notified of future streams.\n\nIn this competition we are tasked with giving feedback on argumentative essays written by U.S students in grades 6-12. Specifically, our task is to predict the human annotations.\n\nThis annotation will be done in 2 steps:\n1. Segment each essay into discrete rhetorical and argumentative elements.\n2. Classify each element.\n\nWhere the classification labels are:\n- `Lead` - Introduction\n- `Position` - Opinion\n- `Claim` - something that supports the position\n- `Counterclaim` - A claim that refutes another claim.\n- `Rebuttal` - A claim that refutes a counterclaim\n- `Evidence` - Examples that support a claims, counterclaims, or rebuttals.\n- `Concluding Statement` - Something that restates the claims in conclusion.","metadata":{}},{"cell_type":"code","source":"# Install nb_black for automatic code formatting\n!cp -r ../input/nbblack-code-base/nb_black-1.0.7/nb_black-1.0.7/ ./\n!pip install --user nb_black-1.0.7/ > /dev/null\n%load_ext lab_black","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-25T17:28:29.546771Z","iopub.execute_input":"2021-12-25T17:28:29.54712Z","iopub.status.idle":"2021-12-25T17:29:00.814079Z","shell.execute_reply.started":"2021-12-25T17:28:29.547028Z","shell.execute_reply":"2021-12-25T17:29:00.813237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom glob import glob\nfrom tqdm.notebook import tqdm\n\nfrom itertools import cycle\n\nplt.style.use(\"ggplot\")\ncolor_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ncolor_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])","metadata":{"execution":{"iopub.status.busy":"2021-12-25T17:29:00.815719Z","iopub.execute_input":"2021-12-25T17:29:00.815935Z","iopub.status.idle":"2021-12-25T17:29:01.650838Z","shell.execute_reply.started":"2021-12-25T17:29:00.815908Z","shell.execute_reply":"2021-12-25T17:29:01.650005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Understanding\n\nThe data is provided in two formats:\n- A train.csv with annotation for essays\n- A train.zip (and train folder) with invidual `.txt` files for each essay.\n\nThe dataset is just over 100MB in size.","metadata":{}},{"cell_type":"code","source":"!ls -GFlash --color ../input/feedback-prize-2021/","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:35:20.192709Z","iopub.execute_input":"2021-12-17T15:35:20.192998Z","iopub.status.idle":"2021-12-17T15:35:20.979801Z","shell.execute_reply.started":"2021-12-17T15:35:20.192966Z","shell.execute_reply":"2021-12-17T15:35:20.978698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 15k + Text files are found in the train folder\n!ls -GFlash --color ../input/feedback-prize-2021/train/ | wc -l","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:35:22.223589Z","iopub.execute_input":"2021-12-17T15:35:22.223944Z","iopub.status.idle":"2021-12-17T15:35:23.873105Z","shell.execute_reply.started":"2021-12-17T15:35:22.2239Z","shell.execute_reply":"2021-12-17T15:35:23.872183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading and exploring the data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-15T03:06:43.580678Z","iopub.execute_input":"2021-12-15T03:06:43.581028Z","iopub.status.idle":"2021-12-15T03:06:43.610093Z","shell.execute_reply.started":"2021-12-15T03:06:43.580931Z","shell.execute_reply":"2021-12-15T03:06:43.609351Z"}}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/feedback-prize-2021/train.csv\")\nss = pd.read_csv(\"../input/feedback-prize-2021/sample_submission.csv\")\ntrain_txt = glob(\"../input/feedback-prize-2021/train/*.txt\")\ntest_txt = glob(\"../input/feedback-prize-2021/test/*.txt\")","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:13:42.927Z","iopub.execute_input":"2021-12-17T17:13:42.927582Z","iopub.status.idle":"2021-12-17T17:13:45.050565Z","shell.execute_reply.started":"2021-12-17T17:13:42.927519Z","shell.execute_reply":"2021-12-17T17:13:45.049694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What Does an essay look like\n\nlets print the first txt file `AFEC37C2D43F.txt` to see what an essay looks like. It's an essay about asking for advice on a certain topic or subject.","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:23:04.071558Z","iopub.execute_input":"2021-12-15T03:23:04.072153Z","iopub.status.idle":"2021-12-15T03:23:04.078358Z","shell.execute_reply.started":"2021-12-15T03:23:04.072094Z","shell.execute_reply":"2021-12-15T03:23:04.077603Z"}}},{"cell_type":"code","source":"!cat ../input/feedback-prize-2021/train/AFEC37C2D43F.txt","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:14:02.492274Z","iopub.execute_input":"2021-12-17T17:14:02.492646Z","iopub.status.idle":"2021-12-17T17:14:03.298502Z","shell.execute_reply.started":"2021-12-17T17:14:02.4926Z","shell.execute_reply":"2021-12-17T17:14:03.297421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also look at the raw annotations for this essay from the train.csv file. This essay has 10 annotations. Most of the annotations for discourse type are for `Claim` is second with 6 and `Evidence` with 3.","metadata":{}},{"cell_type":"code","source":"train.query('id == \"AFEC37C2D43F\"')[\"discourse_type\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:14:03.300803Z","iopub.execute_input":"2021-12-17T17:14:03.301072Z","iopub.status.idle":"2021-12-17T17:14:03.333221Z","shell.execute_reply.started":"2021-12-17T17:14:03.301043Z","shell.execute_reply":"2021-12-17T17:14:03.33265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lets print the same example with the text colored by discourse type.\n\nThere are prettier was of doing this but we are simply going to highlight the text differently based on the discourse type","metadata":{}},{"cell_type":"code","source":"def read_essay(id):\n    with open(f\"../input/feedback-prize-2021/train/{id}.txt\") as f:\n        essay = f.read()\n    return essay\n\n\ndef text_to_color(essay, discourse_type, predictionstring):\n    \"\"\"\n    Takes an entire essay, the discourse type and prediction string.\n    Returns highlighted text for the prediction string\n    \"\"\"\n    discourse_color_map = {\n        \"Lead\": 1,  # 1 red\n        \"Position\": 2,  # 2 green\n        \"Evidence\": 3,  # 3 yellow\n        \"Claim\": 4,  # 4 blue\n        \"Concluding Statement\": 5,  # 5 magenta\n        \"Counterclaim\": 6,  # 6 cyan\n        \"Rebuttal\": 7,  # 7 white\n        \"None\": 9,  # default\n    }\n    hcolor = discourse_color_map[discourse_type]\n    text_index = [int(c) for c in predictionstring.split()]\n    text_subset = \" \".join(np.array(essay.split())[text_index])\n    if discourse_type == \"None\":\n        return f\"\\033[4{hcolor};30m{text_subset}\\033[m\"\n    return f\"\\033[4{hcolor};30m{text_subset}\\033[m\"\n\n\ndef get_non_discourse_df(train, essay, id):\n    all_pred_strings = \" \".join(train.query(\"id == @id\")[\"predictionstring\"].values)\n    all_pred_strings = [int(c) for c in all_pred_strings.split()]\n    # [c for c in all_pred_strings\n\n    non_discourse_df = pd.DataFrame(\n        [c for c in range(len(essay.split())) if c not in all_pred_strings]\n    )\n    non_discourse_df.columns = [\"predictionstring\"]\n    non_discourse_df[\"cluster\"] = (\n        non_discourse_df[\"predictionstring\"].diff().fillna(1) > 1\n    ).cumsum()\n\n    non_discourse_strings = []\n    for i, d in non_discourse_df.groupby(\"cluster\"):\n        pred_string = [str(x) for x in d[\"predictionstring\"].values]\n        non_discourse_strings.append(\" \".join(pred_string))\n    df = pd.DataFrame(non_discourse_strings).rename(columns={0: \"predictionstring\"})\n    df[\"discourse_type\"] = \"None\"\n    return df\n\n\ndef get_colored_essay(train, id):\n    essay = read_essay(id)\n    all_text = \"\"\n    train_subset = train.query(\"id == @id\").copy()\n    df = get_non_discourse_df(train, essay, id)\n    train_subset = pd.concat([train_subset, df])\n    train_subset[\"first_index\"] = (\n        train_subset[\"predictionstring\"].str.split(\" \").str[0].astype(\"int\")\n    )\n    train_subset = train_subset.sort_values(\"first_index\").reset_index(drop=True).copy()\n    for i, d in train_subset.iterrows():\n        colored_text = text_to_color(essay, d.discourse_type, d.predictionstring)\n        all_text += \" \" + colored_text\n    return all_text[1:]\n\n\nall_text = get_colored_essay(train, \"AFEC37C2D43F\")\nprint(all_text)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:43:21.451118Z","iopub.execute_input":"2021-12-17T15:43:21.452018Z","iopub.status.idle":"2021-12-17T15:43:21.554969Z","shell.execute_reply.started":"2021-12-17T15:43:21.451951Z","shell.execute_reply":"2021-12-17T15:43:21.553999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What Type of Annotations are Most Common\n- Are there trends to when and where annotations appear in the text?\n- What type of annotations are important?","metadata":{}},{"cell_type":"code","source":"ax = (\n    train[\"discourse_type\"]\n    .value_counts(ascending=True)\n    .plot(kind=\"barh\", figsize=(10, 5), color=color_pal[1])\n)\nax.set_title(\"Discourse Label Frequency (in train)\", fontsize=16)\nax.bar_label(ax.containers[0], label_type=\"edge\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:43:40.638775Z","iopub.execute_input":"2021-12-17T15:43:40.639328Z","iopub.status.idle":"2021-12-17T15:43:40.962742Z","shell.execute_reply.started":"2021-12-17T15:43:40.639279Z","shell.execute_reply":"2021-12-17T15:43:40.961799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some things to note about this next plot.\n- The lead tends to start at the beginning of the document. Evidence ranges in the area it appears commonly in the document.","metadata":{}},{"cell_type":"code","source":"ax = (\n    train.groupby(\"discourse_type\")[[\"discourse_start\", \"discourse_end\"]]\n    .mean()\n    .sort_values(\"discourse_start\")\n    .plot(\n        kind=\"barh\",\n        figsize=(10, 5),\n    )\n)\nax.set_title(\"Average Discourse Label Start and End\", fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:43:43.553665Z","iopub.execute_input":"2021-12-17T15:43:43.554342Z","iopub.status.idle":"2021-12-17T15:43:43.845486Z","shell.execute_reply.started":"2021-12-17T15:43:43.554302Z","shell.execute_reply":"2021-12-17T15:43:43.844575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The length of each label\ntrain[\"discourse_len\"] = (train[\"discourse_end\"] - train[\"discourse_start\"]).astype(\n    \"int\"\n)\n\nfig, ax = plt.subplots(figsize=(12, 5))\nsns.barplot(x=\"discourse_type\", y=\"discourse_len\", data=train)\nax.set_title(\"The Average Lenth of each Discourse\")\nax.set_xlabel(\"Discourse Type\")\nax.set_ylabel(\"Average Text Length\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:43:45.925624Z","iopub.execute_input":"2021-12-17T15:43:45.926365Z","iopub.status.idle":"2021-12-17T15:43:47.488105Z","shell.execute_reply.started":"2021-12-17T15:43:45.92631Z","shell.execute_reply":"2021-12-17T15:43:47.487203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The txt Files.\n- What is the lengnth of each text file? We add info about the essays to training dataset.\n    - The length of the essay\n    - The number of words in each essay.","metadata":{"execution":{"iopub.status.busy":"2021-12-15T03:43:53.62347Z","iopub.execute_input":"2021-12-15T03:43:53.624744Z","iopub.status.idle":"2021-12-15T03:43:53.644226Z","shell.execute_reply.started":"2021-12-15T03:43:53.624664Z","shell.execute_reply":"2021-12-15T03:43:53.643196Z"}}},{"cell_type":"code","source":"len_dict = {}\nword_dict = {}\nfor t in tqdm(train_txt):\n    with open(t, \"r\") as txt_file:\n        myid = t.split(\"/\")[-1].replace(\".txt\", \"\")\n        data = txt_file.read()\n        mylen = len(data.strip())\n        myword = len(data.split())\n        len_dict[myid] = mylen\n        word_dict[myid] = myword\ntrain[\"essay_len\"] = train[\"id\"].map(len_dict)\ntrain[\"essay_words\"] = train[\"id\"].map(word_dict)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:44:12.357443Z","iopub.execute_input":"2021-12-17T15:44:12.357752Z","iopub.status.idle":"2021-12-17T15:45:15.085499Z","shell.execute_reply.started":"2021-12-17T15:44:12.35772Z","shell.execute_reply":"2021-12-17T15:45:15.084725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, most essays end around 6,000 words (I'm guessing that's the max length allowed for the essay)","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10))\ntrain.groupby(\"id\").first().plot(\n    x=\"essay_len\", y=\"essay_words\", kind=\"scatter\", color=color_pal[3], ax=ax\n)\nax.set_title(\"Word vs Character Length per Essay\", fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:45:15.086984Z","iopub.execute_input":"2021-12-17T15:45:15.08729Z","iopub.status.idle":"2021-12-17T15:45:15.551248Z","shell.execute_reply.started":"2021-12-17T15:45:15.087259Z","shell.execute_reply":"2021-12-17T15:45:15.550516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we know the essay lengths. We can see where the discourse labels tend to appear in relationship to the total essay length.","metadata":{}},{"cell_type":"code","source":"train[\"discourse_start_pct\"] = train[\"discourse_start\"] / train[\"essay_len\"]\ntrain[\"discourse_end_pct\"] = train[\"discourse_end\"] / train[\"essay_len\"]\n\nax = (\n    train.groupby(\"discourse_type\")[[\"discourse_start_pct\", \"discourse_end_pct\"]]\n    .mean()\n    .sort_values(\"discourse_start_pct\")\n    .plot(\n        kind=\"barh\",\n        figsize=(10, 5),\n    )\n)\nax.set_title(\"Label Start and End as Percentage of Total Essay\", fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:45:15.552221Z","iopub.execute_input":"2021-12-17T15:45:15.552458Z","iopub.status.idle":"2021-12-17T15:45:15.852054Z","shell.execute_reply.started":"2021-12-17T15:45:15.552425Z","shell.execute_reply":"2021-12-17T15:45:15.851078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wordclouds by Discourse Type","metadata":{}},{"cell_type":"code","source":"plt.style.use('default')\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfig, axs = plt.subplots(7, 1, figsize=(20, 25))\n\nplt_idx = 0\n\nfor discourse_type, d in train.groupby(\"discourse_type\"):\n    discourse_text = \" \".join(d[\"discourse_text\"].values.tolist())\n    wordcloud = WordCloud(\n        max_font_size=200,\n        max_words=200,\n        width=1200,\n        height=800,\n        background_color=\"white\",\n    ).generate(discourse_text)\n    axs = axs.flatten()\n    axs[plt_idx].imshow(wordcloud, interpolation=\"bilinear\")\n    axs[plt_idx].set_title(discourse_type, fontsize=18)\n    axs[plt_idx].axis(\"off\")\n    plt_idx += 1\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T17:24:24.916742Z","iopub.execute_input":"2021-12-17T17:24:24.917017Z","iopub.status.idle":"2021-12-17T17:24:53.446449Z","shell.execute_reply.started":"2021-12-17T17:24:24.916989Z","shell.execute_reply":"2021-12-17T17:24:53.445666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Metric\n\nThe metric is a version of `micro f1` where matches are made between the predicted text and the ground truth when the overlap is >= 0.5.\n\nThe metric page states that this happens in the process:\n- all ground truths and predictions for a given class are compared.\n- overlap between the ground truth and prediction is >= 0.5, and the overlap between the prediction and the ground truth >= 0.5 is a true positive. If multiple matches exist, the match with the highest pair of overlaps is taken.\n- Any unmatched ground truths are false negatives and any unmatched predictions are false positives.\n\n## Example scoring a single label\n- We will purposefully make the label not overlap and show how it will be a false positive.","metadata":{}},{"cell_type":"code","source":"example_pred = (\n    train[[\"id\", \"discourse_type\", \"predictionstring\"]]\n    .query('id == \"423A1CA112E2\"')\n    .copy()\n)\nexample_pred = example_pred.rename(columns={\"discourse_type\": \"class\"})\nexample_gt = (\n    train[[\"id\", \"discourse_type\", \"predictionstring\"]]\n    .query('id == \"423A1CA112E2\"')\n    .copy()\n)\n\n# Make one prediction wrong on purpose\nexample_pred.loc[0, \"predictionstring\"] = \" \".join(\n    example_gt[\"predictionstring\"].values[0].split(\" \")[:10]\n)\nexample_pred.loc[5, \"class\"] = \"Lead\"\n\n# Step 1. all ground truths and predictions for a given class are compared.\njoined_example = example_pred.merge(\n    example_gt,\n    left_on=[\"id\", \"class\"],\n    right_on=[\"id\", \"discourse_type\"],\n    how=\"outer\",\n    suffixes=(\"_pred\", \"_gt\"),\n)\n\nset_pred = set(joined_example[\"predictionstring_pred\"][0].split(\" \"))\nset_gt = set(joined_example[\"predictionstring_gt\"][0].split(\" \"))\n\n# Find the lengths of the sets for this example\nlen_gt = len(set_gt)\nlen_pred = len(set_pred)\ninter = len(set_gt.intersection(set_pred))\noverlap_1 = inter / len_gt\noverlap_2 = inter / len_pred\nprint(len_gt, len_pred, inter)\n# In this example both overlap percentages are not >= 0.5 so it is not a true positive\nprint(overlap_1, overlap_2)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:54:17.136135Z","iopub.execute_input":"2021-12-17T15:54:17.136594Z","iopub.status.idle":"2021-12-17T15:54:17.216335Z","shell.execute_reply.started":"2021-12-17T15:54:17.13654Z","shell.execute_reply":"2021-12-17T15:54:17.215372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Competition Metric Code\n**Note** @cdeotte noted in the comments. The scoring metric for this competition is `macro_f1` and the`score_feedback_comp` function now computes the macro f1 score to follow the evaluation page.","metadata":{}},{"cell_type":"code","source":"def calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(\" \"))\n    set_gt = set(row.predictionstring_gt.split(\" \"))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter / len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp_micro(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n\n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = (\n        gt_df[[\"id\", \"discourse_type\", \"predictionstring\"]]\n        .reset_index(drop=True)\n        .copy()\n    )\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    pred_df[\"pred_id\"] = pred_df.index\n    gt_df[\"gt_id\"] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(\n        gt_df,\n        left_on=[\"id\", \"class\"],\n        right_on=[\"id\", \"discourse_type\"],\n        how=\"outer\",\n        suffixes=(\"_pred\", \"_gt\"),\n    )\n    joined[\"predictionstring_gt\"] = joined[\"predictionstring_gt\"].fillna(\" \")\n    joined[\"predictionstring_pred\"] = joined[\"predictionstring_pred\"].fillna(\" \")\n\n    joined[\"overlaps\"] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined[\"overlap1\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[0])\n    joined[\"overlap2\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[1])\n\n    joined[\"potential_TP\"] = (joined[\"overlap1\"] >= 0.5) & (joined[\"overlap2\"] >= 0.5)\n    joined[\"max_overlap\"] = joined[[\"overlap1\", \"overlap2\"]].max(axis=1)\n    tp_pred_ids = (\n        joined.query(\"potential_TP\")\n        .sort_values(\"max_overlap\", ascending=False)\n        .groupby([\"id\", \"predictionstring_gt\"])\n        .first()[\"pred_id\"]\n        .values\n    )\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined[\"pred_id\"].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query(\"potential_TP\")[\"gt_id\"].unique()\n    unmatched_gt_ids = [c for c in joined[\"gt_id\"].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    # calc microf1\n    my_f1_score = TP / (TP + 0.5 * (FP + FN))\n    return my_f1_score\n\n\ndef score_feedback_comp(pred_df, gt_df, return_class_scores=False):\n    class_scores = {}\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    for discourse_type, gt_subset in gt_df.groupby(\"discourse_type\"):\n        pred_subset = (\n            pred_df.loc[pred_df[\"class\"] == discourse_type]\n            .reset_index(drop=True)\n            .copy()\n        )\n        class_score = score_feedback_comp_micro(pred_subset, gt_subset)\n        class_scores[discourse_type] = class_score\n    f1 = np.mean([v for v in class_scores.values()])\n    if return_class_scores:\n        return f1, class_scores\n    return f1","metadata":{"execution":{"iopub.status.busy":"2021-12-17T16:31:51.61261Z","iopub.execute_input":"2021-12-17T16:31:51.612911Z","iopub.status.idle":"2021-12-17T16:31:51.695451Z","shell.execute_reply.started":"2021-12-17T16:31:51.612881Z","shell.execute_reply":"2021-12-17T16:31:51.694343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ** Note - Duplicate Labels **\nTwice training set that have multiple labels for the same string. This results in a imperfect score when using the ground truth labels as a submission.\nI've printed these two examples below, they occur when words, seperated by commas are given different labels, while the `split()` keeps them as a single string.","metadata":{}},{"cell_type":"code","source":"# Examples where the same `prediction string has two label.\ntrain.loc[train[[\"id\", \"discourse_type\", \"predictionstring\"]].duplicated(keep=False)][\n    [\n        \"id\",\n        \"discourse_id\",\n        \"discourse_start\",\n        \"discourse_end\",\n        \"discourse_text\",\n        \"discourse_type\",\n        \"discourse_type_num\",\n        \"predictionstring\",\n    ]\n]","metadata":{"execution":{"iopub.status.busy":"2021-12-17T16:31:53.71744Z","iopub.execute_input":"2021-12-17T16:31:53.717747Z","iopub.status.idle":"2021-12-17T16:31:53.867907Z","shell.execute_reply.started":"2021-12-17T16:31:53.717714Z","shell.execute_reply":"2021-12-17T16:31:53.867094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scoring the Ground Truth Labels","metadata":{}},{"cell_type":"code","source":"gt_df = train.copy()\npred_df = gt_df.copy()\npred_df = pred_df.rename(columns={\"discourse_type\": \"class\"})\nmicrof1_score = score_feedback_comp_micro(pred_df, gt_df)\nmacrof1_score, class_scores = score_feedback_comp(\n    pred_df, gt_df, return_class_scores=True\n)\n\nprint(\n    f\"Using the ground truth to predict the micro f1 score is {microf1_score:0.4f} and macro f1 score {macrof1_score:0.4f}\"\n)\nprint(\"The individual class scores are:\")\nprint(class_scores)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T16:31:54.406657Z","iopub.execute_input":"2021-12-17T16:31:54.406961Z","iopub.status.idle":"2021-12-17T16:33:11.068859Z","shell.execute_reply.started":"2021-12-17T16:31:54.406926Z","shell.execute_reply":"2021-12-17T16:33:11.067911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline Model\n\nThe model below is very basic and only used as a baseline.\n\nUsing a simple heuristic based on the average length and position of the `Lead` and `Concluding Statement` label lengths in the training set we:\n- Label the first 51 words of every essay as `Lead`\n- Label the last 60 words as the `Concluding` statement.\n\nNote the average lengths of these sections below:","metadata":{}},{"cell_type":"code","source":"train[\"len_predstring\"] = train[\"predictionstring\"].apply(lambda x: len(x.split(\" \")))\ntrain_meta = (\n    train.groupby(\"id\")[\"discourse_type\"]\n    .value_counts()\n    .unstack()\n    .fillna(0)\n    .astype(\"int\")\n)\n(train_meta[\"Lead\"] > 0).mean(), (train_meta[\"Concluding Statement\"] > 0).mean()\n# Predict the average lenth of `Lead` and `Conclusion`\ntrain.groupby(\"discourse_type\")[\"len_predstring\"].mean()","metadata":{"execution":{"iopub.status.busy":"2021-12-17T16:13:19.157731Z","iopub.execute_input":"2021-12-17T16:13:19.158401Z","iopub.status.idle":"2021-12-17T16:13:19.655737Z","shell.execute_reply.started":"2021-12-17T16:13:19.158356Z","shell.execute_reply":"2021-12-17T16:13:19.654814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_baseline_submission(txt_files, lead_padding=51, conclusion_padding=61):\n    lead_pred = {}\n    conc_pred = {}\n\n    for t in tqdm(txt_files):\n        with open(t, \"r\") as txt_file:\n            myid = t.split(\"/\")[-1].replace(\".txt\", \"\")\n            data = txt_file.read()\n        split_data = data.split()\n        n_strings = len(split_data)\n        lead_str = [\" \".join([str(x + 1) for x in range(lead_padding)])]\n        lead_pred[myid] = lead_str\n        conc_str = [\n            \" \".join([str(x) for x in range(n_strings - conclusion_padding, n_strings)])\n        ]\n        conc_pred[myid] = conc_str\n\n    # Create Dataframes from the predicted lead and conclusions\n    lead_df = pd.DataFrame().from_dict(lead_pred, orient=\"index\")\n    conc_df = pd.DataFrame().from_dict(conc_pred, orient=\"index\")\n    lead_df = lead_df.reset_index().rename(\n        columns={\"index\": \"id\", 0: \"predictionstring\"}\n    )\n    conc_df = conc_df.reset_index().rename(\n        columns={\"index\": \"id\", 0: \"predictionstring\"}\n    )\n    lead_df[\"class\"] = \"Lead\"\n    conc_df[\"class\"] = \"Concluding Statement\"\n\n    # Concating our lead and conclusion datasets\n    sub = pd.concat([lead_df, conc_df]).reset_index(drop=True)\n    # Save as csv\n    sub = sub[ss.columns].copy()\n    return sub","metadata":{"execution":{"iopub.status.busy":"2021-12-17T16:15:19.809184Z","iopub.execute_input":"2021-12-17T16:15:19.809549Z","iopub.status.idle":"2021-12-17T16:15:19.855012Z","shell.execute_reply.started":"2021-12-17T16:15:19.809501Z","shell.execute_reply":"2021-12-17T16:15:19.854286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_train = make_baseline_submission(train_txt)\ngt_df = train[[\"id\", \"discourse_type\", \"predictionstring\"]].copy()\npred_df = sub_train.copy()\npred_df = pred_df.rename(columns={\"discourse_type\": \"class\"})\nmicrof1_score = score_feedback_comp_micro(pred_df, gt_df)\nmacrof1_score, class_scores = score_feedback_comp(\n    pred_df, gt_df, return_class_scores=True\n)\n\nprint(\n    f\"The baseline model on the training set has a micro f1 score is {microf1_score:0.4f} and macro f1 score {macrof1_score:0.4f}\"\n)\nprint(\"The individual class scores are:\")\nprint(class_scores)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T16:45:02.182394Z","iopub.execute_input":"2021-12-17T16:45:02.183035Z","iopub.status.idle":"2021-12-17T16:45:32.09272Z","shell.execute_reply.started":"2021-12-17T16:45:02.182988Z","shell.execute_reply":"2021-12-17T16:45:32.091678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict on Test Set","metadata":{}},{"cell_type":"code","source":"sub = make_baseline_submission(test_txt)\nsub[ss.columns].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T16:17:04.807188Z","iopub.execute_input":"2021-12-17T16:17:04.808043Z","iopub.status.idle":"2021-12-17T16:17:13.023074Z","shell.execute_reply.started":"2021-12-17T16:17:04.807984Z","shell.execute_reply":"2021-12-17T16:17:13.022279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# END","metadata":{}}]}