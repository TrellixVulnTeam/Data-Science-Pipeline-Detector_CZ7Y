{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ADD-ON Dataset\n","metadata":{}},{"cell_type":"markdown","source":"**tez_lib**\n\nhttps://www.kaggle.com/abhishek/tez-lib","metadata":{}},{"cell_type":"markdown","source":"**longformerlarge4096**\n\nhttps://www.kaggle.com/hengzheng/longformerlarge4096","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/tez-lib/\")\nimport tez","metadata":{"execution":{"iopub.status.busy":"2022-03-15T19:48:16.141055Z","iopub.execute_input":"2022-03-15T19:48:16.141302Z","iopub.status.idle":"2022-03-15T19:48:18.215082Z","shell.execute_reply.started":"2022-03-15T19:48:16.14127Z","shell.execute_reply":"2022-03-15T19:48:18.214328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.enable()\n\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport tez\nimport torch\nimport torch.nn as nn\nfrom joblib import Parallel, delayed\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-03-15T19:48:18.2172Z","iopub.execute_input":"2022-03-15T19:48:18.217588Z","iopub.status.idle":"2022-03-15T19:48:18.644213Z","shell.execute_reply.started":"2022-03-15T19:48:18.217554Z","shell.execute_reply":"2022-03-15T19:48:18.643465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"class config:\n    is_amp=True\n    discourse_marker_to_label = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 2,\n    \"I-Position\": 3,\n    \"B-Evidence\": 4,\n    \"I-Evidence\": 5,\n    \"B-Claim\": 6,\n    \"I-Claim\": 7,\n    \"B-Concluding Statement\": 8,\n    \"I-Concluding Statement\": 9,\n    \"B-Counterclaim\": 10,\n    \"I-Counterclaim\": 11,\n    \"B-Rebuttal\": 12,\n    \"I-Rebuttal\": 13,\n    \"O\": 14,\n    \"PAD\": -100,\n    }\n    label_to_discourse_marker = {}\n    num_discourse_marker = 15\n    \ntarget_id_map = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 2,\n    \"I-Position\": 3,\n    \"B-Evidence\": 4,\n    \"I-Evidence\": 5,\n    \"B-Claim\": 6,\n    \"I-Claim\": 7,\n    \"B-Concluding Statement\": 8,\n    \"I-Concluding Statement\": 9,\n    \"B-Counterclaim\": 10,\n    \"I-Counterclaim\": 11,\n    \"B-Rebuttal\": 12,\n    \"I-Rebuttal\": 13,\n    \"O\": 14,\n    \"PAD\": -100,\n}\n\nid_target_map = {v: k for k, v in target_id_map.items()}\n\n# https://www.kaggle.com/kausheeki01/feedback-prize-2022-pytorch-better-parameters\nproba_thresh = {\n    \"Lead\": 0.687,\n    \"Position\": 0.537,\n    \"Evidence\": 0.637,\n    \"Claim\": 0.537,\n    \"Concluding Statement\": 0.687,\n    \"Counterclaim\": 0.537,\n    \"Rebuttal\": 0.537,\n}\n# To remove the data which is less than 2% in word length\n# https://www.kaggle.com/phanttan/ner-longformer\nmin_thresh = {\n    \"Lead\": 9,\n    \"Position\": 5,\n    \"Evidence\": 14,\n    \"Claim\": 3,\n    \"Concluding Statement\": 11,\n    \"Counterclaim\": 6,\n    \"Rebuttal\": 4,\n}\nclass args1:\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/longformerlarge4096/longformer-large-4096/\"\n    tez_model= \"../input/fblongformerlarge1536/\"\n    output = \".\"\n    batch_size = 8\n    max_len = 4096\n    \nclass args2:\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/longformerlarge4096/longformer-large-4096/\"\n    tez_model= \"../input/tez-fb-large/\"\n    output = \".\"\n    batch_size = 8\n    max_len = 4096","metadata":{"execution":{"iopub.status.busy":"2022-03-15T19:48:18.645524Z","iopub.execute_input":"2022-03-15T19:48:18.645897Z","iopub.status.idle":"2022-03-15T19:48:18.660929Z","shell.execute_reply.started":"2022-03-15T19:48:18.645861Z","shell.execute_reply":"2022-03-15T19:48:18.660301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper Functions","metadata":{}},{"cell_type":"code","source":"def time_to_str(t, mode='min'):\n    # t in second\n    if mode=='min':\n        t = int(t)/60\n        minute = t%60\n        hr = t//60\n        return f'%2d hr %02d minute'%(hr, minute)\n    elif mode=='second':\n        second = int(t)\n        minute = second//60\n        return f'%2d min %02d sec'%(min,sec)\n    else:\n        raise NotImplementedError","metadata":{"execution":{"iopub.status.busy":"2022-03-15T19:48:18.665264Z","iopub.execute_input":"2022-03-15T19:48:18.6665Z","iopub.status.idle":"2022-03-15T19:48:18.676241Z","shell.execute_reply.started":"2022-03-15T19:48:18.666462Z","shell.execute_reply":"2022-03-15T19:48:18.675508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackDataset:\n    def __init__(self, samples, max_len, tokenizer):\n        self.samples = samples\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.length = len(samples)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        input_ids = self.samples[idx][\"input_ids\"]\n\n        # add start token id to the input_ids\n        input_ids = [self.tokenizer.cls_token_id] + input_ids\n\n        if len(input_ids) > self.max_len - 1:\n            input_ids = input_ids[: self.max_len - 1]\n\n        # add end token id to the input_ids\n        input_ids = input_ids + [self.tokenizer.sep_token_id]\n        attention_mask = [1] * len(input_ids)\n\n        return {\n            \"ids\": input_ids,\n            \"mask\": attention_mask,\n        }","metadata":{"execution":{"iopub.status.busy":"2022-03-15T19:48:18.679441Z","iopub.execute_input":"2022-03-15T19:48:18.680858Z","iopub.status.idle":"2022-03-15T19:48:18.693787Z","shell.execute_reply.started":"2022-03-15T19:48:18.680821Z","shell.execute_reply":"2022-03-15T19:48:18.692996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Collate Function","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/bacicnikola/sequence-bucketing-pytorch-implementation\nclass Collate:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"ids\"]])\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n        else:\n            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n\n        # convert to tensors\n        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-03-15T19:48:18.698246Z","iopub.execute_input":"2022-03-15T19:48:18.702853Z","iopub.status.idle":"2022-03-15T19:48:18.71629Z","shell.execute_reply.started":"2022-03-15T19:48:18.70282Z","shell.execute_reply":"2022-03-15T19:48:18.715473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/kausheeki01/feedback-prize-2022-pytorch-better-parameters\nclass FeedbackModel(tez.Model):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n        config = AutoConfig.from_pretrained(model_name)\n\n        hidden_dropout_prob: float = 0.22\n        layer_norm_eps: float = 17589e-7\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = AutoModel.from_config(config)\n        self.output = nn.Linear(config.hidden_size, self.num_labels)\n\n    def forward(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        logits = self.output(sequence_output)\n        logits = torch.softmax(logits, dim=-1)\n        return logits, 0, {}","metadata":{"execution":{"iopub.status.busy":"2022-03-15T19:48:18.720656Z","iopub.execute_input":"2022-03-15T19:48:18.723073Z","iopub.status.idle":"2022-03-15T19:48:18.73622Z","shell.execute_reply.started":"2022-03-15T19:48:18.722985Z","shell.execute_reply":"2022-03-15T19:48:18.735407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def word_probability_to_predict_df(text_to_word_probability, id):\n    return","metadata":{"execution":{"iopub.status.busy":"2022-03-15T19:48:18.740751Z","iopub.execute_input":"2022-03-15T19:48:18.740998Z","iopub.status.idle":"2022-03-15T19:48:18.747947Z","shell.execute_reply.started":"2022-03-15T19:48:18.740968Z","shell.execute_reply":"2022-03-15T19:48:18.747084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def do_threshold(submit_df, use=['length', 'probability']):\n    return","metadata":{"execution":{"iopub.status.busy":"2022-03-15T19:48:18.751949Z","iopub.execute_input":"2022-03-15T19:48:18.752188Z","iopub.status.idle":"2022-03-15T19:48:18.7603Z","shell.execute_reply.started":"2022-03-15T19:48:18.752158Z","shell.execute_reply":"2022-03-15T19:48:18.759563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/kaggleqrdl/tensorflow-longformer-ner-postprocessing\ndef jn(pst, start, end):\n    return \" \".join([str(x) for x in pst[start:end]])\ndef link_evidence(oof):\n    thresh = 1\n    idu = oof['id'].unique()\n    idc = idu[1]\n    eoof = oof[oof['class'] == \"Evidence\"]\n    neoof = oof[oof['class'] != \"Evidence\"]\n    thresh2 = 26 # Need the explaination\n    retval = []\n    for idv in idu:\n        for c in  ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n               'Counterclaim', 'Rebuttal']:\n            q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n            if len(q) == 0:\n                continue\n            pst = []\n            for i,r in q.iterrows():\n                pst = pst +[-1] + [int(x) for x in r['predictionstring'].split()]\n            start = 1\n            end = 1\n            for i in range(2,len(pst)):\n                cur = pst[i]\n                end = i\n                #if pst[start] == 205:\n                #   print(cur, pst[start], cur - pst[start])\n                if (cur == -1 and c != 'Evidence') or ((cur == -1) and ((pst[i+1] > pst[end-1] + thresh) or (pst[i+1] - pst[start] > thresh2))):\n                    retval.append((idv, c, jn(pst, start, end)))\n                    start = i + 1\n            v = (idv, c, jn(pst, start, end+1))\n            retval.append(v)\n    roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring']) \n    roof = roof.merge(neoof, how='outer')\n    return roof","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:12:52.204413Z","iopub.execute_input":"2022-03-15T20:12:52.204766Z","iopub.status.idle":"2022-03-15T20:12:52.217072Z","shell.execute_reply.started":"2022-03-15T20:12:52.204724Z","shell.execute_reply":"2022-03-15T20:12:52.21636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_overlap(predict, truth):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    # Length of each and intersection\n    try:\n        len_truth   = len(truth)\n        len_predict = len(predict)\n        intersect = len(truth & predict)\n        overlap1 = intersect/ len_truth\n        overlap2 = intersect/ len_predict\n        return (overlap1, overlap2)\n    except:  # at least one of the input is NaN\n        return (0, 0)\n\ndef compute_f1_score_one(predict_df, truth_df, discourse_type):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n\n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    t_df = truth_df.loc[truth_df['discourse_type'] == discourse_type,   ['id', 'predictionstring']].reset_index(drop=True)\n    p_df = predict_df.loc[predict_df['class'] == discourse_type,  ['id', 'predictionstring']].reset_index(drop=True)\n\n    p_df.loc[:,'predict_id'] = p_df.index\n    t_df.loc[:,'truth_id'] = t_df.index\n    p_df.loc[:,'predictionstring'] = [set(p.split(' ')) for p in p_df['predictionstring']]\n    t_df.loc[:,'predictionstring'] = [set(p.split(' ')) for p in t_df['predictionstring']]\n\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = p_df.merge(t_df,\n                           left_on='id',\n                           right_on='id',\n                           how='outer',\n                           suffixes=('_p','_t')\n                          )\n    overlap = [compute_overlap(*predictionstring) for predictionstring in zip(joined.predictionstring_p, joined.predictionstring_t)]\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['potential_TP'] = [(o[0] >= 0.5 and o[1] >= 0.5) for o in overlap]\n    joined['max_overlap' ] = [max(*o) for o in overlap]\n    joined_tp = joined.query('potential_TP').reset_index(drop=True)\n    tp_pred_ids = joined_tp\\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','truth_id'])['predict_id'].first()\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = set(joined['predict_id'].unique()) - set(tp_pred_ids)\n\n    matched_gt_ids   = joined_tp['truth_id'].unique()\n    unmatched_gt_ids = set(joined['truth_id'].unique()) -  set(matched_gt_ids)\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    f1 = TP / (TP + 0.5*(FP+FN))\n    return f1\n\ndef compute_lb_f1_score(predict_df, truth_df):\n    f1_score = {}\n    for discourse_type in truth_df.discourse_type.unique():\n        f1_score[discourse_type] = compute_f1_score_one(predict_df, truth_df, discourse_type)\n    #f1 = np.mean([v for v in class_scores.values()])\n    return f1_score","metadata":{"execution":{"iopub.status.busy":"2022-03-15T19:48:18.784085Z","iopub.execute_input":"2022-03-15T19:48:18.784581Z","iopub.status.idle":"2022-03-15T19:48:18.806313Z","shell.execute_reply.started":"2022-03-15T19:48:18.784547Z","shell.execute_reply":"2022-03-15T19:48:18.805612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## prepare_test_data","metadata":{}},{"cell_type":"code","source":"def _prepare_test_data_helper(args, tokenizer, ids):\n    test_samples = []\n    for idx in ids:\n        filename = os.path.join(args.input_path, \"test\", idx + \".txt\")\n        with open(filename, \"r\") as f:\n            text = f.read()\n\n        encoded_text = tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        offset_mapping = encoded_text[\"offset_mapping\"]\n\n        sample = {\n            \"id\": idx,\n            \"input_ids\": input_ids,\n            \"text\": text,\n            \"offset_mapping\": offset_mapping,\n        }\n\n        test_samples.append(sample)\n    return test_samples\n\n\ndef prepare_test_data(df, tokenizer, args):\n    test_samples = []\n    ids = df[\"id\"].unique()\n    ids_splits = np.array_split(ids, 4)\n\n    results = Parallel(n_jobs=4, backend=\"multiprocessing\")(\n        delayed(_prepare_test_data_helper)(args, tokenizer, idx) for idx in ids_splits\n    )\n    for result in results:\n        test_samples.extend(result)\n\n    return test_samples","metadata":{"execution":{"iopub.status.busy":"2022-03-15T19:48:18.808162Z","iopub.execute_input":"2022-03-15T19:48:18.808736Z","iopub.status.idle":"2022-03-15T19:48:18.820271Z","shell.execute_reply.started":"2022-03-15T19:48:18.80869Z","shell.execute_reply":"2022-03-15T19:48:18.819581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inferring","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(os.path.join(\"../input/feedback-prize-2021/\", \"sample_submission.csv\"))\ndf_ids = df_test[\"id\"].unique()\n\ntokenizer = AutoTokenizer.from_pretrained(args1.model)\ntest_samples = prepare_test_data(df_test, tokenizer, args1)\ncollate = Collate(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T19:48:18.821813Z","iopub.execute_input":"2022-03-15T19:48:18.822293Z","iopub.status.idle":"2022-03-15T19:48:20.877331Z","shell.execute_reply.started":"2022-03-15T19:48:18.822258Z","shell.execute_reply":"2022-03-15T19:48:20.876367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_preds = []\nfor fold_ in range(10):\n    current_idx = 0\n    test_dataset = FeedbackDataset(test_samples, args1.max_len, tokenizer)\n    \n    if fold_ < 5:\n        model = FeedbackModel(model_name=args1.model, num_labels=len(target_id_map) - 1)\n        model.load(os.path.join(args1.tez_model, f\"model_{fold_}.bin\"), weights_only=True)\n        preds_iter = model.predict(test_dataset, batch_size=args1.batch_size, n_jobs=-1, collate_fn=collate)\n    else:\n        model = FeedbackModel(model_name=args2.model, num_labels=len(target_id_map) - 1)\n        model.load(os.path.join(args2.tez_model, f\"model_{fold_-5}.bin\"), weights_only=True)\n        preds_iter = model.predict(test_dataset, batch_size=args2.batch_size, n_jobs=-1, collate_fn=collate)\n        \n    current_idx = 0\n    \n    for preds in preds_iter:\n        preds = preds.astype(np.float16)\n        preds = preds / 10\n        if fold_ == 0:\n            raw_preds.append(preds)\n        else:\n            raw_preds[current_idx] += preds\n            current_idx += 1\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T19:48:20.878867Z","iopub.execute_input":"2022-03-15T19:48:20.879124Z","iopub.status.idle":"2022-03-15T19:52:28.487395Z","shell.execute_reply.started":"2022-03-15T19:48:20.879087Z","shell.execute_reply":"2022-03-15T19:52:28.486681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = []\nfinal_scores = []\n\nfor rp in raw_preds:\n    pred_class = np.argmax(rp, axis=2)\n    pred_scrs = np.max(rp, axis=2)\n    for pred, pred_scr in zip(pred_class, pred_scrs):\n        pred = pred.tolist()\n        pred_scr = pred_scr.tolist()\n        final_preds.append(pred)\n        final_scores.append(pred_scr)\n\nfor j in range(len(test_samples)):\n    tt = [id_target_map[p] for p in final_preds[j][1:]]\n    tt_score = final_scores[j][1:]\n    test_samples[j][\"preds\"] = tt\n    test_samples[j][\"pred_scores\"] = tt_score","metadata":{"execution":{"iopub.status.busy":"2022-03-15T19:52:28.489014Z","iopub.execute_input":"2022-03-15T19:52:28.489265Z","iopub.status.idle":"2022-03-15T19:52:28.500741Z","shell.execute_reply.started":"2022-03-15T19:52:28.48923Z","shell.execute_reply":"2022-03-15T19:52:28.499916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"submissions = []\nfor sample_idx, sample in enumerate(test_samples):\n    preds = sample['preds']\n    offset_mapping = sample[\"offset_mapping\"]\n    sample_id = sample[\"id\"]\n    sample_text = sample[\"text\"]\n    sample_input_ids = sample[\"input_ids\"]\n    sample_pred_scores = sample[\"pred_scores\"]\n    sample_preds = []\n\n    if len(preds) < len(offset_mapping):\n        preds = preds + [\"O\"] * (len(offset_mapping) - len(preds))\n        sample_pred_scores = sample_pred_scores + [0] * (len(offset_mapping) - len(sample_pred_scores))\n    \n    idx = 0\n    phrase_preds = []\n    while idx < len(offset_mapping):\n        start, _ = offset_mapping[idx]\n        if preds[idx] != \"O\":\n            label = preds[idx][2:]\n        else:\n            label = \"O\"\n        phrase_scores = []\n        phrase_scores.append(sample_pred_scores[idx])\n        idx += 1\n        while idx < len(offset_mapping):\n            if label == \"O\":\n                matching_label = \"O\"\n            else:\n                matching_label = f\"I-{label}\"\n            if preds[idx] == matching_label:\n                _, end = offset_mapping[idx]\n                phrase_scores.append(sample_pred_scores[idx])\n                idx += 1\n            else:\n                break\n        if \"end\" in locals():\n            phrase = sample_text[start:end]\n            phrase_preds.append((phrase, start, end, label, phrase_scores))\n    temp_df = []\n    for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n        word_start = len(sample_text[:start].split())\n        word_end = word_start + len(sample_text[start:end].split())\n        word_end = min(word_end, len(sample_text.split()))\n        ps = \" \".join([str(x) for x in range(word_start, word_end)])\n        if label != \"O\":\n            if sum(phrase_scores) / len(phrase_scores) >= proba_thresh[label]:\n                if len(ps.split()) >= min_thresh[label]:\n                    temp_df.append((sample_id, label, ps))\n    \n    temp_df = pd.DataFrame(temp_df, columns=[\"id\", \"class\", \"predictionstring\"])\n    submissions.append(temp_df)\n\nsubmissions = pd.concat(submissions).reset_index(drop=True)\nsubmissions = link_evidence(submissions)\nsubmissions.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:16:02.3249Z","iopub.execute_input":"2022-03-15T20:16:02.325377Z","iopub.status.idle":"2022-03-15T20:16:02.473523Z","shell.execute_reply.started":"2022-03-15T20:16:02.325331Z","shell.execute_reply":"2022-03-15T20:16:02.472582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submissions.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:16:19.938116Z","iopub.execute_input":"2022-03-15T20:16:19.938729Z","iopub.status.idle":"2022-03-15T20:16:19.956728Z","shell.execute_reply.started":"2022-03-15T20:16:19.938692Z","shell.execute_reply":"2022-03-15T20:16:19.955879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reference\n\nhttps://www.kaggle.com/librauee/infer-fast-ensemble-models/\n\nhttps://www.kaggle.com/kausheeki01/feedback-prize-2022-pytorch-better-parameters/notebook","metadata":{}}]}