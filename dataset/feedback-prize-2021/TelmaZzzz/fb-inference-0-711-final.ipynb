{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The following cell is only necessary if you want to use the fast tokenizer for deberta-v2","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\nimport shutil\n\ninput_dir = Path(\"../input/debertav2xlfasttokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-15T14:43:52.20833Z","iopub.execute_input":"2022-03-15T14:43:52.208717Z","iopub.status.idle":"2022-03-15T14:43:52.234391Z","shell.execute_reply.started":"2022-03-15T14:43:52.208682Z","shell.execute_reply":"2022-03-15T14:43:52.233711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_DebertaV2TokenizerFast(model_name):        \n    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n    return DebertaV2TokenizerFast.from_pretrained(model_name)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:43:52.236521Z","iopub.execute_input":"2022-03-15T14:43:52.236788Z","iopub.status.idle":"2022-03-15T14:43:52.240801Z","shell.execute_reply.started":"2022-03-15T14:43:52.236754Z","shell.execute_reply":"2022-03-15T14:43:52.239997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nimport math\nfrom pathlib import Path\nimport gc\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport copy\nimport torch.cuda.amp as AMP\nfrom transformers import (\n    AutoModel,\n    AutoTokenizer,\n    AutoConfig,\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:43:52.242358Z","iopub.execute_input":"2022-03-15T14:43:52.242632Z","iopub.status.idle":"2022-03-15T14:43:52.250402Z","shell.execute_reply.started":"2022-03-15T14:43:52.242599Z","shell.execute_reply":"2022-03-15T14:43:52.249731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_id_map = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 2,\n    \"I-Position\": 3,\n    \"B-Evidence\": 4,\n    \"I-Evidence\": 5,\n    \"B-Claim\": 6,\n    \"I-Claim\": 7,\n    \"B-Concluding Statement\": 8,\n    \"I-Concluding Statement\": 9,\n    \"B-Counterclaim\": 10,\n    \"I-Counterclaim\": 11,\n    \"B-Rebuttal\": 12,\n    \"I-Rebuttal\": 13,\n    \"O\": 14,\n    \"PAD\": -100,\n}\nid_target_map = {v: k for k, v in target_id_map.items()}","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:43:52.251939Z","iopub.execute_input":"2022-03-15T14:43:52.252265Z","iopub.status.idle":"2022-03-15T14:43:52.259696Z","shell.execute_reply.started":"2022-03-15T14:43:52.252229Z","shell.execute_reply":"2022-03-15T14:43:52.259002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackDatasetValid:\n    def __init__(self, samples, max_len, tokenizer):\n        self.samples = samples\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.length = len(samples)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        input_ids = self.samples[idx][\"input_ids\"]\n        input_ids = [self.tokenizer.cls_token_id] + input_ids\n\n        if len(input_ids) > self.max_len - 1:\n            input_ids = input_ids[: self.max_len - 1]\n\n        # add end token id to the input_ids\n        input_ids = input_ids + [self.tokenizer.sep_token_id]\n        attention_mask = [1] * len(input_ids)\n\n        return {\n            \"ids\": input_ids,\n            \"mask\": attention_mask,\n        }\n\n\nclass Collate:\n    def __init__(self, tokenizer, fix_length=-1):\n        self.tokenizer = tokenizer\n        self.fix_length = fix_length\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"ids\"]])\n        if self.fix_length != -1:\n            batch_max = min(batch_max, self.fix_length)\n#             batch_max = self.fix_length\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n        else:\n            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n\n        # convert to tensors\n        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n\n        return output\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-15T14:43:52.261947Z","iopub.execute_input":"2022-03-15T14:43:52.262239Z","iopub.status.idle":"2022-03-15T14:43:52.276995Z","shell.execute_reply.started":"2022-03-15T14:43:52.262203Z","shell.execute_reply":"2022-03-15T14:43:52.276216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainerConfig(object):\n    def __init__(self):\n        self.model_load = None\n        self.model_name = None\n        self.valid_batch_size = 4\n        self.fix_length = 2048\n\n\nclass ModelConfig(object):\n    def __init__(self):\n        self.pretrain_path = None\n        self.hidden_dropout_prob = 0.1\n        self.layer_norm_eps = 1e-7\n        self.num_labels = 15\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n        self.dropout = 0.1","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:43:52.278684Z","iopub.execute_input":"2022-03-15T14:43:52.278983Z","iopub.status.idle":"2022-03-15T14:43:52.287588Z","shell.execute_reply.started":"2022-03-15T14:43:52.278902Z","shell.execute_reply":"2022-03-15T14:43:52.286864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Predicter(object):\n    def __init__(self, args):\n        self.trainer_config = TrainerConfig()\n        self.model_config = ModelConfig()\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n    def set_pretrain(self, pretrain_path):\n        self.model_config.pretrain_path = pretrain_path\n    \n    def build_model(self):\n        self.model = FeedBackModel(self.model_config)\n    \n    def model_init(self):\n        self.build_model()\n        if self.trainer_config.model_load:\n            self.model.load_state_dict(torch.load(self.trainer_config.model_load, map_location=torch.device('cpu')))\n        self.model.to(self.device)\n        self.model.eval()\n    \n    def get_logits(self, batch, return_loss=False):\n        input_ids = batch[\"ids\"].to(self.device)\n        attention_mask = batch[\"mask\"].to(self.device)\n        if return_loss:\n            labels = batch[\"targets\"].to(self.device)\n            logits, loss = self.model(input_ids, attention_mask, labels=labels)\n            return logits, loss\n        else:\n            logits, _ = self.model(input_ids, attention_mask)\n            return logits\n\n    def model_load(self, path):\n        self.model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n        self.model.to(self.device)\n        self.model.eval()\n        \n    @torch.no_grad()\n    def predict(self, valid_datasets, collate):\n        self.model.eval()\n        valid_iter = torch.utils.data.DataLoader(valid_datasets, batch_size=self.trainer_config.valid_batch_size, collate_fn=collate)\n        preds = []\n        PAD = torch.tensor([0.0] * 14 + [0.1], dtype=torch.float).unsqueeze(0)\n        for batch in tqdm(valid_iter):\n            with AMP.autocast(enabled=True):\n                pred = self.get_logits(batch).cpu()\n                bs, length, dim = pred.shape\n                batch_pad = torch.cat([PAD] * bs, dim=0).unsqueeze(1)\n                pred = torch.cat([pred] + [batch_pad] * (self.trainer_config.fix_length - length), dim=1)\n                preds.append((pred*255).byte().data.cpu().numpy())\n        del valid_iter\n        gc.collect()\n        return preds","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:43:52.28875Z","iopub.execute_input":"2022-03-15T14:43:52.289249Z","iopub.status.idle":"2022-03-15T14:43:52.305428Z","shell.execute_reply.started":"2022-03-15T14:43:52.289213Z","shell.execute_reply":"2022-03-15T14:43:52.304767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedBackModel(nn.Module):\n    def __init__(self, args):\n        super(FeedBackModel, self).__init__()\n        config = AutoConfig.from_pretrained(args.pretrain_path)\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": args.hidden_dropout_prob,\n                \"layer_norm_eps\": args.layer_norm_eps,\n                \"add_pooling_layer\": False,\n                \"num_labels\": args.num_labels,\n            }\n        )\n        self.num_labels = args.num_labels\n        self.transformer = AutoModel.from_config(config)\n        self.dropout = nn.Dropout(args.dropout)\n        self.output = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):\n        if token_type_ids:\n            transformer_out = self.transformer(input_ids, attention_mask, token_type_ids)\n        else:\n            transformer_out = self.transformer(input_ids, attention_mask)\n        sequence_output = transformer_out.last_hidden_state\n        sequence_output = self.dropout(sequence_output)\n        logits = self.output(sequence_output)\n        logits_out = torch.softmax(logits, dim=-1)\n        loss = 0\n        return logits_out, loss","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:43:52.458534Z","iopub.execute_input":"2022-03-15T14:43:52.458721Z","iopub.status.idle":"2022-03-15T14:43:52.466816Z","shell.execute_reply.started":"2022-03-15T14:43:52.458699Z","shell.execute_reply":"2022-03-15T14:43:52.46596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from joblib import Parallel, delayed\n\n\ndef _prepare_test_data_helper(tokenizer, ids):\n    test_samples = []\n    for idx in ids:\n        filename = os.path.join(\"../input/feedback-prize-2021\", \"test\", idx + \".txt\")\n        with open(filename, \"r\") as f:\n            text = f.read()\n\n        encoded_text = tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        offset_mapping = encoded_text[\"offset_mapping\"]\n\n        sample = {\n            \"id\": idx,\n            \"input_ids\": input_ids,\n            \"text\": text,\n            \"offset_mapping\": offset_mapping,\n        }\n\n        test_samples.append(sample)\n    return test_samples\n\n\ndef prepare_test_data(df, tokenizer):\n    test_samples = []\n    ids = df[\"id\"].unique()\n    ids_splits = np.array_split(ids, 4)\n\n    results = Parallel(n_jobs=4, backend=\"multiprocessing\")(\n        delayed(_prepare_test_data_helper)(tokenizer, idx) for idx in ids_splits\n    )\n    for result in results:\n        test_samples.extend(result)\n\n    return test_samples\n\n\ndef prepare_test_data_v2(df, tokenizer):\n    test_samples = []\n    ids = df[\"id\"].unique()\n    for idx in ids:\n        filename = os.path.join(\"../input/feedback-prize-2021\", \"test\", idx + \".txt\")\n        with open(filename, \"r\") as f:\n            text = f.read()\n\n        encoded_text = tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        offset_mapping = encoded_text[\"offset_mapping\"]\n\n        sample = {\n            \"id\": idx,\n            \"input_ids\": input_ids,\n            \"text\": text,\n            \"offset_mapping\": offset_mapping,\n        }\n\n        test_samples.append(sample)\n    return test_samples","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:43:52.468617Z","iopub.execute_input":"2022-03-15T14:43:52.468871Z","iopub.status.idle":"2022-03-15T14:43:52.482878Z","shell.execute_reply.started":"2022-03-15T14:43:52.468837Z","shell.execute_reply":"2022-03-15T14:43:52.481568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_to_word(text):\n    word = text.split()\n    word_offset = []\n\n    start = 0\n    for w in word:\n        r = text[start:].find(w)\n\n        if r==-1:\n            raise NotImplementedError\n        else:\n            start = start+r\n            end   = start+len(w)\n            word_offset.append((start,end))\n            #print('%32s'%w, '%5d'%start, '%5d'%r, text[start:end])\n        start = end\n\n    return word, word_offset\n\n\ndef word_probability_to_predict_df(text_to_word_probability, id):\n    len_word = len(text_to_word_probability)\n    word_predict = text_to_word_probability.argmax(-1)\n    word_score   = text_to_word_probability.max(-1)\n    #########################################################\n#     for i in range(len_word):\n#         if text_to_word_probability[i][10] >= 0.40:\n#             word_predict[i] = 10\n#             word_score[i] = text_to_word_probability[i][10]\n#         if text_to_word_probability[i][11] >= 0.40:\n#             word_predict[i] = 11\n#             word_score[i] = text_to_word_probability[i][11]\n#         if text_to_word_probability[i][12] >= 0.40:\n#             word_predict[i] = 12\n#             word_score[i] = text_to_word_probability[i][12]\n#         if text_to_word_probability[i][13] >= 0.40:\n#             word_predict[i] = 13\n#             word_score[i] = text_to_word_probability[i][13]\n    #########################################################\n    predict_df = []\n\n    t = 0\n    # logging.debug(target_id_map)\n    while 1:\n        if word_predict[t] not in [\n            target_id_map['O'],\n            target_id_map['PAD'],\n        ]:\n            start = t\n            b_marker_label = word_predict[t]\n        else:\n            t = t+1\n            if t== len_word-1: break\n            continue\n\n        t = t+1\n        if t== len_word-1: break\n\n        #----\n        if id_target_map[b_marker_label][0]=='B':\n            i_marker_label = b_marker_label+1\n        elif id_target_map[b_marker_label][0]=='I':\n            i_marker_label = b_marker_label\n        else:\n            raise NotImplementedError\n\n        while 1:\n            #print(t)\n            if (word_predict[t] != i_marker_label) or (t ==len_word-1):\n                end = t\n                prediction_string = ' '.join([str(i) for i in range(start,end)]) #np.arange(start,end).tolist()\n                discourse_type = id_target_map[b_marker_label][2:]\n                discourse_score = word_score[start:end].tolist()\n                predict_df.append((id, discourse_type, prediction_string, str(discourse_score)))\n                #print(predict_df[-1])\n                break\n            else:\n                t = t+1\n                continue\n        if t== len_word-1: break\n\n    predict_df = pd.DataFrame(predict_df, columns=['id', 'class', 'predictionstring', 'score'])\n    return predict_df\n\ndef word_probability_to_prediction_string(text_to_word_probability, text_id, word):\n    # print(11)\n    length_threshold = {\n        \"Lead\": 9,\n        \"Position\": 5,\n        \"Evidence\": 14,\n        \"Claim\": 3,\n        \"Concluding Statement\": 11,\n        \"Counterclaim\": 6,\n        \"Rebuttal\": 4,\n    }\n    word_predict = text_to_word_probability.argmax(-1)\n    word_score = text_to_word_probability.max(-1)\n    predict_df = []\n\n    t = 0\n    while 1:\n        # if word_predict[t] in [1,3,5,7,9,11,13]:\n        if t == len(word):\n            break\n\n        if word_predict[t] in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]:\n            start = t\n            b_marker_label = word_predict[t]\n        else:\n            t = t + 1\n            if t == len(word) - 1:\n                break\n            continue\n\n        t = t + 1\n        if t == len(word) - 1:\n            break\n\n        # i_marker_label = b_marker_label+1\n        i_marker_label = [b_marker_label +\n                          1] if b_marker_label % 2 == 0 else [b_marker_label]\n        marker_text = id_target_map[i_marker_label[0]]\n\n        # modified1\n        consecutive_list = ['Lead', 'Position', 'Concluding', 'Rebuttal']\n        if any([x in marker_text for x in consecutive_list]):\n            i_marker_label.append(i_marker_label[0] - 1)\n            # print(i_marker_label)\n        # i_marker_label = [b_marker_label,b_marker_label+1] if b_marker_label%2==1 else [b_marker_label-1,b_marker_label]\n\n        total_others_count = 0\n        cur_others_count = 0\n        tolerance = 0\n        while 1:\n            # print(t)\n            if t < len(word) and word_predict[t] not in i_marker_label and total_others_count < tolerance:\n                total_others_count += 1\n                cur_others_count += 1\n                t += 1\n            elif t == len(word) or (word_predict[t] not in i_marker_label):\n                t -= cur_others_count\n                end = t\n                # have bug here\n                # ' '.join([str(i) for i in range(start,end)]) #np.arange(start,end).tolist()\n                if 20 > end - start > 7:\n                    prediction_string = [i for i in range(start, end + 1 if end != len(word) else end)]\n                    # ' '.join(word[i] for i in range(start, end))\n                    prediction_text = [word[i] for i in range(start, end + 1 if end != len(word) else end)]\n                else:\n                    prediction_string = [i for i in range(start, end)]\n                    # ' '.join(word[i] for i in range(start, end))\n                    prediction_text = [word[i] for i in range(start, end)]\n                discourse_type = id_target_map[b_marker_label][2:]\n#                 # modified 5\n#                 if end == start or end == start + 1:\n#                     discourse_score = [word_score[start]]\n#                 elif end == start + 2:  # length = 2\n#                     # + [np.mean(word_score[start: end])]\n#                     discourse_score = word_score[start: end - 1].tolist()\n#                 else:\n#                     # + [np.mean(word_score[start: end])]\n#                     discourse_score = word_score[start: end - 1].tolist() + [np.mean(word_score[start:end])]\n\n                discourse_score = word_score[start: end].tolist()\n                # 将Concluding延长到最后一个词语\n                # if 'Concluding' in discourse_type and len(word) - 1 > t >= len(word) - 3:\n                # \tprint(discourse_type)\n                # \tt += 1\n\n                predict_df.append(\n                    (text_id, discourse_type, prediction_text, prediction_string, discourse_score))\n                # print(predict_df[-1])\n                break\n            else:\n                cur_others_count = 0\n                t = t + 1\n                continue\n        if t == len(word) - 1:\n            break\n\n    # modified 3 keep Lead\n    filtered_predict_df = list(\n        filter(lambda x: 'Lead' not in x[1], predict_df))\n    lead_df = list(filter(lambda x: 'Lead' in x[1], predict_df))\n    min_lead_score = 0.95\n    if len(lead_df) > 1:\n        lead_df = sorted(lead_df, key=lambda x: np.mean(x[4]), reverse=True)\n        lead_df = [lead_df[0]] + \\\n                  list(filter(lambda x: np.mean(x[4]) > min_lead_score, lead_df[1:]))\n        begin = min([x[3][0] for x in lead_df])\n        end = max([x[3][-1] for x in lead_df])\n        lead_df = [(lead_df[0][0], lead_df[0][1], [word[i] for i in range(\n            begin, end + 1)], [i for i in range(begin, end + 1)], word_score[begin:end + 1].tolist())]\n        predict_df = lead_df + filtered_predict_df\n\n    # modified4 keep Concluding\n    filtered_predict_df = list(\n        filter(lambda x: 'Concluding' not in x[1], predict_df))\n    con_df = list(filter(lambda x: 'Concluding' in x[1], predict_df))\n    min_con_score = 0.7\n    if len(con_df) > 1:\n        con_df = sorted(con_df, key=lambda x: np.mean(x[4]), reverse=True)\n        # + list(filter(lambda x: np.mean(x[4]) > min_con_score, con_df[1:]))\n        con_df = con_df[:2]\n        begin = min(con_df[0][3][0], con_df[1][3][0])\n        end = max(con_df[0][3][-1], con_df[1][3][-1])\n        con_df = [(con_df[0][0], con_df[0][1], [word[i] for i in range(\n            begin, end + 1)], [i for i in range(begin, end + 1)], word_score[begin + 1:end].tolist())]\n        predict_df = filtered_predict_df + con_df\n\n#     # modified6 keep Position\n#     filtered_predict_df = list(\n#         filter(lambda x: 'Position' not in x[1], predict_df))\n#     pos_df = list(filter(lambda x: 'Position' in x[1] and len(\n#         x[2]) > length_threshold['Position'], predict_df))\n#     min_pos_score = 0.9\n#     if len(pos_df) > 1:\n#         pos_df = sorted(pos_df, key=lambda x: np.mean(x[4]), reverse=True)\n#         pos_df = pos_df[:1] + \\\n#                  list(filter(lambda x: np.mean(x[4]) > min_pos_score, pos_df[1:]))\n#         if len(pos_df) >= 2:\n#             pos_df = sorted(pos_df, key=lambda x: x[3], reverse=False)\n#         # begin = min(pos_df[0][3][0], pos_df[1][3][0])\n#         # end = max(pos_df[0][3][-1], pos_df[1][3][-1])\n#         # pos_df = [(pos_df[0][0], pos_df[0][1], [word[i] for i in range(begin, end + 1)], [i for i in range(begin,end+1)], word_score[begin:end].tolist())]\n#         predict_df = filtered_predict_df + pos_df\n\n    for i in range(len(predict_df)):\n        predict_df[i] = (predict_df[i][0], predict_df[i][1], ' '.join(\n            predict_df[i][2]), ' '.join(str(x) for x in predict_df[i][3]), str(predict_df[i][4]))\n    predict_df = pd.DataFrame(predict_df, columns=[\n        'id', 'class', 'predict_text', 'predictionstring', 'score'])\n    return predict_df\n\n\ndef word_probability_to_prediction_string_v4(text_to_word_probability, text_id, word):\n    # print(11)\n    length_threshold = {\n        \"Lead\": 9,\n        \"Position\": 5,\n        \"Evidence\": 14,\n        \"Claim\": 3,\n        \"Concluding Statement\": 11,\n        \"Counterclaim\": 6,\n        \"Rebuttal\": 4,\n    }\n    word_predict = text_to_word_probability.argmax(-1)\n    word_score = text_to_word_probability.max(-1)\n    predict_df = []\n\n    t = 0\n    while 1:\n        # if word_predict[t] in [1,3,5,7,9,11,13]:\n        if t == len(word):\n            break\n\n        if word_predict[t] in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]:\n            start = t\n            b_marker_label = word_predict[t]\n        else:\n            t = t + 1\n            if t == len(word) - 1:\n                break\n            continue\n\n        t = t + 1\n        if t == len(word) - 1:\n            break\n\n        # i_marker_label = b_marker_label+1\n        i_marker_label = [b_marker_label +\n                          1] if b_marker_label % 2 == 0 else [b_marker_label]\n        marker_text = id_target_map[i_marker_label[0]]\n\n        total_others_count = 0\n        cur_others_count = 0\n        tolerance_cur = 0\n        tolerance_total = 0\n        # modified1\n        consecutive_list = ['Lead', 'Position', 'Concluding', 'Rebuttal']\n        if any([x in marker_text for x in consecutive_list]):\n            i_marker_label.append(i_marker_label[0] - 1)\n            # if any([x in marker_text for x in consecutive_list]):\n            # # modified 7 rebuttal tolerance\n        #     if 'Rebuttal' in marker_text:\n        #         tolerance_cur = 7\n        #         tolerance_total = 15\n        # # i_marker_label = [b_marker_label,b_marker_label+1] if b_marker_label%2==1 else [b_marker_label-1,b_marker_label]\n\n        while 1:\n            # print(t)\n            if t < len(word) and word_predict[t] not in i_marker_label and total_others_count < tolerance_total and cur_others_count < tolerance_cur:\n                total_others_count += 1\n                cur_others_count += 1\n                t += 1\n            elif t == len(word) or (word_predict[t] not in i_marker_label):\n                t -= cur_others_count\n                end = t\n                # ' '.join([str(i) for i in range(start,end)]) #np.arange(start,end).tolist()\n\n                # modified 6\n                if 20 > end - start > 5:\n                    prediction_string = [i for i in range(start, end + 1 if end != len(word) else end)]\n                    # ' '.join(word[i] for i in range(start, end))\n                    prediction_text = [word[i] for i in range(start, end + 1 if end != len(word) else end)]\n                else:\n                    prediction_string = [i for i in range(start, end)]\n                    # ' '.join(word[i] for i in range(start, end))\n                    prediction_text = [word[i] for i in range(start, end)]\n\n                # extend\n\n                # prediction_string = [i for i in range(start, end)]\n                # prediction_text = [word[i] for i in range(start, end)]\n\n\n                discourse_type = id_target_map[b_marker_label][2:]\n#                 # modified 5\n#                 if end == start or end == start + 1:\n#                     discourse_score = [word_score[start]]\n#                 elif end == start + 2:  # length = 2\n#                     # + [np.mean(word_score[start: end])]\n#                     discourse_score = word_score[start: end - 1].tolist()\n#                 else:\n#                     # + [np.mean(word_score[start: end])]\n#                     discourse_score = word_score[start: end - 1].tolist() + [np.mean(word_score[start:end])]\n\n                discourse_score = word_score[start: end].tolist()\n                # 将Concluding延长到最后一个词语\n\n                predict_df.append(\n                    (text_id, discourse_type, prediction_text, prediction_string, discourse_score))\n                # print(predict_df[-1])\n                break\n            else:\n                cur_others_count = 0\n                t = t + 1\n                continue\n        if t == len(word) - 1:\n            break\n\n    # modified 3 keep Lead\n    filtered_predict_df = list(\n        filter(lambda x: 'Lead' not in x[1], predict_df))\n    lead_df = list(filter(lambda x: 'Lead' in x[1], predict_df))\n    min_lead_score = 0.95\n    if len(lead_df) > 1:\n        lead_df = sorted(lead_df, key=lambda x: np.mean(x[4]), reverse=True)\n        lead_df = [lead_df[0]] + \\\n                  list(filter(lambda x: np.mean(x[4]) > min_lead_score, lead_df[1:]))\n        begin = min([x[3][0] for x in lead_df])\n        end = max([x[3][-1] for x in lead_df])\n        lead_df = [(lead_df[0][0], lead_df[0][1], [word[i] for i in range(\n            begin, end + 1)], [i for i in range(begin, end + 1)], word_score[begin:end + 1].tolist())]\n        predict_df = lead_df + filtered_predict_df\n\n    # modified4 keep Concluding\n    filtered_predict_df = list(\n        filter(lambda x: 'Concluding' not in x[1], predict_df))\n    con_df = list(filter(lambda x: 'Concluding' in x[1], predict_df))\n    min_con_score = 0.7\n    if len(con_df) > 1:\n        con_df = sorted(con_df, key=lambda x: np.mean(x[4]), reverse=True)\n        # + list(filter(lambda x: np.mean(x[4]) > min_con_score, con_df[1:]))\n        con_df = con_df[:2]\n        begin = min(con_df[0][3][0], con_df[1][3][0])\n        end = max(con_df[0][3][-1], con_df[1][3][-1])\n        con_df = [(con_df[0][0], con_df[0][1], [word[i] for i in range(\n            begin, end + 1)], [i for i in range(begin, end + 1)], word_score[begin + 1:end].tolist())]\n        predict_df = filtered_predict_df + con_df\n\n    # # modified 8 keep Position\n    # filtered_predict_df = list(\n    #     filter(lambda x: 'Position' not in x[1], predict_df))\n    # pos_df = list(filter(lambda x: 'Position' in x[1] and len(\n    #     x[2]) > length_threshold['Position'], predict_df))\n    # min_pos_score = 0.9\n    # if len(pos_df) > 1:\n    #     pos_df = sorted(pos_df, key=lambda x: np.mean(x[4]), reverse=True)\n    #     pos_df = pos_df[:1] + \\\n    #              list(filter(lambda x: np.mean(x[4]) > min_pos_score, pos_df[1:]))\n    #     if len(pos_df) >= 2:\n    #         pos_df = sorted(pos_df, key=lambda x: x[3], reverse=False)\n    #     # begin = min(pos_df[0][3][0], pos_df[1][3][0])\n    #     # end = max(pos_df[0][3][-1], pos_df[1][3][-1])\n    #     # pos_df = [(pos_df[0][0], pos_df[0][1], [word[i] for i in range(begin, end + 1)], [i for i in range(begin,end+1)], word_score[begin:end].tolist())]\n    #     predict_df = filtered_predict_df + pos_df\n\n    for i in range(len(predict_df)):\n        predict_df[i] = (predict_df[i][0], predict_df[i][1], ' '.join(\n            predict_df[i][2]), ' '.join(str(x) for x in predict_df[i][3]), str(predict_df[i][4]))\n    predict_df = pd.DataFrame(predict_df, columns=[\n        'id', 'class', 'predict_text', 'predictionstring', 'score'])\n    return predict_df\n\n\ndef word_probability_to_prediction_string_v5(text_to_word_probability, text_id, word, convert_table):\n    # print(11)\n\n    word_predict = text_to_word_probability.argmax(-1)\n    word_score = text_to_word_probability.max(-1)\n    predict_df = []\n\n    t = 0\n    while 1:\n        # if word_predict[t] in [1,3,5,7,9,11,13]:\n        if t == len(word):\n            break\n\n        if word_predict[t] in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]:\n            start = t\n            b_marker_label = word_predict[t]\n        else:\n            t = t + 1\n            if t == len(word) - 1:\n                break\n            continue\n\n        t = t + 1\n        if t == len(word) - 1:\n            break\n\n        # i_marker_label = b_marker_label+1\n        i_marker_label = [b_marker_label + 1] if b_marker_label % 2 == 0 else [b_marker_label]\n        marker_text = id_target_map[i_marker_label[0]]\n\n        total_others_count = 0\n        cur_others_count = 0\n        tolerance_cur = 0\n        tolerance_total = 0\n        # modified1\n        consecutive_list = ['Lead', 'Position', 'Concluding', 'Rebuttal']\n        if any([x in marker_text for x in consecutive_list]):\n            i_marker_label.append(i_marker_label[0] - 1)\n            # if any([x in marker_text for x in consecutive_list]):\n            # modified 7 rebuttal tolerance\n            # if 'Rebuttal' in marker_text:\n            #     tolerance_cur = 6\n            #     tolerance_total = 15\n            # #         'Rebuttal': ('0.5308', '0.6101', '0.4697')\n            # # [7, 15] 'Rebuttal': ('0.5249', '0.6109', '0.4600')\n            # # [4, 15] 'Rebuttal': ('0.5241', '0.6090', '0.4600')\n            # # [6, 15] \n\n        # # i_marker_label = [b_marker_label,b_marker_label+1] if b_marker_label%2==1 else [b_marker_label-1,b_marker_label]\n\n        while 1:\n            # print(t)\n            if t < len(word) and word_predict[t] not in i_marker_label and total_others_count < tolerance_total and cur_others_count < tolerance_cur:\n                total_others_count += 1\n                cur_others_count += 1\n                t += 1\n            elif t == len(word) or (word_predict[t] not in i_marker_label):\n                t -= cur_others_count\n                end = t\n                # ' '.join([str(i) for i in range(start,end)]) #np.arange(start,end).tolist()\n\n                # # modified 6\n                if 20 > end - start > 5:\n                    prediction_string = [i for i in range(start, end + 1 if end != len(word) else end)]\n                    # ' '.join(word[i] for i in range(start, end))\n                    prediction_text = [word[i] for i in range(start, end + 1 if end != len(word) else end)]\n                else:\n                    prediction_string = [i for i in range(start, end)]\n                    # ' '.join(word[i] for i in range(start, end))\n                    prediction_text = [word[i] for i in range(start, end)]\n\n                # prediction_string = [i for i in range(start, end)]\n                # prediction_text = [word[i] for i in range(start, end)]\n\n                discourse_type = id_target_map[b_marker_label][2:]\n\n                discourse_score = word_score[start: end].tolist()\n                # 将Concluding延长到最后一个词语\n\n                predict_df.append((text_id, discourse_type, prediction_text, prediction_string, discourse_score))\n                # print(predict_df[-1])\n                break\n            else:\n                cur_others_count = 0\n                t = t + 1\n                continue\n        if t == len(word) - 1:\n            break\n\n    # modified 3 keep Lead\n    filtered_predict_df = list(\n        filter(lambda x: 'Lead' not in x[1], predict_df))\n    lead_df = list(filter(lambda x: 'Lead' in x[1], predict_df))\n    min_lead_score = 0.95\n    if len(lead_df) > 1:\n        lead_df = sorted(lead_df, key=lambda x: np.mean(x[4]), reverse=True)\n        lead_df = [lead_df[0]] + \\\n                  list(filter(lambda x: np.mean(x[4]) > min_lead_score, lead_df[1:]))\n        begin = min([x[3][0] for x in lead_df])\n        end = max([x[3][-1] for x in lead_df])\n        lead_df = [(lead_df[0][0], lead_df[0][1], [word[i] for i in range(\n            begin, end + 1)], [i for i in range(begin, end + 1)], word_score[begin:end + 1].tolist())]\n        predict_df = lead_df + filtered_predict_df\n\n    # modified4 keep Concluding\n    filtered_predict_df = list(\n        filter(lambda x: 'Concluding' not in x[1], predict_df))\n    con_df = list(filter(lambda x: 'Concluding' in x[1], predict_df))\n    min_con_score = 0.7\n    if len(con_df) > 1:\n        con_df = sorted(con_df, key=lambda x: np.mean(x[4]), reverse=True)\n        # + list(filter(lambda x: np.mean(x[4]) > min_con_score, con_df[1:]))\n        con_df = con_df[:2]\n        begin = min(con_df[0][3][0], con_df[1][3][0])\n        end = max(con_df[0][3][-1], con_df[1][3][-1])\n        con_df = [(con_df[0][0], con_df[0][1], [word[i] for i in range(\n            begin, end + 1)], [i for i in range(begin, end + 1)], word_score[begin + 1:end].tolist())]\n        predict_df = filtered_predict_df + con_df\n\n    # convert_table = {\n    #     # 'Lead': {'Lead': 0.9, 'Position': 1.1, 'Evidence': 0.95, 'Claim': 0.98, 'Concluding Statement': 0.9, 'Counterclaim': 0.93, 'Rebuttal': 0.9,},\n    #     'Position': {'Lead': 0.9, 'Position': 0.9, 'Evidence': 1., 'Claim': 1.1, 'Concluding Statement': 0.95, 'Counterclaim': 0.94, 'Rebuttal': 0.9,},\n    #     # 'Evidence': {'Lead': 0.9, 'Position': 0.95, 'Evidence': 1., 'Claim': 1.1, 'Concluding Statement': 1.1, 'Counterclaim': 1., 'Rebuttal': 0.95,},\n    #     # 'Claim': {'Lead': 0.94, 'Position': 0.95, 'Evidence': 1.2, 'Claim': 1.1, 'Concluding Statement': 1, 'Counterclaim': 1, 'Rebuttal': 0.9,},\n    #     # 'Concluding Statement': {'Lead': 0.9, 'Position': 0.94, 'Evidence': 1., 'Claim': 1.1, 'Concluding Statement': 0.95, 'Counterclaim': 0.94, 'Rebuttal': 0.9,},\n    #     # 'Counterclaim': {'Lead': 0.9, 'Position': 1.1, 'Evidence': 1., 'Claim': 1., 'Concluding Statement': 0.95, 'Counterclaim': 1., 'Rebuttal': 0.9,},\n    #     # 'Rebuttal': {'Lead': 0.9, 'Position': 0.95, 'Evidence': 1.1, 'Claim': 1., 'Concluding Statement': 1., 'Counterclaim': 0.95, 'Rebuttal': 0.92,},\n    # }\n#     min_thresh = {\n#         \"Lead\": 9,\n#         \"Position\": 5,\n#         \"Evidence\": 14,\n#         \"Claim\": 3,\n#         \"Concluding Statement\": 11,\n#         \"Counterclaim\": 6,\n#         \"Rebuttal\": 4,\n#     }\n#     proba_thresh = {\n#         \"Lead\": 0.617628220048235, # 0.7\n#         \"Position\": 0.5404662917593531, # 0.55\n#         \"Evidence\": 0.5792470568116815, # 0.65\n#         \"Claim\": 0.5385829262728876, # 0.55\n#         \"Concluding Statement\": 0.6235012425556871, # 0.7\n#         \"Counterclaim\": 0.4975126082187205, # 0.5\n#         \"Rebuttal\": 0.5444709754299981, # 0.55\n#     }\n    min_thresh = {\n        \"Lead\": 8, # 9\n        \"Position\": 4, # 5\n        \"Evidence\": 8, # 14\n        \"Claim\": 1, # 3\n        \"Concluding Statement\": 10, # 11\n        \"Counterclaim\": 9,\n        \"Rebuttal\": 2, # 4\n    }\n    proba_thresh = {\n        \"Lead\": 0.5647964444385352,\n        \"Position\": 0.6211823905704472,\n        \"Evidence\": 0.6037415312070282,\n        \"Claim\": 0.5655015619409717,\n        \"Concluding Statement\": 0.5605916604200145,\n        \"Counterclaim\": 0.5589241228663976,\n        \"Rebuttal\": 0.6279143972926252\n    }\n    predict_df = sorted(predict_df, key=lambda x: x[3][0])\n    for i, predict in enumerate(predict_df[:-1]):\n        cur_class = predict[1]\n        if cur_class in convert_table.keys() and len(predict[3]) >= min_thresh[cur_class] and np.mean(predict[-1]) > proba_thresh[cur_class]:\n            new_score = (np.array(predict_df[i+1][-1]) * convert_table[cur_class][predict_df[i+1][1]]).tolist()\n            predict_df[i+1] = (predict_df[i+1][0], predict_df[i+1][1], predict_df[i+1][2], predict_df[i+1][3], new_score)\n\n    for i in range(len(predict_df)):\n        predict_df[i] = (predict_df[i][0], predict_df[i][1], ' '.join(\n            predict_df[i][2]), ' '.join(str(x) for x in predict_df[i][3]), str(predict_df[i][4]))\n    predict_df = pd.DataFrame(predict_df, columns=[\n        'id', 'class', 'predict_text', 'predictionstring', 'score'])\n    return predict_df\n\n\n# {'min_Lead': 7, 'min_Position': 5, 'min_Evidence': 15, 'min_Claim': 3, 'min_Concluding': 6, \n# 'min_Counterclaim': 6, 'min_Rebuttal': 7, 'proba_Lead': 0.5525631492348035, \n# 'proba_Position': 0.5115443109231739, 'proba_Evidence': 0.5950038120901334, \n# 'proba_Claim': 0.5412112109901169, 'proba_Concluding': 0.6327829537768497, \n# 'proba_Counterclaim': 0.5032089227738017, 'proba_Rebuttal': 0.5479240354533439}\ndef do_threshold(submit_df, use=['length','probability']):\n    df = submit_df.copy()\n    df = df.fillna('')\n    min_thresh = {\n        \"Lead\": 8, # 9\n        \"Position\": 4, # 5\n        \"Evidence\": 8, # 14\n        \"Claim\": 1, # 3\n        \"Concluding Statement\": 10, # 11\n        \"Counterclaim\": 9,\n        \"Rebuttal\": 2, # 4\n    }\n    proba_thresh = {\n        \"Lead\": 0.5647964444385352,\n        \"Position\": 0.6211823905704472,\n        \"Evidence\": 0.6037415312070282,\n        \"Claim\": 0.5655015619409717,\n        \"Concluding Statement\": 0.5605916604200145,\n        \"Counterclaim\": 0.5589241228663976,\n        \"Rebuttal\": 0.6279143972926252\n    }\n    if 'length' in use:\n        df['l'] = df.predictionstring.apply(lambda x: len(x.split()))\n        for key, value in min_thresh.items():\n            #value=3\n            index = df.loc[df['class'] == key].query('l<%d'%value).index\n            df.drop(index, inplace=True)\n\n    if 'probability' in use:\n        df['s'] = df.score.apply(lambda x: np.mean(eval(x)))\n        for key, value in proba_thresh.items():\n            index = df.loc[df['class'] == key].query('s<%f'%value).index\n            df.drop(index, inplace=True)\n\n    df = df[['id', 'class', 'predictionstring']]\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:43:52.712323Z","iopub.execute_input":"2022-03-15T14:43:52.71253Z","iopub.status.idle":"2022-03-15T14:43:52.807969Z","shell.execute_reply.started":"2022-03-15T14:43:52.712503Z","shell.execute_reply":"2022-03-15T14:43:52.807181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config(object):\n    train_path = \"../input/feedback-prize-2021/sample_submission.csv\"\n    fix_length = 2048\n    num_labels = 15\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:43:52.809647Z","iopub.execute_input":"2022-03-15T14:43:52.809955Z","iopub.status.idle":"2022-03-15T14:43:52.818368Z","shell.execute_reply.started":"2022-03-15T14:43:52.809914Z","shell.execute_reply":"2022-03-15T14:43:52.817659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# funnel_large: 18, deberta_v3_large: 62, bigbird_roberta_large: 0, deberta_large: 97, longformer_large_4096: 35\nmodel_list = [\n    # deberta_xlarge\n    ([\n        (\"../input/fb-dxlarge/dx01.bin\", 91/6),\n        (\"../input/fb-dxlarge/dx20.bin\", 91/6),\n        (\"../input/fb-dxlarge/dx10_5fold.bin\", 91/6),\n        (\"../input/fb-dxlarge/dx20_fold5.bin\", 91/6),\n        (\"../input/fb-dxlarge/dx30_5fold.bin\", 91/6),\n        (\"../input/fb-dxlarge/dx40_5fold.bin\", 91/6),\n#         (\"../input/fb-dxlarge/dx30.bin\", 78),\n    ], \"../input/deberta-xlarge\"),\n    # funnel\n    ([\n        (\"../input/fb-model/f03.bin\", 27/2),\n        (\"../input/fb-model/f10.bin\", 27/2),\n    ], \"../input/funneltransformerlarge\"),\n#     deberta_v3\n    ([\n        (\"../input/fb-model/d10.bin\", 39/2),\n        (\"../input/fb-model/d04.bin\", 39/2),\n#         (\"../input/fb-model/d50.bin\", 62/5),\n#         (\"../input/fb-model/d70.bin\", 62/5),\n#         (\"../input/fb-model/d90.bin\", 62/5),\n    ], \"../input/deberta-v3-large/deberta-v3-large\"),\n#     longformer\n    ([\n        (\"../input/tez-fb-large/model_0.bin\", 55/6.),\n        (\"../input/tez-fb-large/model_1.bin\", 55/6.),\n        (\"../input/tez-fb-large/model_2.bin\", 55/6.),\n        ('../input/fblongformerlarge1536/model_3.bin', 55/6.),\n        ('../input/fblongformerlarge1536/model_4.bin', 55/6.),\n        (\"../input/fb-model/l00.bin\", 55/6.),\n    ], \"../input/longformerlarge4096/longformer-large-4096\"),\n#     deberta\n    ([\n        (\"../input/fb-model/dl00.bin\", 45/2),\n        (\"../input/fb-model/dl20.bin\", 45/2),\n#         (\"../input/fb-model/dl01.bin\", 97/7),\n#         (\"../input/fb-model/dl21.bin\", 97/7),\n#         (\"../input/fb-model/dl40.bin\", 97/7),\n#         (\"../input/fb-model/dl60.bin\", 97/7),\n#         (\"../input/fb-model/dl80.bin\", 97/7),\n    ], \"../input/deberta/large\"),\n]\n\n\n\n# model_list = [\n#     (\"../input/fb-model/f03.bin\", \"../input/funneltransformerlarge\", 18),    # 0.6794\n#     (\"../input/fb-model/f10.bin\", \"../input/funneltransformerlarge\", 18),\n#     (\"../input/fb-model/d10.bin\", \"../input/deberta-v3-large/deberta-v3-large\", 62),\n#     (\"../input/fb-model/d04.bin\", \"../input/deberta-v3-large/deberta-v3-large\", 62),\n#     (\"../input/tez-fb-large/model_0.bin\", \"../input/longformerlarge4096/longformer-large-4096\", 35),\n#     (\"../input/tez-fb-large/model_1.bin\", \"../input/longformerlarge4096/longformer-large-4096\", 35),\n#     (\"../input/fb-model/l00.bin\", \"../input/longformerlarge4096/longformer-large-4096\", 35),\n#     (\"../input/fb-model/dl00.bin\", \"../input/deberta/large\", 97),\n#     (\"../input/fb-model/dl20.bin\", \"../input/deberta/large\", 97),\n#     (\"../input/fb-model/dl01.bin\", \"../input/deberta/large\", 97),\n#     (\"../input/fb-model/dl21.bin\", \"../input/deberta/large\", 97),\n#     (\"../input/fb-model/dl40.bin\", \"../input/deberta/large\", 97),\n#     (\"../input/fb-model/dl60.bin\", \"../input/deberta/large\", 97),\n#     (\"../input/fb-model/dl80.bin\", \"../input/deberta/large\", 97),\n# ]\n\n# model_list = [\n#     (\"../input/fb-model/f03.bin\", \"../input/funneltransformerlarge\", 1),    # 0.6794\n#     (\"../input/fb-model/f10.bin\", \"../input/funneltransformerlarge\", 1),\n#     (\"../input/fb-model/d10.bin\", \"../input/deberta-v3-large/deberta-v3-large\", 1),\n#     (\"../input/fb-model/d04.bin\", \"../input/deberta-v3-large/deberta-v3-large\", 1),\n#     (\"../input/tez-fb-large/model_0.bin\", \"../input/longformerlarge4096/longformer-large-4096\", 1),\n#     (\"../input/tez-fb-large/model_1.bin\", \"../input/longformerlarge4096/longformer-large-4096\", 1),\n#     (\"../input/fb-model/l00.bin\", \"../input/longformerlarge4096/longformer-large-4096\", 1),\n#     (\"../input/fb-model/dl00.bin\", \"../input/deberta/large\", 1),\n#     (\"../input/fb-model/dl20.bin\", \"../input/deberta/large\", 1),\n# ]","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:43:52.82061Z","iopub.execute_input":"2022-03-15T14:43:52.821307Z","iopub.status.idle":"2022-03-15T14:43:52.831538Z","shell.execute_reply.started":"2022-03-15T14:43:52.82127Z","shell.execute_reply":"2022-03-15T14:43:52.830852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_list = [\n#     (\"../input/fb-model/f03.bin\", \"../input/funneltransformerlarge\", 18),    # 0.6794\n#     (\"../input/fb-model/d04.bin\", \"../input/deberta-v3-large/deberta-v3-large\", 62),\n#     (\"../input/fb-model/l00.bin\", \"../input/longformerlarge4096/longformer-large-4096\", 35),\n#     (\"../input/fb-model/dl00.bin\", \"../input/deberta/large\", 97),\n#     (\"../input/fb-model/b00.bin\",\"../input/bigbirdrobertalarge/bigbird-roberta-large\", 35),\n# ]","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:43:52.83417Z","iopub.execute_input":"2022-03-15T14:43:52.834373Z","iopub.status.idle":"2022-03-15T14:43:52.842005Z","shell.execute_reply.started":"2022-03-15T14:43:52.834344Z","shell.execute_reply":"2022-03-15T14:43:52.841396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = Config()\nnum_net = len(model_list)\ndf = pd.read_csv(args.train_path)\nvalid_id = df[\"id\"].unique()\n# num_valid = len(valid_id)\n# print(f\"num_valid: {num_valid}\")\ndf_text = []\nfor id in valid_id:\n    text_file = \"../input/feedback-prize-2021/test\" + f\"/{id}.txt\"\n    with open(text_file, \"r\") as f:\n        text = f.read()\n    df_text.append((id, text))\ndf_text = pd.DataFrame(df_text, columns=[\"id\", \"text\"])\ndf_text['text_len'] = df_text['text'].apply(lambda x: len(x))\ndf_text = df_text.sort_values('text_len').reset_index(drop=True)\ndf = df_text\nvalid_id = df[\"id\"].unique()\nnum_valid = len(valid_id)\nprint(f\"num_valid: {num_valid}\")\nresults = []\nweights = []\nweight_sum = 0.0\nmagic_params = [1.0151040677346772, 0.9030902168873051, 1.150794726450591, 1.1071525118845413, 0.8923895084086448, 0.9156323986648087, 0.8376600826469922, 1.0863350013753394, 0.8034240506162517, 0.8838929237772057, 1.167908517815809, 1.0758516272886205, 0.921439773646722, 1.175718027875091, 1.073194555851536]\nmagic_params = np.array(magic_params)\nconvert_table = {\n    'Concluding Statement': {'Lead': 0.6180546139059481, 'Position': 1.106376532299193, 'Evidence': 0.8308079139611104, 'Claim': 0.9263323897452017, 'Concluding Statement': 0.7427996729640948, 'Counterclaim': 1.1255410558775625, 'Rebuttal': 0.9443065711964579},\n    # 'Evidence': {'Lead': 0.6935170533407621, 'Position': 1.1604061898464963, 'Evidence': 1.053929298774972, 'Claim': 1.0154215894307475, 'Concluding Statement': 1.101286190449023, 'Counterclaim': 0.9458791358573785, 'Rebuttal': 1.0100877841954223},\n    'Evidence': {'Lead': 0.7393941341170962, 'Position': 1.0724463092198406, 'Evidence': 1.0058653210627613, 'Claim': 1.022313944728398, 'Concluding Statement': 1.052367501549448, 'Counterclaim': 1.0090474774696865, 'Rebuttal': 1.0333100430656466},\n    'Rebuttal': {'Lead': 0.5399304201316384, 'Position': 1.08856196786258, 'Evidence': 1.0103291548166256, 'Claim': 1.0847765835179881, 'Concluding Statement': 0.9549480752091022, 'Counterclaim': 1.092666136606834, 'Rebuttal': 0.7110649743074405},\n    'Counterclaim': {'Lead': 0.9050794118735646, 'Position': 0.9160850157706609, 'Evidence': 1.1418506967909068, 'Claim': 1.049570707730134, 'Concluding Statement': 1.1146750148380493, 'Counterclaim': 0.9746304751663061, 'Rebuttal': 1.1594205982055437},\n    'Position': {'Lead': 0.8393628836469293, 'Position': 0.792198366631391, 'Evidence': 1.0359996942895344, 'Claim': 1.0163778318063577, 'Concluding Statement': 1.0834964446969553, 'Counterclaim': 1.1085107194393697, 'Rebuttal': 0.6460859598497333},\n}\nfor idx, model in enumerate(model_list):\n#     weights.append(model[2])\n#     weight_sum += model[2]\n    if \"deberta-v3\" in model[1]:\n        tokenizer = get_DebertaV2TokenizerFast(model[1])\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(model[1])\n    valid_samples = prepare_test_data_v2(df, tokenizer)\n    valid_datasets = FeedbackDatasetValid(valid_samples, args.fix_length, tokenizer)\n    collate = Collate(tokenizer, fix_length=args.fix_length)\n    preds_list = []\n    for idxx, model_path in enumerate(model[0]):\n        weights.append(model_path[1])\n        weight_sum += model_path[1]\n        predicter = Predicter(args)\n        predicter.set_pretrain(model[1])\n        predicter.model_init()\n        predicter.model_load(model_path[0])\n        pred = predicter.predict(valid_datasets, collate)\n        pred = np.concatenate(pred)\n        np.save(f\"tmp_{idx}_{idxx}.npy\", pred)\n        preds_list.append(f\"tmp_{idx}_{idxx}.npy\")\n        del pred\n        del predicter.model\n        del predicter\n        gc.collect()\n        torch.cuda.empty_cache()\n    results.append({\n        \"probability\": preds_list,\n        \"token_offset\": [sample[\"offset_mapping\"].copy() for sample in valid_samples],\n    })\n    del valid_samples\n    del valid_datasets\n    del tokenizer\n    del collate\n    gc.collect()\nsubmit_df = []\nfor i in range(len(results)):\n    results[i][\"probability\"] = [np.load(item) for item in results[i][\"probability\"]]\ntime_list = [0,0,0,0,0]\nfor i in tqdm(range(num_valid)):\n    d = df_text.iloc[i]\n    id =  d.id\n    text = d.text\n    word, word_offset = text_to_word(text)\n    token_to_text_probability = np.full((len(text),args.num_labels),0, np.float32)\n    cnt = 0\n    for j in range(num_net):\n        for k in range(len(results[j][\"probability\"])):\n            p = results[j]['probability'][k][i][1:]/255\n            # logging.info(p.shape)\n            for t, (start, end) in enumerate(results[j][\"token_offset\"][i]):\n                if t==args.fix_length-1: break\n                token_to_text_probability[start: end] += p[t] * weights[cnt]\n#             print(weights[cnt])\n            cnt += 1\n    token_to_text_probability = token_to_text_probability / weight_sum\n\n    text_to_word_probability = np.full((len(word),args.num_labels),0, np.float32)\n    for t,(start,end) in enumerate(word_offset):\n        text_to_word_probability[t]=token_to_text_probability[start:end].mean(0)\n#     predict_df = word_probability_to_predict_df(text_to_word_probability, id)\n#     predict_df = word_probability_to_prediction_string(text_to_word_probability, id, word)\n    text_to_word_probability = text_to_word_probability * magic_params\n#     predict_df = word_probability_to_prediction_string_v4(text_to_word_probability, id, word)\n    predict_df = word_probability_to_prediction_string_v5(text_to_word_probability, id, word, convert_table)\n    submit_df.append(predict_df)\n#     del token_to_text_probability, text_to_word_probability\n#     gc.collect()\nsubmit_df = pd.concat(submit_df).reset_index(drop=True)\nsubmit_df = do_threshold(submit_df, use=['length', 'probability'])\nsubmit_df.to_csv(\"submission.csv\", index=False)\n# f1 = Utils.score_feedback_comp_micro","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:43:52.844468Z","iopub.execute_input":"2022-03-15T14:43:52.844754Z","iopub.status.idle":"2022-03-15T14:52:53.458968Z","shell.execute_reply.started":"2022-03-15T14:43:52.844718Z","shell.execute_reply":"2022-03-15T14:52:53.458251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T14:52:53.460287Z","iopub.execute_input":"2022-03-15T14:52:53.461381Z","iopub.status.idle":"2022-03-15T14:52:53.476282Z","shell.execute_reply.started":"2022-03-15T14:52:53.461337Z","shell.execute_reply":"2022-03-15T14:52:53.475485Z"},"trusted":true},"execution_count":null,"outputs":[]}]}