{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:57:07.689065Z","iopub.execute_input":"2022-03-14T12:57:07.689996Z","iopub.status.idle":"2022-03-14T12:57:13.399136Z","shell.execute_reply.started":"2022-03-14T12:57:07.689881Z","shell.execute_reply":"2022-03-14T12:57:13.398442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# my virenv : py37NLP\n\nimport os\nimport pandas as pd\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:57:13.400911Z","iopub.execute_input":"2022-03-14T12:57:13.401168Z","iopub.status.idle":"2022-03-14T12:57:13.463823Z","shell.execute_reply.started":"2022-03-14T12:57:13.401129Z","shell.execute_reply":"2022-03-14T12:57:13.463113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:57:13.465496Z","iopub.execute_input":"2022-03-14T12:57:13.465992Z","iopub.status.idle":"2022-03-14T12:57:13.475415Z","shell.execute_reply.started":"2022-03-14T12:57:13.465954Z","shell.execute_reply":"2022-03-14T12:57:13.474759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainData = pd.read_csv('/kaggle/input/feedback-prize-2021/train.csv')\ntrainData","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:57:13.478093Z","iopub.execute_input":"2022-03-14T12:57:13.478432Z","iopub.status.idle":"2022-03-14T12:57:14.857161Z","shell.execute_reply.started":"2022-03-14T12:57:13.478399Z","shell.execute_reply":"2022-03-14T12:57:14.85642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainData.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:57:14.858444Z","iopub.execute_input":"2022-03-14T12:57:14.858701Z","iopub.status.idle":"2022-03-14T12:57:14.953074Z","shell.execute_reply.started":"2022-03-14T12:57:14.858666Z","shell.execute_reply":"2022-03-14T12:57:14.952333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load dataset\ndef load_df():\n    train_names, train_texts = [], []\n    for f in tqdm(list(os.listdir('/kaggle/input/feedback-prize-2021/train'))):\n        train_names.append(f.replace('.txt', ''))\n        train_texts.append(open('/kaggle/input/feedback-prize-2021/train/' + f, 'r', encoding='UTF-8').read())\n    train_text_df = pd.DataFrame({'id': train_names, 'text': train_texts})\n    return train_text_df","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:57:14.954455Z","iopub.execute_input":"2022-03-14T12:57:14.954789Z","iopub.status.idle":"2022-03-14T12:57:14.960188Z","shell.execute_reply.started":"2022-03-14T12:57:14.954748Z","shell.execute_reply":"2022-03-14T12:57:14.959362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = load_df()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:57:14.961497Z","iopub.execute_input":"2022-03-14T12:57:14.961963Z","iopub.status.idle":"2022-03-14T12:57:57.527823Z","shell.execute_reply.started":"2022-03-14T12:57:14.961927Z","shell.execute_reply":"2022-03-14T12:57:57.526999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the value\ndf_ = df.query(\"id == 'DBF7EB6A9E02'\")['text'].values[0]\ndf_","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:57:57.529197Z","iopub.execute_input":"2022-03-14T12:57:57.529553Z","iopub.status.idle":"2022-03-14T12:57:57.54237Z","shell.execute_reply.started":"2022-03-14T12:57:57.529516Z","shell.execute_reply":"2022-03-14T12:57:57.541539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_df = trainData.query(\"id == 'DBF7EB6A9E02'\")['discourse_text'].values[0]\ntr_df","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:57:57.543807Z","iopub.execute_input":"2022-03-14T12:57:57.544232Z","iopub.status.idle":"2022-03-14T12:57:57.563315Z","shell.execute_reply.started":"2022-03-14T12:57:57.544188Z","shell.execute_reply":"2022-03-14T12:57:57.562464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the length of the sentence to be analyzed is too long to be processed as BERT, the method of comparing and analyzing the input sentence after summarizing it is applied.\n","metadata":{}},{"cell_type":"code","source":"!pip install bert-extractive-summarizer","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:57:57.566776Z","iopub.execute_input":"2022-03-14T12:57:57.567307Z","iopub.status.idle":"2022-03-14T12:58:06.550512Z","shell.execute_reply.started":"2022-03-14T12:57:57.567277Z","shell.execute_reply":"2022-03-14T12:58:06.549652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from transformers import pipeline \n# use bart in pytorch\n#summarizer = pipeline(\"summarization\")\n#ptorch = summarizer(\"An apple a day, keeps the doctor away\", min_length=5, max_length=20)\n\n#print(ptorch)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:06.552436Z","iopub.execute_input":"2022-03-14T12:58:06.552736Z","iopub.status.idle":"2022-03-14T12:58:06.557101Z","shell.execute_reply.started":"2022-03-14T12:58:06.552693Z","shell.execute_reply":"2022-03-14T12:58:06.556359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:06.559053Z","iopub.execute_input":"2022-03-14T12:58:06.559571Z","iopub.status.idle":"2022-03-14T12:58:06.573683Z","shell.execute_reply.started":"2022-03-14T12:58:06.55953Z","shell.execute_reply":"2022-03-14T12:58:06.572877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"import pickle\ndf.to_pickle(\"tr_data_bert.pkl\")","metadata":{"execution":{"iopub.status.busy":"2022-03-06T09:01:51.616077Z","iopub.execute_input":"2022-03-06T09:01:51.616411Z","iopub.status.idle":"2022-03-06T09:01:51.697774Z","shell.execute_reply.started":"2022-03-06T09:01:51.616369Z","shell.execute_reply":"2022-03-06T09:01:51.696597Z"}}},{"cell_type":"code","source":"# load data\nimport pickle\nwith open('../input/studentevaluation-preprocessed-datasets/tr_data_bert.pkl', 'rb') as f:\n    Trdata = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:06.574781Z","iopub.execute_input":"2022-03-14T12:58:06.575056Z","iopub.status.idle":"2022-03-14T12:58:06.958495Z","shell.execute_reply.started":"2022-03-14T12:58:06.575019Z","shell.execute_reply":"2022-03-14T12:58:06.957703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trdata.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:06.9616Z","iopub.execute_input":"2022-03-14T12:58:06.961837Z","iopub.status.idle":"2022-03-14T12:58:06.975708Z","shell.execute_reply.started":"2022-03-14T12:58:06.96181Z","shell.execute_reply":"2022-03-14T12:58:06.974863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess...\n\ndef clean_text(text, remove_stopwords=True, stem_words=False, lemma=True):\n    text = str(text).lower().split()\n    \n    if remove_stopwords:\n        stops = stopwords.words(\"english\")\n        text = [x for x in text if not x in stops]\n        \n    text = ' '.join(text)\n    \n    text = re.sub(r\"[-()\\\"#/<>!@&;*:<>{}`'+=~%|.!?,_]\", \" \", text)\n    text = re.sub(r\"\\]\", \" \", text)\n    text = re.sub(r\"\\[\", \" \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\\\\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"  \", \" \", text)\n    text = re.sub(r\"   \", \" \", text)\n    text = re.sub(r\"   \", \" \", text)\n    text = re.sub(r\"0x00\", \"\", text)\n    text = re.sub(r\"summary_text\", \"\", text)\n    text = re.sub(r\"summary\", \"\", text)\n    text = re.sub(r\"text\", \"\", text)\n    \n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stem_words = [stemmer.stem(x) for x in text]\n        text = \" \".join(text)\n        \n    if lemma:\n        text = text.split()\n        lem = WordNetLemmatizer()\n        lemmatized = [lem.lemmatize(x, \"v\") for x in text]\n        text = \" \".join(text)\n        \n    return text","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:06.977384Z","iopub.execute_input":"2022-03-14T12:58:06.978278Z","iopub.status.idle":"2022-03-14T12:58:06.991645Z","shell.execute_reply.started":"2022-03-14T12:58:06.978238Z","shell.execute_reply":"2022-03-14T12:58:06.99096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom string import punctuation\nfrom nltk.stem.wordnet import WordNetLemmatizer\nnltk.download('omw-1.4')\n\ndf['t_summary'] = Trdata.text_summary.apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:06.992839Z","iopub.execute_input":"2022-03-14T12:58:06.99329Z","iopub.status.idle":"2022-03-14T12:58:16.759018Z","shell.execute_reply.started":"2022-03-14T12:58:06.99325Z","shell.execute_reply":"2022-03-14T12:58:16.758234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:16.760526Z","iopub.execute_input":"2022-03-14T12:58:16.760808Z","iopub.status.idle":"2022-03-14T12:58:16.771209Z","shell.execute_reply.started":"2022-03-14T12:58:16.760768Z","shell.execute_reply":"2022-03-14T12:58:16.770444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainData_ = pd.merge(trainData,df)\ntrainData_[40:80]","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:16.772522Z","iopub.execute_input":"2022-03-14T12:58:16.772931Z","iopub.status.idle":"2022-03-14T12:58:16.915637Z","shell.execute_reply.started":"2022-03-14T12:58:16.772895Z","shell.execute_reply":"2022-03-14T12:58:16.914928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pickle\n# trainData_.to_pickle(\"tr_data_vol4.pkl\")","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:16.917103Z","iopub.execute_input":"2022-03-14T12:58:16.917367Z","iopub.status.idle":"2022-03-14T12:58:16.921153Z","shell.execute_reply.started":"2022-03-14T12:58:16.91733Z","shell.execute_reply":"2022-03-14T12:58:16.920187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load data\nimport pickle\nwith open('../input/train-data-vol4/tr_data_vol4.pkl', 'rb') as f:\n    Trdata = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:16.922677Z","iopub.execute_input":"2022-03-14T12:58:16.922923Z","iopub.status.idle":"2022-03-14T12:58:17.938859Z","shell.execute_reply.started":"2022-03-14T12:58:16.922888Z","shell.execute_reply":"2022-03-14T12:58:17.938005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n\ndef clean_stopwords_shortwords(w):\n    stopwords_list=stopwords.words('english')\n    words = w.split() \n    clean_words = [word for word in words if (word not in stopwords_list) and len(word) > 2]\n    return \" \".join(clean_words) \n\ndef preprocess_sentence(w):\n    w = unicode_to_ascii(w.lower().strip())\n    w = re.sub(r\"([?.!,¿])\", r\" \", w)\n    w = re.sub(r'[\" \"]+', \" \", w)\n    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n    w=clean_stopwords_shortwords(w)\n    w=re.sub(r'@\\w+', '',w)\n    return w","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:17.940428Z","iopub.execute_input":"2022-03-14T12:58:17.940731Z","iopub.status.idle":"2022-03-14T12:58:17.950477Z","shell.execute_reply.started":"2022-03-14T12:58:17.940693Z","shell.execute_reply":"2022-03-14T12:58:17.949804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trdata","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:17.952122Z","iopub.execute_input":"2022-03-14T12:58:17.952466Z","iopub.status.idle":"2022-03-14T12:58:17.986304Z","shell.execute_reply.started":"2022-03-14T12:58:17.952429Z","shell.execute_reply":"2022-03-14T12:58:17.985489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_tr = Trdata[['id', 'text', 'discourse_text', 't_summary', 'discourse_type']]\ndata_tr[:5]","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:17.987375Z","iopub.execute_input":"2022-03-14T12:58:17.987721Z","iopub.status.idle":"2022-03-14T12:58:18.043407Z","shell.execute_reply.started":"2022-03-14T12:58:17.987594Z","shell.execute_reply":"2022-03-14T12:58:18.042733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shaffle datasets\ndata = data_tr.sample(frac=1).reset_index(drop=True)\ndata","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:18.044519Z","iopub.execute_input":"2022-03-14T12:58:18.045397Z","iopub.status.idle":"2022-03-14T12:58:18.114254Z","shell.execute_reply.started":"2022-03-14T12:58:18.045358Z","shell.execute_reply":"2022-03-14T12:58:18.113467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes=len(data.discourse_type.unique())\nnum_classes","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:18.115547Z","iopub.execute_input":"2022-03-14T12:58:18.115807Z","iopub.status.idle":"2022-03-14T12:58:18.133339Z","shell.execute_reply.started":"2022-03-14T12:58:18.115776Z","shell.execute_reply":"2022-03-14T12:58:18.132209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip uninstall tensorflow --yes","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:18.134801Z","iopub.execute_input":"2022-03-14T12:58:18.135058Z","iopub.status.idle":"2022-03-14T12:58:19.72606Z","shell.execute_reply.started":"2022-03-14T12:58:18.135023Z","shell.execute_reply":"2022-03-14T12:58:19.725254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow-gpu==1.15","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:19.72876Z","iopub.execute_input":"2022-03-14T12:58:19.729035Z","iopub.status.idle":"2022-03-14T12:58:27.180091Z","shell.execute_reply.started":"2022-03-14T12:58:19.729003Z","shell.execute_reply":"2022-03-14T12:58:27.179203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:27.187039Z","iopub.execute_input":"2022-03-14T12:58:27.187274Z","iopub.status.idle":"2022-03-14T12:58:30.2126Z","shell.execute_reply.started":"2022-03-14T12:58:27.187243Z","shell.execute_reply":"2022-03-14T12:58:30.211833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install bert-tensorflow","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:30.21413Z","iopub.execute_input":"2022-03-14T12:58:30.214389Z","iopub.status.idle":"2022-03-14T12:58:38.031906Z","shell.execute_reply.started":"2022-03-14T12:58:30.214351Z","shell.execute_reply":"2022-03-14T12:58:38.030834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install bert","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:38.035462Z","iopub.execute_input":"2022-03-14T12:58:38.035732Z","iopub.status.idle":"2022-03-14T12:58:45.468506Z","shell.execute_reply.started":"2022-03-14T12:58:38.035694Z","shell.execute_reply":"2022-03-14T12:58:45.467647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install bert-for-tf2","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:45.471519Z","iopub.execute_input":"2022-03-14T12:58:45.471825Z","iopub.status.idle":"2022-03-14T12:58:52.661228Z","shell.execute_reply.started":"2022-03-14T12:58:45.471784Z","shell.execute_reply":"2022-03-14T12:58:52.660317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import bert\nfrom bert import modeling\n\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport sys\nimport zipfile\n\n#import modeling\nfrom bert import optimization\nfrom bert import run_classifier\nfrom bert import tokenization\n\nfrom bert import bert_tokenization\n\n#from tokenization import FullTokenizer\nfrom sklearn.preprocessing import LabelEncoder\n#import tensorflow as tf\n\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow_hub as hub\nfrom tqdm import tqdm_notebook\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:52.664871Z","iopub.execute_input":"2022-03-14T12:58:52.665107Z","iopub.status.idle":"2022-03-14T12:58:55.253404Z","shell.execute_reply.started":"2022-03-14T12:58:52.665076Z","shell.execute_reply":"2022-03-14T12:58:55.252553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sess = tf.Session()\n \nbert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n\n# max_seq_length 토큰 최대 입력길이로,  128이상은 안됨. ㅠㅠ 그래서 이것을 해결해야 함. \n# LONGFORMER로 해야하지만 에러가 발생. 그래서 토큰을 잘라서 적용하는 방법을 생각해봄 \n\n# 하지만 참조할 것  https://github.com/google-research/bert/blob/master/README.md\n\n# H=768 모델 사용 '12/768 (BERT-Base)''\nmax_seq_length = 512","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:55.255025Z","iopub.execute_input":"2022-03-14T12:58:55.255272Z","iopub.status.idle":"2022-03-14T12:58:55.456878Z","shell.execute_reply.started":"2022-03-14T12:58:55.255238Z","shell.execute_reply":"2022-03-14T12:58:55.454818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'Claim' 'Evidence' 'Position' 'Counterclaim' 'Lead'\n# 'Concluding Statement' 'Rebuttal'\n\n\ndata['label'] = data['discourse_type'].map({'Claim':0,'Evidence':1, 'Position':2,'Counterclaim':3, 'Lead':4, 'Concluding Statement':5, 'Rebuttal':6})\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:55.458453Z","iopub.execute_input":"2022-03-14T12:58:55.459538Z","iopub.status.idle":"2022-03-14T12:58:55.49476Z","shell.execute_reply.started":"2022-03-14T12:58:55.459497Z","shell.execute_reply":"2022-03-14T12:58:55.494016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['t_summary', 'discourse_text']\ndata['text_concat'] = data[cols].apply(lambda row: ':'.join(row.values.astype(str)), axis=1)\ndata","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:55.496103Z","iopub.execute_input":"2022-03-14T12:58:55.496391Z","iopub.status.idle":"2022-03-14T12:58:57.838952Z","shell.execute_reply.started":"2022-03-14T12:58:55.496351Z","shell.execute_reply":"2022-03-14T12:58:57.838053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(data, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:57.840506Z","iopub.execute_input":"2022-03-14T12:58:57.840803Z","iopub.status.idle":"2022-03-14T12:58:57.934298Z","shell.execute_reply.started":"2022-03-14T12:58:57.84076Z","shell.execute_reply":"2022-03-14T12:58:57.93349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:57.935736Z","iopub.execute_input":"2022-03-14T12:58:57.936038Z","iopub.status.idle":"2022-03-14T12:58:57.952313Z","shell.execute_reply.started":"2022-03-14T12:58:57.935996Z","shell.execute_reply":"2022-03-14T12:58:57.951039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_df_sample = train_df.query(\"id == 'D0C7DA676408'\")['text_concat'].values[0]\ntr_df_sample","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:57.954341Z","iopub.execute_input":"2022-03-14T12:58:57.954729Z","iopub.status.idle":"2022-03-14T12:58:57.971396Z","shell.execute_reply.started":"2022-03-14T12:58:57.954686Z","shell.execute_reply":"2022-03-14T12:58:57.970576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, val_df = train_test_split(test_df, test_size=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:57.972846Z","iopub.execute_input":"2022-03-14T12:58:57.973101Z","iopub.status.idle":"2022-03-14T12:58:57.986258Z","shell.execute_reply.started":"2022-03-14T12:58:57.973067Z","shell.execute_reply":"2022-03-14T12:58:57.9855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_encoder = LabelEncoder().fit(pd.concat([train_df['discourse_type'], val_df['discourse_type']]))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:57.98736Z","iopub.execute_input":"2022-03-14T12:58:57.987549Z","iopub.status.idle":"2022-03-14T12:58:57.998174Z","shell.execute_reply.started":"2022-03-14T12:58:57.987525Z","shell.execute_reply":"2022-03-14T12:58:57.997458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check Classes\nprint(label_encoder.classes_)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:58.000306Z","iopub.execute_input":"2022-03-14T12:58:58.000783Z","iopub.status.idle":"2022-03-14T12:58:58.005558Z","shell.execute_reply.started":"2022-03-14T12:58:58.000746Z","shell.execute_reply":"2022-03-14T12:58:58.004744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:58.006843Z","iopub.execute_input":"2022-03-14T12:58:58.00748Z","iopub.status.idle":"2022-03-14T12:58:58.027595Z","shell.execute_reply.started":"2022-03-14T12:58:58.00744Z","shell.execute_reply":"2022-03-14T12:58:58.026552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data concat full text and discourse_text\nX_train_val, X_test = pd.concat([train_df['text_concat'], val_df['text_concat']]).values, test_df['text_concat'].values\n \ny_train_val = label_encoder.fit_transform(pd.concat([train_df['discourse_type'], val_df['discourse_type']]))","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:58.02916Z","iopub.execute_input":"2022-03-14T12:58:58.029668Z","iopub.status.idle":"2022-03-14T12:58:58.049845Z","shell.execute_reply.started":"2022-03-14T12:58:58.029625Z","shell.execute_reply":"2022-03-14T12:58:58.049071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['text_concat'].values","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:58.05113Z","iopub.execute_input":"2022-03-14T12:58:58.051502Z","iopub.status.idle":"2022-03-14T12:58:58.062701Z","shell.execute_reply.started":"2022-03-14T12:58:58.051458Z","shell.execute_reply":"2022-03-14T12:58:58.059516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_val","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:58.064963Z","iopub.execute_input":"2022-03-14T12:58:58.071928Z","iopub.status.idle":"2022-03-14T12:58:58.078691Z","shell.execute_reply.started":"2022-03-14T12:58:58.071846Z","shell.execute_reply":"2022-03-14T12:58:58.077728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:58.080477Z","iopub.execute_input":"2022-03-14T12:58:58.081064Z","iopub.status.idle":"2022-03-14T12:58:58.089481Z","shell.execute_reply.started":"2022-03-14T12:58:58.081021Z","shell.execute_reply":"2022-03-14T12:58:58.088483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_val","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:58.091076Z","iopub.execute_input":"2022-03-14T12:58:58.091541Z","iopub.status.idle":"2022-03-14T12:58:58.098857Z","shell.execute_reply.started":"2022-03-14T12:58:58.0915Z","shell.execute_reply":"2022-03-14T12:58:58.097841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(\n        X_train_val,y_train_val, test_size=0.1, random_state=0, stratify = y_train_val\n        )","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:58.100513Z","iopub.execute_input":"2022-03-14T12:58:58.101094Z","iopub.status.idle":"2022-03-14T12:58:58.124395Z","shell.execute_reply.started":"2022-03-14T12:58:58.101053Z","shell.execute_reply":"2022-03-14T12:58:58.123629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_val","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:58.125761Z","iopub.execute_input":"2022-03-14T12:58:58.126057Z","iopub.status.idle":"2022-03-14T12:58:58.132216Z","shell.execute_reply.started":"2022-03-14T12:58:58.126018Z","shell.execute_reply":"2022-03-14T12:58:58.131254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # max_seq_length의 길리만큼 끊어서 넘파이 어레이로 만듬. 이것을 가지로 학습, 검증, 테스트를 수행 \ntrain_text = X_train\ntrain_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\ntrain_text = np.array(train_text, dtype=object)[:, np.newaxis]\ntrain_label = y_train\n \nval_text = X_val\nval_text = [' '.join(t.split()[0:max_seq_length]) for t in val_text]\nval_text = np.array(val_text, dtype=object)[:, np.newaxis]\nval_label = y_val\n \ntest_text = X_test\ntest_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\ntest_text = np.array(test_text, dtype=object)[:, np.newaxis]","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:58.13396Z","iopub.execute_input":"2022-03-14T12:58:58.134683Z","iopub.status.idle":"2022-03-14T12:58:58.547699Z","shell.execute_reply.started":"2022-03-14T12:58:58.134565Z","shell.execute_reply":"2022-03-14T12:58:58.54691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_text","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:58.54907Z","iopub.execute_input":"2022-03-14T12:58:58.549324Z","iopub.status.idle":"2022-03-14T12:58:58.559681Z","shell.execute_reply.started":"2022-03-14T12:58:58.549287Z","shell.execute_reply":"2022-03-14T12:58:58.558957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_text","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:58.562366Z","iopub.execute_input":"2022-03-14T12:58:58.562557Z","iopub.status.idle":"2022-03-14T12:58:58.570674Z","shell.execute_reply.started":"2022-03-14T12:58:58.562533Z","shell.execute_reply":"2022-03-14T12:58:58.569725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport os\nimport re\nimport numpy as np\nfrom tqdm import tqdm_notebook\nfrom tensorflow import keras \nfrom tensorflow.keras.layers import Layer","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:58.572431Z","iopub.execute_input":"2022-03-14T12:58:58.572922Z","iopub.status.idle":"2022-03-14T12:58:58.579667Z","shell.execute_reply.started":"2022-03-14T12:58:58.572886Z","shell.execute_reply":"2022-03-14T12:58:58.578716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertLayer(Layer):\n    \n     #bert_output = BertLayer(n_fine_tune_layers=n_fine_tune, tf_hub = tf_hub, output_representation = 'mean_pooling', is_trainable = True, output_size = 768, supports_masking = True)(bert_inputs)\n     #TypeError: __init__() missing 3 required positional arguments: 'is_trainble', 'output_size', and 'supports_masking'\n        \n    # init\n    def __init__(self, n_fine_tune_layers=10, tf_hub = None, output_representation = 'pooled_output', trainable = False, **kwargs):\n        \n        self.n_fine_tune_layers = n_fine_tune_layers\n        self.is_trainble = trainable\n        self.output_size = 768\n        self.tf_hub = tf_hub\n        self.output_representation = output_representation\n        self.supports_masking = True\n        \n        super(BertLayer, self).__init__(**kwargs)\n\n        \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'n_fine_tune_layers' : self.n_fine_tune_layers,\n            'tf_hub' : self.tf_hub,\n            'output_representation' : self.output_representation,\n            'is_trainble' : self.is_trainble,\n            'output_size' : self.output_size,\n            'supports_masking' : self.supports_masking,\n        })        \n        return config\n\n    \n    def build(self, input_shape):\n\n        self.bert = hub.Module(\n            self.tf_hub,\n            trainable=self.is_trainble,\n            name=\"{}_module\".format(self.name)\n        )\n        \n        \n        variables = list(self.bert.variable_map.values())\n        if self.is_trainble:\n            # 1 first remove unused layers\n            trainable_vars = [var for var in variables if not \"/cls/\" in var.name]\n            \n            \n            if self.output_representation == \"sequence_output\" or self.output_representation == \"mean_pooling\":\n                # 1 first remove unused pooled layers\n                trainable_vars = [var for var in trainable_vars if not \"/pooler/\" in var.name]\n                \n            # Select how many layers to fine tune\n            trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n            \n            # Add to trainable weights\n            for var in trainable_vars:\n                self._trainable_weights.append(var)\n\n            # Add non-trainable weights\n            for var in self.bert.variables:\n                if var not in self._trainable_weights:\n                    self._non_trainable_weights.append(var)\n                \n        else:\n             for var in variables:\n                self._non_trainable_weights.append(var)\n                \n\n        super(BertLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n        input_ids, input_mask, segment_ids = inputs\n        bert_inputs = dict(\n            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n        )\n        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n        \n        if self.output_representation == \"pooled_output\":\n            pooled = result[\"pooled_output\"]\n            \n        elif self.output_representation == \"mean_pooling\":\n            result_tmp = result[\"sequence_output\"]\n        \n            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n            input_mask = tf.cast(input_mask, tf.float32)\n            pooled = masked_reduce_mean(result_tmp, input_mask)\n            \n        elif self.output_representation == \"sequence_output\":\n            \n            pooled = result[\"sequence_output\"]\n       \n        return pooled\n    \n    def compute_mask(self, inputs, mask=None):\n        \n        if self.output_representation == 'sequence_output':\n            inputs = [K.cast(x, dtype=\"bool\") for x in inputs]\n            mask = inputs[1]\n            \n            return mask\n        else:\n            return None\n        \n        \n    def compute_output_shape(self, input_shape):\n        if self.output_representation == \"sequence_output\":\n            return (input_shape[0][0], input_shape[0][1], self.output_size)\n        else:\n            return (input_shape[0][0], self.output_size)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:58.582908Z","iopub.execute_input":"2022-03-14T12:58:58.583677Z","iopub.status.idle":"2022-03-14T12:58:58.605315Z","shell.execute_reply.started":"2022-03-14T12:58:58.583634Z","shell.execute_reply":"2022-03-14T12:58:58.604522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(max_seq_length, tf_hub, n_classes, n_fine_tune): \n    in_id = keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n    in_mask = keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n    in_segment = keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n    bert_inputs = [in_id, in_mask, in_segment]\n    \n    #bert_output = BertLayer(n_fine_tune_layers=n_fine_tune, tf_hub = tf_hub, output_representation = 'mean_pooling', is_trainable = True)(bert_inputs)\n    bert_output = BertLayer(n_fine_tune_layers=n_fine_tune, tf_hub = tf_hub, output_representation = 'mean_pooling', trainable = True)(bert_inputs)\n    drop = keras.layers.Dropout(0.3)(bert_output)\n    dense = keras.layers.Dense(256, activation='sigmoid')(drop)\n    drop = keras.layers.Dropout(0.3)(dense)\n    dense = keras.layers.Dense(64, activation='sigmoid')(drop)\n    pred = keras.layers.Dense(n_classes, activation='softmax')(dense)\n    \n    model = keras.models.Model(inputs=bert_inputs, outputs=pred)\n    \n    #Adam = keras.optimizers.Adam(lr = 5e-05) ->   only not improved val_loss\n    #Adam = keras.optimizers.Adam(lr = 0.0005) ->   not imporved at all\n    #Adam = keras.optimizers.Adam(lr = 0.000005) -> val loss 0.87\n    #sgd =  keras.optimizers.SGD(lr=0.00005, decay=1e-6, momentum=0.9, nesterov=True) -> need to increse Epoh over 60 + \n    #sgd =  keras.optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n    #sgd =  keras.optimizers.SGD(lr=0.00005, decay=1e-6, momentum=0.9, nesterov=True) # increse Epoh over 60 +\n    #Adam = keras.optimizers.Adam(lr = 5e-05, epsilon=1e-08, decay = 0.01, clipnorm = 1.0) -> loss: 0.8296 - sparse_categorical_accuracy: 0.7596 - val_loss: 0.8668 - val_sparse_categorical_accuracy: 0.7360\n    Adam = keras.optimizers.Adam(lr=1e-5, epsilon=1e-8)\n    \n    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam, metrics=['sparse_categorical_accuracy'])\n    model.summary()\n\n    return model\n\ndef initialize_vars(sess): #tf.compat.v1.keras.backend.set_session\n    sess.run(tf.local_variables_initializer())\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.tables_initializer())\n    K.set_session(sess)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:58.606815Z","iopub.execute_input":"2022-03-14T12:58:58.607301Z","iopub.status.idle":"2022-03-14T12:58:58.619325Z","shell.execute_reply.started":"2022-03-14T12:58:58.607263Z","shell.execute_reply":"2022-03-14T12:58:58.618569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_classes = len(label_encoder.classes_)\nn_fine_tune_layers = 48\nmodel = build_model(max_seq_length, bert_path, n_classes, n_fine_tune_layers)\n\n# Instantiate variables\ninitialize_vars(sess)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:58:58.620642Z","iopub.execute_input":"2022-03-14T12:58:58.621375Z","iopub.status.idle":"2022-03-14T12:59:13.150236Z","shell.execute_reply.started":"2022-03-14T12:58:58.621339Z","shell.execute_reply":"2022-03-14T12:59:13.149443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.trainable_weights","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:59:13.151833Z","iopub.execute_input":"2022-03-14T12:59:13.152097Z","iopub.status.idle":"2022-03-14T12:59:13.159234Z","shell.execute_reply.started":"2022-03-14T12:59:13.152059Z","shell.execute_reply":"2022-03-14T12:59:13.158528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reference : https://github.com/dmis-lab/biobert/blob/master/run_classifier.py\n\nclass PaddingInputExample(object):\n    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n  When running eval/predict on the TPU, we need to pad the number of examples\n  to be a multiple of the batch size, because the TPU requires a fixed batch\n  size. The alternative is to drop the last batch, which is bad because it means\n  the entire output data won't be generated.\n  We use this class instead of `None` because treating `None` as padding\n  battches could cause silent errors.\n  \"\"\"\n\nclass InputExample(object):\n    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        \"\"\"Constructs a InputExample.\n    Args:\n      guid: Unique id for the example.\n      text_a: string. The untokenized text of the first sequence. For single\n        sequence tasks, only this sequence must be specified.\n      text_b: (Optional) string. The untokenized text of the second sequence.\n        Only must be specified for sequence pair tasks.\n      label: (Optional) string. The label of the example. This should be\n        specified for train and dev examples, but not for test examples.\n    \"\"\"\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\ndef create_tokenizer_from_hub_module(tf_hub):\n    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n    bert_module =  hub.Module(tf_hub)\n    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n    vocab_file, do_lower_case = sess.run(\n        [\n            tokenization_info[\"vocab_file\"],\n            tokenization_info[\"do_lower_case\"],\n        ]\n    )\n    \n    return bert_tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n\ndef convert_single_example(tokenizer, example, max_seq_length=256):\n    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n\n    if isinstance(example, PaddingInputExample):\n        input_ids = [0] * max_seq_length\n        input_mask = [0] * max_seq_length\n        segment_ids = [0] * max_seq_length\n        label = 0\n        return input_ids, input_mask, segment_ids, label\n\n    tokens_a = tokenizer.tokenize(example.text_a)\n    if len(tokens_a) > max_seq_length - 2:\n        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n\n    tokens = []\n    segment_ids = []\n    tokens.append(\"[CLS]\")\n    segment_ids.append(0)\n    for token in tokens_a:\n        tokens.append(token)\n        segment_ids.append(0)\n    tokens.append(\"[SEP]\")\n    segment_ids.append(0)\n    \n    #print(tokens)\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    input_mask = [1] * len(input_ids)\n\n    # Zero-pad up to the sequence length.\n    while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n    assert len(input_ids) == max_seq_length\n    assert len(input_mask) == max_seq_length\n    assert len(segment_ids) == max_seq_length\n\n    return input_ids, input_mask, segment_ids, example.label\n\ndef convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n\n    input_ids, input_masks, segment_ids, labels = [], [], [], []\n    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n        input_id, input_mask, segment_id, label = convert_single_example(\n            tokenizer, example, max_seq_length\n        )\n        input_ids.append(input_id)\n        input_masks.append(input_mask)\n        segment_ids.append(segment_id)\n        labels.append(label)\n    return (\n        np.array(input_ids),\n        np.array(input_masks),\n        np.array(segment_ids),\n        np.array(labels).reshape(-1, 1),\n    )\n\ndef convert_text_to_examples(texts, labels):\n    \"\"\"Create InputExamples\"\"\"\n    InputExamples = []\n    for text, label in zip(texts, labels):\n        InputExamples.append(\n            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n        )\n    return InputExamples","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:59:13.16095Z","iopub.execute_input":"2022-03-14T12:59:13.161527Z","iopub.status.idle":"2022-03-14T12:59:13.181097Z","shell.execute_reply.started":"2022-03-14T12:59:13.161483Z","shell.execute_reply":"2022-03-14T12:59:13.180369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Instantiate tokenizer  bert_tokenization.FullTokenizer\ntokenizer = create_tokenizer_from_hub_module(bert_path)\n\n# Convert data to InputExample format\ntrain_examples = convert_text_to_examples(train_text, train_label)\nval_examples = convert_text_to_examples(val_text, val_label)\ntest_examples = convert_text_to_examples(test_text, val_label)\n\n# Convert to features\n(train_input_ids, train_input_masks, train_segment_ids, train_labels \n) = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n(val_input_ids, val_input_masks, val_segment_ids, val_labels\n) = convert_examples_to_features(tokenizer, val_examples, max_seq_length=max_seq_length)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T12:59:13.182878Z","iopub.execute_input":"2022-03-14T12:59:13.183453Z","iopub.status.idle":"2022-03-14T13:00:18.836114Z","shell.execute_reply.started":"2022-03-14T12:59:13.183414Z","shell.execute_reply":"2022-03-14T13:00:18.83526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The following code hans been annotated because it takes a lot of time to learn when executing the code.\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-07T22:50:30.831448Z","iopub.status.idle":"2022-03-07T22:50:30.832207Z","shell.execute_reply.started":"2022-03-07T22:50:30.831954Z","shell.execute_reply":"2022-03-07T22:50:30.83198Z"}}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nBATCH_SIZE = 16\n\n\nMONITOR = 'val_sparse_categorical_accuracy'\nprint('BATCH_SIZE is {}'.format(BATCH_SIZE))\ne_stopping = EarlyStopping(monitor=MONITOR, patience=3, verbose=1, mode='max', restore_best_weights=True)\ncallbacks =  [e_stopping]\n\nhistory = model.fit(\n   [train_input_ids, train_input_masks, train_segment_ids], \n    train_labels,\n    validation_data = ([val_input_ids, val_input_masks, val_segment_ids], val_labels),\n    epochs = 50,\n    verbose = 1,\n    batch_size = BATCH_SIZE,\n    callbacks= callbacks\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T13:00:18.838138Z","iopub.execute_input":"2022-03-14T13:00:18.838727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n \n    input_ids, input_masks, segment_ids, labels = [], [], [], []\n    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n        input_id, input_mask, segment_id, label = convert_single_example(\n            tokenizer, example, max_seq_length\n        )\n        input_ids.append(input_id)\n        input_masks.append(input_mask)\n        segment_ids.append(segment_id)\n        labels.append(label)\n    return (\n        np.array(input_ids),\n        np.array(input_masks),\n        np.array(segment_ids),\n        np.array(labels).reshape(-1, 1),\n    )\n \ndef convert_text_to_examples(texts, labels):\n    \"\"\"Create InputExamples\"\"\"\n    InputExamples = []\n    for text, label in zip(texts, labels):\n        InputExamples.append(\n            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n        )\n    return InputExamples","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(test_input_ids, test_input_masks, test_segment_ids, test_labels\n) = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = model.predict([test_input_ids, test_input_masks, test_segment_ids], verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = label_encoder.classes_[np.argmax(prediction, axis =1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(preds, columns=['label']).to_csv('/kaggle/input/feedback-prize-2021/submission.csv',index_label='id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Save model\nmodel.save('evauation.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ../input/evaluation/evauation.h5\n\n# 'n_fine_tune_layers' : self.n_fine_tune_layers,\n# 'is_trainble' : self.is_trainble,\n# 'output_size' : self.output_size,\n# 'tf_hub' : self.tf_hub,\n# 'output_representation' : self.output_representation,\n# 'supports_masking' : self.supports_masking,\n                \n                \n# from tensorflow import keras\n# #model = keras.models.load_model('../input/evaluation/evauation.h5')\n# savedMode = keras.models.load_model('../input/evaluation/evauation.h5', custom_objects={\"build_model\":build_model})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/feedback-prize-2021/sample_submission.csv')\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_df_test():\n    test_names, test_texts = [], []\n    for f in tqdm(list(os.listdir('test'))):\n        test_names.append(f.replace('.txt', ''))\n        test_texts.append(open('test/' + f, 'r', encoding='UTF-8').read())\n    test_text_df = pd.DataFrame({'id': test_names, 'text': test_texts})\n    return test_text_df\n\ntest_df_sub = load_df_test()\ntest_df_sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_summary_list = []\nfor text in tqdm(test_df_sub['text']):\n    try:\n        ptorch_summary = summarizer(text, min_length=5, max_length=200)\n    except:\n        pass\n    \n    test_summary_list.append(ptorch_summary)\n    \ntest_df_sub['text_summary'] = test_summary_list\ntest_df_sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_sub['cleaned_summary'] = test_df_sub.text_summary.apply(clean_text)\ntest_df_sub","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discouse_text_pre_list = []\nfor text in tqdm(test_df_sub['text']):\n    sent_token = sent_tokenize(text)\n    discouse_text_pre_list.append(sent_token)\n    \ntest_df_sub['discourse_text'] = discouse_text_pre_list\ntest_df_sub","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_sub['id']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for each_id in tqdm(test_df_sub['id']):\n    print(type(each_id))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testList = test_df_sub.query(\"id == '0FB0700DAF44'\")['discourse_text'].values[0]\ntestList[:3]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get_text = test_df_sub.query(\"id == '0FB0700DAF44'\")['text'].values[0]\n# get_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for each_id in tqdm(test_df_sub['id']):\n    #print(\"each_id :\", each_id)\n    get_dis_text_list = test_df_sub.query(\"id == @each_id\")['discourse_text'].values[0]\n    get_text = test_df_sub.query(\"id == @each_id\")['text'].values[0]\n    get_cleaned_summary = test_df_sub.query(\"id == @each_id\")['cleaned_summary'].values[0]\n    for disc_text in get_dis_text_list:\n        data_insert = {'id': each_id, 'text':get_text, 'cleaned_summary': get_cleaned_summary,'new_discourse_text':disc_text }\n        test_df_sub = test_df_sub.append(data_insert, ignore_index=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_sub[['id', 'text', 'cleaned_summary', 'new_discourse_text']][:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_test_df = pd.DataFrame(columns=['id', 'text', 'cleaned_summary', 'new_discourse_text'])\nnew_test_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_test_df = pd.concat([new_test_df, test_df_sub[['id', 'text', 'cleaned_summary', 'new_discourse_text']]])\nnew_test_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_test_df =  new_test_df.drop([0,1,2,3,4])\nnew_test_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_test_df_ = pd.DataFrame(columns=['id', 'text', 'cleaned_summary', 'new_discourse_text'])\nnew_test_df_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_test_df__ = pd.concat([new_test_df_, new_test_df[['id', 'text', 'cleaned_summary', 'new_discourse_text']]])\nnew_test_df___ = new_test_df__.reset_index(drop=True)\nnew_test_df___","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['cleaned_summary','new_discourse_text']\nnew_test_df___['text_concat'] = new_test_df___[cols].apply(lambda row: ':'.join(row.values.astype(str)), axis=1)\nnew_test_df___","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_text = new_test_df___['text_concat'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_examples = convert_text_to_examples(test_text, val_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(test_input_ids, test_input_masks, test_segment_ids, test_labels\n) = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = model.predict([test_input_ids, test_input_masks, test_segment_ids], verbose = 1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = label_encoder.classes_[np.argmax(prediction, axis =1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df['id'] = new_test_df___['id']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df['class'] = preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST_PATH = \"test\"\n\ndef get_test_text(a_id):\n    a_file = f\"{TEST_PATH}/{a_id}.txt\"\n    with open(a_file, \"r\") as fp:\n        txt = fp.read()\n    return txt\n\ndef create_df_test():\n    test_ids = [f[:-4] for f in os.listdir(TEST_PATH)] #Remove the last 4 characters ('.txt') in the filenames such as '0FB0700DAF44.txt'.\n    test_data = []\n    for test_id in test_ids:\n        text = get_test_text(test_id)\n        sentences = nltk.sent_tokenize(text)\n        id_sentences = []\n        idx = 0 \n        for sentence in sentences:\n            id_sentence = []\n            words = sentence.split()\n            # I created this heuristic for mapping words in sentences to \"word indices\"\n            # This is not definitive and might have strong drawbacks and problems\n            for w in words:\n                id_sentence.append(idx)\n                idx+=1\n            id_sentences.append(id_sentence)\n        test_data += list(zip([test_id] * len(sentences), sentences, id_sentences))\n    df_test = pd.DataFrame(test_data, columns=['id', 'discourse_text', 'ids'])\n    return df_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = create_df_test()\ndf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['predictionstring'] = df_test['ids'].apply(lambda x: ' '.join([str(i) for i in x]))\ndf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['predictionstring']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df['predictionstring'] = df_test['predictionstring']\nsubmission_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df[100:120]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pd.DataFrame(submission_df).to_csv('submission_result.csv', index_label='id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_confirm = pd.read_csv('../input/submission-result/submission_result.csv')\n# submission_confirm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(submission_df).to_csv('submit.csv', index_label='id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}