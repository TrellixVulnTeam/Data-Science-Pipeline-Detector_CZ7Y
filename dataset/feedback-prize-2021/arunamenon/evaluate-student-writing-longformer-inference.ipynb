{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Feedback Prize - Evaluating Student Writing\n#### Analyze argumentative writing elements from students grade 6-12\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/31779/logos/header.png?t=2021-11-12-22-52-17&quot)","metadata":{}},{"cell_type":"markdown","source":"https://drive.google.com/file/d/1r9vxKn5Az3ZBoZ7PgkmIh2ov87axyN8B/view","metadata":{}},{"cell_type":"markdown","source":"# Import packages","metadata":{}},{"cell_type":"code","source":"# Install packages","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:25:40.375657Z","iopub.execute_input":"2022-03-12T22:25:40.376481Z","iopub.status.idle":"2022-03-12T22:25:40.380322Z","shell.execute_reply.started":"2022-03-12T22:25:40.376436Z","shell.execute_reply":"2022-03-12T22:25:40.379616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport re\nfrom tqdm import tqdm\ntqdm.pandas()\nimport plotly.express as px\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\nimport torch\nimport torch.nn as nn\nfrom joblib import Parallel, delayed\nfrom transformers import AutoModelForTokenClassification\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, TrainingArguments, Trainer, EvalPrediction\nimport sys\nimport time\nimport gc\nimport pickle\nimport re\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import AdamW\nimport sklearn\nSEED = 42","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:25:40.382308Z","iopub.execute_input":"2022-03-12T22:25:40.382804Z","iopub.status.idle":"2022-03-12T22:25:50.149695Z","shell.execute_reply.started":"2022-03-12T22:25:40.382765Z","shell.execute_reply":"2022-03-12T22:25:50.148981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling - NER","metadata":{}},{"cell_type":"code","source":"tag2idx = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 2,\n    \"I-Position\": 3,\n    \"B-Evidence\": 4,\n    \"I-Evidence\": 5,\n    \"B-Claim\": 6,\n    \"I-Claim\": 7,\n    \"B-Concluding Statement\": 8,\n    \"I-Concluding Statement\": 9,\n    \"B-Counterclaim\": 10,\n    \"I-Counterclaim\": 11,\n    \"B-Rebuttal\": 12,\n    \"I-Rebuttal\": 13,\n    \"O\": 14\n}\n\n\nidx2tag = {v: k for k, v in tag2idx.items()}","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:25:50.150899Z","iopub.execute_input":"2022-03-12T22:25:50.151143Z","iopub.status.idle":"2022-03-12T22:25:50.156247Z","shell.execute_reply.started":"2022-03-12T22:25:50.15111Z","shell.execute_reply":"2022-03-12T22:25:50.155623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_sentence(x):\n    \"takes in a string and returns tokenized list\"\n\n    return [x for x in x.strip().lower().split(\" \") if len(x) > 0]","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:25:50.157889Z","iopub.execute_input":"2022-03-12T22:25:50.158218Z","iopub.status.idle":"2022-03-12T22:25:50.180735Z","shell.execute_reply.started":"2022-03-12T22:25:50.158186Z","shell.execute_reply":"2022-03-12T22:25:50.180018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_tags(sent, rec_id):\n    '''\n    Input: sent as a sentence tokenized as list of tokens\n    output: tags \n    '''\n    sent = [x for x in sent]\n    data = ' '.join(sent)\n    \n    train_data = pd.DataFrame(data = None, columns = ['id'])\n    temp = train_data[train_data['id']==rec_id].reset_index(drop = True)\n    all_tags = []\n    correct_tags = []\n    \n    if temp.shape[0] > 0:\n        for j in range(0, len(temp)):     \n            pred_string = temp['discourse_text'].iloc[j]\n            pred_string = re.sub('\\n+', ' ', pred_string).strip().replace(\"\\xa0\",\" \").lower()\n            discourse_type = temp['discourse_type'].iloc[j]\n            ep = tokenize_sentence(pred_string)\n\n            start_index = data.find(pred_string)\n            stop_index = data.find(pred_string) + len(pred_string)\n            tokens_for_tagging = data[start_index:stop_index].split(' ')\n\n            i = 0\n            tag = []\n            tag_flag = False\n            while i < len(sent):\n                if (start_index == 0) & (i == 0) :\n                    tag.append('B-' + discourse_type)\n                    i+=1\n\n                    tag = tag + ['I-' + discourse_type] * (len(ep)-1)\n                    i+=(len(ep)-1)\n                    tag_flag = True\n                elif (data.split(' ')[i] == tokens_for_tagging[0]) & (len(' '.join(data.split(' ')[0:i]))+1 == start_index) & (not tag_flag):\n                    tag.append('B-' + discourse_type)\n                    i+=1\n\n                    tag = tag + ['I-' + discourse_type] * (len(ep)-1)\n                    i+=(len(ep)-1)\n                else:\n                    tag.append('O')                \n                    i+=1\n            all_tags.append(tag)\n            # Update text to mask already tagged tokens\n            data = ' '.join(['masked_token' if (('B' in tag[x]) or ('I' in tag[x])) else data.split(' ')[x] for x in range(0, len(data.split(' ')))])\n\n        for i in range(0, len(all_tags[0])):\n            sum_O_tags = 0\n            tags = []\n            for j in range(0, len(all_tags)):\n                if all_tags[j][i] == 'O':\n                    sum_O_tags += 1\n                else:\n                    tags.append(all_tags[j][i])\n            if sum_O_tags == len(all_tags):\n                correct_tags.append('O')\n            else:\n                if len(tags)>1:\n                    print('More than 1 tag for a token in record_id', rec_id)\n                    print(tags)\n                correct_tags.append(tags[0])\n\n    else:\n        correct_tags = ['O'] * len(sent)\n    return correct_tags","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:25:50.182255Z","iopub.execute_input":"2022-03-12T22:25:50.182546Z","iopub.status.idle":"2022-03-12T22:25:50.201411Z","shell.execute_reply.started":"2022-03-12T22:25:50.182513Z","shell.execute_reply":"2022-03-12T22:25:50.200697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vocab_sent_tokenize_label(sent_tokenized, token_tag):\n    try:\n        vocab_sent_token = []\n        sent_input_ids = []\n        vocab_token_tag = []\n        token_tag_ids = []\n        for sent_token_, tag_ in zip(sent_tokenized, token_tag):\n            _vocab_sent_token = tokenizer.tokenize(sent_token_)\n            _sent_input_ids = [\n                tokenizer.convert_tokens_to_ids(x) for x in _vocab_sent_token\n            ]\n            _vocab_token_tag = [tag_] * len(_vocab_sent_token)\n            _token_tag_ids = [tag2idx[x] for x in _vocab_token_tag]\n\n            vocab_sent_token.extend(_vocab_sent_token)\n            sent_input_ids.extend(_sent_input_ids)\n            vocab_token_tag.extend(_vocab_token_tag)\n            token_tag_ids.extend(_token_tag_ids)\n        return vocab_sent_token, sent_input_ids, vocab_token_tag, token_tag_ids\n    except Exception as e:\n        print(f\"Error in line no: {sys.exc_info()[2].tb_lineno}\")\n        print(e)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:25:50.204708Z","iopub.execute_input":"2022-03-12T22:25:50.204933Z","iopub.status.idle":"2022-03-12T22:25:50.213068Z","shell.execute_reply.started":"2022-03-12T22:25:50.20491Z","shell.execute_reply":"2022-03-12T22:25:50.212321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sent_tag_tokenization(data):\n    try:\n        model_data_preprocessing = data.copy(deep=True)\n        model_data_preprocessing['sent_tokenized'] = model_data_preprocessing[\n            'cleaned_text'].progress_apply(tokenize_sentence)\n\n        model_data_preprocessing[\n            'token_tag'] = model_data_preprocessing.progress_apply(\n                lambda x: get_tags(sent=x['sent_tokenized'], rec_id=x['id'])\n                if (isinstance(x['sent_tokenized'], list)) else np.nan, axis=1)\n\n        model_data_preprocessing['vocab_sent_tokenized'], model_data_preprocessing[\n            'sent_input_ids'], model_data_preprocessing[\n                'vocab_token_tag'], model_data_preprocessing['token_tag_ids'] = zip(\n                    *model_data_preprocessing.\n                    progress_apply(lambda x: vocab_sent_tokenize_label(\n                        sent_tokenized=x['sent_tokenized'], token_tag=x['token_tag'])\n                                   if isinstance(x['token_tag'], list) else np.nan,\n                                   axis=1))\n        return model_data_preprocessing\n    except Exception as e:\n        print(f\"Error in line no: {sys.exc_info()[2].tb_lineno}\")\n        print(e)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:25:50.214789Z","iopub.execute_input":"2022-03-12T22:25:50.215225Z","iopub.status.idle":"2022-03-12T22:25:50.226503Z","shell.execute_reply.started":"2022-03-12T22:25:50.21519Z","shell.execute_reply":"2022-03-12T22:25:50.225583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad_data(input_ids, token_ids, max_token_length):\n    try:\n        attention_mask = []\n        for input_ in tqdm(input_ids):\n            attention_mask.append(torch.ones(len(input_[:max_token_length])))\n\n        padded_attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask,\n                                                                batch_first=True,\n                                                                padding_value=0.0)\n\n        padded_input_ids = torch.nn.utils.rnn.pad_sequence(\n            [torch.tensor(input_[:max_token_length]) for input_ in input_ids],\n            batch_first=True,\n            padding_value=0.0)\n\n        padded_tags = torch.nn.utils.rnn.pad_sequence(\n            [torch.tensor(tag_[:max_token_length]) for tag_ in token_ids],\n            batch_first=True,\n            padding_value=0.0)\n        return padded_input_ids, padded_attention_mask, padded_tags\n    except Exception as e:\n        print(f\"Error in line no: {sys.exc_info()[2].tb_lineno}\")\n        print(e)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:25:50.228904Z","iopub.execute_input":"2022-03-12T22:25:50.231247Z","iopub.status.idle":"2022-03-12T22:25:50.241464Z","shell.execute_reply.started":"2022-03-12T22:25:50.231214Z","shell.execute_reply":"2022-03-12T22:25:50.240727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dataloader(token_ids, masks, tags, batch_size=16, val=False):\n    try:\n        # wrap tensors\n        data = TensorDataset(token_ids, masks, tags)\n\n        if val:\n            # sampler for sampling the data during training\n            sampler = SequentialSampler(data)\n            \n            # dataLoader for validation set\n            dataloader = DataLoader(data,\n                                    sampler=sampler,\n                                    batch_size=batch_size)\n        else:    \n            # sampler for sampling the data during training\n            sampler = RandomSampler(data)\n            # dataLoader for train set\n            dataloader = DataLoader(data,\n                                    sampler=sampler,\n                                    batch_size=batch_size)\n        return dataloader\n    except Exception as e:\n        print(f\"Error in line no: {sys.exc_info()[2].tb_lineno}\")\n        print(e)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:25:50.245149Z","iopub.execute_input":"2022-03-12T22:25:50.24536Z","iopub.status.idle":"2022-03-12T22:25:50.252145Z","shell.execute_reply.started":"2022-03-12T22:25:50.245337Z","shell.execute_reply":"2022-03-12T22:25:50.251427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('../input/longformer-files')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:25:50.253365Z","iopub.execute_input":"2022-03-12T22:25:50.2538Z","iopub.status.idle":"2022-03-12T22:25:50.46687Z","shell.execute_reply.started":"2022-03-12T22:25:50.253766Z","shell.execute_reply":"2022-03-12T22:25:50.466132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:25:50.46895Z","iopub.execute_input":"2022-03-12T22:25:50.469448Z","iopub.status.idle":"2022-03-12T22:25:50.670411Z","shell.execute_reply.started":"2022-03-12T22:25:50.46941Z","shell.execute_reply":"2022-03-12T22:25:50.669291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:25:50.672013Z","iopub.execute_input":"2022-03-12T22:25:50.67245Z","iopub.status.idle":"2022-03-12T22:25:50.718354Z","shell.execute_reply.started":"2022-03-12T22:25:50.672412Z","shell.execute_reply":"2022-03-12T22:25:50.717482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for evaluating the model\ndef do_evaluate(model, val_dataloader, loss_criteria):\n    print(\"\\nEvaluating...\")\n    \n    # deactivate dropout layers\n    model.eval()\n\n    total_loss, total_accuracy = 0, 0\n\n    # empty list to save the model predictions\n    total_logits = []\n\n    # iterate over batches\n    for step, batch in enumerate(val_dataloader):\n\n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            #             elapsed = format_time(time.time() - t0)\n\n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n\n        # push the batch to gpu\n        batch = [t.to(device) for t in batch]\n\n        sent_id, mask, labels = batch\n\n        # deactivate autograd\n        with torch.no_grad():\n\n            # model predictions\n            logits = model(sent_id.to(device), mask.to(device))\n\n            # compute the validation loss between actual and predicted values\n            loss = loss_criteria(logits.logits.permute(0, 2, 1), labels)\n\n            total_loss = total_loss + loss.item()\n\n            logits = logits.logits.detach().cpu().numpy()\n\n            total_logits.append(logits)\n\n    # compute the validation loss of the epoch\n    avg_loss = total_loss / len(val_dataloader)\n\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_logits = np.concatenate(total_logits, axis=0)\n\n    return avg_loss, total_logits","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:25:50.719852Z","iopub.execute_input":"2022-03-12T22:25:50.720122Z","iopub.status.idle":"2022-03-12T22:25:50.729154Z","shell.execute_reply.started":"2022-03-12T22:25:50.720087Z","shell.execute_reply":"2022-03-12T22:25:50.728407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load fine-tuned model\nmodel = AutoModelForTokenClassification.from_pretrained('../input/longformer-1000-seq-len-ner-model')\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:25:50.730452Z","iopub.execute_input":"2022-03-12T22:25:50.731336Z","iopub.status.idle":"2022-03-12T22:26:03.055672Z","shell.execute_reply.started":"2022-03-12T22:25:50.7313Z","shell.execute_reply":"2022-03-12T22:26:03.054935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_prediction_from_logits(logits):\n    try:\n        tag_prob = nn.Softmax(dim=2)(logits)\n        tag_prediction = torch.argmax(tag_prob, dim=2).detach().cpu().numpy()\n        return tag_prediction\n    except Exception as e:\n        print(f\"Error in line: {sys.exc_info()[2].tb_lineno}\")\n        print(e)\n        \ndef classification_result(tag2idx, c_tag_id):\n    try:\n        prediction_result = []\n        for sent_ in c_tag_id:\n            prediction_result.append(\n                list(map(lambda x: list(tag2idx.keys())[list(tag2idx.values()).index(x)], sent_))\n            )\n            \n        tagged_entity = np.concatenate(prediction_result, axis=0)\n        return tagged_entity\n    except Exception as e:\n        print(f\"Error in line: {sys.exc_info()[2].tb_lineno}\")\n        print(e) ","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:26:03.057987Z","iopub.execute_input":"2022-03-12T22:26:03.058509Z","iopub.status.idle":"2022-03-12T22:26:03.066223Z","shell.execute_reply.started":"2022-03-12T22:26:03.058469Z","shell.execute_reply":"2022-03-12T22:26:03.065398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions on Test data","metadata":{}},{"cell_type":"code","source":"filepath = '../input/feedback-prize-2021/'\nsubmission_data = pd.read_csv('../input/feedback-prize-2021/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:26:03.067273Z","iopub.execute_input":"2022-03-12T22:26:03.067451Z","iopub.status.idle":"2022-03-12T22:26:03.08732Z","shell.execute_reply.started":"2022-03-12T22:26:03.067429Z","shell.execute_reply":"2022-03-12T22:26:03.086708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_for_modelling = submission_data[['id']].drop_duplicates().reset_index(drop = True)\ntest_data_for_modelling['Text'] = np.NaN\nfor i in tqdm(range(0, test_data_for_modelling.shape[0])):\n    record_id = test_data_for_modelling['id'].iloc[i]\n    with open(filepath + 'test/' + record_id+'.txt','r') as f:\n        data = f.read()\n    test_data_for_modelling['Text'].iloc[i] = data\n    \ntest_data_for_modelling['cleaned_text'] = test_data_for_modelling['Text'].apply(lambda x: re.sub('\\n+',' ',x).strip().replace(\"\\xa0\",\" \"))","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:26:03.089785Z","iopub.execute_input":"2022-03-12T22:26:03.090302Z","iopub.status.idle":"2022-03-12T22:26:03.149339Z","shell.execute_reply.started":"2022-03-12T22:26:03.090274Z","shell.execute_reply":"2022-03-12T22:26:03.148605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_data_for_modelling.shape)\ntest_data_for_modelling.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:26:03.15034Z","iopub.execute_input":"2022-03-12T22:26:03.150613Z","iopub.status.idle":"2022-03-12T22:26:03.16645Z","shell.execute_reply.started":"2022-03-12T22:26:03.15055Z","shell.execute_reply":"2022-03-12T22:26:03.165767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_data_for_modelling = pd.concat([test_data_for_modelling]*2000, axis = 0).reset_index(drop = True)\n# print(test_data_for_modelling.shape)\n# test_data_for_modelling.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:57:33.858268Z","iopub.execute_input":"2022-03-12T22:57:33.858522Z","iopub.status.idle":"2022-03-12T22:57:33.86292Z","shell.execute_reply.started":"2022-03-12T22:57:33.858493Z","shell.execute_reply":"2022-03-12T22:57:33.862163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_tagged_data = sent_tag_tokenization(data=test_data_for_modelling)\n\ninput_ids = processed_tagged_data['sent_input_ids'].tolist()\ntoken_ids = processed_tagged_data['token_tag_ids'].tolist()\n\npadded_input_ids, padded_attention_mask, padded_tags = pad_data(\n    input_ids=input_ids, \n    token_ids=token_ids,\n    max_token_length = 1000\n)\n\nbatch_size = 50\n\ntest_dataloader = create_dataloader(\n    token_ids=padded_input_ids, \n    masks=padded_attention_mask, \n    tags=padded_tags, \n    batch_size=batch_size, \n    val=True\n)\n\npred_tags = []\nfor batch in tqdm(test_dataloader):\n    # deactivate dropout layers\n    model.eval()\n\n    sent_id, mask, labels = batch\n\n    # deactivate autograd\n    with torch.no_grad():\n        # model predictions\n        logits = model(sent_id.to(device), mask.to(device))\n        logits = get_prediction_from_logits(logits['logits'])\n    pred_tags.append(logits)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:26:03.464549Z","iopub.execute_input":"2022-03-12T22:26:03.465023Z","iopub.status.idle":"2022-03-12T22:42:08.07432Z","shell.execute_reply.started":"2022-03-12T22:26:03.464983Z","shell.execute_reply":"2022-03-12T22:42:08.073618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_for_modelling['Predicted tags'] = 'to_be_filled_with_predicted_tags'\npred_tags = np.concatenate(pred_tags)\nfor i in range(0, len(test_data_for_modelling)):\n    test_data_for_modelling.at[i, 'Predicted tags'] = np.array(pred_tags[i], dtype=object)\ntest_data_for_modelling['Predicted tags'] = test_data_for_modelling['Predicted tags'].apply(lambda x: [idx2tag[t] for t in x])","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:42:08.075477Z","iopub.execute_input":"2022-03-12T22:42:08.07618Z","iopub.status.idle":"2022-03-12T22:42:09.240769Z","shell.execute_reply.started":"2022-03-12T22:42:08.076139Z","shell.execute_reply":"2022-03-12T22:42:09.240049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_data_for_modelling.shape)\ntest_data_for_modelling.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:55:16.976753Z","iopub.execute_input":"2022-03-12T22:55:16.977326Z","iopub.status.idle":"2022-03-12T22:55:16.994413Z","shell.execute_reply.started":"2022-03-12T22:55:16.977287Z","shell.execute_reply":"2022-03-12T22:55:16.993427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get prediction string as per submission format\ntest_data_for_submission_final = pd.DataFrame(data = None, columns = ['id','class','predictionstring','token','discourse type count']) \nfor i in tqdm(range(0, len(test_data_for_modelling))):\n    test_data_for_submission = pd.DataFrame(data = None, columns = ['id','class','predictionstring','token']) \n    record_id = test_data_for_modelling['id'].iloc[i]\n    pred_tags = test_data_for_modelling['Predicted tags'].iloc[i]\n    record_text = test_data_for_modelling['cleaned_text'].iloc[i].split(' ')\n    count = 0\n    for j in range(0, len(pred_tags)):\n        token = pred_tags[j]\n        if j >= len(record_text):\n            break\n        if (token != 'O'):\n            discourse_type = token.split('-')[1]\n            test_data_for_submission.loc[count] = [record_id, discourse_type, count, token]\n            count += 1\n        else:\n            discourse_type = np.NaN\n\n    # Replace 2 adjacent B tags of same type together\n    test_data_for_submission['actual token'] = test_data_for_submission['token']\n    for j in range(1, len(test_data_for_submission)):\n        if (test_data_for_submission['actual token'].iloc[j].split('-')[0] == 'B') & (test_data_for_submission['actual token'].iloc[j-1] == test_data_for_submission['actual token'].iloc[j]):\n            test_data_for_submission['token'].iloc[j] = 'I-' + test_data_for_submission['token'].iloc[j].split('-')[1]\n    \n#     count = 0\n#     test_data_for_submission['discourse type count'] = np.NaN\n#     for k in range(0, len(test_data_for_submission)):\n#         if test_data_for_submission['token'].iloc[k].split('-')[0] == 'B':\n#             discourse_type = test_data_for_submission['token'].iloc[k].split('-')[1]\n#             test_data_for_submission['discourse type count'].iloc[k] = count\n#             for j in range(k+1, len(test_data_for_submission)):\n#                 if (test_data_for_submission['token'].iloc[j].split('-')[0] == 'I') & (test_data_for_submission['token'].iloc[j].split('-')[1] == discourse_type):\n#                     test_data_for_submission['discourse type count'].iloc[j] = count\n#                 else:\n#                     break\n#             count = count + 1\n\n    count = 0\n    k = 0\n    end_flag = False\n    test_data_for_submission['discourse type count'] = np.NaN\n    while (not (end_flag)):\n        if test_data_for_submission['token'].iloc[k].split('-')[0] == 'B':\n            discourse_type = test_data_for_submission['token'].iloc[k].split('-')[1]\n            test_data_for_submission['discourse type count'].iloc[k] = count\n            for j in range(k+1, len(test_data_for_submission)):\n                if (test_data_for_submission['token'].iloc[j].split('-')[0] == 'I') & (test_data_for_submission['token'].iloc[j].split('-')[1] == discourse_type):\n                    test_data_for_submission['discourse type count'].iloc[j] = count\n                else:\n                    k = j\n                    break\n            count = count + 1\n        else:\n            k = k + 1\n        if (j== len(test_data_for_submission)-1) | (k== len(test_data_for_submission)-1):\n            end_flag = True\n\n    test_data_for_submission_final = pd.concat([test_data_for_submission_final, test_data_for_submission], axis = 0).reset_index(drop = True)\n    \nkeys = test_data_for_submission_final['id'].unique().tolist()\nvalues = [x for x in range(0, len(test_data_for_submission_final['id'].unique().tolist()))]\ntemp = dict(zip(keys, values))\ntest_data_for_submission_final['rec_num'] = test_data_for_submission_final['id'].map(temp) \n\ntest_data_for_submission_final['predictionstring'] = test_data_for_submission_final['predictionstring'].astype(str)\ntest_data_for_submission_final = test_data_for_submission_final.groupby(['id','class','discourse type count','rec_num']).agg({'predictionstring': ' '.join}).reset_index().sort_values(by = ['rec_num','discourse type count'])[['id','class','predictionstring']]","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:55:22.696919Z","iopub.execute_input":"2022-03-12T22:55:22.697437Z","iopub.status.idle":"2022-03-12T22:57:01.575335Z","shell.execute_reply.started":"2022-03-12T22:55:22.697399Z","shell.execute_reply":"2022-03-12T22:57:01.574147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_for_submission_final.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:57:08.720515Z","iopub.execute_input":"2022-03-12T22:57:08.720829Z","iopub.status.idle":"2022-03-12T22:57:08.744077Z","shell.execute_reply.started":"2022-03-12T22:57:08.720785Z","shell.execute_reply":"2022-03-12T22:57:08.743481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_for_submission_final.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T22:42:09.637809Z","iopub.status.idle":"2022-03-12T22:42:09.638574Z","shell.execute_reply.started":"2022-03-12T22:42:09.638328Z","shell.execute_reply":"2022-03-12T22:42:09.638355Z"},"trusted":true},"execution_count":null,"outputs":[]}]}