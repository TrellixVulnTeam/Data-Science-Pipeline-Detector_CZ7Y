{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Feedback Prize - Evaluating Student Writing\n#### Analyze argumentative writing elements from students grade 6-12\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/31779/logos/header.png?t=2021-11-12-22-52-17&quot)","metadata":{}},{"cell_type":"markdown","source":"https://drive.google.com/file/d/1r9vxKn5Az3ZBoZ7PgkmIh2ov87axyN8B/view","metadata":{}},{"cell_type":"markdown","source":"# Import packages","metadata":{}},{"cell_type":"code","source":"# Install packages","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:40:31.655058Z","iopub.execute_input":"2022-02-27T10:40:31.655458Z","iopub.status.idle":"2022-02-27T10:40:31.688903Z","shell.execute_reply.started":"2022-02-27T10:40:31.655342Z","shell.execute_reply":"2022-02-27T10:40:31.687777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport re\nfrom tqdm import tqdm\ntqdm.pandas()\nimport plotly.express as px\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\nimport torch\nimport torch.nn as nn\nfrom joblib import Parallel, delayed\nfrom transformers import AutoModelForTokenClassification\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, TrainingArguments, Trainer, EvalPrediction\nimport sys\nimport time\nimport gc\nimport pickle\nimport re\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import AdamW\nimport sklearn\nSEED = 42","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:40:31.690891Z","iopub.execute_input":"2022-02-27T10:40:31.695238Z","iopub.status.idle":"2022-02-27T10:40:43.469707Z","shell.execute_reply.started":"2022-02-27T10:40:31.695196Z","shell.execute_reply":"2022-02-27T10:40:43.468834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('../input/feedback-prize-2021/train.csv')\nsubmission_data = pd.read_csv('../input/feedback-prize-2021/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:40:43.471513Z","iopub.execute_input":"2022-02-27T10:40:43.471825Z","iopub.status.idle":"2022-02-27T10:40:45.068555Z","shell.execute_reply.started":"2022-02-27T10:40:43.471782Z","shell.execute_reply":"2022-02-27T10:40:45.067602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"print(train_data.shape)\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:40:45.071502Z","iopub.execute_input":"2022-02-27T10:40:45.072606Z","iopub.status.idle":"2022-02-27T10:40:45.104418Z","shell.execute_reply.started":"2022-02-27T10:40:45.072503Z","shell.execute_reply":"2022-02-27T10:40:45.103483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Discourse types in train:\\n', len(train_data['discourse_type'].unique().tolist()), 'types -', train_data['discourse_type'].unique().tolist())","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:40:45.106412Z","iopub.execute_input":"2022-02-27T10:40:45.107616Z","iopub.status.idle":"2022-02-27T10:40:45.142705Z","shell.execute_reply.started":"2022-02-27T10:40:45.107567Z","shell.execute_reply":"2022-02-27T10:40:45.141086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('# Splits for first passage:', train_data[train_data['id']==train_data['id'].iloc[0]]['discourse_id'].nunique())\ntrain_data[train_data['id']==train_data['id'].iloc[0]]","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:40:45.144031Z","iopub.execute_input":"2022-02-27T10:40:45.144334Z","iopub.status.idle":"2022-02-27T10:40:45.210745Z","shell.execute_reply.started":"2022-02-27T10:40:45.144291Z","shell.execute_reply":"2022-02-27T10:40:45.209767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(submission_data.shape)\nsubmission_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:40:45.212232Z","iopub.execute_input":"2022-02-27T10:40:45.21319Z","iopub.status.idle":"2022-02-27T10:40:45.22805Z","shell.execute_reply.started":"2022-02-27T10:40:45.213124Z","shell.execute_reply":"2022-02-27T10:40:45.226944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check prediction string","metadata":{}},{"cell_type":"code","source":"''.join(train_data[train_data['id']==train_data['id'].unique()[2]]['predictionstring'].tolist())","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:40:45.229828Z","iopub.execute_input":"2022-02-27T10:40:45.230382Z","iopub.status.idle":"2022-02-27T10:40:45.271604Z","shell.execute_reply.started":"2022-02-27T10:40:45.230336Z","shell.execute_reply":"2022-02-27T10:40:45.270513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(' '.join(train_data[train_data['id']==train_data['id'].unique()[2]]['discourse_text'].apply(lambda x: re.sub('\\n+',' ',x)).str.strip().tolist()).split(' ')), len(' '.join(train_data[train_data['id']==train_data['id'].unique()[2]]['predictionstring'].tolist()).split(' '))","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:40:45.272908Z","iopub.execute_input":"2022-02-27T10:40:45.273509Z","iopub.status.idle":"2022-02-27T10:40:45.34962Z","shell.execute_reply.started":"2022-02-27T10:40:45.273463Z","shell.execute_reply":"2022-02-27T10:40:45.348566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filepath = '../input/feedback-prize-2021/'\nrecord_id = 'A97DE0D49AEA'\n# record_id = '1CEDD5563788'\nwith open(filepath + 'train/' + record_id+'.txt','r') as f:\n    data = f.read()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:40:45.35399Z","iopub.execute_input":"2022-02-27T10:40:45.35426Z","iopub.status.idle":"2022-02-27T10:40:45.363719Z","shell.execute_reply.started":"2022-02-27T10:40:45.35423Z","shell.execute_reply":"2022-02-27T10:40:45.36275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[train_data['id']==record_id]","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:40:45.365457Z","iopub.execute_input":"2022-02-27T10:40:45.365963Z","iopub.status.idle":"2022-02-27T10:40:45.40819Z","shell.execute_reply.started":"2022-02-27T10:40:45.365912Z","shell.execute_reply":"2022-02-27T10:40:45.407115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[train_data['id']==record_id]['discourse_text'].iloc[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:40:45.410023Z","iopub.execute_input":"2022-02-27T10:40:45.410412Z","iopub.status.idle":"2022-02-27T10:40:45.440882Z","shell.execute_reply.started":"2022-02-27T10:40:45.410354Z","shell.execute_reply":"2022-02-27T10:40:45.43957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"re.sub('\\n+',' ', data).strip().replace(\"\\xa0\",\" \").replace('\"','')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:40:45.44274Z","iopub.execute_input":"2022-02-27T10:40:45.443082Z","iopub.status.idle":"2022-02-27T10:40:45.450973Z","shell.execute_reply.started":"2022-02-27T10:40:45.443038Z","shell.execute_reply":"2022-02-27T10:40:45.449553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Combined words create an issue e.g: \"DrivingHundreds\" in the above passage\n\nAlso references are not to be tagged under the 7 classes","metadata":{}},{"cell_type":"code","source":"re.sub('\\n+',' ',train_data[train_data['id']==record_id]['discourse_text'].iloc[0].strip().replace(\"\\xa0\",\" \").replace('\"',''))","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:40:45.453211Z","iopub.execute_input":"2022-02-27T10:40:45.453562Z","iopub.status.idle":"2022-02-27T10:40:45.485573Z","shell.execute_reply.started":"2022-02-27T10:40:45.453497Z","shell.execute_reply":"2022-02-27T10:40:45.484538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Prediction string in train data should start at:', len(re.sub('\\n+',' ', data).strip().replace(\"\\xa0\",\" \").replace('\"','')[0:re.sub('\\n+',' ', data).strip().replace(\"\\xa0\",\" \").replace('\"','').find(' '.join(re.sub('\\n+',' ',train_data[train_data['id']==record_id]['discourse_text'].iloc[0].strip().replace(\"\\xa0\",\" \").replace('\"','')).split(' ')[0:4]))].strip().split(' ')))","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:40:45.487175Z","iopub.execute_input":"2022-02-27T10:40:45.487676Z","iopub.status.idle":"2022-02-27T10:40:45.51884Z","shell.execute_reply.started":"2022-02-27T10:40:45.487629Z","shell.execute_reply":"2022-02-27T10:40:45.517628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(re.sub('\\n+',' ',train_data[train_data['id']==record_id]['discourse_text'].iloc[0]).strip().replace(\"\\xa0\",\" \").replace('\"','').split(' ')), re.sub('\\n+',' ',train_data[train_data['id']==record_id]['discourse_text'].iloc[0]).strip().replace(\"\\xa0\",\" \").replace('\"','').split(' ')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:40:45.52081Z","iopub.execute_input":"2022-02-27T10:40:45.521438Z","iopub.status.idle":"2022-02-27T10:40:45.573201Z","shell.execute_reply.started":"2022-02-27T10:40:45.521391Z","shell.execute_reply":"2022-02-27T10:40:45.572004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data[train_data['id']==record_id]['predictionstring'].iloc[0].split(' ')), train_data[train_data['id']==record_id]['predictionstring'].iloc[0].split(' ')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:40:45.574634Z","iopub.execute_input":"2022-02-27T10:40:45.575226Z","iopub.status.idle":"2022-02-27T10:40:45.624759Z","shell.execute_reply.started":"2022-02-27T10:40:45.575181Z","shell.execute_reply":"2022-02-27T10:40:45.623587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['cleaned_discourse_text'] = train_data['discourse_text'].apply(lambda x: re.sub('\\n+',' ',x).strip().replace(\"\\xa0\",\" \"))\ntrain_data['Flag - pred len and text len'] = np.where(len(train_data['cleaned_discourse_text'].str.split(' '))==len(train_data['predictionstring'].str.split(' ')), 1, 0)\n\nmismatches_pred_string = []\nrecords_with_name_mask = []\nrecord_id_list = []\n\nfor i in tqdm(range(0, train_data.shape[0])):\n    record_id = train_data['id'].iloc[i]\n    if record_id not in record_id_list:\n        with open(filepath + 'train/' + record_id+'.txt','r') as f:\n            data = f.read()\n\n        data = re.sub('\\n+',' ', data).strip().replace(\"\\xa0\",\" \")\n        str_index = data.find(train_data[train_data['id']==record_id]['cleaned_discourse_text'].iloc[0])\n        words_before_pred_string_start = data[:str_index].strip().split(' ')\n\n        if str_index != -1:\n            if (len(words_before_pred_string_start) != int(train_data['predictionstring'].iloc[i].strip().split(' ')[0])) and (words_before_pred_string_start[0] != ''):\n                mismatches_pred_string.append(record_id)\n            if ('PROPER_NAME' in data) or ('GENERIC_NAME' in data) or ('SCHOOL_NAME' in data):\n                records_with_name_mask.append(record_id)\n        else:   \n            # At least first 4 words are correct\n            if (len(data[:data.find(' '.join(train_data[train_data['id']==record_id]['cleaned_discourse_text'].iloc[0].split(' ')[0:4]))].strip().split(' ')) != int(train_data['predictionstring'].iloc[i].strip().split(' ')[0])) and (data[:data.find(' '.join(train_data[train_data['id']==record_id]['cleaned_discourse_text'].iloc[0].split(' ')[0:4]))].strip().split(' ')[0] != ''):\n                mismatches_pred_string.append(record_id)\n                if ('PROPER_NAME' in data) or ('GENERIC_NAME' in data) or ('SCHOOL_NAME' in data):\n                    records_with_name_mask.append(record_id)\n        record_id_list.append(record_id)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:40:45.626714Z","iopub.execute_input":"2022-02-27T10:40:45.627051Z","iopub.status.idle":"2022-02-27T10:47:36.535204Z","shell.execute_reply.started":"2022-02-27T10:40:45.627007Z","shell.execute_reply":"2022-02-27T10:47:36.534198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('# Records with number of words in discourse text not equal to number of indices in prediction string:', train_data[train_data['Flag - pred len and text len'] == 0].shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:47:36.536898Z","iopub.execute_input":"2022-02-27T10:47:36.537175Z","iopub.status.idle":"2022-02-27T10:47:36.543857Z","shell.execute_reply.started":"2022-02-27T10:47:36.537143Z","shell.execute_reply":"2022-02-27T10:47:36.542604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('# Misatches:', len(mismatches_pred_string), 'out of', train_data['id'].nunique(), 'ids in train data', '(' + str(len(mismatches_pred_string)/train_data['id'].nunique() * 100) + '%)')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:47:36.545838Z","iopub.execute_input":"2022-02-27T10:47:36.546389Z","iopub.status.idle":"2022-02-27T10:47:36.591142Z","shell.execute_reply.started":"2022-02-27T10:47:36.54633Z","shell.execute_reply":"2022-02-27T10:47:36.590197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mismatches_pred_string","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:47:36.592563Z","iopub.execute_input":"2022-02-27T10:47:36.592901Z","iopub.status.idle":"2022-02-27T10:47:36.600435Z","shell.execute_reply.started":"2022-02-27T10:47:36.592848Z","shell.execute_reply":"2022-02-27T10:47:36.599187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len([x for x in mismatches_pred_string if x not in records_with_name_mask]), [x for x in mismatches_pred_string if x not in records_with_name_mask]","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:47:36.602392Z","iopub.execute_input":"2022-02-27T10:47:36.603017Z","iopub.status.idle":"2022-02-27T10:47:36.613184Z","shell.execute_reply.started":"2022-02-27T10:47:36.602972Z","shell.execute_reply":"2022-02-27T10:47:36.611974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"record_id = '4E1C636ABEAD'\n\nwith open(filepath + 'train/' + record_id+'.txt','r') as f:\n    data = f.read()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:47:36.615023Z","iopub.execute_input":"2022-02-27T10:47:36.615365Z","iopub.status.idle":"2022-02-27T10:47:36.622575Z","shell.execute_reply.started":"2022-02-27T10:47:36.615308Z","shell.execute_reply":"2022-02-27T10:47:36.621467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[train_data['id']==record_id]","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:47:36.624068Z","iopub.execute_input":"2022-02-27T10:47:36.626122Z","iopub.status.idle":"2022-02-27T10:47:36.669601Z","shell.execute_reply.started":"2022-02-27T10:47:36.626083Z","shell.execute_reply":"2022-02-27T10:47:36.668568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"re.sub('\\n+',' ', data).strip().replace(\"\\xa0\",\" \").replace('\"','')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:47:36.671021Z","iopub.execute_input":"2022-02-27T10:47:36.67143Z","iopub.status.idle":"2022-02-27T10:47:36.681879Z","shell.execute_reply.started":"2022-02-27T10:47:36.671382Z","shell.execute_reply":"2022-02-27T10:47:36.680581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"re.sub('\\n+',' ',train_data[train_data['id']==record_id]['discourse_text'].iloc[4].strip().replace(\"\\xa0\",\" \").replace('\"',''))","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:47:36.684276Z","iopub.execute_input":"2022-02-27T10:47:36.684657Z","iopub.status.idle":"2022-02-27T10:47:36.71554Z","shell.execute_reply.started":"2022-02-27T10:47:36.684609Z","shell.execute_reply":"2022-02-27T10:47:36.714152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"re.sub('\\n+',' ',train_data[train_data['id']==record_id]['discourse_text'].iloc[5].strip().replace(\"\\xa0\",\" \").replace('\"',''))","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:47:36.717083Z","iopub.execute_input":"2022-02-27T10:47:36.718142Z","iopub.status.idle":"2022-02-27T10:47:36.747196Z","shell.execute_reply.started":"2022-02-27T10:47:36.718071Z","shell.execute_reply":"2022-02-27T10:47:36.746115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The sentence \"Therefore, driving can cause many accidents that can be fatal to the driver and passengers if there is any and cell phones should only be used when not operating a vehicle.\" does not have any tagged class","metadata":{}},{"cell_type":"markdown","source":"## Text length distribution","metadata":{}},{"cell_type":"code","source":"len_text_df = pd.DataFrame(data = None, columns = ['record_id', '# Words in text'])\nfor i in tqdm(range(0, len(train_data['id'].unique().tolist()))):\n    record_id = train_data['id'].unique().tolist()[i]\n    with open(filepath + 'train/' + record_id+'.txt','r') as f:\n        data = f.read()\n    len_text_df.loc[i] = record_id, len(data.strip().replace(\"\\xa0\",\" \").split(' '))","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:47:36.754637Z","iopub.execute_input":"2022-02-27T10:47:36.755132Z","iopub.status.idle":"2022-02-27T10:51:46.400991Z","shell.execute_reply.started":"2022-02-27T10:47:36.755086Z","shell.execute_reply":"2022-02-27T10:51:46.399927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(len_text_df, x='# Words in text', title = 'Distribution of discourse text length')\npy.offline.iplot(fig)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:51:46.402844Z","iopub.execute_input":"2022-02-27T10:51:46.403316Z","iopub.status.idle":"2022-02-27T10:51:47.520624Z","shell.execute_reply.started":"2022-02-27T10:51:46.403274Z","shell.execute_reply":"2022-02-27T10:51:47.51961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling - NER","metadata":{}},{"cell_type":"code","source":"tag2idx = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 2,\n    \"I-Position\": 3,\n    \"B-Evidence\": 4,\n    \"I-Evidence\": 5,\n    \"B-Claim\": 6,\n    \"I-Claim\": 7,\n    \"B-Concluding Statement\": 8,\n    \"I-Concluding Statement\": 9,\n    \"B-Counterclaim\": 10,\n    \"I-Counterclaim\": 11,\n    \"B-Rebuttal\": 12,\n    \"I-Rebuttal\": 13,\n    \"O\": 14\n}\n\n\nidx2tag = {v: k for k, v in tag2idx.items()}","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:51:47.521791Z","iopub.execute_input":"2022-02-27T10:51:47.522417Z","iopub.status.idle":"2022-02-27T10:51:47.531024Z","shell.execute_reply.started":"2022-02-27T10:51:47.522364Z","shell.execute_reply":"2022-02-27T10:51:47.530188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_for_modelling = train_data[['id']].drop_duplicates().reset_index(drop = True)\ntrain_data_for_modelling['Text'] = np.NaN\nfor i in tqdm(range(0, train_data_for_modelling.shape[0])):\n    record_id = train_data_for_modelling['id'].iloc[i]\n    with open(filepath + 'train/' + record_id+'.txt','r') as f:\n        data = f.read()\n    train_data_for_modelling['Text'].iloc[i] = data\n    \ntrain_data_for_modelling['cleaned_text'] = train_data_for_modelling['Text'].apply(lambda x: re.sub('\\n+',' ',x).strip().replace(\"\\xa0\",\" \"))","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:51:47.532635Z","iopub.execute_input":"2022-02-27T10:51:47.533214Z","iopub.status.idle":"2022-02-27T10:52:02.55208Z","shell.execute_reply.started":"2022-02-27T10:51:47.53317Z","shell.execute_reply":"2022-02-27T10:52:02.551125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_for_modelling.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:02.553422Z","iopub.execute_input":"2022-02-27T10:52:02.554035Z","iopub.status.idle":"2022-02-27T10:52:02.569313Z","shell.execute_reply.started":"2022-02-27T10:52:02.553991Z","shell.execute_reply":"2022-02-27T10:52:02.568196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_sentence(x):\n    \"takes in a string and returns tokenized list\"\n\n    return [x for x in x.strip().lower().split(\" \") if len(x) > 0]","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:02.570827Z","iopub.execute_input":"2022-02-27T10:52:02.571321Z","iopub.status.idle":"2022-02-27T10:52:02.580579Z","shell.execute_reply.started":"2022-02-27T10:52:02.571278Z","shell.execute_reply":"2022-02-27T10:52:02.579576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_tags(sent, rec_id):\n    '''\n    Input: sent as a sentence tokenized as list of tokens\n    output: tags \n    '''\n    sent = [x for x in sent]\n    data = ' '.join(sent)\n    \n    temp = train_data[train_data['id']==rec_id].reset_index(drop = True)\n    all_tags = []\n    correct_tags = []\n    \n    if temp.shape[0] > 0:\n        for j in range(0, len(temp)):     \n            pred_string = temp['discourse_text'].iloc[j]\n            pred_string = re.sub('\\n+', ' ', pred_string).strip().replace(\"\\xa0\",\" \").lower()\n            discourse_type = temp['discourse_type'].iloc[j]\n            ep = tokenize_sentence(pred_string)\n\n            start_index = data.find(pred_string)\n            stop_index = data.find(pred_string) + len(pred_string)\n            tokens_for_tagging = data[start_index:stop_index].split(' ')\n\n            i = 0\n            tag = []\n            tag_flag = False\n            while i < len(sent):\n                if (start_index == 0) & (i == 0) :\n                    tag.append('B-' + discourse_type)\n                    i+=1\n\n                    tag = tag + ['I-' + discourse_type] * (len(ep)-1)\n                    i+=(len(ep)-1)\n                    tag_flag = True\n                elif (data.split(' ')[i] == tokens_for_tagging[0]) & (len(' '.join(data.split(' ')[0:i]))+1 == start_index) & (not tag_flag):\n                    tag.append('B-' + discourse_type)\n                    i+=1\n\n                    tag = tag + ['I-' + discourse_type] * (len(ep)-1)\n                    i+=(len(ep)-1)\n                else:\n                    tag.append('O')                \n                    i+=1\n            all_tags.append(tag)\n            # Update text to mask already tagged tokens\n            data = ' '.join(['masked_token' if (('B' in tag[x]) or ('I' in tag[x])) else data.split(' ')[x] for x in range(0, len(data.split(' ')))])\n\n        for i in range(0, len(all_tags[0])):\n            sum_O_tags = 0\n            tags = []\n            for j in range(0, len(all_tags)):\n                if all_tags[j][i] == 'O':\n                    sum_O_tags += 1\n                else:\n                    tags.append(all_tags[j][i])\n            if sum_O_tags == len(all_tags):\n                correct_tags.append('O')\n            else:\n                if len(tags)>1:\n                    print('More than 1 tag for a token in record_id', rec_id)\n                    print(tags)\n                correct_tags.append(tags[0])\n\n    else:\n        correct_tags = ['O'] * len(sent)\n    return correct_tags","metadata":{"execution":{"iopub.status.busy":"2022-02-27T18:29:04.937722Z","iopub.execute_input":"2022-02-27T18:29:04.938101Z","iopub.status.idle":"2022-02-27T18:29:04.960737Z","shell.execute_reply.started":"2022-02-27T18:29:04.938054Z","shell.execute_reply":"2022-02-27T18:29:04.959491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Check tagging cdde\n# rec_id = '6C8D8C5145DB'\n# temp = train_data[train_data['id']==rec_id].reset_index(drop = True)\n# sent = train_data_for_modelling[train_data_for_modelling['id']==rec_id]['cleaned_text'].iloc[0].lower()\n# data = ' '.join(sent.split(' '))\n# sent = sent.split(' ')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:02.605458Z","iopub.execute_input":"2022-02-27T10:52:02.606433Z","iopub.status.idle":"2022-02-27T10:52:02.618756Z","shell.execute_reply.started":"2022-02-27T10:52:02.606368Z","shell.execute_reply":"2022-02-27T10:52:02.617893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vocab_sent_tokenize_label(sent_tokenized, token_tag):\n    try:\n        vocab_sent_token = []\n        sent_input_ids = []\n        vocab_token_tag = []\n        token_tag_ids = []\n        for sent_token_, tag_ in zip(sent_tokenized, token_tag):\n            _vocab_sent_token = tokenizer.tokenize(sent_token_)\n            _sent_input_ids = [\n                tokenizer.convert_tokens_to_ids(x) for x in _vocab_sent_token\n            ]\n            _vocab_token_tag = [tag_] * len(_vocab_sent_token)\n            _token_tag_ids = [tag2idx[x] for x in _vocab_token_tag]\n\n            vocab_sent_token.extend(_vocab_sent_token)\n            sent_input_ids.extend(_sent_input_ids)\n            vocab_token_tag.extend(_vocab_token_tag)\n            token_tag_ids.extend(_token_tag_ids)\n        return vocab_sent_token, sent_input_ids, vocab_token_tag, token_tag_ids\n    except Exception as e:\n        print(f\"Error in line no: {sys.exc_info()[2].tb_lineno}\")\n        print(e)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:02.620153Z","iopub.execute_input":"2022-02-27T10:52:02.621235Z","iopub.status.idle":"2022-02-27T10:52:02.632304Z","shell.execute_reply.started":"2022-02-27T10:52:02.621191Z","shell.execute_reply":"2022-02-27T10:52:02.631245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sent_tag_tokenization(data):\n    try:\n        model_data_preprocessing = data.copy(deep=True)\n        model_data_preprocessing['sent_tokenized'] = model_data_preprocessing[\n            'cleaned_text'].progress_apply(tokenize_sentence)\n\n        model_data_preprocessing[\n            'token_tag'] = model_data_preprocessing.progress_apply(\n                lambda x: get_tags(sent=x['sent_tokenized'], rec_id=x['id'])\n                if (isinstance(x['sent_tokenized'], list)) else np.nan, axis=1)\n\n        model_data_preprocessing['vocab_sent_tokenized'], model_data_preprocessing[\n            'sent_input_ids'], model_data_preprocessing[\n                'vocab_token_tag'], model_data_preprocessing['token_tag_ids'] = zip(\n                    *model_data_preprocessing.\n                    progress_apply(lambda x: vocab_sent_tokenize_label(\n                        sent_tokenized=x['sent_tokenized'], token_tag=x['token_tag'])\n                                   if isinstance(x['token_tag'], list) else np.nan,\n                                   axis=1))\n        return model_data_preprocessing\n    except Exception as e:\n        print(f\"Error in line no: {sys.exc_info()[2].tb_lineno}\")\n        print(e)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:02.633895Z","iopub.execute_input":"2022-02-27T10:52:02.635156Z","iopub.status.idle":"2022-02-27T10:52:02.647865Z","shell.execute_reply.started":"2022-02-27T10:52:02.635053Z","shell.execute_reply":"2022-02-27T10:52:02.646587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad_data(input_ids, token_ids, max_token_length):\n    try:\n        attention_mask = []\n        for input_ in tqdm(input_ids):\n            attention_mask.append(torch.ones(len(input_[:max_token_length])))\n\n        padded_attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask,\n                                                                batch_first=True,\n                                                                padding_value=0.0)\n\n        padded_input_ids = torch.nn.utils.rnn.pad_sequence(\n            [torch.tensor(input_[:max_token_length]) for input_ in input_ids],\n            batch_first=True,\n            padding_value=0.0)\n\n        padded_tags = torch.nn.utils.rnn.pad_sequence(\n            [torch.tensor(tag_[:max_token_length]) for tag_ in token_ids],\n            batch_first=True,\n            padding_value=0.0)\n        return padded_input_ids, padded_attention_mask, padded_tags\n    except Exception as e:\n        print(f\"Error in line no: {sys.exc_info()[2].tb_lineno}\")\n        print(e)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:02.651053Z","iopub.execute_input":"2022-02-27T10:52:02.651864Z","iopub.status.idle":"2022-02-27T10:52:02.663564Z","shell.execute_reply.started":"2022-02-27T10:52:02.651803Z","shell.execute_reply":"2022-02-27T10:52:02.662477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dataloader(token_ids, masks, tags, batch_size=16, val=False):\n    try:\n        # wrap tensors\n        data = TensorDataset(token_ids, masks, tags)\n\n        if val:\n            # sampler for sampling the data during training\n            sampler = SequentialSampler(data)\n            \n            # dataLoader for validation set\n            dataloader = DataLoader(data,\n                                    sampler=sampler,\n                                    batch_size=batch_size)\n        else:    \n            # sampler for sampling the data during training\n            sampler = RandomSampler(data)\n            # dataLoader for train set\n            dataloader = DataLoader(data,\n                                    sampler=sampler,\n                                    batch_size=batch_size)\n        return dataloader\n    except Exception as e:\n        print(f\"Error in line no: {sys.exc_info()[2].tb_lineno}\")\n        print(e)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:02.665952Z","iopub.execute_input":"2022-02-27T10:52:02.666565Z","iopub.status.idle":"2022-02-27T10:52:02.678443Z","shell.execute_reply.started":"2022-02-27T10:52:02.6665Z","shell.execute_reply":"2022-02-27T10:52:02.677402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:02.680183Z","iopub.execute_input":"2022-02-27T10:52:02.680828Z","iopub.status.idle":"2022-02-27T10:52:10.445154Z","shell.execute_reply.started":"2022-02-27T10:52:02.68078Z","shell.execute_reply":"2022-02-27T10:52:10.444266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# processed_tagged_full_data = sent_tag_tokenization(data=train_data_for_modelling)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:10.446939Z","iopub.execute_input":"2022-02-27T10:52:10.447264Z","iopub.status.idle":"2022-02-27T10:52:10.451803Z","shell.execute_reply.started":"2022-02-27T10:52:10.447205Z","shell.execute_reply":"2022-02-27T10:52:10.450522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('../input/student-written-text-bio-tagged-data/processed_tagged_full_data.pkl', 'rb') as file:\n    processed_tagged_full_data = pickle.load(file)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:10.454043Z","iopub.execute_input":"2022-02-27T10:52:10.454879Z","iopub.status.idle":"2022-02-27T10:52:17.863422Z","shell.execute_reply.started":"2022-02-27T10:52:10.454836Z","shell.execute_reply":"2022-02-27T10:52:17.862458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train-validation split\ntrain_ids, valid_ids = sklearn.model_selection.train_test_split(processed_tagged_full_data['id'].unique().tolist(), test_size = 0.2, random_state = SEED)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:17.864854Z","iopub.execute_input":"2022-02-27T10:52:17.865959Z","iopub.status.idle":"2022-02-27T10:52:17.884398Z","shell.execute_reply.started":"2022-02-27T10:52:17.865911Z","shell.execute_reply":"2022-02-27T10:52:17.883519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_tagged_full_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:17.885975Z","iopub.execute_input":"2022-02-27T10:52:17.886307Z","iopub.status.idle":"2022-02-27T10:52:17.942384Z","shell.execute_reply.started":"2022-02-27T10:52:17.886264Z","shell.execute_reply":"2022-02-27T10:52:17.941312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with open('processed_tagged_full_data.pkl', 'wb') as file:\n#     pickle.dump(processed_tagged_full_data, file)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:17.943886Z","iopub.execute_input":"2022-02-27T10:52:17.94431Z","iopub.status.idle":"2022-02-27T10:52:17.950085Z","shell.execute_reply.started":"2022-02-27T10:52:17.944268Z","shell.execute_reply":"2022-02-27T10:52:17.948692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 4\n    \ninput_ids = processed_tagged_full_data[processed_tagged_full_data['id'].isin(train_ids)]['sent_input_ids'].tolist()\ntoken_ids = processed_tagged_full_data[processed_tagged_full_data['id'].isin(train_ids)]['token_tag_ids'].tolist()\n\npadded_input_ids, padded_attention_mask, padded_tags = pad_data(\n    input_ids=input_ids, \n    token_ids=token_ids,\n    max_token_length = 1000\n)\n\ntrain_dataloader = create_dataloader(\n    token_ids=padded_input_ids, \n    masks=padded_attention_mask, \n    tags=padded_tags, \n    batch_size=batch_size, \n    val=False\n)\n\ninput_ids = processed_tagged_full_data[processed_tagged_full_data['id'].isin(valid_ids)]['sent_input_ids'].tolist()\ntoken_ids = processed_tagged_full_data[processed_tagged_full_data['id'].isin(valid_ids)]['token_tag_ids'].tolist()\n\npadded_input_ids, padded_attention_mask, padded_tags = pad_data(\n    input_ids=input_ids, \n    token_ids=token_ids,\n    max_token_length = 1000\n)\n\nval_dataloader = create_dataloader(\n    token_ids=padded_input_ids, \n    masks=padded_attention_mask, \n    tags=padded_tags, \n    batch_size=batch_size, \n    val=True\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:17.952381Z","iopub.execute_input":"2022-02-27T10:52:17.953141Z","iopub.status.idle":"2022-02-27T10:52:20.705508Z","shell.execute_reply.started":"2022-02-27T10:52:17.953081Z","shell.execute_reply":"2022-02-27T10:52:20.704587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:20.707153Z","iopub.execute_input":"2022-02-27T10:52:20.707459Z","iopub.status.idle":"2022-02-27T10:52:21.453288Z","shell.execute_reply.started":"2022-02-27T10:52:20.707402Z","shell.execute_reply":"2022-02-27T10:52:21.452092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(\n    \"allenai/longformer-base-4096\", \n    num_labels=len(tag2idx),\n    id2label=idx2tag,\n    label2id=tag2idx\n)\nmodel = AutoModelForTokenClassification.from_pretrained(\"allenai/longformer-base-4096\", config=config)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:21.455802Z","iopub.execute_input":"2022-02-27T10:52:21.456638Z","iopub.status.idle":"2022-02-27T10:52:45.721516Z","shell.execute_reply.started":"2022-02-27T10:52:21.456582Z","shell.execute_reply":"2022-02-27T10:52:45.720596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:45.723264Z","iopub.execute_input":"2022-02-27T10:52:45.723621Z","iopub.status.idle":"2022-02-27T10:52:45.733816Z","shell.execute_reply.started":"2022-02-27T10:52:45.723576Z","shell.execute_reply":"2022-02-27T10:52:45.732611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(device)\n\nmodel = model.to(device)\n\noptimizer = AdamW(model.parameters(),\n                  lr = 1e-5) # learning rate\n\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:45.736853Z","iopub.execute_input":"2022-02-27T10:52:45.737209Z","iopub.status.idle":"2022-02-27T10:52:52.061044Z","shell.execute_reply.started":"2022-02-27T10:52:45.73715Z","shell.execute_reply":"2022-02-27T10:52:52.060102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def do_train(model, optimizer, loss_criteria, train_dataloader):\n    try:\n        model.train()\n\n        total_loss = 0\n        total_logits = []\n\n        # iterate over batches\n        for step, batch in enumerate(train_dataloader):\n\n            # progress update after every 50 batches.\n            if step % 50 == 0 and not step == 0:\n                print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n\n            # push the batch to gpu\n            batch = [r.to(device) for r in batch]\n\n            sent_id, mask, labels = batch\n            \n            gc.collect()\n            torch.cuda.empty_cache()\n            # clear previously calculated gradients\n            model.zero_grad()\n\n            # get model predictions for the current batch\n            logits = model(sent_id.to(device), mask.to(device))\n\n            # compute the loss between actual and predicted values\n            loss = loss_criteria(logits.logits.permute(0, 2, 1), labels)\n\n            # add on to the total loss\n            total_loss = total_loss + loss.item()\n\n            # backward pass to calculate the gradients\n            loss.backward()\n\n            # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # update parameters\n            optimizer.step()\n\n            # model predictions are stored on GPU. So, push it to CPU\n            logits = logits.logits.detach().cpu().numpy()\n\n            # append the model predictions\n            total_logits.append(logits)\n\n        # compute the training loss of the epoch\n        avg_loss = total_loss / len(train_dataloader)\n\n        total_logits = np.concatenate(total_logits, axis=0)\n\n\n        return avg_loss, total_logits\n    except Exception as e:\n        print(f\"Error during training the model on line: {sys.exc_info()[2].tb_lineno}\")\n        print(e)\n\n\n# function for evaluating the model\ndef do_evaluate(model, val_dataloader, loss_criteria):\n    print(\"\\nEvaluating...\")\n    \n    # deactivate dropout layers\n    model.eval()\n\n    total_loss, total_accuracy = 0, 0\n\n    # empty list to save the model predictions\n    total_logits = []\n\n    # iterate over batches\n    for step, batch in enumerate(val_dataloader):\n\n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            #             elapsed = format_time(time.time() - t0)\n\n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n\n        # push the batch to gpu\n        batch = [t.to(device) for t in batch]\n\n        sent_id, mask, labels = batch\n\n        # deactivate autograd\n        with torch.no_grad():\n\n            # model predictions\n            logits = model(sent_id.to(device), mask.to(device))\n\n            # compute the validation loss between actual and predicted values\n            loss = loss_criteria(logits.logits.permute(0, 2, 1), labels)\n\n            total_loss = total_loss + loss.item()\n\n            logits = logits.logits.detach().cpu().numpy()\n\n            total_logits.append(logits)\n\n    # compute the validation loss of the epoch\n    avg_loss = total_loss / len(val_dataloader)\n\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_logits = np.concatenate(total_logits, axis=0)\n\n    return avg_loss, total_logits","metadata":{"execution":{"iopub.status.busy":"2022-02-27T10:52:52.062576Z","iopub.execute_input":"2022-02-27T10:52:52.062891Z","iopub.status.idle":"2022-02-27T10:52:52.104833Z","shell.execute_reply.started":"2022-02-27T10:52:52.062832Z","shell.execute_reply":"2022-02-27T10:52:52.103896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# set initial loss to infinity\nbest_valid_loss = float('inf')\n\nepochs=4 # Can increase this and see better performance\n\n# empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n#for each epoch\n\nfor epoch in range(epochs):\n    start = time.time()\n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n\n    #train model\n    train_loss, _ = do_train(\n        model = model,\n        optimizer = optimizer, \n        loss_criteria = criterion, \n        train_dataloader = train_dataloader \n    )\n    \n    #evaluate model\n    valid_loss, _ = do_evaluate(\n        model = model, \n        val_dataloader = val_dataloader, \n        loss_criteria = criterion\n    )\n    \n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        model.save_pretrained('longformer_model_files')\n        tokenizer.save_pretrained('tokenizer_file')\n        model.config.to_json_file('config_np_new.json')\n\n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n\n    print(f\"Time taken: {time.time() - start}\")\n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:36:51.868681Z","iopub.execute_input":"2022-02-27T12:36:51.869224Z","iopub.status.idle":"2022-02-27T17:44:01.966988Z","shell.execute_reply.started":"2022-02-27T12:36:51.869183Z","shell.execute_reply":"2022-02-27T17:44:01.965914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Load fine-tuned model\n# model = AutoModelForTokenClassification.from_pretrained('../input/model_longformer')\n# model = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T17:55:40.334914Z","iopub.execute_input":"2022-02-27T17:55:40.335199Z","iopub.status.idle":"2022-02-27T17:55:40.340165Z","shell.execute_reply.started":"2022-02-27T17:55:40.335168Z","shell.execute_reply":"2022-02-27T17:55:40.338987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_prediction_from_logits(logits):\n    try:\n        tag_prob = nn.Softmax(dim=2)(logits)\n        tag_prediction = torch.argmax(tag_prob, dim=2).detach().cpu().numpy()\n        return tag_prediction\n    except Exception as e:\n        print(f\"Error in line: {sys.exc_info()[2].tb_lineno}\")\n        print(e)\n        \ndef classification_result(tag2idx, c_tag_id):\n    try:\n        prediction_result = []\n        for sent_ in c_tag_id:\n            prediction_result.append(\n                list(map(lambda x: list(tag2idx.keys())[list(tag2idx.values()).index(x)], sent_))\n            )\n            \n        tagged_entity = np.concatenate(prediction_result, axis=0)\n        return tagged_entity\n    except Exception as e:\n        print(f\"Error in line: {sys.exc_info()[2].tb_lineno}\")\n        print(e) ","metadata":{"execution":{"iopub.status.busy":"2022-02-27T18:10:19.854955Z","iopub.execute_input":"2022-02-27T18:10:19.855271Z","iopub.status.idle":"2022-02-27T18:10:19.865688Z","shell.execute_reply.started":"2022-02-27T18:10:19.855239Z","shell.execute_reply":"2022-02-27T18:10:19.864362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"record_id = '423A1CA112E2' # Specify any record id for which predictions are to be seen\n\nprocessed_tagged_data = sent_tag_tokenization(data=train_data_for_modelling[train_data_for_modelling['id']==record_id].reset_index(drop = True))\n\ninput_ids = processed_tagged_data['sent_input_ids'].tolist()\ntoken_ids = processed_tagged_data['token_tag_ids'].tolist()\n\npadded_input_ids, padded_attention_mask, padded_tags = pad_data(\n    input_ids=input_ids, \n    token_ids=token_ids,\n    max_token_length = 1000\n)\n\nval_dataloader = create_dataloader(\n    token_ids=padded_input_ids, \n    masks=padded_attention_mask, \n    tags=padded_tags, \n    batch_size=batch_size, \n    val=True\n)\n\nfor batch in val_dataloader:\n    break\n    \n# deactivate dropout layers\nmodel.eval()\n\nsent_id, mask, labels = batch\n\n# deactivate autograd\nwith torch.no_grad():\n    # model predictions\n    logits = model(sent_id.to(device), mask.to(device))\n    logits = get_prediction_from_logits(logits['logits'])\n    \ndisplay(train_data[train_data['id']==record_id])\n\nprint('Complete text:\\n')\ndisplay(train_data_for_modelling[train_data_for_modelling['id']==record_id]['cleaned_text'].iloc[0])\nprint('Predicted tags:\\n')\ndisplay(np.array([idx2tag[x] for x in logits[0]]))","metadata":{"execution":{"iopub.status.busy":"2022-02-27T18:16:57.506454Z","iopub.execute_input":"2022-02-27T18:16:57.506792Z","iopub.status.idle":"2022-02-27T18:16:57.892811Z","shell.execute_reply.started":"2022-02-27T18:16:57.506761Z","shell.execute_reply":"2022-02-27T18:16:57.891672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions on Test data","metadata":{}},{"cell_type":"code","source":"test_data_for_modelling = submission_data[['id']].drop_duplicates().reset_index(drop = True)\ntest_data_for_modelling['Text'] = np.NaN\nfor i in tqdm(range(0, test_data_for_modelling.shape[0])):\n    record_id = test_data_for_modelling['id'].iloc[i]\n    with open(filepath + 'test/' + record_id+'.txt','r') as f:\n        data = f.read()\n    test_data_for_modelling['Text'].iloc[i] = data\n    \ntest_data_for_modelling['cleaned_text'] = test_data_for_modelling['Text'].apply(lambda x: re.sub('\\n+',' ',x).strip().replace(\"\\xa0\",\" \"))","metadata":{"execution":{"iopub.status.busy":"2022-02-27T18:20:05.363043Z","iopub.execute_input":"2022-02-27T18:20:05.363368Z","iopub.status.idle":"2022-02-27T18:20:05.408901Z","shell.execute_reply.started":"2022-02-27T18:20:05.363338Z","shell.execute_reply":"2022-02-27T18:20:05.406901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_for_modelling.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T18:20:12.232593Z","iopub.execute_input":"2022-02-27T18:20:12.232987Z","iopub.status.idle":"2022-02-27T18:20:12.247856Z","shell.execute_reply.started":"2022-02-27T18:20:12.232946Z","shell.execute_reply":"2022-02-27T18:20:12.246623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_tagged_data = sent_tag_tokenization(data=test_data_for_modelling)\n\ninput_ids = processed_tagged_data['sent_input_ids'].tolist()\ntoken_ids = processed_tagged_data['token_tag_ids'].tolist()\n\npadded_input_ids, padded_attention_mask, padded_tags = pad_data(\n    input_ids=input_ids, \n    token_ids=token_ids,\n    max_token_length = 1000\n)\n\nbatch_size = 4\n\ntest_dataloader = create_dataloader(\n    token_ids=padded_input_ids, \n    masks=padded_attention_mask, \n    tags=padded_tags, \n    batch_size=batch_size, \n    val=True\n)\n\npred_tags = []\nfor batch in test_dataloader:\n    # deactivate dropout layers\n    model.eval()\n\n    sent_id, mask, labels = batch\n\n    # deactivate autograd\n    with torch.no_grad():\n        # model predictions\n        logits = model(sent_id.to(device), mask.to(device))\n        logits = get_prediction_from_logits(logits['logits'])\n    pred_tags.append(logits)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T18:45:20.206167Z","iopub.execute_input":"2022-02-27T18:45:20.206454Z","iopub.status.idle":"2022-02-27T18:45:20.956649Z","shell.execute_reply.started":"2022-02-27T18:45:20.206424Z","shell.execute_reply":"2022-02-27T18:45:20.95578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_for_modelling['Predicted tags'] = 'to_be_filled_with_predicted_tags'\npred_tags = np.concatenate(pred_tags)\nfor i in range(0, len(test_data_for_modelling)):\n    test_data_for_modelling.at[i, 'Predicted tags'] = np.array(pred_tags[i], dtype=object)\ntest_data_for_modelling['Predicted tags'] = test_data_for_modelling['Predicted tags'].apply(lambda x: [idx2tag[t] for t in x])","metadata":{"execution":{"iopub.status.busy":"2022-02-27T18:45:22.79217Z","iopub.execute_input":"2022-02-27T18:45:22.792461Z","iopub.status.idle":"2022-02-27T18:45:22.801176Z","shell.execute_reply.started":"2022-02-27T18:45:22.792429Z","shell.execute_reply":"2022-02-27T18:45:22.799721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_for_modelling.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:03:57.947768Z","iopub.execute_input":"2022-02-27T19:03:57.948048Z","iopub.status.idle":"2022-02-27T19:03:57.965892Z","shell.execute_reply.started":"2022-02-27T19:03:57.948019Z","shell.execute_reply":"2022-02-27T19:03:57.964809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get prediction string as per submission format\ntest_data_for_submission_final = pd.DataFrame(data = None, columns = ['id','class','predictionstring','token','discourse type count']) \nfor i in tqdm(range(0, len(test_data_for_modelling))):\n    test_data_for_submission = pd.DataFrame(data = None, columns = ['id','class','predictionstring','token']) \n    record_id = test_data_for_modelling['id'].iloc[i]\n    pred_tags = test_data_for_modelling['Predicted tags'].iloc[i]\n    count = 0\n    for j in range(0, len(pred_tags)):\n        token = pred_tags[j]\n        if token != 'O':\n            discourse_type = token.split('-')[1]\n            test_data_for_submission.loc[count] = [record_id, discourse_type, count, token]\n            count += 1\n        else:\n            discourse_type = np.NaN\n\n    count = 0\n    test_data_for_submission['discourse type count'] = np.NaN\n    for k in range(0, len(test_data_for_submission)):\n        if test_data_for_submission['token'].iloc[k].split('-')[0] == 'B':\n            discourse_type = test_data_for_submission['token'].iloc[k].split('-')[1]\n            test_data_for_submission['discourse type count'].iloc[k] = count\n            for j in range(k+1, len(test_data_for_submission)):\n                if (test_data_for_submission['token'].iloc[j].split('-')[0] == 'I') & (test_data_for_submission['token'].iloc[j].split('-')[1] == discourse_type):\n                    test_data_for_submission['discourse type count'].iloc[j] = count\n                else:\n                    break\n            count = count + 1\n    test_data_for_submission_final = pd.concat([test_data_for_submission_final, test_data_for_submission], axis = 0).reset_index(drop = True)\n    \nkeys = test_data_for_submission_final['id'].unique().tolist()\nvalues = [x for x in range(0, len(test_data_for_submission_final['id'].unique().tolist()))]\ntemp = dict(zip(keys, values))\ntest_data_for_submission_final['rec_num'] = test_data_for_submission_final['id'].map(temp) \n\ntest_data_for_submission_final['predictionstring'] = test_data_for_submission_final['predictionstring'].astype(str)\ntest_data_for_submission_final = test_data_for_submission_final.groupby(['id','class','discourse type count','rec_num']).agg({'predictionstring': ' '.join}).reset_index().sort_values(by = ['rec_num','discourse type count'])[['id','class','predictionstring']]","metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:54:35.344144Z","iopub.execute_input":"2022-02-27T19:54:35.344833Z","iopub.status.idle":"2022-02-27T19:54:53.88836Z","shell.execute_reply.started":"2022-02-27T19:54:35.344798Z","shell.execute_reply":"2022-02-27T19:54:53.8866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_for_submission_final.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T19:54:53.890717Z","iopub.execute_input":"2022-02-27T19:54:53.89106Z","iopub.status.idle":"2022-02-27T19:54:53.905189Z","shell.execute_reply.started":"2022-02-27T19:54:53.891015Z","shell.execute_reply":"2022-02-27T19:54:53.903823Z"},"trusted":true},"execution_count":null,"outputs":[]}]}