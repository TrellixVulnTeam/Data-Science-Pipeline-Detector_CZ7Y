{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hello Fellow Kagglers,\n\nThis notebook demonstrates the preprocessing of the data for the Feedback Price competition by tokenizing the excerpts and oversampling the minority classes.\n\nThis oversampling process reduces the class inbalance and should make the model less biased towards the majority class.\n\n[Training Notebook](https://www.kaggle.com/markwijkhuizen/training-longformer-gradient-accumulation)\n\nInference Notebook Coming Soon","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom transformers import LongformerTokenizer\nfrom tqdm.notebook import tqdm\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom sklearn.model_selection import train_test_split\n\nimport glob\nimport re\n\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:09.573155Z","iopub.execute_input":"2022-02-04T12:08:09.57424Z","iopub.status.idle":"2022-02-04T12:08:17.918332Z","shell.execute_reply.started":"2022-02-04T12:08:09.574096Z","shell.execute_reply":"2022-02-04T12:08:17.917589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In debug mode a subset of the training dataset is used\nDEBUG = False","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:17.919932Z","iopub.execute_input":"2022-02-04T12:08:17.920161Z","iopub.status.idle":"2022-02-04T12:08:17.926006Z","shell.execute_reply.started":"2022-02-04T12:08:17.920133Z","shell.execute_reply":"2022-02-04T12:08:17.923116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read Train DataFrame","metadata":{}},{"cell_type":"code","source":"# Column Data Types\ndtype = {\n    'id': 'string',\n    'discourse_id': np.uint64,\n    'discourse_start': np.uint16,\n    'discourse_end': np.uint16,\n    'discourse_text': 'string',\n    'discourse_type': 'category',\n    'discourse_type_num': 'category',\n    'predictionstring': 'string',\n}\n\nif DEBUG:\n    train = pd.read_csv('/kaggle/input/feedback-prize-2021/train.csv', dtype=dtype).head(int(10e3))\nelse:\n    train = pd.read_csv('/kaggle/input/feedback-prize-2021/train.csv', dtype=dtype)\n\ndisplay(train.head())\n\ndisplay(train.info())","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:17.927857Z","iopub.execute_input":"2022-02-04T12:08:17.928167Z","iopub.status.idle":"2022-02-04T12:08:19.893094Z","shell.execute_reply.started":"2022-02-04T12:08:17.928125Z","shell.execute_reply":"2022-02-04T12:08:19.892323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word and Sentence Count","metadata":{}},{"cell_type":"code","source":"# Number of Annotated Words, Ignoring Double Annotated Words\ndef sample_ann_word_count(predictionstrings):\n    s = set()\n    \n    for l in predictionstrings.str.split():\n        for e in l:\n            s.add(int(e))\n        \n    return [len(s)] * len(predictionstrings)\n\n# Text Word Count\ntrain['word_count'] = train['discourse_text'].apply(word_tokenize).apply(len).astype(np.uint16)\n\n# Text Word Count\ntrain['ann_word_count'] = train['predictionstring'].str.split(' ').apply(len).astype(np.uint16)\n\n# Sample Word Count\ntrain['sample_ann_word_count'] = train.groupby('id')['predictionstring'].transform(sample_ann_word_count).astype(np.uint16)\n\n# Text Sentence Count\ntrain['sentence_count'] = train['discourse_text'].apply(sent_tokenize).apply(len).astype(np.uint16)\n\n# Maximum Word Index\ntrain['max_word_index'] = train['predictionstring'].str.split(' ').apply(lambda l: int(l[-1])).astype(np.uint16)\n\n# Max Word Index of Text ID\ntrain['sample_max_word_index'] = train.groupby('id')['max_word_index'].transform('max')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:19.894444Z","iopub.execute_input":"2022-02-04T12:08:19.89496Z","iopub.status.idle":"2022-02-04T12:08:28.121423Z","shell.execute_reply.started":"2022-02-04T12:08:19.894886Z","shell.execute_reply":"2022-02-04T12:08:28.120579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text ID to Word Count\ndef id2sample_word_count(text_ids):\n    text_id = text_ids.values[0]\n    # Read Text File\n    with open(f'/kaggle/input/feedback-prize-2021/train/{text_id}.txt', 'r') as f:\n        text = f.read().split()\n        word_count = len(text)\n        \n    return [word_count] * len(text_ids)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:28.124204Z","iopub.execute_input":"2022-02-04T12:08:28.124526Z","iopub.status.idle":"2022-02-04T12:08:28.12983Z","shell.execute_reply.started":"2022-02-04T12:08:28.124484Z","shell.execute_reply":"2022-02-04T12:08:28.12919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample Word Count\ntrain['sample_word_count'] = train.groupby('id')['id'].transform(id2sample_word_count).astype(np.uint16)\n\n# Ratio of Annotated Words\ntrain['ann_ratio'] = (train['sample_ann_word_count'] / train['sample_word_count']).astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:28.130617Z","iopub.execute_input":"2022-02-04T12:08:28.130989Z","iopub.status.idle":"2022-02-04T12:08:33.274659Z","shell.execute_reply.started":"2022-02-04T12:08:28.130964Z","shell.execute_reply":"2022-02-04T12:08:33.27381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train.head(10))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:33.276003Z","iopub.execute_input":"2022-02-04T12:08:33.276411Z","iopub.status.idle":"2022-02-04T12:08:33.297042Z","shell.execute_reply.started":"2022-02-04T12:08:33.276373Z","shell.execute_reply":"2022-02-04T12:08:33.29634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Discourse Class Distribution\n\nThe discourse types are unbalances, which occurances the range of 2.7% to 33.2%.","metadata":{}},{"cell_type":"code","source":"# Discourse Type Distribution\nplt.figure(figsize=(10,10))\ntrain.groupby('discourse_type')['discourse_type'].count().plot(kind='pie', autopct='%1.1f%%', textprops={'fontsize': 16}, startangle=0)\nplt.title('Discourse Type Distribution', size=24)\nplt.ylabel('')\npass","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:33.299198Z","iopub.execute_input":"2022-02-04T12:08:33.299484Z","iopub.status.idle":"2022-02-04T12:08:33.506693Z","shell.execute_reply.started":"2022-02-04T12:08:33.299448Z","shell.execute_reply":"2022-02-04T12:08:33.505783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When looking at the number of annotated words per discourse type the inbalance becomes even more severe, with occurances ranging from 1.7% to 57.5%.","metadata":{}},{"cell_type":"code","source":"# Discourse Type Distribution\nplt.figure(figsize=(10,10))\ntrain.groupby('discourse_type')['ann_word_count'].sum().plot(kind='pie', autopct='%1.1f%%', textprops={'fontsize': 16}, startangle=0)\nplt.title('Discourse Type Annotated Words Distribution', size=24)\nplt.ylabel('')\npass","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:33.507894Z","iopub.execute_input":"2022-02-04T12:08:33.50816Z","iopub.status.idle":"2022-02-04T12:08:33.671116Z","shell.execute_reply.started":"2022-02-04T12:08:33.508124Z","shell.execute_reply":"2022-02-04T12:08:33.670355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The annotated word count inbalance can be explained by the difference in discourse type size\ndisplay(train.groupby(['discourse_type'])['ann_word_count'].describe())","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:33.672152Z","iopub.execute_input":"2022-02-04T12:08:33.67237Z","iopub.status.idle":"2022-02-04T12:08:33.709819Z","shell.execute_reply.started":"2022-02-04T12:08:33.672344Z","shell.execute_reply":"2022-02-04T12:08:33.709141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Annotation Count","metadata":{}},{"cell_type":"code","source":"# Word Count Distribution\nplt.figure(figsize=(15, 8))\ntrain.groupby('id')['word_count'].sum().plot(kind='hist', bins=32)\nplt.title('Word Count Distribution', size=24)\nplt.xlabel('Word Count', size=18)\nplt.ylabel('Frequency', size=18)\nplt.xticks(size=16)\nplt.yticks(size=16)\nplt.grid()\npass","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:33.71071Z","iopub.execute_input":"2022-02-04T12:08:33.71088Z","iopub.status.idle":"2022-02-04T12:08:34.03073Z","shell.execute_reply.started":"2022-02-04T12:08:33.710858Z","shell.execute_reply":"2022-02-04T12:08:34.030038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word Count Distribution\nplt.figure(figsize=(15, 8))\ntrain.groupby('id')['ann_word_count'].sum().plot(kind='hist', bins=32)\nplt.title('Text Annotated Word Count Distribution', size=24)\nplt.xlabel('Text Annotated Word Count', size=18)\nplt.ylabel('Frequency', size=18)\nplt.xticks(size=16)\nplt.yticks(size=16)\nplt.grid()\npass","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:34.031845Z","iopub.execute_input":"2022-02-04T12:08:34.032018Z","iopub.status.idle":"2022-02-04T12:08:34.350989Z","shell.execute_reply.started":"2022-02-04T12:08:34.031996Z","shell.execute_reply":"2022-02-04T12:08:34.350351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word Count Distribution\nplt.figure(figsize=(15, 8))\ntrain.groupby('id')['sample_word_count'].first().plot(kind='hist', bins=32)\nplt.title('Sample Word Count Distribution', size=24)\nplt.xlabel('Sample Word Count', size=18)\nplt.ylabel('Frequency', size=18)\nplt.xticks(size=16)\nplt.yticks(size=16)\nplt.grid()\npass","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:34.352147Z","iopub.execute_input":"2022-02-04T12:08:34.352354Z","iopub.status.idle":"2022-02-04T12:08:34.667088Z","shell.execute_reply.started":"2022-02-04T12:08:34.352329Z","shell.execute_reply":"2022-02-04T12:08:34.666367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word Count Distribution\nplt.figure(figsize=(15, 8))\ntrain.groupby('id')['sentence_count'].sum().plot(kind='hist', bins=32)\nplt.title('Sentence Count Distribution', size=24)\nplt.xlabel('Sentence Count', size=18)\nplt.ylabel('Frequency', size=18)\nplt.xticks(size=16)\nplt.yticks(size=16)\nplt.grid()\npass","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:34.670204Z","iopub.execute_input":"2022-02-04T12:08:34.670428Z","iopub.status.idle":"2022-02-04T12:08:34.986073Z","shell.execute_reply.started":"2022-02-04T12:08:34.670401Z","shell.execute_reply":"2022-02-04T12:08:34.985319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Annotation Ratio\n\nMost texts have an annotation ratio, which is the ratio of words which are annotated in a text, close to 100%. There are however a handful of texts which have less than 70% of words annotated which will be filtered out.","metadata":{}},{"cell_type":"code","source":"# Annotation Ratio Distribution\nplt.figure(figsize=(20, 8))\ntrain.groupby('id')['ann_ratio'].first().plot(kind='hist', bins=32)\nplt.title('Annotation Ratio', size=24)\nplt.xlabel('Annotation Ration', size=18)\nplt.ylabel('Frequency', size=18)\nplt.xticks([i * 0.05 for i in range(21)], size=14)\nplt.yticks(size=16)\nplt.grid()\npass","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:34.987218Z","iopub.execute_input":"2022-02-04T12:08:34.987439Z","iopub.status.idle":"2022-02-04T12:08:35.371123Z","shell.execute_reply.started":"2022-02-04T12:08:34.987413Z","shell.execute_reply":"2022-02-04T12:08:35.370264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Longformer Tokenizer","metadata":{}},{"cell_type":"code","source":"# Input Sequence Length\nSEQ_LENGTH= 4096","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:35.372489Z","iopub.execute_input":"2022-02-04T12:08:35.372692Z","iopub.status.idle":"2022-02-04T12:08:35.37671Z","shell.execute_reply.started":"2022-02-04T12:08:35.372667Z","shell.execute_reply":"2022-02-04T12:08:35.37596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the Tokenizer\ntokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-large-4096')\n\n# Save Tokenizer\ntokenizer.save_pretrained('./tokenizer/')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:09:55.204377Z","iopub.execute_input":"2022-02-04T12:09:55.204718Z","iopub.status.idle":"2022-02-04T12:10:03.114074Z","shell.execute_reply.started":"2022-02-04T12:09:55.204662Z","shell.execute_reply":"2022-02-04T12:10:03.113267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This function tokenize the text according to a transformers model tokenizer\ndef tokenize(excerpt, padding='max_length', max_length=SEQ_LENGTH):\n    enc_di = tokenizer.encode_plus(\n        excerpt,\n        padding = padding,\n        truncation = True,\n        max_length = max_length,\n    )\n    \n    return np.array(enc_di['input_ids'], dtype=np.int32)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:50.151587Z","iopub.execute_input":"2022-02-04T12:08:50.151802Z","iopub.status.idle":"2022-02-04T12:08:50.156958Z","shell.execute_reply.started":"2022-02-04T12:08:50.151778Z","shell.execute_reply":"2022-02-04T12:08:50.156128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Token Length","metadata":{}},{"cell_type":"code","source":"def get_token_lengths(text_ids):\n    text_id = text_ids.values[0]\n    # Read Text File\n    with open(f'/kaggle/input/feedback-prize-2021/train/{text_id}.txt', 'r') as f:\n        text = f.read().split(' ')\n        \n    # Tokenize Text\n    text_encoded = tokenize(text, padding='do_not_pad', max_length=np.PINF)\n    \n    return [len(text_encoded)] * len(text_ids)\n\n# Get Token Length\ntrain['token_len'] = train.groupby('id')['id'].progress_transform(get_token_lengths).astype(np.uint16)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:50.158079Z","iopub.execute_input":"2022-02-04T12:08:50.158298Z","iopub.status.idle":"2022-02-04T12:08:52.09187Z","shell.execute_reply.started":"2022-02-04T12:08:50.158273Z","shell.execute_reply":"2022-02-04T12:08:52.091168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Token Length Distribution\nplt.figure(figsize=(15, 8))\ntrain.groupby('id')['token_len'].first().plot(kind='hist', bins=64)\nplt.title('Token Length Distribution', size=24)\nplt.xlabel('Token Length', size=18)\nplt.ylabel('Frequency', size=18)\nplt.xticks(size=16)\nplt.yticks(size=16)\nplt.grid()\npass","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:52.092814Z","iopub.execute_input":"2022-02-04T12:08:52.093066Z","iopub.status.idle":"2022-02-04T12:08:52.46402Z","shell.execute_reply.started":"2022-02-04T12:08:52.093038Z","shell.execute_reply":"2022-02-04T12:08:52.463226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Filter Samples","metadata":{}},{"cell_type":"code","source":"# Minimum Annotation Ratio\nANN_RATIO_THRESHOLD = 0.70\n\n# All Text Ids\nALL_TEXT_IDS = train['id'].nunique()\n\n# Drop Sample with Word Count Above Threshold and Annotation Ratio Below Threshold\ndrop_idxs = train[\n    (train['token_len'] > SEQ_LENGTH) | # Token Length Larger than Max Sequence Length\n    (train['ann_ratio'] < ANN_RATIO_THRESHOLD) | # Annotation Ratio Below Threshold\n    (train['max_word_index'] >= SEQ_LENGTH) # Max Word Index Larger than Max Sequence Length\n].index\ntrain = train.drop(drop_idxs, axis=0).reset_index(drop=True)\n\n# Valid Text Ids\nVALID_TEXT_IDS = list(train['id'].unique())\nN_VALID_TEXT_IDS = len(VALID_TEXT_IDS)\nprint(f'{N_VALID_TEXT_IDS} valid text ids out of {ALL_TEXT_IDS} text ids')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:52.465482Z","iopub.execute_input":"2022-02-04T12:08:52.466342Z","iopub.status.idle":"2022-02-04T12:08:52.494343Z","shell.execute_reply.started":"2022-02-04T12:08:52.466292Z","shell.execute_reply":"2022-02-04T12:08:52.493492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train/Validation Split","metadata":{}},{"cell_type":"code","source":"# We will be using 1024 Validation Samples\nN_VAL = 1024\nX_train_ids, X_val_ids = train_test_split(train['id'].unique(), test_size=N_VAL, random_state=42)\nprint(f'X_train_ids shape: {X_train_ids.shape}, X_val_ids shape: {X_val_ids.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:52.495486Z","iopub.execute_input":"2022-02-04T12:08:52.495715Z","iopub.status.idle":"2022-02-04T12:08:52.505313Z","shell.execute_reply.started":"2022-02-04T12:08:52.495686Z","shell.execute_reply":"2022-02-04T12:08:52.504478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_ids = list(X_train_ids)\nX_val_ids = list(X_val_ids)\n\nDISCOURSE_TYPES = train['discourse_type'].unique().tolist()\n\nid2discourse_types_dict = train.groupby('id')['discourse_type'].unique().to_dict()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:52.506305Z","iopub.execute_input":"2022-02-04T12:08:52.50686Z","iopub.status.idle":"2022-02-04T12:08:52.610498Z","shell.execute_reply.started":"2022-02-04T12:08:52.506815Z","shell.execute_reply":"2022-02-04T12:08:52.609726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Oversampling","metadata":{}},{"cell_type":"code","source":"# Dicsourse Type Count\ndef train_ids2discourse_type_counts():\n    discourse_type_counts_train_ids = dict([(dt, 0) for dt  in DISCOURSE_TYPES])\n    \n    for train_id in X_train_ids:\n        for dt in id2discourse_types_dict[train_id]:\n            discourse_type_counts_train_ids[dt] += 1\n            \n    return pd.Series(discourse_type_counts_train_ids).sort_values()\n\ndiscourse_type_counts0 = train_ids2discourse_type_counts()\n\n# Discour Type Count\ndisplay(discourse_type_counts0.sort_index())","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:52.611894Z","iopub.execute_input":"2022-02-04T12:08:52.612139Z","iopub.status.idle":"2022-02-04T12:08:52.624181Z","shell.execute_reply.started":"2022-02-04T12:08:52.612113Z","shell.execute_reply":"2022-02-04T12:08:52.623483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill All Classes to the Majority Class\nDISCOURSE_TYPES = discourse_type_counts0.index.tolist()\nFILL_TO = max(discourse_type_counts0)\n\nprint(f'DISCOURSE_TYPES: {DISCOURSE_TYPES}, FILL_TO: {FILL_TO}')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:52.625462Z","iopub.execute_input":"2022-02-04T12:08:52.625934Z","iopub.status.idle":"2022-02-04T12:08:52.630177Z","shell.execute_reply.started":"2022-02-04T12:08:52.625887Z","shell.execute_reply":"2022-02-04T12:08:52.629596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Oversample to Maximum Sample Count\nfor dt in tqdm(DISCOURSE_TYPES):\n    # Get current Discourse Type Count\n    discourse_type_counts = train_ids2discourse_type_counts()\n    samples_discourse_type = discourse_type_counts[dt]\n    if samples_discourse_type < FILL_TO:\n        while samples_discourse_type < FILL_TO:\n            # Take Random ID\n            random_id = str(np.random.choice(X_train_ids, 1).squeeze())\n            if dt in id2discourse_types_dict[random_id]:\n                X_train_ids.append(random_id)\n                samples_discourse_type += 1","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:52.631246Z","iopub.execute_input":"2022-02-04T12:08:52.631491Z","iopub.status.idle":"2022-02-04T12:08:52.703374Z","shell.execute_reply.started":"2022-02-04T12:08:52.631466Z","shell.execute_reply":"2022-02-04T12:08:52.702715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Oversample Statistics","metadata":{}},{"cell_type":"code","source":"print('=== BEFORE ===')\ndisplay(discourse_type_counts0.to_frame().sort_index())\nprint('=== AFTER ===')\ndisplay(discourse_type_counts.to_frame().sort_index())\nprint('=== DIFFERENCE PERCENTAGE ===')\npercentual_increase = ((discourse_type_counts - discourse_type_counts0) / discourse_type_counts0 * 100)\npercentual_increase = percentual_increase.apply(lambda i: f'{int(i)}%')\npercentual_increase = percentual_increase.to_frame(name='Percentage Increase')\ndisplay(percentual_increase.sort_index())","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:52.704272Z","iopub.execute_input":"2022-02-04T12:08:52.704958Z","iopub.status.idle":"2022-02-04T12:08:52.728009Z","shell.execute_reply.started":"2022-02-04T12:08:52.704925Z","shell.execute_reply":"2022-02-04T12:08:52.727214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Tokens","metadata":{}},{"cell_type":"code","source":"N_TRAIN_SAMPLES = len(X_train_ids)\nN_VAL_SAMPLES = len(X_val_ids)\n\nprint(f'N_TRAIN_SAMPLES: {N_TRAIN_SAMPLES}, N_VAL_SAMPLES: {N_VAL_SAMPLES}')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:52.729465Z","iopub.execute_input":"2022-02-04T12:08:52.730089Z","iopub.status.idle":"2022-02-04T12:08:52.735271Z","shell.execute_reply.started":"2022-02-04T12:08:52.73003Z","shell.execute_reply":"2022-02-04T12:08:52.734495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Train Directory\n!rm -rf train val\n!mkdir train val","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:52.73632Z","iopub.execute_input":"2022-02-04T12:08:52.737014Z","iopub.status.idle":"2022-02-04T12:08:54.286322Z","shell.execute_reply.started":"2022-02-04T12:08:52.736958Z","shell.execute_reply":"2022-02-04T12:08:54.285396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Tokens","metadata":{}},{"cell_type":"code","source":"train_tokens = np.empty(shape=(N_TRAIN_SAMPLES, SEQ_LENGTH), dtype=np.uint16)\n\n# === TRAIN ===\nfor idx, text_id in enumerate(tqdm(X_train_ids)):\n    \n    # Read Text File\n    with open(f'/kaggle/input/feedback-prize-2021/train/{text_id}.txt', 'r') as f:\n        text = f.read()\n        \n    # Tokenize Text\n    text_encoded = tokenize(text)\n    \n    # Add to Train Tokens Array\n    train_tokens[idx] = text_encoded\n    \n# Save Train Tokens as Numpy Array\nnp.save('train/train_tokens.npy', train_tokens)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:54.290833Z","iopub.execute_input":"2022-02-04T12:08:54.2925Z","iopub.status.idle":"2022-02-04T12:08:55.069003Z","shell.execute_reply.started":"2022-02-04T12:08:54.292459Z","shell.execute_reply":"2022-02-04T12:08:55.068249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation Tokens","metadata":{}},{"cell_type":"code","source":"val_tokens = np.empty(shape=(N_VAL_SAMPLES, SEQ_LENGTH), dtype=np.uint16)\n\n# === VALIDATION ===\nfor idx, text_id in enumerate(tqdm(X_val_ids)):\n    \n    # Read Text File\n    with open(f'/kaggle/input/feedback-prize-2021/train/{text_id}.txt', 'r') as f:\n        text = f.read()\n        \n    # Tokenize Text\n    text_encoded = tokenize(text)\n    \n    # Add to Val Tokens Array\n    val_tokens[idx] = text_encoded\n    \n# Save Val Tokens as Numpy Array\nnp.save('val/val_tokens.npy', val_tokens)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:08:55.070195Z","iopub.execute_input":"2022-02-04T12:08:55.070427Z","iopub.status.idle":"2022-02-04T12:09:02.484017Z","shell.execute_reply.started":"2022-02-04T12:08:55.070399Z","shell.execute_reply":"2022-02-04T12:09:02.483299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Labels","metadata":{}},{"cell_type":"code","source":"# Text Id to Token Length Mapping\nID2TOKEN_LEN = train[['id', 'token_len']].set_index('id').squeeze().to_dict()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:09:02.485251Z","iopub.execute_input":"2022-02-04T12:09:02.485631Z","iopub.status.idle":"2022-02-04T12:09:02.504745Z","shell.execute_reply.started":"2022-02-04T12:09:02.485581Z","shell.execute_reply":"2022-02-04T12:09:02.50422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get all labels sorted for reproducibility\nLABELS = train['discourse_type'].unique().sort_values().tolist()\n# Add extra non-annotated and padding label\nN_LABELS = len(LABELS) + 2\n# Not Annotated Class\nNA_CLASS = len(LABELS)\n# Padding Class\nPAD_CLASS = len(LABELS) + 1\n\nprint(f'N_LABELS: {N_LABELS}, NA_CLASS: {NA_CLASS}, PAD_CLASS: {PAD_CLASS}')\nprint(f'LABELS: {LABELS}')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:09:02.506342Z","iopub.execute_input":"2022-02-04T12:09:02.506847Z","iopub.status.idle":"2022-02-04T12:09:02.514227Z","shell.execute_reply.started":"2022-02-04T12:09:02.506804Z","shell.execute_reply":"2022-02-04T12:09:02.513519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text Id to Label\ndef id2label(text_id):\n    group = train[train['id'] == text_id]\n    \n    labels = np.full(fill_value=NA_CLASS, shape=SEQ_LENGTH, dtype=np.int8)\n    # Set with set indices\n    idxs_set = set()\n    \n    # Set Labels\n    for _, row in group.iterrows():\n        # Discourse Type\n        discourse_type = row['discourse_type']\n        # Discourse Label\n        discourse_type_int = LABELS.index(discourse_type)\n        idxs = np.array(row['predictionstring'].split(' '), dtype=np.int16)\n        # filter on indices that are already set\n        idxs = idxs[[e not in idxs_set for e in idxs]]\n        # Set Discourse Labels to 1\n        labels[idxs] = discourse_type_int\n        # Update Indices Seen\n        idxs_set.update(idxs)\n        \n    # Set Padding Class\n    token_len = ID2TOKEN_LEN[text_id]\n    labels[token_len:] = PAD_CLASS\n        \n    return labels","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:09:02.515285Z","iopub.execute_input":"2022-02-04T12:09:02.515779Z","iopub.status.idle":"2022-02-04T12:09:02.523478Z","shell.execute_reply.started":"2022-02-04T12:09:02.515749Z","shell.execute_reply":"2022-02-04T12:09:02.522941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# === TRAIN ===\ntrain_labels = np.zeros(shape=(N_TRAIN_SAMPLES, SEQ_LENGTH), dtype=np.int8)\nprint(f'train_labels shape: {train_labels.shape}')\n\n# Generate Labels\nfor idx, text_id in enumerate(tqdm(X_train_ids)):\n    train_labels[idx] = id2label(text_id)\n    \n# Save Train Labels as Numpy Array\nnp.save('train/train_labels.npy', train_labels)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:09:02.524252Z","iopub.execute_input":"2022-02-04T12:09:02.524951Z","iopub.status.idle":"2022-02-04T12:09:02.914678Z","shell.execute_reply.started":"2022-02-04T12:09:02.524921Z","shell.execute_reply":"2022-02-04T12:09:02.913836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# === VALIDATION ===\nval_labels = np.zeros(shape=(N_VAL_SAMPLES, SEQ_LENGTH), dtype=np.int8)\nprint(f'val_labels shape: {val_labels.shape}')\n\n# Generate Labels\nfor idx, text_id in enumerate(tqdm(X_val_ids)):\n    val_labels[idx] = id2label(text_id)\n    \n# Save Val Labels as Numpy Array\nnp.save('val/val_labels.npy', val_labels)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:09:02.915861Z","iopub.execute_input":"2022-02-04T12:09:02.916088Z","iopub.status.idle":"2022-02-04T12:09:06.899499Z","shell.execute_reply.started":"2022-02-04T12:09:02.91606Z","shell.execute_reply":"2022-02-04T12:09:06.898397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attention Mask ","metadata":{}},{"cell_type":"code","source":"# === TRAIN ===\ntrain_attention_masks = (train_labels != PAD_CLASS).astype(np.int8)\nprint(f'train_attention_masks shape: {train_attention_masks.shape}')\n    \n# Save as Numpy Array\nnp.save('train/train_attention_masks.npy', train_attention_masks)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:09:06.900906Z","iopub.execute_input":"2022-02-04T12:09:06.901291Z","iopub.status.idle":"2022-02-04T12:09:06.908061Z","shell.execute_reply.started":"2022-02-04T12:09:06.901247Z","shell.execute_reply":"2022-02-04T12:09:06.907377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # === VALIDATION ===\nval_attention_masks = (val_labels != PAD_CLASS).astype(np.int8)\nprint(f'val_attention_masks shape: {val_attention_masks.shape}')\n\n# Save as Numpy Array\nnp.save('val/val_attention_masks.npy', val_attention_masks)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:09:06.909141Z","iopub.execute_input":"2022-02-04T12:09:06.90939Z","iopub.status.idle":"2022-02-04T12:09:06.929142Z","shell.execute_reply.started":"2022-02-04T12:09:06.909362Z","shell.execute_reply":"2022-02-04T12:09:06.928399Z"},"trusted":true},"execution_count":null,"outputs":[]}]}