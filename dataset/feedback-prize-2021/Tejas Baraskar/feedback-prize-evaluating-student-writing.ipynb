{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Description","metadata":{"execution":{"iopub.status.busy":"2022-02-18T11:01:52.009876Z","iopub.execute_input":"2022-02-18T11:01:52.010153Z","iopub.status.idle":"2022-02-18T11:01:52.013587Z","shell.execute_reply.started":"2022-02-18T11:01:52.010125Z","shell.execute_reply":"2022-02-18T11:01:52.012742Z"}}},{"cell_type":"markdown","source":"The dataset contains argumentative essays written by U.S students in grades 6-12. The essays were annotated by expert raters for elements commonly found in argumentative writing. The task is to predict the human annotations. You will first need to segment each essay into discrete rhetorical and argumentative elements (i.e., discourse elements) and then classify each element as one of the following: Lead - an introduction that begins with a statistic, a quotation, a description, or some other device to grab the readerâ€™s attention and point toward the thesis\n\n**Position**- an opinion or conclusion on the main question\n\n**Claim** - a claim that supports the position\n\n**Counterclaim** - a claim that refutes another claim or gives an opposing reason to the position\n\n**Rebuttal**- a claim that refutes a counterclaim\n\n**Evidence**- ideas or examples that support claims, counterclaims, or rebuttals.\n\n**Concluding Statement**- a concluding statement that restates the claims","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-18T11:17:43.338421Z","iopub.execute_input":"2022-02-18T11:17:43.339293Z","iopub.status.idle":"2022-02-18T11:17:48.654371Z","shell.execute_reply.started":"2022-02-18T11:17:43.339257Z","shell.execute_reply":"2022-02-18T11:17:48.653427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk","metadata":{"execution":{"iopub.status.busy":"2022-02-18T11:17:48.656395Z","iopub.execute_input":"2022-02-18T11:17:48.656645Z","iopub.status.idle":"2022-02-18T11:17:50.290283Z","shell.execute_reply.started":"2022-02-18T11:17:48.656614Z","shell.execute_reply":"2022-02-18T11:17:50.289487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/feedback-prize-2021/train.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T11:17:50.291664Z","iopub.execute_input":"2022-02-18T11:17:50.292005Z","iopub.status.idle":"2022-02-18T11:17:52.195613Z","shell.execute_reply.started":"2022-02-18T11:17:50.291965Z","shell.execute_reply":"2022-02-18T11:17:52.194697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-18T11:17:52.197161Z","iopub.execute_input":"2022-02-18T11:17:52.197393Z","iopub.status.idle":"2022-02-18T11:17:52.203457Z","shell.execute_reply.started":"2022-02-18T11:17:52.197366Z","shell.execute_reply":"2022-02-18T11:17:52.202538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Libraries For Plot","metadata":{}},{"cell_type":"code","source":"import seaborn as sb\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-02-18T11:17:52.206462Z","iopub.execute_input":"2022-02-18T11:17:52.20679Z","iopub.status.idle":"2022-02-18T11:17:52.356734Z","shell.execute_reply.started":"2022-02-18T11:17:52.206757Z","shell.execute_reply":"2022-02-18T11:17:52.355929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsb.set_style('whitegrid')\nax = sb.countplot(x='discourse_type',data = df,palette = 'rainbow')\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.xlabel('Discourse Type')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T11:17:52.358196Z","iopub.execute_input":"2022-02-18T11:17:52.358436Z","iopub.status.idle":"2022-02-18T11:17:52.835557Z","shell.execute_reply.started":"2022-02-18T11:17:52.358408Z","shell.execute_reply":"2022-02-18T11:17:52.834672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Creating List And Array","metadata":{}},{"cell_type":"code","source":"train_texts = list(df.discourse_text)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T11:17:52.836896Z","iopub.execute_input":"2022-02-18T11:17:52.83712Z","iopub.status.idle":"2022-02-18T11:17:52.872178Z","shell.execute_reply.started":"2022-02-18T11:17:52.837093Z","shell.execute_reply":"2022-02-18T11:17:52.870996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels = np.array(df.discourse_type)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T11:17:52.873809Z","iopub.execute_input":"2022-02-18T11:17:52.874048Z","iopub.status.idle":"2022-02-18T11:17:52.883681Z","shell.execute_reply.started":"2022-02-18T11:17:52.87402Z","shell.execute_reply":"2022-02-18T11:17:52.882977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif","metadata":{"execution":{"iopub.status.busy":"2022-02-18T11:17:52.884759Z","iopub.execute_input":"2022-02-18T11:17:52.885325Z","iopub.status.idle":"2022-02-18T11:17:52.973522Z","shell.execute_reply.started":"2022-02-18T11:17:52.88529Z","shell.execute_reply":"2022-02-18T11:17:52.972798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use TF-IDF Vectorizer and SelectKBest","metadata":{}},{"cell_type":"code","source":"# n_gram range for tokenizing text\nngram_range = (1,2)\n\n# Limiting features\ntop_k = 20000\n\n# Whether text should be split into word or character n-grams.\ntoken_mode = 'word'\n\n# Document/corpus frequency below which a token will be discarded.\nMIN_df = 2\n\n# Arguments for tf-idf vectorizer.\nkwargs = {\n        'ngram_range': ngram_range, \n        'dtype': 'int32',\n        'strip_accents': 'unicode',\n        'decode_error': 'replace',\n        'analyzer': token_mode,  # Split text into word tokens.\n        'min_df': MIN_df,\n}\n\nvectorizer = TfidfVectorizer(**kwargs)\n\n# Learn vocabulary from training texts and vectorize training texts.\nx_train = vectorizer.fit_transform(train_texts)\n\n# Select top 'k' of the vectorized features.\nselector = SelectKBest(f_classif, k=min(top_k, x_train.shape[1]))\nselector.fit(x_train, train_labels)\nx_train = selector.transform(x_train).astype('float32')","metadata":{"execution":{"iopub.status.busy":"2022-02-18T11:17:52.974598Z","iopub.execute_input":"2022-02-18T11:17:52.975451Z","iopub.status.idle":"2022-02-18T11:18:13.297738Z","shell.execute_reply.started":"2022-02-18T11:17:52.975416Z","shell.execute_reply":"2022-02-18T11:18:13.297068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing LinearSVC","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\nmodel = LinearSVC()\n\nmodel.fit(x_train, train_labels)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T11:18:13.298964Z","iopub.execute_input":"2022-02-18T11:18:13.299307Z","iopub.status.idle":"2022-02-18T11:18:29.964357Z","shell.execute_reply.started":"2022-02-18T11:18:13.299273Z","shell.execute_reply":"2022-02-18T11:18:29.963426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create dict with each testing essay's text and id\ndef create_test_list():\n    total_list = []\n    \n    Test_Dict = \"../input/feedback-prize-2021/test\"\n    for filename in os.listdir(Test_Dict):\n        file_path = os.path.join(Test_Dict, filename)\n        # checking if it is a file\n        if os.path.isfile(file_path) and os.path.splitext(file_path)[1] == \".txt\":\n            with open(file_path) as f:\n                    total_list.append({\n                        'text' : f.read(), \n                        'id' : os.path.splitext(filename)[0]\n                    })\n    \n    return total_list","metadata":{"execution":{"iopub.status.busy":"2022-02-18T11:18:29.965636Z","iopub.execute_input":"2022-02-18T11:18:29.965903Z","iopub.status.idle":"2022-02-18T11:18:29.972525Z","shell.execute_reply.started":"2022-02-18T11:18:29.965873Z","shell.execute_reply":"2022-02-18T11:18:29.971584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_texts =  create_test_list()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T11:18:29.973928Z","iopub.execute_input":"2022-02-18T11:18:29.974163Z","iopub.status.idle":"2022-02-18T11:18:30.007504Z","shell.execute_reply.started":"2022-02-18T11:18:29.974135Z","shell.execute_reply":"2022-02-18T11:18:30.006769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_texts","metadata":{"execution":{"iopub.status.busy":"2022-02-18T11:18:30.010853Z","iopub.execute_input":"2022-02-18T11:18:30.011485Z","iopub.status.idle":"2022-02-18T11:18:30.018981Z","shell.execute_reply.started":"2022-02-18T11:18:30.011443Z","shell.execute_reply":"2022-02-18T11:18:30.017944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"pred_dicts_list = []\n\nfor test_text in test_texts:\n    \n    total_word_count = 0\n    \n    tokenized_sentences = nltk.sent_tokenize(test_text[\"text\"])\n    \n    x_test = vectorizer.transform(tokenized_sentences)\n    x_test = selector.transform(x_test).astype('float32')\n    preds = model.predict(x_test) #Returns list\n    \n    \n    for i, pred in enumerate(preds):\n        \n        # Generate prediction strings for each predicted discourse\n        tokenized_sentence = tokenized_sentences[i]\n        \n        if i == 0 or preds[i-1] != pred:\n            prediction_string = \"\"\n        \n        for x in range(total_word_count, total_word_count + len(tokenized_sentence.split())):\n            prediction_string += f\"{x} \"\n        \n        total_word_count += len(tokenized_sentence.split())\n        \n        try:\n            if preds[i+1] == pred:\n                continue\n        except:\n            pass\n        \n        pred_dicts_list.append({\n            \"id\" : test_text[\"id\"],\n            \"class\" : pred, \n            \"predictionstring\" : prediction_string.strip()\n        })","metadata":{"execution":{"iopub.status.busy":"2022-02-18T11:18:30.020331Z","iopub.execute_input":"2022-02-18T11:18:30.020973Z","iopub.status.idle":"2022-02-18T11:18:30.316287Z","shell.execute_reply.started":"2022-02-18T11:18:30.020928Z","shell.execute_reply":"2022-02-18T11:18:30.31572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"submit = pd.DataFrame(pred_dicts_list)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T11:18:30.317173Z","iopub.execute_input":"2022-02-18T11:18:30.317725Z","iopub.status.idle":"2022-02-18T11:18:30.322301Z","shell.execute_reply.started":"2022-02-18T11:18:30.317682Z","shell.execute_reply":"2022-02-18T11:18:30.321755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T11:18:30.323346Z","iopub.execute_input":"2022-02-18T11:18:30.323986Z","iopub.status.idle":"2022-02-18T11:18:30.336919Z","shell.execute_reply.started":"2022-02-18T11:18:30.323952Z","shell.execute_reply":"2022-02-18T11:18:30.336145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}