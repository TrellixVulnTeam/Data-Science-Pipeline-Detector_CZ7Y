{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\ngc.enable()\n\nimport sys\nsys.path.append(\"../input/tez-lib/\")\n\nimport os\nfrom os.path import exists\n\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport tez\nimport torch\nimport torch.nn as nn\nfrom joblib import Parallel, delayed\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\n%env TOKENIZERS_PARALLELISM=true","metadata":{"_cell_guid":"e0e4bdf6-bcd9-4f82-a53b-9c5f685500a8","_uuid":"66c995cf-7cbb-421b-bdd3-24906f5fb886","jupyter":{"outputs_hidden":false},"collapsed":false,"execution":{"iopub.status.busy":"2022-03-14T03:57:47.31893Z","iopub.execute_input":"2022-03-14T03:57:47.319221Z","iopub.status.idle":"2022-03-14T03:57:47.330943Z","shell.execute_reply.started":"2022-03-14T03:57:47.319185Z","shell.execute_reply":"2022-03-14T03:57:47.32999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_oof(name, oof):\n    f_name = f\"{name}\"\n    print(f\"Saving OOF {name}\")\n    with open(f_name, \"wb\") as f:\n        pickle.dump(oof, f)\n    \ndef load_oof(name):\n    f_name = f\"{name}\"\n    print(f\"Loading OOF {name}\")\n    with open(f_name, \"rb\") as input_file:\n        return pickle.load(input_file)","metadata":{"execution":{"iopub.status.busy":"2022-03-14T03:57:47.332707Z","iopub.execute_input":"2022-03-14T03:57:47.333315Z","iopub.status.idle":"2022-03-14T03:57:47.340262Z","shell.execute_reply.started":"2022-03-14T03:57:47.333255Z","shell.execute_reply":"2022-03-14T03:57:47.339536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import functools\nimport datetime\n\ndef _Try():\n    def deco(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except Exception as e:\n                 print('{} {} {} {} {}\\n'.format(datetime.datetime.now(), \n                                                        type(e), e, args, kwargs)) \n        return wrapper\n    return deco","metadata":{"execution":{"iopub.status.busy":"2022-03-14T03:57:47.341613Z","iopub.execute_input":"2022-03-14T03:57:47.342078Z","iopub.status.idle":"2022-03-14T03:57:47.349838Z","shell.execute_reply.started":"2022-03-14T03:57:47.342041Z","shell.execute_reply":"2022-03-14T03:57:47.34905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_id_map = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 2,\n    \"I-Position\": 3,\n    \"B-Evidence\": 4,\n    \"I-Evidence\": 5,\n    \"B-Claim\": 6,\n    \"I-Claim\": 7,\n    \"B-Concluding Statement\": 8,\n    \"I-Concluding Statement\": 9,\n    \"B-Counterclaim\": 10,\n    \"I-Counterclaim\": 11,\n    \"B-Rebuttal\": 12,\n    \"I-Rebuttal\": 13,\n    \"O\": 14,\n    \"PAD\": -100,\n}\n\n\nid_target_map = {v: k for k, v in target_id_map.items()}\n\n\n\n\nclass args1:\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/longformerlarge4096/longformer-large-4096/\"\n    tez_model= \"../input/fblongformerlarge1536/\"\n    output = \".\"\n    batch_size = 8\n    max_len = 1600\n    prefix = \"model_\"\n    weights_only = True\n    folds_use = [0,1,2,3,4]\nclass args2:\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/longformerlarge4096/longformer-large-4096/\"\n    tez_model= \"../input/tez-fb-large/\"\n    output = \".\"\n    batch_size = 8\n    max_len = 1600\n    prefix = \"model_\"\n    weights_only = True    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-14T03:57:47.352731Z","iopub.execute_input":"2022-03-14T03:57:47.353164Z","iopub.status.idle":"2022-03-14T03:57:47.364262Z","shell.execute_reply.started":"2022-03-14T03:57:47.353126Z","shell.execute_reply":"2022-03-14T03:57:47.363445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackDataset:\n    def __init__(self, samples, max_len, tokenizer):\n        self.samples = samples\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.length = len(samples)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        input_ids = self.samples[idx][\"input_ids\"]\n        # print(input_ids)\n        # print(input_labels)\n\n        # add start token id to the input_ids\n        input_ids = [self.tokenizer.cls_token_id] + input_ids\n\n        if len(input_ids) > self.max_len - 1:\n            input_ids = input_ids[: self.max_len - 1]\n\n        # add end token id to the input_ids\n        input_ids = input_ids + [self.tokenizer.sep_token_id]\n        attention_mask = [1] * len(input_ids)\n\n        # padding_length = self.max_len - len(input_ids)\n        # if padding_length > 0:\n        #     if self.tokenizer.padding_side == \"right\":\n        #         input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n        #         attention_mask = attention_mask + [0] * padding_length\n        #     else:\n        #         input_ids = [self.tokenizer.pad_token_id] * padding_length + input_ids\n        #         attention_mask = [0] * padding_length + attention_mask\n\n        # return {\n        #     \"ids\": torch.tensor(input_ids, dtype=torch.long),\n        #     \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n        # }\n\n        return {\n            \"ids\": input_ids,\n            \"mask\": attention_mask,\n        }","metadata":{"execution":{"iopub.status.busy":"2022-03-14T03:57:47.365836Z","iopub.execute_input":"2022-03-14T03:57:47.366088Z","iopub.status.idle":"2022-03-14T03:57:47.376787Z","shell.execute_reply.started":"2022-03-14T03:57:47.366054Z","shell.execute_reply":"2022-03-14T03:57:47.37558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Collate:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"ids\"]])\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n        else:\n            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n\n        # convert to tensors\n        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-03-14T03:57:47.377992Z","iopub.execute_input":"2022-03-14T03:57:47.378683Z","iopub.status.idle":"2022-03-14T03:57:47.392489Z","shell.execute_reply.started":"2022-03-14T03:57:47.378646Z","shell.execute_reply":"2022-03-14T03:57:47.391821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackModel(tez.Model):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n        config = AutoConfig.from_pretrained(model_name)\n\n        hidden_dropout_prob: float = 0.1\n        layer_norm_eps: float = 1e-7\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = AutoModel.from_config(config)\n        self.output = nn.Linear(config.hidden_size, self.num_labels)\n\n    def forward(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        logits = self.output(sequence_output)\n        logits = torch.softmax(logits, dim=-1)\n        return logits, 0, {}","metadata":{"execution":{"iopub.status.busy":"2022-03-14T03:57:47.393673Z","iopub.execute_input":"2022-03-14T03:57:47.394067Z","iopub.status.idle":"2022-03-14T03:57:47.405971Z","shell.execute_reply.started":"2022-03-14T03:57:47.394031Z","shell.execute_reply":"2022-03-14T03:57:47.405226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _prepare_test_data_helper(args, tokenizer, ids):\n    test_samples = []\n    for idx in ids:\n        filename = os.path.join(args.input_path, \"test\", idx + \".txt\")\n        with open(filename, \"r\") as f:\n            text = f.read()\n\n        encoded_text = tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        offset_mapping = encoded_text[\"offset_mapping\"]\n\n        sample = {\n            \"id\": idx,\n            \"input_ids\": input_ids,\n            \"text\": text,\n            \"offset_mapping\": offset_mapping,\n        }\n\n        test_samples.append(sample)\n    return test_samples\n\n\ndef prepare_test_data(df, tokenizer, args):\n    test_samples = []\n    ids = df[\"id\"].unique()\n    ids_splits = np.array_split(ids, 4)\n\n    results = Parallel(n_jobs=4, backend=\"multiprocessing\")(\n        delayed(_prepare_test_data_helper)(args, tokenizer, idx) for idx in ids_splits\n    )\n    for result in results:\n        test_samples.extend(result)\n\n    return test_samples","metadata":{"execution":{"iopub.status.busy":"2022-03-14T03:57:47.408444Z","iopub.execute_input":"2022-03-14T03:57:47.408705Z","iopub.status.idle":"2022-03-14T03:57:47.41782Z","shell.execute_reply.started":"2022-03-14T03:57:47.408674Z","shell.execute_reply":"2022-03-14T03:57:47.417101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tqdm\nimport copy\ndef _prepare_training_data_helper(args, tokenizer,  train_ids, df):\n    training_samples = []\n    for idx in train_ids:\n        filename = os.path.join(args.input_path, \"train\", idx + \".txt\")\n        with open(filename, \"r\") as f:\n            text = f.read()\n\n        encoded_text = tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        input_labels = copy.deepcopy(input_ids)\n        offset_mapping = encoded_text[\"offset_mapping\"]\n\n        for k in range(len(input_labels)):\n            input_labels[k] = \"O\"\n\n        sample = {\n            \"id\": idx,\n            \"input_ids\": input_ids,\n            \"text\": text,\n            \"offset_mapping\": offset_mapping,\n        }\n\n        temp_df = df[df[\"id\"] == idx]\n        for _, row in temp_df.iterrows():\n            text_labels = [0] * len(text)\n            discourse_start = int(row[\"discourse_start\"])\n            discourse_end = int(row[\"discourse_end\"])\n            prediction_label = row[\"discourse_type\"]\n            text_labels[discourse_start:discourse_end] = [1] * (discourse_end - discourse_start)\n            target_idx = []\n            for map_idx, (offset1, offset2) in enumerate(encoded_text[\"offset_mapping\"]):\n                if sum(text_labels[offset1:offset2]) > 0:\n                    if len(text[offset1:offset2].split()) > 0:\n                        target_idx.append(map_idx)\n\n            targets_start = target_idx[0]\n            targets_end = target_idx[-1]\n            pred_start = \"B-\" + prediction_label\n            pred_end = \"I-\" + prediction_label\n            input_labels[targets_start] = pred_start\n            input_labels[targets_start + 1 : targets_end + 1] = [pred_end] * (targets_end - targets_start)\n\n        sample[\"input_ids\"] = input_ids\n        sample[\"input_labels\"] = input_labels\n        training_samples.append(sample)\n    return training_samples\n\n\ndef prepare_training_data(df, tokenizer, args, num_jobs=4):\n    training_samples = []\n    train_ids = df[\"id\"].unique()\n\n    train_ids_splits = np.array_split(train_ids, num_jobs)\n\n    results = Parallel(n_jobs=num_jobs, backend=\"multiprocessing\")(\n        delayed(_prepare_training_data_helper)(args, tokenizer,  idx, df) for idx in train_ids_splits\n    )\n    for result in results:\n        training_samples.extend(result)\n\n    return training_samples","metadata":{"execution":{"iopub.status.busy":"2022-03-14T03:57:47.419238Z","iopub.execute_input":"2022-03-14T03:57:47.419499Z","iopub.status.idle":"2022-03-14T03:57:47.435205Z","shell.execute_reply.started":"2022-03-14T03:57:47.419468Z","shell.execute_reply":"2022-03-14T03:57:47.434562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackDatasetValid:\n    def __init__(self, samples, max_len, tokenizer):\n        self.samples = samples\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.length = len(samples)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        input_ids = self.samples[idx][\"input_ids\"]\n        input_ids = [self.tokenizer.cls_token_id] + input_ids\n\n        if len(input_ids) > self.max_len - 1:\n            input_ids = input_ids[: self.max_len - 1]\n\n        # add end token id to the input_ids\n        input_ids = input_ids + [self.tokenizer.sep_token_id]\n        attention_mask = [1] * len(input_ids)\n\n        return {\n            \"ids\": input_ids,\n            \"mask\": attention_mask,\n        }","metadata":{"execution":{"iopub.status.busy":"2022-03-14T03:57:47.438184Z","iopub.execute_input":"2022-03-14T03:57:47.438465Z","iopub.status.idle":"2022-03-14T03:57:47.447473Z","shell.execute_reply.started":"2022-03-14T03:57:47.438431Z","shell.execute_reply":"2022-03-14T03:57:47.446736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(\" \"))\n    set_gt = set(row.predictionstring_gt.split(\" \"))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter / len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp_micro(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n\n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = (\n        gt_df[[\"id\", \"discourse_type\", \"predictionstring\"]]\n        .reset_index(drop=True)\n        .copy()\n    )\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    pred_df[\"pred_id\"] = pred_df.index\n    gt_df[\"gt_id\"] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(\n        gt_df,\n        left_on=[\"id\", \"class\"],\n        right_on=[\"id\", \"discourse_type\"],\n        how=\"outer\",\n        suffixes=(\"_pred\", \"_gt\"),\n    )\n    joined[\"predictionstring_gt\"] = joined[\"predictionstring_gt\"].fillna(\" \")\n    joined[\"predictionstring_pred\"] = joined[\"predictionstring_pred\"].fillna(\" \")\n\n    joined[\"overlaps\"] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined[\"overlap1\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[0])\n    joined[\"overlap2\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[1])\n\n    joined[\"potential_TP\"] = (joined[\"overlap1\"] >= 0.5) & (joined[\"overlap2\"] >= 0.5)\n    joined[\"max_overlap\"] = joined[[\"overlap1\", \"overlap2\"]].max(axis=1)\n    tp_pred_ids = (\n        joined.query(\"potential_TP\")\n        .sort_values(\"max_overlap\", ascending=False)\n        .groupby([\"id\", \"predictionstring_gt\"])\n        .first()[\"pred_id\"]\n        .values\n    )\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined[\"pred_id\"].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query(\"potential_TP\")[\"gt_id\"].unique()\n    unmatched_gt_ids = [c for c in joined[\"gt_id\"].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    # calc microf1\n    my_f1_score = TP / (TP + 0.5 * (FP + FN))\n    return my_f1_score\n\n\ndef score_feedback_comp(pred_df, gt_df, return_class_scores=False):\n    class_scores = {}\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    for discourse_type, gt_subset in gt_df.groupby(\"discourse_type\"):\n        pred_subset = (\n            pred_df.loc[pred_df[\"class\"] == discourse_type]\n            .reset_index(drop=True)\n            .copy()\n        )\n        class_score = score_feedback_comp_micro(pred_subset, gt_subset)\n        class_scores[discourse_type] = class_score\n    f1 = np.mean([v for v in class_scores.values()])\n    if return_class_scores:\n        return f1, class_scores\n    return f1","metadata":{"execution":{"iopub.status.busy":"2022-03-14T03:57:47.448718Z","iopub.execute_input":"2022-03-14T03:57:47.449141Z","iopub.status.idle":"2022-03-14T03:57:47.467761Z","shell.execute_reply.started":"2022-03-14T03:57:47.449105Z","shell.execute_reply":"2022-03-14T03:57:47.467019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"proba_thresh = {\n    \"Lead\": 0.7,\n    \"Position\": 0.55,\n    \"Evidence\": 0.65,\n    \"Claim\": 0.55,\n    \"Concluding Statement\": 0.7,\n    \"Counterclaim\": 0.5,\n    \"Rebuttal\": 0.55,\n}\n\nmin_thresh = {\n    \"Lead\": 9,\n    \"Position\": 5,\n    \"Evidence\": 14,\n    \"Claim\": 3,\n    \"Concluding Statement\": 11,\n    \"Counterclaim\": 6,\n    \"Rebuttal\": 4,\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-14T03:57:47.469079Z","iopub.execute_input":"2022-03-14T03:57:47.469563Z","iopub.status.idle":"2022-03-14T03:57:47.4779Z","shell.execute_reply.started":"2022-03-14T03:57:47.469527Z","shell.execute_reply":"2022-03-14T03:57:47.477188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jn(pst, start, end):\n    return \" \".join([str(x) for x in pst[start:end]])\n\n\ndef link_evidence(oof):\n    thresh = 1\n    idu = oof['id'].unique()\n    idc = idu[1]\n    eoof = oof[oof['class'] == \"Evidence\"]\n    neoof = oof[oof['class'] != \"Evidence\"]\n    for thresh2 in range(26,27, 1):\n        retval = []\n        for idv in idu:\n            for c in  ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n                   'Counterclaim', 'Rebuttal']:\n                q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n                if len(q) == 0:\n                    continue\n                pst = []\n                for i,r in q.iterrows():\n                    pst = pst +[-1] + [int(x) for x in r['predictionstring'].split()]\n                start = 1\n                end = 1\n                for i in range(2,len(pst)):\n                    cur = pst[i]\n                    end = i\n                    #if pst[start] == 205:\n                    #   print(cur, pst[start], cur - pst[start])\n                    if (cur == -1 and c != 'Evidence') or ((cur == -1) and ((pst[i+1] > pst[end-1] + thresh) or (pst[i+1] - pst[start] > thresh2))):\n                        retval.append((idv, c, jn(pst, start, end)))\n                        start = i + 1\n                v = (idv, c, jn(pst, start, end+1))\n                #print(v)\n                retval.append(v)\n        roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring']) \n        roof = roof.merge(neoof, how='outer')\n        return roof","metadata":{"execution":{"iopub.status.busy":"2022-03-14T03:57:47.481013Z","iopub.execute_input":"2022-03-14T03:57:47.481438Z","iopub.status.idle":"2022-03-14T03:57:47.492944Z","shell.execute_reply.started":"2022-03-14T03:57:47.48141Z","shell.execute_reply":"2022-03-14T03:57:47.492141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"map_clip = {'Lead':9, 'Position':5, 'Evidence':14, 'Claim':3, 'Concluding Statement':11,\n             'Counterclaim':6, 'Rebuttal':4}\ndef threshold(df):\n    df = df.copy()\n    for key, value in map_clip.items():\n    # if df.loc[df['class']==key,'len'] < value \n        index = df.loc[df['class']==key].query(f'len<{value}').index\n        df.drop(index, inplace = True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-03-14T03:57:47.494602Z","iopub.execute_input":"2022-03-14T03:57:47.495417Z","iopub.status.idle":"2022-03-14T03:57:47.503043Z","shell.execute_reply.started":"2022-03-14T03:57:47.495381Z","shell.execute_reply":"2022-03-14T03:57:47.50231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change to ur dataset\nvdf=pd.read_csv(\"../input/creating-folds-properly-hopefully-p/train_folds.csv\")\n#vdf = pd.read_csv(\"../input/feedback-folds-generator/train_folds_10.csv\")\nprint(f\"Train  size {len(vdf)}\")\ntokenizer = AutoTokenizer.from_pretrained(args1.model)\ncollate = Collate(tokenizer=tokenizer)\n\n# On old \nnp.random.seed(42)\nIDS = vdf.id.unique()\ntrain_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\nvalid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\n\n# Doing CV (Validation)\nraw_preds = []\nno_of_models = 10\nFOLDS = 5\n\ndef get_fold_based_cv():\n    for fold_ in range(no_of_models):\n        current_idx = 0\n        model_idx = int(fold_/FOLDS+1)\n        modelArgs = globals()[f\"args{model_idx}\"]\n        fold = fold_%FOLDS\n        preds_file = f\"oof_{modelArgs.prefix}{fold}.pickle\"\n        \n        print(f\"Using {model_idx} Model {modelArgs.tez_model} fold {fold}\" )\n        tokenizer = AutoTokenizer.from_pretrained(modelArgs.model)\n        collate = Collate(tokenizer=tokenizer)\n        valid_df = vdf[vdf[\"kfold\"] == fold].reset_index(drop=True)\n        print(valid_df.shape)\n\n        valid_samples = prepare_training_data(valid_df, tokenizer, args1)\n        valid_dataset = FeedbackDatasetValid(valid_samples, modelArgs.max_len, tokenizer)\n        model = FeedbackModel(model_name=modelArgs.model, num_labels=len(target_id_map) - 1)\n        \n        \n        if exists(preds_file):\n            preds_iter = load_oof(preds_file)\n        else:    \n            model.load(os.path.join(modelArgs.tez_model, f\"{modelArgs.prefix}{fold}.bin\"), weights_only=modelArgs.weights_only)\n            preds_iter = model.predict(valid_dataset, batch_size=modelArgs.batch_size, n_jobs=-1, collate_fn=collate)\n        \n        current_idx = 0\n        final_preds = []\n        final_scores = []\n        raw_preds_oof = []\n        for preds in preds_iter:\n            raw_preds_oof.append(preds)\n            pred_class = np.argmax(preds, axis=2)\n            pred_scrs = np.max(preds, axis=2)\n            for pred, pred_scr in zip(pred_class, pred_scrs):\n                final_preds.append(pred.tolist())\n                final_scores.append(pred_scr.tolist())\n                \n        # Save Raw preds for reuse \n        \"\" if exists(preds_file) else save_oof(preds_file, raw_preds_oof)\n        \n        for j in range(len(valid_samples)):\n            tt = [id_target_map[p] for p in final_preds[j][1:]]\n            tt_score = final_scores[j][1:]\n            valid_samples[j][\"preds\"] = tt\n            valid_samples[j][\"pred_scores\"] = tt_score\n\n        submission = []\n\n\n        for _, sample in enumerate(valid_samples):\n            preds = sample[\"preds\"]\n            offset_mapping = sample[\"offset_mapping\"]\n            sample_id = sample[\"id\"]\n            sample_text = sample[\"text\"]\n            sample_pred_scores = sample[\"pred_scores\"]\n\n            # pad preds to same length as offset_mapping\n            if len(preds) < len(offset_mapping):\n                preds = preds + [\"O\"] * (len(offset_mapping) - len(preds))\n                sample_pred_scores = sample_pred_scores + [0] * (len(offset_mapping) - len(sample_pred_scores))\n\n            idx = 0\n            phrase_preds = []\n            while idx < len(offset_mapping):\n                start, _ = offset_mapping[idx]\n                if preds[idx] != \"O\":\n                    label = preds[idx][2:]\n                else:\n                    label = \"O\"\n                phrase_scores = []\n                phrase_scores.append(sample_pred_scores[idx])\n                idx += 1\n                while idx < len(offset_mapping):\n                    if label == \"O\":\n                        matching_label = \"O\"\n                    else:\n                        matching_label = f\"I-{label}\"\n                    if preds[idx] == matching_label:\n                        _, end = offset_mapping[idx]\n                        phrase_scores.append(sample_pred_scores[idx])\n                        idx += 1\n                    else:\n                        break\n                if \"end\" in locals():\n                    phrase = sample_text[start:end]\n                    phrase_preds.append((phrase, start, end, label, phrase_scores))\n\n            temp_df = []\n            for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n                word_start = len(sample_text[:start].split())\n                word_end = word_start + len(sample_text[start:end].split())\n                word_end = min(word_end, len(sample_text.split()))\n                ps = \" \".join([str(x) for x in range(word_start, word_end)])\n                if label != \"O\":\n                    if sum(phrase_scores) / len(phrase_scores) >= proba_thresh[label]:\n                        temp_df.append((sample_id, label, ps))\n\n            temp_df = pd.DataFrame(temp_df, columns=[\"id\", \"class\", \"predictionstring\"])\n\n            submission.append(temp_df)\n\n        submission = pd.concat(submission).reset_index(drop=True)\n        submission[\"len\"] = submission.predictionstring.apply(lambda x: len(x.split()))\n        submission = threshold(submission)\n        # drop len\n        submission = submission.drop(columns=[\"len\"])\n        scr = score_feedback_comp(submission,valid_df, return_class_scores=True)\n        print(f\"Model {fold_} {scr}\")\n        torch.cuda.empty_cache()\n        del valid_df\n        del valid_samples\n        del valid_dataset\n        gc.collect()\n    \n\nget_fold_based_cv()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-14T03:57:47.504586Z","iopub.execute_input":"2022-03-14T03:57:47.505066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scoring Ensemble \n\ndef get_score(raw_preds,valid_samples):\n    final_preds = []\n    final_scores = []\n    for preds in raw_preds:\n        pred_class = np.argmax(preds, axis=2)\n        pred_scrs = np.max(preds, axis=2)\n        for pred, pred_scr in zip(pred_class, pred_scrs):\n            final_preds.append(pred.tolist())\n            final_scores.append(pred_scr.tolist())\n\n    for j in range(len(valid_samples)):\n        tt = [id_target_map[p] for p in final_preds[j][1:]]\n        tt_score = final_scores[j][1:]\n        valid_samples[j][\"preds\"] = tt\n        valid_samples[j][\"pred_scores\"] = tt_score\n\n    submission = []\n\n\n    for _, sample in enumerate(valid_samples):\n        preds = sample[\"preds\"]\n        offset_mapping = sample[\"offset_mapping\"]\n        sample_id = sample[\"id\"]\n        sample_text = sample[\"text\"]\n        sample_pred_scores = sample[\"pred_scores\"]\n\n        # pad preds to same length as offset_mapping\n        if len(preds) < len(offset_mapping):\n            preds = preds + [\"O\"] * (len(offset_mapping) - len(preds))\n            sample_pred_scores = sample_pred_scores + [0] * (len(offset_mapping) - len(sample_pred_scores))\n\n        idx = 0\n        phrase_preds = []\n        while idx < len(offset_mapping):\n            start, _ = offset_mapping[idx]\n            if preds[idx] != \"O\":\n                label = preds[idx][2:]\n            else:\n                label = \"O\"\n            phrase_scores = []\n            phrase_scores.append(sample_pred_scores[idx])\n            idx += 1\n            while idx < len(offset_mapping):\n                if label == \"O\":\n                    matching_label = \"O\"\n                else:\n                    matching_label = f\"I-{label}\"\n                if preds[idx] == matching_label:\n                    _, end = offset_mapping[idx]\n                    phrase_scores.append(sample_pred_scores[idx])\n                    idx += 1\n                else:\n                    break\n            if \"end\" in locals():\n                phrase = sample_text[start:end]\n                phrase_preds.append((phrase, start, end, label, phrase_scores))\n\n        temp_df = []\n        for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n            word_start = len(sample_text[:start].split())\n            word_end = word_start + len(sample_text[start:end].split())\n            word_end = min(word_end, len(sample_text.split()))\n            ps = \" \".join([str(x) for x in range(word_start, word_end)])\n            if label != \"O\":\n                if sum(phrase_scores) / len(phrase_scores) >= proba_thresh[label]:\n                    temp_df.append((sample_id, label, ps))\n\n        temp_df = pd.DataFrame(temp_df, columns=[\"id\", \"class\", \"predictionstring\"])\n\n        submission.append(temp_df)\n\n    submission = pd.concat(submission).reset_index(drop=True)\n    submission[\"len\"] = submission.predictionstring.apply(lambda x: len(x.split()))\n    submission = threshold(submission)\n    # drop len\n    submission = submission.drop(columns=[\"len\"])\n\n    scr = score_feedback_comp(submission,valid_df, return_class_scores=True)\n    print(f\"Model Ensemble {scr}\")\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_DebertaV3TokenizerFast(model_name):        \n    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n    tokenizer = DebertaV2TokenizerFast.from_pretrained(model_name)\n    return tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nvdf=pd.read_csv(\"../input/creating-folds-properly-hopefully-p/train_folds.csv\")\n\n# Currently mixing deberta v3 and large \n#tokenizer = get_DebertaV3TokenizerFast(args2.model)\nprint(args1.model)\n#from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n#tokenizer = DebertaV2TokenizerFast.from_pretrained(args1.model)\ntokenizer = AutoTokenizer.from_pretrained(args1.model)\ncollate = Collate(tokenizer=tokenizer)\nno_of_models = 2\nFOLDS = 1\ntest_samples_m1 = None\nvalid_data = []\n\n#@_Try()\ndef inference(no_of_models):\n    raw_preds = []\n    test_samples = None\n    for fold_ in range(no_of_models):\n        current_idx = 0\n        model_idx = int(fold_/FOLDS+1)\n        fold = fold_%FOLDS\n        modelArgs = globals()[f\"args{model_idx}\"]\n        preds_file = f\"oof_{modelArgs.prefix}{fold}.pickle\"\n        \n        print(f\"Using [{model_idx}] Model [{modelArgs.tez_model}/{modelArgs.prefix}{fold}.bin] fold [{fold}]\" )\n        tokenizer = AutoTokenizer.from_pretrained(modelArgs.model)\n        collate = Collate(tokenizer=tokenizer)\n        df = vdf[vdf[\"kfold\"] == 0].reset_index(drop=True)\n        test_samples = prepare_training_data(df, tokenizer, modelArgs)\n        if model_idx == 1:\n            valid_data.append(df)\n            test_samples_m1 = test_samples\n        test_dataset = FeedbackDataset(test_samples, modelArgs.max_len, tokenizer)\n        model = FeedbackModel(model_name=modelArgs.model, num_labels=len(target_id_map) - 1)\n        \n        # If oof saved used them \n        if exists(preds_file):\n            preds_iter = load_oof(preds_file)\n        else:    \n            model.load(os.path.join(modelArgs.tez_model, f\"{modelArgs.prefix}{fold}.bin\"), weights_only=modelArgs.weights_only)\n            preds_iter = model.predict(valid_dataset, batch_size=modelArgs.batch_size, n_jobs=-1, collate_fn=collate)\n            \n        #model.load(os.path.join(modelArgs.tez_model, f\"{modelArgs.prefix}{fold}.bin\"), weights_only=modelArgs.weights_only)\n        #preds_iter = model.predict(test_dataset, batch_size=modelArgs.batch_size, n_jobs=-1, collate_fn=collate)    \n        \n        current_idx = 0 \n        for preds in preds_iter:\n            preds = preds.astype(np.float16)\n            preds = preds / no_of_models\n            if fold_ == 0:\n                raw_preds.append(preds)\n            else:\n                raw_preds[current_idx] += preds\n                current_idx += 1\n        \n        valid_df = df        \n        get_score(raw_preds,test_samples)       \n        \n        torch.cuda.empty_cache()\n        del test_dataset\n        del model\n        gc.collect()\n\n    return raw_preds, test_samples\n\nraw_preds, valid_samples = inference(no_of_models)\n# Scoring\nvalid_df =  pd.concat(valid_data).reset_index(drop=True)\n#valid_df = vdf[vdf[\"kfold\"] == 1].reset_index(drop=True) \n#collate = Collate(tokenizer=tokenizer)\n#valid_samples = prepare_training_data(valid_df, tokenizer, args1) \nget_score(raw_preds,valid_samples)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport optuna\noptuna.logging.set_verbosity(optuna.logging.ERROR)\n# Initialize Constants\nstudy_name = \"VAL_BLEND\"\nmodels = MODELS\nrand_seed = 2021\nn_trials = 1000\nrun_submit_script = True\n\ndef objective(trial):\n    weights = []\n    for i in range(len(models)):\n        weights.append(trial.suggest_uniform(f\"w{i}\", 0.144, 1.0))\n    w = [weights[i]/sum(weights) for i in range(len(models))]   \n    #print(w)\n    temp_df = m1_df.copy()    \n    score = np.zeros(m1_df['worker'].to_numpy().shape)\n    for i,m in enumerate(MODELS):\n        score += w[i] * m1_df[f'score_{m}'].to_numpy()\n    temp_df['score'] = score\n    bad_df, loss =pack_and_validate(temp_df)\n    #print(f\"Score {loss}\")\n    return loss\n\ndef optimize_weights():\n    pruner = optuna.pruners.MedianPruner(\n        n_startup_trials=5,\n        n_warmup_steps=0,\n        interval_steps=1,\n    )\n    #pruner=optuna.pruners.NopPruner()\n    sampler = optuna.samplers.TPESampler(seed=rand_seed)\n    study = optuna.create_study(direction=\"maximize\",\n                                pruner=pruner,\n                                sampler=sampler,\n                                study_name=study_name,\n                                load_if_exists=True)\n\n    study.optimize(objective,\n                   n_trials=n_trials,\n                   timeout=None,\n                   gc_after_trial=True,\n                   n_jobs=-1,\n                   show_progress_bar=True)\n\n    trial = study.best_trial\n\n\n    print(\"\\n[Optuna]\")\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    print(\"Best trial:\")\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value)) \n    w = []\n    for i in range(len(MODELS)):\n        w.append(trial.params[f\"w{i}\"])\n    return w\n\n#w = optimize_weights() \n#print(f\"Weights to Use {w} \\n Total {len(w)} \\n Weight Sum {sum(w)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Funnel Models\nclass args1:\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/funnel-transformer-large\"\n    tez_model= \"../input/feedback-prize-submit/funnel-transformer-large-i/funnel-transformer-large-i\"\n    output = \".\"\n    batch_size = 8\n    max_len = 1600\n    prefix = \"model_funnel\"\n    weights_only = True\n    folds_use = [0]\n    \nclass args2:\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/funneltransformermedium\"\n    tez_model= \"../input/feedback-prize-submit/funnel-transformer-medium/funnel-transformer-medium\"\n    output = \".\"\n    batch_size = 8\n    max_len = 1600\n    prefix = \"model_medium\"\n    weights_only = True\n    folds_use = [2]\n    \nclass args3:\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/funneltransformermedium\"\n    tez_model= \"../input/feedback-fn-bs\"\n    output = \".\"\n    batch_size = 8\n    max_len = 1600\n    prefix = \"model_\"\n    weights_only = False\n    folds_use = [1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fold mixup Inference \n#vdf=pd.read_csv(\"../input/creating-folds-properly-hopefully-p/train_folds.csv\")\nvdf = pd.read_csv(\"../input/feedback-prize-submit/train_10-folds.csv/train_10-folds.csv\")\nvdf = pd.read_csv(\"../input/feedback-folds-generator/train_folds_10.csv\")\n\nno_of_models = 3\nFOLDS = 1\ntest_samples_arr = []\nvalid_data = []\n\n@_Try()\ndef inference_fold_mix(raw_preds,fold = 0):\n    raw_preds = []\n    test_samples = None\n    total_models = sum([len(globals()[f\"args{i+1}\"].folds_use) for i in range(0,no_of_models)])\n    print(f\"Total Models to infer {total_models}\")\n    \n    for fold_ in range(0,no_of_models):\n        current_idx = 0\n        modelArgs = globals()[f\"args{model_idx+1}\"]\n        \n        print(f\"Using [{model_idx}] Model [{modelArgs.tez_model}/{modelArgs.prefix}{fold}.bin] fold [{fold}]\" )\n        tokenizer = AutoTokenizer.from_pretrained(modelArgs.model)\n        collate = Collate(tokenizer=tokenizer)\n        df = vdf[vdf[\"kfold\"] == fold].reset_index(drop=True)\n        if model_idx == 1:\n            valid_data.append(df)\n        test_samples = prepare_training_data(df, tokenizer, modelArgs)\n        test_dataset = FeedbackDataset(test_samples, modelArgs.max_len, tokenizer)\n        \n        for fold in modelArgs.folds_use:\n            print(f\"Inferring {model_idx} Model {modelArgs.tez_model} fold {fold}\" )\n            model = FeedbackModel(model_name=modelArgs.model, num_labels=len(target_id_map) - 1)\n            model.load(os.path.join(modelArgs.tez_model, f\"{modelArgs.prefix}{fold}.bin\"), weights_only=modelArgs.weights_only)\n            preds_iter = model.predict(test_dataset, batch_size=modelArgs.batch_size, n_jobs=-1, collate_fn=collate)    \n            current_idx = 0 \n            for preds in preds_iter:\n                preds = preds.astype(np.float16)\n                preds = preds / total_models\n                if fold_ == 0:\n                    raw_preds.append(preds)\n                else:\n                    raw_preds[current_idx] += preds\n                    current_idx += 1\n            torch.cuda.empty_cache()\n            del model\n            gc.collect()\n        del test_dataset       \n        gc.collect()\n\n    return raw_preds, test_samples\n\nraw_preds, valid_samples = inference_fold_mix(no_of_models)\n\nvalid_df = pd.concat(valid_data).reset_index(drop=True)\ntokenizer = AutoTokenizer.from_pretrained(args1.model)\ncollate = Collate(tokenizer=tokenizer)\nvalid_samples = prepare_training_data(valid_df, tokenizer, args1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef test_inference():\n    df = pd.read_csv(os.path.join(\"../input/feedback-prize-2021/\", \"sample_submission.csv\"))\n    df_ids = df[\"id\"].unique()\n    test_samples = prepare_test_data(df, tokenizer, args1)\n    train_df = pd.read_csv('../input/feedback-prize-2021/train.csv')\n\n    raw_preds = []\n    for fold_ in range(10):\n        current_idx = 0\n        test_dataset = FeedbackDataset(test_samples, args1.max_len, tokenizer)\n\n        if fold_ < 5:\n            model = FeedbackModel(model_name=args1.model, num_labels=len(target_id_map) - 1)\n            model.load(os.path.join(args1.tez_model, f\"model_{fold_}.bin\"), weights_only=True)\n            preds_iter = model.predict(test_dataset, batch_size=args1.batch_size, n_jobs=-1, collate_fn=collate)\n        else:\n            model = FeedbackModel(model_name=args2.model, num_labels=len(target_id_map) - 1)\n            model.load(os.path.join(args2.tez_model, f\"model_{fold_-5}.bin\"), weights_only=True)\n            preds_iter = model.predict(test_dataset, batch_size=args2.batch_size, n_jobs=-1, collate_fn=collate)\n\n        current_idx = 0\n\n        for preds in preds_iter:\n            preds = preds.astype(np.float16)\n            preds = preds / 10\n            if fold_ == 0:\n                raw_preds.append(preds)\n            else:\n                raw_preds[current_idx] += preds\n                current_idx += 1\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    final_preds = []\n    final_scores = []\n\n    for rp in raw_preds:\n        pred_class = np.argmax(rp, axis=2)\n        pred_scrs = np.max(rp, axis=2)\n        for pred, pred_scr in zip(pred_class, pred_scrs):\n            pred = pred.tolist()\n            pred_scr = pred_scr.tolist()\n            final_preds.append(pred)\n            final_scores.append(pred_scr)\n\n    for j in range(len(test_samples)):\n        tt = [id_target_map[p] for p in final_preds[j][1:]]\n        tt_score = final_scores[j][1:]\n        test_samples[j][\"preds\"] = tt\n        test_samples[j][\"pred_scores\"] = tt_score\n\n    submission = []\n    for sample_idx, sample in enumerate(test_samples):\n        preds = sample[\"preds\"]\n        offset_mapping = sample[\"offset_mapping\"]\n        sample_id = sample[\"id\"]\n        sample_text = sample[\"text\"]\n        sample_input_ids = sample[\"input_ids\"]\n        sample_pred_scores = sample[\"pred_scores\"]\n        sample_preds = []\n\n        if len(preds) < len(offset_mapping):\n            preds = preds + [\"O\"] * (len(offset_mapping) - len(preds))\n            sample_pred_scores = sample_pred_scores + [0] * (len(offset_mapping) - len(sample_pred_scores))\n\n        idx = 0\n        phrase_preds = []\n        while idx < len(offset_mapping):\n            start, _ = offset_mapping[idx]\n            if preds[idx] != \"O\":\n                label = preds[idx][2:]\n            else:\n                label = \"O\"\n            phrase_scores = []\n            phrase_scores.append(sample_pred_scores[idx])\n            idx += 1\n            while idx < len(offset_mapping):\n                if label == \"O\":\n                    matching_label = \"O\"\n                else:\n                    matching_label = f\"I-{label}\"\n                if preds[idx] == matching_label:\n                    _, end = offset_mapping[idx]\n                    phrase_scores.append(sample_pred_scores[idx])\n                    idx += 1\n                else:\n                    break\n            if \"end\" in locals():\n                phrase = sample_text[start:end]\n                phrase_preds.append((phrase, start, end, label, phrase_scores))\n\n        temp_df = []\n        for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n            word_start = len(sample_text[:start].split())\n            word_end = word_start + len(sample_text[start:end].split())\n            word_end = min(word_end, len(sample_text.split()))\n            ps = \" \".join([str(x) for x in range(word_start, word_end)])\n            if label != \"O\":\n                if sum(phrase_scores) / len(phrase_scores) >= proba_thresh[label]:\n                    if len(ps.split()) >= min_thresh[label]:\n                        temp_df.append((sample_id, label, ps))\n\n        temp_df = pd.DataFrame(temp_df, columns=[\"id\", \"class\", \"predictionstring\"])\n        submission.append(temp_df)\n\n\n\n    def threshold(df):\n        df = df.copy()\n        for key, value in map_clip.items():\n        # if df.loc[df['class']==key,'len'] < value \n            index = df.loc[df['class']==key].query(f'len<{value}').index\n            df.drop(index, inplace = True)\n        return df\n\n    submission = pd.concat(submission).reset_index(drop=True)\n    submission = link_evidence(submission)\n    submission.head()\n\n    submission['len'] = submission['predictionstring'].apply(lambda x:len(x.split()))\n    submission = threshold(submission)\n    submission.head()\n\n    submission[['id','class','predictionstring']].to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}