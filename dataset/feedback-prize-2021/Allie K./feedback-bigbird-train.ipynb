{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n\nVER = 13\n\nLOAD_TOKENS_FROM = '../input/mybibi/BigBird'\n\nLOAD_MODEL_FROM = None\n\nDOWNLOADED_MODEL_PATH = '../input/mybibi/BigBird' \n\n#if DOWNLOADED_MODEL_PATH is None:\n#    DOWNLOADED_MODEL_PATH = 'model'    \nMODEL_NAME = 'google/bigbird-roberta-base'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import cuda\nconfig = {'model_name': MODEL_NAME,   \n         'max_length': 1024,\n         'train_batch_size':4,\n         'valid_batch_size':4,\n         'epochs':13,\n         'learning_rates': [2.5e-5, 2.5e-5, 2e-5, 1.5e-5, 1e-5, 8e-6, 6e-6, 4e-6, 1e-6, 1e-6, 8e-7, 5e-7, 2.5e-7],\n         'max_grad_norm':10,\n         'device': 'cuda' if cuda.is_available() else 'cpu'}\n\nCOMPUTE_VAL_SCORE = True\nif len( os.listdir('../input/feedback-prize-2021/test') )>5:\n      COMPUTE_VAL_SCORE = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np, os \nimport pandas as pd, gc \nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nfrom sklearn.metrics import accuracy_score\n\ntrain_df = pd.read_csv('../input/feedback-prize-2021/train.csv')\nprint( train_df.shape )\ntrain_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_names, test_texts = [], []\nfor f in list(os.listdir('../input/feedback-prize-2021/test')):\n    test_names.append(f.replace('.txt', ''))\n    test_texts.append(open('../input/feedback-prize-2021/test/' + f, 'r').read())\ntest_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\ntest_texts.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_names, train_texts = [], []\nfor f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n    test_names.append(f.replace('.txt', ''))\n    train_texts.append(open('../input/feedback-prize-2021/train/' + f, 'r').read())\ntrain_text_df = pd.DataFrame({'id': test_names, 'text': train_texts})\ntrain_text_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not LOAD_TOKENS_FROM:\n    all_entities = []\n    for ii,i in enumerate(train_text_df.iterrows()):\n        if ii%100==0: print(ii,', ',end='')\n        total = i[1]['text'].split().__len__()\n        entities = [\"O\"]*total\n        for j in train_df[train_df['id'] == i[1]['id']].iterrows():\n            discourse = j[1]['discourse_type']\n            list_ix = [int(x) for x in j[1]['predictionstring'].split(' ')]\n            entities[list_ix[0]] = f\"B-{discourse}\"\n            for k in list_ix[1:]: entities[k] = f\"I-{discourse}\"\n        all_entities.append(entities)\n    train_text_df['entities'] = all_entities\n    train_text_df.to_csv('train_NER.csv',index=False)\n    \nelse:\n    from ast import literal_eval\n    train_text_df = pd.read_csv(f'{LOAD_TOKENS_FROM}/train_NER.csv')\n    train_text_df.entities = train_text_df.entities.apply(lambda x: literal_eval(x) )\n    \nprint( train_text_df.shape )\ntrain_text_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', \n                 'B-Counterclaim', 'I-Counterclaim', 'B-Rebuttal', 'I-Rebuttal', \n                 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n\nlabels_to_ids = {v:k for k,v in enumerate(output_labels)}\nids_to_labels = {k:v for k,v in enumerate(output_labels)}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_to_ids","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import stats\n\ndef split_mapping(unsplit):\n    splt = unsplit.split()\n    offset_to_wordidx = np.full(len(unsplit),-1)\n    txt_ptr = 0\n    for split_index, full_word in enumerate(splt):\n        while unsplit[txt_ptr:txt_ptr + len(full_word)] != full_word:\n            txt_ptr += 1\n        offset_to_wordidx[txt_ptr:txt_ptr + len(full_word)] = split_index\n        txt_ptr += len(full_word)\n    return offset_to_wordidx","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LABEL_ALL_SUBTOKENS = True\n\nclass dataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len, get_wids):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.get_wids = get_wids # for validation\n\n    def __getitem__(self, index):\n        text = self.data.text[index]        \n        word_labels = self.data.entities[index] if not self.get_wids else None\n        encoding = self.tokenizer(text, return_offsets_mapping=True, padding='max_length', \n                                  truncation=True, max_length=self.max_len)\n        \n        word_ids = encoding.word_ids()  \n        split_word_ids = np.full(len(word_ids),-1)\n        offset_to_wordidx = split_mapping(text)\n        offsets = encoding['offset_mapping']\n        \n        label_ids = []\n        for token_idx, word_idx in reversed(list(enumerate(word_ids))):\n            \n            if word_idx is None:\n                if not self.get_wids: label_ids.append(-100)\n            else:\n                if offsets[token_idx] != (0,0):\n                    split_idxs = offset_to_wordidx[offsets[token_idx][0]:offsets[token_idx][1]]\n                    split_index = stats.mode(split_idxs[split_idxs != -1]).mode[0] if len(np.unique(split_idxs)) > 1 else split_idxs[0]\n                    \n                    if split_index != -1: \n                        if not self.get_wids: label_ids.append( labels_to_ids[word_labels[split_index]] )\n                        split_word_ids[token_idx] = split_index\n                    else:\n                        if label_ids and label_ids[-1] != -100 and ids_to_labels[label_ids[-1]][0] == 'I':\n                            split_word_ids[token_idx] = split_word_ids[token_idx + 1]\n                            if not self.get_wids: label_ids.append(label_ids[-1])\n                        else:\n                            if not self.get_wids: label_ids.append(-100)\n                else:\n                    if not self.get_wids: label_ids.append(-100)\n        \n        encoding['labels'] = list(reversed(label_ids))\n\n        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n        if self.get_wids: \n            item['wids'] = torch.as_tensor(split_word_ids)\n               \n        return item\n\n    def __len__(self):\n        return self.len","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IDS = train_df.id.unique()\nprint('There are',len(IDS),'train texts. We will split 90% 10% for validation.')\n\nnp.random.seed(42)\ntrain_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\nvalid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\nnp.random.seed(None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = train_text_df[['id','text', 'entities']]\ntrain_dataset = data.loc[data['id'].isin(IDS[train_idx]),['text', 'entities']].reset_index(drop=True)\ntest_dataset = data.loc[data['id'].isin(IDS[valid_idx])].reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(data.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\ntokenizer = AutoTokenizer.from_pretrained(DOWNLOADED_MODEL_PATH) \ntraining_set = dataset(train_dataset, tokenizer, config['max_length'], False)\ntesting_set = dataset(test_dataset, tokenizer, config['max_length'], True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_params = {'batch_size': config['train_batch_size'], 'shuffle': True,\n                'num_workers': 2, 'pin_memory':True }\n\ntest_params = {'batch_size': config['valid_batch_size'], 'shuffle': False,\n               'num_workers': 2,  'pin_memory':True\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)\n\ntest_texts_set = dataset(test_texts, tokenizer, config['max_length'], True)\ntest_texts_loader = DataLoader(test_texts_set, **test_params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(epoch):\n    tr_loss, tr_accuracy = 0, 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    \n    model.train()\n    \n    for idx, batch in enumerate(training_loader):\n        \n        ids = batch['input_ids'].to(config['device'], dtype = torch.long)\n        mask = batch['attention_mask'].to(config['device'], dtype = torch.long)\n        labels = batch['labels'].to(config['device'], dtype = torch.long)\n\n        loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels,\n                               return_dict=False)\n        tr_loss += loss.item()\n\n        nb_tr_steps += 1\n        nb_tr_examples += labels.size(0)\n        \n        if idx % 200==0:\n            loss_step = tr_loss/nb_tr_steps\n            print(f\"Training loss after {idx:04d} training steps: {loss_step}\")\n           \n        flattened_targets = labels.view(-1) \n        active_logits = tr_logits.view(-1, model.num_labels) \n        flattened_predictions = torch.argmax(active_logits, axis=1) \n        \n        active_accuracy = labels.view(-1) != -100 \n                \n        labels = torch.masked_select(flattened_targets, active_accuracy)\n        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n        \n        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n        tr_accuracy += tmp_tr_accuracy\n    \n        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=config['max_grad_norm'])\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    epoch_loss = tr_loss / nb_tr_steps\n    tr_accuracy = tr_accuracy / nb_tr_steps\n    print(f\"Training loss epoch: {epoch_loss}\")\n    print(f\"Training accuracy epoch: {tr_accuracy}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config_model = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json') \nmodel = AutoModelForTokenClassification.from_pretrained('../input/fmodels/bibi_v26.pt', config=config_model)\nmodel.to(config['device'])\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=config['learning_rates'][0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not LOAD_MODEL_FROM:\n    for epoch in range(config['epochs']):\n        \n        print(f\"### Training epoch: {epoch + 1}\")\n        for g in optimizer.param_groups: \n            g['lr'] = config['learning_rates'][epoch]\n        lr = optimizer.param_groups[0]['lr']\n        print(f'### LR = {lr}\\n')\n        \n        train(epoch)\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n    torch.save(model.state_dict(), f'bigbird_v{VER}.pt')\nelse:\n    model.load_state_dict(torch.load('../input/fmodels/bibi_v26.pt'))\n    print('Model loaded.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(batch):\n                \n    ids = batch[\"input_ids\"].to(config['device'])\n    mask = batch[\"attention_mask\"].to(config['device'])\n    outputs = model(ids, attention_mask=mask, return_dict=False)  # b, 1024, 15\n    all_preds = torch.argmax(outputs[0], axis=-1).cpu().numpy() \n\n    predictions = []\n    for k,text_preds in enumerate(all_preds):\n        token_preds = [ids_to_labels[i] for i in text_preds]\n\n        prediction = []\n        word_ids = batch['wids'][k].numpy()  \n        previous_word_idx = -1\n        for idx,word_idx in enumerate(word_ids):                            \n            if word_idx == -1:\n                pass\n            elif word_idx != previous_word_idx:              \n                prediction.append(token_preds[idx])\n                previous_word_idx = word_idx\n\n        predictions.append(prediction)\n\n    return predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predictions(df=test_dataset, loader=testing_loader):\n    \n    model.eval()\n    \n    y_pred2 = []\n    for batch in loader:\n        labels = inference(batch)\n        y_pred2.extend(labels)\n        \n    final_preds2 = []\n    for i in range(len(df)):\n\n        idx = df.id.values[i]\n        pred = y_pred2[i] # Leave \"B\" and \"I\"\n        preds = []\n        \n        j = 0\n        while j < len(pred):\n            cls = pred[j]\n            if cls == 'O': \n                j += 1   # token\n            else: \n                cls = cls.replace('B','I') \n            end = j + 1\n            while end < len(pred) and pred[end] == cls:\n                end += 1\n            \n            if cls != 'O' and cls != '' and end - j > 7:  # 7\n                final_preds2.append((idx, cls.replace('I-',''), ' '.join(map(str, list(range(j, end))))))\n        \n            j = end\n        \n    oof = pd.DataFrame(final_preds2)\n    oof.columns = ['id','class','predictionstring']\n\n    return oof","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and ground truth and \n    overlap percentages used for determining true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(' '))\n    set_gt = set(row.predictionstring_gt.split(' '))\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle Student Writing Competition\n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','class','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df, left_on=['id','class'], right_on=['id','discourse_type'],\n                           how='outer', suffixes=('_pred','_gt')                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    my_f1_score = TP / (TP + 0.5*(FP+FN))\n    return my_f1_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_VAL_SCORE:\n    valid = train_df.loc[train_df['id'].isin(IDS[valid_idx])]\n    oof = get_predictions(test_dataset, testing_loader)\n    f1s = []\n    CLASSES = oof['class'].unique()\n    print()\n    for c in CLASSES:\n        pred_df = oof.loc[oof['class']==c].copy()\n        gt_df = valid.loc[valid['discourse_type']==c].copy()\n        f1 = score_feedback_comp(pred_df, gt_df)\n        print(c,f1)\n        f1s.append(f1)\n    print()\n    print('Overall',np.mean(f1s))\n    print()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = get_predictions(test_texts, test_texts_loader)\nsub.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}