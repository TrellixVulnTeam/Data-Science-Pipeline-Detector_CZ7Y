{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-27T15:25:41.527135Z","iopub.execute_input":"2022-02-27T15:25:41.527938Z","iopub.status.idle":"2022-02-27T15:25:41.557657Z","shell.execute_reply.started":"2022-02-27T15:25:41.52785Z","shell.execute_reply":"2022-02-27T15:25:41.557021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport nltk\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast\n\n# specify GPU\ndevice = torch.device(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:25:41.559174Z","iopub.execute_input":"2022-02-27T15:25:41.559473Z","iopub.status.idle":"2022-02-27T15:25:49.348083Z","shell.execute_reply.started":"2022-02-27T15:25:41.559439Z","shell.execute_reply":"2022-02-27T15:25:49.347334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import BERT-base pretrained model\nbert = AutoModel.from_pretrained('../input/feedback-model/bert',return_dict=False)\n# Load the BERT tokenizer\ntokenizer = BertTokenizerFast.from_pretrained('../input/feedback-model/tokenizer')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:25:49.349337Z","iopub.execute_input":"2022-02-27T15:25:49.349584Z","iopub.status.idle":"2022-02-27T15:25:55.009742Z","shell.execute_reply.started":"2022-02-27T15:25:49.349552Z","shell.execute_reply":"2022-02-27T15:25:55.009033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# freeze all the parameters\nfor param in bert.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:25:55.011539Z","iopub.execute_input":"2022-02-27T15:25:55.011768Z","iopub.status.idle":"2022-02-27T15:25:55.016594Z","shell.execute_reply.started":"2022-02-27T15:25:55.011737Z","shell.execute_reply":"2022-02-27T15:25:55.015967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERT_Arch(nn.Module):\n\n    def __init__(self, bert):\n      \n      super(BERT_Arch, self).__init__()\n\n      self.bert = bert \n      \n      # dropout layer\n      self.dropout = nn.Dropout(0.1)\n      \n      # relu activation function\n      self.relu =  nn.ReLU()\n\n      # dense layer 1\n      self.fc1 = nn.Linear(768,512)\n      \n      # dense layer 8 (Output layer)\n      self.fc2 = nn.Linear(512,8)\n\n      #softmax activation function\n      self.softmax = nn.LogSoftmax(dim=1)\n\n    #define the forward pass\n    def forward(self, sent_id, mask):\n\n      #pass the inputs to the model  \n      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n      \n      x = self.fc1(cls_hs)\n\n      x = self.relu(x)\n\n      x = self.dropout(x)\n\n      # output layer\n      x = self.fc2(x)\n      \n      # apply softmax activation\n      x = self.softmax(x)\n\n      return x","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:25:55.017959Z","iopub.execute_input":"2022-02-27T15:25:55.01847Z","iopub.status.idle":"2022-02-27T15:25:55.041053Z","shell.execute_reply.started":"2022-02-27T15:25:55.018437Z","shell.execute_reply":"2022-02-27T15:25:55.040406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pass the pre-trained BERT to our define architecture\nmodel = BERT_Arch(bert)\n\n# push the model to GPU\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:25:55.042373Z","iopub.execute_input":"2022-02-27T15:25:55.042635Z","iopub.status.idle":"2022-02-27T15:25:59.732291Z","shell.execute_reply.started":"2022-02-27T15:25:55.0426Z","shell.execute_reply":"2022-02-27T15:25:59.731547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load weights of best model\npath = '../input/feedback-model/saved_weights.pt'\nmodel.load_state_dict(torch.load(path))","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:25:59.733805Z","iopub.execute_input":"2022-02-27T15:25:59.734219Z","iopub.status.idle":"2022-02-27T15:26:04.335574Z","shell.execute_reply.started":"2022-02-27T15:25:59.734182Z","shell.execute_reply":"2022-02-27T15:26:04.334887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:26:04.336844Z","iopub.execute_input":"2022-02-27T15:26:04.337095Z","iopub.status.idle":"2022-02-27T15:26:04.342422Z","shell.execute_reply.started":"2022-02-27T15:26:04.337061Z","shell.execute_reply":"2022-02-27T15:26:04.341673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_test_texts_list():\n    total_list = []\n    \n    test_dir = \"../input/feedback-prize-2021/test\"\n    for filename in os.listdir(test_dir):\n        file_path = os.path.join(test_dir, filename)\n        # checking if it is a file\n        if os.path.isfile(file_path) and os.path.splitext(file_path)[1] == \".txt\":\n            with open(file_path) as f:\n                    total_list.append({\n                        'text' : f.read(), \n                        'id' : os.path.splitext(filename)[0]\n                    })\n    \n    return total_list","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:26:04.343903Z","iopub.execute_input":"2022-02-27T15:26:04.344435Z","iopub.status.idle":"2022-02-27T15:26:04.352818Z","shell.execute_reply.started":"2022-02-27T15:26:04.344398Z","shell.execute_reply":"2022-02-27T15:26:04.352004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nnlp = spacy.load('en_core_web_sm')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:26:04.355348Z","iopub.execute_input":"2022-02-27T15:26:04.355643Z","iopub.status.idle":"2022-02-27T15:26:09.517414Z","shell.execute_reply.started":"2022-02-27T15:26:04.355609Z","shell.execute_reply":"2022-02-27T15:26:09.516653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq_len = 50\ndef tokenize(text):\n    return tokenizer.batch_encode_plus(\n        text,\n        max_length = max_seq_len,\n        pad_to_max_length=True,\n        truncation=True,\n        return_token_type_ids=False,\n    )\n","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:26:09.518925Z","iopub.execute_input":"2022-02-27T15:26:09.51919Z","iopub.status.idle":"2022-02-27T15:26:09.52454Z","shell.execute_reply.started":"2022-02-27T15:26:09.519143Z","shell.execute_reply":"2022-02-27T15:26:09.52367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get predictions for test data\nimport torch, gc\n\ndef predict(seq,mask):\n    test_seq = torch.tensor(seq)\n    test_mask = torch.tensor(mask)\n    gc.collect()\n    torch.cuda.empty_cache()\n    with torch.no_grad():\n        preds = model(test_seq.to(device), test_mask.to(device))\n        _, prediction = torch.max(preds, dim=1)\n        return prediction.detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:26:09.526102Z","iopub.execute_input":"2022-02-27T15:26:09.526384Z","iopub.status.idle":"2022-02-27T15:26:09.538409Z","shell.execute_reply.started":"2022-02-27T15:26:09.526348Z","shell.execute_reply":"2022-02-27T15:26:09.537475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_dict = {\n    0:\"Claim\",\n    1:\"Concluding Statement\",\n    2:\"Counterclaim\",\n    3:\"Evidence\",\n    4:\"Lead\",\n    5:\"Other\",\n    6:\"Position\",\n    7:\"Rebuttal\"\n}","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:26:09.539589Z","iopub.execute_input":"2022-02-27T15:26:09.540452Z","iopub.status.idle":"2022-02-27T15:26:09.550064Z","shell.execute_reply.started":"2022-02-27T15:26:09.540414Z","shell.execute_reply":"2022-02-27T15:26:09.549273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_texts = create_test_texts_list()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:26:09.551411Z","iopub.execute_input":"2022-02-27T15:26:09.551974Z","iopub.status.idle":"2022-02-27T15:26:09.577358Z","shell.execute_reply.started":"2022-02-27T15:26:09.551936Z","shell.execute_reply":"2022-02-27T15:26:09.57669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_dicts_list = []\n\nfor test_text in test_texts:\n    \n    total_word_count = 0\n    \n    tokenized_sentences = nltk.sent_tokenize(test_text[\"text\"])\n    tokenized_text = tokenize(tokenized_sentences)\n    preds = predict(tokenized_text['input_ids'],tokenized_text['attention_mask'])\n#     x_test = vectorizer.transform(tokenized_sentences)\n#     x_test = selector.transform(x_test).astype('float32')\n#     preds = svm_model.predict(x_test) #Returns list\n    \n    \n    for i, pred in enumerate(preds):\n        if pred == 5:\n            continue\n        # Generate prediction strings for each predicted discourse\n        tokenized_sentence = tokenized_sentences[i]\n        \n        if i == 0 or preds[i-1] != pred:\n            prediction_string = \"\"\n        \n        for x in range(total_word_count, total_word_count + len(tokenized_sentence.split())):\n            prediction_string += f\"{x} \"\n        \n        total_word_count += len(tokenized_sentence.split())\n        \n        try:\n            if preds[i+1] == pred:\n                continue\n        except:\n            pass\n        \n        pred_dicts_list.append({\n            \"id\" : test_text[\"id\"],\n            \"class\" : label_dict.get(pred), \n            \"predictionstring\" : prediction_string.strip()\n        })","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:26:09.578608Z","iopub.execute_input":"2022-02-27T15:26:09.57886Z","iopub.status.idle":"2022-02-27T15:26:14.62387Z","shell.execute_reply.started":"2022-02-27T15:26:09.578828Z","shell.execute_reply":"2022-02-27T15:26:14.623159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame(pred_dicts_list)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:26:14.625126Z","iopub.execute_input":"2022-02-27T15:26:14.625388Z","iopub.status.idle":"2022-02-27T15:26:14.634792Z","shell.execute_reply.started":"2022-02-27T15:26:14.625354Z","shell.execute_reply":"2022-02-27T15:26:14.633913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:26:14.636403Z","iopub.execute_input":"2022-02-27T15:26:14.636837Z","iopub.status.idle":"2022-02-27T15:26:14.665914Z","shell.execute_reply.started":"2022-02-27T15:26:14.636792Z","shell.execute_reply":"2022-02-27T15:26:14.66529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T15:26:14.666998Z","iopub.execute_input":"2022-02-27T15:26:14.667315Z","iopub.status.idle":"2022-02-27T15:26:14.675036Z","shell.execute_reply.started":"2022-02-27T15:26:14.66728Z","shell.execute_reply":"2022-02-27T15:26:14.674334Z"},"trusted":true},"execution_count":null,"outputs":[]}]}