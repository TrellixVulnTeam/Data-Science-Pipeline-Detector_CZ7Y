{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nIn this notebook, I'll explore the texts by making images using the labels of their words. I don't know if this has any particular use case in this competition, but I thought it would be fun to implement it anyways, and maybe someone can use it. I don't know. Anyways, let's get going.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if dirname.split('/')[-1] not in ['train', 'test']:\n            print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-30T07:51:29.112061Z","iopub.execute_input":"2022-01-30T07:51:29.112379Z","iopub.status.idle":"2022-01-30T07:51:32.129463Z","shell.execute_reply.started":"2022-01-30T07:51:29.112349Z","shell.execute_reply":"2022-01-30T07:51:32.128374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/feedback-prize-2021/train.csv')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T07:51:56.128813Z","iopub.execute_input":"2022-01-30T07:51:56.129124Z","iopub.status.idle":"2022-01-30T07:51:58.106738Z","shell.execute_reply.started":"2022-01-30T07:51:56.129083Z","shell.execute_reply":"2022-01-30T07:51:58.105677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How can I implement this?\n\nFirst I think that either need to impose a cut off on the length of texts as when using transformers, so maybe this cutoff can be 1024 words. Or maybe I can use the the dimensions of the largest text as default.\n\nSo let's check the length of texts to get an idea about how we shall proceed.","metadata":{}},{"cell_type":"code","source":"def read_essay_txt(essay_id, path='train'):\n    essay_file_path = f\"../input/feedback-prize-2021/{path}/{essay_id}.txt\"\n    with open(essay_file_path, 'r') as essay_file:\n        return essay_file.read()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T08:01:15.214882Z","iopub.execute_input":"2022-01-30T08:01:15.215551Z","iopub.status.idle":"2022-01-30T08:01:15.224042Z","shell.execute_reply.started":"2022-01-30T08:01:15.215507Z","shell.execute_reply":"2022-01-30T08:01:15.222741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = {id_: read_essay_txt(id_) for id_ in train_df.id.unique()}\ntexts_len = {id_: len(text.split()) for id_, text in texts.items()}","metadata":{"execution":{"iopub.status.busy":"2022-01-30T08:02:31.535344Z","iopub.execute_input":"2022-01-30T08:02:31.535631Z","iopub.status.idle":"2022-01-30T08:03:25.93334Z","shell.execute_reply.started":"2022-01-30T08:02:31.535601Z","shell.execute_reply":"2022-01-30T08:03:25.931932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nsns.histplot(texts_len.values());\nprint('Mean Length:', np.mean(list(texts_len.values())))\nprint('Length > 1024: {:.2f}%'.format(np.mean([len_ > 1024 for len_ in texts_len.values()]) * 100))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T08:08:42.624427Z","iopub.execute_input":"2022-01-30T08:08:42.625274Z","iopub.status.idle":"2022-01-30T08:08:43.131735Z","shell.execute_reply.started":"2022-01-30T08:08:42.625218Z","shell.execute_reply":"2022-01-30T08:08:43.130676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that the mean is 421 tokens, and almost 1% of texts are larger than 1024. **So I'll stick with 1024.**\n\n## Getting RGB values\n\nNow I need to set an rgb value for each label, then I need to make a 3d array `(32, 32, 3)` for each text, and then I can visualize them.","metadata":{}},{"cell_type":"code","source":"train_df.discourse_type.unique()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T08:13:09.492069Z","iopub.execute_input":"2022-01-30T08:13:09.492726Z","iopub.status.idle":"2022-01-30T08:13:09.512669Z","shell.execute_reply.started":"2022-01-30T08:13:09.492684Z","shell.execute_reply":"2022-01-30T08:13:09.511753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import colors\n\nlabel_color_dict = {\n    'Lead': 'royalblue',\n    'Position': 'violet',\n    'Evidence': 'crimson',\n    'Claim': 'magenta',\n    'Counterclaim': 'darkorange',\n    'Rebuttal': 'lime',\n    'Concluding Statement': 'red'\n}\n\n\nlabel_rgb_dict = {label: colors.to_rgb(color) for label, color in label_color_dict.items()}\n\nlabel_rgb_dict","metadata":{"execution":{"iopub.status.busy":"2022-01-30T08:17:59.821313Z","iopub.execute_input":"2022-01-30T08:17:59.821892Z","iopub.status.idle":"2022-01-30T08:17:59.831311Z","shell.execute_reply.started":"2022-01-30T08:17:59.82185Z","shell.execute_reply":"2022-01-30T08:17:59.830516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have the rgb values, the next part is supposed to be easy. For each text we shall create a 2d array of of dims `(1024, 3)` then reshape this array into a 3d array of dims `(32, 32, 3)`.\n\nThe easiest way I can think of is to use the prediction string of each text to create a dict where keys are token ids and values are label. If the a token isn't in the dict, then we shall skip it and not set it's value in the array, and hence it will remain `(0, 0, 0)` which is black.","metadata":{}},{"cell_type":"code","source":"def get_text_arr(text_id, max_len=1024):\n    text_arr = np.zeros((max_len, 3))\n    text = texts[text_id]\n\n    token_label_dict = {}\n    for i, row in train_df.query('id == @text_id')[['discourse_type', 'predictionstring']].iterrows():\n        for token in row['predictionstring'].split(' '):\n            token_label_dict[int(token)] = row['discourse_type']\n\n    for i in range(max_len):\n        if i in token_label_dict.keys():\n            text_arr[i] = label_rgb_dict[token_label_dict[i]]\n    \n    dims = (int(np.sqrt(max_len)), int(np.sqrt(max_len)), 3)\n    text_arr = np.reshape(text_arr, dims)\n    \n    return text_arr","metadata":{"execution":{"iopub.status.busy":"2022-01-30T08:48:26.094569Z","iopub.execute_input":"2022-01-30T08:48:26.094939Z","iopub.status.idle":"2022-01-30T08:48:26.105048Z","shell.execute_reply.started":"2022-01-30T08:48:26.094901Z","shell.execute_reply":"2022-01-30T08:48:26.103588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_ids = train_df.id.unique()\n\nncols = 5\nnrows = 10\nmax_len = 1024\n\nfor nrow in range(nrows):\n    fig, axes = plt.subplots(1, ncols, figsize=(20, 5))\n    for i, text_id in enumerate(text_ids[nrow*ncols:(nrow*ncols)+ncols]):\n        axes[i].imshow(get_text_arr(text_id, max_len))\n        axes[i].set_title(text_id)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T08:51:46.395701Z","iopub.execute_input":"2022-01-30T08:51:46.396803Z","iopub.status.idle":"2022-01-30T08:51:54.276591Z","shell.execute_reply.started":"2022-01-30T08:51:46.396744Z","shell.execute_reply":"2022-01-30T08:51:54.275661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### And that's it. Thanks for reading.","metadata":{}}]}