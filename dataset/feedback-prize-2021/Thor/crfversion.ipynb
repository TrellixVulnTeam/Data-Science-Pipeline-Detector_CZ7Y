{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\ngc.enable()\n\nimport sys\nsys.path.append(\"../input/tez-lib/\")\nsys.path.append(\"../input/pytorch-crf/\")\nimport os\nfrom tqdm import tqdm\nfrom torchcrf import CRF\nimport numpy as np\nimport pandas as pd\nimport tez\nimport torch\nimport torch.nn as nn\n# from torch.optim import Optimizer\nfrom torch.optim import Adam\nfrom joblib import Parallel, delayed\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer","metadata":{"_uuid":"66c995cf-7cbb-421b-bdd3-24906f5fb886","_cell_guid":"e0e4bdf6-bcd9-4f82-a53b-9c5f685500a8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-14T04:45:21.909291Z","iopub.execute_input":"2022-02-14T04:45:21.909578Z","iopub.status.idle":"2022-02-14T04:45:28.729972Z","shell.execute_reply.started":"2022-02-14T04:45:21.909527Z","shell.execute_reply":"2022-02-14T04:45:28.729114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_id_map = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 2,\n    \"I-Position\": 3,\n    \"B-Evidence\": 4,\n    \"I-Evidence\": 5,\n    \"B-Claim\": 6,\n    \"I-Claim\": 7,\n    \"B-Concluding Statement\": 8,\n    \"I-Concluding Statement\": 9,\n    \"B-Counterclaim\": 10,\n    \"I-Counterclaim\": 11,\n    \"B-Rebuttal\": 12,\n    \"I-Rebuttal\": 13,\n    \"O\": 14,\n    \"PAD\": -100,\n}\n\n\nid_target_map = {v: k for k, v in target_id_map.items()}\n\nclass args1:\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/longformerlarge4096/longformer-large-4096/\"\n    tez_model= \"../input/fblongformerlarge1536/\"\n    output = \".\"\n    batch_size = 8\n    max_len = 2048\n    \nclass args2:\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/longformerlarge4096/longformer-large-4096/\"\n    tez_model= \"../input/tez-fb-large/\"\n    output = \".\"\n    batch_size = 8\n    max_len = 2048","metadata":{"execution":{"iopub.status.busy":"2022-02-14T04:45:28.731758Z","iopub.execute_input":"2022-02-14T04:45:28.732016Z","iopub.status.idle":"2022-02-14T04:45:28.741012Z","shell.execute_reply.started":"2022-02-14T04:45:28.731981Z","shell.execute_reply":"2022-02-14T04:45:28.740299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackDataset:\n    def __init__(self, samples, max_len, tokenizer):\n        self.samples = samples\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.length = len(samples)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        input_ids = self.samples[idx][\"input_ids\"]\n        # print(input_ids)\n        # print(input_labels)\n\n        # add start token id to the input_ids\n        input_ids = [self.tokenizer.cls_token_id] + input_ids\n\n        if len(input_ids) > self.max_len - 1:\n            input_ids = input_ids[: self.max_len - 1]\n\n        # add end token id to the input_ids\n        input_ids = input_ids + [self.tokenizer.sep_token_id]\n        attention_mask = [1] * len(input_ids)\n\n        # padding_length = self.max_len - len(input_ids)\n        # if padding_length > 0:\n        #     if self.tokenizer.padding_side == \"right\":\n        #         input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n        #         attention_mask = attention_mask + [0] * padding_length\n        #     else:\n        #         input_ids = [self.tokenizer.pad_token_id] * padding_length + input_ids\n        #         attention_mask = [0] * padding_length + attention_mask\n\n        # return {\n        #     \"ids\": torch.tensor(input_ids, dtype=torch.long),\n        #     \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n        # }\n\n        return {\n            \"ids\": input_ids,\n            \"mask\": attention_mask,\n        }","metadata":{"execution":{"iopub.status.busy":"2022-02-14T04:45:28.743101Z","iopub.execute_input":"2022-02-14T04:45:28.743913Z","iopub.status.idle":"2022-02-14T04:45:28.758217Z","shell.execute_reply.started":"2022-02-14T04:45:28.743836Z","shell.execute_reply":"2022-02-14T04:45:28.757481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Collate:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = 1860\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n        else:\n            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n\n        # convert to tensors\n        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-02-14T04:45:28.761226Z","iopub.execute_input":"2022-02-14T04:45:28.761758Z","iopub.status.idle":"2022-02-14T04:45:28.772588Z","shell.execute_reply.started":"2022-02-14T04:45:28.761721Z","shell.execute_reply":"2022-02-14T04:45:28.771902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackModel(tez.Model):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n        config = AutoConfig.from_pretrained(model_name)\n\n        hidden_dropout_prob: float = 0.1\n        layer_norm_eps: float = 1e-7\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = AutoModel.from_config(config)\n        self.output = nn.Linear(config.hidden_size, self.num_labels)\n\n    def forward(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        logits = self.output(sequence_output)\n        logits = torch.softmax(logits, dim=-1)\n        return logits, 0, {}\n    \n    def get_sequence_output(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        logits = self.output(sequence_output)\n        return logits\n    \nclass CRFModel(tez.Model):\n    def __init__(self, longformer_model, num_labels):\n        super().__init__()\n        self.longformer_model = longformer_model\n        self.crf = CRF(num_labels)\n        \n    def forward(self, ids, mask, label=None):\n        longformer_output = self.longformer_model.get_sequence_output(ids, mask)\n        if label is not None:\n            crf_output = self.crf(longformer_output, label)\n            return crf_output, -crf_output, {}\n        else:\n#             crf_output = self.crf(longformer_output)\n            crf_output = self.crf.decode(longformer_output)\n            return torch.tensor(crf_output, dtype=torch.long),torch.tensor(crf_output, dtype=torch.long),{}\n#         return crf_output, crf_output, {}","metadata":{"execution":{"iopub.status.busy":"2022-02-14T05:01:52.129264Z","iopub.execute_input":"2022-02-14T05:01:52.130014Z","iopub.status.idle":"2022-02-14T05:01:52.143141Z","shell.execute_reply.started":"2022-02-14T05:01:52.12997Z","shell.execute_reply":"2022-02-14T05:01:52.14248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _prepare_train_data_helper(args, tokenizer, df, ids):\n    train_samples = []\n    for idx in ids:\n        filename = os.path.join(args.input_path, \"train\", idx + \".txt\")\n        with open(filename, \"r\") as f:\n            text = f.read()\n        text = text.rstrip()\n        idx_info = df[df['id'] == idx]\n        former_end = 0\n        #deal with each idx_info\n        train_X = []\n        train_Y = []\n        for i, row in idx_info.iterrows():\n            start = int(row['discourse_start'])\n            end = int(row['discourse_end'])\n            discourse_type = row['discourse_type']\n            encoded_text = tokenizer.encode_plus(\n                text[former_end: start],\n                add_special_tokens=False\n            )\n            train_X += encoded_text[\"input_ids\"]\n            train_Y+=[target_id_map.get('O')]*len(encoded_text[\"input_ids\"])\n\n            encoded_text = tokenizer.encode_plus(\n                text[start: end],\n                add_special_tokens=False\n            )\n            train_X += encoded_text[\"input_ids\"]\n            train_Y+=[target_id_map.get('B-'+discourse_type)]\n            train_Y+=[target_id_map.get('I-'+discourse_type)]*(len(encoded_text[\"input_ids\"])-1)\n            former_end = end\n\n        encoded_text = tokenizer.encode_plus(\n            text[former_end:],\n            add_special_tokens=False\n        )\n        train_X += encoded_text[\"input_ids\"]\n        train_Y += [target_id_map.get('O')]*len(encoded_text[\"input_ids\"])\n\n        assert len(train_X) == len(train_Y)\n        sample = {\n            \"id\": idx,\n            \"input_ids\": train_X,\n            \"text\": text,\n            \"label\": train_Y\n        }\n\n        train_samples.append(sample)\n    return train_samples\n\n\ndef prepare_train_data(df, tokenizer, args):\n    train_samples = []\n    ids = df[\"id\"].unique()\n    ids_splits = np.array_split(ids, 4)\n\n    results = Parallel(n_jobs=4, backend=\"multiprocessing\")(\n        delayed(_prepare_train_data_helper)(args, tokenizer, df, idx) for idx in ids_splits\n    )\n    for result in results:\n        train_samples.extend(result)\n\n    return train_samples","metadata":{"execution":{"iopub.status.busy":"2022-02-14T04:54:07.279653Z","iopub.execute_input":"2022-02-14T04:54:07.280126Z","iopub.status.idle":"2022-02-14T04:54:07.297727Z","shell.execute_reply.started":"2022-02-14T04:54:07.280091Z","shell.execute_reply":"2022-02-14T04:54:07.296709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _prepare_test_data_helper(args, tokenizer, ids):\n    test_samples = []\n    for idx in ids:\n        filename = os.path.join(args.input_path, \"test\", idx + \".txt\")\n        with open(filename, \"r\") as f:\n            text = f.read()\n\n        encoded_text = tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        offset_mapping = encoded_text[\"offset_mapping\"]\n        sample = {\n            \"id\": idx,\n            \"input_ids\": input_ids,\n            \"text\": text,\n            \"offset_mapping\": offset_mapping,\n        }\n\n        test_samples.append(sample)\n    return test_samples\n\n\ndef prepare_test_data(df, tokenizer, args):\n    test_samples = []\n    ids = df[\"id\"].unique()\n    ids_splits = np.array_split(ids, 4)\n\n    results = Parallel(n_jobs=4, backend=\"multiprocessing\")(\n        delayed(_prepare_test_data_helper)(args, tokenizer, idx) for idx in ids_splits\n    )\n    for result in results:\n        test_samples.extend(result)\n\n    return test_samples","metadata":{"execution":{"iopub.status.busy":"2022-02-14T04:54:07.544169Z","iopub.execute_input":"2022-02-14T04:54:07.545181Z","iopub.status.idle":"2022-02-14T04:54:07.555582Z","shell.execute_reply.started":"2022-02-14T04:54:07.545135Z","shell.execute_reply":"2022-02-14T04:54:07.55451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainDataSet(Dataset):\n    def __init__(self, train_samples, tokenizer, max_len=1860):\n        self.train_samples = train_samples\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __getitem__(self, index):\n\n            \n        data = self.train_samples[index]\n        input_ids = data[\"input_ids\"]\n        label = data['label']\n        input_ids = [self.tokenizer.cls_token_id] + input_ids + [self.tokenizer.sep_token_id]\n        mask = [1] * len(input_ids)\n        label = [target_id_map.get('O')]+label\n        \n        if len(input_ids) > self.max_len - 1:\n            input_ids = input_ids[: self.max_len - 1]\n            mask = mask[: self.max_len - 1]\n            label = label[: self.max_len - 1]\n        input_ids = input_ids + (self.max_len - len(input_ids)) * [self.tokenizer.pad_token_id] \n        mask = mask + (self.max_len - len(mask)) * [0]\n        label = label + (self.max_len - len(label)) * [target_id_map.get('O')]\n        \n        assert len(mask) == len(input_ids)\n        input_ids = torch.tensor(input_ids, dtype=torch.long)\n        mask = torch.tensor(mask, dtype=torch.long)\n        label = torch.tensor(label, dtype=torch.long)\n        return input_ids.cuda(), mask.cuda(), label.cuda()\n    \n    def __len__(self):\n        return len(self.train_samples)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T04:54:07.802383Z","iopub.execute_input":"2022-02-14T04:54:07.802905Z","iopub.status.idle":"2022-02-14T04:54:07.814277Z","shell.execute_reply.started":"2022-02-14T04:54:07.802871Z","shell.execute_reply":"2022-02-14T04:54:07.813486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"longformer_model = FeedbackModel(model_name=args1.model, num_labels=len(target_id_map) - 1)\nlongformer_model.load(os.path.join(args1.tez_model, f\"model_1.bin\"), weights_only=True)\nfor p in longformer_model.parameters():\n    p.requires_grad = False\nmodel = CRFModel(longformer_model, len(target_id_map) - 1)\n\ntrain_para = []\nfor name, param in model.named_parameters():\n    if name.startswith('crf'):\n        train_para.append(param)\n        \nmodel = model.cuda()\noptimizer = Adam(train_para, 0.01, betas=(0.9, 0.999), eps=1e-6)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T05:02:05.197209Z","iopub.execute_input":"2022-02-14T05:02:05.197473Z","iopub.status.idle":"2022-02-14T05:02:13.502657Z","shell.execute_reply.started":"2022-02-14T05:02:05.197442Z","shell.execute_reply":"2022-02-14T05:02:13.501885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(args1.model)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T05:02:13.504271Z","iopub.execute_input":"2022-02-14T05:02:13.504615Z","iopub.status.idle":"2022-02-14T05:02:13.646295Z","shell.execute_reply.started":"2022-02-14T05:02:13.504577Z","shell.execute_reply":"2022-02-14T05:02:13.645487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(os.path.join(\"../input/feedback-prize-2021/\", \"train.csv\"))\ndf_ids = df[\"id\"].unique()\n\ntrain_samples = prepare_train_data(df, tokenizer, args1)\ntrain_dataset = TrainDataSet(train_samples, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T05:02:13.647567Z","iopub.execute_input":"2022-02-14T05:02:13.647816Z","iopub.status.idle":"2022-02-14T05:07:08.928444Z","shell.execute_reply.started":"2022-02-14T05:02:13.647782Z","shell.execute_reply":"2022-02-14T05:07:08.927513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T05:07:08.930864Z","iopub.execute_input":"2022-02-14T05:07:08.93112Z","iopub.status.idle":"2022-02-14T05:07:09.132499Z","shell.execute_reply.started":"2022-02-14T05:07:08.931081Z","shell.execute_reply":"2022-02-14T05:07:09.131698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_mean = 0\nbatch_size = 16\nmodel.train()\nfor epoch in range(1):\n    \n    for i, data in tqdm(enumerate(train_loader)):\n        inputs, mask, label = data\n        output, crf_loss, _ = model(inputs, mask, label)\n        loss = crf_loss/batch_size\n        loss.backward()\n        if i%10==0:\n            optimizer.step()\n            optimizer.zero_grad()\n        \n        loss_mean *= i\n        loss_mean += crf_loss.item()\n        loss_mean /= (i+1)\n        if i%100==0:\n            print(loss_mean)\n    optimizer.step()\n    optimizer.zero_grad()\n    print(loss_mean)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T05:07:09.134605Z","iopub.execute_input":"2022-02-14T05:07:09.135108Z","iopub.status.idle":"2022-02-14T06:49:25.641667Z","shell.execute_reply.started":"2022-02-14T05:07:09.135069Z","shell.execute_reply":"2022-02-14T06:49:25.640765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('crf1.bin')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:49:25.642896Z","iopub.execute_input":"2022-02-14T06:49:25.64369Z","iopub.status.idle":"2022-02-14T06:49:29.359224Z","shell.execute_reply.started":"2022-02-14T06:49:25.643649Z","shell.execute_reply":"2022-02-14T06:49:29.358363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"longformer_model = FeedbackModel(model_name=args1.model, num_labels=len(target_id_map) - 1)\n# longformer_model.load(os.path.join(args1.tez_model, f\"model_1.bin\"), weights_only=True)\n# for p in longformer_model.parameters():\n#     p.requires_grad = False\nmodel = CRFModel(longformer_model, len(target_id_map) - 1)\nmodel = model.cuda()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:50:02.880981Z","iopub.execute_input":"2022-02-14T06:50:02.881613Z","iopub.status.idle":"2022-02-14T06:50:10.007471Z","shell.execute_reply.started":"2022-02-14T06:50:02.881559Z","shell.execute_reply":"2022-02-14T06:50:10.006715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(os.path.join(\"../input/feedback-prize-2021/\", \"sample_submission.csv\"))\ndf_ids = df[\"id\"].unique()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:50:10.009297Z","iopub.execute_input":"2022-02-14T06:50:10.009551Z","iopub.status.idle":"2022-02-14T06:50:10.046231Z","shell.execute_reply.started":"2022-02-14T06:50:10.009502Z","shell.execute_reply":"2022-02-14T06:50:10.045361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load('./crf1.bin')\n# model.load('../input/crfversion/crf1.bin')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:50:20.425684Z","iopub.execute_input":"2022-02-14T06:50:20.426104Z","iopub.status.idle":"2022-02-14T06:50:21.33178Z","shell.execute_reply.started":"2022-02-14T06:50:20.426062Z","shell.execute_reply":"2022-02-14T06:50:21.330986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collate = Collate(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:50:24.77996Z","iopub.execute_input":"2022-02-14T06:50:24.780219Z","iopub.status.idle":"2022-02-14T06:50:24.785045Z","shell.execute_reply.started":"2022-02-14T06:50:24.780187Z","shell.execute_reply":"2022-02-14T06:50:24.783993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_samples = prepare_test_data(df, tokenizer, args1)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:50:26.25937Z","iopub.execute_input":"2022-02-14T06:50:26.259642Z","iopub.status.idle":"2022-02-14T06:50:27.586139Z","shell.execute_reply.started":"2022-02-14T06:50:26.25961Z","shell.execute_reply":"2022-02-14T06:50:27.585203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_preds = []\ntest_dataset = FeedbackDataset(test_samples, args1.max_len, tokenizer)\n# model.eval()\n# model.cuda()\n\npreds_iter = model.predict(test_dataset, batch_size=args1.batch_size, n_jobs=0, collate_fn=collate)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:50:27.589127Z","iopub.execute_input":"2022-02-14T06:50:27.589623Z","iopub.status.idle":"2022-02-14T06:50:27.596307Z","shell.execute_reply.started":"2022-02-14T06:50:27.589576Z","shell.execute_reply":"2022-02-14T06:50:27.595589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.multiprocessing.set_start_method('spawn')\nfor preds in preds_iter:\n    preds = preds.astype(np.float16)\n    raw_preds.append(preds)\n    \n# torch.cuda.empty_cache()\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:50:28.657799Z","iopub.execute_input":"2022-02-14T06:50:28.658417Z","iopub.status.idle":"2022-02-14T06:50:30.687913Z","shell.execute_reply.started":"2022-02-14T06:50:28.658377Z","shell.execute_reply":"2022-02-14T06:50:30.687211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = []\n\nfor rp in raw_preds:\n    pred_class = raw_preds[0].T\n    for pred in pred_class:\n        pred = pred.tolist()\n        final_preds.append(pred)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:50:30.689861Z","iopub.execute_input":"2022-02-14T06:50:30.69012Z","iopub.status.idle":"2022-02-14T06:50:30.69518Z","shell.execute_reply.started":"2022-02-14T06:50:30.690084Z","shell.execute_reply":"2022-02-14T06:50:30.694405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final_preds = []\n# final_scores = []\n\n# for rp in raw_preds:\n#     pred_class = np.argmax(rp, axis=2)\n#     pred_scrs = np.max(rp, axis=2)\n#     for pred, pred_scr in zip(pred_class, pred_scrs):\n#         pred = pred.tolist()\n#         pred_scr = pred_scr.tolist()\n#         final_preds.append(pred)\n#         final_scores.append(pred_scr)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:50:32.408467Z","iopub.execute_input":"2022-02-14T06:50:32.409196Z","iopub.status.idle":"2022-02-14T06:50:32.414647Z","shell.execute_reply.started":"2022-02-14T06:50:32.409156Z","shell.execute_reply":"2022-02-14T06:50:32.412994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for j in range(len(test_samples)):\n    tt = [id_target_map.get(p, 14) for p in final_preds[j][1:]]\n#     tt_score = final_scores[j][1:]\n    test_samples[j][\"preds\"] = tt\n#     test_samples[j][\"pred_scores\"] = tt_score","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:50:32.945367Z","iopub.execute_input":"2022-02-14T06:50:32.945707Z","iopub.status.idle":"2022-02-14T06:50:32.952213Z","shell.execute_reply.started":"2022-02-14T06:50:32.945676Z","shell.execute_reply":"2022-02-14T06:50:32.95123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jn(pst, start, end):\n    return \" \".join([str(x) for x in pst[start:end]])\n\n\ndef link_evidence(oof):\n    thresh = 1\n    idu = oof['id'].unique()\n    idc = idu[1]\n    eoof = oof[oof['class'] == \"Evidence\"]\n    neoof = oof[oof['class'] != \"Evidence\"]\n    for thresh2 in range(26,27, 1):\n        retval = []\n        for idv in idu:\n            for c in  ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n                   'Counterclaim', 'Rebuttal']:\n                q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n                if len(q) == 0:\n                    continue\n                pst = []\n                for i,r in q.iterrows():\n                    pst = pst +[-1] + [int(x) for x in r['predictionstring'].split()]\n                start = 1\n                end = 1\n                for i in range(2,len(pst)):\n                    cur = pst[i]\n                    end = i\n                    #if pst[start] == 205:\n                    #   print(cur, pst[start], cur - pst[start])\n                    if (cur == -1 and c != 'Evidence') or ((cur == -1) and ((pst[i+1] > pst[end-1] + thresh) or (pst[i+1] - pst[start] > thresh2))):\n                        retval.append((idv, c, jn(pst, start, end)))\n                        start = i + 1\n                v = (idv, c, jn(pst, start, end+1))\n                #print(v)\n                retval.append(v)\n        roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring']) \n        roof = roof.merge(neoof, how='outer')\n        return roof","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:50:33.325176Z","iopub.execute_input":"2022-02-14T06:50:33.325491Z","iopub.status.idle":"2022-02-14T06:50:33.338678Z","shell.execute_reply.started":"2022-02-14T06:50:33.325461Z","shell.execute_reply":"2022-02-14T06:50:33.336602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"proba_thresh = {\n    \"Lead\": 0.7,\n    \"Position\": 0.55,\n    \"Evidence\": 0.65,\n    \"Claim\": 0.55,\n    \"Concluding Statement\": 0.7,\n    \"Counterclaim\": 0.5,\n    \"Rebuttal\": 0.55,\n}\n\nmin_thresh = {\n    \"Lead\": 9,\n    \"Position\": 5,\n    \"Evidence\": 14,\n    \"Claim\": 3,\n    \"Concluding Statement\": 11,\n    \"Counterclaim\": 6,\n    \"Rebuttal\": 4,\n}\n","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:50:33.871405Z","iopub.execute_input":"2022-02-14T06:50:33.871671Z","iopub.status.idle":"2022-02-14T06:50:33.877423Z","shell.execute_reply.started":"2022-02-14T06:50:33.871643Z","shell.execute_reply":"2022-02-14T06:50:33.876513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = []\nfor sample_idx, sample in enumerate(test_samples):\n    preds = sample[\"preds\"]\n    offset_mapping = sample[\"offset_mapping\"]\n    sample_id = sample[\"id\"]\n    sample_text = sample[\"text\"]\n    sample_input_ids = sample[\"input_ids\"]\n#     sample_pred_scores = sample[\"pred_scores\"]\n    sample_preds = []\n\n    if len(preds) < len(offset_mapping):\n        preds = preds + [\"O\"] * (len(offset_mapping) - len(preds))\n#         sample_pred_scores = sample_pred_scores + [0] * (len(offset_mapping) - len(sample_pred_scores))\n    \n    idx = 0\n    phrase_preds = []\n\n    while idx < len(offset_mapping):\n        start, _ = offset_mapping[idx]\n        if preds[idx] != \"O\":\n            label = preds[idx][2:]\n        else:\n            label = \"O\"\n        phrase_scores = []\n#         phrase_scores.append(sample_pred_scores[idx])\n        idx += 1\n        while idx < len(offset_mapping):\n            if label == \"O\":\n                matching_label = \"O\"\n            else:\n                matching_label = f\"I-{label}\"\n            if preds[idx] == matching_label:\n                _, end = offset_mapping[idx]\n#                 phrase_scores.append(sample_pred_scores[idx])\n                idx += 1\n            else:\n                break\n        if \"end\" in locals():\n            phrase = sample_text[start:end]\n            phrase_preds.append((phrase, start, end, label, phrase_scores))\n    temp_df = []\n    for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n        word_start = len(sample_text[:start].split())\n        word_end = word_start + len(sample_text[start:end].split())\n        word_end = min(word_end, len(sample_text.split()))\n        ps = \" \".join([str(x) for x in range(word_start, word_end)])\n        if label != \"O\":\n#             if sum(phrase_scores) / len(phrase_scores) >= proba_thresh[label]:\n            if len(ps.split()) >= min_thresh[label]:\n                temp_df.append((sample_id, label, ps))\n\n    temp_df = pd.DataFrame(temp_df, columns=[\"id\", \"class\", \"predictionstring\"])\n    submission.append(temp_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:50:34.346798Z","iopub.execute_input":"2022-02-14T06:50:34.347233Z","iopub.status.idle":"2022-02-14T06:50:34.37205Z","shell.execute_reply.started":"2022-02-14T06:50:34.347197Z","shell.execute_reply":"2022-02-14T06:50:34.371396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.concat(submission).reset_index(drop=True)\nsubmission = link_evidence(submission)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:50:35.420372Z","iopub.execute_input":"2022-02-14T06:50:35.420924Z","iopub.status.idle":"2022-02-14T06:50:35.473341Z","shell.execute_reply.started":"2022-02-14T06:50:35.420887Z","shell.execute_reply":"2022-02-14T06:50:35.472683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T06:50:35.6448Z","iopub.execute_input":"2022-02-14T06:50:35.644998Z","iopub.status.idle":"2022-02-14T06:50:35.660201Z","shell.execute_reply.started":"2022-02-14T06:50:35.644974Z","shell.execute_reply":"2022-02-14T06:50:35.659584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}