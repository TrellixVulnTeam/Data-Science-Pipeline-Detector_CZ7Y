{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\ngc.enable()\n\nimport sys\nsys.path.append(\"../input/tez-lib/\")\n\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport tez\nimport torch\nimport torch.nn as nn\nfrom joblib import Parallel, delayed\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer","metadata":{"_uuid":"66c995cf-7cbb-421b-bdd3-24906f5fb886","_cell_guid":"e0e4bdf6-bcd9-4f82-a53b-9c5f685500a8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-15T03:27:26.151727Z","iopub.execute_input":"2022-03-15T03:27:26.152483Z","iopub.status.idle":"2022-03-15T03:27:33.063092Z","shell.execute_reply.started":"2022-03-15T03:27:26.152382Z","shell.execute_reply":"2022-03-15T03:27:33.06234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_id_map = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 2,\n    \"I-Position\": 3,\n    \"B-Evidence\": 4,\n    \"I-Evidence\": 5,\n    \"B-Claim\": 6,\n    \"I-Claim\": 7,\n    \"B-Concluding Statement\": 8,\n    \"I-Concluding Statement\": 9,\n    \"B-Counterclaim\": 10,\n    \"I-Counterclaim\": 11,\n    \"B-Rebuttal\": 12,\n    \"I-Rebuttal\": 13,\n    \"O\": 14,\n    \"PAD\": -100,\n}\n\n\nid_target_map = {v: k for k, v in target_id_map.items()}\n\n\n\nclass args4:\n    #4\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/longformerlarge4096/longformer-large-4096/\"\n    tez_model= \"../input/longformer-large-4096-5-fold\"\n    output = \".\"\n    batch_size = 8\n    max_len = 4096\n    # [0.6635,0.6638,0.6689,0.6651,0.6532]\n    folds = [2]\n#     folds = [0,1,2,3,4]\n    \nclass args13:\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/allenai-longformer-large-4096\"\n    tez_model= \"../input/longformer-large-post-train-5-fold-cp-30000\"\n    output = \".\"\n    batch_size = 8\n    max_len = 4096\n    #[0.6748,0.668,0.6742,0.6658,0.6552]\n    folds = [0,2]\n#     folds = [0,1,2,3,4]\n    \nclass args14:\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/allenai-longformer-large-4096\"\n    tez_model= \"../input/longformer-large-post-train-5-fold-cp-33000\"\n    output = \".\"\n    batch_size = 8\n    max_len = 4096\n    #[0.6651,0.6694,0.67,0.6631,0.6581]\n    folds = [1,2]\n#     folds = [0,1,2,3,4]\n    \nclass args20:\n    #20\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/longformerlarge4096/longformer-large-4096/\"\n    tez_model= \"../input/longformer-large-post-train-1024-cp-30-refit\"\n    output = \".\"\n    batch_size = 8\n    max_len = 4096\n    #[0.6694,0.6652,0.6657,0.6647,0.6703]\n    folds = [4]\n#     folds = [0,1,2,3,4]\n\nclass args24:\n    #24\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/allenai-longformer-large-4096\"\n    tez_model= \"../input/longformer-large-posttrain-cp30-refit-10-5\"\n    output = \".\"\n    batch_size = 8\n    max_len = 4096\n    #[0.664,0.6691,0.6676,0.6685,0.6599]\n    folds = [3]\n#     folds = [0,1,2,3,4]\n\nclass args56:\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/allenai-longformer-large-4096\"\n    tez_model= \"../input/longformer-l-10fold-md-s2022-l16\"\n    output = \".\"\n    batch_size = 8\n    max_len = 4096\n    #[0.6693,0.6662,0.6785,0.6581,0.6835]\n    folds = [2,4]\n#     folds = [0,1,2,3,4]\n        \nclass args57:\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/allenai-longformer-large-4096\"\n    tez_model= \"../input/longformer-l-10fold-md-s42-l16\"\n    output = \".\"\n    batch_size = 8\n    max_len = 4096\n    #[0.6794,0.6728,0.676,0.6687,0.67]\n    folds = [0,2]\n#     folds = [0,1,2,3,4] \n    \n    \nclass argsk1:\n    #开源1\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/allenai-longformer-large-4096\"\n    tez_model= \"../input/fblongformerlarge1536\"\n    output = \".\"\n    batch_size = 8\n    max_len = 4096\n    folds = [2]\n#     folds = [0,1,2,3,4]\n\nclass argsk2:\n    #开源2\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/allenai-longformer-large-4096\"\n    tez_model= \"../input/tez-fb-large\"\n    output = \".\"\n    batch_size = 8\n    max_len = 4096\n    folds = [0]\n#     folds = [0,1,2,3,4]\n\n\n\nclass short_args1:\n    # pretrain\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/facebook-bart-large-mnli\"\n    tez_model= \"../input/short-models\"\n    output = \".\"\n    batch_size = 16\n    max_len = 512\n    folds = [0,1,2,3,4,5,6,7,8,9,10,11,12]\n    \nclass test_args:\n    # pretrain\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/facebook-bart-large-mnli\"\n    tez_model= \"../input/bart-large-mnli-5-fold-md5\"\n    output = \".\"\n    batch_size = 16\n    max_len = 512\n    folds = [0,4]","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:27:33.067025Z","iopub.execute_input":"2022-03-15T03:27:33.067256Z","iopub.status.idle":"2022-03-15T03:27:33.086036Z","shell.execute_reply.started":"2022-03-15T03:27:33.067231Z","shell.execute_reply":"2022-03-15T03:27:33.084918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackDataset:\n    def __init__(self, samples, max_len, tokenizer):\n        self.samples = samples\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.length = len(samples)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        input_ids = self.samples[idx][\"input_ids\"]\n        # print(input_ids)\n        # print(input_labels)\n\n        # add start token id to the input_ids\n        input_ids = [self.tokenizer.cls_token_id] + input_ids\n\n        if len(input_ids) > self.max_len - 1:\n            input_ids = input_ids[: self.max_len - 1]\n\n        # add end token id to the input_ids\n        input_ids = input_ids + [self.tokenizer.sep_token_id]\n        attention_mask = [1] * len(input_ids)\n\n        return {\n            \"ids\": input_ids,\n            \"mask\": attention_mask,\n        }","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:27:33.087014Z","iopub.execute_input":"2022-03-15T03:27:33.087252Z","iopub.status.idle":"2022-03-15T03:27:33.100913Z","shell.execute_reply.started":"2022-03-15T03:27:33.087221Z","shell.execute_reply":"2022-03-15T03:27:33.100195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Collate:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"ids\"]])\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n        else:\n            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n\n        # convert to tensors\n        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:27:33.103646Z","iopub.execute_input":"2022-03-15T03:27:33.104474Z","iopub.status.idle":"2022-03-15T03:27:33.114883Z","shell.execute_reply.started":"2022-03-15T03:27:33.104427Z","shell.execute_reply":"2022-03-15T03:27:33.114198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackModel(tez.Model):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n        config = AutoConfig.from_pretrained(model_name)\n\n        hidden_dropout_prob: float = 0.1\n        layer_norm_eps: float = 1e-7\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = AutoModel.from_config(config)\n        self.output = nn.Linear(config.hidden_size, self.num_labels)\n\n    def forward(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        logits = self.output(sequence_output)\n        logits = torch.softmax(logits, dim=-1)\n        return logits, 0, {}","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:27:33.115875Z","iopub.execute_input":"2022-03-15T03:27:33.116753Z","iopub.status.idle":"2022-03-15T03:27:33.125105Z","shell.execute_reply.started":"2022-03-15T03:27:33.116718Z","shell.execute_reply":"2022-03-15T03:27:33.124405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _prepare_test_data_helper(args, tokenizer, ids,flag='test'):\n    test_samples = []\n    for idx in ids:\n        filename = os.path.join(args.input_path, flag, idx + \".txt\")\n#         filename = os.path.join(args.input_path, \"train\", idx + \".txt\")\n        with open(filename, \"r\") as f:\n            text = f.read()\n\n        encoded_text = tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        offset_mapping = encoded_text[\"offset_mapping\"]\n\n        sample = {\n            \"id\": idx,\n            \"input_ids\": input_ids,\n            \"text\": text,\n            \"offset_mapping\": offset_mapping,\n        }\n\n        test_samples.append(sample)\n    return test_samples\n\n\ndef prepare_test_data(df, tokenizer, args,flag='test'):\n    test_samples = []\n    ids = df[\"id\"].unique()\n    ids_splits = np.array_split(ids, 2)\n\n    results = Parallel(n_jobs=2, backend=\"multiprocessing\")(\n        delayed(_prepare_test_data_helper)(args, tokenizer,idx,flag) for idx in ids_splits\n    )\n    for result in results:\n        test_samples.extend(result)\n\n    return test_samples","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:27:33.126358Z","iopub.execute_input":"2022-03-15T03:27:33.126692Z","iopub.status.idle":"2022-03-15T03:27:33.136537Z","shell.execute_reply.started":"2022-03-15T03:27:33.12666Z","shell.execute_reply":"2022-03-15T03:27:33.135657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"is_test = False","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:27:33.138168Z","iopub.execute_input":"2022-03-15T03:27:33.138462Z","iopub.status.idle":"2022-03-15T03:27:33.14515Z","shell.execute_reply.started":"2022-03-15T03:27:33.138429Z","shell.execute_reply":"2022-03-15T03:27:33.144351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # df = pd.read_csv(os.path.join(\"../input/feedback-prize-2021/\", \"sample_submission.csv\"))#dev\nif is_test:\n    df = pd.read_csv(os.path.join(\"../input/feedback-prize-2021/\", \"train.csv\"))#train\n    df_ids = df[\"id\"].unique()[:100]\n    df = df[df['id'].isin(df_ids)]\n    print(len(df_ids))\n#     test_samples = prepare_test_data(df, tokenizer, args1,'train')\nelse:\n    df = pd.read_csv(os.path.join(\"../input/feedback-prize-2021/\", \"sample_submission.csv\"))\n    df_ids = df[\"id\"].unique()\n#     test_samples = prepare_test_data(df, tokenizer, args1,'test')","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:27:33.14672Z","iopub.execute_input":"2022-03-15T03:27:33.146976Z","iopub.status.idle":"2022-03-15T03:27:33.167662Z","shell.execute_reply.started":"2022-03-15T03:27:33.146944Z","shell.execute_reply":"2022-03-15T03:27:33.167059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_multi_models(args_list=[],mode='short'):\n    raw_preds = []\n    first = True\n    n = sum([len(x.folds) for x in args_list])\n    print(f\"total n is {n}\")\n    for args in args_list:\n        # process data\n        tokenizer = AutoTokenizer.from_pretrained(args.model)\n        collate = Collate(tokenizer=tokenizer)\n        test_samples = prepare_test_data(df, tokenizer, args,'test')\n        test_samples = sorted(test_samples,key=lambda x:len(x['input_ids']))\n        \n        # find i\n        for split_i,data in enumerate(test_samples):\n            if len(data['input_ids'])>=512:\n                break\n        print(f\"split_i is {split_i}\")\n        if mode=='short':\n            test_samples = test_samples[:split_i]\n        elif mode=='long':\n            test_samples = test_samples[split_i:]\n        else:\n            print(\"use all\")\n        \n        test_dataset = FeedbackDataset(test_samples, args.max_len, tokenizer)\n        model = FeedbackModel(model_name=args.model, num_labels=len(target_id_map) - 1)\n        for fold_ in args.folds:\n            print(f\"start model: {args.tez_model},fold {fold_}\")\n            model.load(os.path.join(args.tez_model, f\"model_{fold_}.bin\"), weights_only=True)\n#             # 选择batch size\n#             if mode=='short' and args.batch_size==8:\n#                 batch_size = 16\n#             else:\n            batch_size = args.batch_size\n            preds_iter = model.predict(test_dataset, batch_size=batch_size, n_jobs=-1, collate_fn=collate)\n            current_idx = 0\n            for preds in preds_iter:\n                preds = preds.astype(np.float16)\n                preds = preds / n\n                if first:\n                    raw_preds.append(preds)\n                else:\n                    raw_preds[current_idx] += preds\n                    current_idx += 1\n            if first:\n                first=False\n            torch.cuda.empty_cache()\n            gc.collect()\n    return raw_preds,test_samples","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:27:33.168793Z","iopub.execute_input":"2022-03-15T03:27:33.170198Z","iopub.status.idle":"2022-03-15T03:27:33.18184Z","shell.execute_reply.started":"2022-03-15T03:27:33.170161Z","shell.execute_reply":"2022-03-15T03:27:33.181096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if len(df)>100:\n#     short_args_list = [args4,args13,args14,args20,args24]\n#     long_args_list = [args4,args13,args14,args20,args24]\n#     short_args_list = [args4,args20,args24,args57]#4,20,24,57\n#     long_args_list = [args4,args20,args24,args57]\n    short_args_list = [short_args1]\n    long_args_list = [args4,args13,args14,args20,args24,args56,args57,argsk1,argsk2]\nelse:\n    short_args_list = [test_args]\n    long_args_list = [test_args]","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:27:33.186111Z","iopub.execute_input":"2022-03-15T03:27:33.186943Z","iopub.status.idle":"2022-03-15T03:27:33.192121Z","shell.execute_reply.started":"2022-03-15T03:27:33.186897Z","shell.execute_reply":"2022-03-15T03:27:33.191456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_preds_short,samples_short = eval_multi_models(args_list=short_args_list,mode='short')\nraw_preds_long,samples_long = eval_multi_models(args_list=long_args_list,mode='long')\nraw_preds = raw_preds_short + raw_preds_long\ntest_samples = samples_short + samples_long","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:27:33.202376Z","iopub.execute_input":"2022-03-15T03:27:33.202619Z","iopub.status.idle":"2022-03-15T03:27:33.208525Z","shell.execute_reply.started":"2022-03-15T03:27:33.202587Z","shell.execute_reply":"2022-03-15T03:27:33.207808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# raw_preds,test_samples = eval_multi_models(args_list=short_args_list,mode='all')","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:27:33.209774Z","iopub.execute_input":"2022-03-15T03:27:33.210016Z","iopub.status.idle":"2022-03-15T03:28:21.434913Z","shell.execute_reply.started":"2022-03-15T03:27:33.209984Z","shell.execute_reply":"2022-03-15T03:28:21.43419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# raw_preds_short[0].shape,raw_preds_long[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:28:21.437119Z","iopub.execute_input":"2022-03-15T03:28:21.437667Z","iopub.status.idle":"2022-03-15T03:28:21.441551Z","shell.execute_reply.started":"2022-03-15T03:28:21.437628Z","shell.execute_reply":"2022-03-15T03:28:21.440768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(raw_preds),len(test_samples)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:28:21.443111Z","iopub.execute_input":"2022-03-15T03:28:21.443629Z","iopub.status.idle":"2022-03-15T03:28:21.455465Z","shell.execute_reply.started":"2022-03-15T03:28:21.443593Z","shell.execute_reply":"2022-03-15T03:28:21.4548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = []\nfinal_scores = []\n\nfor rp in raw_preds:\n    pred_class = np.argmax(rp, axis=2)\n    pred_scrs = np.max(rp, axis=2)\n    for pred, pred_scr in zip(pred_class, pred_scrs):\n        pred = pred.tolist()\n        pred_scr = pred_scr.tolist()\n        final_preds.append(pred)\n        final_scores.append(pred_scr)\n\nfor j in range(len(test_samples)):\n    tt = [id_target_map[p] for p in final_preds[j][1:]]\n    tt_score = final_scores[j][1:]\n    test_samples[j][\"preds\"] = tt\n    test_samples[j][\"pred_scores\"] = tt_score","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:28:21.456534Z","iopub.execute_input":"2022-03-15T03:28:21.456865Z","iopub.status.idle":"2022-03-15T03:28:21.466387Z","shell.execute_reply.started":"2022-03-15T03:28:21.456829Z","shell.execute_reply":"2022-03-15T03:28:21.465597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jn(pst, start, end):\n    return \" \".join([str(x) for x in pst[start:end]])\n\n\ndef link_evidence(oof):\n    thresh = 1\n    idu = oof['id'].unique()\n    idc = idu[1]\n    eoof = oof[oof['class'] == \"Evidence\"]\n    neoof = oof[oof['class'] != \"Evidence\"]\n    for thresh2 in range(26,27, 1):\n        retval = []\n        for idv in idu:\n            for c in  ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n                   'Counterclaim', 'Rebuttal']:\n                q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n                if len(q) == 0:\n                    continue\n                pst = []\n                for i,r in q.iterrows():\n                    pst = pst +[-1] + [int(x) for x in r['predictionstring'].split()]\n                start = 1\n                end = 1\n                for i in range(2,len(pst)):\n                    cur = pst[i]\n                    end = i\n                    #if pst[start] == 205:\n                    #   print(cur, pst[start], cur - pst[start])\n                    if (cur == -1 and c != 'Evidence') or ((cur == -1) and ((pst[i+1] > pst[end-1] + thresh) or (pst[i+1] - pst[start] > thresh2))):\n                        retval.append((idv, c, jn(pst, start, end)))\n                        start = i + 1\n                v = (idv, c, jn(pst, start, end+1))\n                #print(v)\n                retval.append(v)\n        roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring']) \n        roof = roof.merge(neoof, how='outer')\n        return roof\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:28:21.467835Z","iopub.execute_input":"2022-03-15T03:28:21.468259Z","iopub.status.idle":"2022-03-15T03:28:21.481741Z","shell.execute_reply.started":"2022-03-15T03:28:21.468223Z","shell.execute_reply":"2022-03-15T03:28:21.48101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"proba_thresh = {\n\"Lead\": 0.687,\n\"Position\": 0.537,\n\"Evidence\": 0.637,\n\"Claim\": 0.537,\n\"Concluding Statement\": 0.687,\n\"Counterclaim\": 0.537,\n\"Rebuttal\": 0.537,\n}\n\nmin_thresh = {\n\"Lead\": 9,\n\"Position\": 5,\n\"Evidence\": 14,\n\"Claim\": 3,\n\"Concluding Statement\": 11,\n\"Counterclaim\": 6,\n\"Rebuttal\": 4,\n}\n\nsubmission = []\nfor sample_idx, sample in enumerate(test_samples):\n    preds = sample[\"preds\"]\n    offset_mapping = sample[\"offset_mapping\"]\n    sample_id = sample[\"id\"]\n    sample_text = sample[\"text\"]\n    sample_input_ids = sample[\"input_ids\"]\n    sample_pred_scores = sample[\"pred_scores\"]\n    sample_preds = []\n\n    if len(preds) < len(offset_mapping):\n        preds = preds + [\"O\"] * (len(offset_mapping) - len(preds))\n        sample_pred_scores = sample_pred_scores + [0] * (len(offset_mapping) - len(sample_pred_scores))\n    \n    idx = 0\n    phrase_preds = []\n    while idx < len(offset_mapping):\n        start, _ = offset_mapping[idx]\n        if preds[idx] != \"O\":\n            label = preds[idx][2:]\n        else:\n            label = \"O\"\n        phrase_scores = []\n        phrase_scores.append(sample_pred_scores[idx])\n        idx += 1\n        while idx < len(offset_mapping):\n            if label == \"O\":\n                matching_label = \"O\"\n            else:\n                matching_label = f\"I-{label}\"\n            if preds[idx] == matching_label:\n                _, end = offset_mapping[idx]\n                phrase_scores.append(sample_pred_scores[idx])\n                idx += 1\n            else:\n                break\n        if \"end\" in locals():\n            phrase = sample_text[start:end]\n            phrase_preds.append((phrase, start, end, label, phrase_scores))\n\n    temp_df = []\n    for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n        word_start = len(sample_text[:start].split())\n        word_end = word_start + len(sample_text[start:end].split())\n        word_end = min(word_end, len(sample_text.split()))\n        ps = \" \".join([str(x) for x in range(word_start, word_end)])\n        if label != \"O\":\n            if sum(phrase_scores) / len(phrase_scores) >= proba_thresh[label]:\n                if len(ps.split()) >= min_thresh[label]:\n                    temp_df.append((sample_id, label, ps))\n    \n    temp_df = pd.DataFrame(temp_df, columns=[\"id\", \"class\", \"predictionstring\"])\n    submission.append(temp_df)\n\nsubmission = pd.concat(submission).reset_index(drop=True)\nsubmission = link_evidence(submission)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:28:21.483095Z","iopub.execute_input":"2022-03-15T03:28:21.483472Z","iopub.status.idle":"2022-03-15T03:28:21.547907Z","shell.execute_reply.started":"2022-03-15T03:28:21.483435Z","shell.execute_reply":"2022-03-15T03:28:21.547121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:28:21.549442Z","iopub.execute_input":"2022-03-15T03:28:21.549695Z","iopub.status.idle":"2022-03-15T03:28:21.570013Z","shell.execute_reply.started":"2022-03-15T03:28:21.54966Z","shell.execute_reply":"2022-03-15T03:28:21.569103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}