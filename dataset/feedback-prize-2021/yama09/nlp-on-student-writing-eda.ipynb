{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Feedback Prize - Evaluating Student Writing\n\nジョージア州立大学（GSU）は、アトランタにある学部および大学院の都市型公立研究機関です。U.S. News & World Report誌は、GSUを全米で最も革新的な大学のひとつに位置づけています。GSUは、アフリカ系アメリカ人に授与する学士号の数が、国内の他のどの非営利大学よりも多いのが特徴です。GSUとアリゾナ州に拠点を置く独立非営利団体The Learning Agency Labは、社会的利益を目的とした学習ベースのツールやプログラムを科学的に開発することに注力しています。\n\nこのコンペティションでは、学生の文章に含まれる要素を特定します。具体的には、6年生から12年生の生徒が書いたエッセイのテキストを自動的に分割し、論証的・修辞的な要素を分類します。これまでに公開された学生の作文に関する最大のデータセットにアクセスし、データサイエンスの分野で急成長している自然言語処理に関するあなたのスキルを試してみてください。\n\n成功すれば、生徒が自分の作文に対するフィードバックを受けやすくなり、作文の成果を向上させる機会が増えるでしょう。バーチャルライティングチューターや自動ライティングシステムはこれらのアルゴリズムを活用でき、教師は採点時間の短縮に利用できるかもしれません。あなたが開発したオープンソースのアルゴリズムは、どのような教育機関でも、若い作家の成長をより良く支援することができます。","metadata":{"papermill":{"duration":0.043611,"end_time":"2022-02-06T13:43:52.364884","exception":false,"start_time":"2022-02-06T13:43:52.321273","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom glob import glob\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\nfrom matplotlib.ticker import FuncFormatter\nfrom nltk.corpus import stopwords\nfrom tqdm.notebook import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\nimport spacy\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport os","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-02-06T13:43:52.452622Z","iopub.status.busy":"2022-02-06T13:43:52.451369Z","iopub.status.idle":"2022-02-06T13:44:05.03955Z","shell.execute_reply":"2022-02-06T13:44:05.038791Z","shell.execute_reply.started":"2022-02-06T13:05:02.819938Z"},"papermill":{"duration":12.632175,"end_time":"2022-02-06T13:44:05.039722","exception":false,"start_time":"2022-02-06T13:43:52.407547","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/feedback-prize-2021/train.csv')\ntrain[['discourse_id', 'discourse_start', 'discourse_end']] = train[['discourse_id', 'discourse_start', 'discourse_end']].astype(int)\n\nsample_submission = pd.read_csv('../input/feedback-prize-2021/sample_submission.csv')\n\n#The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell\ntrain_txt = glob('../input/feedback-prize-2021/train/*.txt') \ntest_txt = glob('../input/feedback-prize-2021/test/*.txt')","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-02-06T13:44:05.129137Z","iopub.status.busy":"2022-02-06T13:44:05.128201Z","iopub.status.idle":"2022-02-06T13:44:07.867254Z","shell.execute_reply":"2022-02-06T13:44:07.866691Z","shell.execute_reply.started":"2022-02-06T13:05:13.761069Z"},"papermill":{"duration":2.786403,"end_time":"2022-02-06T13:44:07.86746","exception":false,"start_time":"2022-02-06T13:44:05.081057","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# コンペティションの紹介\n\n基本的には、12歳から18歳くらいの子供たちが書いた作文の中から、7つの「談話型」に分類される言葉の並びを探すというものです。その7つとは\n\n- リード - 統計、引用、説明など、読者の注意を引き、論文に向かわせる工夫で始まる導入部。\n- ポジション - 主な質問に対する意見または結論\n- 主張 - その立場を支持する主張\n- 反証 - 他の主張に反論する、または立場に反対する理由を示す主張。\n- 反証 - 反証に反論する主張\n- 証拠 - 主張、反論、または反証をサポートするアイデアや例。\n- Conclusion Statement - 主張を再表現する結論となる文。\n\nまず、あるエッセイの全文を見てみましょう。","metadata":{"papermill":{"duration":0.04211,"end_time":"2022-02-06T13:44:07.953245","exception":false,"start_time":"2022-02-06T13:44:07.911135","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!cat ../input/feedback-prize-2021/train/423A1CA112E2.txt","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:44:08.040361Z","iopub.status.busy":"2022-02-06T13:44:08.038928Z","iopub.status.idle":"2022-02-06T13:44:08.83206Z","shell.execute_reply":"2022-02-06T13:44:08.832606Z","shell.execute_reply.started":"2022-02-06T13:05:15.925429Z"},"papermill":{"duration":0.837797,"end_time":"2022-02-06T13:44:08.832796","exception":false,"start_time":"2022-02-06T13:44:07.994999","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"訓練データセットからは、このエッセイから抽出された以下のような人間のアノテーションが得られる。","metadata":{"papermill":{"duration":0.041503,"end_time":"2022-02-06T13:44:08.916502","exception":false,"start_time":"2022-02-06T13:44:08.874999","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train.query('id == \"423A1CA112E2\"')","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:44:09.007738Z","iopub.status.busy":"2022-02-06T13:44:09.006655Z","iopub.status.idle":"2022-02-06T13:44:09.051922Z","shell.execute_reply":"2022-02-06T13:44:09.052467Z","shell.execute_reply.started":"2022-02-06T13:05:16.705196Z"},"papermill":{"duration":0.092419,"end_time":"2022-02-06T13:44:09.052641","exception":false,"start_time":"2022-02-06T13:44:08.960222","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Kaggleでは、以下のようなフィールドの説明があります。\n- id - エッセイの応答のためのIDコード\n- discourse_id - 談話要素のIDコード\n- discourse_start - エッセイの回答で談話要素が始まる文字の位置です。\n- discourse_end - エッセイの回答で、談話要素が終了する文字の位置です。\n- discourse_text - 談話要素のテキスト．\n- discourse_type - 談話要素の分類\n- discourse_type_num - 談話要素の列挙されたクラスラベル．\n- predictionstring - 予測に必要な、学習サンプルの単語インデックス。\n\nここでのGround Truthは、discourse typeとprediction stringの組み合わせである。予測文字列はエッセイの単語のインデックスに対応し、この単語の並びに対して予測される談話タイプは正しいはずである。もし、正しいディスコース・タイプが予測されたとしても、Ground Truthで指定されたものより長い、あるいは短い単語列であれば、部分一致となる可能性がある。\n\nこのように、必ずしもエッセイのすべてのテキストが談話に含まれるとは限らない。この場合、タイトルはどの談話にも属さない。\n\n\n# 談話の長さ_テキストと予測文字列の長さ\n\nまず、discourse_textとpredictionstringが常に同じ単語数であるか（あるべき姿）を確認したいと思います。","metadata":{"papermill":{"duration":0.042381,"end_time":"2022-02-06T13:44:09.137617","exception":false,"start_time":"2022-02-06T13:44:09.095236","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#add columns\ntrain[\"discourse_len\"] = train[\"discourse_text\"].apply(lambda x: len(x.split()))\ntrain[\"pred_len\"] = train[\"predictionstring\"].apply(lambda x: len(x.split()))\n\n\ncols_to_display = ['discourse_id', 'discourse_text', 'discourse_type','predictionstring', 'discourse_len', 'pred_len']\ntrain[cols_to_display].head()","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:44:09.228972Z","iopub.status.busy":"2022-02-06T13:44:09.228249Z","iopub.status.idle":"2022-02-06T13:44:10.19673Z","shell.execute_reply":"2022-02-06T13:44:10.197224Z","shell.execute_reply.started":"2022-02-06T13:05:16.753674Z"},"papermill":{"duration":1.017913,"end_time":"2022-02-06T13:44:10.197435","exception":false,"start_time":"2022-02-06T13:44:09.179522","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"これは常に正しいのでしょうか？いいえ、私はこれが間違っている（一語の差で）468の談話を見つけました。","metadata":{"papermill":{"duration":0.04285,"end_time":"2022-02-06T13:44:10.283414","exception":false,"start_time":"2022-02-06T13:44:10.240564","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(f\"The total number of discourses is {len(train)}\")\ntrain.query('discourse_len != pred_len')[cols_to_display]","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:44:10.372053Z","iopub.status.busy":"2022-02-06T13:44:10.371379Z","iopub.status.idle":"2022-02-06T13:44:10.39373Z","shell.execute_reply":"2022-02-06T13:44:10.394269Z","shell.execute_reply.started":"2022-02-06T13:05:17.732898Z"},"papermill":{"duration":0.068268,"end_time":"2022-02-06T13:44:10.394488","exception":false,"start_time":"2022-02-06T13:44:10.32622","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"まず1つ目を確認してみましょう。","metadata":{"papermill":{"duration":0.043749,"end_time":"2022-02-06T13:44:10.482174","exception":false,"start_time":"2022-02-06T13:44:10.438425","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(train.query('discourse_id == 1622473475289')['discourse_text'].values[0])\nprint(train.query('discourse_id == 1622473475289')['discourse_text'].values[0].split())\nprint(len(train.query('discourse_id == 1622473475289')['discourse_text'].values[0].split()))","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:44:10.579865Z","iopub.status.busy":"2022-02-06T13:44:10.578825Z","iopub.status.idle":"2022-02-06T13:44:10.59474Z","shell.execute_reply":"2022-02-06T13:44:10.594007Z","shell.execute_reply.started":"2022-02-06T13:05:17.761778Z"},"papermill":{"duration":0.067548,"end_time":"2022-02-06T13:44:10.594895","exception":false,"start_time":"2022-02-06T13:44:10.527347","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"19字という長さは正しいように思いますし、予測文字列の長さも本当は18字のような気がします。\n\n**Update:** その答えは、ディスカッショントピックにあります。: [Mystery Solved - Discrepancy Between PredictionString and DiscourseText](https://www.kaggle.com/c/feedback-prize-2021/discussion/297591)","metadata":{"papermill":{"duration":0.043589,"end_time":"2022-02-06T13:44:10.683438","exception":false,"start_time":"2022-02-06T13:44:10.639849","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(train.query('discourse_id == 1622473475289')['predictionstring'].values[0])\nprint(train.query('discourse_id == 1622473475289')['predictionstring'].values[0].split())\nprint(len(train.query('discourse_id == 1622473475289')['predictionstring'].values[0].split()))","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:44:10.783115Z","iopub.status.busy":"2022-02-06T13:44:10.78042Z","iopub.status.idle":"2022-02-06T13:44:10.797854Z","shell.execute_reply":"2022-02-06T13:44:10.798403Z","shell.execute_reply.started":"2022-02-06T13:05:17.785767Z"},"papermill":{"duration":0.071035,"end_time":"2022-02-06T13:44:10.798583","exception":false,"start_time":"2022-02-06T13:44:10.727548","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 談話タイプごとの長さと頻度、相対的な位置づけ\n\n談話の長さとクラス（discourse_type）には相関関係があるのでしょうか？はい、あります。Evidenceは平均して最も長いディスコースタイプです。出現頻度を見ると、CounterclaimとRebuttalは比較的まれであることがわかります。","metadata":{"papermill":{"duration":0.044746,"end_time":"2022-02-06T13:44:10.888764","exception":false,"start_time":"2022-02-06T13:44:10.844018","status":"completed"},"tags":[]}},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\n\nax1 = fig.add_subplot(211)\nax1 = train.groupby('discourse_type')['discourse_len'].mean().sort_values().plot(kind=\"barh\")\nax1.set_title(\"Average number of words versus Discourse Type\", fontsize=14, fontweight = 'bold')\nax1.set_xlabel(\"Average number of words\", fontsize = 10)\nax1.set_ylabel(\"\")\n\nax2 = fig.add_subplot(212)\nax2 = train.groupby('discourse_type')['discourse_type'].count().sort_values().plot(kind=\"barh\")\nax2.get_xaxis().set_major_formatter(FuncFormatter(lambda x, p: format(int(x), ','))) #add thousands separator\nax2.set_title(\"Frequency of Discourse Type in all essays\", fontsize=14, fontweight = 'bold')\nax2.set_xlabel(\"Frequency\", fontsize = 10)\nax2.set_ylabel(\"\")\n\nplt.tight_layout(pad=2)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-02-06T13:44:10.997198Z","iopub.status.busy":"2022-02-06T13:44:10.989647Z","iopub.status.idle":"2022-02-06T13:44:11.571834Z","shell.execute_reply":"2022-02-06T13:44:11.572395Z","shell.execute_reply.started":"2022-02-06T13:05:17.807334Z"},"papermill":{"duration":0.637721,"end_time":"2022-02-06T13:44:11.572582","exception":false,"start_time":"2022-02-06T13:44:10.934861","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"discourse_type_numというフィールドがあります。Evidence1、Position1、Claim1 は、小論文の中でほとんど常に存在することがわかります。また、ほとんどの学生が少なくとも1つのConcluding Statementを持っています。驚くべきは、約40%の小論文でLeadが欠落していることです（Lead 1はほぼ60%の小論文に見られます）。\n\nこのグラフでは、少なくとも3％の作文に含まれるdiscourse_type_numのみをプロットしています。","metadata":{"papermill":{"duration":0.04586,"end_time":"2022-02-06T13:44:11.665568","exception":false,"start_time":"2022-02-06T13:44:11.619708","status":"completed"},"tags":[]}},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\nav_per_essay = train['discourse_type_num'].value_counts(ascending = True).rename_axis('discourse_type_num').reset_index(name='count')\nav_per_essay['perc'] = round((av_per_essay['count'] / train.id.nunique()),3)\nav_per_essay = av_per_essay.set_index('discourse_type_num')\nax = av_per_essay.query('perc > 0.03')['perc'].plot(kind=\"barh\")\nax.set_title(\"discourse_type_num: Percent present in essays\", fontsize=20, fontweight = 'bold')\nax.bar_label(ax.containers[0], label_type=\"edge\")\nax.set_xlabel(\"Percent\")\nax.set_ylabel(\"\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-02-06T13:44:11.763683Z","iopub.status.busy":"2022-02-06T13:44:11.762899Z","iopub.status.idle":"2022-02-06T13:44:12.196337Z","shell.execute_reply":"2022-02-06T13:44:12.196849Z","shell.execute_reply.started":"2022-02-06T13:05:18.371438Z"},"papermill":{"duration":0.485511,"end_time":"2022-02-06T13:44:12.197026","exception":false,"start_time":"2022-02-06T13:44:11.711515","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"以下に、談話の開始と終了の平均位置をプロットしたものを示します。","metadata":{"papermill":{"duration":0.049391,"end_time":"2022-02-06T13:44:12.295102","exception":false,"start_time":"2022-02-06T13:44:12.245711","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data = train.groupby(\"discourse_type\")[['discourse_end', 'discourse_start']].mean().reset_index().sort_values(by = 'discourse_start', ascending = False)\ndata.plot(x='discourse_type',\n        kind='barh',\n        stacked=False,\n        title='Average start and end position absolute',\n        figsize=(12,4))\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:44:12.414272Z","iopub.status.busy":"2022-02-06T13:44:12.407154Z","iopub.status.idle":"2022-02-06T13:44:12.632931Z","shell.execute_reply":"2022-02-06T13:44:12.632405Z","shell.execute_reply.started":"2022-02-06T13:05:18.81135Z"},"papermill":{"duration":0.28928,"end_time":"2022-02-06T13:44:12.633078","exception":false,"start_time":"2022-02-06T13:44:12.343798","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"また、エッセイと談話のタイプの相対的な位置づけにも興味がある。以下は、最初と最後に確認された言説の言説タイプの分布である。","metadata":{"papermill":{"duration":0.051099,"end_time":"2022-02-06T13:44:12.734248","exception":false,"start_time":"2022-02-06T13:44:12.683149","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_first = train.drop_duplicates(subset = \"id\", keep = \"first\").discourse_type.value_counts().rename_axis('discourse_type').reset_index(name='counts_first')\ntrain_first['percent_first'] = round((train_first['counts_first']/train.id.nunique()),2)\ntrain_last = train.drop_duplicates(subset = \"id\", keep = \"last\").discourse_type.value_counts().rename_axis('discourse_type').reset_index(name='counts_last')\ntrain_last['percent_last'] = round((train_last['counts_last']/train.id.nunique()),2)\ntrain_first_last = train_first.merge(train_last, on = \"discourse_type\", how = \"left\")\ntrain_first_last","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:44:12.848173Z","iopub.status.busy":"2022-02-06T13:44:12.847474Z","iopub.status.idle":"2022-02-06T13:44:12.945592Z","shell.execute_reply":"2022-02-06T13:44:12.945025Z","shell.execute_reply.started":"2022-02-06T13:05:19.092044Z"},"papermill":{"duration":0.161208,"end_time":"2022-02-06T13:44:12.945746","exception":false,"start_time":"2022-02-06T13:44:12.784538","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"また、約40%のエッセイでLeadが欠落していることが分かっている。リードがある場合、それはほとんど常にエッセイの中で最初に確認される談話であることがわかります（リード2はとにかく非常に稀です）。","metadata":{"papermill":{"duration":0.052158,"end_time":"2022-02-06T13:44:13.049027","exception":false,"start_time":"2022-02-06T13:44:12.996869","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train['discourse_nr'] = 1\ncounter = 1\n\nfor i in tqdm(range(1, len(train))):\n    if train.loc[i, 'id'] == train.loc[i-1, 'id']:\n        counter += 1\n        train.loc[i, 'discourse_nr'] = counter\n    else:\n        counter = 1\n        train.loc[i, 'discourse_nr'] = counter\n\n#if you are interested in other discourse_types you can add them to the list in df.query\ntrain.query('discourse_type in [\"Lead\"]').groupby('discourse_type_num')['discourse_nr'].value_counts().to_frame('occurences')","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:44:13.155223Z","iopub.status.busy":"2022-02-06T13:44:13.154535Z","iopub.status.idle":"2022-02-06T13:45:55.77051Z","shell.execute_reply":"2022-02-06T13:45:55.769936Z","shell.execute_reply.started":"2022-02-06T13:05:19.200023Z"},"papermill":{"duration":102.669421,"end_time":"2022-02-06T13:45:55.770684","exception":false,"start_time":"2022-02-06T13:44:13.101263","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# アノテーションの隙間を調査する（discourse_textとして使用しないテキスト）","metadata":{"papermill":{"duration":0.052857,"end_time":"2022-02-06T13:45:55.87624","exception":false,"start_time":"2022-02-06T13:45:55.823383","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"最後のdiscourse_endを電車で取るだけでは、最後の文章が談話として使われていない可能性があり、完全に正しいとは言えない。したがって、私はエッセイを調べて本当のエンドを見つけることにする。え......そういえば、Rob Mullaが優れたEDAですでにそれをやっていたことを思い出すまでは。[Student Writing Competition [Twitch Stream]](https://www.kaggle.com/robikscube/student-writing-competition-twitch) ;-). ","metadata":{"papermill":{"duration":0.052078,"end_time":"2022-02-06T13:45:55.98035","exception":false,"start_time":"2022-02-06T13:45:55.928272","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# this code chunk is copied from Rob Mulla\nlen_dict = {}\nword_dict = {}\nfor t in tqdm(train_txt):\n    with open(t, \"r\") as txt_file:\n        myid = t.split(\"/\")[-1].replace(\".txt\", \"\")\n        data = txt_file.read()\n        mylen = len(data.strip())\n        myword = len(data.split())\n        len_dict[myid] = mylen\n        word_dict[myid] = myword\ntrain[\"essay_len\"] = train[\"id\"].map(len_dict)\ntrain[\"essay_words\"] = train[\"id\"].map(word_dict)","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:45:56.119724Z","iopub.status.busy":"2022-02-06T13:45:56.11886Z","iopub.status.idle":"2022-02-06T13:47:08.609974Z","shell.execute_reply":"2022-02-06T13:47:08.609204Z","shell.execute_reply.started":"2022-02-06T13:06:53.860062Z"},"papermill":{"duration":72.576941,"end_time":"2022-02-06T13:47:08.610152","exception":false,"start_time":"2022-02-06T13:45:56.033211","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"各エッセイの最後の談話のdiscourse_endを比較すると、discourse_endがessay_lenより大きい場合があることがわかる。これは正しいとは思えませんが、それらは確かにエッセイの最後の文章であると仮定します。","metadata":{"papermill":{"duration":0.054548,"end_time":"2022-02-06T13:47:08.720231","exception":false,"start_time":"2022-02-06T13:47:08.665683","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#initialize column\ntrain['gap_length'] = np.nan\n\n#set the first one\ntrain.loc[0, 'gap_length'] = 7 #discourse start - 1 (previous end is always -1)\n\n#loop over rest\nfor i in tqdm(range(1, len(train))):\n    #gap if difference is not 1 within an essay\n    if ((train.loc[i, \"id\"] == train.loc[i-1, \"id\"])\\\n        and (train.loc[i, \"discourse_start\"] - train.loc[i-1, \"discourse_end\"] > 1)):\n        train.loc[i, 'gap_length'] = train.loc[i, \"discourse_start\"] - train.loc[i-1, \"discourse_end\"] - 2\n        #minus 2 as the previous end is always -1 and the previous start always +1\n    #gap if the first discourse of an new essay does not start at 0\n    elif ((train.loc[i, \"id\"] != train.loc[i-1, \"id\"])\\\n        and (train.loc[i, \"discourse_start\"] != 0)):\n        train.loc[i, 'gap_length'] = train.loc[i, \"discourse_start\"] -1\n\n\n #is there any text after the last discourse of an essay?\nlast_ones = train.drop_duplicates(subset=\"id\", keep='last')\nlast_ones['gap_end_length'] = np.where((last_ones.discourse_end < last_ones.essay_len),\\\n                                       (last_ones.essay_len - last_ones.discourse_end),\\\n                                       np.nan)\n\ncols_to_merge = ['id', 'discourse_id', 'gap_end_length']\ntrain = train.merge(last_ones[cols_to_merge], on = [\"id\", \"discourse_id\"], how = \"left\")","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:47:08.834233Z","iopub.status.busy":"2022-02-06T13:47:08.833512Z","iopub.status.idle":"2022-02-06T13:47:44.055736Z","shell.execute_reply":"2022-02-06T13:47:44.056207Z","shell.execute_reply.started":"2022-02-06T13:07:43.684306Z"},"papermill":{"duration":35.284433,"end_time":"2022-02-06T13:47:44.056432","exception":false,"start_time":"2022-02-06T13:47:08.771999","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#display an example\ncols_to_display = ['id', 'discourse_start', 'discourse_end', 'discourse_type', 'essay_len', 'gap_length', 'gap_end_length']\ntrain[cols_to_display].query('id == \"AFEC37C2D43F\"')","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:47:44.168385Z","iopub.status.busy":"2022-02-06T13:47:44.167673Z","iopub.status.idle":"2022-02-06T13:47:44.228946Z","shell.execute_reply":"2022-02-06T13:47:44.229452Z","shell.execute_reply.started":"2022-02-06T13:08:17.021319Z"},"papermill":{"duration":0.119834,"end_time":"2022-02-06T13:47:44.229623","exception":false,"start_time":"2022-02-06T13:47:44.109789","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#how many pieces of tekst are not used as discourses?\nprint(f\"Besides the {len(train)} discourse texts, there are {len(train.query('gap_length.notna()', engine='python'))+ len(train.query('gap_end_length.notna()', engine='python'))} pieces of text not classified.\")","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:47:44.341822Z","iopub.status.busy":"2022-02-06T13:47:44.339642Z","iopub.status.idle":"2022-02-06T13:47:44.361949Z","shell.execute_reply":"2022-02-06T13:47:44.362993Z","shell.execute_reply.started":"2022-02-06T13:08:17.087847Z"},"papermill":{"duration":0.080122,"end_time":"2022-02-06T13:47:44.363283","exception":false,"start_time":"2022-02-06T13:47:44.283161","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"上記の例のようなギャップは小さいが、多くのエッセイで大きなギャップがある","metadata":{"papermill":{"duration":0.052699,"end_time":"2022-02-06T13:47:44.470668","exception":false,"start_time":"2022-02-06T13:47:44.417969","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train.sort_values(by = \"gap_length\", ascending = False)[cols_to_display].head()","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:47:44.581753Z","iopub.status.busy":"2022-02-06T13:47:44.58105Z","iopub.status.idle":"2022-02-06T13:47:44.628395Z","shell.execute_reply":"2022-02-06T13:47:44.62775Z","shell.execute_reply.started":"2022-02-06T13:08:17.109435Z"},"papermill":{"duration":0.103633,"end_time":"2022-02-06T13:47:44.628562","exception":false,"start_time":"2022-02-06T13:47:44.524929","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.sort_values(by = \"gap_end_length\", ascending = False)[cols_to_display].head()","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:47:44.743136Z","iopub.status.busy":"2022-02-06T13:47:44.742475Z","iopub.status.idle":"2022-02-06T13:47:44.785114Z","shell.execute_reply":"2022-02-06T13:47:44.784513Z","shell.execute_reply.started":"2022-02-06T13:08:17.159489Z"},"papermill":{"duration":0.101666,"end_time":"2022-02-06T13:47:44.785265","exception":false,"start_time":"2022-02-06T13:47:44.683599","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"以下に、異常値を取り除いたすべてのギャップの長さのヒストグラムを示します（300文字より長いすべてのギャップ）。","metadata":{"papermill":{"duration":0.056313,"end_time":"2022-02-06T13:47:44.897591","exception":false,"start_time":"2022-02-06T13:47:44.841278","status":"completed"},"tags":[]}},{"cell_type":"code","source":"all_gaps = (train.gap_length[~train.gap_length.isna()]).append((train.gap_end_length[~train.gap_end_length.isna()]), ignore_index= True)\n#filter outliers\nall_gaps = all_gaps[all_gaps<300]\nfig = plt.figure(figsize=(12,6))\nall_gaps.plot.hist(bins=100)\nplt.title(\"Histogram of gap length (gaps up to 300 characters only)\")\nplt.xticks(rotation=0)\nplt.xlabel(\"Length of gaps in characters\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-02-06T13:47:45.016599Z","iopub.status.busy":"2022-02-06T13:47:45.015819Z","iopub.status.idle":"2022-02-06T13:47:45.392704Z","shell.execute_reply":"2022-02-06T13:47:45.392129Z","shell.execute_reply.started":"2022-02-06T13:08:17.200828Z"},"papermill":{"duration":0.440058,"end_time":"2022-02-06T13:47:45.392861","exception":false,"start_time":"2022-02-06T13:47:44.952803","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 本当にひどい作文（分類されていない文章の割合が多い）が多いのか？\nはい、ありますね。談話タイプに分類されない文章が90％くらいあるものもあります。\n\ngap_end_length 7348の方ですが、この方は同じ文章を何度もコピー＆ペーストしてエッセイにしていることが分かりました。ディスカッショントピック参照: [Finding: essay with all text repeated many times](https://www.kaggle.com/c/feedback-prize-2021/discussion/298193).","metadata":{"papermill":{"duration":0.056164,"end_time":"2022-02-06T13:47:45.505315","exception":false,"start_time":"2022-02-06T13:47:45.449151","status":"completed"},"tags":[]}},{"cell_type":"code","source":"total_gaps = train.groupby('id').agg({'essay_len': 'first',\\\n                                               'gap_length': 'sum',\\\n                                               'gap_end_length': 'sum'})\ntotal_gaps['perc_not_classified'] = round(((total_gaps.gap_length + total_gaps.gap_end_length)/total_gaps.essay_len),2)\n\ntotal_gaps.sort_values(by = 'perc_not_classified', ascending = False).head()","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:47:45.621717Z","iopub.status.busy":"2022-02-06T13:47:45.620451Z","iopub.status.idle":"2022-02-06T13:47:45.686258Z","shell.execute_reply":"2022-02-06T13:47:45.685711Z","shell.execute_reply.started":"2022-02-06T13:08:17.607463Z"},"papermill":{"duration":0.125624,"end_time":"2022-02-06T13:47:45.686445","exception":false,"start_time":"2022-02-06T13:47:45.560821","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 隙間も含めたカラー印刷エッセイ\n\nSanskar Hasija (https://www.kaggle.com/odins0n/feedback-prize-eda) が作ったノートブックに、とてもきれいなやり方が載っていました。このコードは素晴らしいのですが、まだギャップを表示していません。以下では、エッセイのすべてのギャップを、談話タイプ \"Nothing\" の行として追加する関数を作成します。","metadata":{"papermill":{"duration":0.057853,"end_time":"2022-02-06T13:47:45.801995","exception":false,"start_time":"2022-02-06T13:47:45.744142","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def add_gap_rows(essay):\n    cols_to_keep = ['discourse_start', 'discourse_end', 'discourse_type', 'gap_length', 'gap_end_length']\n    df_essay = train.query('id == @essay')[cols_to_keep].reset_index(drop = True)\n\n    #index new row\n    insert_row = len(df_essay)\n   \n    for i in range(1, len(df_essay)):          \n        if df_essay.loc[i,\"gap_length\"] >0:\n            if i == 0:\n                start = 0 #as there is no i-1 for first row\n                end = df_essay.loc[0, 'discourse_start'] -1\n                disc_type = \"Nothing\"\n                gap_end = np.nan\n                gap = np.nan\n                df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n                insert_row += 1\n            else:\n                start = df_essay.loc[i-1, \"discourse_end\"] + 1\n                end = df_essay.loc[i, 'discourse_start'] -1\n                disc_type = \"Nothing\"\n                gap_end = np.nan\n                gap = np.nan\n                df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n                insert_row += 1\n\n    df_essay = df_essay.sort_values(by = \"discourse_start\").reset_index(drop=True)\n\n    #add gap at end\n    if df_essay.loc[(len(df_essay)-1),'gap_end_length'] > 0:\n        start = df_essay.loc[(len(df_essay)-1), \"discourse_end\"] + 1\n        end = start + df_essay.loc[(len(df_essay)-1), 'gap_end_length']\n        disc_type = \"Nothing\"\n        gap_end = np.nan\n        gap = np.nan\n        df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n        \n    return(df_essay)","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:47:45.926835Z","iopub.status.busy":"2022-02-06T13:47:45.926051Z","iopub.status.idle":"2022-02-06T13:47:45.933372Z","shell.execute_reply":"2022-02-06T13:47:45.933916Z","shell.execute_reply.started":"2022-02-06T13:08:17.679074Z"},"papermill":{"duration":0.0748,"end_time":"2022-02-06T13:47:45.934099","exception":false,"start_time":"2022-02-06T13:47:45.859299","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"add_gap_rows(\"129497C3E0FC\")","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:47:46.052378Z","iopub.status.busy":"2022-02-06T13:47:46.051649Z","iopub.status.idle":"2022-02-06T13:47:46.087398Z","shell.execute_reply":"2022-02-06T13:47:46.087942Z","shell.execute_reply.started":"2022-02-06T13:08:17.698052Z"},"papermill":{"duration":0.096602,"end_time":"2022-02-06T13:47:46.088119","exception":false,"start_time":"2022-02-06T13:47:45.991517","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"これにより、Sanskar Hasija氏が作成したコードを利用して、隙間を含むエッセイをカラー印刷する機能を作ることができるようになりました。","metadata":{"papermill":{"duration":0.058607,"end_time":"2022-02-06T13:47:46.204803","exception":false,"start_time":"2022-02-06T13:47:46.146196","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def print_colored_essay(essay):\n    df_essay = add_gap_rows(essay)\n    #code from https://www.kaggle.com/odins0n/feedback-prize-eda, but adjusted to df_essay\n    essay_file = \"../input/feedback-prize-2021/train/\" + essay + \".txt\"\n\n    ents = []\n    for i, row in df_essay.iterrows():\n        ents.append({\n                        'start': int(row['discourse_start']), \n                         'end': int(row['discourse_end']), \n                         'label': row['discourse_type']\n                    })\n\n    with open(essay_file, 'r') as file: data = file.read()\n\n    doc2 = {\n        \"text\": data,\n        \"ents\": ents,\n    }\n\n    colors = {'Lead': '#EE11D0','Position': '#AB4DE1','Claim': '#1EDE71','Evidence': '#33FAFA','Counterclaim': '#4253C1','Concluding Statement': 'yellow','Rebuttal': 'red'}\n    options = {\"ents\": df_essay.discourse_type.unique().tolist(), \"colors\": colors}\n    spacy.displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True);","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:47:46.336185Z","iopub.status.busy":"2022-02-06T13:47:46.334964Z","iopub.status.idle":"2022-02-06T13:47:46.33727Z","shell.execute_reply":"2022-02-06T13:47:46.337916Z","shell.execute_reply.started":"2022-02-06T13:08:17.753218Z"},"papermill":{"duration":0.073031,"end_time":"2022-02-06T13:47:46.338095","exception":false,"start_time":"2022-02-06T13:47:46.265064","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_colored_essay(\"7330313ED3F0\")","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:47:46.459621Z","iopub.status.busy":"2022-02-06T13:47:46.458931Z","iopub.status.idle":"2022-02-06T13:47:46.48262Z","shell.execute_reply":"2022-02-06T13:47:46.483249Z","shell.execute_reply.started":"2022-02-06T13:08:17.765083Z"},"papermill":{"duration":0.086354,"end_time":"2022-02-06T13:47:46.483456","exception":false,"start_time":"2022-02-06T13:47:46.397102","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 談話タイプごとの最頻出語\n\n最初は、どの単語がよく使われているかを手作業で調べました。ストップワードを取り除き、すべてのテキストを小文字に変換し、句読点はそのまま残しました。また、各discourse_typeの図に散見される余分な単語を削除しました。このような作業を経て、これがどの程度有用なのか、よくわからなくなった。ひとつ気になるのは、「しかし、」がRebuttalで多用されていることだ。\n\nその後、全てのn_gramに対して一つの関数を作るのが良いと判断しました。もし、まだ、私が手作業で行った単一単語への取り組みに興味があれば、下のセルのコードを非表示にすることができます。","metadata":{"papermill":{"duration":0.05854,"end_time":"2022-02-06T13:47:46.600585","exception":false,"start_time":"2022-02-06T13:47:46.542045","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train['discourse_text'] = train['discourse_text'].str.lower()\n\n#get stopwords from nltk library\nstop_english = stopwords.words(\"english\")\nother_words_to_take_out = ['school', 'students', 'people', 'would', 'could', 'many']\nstop_english.extend(other_words_to_take_out)\n\n#put dataframe of Top-10 words in dict for all discourse types\ncounts_dict = {}\nfor dt in train['discourse_type'].unique():\n    df = train.query('discourse_type == @dt')\n    text = df.discourse_text.apply(lambda x: x.split()).tolist()\n    text = [item for elem in text for item in elem]\n    df1 = pd.Series(text).value_counts().to_frame().reset_index()\n    df1.columns = ['Word', 'Frequency']\n    df1 = df1[~df1.Word.isin(stop_english)].head(10)\n    df1 = df1.set_index(\"Word\").sort_values(by = \"Frequency\", ascending = True)\n    counts_dict[dt] = df1\n\nplt.figure(figsize=(15, 12))\nplt.subplots_adjust(hspace=0.5)\n\nkeys = list(counts_dict.keys())\n\nfor n, key in enumerate(keys):\n    ax = plt.subplot(4, 2, n + 1)\n    ax.set_title(f\"Most used words in {key}\")\n    counts_dict[keys[n]].plot(ax=ax, kind = 'barh')\n    plt.ylabel(\"\")\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-02-06T13:47:46.754795Z","iopub.status.busy":"2022-02-06T13:47:46.72494Z","iopub.status.idle":"2022-02-06T13:47:52.409877Z","shell.execute_reply":"2022-02-06T13:47:52.410426Z","shell.execute_reply.started":"2022-02-06T13:08:17.805743Z"},"papermill":{"duration":5.749522,"end_time":"2022-02-06T13:47:52.41061","exception":false,"start_time":"2022-02-06T13:47:46.661088","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 談話の種類ごとにn_gramを作成する\n\n上記の手作業の後、結果に満足しきれなかったので、CountVectorizer()を使って、割引の種類ごとにTop-10のn_gramを合成する関数を作りたいと思いました。この関数は、単一単語の場合にも使えるはずです（n_grams =1で実行すればよい）。","metadata":{"papermill":{"duration":0.06137,"end_time":"2022-02-06T13:47:52.534893","exception":false,"start_time":"2022-02-06T13:47:52.473523","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_n_grams(n_grams, top_n = 10):\n    df_words = pd.DataFrame()\n    for dt in tqdm(train['discourse_type'].unique()):\n        df = train.query('discourse_type == @dt')\n        texts = df['discourse_text'].tolist()\n        vec = CountVectorizer(lowercase = True, stop_words = 'english',\\\n                              ngram_range=(n_grams, n_grams)).fit(texts)\n        bag_of_words = vec.transform(texts)\n        sum_words = bag_of_words.sum(axis=0)\n        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n        cvec_df = pd.DataFrame.from_records(words_freq,\\\n                                            columns= ['words', 'counts']).sort_values(by=\"counts\", ascending=False)\n        cvec_df.insert(0, \"Discourse_type\", dt)\n        cvec_df = cvec_df.iloc[:top_n,:]\n        df_words = df_words.append(cvec_df)\n    return df_words","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:47:52.665103Z","iopub.status.busy":"2022-02-06T13:47:52.664408Z","iopub.status.idle":"2022-02-06T13:47:52.67466Z","shell.execute_reply":"2022-02-06T13:47:52.674036Z","shell.execute_reply.started":"2022-02-06T13:08:23.425982Z"},"papermill":{"duration":0.076141,"end_time":"2022-02-06T13:47:52.674833","exception":false,"start_time":"2022-02-06T13:47:52.598692","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"この関数は、70行からなる1つのデータフレームを返す（各談話タイプで最も使用された上位10個のn-gram）。","metadata":{"papermill":{"duration":0.062281,"end_time":"2022-02-06T13:47:52.799073","exception":false,"start_time":"2022-02-06T13:47:52.736792","status":"completed"},"tags":[]}},{"cell_type":"code","source":"bigrams = get_n_grams(n_grams = 2, top_n=10)\nbigrams.head()","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:47:52.936697Z","iopub.status.busy":"2022-02-06T13:47:52.935929Z","iopub.status.idle":"2022-02-06T13:48:18.423575Z","shell.execute_reply":"2022-02-06T13:48:18.42289Z","shell.execute_reply.started":"2022-02-06T13:08:23.441806Z"},"papermill":{"duration":25.563253,"end_time":"2022-02-06T13:48:18.423732","exception":false,"start_time":"2022-02-06T13:47:52.860479","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"以下、このデータフレーム内の結果をサブプロットとして出力する関数も作ってみました。","metadata":{"papermill":{"duration":0.063593,"end_time":"2022-02-06T13:48:18.552695","exception":false,"start_time":"2022-02-06T13:48:18.489102","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def plot_ngram(df, type = \"bigrams\"):\n    plt.figure(figsize=(15, 12))\n    plt.subplots_adjust(hspace=0.5)\n\n    for n, dt in enumerate(df.Discourse_type.unique()):\n        ax = plt.subplot(4, 2, n + 1)\n        ax.set_title(f\"Most used {type} in {dt}\")\n        data = df.query('Discourse_type == @dt')[['words', 'counts']].set_index(\"words\").sort_values(by = \"counts\", ascending = True)\n        data.plot(ax=ax, kind = 'barh')\n        plt.ylabel(\"\")\n    plt.tight_layout()\n    plt.show()\n    \nplot_ngram(bigrams)","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:48:18.711235Z","iopub.status.busy":"2022-02-06T13:48:18.710511Z","iopub.status.idle":"2022-02-06T13:48:20.300107Z","shell.execute_reply":"2022-02-06T13:48:20.300636Z","shell.execute_reply.started":"2022-02-06T13:08:47.612558Z"},"papermill":{"duration":1.684094,"end_time":"2022-02-06T13:48:20.300811","exception":false,"start_time":"2022-02-06T13:48:18.616717","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"下は、両方の関数を使った三角波も一度にプロットしています。","metadata":{"execution":{"iopub.execute_input":"2021-12-28T14:49:19.545062Z","iopub.status.busy":"2021-12-28T14:49:19.543924Z","iopub.status.idle":"2021-12-28T14:49:21.441599Z","shell.execute_reply":"2021-12-28T14:49:21.4376Z","shell.execute_reply.started":"2021-12-28T14:49:19.545018Z"},"papermill":{"duration":0.069866,"end_time":"2022-02-06T13:48:20.440993","exception":false,"start_time":"2022-02-06T13:48:20.371127","status":"completed"},"tags":[]}},{"cell_type":"code","source":"trigrams = get_n_grams(n_grams = 3, top_n=10)\nplot_ngram(trigrams, type = \"trigrams\")","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:48:20.596556Z","iopub.status.busy":"2022-02-06T13:48:20.584981Z","iopub.status.idle":"2022-02-06T13:48:53.132044Z","shell.execute_reply":"2022-02-06T13:48:53.132716Z","shell.execute_reply.started":"2022-02-06T13:08:49.240635Z"},"papermill":{"duration":32.621724,"end_time":"2022-02-06T13:48:53.132923","exception":false,"start_time":"2022-02-06T13:48:20.511199","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NERの紹介\n\n名前付き固有表現認識（NER）は、このチャレンジに最適な技術です。これに関する詳細な情報をお探しの場合は、Hugging Faceの無料講座が強く推奨されます。Token Classification](https://huggingface.co/course/chapter7/2?fw=pt)のセクションでは、ここに関連する以下の事柄を見つけることができます。\n- 名前付き実体認識(NER)。文中の実体（人物、場所、組織など）を見つける。これは、エンティティごとに1クラス、\"エンティティなし \"に1クラスを用意し、各トークンにラベルを帰属させるという形で定式化できる。\n- チャンキングを行う。同じエンティティに属するトークンを探す。このタスク（品詞推定やNERと組み合わせることができる）は、チャンクの先頭にあるトークンには1つのラベル（通常B-）を、チャンクの内部にあるトークンには別のラベル（通常I-）を、どのチャンクにも属さないトークンには第3のラベル（通常O）を付けると定式化することができる。\n\n基本的に、このコンペティションで使用されているのはNER Chunkingです。Darek Kłeczekがこの背後にある考え方を説明する素晴らしいノートブックを書いています（ぜひupvoteしてください！）。[Visual Tutorial NER Chunking Token Classification](https://www.kaggle.com/thedrcat/visual-tutorial-ner-chunking-token-classification).\n\nこのセクションでは、このコンペティションのために、これらのNERラベルをどのように作ることができるかを示すだけです。基本的にはChris Deotteの素晴らしいノートブック [PyTorch - BigBird - NER - [CV 0.615]](https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615) (Please upvote his notebook!) にあるループを使っていますが、少しわかりやすくしてみました。また、df.iterrowsの代わりにdf.locを使っています。\n\nまず、エッセイの全文が入ったデータフレームを作ります。","metadata":{"papermill":{"duration":0.075665,"end_time":"2022-02-06T13:48:53.286223","exception":false,"start_time":"2022-02-06T13:48:53.210558","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\ntest_names, train_texts = [], []\nfor f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n    test_names.append(f.replace('.txt', ''))\n    train_texts.append(open('../input/feedback-prize-2021/train/' + f, 'r').read())\ntrain_text_df = pd.DataFrame({'id': test_names, 'text': train_texts})\ntrain_text_df.head()","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:48:53.446351Z","iopub.status.busy":"2022-02-06T13:48:53.445281Z","iopub.status.idle":"2022-02-06T13:49:03.334476Z","shell.execute_reply":"2022-02-06T13:49:03.33498Z","shell.execute_reply.started":"2022-02-06T13:09:20.85507Z"},"papermill":{"duration":9.970722,"end_time":"2022-02-06T13:49:03.335172","exception":false,"start_time":"2022-02-06T13:48:53.36445","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"これで、このデータフレームにNERエンティティを含む列を追加する準備が整いました。","metadata":{"papermill":{"duration":0.085572,"end_time":"2022-02-06T13:49:03.515979","exception":false,"start_time":"2022-02-06T13:49:03.430407","status":"completed"},"tags":[]}},{"cell_type":"code","source":"all_entities = []\n#loop over dataframe with all full texts\nfor i in tqdm(range(len(train_text_df))):\n    total = len(train_text_df.loc[i, 'text'].split())\n    #now a list with length the total number of words in an essay is initialised with all values being \"O\"\n    entities = [\"O\"]*total\n    #now loop over dataframe with all discourses of this particular essay\n    discourse_id = train_text_df.loc[i, 'id']\n    train_df_id = train.query('id == @discourse_id').reset_index(drop=True)\n    for j in range(len(train_df_id)):\n        discourse = train_df_id.loc[j, 'discourse_type']\n        #make a list with the position numbers in predictionstring converted into integer\n        list_ix = [int(x) for x in train_df_id.loc[j, 'predictionstring'].split(' ')]\n        #now the entities lists gets overwritten where there are discourse identified by the experts\n        #the first word of each discourse gets prefix \"Beginning\"\n        entities[list_ix[0]] = f\"B-{discourse}\"\n        #the other ones get prefix I\n        for k in list_ix[1:]: entities[k] = f\"I-{discourse}\"\n    all_entities.append(entities)\n    \n    \ntrain_text_df['entities'] = all_entities","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:49:03.697014Z","iopub.status.busy":"2022-02-06T13:49:03.688508Z","iopub.status.idle":"2022-02-06T13:51:36.725708Z","shell.execute_reply":"2022-02-06T13:51:36.726314Z","shell.execute_reply.started":"2022-02-06T13:09:28.324801Z"},"papermill":{"duration":153.131048,"end_time":"2022-02-06T13:51:36.72673","exception":false,"start_time":"2022-02-06T13:49:03.595682","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_text_df.head()","metadata":{"execution":{"iopub.execute_input":"2022-02-06T13:51:36.88841Z","iopub.status.busy":"2022-02-06T13:51:36.887386Z","iopub.status.idle":"2022-02-06T13:51:36.905134Z","shell.execute_reply":"2022-02-06T13:51:36.905633Z","shell.execute_reply.started":"2022-02-06T13:12:05.470089Z"},"papermill":{"duration":0.101022,"end_time":"2022-02-06T13:51:36.905818","exception":false,"start_time":"2022-02-06T13:51:36.804796","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}