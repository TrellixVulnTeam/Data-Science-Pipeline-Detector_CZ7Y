{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Feedback Prize EDA\n\nNLPファンの皆さん、今頃またNLPのコンペがあるとは思いませんでしたが、いかがでしょうか？今回はどんなものか見てみましょう :) まずはデータを見て、それから主催者からの情報を見ていく、この順番が私にはベストです :) ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom pathlib import Path\nimport spacy\nfrom spacy import displacy\nfrom pylab import cm, matplotlib","metadata":{"execution":{"iopub.status.busy":"2022-02-04T22:12:29.372259Z","iopub.execute_input":"2022-02-04T22:12:29.372814Z","iopub.status.idle":"2022-02-04T22:12:40.701893Z","shell.execute_reply.started":"2022-02-04T22:12:29.372721Z","shell.execute_reply":"2022-02-04T22:12:40.701063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### トレーニングデータ\n\nテキストファイルの束と、ラベルを含む別の `train.csv` があるように見えます。まず、csvを見てみましょう。テキストファイルへの参照と、特定の **談話タイプ** を示すスパンを持つテキストファイルごとの複数行が得られます。以下では、談話のユニークなタイプも見てみましょう。","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/feedback-prize-2021/train.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T22:12:40.703761Z","iopub.execute_input":"2022-02-04T22:12:40.704848Z","iopub.status.idle":"2022-02-04T22:12:42.955907Z","shell.execute_reply.started":"2022-02-04T22:12:40.704791Z","shell.execute_reply":"2022-02-04T22:12:42.955163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.discourse_type.unique().tolist()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T22:12:42.957991Z","iopub.execute_input":"2022-02-04T22:12:42.958644Z","iopub.status.idle":"2022-02-04T22:12:42.986645Z","shell.execute_reply.started":"2022-02-04T22:12:42.958593Z","shell.execute_reply":"2022-02-04T22:12:42.985707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `displacy` による可視化\n\ncsvファイルからテキストファイルにスパンを重ね合わせることができれば、最も効果的だと思います。Spacyには、このための素晴らしいビジュアライザーがあります。displacy`です。これを使って、1つの例がどのように見えるか見てみましょう!","metadata":{}},{"cell_type":"code","source":"path = Path('../input/feedback-prize-2021/train')\n\ncolors = {\n            'Lead': '#8000ff',\n            'Position': '#2b7ff6',\n            'Evidence': '#2adddd',\n            'Claim': '#80ffb4',\n            'Concluding Statement': 'd4dd80',\n            'Counterclaim': '#ff8042',\n            'Rebuttal': '#ff0000'\n         }\n\ndef visualize(example):\n    ents = []\n    for i, row in train[train['id'] == example].iterrows():\n        ents.append({\n                        'start': int(row['discourse_start']), \n                         'end': int(row['discourse_end']), \n                         'label': row['discourse_type']\n                    })\n\n    with open(path/f'{example}.txt', 'r') as file: data = file.read()\n\n    doc2 = {\n        \"text\": data,\n        \"ents\": ents,\n        \"title\": example\n    }\n\n    options = {\"ents\": train.discourse_type.unique().tolist(), \"colors\": colors}\n    displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-04T22:12:42.989Z","iopub.execute_input":"2022-02-04T22:12:42.989308Z","iopub.status.idle":"2022-02-04T22:12:42.999857Z","shell.execute_reply.started":"2022-02-04T22:12:42.989268Z","shell.execute_reply":"2022-02-04T22:12:42.998931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"examples = train['id'].sample(n=5, random_state=42).values.tolist()\n\nfor ex in examples:\n    visualize(ex)\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T22:12:43.001228Z","iopub.execute_input":"2022-02-04T22:12:43.001502Z","iopub.status.idle":"2022-02-04T22:12:43.261854Z","shell.execute_reply.started":"2022-02-04T22:12:43.001462Z","shell.execute_reply":"2022-02-04T22:12:43.26076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### インサイト\n\nなるほど～、これは面白そうだ！」と思いました。\n- いくつかの例では、テキスト全体が異なるカテゴリのスパンに密に分割されています。他の例では、アノテーターがいくつかの単語を省略し、分割が非常に主観的に見える。これは、アノテーションがノイズになっている可能性を示す指標です。\n- リードで始まり、主張と証拠を混ぜ、結論の文で終わるという順序が重要なようです。これをモデルに取り入れる必要があるかもしれない。\n- 同じクラスで2つのスパンが隣り合っている場合がある - 分離することが重要でしょう\n\n### テキストの長さ\n\nもう一つの重要なデータポイントは、テキストの長さです。彼らは`roberta`のようなモデルに適合するでしょうか？確認してみましょう","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('roberta-base')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T22:12:43.263981Z","iopub.execute_input":"2022-02-04T22:12:43.264313Z","iopub.status.idle":"2022-02-04T22:12:52.861039Z","shell.execute_reply.started":"2022-02-04T22:12:43.264267Z","shell.execute_reply":"2022-02-04T22:12:52.860047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = []\n\nids = train.sample(n=500)['id'].unique().tolist()\nlen(ids)\n\nfor example in ids:\n    with open(path/f'{example}.txt', 'r') as file: data = file.read()\n    texts.append({\n        'text': data,\n        'n_tokens': len(tokenizer(data)['input_ids'])\n    })\n     \ndf = pd.DataFrame(texts)\n\nprint(len(df[df.n_tokens > 512])/len(df))\n\ndf.n_tokens.hist();","metadata":{"execution":{"iopub.status.busy":"2022-02-04T22:12:52.864104Z","iopub.execute_input":"2022-02-04T22:12:52.864585Z","iopub.status.idle":"2022-02-04T22:12:56.656104Z","shell.execute_reply.started":"2022-02-04T22:12:52.864412Z","shell.execute_reply":"2022-02-04T22:12:56.655143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**重要** 半数以上のテキストが標準的な最大長である512トークンを超えているため、スライディングウィンドウやその他のチャンキングアプローチをデータに適用する必要がありそうです。","metadata":{}},{"cell_type":"markdown","source":"### テキストのスパン数\n\nまた、1つのテキストに何個のスパンがあるか見てみましょう。また、スパンが最も少ないテキストと最も多いテキストを可視化してみましょう。","metadata":{}},{"cell_type":"code","source":"dist = train.groupby('id')['discourse_type'].apply(lambda x: len(list(x)))\n\nprint(f'Min: {dist.min()}')\nprint(f'Max: {dist.max()}')\n\ndist.hist();","metadata":{"execution":{"iopub.status.busy":"2022-02-04T22:20:27.055006Z","iopub.execute_input":"2022-02-04T22:20:27.055306Z","iopub.status.idle":"2022-02-04T22:20:27.649722Z","shell.execute_reply.started":"2022-02-04T22:20:27.055272Z","shell.execute_reply":"2022-02-04T22:20:27.648147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dist.sort_values()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T22:21:28.825862Z","iopub.execute_input":"2022-02-04T22:21:28.826186Z","iopub.status.idle":"2022-02-04T22:21:28.842246Z","shell.execute_reply.started":"2022-02-04T22:21:28.826149Z","shell.execute_reply":"2022-02-04T22:21:28.841513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize('FFFF80B8CC2F')\nvisualize('FC7A3692794B')\nvisualize('149E8C278863')\nvisualize('71259B3EA87F')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T22:23:33.742711Z","iopub.execute_input":"2022-02-04T22:23:33.742985Z","iopub.status.idle":"2022-02-04T22:23:33.945877Z","shell.execute_reply.started":"2022-02-04T22:23:33.742954Z","shell.execute_reply":"2022-02-04T22:23:33.944857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### テスト","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv('../input/feedback-prize-2021/sample_submission.csv')\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T22:12:56.657935Z","iopub.execute_input":"2022-02-04T22:12:56.658203Z","iopub.status.idle":"2022-02-04T22:12:56.674205Z","shell.execute_reply.started":"2022-02-04T22:12:56.658172Z","shell.execute_reply":"2022-02-04T22:12:56.673304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"なるほど、テストでも同じような形式のファイルを取得し、クラスと予測文字列を予測する必要があるようですね。あとは、ホストから提供される情報を見て、理解を深めるしかないですね :) \n\n> 単語のインデックスは、Pythonの.split()関数を使って計算し、結果のリストのインデックスを取ります。2つのオーバーラップは、グランドトゥルース/予測ペアのインデックスの各リストのset()を取り、各セットの長さで割った2つのセットの交点を計算することによって算出されます。\n\nつまり、`predictionstring`フィールドに予測スパンに含まれる単語のインデックスを提供する必要があるということだと思います。","metadata":{}},{"cell_type":"markdown","source":"## ベースライン\n\nこれは面白いタスクだ! いくつかの方法でアプローチすることができます。\n1. テキストを文単位で分割し、各文を分類する。\n2. NER：個々のトークン（単語）を分類する。\n3. 質問応答：談話の種類を質問として使用し、回答スパンを予測する。\n\nそれぞれのアプローチには、デメリットや複雑な点がありますが...。ベースラインを構築するのに十分なデータが揃ったと思います。別のノートブックでベースラインを作成し、後でこのEDAに戻ることにします。","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}