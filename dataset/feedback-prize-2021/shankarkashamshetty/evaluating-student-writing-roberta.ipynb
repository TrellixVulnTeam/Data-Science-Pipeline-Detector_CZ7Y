{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom tqdm import tqdm \n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-16T08:11:12.134055Z","iopub.execute_input":"2022-01-16T08:11:12.134642Z","iopub.status.idle":"2022-01-16T08:11:15.143497Z","shell.execute_reply.started":"2022-01-16T08:11:12.134543Z","shell.execute_reply":"2022-01-16T08:11:15.142783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n# DECLARE HOW MANY GPUS YOU WISH TO USE. \n# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n\n# VERSION FOR SAVING MODEL WEIGHTS\nVER=26\n\n# IF VARIABLE IS NONE, THEN NOTEBOOK COMPUTES TOKENS\n# OTHERWISE NOTEBOOK LOADS TOKENS FROM PATH\n#LOAD_TOKENS_FROM = '../input/bigbirdrobertamodel'\nLOAD_TOKENS_FROM = None\n\n# IF VARIABLE IS NONE, THEN NOTEBOOK TRAINS A NEW MODEL\n# OTHERWISE IT LOADS YOUR PREVIOUSLY TRAINED MODEL\n#LOAD_MODEL_FROM = '../input/bigbirdrobertamodel'\nLOAD_MODEL_FROM = None\n\n# IF FOLLOWING IS NONE, THEN NOTEBOOK \n# USES INTERNET AND DOWNLOADS HUGGINGFACE \n# CONFIG, TOKENIZER, AND MODEL\nDOWNLOADED_MODEL_PATH = '../input/bigbirdrobertamodel' \n\nif DOWNLOADED_MODEL_PATH is None:\n    DOWNLOADED_MODEL_PATH = 'model'    \nMODEL_NAME = 'google/bigbird-roberta-base'","metadata":{"execution":{"iopub.status.busy":"2022-01-16T13:20:46.380469Z","iopub.execute_input":"2022-01-16T13:20:46.380739Z","iopub.status.idle":"2022-01-16T13:20:46.388584Z","shell.execute_reply.started":"2022-01-16T13:20:46.38071Z","shell.execute_reply":"2022-01-16T13:20:46.387929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import cuda\nconfig = {'model_name': MODEL_NAME,   \n         'max_length': 1024,\n         'train_batch_size':4,\n         'valid_batch_size':4,\n         'epochs':5,\n         'learning_rates': [2.5e-5, 2.5e-5, 2.5e-6, 2.5e-6, 2.5e-7],\n         'max_grad_norm':10,\n         'device': 'cuda' if cuda.is_available() else 'cpu'}\n\n# THIS WILL COMPUTE VAL SCORE DURING COMMIT BUT NOT DURING SUBMIT\nCOMPUTE_VAL_SCORE = True\nif len( os.listdir('../input/feedback-prize-2021/test') )>5:\n      COMPUTE_VAL_SCORE = False","metadata":{"execution":{"iopub.status.busy":"2022-01-16T12:44:36.417802Z","iopub.execute_input":"2022-01-16T12:44:36.418497Z","iopub.status.idle":"2022-01-16T12:44:36.42775Z","shell.execute_reply.started":"2022-01-16T12:44:36.41846Z","shell.execute_reply":"2022-01-16T12:44:36.426988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import *\nif DOWNLOADED_MODEL_PATH == 'model':\n    os.mkdir('model')\n    \n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n    tokenizer.save_pretrained('model')\n\n    config_model = AutoConfig.from_pretrained(MODEL_NAME) \n    config_model.num_labels = 15\n    config_model.save_pretrained('model')\n\n    backbone = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, \n                                                               config=config_model)\n    backbone.save_pretrained('model')","metadata":{"execution":{"iopub.status.busy":"2022-01-16T12:45:08.172687Z","iopub.execute_input":"2022-01-16T12:45:08.172977Z","iopub.status.idle":"2022-01-16T12:45:12.168393Z","shell.execute_reply.started":"2022-01-16T12:45:08.172938Z","shell.execute_reply":"2022-01-16T12:45:12.167504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data and Libraries\nIn addition to loading the train dataframe, we will load all the train and text files and save them in a dataframe.","metadata":{}},{"cell_type":"code","source":"import numpy as np, os \nimport pandas as pd, gc \nfrom tqdm import tqdm\n\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2022-01-16T12:47:05.868547Z","iopub.execute_input":"2022-01-16T12:47:05.868863Z","iopub.status.idle":"2022-01-16T12:47:05.875492Z","shell.execute_reply.started":"2022-01-16T12:47:05.868825Z","shell.execute_reply":"2022-01-16T12:47:05.87461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/feedback-prize-2021/train.csv')\nprint(train_df.shape)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T12:48:13.086413Z","iopub.execute_input":"2022-01-16T12:48:13.086694Z","iopub.status.idle":"2022-01-16T12:48:13.905281Z","shell.execute_reply.started":"2022-01-16T12:48:13.08666Z","shell.execute_reply":"2022-01-16T12:48:13.904422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_path = '/kaggle/input/feedback-prize-2021/'","metadata":{"execution":{"iopub.status.busy":"2022-01-16T12:48:43.01457Z","iopub.execute_input":"2022-01-16T12:48:43.015363Z","iopub.status.idle":"2022-01-16T12:48:43.020463Z","shell.execute_reply.started":"2022-01-16T12:48:43.01532Z","shell.execute_reply":"2022-01-16T12:48:43.01963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_names, test_texts = [],[]\nfor f in tqdm(os.listdir(base_path + 'test/')):\n    test_names.append(f.replace(\".txt\",\"\"))\n    test_texts.append(open(base_path+\"test/\"+f , 'r').read())\ntest_text_df = pd.DataFrame({\"id\":test_names,'text':test_texts})\ntest_text_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T12:49:11.387782Z","iopub.execute_input":"2022-01-16T12:49:11.388343Z","iopub.status.idle":"2022-01-16T12:49:11.412204Z","shell.execute_reply.started":"2022-01-16T12:49:11.388299Z","shell.execute_reply":"2022-01-16T12:49:11.411464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_names, train_texts = [],[]\nfor f in tqdm(os.listdir(base_path + \"train/\")):\n    train_names.append(f.replace(\".txt\",\"\"))\n    train_texts.append(open(base_path + \"train/\" + f, 'r').read())\ntrain_text_df = pd.DataFrame({\"id\":train_names,'text':train_texts})\ntrain_text_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T12:49:26.924503Z","iopub.execute_input":"2022-01-16T12:49:26.924775Z","iopub.status.idle":"2022-01-16T12:49:33.19388Z","shell.execute_reply.started":"2022-01-16T12:49:26.924744Z","shell.execute_reply":"2022-01-16T12:49:33.193104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not LOAD_TOKENS_FROM:\n    all_entities = []\n    for ii,i in enumerate(train_text_df.iterrows()):\n        if ii%100==0: print(ii,', ',end='')\n        total = i[1]['text'].split().__len__()\n        entities = [\"O\"]*total\n        for j in train_df[train_df['id'] == i[1]['id']].iterrows():\n            discourse = j[1]['discourse_type']\n            list_ix = [int(x) for x in j[1]['predictionstring'].split(' ')]\n            entities[list_ix[0]] = f\"B-{discourse}\"\n            for k in list_ix[1:]: entities[k] = f\"I-{discourse}\"\n        all_entities.append(entities)\n    train_text_df['entities'] = all_entities\n    train_text_df.to_csv('train_NER.csv',index=False)\n    \nelse:\n    from ast import literal_eval\n    train_text_df = pd.read_csv(f'{LOAD_TOKENS_FROM}/train_NER.csv')\n    # pandas saves lists as string, we must convert back\n    train_text_df.entities = train_text_df.entities.apply(lambda x: literal_eval(x) )\n    \nprint( train_text_df.shape )\ntrain_text_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T12:54:31.432561Z","iopub.execute_input":"2022-01-16T12:54:31.432828Z","iopub.status.idle":"2022-01-16T13:00:18.480693Z","shell.execute_reply.started":"2022-01-16T12:54:31.432798Z","shell.execute_reply":"2022-01-16T13:00:18.479906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n\nlabels_to_ids = {v:k for k,v in enumerate(output_labels)}\nids_to_labels = {k:v for k,v in enumerate(output_labels)}","metadata":{"execution":{"iopub.status.busy":"2022-01-16T13:01:45.259895Z","iopub.execute_input":"2022-01-16T13:01:45.26067Z","iopub.status.idle":"2022-01-16T13:01:45.265523Z","shell.execute_reply.started":"2022-01-16T13:01:45.260627Z","shell.execute_reply":"2022-01-16T13:01:45.264873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_to_ids","metadata":{"execution":{"iopub.status.busy":"2022-01-16T13:01:54.612678Z","iopub.execute_input":"2022-01-16T13:01:54.613032Z","iopub.status.idle":"2022-01-16T13:01:54.626528Z","shell.execute_reply.started":"2022-01-16T13:01:54.612984Z","shell.execute_reply":"2022-01-16T13:01:54.625058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LABEL_ALL_SUBTOKENS = True\n\nclass dataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len, get_wids):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.get_wids = get_wids # for validation\n\n    def __getitem__(self, index):\n        # GET TEXT AND WORD LABELS \n        text = self.data.text[index]        \n        word_labels = self.data.entities[index] if not self.get_wids else None\n\n        # TOKENIZE TEXT\n        encoding = self.tokenizer(text.split(),\n                             is_split_into_words=True,\n                             #return_offsets_mapping=True, \n                             padding='max_length', \n                             truncation=True, \n                             max_length=self.max_len)\n        word_ids = encoding.word_ids()  \n        \n        # CREATE TARGETS\n        if not self.get_wids:\n            previous_word_idx = None\n            label_ids = []\n            for word_idx in word_ids:                            \n                if word_idx is None:\n                    label_ids.append(-100)\n                elif word_idx != previous_word_idx:              \n                    label_ids.append( labels_to_ids[word_labels[word_idx]] )\n                else:\n                    if LABEL_ALL_SUBTOKENS:\n                        label_ids.append( labels_to_ids[word_labels[word_idx]] )\n                    else:\n                        label_ids.append(-100)\n                previous_word_idx = word_idx\n            encoding['labels'] = label_ids\n\n        # CONVERT TO TORCH TENSORS\n        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n        if self.get_wids: \n            word_ids2 = [w if w is not None else -1 for w in word_ids]\n            item['wids'] = torch.as_tensor(word_ids2)\n        \n        return item\n\n    def __len__(self):\n        return self.len\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-16T13:07:34.944612Z","iopub.execute_input":"2022-01-16T13:07:34.944892Z","iopub.status.idle":"2022-01-16T13:07:34.958155Z","shell.execute_reply.started":"2022-01-16T13:07:34.944861Z","shell.execute_reply":"2022-01-16T13:07:34.957055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CHOOSE VALIDATION INDEXES (that match my TF notebook)\nIDS = train_df.id.unique()\nprint('There are',len(IDS),'train texts. We will split 90% 10% for validation.')\n\n# TRAIN VALID SPLIT 90% 10%\nnp.random.seed(42)\ntrain_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\nvalid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\nnp.random.seed(None)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T13:07:40.752231Z","iopub.execute_input":"2022-01-16T13:07:40.752499Z","iopub.status.idle":"2022-01-16T13:07:40.777955Z","shell.execute_reply.started":"2022-01-16T13:07:40.75247Z","shell.execute_reply":"2022-01-16T13:07:40.776795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CREATE TRAIN SUBSET AND VALID SUBSET\ndata = train_text_df[['id','text', 'entities']]\ntrain_dataset = data.loc[data['id'].isin(IDS[train_idx]),['text', 'entities']].reset_index(drop=True)\ntest_dataset = data.loc[data['id'].isin(IDS[valid_idx])].reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(data.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\ntokenizer = AutoTokenizer.from_pretrained(DOWNLOADED_MODEL_PATH) \ntraining_set = dataset(train_dataset, tokenizer, config['max_length'], False)\ntesting_set = dataset(test_dataset, tokenizer, config['max_length'], True)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T13:07:42.084472Z","iopub.execute_input":"2022-01-16T13:07:42.085119Z","iopub.status.idle":"2022-01-16T13:07:42.242485Z","shell.execute_reply.started":"2022-01-16T13:07:42.085078Z","shell.execute_reply":"2022-01-16T13:07:42.241719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/raghavendrakotala/fine-tunned-on-roberta-base-as-ner-problem-0-533\ndef train(epoch):\n    tr_loss, tr_accuracy = 0, 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    #tr_preds, tr_labels = [], []\n    \n    # put model in training mode\n    model.train()\n    \n    for idx, batch in enumerate(training_loader):\n        \n        ids = batch['input_ids'].to(config['device'], dtype = torch.long)\n        mask = batch['attention_mask'].to(config['device'], dtype = torch.long)\n        labels = batch['labels'].to(config['device'], dtype = torch.long)\n\n        loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels,\n                               return_dict=False)\n        tr_loss += loss.item()\n\n        nb_tr_steps += 1\n        nb_tr_examples += labels.size(0)\n        \n        if idx % 200==0:\n            loss_step = tr_loss/nb_tr_steps\n            print(f\"Training loss after {idx:04d} training steps: {loss_step}\")\n           \n        # compute training accuracy\n        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n        \n        # only compute accuracy at active labels\n        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n        \n        labels = torch.masked_select(flattened_targets, active_accuracy)\n        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n        \n        #tr_labels.extend(labels)\n        #tr_preds.extend(predictions)\n\n        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n        tr_accuracy += tmp_tr_accuracy\n    \n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(\n            parameters=model.parameters(), max_norm=config['max_grad_norm']\n        )\n        \n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    epoch_loss = tr_loss / nb_tr_steps\n    tr_accuracy = tr_accuracy / nb_tr_steps\n    print(f\"Training loss epoch: {epoch_loss}\")\n    print(f\"Training accuracy epoch: {tr_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-16T13:09:08.61258Z","iopub.execute_input":"2022-01-16T13:09:08.612868Z","iopub.status.idle":"2022-01-16T13:09:08.626007Z","shell.execute_reply.started":"2022-01-16T13:09:08.612833Z","shell.execute_reply":"2022-01-16T13:09:08.624093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CREATE MODEL\nconfig_model = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json') \nmodel = AutoModelForTokenClassification.from_pretrained(\n                   DOWNLOADED_MODEL_PATH+'/pytorch_model.bin',config=config_model)\nmodel.to(config['device'])\noptimizer = torch.optim.Adam(params=model.parameters(), lr=config['learning_rates'][0])","metadata":{"execution":{"iopub.status.busy":"2022-01-16T13:09:31.406916Z","iopub.execute_input":"2022-01-16T13:09:31.407554Z","iopub.status.idle":"2022-01-16T13:09:33.211688Z","shell.execute_reply.started":"2022-01-16T13:09:31.407514Z","shell.execute_reply":"2022-01-16T13:09:33.210878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LOOP TO TRAIN MODEL (or load model)\nif not LOAD_MODEL_FROM:\n    for epoch in range(config['epochs']):\n        \n        print(f\"### Training epoch: {epoch + 1}\")\n        for g in optimizer.param_groups: \n            g['lr'] = config['learning_rates'][epoch]\n        lr = optimizer.param_groups[0]['lr']\n        print(f'### LR = {lr}\\n')\n        \n        train(epoch)\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n    torch.save(model.state_dict(), f'bigbird_v{VER}.pt')\nelse:\n    model.load_state_dict(torch.load(f'{LOAD_MODEL_FROM}/bigbird_v{VER}.pt'))\n    print('Model loaded.')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-16T14:37:16.611888Z","iopub.execute_input":"2022-01-16T14:37:16.61259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference and Validation Code","metadata":{}},{"cell_type":"code","source":"def inference(batch):\n                \n    # MOVE BATCH TO GPU AND INFER\n    ids = batch[\"input_ids\"].to(config['device'])\n    mask = batch[\"attention_mask\"].to(config['device'])\n    outputs = model(ids, attention_mask=mask, return_dict=False)\n    all_preds = torch.argmax(outputs[0], axis=-1).cpu().numpy() \n\n    # INTERATE THROUGH EACH TEXT AND GET PRED\n    predictions = []\n    for k,text_preds in enumerate(all_preds):\n        token_preds = [ids_to_labels[i] for i in text_preds]\n\n        prediction = []\n        word_ids = batch['wids'][k].numpy()  \n        previous_word_idx = -1\n        for idx,word_idx in enumerate(word_ids):                            \n            if word_idx == -1:\n                pass\n            elif word_idx != previous_word_idx:              \n                prediction.append(token_preds[idx])\n                previous_word_idx = word_idx\n        predictions.append(prediction)\n    \n    return predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/zzy990106/pytorch-ner-infer\n# code has been modified from original\ndef get_predictions(df=test_dataset, loader=testing_loader):\n    \n    # put model in training mode\n    model.eval()\n    \n    # GET WORD LABEL PREDICTIONS\n    y_pred2 = []\n    for batch in loader:\n        labels = inference(batch)\n        y_pred2.extend(labels)\n\n    final_preds2 = []\n    for i in range(len(df)):\n\n        idx = df.id.values[i]\n        #pred = [x.replace('B-','').replace('I-','') for x in y_pred2[i]]\n        pred = y_pred2[i] # Leave \"B\" and \"I\"\n        preds = []\n        j = 0\n        while j < len(pred):\n            cls = pred[j]\n            if cls == 'O': j += 1\n            else: cls = cls.replace('B','I') # spans start with B\n            end = j + 1\n            while end < len(pred) and pred[end] == cls:\n                end += 1\n            \n            if cls != 'O' and cls != '' and end - j > 7:\n                final_preds2.append((idx, cls.replace('I-',''),\n                                     ' '.join(map(str, list(range(j, end))))))\n        \n            j = end\n        \n    oof = pd.DataFrame(final_preds2)\n    oof.columns = ['id','class','predictionstring']\n\n    return oof","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(' '))\n    set_gt = set(row.predictionstring_gt.split(' '))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','class','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on=['id','class'],\n                           right_on=['id','discourse_type'],\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    my_f1_score = TP / (TP + 0.5*(FP+FN))\n    return my_f1_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_VAL_SCORE: # note this doesn't run during submit\n    # VALID TARGETS\n    valid = train_df.loc[train_df['id'].isin(IDS[valid_idx])]\n\n    # OOF PREDICTIONS\n    oof = get_predictions(test_dataset, testing_loader)\n\n    # COMPUTE F1 SCORE\n    f1s = []\n    CLASSES = oof['class'].unique()\n    print()\n    for c in CLASSES:\n        pred_df = oof.loc[oof['class']==c].copy()\n        gt_df = valid.loc[valid['discourse_type']==c].copy()\n        f1 = score_feedback_comp(pred_df, gt_df)\n        print(c,f1)\n        f1s.append(f1)\n    print()\n    print('Overall',np.mean(f1s))\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = get_predictions(test_texts, test_texts_loader)\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}