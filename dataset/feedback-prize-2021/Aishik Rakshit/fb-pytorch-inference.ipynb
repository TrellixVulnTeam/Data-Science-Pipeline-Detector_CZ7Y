{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n# This must be done before importing transformers\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-05T23:14:48.103326Z","iopub.execute_input":"2022-03-05T23:14:48.104482Z","iopub.status.idle":"2022-03-05T23:14:48.123827Z","shell.execute_reply.started":"2022-03-05T23:14:48.104283Z","shell.execute_reply":"2022-03-05T23:14:48.122844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.cuda.amp import autocast\nfrom joblib import Parallel, delayed\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\nfrom transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:14:48.12719Z","iopub.execute_input":"2022-03-05T23:14:48.127408Z","iopub.status.idle":"2022-03-05T23:14:48.742819Z","shell.execute_reply.started":"2022-03-05T23:14:48.127366Z","shell.execute_reply":"2022-03-05T23:14:48.742022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntarget_id_map = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 2,\n    \"I-Position\": 3,\n    \"B-Evidence\": 4,\n    \"I-Evidence\": 5,\n    \"B-Claim\": 6,\n    \"I-Claim\": 7,\n    \"B-Concluding Statement\": 8,\n    \"I-Concluding Statement\": 9,\n    \"B-Counterclaim\": 10,\n    \"I-Counterclaim\": 11,\n    \"B-Rebuttal\": 12,\n    \"I-Rebuttal\": 13,\n    \"O\": 14,\n    \"PAD\": -100,\n}\n\n\nid_target_map = {v: k for k, v in target_id_map.items()}","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:14:48.744199Z","iopub.execute_input":"2022-03-05T23:14:48.74445Z","iopub.status.idle":"2022-03-05T23:14:48.772293Z","shell.execute_reply.started":"2022-03-05T23:14:48.744413Z","shell.execute_reply":"2022-03-05T23:14:48.771626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackDatasetTest:\n    def __init__(self, samples, max_len, tokenizer):\n        self.samples = samples\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.length = len(samples)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        input_ids = self.samples[idx][\"input_ids\"]\n        input_ids = [self.tokenizer.cls_token_id] + input_ids\n\n        if len(input_ids) > self.max_len - 1:\n            input_ids = input_ids[: self.max_len - 1]\n\n        # add end token id to the input_ids\n        input_ids = input_ids + [self.tokenizer.sep_token_id]\n        attention_mask = [1] * len(input_ids)\n\n        return {\n            \"ids\": input_ids,\n            \"mask\": attention_mask,\n        }","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:14:48.774552Z","iopub.execute_input":"2022-03-05T23:14:48.774976Z","iopub.status.idle":"2022-03-05T23:14:48.78391Z","shell.execute_reply.started":"2022-03-05T23:14:48.774939Z","shell.execute_reply":"2022-03-05T23:14:48.783155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackModel(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n        config = AutoConfig.from_pretrained(model_name)\n\n        hidden_dropout_prob: float = 0.2\n        layer_norm_eps: float = 17589e-7\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = AutoModel.from_config(config)\n        self.output = nn.Linear(config.hidden_size, self.num_labels)\n\n    def forward(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        logits = self.output(sequence_output)\n        logits = torch.softmax(logits, dim=-1)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:14:48.786839Z","iopub.execute_input":"2022-03-05T23:14:48.787035Z","iopub.status.idle":"2022-03-05T23:14:48.796323Z","shell.execute_reply.started":"2022-03-05T23:14:48.787006Z","shell.execute_reply":"2022-03-05T23:14:48.795614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _prepare_test_data_helper(args, tokenizer, ids):\n    test_samples = []\n    for idx in ids:\n        filename = os.path.join(\"../input/feedback-prize-2021\", \"test\", idx + \".txt\")\n        with open(filename, \"r\") as f:\n            text = f.read()\n\n        encoded_text = tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        offset_mapping = encoded_text[\"offset_mapping\"]\n\n        sample = {\n            \"id\": idx,\n            \"input_ids\": input_ids,\n            \"text\": text,\n            \"offset_mapping\": offset_mapping,\n        }\n\n        test_samples.append(sample)\n    return test_samples\n\n\ndef prepare_test_data(df_ids, tokenizer, args):\n    test_samples = []\n    ids_splits = np.array_split(df_ids, 4)\n\n    results = Parallel(n_jobs=4, backend=\"multiprocessing\")(\n        delayed(_prepare_test_data_helper)(args, tokenizer, idx) for idx in ids_splits\n    )\n    for result in results:\n        test_samples.extend(result)\n\n    return test_samples","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:14:48.798154Z","iopub.execute_input":"2022-03-05T23:14:48.798349Z","iopub.status.idle":"2022-03-05T23:14:48.809296Z","shell.execute_reply.started":"2022-03-05T23:14:48.798326Z","shell.execute_reply":"2022-03-05T23:14:48.808634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Collate:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = 2048\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n        else:\n            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n\n        # convert to tensors\n        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:14:48.810777Z","iopub.execute_input":"2022-03-05T23:14:48.811037Z","iopub.status.idle":"2022-03-05T23:14:48.82243Z","shell.execute_reply.started":"2022-03-05T23:14:48.811003Z","shell.execute_reply":"2022-03-05T23:14:48.821612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jn(pst, start, end):\n    return \" \".join([str(x) for x in pst[start:end]])\n\n\ndef link_evidence(oof):\n    thresh = 1\n    idu = oof['id'].unique()\n    idc = idu[1]\n    eoof = oof[oof['class'] == \"Evidence\"]\n    neoof = oof[oof['class'] != \"Evidence\"]\n    for thresh2 in range(26,27, 1):\n        retval = []\n        for idv in idu:\n            for c in  ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n                   'Counterclaim', 'Rebuttal']:\n                q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n                if len(q) == 0:\n                    continue\n                pst = []\n                for i,r in q.iterrows():\n                    pst = pst +[-1] + [int(x) for x in r['predictionstring'].split()]\n                start = 1\n                end = 1\n                for i in range(2,len(pst)):\n                    cur = pst[i]\n                    end = i\n                    #if pst[start] == 205:\n                    #   print(cur, pst[start], cur - pst[start])\n                    if (cur == -1 and c != 'Evidence') or ((cur == -1) and ((pst[i+1] > pst[end-1] + thresh) or (pst[i+1] - pst[start] > thresh2))):\n                        retval.append((idv, c, jn(pst, start, end)))\n                        start = i + 1\n                v = (idv, c, jn(pst, start, end+1))\n                #print(v)\n                retval.append(v)\n        roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring']) \n        roof = roof.merge(neoof, how='outer')\n        return roof","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:14:48.82335Z","iopub.execute_input":"2022-03-05T23:14:48.823529Z","iopub.status.idle":"2022-03-05T23:14:48.839482Z","shell.execute_reply.started":"2022-03-05T23:14:48.823505Z","shell.execute_reply":"2022-03-05T23:14:48.8386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_submission(test_samples):\n    proba_thresh = {\n        \"Lead\": 0.687,\n        \"Position\": 0.537,\n        \"Evidence\": 0.637,\n        \"Claim\": 0.537,\n        \"Concluding Statement\": 0.687,\n        \"Counterclaim\": 0.537,\n        \"Rebuttal\": 0.537,\n    }\n\n    min_thresh = {\n        \"Lead\": 9,\n        \"Position\": 5,\n        \"Evidence\": 14,\n        \"Claim\": 3,\n        \"Concluding Statement\": 11,\n        \"Counterclaim\": 6,\n        \"Rebuttal\": 4,\n    }\n\n    submission = []\n    for sample_idx, sample in enumerate(test_samples):\n        preds = sample[\"preds\"]\n        offset_mapping = sample[\"offset_mapping\"]\n        sample_id = sample[\"id\"]\n        sample_text = sample[\"text\"]\n        sample_input_ids = sample[\"input_ids\"]\n        sample_pred_scores = sample[\"pred_scores\"]\n        sample_preds = []\n\n        if len(preds) < len(offset_mapping):\n            preds = preds + [\"O\"] * (len(offset_mapping) - len(preds))\n            sample_pred_scores = sample_pred_scores + [0] * (len(offset_mapping) - len(sample_pred_scores))\n\n        idx = 0\n        phrase_preds = []\n        while idx < len(offset_mapping):\n            start, _ = offset_mapping[idx]\n            if preds[idx] != \"O\":\n                label = preds[idx][2:]\n            else:\n                label = \"O\"\n            phrase_scores = []\n            phrase_scores.append(sample_pred_scores[idx])\n            idx += 1\n            while idx < len(offset_mapping):\n                if label == \"O\":\n                    matching_label = \"O\"\n                else:\n                    matching_label = f\"I-{label}\"\n                if preds[idx] == matching_label:\n                    _, end = offset_mapping[idx]\n                    phrase_scores.append(sample_pred_scores[idx])\n                    idx += 1\n                else:\n                    break\n            if \"end\" in locals():\n                phrase = sample_text[start:end]\n                phrase_preds.append((phrase, start, end, label, phrase_scores))\n\n        temp_df = []\n        for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n            word_start = len(sample_text[:start].split())\n            word_end = word_start + len(sample_text[start:end].split())\n            word_end = min(word_end, len(sample_text.split()))\n            ps = \" \".join([str(x) for x in range(word_start, word_end)])\n            if label != \"O\":\n#                 print(label)\n#                 print(sum(phrase_scores) / len(phrase_scores))\n                if sum(phrase_scores) / len(phrase_scores) >= proba_thresh[label]:\n                    if len(ps.split()) >= min_thresh[label]:\n                        temp_df.append((sample_id, label, ps))\n\n        temp_df = pd.DataFrame(temp_df, columns=[\"id\", \"class\", \"predictionstring\"])\n        submission.append(temp_df)\n\n    submission = pd.concat(submission).reset_index(drop=True)\n    submission = link_evidence(submission)\n    submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:14:48.840467Z","iopub.execute_input":"2022-03-05T23:14:48.840678Z","iopub.status.idle":"2022-03-05T23:14:48.860897Z","shell.execute_reply.started":"2022-03-05T23:14:48.840646Z","shell.execute_reply":"2022-03-05T23:14:48.860174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef inference(model, weight, test_loader):\n    model.load_state_dict(torch.load(weight))\n    model.eval()\n    test_pbar = tqdm(enumerate(test_loader), total = len(test_loader))\n    test_preds = []\n    for step, data in test_pbar:\n        input_ids = data[\"ids\"].to(device)\n        input_mask = data[\"mask\"].to(device)\n        with autocast(enabled = True):\n            logits = model(input_ids,\n                          input_mask)\n        yield logits.cpu().detach().numpy()","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:14:48.863816Z","iopub.execute_input":"2022-03-05T23:14:48.864154Z","iopub.status.idle":"2022-03-05T23:14:48.87217Z","shell.execute_reply.started":"2022-03-05T23:14:48.864116Z","shell.execute_reply":"2022-03-05T23:14:48.87148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dict = dict(\n#     longformer_base1 = dict(\n#         model_name = \"../input/fb-longformer/longformer-base/longformer-base\",\n#         config_name = \"../input/fb-longformer/longformer-base/longformer-base/config.json\",\n#         weights = [f\"../input/fb-longformer/FB_longformer-base/models/model_{fold}\" for fold in [0, 1, 2]],\n#         max_len = 1600\n#     ),\n#     longformer_base2 = dict(\n#         model_name = \"../input/fb-longformer/longformer-base/longformer-base\",\n#         config_name = \"../input/fb-longformer/longformer-base/longformer-base/config.json\",\n#         weights = [f\"../input/fb-longformer/FB_longformer-base-squadv2/models/model_{fold}\" for fold in [3, 4]],\n#         max_len = 1600\n#     ),\n#     longformer_large1 = dict(\n#         model_name = \"../input/fb-longformer/longformer-large/longformer-large\",\n#         config_name = \"../input/fb-longformer/longformer-large/longformer-large/config.json\",\n#         weights = [f\"../input/fb-longformer/FB_longformer-large-trivia/models/model_{fold}\" for fold in range(3,5)],\n#         max_len = 1600\n#     ),\n#     longformer_large2 = dict(\n#         model_name = \"../input/fb-longformer/longformer-large/longformer-large\",\n#         config_name = \"../input/fb-longformer/longformer-large/longformer-large/config.json\",\n#         weights = [f\"../input/fb-longformer/FB_longformer-large/models/model_{fold}\" for fold in range(3)],\n#         max_len = 1600\n#     ),\n    deberta_large1 = dict(\n        model_name = \"../input/fb-deberta/deberta-large/deberta-large\",\n        config_name = \"../input/fb-deberta/deberta-large/deberta-large/config.json\",\n        weights = [f\"../input/fb-deberta/FB_deberta-large/models/model_{fold}\" for fold in range(1,5)],\n        max_len = 1600\n    ),\n#     deberta_largev3_1 = dict(\n#         model_name = \"../input/fb-deberta/deberta-v3-large/deberta-v3-large\",\n#         config_name = \"../input/fb-deberta/deberta-v3-large/deberta-v3-large/config.json\",\n#         weights = [f\"../input/fb-deberta/FB_debertav3-large/models/model_{fold}\" for fold in range(1,5)],\n#         max_len = 1024\n#     ),\n#     deberta_xlargev_1 = dict(\n#         model_name = \"../input/fb-deberta/deberta-xlarge/deberta-xlarge\",\n#         config_name = \"../input/fb-deberta/deberta-xlarge/deberta-xlarge/config.json\",\n#         weights = [f\"../input/fb-deberta/FB_deberta-xlarge/models/model_{fold}\" for fold in range(3,5)],\n#         max_len = 1600\n#     ),\n#     bigbird_base1 = dict(\n#         model_name = \"../input/fb-bigbird/bigbird-base/bigbird-base\",\n#         config_name = \"../input/fb-bigbird/bigbird-base/bigbird-base/config.json\",\n#         weights = [f\"../input/fb-bigbird/FB_bigbird-base-trivia/models/model_{fold}\" for fold in range(5)],\n#         max_len = 4096\n#     )\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:14:48.874593Z","iopub.execute_input":"2022-03-05T23:14:48.875267Z","iopub.status.idle":"2022-03-05T23:14:48.882757Z","shell.execute_reply.started":"2022-03-05T23:14:48.87517Z","shell.execute_reply":"2022-03-05T23:14:48.88206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    df = pd.read_csv(os.path.join(\"../input/feedback-prize-2021/\", \"sample_submission.csv\"))\n    df_ids = df[\"id\"].unique()\n    counter = 0\n    raw_preds = []\n    for key, item in model_dict.items():\n        print(f\"Predicting {key}\")\n        tokenizer = AutoTokenizer.from_pretrained(item[\"model_name\"])\n        test_samples = prepare_test_data(df_ids, tokenizer, item)\n        collate = Collate(tokenizer=tokenizer)\n        test_dataset = FeedbackDatasetTest(test_samples,\n                                           item[\"max_len\"],\n                                           tokenizer)\n        test_loader = torch.utils.data.DataLoader(test_dataset,\n                                                    batch_size = 8,\n                                                    collate_fn = collate,\n                                                    num_workers = 2,  \n                                                    shuffle = False)\n        model = FeedbackModel(item[\"model_name\"], 15)\n        model.to(device)\n        for weight in item[\"weights\"]:\n            test_preds = inference(model, weight, test_loader)\n            for idx,pred in enumerate(test_preds):\n                pred = pred.astype(np.float16) / 4\n                if counter == 0:\n                    raw_preds.append(pred)\n                else:\n                    raw_preds[idx] += pred\n            counter += 1\n        del model, tokenizer, test_dataset, test_loader, test_preds\n        gc.collect()\n            \n    final_preds = []\n    final_scores = []\n    for rp in raw_preds:\n        pred_class = np.argmax(rp, axis=2)\n        pred_scrs = np.max(rp, axis=2)\n        for pred, pred_scr in zip(pred_class, pred_scrs):\n            pred = pred.tolist()\n            pred_scr = pred_scr.tolist()\n            final_preds.append(pred)\n            final_scores.append(pred_scr)\n\n    for j in range(len(test_samples)):\n        tt = [id_target_map[p] for p in final_preds[j][1:]]\n        tt_score = final_scores[j][1:]\n        test_samples[j][\"preds\"] = tt\n        test_samples[j][\"pred_scores\"] = tt_score\n    get_submission(test_samples)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:14:48.885745Z","iopub.execute_input":"2022-03-05T23:14:48.885948Z","iopub.status.idle":"2022-03-05T23:15:23.543052Z","shell.execute_reply.started":"2022-03-05T23:14:48.885924Z","shell.execute_reply":"2022-03-05T23:15:23.542332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(\"submission.csv\").head()","metadata":{"execution":{"iopub.status.busy":"2022-03-05T23:15:23.544252Z","iopub.execute_input":"2022-03-05T23:15:23.544664Z","iopub.status.idle":"2022-03-05T23:15:23.566411Z","shell.execute_reply.started":"2022-03-05T23:15:23.544626Z","shell.execute_reply":"2022-03-05T23:15:23.565795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}