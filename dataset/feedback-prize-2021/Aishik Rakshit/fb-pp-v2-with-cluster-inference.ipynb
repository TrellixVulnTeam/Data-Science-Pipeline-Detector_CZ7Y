{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ! pip install ../input/xgboost-090/xgboost-0.90-py2.py3-none-manylinux1_x86_64.whl --no-dependencies","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-15T15:47:05.98854Z","iopub.execute_input":"2022-03-15T15:47:05.989083Z","iopub.status.idle":"2022-03-15T15:47:06.006064Z","shell.execute_reply.started":"2022-03-15T15:47:05.988983Z","shell.execute_reply":"2022-03-15T15:47:06.005375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# general\nimport pandas as pd\nimport numpy as np\nimport os\nimport copy\nimport pickle\nimport random\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import UMAP\nfrom cuml import KMeans\nfrom joblib import Parallel, delayed\nfrom multiprocessing import Manager\nfrom tqdm.notebook import tqdm\nfrom scipy import stats\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\nfrom collections import Counter\nfrom bisect import bisect_left\nfrom sklearn.model_selection import cross_val_score, GroupKFold\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom skopt.space import Real\nfrom skopt import gp_minimize\nimport sys\nimport xgboost\nimport gc\nfrom collections import defaultdict\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# nlp\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_cosine_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:47:06.007913Z","iopub.execute_input":"2022-03-15T15:47:06.008246Z","iopub.status.idle":"2022-03-15T15:47:15.191331Z","shell.execute_reply.started":"2022-03-15T15:47:06.008209Z","shell.execute_reply":"2022-03-15T15:47:15.190606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    savename = \"deberta-base-v2\"\n    n_folds = 5\n    num_workers = 12\n    fold = 0\n    model = \"microsoft/deberta-base\"\n    lr = 2.5e-5\n    n_accum = 1\n    max_grad_norm = 10\n    output = \"/content/model\"\n    input = \"/content/data\"\n    max_len = 1600\n    max_len_valid = 1600\n    num_labels = 15\n    batch_size = 4\n    valid_batch_size = 4\n    epochs = 6\n    accumulation_steps = 1\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    apex = True\n    debug = False\n    if debug:\n        n_folds = 2\n        epochs = 2","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:47:15.192721Z","iopub.execute_input":"2022-03-15T15:47:15.192942Z","iopub.status.idle":"2022-03-15T15:47:15.199774Z","shell.execute_reply.started":"2022-03-15T15:47:15.192908Z","shell.execute_reply":"2022-03-15T15:47:15.199165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n\nlabels_to_ids = {v:k for k,v in enumerate(output_labels)}\nids_to_labels = {k:v for k,v in enumerate(output_labels)}\ndisc_type_to_ids = {'Evidence':(11,12),'Claim':(5,6),'Lead':(1,2),'Position':(3,4),'Counterclaim':(7,8),'Rebuttal':(9,10),'Concluding Statement':(13,14)}","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:47:15.20169Z","iopub.execute_input":"2022-03-15T15:47:15.202366Z","iopub.status.idle":"2022-03-15T15:47:15.209938Z","shell.execute_reply.started":"2022-03-15T15:47:15.20233Z","shell.execute_reply":"2022-03-15T15:47:15.20916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_texts(path):\n    names, texts = [], []\n    for f in list(os.listdir(path)):\n        names.append(f.replace('.txt', ''))\n        texts.append(open(path + f, 'r').read())\n    texts = pd.DataFrame({'id': names, 'text': texts})\n    return texts","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:47:15.212073Z","iopub.execute_input":"2022-03-15T15:47:15.212512Z","iopub.status.idle":"2022-03-15T15:47:15.220644Z","shell.execute_reply.started":"2022-03-15T15:47:15.212421Z","shell.execute_reply":"2022-03-15T15:47:15.219993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_mapping(unsplit):\n    splt = unsplit.split()\n    offset_to_wordidx = np.full(len(unsplit),-1)\n    txt_ptr = 0\n    for split_index, full_word in enumerate(splt):\n        while unsplit[txt_ptr:txt_ptr + len(full_word)] != full_word:\n            txt_ptr += 1\n        offset_to_wordidx[txt_ptr:txt_ptr + len(full_word)] = split_index\n        txt_ptr += len(full_word)\n    return offset_to_wordidx","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:47:15.222212Z","iopub.execute_input":"2022-03-15T15:47:15.22274Z","iopub.status.idle":"2022-03-15T15:47:15.229477Z","shell.execute_reply.started":"2022-03-15T15:47:15.222702Z","shell.execute_reply":"2022-03-15T15:47:15.228767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class feedbackDataset(Dataset):\n  def __init__(self, dataframe, tokenizer, max_len, get_wids):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.get_wids = get_wids # for validation\n\n  def __getitem__(self, index):\n        # GET TEXT AND WORD LABELS \n        text = self.data.text[index]        \n        word_labels = self.data.entities[index] if not self.get_wids else None\n\n        # TOKENIZE TEXT\n        encoding = self.tokenizer(text,\n                             return_offsets_mapping=True, \n                             padding='max_length', \n                             truncation=True, \n                             max_length=self.max_len)\n        \n        word_ids = encoding.word_ids()  \n        split_word_ids = np.full(len(word_ids),-1)\n        offset_to_wordidx = split_mapping(text)\n        offsets = encoding['offset_mapping']\n        \n        # CREATE TARGETS AND MAPPING OF TOKENS TO SPLIT() WORDS\n        label_ids = []\n        # Iterate in reverse to label whitespace tokens until a Begin token is encountered\n        for token_idx, word_idx in reversed(list(enumerate(word_ids))):\n            \n            if word_idx is None:\n                if not self.get_wids: label_ids.append(-100)\n            else:\n                if offsets[token_idx] != (0,0):\n                    #Choose the split word that shares the most characters with the token if any\n                    split_idxs = offset_to_wordidx[offsets[token_idx][0]:offsets[token_idx][1]]\n                    split_index = stats.mode(split_idxs[split_idxs != -1]).mode[0] if len(np.unique(split_idxs)) > 1 else split_idxs[0]\n                    \n                    if split_index != -1: \n                        if not self.get_wids: label_ids.append( labels_to_ids[word_labels[split_index]] )\n                        split_word_ids[token_idx] = split_index\n                    else:\n                        # Even if we don't find a word, continue labeling 'I' tokens until a 'B' token is found\n                        if label_ids and label_ids[-1] != -100 and ids_to_labels[label_ids[-1]][0] == 'I':\n                            split_word_ids[token_idx] = split_word_ids[token_idx + 1]\n                            if not self.get_wids: label_ids.append(label_ids[-1])\n                        else:\n                            if not self.get_wids: label_ids.append(-100)\n                else:\n                    if not self.get_wids: label_ids.append(-100)\n        \n        encoding['labels'] = list(reversed(label_ids))\n\n        # CONVERT TO TORCH TENSORS\n        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n        if self.get_wids: \n            item['wids'] = torch.as_tensor(split_word_ids)\n        \n        return item\n\n  def __len__(self):\n        return self.len","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:47:15.230787Z","iopub.execute_input":"2022-03-15T15:47:15.231117Z","iopub.status.idle":"2022-03-15T15:47:15.24736Z","shell.execute_reply.started":"2022-03-15T15:47:15.231083Z","shell.execute_reply":"2022-03-15T15:47:15.246626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackModel(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n\n        hidden_dropout_prob: float = 0.1\n        layer_norm_eps: float = 1e-7\n\n        config = AutoConfig.from_pretrained(model_name)\n\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n                \"num_labels\": self.num_labels,\n            }\n        )\n        self.transformer = AutoModel.from_config(config=config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n        self.output = nn.Linear(config.hidden_size, self.num_labels)\n\n    def forward(self, ids, mask, token_type_ids=None):\n\n        if token_type_ids:\n            transformer_out = self.transformer(ids, mask, token_type_ids)\n        else:\n            transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        sequence_output = self.dropout(sequence_output)\n\n        logits1 = self.output(self.dropout1(sequence_output))\n        logits2 = self.output(self.dropout2(sequence_output))\n        logits3 = self.output(self.dropout3(sequence_output))\n        logits4 = self.output(self.dropout4(sequence_output))\n        logits5 = self.output(self.dropout5(sequence_output))\n\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n        return logits, logits1, logits2, logits3, logits4, logits5","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:47:15.248757Z","iopub.execute_input":"2022-03-15T15:47:15.249241Z","iopub.status.idle":"2022-03-15T15:47:15.262159Z","shell.execute_reply.started":"2022-03-15T15:47:15.249206Z","shell.execute_reply":"2022-03-15T15:47:15.261384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predictions(all_labels, all_scores, df):    \n    proba_thresh = {\n        \"Lead\": 0.7,\n        \"Position\": 0.55,\n        \"Evidence\": 0.65,\n        \"Claim\": 0.55,\n        \"Concluding Statement\": 0.7,\n        \"Counterclaim\": 0.5,\n        \"Rebuttal\": 0.55,\n    }\n    final_preds = []\n    \n    for i in range(len(df)):\n        idx = df.id.values[i]\n        pred = all_labels[i]\n        score = all_scores[i]\n        preds = []\n        j = 0\n        \n        while j < len(pred):\n            cls = pred[j]\n            if cls == 'O': \n                pass\n            else: \n                cls = cls.replace('B','I')\n            end = j + 1\n            while end < len(pred) and pred[end] == cls:\n                end += 1\n            # print(end - j)\n            if cls != 'O' and cls != '' and end - j > 7:\n                if np.mean(score[j:end]) > proba_thresh[cls.replace('I-','')]:\n                    final_preds.append((idx, cls.replace('I-',''), \n                                        ' '.join(map(str, list(range(j, end))))))\n            j = end\n    df_pred = pd.DataFrame(final_preds)\n    df_pred.columns = ['id','class','predictionstring']\n    return df_pred\n\ndef threshold(df):\n\n    min_thresh = {\n        \"Lead\": 9,\n        \"Position\": 5,\n        \"Evidence\": 14,\n        \"Claim\": 3,\n        \"Concluding Statement\": 11,\n        \"Counterclaim\": 6,\n        \"Rebuttal\": 4,\n    }\n\n    df = df.copy()\n    for key, value in min_thresh.items():\n        index = df.loc[df[\"class\"] == key].query(f\"len<{value}\").index\n        df.drop(index, inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:47:15.265106Z","iopub.execute_input":"2022-03-15T15:47:15.265526Z","iopub.status.idle":"2022-03-15T15:47:15.277621Z","shell.execute_reply.started":"2022-03-15T15:47:15.265492Z","shell.execute_reply":"2022-03-15T15:47:15.276954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef inference(model, data_loader, weights):\n    model.eval()\n\n    ensemble_preds = np.zeros((len(data_loader.dataset), Config.max_len, len(labels_to_ids)), dtype=np.float32)\n    wids = np.full((len(data_loader.dataset), Config.max_len), -100)\n\n    for weight in weights:\n        model.load_state_dict(torch.load(weight))\n        infer_pbar = tqdm(enumerate(data_loader), total = len(data_loader))\n\n        for step, data in infer_pbar:\n            wids[step*Config.valid_batch_size:(step+1)*Config.valid_batch_size] = data['wids'].numpy()\n\n            input_ids = data[\"input_ids\"].to(Config.device)\n            input_mask = data[\"attention_mask\"].to(Config.device)\n\n            batch_size = input_ids.shape[0]\n            logits, logits1, logits2, logits3, logits4, logits5 = model(input_ids,\n                                                                            input_mask)\n            \n            val_preds = logits.cpu().tolist()\n            all_preds = torch.nn.functional.softmax(logits, dim=2).cpu().detach().numpy() \n            ensemble_preds[step*Config.valid_batch_size:(step+1)*Config.valid_batch_size] += all_preds / 5\n\n    predictions = []\n    # INTERATE THROUGH EACH TEXT AND GET PRED\n    for text_i in range(ensemble_preds.shape[0]):\n        token_preds = ensemble_preds[text_i]\n\n        prediction = []\n        previous_word_idx = -1\n        prob_buffer = []\n        word_ids = wids[text_i][wids[text_i] != -100]\n        for idx,word_idx in enumerate(word_ids):                            \n            if word_idx == -1:\n                pass\n            elif word_idx != previous_word_idx:              \n                prediction.append(token_preds[idx])\n                previous_word_idx = word_idx\n        predictions.append(prediction)\n                \n    \n    return predictions  \n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:47:15.280871Z","iopub.execute_input":"2022-03-15T15:47:15.281625Z","iopub.status.idle":"2022-03-15T15:47:15.294353Z","shell.execute_reply.started":"2022-03-15T15:47:15.281589Z","shell.execute_reply":"2022-03-15T15:47:15.293696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @torch.no_grad()\n# def inference(model, data_loader, weights):\n#     model.eval()\n\n#     ensemble_preds = np.zeros((len(data_loader.dataset), Config.max_len, len(labels_to_ids)), dtype=np.float32)\n#     wids = np.full((len(data_loader.dataset), Config.max_len), -100)\n\n#     for weight in weights:\n#         model.load_state_dict(torch.load(weight))\n#         infer_pbar = tqdm(enumerate(data_loader), total = len(data_loader))\n\n#         for step, data in infer_pbar:\n#             wids[step*Config.valid_batch_size:(step+1)*Config.valid_batch_size] = data['wids'].numpy()\n\n#             input_ids = data[\"input_ids\"].to(Config.device)\n#             input_mask = data[\"attention_mask\"].to(Config.device)\n\n#             batch_size = input_ids.shape[0]\n#             logits, logits1, logits2, logits3, logits4, logits5 = model(input_ids,\n#                                                                             input_mask)\n            \n#             val_preds = logits.cpu().tolist()\n#             all_preds = torch.nn.functional.softmax(logits, dim=2).cpu().detach().numpy() \n#             ensemble_preds[step*Config.valid_batch_size:(step+1)*Config.valid_batch_size] += all_preds / 5\n\n#     predictions = []\n#     # INTERATE THROUGH EACH TEXT AND GET PRED\n#     for text_i in range(ensemble_preds.shape[0]):\n#         token_preds = ensemble_preds[text_i]\n        \n#         prediction = []\n#         previous_word_idx = -1\n#         prob_buffer = []\n#         word_ids = wids[text_i][wids[text_i] != -100]\n#         for idx,word_idx in enumerate(word_ids):                            \n#             if word_idx == -1:\n#                 pass\n#             elif word_idx != previous_word_idx:              \n#                 if prob_buffer:\n#                     prediction.append(np.mean(prob_buffer, dtype=np.float32, axis=0))\n#                     prob_buffer = []\n#                 prob_buffer.append(token_preds[idx])\n#                 previous_word_idx = word_idx\n#             else: \n#                 prob_buffer.append(token_preds[idx])\n#         prediction.append(np.mean(prob_buffer, dtype=np.float32, axis=0))\n#         predictions.append(prediction)\n                \n    \n#     return predictions  \n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:47:15.296687Z","iopub.execute_input":"2022-03-15T15:47:15.296902Z","iopub.status.idle":"2022-03-15T15:47:15.305448Z","shell.execute_reply.started":"2022-03-15T15:47:15.296878Z","shell.execute_reply":"2022-03-15T15:47:15.304687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/fb-corrected-train/corrected_train.csv\")\nMAX_SEQ_LEN = {}\ntrain_df['len'] = train_df['predictionstring'].apply(lambda x:len(x.split()))\nmax_lens = train_df.groupby('discourse_type')['len'].quantile(.995)\nfor disc_type in disc_type_to_ids:\n    MAX_SEQ_LEN[disc_type] = int(max_lens[disc_type])\n\n#The minimum probability prediction for a 'B'egin class for which we will evaluate a word sequence\nMIN_BEGIN_PROB = {\n    'Claim': .35,\n    'Concluding Statement': .15,\n    'Counterclaim': .04,\n    'Evidence': .1,\n    'Lead': .32,\n    'Position': .25,\n    'Rebuttal': .01,\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:47:15.308264Z","iopub.execute_input":"2022-03-15T15:47:15.308624Z","iopub.status.idle":"2022-03-15T15:47:19.123572Z","shell.execute_reply.started":"2022-03-15T15:47:15.308592Z","shell.execute_reply":"2022-03-15T15:47:19.122711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SeqDataset(object):\n    \n    def __init__(self, features, labels, groups, wordRanges, truePos):\n        \n        self.features = np.array(features, dtype=np.float32)\n        self.labels = np.array(labels)\n        self.groups = np.array(groups, dtype=np.int16)\n        self.wordRanges = np.array(wordRanges, dtype=np.int16)\n        self.truePos = np.array(truePos)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:47:19.124867Z","iopub.execute_input":"2022-03-15T15:47:19.125137Z","iopub.status.idle":"2022-03-15T15:47:19.132447Z","shell.execute_reply.started":"2022-03-15T15:47:19.125089Z","shell.execute_reply":"2022-03-15T15:47:19.130958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seq_dataset(disc_type, test_word_preds, clusters, pred_indices=None):\n    word_preds = test_word_preds\n    window = pred_indices if pred_indices else range(len(word_preds))\n    X = np.empty((int(1e6),14), dtype=np.float32)\n    X_ind = 0\n    y = []\n    truePos = []\n    wordRanges = []\n    groups = []\n    for text_i in tqdm(window):\n        cluster = clusters[text_i]\n        text_preds = np.array(word_preds[text_i])\n        num_words = len(text_preds)\n        disc_begin, disc_inside = disc_type_to_ids[disc_type]\n        \n        # The probability that a word corresponds to either a 'B'-egin or 'I'-nside token for a class\n        prob_or = lambda word_preds: (1-(1-word_preds[:,disc_begin]) * (1-word_preds[:,disc_inside]))\n        \n        # Iterate over every sub-sequence in the text\n        quants = np.linspace(0,1,7)\n        prob_begins = np.copy(text_preds[:,disc_begin])\n        min_begin = MIN_BEGIN_PROB[disc_type]\n        for pred_start in range(num_words):\n            prob_begin = prob_begins[pred_start]\n            if prob_begin > min_begin:\n                begin_or_inside = []\n                for pred_end in range(pred_start+1,min(num_words+1, pred_start+MAX_SEQ_LEN[disc_type]+1)):\n                    \n                    new_prob = prob_or(text_preds[pred_end-1:pred_end])\n                    insert_i = bisect_left(begin_or_inside, new_prob)\n                    begin_or_inside.insert(insert_i, new_prob[0])\n\n                    # Generate features for a word sub-sequence\n\n                    # The length and position of start/end of the sequence\n                    features = [pred_end - pred_start, pred_start / float(num_words), pred_end / float(num_words)]\n                    \n                    # 7 evenly spaced quantiles of the distribution of relevant class probabilities for this sequence\n                    features.extend(list(sorted_quantile(begin_or_inside, quants)))\n\n                    # The probability that words on either edge of the current sub-sequence belong to the class of interest\n                    features.append(prob_or(text_preds[pred_start-1:pred_start])[0] if pred_start > 0 else 0)\n                    features.append(prob_or(text_preds[pred_end:pred_end+1])[0] if pred_end < num_words else 0)\n\n                    # The probability that the first word corresponds to a 'B'-egin token\n                    features.append(text_preds[pred_start,disc_begin])\n\n                    exact_match = None\n\n                    true_pos = None\n\n                    # For efficiency, use a numpy array instead of a list that doubles in size when full to conserve constant \"append\" time complexity\n                    if X_ind >= X.shape[0]:\n                        new_X = np.empty((X.shape[0]*2,14), dtype=np.float32)\n                        new_X[:X.shape[0]] = X\n                        X = new_X\n                    X[X_ind, :13] = features\n                    X[X_ind, 13] = cluster\n                    X_ind += 1\n                    \n                    y.append(exact_match)\n                    truePos.append(true_pos)\n                    wordRanges.append((np.int16(pred_start), np.int16(pred_end)))\n                    groups.append(np.int16(text_i))\n\n    return SeqDataset(X[:X_ind], y, groups, wordRanges, truePos)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:47:19.13428Z","iopub.execute_input":"2022-03-15T15:47:19.134759Z","iopub.status.idle":"2022-03-15T15:47:19.151348Z","shell.execute_reply.started":"2022-03-15T15:47:19.134705Z","shell.execute_reply":"2022-03-15T15:47:19.150693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sorted_quantile(array, q):\n    array = np.array(array)\n    n = len(array)\n    index = (n - 1) * q\n    left = np.floor(index).astype(int)\n    fraction = index - left\n    right = left\n    right = right + (fraction > 0).astype(int)\n    i, j = array[left], array[right]\n    return i + (j - i) * fraction","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:47:19.152054Z","iopub.execute_input":"2022-03-15T15:47:19.15225Z","iopub.status.idle":"2022-03-15T15:47:19.162401Z","shell.execute_reply.started":"2022-03-15T15:47:19.152227Z","shell.execute_reply":"2022-03-15T15:47:19.161595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_strings(disc_type, probThresh, test_groups, train_ind=None):\n    string_preds = []\n    submitSeqDs = submitSeqSets[disc_type]\n    \n    # Average the probability predictions of a set of classifiers\n    get_tp_prob = lambda testDs, classifiers: np.mean([clf.predict_proba(testDs.features)[:,1] for clf in classifiers], axis=0) if testDs.features.shape[0] > 0 else np.array([])\n    \n    \n    # Point to submission set values\n    predict_df = test_texts\n    text_df = test_texts\n    groupIdx = np.isin(submitSeqDs.groups, test_groups)\n    testDs = SeqDataset(submitSeqDs.features[groupIdx], submitSeqDs.labels[groupIdx], submitSeqDs.groups[groupIdx], submitSeqDs.wordRanges[groupIdx], submitSeqDs.truePos[groupIdx])\n\n    # Classifiers are always loaded from disc during submission\n    with open( f\"../input/fb-deberta-3/drive-download-20220315T155246Z-001/FB_deberta-large-v2-ITPT_ens_NF8-clf5-cluster-5/clfs/{disc_type}_clf.p\", \"rb\" ) as clfFile:\n        classifiers = pickle.load( clfFile )  \n    prob_tp = get_tp_prob(testDs, classifiers)\n        \n    text_to_seq = {}\n    for text_idx in test_groups:\n        # The probability of true positive and (start,end) of each sub-sequence in the curent text\n        prob_tp_curr = prob_tp[testDs.groups == text_idx]\n        word_ranges_curr = testDs.wordRanges[testDs.groups == text_idx]\n        sorted_seqs = list(reversed(sorted(zip(prob_tp_curr, [tuple(wr) for wr in word_ranges_curr]))))\n        text_to_seq[text_idx] = sorted_seqs\n    \n    for text_idx in test_groups:\n        \n        i = 1\n        split_text = text_df.loc[text_df.id == predict_df.id.values[text_idx]].iloc[0].text.split()\n        \n        # Start and end word indices of sequence candidates kept in sorted order for efficiency\n        starts = []\n        ends = []\n        \n        # Include the sub-sequence predictions in order of predicted probability\n        for prob, wordRange in text_to_seq[text_idx]:\n            \n            # Until the predicted probability is lower than the tuned threshold\n            if prob < probThresh: break\n                \n            # Binary search already-placed word sequence intervals, and insert the new word sequence interval if it does not intersect an existing interval.\n            insert = bisect_left(starts, wordRange[0])\n            if (insert == 0 or ends[insert-1] <= wordRange[0]) and (insert == len(starts) or starts[insert] >= wordRange[1]):\n                starts.insert(insert, wordRange[0])\n                ends.insert(insert, wordRange[1])\n                string_preds.append((predict_df.id.values[text_idx], disc_type, ' '.join(map(str, list(range(wordRange[0], wordRange[1]))))))\n                i += 1     \n    return string_preds\n\ndef sub_df(string_preds):\n    return pd.DataFrame(string_preds, columns=['id','class','predictionstring'])\n    \n# Convert skopt's uniform distribution over the tuning threshold to a distribution that exponentially decays from 100% to 0%\ndef prob_thresh(x): \n    return .01*(100-np.exp(100*x))\n\n# Convert back to the scalar supplied by skopt\ndef skopt_thresh(x): \n    return np.log((x/.01-100.)/-1.)/100.","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:47:19.163645Z","iopub.execute_input":"2022-03-15T15:47:19.163974Z","iopub.status.idle":"2022-03-15T15:47:19.180514Z","shell.execute_reply.started":"2022-03-15T15:47:19.163936Z","shell.execute_reply":"2022-03-15T15:47:19.179709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jn(pst, start, end):\n    return \" \".join([str(x) for x in pst[start:end]])\n\n\ndef link_evidence(oof):\n    thresh = 1\n    idu = oof['id'].unique()\n    idc = idu[1]\n    eoof = oof[oof['class'] == \"Evidence\"]\n    neoof = oof[oof['class'] != \"Evidence\"]\n    for thresh2 in range(26,27, 1):\n        retval = []\n        for idv in idu:\n            for c in  ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n                   'Counterclaim', 'Rebuttal']:\n                q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n                if len(q) == 0:\n                    continue\n                pst = []\n                for i,r in q.iterrows():\n                    pst = pst +[-1] + [int(x) for x in r['predictionstring'].split()]\n                start = 1\n                end = 1\n                for i in range(2,len(pst)):\n                    cur = pst[i]\n                    end = i\n                    #if pst[start] == 205:\n                    #   print(cur, pst[start], cur - pst[start])\n                    if (cur == -1 and c != 'Evidence') or ((cur == -1) and ((pst[i+1] > pst[end-1] + thresh) or (pst[i+1] - pst[start] > thresh2))):\n                        retval.append((idv, c, jn(pst, start, end)))\n                        start = i + 1\n                v = (idv, c, jn(pst, start, end+1))\n                #print(v)\n                retval.append(v)\n        roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring']) \n        roof = roof.merge(neoof, how='outer')\n        return roof","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:47:19.181624Z","iopub.execute_input":"2022-03-15T15:47:19.182424Z","iopub.status.idle":"2022-03-15T15:47:19.195586Z","shell.execute_reply.started":"2022-03-15T15:47:19.18239Z","shell.execute_reply":"2022-03-15T15:47:19.194826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dict = dict(\n#         deberta_base_v2 = dict(\n#             model_name = \"../input/fb-deberta/deberta-base/deberta-base\",\n#             config = \"../input/fb-deberta/deberta-base/deberta-base/config.json\",\n#             weights = [f\"../input/fb-deberta-2/FB_deberta-base-PP-V2/FB_deberta-base-PP-V2/FB_deberta-base-v2/models/model_{fold}\" for fold in range(5)]\n#         ),\n#     funnel_transformer_v2 = dict(\n#             model_name = \"../input/fb-funnel-transformer/funnel-transformer-intermediate/funnel-transformer-intermediate\",\n#             config = \"../input/fb-funnel-transformer/funnel-transformer-intermediate/funnel-transformer-intermediate/config.json\",\n#             weights = [f\"../input/fb-funnel-transformer/funnel-transformer-intermediate-PP-v2/funnel-transformer-intermediate-weights/models/model_{fold}\" for fold in range(5)]\n#         ),\n    deberta_large_v2 = dict(\n            model_name = \"../input/fb-deberta/deberta-large/deberta-large\",\n            config = \"../input/fb-deberta/deberta-large/deberta-large/config.json\",\n        weights = [\"../input/fb-deberta-2/FB_deberta_large_PP_v2/FB_deberta_large_PP_v2/deberta-large-v2/models/model_0\",\n                   \"../input/fb-deberta-2/FB_deberta_large_PP_v2/FB_deberta_large_PP_v2/deberta-large-v2/models/model_1\",\n                  \"../input/fb-deberta-2/FB_deberta_large_PP_v2/FB_deberta_large_PP_v2/deberta-large-v2/models/model_2\",\n                  \"../input/fb-deberta-2/FB_deberta_large_PP_v2/FB_deberta_large_PP_v2/deberta-large-v2/models/model_3\",\n                  \"../input/fb-deberta-2/FB_deberta_large_PP_v2/FB_deberta_large_PP_v2/deberta-large-v2/models/model_4\",]\n#             weights = [f\"../input/fb-deberta-2/FB_deberta-large-ITPT-tuned-PP-v2/deberta-large-ITPT-tuned/models/model_{fold}\" for fold in range(5)]\n        )\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:47:19.196675Z","iopub.execute_input":"2022-03-15T15:47:19.196929Z","iopub.status.idle":"2022-03-15T15:47:19.206092Z","shell.execute_reply.started":"2022-03-15T15:47:19.196893Z","shell.execute_reply":"2022-03-15T15:47:19.205317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    tfidf = TfidfVectorizer(stop_words='english', binary=True, max_features=25000)\n    df_text = get_texts(\"../input/feedback-prize-2021/train/\")\n    df_text_ = cudf.DataFrame(df_text)\n    text_embeddings = tfidf.fit_transform( df_text_.text ).toarray()\n    umap = UMAP()\n    embed_2d = umap.fit_transform(text_embeddings)\n    embed_2d = cupy.asnumpy( embed_2d )\n    kmeans = cuml.KMeans(n_clusters=15)\n    kmeans.fit(embed_2d)\n    \n    test_texts = get_texts(\"../input/feedback-prize-2021/test/\")\n    test_texts_ = cudf.DataFrame(test_texts)\n    test_embeddings = tfidf.transform( test_texts_.text ).toarray()\n    embed_2d_test = cupy.asnumpy(umap.transform(test_embeddings))\n    test_cluster = kmeans.predict(embed_2d_test)\n    \n    for key, item in model_dict.items():        \n        manager = Manager()\n        tokenizer = AutoTokenizer.from_pretrained(item[\"model_name\"])\n        test_dataset = feedbackDataset(test_texts, tokenizer, Config.max_len, True)\n        test_loader = torch.utils.data.DataLoader(test_dataset,\n                                                batch_size = Config.valid_batch_size,\n                                                pin_memory = True,\n                                                num_workers = Config.num_workers,  \n                                                shuffle = False)\n        model = FeedbackModel(item[\"config\"],\n                             Config.num_labels)\n        model.to(Config.device)\n        predictions = inference(model, test_loader, item[\"weights\"])\n        uniqueSubmitGroups = range(len(predictions))\n        \n        print('Making submit sequence datasets...')\n        submitSeqSets = manager.dict()\n        \n        def sequenceDataset(disc_type, submit=False):\n            print(f\"Making {disc_type} dataset\")\n            submitSeqSets[disc_type] = seq_dataset(disc_type, predictions, test_cluster)\n            \n        Parallel(n_jobs=-1, backend='multiprocessing')(\n                delayed(sequenceDataset)(disc_type, True) \n               for disc_type in disc_type_to_ids\n            )\n        print('Done.')\n        seq_cache = {} \n        clfs = []\n        thresholds = {}\n        for disc_type in disc_type_to_ids:\n            with open( f\"../input/fb-deberta-3/drive-download-20220315T155246Z-001/FB_deberta-large-v2-ITPT_ens_NF8-result5-cluster-5/results/{disc_type}_res.p\", \"rb\" ) as res_file:\n                train_result = pickle.load( res_file )  \n            thresholds[disc_type] = train_result['pred_thresh']\n            print(disc_type, train_result)\n            \n        sub = pd.concat([sub_df(predict_strings(disc_type, thresholds[disc_type], uniqueSubmitGroups)) for disc_type in disc_type_to_ids ]).reset_index(drop=True)\n        sub.to_csv(\"submission.csv\", index = None)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:47:19.207349Z","iopub.execute_input":"2022-03-15T15:47:19.207752Z","iopub.status.idle":"2022-03-15T15:50:00.537948Z","shell.execute_reply.started":"2022-03-15T15:47:19.207716Z","shell.execute_reply":"2022-03-15T15:50:00.537117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T15:50:00.550262Z","iopub.execute_input":"2022-03-15T15:50:00.550604Z","iopub.status.idle":"2022-03-15T15:50:00.565797Z","shell.execute_reply.started":"2022-03-15T15:50:00.550569Z","shell.execute_reply":"2022-03-15T15:50:00.565015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}