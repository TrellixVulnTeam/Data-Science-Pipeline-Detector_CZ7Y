{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n# This must be done before importing transformers\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T17:01:56.257234Z","iopub.execute_input":"2022-03-03T17:01:56.257849Z","iopub.status.idle":"2022-03-03T17:01:56.281786Z","shell.execute_reply.started":"2022-03-03T17:01:56.257757Z","shell.execute_reply":"2022-03-03T17:01:56.281067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# general\nimport pandas as pd\nimport numpy as np\nimport os\nimport ast\nimport copy\nimport random\nfrom joblib import Parallel, delayed\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\nimport gc\nfrom collections import defaultdict\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# nlp\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_cosine_schedule_with_warmup\nfrom transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler","metadata":{"execution":{"iopub.status.busy":"2022-03-03T17:01:56.288331Z","iopub.execute_input":"2022-03-03T17:01:56.289114Z","iopub.status.idle":"2022-03-03T17:01:58.647398Z","shell.execute_reply.started":"2022-03-03T17:01:56.289077Z","shell.execute_reply":"2022-03-03T17:01:58.646697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    savename = \"deberta-large\"\n    seed = 2022\n    n_folds = 5\n    num_workers = 2\n    fold = 0\n    model = \"microsoft/deberta-large\"\n    lr = 2.5e-5\n    n_accum = 1\n    output = \"/content/model\"\n    input = \"/content/data/\"\n    ner_csv = \"/content/train_NER.csv\"\n    max_len = 512\n    stride = 128\n    num_labels = 15\n    batch_size = 4\n    valid_batch_size = 4\n    epochs = 6\n    accumulation_steps = 1\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    apex = True\n    debug = False\n    if debug:\n        n_folds = 2\n        epochs = 2","metadata":{"execution":{"iopub.status.busy":"2022-03-03T17:01:58.649755Z","iopub.execute_input":"2022-03-03T17:01:58.650031Z","iopub.status.idle":"2022-03-03T17:01:58.679495Z","shell.execute_reply.started":"2022-03-03T17:01:58.649993Z","shell.execute_reply":"2022-03-03T17:01:58.678367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n\nLABELS_TO_IDS = {v:k for k,v in enumerate(output_labels)}\nLABELS_TO_IDS[\"PAD\"] = -100\nIDS_TO_LABELS = {k:v for k,v in enumerate(output_labels)}\nIDS_TO_LABELS[-100] = \"PAD\"","metadata":{"execution":{"iopub.status.busy":"2022-03-03T17:01:58.680994Z","iopub.execute_input":"2022-03-03T17:01:58.681253Z","iopub.status.idle":"2022-03-03T17:01:58.693599Z","shell.execute_reply.started":"2022-03-03T17:01:58.681222Z","shell.execute_reply":"2022-03-03T17:01:58.692842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(Config.seed)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T17:01:58.694683Z","iopub.execute_input":"2022-03-03T17:01:58.694999Z","iopub.status.idle":"2022-03-03T17:01:58.704374Z","shell.execute_reply.started":"2022-03-03T17:01:58.694963Z","shell.execute_reply":"2022-03-03T17:01:58.703725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_df_test():\n    test_names, df_test = [], []\n    for f in list(os.listdir('../input/feedback-prize-2021/test')):\n        test_names.append(f.replace('.txt', ''))\n        df_test.append(open('../input/feedback-prize-2021/test/' + f, 'r').read())\n    df_test = pd.DataFrame({'id': test_names, 'text': df_test})\n    df_test['text_split'] = df_test.text.str.split()\n    return df_test","metadata":{"execution":{"iopub.status.busy":"2022-03-03T17:01:58.705638Z","iopub.execute_input":"2022-03-03T17:01:58.706047Z","iopub.status.idle":"2022-03-03T17:01:58.712603Z","shell.execute_reply.started":"2022-03-03T17:01:58.706011Z","shell.execute_reply":"2022-03-03T17:01:58.711912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def load_df_test():\n#     test_names, df_test = [], []\n#     for f in list(os.listdir('../input/feedback-prize-2021/train')):\n#         test_names.append(f.replace('.txt', ''))\n#         df_test.append(open('../input/feedback-prize-2021/train/' + f, 'r').read())\n#     df_test = pd.DataFrame({'id': test_names[:10000], 'text': df_test[:10000]})\n#     df_test['text_split'] = df_test.text.str.split()\n#     return df_test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(df, max_len, tokenizer, to_tensor=True, with_labels=False):\n    \n    # This is what's different from a longformer\n    # Read the parameters with attention\n    encoded = tokenizer(df['text_split'].tolist(),\n                        is_split_into_words=True,\n                        return_overflowing_tokens=True,\n                        stride=128,\n                        max_length=max_len,\n                        padding=\"max_length\",\n                        truncation=True)\n\n    if with_labels:\n        encoded['labels'] = []\n\n    encoded['wids'] = []\n    n = len(encoded['overflow_to_sample_mapping'])\n    for i in range(n):\n\n        # Map back to original row\n        text_idx = encoded['overflow_to_sample_mapping'][i]\n        \n        # Get word indexes (this is a global index that takes into consideration the chunking :D )\n        word_ids = encoded.word_ids(i)\n        \n        if with_labels:\n            # Get word labels of the full un-chunked text\n            word_labels = df['entities'].iloc[text_idx]\n        \n            # Get the labels associated with the word indexes\n            label_ids = get_labels(word_ids, word_labels)\n            encoded['labels'].append(label_ids)\n        encoded['wids'].append([w if w is not None else -1 for w in word_ids])\n    \n    if to_tensor:\n        encoded = {key: torch.as_tensor(val) for key, val in encoded.items()}\n    return encoded","metadata":{"execution":{"iopub.status.busy":"2022-03-03T17:01:58.713947Z","iopub.execute_input":"2022-03-03T17:01:58.714449Z","iopub.status.idle":"2022-03-03T17:01:58.723845Z","shell.execute_reply.started":"2022-03-03T17:01:58.71431Z","shell.execute_reply":"2022-03-03T17:01:58.723167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackDatasetTest(Dataset):\n    def __init__(self, tokenized_ds):\n        self.data = tokenized_ds\n\n    def __getitem__(self, index):\n        item = {k: self.data[k][index] for k in self.data.keys()}\n        return item\n\n    def __len__(self):\n        return len(self.data['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2022-03-03T17:01:58.725116Z","iopub.execute_input":"2022-03-03T17:01:58.725553Z","iopub.status.idle":"2022-03-03T17:01:58.73533Z","shell.execute_reply.started":"2022-03-03T17:01:58.725491Z","shell.execute_reply":"2022-03-03T17:01:58.734561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackModel(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n\n        hidden_dropout_prob: float = 0.2\n        layer_norm_eps: float = 17589e-7\n\n        config = AutoConfig.from_pretrained(model_name)\n\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n                \"num_labels\": self.num_labels,\n            }\n        )\n        self.transformer = AutoModel.from_config(config=config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dropout3 = nn.Dropout(0.3)\n        self.dropout4 = nn.Dropout(0.4)\n        self.dropout5 = nn.Dropout(0.5)\n        self.output = nn.Linear(config.hidden_size, self.num_labels)\n\n    def forward(self, ids, mask, token_type_ids=None):\n\n        if token_type_ids:\n            transformer_out = self.transformer(ids, mask, token_type_ids)\n        else:\n            transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        sequence_output = self.dropout(sequence_output)\n\n        logits1 = self.output(self.dropout1(sequence_output))\n        logits2 = self.output(self.dropout2(sequence_output))\n        logits3 = self.output(self.dropout3(sequence_output))\n        logits4 = self.output(self.dropout4(sequence_output))\n        logits5 = self.output(self.dropout5(sequence_output))\n\n        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n        return logits, logits1, logits2, logits3, logits4, logits5","metadata":{"execution":{"iopub.status.busy":"2022-03-03T17:01:58.736673Z","iopub.execute_input":"2022-03-03T17:01:58.737079Z","iopub.status.idle":"2022-03-03T17:01:58.749699Z","shell.execute_reply.started":"2022-03-03T17:01:58.737044Z","shell.execute_reply":"2022-03-03T17:01:58.749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predictions(all_labels, all_scores, df):    \n    proba_thresh = {\n        \"Lead\": 0.7,\n        \"Position\": 0.55,\n        \"Evidence\": 0.65,\n        \"Claim\": 0.55,\n        \"Concluding Statement\": 0.7,\n        \"Counterclaim\": 0.5,\n        \"Rebuttal\": 0.55,\n    }\n    final_preds = []\n    \n    for i in range(len(df)):\n        idx = df.id.values[i]\n        pred = all_labels[i]\n        score = all_scores[i]\n        preds = []\n        j = 0\n        \n        while j < len(pred):\n            cls = pred[j]\n            if cls == 'O': pass\n            else: cls = cls.replace('B','I')\n            end = j + 1\n            while end < len(pred) and pred[end] == cls:\n                end += 1\n            if cls != 'O' and cls != '' and end - j > 7:\n                if np.mean(score[j:end]) > proba_thresh[cls.replace('I-','')]:\n                    final_preds.append((idx, cls.replace('I-',''), \n                                        ' '.join(map(str, list(range(j, end))))))\n            j = end\n    df_pred = pd.DataFrame(final_preds)\n    df_pred.columns = ['id','class','predictionstring']\n    return df_pred\n\ndef threshold(df):\n\n    min_thresh = {\n        \"Lead\": 9,\n        \"Position\": 5,\n        \"Evidence\": 14,\n        \"Claim\": 3,\n        \"Concluding Statement\": 11,\n        \"Counterclaim\": 6,\n        \"Rebuttal\": 4,\n    }\n\n    df = df.copy()\n    for key, value in min_thresh.items():\n        index = df.loc[df[\"class\"] == key].query(f\"len<{value}\").index\n        df.drop(index, inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-03-03T17:01:58.752661Z","iopub.execute_input":"2022-03-03T17:01:58.75312Z","iopub.status.idle":"2022-03-03T17:01:58.766697Z","shell.execute_reply.started":"2022-03-03T17:01:58.75309Z","shell.execute_reply":"2022-03-03T17:01:58.766043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef inference(model, weight, test_loader):\n    model.load_state_dict(torch.load(weight))\n    model.eval()\n    test_pbar = tqdm(enumerate(test_loader), total = len(test_loader))\n    for step, data in test_pbar:\n        input_ids = data[\"input_ids\"].to(Config.device)\n        input_mask = data[\"attention_mask\"].to(Config.device)\n\n        batch_size = input_ids.shape[0]\n        logits, logits1, logits2, logits3, logits4, logits5 = model(input_ids,\n                                                                        input_mask)\n\n        val_preds = logits.detach().cpu().numpy()\n        \n        yield val_preds    ","metadata":{"execution":{"iopub.status.busy":"2022-03-03T17:01:58.767891Z","iopub.execute_input":"2022-03-03T17:01:58.768577Z","iopub.status.idle":"2022-03-03T17:01:58.778744Z","shell.execute_reply.started":"2022-03-03T17:01:58.76854Z","shell.execute_reply":"2022-03-03T17:01:58.778013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jn(pst, start, end):\n    return \" \".join([str(x) for x in pst[start:end]])\n\n\ndef link_evidence(oof):\n    thresh = 1\n    idu = oof['id'].unique()\n    idc = idu[1]\n    eoof = oof[oof['class'] == \"Evidence\"]\n    neoof = oof[oof['class'] != \"Evidence\"]\n    for thresh2 in range(26,27, 1):\n        retval = []\n        for idv in idu:\n            for c in  ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n                   'Counterclaim', 'Rebuttal']:\n                q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n                if len(q) == 0:\n                    continue\n                pst = []\n                for i,r in q.iterrows():\n                    pst = pst +[-1] + [int(x) for x in r['predictionstring'].split()]\n                start = 1\n                end = 1\n                for i in range(2,len(pst)):\n                    cur = pst[i]\n                    end = i\n                    #if pst[start] == 205:\n                    #   print(cur, pst[start], cur - pst[start])\n                    if (cur == -1 and c != 'Evidence') or ((cur == -1) and ((pst[i+1] > pst[end-1] + thresh) or (pst[i+1] - pst[start] > thresh2))):\n                        retval.append((idv, c, jn(pst, start, end)))\n                        start = i + 1\n                v = (idv, c, jn(pst, start, end+1))\n                #print(v)\n                retval.append(v)\n        roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring']) \n        roof = roof.merge(neoof, how='outer')\n        return roof","metadata":{"execution":{"iopub.status.busy":"2022-03-03T17:01:58.783193Z","iopub.execute_input":"2022-03-03T17:01:58.783712Z","iopub.status.idle":"2022-03-03T17:01:58.795428Z","shell.execute_reply.started":"2022-03-03T17:01:58.783679Z","shell.execute_reply":"2022-03-03T17:01:58.79473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocess(test_preds, test_loader, test_df):\n    predictions = defaultdict(list)\n    prediction_scores = defaultdict(list)\n    seen_words_idx = defaultdict(list)\n    test_pbar = tqdm(enumerate(test_loader), total = len(test_loader))\n    for step, data in test_pbar:\n        pred = test_preds[step]\n        batch_preds = np.argmax(pred, axis=-1)\n        batch_scores = np.max(pred, axis=-1)\n        for k, (chunk_preds, chunk_scores, text_id) in enumerate(zip(batch_preds, batch_scores, data[\"overflow_to_sample_mapping\"].tolist())):\n            # The word_ids are absolute references in the original text\n            word_ids = data['wids'][k].numpy()\n\n            # Map from ids to labels\n            chunk_preds = [IDS_TO_LABELS[i] for i in chunk_preds]        \n\n            for idx, word_idx in enumerate(word_ids):                            \n                if word_idx == -1:\n                    pass\n                elif word_idx not in seen_words_idx[text_id]:\n                    # Add predictions if the word doesn't have a prediction from a previous chunk\n                    predictions[text_id].append(chunk_preds[idx])\n                    prediction_scores[text_id].append(chunk_scores[idx])\n                    seen_words_idx[text_id].append(word_idx)\n                    \n        final_predictions = [predictions[k] for k in sorted(predictions.keys())]\n        final_scores = [prediction_scores[k] for k in sorted(prediction_scores.keys())]\n        \n    df_pred = get_predictions(final_predictions, final_scores, test_df)\n    df_pred[\"len\"] = df_pred[\"predictionstring\"].apply(lambda x: len(x.split()))\n    print(df_pred[\"len\"])\n    df_pred = threshold(df_pred)\n    df_pred = link_evidence(df_pred)\n    df_pred[[\"id\", \"class\", \"predictionstring\"]].to_csv(\"submission.csv\", index = None)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T17:01:58.796613Z","iopub.execute_input":"2022-03-03T17:01:58.797155Z","iopub.status.idle":"2022-03-03T17:01:58.81098Z","shell.execute_reply.started":"2022-03-03T17:01:58.797119Z","shell.execute_reply":"2022-03-03T17:01:58.810147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dict = dict(\n    deberta_v3_l = dict(\n        model_name = \"../input/fb-deberta/deberta-v3-large/deberta-v3-large\",\n        weights = [f\"../input/fb-deberta/FB_debertav3-large_chunks/models/model_{fold}\" for fold in range(5)],\n        max_len = 512,\n        config_name = \"../input/fb-deberta/deberta-v3-large/deberta-v3-large/config.json\"\n    ),\n#     deberta_l = dict(\n#         model_name = \"../input/fb-deberta/deberta-large/deberta-large\",\n#         weights = [f\"../input/fb-deberta/FB_deberta-large_chunks/models/model_{fold}\" for fold in range(5)],\n#         max_len = 512,\n#         config_name = \"../input/fb-deberta/deberta-large/deberta-large/config.json\"\n#     )\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T17:01:58.812254Z","iopub.execute_input":"2022-03-03T17:01:58.812734Z","iopub.status.idle":"2022-03-03T17:01:58.822275Z","shell.execute_reply.started":"2022-03-03T17:01:58.812697Z","shell.execute_reply":"2022-03-03T17:01:58.821495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    test_df = load_df_test()\n    counter = 0\n    raw_preds = []\n    for key, item in model_dict.items():\n        print(f\"Predicting {key}\")\n        tokenizer = DebertaV2TokenizerFast.from_pretrained(item[\"model_name\"], add_prefix_space=True)\n        tokenized_test = tokenize(test_df, max_len = item[\"max_len\"], tokenizer = tokenizer)\n        test_dataset = FeedbackDatasetTest(tokenized_test)\n        test_loader = torch.utils.data.DataLoader(test_dataset,\n                                                    batch_size = 8,\n                                                    num_workers = 2,  \n                                                    shuffle = False)\n        model = FeedbackModel(item[\"model_name\"], 15)\n        model.to(Config.device)\n        for weight in item[\"weights\"]:\n            test_preds = inference(model, weight, test_loader)\n            for idx,pred in enumerate(test_preds):\n                pred = pred.astype(np.float16) / 5\n                if counter == 0:\n                    raw_preds.append(pred)\n                else:\n                    raw_preds[idx] += pred\n            counter += 1\n        del model, tokenizer, test_dataset, test_preds\n        gc.collect()\n        print(len(raw_preds))\n    postprocess(raw_preds, test_loader, test_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T17:01:58.823751Z","iopub.execute_input":"2022-03-03T17:01:58.824055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(\"submission.csv\").head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}