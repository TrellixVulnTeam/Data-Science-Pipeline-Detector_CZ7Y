{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HuggingFace Training Baseline\n\nI wanted to create my own baseline for this competition, and I tried to do so \"without peeking\" at the kernels published by others. Ideally this can be used for training on a Kaggle kernel. Let's see how good we can get. \n\nThis baseline is based on the following notebook by Sylvain Gugger: https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb\n\nI initially started building with Roberta - thanks to Chris Deotte for pointing to Longformer :) The evaluation code is from Rob Mulla.\n\nThe notebook requires a couple of hours to run, so we'll use W&B to be able to monitor it along the way and keep the record of our experiments. ","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"SAMPLE = False # set True for debugging","metadata":{"execution":{"iopub.status.busy":"2021-12-23T22:59:40.43361Z","iopub.execute_input":"2021-12-23T22:59:40.434Z","iopub.status.idle":"2021-12-23T22:59:40.438896Z","shell.execute_reply.started":"2021-12-23T22:59:40.433966Z","shell.execute_reply":"2021-12-23T22:59:40.437857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install seqeval -qq # evaluation metrics for training (not the competition metric)\n!pip install --upgrade wandb -qq # experiment tracking","metadata":{"execution":{"iopub.status.busy":"2021-12-23T22:59:40.441479Z","iopub.execute_input":"2021-12-23T22:59:40.442112Z","iopub.status.idle":"2021-12-23T23:00:00.091404Z","shell.execute_reply.started":"2021-12-23T22:59:40.442065Z","shell.execute_reply":"2021-12-23T23:00:00.090244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setup wandb for experiment tracking\n# source: https://www.kaggle.com/debarshichanda/pytorch-w-b-jigsaw-starter\n\nimport wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    wandb.init(project=\"feedback_prize\", entity=\"darek\")\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:00.094757Z","iopub.execute_input":"2021-12-23T23:00:00.095189Z","iopub.status.idle":"2021-12-23T23:00:08.865381Z","shell.execute_reply.started":"2021-12-23T23:00:00.095139Z","shell.execute_reply":"2021-12-23T23:00:08.86421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CONFIG\n\nEXP_NUM = 4\ntask = \"ner\"\nmodel_checkpoint = \"allenai/longformer-base-4096\"\nmax_length = 1024\nstride = 128\nmin_tokens = 6\nmodel_path = f'{model_checkpoint.split(\"/\")[-1]}-{EXP_NUM}'\n\n# TRAINING HYPERPARAMS\nBS = 4\nGRAD_ACC = 8\nLR = 5e-5\nWD = 0.01\nWARMUP = 0.1\nN_EPOCHS = 5","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-12-23T23:00:08.872471Z","iopub.execute_input":"2021-12-23T23:00:08.875384Z","iopub.status.idle":"2021-12-23T23:00:09.613866Z","shell.execute_reply.started":"2021-12-23T23:00:08.875328Z","shell.execute_reply":"2021-12-23T23:00:09.612856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# read train data\ntrain = pd.read_csv('../input/feedback-prize-2021/train.csv')\ntrain.head(1)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:09.615125Z","iopub.execute_input":"2021-12-23T23:00:09.615508Z","iopub.status.idle":"2021-12-23T23:00:11.240349Z","shell.execute_reply.started":"2021-12-23T23:00:09.615458Z","shell.execute_reply":"2021-12-23T23:00:11.239275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check unique classes\nclasses = train.discourse_type.unique().tolist()\nclasses","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:11.245598Z","iopub.execute_input":"2021-12-23T23:00:11.248663Z","iopub.status.idle":"2021-12-23T23:00:12.088646Z","shell.execute_reply.started":"2021-12-23T23:00:11.248611Z","shell.execute_reply":"2021-12-23T23:00:12.087709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setup label indices\n\nfrom collections import defaultdict\ntags = defaultdict()\n\nfor i, c in enumerate(classes):\n    tags[f'B-{c}'] = i\n    tags[f'I-{c}'] = i + len(classes)\ntags[f'O'] = len(classes) * 2\ntags[f'Special'] = -100\n    \nl2i = dict(tags)\n\ni2l = defaultdict()\nfor k, v in l2i.items(): \n    i2l[v] = k\ni2l[-100] = 'Special'\n\ni2l = dict(i2l)\n\nN_LABELS = len(i2l) - 1 # not accounting for -100","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:12.090074Z","iopub.execute_input":"2021-12-23T23:00:12.090401Z","iopub.status.idle":"2021-12-23T23:00:12.909927Z","shell.execute_reply.started":"2021-12-23T23:00:12.090357Z","shell.execute_reply":"2021-12-23T23:00:12.908979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some helper functions\n\nfrom pathlib import Path\n\npath = Path('../input/feedback-prize-2021/train')\n\ndef get_raw_text(ids):\n    with open(path/f'{ids}.txt', 'r') as file: data = file.read()\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:12.913651Z","iopub.execute_input":"2021-12-23T23:00:12.913893Z","iopub.status.idle":"2021-12-23T23:00:13.630498Z","shell.execute_reply.started":"2021-12-23T23:00:12.913861Z","shell.execute_reply":"2021-12-23T23:00:13.629554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# group training labels by text file\n\ndf1 = train.groupby('id')['discourse_type'].apply(list).reset_index(name='classlist')\ndf2 = train.groupby('id')['discourse_start'].apply(list).reset_index(name='starts')\ndf3 = train.groupby('id')['discourse_end'].apply(list).reset_index(name='ends')\ndf4 = train.groupby('id')['predictionstring'].apply(list).reset_index(name='predictionstrings')\n\ndf = pd.merge(df1, df2, how='inner', on='id')\ndf = pd.merge(df, df3, how='inner', on='id')\ndf = pd.merge(df, df4, how='inner', on='id')\ndf['text'] = df['id'].apply(get_raw_text)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:13.634902Z","iopub.execute_input":"2021-12-23T23:00:13.635138Z","iopub.status.idle":"2021-12-23T23:00:24.829274Z","shell.execute_reply.started":"2021-12-23T23:00:13.635107Z","shell.execute_reply":"2021-12-23T23:00:24.828189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# debugging\nif SAMPLE: df = df.sample(n=100).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:24.831063Z","iopub.execute_input":"2021-12-23T23:00:24.831421Z","iopub.status.idle":"2021-12-23T23:00:25.596595Z","shell.execute_reply.started":"2021-12-23T23:00:24.831375Z","shell.execute_reply":"2021-12-23T23:00:25.595633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will use HuggingFace datasets\nfrom datasets import Dataset, load_metric\n\nds = Dataset.from_pandas(df)\ndatasets = ds.train_test_split(test_size=0.1, shuffle=True, seed=42)\ndatasets","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:25.59961Z","iopub.execute_input":"2021-12-23T23:00:25.600322Z","iopub.status.idle":"2021-12-23T23:00:26.415085Z","shell.execute_reply.started":"2021-12-23T23:00:25.600259Z","shell.execute_reply":"2021-12-23T23:00:26.413987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n    \ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:26.416852Z","iopub.execute_input":"2021-12-23T23:00:26.417192Z","iopub.status.idle":"2021-12-23T23:00:31.722501Z","shell.execute_reply.started":"2021-12-23T23:00:26.417127Z","shell.execute_reply":"2021-12-23T23:00:31.721572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Not sure if this is needed, but in case we create a span with certain class without starting token of that class,\n# let's convert the first token to be the starting token.\n\ne = [0,7,7,7,1,1,8,8,8,9,9,9,14,4,4,4]\n\ndef fix_beginnings(labels):\n    for i in range(1,len(labels)):\n        curr_lab = labels[i]\n        prev_lab = labels[i-1]\n        if curr_lab in range(7,14):\n            if prev_lab != curr_lab and prev_lab != curr_lab - 7:\n                labels[i] = curr_lab -7\n    return labels\n\nfix_beginnings(e)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:31.724112Z","iopub.execute_input":"2021-12-23T23:00:31.724482Z","iopub.status.idle":"2021-12-23T23:00:32.494243Z","shell.execute_reply.started":"2021-12-23T23:00:31.724438Z","shell.execute_reply":"2021-12-23T23:00:32.49297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenize and add labels\ndef tokenize_and_align_labels(examples):\n\n    o = tokenizer(examples['text'], truncation=True, padding=True, return_offsets_mapping=True, max_length=max_length, stride=stride, return_overflowing_tokens=True)\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = o[\"overflow_to_sample_mapping\"]\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = o[\"offset_mapping\"]\n    \n    o[\"labels\"] = []\n\n    for i in range(len(offset_mapping)):\n                   \n        sample_index = sample_mapping[i]\n\n        labels = [l2i['O'] for i in range(len(o['input_ids'][i]))]\n\n        for label_start, label_end, label in \\\n        list(zip(examples['starts'][sample_index], examples['ends'][sample_index], examples['classlist'][sample_index])):\n            for j in range(len(labels)):\n                token_start = offset_mapping[i][j][0]\n                token_end = offset_mapping[i][j][1]\n                if token_start == label_start: \n                    labels[j] = l2i[f'B-{label}']    \n                if token_start > label_start and token_end <= label_end: \n                    labels[j] = l2i[f'I-{label}']\n\n        for k, input_id in enumerate(o['input_ids'][i]):\n            if input_id in [0,1,2]:\n                labels[k] = -100\n\n        labels = fix_beginnings(labels)\n                   \n        o[\"labels\"].append(labels)\n        \n    return o","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:32.495836Z","iopub.execute_input":"2021-12-23T23:00:32.496208Z","iopub.status.idle":"2021-12-23T23:00:33.263669Z","shell.execute_reply.started":"2021-12-23T23:00:32.49614Z","shell.execute_reply":"2021-12-23T23:00:33.262629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True, \\\n                                  batch_size=20000, remove_columns=datasets[\"train\"].column_names)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:33.265142Z","iopub.execute_input":"2021-12-23T23:00:33.265646Z","iopub.status.idle":"2021-12-23T23:00:35.856612Z","shell.execute_reply.started":"2021-12-23T23:00:33.265601Z","shell.execute_reply":"2021-12-23T23:00:35.855589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:35.858326Z","iopub.execute_input":"2021-12-23T23:00:35.858635Z","iopub.status.idle":"2021-12-23T23:00:36.592654Z","shell.execute_reply.started":"2021-12-23T23:00:35.85859Z","shell.execute_reply":"2021-12-23T23:00:36.591606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model and Training","metadata":{}},{"cell_type":"code","source":"# we will use auto model for token classification\n\nfrom transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n\nmodel = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=N_LABELS)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:36.59433Z","iopub.execute_input":"2021-12-23T23:00:36.594634Z","iopub.status.idle":"2021-12-23T23:00:40.685632Z","shell.execute_reply.started":"2021-12-23T23:00:36.594593Z","shell.execute_reply":"2021-12-23T23:00:40.684693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = model_checkpoint.split(\"/\")[-1]\nargs = TrainingArguments(\n    f\"{model_name}-finetuned-{task}\",\n    evaluation_strategy = \"epoch\",\n    logging_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=LR,\n    per_device_train_batch_size=BS,\n    per_device_eval_batch_size=BS,\n    num_train_epochs=N_EPOCHS,\n    weight_decay=WD,\n    report_to='wandb', \n    gradient_accumulation_steps=GRAD_ACC,\n    warmup_ratio=WARMUP\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:40.690854Z","iopub.execute_input":"2021-12-23T23:00:40.693718Z","iopub.status.idle":"2021-12-23T23:00:41.535273Z","shell.execute_reply.started":"2021-12-23T23:00:40.693672Z","shell.execute_reply":"2021-12-23T23:00:41.534215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:41.53676Z","iopub.execute_input":"2021-12-23T23:00:41.537608Z","iopub.status.idle":"2021-12-23T23:00:42.282789Z","shell.execute_reply.started":"2021-12-23T23:00:41.537572Z","shell.execute_reply":"2021-12-23T23:00:42.281853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this is not the competition metric, but for now this will be better than nothing...\n\nmetric = load_metric(\"seqeval\")","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:42.284192Z","iopub.execute_input":"2021-12-23T23:00:42.284501Z","iopub.status.idle":"2021-12-23T23:00:43.656933Z","shell.execute_reply.started":"2021-12-23T23:00:42.284458Z","shell.execute_reply":"2021-12-23T23:00:43.655937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [i2l[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [i2l[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:43.658571Z","iopub.execute_input":"2021-12-23T23:00:43.658881Z","iopub.status.idle":"2021-12-23T23:00:44.386693Z","shell.execute_reply.started":"2021-12-23T23:00:43.658824Z","shell.execute_reply":"2021-12-23T23:00:44.385607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics, \n)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:44.388421Z","iopub.execute_input":"2021-12-23T23:00:44.388744Z","iopub.status.idle":"2021-12-23T23:00:45.313179Z","shell.execute_reply.started":"2021-12-23T23:00:44.38869Z","shell.execute_reply":"2021-12-23T23:00:45.312215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:00:45.314663Z","iopub.execute_input":"2021-12-23T23:00:45.318411Z","iopub.status.idle":"2021-12-23T23:03:13.651205Z","shell.execute_reply.started":"2021-12-23T23:00:45.318345Z","shell.execute_reply":"2021-12-23T23:03:13.650259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model(model_path)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:03:13.656546Z","iopub.execute_input":"2021-12-23T23:03:13.656788Z","iopub.status.idle":"2021-12-23T23:03:15.317965Z","shell.execute_reply.started":"2021-12-23T23:03:13.656757Z","shell.execute_reply":"2021-12-23T23:03:15.316868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation","metadata":{}},{"cell_type":"code","source":"def tokenize_for_validation(examples):\n\n    o = tokenizer(examples['text'], truncation=True, return_offsets_mapping=True, max_length=4096)\n\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = o[\"offset_mapping\"]\n    \n    o[\"labels\"] = []\n\n    for i in range(len(offset_mapping)):\n                   \n        labels = [l2i['O'] for i in range(len(o['input_ids'][i]))]\n\n        for label_start, label_end, label in \\\n        list(zip(examples['starts'][i], examples['ends'][i], examples['classlist'][i])):\n            for j in range(len(labels)):\n                token_start = offset_mapping[i][j][0]\n                token_end = offset_mapping[i][j][1]\n                if token_start == label_start: \n                    labels[j] = l2i[f'B-{label}']    \n                if token_start > label_start and token_end <= label_end: \n                    labels[j] = l2i[f'I-{label}']\n\n        for k, input_id in enumerate(o['input_ids'][i]):\n            if input_id in [0,1,2]:\n                labels[k] = -100\n\n        labels = fix_beginnings(labels)\n                   \n        o[\"labels\"].append(labels)\n        \n    return o","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:03:15.31952Z","iopub.execute_input":"2021-12-23T23:03:15.319834Z","iopub.status.idle":"2021-12-23T23:03:15.332639Z","shell.execute_reply.started":"2021-12-23T23:03:15.319782Z","shell.execute_reply":"2021-12-23T23:03:15.331235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_val = datasets.map(tokenize_for_validation, batched=True)\ntokenized_val","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:03:15.334494Z","iopub.execute_input":"2021-12-23T23:03:15.335669Z","iopub.status.idle":"2021-12-23T23:03:16.652272Z","shell.execute_reply.started":"2021-12-23T23:03:15.335596Z","shell.execute_reply":"2021-12-23T23:03:16.651209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ground truth for validation\n\nl = []\nfor example in tokenized_val['test']:\n    for c, p in list(zip(example['classlist'], example['predictionstrings'])):\n        l.append({\n            'id': example['id'],\n            'discourse_type': c,\n            'predictionstring': p,\n        })\n    \ngt_df = pd.DataFrame(l)\ngt_df","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:03:16.654017Z","iopub.execute_input":"2021-12-23T23:03:16.654625Z","iopub.status.idle":"2021-12-23T23:03:16.711036Z","shell.execute_reply.started":"2021-12-23T23:03:16.654567Z","shell.execute_reply":"2021-12-23T23:03:16.710012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualization with displacy\n\nimport pandas as pd\nimport os\nfrom pathlib import Path\nimport spacy\nfrom spacy import displacy\nfrom pylab import cm, matplotlib","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:03:16.712458Z","iopub.execute_input":"2021-12-23T23:03:16.713221Z","iopub.status.idle":"2021-12-23T23:03:16.719502Z","shell.execute_reply.started":"2021-12-23T23:03:16.713168Z","shell.execute_reply":"2021-12-23T23:03:16.718212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = Path('../input/feedback-prize-2021/train')\n\ncolors = {\n            'Lead': '#8000ff',\n            'Position': '#2b7ff6',\n            'Evidence': '#2adddd',\n            'Claim': '#80ffb4',\n            'Concluding Statement': 'd4dd80',\n            'Counterclaim': '#ff8042',\n            'Rebuttal': '#ff0000',\n            'Other': '#007f00',\n         }\n\ndef visualize(df, text):\n    ents = []\n    example = df['id'].loc[0]\n\n    for i, row in df.iterrows():\n        ents.append({\n                        'start': int(row['discourse_start']), \n                         'end': int(row['discourse_end']), \n                         'label': row['discourse_type']\n                    })\n\n    doc2 = {\n        \"text\": text,\n        \"ents\": ents,\n        \"title\": example\n    }\n\n    options = {\"ents\": train.discourse_type.unique().tolist() + ['Other'], \"colors\": colors}\n    displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:03:16.721142Z","iopub.execute_input":"2021-12-23T23:03:16.721798Z","iopub.status.idle":"2021-12-23T23:03:16.733508Z","shell.execute_reply.started":"2021-12-23T23:03:16.721753Z","shell.execute_reply":"2021-12-23T23:03:16.732443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions, labels, _ = trainer.predict(tokenized_val['test'])","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:03:16.735115Z","iopub.execute_input":"2021-12-23T23:03:16.736247Z","iopub.status.idle":"2021-12-23T23:03:17.621012Z","shell.execute_reply.started":"2021-12-23T23:03:16.736199Z","shell.execute_reply":"2021-12-23T23:03:17.619921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.argmax(predictions, axis=-1)\npreds.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:03:17.622787Z","iopub.execute_input":"2021-12-23T23:03:17.623357Z","iopub.status.idle":"2021-12-23T23:03:17.632659Z","shell.execute_reply.started":"2021-12-23T23:03:17.623297Z","shell.execute_reply":"2021-12-23T23:03:17.631425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# code that will convert our predictions into prediction strings, and visualize it at the same time\n# this most likely requires some refactoring\n\ndef get_class(c):\n    if c == 14: return 'Other'\n    else: return i2l[c][2:]\n\ndef pred2span(pred, example, viz=False, test=False):\n    example_id = example['id']\n    n_tokens = len(example['input_ids'])\n    classes = []\n    all_span = []\n    for i, c in enumerate(pred.tolist()):\n        if i == n_tokens-1:\n            break\n        if i == 0:\n            cur_span = example['offset_mapping'][i]\n            classes.append(get_class(c))\n        elif i > 0 and (c == pred[i-1] or (c-7) == pred[i-1]):\n            cur_span[1] = example['offset_mapping'][i][1]\n        else:\n            all_span.append(cur_span)\n            cur_span = example['offset_mapping'][i]\n            classes.append(get_class(c))\n    all_span.append(cur_span)\n    \n    if test: text = get_test_text(example_id)\n    else: text = get_raw_text(example_id)\n    \n    # abra ka dabra se soli fanta ko pelo\n    \n    # map token ids to word (whitespace) token ids\n    predstrings = []\n    for span in all_span:\n        span_start = span[0]\n        span_end = span[1]\n        before = text[:span_start]\n        token_start = len(before.split())\n        if len(before) == 0: token_start = 0\n        elif before[-1] != ' ': token_start -= 1\n        num_tkns = len(text[span_start:span_end+1].split())\n        tkns = [str(x) for x in range(token_start, token_start+num_tkns)]\n        predstring = ' '.join(tkns)\n        predstrings.append(predstring)\n                    \n    rows = []\n    for c, span, predstring in zip(classes, all_span, predstrings):\n        e = {\n            'id': example_id,\n            'discourse_type': c,\n            'predictionstring': predstring,\n            'discourse_start': span[0],\n            'discourse_end': span[1],\n            'discourse': text[span[0]:span[1]+1]\n        }\n        rows.append(e)\n\n\n    df = pd.DataFrame(rows)\n    df['length'] = df['discourse'].apply(lambda t: len(t.split()))\n    \n    # short spans are likely to be false positives, we can choose a min number of tokens based on validation\n    df = df[df.length > min_tokens].reset_index(drop=True)\n    if viz: visualize(df, text)\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:03:17.634765Z","iopub.execute_input":"2021-12-23T23:03:17.63535Z","iopub.status.idle":"2021-12-23T23:03:17.655065Z","shell.execute_reply.started":"2021-12-23T23:03:17.635228Z","shell.execute_reply":"2021-12-23T23:03:17.653955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred2span(preds[0], tokenized_val['test'][0], viz=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:03:17.658868Z","iopub.execute_input":"2021-12-23T23:03:17.659221Z","iopub.status.idle":"2021-12-23T23:03:17.712976Z","shell.execute_reply.started":"2021-12-23T23:03:17.659184Z","shell.execute_reply":"2021-12-23T23:03:17.711747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred2span(preds[1], tokenized_val['test'][1], viz=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:03:17.71609Z","iopub.execute_input":"2021-12-23T23:03:17.716626Z","iopub.status.idle":"2021-12-23T23:03:17.757272Z","shell.execute_reply.started":"2021-12-23T23:03:17.716588Z","shell.execute_reply":"2021-12-23T23:03:17.756227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs = []\nfor i in range(len(tokenized_val['test'])):\n    dfs.append(pred2span(preds[i], tokenized_val['test'][i]))\n\npred_df = pd.concat(dfs, axis=0)\npred_df['class'] = pred_df['discourse_type']\npred_df","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:03:17.759337Z","iopub.execute_input":"2021-12-23T23:03:17.760071Z","iopub.status.idle":"2021-12-23T23:03:17.883329Z","shell.execute_reply.started":"2021-12-23T23:03:17.760003Z","shell.execute_reply":"2021-12-23T23:03:17.8822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# source: https://www.kaggle.com/robikscube/student-writing-competition-twitch#Competition-Metric-Code\n\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(\" \"))\n    set_gt = set(row.predictionstring_gt.split(\" \"))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter / len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp_micro(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n\n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = (\n        gt_df[[\"id\", \"discourse_type\", \"predictionstring\"]]\n        .reset_index(drop=True)\n        .copy()\n    )\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    pred_df[\"pred_id\"] = pred_df.index\n    gt_df[\"gt_id\"] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(\n        gt_df,\n        left_on=[\"id\", \"class\"],\n        right_on=[\"id\", \"discourse_type\"],\n        how=\"outer\",\n        suffixes=(\"_pred\", \"_gt\"),\n    )\n    joined[\"predictionstring_gt\"] = joined[\"predictionstring_gt\"].fillna(\" \")\n    joined[\"predictionstring_pred\"] = joined[\"predictionstring_pred\"].fillna(\" \")\n\n    joined[\"overlaps\"] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5,\n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined[\"overlap1\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[0])\n    joined[\"overlap2\"] = joined[\"overlaps\"].apply(lambda x: eval(str(x))[1])\n\n    joined[\"potential_TP\"] = (joined[\"overlap1\"] >= 0.5) & (joined[\"overlap2\"] >= 0.5)\n    joined[\"max_overlap\"] = joined[[\"overlap1\", \"overlap2\"]].max(axis=1)\n    tp_pred_ids = (\n        joined.query(\"potential_TP\")\n        .sort_values(\"max_overlap\", ascending=False)\n        .groupby([\"id\", \"predictionstring_gt\"])\n        .first()[\"pred_id\"]\n        .values\n    )\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined[\"pred_id\"].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query(\"potential_TP\")[\"gt_id\"].unique()\n    unmatched_gt_ids = [c for c in joined[\"gt_id\"].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    # calc microf1\n    my_f1_score = TP / (TP + 0.5 * (FP + FN))\n    return my_f1_score\n\n\ndef score_feedback_comp(pred_df, gt_df, return_class_scores=False):\n    class_scores = {}\n    pred_df = pred_df[[\"id\", \"class\", \"predictionstring\"]].reset_index(drop=True).copy()\n    for discourse_type, gt_subset in gt_df.groupby(\"discourse_type\"):\n        pred_subset = (\n            pred_df.loc[pred_df[\"class\"] == discourse_type]\n            .reset_index(drop=True)\n            .copy()\n        )\n        class_score = score_feedback_comp_micro(pred_subset, gt_subset)\n        class_scores[discourse_type] = class_score\n    f1 = np.mean([v for v in class_scores.values()])\n    if return_class_scores:\n        return f1, class_scores\n    return f1","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:03:17.885121Z","iopub.execute_input":"2021-12-23T23:03:17.885735Z","iopub.status.idle":"2021-12-23T23:03:17.908285Z","shell.execute_reply.started":"2021-12-23T23:03:17.88567Z","shell.execute_reply":"2021-12-23T23:03:17.907198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CV Score","metadata":{}},{"cell_type":"code","source":"score_feedback_comp(pred_df, gt_df, return_class_scores=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-23T23:03:17.910018Z","iopub.execute_input":"2021-12-23T23:03:17.910701Z","iopub.status.idle":"2021-12-23T23:03:18.110011Z","shell.execute_reply.started":"2021-12-23T23:03:17.910652Z","shell.execute_reply":"2021-12-23T23:03:18.108723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## End\n\nI'll appreciate every upvote or comment!","metadata":{}}]}