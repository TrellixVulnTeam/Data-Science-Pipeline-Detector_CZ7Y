{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **EDA Simplified: Feedback Prize (Adapted by Ibrez and Sanskar Hasija)**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Introduction\nIn many schools across the entire country, writing is a key to success for a lot of students because they want to get into advanced universities, although I want to improve my writing in this notebook. But unfortunately, just for low-income, Black, and Hispanic students fare even worse, with less than 15 percent demonstrating writing proficiency. One way to help students improve their writing is via automated feedback tools, which evaluate student writing and provide personalized feedback. How? We can identify and analyze argumentative writing elements from every students' writing starting from grade 6 to grade 12. So, let's go on to it! ","metadata":{}},{"cell_type":"markdown","source":"## Imports and Setup\nBefore we analyze argumentative writing elements, first things first, we import stuff! First of all, import the os module for files and directories. Then, we also need to import SpaCy (spacy) and wordcloud modules because they were used to analyze text from any resource. We then import the NumPy (numpy) module as np and the Pandas (pandas) module as pd since these modules contribute machine learning and data analysis to the 6th to 12th grade students' essays. Finally, import the plotting modules, Seaborn (seaborn) as sns, Plotly with the express attribute as px, the famous Matplotlib module that has the pyplot attribute as plt, and finally, another Plotly module with the graph_objects attribute as go.","metadata":{}},{"cell_type":"code","source":"# Files and Directories\nimport os\n\n# Text Analysis\nimport spacy\nimport wordcloud\n\n# Machine Learning and Data Analysis\nimport pandas as pd\nimport numpy as np\n\n# Plotting\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:36:16.822415Z","iopub.execute_input":"2022-02-20T07:36:16.823092Z","iopub.status.idle":"2022-02-20T07:36:26.753809Z","shell.execute_reply.started":"2022-02-20T07:36:16.822981Z","shell.execute_reply":"2022-02-20T07:36:26.753186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then setup the train and test directories by defining two variables: TRAIN_DIR and TEST_DIR. We assign TRAIN_DIR to the training path (and also TEST_DIR to the testing path) of the feedback-prize-2021 competition data. We then assign two variables: train_files and test_files to a list containing the names of the entries in the directories of the TRAIN_DIR and TEST_DIR image paths. We now create a for loop, looping the variable file in the range between the number of entities (len function) of train_files (and test_files). Inside the two for loops, the train_file and the test_file variable with the file key as variable index is assigned to the TRAIN_DIR and TEST_DIR variables plus the \"/\" string and again plus the string version of themselves with the file variable as the key index again. Now, we assign the train_df variable to the pandas module reading the train.csv from the feedback-prize-2021 competition dataset along with the sub_df variable, but the pandas module read the csv file, sample_submission.csv. ","metadata":{}},{"cell_type":"code","source":"# ℹ️: You can copy file path in data sections if you would want!\nTRAIN_DIR = \"../input/feedback-prize-2021/train\"\nTEST_DIR = \"../input/feedback-prize-2021/test\"\ntrain_files = os.listdir(TRAIN_DIR)\ntest_files = os.listdir(TEST_DIR)\n\nfor file in range(len(train_files)):\n    train_files[file] = TRAIN_DIR + \"/\" + str(train_files[file])\n    \nfor file in range(len(test_files)):\n    test_files[file] = TEST_DIR + \"/\" + str(test_files[file])\n    \ntrain_df = pd.read_csv(\"../input/feedback-prize-2021/train.csv\")\nsub_df = pd.read_csv(\"../input/feedback-prize-2021/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:36:26.755047Z","iopub.execute_input":"2022-02-20T07:36:26.755715Z","iopub.status.idle":"2022-02-20T07:36:28.677533Z","shell.execute_reply.started":"2022-02-20T07:36:26.755686Z","shell.execute_reply":"2022-02-20T07:36:28.676641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA\nNow, let's do the EDA part! First, we will print out the number of train and test files by using the train_files and test_files variable that are caked inside the len function.","metadata":{}},{"cell_type":"code","source":"print(\"No. of train files: \", len(train_files))\nprint(\"No. of test files: \", len(test_files))","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:36:28.679136Z","iopub.execute_input":"2022-02-20T07:36:28.679453Z","iopub.status.idle":"2022-02-20T07:36:28.685173Z","shell.execute_reply.started":"2022-02-20T07:36:28.679412Z","shell.execute_reply":"2022-02-20T07:36:28.684281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After running this code cell above, we can see that there are 15,594 train files and just 5 test files counted! Now, let's analyze one of the train essays that the anonyomous 6th-to-12th grade students wrote!","metadata":{}},{"cell_type":"markdown","source":"### Analysis of Train Essay Sample\nIn order to analyze the sample of the train essay, we define a variable called f and assign it to open the train_files with any index number (mine's 9) and read it (as \"r\"). We then read it using the f variable with the read function.","metadata":{}},{"cell_type":"code","source":"f = open(train_files[9], \"r\")\nprint(f.read())","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:36:28.688333Z","iopub.execute_input":"2022-02-20T07:36:28.688648Z","iopub.status.idle":"2022-02-20T07:36:28.702121Z","shell.execute_reply.started":"2022-02-20T07:36:28.688608Z","shell.execute_reply":"2022-02-20T07:36:28.701426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see here, we analyzed an anonymous student wrote an argument about how less driving reduces pollution and protects the environment. But what about the test essay sample? Well, we just move on...","metadata":{}},{"cell_type":"markdown","source":"### Analysis of Test Essay Sample\nLike the part where we analyzed the train essay sample, let's now analyze the test essay sample. It's the same code as the part where we analyzed the train essay sample but the f variable is assigned to open and read the test_files variable that has the slice index of any number (mine's 1).","metadata":{}},{"cell_type":"code","source":"f = open(test_files[0], \"r\")\nprint(f.read())","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:36:28.703278Z","iopub.execute_input":"2022-02-20T07:36:28.703499Z","iopub.status.idle":"2022-02-20T07:36:28.710295Z","shell.execute_reply.started":"2022-02-20T07:36:28.703474Z","shell.execute_reply":"2022-02-20T07:36:28.709518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After analyzing the test essay sample we had, we can see that another anonyomous student argues about asking for other's opinions is especially beneficial as wise. Now, without further ado, let's dive in to the basics of the train tabular dataframe part!","metadata":{}},{"cell_type":"markdown","source":"### Train Tabular Dataframe\nHere are the basics of the train tabular dataframe!\n* **id**: This is an ID Code, just for essay response\n* **discourse_id**: This is an ID Code again, but for discourse element.\n* **discourse_start**: A character position where the discourse element starts in the essay response.\n* **discourse_end**: Same as discourse_start but the discourse element ends in the essay response.\n* **discourse_text**: This represents the text of the discourse element.\n* **discourse_type**: That is the classification of discourse element.\n* **discourse_type_num**: That is the enumerated class label of the discourse element.\n* **predictionstring**: The word indices extracted from the training sample, obligatory for predictions.","metadata":{}},{"cell_type":"markdown","source":"### Sneak Peek of a Train Dataframe\nAfter showing off the basics of the training columns of this training DataFrame, let's now take a peek of the train_df dataframe by displaying the first 5 rows using the head function!","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:36:28.712608Z","iopub.execute_input":"2022-02-20T07:36:28.71323Z","iopub.status.idle":"2022-02-20T07:36:28.735756Z","shell.execute_reply.started":"2022-02-20T07:36:28.713199Z","shell.execute_reply":"2022-02-20T07:36:28.73499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As mentioned, there are eight entities of data stored in a training DataFrame! But we now go further on analyzing the statistics of our sample of the training data!","metadata":{}},{"cell_type":"markdown","source":"### Analyzing the Basic Statistics of Training Data\nFirst of all, we need to find out the number of rows. Without further ado, we print out the number of rows in the training dataframe by measuring the number of entities of the train_df dataframe using the len function!","metadata":{}},{"cell_type":"code","source":"print(\"No of rows in the training dataframe: \", len(train_df))","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:36:28.737078Z","iopub.execute_input":"2022-02-20T07:36:28.737534Z","iopub.status.idle":"2022-02-20T07:36:28.742979Z","shell.execute_reply.started":"2022-02-20T07:36:28.737489Z","shell.execute_reply":"2022-02-20T07:36:28.742005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After running this code cell up above, we can see that there are 144,293 rows in the training dataframe. But, there is one more thing to find, finding the std, mean, min, max, and the iqr of the data in discourse_id, discourse_start, and discourse_end. Well, we can find the number in std, mean, min, max, and the iqr for that by using plugging in the describe function in the train_df dataframe. ","metadata":{}},{"cell_type":"markdown","source":"Now, we start to find the statistics of our train_df DataFrame by printing out the train_df dataframe plugged in with the describe function.","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:36:28.744462Z","iopub.execute_input":"2022-02-20T07:36:28.744854Z","iopub.status.idle":"2022-02-20T07:36:28.790029Z","shell.execute_reply.started":"2022-02-20T07:36:28.744802Z","shell.execute_reply":"2022-02-20T07:36:28.789107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now analyze the basic statistics of discourse_id, discourse_start, and discourse_end data entities.","metadata":{}},{"cell_type":"markdown","source":"### Null Values Check\nMost importantly, we need to check whether our train_df dataframe has any null values. We checked it by counting the null values with the train_df variable that is plugged with the isnull function and the sum function. Let's do it!","metadata":{}},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:36:28.791237Z","iopub.execute_input":"2022-02-20T07:36:28.791476Z","iopub.status.idle":"2022-02-20T07:36:28.84761Z","shell.execute_reply.started":"2022-02-20T07:36:28.791449Z","shell.execute_reply":"2022-02-20T07:36:28.846825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see this, we counted zero entities of null values in each data in the train_df dataframe! Now, let's get started on distributing data!","metadata":{}},{"cell_type":"markdown","source":"## Distributing the Data\nLet's now distribute out the data! First of all, let's analyze the seven different discourse type students wrote out! \n* Lead: a compelling and interesting first section of your paper that tells about the issue and has an attention getting hook (Also known as the inviting introduction).\n* Position: an opinion or conclusion on the main question or problem.\n* Claim: an argument based on facts and reasoning.\n* Counterclaim: a claim that refutes another claim or give out an opposing reason to the position.\n* Rebuttal: a claim that opposes a counterclaim\n* Evidence: personal experiences, definitions, facts, research, data, quotes from an authority in the field, or statistical graphs which tend to support or prove something.\n* Concluding Statement: the final section of your paper that clearly summarizes the points made and is supported by evidence (Also known as the purposeful conclusion).","metadata":{}},{"cell_type":"markdown","source":"### Distributing the Discourse Type\nLet's count the seven discourse types of the argumentative essay, and now we plot them together! First, we define a variable that sets up our graph figure with the plotly express module px and set it to the bar figure graph using the bar function, which contains the x variable to being defined to np module that gives out the unique variables of the discourse_type data key in the train_df dataframe using the unique function. We then define another variable called y to return a list containing a key data, discourse_type which it is in the train_df dataframe and then count them with the count function containing the i variable, which loops in the np module that gives out the unique values (with the unique function) of the discourse_type data key in the train_df dataframe, thus having it surrounded by parentheses. We then set up the colors of the data counts in the train_df dataframe by define color as our variable to the np module that give out the unique values of the discourse_type key in the train_df dataframe using the unique function, and set the color_continuous_scale variable to Emrld, or Peach or Mint, or whatever you want! (ℹ️: the variables, y, color, and color_continuous_scale all belongs inside the px.bar figure setup.) We then update the x-axis and y-axis with the labels in our fig graph figure variable by using the update_xaxes and update_yaxes function, which contained a title attribute to specify the labels for both x and y axis to Classes (for x-axis) and Number of Rows (for y-axis). Finally, on our setup, we update layout for our graph figure by using the fig variable with the update_layout function with the showlegend hyperparameter being set to true, and the title set to a dictionary consisting of setting the keys: text set to Discourse Type Distribution, y set to 0.95, x set to 0.5, xanchor set to center, and yanchor set to top, thus, the template hyperparameter is set to... whatever built-in template you like (mine's seaborn). And thus, showing our figure using the show function that is plugged in with the fig variable.","metadata":{}},{"cell_type":"code","source":"fig = px.bar(x=np.unique(train_df[\"discourse_type\"]), y=[list(train_df[\"discourse_type\"]).count(i) for i in np.unique(train_df[\"discourse_type\"])], color=np.unique(train_df[\"discourse_type\"]), color_continuous_scale=\"Mint\")\nfig.update_xaxes(title=\"Classes\")\nfig.update_yaxes(title=\"Number of Rows\")\nfig.update_layout(showlegend=True, \n                  title={\n                      'text':'Discourse Type Distribution', \n                      'y':0.95, \n                      'x':0.5, \n                      'xanchor':'center', \n                      'yanchor':'top'}, template=\"seaborn\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:36:28.84951Z","iopub.execute_input":"2022-02-20T07:36:28.849773Z","iopub.status.idle":"2022-02-20T07:36:30.217384Z","shell.execute_reply.started":"2022-02-20T07:36:28.849745Z","shell.execute_reply":"2022-02-20T07:36:30.216588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, The number of Claim stated in the essay is 50K, Concluding Statement is about 13K, Counterclaim is 5817, Lead is 9305, Position is about 15K, and Rebuttal is 4337.","metadata":{}},{"cell_type":"markdown","source":"### Enumeration Over The Enumerated Class Label of Discourse Element Distribution\nWe now create another graph for sure! It's similar to the one we distributed the discourse type, but we were evaluating the enumerated class labels of Enumerated class label of Discourse Element Distribution for our title in the update_layout function parameters, and we are creating a bar graph consisting of the key in train_df, discourse_type_num.","metadata":{}},{"cell_type":"code","source":"fig = px.bar(x=np.unique(train_df[\"discourse_type_num\"]), y=[list(train_df[\"discourse_type_num\"]).count(i) for i in np.unique(train_df[\"discourse_type_num\"])], color=np.unique(train_df[\"discourse_type_num\"]), color_continuous_scale=\"Mint\")\nfig.update_xaxes(title=\"Classes\")\nfig.update_yaxes(title=\"Number of Rows\")\nfig.update_layout(showlegend=True, \n                  title={\n                      'text':'Enumerated class label of Discourse Element Distribution', \n                      'y':0.95, \n                      'x':0.5, \n                      'xanchor':'center', \n                      'yanchor':'top'}, template=\"seaborn\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:36:30.218844Z","iopub.execute_input":"2022-02-20T07:36:30.219224Z","iopub.status.idle":"2022-02-20T07:36:31.303613Z","shell.execute_reply.started":"2022-02-20T07:36:30.219182Z","shell.execute_reply":"2022-02-20T07:36:31.302989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When the graph has been done running, there are a lot of counts over the discourse text so that we can't explain out of it.","metadata":{}},{"cell_type":"markdown","source":"## Distributing the Discourse Text\nNow, after distributing the Discourse Elements, let's now analyze every segment of the discourse text! So what are we stallin' for? Let's move on!","metadata":{}},{"cell_type":"markdown","source":"### Length of Discourse Text\nTo start measuring the length of our discourse text, we first append discourse_len to our train_df dataframe by defining a new key-set, discourse_len to the difference between the discourse_end and the discourse_start key. Next, we created another figure by defining a variable, fig to create a histogram with the px module that is connected by the histogram function, containing four parameters: data_frame set to the train_df dataframe, x (that represents the x-axis) set to discourse_len, marginal set to violin, and the nbins value set to 400. Finally, update our layout of the graph with the px module that sticks to the update_layout function containing a parameter, template (to represent the layout) to, again, any layout if you'd like and show the figure using the show function to stick to the fig variable figure.","metadata":{}},{"cell_type":"code","source":"train_df[\"discourse_len\"] = train_df[\"discourse_end\"] - train_df[\"discourse_start\"]\nfig = px.histogram(data_frame=train_df, x=\"discourse_len\", marginal=\"violin\", nbins=400)\nfig.update_layout(template=\"presentation\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:36:31.30483Z","iopub.execute_input":"2022-02-20T07:36:31.305522Z","iopub.status.idle":"2022-02-20T07:36:32.105451Z","shell.execute_reply.started":"2022-02-20T07:36:31.305486Z","shell.execute_reply":"2022-02-20T07:36:32.104302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Starting Point of Discourse Text\nNow, we measure the starting point of the discourse text! It's kinda similar to the way we did to measure the length of discourse text, but the x parameter in the histrogram function is assigned to the key data of discourse_start in train_df.","metadata":{}},{"cell_type":"code","source":"fig = px.histogram(data_frame=train_df, x=\"discourse_start\", marginal=\"violin\", nbins=400)\nfig.update_layout(template=\"presentation\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:36:32.106644Z","iopub.execute_input":"2022-02-20T07:36:32.106982Z","iopub.status.idle":"2022-02-20T07:36:32.829061Z","shell.execute_reply.started":"2022-02-20T07:36:32.106924Z","shell.execute_reply":"2022-02-20T07:36:32.82803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Wordclouding\nNow, let's wordcloud the words! We first define a variable called wordcloud to the wordcloud module with the WordCloud function containing 6 parameters (stopwords to wordcloud module with the STOPWORDS attribute call, max_font_size value set to 80 (or 90), max_words value set to 5000 (or 4500), width value set to 600, height value set to 400, and background_color set to black) connected by the generate function after the WordCloud function with 6 parameters, which contained a sub-code thing in which a spaced text (' ') is joined together using the join function, containing the variable txt being looped in the discourse_text key in train_df. Next, we define two variables, fig and ax to return the subplot axes of our wordcloud model using the plt module with the subplots function, containing the figsize parameter, to set the window size to 14 units and 10 units. We then use the ax variable to plug in the imshow function to show the image graph to the wordcloud image graph figure, and set the interpolation parameter to bilinear. Finally, we set the axis in our figure to off by using the ax variable to call the set_axis_off function and call the imshow function again using plt to show the wordcloud data.","metadata":{}},{"cell_type":"code","source":"wordcloud = wordcloud.WordCloud(stopwords=wordcloud.STOPWORDS, max_font_size=90, max_words=4500, width=600, height=400, background_color='black').generate(' '.join(txt for txt in train_df[\"discourse_text\"]))\nfig, ax = plt.subplots(figsize=(14,10))\nax.imshow(wordcloud, interpolation='bilinear')\nax.set_axis_off()\nplt.imshow(wordcloud);","metadata":{"execution":{"iopub.status.busy":"2022-02-20T07:57:19.860317Z","iopub.execute_input":"2022-02-20T07:57:19.860617Z","iopub.status.idle":"2022-02-20T07:57:36.115034Z","shell.execute_reply.started":"2022-02-20T07:57:19.860586Z","shell.execute_reply":"2022-02-20T07:57:36.114232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, there were a lot of words scrambled in every place, about every topic at a time students from grade 6 to 12 wrote. But wait, there's more!","metadata":{}},{"cell_type":"markdown","source":"### Wordclouding using stylecloud\nThis is the most part where we wordcloud every students' topic into F-U-N. We do it with stylecloud! But first, we need to install and import stylecloud. Inspired from: https://www.kaggle.com/kapakudaibergenov/stylecloud","metadata":{}},{"cell_type":"code","source":"!pip install stylecloud\nimport stylecloud","metadata":{"execution":{"iopub.status.busy":"2022-02-20T08:01:04.071337Z","iopub.execute_input":"2022-02-20T08:01:04.071909Z","iopub.status.idle":"2022-02-20T08:01:21.625195Z","shell.execute_reply.started":"2022-02-20T08:01:04.07185Z","shell.execute_reply":"2022-02-20T08:01:21.624343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we use stylecloud, we need to concat our text data! We define a variable called concated_discourse_text to the blank string followed by a join function, containing a variable, i to looping train_df with the discourse_text key with the astype function containing a str type surrounded by the square brackets.","metadata":{}},{"cell_type":"code","source":"concated_discourse_text = ' '.join([i for i in train_df.discourse_text.astype(str)])","metadata":{"execution":{"iopub.status.busy":"2022-02-20T08:14:39.485104Z","iopub.execute_input":"2022-02-20T08:14:39.485836Z","iopub.status.idle":"2022-02-20T08:14:39.613568Z","shell.execute_reply.started":"2022-02-20T08:14:39.485793Z","shell.execute_reply":"2022-02-20T08:14:39.612762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now generate our wordcloud but in a artistic way! We generate our customized wordcloud by calling out the stylecloud module with the gen_stylecloud function, containing the 5 parameters: text set to the concated discourse_text data, icon_name to any font awesome icons, palette set to any of the color formats, (mine's colorbrewer.diverging.Spectral_11), background_color set to black, and finally, size value to 1024. ","metadata":{}},{"cell_type":"code","source":"stylecloud.gen_stylecloud(text=concated_discourse_text, icon_name=\"fab fa-google\", palette=\"colorbrewer.diverging.Spectral_11\", background_color=\"black\", size=1024)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T08:16:04.385132Z","iopub.execute_input":"2022-02-20T08:16:04.385436Z","iopub.status.idle":"2022-02-20T08:16:37.127598Z","shell.execute_reply.started":"2022-02-20T08:16:04.385399Z","shell.execute_reply":"2022-02-20T08:16:37.12666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that, we call out the Image function, containing three hyperparameters: filename set to the output of stylecloud.png image file, width value set to 1024, and so did the same value for height. But before that, we need to import Image from the IPython module with the display submodule.","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage(filename='./stylecloud.png', width=1024, height=1024)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T08:22:30.100254Z","iopub.execute_input":"2022-02-20T08:22:30.100518Z","iopub.status.idle":"2022-02-20T08:22:30.117866Z","shell.execute_reply.started":"2022-02-20T08:22:30.100491Z","shell.execute_reply":"2022-02-20T08:22:30.117017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Waouh! What a masterpiece! We just displayed a the logo forming a Google-G logo, but with random colors and word topics students wrote on their essays! Finally, let's display out our visualization of the text in one of the student's argumentative essays!","metadata":{}},{"cell_type":"markdown","source":"## Text Visualization\nNow we've come to our final part of our EDA, visualizing our text! we first set the r variable to any number you'd like and set the ents variable to an empty array. We now loop over the variables i and row to our train_df function containing the id data from the train_df dataframe equally comparing to the r value number in train_files, setting the slicing ratio to 35 over -4 to calling the iterrows function to iter every row in train_df dataframe. Inside of it, we make the ents array append the dictionary keys which were: 'start' assigned to the row dataframe of discourse_start in int type, 'end' assigned to the row dataframe of discourse_end in int type, and 'label' assigned to the row dataframe of the discourse_type. We now open (using the with open keyword to) the train_files' id and read it 'r' as the file variable, thus assigning the data variable to read the file variable with the file in it by calling the read function. We now create a dictionary, doc2 to assign text key to the data variable, and ents key to the ents variable. Furthermore, we create a colors dictionary to assign the discourse elements key to any color you'd like and create an options dictionary, in which ents key is assigned to the train_df dataframe leading to discourse_type key and the unique function, to find the unique elements of a dataframe thus converting to list using the tolist function and colors key is assigned to the colors dictionary. Finally, display out the highlighted text by using the spacy function with the displacy attribute and render the text by using the render function, in which doc2 variable is placed in, the style parameter set to  ent, options parameter set to options dictionary, and setting the manual and jupyter parameters to True.","metadata":{}},{"cell_type":"code","source":"r = 30\nents = []\nfor i, row in train_df[train_df['id'] == train_files[r][35:-4]].iterrows():\n    ents.append({\n                    'start': int(row['discourse_start']), \n                     'end': int(row['discourse_end']), \n                     'label': row['discourse_type']\n                })\n\nwith open(train_files[r], 'r') as file: data = file.read()\n\ndoc2 = {\n    \"text\": data,\n    \"ents\": ents,\n}\n\ncolors = {'Lead': 'magenta','Position': 'purple','Claim': 'orange','Evidence': 'green','Counterclaim': 'blue','Concluding Statement': 'yellow','Rebuttal': 'red'}\noptions = {\"ents\": train_df.discourse_type.unique().tolist(), \"colors\": colors}\nspacy.displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True);","metadata":{"execution":{"iopub.status.busy":"2022-02-20T08:55:27.978688Z","iopub.execute_input":"2022-02-20T08:55:27.979031Z","iopub.status.idle":"2022-02-20T08:55:28.017502Z","shell.execute_reply.started":"2022-02-20T08:55:27.978994Z","shell.execute_reply":"2022-02-20T08:55:28.016668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\nAnd so we did it! We analyzed our student's argumentative essays and ready to move on to identifying them! After analyzing em, what should we do now? We could submit our argumentative essays to Feedback Prize about how Ponyboy in *The Outsiders* rebeled against the Greasers stereotypes, or, identify more essays with discourse text identification... But, it's our choice!","metadata":{}},{"cell_type":"markdown","source":"## **WORKS CITED**","metadata":{}},{"cell_type":"markdown","source":"* odins0n (Sanskar Hasija). (2022, January 25). 📊 feedback prize - eda 📊. Kaggle. Retrieved February 20, 2022, from https://www.kaggle.com/odins0n/feedback-prize-eda \n* Kapakudaibergenov (Kapa Kudaibergenov). (2021, June 3). Stylecloud. Kaggle. Retrieved February 20, 2022, from https://www.kaggle.com/kapakudaibergenov/stylecloud ","metadata":{}}]}