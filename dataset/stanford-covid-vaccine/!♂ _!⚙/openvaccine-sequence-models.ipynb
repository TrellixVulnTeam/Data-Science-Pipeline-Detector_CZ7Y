{"cells":[{"metadata":{},"cell_type":"markdown","source":"# cool links\nhttp://rna.tbi.univie.ac.at/forna/ -- for RNA molecule visualization\n\nhttps://github.com/DasLab/draw_rna - a python library\n\nhttps://academic.oup.com/nar/article/46/11/5381/4994207\n\n\nhttps://www.tensorflow.org/lattice -- maybe try this out sometime?"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#add the draw_rna library to this environment\n! git clone https://github.com/DasLab/draw_rna.git #need to find a way to say yes\n! python draw_rna/setup.py install\n\n#add forna library\n'''\n! git clone https://github.com/ViennaRNA/forna.git\n! cd forna/\n! ls forna/'''"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sklearn\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n'''\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#these are the columns we have to predict, in same order as submission file format\ntarget_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_json(filename):\n    '''\n    reads in train/test json data as pandas DataFrame\n    '''\n    file = open(filename)\n    df = pd.read_json(path_or_buf = file, orient = 'records', lines = True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load in training set tabular data\n\ntrain_df = read_json('../input/stanford-covid-vaccine/train.json')\n\nprint(train_df['id'].nunique())\nprint(train_df.columns)\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load in test set tabular data\n\ntest_df = read_json('../input/stanford-covid-vaccine/test.json')\n\n\n#see which feature columns are only in the training set & not the test set\nprint('Features only in training set (not including target columns):') \nset(train_df.columns) - set(test_df.columns) - set(target_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''! ls\n#! ls draw_rna\n! ls forna\n\n! python forna/forna_server.py -s -d'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''seq = train_df.loc[0, 'sequence']\nstruct = train_df.loc[0, 'structure']\n\nseq, struct'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocess and Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"figure out way to encode structure\n\n* try looking at how the **different target values are correlated** -- maybe we can use the non-scored ones to predict the scored ones?\n    * take care to avoid leakage -- you won't have some target values at test time, but maybe we can have separate model to predict those columns beforehand?\n    \n    \n* try integrating bpps matrices"},{"metadata":{"trusted":true},"cell_type":"code","source":"def unpack_df_lists(df, col_names):\n    '''\n    turn list-like elements of dataframe into tabular data\n    \n    works great\n    '''\n    if isinstance(col_names, str): #if string is passed in, convert to list for convenience\n        col_names = [col_names]\n    \n    all_series = [df[c] for c in col_names] #select relevant columns\n    unpacked = [ser.explode() for ser in all_series] #unpack lists for each feature series\n    \n    data = pd.concat(unpacked, axis = 1) #concat unpacked columns together\n    \n    #merge unpacked columns with original\n    original = df.drop(col_names, axis = 1) #drop columns with list elements\n    data = original.join(data) #then join unpacked data to original df\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#modify this to work with sequence models\n'''\ndef feature_engineer(df, train = True, **kwargs):\n    \n    \n    unpack_cols = ['reactivity_error', 'deg_error_Mg_pH10', 'deg_error_pH10',\n       'deg_error_Mg_50C', 'deg_error_50C', 'reactivity', 'deg_Mg_pH10',\n       'deg_pH10', 'deg_Mg_50C', 'deg_50C'] #only need to unpack things in training set\n    \n    #unpack list elements (only training set needs to be unpacked)\n    if train:\n        data = unpack_df_lists(df, unpack_cols)\n    else: #if test data, need to add rows manually\n        data = df.copy()\n        data['temp'] = data.apply(lambda row: [0] * row['seq_length'], axis = 1) #adds temp column with list-like elements, of len(seq_scored) for that row \n        data = unpack_df_lists(data, 'temp') #unpack to right length using this function\n        del data['temp'] #delete the temp column\n        #this works great!\n        \n    #adds seqpos column to record position of each row in each id's individual sequence\n    data['seqpos'] = 1\n    data['seqpos'] = data.groupby('id').cumsum()['seqpos'] - 1\n    \n    #adds nucleotide column to record base (A,C,G,U) at position seqpos\n    seq_temp = pd.concat([data['sequence'],data['seqpos']], axis = 1)\n    data['nucleotide'] = seq_temp.apply(lambda row: row['sequence'][row['seqpos']], axis = 1) #get base at seqpos in sequence string\n    \n    #adds pred_loop_seqpos column to record predicted loop type at position seqpos\n    loop_temp = pd.concat([data['predicted_loop_type'],data['seqpos']], axis = 1)\n    data['pred_loop_seqpos'] = loop_temp.apply(lambda row: row['predicted_loop_type'][row['seqpos']], axis = 1) #get type at seqpos in predicted_loop_type string \n    \n    #do one-hot encoding on nucleotide column? or label encoding?\n    data = pd.get_dummies(data, columns = ['nucleotide','pred_loop_seqpos']) #do one-hot encoding on predicted_loop_type & nucleotide column\n    \n    return data\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"feature engineering notes:\n\n3.61 s ± 37 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) -- with nucleotide column engineering\n\n266 ms ± 7.71 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) -- without\n\n6.31 s ± 55 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) -- seq_temp groupby\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#prob better to use tokenizer from tensorflow keras\n\ntokenize_cols = ['sequence', 'structure', 'predicted_loop_type']\ndef tokenize_df(df, tokenizer, cols = tokenize_cols):\n    '''\n    tokenizer is a tensorflow keras Tokenizer that has been already fitted on text\n    '''\n    data = df.copy()\n    for c in tokenize_cols:\n        data[c] = tokenizer.texts_to_sequences(data[c])\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(filters = None, lower = False, char_level = True)\ntokenizer.fit_on_texts('().ACGUBEHIMSX')\n\n#filter and tokenize\ntemp = train_df[train_df['SN_filter'] == 1]\ntemp = tokenize_df(temp, tokenizer)\n\ntrain_df = temp\n\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#load in bpp data as well\n\ndef read_bpp(ids):\n    base_path = '../input/stanford-covid-vaccine/bpps/' #where all bpp files are\n    filepaths = [f'{base_path}{rna_id}.npy' for rna_id in ids]\n    bpps = [np.load(fp) for fp in filepaths]\n    return np.stack(bpps, axis = 0) #might not staa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this cell is to load bpp train data\n\n#get ids of training data to create files\nids = train_df['id']\nbpp_train = read_bpp(ids)\n\nbpp_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# this is for tabular data model input -- sequence model doesn't use\n\n#unpack the lists and add a seqpos column\n#also convert dtypes for memory savings\nunpack_cols = ['reactivity_error', 'deg_error_Mg_pH10', 'deg_error_pH10',\n       'deg_error_Mg_50C', 'deg_error_50C', 'reactivity', 'deg_Mg_pH10',\n       'deg_pH10', 'deg_Mg_50C', 'deg_50C'] #only need to unpack things in training set\n\n\ntemp = train_df[train_df['SN_filter'] == 1] #only select good quality examples to train on\ntemp = feature_engineer(temp)\n\n#check proportions of nucleotides and loop types\nfor b in ['A','C','G','U']:\n    print(b, temp['nucleotide_'+b].mean())\n    \nfor b in ['S','M','I','B','H','E','X']:\n    print(b, temp['pred_loop_seqpos_'+b].mean())\n\n\n#now convert dtypes (should put this code in function)\nprint('train_df memory (MB):', train_df.memory_usage(deep = True).sum() * 1e-6)\nprint('temp_df memory before (MB):', temp.memory_usage(deep = True).sum() * 1e-6)\n\ntemp['SN_filter'] = temp['SN_filter'].astype('uint8')\ntemp['signal_to_noise'] = temp['signal_to_noise'].astype('float16') #seems like signal_to_noise isn't too precise\ntemp[['seq_length','seq_scored']] = temp[['seq_length','seq_scored']].astype('uint8') #seq_length and seq_scored should only be 107 or 130\ntemp[unpack_cols] = temp[unpack_cols].astype('float16') #looks like we don't need much precision to represent these cols -- may need to verify if model suffers\ntemp['seqpos'] = temp['seqpos'].astype('uint8') #max of uint8 is 255, which is fine -- seqpos is only ever 68 or 91\n\nprint('temp_df memory after (MB):', temp.memory_usage(deep = True).sum() * 1e-6)\nprint(temp.dtypes)\n\n\ntrain_df = temp\ntrain_df"},{"metadata":{},"cell_type":"markdown","source":"* Idea: use signal_to_noise ratio column as example weight? seems like higher signal-to-noise means that that example is more useful; maybe something to do with Bayesian regression -- less confidence in low signal-to-noise, higher confidence in high signal-to-noise\n    * look into \"sample_weight\" in tf keras model.fit or sklearn model.fit\n    * Even if filtering, maybe can still use signal_to_noise as a sample weight -- surely high signal_to_noise is still better overall\n\n\n\"[Signal/noise is defined as mean( measurement value over 68 nts )/mean( statistical error in measurement value over 68 nts)]\""},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/isaienkov/openvaccine-eda-feature-engineering-modeling\n\n* he splits the RNA structure into groups of 3 -- possibly good idea, given 3 nucleotides needed for 1 amino acid (may not be wise, has been mentioned before by organizer of competition)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Target EDA\n\n\nsee here for more: https://www.kaggle.com/robikscube/openvaccine-covid-19-mrna-starter-eda"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ncorr_data = train_df.drop(['index','id','sequence','structure','predicted_loop_type', 'seq_length','seq_scored'], axis = 1)\ncorr_data = unpack_df_lists(corr_data, ['reactivity_error', 'deg_error_Mg_pH10', 'deg_error_pH10',\n       'deg_error_Mg_50C', 'deg_error_50C', 'reactivity', 'deg_Mg_pH10',\n       'deg_pH10', 'deg_Mg_50C', 'deg_50C']).convert_dtypes()\n\n\nprint(corr_data)\n\ncorr_data = corr_data.corr()\n\ncorr_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = np.ma.masked_inside(corr_data.values, -0.15, 0.15).mask #get most powerful features\n\nplt.figure(figsize = (8,8))\nsns.heatmap(corr_data, annot = True, mask = mask)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"EDA notes:\n\n**With Filtering SN_filter == 1:**\n* even higher correlation between targets! (0.6 - 0.8)\n\n\n**W/O Filtering SN_filter == 1:**\n* having a predicted loop type of S seems to be decently negatively correlated (-0.2) with some target values\n\n* seqpos seems to not be strongly correlated to target values over all data -- however, for some individual RNA sequences, it seems that there is a correlation (see robikscube EDA)\n\n* seems like there's a pretty good (~0.4 - 0.5) correlation between each of the target values, at least in train set\n* maybe use one target column's values to predict the others?\n* how to exploit this -- what if i predicted one target column at a time, then used that column to \"bootstrap\" off -- used the columns I predicted to predict another column\n    * issue -- surely the ordering of which column is predicted first will greatly affect other predictions\n    * idea -- could try different orderings of prediction order, then average out -- maybe try Shapley value?"},{"metadata":{},"cell_type":"markdown","source":"# Define Metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nFor tensorflow compatibility, metrics should have signature f(y_true, y_pred)\nFor sklearn compatibility, metrics should have signature f(y_true, y_pred, **kwargs)\n'''\n\n\ndef score(raw_values = False, use_tf = False, **kwargs):\n    '''\n    This is competition metric: Mean Columnwise Root Mean Square Error (MCRMSE)\n    Averages RMSE loss over all scored columns (only 3 are scored)\n    \n    Parameters:\n    For now, kwargs is ignored\n    tf -- True if using in tensorflow, false if not\n    col_dict is a dictionary that maps column number index to column name\n        keys 'reactivity', 'deg_Mg_pH10', 'deg_Mg_50C'\n        values are numeric index of that column in y_pred\n    raw_values determines if losses for each column are returned or just the average\n        if True, losses for each of columns are returned\n        if False, only average is returned\n    \n    Returns a loss function that computes MCRMSE for scored columns\n    '''    \n    \n    #these columns are scored\n    col_dict = {\n        'reactivity':0,\n        'deg_Mg_pH10':1,\n        'deg_Mg_50C':3\n    }\n    \n    #these are the columns to not score\n    unscored = set([0,1,2,3,4]) - set(col_dict.values())\n    \n    #choose output format of metric\n    multi = 'uniform_average'\n    if raw_values:\n        multi = 'raw_values'\n    \n    def loss(y_true, y_pred):\n        '''\n        y_true & y_pred may have more columns than needed for scoring\n        select only necessary ones for scoring\n\n        y_true & y_pred have shapes (n, 5, len), where n is # of id_seqpos combos, len is length of sequence\n        '''\n        from sklearn.metrics import mean_squared_error\n        y_true = np.array(y_true) #convert to np for convenience\n        y_pred = np.array(y_pred)\n\n        \n        #only select scored columns\n        y_true = y_true[:, list(col_dict.values())]\n        y_pred = y_pred[:, list(col_dict.values())]\n        \n\n        #sqrt all outputs, then average them (if not raw_values)\n        metric = mean_squared_error(y_true, y_pred, squared = False, multioutput = multi)\n        return metric\n    \n    def loss_tf(y_true, y_pred):\n        '''\n        WIP -- need to prioritize scored columns\n        \n        '''\n        \n        import tensorflow as tf\n        import tensorflow.keras.backend as tf_kb\n        \n        #print(type(y_true),type(y_pred))\n        \n        #note -- y_true may be in different format than y_pred\n        #y_true = tf.convert_to_tensor(y_true)\n        #y_pred = tf.convert_to_tensor(y_pred)\n        \n        \n        #set values in y_pred to be same as y_true for unscored columns\n        #this ensures that these columns do not contribute to loss\n        '''for c in unscored:\n            y_pred[:, c] = y_true[:, c]'''\n            \n        '''\n        colwise_mse = tf_kb.mean(tf_kb.square(y_true - y_pred)) #first take average squared difference over row examples (shape 1x3)\n        return tf_kb.mean(tf_kb.sqrt(colwise_mse)) #then sqrt and take average over columns (shape 1x1 / scalar)\n        '''        \n        \n        \n        colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n        return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n        \n        \n    if not use_tf:\n        return loss\n    else:\n        return loss_tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define X, y\n\n(no need to define sample weights anymore, since private LB is also filtered now)"},{"metadata":{},"cell_type":"markdown","source":"try RNN models -- they seem to work good\nhttps://www.kaggle.com/neithermannormachine/openvaccine-gru-lstm/edit\n\nor maybe Conv1D?\n\nhttps://github.com/tkipf/keras-gcn -- graph convolutions (may be good for nonlinear RNA sequence structure)\nhttps://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780\n\npossibly regression chaining using target correlation\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define the training data\n#create X_train and y_train\n\n#drop for now when training, unless i can find a way to use them for feature engineering\ntrain_only_cols = ['reactivity_error', 'deg_error_Mg_pH10', 'deg_error_pH10', 'deg_error_Mg_50C', 'deg_error_50C'] #features only in train set\nsignal_cols = ['signal_to_noise','SN_filter']\ndrop_cols = [\n             'seq_length', 'seq_scored', #don't actually use seq_length and seq_scored for training - just metadata\n            'index', 'id']  #also not actually useful for training\ntrain_drop_cols = drop_cols + target_cols + train_only_cols + signal_cols\n\n\n#X_train should be an iterable, with shape (n, 107, 3)\n#each row should be a single np array\nX_train = (train_df.drop(train_drop_cols, axis = 1) #selects only relevant columns\n                    .apply(lambda row: [e for e in row], axis = 1) #concatenates each element in row to list\n                    .apply(lambda e : np.array(e)) #creates 2d numpy array from those elements\n          )\nX_train = np.stack(X_train.values, axis = 0) #stacking might not work for test data, since there is two different shapes\n\ny_train = (train_df[target_cols]\n               .apply(lambda row: [e for e in row], axis = 1) #concatenates each element in row to list\n                .apply(lambda e : np.array(e)) #creates 2d numpy array from those elements\n          )\ny_train = np.stack(y_train.values, axis = 0)\n\n'''\n#maybe can use this as example weights -- higher signal_to_noise means higher weight?\n#probably gotta make sure to cap the weight though, otherwise training dominated by top signal_to_noise\nsignal_to_noise = train_df[signal_cols]  #not necessary anymore, although might still be useful -- try experimenting\n'''\n\n\n#np.array(train_df.drop(train_drop_cols, axis = 1).iloc[0].explode()).reshape(3,-1).astype('int')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#even when filtered, it still might be useful to try sample weighting\nsw = train_df['signal_to_noise'].values\n\n\nimport matplotlib.pyplot as plt\n\nq = np.quantile(sw, np.linspace(0,1,21))\nplt.plot(q, np.log1p(q + 5)/2)\n\n\nsw = np.log1p(sw + 5)/2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define and Train Model\n\nwork on framework for save/loading weights"},{"metadata":{},"cell_type":"markdown","source":"try integrating bpp as well\n\nafter this try graph network\n\n\nSequence model IO:\n\ninput: 3 sequences of same length (len == seq_length, which is either 68 or 91);\n* input shape (n, 3, 107) or (n, 3, 130)\n\noutput: 5 sequences of seq_scored length;\n* output shape (n, 5, 68) or (n, 5, 91)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.optimizers import Adam\n#from tensorflow_addons.metrics import RSquare\n\n#model should be highly generalizable, given limited public LB test data\n#don't rely on public LB score too much\n#input shape should be variable, since difference in input size for public vs private test set\n#if taking in tabular data (ie for linear model), then input shape shouldn't matter\n\n\n#lr = 0.01 seems pretty good for adam\nTF_COMPILEPARAMS = {\n    'optimizer': Adam(learning_rate = 0.01), #try different learning rates, optimizer hyperparameters\n    'loss': score(use_tf = True),\n    'metrics': ['mse']\n}\n\n\ndef make_model():\n    '''\n    Creates a tensorflow keras sequence model\n    '''\n    #parameters passed into embedding layer\n    EMBEDDING_PARAMS = {'input_dim': len(tokenizer.word_index) + 1,\n                        'output_dim': 100, #play around with this number\n                       }\n    \n    #should make this variable less prone to \n    shape = (3,None) #3 sequences of unknown length (should all be same length though)\n    \n    #create the model\n    seq_inputs = tf.keras.Input(shape = shape) #shape (n, 3, seq_length)\n    embed = layers.Embedding(**EMBEDDING_PARAMS)(seq_inputs) #(n, 3, seq_length, output_dim)\n    \n    #idea: 1 rnn per sequence, combine in single layer\n    def rnn_layer(num_neurons):\n        return layers.Bidirectional(layers.LSTM(num_neurons, return_sequences = True, dropout = 0.2)) #define the rnn type to use\n    \n    #create list of rnn layers, one for each sequence, then concatenate\n    rnn_layers = []\n    for i in range(embed.shape[1]): #loop thru sequences\n        #one 2-layer bidirectional rnn per sequence\n        r = rnn_layer(30)(embed[:,i])\n        r = rnn_layer(30)(r)\n        rnn_layers.append(r) #r is shape (n, seq_length, num_rnn_neurons * 2 (b/c bidirectional))\n    \n    x = layers.Concatenate()(rnn_layers) #concatenate the rnn layers for each sequence (n, seq_length, num_rnn_neurons * 2 * 3)\n    x = layers.Dense(100, activation = 'relu')(x) #(n, seq_length, 100)\n    x = layers.Dense(5, activation = 'linear')(x) #need output layer of 5 x seq_scored; shape (n, seq_length, 5)\n    \n    x = tf.transpose(x, (0,2,1)) #reshape prediction for submitting output & computing loss -- (n, 5, seq_length)\n    x = x[:, :, :-39] #compact sequence of 107/130 into 68/91 by removing last 39 elements (n, 5, seq_scored)\n    \n    #this means one model works for both length inputs (really, any length)\n    model = tf.keras.Model(inputs = seq_inputs, outputs = x)\n    '''\n    #lr = 0.01 seems pretty good for adam\n    optimizer = Adam(learning_rate = 0.01) #try different learning rates, optimizer hyperparameters\n    loss = score(use_tf = True)\n    #loss = 'mse'\n    '''\n    model.compile(**TF_COMPILEPARAMS)\n    \n    \n    return model\n\ndef make_model_bpp(seq_model):\n    #right now this is temp code -- refactor when done experimenting\n    '''\n    maybe force seq_model to be fitted first\n    seq_model is a fitted sequence model, as defined in make_model\n    combines sequence model predictions with bpp features\n    \n    by bootstrapping off trained seq_model, this should verify whether scores are indeed improving after adding bpp features\n    '''\n    \n    #get the sequence model first\n    #seq_model = make_model()\n    seq_input = seq_model.input\n    seq_layers = seq_model.layers\n    intercept_layer = 12 #the layer we'll start adding stuff to\n    for l in seq_layers[:intercept_layer]: #seq_model should be fitted already, don't train first few layers again\n        l.trainable = False\n        \n    #[print(i, l) for i,l in enumerate(seq_layers)]\n    \n    #work on fixing bpp_shape to accept variable length input\n    bpp_shape = (107, 107) #bpp shape is (seq_length, seq_length)\n    bpp_input = tf.keras.Input(shape = bpp_shape) #this is the bpp feature input\n    \n    #intercept sequence model after the 1st Dense layer\n    x = seq_layers[12].output #shape (n, seq_length, 100)\n    x = layers.Concatenate()([x, bpp_input]) #combine bpp data with sequence model output (n, seq_length, 100 + seq_length)\n    x = layers.Dense(250, activation = 'relu')(x) #then train layer to incorporate bpp features (n, seq_length, 100)\n    x = layers.Dense(100, activation = 'relu')(x) #then train layer to incorporate bpp features (n, seq_length, 100)\n\n    \n    #now plug this back into the original model (later layers will need to train again)\n    for l in seq_layers[intercept_layer+1:]: #get later layers\n        x = l(x) #apply later layers to x\n    \n    model = tf.keras.Model(inputs = [seq_input, bpp_input], outputs = x)\n    model.compile(**TF_COMPILEPARAMS)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DO THE TRAINING\n'''\nwork on callbacks (lr scheduling, logging, etc)\n'''\nfrom tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\nfrom sklearn.model_selection import train_test_split\n\n#these are the additional parameters that go into keras model fit\nTF_FITPARAMS = {'epochs': 150,\n               'batch_size':100,\n               'validation_batch_size':50}\nfp = TF_FITPARAMS\n\ndef schedule_func(epoch, lr):\n    '''\n    function passed to LearningRateScheduler to determine learning rate at each epoch\n    keeps lr constant until certain percent of epochs have elapsed, then exponentially decreases lr\n    '''\n    #percent = epoch / fp['epochs'] #get epoch progress\n    if epoch < 50:\n        return lr\n    else:\n        return lr * np.exp(-0.13)\n    \ncallbacks = [\n    LearningRateScheduler(schedule_func),\n    EarlyStopping(monitor = 'val_loss', mode = 'min', min_delta = 5e-5, patience = 10)\n]\n\n\n\n#instantiate model\nmodel = make_model()\nmodel.summary()\nprint(X_train.shape, y_train.shape)\n\n\n\n\n#split data into training and validation\n#debate about test_size -- models without train_test_split may just be better on public LB due to more training data\nidx_tr, idx_val = train_test_split(range(len(X_train)), test_size = 0.1) #split indices rather than X & y directly, so that we can also corroborate X with bpp (same indices)\n\nX_tr, y_tr = X_train[idx_tr], y_train[idx_tr]\nX_val, y_val = X_train[idx_val], y_train[idx_val]\n\n#then do fitting\n%time history = model.fit(X_tr, y_tr, validation_data = (X_val, y_val), callbacks = callbacks, **fp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#try out bpp model\n\nmodel_bpp = make_model_bpp(model)\nmodel_bpp.summary()\n\nbpp_tr, bpp_val = bpp_train[idx_tr], bpp_train[idx_val]\n\n%time history_bpp = model_bpp.fit([X_tr, bpp_tr], y_tr, validation_data = ([X_val, bpp_val], y_val), callbacks = callbacks, **fp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.utils.plot_model(model_bpp, show_shapes = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_model(model):\n    '''\n    WIP\n    model is a fitted tensorflow keras model \n    '''\n    \n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef plot_learning_curves(metric_dict):\n    '''\n    metric_dict is of similar form to tensorflow history.history object\n    '''\n    \n    #plot loss and all metrics\n    x_len = len(metric_dict['loss']) / 10 #length of plot proportional to epochs\n    plt.figure(figsize = (x_len,13))\n\n    all_metrics = set(metric_dict.keys()) - set(['lr']) #plot learning rate seperately\n\n    #first plot loss and related metrics\n    plt.subplot(2,1,1)\n    for metric in all_metrics:\n        metric_history = metric_dict[metric]\n        plt.plot(metric_history, label = metric)\n    plt.legend()\n\n    #then plot learning rate\n    plt.subplot(2,1,2)\n    plt.plot(metric_dict['lr'], label = 'lr')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot history of model\n\nplot_learning_curves(history.history)\nplt.title('Sequence Model')\nplt.show()\nplot_learning_curves(history_bpp.history)\nplt.title('Sequence Model w/ BPP')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cross Validate\n\nconsidering low amount of examples in public LB, good cross-val incredibly important!\n\nlooks like you should aim for top 10% on public LB (but not too aggressively push for top 3) if aiming to win on private LB\n\nkeep good logs of cross-validation\n\nHow to determine if **good public LB** == **good private LB**? longer sequence may mean worse performance, not generalizable\n\n* ### Idea: Try \"cutting\" each sequence given in training set (only train on 90 out of 107 elements or so per sequence), train on that, and then predict public LB or even training set -- see how this affects score"},{"metadata":{},"cell_type":"markdown","source":"alternatively, try bayesian optimization for hyperparameter tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n#wip\n\nparam_grid = {}\nmodel_sk = KerasRegressor(build_fn = make_model) #make sk_learn wrapped model to use sklearn hyperparameter optimization\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# disable when submitting, takes long\n\n\nfrom sklearn.model_selection import cross_val_score, KFold, GroupKFold\nfrom sklearn.metrics import make_scorer, mean_squared_error\n\n#use groupkfold, where each group is single RNA seequence\ngroups = train_df['id'].values\ncv = GroupKFold(n_splits = 5)\n#cv = KFold(n_splits = 5)\n#scorer = make_scorer(lambda y_true, y_pred: mean_squared_error(y_true, y_pred, squared = False))\nscorer = make_scorer(score())\n\n\ncv_scores = cross_val_score(model, X_train, y_train, cv = cv, groups = groups, scoring = scorer,\n                            fit_params = {**fp})\nprint(cv_scores)\nsns.boxplot(cv_scores)\nf'{cv_scores.mean():.6f}', f'{cv_scores.std():.6f}'\n#consider what fold class to use -- seems like same id should not be in train and val sets\n#consider a groupkfold\n#if one id is one row, however, can probably just use regular KFold - depends on format of train_df\n#for now, doesn't seem like we need to do any scaling/normalization on entire train set, so we're good"},{"metadata":{},"cell_type":"markdown","source":"**cv scores (mean, stdev):**\nNeed to redo scores because we initially trained on all data, not just good data (SN_filter == 1)\n\nNo SW Simple Dense NN: ('0.392408', '0.000757')\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot effect of tuning hyperparameter on score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#log cross-val scores and params to a separate file","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submit\n\nwe predict public and private test data seperately, to avoid numpy confusion"},{"metadata":{},"cell_type":"markdown","source":"need to have all 107/130 (seq_length) rows in actual submission, despite training on 68 rows only\n\nright now model only works for public data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = read_json('../input/stanford-covid-vaccine/test.json')\n\ntest_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filter and tokenize\ntest_public = test_df[test_df['seq_length'] == 107]\ntest_public = tokenize_df(test_public, tokenizer)\n\ntest_public","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_private = test_df[test_df['seq_length'] == 130]\ntest_private = tokenize_df(test_private, tokenizer)\n\n\ntest_private","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create public test set data\nX_test_public = (test_public.drop(drop_cols, axis = 1) #selects only relevant columns\n                    .apply(lambda row: [e for e in row], axis = 1) #concatenates each element in row to list\n                    .apply(lambda e : np.array(e)) #creates 2d numpy array from those elements\n          )\nX_test_public = np.stack(X_test_public.values, axis = 0) #shape (n,3,107)\n\n#create private test set data\nX_test_private = (test_private.drop(drop_cols, axis = 1) #selects only relevant columns\n                    .apply(lambda row: [e for e in row], axis = 1) #concatenates each element in row to list\n                    .apply(lambda e : np.array(e)) #creates 2d numpy array from those elements\n          )\nX_test_private = np.stack(X_test_private.values, axis = 0) #shape (n,3,130)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_sub_df(test_df, predictions, length):\n    '''\n    this method creates submission dataframe from a df with data, predictions for that data, and sequence length\n    '''\n    sub_df = test_df.drop(tokenize_cols + ['index','seq_scored'], axis = 1) #first drop feature and irrelevant columns\n    #create a seqpos column, which is a list that holds seqpos\n    sub_df['seqpos'] = sub_df.apply(lambda row: list(range(row['seq_length'])), axis = 1)\n    sub_df = unpack_df_lists(sub_df, 'seqpos') #now unroll\n    sub_df['id_seqpos'] = sub_df.apply(lambda row: row['id'] + '_' + str(row['seqpos']), axis = 1)\n    \n    \n    def pad_pred(p, i):\n        '''\n        p is the prediction\n        i is final length\n        fills unscored rows with 0s\n        '''\n        start = list(p) #first elements\n        padding = [0] * (i - len(start)) #add padding to make up difference\n        return start + padding\n\n    pred_df = pd.DataFrame([[pad_pred(l, length) for l in e] for e in predictions], columns = target_cols) #here, each row holds a list in each column\n    pred_df = unpack_df_lists(pred_df, target_cols) #then unroll those lists\n    pred_df.index = sub_df.index\n    pred_df = pred_df.reset_index()\n\n    sub_df = sub_df.reset_index()\n    sub_df = sub_df[['id_seqpos']]\n    \n    print(sub_df.shape, pred_df.shape)\n    \n    sub_df = sub_df.join(pred_df).set_index('index')\n\n    \n    return sub_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#do actual prediction\n#test_pred_public = model.predict(X_test_public) #shape (n,5,68)\n#test_pred_private = model.predict(X_test_private) #shape (n,5,91)\n\n#try out model bpp\ntest_pred_public = model_bpp.predict([X_test_public, read_bpp(test_public['id'])]) #shape (n,5,68)\n\n'''temp code to fill in private predictions with zeros'''\ntest_pred_private = np.zeros((X_test_private.shape[0], 5, 91))\n\n#create submission dataframes\nsub_public = create_sub_df(test_public, test_pred_public, 107)\nsub_private = create_sub_df(test_private, test_pred_private, 130)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this temporary cell is for filling in private data with 0s\n\"\"\"\ntest_private = test_df[test_df['seq_length'] == 130]\n\nsub_private = test_private[['id', 'seq_length']]\nsub_private['seqpos'] = sub_private.apply(lambda row: list(range(row['seq_length'])), axis = 1) #create seqpos column\n\n\ntest_pred_private = model.predict(X_)\n\nfor c in target_cols: #fill in targets with 0s\n    sub_private[c] = sub_private.apply(lambda row: [0] * row['seq_length'], axis = 1)\n\nsub_private = unpack_df_lists(sub_private, ['seqpos'] + target_cols) #unroll lists\nsub_private['id_seqpos'] = sub_private.apply(lambda row: row[\"id\"]+'_'+str(row[\"seqpos\"]), axis = 1) #create id_seqpos column\nsub_private = sub_private.drop(['id','seq_length','seqpos'], axis = 1)\nsub_private = sub_private[['id_seqpos'] + target_cols] #rearrange column order\n\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#concat public and private predictions into single submission dataframe\n\nsub_df = pd.concat([sub_public, sub_private]).convert_dtypes()\nsub_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"sns.pairplot(pd.DataFrame(y_train.reshape(-1,5), columns = target_cols))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#fig = plt.figure(figsize = (20,10))\n#ax = plt.subplot(1,1,1)\n\nsns.pairplot(sub_df)\n#sub_df.plot(y='deg_Mg_pH10', ax = ax)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"sns.heatmap((sub_df[sub_df['reactivity'] != 0]).corr(), annot = True)\n\n#good -- seems like predicted targets do have significant correlation between each other, as seen in training data\n#although might be higher correlation than reality? try regularizing/dropout"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error\n\na = model.predict(X_test_public).ravel()\nb = model_bpp.predict([X_test_public, read_bpp(test_public['id'])]).ravel()\n\nmean_absolute_error(a,b), np.sqrt(mean_squared_error(a,b)) #just check how big the difference in prediction is with and w/o bpp"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}