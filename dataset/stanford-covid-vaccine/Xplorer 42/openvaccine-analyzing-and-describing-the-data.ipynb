{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://ib.bioninja.com.au/_Media/trna-structure_med.jpeg)\nIn this Notebook I'll try to go through all the data avialable in this competition and give meaning to each aspect, I'll also do some calculations that will shed some light on the nature of this data.\n\n**Side note:** I'm trying to keep this notebook as short as possible wihle highlighiting the most important parts, I'll be updating this notebook regularly to keep up with the  most relevant information related to the problem."},{"metadata":{},"cell_type":"markdown","source":"#### Imported packages:\n[draw_rna](https://github.com/DasLab/draw_rna)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/draw-rna')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\n\nfrom ipynb.draw import draw_struct\nimport seaborn as sns\n\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loding the json data:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train = pd.read_json('/kaggle/input/stanford-covid-vaccine/train.json', lines=True)\ntest = pd.read_json('/kaggle/input/stanford-covid-vaccine/test.json', lines=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# we define the columns separately (this is gonna be usefull later)\nerr_cols = ['reactivity_error', 'deg_error_Mg_pH10', 'deg_error_pH10', 'deg_error_Mg_50C', 'deg_error_50C']\nmes_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you see the train dataframe has 10 more columns, all these columns are about the measurements and their errors.\n\n#### Our goal is to predict the measurement columns, the errors might be usefull for training the model.\n\nBelow we take a look at each column, describe it and try to analyze it."},{"metadata":{},"cell_type":"markdown","source":"## Sequence:"},{"metadata":{},"cell_type":"markdown","source":"(working on it)"},{"metadata":{},"cell_type":"markdown","source":"## Structure:"},{"metadata":{},"cell_type":"markdown","source":"The structure column is a string with the same length as the sequence, it indicates if the two bases in the sequence are connected which is noted by parentheses.\n\nWe're gonna use the draw_rna tool to plot the structure and also see how the structure impact the measurements."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#taking a sample from our training data\n    #samples with good confidence \ntrain_hsnr = train.query('signal_to_noise > 5')\nsample = train_hsnr.iloc[49, :]\nseq = sample['sequence']\nstruc = sample['structure']\n\n#ploting the structure with different measurements as alpha\n\nfig, axs = plt.subplots(3, 2, figsize=(20,20))\naxs = axs.flatten()\n    #for figure Legend\n_measured = mlines.Line2D([], [], color='blue', linestyle='None', marker='o', markersize=15, label='Measured')\n_unmeasured = mlines.Line2D([], [], color='red', linestyle='None', marker='o', markersize=15, label='Unmeasured')\n\nfor i, mes_col in enumerate(mes_cols):\n    measure = np.array(sample[mes_col])\n    #the last 39 bases aren't measured\n    unmeasured = len(seq) - len(measure)\n    #normalized the measurement vector (alpha require [0-1] range values)\n    norm = (measure - measure.min()) / ( measure.max() - measure.min())\n    #padding with ones to have same length\n    alpha = np.concatenate((norm, np.ones(unmeasured)))\n    #this is to distiguich measured/unmeasured bases\n    color = np.concatenate((np.zeros(len(measure)), np.ones(unmeasured)))\n    draw_struct(seq, struc, c=color, cmap='bwr', alpha=alpha, ax=axs[i])\n    axs[i].set_title(mes_col)\n    axs[i].legend(handles=[_measured, _unmeasured])\n    \naxs[-1].axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The measured bases seems to have corrolated values (bases on same positions have same opacity for different measurements), we will investigate this further when we analyse these columns.\n\nI have only used a single sample as a visualization, you can be creative try to plot accross different sample taking in regard other parameters."},{"metadata":{},"cell_type":"markdown","source":"## Predicted Loop-type:"},{"metadata":{},"cell_type":"markdown","source":"This feature was creted from the structure, you think of it as an engineered feature (no additional information is added)."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Signal to noise:"},{"metadata":{},"cell_type":"markdown","source":"Signal to noise is a sort of quality control feature, it's the measurements relative to their errors (we're gonna calculate this column in the next section), this feature can tell us how confident measurements are, the higher value the more confident measurements are.\n\nLet's see how these numbers are distributed:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = train['signal_to_noise'].plot(kind='hist', bins=40, figsize=(15,3))\ntrain['signal_to_noise'].plot(kind='kde', ax=ax, secondary_y=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be two clusters in this distribution, one in the middle and the other around 0 seems to be some outliers."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#pd.DataFrame(train.query('signal_to_noise == 0').reactivity.values.tolist()).head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#pd.DataFrame(train.query('signal_to_noise == 0').reactivity_error.values.tolist()).head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['signal_to_noise'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the values varies significantly from 2 folds relative to the error to 6 folds, I guess a value of 6 is much more confident than 2.\nThis feature should be taken is consideration when making a model, for example you could use it as training weights."},{"metadata":{},"cell_type":"markdown","source":"## How is Signal to noise calculated:"},{"metadata":{},"cell_type":"markdown","source":"This value is calculated by dividing the each base measurement with it corresponding error value, then averaging all these ratios, as it is shown in the formula below:"},{"metadata":{},"cell_type":"markdown","source":"$$snr = Avg({\\dfrac{measurement_{base}}{error_{base}}})$$"},{"metadata":{},"cell_type":"markdown","source":"I'm going to use a custom function to caculate the ratio with respect to each row and add it as a new column:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_snr(sample):\n    \"\"\"This function takes a row and return signal to noise\n        ratio accross all measurments\"\"\"\n    ratios = np.zeros(5)\n    for i , (deg, err) in enumerate(zip(mes_cols, err_cols)):\n        ratio = (np.array(sample[deg]) / np.array(sample[err])).mean()\n        ratios[i] = ratio\n    return ratios.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['snr'] = train.apply(calc_snr, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['signal_to_noise', 'snr']].sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The numbers are pretty close to what we calculated, let's see how far the calculated numbers are far from the numbers on the dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.eval('signal_to_noise - snr').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are off with just a 1, probably this is due to the fact that the numbers where reduced to lower precision floating numbers."},{"metadata":{},"cell_type":"markdown","source":"## SN Filter:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.SN_filter.hist(bins=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Measurement errors:"},{"metadata":{},"cell_type":"markdown","source":"I don't know exactly what the statistical significance of these numbers is, one of the Hosts posted a reference about how these columns were obtained: [https://www.kaggle.com/c/stanford-covid-vaccine/discussion/182417](https://www.kaggle.com/c/stanford-covid-vaccine/discussion/182417)"},{"metadata":{},"cell_type":"markdown","source":"Now we're gonna analyse these errors, let's start by getting the average:\n\n-- we will first avrage with respect to each row then we aggregate:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['avg_err'] = train[err_cols].applymap(np.mean).mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at how they are distributed:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['avg_err'].plot.hist(bins=10)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the distribution has a long tail, 25000 as an error is probably an outlier, let's look upclose:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['avg_err'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The majority of the values are lower than 1 as we can see from the quantiles.\n\nLet's look why some datapoints have a great error value:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['avg_err'].quantile(0.94)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#mask = train.query('signal_to_noise > 1')[mes_cols].apply(lambda row: np.any([np.any(np.array(c) == 0) for c in row]), axis=1)\n#filtred = train.query('signal_to_noise > 1')[mask]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting errors for one sample. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axs = plt.subplots(5,1, figsize=(10, 15))\naxs.flatten()\nsample = train[train.SN_filter == 1].sample(1).iloc[0]\n\nfor i, err_col in enumerate(err_cols):\n    axs[i].plot(sample[err_col],color='red', drawstyle='steps-mid')\n    axs[i].set_title(err_col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some correlation is indeed present. let see the average overage all samples."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axs = plt.subplots(5,1, figsize=(10, 15))\naxs.flatten()\n\nfor i, err_col in enumerate(err_cols):\n    errs = np.array(train[train.signal_to_noise > 1][err_col].values.tolist())\n    for err in errs:\n        axs[i].plot(err,color='black', alpha=0.01, zorder=-32)\n    errs_avg = errs.mean(axis=0)\n    errs_std = errs.std(axis=0)\n    axs[i].errorbar(np.arange(68), errs_avg, yerr=errs_std, color='red', ecolor='yellow',  drawstyle='steps-mid')\n    axs[i].set_ylim(0, 0.7)\n    axs[i].set_title(err_col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The errors tend to decrease and rise slightly by the end of the sequence."},{"metadata":{},"cell_type":"markdown","source":"## Estimating the irreducible error:"},{"metadata":{},"cell_type":"markdown","source":"I'm assuming the errors are one standard deviation of the mean value accross multiple measurements. We're gonna use only the filtered instances since the private data will be filtered too and also they don't have outliers with huge error values."},{"metadata":{},"cell_type":"markdown","source":"$$\\text{MSE} = E\\big[(y - \\hat{f}(x))^2\\big]$$\n$$= \\text{Var}(y) + \\text{Var}(\\hat{f}(x)) + (y - E[\\hat{f}(x)])^2$$\n$$= \\varepsilon^2 + \\text{Var}[\\hat{f}(x)] + \\text{Bias}[\\hat{f}(x)]^2$$\n\nby putting $$\\hat{f}(x) = f(x)$$ the bias and variance terms cancel out and we get: $$\\text{MSE} = \\varepsilon^2$$\n\n(for more details check this [notebook](https://www.kaggle.com/residentmario/bias-variance-tradeoff))."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating the variances for each measurements\nall_var = np.array(train.query('SN_filter == 1')[err_cols].applymap(lambda c: np.array(c) ** 2).values.tolist()) # shape: (1589, 5, 68)\n\n#averaging along the sequence and samples axis\nmse = all_var.mean(2).mean(0)\n\n#square root and column-wise mean\nmcrmse = np.sqrt(mse).mean()\nmcrmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fair enough!"},{"metadata":{},"cell_type":"markdown","source":"## The measurements:"},{"metadata":{},"cell_type":"markdown","source":"Averaging over all instances with standard deviation as errorbar."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axs = plt.subplots(5,1, figsize=(10, 15))\naxs.flatten()\n\nfor i, mes_col in enumerate(mes_cols):\n    mess = np.array(train[train.SN_filter == 1][mes_col].values.tolist())\n    for mes in mess:\n        axs[i].plot(mes,color='black', alpha=0.01, zorder=-32)\n    mess_avg = mess.mean(axis=0)\n    mess_std = mess.std(axis=0)\n    axs[i].errorbar(np.arange(68), mess_avg, yerr=mess_std, color='lime', ecolor='yellow', drawstyle='steps-mid')\n    axs[i].set_ylim(0, 4)\n    axs[i].set_title(err_col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ploting one sample with the corresponding error as error bars. (you'll get different sample each time you run the cell bellow ;) )."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axs = plt.subplots(5,1, figsize=(10, 15))\naxs.flatten()\nsample = train[train.SN_filter == 1].sample(1).iloc[0]\n\nfor i, (mes_col, err_col) in enumerate(zip(mes_cols, err_cols)):\n    err = np.array(sample[err_col])\n    mes = np.array(sample[mes_col])\n    axs[i].errorbar(np.arange(68), mes, yerr=err, color='blue', ecolor='red', drawstyle='steps-mid', barsabove=True)\n    axs[i].set_title(mes_col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is an obvious correlation between the mesurements."},{"metadata":{},"cell_type":"markdown","source":"## bbps files:"},{"metadata":{},"cell_type":"markdown","source":"The numpy arrays in this folder gives the the probability distribution for each base pair to be connected, the structure feature was basically sampled from these distributions with highest probability. For more details about this refferer to: [What is the bpps folder and the data in each file?](https://www.kaggle.com/c/stanford-covid-vaccine/discussion/182021).\n\nLet's plot the distribution:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_id = 'id_20dec87f6'\nsample_dist = np.load('../input/stanford-covid-vaccine/bpps/' + sample_id + '.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nplt.imshow(sample_dist)\nplt.colorbar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have made a small algorithm that samples from this distribution, this algorithm gives advantage to pair-base with higher probabilities and has a probability threshold."},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_loop(sub_struc):\n    balance = 0\n    for c in sub_struc:\n        if balance < 0:\n            return True\n        if c == b'(':\n            balance += 1\n        if c == b')':\n            balance -= 1\n    return balance != 0\n    \ndef sample_struc(_id, min_prob = 0.4):\n    dist = np.load('../input/stanford-covid-vaccine/bpps/' + _id + '.npy')\n    struc = np.chararray(dist.shape[0])\n    struc[:] = '.'\n    dist = np.tril(dist)\n    while(True):\n        if dist.max() < min_prob:\n            break\n        args = np.argwhere(dist == dist.max())\n        for [i, j] in args:\n            if not is_loop(struc[j:i]) and struc[i] == b'.':\n                #print([j, i, dist.max()])\n                struc[i] = ')'\n                struc[j] = '('\n            dist[i, j] = 0\n    return struc.tostring().decode(\"utf-8\") \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_struct = sample_struc(sample_id)\nsample_struct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.query('id == @sample_id').iloc[0]['structure']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On this sample the algorithm ouputed the same structure from the dataset, let's see how accurate the algorithm is with all the structures in the instances:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ds = pd.concat([train, test])\nds['sample_struc'] = ds['id'].apply(sample_struc)\nshape = ds.query('structure == sample_struc').shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.query('structure == sample_struc').shape[0] / ds.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"About 42% of the instances have the same structure sampled by this simple algorithm, other algorithms probably use other assumptions like give priority to structures with lower energy."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"true = list()\nfor i, c in enumerate(true_struc):\n    if c == '(':\n        balance = 0\n        for j , _c in enumerate(true_struc[i + 1:]):\n            if _c == ')' and balance == 0:\n                true.append(((i, i + j + 1), sample_dist[i][i + j + 1]))\n                break;\n            if _c == ')':\n                balance -= 1\n            if _c == '(':\n                balance += 1\ntrue.sort(key=lambda x: x[1], reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"smpld = list()\ndist = np.tril(sample_dist)\nmin_prob = 0.1\nwhile(True):\n    if dist.max() < min_prob:\n        break\n    args = np.argwhere(dist == dist.max())\n    for [i, j] in args:\n        smpld.append(((j,i), dist.max()))\n        dist[i, j] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}