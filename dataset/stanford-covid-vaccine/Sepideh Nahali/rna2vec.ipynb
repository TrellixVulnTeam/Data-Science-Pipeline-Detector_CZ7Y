{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-28T17:02:13.696522Z","iopub.execute_input":"2021-09-28T17:02:13.69739Z","iopub.status.idle":"2021-09-28T17:02:13.701633Z","shell.execute_reply.started":"2021-09-28T17:02:13.69734Z","shell.execute_reply":"2021-09-28T17:02:13.701001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"source: https://github.com/xypan1232/iDeepV/blob/master/RNA2Vec.py","metadata":{}},{"cell_type":"code","source":"'''\nThis script performs learning the distributed representation for 6-mers using the continuous skip-gram model with 5 sample negative sampling\n'''\nimport sys\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport pdb\n\nmin_count = 5\ndims = [50,]\nwindows = [5,]\nallWeights = []\n\ndef get_6_trids():\n    nucle_com = []\n    chars = ['A', 'C', 'G', 'U']\n    base=len(chars)\n    end=len(chars)**6\n    for i in range(0,end):\n        n=i\n        ch0=chars[n%base]\n        n=n//base\n        ch1=chars[n%base]\n        n=n//base\n        ch2=chars[n%base]\n        n=n//base\n        ch3=chars[n%base]\n        n=n//base\n        ch4=chars[n%base]\n        n=n//base\n        ch5=chars[n%base]\n        nucle_com.append(ch0 + ch1 + ch2 + ch3 + ch4 + ch5)\n    return  nucle_com   \n\ndef get_7_trids():\n    nucle_com = []\n    chars = ['A', 'C', 'G', 'U']\n    base=len(chars)\n    end=len(chars)**7\n    for i in range(0,end):\n        n=i\n        ch0=chars[n%base]\n        n=n//base\n        ch1=chars[n%base]\n        n=n//base\n        ch2=chars[n%base]\n        n=n//base\n        ch3=chars[n%base]\n        n=n//base\n        ch4=chars[n%base]\n        n=n//base\n        ch5=chars[n%base]\n        n=n//base\n        ch6=chars[n%base]\n        nucle_com.append(ch0 + ch1 + ch2 + ch3 + ch4 + ch5 + ch6)\n    return  nucle_com   \n\ndef get_4_nucleotide_composition(tris, seq):\n    seq_len = len(seq)\n    tri_feature = []\n    k = len(tris[0])\n    #tmp_fea = [0] * len(tris)\n    for x in range(len(seq) + 1- k):\n        kmer = seq[x:x+k]\n        if kmer in tris:\n            ind = tris.index(kmer)\n            tri_feature.append(str(ind))\n    #tri_feature = [float(val)/seq_len for val in tmp_fea]\n        #pdb.set_trace()        \n    return tri_feature\n\ndef seq2words(sequ):\n    tris = get_6_trids()\n    seq = sequ\n    seq = seq.replace('T', 'U')\n#     pdb.set_trace()\n    trvec = get_4_nucleotide_composition(tris, seq)\n    return trvec\n\n\ndef train_rnas(seq_file , outfile= 'rnaEmbedding25.pickle'):\n    min_count = 1\n    dim = 107\n    window = 5\n\n    print('dim: ' + str(dim) + ', window: ' + str(window))\n    train = pd.read_json(seq_file, lines=True)\n    df_Seq = train['sequence']\n    seq_dict = df_Seq.to_dict()\n    #text = seq_dict.values()\n    tris = get_6_trids()# all poibl compoitions\n    sentences = []\n    for seq in seq_dict.values():\n        seq = seq.replace('T', 'U')\n        trvec = get_4_nucleotide_composition(tris, seq)\n        #for aa in range(len(text)):\n        sentences.append(trvec)\n\n#     print('sentences',sentences)\n\n    model = None\n\n    model = Word2Vec(sentences, min_count=min_count, vector_size =dim, window=window, sg=1, batch_words=100)\n    \n\n    vocab = list(model.wv.index_to_key)\n    fw = open('rna_dict', 'w')\n    for val in vocab:\n        fw.write(val + '\\n')\n    fw.close()\n\n    embeddingWeights = np.empty([len(vocab), dim])\n\n    for i in range(len(vocab)):\n        embeddingWeights[i,:] = model.wv[vocab[i]]  \n\n    allWeights.append(embeddingWeights)\n\n    with open(outfile, 'wb') as f:\n        pickle.dump(allWeights, f)\n        \n    return model\n    \ndef testseq(model,sequence_):  \n    ##############################################vector of Sequence\n    word_vectors = model.wv\n    word_vectors.save(\"word2vec.wordvectors\")\n    wv = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')\n    sentencevector=[]\n    wordslist=seq2words(sequence_)\n#     print('wordslist:',wordslist,'length=',len(wordslist))\n#     print('#')\n\n    for i in wordslist:\n            vector = wv[i]  # Get numpy vector of a word (Size= 25)\n            sentencevector.append(vector)\n    ##############################################vector of Sequence\n    return sentencevector\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T17:02:13.839054Z","iopub.execute_input":"2021-09-28T17:02:13.83977Z","iopub.status.idle":"2021-09-28T17:02:13.863521Z","shell.execute_reply.started":"2021-09-28T17:02:13.839715Z","shell.execute_reply":"2021-09-28T17:02:13.862728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TEST_TRAIN_W2V='train'\n# TEST_TRAIN_W2V='public_test'\nTEST_TRAIN_W2V='private_test'\n\nif TEST_TRAIN_W2V=='train vectors are training':\n    print('train')\n    input_file = '../input/stanford-covid-vaccine/train.json'\n    model=train_rnas(input_file)\n    train = pd.read_json('../input/stanford-covid-vaccine/train.json', lines=True)\n    df_Seq = train['sequence']\n    \nelif TEST_TRAIN_W2V=='public_test vectors are training':\n    print('public_test vectors are training')\n    input_file = '../input/stanford-covid-vaccine/test.json'\n    model=train_rnas(input_file)\n    test = pd.read_json('../input/stanford-covid-vaccine/test.json', lines=True)\n    public_df = test.query(\"seq_length == 107\").copy()\n    df_Seq = public_df['sequence']\n\nelse:\n    print('private_test')\n    input_file = '../input/stanford-covid-vaccine/test.json'\n    model=train_rnas(input_file)\n    test = pd.read_json('../input/stanford-covid-vaccine/test.json', lines=True)\n    private_df = test.query(\"seq_length == 130\").copy()\n    df_Seq = private_df['sequence']  \n    \nallseqvecs={}\nfor i,s in df_Seq.items():\n    vec=testseq(model,s)\n    allseqvecs[i]=vec\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T17:02:14.433214Z","iopub.execute_input":"2021-09-28T17:02:14.433821Z","iopub.status.idle":"2021-09-28T17:04:27.507097Z","shell.execute_reply.started":"2021-09-28T17:02:14.433779Z","shell.execute_reply":"2021-09-28T17:04:27.506196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(allseqvecs[1])","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-28T17:04:27.509372Z","iopub.execute_input":"2021-09-28T17:04:27.509908Z","iopub.status.idle":"2021-09-28T17:04:27.515893Z","shell.execute_reply.started":"2021-09-28T17:04:27.509853Z","shell.execute_reply":"2021-09-28T17:04:27.515026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ntrain_Seq_Vec=[]\n\nfor i,_ in df_Seq.items():\n    Seq_each_Word_vec = np.array(allseqvecs[i])\n    Seq_vec=np.average(Seq_each_Word_vec, axis=0)\n    train_Seq_Vec.append(Seq_vec)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T17:04:27.517285Z","iopub.execute_input":"2021-09-28T17:04:27.517569Z","iopub.status.idle":"2021-09-28T17:04:27.841478Z","shell.execute_reply.started":"2021-09-28T17:04:27.517536Z","shell.execute_reply":"2021-09-28T17:04:27.840518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(TEST_TRAIN_W2V, 'wb') as f:\n        pickle.dump(train_Seq_Vec, f)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T17:04:27.843283Z","iopub.execute_input":"2021-09-28T17:04:27.843518Z","iopub.status.idle":"2021-09-28T17:04:27.880236Z","shell.execute_reply.started":"2021-09-28T17:04:27.843491Z","shell.execute_reply":"2021-09-28T17:04:27.879258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2vAVG = open(TEST_TRAIN_W2V,'rb')\nnew_dict = pickle.load(w2vAVG)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T17:04:27.881476Z","iopub.execute_input":"2021-09-28T17:04:27.881758Z","iopub.status.idle":"2021-09-28T17:04:27.90158Z","shell.execute_reply.started":"2021-09-28T17:04:27.881727Z","shell.execute_reply":"2021-09-28T17:04:27.900692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(new_dict)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T17:06:40.800424Z","iopub.execute_input":"2021-09-28T17:06:40.801338Z","iopub.status.idle":"2021-09-28T17:06:40.807695Z","shell.execute_reply.started":"2021-09-28T17:06:40.801285Z","shell.execute_reply":"2021-09-28T17:06:40.807114Z"},"trusted":true},"execution_count":null,"outputs":[]}]}