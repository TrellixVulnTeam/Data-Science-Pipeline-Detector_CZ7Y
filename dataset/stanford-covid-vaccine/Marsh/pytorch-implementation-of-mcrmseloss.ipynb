{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Implementation of MCRMSELoss\n\nIn this notebook, MCRMSE is implemented according to [this resource](https://www.kaggle.com/c/stanford-covid-vaccine/overview/evaluation) and some examples are shown.\nExample section is based on the [@hiroshun's notebook](https://www.kaggle.com/hiroshun/pytorch-implementation-gru-lstm). Thank you for publishing a good implementation.\n\nUsing this MCRMSELoss and `SN_filter` `df = df[df.SN_filter == 1]`, you can simulate the LB score. However you got to be carefull that the private score does not use `SN_filter`."},{"metadata":{},"cell_type":"markdown","source":"## MCRMSELoss"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport random\n\n#the basics\nimport pandas as pd, numpy as np, seaborn as sns\nimport math, json\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\n\n#for model evaluation\nfrom sklearn.model_selection import train_test_split, KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nSEED = 2020\n\n\ndef seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\n\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from torch import nn\n\n\nclass RMSELoss(nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        self.eps = eps\n\n    def forward(self, yhat, y):\n        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n        return loss\n\n\nclass MCRMSELoss(nn.Module):\n    def __init__(self, num_scored=3):\n        super().__init__()\n        self.rmse = RMSELoss()\n        self.num_scored = num_scored\n\n    def forward(self, yhat, y):\n        score = 0\n        for i in range(self.num_scored):\n            score += self.rmse(yhat[:, :, i], y[:, :, i]) / self.num_scored\n\n        return score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## usage"},{"metadata":{},"cell_type":"markdown","source":"just as we use the loss,\n```python\ncriterion = MCRMSELoss()\npredictions = model(data)\nloss = criterion(predictions, targets)\n```"},{"metadata":{},"cell_type":"markdown","source":"if you use `SN_filter`, you can get the LB-like score. \nLet's see how it works."},{"metadata":{},"cell_type":"markdown","source":"### Dataload"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n\ndef load_json(path):\n    return pd.read_json(path, lines=True)\n\ndf = load_json('/kaggle/input/stanford-covid-vaccine/train.json')\ndf_test = load_json('/kaggle/input/stanford-covid-vaccine/test.json')\nsample_sub = pd.read_csv('/kaggle/input/stanford-covid-vaccine/sample_submission.csv')\ndf = df[df.SN_filter == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### preprocess"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"target_cols = [\"reactivity\", \"deg_Mg_pH10\", \"deg_Mg_50C\"]\ntoken2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n\n\ndef preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    return np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )\n\ntrain_inputs = torch.tensor(preprocess_inputs(df)).to(device)\nprint(\"input shape: \", train_inputs.shape)\ntrain_labels = torch.tensor(\n    np.array(df[target_cols].values.tolist()).transpose(0, 2, 1)\n).float().to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTM_model(nn.Module):\n    def __init__(\n        self, seq_len=107, pred_len=68, dropout=0.5, embed_dim=100, hidden_dim=128, hidden_layers=3\n    ):\n        super(LSTM_model, self).__init__()\n        self.pred_len = pred_len\n\n        self.embeding = nn.Embedding(num_embeddings=len(token2int), embedding_dim=embed_dim)\n        self.gru = nn.LSTM(\n            input_size=embed_dim * 3,\n            hidden_size=hidden_dim,\n            num_layers=hidden_layers,\n            dropout=dropout,\n            bidirectional=True,\n            batch_first=True,\n        )\n        self.linear = nn.Linear(hidden_dim * 2, len(target_cols))\n\n    def forward(self, seqs):\n        embed = self.embeding(seqs)\n        reshaped = torch.reshape(embed, (-1, embed.shape[1], embed.shape[2] * embed.shape[3]))\n        output, hidden = self.gru(reshaped)\n        truncated = output[:, : self.pred_len, :]\n        out = self.linear(truncated)\n        return out\n\ncriterion = MCRMSELoss(len(target_cols))\n\ndef compute_loss(batch_X, batch_Y, model, optimizer=None, is_train=True):\n    model.train(is_train)\n\n    pred_Y = model(batch_X)\n\n    loss = criterion(pred_Y, batch_Y)\n\n    if is_train:\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    return loss.item()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"FOLDS = 4\nEPOCHS = 90\nBATCH_SIZE = 64\nVERBOSE = 2\nLR = 0.01","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#get different test sets and process each\npublic_df = df_test.query(\"seq_length == 107\").copy()\nprivate_df = df_test.query(\"seq_length == 130\").copy()\n\npublic_inputs = torch.tensor(preprocess_inputs(public_df)).to(device)\nprivate_inputs = torch.tensor(preprocess_inputs(private_df)).to(device)\n\npublic_loader = DataLoader(TensorDataset(public_inputs), shuffle=False, batch_size=BATCH_SIZE)\nprivate_loader = DataLoader(TensorDataset(private_inputs), shuffle=False, batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KFold Training and Inference"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"lstm_histories = []\nlstm_private_preds = np.zeros((private_df.shape[0], 130, len(target_cols)))\nlstm_public_preds = np.zeros((public_df.shape[0], 107, len(target_cols)))\n\ncriterion = MCRMSELoss()\n\nkfold = KFold(FOLDS, shuffle=True, random_state=2020)\n\nfor k, (train_index, val_index) in enumerate(kfold.split(train_inputs)):\n    train_dataset = TensorDataset(train_inputs[train_index], train_labels[train_index])\n    val_dataset = TensorDataset(train_inputs[val_index], train_labels[val_index])\n\n    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n    val_loader = DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE)\n\n    model = LSTM_model().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=LR)\n\n    train_losses = []\n    val_losses = []\n    for epoch in tqdm(range(EPOCHS)):\n        train_losses_batch = []\n        val_losses_batch = []\n        for (batch_X, batch_Y) in train_loader:\n            train_loss = compute_loss(batch_X, batch_Y, model, optimizer=optimizer, is_train=True)\n            train_losses_batch.append(train_loss)\n        for (batch_X, batch_Y) in val_loader:\n            val_loss = compute_loss(batch_X, batch_Y, model, optimizer=optimizer, is_train=False)\n            val_losses_batch.append(val_loss)\n        train_losses.append(np.mean(train_losses_batch))\n        val_losses.append(np.mean(val_losses_batch))\n    model_state = model.state_dict()\n    del model\n            \n    lstm_histories.append({'train_loss': train_losses, 'val_loss': val_losses})\n\n\n    lstm_short = LSTM_model(seq_len=107, pred_len=107).to(device)\n    lstm_short.load_state_dict(model_state)\n    lstm_short.eval()\n    lstm_public_pred = np.ndarray((0, 107, len(target_cols)))\n    for batch in public_loader:\n        batch_X = batch[0]\n        pred = lstm_short(batch_X).detach().cpu().numpy()\n        lstm_public_pred = np.concatenate([lstm_public_pred, pred], axis=0)\n    lstm_public_preds += lstm_public_pred / FOLDS\n\n    lstm_long = LSTM_model(seq_len=130, pred_len=130).to(device)\n    lstm_long.load_state_dict(model_state)\n    lstm_long.eval()\n    lstm_private_pred = np.ndarray((0, 130, len(target_cols)))\n    for batch in private_loader:\n        batch_X = batch[0]\n        pred = lstm_long(batch_X).detach().cpu().numpy()\n        lstm_private_pred = np.concatenate([lstm_private_pred, pred], axis=0)\n    lstm_private_preds += lstm_private_pred / FOLDS\n    \n    del lstm_short, lstm_long","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\nfig, ax = plt.subplots(1, 1, figsize = (20, 10))\n\nfor history in lstm_histories:\n    ax.plot(history['train_loss'], 'b')\n    ax.plot(history['val_loss'], 'r')\n\nax.set_title('LSTM')\n\nax.legend(['train', 'validation'], loc = 'upper right')\n\nax.set_ylabel('Loss')\nax.set_xlabel('Epoch');\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"public_df = df_test.query(\"seq_length == 107\").copy()\nprivate_df = df_test.query(\"seq_length == 130\").copy()\n\npublic_inputs = preprocess_inputs(public_df)\nprivate_inputs = preprocess_inputs(private_df)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"preds_lstm = []\n\nfor df, preds in [(public_df, lstm_public_preds), (private_df, lstm_private_preds)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=target_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_lstm.append(single_df)\n\npreds_lstm_df = pd.concat(preds_lstm)\npreds_lstm_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"submission = sample_sub[['id_seqpos']].merge(preds_lstm_df, on=['id_seqpos'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"submission['deg_pH10'] = 0\nsubmission['deg_50C'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nprint('Submission saved')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}