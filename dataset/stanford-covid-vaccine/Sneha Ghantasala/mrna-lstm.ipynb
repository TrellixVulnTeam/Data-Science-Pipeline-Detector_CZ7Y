{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Have referred the code for data analysis and usage from some notebooks in the competition thread."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport json\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_json(r'/kaggle/input/stanford-covid-vaccine/train.json', lines=True)\nif ~train_df.isnull().values.any(): print('No missing values')\ntrain_df.head()\n\n\nlimit = 68\ntrain_df['sequence'] = train_df['sequence'].str.slice(0, limit)\ntrain_df['structure'] = train_df['structure'].str.slice(0, limit)\ntrain_df['predicted_loop_type'] = train_df['predicted_loop_type'].str.slice(0, limit)\n\ntest_df = pd.read_json(r'/kaggle/input/stanford-covid-vaccine/test.json', lines=True)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_tokens = {'A': 0.2, 'U': 0.4, 'G': 0.6, 'C': 0.8}\nstructure_tokens = {'(': 0.33, ')': 1, '.': 0.66}\npredictedlooptype_tokens = {'S': 0.125, 'M': 0.25, 'I': 0.375, 'E': 0.5, 'X': 0.625, 'H': 0.75, 'B': 0.875}\nall_tokens = {'A': 0.2, 'U': 0.4, 'G': 0.6, 'C': 0.8, '(': 0.33, ')': 1, '.': 0.66, 'S': 0.125, 'M': 0.25, 'I': 0.375, 'E': 0.5, 'X': 0.625, 'H': 0.75, 'B': 0.875}\n\nprint(all_tokens['A'])\n# train_input_cols=['sequence', 'structure', 'predicted_loop_type']\n# train_df['tokenized_sequence'] = train_df['tokenized_sequence'].astype(float)\n# train_df['tokenized_structure'] = train_df['tokenized_structure'].astype(float)\n# train_df['tokenized_predicted_loop_type'] = train_df['tokenized_predicted_loop_type'].astype(float)\npred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C']\n\ntokenized_sequence_arr = []\ntokenized_structure_arr = []\ntokenized_predicted_loop_type_arr = []\n\nfor idx in range(0, train_df.shape[0]):\n    seq = train_df.iloc[idx, train_df.columns.get_loc('sequence')]\n    arr = []\n    for char in seq:\n        arr.append(float(all_tokens[char]))\n    tokenized_sequence_arr.append(arr)\n    \n    seq = train_df.iloc[idx, train_df.columns.get_loc('structure')]\n    arr = []\n    for char in seq:\n        arr.append(all_tokens[char])\n    tokenized_structure_arr.append(arr)\n\n    seq = train_df.iloc[idx, train_df.columns.get_loc('predicted_loop_type')]\n    arr = []\n    for char in seq:\n        arr.append(all_tokens[char])\n    tokenized_predicted_loop_type_arr.append(arr)\n    \ncreated_df = pd.DataFrame({'tokenized_sequence': tokenized_sequence_arr, 'tokenized_structure': tokenized_structure_arr, \n                           'tokenized_predicted_loop_type': tokenized_predicted_loop_type_arr})\ntrain_input_cols = ['tokenized_sequence', 'tokenized_structure', 'tokenized_predicted_loop_type']\n# train_inputs = np.array(\n#             created_df[train_input_cols].values.tolist()\n#         )\n# train_labels = np.array(train_df[pred_cols].values.tolist())\ntrain_inputs= np.transpose(\n        np.array(\n            created_df[train_input_cols].values.tolist()\n        ),\n        (0, 2, 1)\n    )\ntrain_labels = np.transpose(\n        np.array(train_df[pred_cols].values.tolist()),\n        (0, 2, 1)\n    )\nprint(train_inputs.shape)\nprint(train_labels.shape)\n# print(train_inputs[0])\n# print(train_labels[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, utils\nclass MrnaDataset(Dataset):\n    \"\"\"Mrna dataset.\"\"\"\n\n    def __init__(self, dataframe, labels):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.train_inputs = dataframe\n        self.train_labels = labels\n\n    def __len__(self):\n        return self.train_inputs.shape[0]\n\n    def __getitem__(self, idx):\n        data = self.train_inputs[idx]\n        labels = self.train_labels[idx]\n        sample = {'inputs': data, 'labels': labels}\n#         print(sample)\n        return sample\n\nfrom torch.utils.data.dataloader import default_collate \n\n\ndef id_collate(batch):\n    new_batch = []\n    ids = []\n    for _batch in batch:\n        new_batch.append(_batch['inputs'])\n        ids.append(_batch['labels'])\n    print(ids)\n    return default_collate(new_batch), ids\n\nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\n    def __call__(self, sample):\n        inputs, labels = sample['inputs'], sample['labels']\n        return {'inputs': torch.from_numpy(inputs), 'labels': labels}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nclass LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, pred_len):\n        super(LSTMModel, self).__init__()\n        \n        # Number of hidden dimensions\n        self.hidden_dim = hidden_dim\n        \n        # Number of hidden layers\n        self.layer_dim = layer_dim\n        \n        # LSTM\n        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n        \n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        \n        self.pred_len = pred_len\n    \n    def forward(self, x):\n        \n        # Initialize hidden state with zeros\n#         h0 = torch.randn(self.layer_dim, x.shape[0], self.hidden_dim)\n#         c0 = torch.randn(self.layer_dim, x.shape[0], self.hidden_dim)\n#         h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n            \n        # One time step\n#         out, hn = self.lstm(x)\n        output, (hn, cn) = self.lstm(x)\n        out = output[:, : self.pred_len, :]\n#         out = self.fc(out[:, -1, :]) \n        return out, hn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SAVE_WEIGHTS_PATH = r'/kaggle/input/lstm-weights/weights.pth'\n# batch_size, epoch and iteration\nbatch_size = 64\n# n_iters = 8000\n# num_epochs = n_iters / (len(features_train) / batch_size)\n# num_epochs = int(num_epochs)\n\n# data loader\ndataset = MrnaDataset(train_inputs, train_labels)\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle = False)\n# test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n    \n# Create RNN\ninput_dim =  3   # input dimension, number of features as input for each lstm cell\nhidden_dim = 5  # hidden layer dimension\nlayer_dim = 107     # number of lstm cells\noutput_dim = 5\npred_len = 68\n\n# model = melanomaModel().cuda() if params.cuda else melanomaModel()\nif torch.cuda.is_available():\n    model = LSTMModel(input_dim, hidden_dim, layer_dim, pred_len).cuda()\nelse:\n    model = LSTMModel(input_dim, hidden_dim, layer_dim, pred_len)\n\n# model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n\n# Cross Entropy Loss \n# error = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.001\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\nloss_function = nn.MSELoss()\nrestore_file = True\n\nif restore_file:\n    print('restored file')\n    checkpoint = torch.load(SAVE_WEIGHTS_PATH)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n# See what the scores are before training\n# Note that element i,j of the output is the score for tag j for word i.\n# Here we don't need to train, so the code is wrapped in torch.no_grad()\n# with torch.no_grad():\n#     inputs = prepare_sequence(training_data[0][0], word_to_ix)\n#     tag_scores = model(inputs)\n#     print(tag_scores)\n\nfor epoch in range(2):  # again, normally you would NOT do 300 epochs, it is toy data\n    print('epoch: ',epoch)\n    for index, sampled_batch in enumerate(train_loader):\n        # Step 1. Remember that Pytorch accumulates gradients.\n        # We need to clear them out before each instance\n        inputs, labels = sampled_batch['inputs'].float(), sampled_batch['labels'].float()\n        if torch.cuda.is_available():\n            inputs, labels = inputs.cuda(non_blocking=True), labels.cuda(non_blocking=True)\n            \n#         print(inputs.shape)\n#         print(labels.shape)\n        model.zero_grad()\n\n        # Step 3. Run our forward pass.\n        batch_output, states = model(inputs)\n#         print('--------------')\n#         print(batch_output.shape)\n#         print(states)\n#         print(labels.shape)\n#         print(batch_output)\n\n        # Step 4. Compute the loss, gradients, and update the parameters by\n        #  calling optimizer.step()\n        loss = loss_function(batch_output, labels)\n#         print(loss)\n        loss.backward()\n        optimizer.step()\n    print(loss)\n#     torch.save({\n#         'epoch': epoch+1,\n#         'model_state_dict': model.state_dict(),\n#         'optimizer_state_dict': optimizer.state_dict(),\n# #             'loss': loss\n# #             is_best=is_best\n#         }, SAVE_WEIGHTS_PATH)\n\n# See what the scores are after training\n# with torch.no_grad():\n#     tag_scores = model(inputs)\n\n    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n    # for word i. The predicted tag is the maximum scoring tag.\n    # Here, we can see the predicted sequence below is 0 1 2 0 1\n    # since 0 is index of the maximum value of row 1,\n    # 1 is the index of maximum value of row 2, etc.\n    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n#     print(tag_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_public_df = test_df.query(\"seq_length == 107\")\ntest_private_df = test_df.query(\"seq_length == 130\")\nlimit = 68\ntest_public_df['sequence'] = test_public_df['sequence'].str.slice(0, limit)\ntest_public_df['structure'] = test_public_df['structure'].str.slice(0, limit)\ntest_public_df['predicted_loop_type'] = test_public_df['predicted_loop_type'].str.slice(0, limit)\n\nlimit = 91\ntest_private_df['sequence'] = test_private_df['sequence'].str.slice(0, limit)\ntest_private_df['structure'] = test_private_df['structure'].str.slice(0, limit)\ntest_private_df['predicted_loop_type'] = test_private_df['predicted_loop_type'].str.slice(0, limit)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C']\n\ndef tokenize_inputs(df):\n    tokenized_sequence_arr = []\n    tokenized_structure_arr = []\n    tokenized_predicted_loop_type_arr = []\n    ids = []\n    for idx in range(0, df.shape[0]):\n        ids.append(df.iloc[idx, df.columns.get_loc('id')])\n        seq = df.iloc[idx, df.columns.get_loc('sequence')]\n        arr = []\n        for char in seq:\n            arr.append(float(all_tokens[char]))\n        tokenized_sequence_arr.append(arr)\n\n        seq = df.iloc[idx, df.columns.get_loc('structure')]\n        arr = []\n        for char in seq:\n            arr.append(all_tokens[char])\n        tokenized_structure_arr.append(arr)\n\n        seq = df.iloc[idx, df.columns.get_loc('predicted_loop_type')]\n        arr = []\n        for char in seq:\n            arr.append(all_tokens[char])\n        tokenized_predicted_loop_type_arr.append(arr)\n\n    created_df = pd.DataFrame({'tokenized_sequence': tokenized_sequence_arr, 'tokenized_structure': tokenized_structure_arr, \n                           'tokenized_predicted_loop_type': tokenized_predicted_loop_type_arr, 'id': ids})\n    created_ids = pd.DataFrame({'id': ids})\n    return created_df, created_ids\n\ntest_input_cols = ['tokenized_sequence', 'tokenized_structure', 'tokenized_predicted_loop_type']\ntrain_input_cols = ['tokenized_sequence', 'tokenized_structure', 'tokenized_predicted_loop_type']\n\ncreated_df_public, created_ids_public = tokenize_inputs(test_public_df)\ncreated_df_private, created_ids_private = tokenize_inputs(test_private_df)\n\ntest_inputs_public = np.transpose(\n        np.array(\n            created_df_public[train_input_cols].values.tolist()\n        ),\n        (0, 2, 1)\n    )\ntest_ids_public = np.transpose(\n        np.array(\n            created_ids_public[['id']].values.tolist()\n        ),\n        (0, 1)\n    )\ntest_inputs_private = np.transpose(\n        np.array(\n            created_df_private[train_input_cols].values.tolist()\n        ),\n        (0, 2, 1)\n    )\ntest_ids_private = np.transpose(\n        np.array(\n            created_ids_private[['id']].values.tolist()\n        ),\n        (0, 1)\n    )\nprint(test_inputs_public.shape)\nprint(test_ids_public.shape)\nprint(test_inputs_private.shape)\nprint(test_ids_private.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    \n# Create RNN\ninput_dim =  3   # input dimension, number of features as input for each lstm cell\nhidden_dim = 5  # hidden layer dimension\nlayer_dim = 107     # number of lstm cells\noutput_dim = 5\npred_len = 68\n\n# pred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C']\n# model = melanomaModel().cuda() if params.cuda else melanomaModel()\nif torch.cuda.is_available():\n    model = LSTMModel(input_dim, hidden_dim, layer_dim, pred_len).cuda()\nelse:\n    model = LSTMModel(input_dim, hidden_dim, layer_dim, pred_len)\n\ncheckpoint = torch.load(SAVE_WEIGHTS_PATH)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nvalues = []\n\ninputs, labels = test_inputs_public, test_ids_public\nif torch.cuda.is_available():\n    inputs = torch.from_numpy(inputs).float().cuda(non_blocking=True)\nbatch_output, states = model(inputs)\noutputs = batch_output.data.cpu().numpy()\noutputs = np.array(np.transpose(outputs, (0, 2, 1)))\n#     print(reactivity)\nprint(labels[0])\nreactivity = np.array(outputs[:, 0, :]).tolist()\ndeg_Mg_pH10 = np.array(outputs[:, 0, :]).tolist()\ndeg_Mg_50C = np.array(outputs[:, 0, :]).tolist()\ndeg_pH10 = np.array(outputs[:, 0, :]).tolist()\ndeg_50C = np.array(outputs[:, 0, :]).tolist()\nfor i in range(outputs.shape[0]):\n    for j in range(68):\n        values.append([str(labels[i][0])+ '_' + str(j), reactivity[i][j], deg_Mg_pH10[i][j], deg_Mg_50C[i][j], deg_pH10[i][j], deg_50C[i][j]])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create RNN\ninput_dim =  3   # input dimension, number of features as input for each lstm cell\nhidden_dim = 5  # hidden layer dimension\nlayer_dim = 107    # number of lstm cells\noutput_dim = 5\npred_len = 91\n\n# pred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C']\n# model = melanomaModel().cuda() if params.cuda else melanomaModel()\nif torch.cuda.is_available():\n    model = LSTMModel(input_dim, hidden_dim, layer_dim, pred_len).cuda()\nelse:\n    model = LSTMModel(input_dim, hidden_dim, layer_dim, pred_len)\n\ncheckpoint = torch.load(SAVE_WEIGHTS_PATH)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\ninputs, labels = test_inputs_private, test_ids_private\nif torch.cuda.is_available():\n    inputs = torch.from_numpy(inputs).float().cuda(non_blocking=True)\nbatch_output, states = model(inputs)\noutputs = batch_output.data.cpu().numpy()\noutputs = np.array(np.transpose(outputs, (0, 2, 1)))\n#     print(reactivity)\nprint(labels[0])\nreactivity = np.array(outputs[:, 0, :]).tolist()\ndeg_Mg_pH10 = np.array(outputs[:, 0, :]).tolist()\ndeg_Mg_50C = np.array(outputs[:, 0, :]).tolist()\ndeg_pH10 = np.array(outputs[:, 0, :]).tolist()\ndeg_50C = np.array(outputs[:, 0, :]).tolist()\nfor i in range(outputs.shape[0]):\n    for j in range(91):\n        values.append([str(labels[i][0])+ '_' + str(j), reactivity[i][j], deg_Mg_pH10[i][j], deg_Mg_50C[i][j], deg_pH10[i][j], deg_50C[i][j]])\n\n\n    \nsample_submission = pd.DataFrame(values, columns=['id_seqpos', 'reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C'])\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}