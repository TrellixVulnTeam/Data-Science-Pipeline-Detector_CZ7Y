{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing basic libraries\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf\n\nfrom numpy import zeros, newaxis\nfrom pathlib import Path\nfrom collections import OrderedDict\n\n# fix random seed for reproducibility\nSEED = 101\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n               \n# Importing the training set\nDATA_DIR = Path(\"../input/stanford-covid-vaccine/\")\nBPPS_DIR = DATA_DIR / \"bpps\"\n\ntrain = pd.read_json(DATA_DIR / \"train.json\", lines=True)\ntest = pd.read_json(DATA_DIR / \"test.json\", lines=True)\naug_df = pd.read_csv('/kaggle/input/how-to-generate-augmentation-data/aug_data.csv')\n\nbppm_paths = list(BPPS_DIR.glob(\"*.npy\"))\n\n# settings\ndebug = False\nTPU = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Light data exploration, to check features lengths\nlen(train['sequence'][0]), len(train['structure'][0]), len(train['predicted_loop_type'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Examine features\ntrain['sequence'][50], train['structure'][50] ,train['predicted_loop_type'][50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Processing\n# Set alphabets\nalphabet = 'AGCU().MXBISHE'\nalphabet_rna = 'AGCU'\nalphabet_struc = '()XXX.'\nalphabet_loop = 'MXB...I...S.H.E'\n\n# Set target_cols\ntarget_cols  = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\nnon_target_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\nmarked_target_cols  = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']\n\ndef get_bppm(id_):\n    return np.load(BPPS_DIR / f\"{id_}.npy\")\n\ndef get_bpps_nb(id_):\n     # from https://www.kaggle.com/symyksr/openvaccine-deepergcn \n    bpps_nb_mean = 0.077522 # mean of bpps_nb across all training data\n    bpps_nb_std = 0.08914   # std of bpps_nb across all training data\n    bpps = get_bppm(id_)\n    bpps_nb = (bpps > 0).sum(axis=0) / bpps.shape[0]\n    bpps_nb = (bpps_nb - bpps_nb_mean) / bpps_nb_std\n    return bpps_nb\n\ndef mk_pair_map(structure, type='pm'):\n    pm = np.full(len(structure), 0, dtype=int)\n    pd = np.full(len(structure), 0, dtype=int)\n    queue = []\n    for i, s in enumerate(structure):\n        if s == \"(\":\n            queue.append(i)\n        elif s == \")\":\n            j = queue.pop()\n            pm[i] = j\n            pm[j] = i\n            pd[i] = i-j\n            pd[j] = i-j\n    if type == 'pm':\n        return pm\n    elif type == 'pd':\n        return pd\n\ndef get_structure_adj(seq_length, structure, sequence):\n    Ss = []\n    cue = []\n    a_structures = OrderedDict([\n        ((\"A\", \"U\"), np.zeros([seq_length, seq_length])),\n        ((\"C\", \"G\"), np.zeros([seq_length, seq_length])),\n        ((\"U\", \"G\"), np.zeros([seq_length, seq_length])),\n        ((\"U\", \"A\"), np.zeros([seq_length, seq_length])),\n        ((\"G\", \"C\"), np.zeros([seq_length, seq_length])),\n        ((\"G\", \"U\"), np.zeros([seq_length, seq_length])),\n    ])\n    for j in range(seq_length):\n        if structure[j] == \"(\":\n            cue.append(j)\n        elif structure[j] == \")\":\n            start = cue.pop()\n            a_structures[(sequence[start], sequence[j])][start, j] = 1\n            a_structures[(sequence[j], sequence[start])][j, start] = 1\n\n    a_strc = np.stack([a for a in a_structures.values()], axis=2)\n    a_strc = np.sum(a_strc, axis=2, keepdims=False)\n    return a_strc\n    \ndef preprocess_data(data):\n    data = data.loc[data['SN_filter'] == 1].copy()\n    data = data.reset_index(drop=True)\n    return data\n\ndef step(seq_length):\n    data = list(range(int(seq_length)))\n    newList = []\n    newList = [x / seq_length for x in data]\n    return newList\n\ndef preprocess_features(data):\n    data['seq'] = data.apply(lambda x: integer_encoder(x['sequence'], alphabet_rna), axis=1)\n    data['struc'] = data.apply(lambda x: integer_encoder(x['structure'], alphabet_struc), axis=1)\n    data['loop'] = data.apply(lambda x: integer_encoder(x['predicted_loop_type'], alphabet_loop), axis=1)\n    data['step'] = data.apply(lambda x: step(x['seq_length']), axis=1) # Doesn't help, not used.\n    data['pair_dist'] = data.structure.apply(mk_pair_map, type='pd') #Not used.\n    data['pair_map'] = data.structure.apply(mk_pair_map, type='pm')\n    data['bppm_max'] = data.apply(lambda x: get_bppm(x['id']).max(0), axis=1)\n    data['bppm_sum'] = data.apply(lambda x: get_bppm(x['id']).sum(0), axis=1)\n    data['bppm_nb'] = data.apply(lambda x: get_bpps_nb(x['id']), axis=1)\n    data['adj_struc'] = data.apply(lambda x: get_structure_adj(x['seq_length'], x['structure'], x['sequence']).sum(0), axis=1)\n    a = np.array(data['seq'].values.tolist())[:,:,newaxis]\n    b = np.array(data['struc'].values.tolist())[:,:,newaxis]\n    c = np.array(data['loop'].values.tolist())[:,:,newaxis]\n    d = np.array(data['adj_struc'].values.tolist())[:,:,newaxis]\n    f = np.array(data['bppm_max'].values.tolist())[:,:,newaxis]\n    g = np.array(data['bppm_sum'].values.tolist())[:,:,newaxis]\n    h = np.array(data['bppm_nb'].values.tolist())[:,:,newaxis] \n    features_all=np.concatenate((a,b,c,d,f,g,h), axis = 2) \n    return features_all\n\ndef preprocess_labels(data):\n    labels = data[target_cols].copy()\n    return np.array(labels.values.tolist())\n\ndef integer_encoder(my_string, alphabet):\n    data = my_string.lower()\n    alphabet = alphabet.lower()\n    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n    \n    # integer encode input data\n    integer_encoded = [char_to_int[char] for char in data]\n        \n    return np.array(integer_encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def aug_data(df):\n    target_df = df.copy()\n    new_df = aug_df[aug_df['id'].isin(target_df['id'])]\n                         \n    del target_df['structure']\n    del target_df['predicted_loop_type']\n    new_df = new_df.merge(target_df, on=['id','sequence'], how='left')\n\n    df['cnt'] = df['id'].map(new_df[['id','cnt']].set_index('id').to_dict()['cnt'])\n    df['log_gamma'] = 100\n    df['score'] = 1.0\n    df = df.append(new_df[df.columns])\n    return df\ntrain = aug_data(train)\ntest = aug_data(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare features\ntrain_features = preprocess_features(preprocess_data(train))\n\n# Prepare labels data\ntrain_labels = preprocess_labels(preprocess_data(train)).transpose(0,2,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape, train_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_structures(features: np.ndarray, labels: np.ndarray):\n    fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n    axes[0].imshow(features.T)\n    axes[0].set_title(\"Features\")\n    axes[1].imshow(labels.T)\n    axes[1].set_title(\"Labels\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Examine features\ntrain['sequence'][2], train['structure'][2] ,train['predicted_loop_type'][2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature = train_features[2:3,:,:]\nfeature = feature[0,:,:]\nlabels = train_labels[2:3,:,:]\nlabels = labels[0,:,:]\n\nplot_structures(feature, labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing ML libraries\nimport keras\nimport keras.backend as K\nimport tensorflow as tf\n\nimport tensorflow.keras.layers as L\nfrom keras.models import Sequential\nfrom keras.layers import Dropout, Embedding, LSTM, Dense, Bidirectional, Activation, Flatten, GRU\nfrom keras.layers import BatchNormalization, SpatialDropout1D, InputLayer, Reshape, Lambda\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.optimizers import SGD, Adam, Adadelta, RMSprop\nfrom keras.layers.convolutional import Convolution1D, MaxPooling1D\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loss function\ndef MCRMSE(y_true, y_pred):\n    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TPU:\n    # detect and init the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    \n    # instantiate a distribution strategy\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build model functions\ndef gru_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.GRU(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer = 'orthogonal'))\n\ndef lstm_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer = 'orthogonal'))\n\ndef conv1d_layer(filters, kernel_size):\n    return L.Conv1D(filters=filters, kernel_size=kernel_size, padding='valid')\n\ndef build_model(seq_len=107, pred_len=68, dropout1=0.0, dropout2=0.3, embed_dim=230, hidden_dim1=220, hidden_dim2=330, \n                type=0, filters=255, kernel_size=5):\n\n    lr = 0.0010953574938066576\n    \n    inputs = L.Input(shape=(seq_len, train_features.shape[2]))\n   \n    # split integer and float features and concatenate them later.\n    integer_fea_seq = inputs[:, :, :1]\n    embed_seq = L.Embedding(input_dim=len(alphabet_rna)+1, output_dim=embed_dim)(integer_fea_seq)\n    reshaped_seq = tf.reshape(embed_seq, shape=(-1, embed_seq.shape[1],  embed_seq.shape[2] * embed_seq.shape[3]))\n\n    integer_fea_struc = inputs[:, :, 1:2]\n    embed_struc = L.Embedding(input_dim=len(alphabet_struc)+1, output_dim=embed_dim)(integer_fea_struc)\n    reshaped_struc = tf.reshape(embed_struc, shape=(-1, embed_struc.shape[1],  embed_struc.shape[2] * embed_struc.shape[3]))\n    \n    integer_fea_loop = inputs[:, :, 2:3]\n    embed_loop = L.Embedding(input_dim=len(alphabet_loop)+1, output_dim=embed_dim)(integer_fea_loop)\n    reshaped_loop = tf.reshape(embed_loop, shape=(-1, embed_loop.shape[1],  embed_loop.shape[2] * embed_loop.shape[3]))\n    \n    float_fea = inputs[:, :, 3:]\n    concat = L.concatenate([reshaped_seq, reshaped_struc, reshaped_loop, float_fea], axis=2)\n    \n    if type == 0:\n        hidden = lstm_layer(hidden_dim1, dropout1)(concat)\n        hidden = gru_layer(hidden_dim2, dropout2)(hidden)\n    elif type == 1:\n        hidden = gru_layer(hidden_dim1, dropout1)(concat)\n        hidden = gru_layer(hidden_dim2, dropout2)(hidden)    \n    elif type == 2:\n        hidden = gru_layer(hidden_dim1, dropout1)(concat)\n        hidden = lstm_layer(hidden_dim2, dropout2)(hidden)\n    elif type == 3:\n        hidden = lstm_layer(hidden_dim1, dropout1)(concat)\n        hidden = lstm_layer(hidden_dim2, dropout2)(hidden)\n    \n    truncated = hidden[:, :pred_len]\n    out = L.Dense(5, activation='linear')(truncated)\n    model = tf.keras.Model(inputs=inputs, outputs=out)\n    model.compile(tf.keras.optimizers.Adam(lr=lr), loss=MCRMSE)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if debug:\n    train = train[:20]\n    test = test[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if debug:\n    n_clusters = 20\nelse:\n    n_clusters = 200\n\nkmeans_model = KMeans(n_clusters=n_clusters, random_state=110).fit(preprocess_features(train)[:,:,2]) #clustering with loop\ntrain['cluster_id'] = kmeans_model.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64\n    \ndef train_and_predict(type = 0, FOLD_N = 5, Ver=1):\n    \n    if debug:\n        FOLD_N = 2\n    \n    gkf = GroupKFold(n_splits=FOLD_N)\n\n    test_107 = test.query(\"seq_length == 107\").copy()\n    test_130  = test.query(\"seq_length == 130\").copy()\n    \n    inputs_107 = preprocess_features(test_107)\n    inputs_130 = preprocess_features(test_130)\n    \n    holdouts = []\n    holdout_preds = []\n\n    for cv, (train_index, test_index) in enumerate(gkf.split(train,  train['deg_Mg_pH10'], train['cluster_id'])):\n        \n        trn = train.iloc[train_index].copy()\n        X_train = preprocess_features(trn)\n        y_train = np.array(trn[target_cols].values.tolist()).transpose((0, 2, 1))\n\n        val = train.iloc[test_index].copy()\n        x_val_all = preprocess_features(val)\n        val = val[val.SN_filter == 1]\n        X_test = preprocess_features(val)\n        y_test = np.array(val[target_cols].values.tolist()).transpose((0, 2, 1))\n        sample_weight = np.log(trn.signal_to_noise+1.1)*2 \n        \n        if TPU:\n            with tpu_strategy.scope():\n                model = build_model(type=type)\n                model_107 = build_model(seq_len=107, pred_len=107,type=type)\n                model_130 = build_model(seq_len=130, pred_len=130,type=type)\n        else:\n            model = build_model(type=type)\n            model_107 = build_model(seq_len=107, pred_len=107,type=type)\n            model_130 = build_model(seq_len=130, pred_len=130,type=type)\n        \n        history = model.fit(\n            X_train, y_train,\n            validation_data = (X_test, y_test),\n            batch_size=batch_size,\n            epochs=105,\n            sample_weight=sample_weight,\n            callbacks=[\n                tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=7, restore_best_weights=True),\n                tf.keras.callbacks.ReduceLROnPlateau(),\n                tf.keras.callbacks.ModelCheckpoint(f'model{Ver}_cv{cv}.h5', save_weights_only=True)\n            ]\n        )\n        \n        model.load_weights(f'model{Ver}_cv{cv}.h5')\n        model_107.load_weights(f'model{Ver}_cv{cv}.h5')\n        model_130.load_weights(f'model{Ver}_cv{cv}.h5')\n        \n        holdouts.append(train.iloc[test_index].copy())\n        holdout_preds.append(model.predict(x_val_all))\n        \n        if cv == 0:\n            preds_107 = model_107.predict(inputs_107)/FOLD_N\n            preds_130 = model_130.predict(inputs_130)/FOLD_N\n        else:\n            preds_107 += model_107.predict(inputs_107)/FOLD_N\n            preds_130 += model_130.predict(inputs_130)/FOLD_N\n    return holdouts, holdout_preds, test_107, preds_107, test_130, preds_130\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_df, val_preds, test_df, test_preds = [], [], [], []\n\nn_model = 4\n\nfor i in range(n_model):\n    holdouts, holdout_preds, test_107, preds_107, test_130, preds_130 = train_and_predict(i)\n    val_df += holdouts\n    val_preds += holdout_preds\n    test_df.append(test_107)\n    test_df.append(test_130)\n    test_preds.append(preds_107)\n    test_preds.append(preds_130)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_ls = []\nfor df, preds in zip(test_df, test_preds):\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n        single_df = pd.DataFrame(single_pred, columns=target_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n        preds_ls.append(single_df)\npreds_df = pd.concat(preds_ls).groupby('id_seqpos').mean().reset_index()\n# .mean() is for\n# 1, Predictions from multiple models\n# 2, TTA (augmented test data)\n\npreds_ls = []\nfor df, preds in zip(val_df, val_preds):\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n        single_df = pd.DataFrame(single_pred, columns=target_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n        single_df['SN_filter'] = df[df['id'] == uid].SN_filter.values[0]\n        preds_ls.append(single_df)\nholdouts_df = pd.concat(preds_ls).groupby('id_seqpos').mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = preds_df[['id_seqpos', 'reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']]\nsubmission.to_csv(f'submission.csv', index=False)\nprint(f'wrote to submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_mse(prd):\n    val = pd.read_json('../input/stanford-covid-vaccine/train.json', lines=True)\n\n    val_data = []\n    for mol_id in val['id'].unique():\n        sample_data = val.loc[val['id'] == mol_id]\n        sample_seq_length = sample_data.seq_length.values[0]\n        for i in range(68):\n            sample_dict = {\n                           'id_seqpos' : sample_data['id'].values[0] + '_' + str(i),\n                           'reactivity_gt' : sample_data['reactivity'].values[0][i],\n                           'deg_Mg_pH10_gt' : sample_data['deg_Mg_pH10'].values[0][i],\n                           'deg_Mg_50C_gt' : sample_data['deg_Mg_50C'].values[0][i],\n                           }\n            val_data.append(sample_dict)\n    val_data = pd.DataFrame(val_data)\n    val_data = val_data.merge(prd, on='id_seqpos')\n\n    rmses = []\n    mses = []\n    for col in ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']:\n        rmse = ((val_data[col] - val_data[col+'_gt']) ** 2).mean() ** .5\n        mse = ((val_data[col] - val_data[col+'_gt']) ** 2).mean()\n        rmses.append(rmse)\n        mses.append(mse)\n        print(col, rmse, mse)\n    print(np.mean(rmses), np.mean(mses))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_mse(holdouts_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_mse(holdouts_df[holdouts_df.SN_filter == 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}