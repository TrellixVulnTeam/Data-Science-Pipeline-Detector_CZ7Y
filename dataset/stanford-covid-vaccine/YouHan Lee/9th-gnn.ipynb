{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# #!/usr/bin/env python\n# # coding: utf-8\n\n# # In[1]:\n\n\n# import os\n# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n# import math\n# import torch.nn as nn\n# # fix seed ----------------------------------------------------------------------\n# import torch\n# import numpy as np\n# import random\n# import copy\n# from tqdm.notebook import tqdm\n\n# NAME = 'final_gnn_genconv_815'\n\n# seed = 815\n# def seed_torch(seed):\n#     torch.manual_seed(seed)\n#     torch.cuda.manual_seed_all(seed)\n#     return seed\n\n# def seed_py(seed):\n#     random.seed(seed)\n#     np.random.seed(seed)\n#     return seed\n\n# seed_torch(seed)\n# seed_py(seed)\n# torch.backends.cudnn.deterministic = True\n# torch.backends.cudnn.benchmark = False\n\n# print('!!!!! seed=%d'%seed)\n# #---------------------------------------------------------------------------------\n# ## https://www.kaggle.com/symyksr/openvaccine-deepergcn #########################\n\n\n# from torch.nn import Linear, LayerNorm, ReLU, Dropout\n# from torch_geometric.nn import ChebConv, NNConv, DeepGCNLayer, ARMAConv, ClusterGCNConv, GENConv\n# from torch_geometric.data import Data, DataLoader\n# from sklearn.model_selection import StratifiedKFold\n\n# import os\n# import pandas as pd\n# #---------------------------------------------------------------------------------\n\n# data_dir = '/kaggle/input/stanford-covid-vaccine/'\n# train_file = data_dir+'/train.json'\n# test_file  = data_dir+'/test.json'\n# bpps_top   = data_dir+'/bpps'\n\n\n# # settings\n\n# train_with_noisy_data     = True\n# add_edge_for_paired_nodes = True\n# add_codon_nodes           = True\n\n# bpps_nb_mean = 0.077522 # mean of bpps_nb across all training data\n# bpps_nb_std  = 0.08914   # std of bpps_nb across all training data\n# error_mean_limit = 0.5\n\n# nb_fold    = 5\n# device     = 'cuda'\n# batch_size = 16\n# epochs     = 200\n# lr         = 0.001\n# T = 5\n# node_hidden_channels = 144\n# edge_hidden_channels = 144\n# hidden_channels3 = 144\n# num_layers = 10\n# dropout1 = 0.1\n# dropout2 = 0.1\n# dropout3 = 0.1\n\n# ##################### all data preparation ####################################\n\n# def match_pair(structure):\n#     pair = [-1] * len(structure)\n#     pair_no = -1\n\n#     pair_no_stack = []\n#     for i, c in enumerate(structure):\n#         if c == '(':\n#             pair_no += 1\n#             pair[i] = pair_no\n#             pair_no_stack.append(pair_no)\n#         elif c == ')':\n#             pair[i] = pair_no_stack.pop()\n#     return pair\n\n# class MyData(Data):\n#     def __init__(self, x=None, edge_index=None, edge_attr=None, y=None,\n#                  pos=None, norm=None, face=None, weight=None, **kwargs):\n#         super(MyData, self).__init__(x=x, edge_index=edge_index,\n#                                      edge_attr=edge_attr, y=y, pos=pos,\n#                                      norm=norm, face=face, **kwargs)\n#         self.weight = weight\n\n\n# def calc_error_mean(row):\n#     reactivity_error = row['reactivity_error']\n#     deg_error_Mg_pH10 = row['deg_error_Mg_pH10']\n#     deg_error_Mg_50C = row['deg_error_Mg_50C']\n\n#     return np.mean(np.abs(reactivity_error) +\n#                    np.abs(deg_error_Mg_pH10) + \\\n#                    np.abs(deg_error_Mg_50C)) / 3\n\n\n# def calc_sample_weight(row):\n#     if sample_is_clean(row):\n#         return 1.\n#     else:\n#         error_mean = calc_error_mean(row)\n#         if error_mean >= error_mean_limit:\n#             return 0.\n\n#         return 1. - error_mean / error_mean_limit\n\n\n# # add directed edge for node1 -> node2 and for node2 -> node1\n# def add_edges(edge_index, edge_features, node1, node2, feature1, feature2):\n#     edge_index.append([node1, node2])\n#     edge_features.append(feature1)\n#     edge_index.append([node2, node1])\n#     edge_features.append(feature2)\n\n\n# def add_edges_between_base_nodes(edge_index, edge_features, node1, node2):\n#     edge_feature1 = [\n#         0, # is edge for paired nodes\n#         0, # is edge between codon node and base node\n#         0, # is edge between coden nodes\n#         1, # forward edge: 1, backward edge: -1\n#         1, # bpps if edge is for paired nodes\n#     ]\n#     edge_feature2 = [\n#         0, # is edge for paired nodes\n#         0, # is edge between codon node and base node\n#         0, # is edge between coden nodes\n#         -1, # forward edge: 1, backward edge: -1\n#         1, # bpps if edge is for paired nodes\n#     ]\n#     add_edges(edge_index, edge_features, node1, node2,\n#               edge_feature1, edge_feature2)\n\n# def add_edges_between_paired_nodes(edge_index, edge_features, node1, node2,\n#                                    bpps_value):\n#     edge_feature1 = [\n#         1, # is edge for paired nodes\n#         0, # is edge between codon node and base node\n#         0, # is edge between coden nodes\n#         0, # forward edge: 1, backward edge: -1\n#         bpps_value, # bpps if edge is for paired nodes\n#     ]\n#     edge_feature2 = [\n#         1, # is edge for paired nodes\n#         0, # is edge between codon node and base node\n#         0, # is edge between coden nodes\n#         0, # forward edge: 1, backward edge: -1\n#         bpps_value, # bpps if edge is for paired nodes\n#     ]\n#     add_edges(edge_index, edge_features, node1, node2,\n#               edge_feature1, edge_feature2)\n\n# def add_edges_between_codon_nodes(edge_index, edge_features, node1, node2):\n#     edge_feature1 = [\n#         0, # is edge for paired nodes\n#         0, # is edge between codon node and base node\n#         1, # is edge between coden nodes\n#         1, # forward edge: 1, backward edge: -1\n#         0, # bpps if edge is for paired nodes\n#     ]\n#     edge_feature2 = [\n#         0, # is edge for paired nodes\n#         0, # is edge between codon node and base node\n#         1, # is edge between coden nodes\n#         -1, # forward edge: 1, backward edge: -1\n#         0, # bpps if edge is for paired nodes\n#     ]\n#     add_edges(edge_index, edge_features, node1, node2,\n#               edge_feature1, edge_feature2)\n\n# def add_edges_between_codon_and_base_node(edge_index, edge_features,\n#                                           node1, node2):\n#     edge_feature1 = [\n#         0, # is edge for paired nodes\n#         1, # is edge between codon node and base node\n#         0, # is edge between coden nodes\n#         0, # forward edge: 1, backward edge: -1\n#         0, # bpps if edge is for paired nodes\n#     ]\n#     edge_feature2 = [\n#         0, # is edge for paired nodes\n#         1, # is edge between codon node and base node\n#         0, # is edge between coden nodes\n#         0, # forward edge: 1, backward edge: -1\n#         0, # bpps if edge is for paired nodes\n#     ]\n#     add_edges(edge_index, edge_features, node1, node2,\n#               edge_feature1, edge_feature2)\n\n# def add_node(node_features, feature):\n#     node_features.append(feature)\n\n\n# def add_base_node(node_features, sequence, predicted_loop_type,\n#                   bpps_sum, bpps_nb):\n#     feature = [\n#         0, # is codon node\n#         sequence == 'A',\n#         sequence == 'C',\n#         sequence == 'G',\n#         sequence == 'U',\n#         predicted_loop_type == 'S',\n#         predicted_loop_type == 'M',\n#         predicted_loop_type == 'I',\n#         predicted_loop_type == 'B',\n#         predicted_loop_type == 'H',\n#         predicted_loop_type == 'E',\n#         predicted_loop_type == 'X',\n#         bpps_sum,\n#         bpps_nb,\n#     ]\n#     add_node(node_features, feature)\n\n# def add_codon_node(node_features):\n#     feature = [\n#         1, # is codon node\n#         0, # sequence == 'A',\n#         0, # sequence == 'C',\n#         0, # sequence == 'G',\n#         0, # sequence == 'U',\n#         0, # predicted_loop_type == 'S',\n#         0, # predicted_loop_type == 'M',\n#         0, # predicted_loop_type == 'I',\n#         0, # predicted_loop_type == 'B',\n#         0, # predicted_loop_type == 'H',\n#         0, # predicted_loop_type == 'E',\n#         0, # predicted_loop_type == 'X',\n#         0, # bpps_sum\n#         0, # bpps_nb\n#     ]\n#     add_node(node_features, feature)\n\n# def build_data(df, is_train):\n#     data = []\n#     for i in range(len(df)):\n#         targets = []\n#         node_features = []\n#         edge_features = []\n#         edge_index = []\n#         train_mask = []\n#         test_mask = []\n#         weights = []\n\n#         id = df.loc[i, 'id']\n#         path = os.path.join(bpps_top, id + '.npy')\n#         bpps = np.load(path)\n#         bpps_sum = bpps.sum(axis=0)\n#         sequence = df.loc[i, 'sequence']\n#         structure = df.loc[i, 'structure']\n#         pair_info = match_pair(structure)\n#         predicted_loop_type = df.loc[i, 'predicted_loop_type']\n#         seq_length = df.loc[i, 'seq_length']\n#         seq_scored = df.loc[i, 'seq_scored']\n#         bpps_nb = (bpps > 0).sum(axis=0) / seq_length\n#         bpps_nb = (bpps_nb - bpps_nb_mean) / bpps_nb_std\n#         if is_train:\n#             sample_weight = calc_sample_weight(df.loc[i])\n\n#             reactivity = df.loc[i, 'reactivity']\n#             deg_Mg_pH10 = df.loc[i, 'deg_Mg_pH10']\n#             deg_Mg_50C = df.loc[i, 'deg_Mg_50C']\n\n#             for j in range(seq_length):\n#                 if j < seq_scored:\n#                     targets.append([\n#                         reactivity[j],\n#                         deg_Mg_pH10[j],\n#                         deg_Mg_50C[j],\n#                         ])\n#                 else:\n#                     targets.append([0, 0, 0])\n\n#         paired_nodes = {}\n#         for j in range(seq_length):\n#             add_base_node(node_features, sequence[j], predicted_loop_type[j],\n#                           bpps_sum[j], bpps_nb[j])\n\n#             if j + 1 < seq_length: # edge between current node and next node\n#                 add_edges_between_base_nodes(edge_index, edge_features,\n#                                              j, j + 1)\n\n#             if pair_info[j] != -1:\n#                 if pair_info[j] not in paired_nodes:\n#                     paired_nodes[pair_info[j]] = [j]\n#                 else:\n#                     paired_nodes[pair_info[j]].append(j)\n\n#             train_mask.append(j < seq_scored)\n#             test_mask.append(True)\n#             if is_train:\n#                 weights.append(sample_weight)\n\n\n#         if add_edge_for_paired_nodes:\n#             for pair in paired_nodes.values():\n#                 bpps_value = bpps[pair[0], pair[1]]\n#                 add_edges_between_paired_nodes(edge_index, edge_features,\n#                                                pair[0], pair[1], bpps_value)\n\n\n\n#         if add_codon_nodes:\n#             codon_node_idx = seq_length - 1\n#             for j in range(seq_length):\n#                 if j % 3 == 0:\n#                     # add codon node\n#                     add_codon_node(node_features)\n#                     codon_node_idx += 1\n#                     train_mask.append(False)\n#                     test_mask.append(False)\n#                     if is_train:\n#                         weights.append(0)\n#                         targets.append([0, 0, 0])\n\n#                     if codon_node_idx > seq_length:\n#                         # add edges between adjacent codon nodes\n#                         add_edges_between_codon_nodes(edge_index, edge_features,\n#                                                       codon_node_idx - 1,\n#                                                       codon_node_idx)\n\n#                 # add edges between codon node and base node\n#                 add_edges_between_codon_and_base_node(edge_index, edge_features,\n#                                                       j, codon_node_idx)\n\n#         node_features = torch.tensor(node_features, dtype=torch.float)\n#         edge_index    = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n#         edge_features = torch.tensor(edge_features, dtype=torch.float)\n\n#         if is_train:\n#             data.append(MyData(x=node_features, edge_index=edge_index,\n#                                edge_attr=edge_features,\n#                                train_mask=torch.tensor(train_mask),\n#                                weight=torch.tensor(weights, dtype=torch.float),\n#                                y=torch.tensor(targets, dtype=torch.float)))\n#         else:\n#             data.append(MyData(x=node_features, edge_index=edge_index,\n#                                edge_attr=edge_features,\n#                                test_mask=torch.tensor(test_mask)))\n\n#     return data\n\n# #---------------------------------------------------------------------------------\n# def build_id_seqpos(df):\n#     id_seqpos = []\n#     for i in range(len(df)):\n#         id = df.loc[i, 'id']\n#         seq_length = df.loc[i, 'seq_length']\n#         for seqpos in range(seq_length):\n#             id_seqpos.append(id + '_' + str(seqpos))\n#     return id_seqpos\n\n# def sample_is_clean(row):\n#     return row['SN_filter'] == 1\n#     #return row['signal_to_noise'] > 1 and \\\n#     #       min((min(row['reactivity']),\n#     #            min(row['deg_Mg_pH10']),\n#     #            min(row['deg_pH10']),\n#     #            min(row['deg_Mg_50C']),\n#     #            min(row['deg_50C']))) > -0.5\n\n# # categorical value for target (used for stratified kfold)\n# def add_y_cat(df):\n#     target_mean = df['reactivity'].apply(np.mean) +                   df['deg_Mg_pH10'].apply(np.mean) +                   df['deg_Mg_50C'].apply(np.mean)\n#     df['y_cat'] = pd.qcut(np.array(target_mean), q=20).codes\n\n# ##################### all model preparation ####################################\n\n# # originally copied from\n# # https://github.com/rusty1s/pytorch_geometric/blob/master/examples/ogbn_proteins_deepgcn.py\n# #\n# class MapE2NxN(torch.nn.Module):\n#     def __init__(self, in_channels, out_channels, hidden_channels):\n#         super(MapE2NxN, self).__init__()\n#         self.linear1 = Linear(in_channels, hidden_channels)\n#         self.linear2 = Linear(hidden_channels, out_channels)\n#         self.dropout = Dropout(dropout3)\n\n#     def forward(self, x):\n#         x = self.linear1(x)\n#         x = self.dropout(x)\n#         x = self.linear2(x)\n#         return x\n\n\n# class PositionEncode(nn.Module):\n#     def __init__(self, dim, length=174):\n#         super(PositionEncode, self).__init__()\n#         position = torch.zeros(length,dim)\n#         p = torch.arange(0, length, dtype=torch.float).unsqueeze(1)\n#         div = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n#         position[:,0::2] = torch.sin(p * div)\n#         position[:,1::2] = torch.cos(p * div)\n#         #position = position.transpose(0, 1).reshape(1,dim,length) #.contiguous()\n#         position = position.reshape(length, 1, dim) #.contiguous()\n#         self.register_buffer('position', position)\n\n#         #self.position = nn.Parameter( torch.randn(1, dim, length) ) #random\n\n#     def forward(self, x):\n#         length, batch_size, _ = x.shape\n#         position = self.position.repeat(1, batch_size, 1)\n#         position = position[:length, :, :, ].contiguous()\n#         return position\n\n\n# class MyDeeperGCN(torch.nn.Module):\n#     def __init__(self, num_node_features, num_edge_features,\n#                  node_hidden_channels,\n#                  edge_hidden_channels,\n#                  num_layers, num_classes, seq_length = 143):\n#         super(MyDeeperGCN, self).__init__()\n\n#         self.node_encoder = GENConv(num_node_features, node_hidden_channels)\n#         self.edge_encoder = Linear(num_edge_features, edge_hidden_channels)\n\n#         self.layers = torch.nn.ModuleList()\n#         for i in range(1, num_layers + 1):\n#             conv = NNConv(node_hidden_channels, node_hidden_channels,\n#                           MapE2NxN(edge_hidden_channels,\n#                                    node_hidden_channels * node_hidden_channels,\n#                                    hidden_channels3))\n#             norm = LayerNorm(node_hidden_channels, elementwise_affine=True)\n#             act = ReLU(inplace=True)\n\n#             layer = DeepGCNLayer(conv, norm, act, block='res+',\n#                                  dropout=dropout1, ckpt_grad=i % 3)\n#             self.layers.append(layer)\n\n#         self.postition = PositionEncode(node_hidden_channels)\n#         self.transformer = torch.nn.TransformerEncoder(\n#             torch.nn.TransformerEncoderLayer(node_hidden_channels, 8, 256, dropout=0.2, activation='relu'),\n#             2\n#         )\n\n#         self.lin = Linear(node_hidden_channels, num_classes)\n#         self.dropout = Dropout(dropout2)\n#         self.seq_length = seq_length\n\n#     def forward(self, data):\n#         batch_size = data.x.shape[0]//self.seq_length ##hard code\n\n\n#         x = data.x\n#         edge_index = data.edge_index\n#         edge_attr = data.edge_attr\n\n#         # edge for paired nodes are excluded for encoding node\n#         seq_edge_index = edge_index[:, edge_attr[:,0] == 0]\n#         x = self.node_encoder(x, seq_edge_index)\n\n#         edge_attr = self.edge_encoder(edge_attr)\n\n#         x = self.layers[0].conv(x, edge_index, edge_attr)\n\n#         for layer in self.layers[1:]:\n#             x = layer(x, edge_index, edge_attr)\n\n#         x = self.layers[0].act(self.layers[0].norm(x))\n#         x = self.dropout(x)\n\n#         #----\n#         #hard code\n#         x = x.reshape(batch_size, -1, node_hidden_channels)\n#         x = x.permute(1,0,2).contiguous() #length, batch_size, 128\n        \n#         if 1:\n#             pos = self.postition(x)\n#             x = self.transformer(x+pos)\n\n#         #----\n#         predict = self.lin(x)\n#         predict = predict.permute(1,0,2).contiguous()\n#         predict = predict.reshape(-1, 3)  #to keep compatible with rest of code\n#         return predict\n\n# def weighted_mse_loss(prds, tgts, weight):\n#     return torch.mean(weight * (prds - tgts)**2)\n\n# def criterion(prds, tgts, weight=None):\n#     if weight is None:\n#         return (torch.sqrt(torch.nn.MSELoss()(prds[:,0], tgts[:,0])) +\n#                 torch.sqrt(torch.nn.MSELoss()(prds[:,1], tgts[:,1])) +\n#                 torch.sqrt(torch.nn.MSELoss()(prds[:,2], tgts[:,2]))) / 3\n#     else:\n#         return (torch.sqrt(weighted_mse_loss(prds[:,0], tgts[:,0], weight)) +\n#                 torch.sqrt(weighted_mse_loss(prds[:,1], tgts[:,1], weight)) +\n#                 torch.sqrt(weighted_mse_loss(prds[:,2], tgts[:,2], weight))) / 3\n\n\n# # In[ ]:\n\n\n\n\n\n# # In[2]:\n\n\n# print('Reading', train_file)\n# df_tr = pd.read_json(train_file, lines=True)\n# add_y_cat(df_tr)\n\n# is_clean = df_tr.apply(sample_is_clean, axis=1)\n# df_clean = df_tr[is_clean].reset_index(drop=True)\n# df_noisy = df_tr[is_clean==False].reset_index(drop=True)\n# del df_tr\n\n# #------------------------------------------------------------\n# print('Training')\n# all_ys = torch.zeros((0, 3)).to(device).detach()\n# all_outs = torch.zeros((0, 3)).to(device).detach()\n# best_model_states = []\n# kf = StratifiedKFold(nb_fold, shuffle=True, random_state=seed)\n# for fold, ((clean_train_idx, clean_valid_idx),\n#            (noisy_train_idx, noisy_valid_idx)) \\\n#                in enumerate(zip(kf.split(df_clean, df_clean['y_cat']),\n#                                 kf.split(df_noisy, df_noisy['y_cat']))):\n#     print('Fold', fold)\n#     #epochs = 5\n#     #start_timer = timer()\n\n#     out_dir = f'./{NAME}/sn1-fold-{fold}'#%fold\n#     initial_checkpoint =         None #out_dir + '/checkpoint/00007200_model.pth' #\n\n\n#     ## setup  ----------------------------------------\n#     for f in ['checkpoint',] : os.makedirs(out_dir +'/'+f, exist_ok=True)\n# #     log = Logger()\n# #     log.open(out_dir+'/log.train.txt',mode='a')\n# #     log.write('\\n--- [START %s] %s\\n\\n' % (IDENTIFIER, '-' * 64))\n# #     log.write('\\t%s\\n' % COMMON_STRING)\n# #     log.write('\\t__file__ = %s\\n' % __file__)\n# #     log.write('\\tout_dir  = %s\\n' % out_dir)\n# #     log.write('\\n')\n\n\n\n#     #----------------------------------------------------------------------------\n#     # build train data\n#     df_train = df_clean.loc[clean_train_idx]\n#     if train_with_noisy_data:\n#         df_train_noisy = df_noisy.loc[noisy_train_idx]\n#         df_train_noisy =            df_train_noisy[df_train_noisy.apply(calc_error_mean, axis=1) <=                           error_mean_limit]\n#         df_train = df_train.append(df_train_noisy)\n#     data_train = build_data(df_train.reset_index(drop=True), True)\n#     del df_train\n#     loader_train = DataLoader(data_train, batch_size=batch_size,\n#                               shuffle=True)\n\n#     # build validation data\n#     df_valid_clean = df_clean.loc[clean_valid_idx].reset_index(drop=True)\n#     data_valid_clean = build_data(df_valid_clean, True)\n#     del df_valid_clean\n#     loader_valid_clean = DataLoader(data_valid_clean, batch_size=batch_size,\n#                                     shuffle=False)\n\n#     model = MyDeeperGCN(data_train[0].num_node_features,\n#                         data_train[0].num_edge_features,\n#                         node_hidden_channels=node_hidden_channels,\n#                         edge_hidden_channels=edge_hidden_channels,\n#                         num_layers=num_layers,\n#                         num_classes=3, seq_length=143).to(device)\n\n\n\n#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=True)\n#     best_mcrmse = np.inf\n#     for epoch in tqdm(range(epochs)):\n#         #print('Epoch', epoch)\n#         model.train()\n#         train_loss = 0.0\n#         nb = 0\n#         for data in (loader_train):\n#             data = data.to(device)\n#             mask = data.train_mask\n#             weight = data.weight[mask]\n\n#             optimizer.zero_grad()\n#             out = model(data)[mask]\n#             y = data.y[mask]\n#             loss = criterion(out, y, weight)\n#             loss.backward()\n#             optimizer.step()\n#             train_loss += loss.item() * y.size(0)\n#             nb += y.size(0)\n\n#             del data\n#             del out\n#             del y\n#             del loss\n#             #gc.collect()\n#             #torch.cuda.empty_cache()\n#         train_loss /= nb\n\n#         model.eval()\n#         valid_loss = 0.0\n#         nb = 0\n#         ys = torch.zeros((0, 3)).to(device).detach()\n#         outs = torch.zeros((0, 3)).to(device).detach()\n#         for data in (loader_valid_clean):\n#             data = data.to(device)\n#             mask = data.train_mask\n\n#             out = model(data)[mask].detach()\n#             y = data.y[mask].detach()\n#             loss = criterion(out, y).detach()\n#             valid_loss += loss.item() * y.size(0)\n#             nb += y.size(0)\n\n#             outs = torch.cat((outs, out), dim=0)\n#             ys = torch.cat((ys, y), dim=0)\n\n#             del data\n#             del out\n#             del y\n#             del loss\n#             #gc.collect()\n#             #torch.cuda.empty_cache()\n#         valid_loss /= nb\n\n#         mcrmse = criterion(outs, ys).item()\n#         print('Epoch %3d |  T Loss: %0.5f  |  V Loss: %0.5f  |  V MCRMSE: %0.5f | %s \\n'%(\n#             epoch, train_loss, valid_loss, mcrmse, 'min'))\n        \n#         scheduler.step(mcrmse)\n        \n#         if mcrmse < best_mcrmse:\n#             #print('Best valid MCRMSE updated to', mcrmse)\n#             best_mcrmse = mcrmse\n#             best_model_state = copy.deepcopy(model.state_dict())\n\n#         # --------------------------------------------------\n#         if int(epoch)%5 ==0:\n#             torch.save({'state_dict': model.state_dict(),}, out_dir + '/checkpoint/%08d_model.pth' % (int(epoch)))\n#     print('#'*20, best_mcrmse)\n#     del data_train\n#     del data_valid_clean\n#     #gc.collect()\n#     #torch.cuda.empty_cache()\n\n\n\n#     #--------------------------------------------------\n#     best_model_states.append(best_model_state)\n\n#     # predict for CV\n#     model.load_state_dict(best_model_state)\n#     model.eval()\n#     for data in loader_valid_clean:\n#         data = data.to(device)\n#         mask = data.train_mask\n\n#         out = model(data)[mask].detach()\n#         y = data.y[mask].detach()\n\n#         all_ys = torch.cat((all_ys, y), dim=0)\n#         all_outs = torch.cat((all_outs, out), dim=0)\n\n#         del data\n#         del out\n#         del y\n#         #gc.collect()\n#         #torch.cuda.empty_cache()\n\n# # calculate MCRMSE by all training data\n# print('CV MCRMSE ', criterion(all_outs, all_ys).item())\n# del all_outs\n# del all_ys\n# #gc.collect()\n# #torch.cuda.empty_cache()\n\n\n# # In[3]:\n\n\n# # predict for test data\n# print('Predicting test data')\n# print('Reading', test_file)\n# df_te = pd.read_json(test_file, lines=True)\n\n# df_te = df_te.query('seq_length==107').reset_index(drop=True)\n\n# data_test = build_data(df_te, False)\n# loader_test = DataLoader(data_test, batch_size=batch_size, shuffle=False)\n# id_seqpos = build_id_seqpos(df_te)\n\n# model = MyDeeperGCN(data_test[0].num_node_features,\n#                     data_test[0].num_edge_features,\n#                     node_hidden_channels=node_hidden_channels,\n#                     edge_hidden_channels=edge_hidden_channels,\n#                     num_layers=num_layers,\n#                     num_classes=3, seq_length=143).to(device)\n\n# preds = torch.zeros((len(id_seqpos), 3)).to(device).detach()\n# for best_model_state in best_model_states:\n#     model.load_state_dict(best_model_state)\n#     model.eval()\n\n#     outs = torch.zeros((0, 3)).to(device).detach()\n#     for data in loader_test:\n#         data = data.to(device)\n#         mask = data.test_mask\n\n#         out = model(data)[mask].detach()\n#         outs = torch.cat((outs, out), dim=0)\n\n#         del data\n#         del out\n#         #gc.collect()\n#         #torch.cuda.empty_cache()\n#     preds += outs\n# preds /= len(best_model_states)\n# preds = preds.cpu().numpy()\n\n# df_sub_pub = pd.DataFrame({'id_seqpos': id_seqpos,\n#                        'reactivity': preds[:,0],\n#                        'deg_Mg_pH10': preds[:,1],\n#                        'deg_pH10': 0,\n#                        'deg_Mg_50C': preds[:,2],\n#                        'deg_50C': 0})\n# print('Writing submission.csv')\n\n\n# # In[4]:\n\n\n# # predict for test data\n# print('Predicting test data')\n# print('Reading', test_file)\n# df_te = pd.read_json(test_file, lines=True)\n\n# df_te = df_te.query('seq_length==130').reset_index(drop=True)\n\n# data_test = build_data(df_te, False)\n# loader_test = DataLoader(data_test, batch_size=batch_size, shuffle=False)\n# id_seqpos = build_id_seqpos(df_te)\n\n# model = MyDeeperGCN(data_test[0].num_node_features,\n#                     data_test[0].num_edge_features,\n#                     node_hidden_channels=node_hidden_channels,\n#                     edge_hidden_channels=edge_hidden_channels,\n#                     num_layers=num_layers,\n#                     num_classes=3, seq_length=174).to(device)\n\n# preds = torch.zeros((len(id_seqpos), 3)).to(device).detach()\n# for best_model_state in best_model_states:\n#     model.load_state_dict(best_model_state)\n#     model.eval()\n\n#     outs = torch.zeros((0, 3)).to(device).detach()\n#     for data in loader_test:\n#         data = data.to(device)\n#         mask = data.test_mask\n\n#         out = model(data)[mask].detach()\n#         outs = torch.cat((outs, out), dim=0)\n\n#         del data\n#         del out\n#         #gc.collect()\n#         #torch.cuda.empty_cache()\n#     preds += outs\n# preds /= len(best_model_states)\n# preds = preds.cpu().numpy()\n\n# df_sub_pri = pd.DataFrame({'id_seqpos': id_seqpos,\n#                        'reactivity': preds[:,0],\n#                        'deg_Mg_pH10': preds[:,1],\n#                        'deg_pH10': 0,\n#                        'deg_Mg_50C': preds[:,2],\n#                        'deg_50C': 0})\n# print('Writing submission.csv')\n\n\n# # In[5]:\n\n\n# df_sub = pd.concat([df_sub_pub, df_sub_pri]).reset_index(drop=True).sort_values('id_seqpos').reset_index(drop=True) \n\n\n# # In[6]:\n\n\n# best = pd.read_csv('ensemble_v37.csv').reset_index(drop=True).sort_values('id_seqpos').reset_index(drop=True) \n\n# pred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n\n# print(np.corrcoef(df_sub[pred_cols[0]], best[pred_cols[0]]))\n# print(np.corrcoef(df_sub[pred_cols[1]], best[pred_cols[1]]))\n# print(np.corrcoef(df_sub[pred_cols[3]], best[pred_cols[3]]))\n\n# df_sub.to_csv(f'{NAME}.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}