{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import json\n# import tensorflow.keras.layers as L\n# import tensorflow as tf\n# from tensorflow.keras import backend as K\n\n# def attention(x_inner, x_outer, n_factor, dropout):\n#     x_Q =  L.Conv1D(n_factor, 1, activation='linear', \n#                   kernel_initializer='glorot_uniform',\n#                   bias_initializer='glorot_uniform',\n#                  )(x_inner)\n#     x_K =  L.Conv1D(n_factor, 1, activation='linear', \n#                   kernel_initializer='glorot_uniform',\n#                   bias_initializer='glorot_uniform',\n#                  )(x_outer)\n#     x_V =  L.Conv1D(n_factor, 1, activation='linear', \n#                   kernel_initializer='glorot_uniform',\n#                   bias_initializer='glorot_uniform',\n#                  )(x_outer)\n#     x_KT = L.Permute((2, 1))(x_K)\n#     res = L.Lambda(lambda c: K.batch_dot(c[0], c[1]) / np.sqrt(n_factor))([x_Q, x_KT])\n# #     res = tf.expand_dims(res, axis = 3)\n# #     res = L.Conv2D(16, 3, 1, padding = \"same\", activation = \"relu\")(res)\n# #     res = L.Conv2D(1, 3, 1, padding = \"same\", activation = \"relu\")(res)\n# #     res = tf.squeeze(res, axis = 3)\n#     att = L.Lambda(lambda c: K.softmax(c, axis=-1))(res)\n#     att = L.Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n#     return att\n\n# def multi_head_attention(x, y, n_factor, n_head, dropout):\n#     if n_head == 1:\n#         att = attention(x, y, n_factor, dropout)\n#     else:\n#         n_factor_head = n_factor // n_head\n#         heads = [attention(x, y, n_factor_head, dropout) for i in range(n_head)]\n#         att = L.Concatenate()(heads)\n#         att = L.Dense(n_factor, \n#                       kernel_initializer='glorot_uniform',\n#                       bias_initializer='glorot_uniform',\n#                      )(att)\n#     x = L.Add()([x, att])\n#     x = L.LayerNormalization()(x)\n#     if dropout > 0:\n#         x = L.Dropout(dropout)(x)\n#     return x\n\n# def res(x, unit, kernel = 3, rate = 0.1):\n#     h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n#     h = L.LayerNormalization()(h)\n#     h = L.LeakyReLU()(h)\n#     h = L.Dropout(rate)(h)\n#     return L.Add()([x, h])\n\n# def forward(x, unit, kernel = 3, rate = 0.1):\n# #     h = L.Dense(unit, None)(x)\n#     h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n#     h = L.LayerNormalization()(h)\n#     h = L.Dropout(rate)(h)\n# #         h = tf.keras.activations.swish(h)\n#     h = L.LeakyReLU()(h)\n#     h = res(h, unit, kernel, rate)\n#     return h\n\n# def adj_attn(x, adj, unit, n = 2, rate = 0.1):\n#     x_a = x\n#     x_as = []\n#     for i in range(n):\n#         x_a = forward(x_a, unit)\n#         x_a = tf.matmul(adj, x_a)\n#         x_as.append(x_a)\n#     if n == 1:\n#         x_a = x_as[0]\n#     else:\n#         x_a = L.Concatenate()(x_as)\n#     x_a = forward(x_a, unit)\n#     return x_a\n\n# import os\n# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n\n\n\n# NAME = 'tf_reverseTrain_transformer_convattention_morebpps_feat_convforward_adjAtt_nowave'\n\n# pred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n\n# def gru_layer(hidden_dim, dropout):\n#     return tf.keras.layers.Bidirectional(\n#                                 tf.keras.layers.GRU(hidden_dim,\n#                                 dropout=dropout,\n#                                 return_sequences=True,\n#                                 kernel_initializer = 'orthogonal'))\n\n# def lstm_layer(hidden_dim, dropout):\n#     return tf.keras.layers.Bidirectional(\n#                                 tf.keras.layers.LSTM(hidden_dim,\n#                                 dropout=dropout,\n#                                 return_sequences=True,\n#                                 kernel_initializer = 'orthogonal'))\n\n# def build_model(gru=False,seq_len=107, pred_len=68, dropout=0.25,\n#                 embed_dim=128, hidden_dim=128, reverse=False):\n    \n#     def wave_block(x, filters, kernel_size, n):\n#         dilation_rates = [2 ** i for i in range(n)]\n#         x = tf.keras.layers.Conv1D(filters = filters, \n#                                    kernel_size = 1,\n#                                    padding = 'same')(x)\n#         res_x = x\n#         for dilation_rate in dilation_rates:\n#             tanh_out = tf.keras.layers.Conv1D(filters = filters,\n#                               kernel_size = kernel_size,\n#                               padding = 'same', \n#                               activation = 'tanh', \n#                               dilation_rate = dilation_rate)(x)\n#             sigm_out = tf.keras.layers.Conv1D(filters = filters,\n#                               kernel_size = kernel_size,\n#                               padding = 'same',\n#                               activation = 'sigmoid', \n#                               dilation_rate = dilation_rate)(x)\n#             x = tf.keras.layers.Multiply()([tanh_out, sigm_out])\n#             x = tf.keras.layers.Conv1D(filters = filters,\n#                        kernel_size = 1,\n#                        padding = 'same')(x)\n#             res_x = tf.keras.layers.Add()([res_x, x])\n#         return res_x\n    \n#     inputs = tf.keras.layers.Input(shape=(seq_len, 3))\n#     inputs_bpps = tf.keras.layers.Input(shape=(seq_len, 6), name='input_bpps')\n#     inputs_nums = tf.keras.layers.Input(shape=(seq_len, 18), name='input_nums')\n#     adj = tf.keras.Input(shape = (None, None, train_As.shape[3]), name = \"adj\")\n    \n#     adj_learned = L.Dense(64, \"relu\")(adj)\n#     adj_learned = L.Dense(1, \"relu\")(adj_learned)\n    \n#     adj_all = L.Concatenate(axis = 3)([adj, adj_learned])\n    \n\n#     embed0 = tf.keras.layers.Embedding(input_dim=len(token2int0), output_dim=embed_dim)(inputs[:, :, 0])\n#     embed1 = tf.keras.layers.Embedding(input_dim=len(token2int1), output_dim=embed_dim)(inputs[:, :, 1])\n#     embed2 = tf.keras.layers.Embedding(input_dim=len(token2int2), output_dim=embed_dim)(inputs[:, :, 2])\n    \n    \n#     embed0 = tf.keras.layers.SpatialDropout1D(.2)(embed0)\n#     embed1 = tf.keras.layers.SpatialDropout1D(.2)(embed1)\n#     embed2 = tf.keras.layers.SpatialDropout1D(.2)(embed2)\n    \n#     embed = tf.concat([embed0, embed1, embed2], axis=2)\n    \n#     #reshaped = tf.reshape(\n#     #    embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n    \n#     embed = tf.keras.layers.SpatialDropout1D(.2)(embed)\n    \n#     bpps = tf.keras.layers.Dense(embed_dim, activation='relu', name='dense_bpps')(inputs_bpps)\n#     nums = tf.keras.layers.Dense(embed_dim, activation='relu', name='dense_nums')(inputs_nums)\n    \n#     print(nums.shape)\n#     embed = tf.concat([embed, bpps, nums], axis=2)\n#     embed = tf.keras.layers.Conv1D(filters = 256,\n#                               kernel_size = 1,\n#                               padding = 'same', \n#                               activation = 'relu')(embed)\n    \n#     transformer_block = TransformerBlock(256, 4, 256)\n#     embed = transformer_block(embed)\n    \n#     xs = []\n#     xs.append(embed)\n#     x2 = forward(embed, 64, kernel = 6, rate = 0.0)\n#     x3 = forward(x2, 32, kernel = 12, rate = 0.0)\n#     x4 = forward(x3, 16, kernel = 24, rate = 0.0)\n#     x5 = forward(x4, 8, kernel = 48, rate = 0.0)\n    \n#     x = L.Concatenate()([embed, x2, x3, x4, x5])\n    \n#     for unit in [64, 48, 32]:\n#         x_as = []\n#         for i in range(adj_all.shape[3]):\n#             x_a = adj_attn(x, adj_all[:, :, :, i], unit, rate = 0.0)\n#             x_as.append(x_a)\n#         x_c = forward(x, unit, kernel = 32)\n        \n#         x = L.Concatenate()(x_as + [x_c])\n#         x = forward(x, unit)\n#         x = multi_head_attention(x, x, unit, 4, 0.0)\n#         xs.append(x)\n        \n#     x = L.Concatenate()(xs)\n    \n    \n#     hidden = gru_layer(hidden_dim, dropout)(x)\n#     hidden = gru_layer(hidden_dim, dropout)(hidden)\n#     x = gru_layer(hidden_dim, dropout)(hidden)\n    \n#     x = forward(x, 256, kernel = 3, rate = 0.1)\n#     x = tf.keras.layers.BatchNormalization()(x)\n#     x = tf.keras.layers.Dropout(dropout)(x)\n    \n\n#     x = forward(x, 128, kernel = 3, rate = 0.1)\n#     x = tf.keras.layers.BatchNormalization()(x)\n#     x = tf.keras.layers.Dropout(dropout)(x)\n    \n#     x = forward(x, 128, kernel = 3, rate = 0.1)\n#     x = tf.keras.layers.BatchNormalization()(x)\n#     x = tf.keras.layers.Dropout(dropout)(x)\n    \n#     #only making predictions on the first part of each sequence\n#     if reverse:\n#         truncated = x[:, -pred_len:]\n#     else:\n#         truncated = x[:, :pred_len]\n    \n#     out1 = tf.keras.layers.Dense(5, activation='linear', name='out1')(truncated)\n#     out2 = tf.keras.layers.Dense(5, activation='linear', name='out2')(truncated)\n\n#     model = tf.keras.Model(inputs=[inputs, inputs_bpps, inputs_nums, adj], outputs=[out1, out2])\n\n#     #some optimizers\n#     adam = tf.optimizers.Adam()\n#     def MCRMSE(y_true, y_pred):\n#         colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n#         return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n    \n#     model.compile(optimizer = adam, loss={'out1': MCRMSE, 'out2': 'mae'}, loss_weights={'out1': 0.7, 'out2': 0.3})\n    \n#     return model\n\n# from tqdm import tqdm_notebook\n\n# def process_inputs_2(df):\n#     df1 = df.copy()\n#     df2 = df.copy()\n#     df3 = df.copy()\n#     df4 = df.copy()\n#     df5 = df.copy()\n#     from collections import Counter as count\n#     bases = []\n#     for j in range(len(df1)):\n#         counts = dict(count(df1.iloc[j]['sequence']))\n#         bases.append((\n#             counts['A'] / df1.iloc[j]['seq_length'],\n#             counts['G'] / df1.iloc[j]['seq_length'],\n#             counts['C'] / df1.iloc[j]['seq_length'],\n#             counts['U'] / df1.iloc[j]['seq_length']\n#         ))\n\n#     bases = pd.DataFrame(bases, columns=['A_percent', 'G_percent', 'C_percent', 'U_percent'])\n#     del df1\n    \n#     pairs = []\n#     all_partners = []\n#     for j in tqdm_notebook(range(len(df2))):\n#         partners = [-1 for i in range(130)]\n#         pairs_dict = {}\n#         queue = []\n#         for i in range(0, len(df2.iloc[j]['structure'])):\n#             if df2.iloc[j]['structure'][i] == '(':\n#                 queue.append(i)\n#             if df2.iloc[j]['structure'][i] == ')':\n#                 first = queue.pop()\n#                 try:\n#                     pairs_dict[(df2.iloc[j]['sequence'][first], df2.iloc[j]['sequence'][i])] += 1\n#                 except:\n#                     pairs_dict[(df2.iloc[j]['sequence'][first], df2.iloc[j]['sequence'][i])] = 1\n\n#                 partners[first] = i\n#                 partners[i] = first\n\n#         all_partners.append(partners)\n\n#         pairs_num = 0\n#         pairs_unique = [('U', 'G'), ('C', 'G'), ('U', 'A'), ('G', 'C'), ('A', 'U'), ('G', 'U')]\n#         for item in pairs_dict:\n#             pairs_num += pairs_dict[item]\n#         add_tuple = list()\n#         for item in pairs_unique:\n#             try:\n#                 add_tuple.append(pairs_dict[item]/pairs_num)\n#             except:\n#                 add_tuple.append(0)\n#         pairs.append(add_tuple)\n\n#     pairs = pd.DataFrame(pairs, columns=['U-G', 'C-G', 'U-A', 'G-C', 'A-U', 'G-U'])\n#     del df2\n    \n#     pairs_rate = []\n#     for j in range(len(df3)):\n#         res = dict(count(df3.iloc[j]['structure']))\n#         pairs_rate.append(res['('] / (df3.iloc[j]['seq_length']/2))\n\n#     pairs_rate = pd.DataFrame(pairs_rate, columns=['pairs_rate'])\n#     del df3\n    \n#     loops = []\n#     for j in range(len(df4)):\n#         counts = dict(count(df4.iloc[j]['predicted_loop_type']))\n#         available = ['E', 'S', 'H', 'B', 'X', 'I', 'M']\n#         row = []\n#         for item in available:\n#             try:\n#                 row.append(counts[item] / df4.iloc[j]['seq_length'])\n#             except:\n#                 row.append(0)\n#         loops.append(row)\n\n#     loops = pd.DataFrame(loops, columns=available)\n#     del df4\n    \n#     return pd.concat([df5, bases, pairs, loops, pairs_rate], axis=1)\n\n# token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n\n# def preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n#     return np.transpose(\n#         np.array(\n#             df[cols]\n#             .applymap(lambda seq: [token2int[x] for x in seq])\n#             .values\n#             .tolist()\n#         ),\n#         (0, 2, 1)\n#     )\n\n# train = pd.read_json('../input//train.json', lines=True)\n# test = pd.read_json('../input//test.json', lines=True)\n# sample_df = pd.read_csv('../input//sample_submission.csv')\n\n# #train = process_inputs_2(train)\n# #test = process_inputs_2(test)\n\n# target_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n\n# token2int0 = {'G': 0, 'A': 1, 'C': 2, 'U': 3}\n# token2int1 = {'.': 0,  '(': 1, ')': 2}\n# token2int2 = {'E': 0, 'S': 1, 'H': 2, 'B': 3, 'X': 4, 'I': 5, 'M': 6}\n\n# def convert_seq(x, tmp_dict):\n#     return [tmp_dict[ele] for ele in x]\n\n# train['sequence'] = train['sequence'].apply(lambda x: [token2int0[ele] for ele in x])\n# train['structure'] = train['structure'].apply(lambda x: [token2int1[ele] for ele in x])\n# train['predicted_loop_type'] = train['predicted_loop_type'].apply(lambda x: [token2int2[ele] for ele in x])\n# train_inputs = np.transpose(np.array(train[['sequence', 'structure', 'predicted_loop_type']].values.tolist()), (0, 2, 1))\n\n# train_inputs = train_inputs[train.signal_to_noise > 1]\n# train_labels = np.array(train[train.signal_to_noise > 1][target_cols].values.tolist()).transpose((0, 2, 1))\n\n\n# train_bpps1 = np.stack([1 - np.load(f'../input/bpps/{ele}.npy').sum(1) for ele in train['id']])\n# train_bpps1 = train_bpps1[train.signal_to_noise > 1][:, :, np.newaxis]\n\n# train_bpps2 = np.stack([1 - np.load(f'../input/bpps_nupack/{ele}.npy').sum(1) for ele in train['id']])\n# train_bpps2 = train_bpps2[train.signal_to_noise > 1][:, :, np.newaxis]\n\n# train_bpps3 = np.stack([1 - np.load(f'../input/bpps_contrafold//{ele}.npy').sum(1) for ele in train['id']])\n# train_bpps3 = train_bpps3[train.signal_to_noise > 1][:, :, np.newaxis]\n\n# train_bpps4 = np.stack([1 - np.load(f'../input/bpps_contrafold_linear//{ele}.npy').sum(1) for ele in train['id']])\n# train_bpps4 = train_bpps4[train.signal_to_noise > 1][:, :, np.newaxis]\n\n# train_bpps5 = np.stack([1 - np.load(f'../input/bpps_eternafold/{ele}.npy').sum(1) for ele in train['id']])\n# train_bpps5 = train_bpps5[train.signal_to_noise > 1][:, :, np.newaxis]\n\n# train_bpps6 = np.stack([1 - np.load(f'../input/bpps_vienna_linear/{ele}.npy').sum(1) for ele in train['id']])\n# train_bpps6 = train_bpps6[train.signal_to_noise > 1][:, :, np.newaxis]\n\n# train_bpps = np.concatenate([train_bpps1, train_bpps2, train_bpps3, train_bpps4, train_bpps5, train_bpps6], axis=-1)\n\n# def preprocess_ns(df, pred_len = 68):\n#     ns_columns = ['A_percent',\n#        'G_percent', 'C_percent', 'U_percent', 'U-G', 'C-G', 'U-A', 'G-C',\n#        'A-U', 'G-U', 'E', 'S', 'H', 'B', 'X', 'I', 'M', 'pairs_rate']\n#     z = np.array(df[ns_columns])\n#     b = np.repeat(z[:, np.newaxis,:], pred_len, axis=1)\n#     return b\n\n# #train_nums = preprocess_ns(train, 107)\n# #np.save('../input/train_nums.npy', train_nums)\n\n# train_nums = np.load('../input/train_nums.npy')[train.signal_to_noise > 1]\n\n# #[train.signal_to_noise > 1]\n\n# #test_nums = preprocess_ns(test, 108)\n\n# # public_df = test.query(\"seq_length == 107\").copy()\n# # private_df = test.query(\"seq_length == 130\").copy()\n\n# # public_nums = preprocess_ns(public_df, 107)\n# # private_nums = preprocess_ns(private_df, 130)\n# # np.save('../input/public_nums.npy', public_nums)\n# # np.save('../input/private_nums.npy', private_nums)\n\n# train_As = np.load('../input/train_As.npy')\n\n# from sklearn.model_selection import KFold\n\n\n# # In[28]:\n\n\n# import tensorflow as tf\n# from tensorflow import keras\n# from tensorflow.keras import layers\n\n# class MultiHeadSelfAttention(layers.Layer):\n#     def __init__(self, embed_dim, num_heads=8):\n#         super(MultiHeadSelfAttention, self).__init__()\n#         self.embed_dim = embed_dim\n#         self.num_heads = num_heads\n#         if embed_dim % num_heads != 0:\n#             raise ValueError(\n#                 f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n#             )\n#         self.projection_dim = embed_dim // num_heads\n#         self.query_dense = layers.Conv1D(embed_dim, 1)\n#         self.key_dense = layers.Conv1D(embed_dim, 1)\n#         self.value_dense = layers.Conv1D(embed_dim, 1)\n#         self.combine_heads = layers.Conv1D(embed_dim, 1)\n\n#     def attention(self, query, key, value):\n#         score = tf.matmul(query, key, transpose_b=True)\n#         dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n#         scaled_score = score / tf.math.sqrt(dim_key)\n#         weights = tf.nn.softmax(scaled_score, axis=-1)\n#         output = tf.matmul(weights, value)\n#         return output, weights\n\n#     def separate_heads(self, x, batch_size):\n#         x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n#         return tf.transpose(x, perm=[0, 2, 1, 3])\n\n#     def call(self, inputs):\n#         # x.shape = [batch_size, seq_len, embedding_dim]\n#         batch_size = tf.shape(inputs)[0]\n#         query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n#         key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n#         value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n#         query = self.separate_heads(\n#             query, batch_size\n#         )  # (batch_size, num_heads, seq_len, projection_dim)\n#         key = self.separate_heads(\n#             key, batch_size\n#         )  # (batch_size, num_heads, seq_len, projection_dim)\n#         value = self.separate_heads(\n#             value, batch_size\n#         )  # (batch_size, num_heads, seq_len, projection_dim)\n#         attention, weights = self.attention(query, key, value)\n#         attention = tf.transpose(\n#             attention, perm=[0, 2, 1, 3]\n#         )  # (batch_size, seq_len, num_heads, projection_dim)\n#         concat_attention = tf.reshape(\n#             attention, (batch_size, -1, self.embed_dim)\n#         )  # (batch_size, seq_len, embed_dim)\n#         output = self.combine_heads(\n#             concat_attention\n#         )  # (batch_size, seq_len, embed_dim)\n#         return output\n    \n# class TransformerBlock(layers.Layer):\n#     def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n#         super(TransformerBlock, self).__init__()\n#         self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n#         self.ffn = keras.Sequential(\n#             [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n#         )\n#         self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n#         self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n#         self.dropout1 = layers.Dropout(rate)\n#         self.dropout2 = layers.Dropout(rate)\n\n#     def call(self, inputs, training):\n#         attn_output = self.att(inputs)\n#         attn_output = self.dropout1(attn_output, training=training)\n#         out1 = self.layernorm1(inputs + attn_output)\n#         ffn_output = self.ffn(out1)\n#         ffn_output = self.dropout2(ffn_output, training=training)\n#         return self.layernorm2(out1 + ffn_output)\n    \n#     def get_config(self):\n#         config = super().get_config().copy()\n#         return config\n    \n# class TokenAndPositionEmbedding(layers.Layer):\n#     def __init__(self, maxlen, vocab_size, embed_dim):\n#         super(TokenAndPositionEmbedding, self).__init__()\n#         self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n#         self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n#     def call(self, x):\n#         maxlen = tf.shape(x)[-1]\n#         positions = tf.range(start=0, limit=maxlen, delta=1)\n#         positions = self.pos_emb(positions)\n#         x = self.token_emb(x)\n#         return x + positions\n\n\n# FOLDS = KFold(n_splits=5, random_state=42, shuffle=True)\n\n# oofs_pred = np.zeros_like(train_labels)\n# public_preds_array = []\n# private_preds_array = []\n# result_array1 = []\n# result_array2 = []\n\n# for i, (trn_idx, vld_idx) in enumerate(FOLDS.split(train_inputs)):\n#     trn_inputs = train_inputs[trn_idx]\n#     vld_inputs = train_inputs[vld_idx]\n    \n#     trn_inputs_bpps = train_bpps[trn_idx]\n#     vld_inputs_bpps = train_bpps[vld_idx]\n    \n#     trn_inputs_nums = train_nums[trn_idx]\n#     vld_inputs_nums = train_nums[vld_idx]\n    \n#     trn_inputs_adjs = train_As[trn_idx]\n#     vld_inputs_adjs = train_As[vld_idx]\n\n#     trn_labels = train_labels[trn_idx]\n#     vld_labels = train_labels[vld_idx]\n    \n#     vld_labels_orig = vld_labels.copy()\n    \n#     model = build_model()\n#     model.summary()\n\n#     history = model.fit(\n#         [trn_inputs, trn_inputs_bpps, trn_inputs_nums, trn_inputs_adjs], [trn_labels, trn_labels], \n#         validation_data=([vld_inputs, vld_inputs_bpps, vld_inputs_nums, vld_inputs_adjs], [vld_labels, vld_labels]),\n#         batch_size=32,\n#         epochs=120,\n#         callbacks=[\n#             tf.keras.callbacks.ReduceLROnPlateau(),\n#             tf.keras.callbacks.ModelCheckpoint(f'{NAME}_1.h5')\n#         ],\n#         verbose=2,\n#     )\n    \n#     model.load_weights(f'{NAME}_1.h5')\n#     outputs_1, outputs_ = model.predict([vld_inputs, vld_inputs_bpps, vld_inputs_nums, vld_inputs_adjs])\n    \n# #     trn_inputs = trn_inputs[:, ::-1, :]\n# #     trn_inputs_bpps = trn_inputs_bpps[:, ::-1, :]\n# #     trn_labels = trn_labels[:, ::-1, :]\n    \n# #     vld_inputs = vld_inputs[:, ::-1, :]\n# #     vld_inputs_bpps = vld_inputs_bpps[:, ::-1, :]\n# #     vld_labels = vld_labels[:, ::-1, :]\n    \n    \n# #     model2 = build_model(reverse=True)\n# #     model2.summary()\n\n# #     history = model2.fit(\n# #         [trn_inputs, trn_inputs_bpps], trn_labels, \n# #         validation_data=([vld_inputs, vld_inputs_bpps], vld_labels),\n# #         batch_size=32,\n# #         epochs=120,\n# #         callbacks=[\n# #             tf.keras.callbacks.ReduceLROnPlateau(),\n# #             tf.keras.callbacks.ModelCheckpoint(f'{NAME}_2.h5')\n# #         ],\n# #         verbose=2,\n# #     )\n    \n# #     model.load_weights(f'{NAME}_2.h5')\n# #     outputs_2, outputs_ =  model2.predict([vld_inputs, vld_inputs_bpps])\n# #     #trn_inputs = np.concatenate([trn_inputs, trn_inputs[:, ::-1, :]], axis=0)\n#     outputs = outputs_1#0.5 * (outputs_1 + outputs_2[:, ::-1, :])\n#     oofs_pred[vld_idx] = outputs\n    \n#     from sklearn.metrics import mean_squared_error\n#     errors = []\n#     for idx in range(5):\n#          errors.append(np.sqrt(mean_squared_error(vld_labels_orig[:, idx], outputs[:, idx])))\n#     final_error = np.mean(errors)\n#     print('#'*20, final_error)\n    \n#     result_array1.append(final_error)\n    \n#     oofs_ls = []\n#     for idx in vld_idx:\n#         single_pred = oofs_pred[idx]\n#         single_df = pd.DataFrame(single_pred, columns=pred_cols)\n#         oofs_ls.append(single_df)\n#     oofs_df = pd.concat(oofs_ls).values\n    \n#     target_ls = []\n#     for idx in range(len(vld_labels)):\n#         single_pred = vld_labels_orig[idx]\n#         single_df = pd.DataFrame(single_pred, columns=pred_cols)\n#         target_ls.append(single_df)\n#     target_df = pd.concat(target_ls).values\n    \n#     from sklearn.metrics import mean_squared_error\n#     errors = []\n#     for idx in range(5):\n#          errors.append(np.sqrt(mean_squared_error(target_df[:, idx], oofs_df[:, idx])))\n#     final_error = np.mean(errors)\n#     print('#'*20, final_error)\n    \n#     result_array2.append(final_error)\n    \n    \n\n#     public_df = test.query(\"seq_length == 107\").copy()\n#     private_df = test.query(\"seq_length == 130\").copy()\n    \n#     public_df['sequence'] = public_df['sequence'].apply(lambda x: [token2int0[ele] for ele in x])\n#     public_df['structure'] = public_df['structure'].apply(lambda x: [token2int1[ele] for ele in x])\n#     public_df['predicted_loop_type'] = public_df['predicted_loop_type'].apply(lambda x: [token2int2[ele] for ele in x])\n#     public_inputs = np.transpose(np.array(public_df[['sequence', 'structure', 'predicted_loop_type']].values.tolist()), (0, 2, 1))\n\n#     private_df['sequence'] = private_df['sequence'].apply(lambda x: [token2int0[ele] for ele in x])\n#     private_df['structure'] = private_df['structure'].apply(lambda x: [token2int1[ele] for ele in x])\n#     private_df['predicted_loop_type'] = private_df['predicted_loop_type'].apply(lambda x: [token2int2[ele] for ele in x])\n#     private_inputs = np.transpose(np.array(private_df[['sequence', 'structure', 'predicted_loop_type']].values.tolist()), (0, 2, 1))\n\n#     public_bpps1 = np.stack([1 - np.load(f'../input/bpps/{ele}.npy').sum(1) for ele in public_df['id']])\n#     public_bpps1 = public_bpps1[:, :, np.newaxis]\n\n#     public_bpps2 = np.stack([1 - np.load(f'../input/bpps_nupack/{ele}.npy').sum(1) for ele in public_df['id']])\n#     public_bpps2 = public_bpps2[:, :, np.newaxis]\n\n#     public_bpps3 = np.stack([1 - np.load(f'../input/bpps_contrafold//{ele}.npy').sum(1) for ele in public_df['id']])\n#     public_bpps3 = public_bpps3[:, :, np.newaxis]\n\n#     public_bpps4 = np.stack([1 - np.load(f'../input/bpps_contrafold_linear//{ele}.npy').sum(1) for ele in public_df['id']])\n#     public_bpps4 = public_bpps4[:, :, np.newaxis]\n\n#     public_bpps5 = np.stack([1 - np.load(f'../input/bpps_eternafold/{ele}.npy').sum(1) for ele in public_df['id']])\n#     public_bpps5 = public_bpps5[:, :, np.newaxis]\n\n#     public_bpps6 = np.stack([1 - np.load(f'../input/bpps_vienna_linear/{ele}.npy').sum(1) for ele in public_df['id']])\n#     public_bpps6 = public_bpps6[:, :, np.newaxis]\n\n#     public_bpps = np.concatenate([public_bpps1, public_bpps2, public_bpps3, public_bpps4, public_bpps5, public_bpps6], axis=-1)\n    \n#     private_bpps1 = np.stack([1 - np.load(f'../input/bpps/{ele}.npy').sum(1) for ele in private_df['id']])\n#     private_bpps1 = private_bpps1[:, :, np.newaxis]\n\n#     private_bpps2 = np.stack([1 - np.load(f'../input/bpps_nupack/{ele}.npy').sum(1) for ele in private_df['id']])\n#     private_bpps2 = private_bpps2[:, :, np.newaxis]\n\n#     private_bpps3 = np.stack([1 - np.load(f'../input/bpps_contrafold//{ele}.npy').sum(1) for ele in private_df['id']])\n#     private_bpps3 = private_bpps3[:, :, np.newaxis]\n\n#     private_bpps4 = np.stack([1 - np.load(f'../input/bpps_contrafold_linear//{ele}.npy').sum(1) for ele in private_df['id']])\n#     private_bpps4 = private_bpps4[:, :, np.newaxis]\n\n#     private_bpps5 = np.stack([1 - np.load(f'../input/bpps_eternafold/{ele}.npy').sum(1) for ele in private_df['id']])\n#     private_bpps5 = private_bpps5[:, :, np.newaxis]\n\n#     private_bpps6 = np.stack([1 - np.load(f'../input/bpps_vienna_linear/{ele}.npy').sum(1) for ele in private_df['id']])\n#     private_bpps6 = private_bpps6[:, :, np.newaxis]\n\n#     private_bpps = np.concatenate([private_bpps1, private_bpps2, private_bpps3, private_bpps4, private_bpps5, private_bpps6], axis=-1)\n\n    \n#     public_nums = np.load('../input/public_nums.npy')\n#     private_nums = np.load('../input/private_nums.npy')\n    \n#     public_adjs = np.load('../input/public_As.npy')\n#     private_adjs = np.load('../input/private_As.npy')\n    \n    \n#     # Caveat: The prediction format requires the output to be the same length as the input,\n#     # although it's not the case for the training data.\n#     model_short = build_model(seq_len=107, pred_len=107)\n#     model_long = build_model(seq_len=130, pred_len=130)\n    \n#     model_short2 = build_model(seq_len=107, pred_len=107, reverse=True)\n#     model_long2 = build_model(seq_len=130, pred_len=130, reverse=True)\n\n#     model_short.load_weights(f'{NAME}_1.h5')\n#     model_long.load_weights(f'{NAME}_1.h5')\n\n#     public_preds_1, outputs2 = model_short.predict([public_inputs, public_bpps, public_nums, public_adjs])\n#     private_preds_1, outputs2 = model_long.predict([private_inputs, private_bpps, private_nums, private_adjs])\n    \n    \n# #     model_short2.load_weights(f'{NAME}_2.h5')\n# #     model_long2.load_weights(f'{NAME}_2.h5')\n\n# #     public_inputs = public_inputs[:, ::-1, :]\n# #     public_bpps = public_bpps[:, ::-1, :]\n    \n# #     private_inputs = private_inputs[:, ::-1, :]\n# #     private_bpps = private_bpps[:, ::-1, :]\n    \n# #     public_preds_2, outputs2 = model_short2.predict([public_inputs, public_bpps])\n# #     private_preds_2, outputs2 = model_long2.predict([private_inputs, private_bpps])\n    \n    \n#     public_preds = public_preds_1# 0.5 * (public_preds_1 + public_preds_2[:, ::-1, :])\n#     private_preds = private_preds_1#0.5 * (private_preds_1 + private_preds_2[:, ::-1, :])\n    \n#     public_preds_array.append(public_preds)\n#     private_preds_array.append(private_preds)\n\n#     print(public_preds.shape, private_preds.shape)\n\n#     preds_ls = []\n\n#     for df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n#         for idx, uid in enumerate(df.id):\n#             single_pred = preds[idx]\n\n#             single_df = pd.DataFrame(single_pred, columns=pred_cols)\n#             single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n#             preds_ls.append(single_df)\n\n#     preds_df = pd.concat(preds_ls)\n\n#     submission = sample_df[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\n#     submission.to_csv(f'{NAME}_{i}.csv', index=False)\n    \n# print(result_array1)\n# print(result_array2)\n\n\n\n\n\n\n\n# trn_inputs_bpps.shape\n\n# trn_inputs_nums.shape\n\n# # - [0.3643929474422003, 0.35387031483746306, 0.35421918628471893, 0.3625872161452601, 0.34353836209736854]\n# # - [0.22639929831042677, 0.2168258249933515, 0.23131894944245507, 0.2370713470214479, 0.2271213424723008]\n# # - 0.2278776791567095\n\n\n# oofs_ls = []\n# for idx in range(len(oofs_pred)):\n#     single_pred = oofs_pred[idx]\n#     single_df = pd.DataFrame(single_pred, columns=pred_cols)\n#     oofs_ls.append(single_df)\n# oofs_df = pd.concat(oofs_ls).values\n\n# target_ls = []\n# for idx in range(len(train_labels)):\n#     single_pred = train_labels[idx]\n#     single_df = pd.DataFrame(single_pred, columns=pred_cols)\n#     target_ls.append(single_df)\n# target_df = pd.concat(target_ls).values\n\n# from sklearn.metrics import mean_squared_error\n# errors = []\n# for idx in range(5):\n#      errors.append(np.sqrt(mean_squared_error(target_df[:, idx], oofs_df[:, idx])))\n# final_error = np.mean(errors)\n# print('#'*20, final_error)\n\n# train_clean = train[train.signal_to_noise > 1].reset_index(drop=True)\n\n\n# preds_ls = []\n# for idx, uid in enumerate(train_clean.id):\n#     single_pred = oofs_pred[idx]\n\n#     single_df = pd.DataFrame(single_pred, columns=pred_cols)\n#     single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n#     preds_ls.append(single_df)\n# preds_df = pd.concat(preds_ls)\n\n# preds_df.to_csv(f'../src_oofs/oofs.{NAME}.csv', index=False)\n\n# sub1 = pd.read_csv(f'./{NAME}_0.csv')\n# sub2 = pd.read_csv(f'./{NAME}_1.csv')\n# sub3 = pd.read_csv(f'./{NAME}_2.csv')\n# sub4 = pd.read_csv(f'./{NAME}_3.csv')\n# sub5 = pd.read_csv(f'./{NAME}_4.csv')\n\n# new_sub = sub1.copy()\n\n# new_sub[pred_cols] = (1/5) * (sub1[pred_cols].values + sub2[pred_cols].values + sub3[pred_cols].values + sub4[pred_cols].values + sub5[pred_cols].values)\n\n# new_sub.to_csv(f'./{NAME}_5fold.csv', index=False)\n\n# NAME\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}