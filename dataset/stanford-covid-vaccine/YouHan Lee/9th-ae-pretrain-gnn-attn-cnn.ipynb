{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"# import os\n# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n\n\n# pretrain_dir = None#\"/kaggle/input/covid-v9-no-consis/\"\n\n# one_fold = False\n# # one_fold = True#False\n# # with_ae = False#True\n# run_test = False\n# # run_test = True\n# denoise = True\n\n# ae_epochs = 28\n# ae_epochs_each = 7\n# ae_batch_size = 32\n\n# epochs_list = [32, 16, 16, 16, 8, 8]\n# batch_size_list = [8, 16, 32, 64, 128, 256]\n\n# ## copy pretrain model to working dir\n# import shutil\n# import glob\n# if pretrain_dir is not None:\n#     for d in glob.glob(pretrain_dir + \"*\"):\n#         shutil.copy(d, \".\")\n    \n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# import gc\n# import os\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n\n# NAME = 'covid_ae_pretrain_gnn_attn_cnn_addAdj_more_deep_gaussDist_struct_gru_feats_Se'\n\n# ## load\n\n# import json\n# import glob\n# from tqdm.notebook import tqdm\n\n# train = pd.read_json(\"../input//train.json\",lines=True)\n# mask = train.signal_to_noise > 1\n# if denoise:\n#     train = train[train.signal_to_noise > 1].reset_index(drop = True)\n# test  = pd.read_json(\"../input//test.json\",lines=True)\n# test_pub = test[test[\"seq_length\"] == 107]\n# test_pri = test[test[\"seq_length\"] == 130]\n# sub = pd.read_csv(\"../input//sample_submission.csv\")\n\n# if run_test:\n#     train = train[:30]\n#     test_pub = test_pub[:30]\n#     test_pri = test_pri[:30]\n\n# As = []\n# for id in tqdm(train[\"id\"]):\n#     a = np.load(f\"../input//bpps/{id}.npy\")\n#     As.append(a)\n# As = np.array(As)\n# As_pub = []\n# for id in tqdm(test_pub[\"id\"]):\n#     a = np.load(f\"../input//bpps/{id}.npy\")\n#     As_pub.append(a)\n# As_pub = np.array(As_pub)\n# As_pri = []\n# for id in tqdm(test_pri[\"id\"]):\n#     a = np.load(f\"../input//bpps/{id}.npy\")\n#     As_pri.append(a)\n# As_pri = np.array(As_pri)\n\n# As1 = []\n# for id in tqdm(train[\"id\"]):\n#     a = np.load(f\"../input_bpps/bpps_contrafold/{id}.npy\")\n#     As1.append(a)\n# As1 = np.array(As1)\n# As1_pub = []\n# for id in tqdm(test_pub[\"id\"]):\n#     a = np.load(f\"../input_bpps/bpps_contrafold/{id}.npy\")\n#     As1_pub.append(a)\n# As1_pub = np.array(As1_pub)\n# As1_pri = []\n# for id in tqdm(test_pri[\"id\"]):\n#     a = np.load(f\"../input_bpps/bpps_contrafold/{id}.npy\")\n#     As1_pri.append(a)\n# As1_pri = np.array(As1_pri)\n\n# As2 = []\n# for id in tqdm(train[\"id\"]):\n#     a = np.load(f\"../input_bpps/bpps_eternafold//{id}.npy\")\n#     As2.append(a)\n# As2 = np.array(As2)\n# As2_pub = []\n# for id in tqdm(test_pub[\"id\"]):\n#     a = np.load(f\"../input_bpps/bpps_eternafold//{id}.npy\")\n#     As2_pub.append(a)\n# As2_pub = np.array(As2_pub)\n# As2_pri = []\n# for id in tqdm(test_pri[\"id\"]):\n#     a = np.load(f\"../input_bpps/bpps_eternafold//{id}.npy\")\n#     As2_pri.append(a)\n# As2_pri = np.array(As2_pri)\n\n# As3 = []\n# for id in tqdm(train[\"id\"]):\n#     a = np.load(f\"../input_bpps/bpps_nupack///{id}.npy\")\n#     As3.append(a)\n# As3 = np.array(As3)\n# As3_pub = []\n# for id in tqdm(test_pub[\"id\"]):\n#     a = np.load(f\"../input_bpps/bpps_nupack//{id}.npy\")\n#     As3_pub.append(a)\n# As3_pub = np.array(As3_pub)\n# As3_pri = []\n# for id in tqdm(test_pri[\"id\"]):\n#     a = np.load(f\"../input_bpps/bpps_nupack//{id}.npy\")\n#     As3_pri.append(a)\n# As3_pri = np.array(As3_pri)\n\n# As4 = []\n# for id in tqdm(train[\"id\"]):\n#     a = np.load(f\"../input_bpps/bpps_contrafold_linear//{id}.npy\")\n#     As4.append(a)\n# As4 = np.array(As4)\n# As4_pub = []\n# for id in tqdm(test_pub[\"id\"]):\n#     a = np.load(f\"../input_bpps/bpps_contrafold_linear//{id}.npy\")\n#     As4_pub.append(a)\n# As4_pub = np.array(As4_pub)\n# As4_pri = []\n# for id in tqdm(test_pri[\"id\"]):\n#     a = np.load(f\"../input_bpps/bpps_contrafold_linear//{id}.npy\")\n#     As4_pri.append(a)\n# As4_pri = np.array(As4_pri)\n\n# As5 = []\n# for id in tqdm(train[\"id\"]):\n#     a = np.load(f\"../input_bpps/bpps_vienna_linear///{id}.npy\")\n#     As5.append(a)\n# As5 = np.array(As5)\n# As5_pub = []\n# for id in tqdm(test_pub[\"id\"]):\n#     a = np.load(f\"../input_bpps/bpps_vienna_linear//{id}.npy\")\n#     As5_pub.append(a)\n# As5_pub = np.array(As5_pub)\n# As5_pri = []\n# for id in tqdm(test_pri[\"id\"]):\n#     a = np.load(f\"../input_bpps/bpps_vienna_linear//{id}.npy\")\n#     As5_pri.append(a)\n# As5_pri = np.array(As5_pri)\n\n# print(train.shape)\n# train.head()\n\n# print(test.shape)\n# test.head()\n\n# print(sub.shape)\n# sub.head()\n\n# ## target\n\n# targets = list(sub.columns[1:])\n# print(targets)\n\n# y_train = []\n# seq_len = train[\"seq_length\"].iloc[0]\n# seq_len_target = train[\"seq_scored\"].iloc[0]\n# ignore = -10000\n# ignore_length = seq_len - seq_len_target\n# for target in targets:\n#     y = np.vstack(train[target])\n#     dummy = np.zeros([y.shape[0], ignore_length]) + ignore\n#     y = np.hstack([y, dummy])\n#     y_train.append(y)\n# y = np.stack(y_train, axis = 2)\n# y.shape\n\n# ## structure adj\n\n# def get_structure_adj(train):\n#     Ss = []\n#     for i in tqdm(range(len(train))):\n#         seq_length = train[\"seq_length\"].iloc[i]\n#         structure = train[\"structure\"].iloc[i]\n#         sequence = train[\"sequence\"].iloc[i]\n\n#         cue = []\n#         a_structures = {\n#             (\"A\", \"U\") : np.zeros([seq_length, seq_length]),\n#             (\"C\", \"G\") : np.zeros([seq_length, seq_length]),\n#             (\"U\", \"G\") : np.zeros([seq_length, seq_length]),\n#             (\"U\", \"A\") : np.zeros([seq_length, seq_length]),\n#             (\"G\", \"C\") : np.zeros([seq_length, seq_length]),\n#             (\"G\", \"U\") : np.zeros([seq_length, seq_length]),\n#         }\n#         a_structure = np.zeros([seq_length, seq_length])\n#         for i in range(seq_length):\n#             if structure[i] == \"(\":\n#                 cue.append(i)\n#             elif structure[i] == \")\":\n#                 start = cue.pop()\n# #                 a_structure[start, i] = 1\n# #                 a_structure[i, start] = 1\n#                 a_structures[(sequence[start], sequence[i])][start, i] = 1\n#                 a_structures[(sequence[i], sequence[start])][i, start] = 1\n        \n#         a_strc = np.stack([a for a in a_structures.values()], axis = 2)\n#         a_strc = np.sum(a_strc, axis = 2, keepdims = True)\n#         Ss.append(a_strc)\n    \n#     Ss = np.array(Ss)\n#     print(Ss.shape)\n#     return Ss\n# Ss = get_structure_adj(train)\n# Ss_pub = get_structure_adj(test_pub)\n# Ss_pri = get_structure_adj(test_pri)\n\n# ## distance adj\n\n# def get_distance_matrix(As):\n#     idx = np.arange(As.shape[1])\n#     Ds = []\n#     for i in range(len(idx)):\n#         d = np.abs(idx[i] - idx)\n#         Ds.append(d)\n\n#     Ds = np.array(Ds) + 1\n#     Ds = 1/Ds\n#     Ds = Ds[None, :,:]\n#     Ds = np.repeat(Ds, len(As), axis = 0)\n    \n#     Dss = []\n#     for i in [1, 2, 4]:\n#         Dss.append(Ds ** i)\n#     #Ds = np.stack(Dss, axis = 3)\n#     for gamma in np.arange(10):\n#         Dss.append(np.exp(-np.power(Ds, 2) * gamma))\n#     Ds = np.stack(Dss, axis = 3)\n\n#     print(Ds.shape)\n#     return Ds\n\n# Ds = get_distance_matrix(As)\n# Ds_pub = get_distance_matrix(As_pub)\n# Ds_pri = get_distance_matrix(As_pri)\n\n# ## concat adjecent\n# As = np.concatenate([As[:,:,:,None], As1[:,:,:,None], As2[:,:,:,None], As3[:,:,:,None], As4[:,:,:,None], As5[:,:,:,None], Ss, Ds], axis = 3).astype(np.float32)\n# As_pub = np.concatenate([As_pub[:,:,:,None], As1_pub[:,:,:,None], As2_pub[:,:,:,None], As3_pub[:,:,:,None], As4_pub[:,:,:,None], As5_pub[:,:,:,None],\n#                          Ss_pub, Ds_pub], axis = 3).astype(np.float32)\n# As_pri = np.concatenate([As_pri[:,:,:,None], As1_pri[:,:,:,None], As2_pri[:,:,:,None], As3_pri[:,:,:,None], As4_pri[:,:,:,None], As5_pri[:,:,:,None],\n#                          Ss_pri, Ds_pri], axis = 3).astype(np.float32)\n# del Ss, Ds, Ss_pub, Ds_pub, Ss_pri, Ds_pri\n# As.shape, As_pub.shape, As_pri.shape\n\n# np.save('../input/train_As.npy', As)\n\n# np.save('../input/public_As.npy', As_pub)\n\n# np.save('../input/private_As.npy', As_pri)\n\n\n\n# ## node\n\n# ## sequence\n# def return_ohe(n, i):\n#     tmp = [0] * n\n#     tmp[i] = 1\n#     return tmp\n\n# def get_input(train):\n#     mapping = {}\n#     vocab = [\"A\", \"G\", \"C\", \"U\"]\n#     for i, s in enumerate(vocab):\n#         mapping[s] = return_ohe(len(vocab), i)\n#     X_node = np.stack(train[\"sequence\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n\n#     mapping = {}\n#     vocab = [\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n#     for i, s in enumerate(vocab):\n#         mapping[s] = return_ohe(len(vocab), i)\n#     X_loop = np.stack(train[\"predicted_loop_type\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    \n#     mapping = {}\n#     vocab = [\".\", \"(\", \")\"]\n#     for i, s in enumerate(vocab):\n#         mapping[s] = return_ohe(len(vocab), i)\n#     X_structure = np.stack(train[\"structure\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    \n    \n#     X_node = np.concatenate([X_node, X_loop, X_structure], axis = 2)\n    \n#     ## interaction\n#     a = np.sum(X_node * (2 ** np.arange(X_node.shape[2])[None, None, :]), axis = 2)\n#     vocab = sorted(set(a.flatten()))\n#     print(vocab)\n#     ohes = []\n#     for v in vocab:\n#         ohes.append(a == v)\n#     ohes = np.stack(ohes, axis = 2)\n#     X_node = np.concatenate([X_node, ohes], axis = 2).astype(np.float32)\n    \n    \n#     print(X_node.shape)\n#     return X_node\n\n# X_node = get_input(train)\n# X_node_pub = get_input(test_pub)\n# X_node_pri = get_input(test_pri)\n\n\n\n# ## model\n\n# import tensorflow as tf\n# from tensorflow.keras import layers as L\n# import tensorflow_addons as tfa\n# from tensorflow.keras import backend as K\n\n# def mcrmse(t, p, seq_len_target = seq_len_target):\n#     score = np.mean(np.sqrt(np.mean((p - y_va) ** 2, axis = 2))[:, :seq_len_target])\n#     return score\n\n# def mcrmse_loss(t, y, seq_len_target = seq_len_target):\n#     t = t[:, :seq_len_target]\n#     y = y[:, :seq_len_target]\n    \n#     loss = tf.reduce_mean(tf.sqrt(tf.reduce_mean((t - y) ** 2, axis = 2)))\n#     return loss\n\n# def se_block(x_in, layer_n):\n#     x = L.GlobalAveragePooling1D()(x_in)\n#     x = L.Dense(layer_n//8, activation=\"relu\")(x)\n#     x = L.Dense(layer_n, activation=\"sigmoid\")(x)\n#     x_out= L.Multiply()([x_in, x])\n#     return x_out\n\n# def attention(x_inner, x_outer, n_factor, dropout):\n#     x_Q =  L.Conv1D(n_factor, 1, activation='linear', \n#                   kernel_initializer='glorot_uniform',\n#                   bias_initializer='glorot_uniform',\n#                  )(x_inner)\n#     x_K =  L.Conv1D(n_factor, 1, activation='linear', \n#                   kernel_initializer='glorot_uniform',\n#                   bias_initializer='glorot_uniform',\n#                  )(x_outer)\n#     x_V =  L.Conv1D(n_factor, 1, activation='linear', \n#                   kernel_initializer='glorot_uniform',\n#                   bias_initializer='glorot_uniform',\n#                  )(x_outer)\n#     x_KT = L.Permute((2, 1))(x_K)\n#     res = L.Lambda(lambda c: K.batch_dot(c[0], c[1]) / np.sqrt(n_factor))([x_Q, x_KT])\n# #     res = tf.expand_dims(res, axis = 3)\n# #     res = L.Conv2D(16, 3, 1, padding = \"same\", activation = \"relu\")(res)\n# #     res = L.Conv2D(1, 3, 1, padding = \"same\", activation = \"relu\")(res)\n# #     res = tf.squeeze(res, axis = 3)\n#     att = L.Lambda(lambda c: K.softmax(c, axis=-1))(res)\n#     att = L.Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n#     return att\n\n# def multi_head_attention(x, y, n_factor, n_head, dropout):\n#     if n_head == 1:\n#         att = attention(x, y, n_factor, dropout)\n#     else:\n#         n_factor_head = n_factor // n_head\n#         heads = [attention(x, y, n_factor_head, dropout) for i in range(n_head)]\n#         att = L.Concatenate()(heads)\n#         att = L.Dense(n_factor, \n#                       kernel_initializer='glorot_uniform',\n#                       bias_initializer='glorot_uniform',\n#                      )(att)\n#     x = L.Add()([x, att])\n#     x = L.LayerNormalization()(x)\n#     if dropout > 0:\n#         x = L.Dropout(dropout)(x)\n#     return x\n\n# def res(x, unit, kernel = 3, rate = 0.1):\n#     h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n#     h = se_block(h, unit)\n#     h = L.LayerNormalization()(h)\n#     h = L.LeakyReLU()(h)\n#     h = L.Dropout(rate)(h)\n#     return L.Add()([x, h])\n\n# def forward(x, unit, kernel = 3, rate = 0.1):\n# #     h = L.Dense(unit, None)(x)\n#     h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n#     h = L.LayerNormalization()(h)\n#     h = L.Dropout(rate)(h)\n# #         h = tf.keras.activations.swish(h)\n#     h = L.LeakyReLU()(h)\n#     h = res(h, unit, kernel, rate)\n#     return h\n\n# def adj_attn(x, adj, unit, n = 2, rate = 0.1):\n#     x_a = x\n#     x_as = []\n#     for i in range(n):\n#         x_a = forward(x_a, unit)\n#         x_a = tf.matmul(adj, x_a)\n#         x_as.append(x_a)\n#     if n == 1:\n#         x_a = x_as[0]\n#     else:\n#         x_a = L.Concatenate()(x_as)\n#     x_a = forward(x_a, unit)\n#     return x_a\n\n\n# def get_base(config):\n#     node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n#     adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n    \n#     adj_learned = L.Dense(64, \"relu\")(adj)\n#     adj_learned = L.Dense(1, \"relu\")(adj_learned)\n    \n    \n#     adj_all = L.Concatenate(axis = 3)([adj, adj_learned])\n        \n#     xs = []\n#     xs.append(node)\n#     x1 = forward(node, 128, kernel = 3, rate = 0.0)\n#     x2 = forward(x1, 64, kernel = 6, rate = 0.0)\n#     x3 = forward(x2, 32, kernel = 12, rate = 0.0)\n#     x4 = forward(x3, 16, kernel = 24, rate = 0.0)\n#     x5 = forward(x4, 8, kernel = 48, rate = 0.0)\n#     x = L.Concatenate()([x1, x2, x3, x4, x5])\n    \n#     for unit in [64, 48, 32]:\n#         x_as = []\n#         for i in range(adj_all.shape[3]):\n#             x_a = adj_attn(x, adj_all[:, :, :, i], unit, rate = 0.0)\n#             x_as.append(x_a)\n#         x_c = forward(x, unit, kernel = 32)\n        \n#         x = L.Concatenate()(x_as + [x_c])\n#         x = forward(x, unit)\n#         x = multi_head_attention(x, x, unit, 4, 0.0)\n#         xs.append(x)\n        \n#     x = L.Concatenate()(xs)\n\n#     model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n#     return model\n\n\n# def get_ae_model(base, config):\n#     node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n#     adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n\n#     x = base([L.SpatialDropout1D(0.3)(node), adj])\n#     x = forward(x, 64, rate = 0.3)\n#     p = L.Dense(X_node.shape[2], \"sigmoid\")(x)\n    \n#     loss = - tf.reduce_mean(20 * node * tf.math.log(p + 1e-4) + (1 - node) * tf.math.log(1 - p + 1e-4))\n#     model = tf.keras.Model(inputs = [node, adj], outputs = [loss])\n    \n#     opt = get_optimizer()\n#     model.compile(optimizer = opt, loss = lambda t, y : y)\n#     return model\n\n# def gru_layer(hidden_dim, dropout):\n#     return tf.keras.layers.Bidirectional(\n#                                 tf.keras.layers.GRU(hidden_dim,\n#                                 dropout=dropout,\n#                                 return_sequences=True,\n#                                 kernel_initializer = 'orthogonal'))\n\n# def get_model(base, config):\n#     node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n#     adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n#     inputs_nums = tf.keras.layers.Input(shape=(seq_len, 18), name='input_nums')\n#     x = base([node, adj])\n#     nums = tf.keras.layers.Dense(128, activation='relu')(inputs_nums)\n\n#     x = L.Concatenate()([x, nums])\n    \n#     dropout = 0.1\n#     x = gru_layer(128, dropout)(x)\n#     x = forward(x, 128, rate = 0.4)\n#     x = gru_layer(128, dropout)(x)\n#     x = forward(x, 128, rate = 0.4)\n    \n#     x = L.Dense(5, None)(x)\n\n#     model = tf.keras.Model(inputs = [node, adj, inputs_nums], outputs = [x])\n    \n#     opt = get_optimizer()\n#     model.compile(optimizer = opt, loss = mcrmse_loss)\n#     return model\n\n# def get_optimizer():\n# #     sgd = tf.keras.optimizers.SGD(0.05, momentum = 0.9, nesterov=True)\n#     adam = tf.optimizers.Adam()\n# #     radam = tfa.optimizers.RectifiedAdam()\n# #     lookahead = tfa.optimizers.Lookahead(adam, sync_period=6)\n# #     swa = tfa.optimizers.SWA(adam)\n#     return adam\n\n# train_nums = np.load('../input/train_nums.npy')[mask]\n# public_nums = np.load('../input/public_nums.npy')\n# private_nums = np.load('../input/private_nums.npy')\n\n# ## pretrain\n\n# config = {}\n\n# if ae_epochs > 0:\n#     base = get_base(config)\n#     ae_model = get_ae_model(base, config)\n#     ## TODO : simultaneous train\n#     for i in range(ae_epochs//ae_epochs_each):\n#         print(f\"------ {i} ------\")\n#         print(\"--- train ---\")\n#         ae_model.fit([X_node, As], [X_node[:,0]],\n#                   epochs = ae_epochs_each,\n#                   batch_size = ae_batch_size,\n#                   callbacks=[tf.keras.callbacks.ModelCheckpoint(f\"./base_ae_{NAME}\", save_weights_only=True)])\n#         print(\"--- public ---\")\n#         ae_model.fit([X_node_pub, As_pub], [X_node_pub[:,0]],\n#                   epochs = ae_epochs_each,\n#                   batch_size = ae_batch_size,\n#                      callbacks=[tf.keras.callbacks.ModelCheckpoint(f\"./base_ae_{NAME}\", save_weights_only=True)])\n#         print(\"--- private ---\")\n#         ae_model.fit([X_node_pri, As_pri], [X_node_pri[:,0]],\n#                   epochs = ae_epochs_each,\n#                   batch_size = ae_batch_size,\n#                     callbacks=[tf.keras.callbacks.ModelCheckpoint(f\"./base_ae_{NAME}\", save_weights_only=True)])\n#         gc.collect()\n#     print(\"****** save ae model ******\")\n#     base.save_weights(f\"./base_ae_{NAME}\")\n\n# ## train\n\n# from sklearn.model_selection import KFold\n# kfold = KFold(5, shuffle = True, random_state = 42)\n\n# scores = []\n# preds = np.zeros([len(X_node), X_node.shape[1], 5])\n# for i, (tr_idx, va_idx) in enumerate(kfold.split(X_node, As)):\n#     print(f\"------ fold {i} start -----\")\n#     print(f\"------ fold {i} start -----\")\n#     print(f\"------ fold {i} start -----\")\n#     X_node_tr = X_node[tr_idx]\n#     X_node_va = X_node[va_idx]\n#     X_nums_tr = train_nums[tr_idx]\n#     X_nums_va = train_nums[va_idx]\n#     As_tr = As[tr_idx]\n#     As_va = As[va_idx]\n#     y_tr = y[tr_idx]\n#     y_va = y[va_idx]\n    \n#     base = get_base(config)\n#     if ae_epochs > 0:\n#         print(\"****** load ae model ******\")\n#         base.load_weights(f\"./base_ae_{NAME}\")\n#     model = get_model(base, config)\n#     if pretrain_dir is not None:\n#         d = f\"./model_addmore{i}_{NAME}\"\n#         print(f\"--- load from {d} ---\")\n#         model.load_weights(d)\n#     for epochs, batch_size in zip(epochs_list, batch_size_list):\n#         print(f\"epochs : {epochs}, batch_size : {batch_size}\")\n#         model.fit([X_node_tr, As_tr, X_nums_tr], [y_tr],\n#                   validation_data=([X_node_va, As_va, X_nums_va], [y_va]),\n#                   epochs = epochs,\n#                   batch_size = batch_size, validation_freq = 1,\n#                   callbacks=[tf.keras.callbacks.ModelCheckpoint(f\"./model_addmore{i}_{NAME}\", save_weights_only=True)])\n        \n#     model.load_weights(f\"./model_addmore{i}_{NAME}\")\n#     p = model.predict([X_node_va, As_va, X_nums_va])\n#     scores.append(mcrmse(y_va, p))\n#     print(f\"fold {i}: mcrmse {scores[-1]}\")\n#     preds[va_idx] = p\n#     if one_fold:\n#         break\n        \n# pd.to_pickle(preds, f\"oof_addmore_{NAME}.pkl\")\n\n# print(scores)\n\n# ## predict\n\n# p_pub = 0\n# p_pri = 0\n# for i in range(5):\n#     model.load_weights(f\"./model_addmore{i}_{NAME}\")\n#     p_pub += model.predict([X_node_pub, As_pub, public_nums]) / 5\n#     p_pri += model.predict([X_node_pri, As_pri, private_nums]) / 5\n#     if one_fold:\n#         p_pub *= 5\n#         p_pri *= 5\n#         break\n\n# for i, target in enumerate(targets):\n#     test_pub[target] = [list(p_pub[k, :, i]) for k in range(p_pub.shape[0])]\n#     test_pri[target] = [list(p_pri[k, :, i]) for k in range(p_pri.shape[0])]\n\n# ## sub\n\n# preds_ls = []\n# for df, preds in [(test_pub, p_pub), (test_pri, p_pri)]:\n#     for i, uid in enumerate(df.id):\n#         single_pred = preds[i]\n\n#         single_df = pd.DataFrame(single_pred, columns=targets)\n#         single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n#         preds_ls.append(single_df)\n\n# preds_df = pd.concat(preds_ls)\n# preds_df.to_csv(f\"{NAME}.csv\", index = False)\n# preds_df.head()\n\n# print(scores)\n# print(np.mean(scores))\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}