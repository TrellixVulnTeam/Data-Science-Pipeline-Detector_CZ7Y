{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tutorials on DeepRNA modeling\n\nThis tutorial teaches you how to make / finetune your own RNA Deep-Learning model with [DeepRNA repository](https://github.com/ratthachat/deep-rna).\nThis repo has been inspired by many wonderful works on [Kaggle RNA OpenVaccine competition](https://www.kaggle.com/c/stanford-covid-vaccine) which is a competition about \"prediction on properties of RNA strings\". The repo has an objective to unify [many advanced and insightful ideas with SOTA performance](https://www.kaggle.com/c/stanford-covid-vaccine/code?competitionId=22111&sortBy=scoreAscending) into one single place and to be applicable to other general RNA prediction problems where we assume only \"RNA strings\" and their predicted-targets are known apriori. So hopefully, this repo can be some small contribution for people working on advanced RNA technology. The repo is implemented by **Tensorflow 2 with a Keras interface** by the author of this tutorial.\n\nWith this repo, it's now easy to employ pretrained SOTA (ie. top Kaggle models-like) to your own RNA prediction with lots of flexible options!\nIn this tutorial we provide a **\"quick tutorial\"** on how to load and finetune with the pretrained model easily, and on Section 2,\nthere will be a **\"detailed tutorial\"** if you want to adjust your own model and train from scratch with uncertainty-modeling, self-supervised learning, Kfolds, pseudo-labeling and more!\n\n<img src=https://i.ibb.co/r3WB24R/RNA-feature-and-model.png width=\"750\">\n\n# 1. Quick Tutorial\n\nFirst of all, you need to preprocess RNA strings to extract their important features for a DeepRNA model. We prepare the companion [preprocessing tutorial](https://www.kaggle.com/ratthachat/preprocessing-deep-learning-input-from-rna-string) which is a step-by-step and 100% reproducible notebook within Kaggle fixed environment. Once you are done with your RNA feature extraction, by following the above preprocessing tutorial, you can import the preprocessed data into this Kaggle notebook easily by click \"+ Add data\" in the Kaggle notebook editor . Kaggle's free data storage and its capability to transfer any data into any notebooks easily is one of the strongest points of Kaggle working environment versus e.g. Colab.\n\n![](https://i.ibb.co/NVrSKqt/Kaggle-add-data.png)\n\nOnce the data is added, you can investigate your own preprocessed data like this:\n(the data path in Kaggle is sometimes unstable and changed by itself, so we have to make a messy **`if`** message about `INPUT_DIR` below -- nevertheless, this is a minor issue on our tutorial)","metadata":{}},{"cell_type":"code","source":"import os\n\nif os.path.exists('/kaggle/input/k/ratthachat/k/ratthachat/'):\n    INPUT_DIR = '/kaggle/input/k/ratthachat/k/ratthachat/preprocessing-deep-learning-input-from-rna-string/'\nelif os.path.exists('/kaggle/input/k/ratthachat/preprocessing-deep-learning-input-from-rna-string/'):\n    INPUT_DIR = '/kaggle/input/k/ratthachat/preprocessing-deep-learning-input-from-rna-string/'\nelse:\n    INPUT_DIR = '/kaggle/input/preprocessing-deep-learning-input-from-rna-string/'\n\nprint('Preprocessed data path is : ',INPUT_DIR)\n!ls -sh {INPUT_DIR}","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-02-19T14:45:10.301788Z","iopub.execute_input":"2022-02-19T14:45:10.302486Z","iopub.status.idle":"2022-02-19T14:45:10.99186Z","shell.execute_reply.started":"2022-02-19T14:45:10.302398Z","shell.execute_reply":"2022-02-19T14:45:10.991084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"where RNA node features can be extracted from either `node_features.zip` or `advanced_node_features.zip`, and edge features can be extracted from `bpps.zip`.","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Set things up. Extracting data and cloning the repo\n\nHere, we simply unzip the preprocess data and clone the repo to work with this notebook.","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm.notebook import tqdm\nimport gc\nfrom IPython.display import display\nprint('Tensorflow version: ', tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T14:45:10.994056Z","iopub.execute_input":"2022-02-19T14:45:10.994353Z","iopub.status.idle":"2022-02-19T14:45:15.437309Z","shell.execute_reply.started":"2022-02-19T14:45:10.994302Z","shell.execute_reply":"2022-02-19T14:45:15.436561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"WORKING_DIR = '/kaggle/working/'\nNODE_DIR = WORKING_DIR+'advanced_node_features/'\nos.mkdir(NODE_DIR) \n\n!unzip -qq {INPUT_DIR}advanced_node_features.zip -d /\n!ls -h {NODE_DIR} | head\n!ls -h {NODE_DIR} | wc","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-19T14:45:15.438463Z","iopub.execute_input":"2022-02-19T14:45:15.439141Z","iopub.status.idle":"2022-02-19T14:45:17.436536Z","shell.execute_reply.started":"2022-02-19T14:45:15.439092Z","shell.execute_reply":"2022-02-19T14:45:17.435726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EDGE_DIR = WORKING_DIR+'bpps/'\nos.mkdir(EDGE_DIR)\n\n!unzip -qq {INPUT_DIR}bpps.zip -d /\n!ls -h {EDGE_DIR} | head\n!ls -h {EDGE_DIR} | wc","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-19T14:45:17.438701Z","iopub.execute_input":"2022-02-19T14:45:17.438972Z","iopub.status.idle":"2022-02-19T14:45:19.494346Z","shell.execute_reply.started":"2022-02-19T14:45:17.438933Z","shell.execute_reply":"2022-02-19T14:45:19.493553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://ratthachat@github.com/ratthachat/deep-rna.git\n!cp -rf ./deep-rna/deep_rna ./","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-19T14:45:19.498331Z","iopub.execute_input":"2022-02-19T14:45:19.498552Z","iopub.status.idle":"2022-02-19T14:45:21.899005Z","shell.execute_reply.started":"2022-02-19T14:45:19.498525Z","shell.execute_reply":"2022-02-19T14:45:21.898062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Load your data with RNADataset and BatchLoader\n\nNote that in the preprocessing tutorial, once finished, it will store the information of all your RNA strings in the csv file : `most_probable_structure.csv`, and we can easily extract all RNA ids as reference. These RNA ids will also be used as references to extract the correct corresponding features from node_features and edge_features.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(INPUT_DIR+'most_probable_structure.csv')\ndisplay(df.head(5))\n\nrna_id_list = df.id.unique()\nprint(f'We have totally unique {len(rna_id_list)} RNA strings where examples are :', rna_id_list[:5])","metadata":{"execution":{"iopub.status.busy":"2022-02-19T14:45:21.901471Z","iopub.execute_input":"2022-02-19T14:45:21.901907Z","iopub.status.idle":"2022-02-19T14:45:21.938382Z","shell.execute_reply.started":"2022-02-19T14:45:21.901866Z","shell.execute_reply":"2022-02-19T14:45:21.937579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By using `RNADataset`, we can easily load all node and edge features 🔥 !! \nWe can also generate manhattan edge feature for each RNA graph automatically. \nLook at the below example","metadata":{}},{"cell_type":"code","source":"from deep_rna.dataset import RNADataset\n\nrna_dataset = RNADataset(rna_id_list,\n                          node_dir=NODE_DIR,\n                          edge_dir=EDGE_DIR,\n                          manhattan_edge_feature=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T14:45:21.939755Z","iopub.execute_input":"2022-02-19T14:45:21.940016Z","iopub.status.idle":"2022-02-19T14:45:23.071338Z","shell.execute_reply.started":"2022-02-19T14:45:21.939982Z","shell.execute_reply":"2022-02-19T14:45:23.070612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that it's not necessary that each RNA has the same string length, RNADataset will group all of them for you and the BatchLoader (see below) will automatically handle a padding (and masking) for short strings so that model training, finetuning or inferencing can be done seamlessly 💥! \n\nBy the way, if this not the case such as [ones previously used in the competition](https://www.kaggle.com/c/stanford-covid-vaccine/code?competitionId=22111&sortBy=scoreAscending), the user has to handle each RNA-length group separately. This practice is not generalizable to other contexts where we have a lot varying RNA string lengths. To summarize, `DeepRNA` repo solves this problem with `RNADataset` and `BatchLoader` explained below","metadata":{}},{"cell_type":"code","source":"print(rna_dataset,'\\n')\nprint('First RNA:', rna_dataset[0])\nprint('Last RNA:',rna_dataset[-1],': note that n_nodes, ie. length of RNA, can be different from item[0]\\n')\nprint('Node features shape of the first RNA: ', rna_dataset[0].x.shape)\nprint('Edge features shape of the first RNA: ',rna_dataset[0].e.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T14:45:23.072497Z","iopub.execute_input":"2022-02-19T14:45:23.072928Z","iopub.status.idle":"2022-02-19T14:45:23.085612Z","shell.execute_reply.started":"2022-02-19T14:45:23.072887Z","shell.execute_reply":"2022-02-19T14:45:23.08484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To load the dataset in batch for Keras, we will use `BatchLoader` which will handle \n\n* (1) auto-padding for `node_features`, `edge_features` and `labels` for each short RNA in the same batch to have equal length to the longest one in the batch.\n\n* (2) auto-masking so that the padding information will be known to the deep-learning keras model (our `RNAPretrainedModel`)\n\nThese are done automatically by `BatchLoader` and our `RNAPretrainedModel` keras model will handle this dynamic masking loss (varying from each batch) so that the padded nodes will not affect training process under the hood ☄️!","metadata":{}},{"cell_type":"code","source":"from deep_rna.spektral.data import BatchLoader\nbatch_loader = BatchLoader(rna_dataset, batch_size=128, mask=True, shuffle=True, epochs=1) # set epochs=None to load indefinitly","metadata":{"execution":{"iopub.status.busy":"2022-02-19T14:45:23.087236Z","iopub.execute_input":"2022-02-19T14:45:23.088233Z","iopub.status.idle":"2022-02-19T14:45:23.096451Z","shell.execute_reply.started":"2022-02-19T14:45:23.088196Z","shell.execute_reply":"2022-02-19T14:45:23.09564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Load and generate RNA embedding vectors by the pretrained model\nOur `RNAPretrainedModel` has a format very similar to keras pretrained model. \nHere, we load the pretrained weights without the final class layer so that the model can generate **the embedding RNA vector**.\n\nMore details about loading option can be seen in [the docstring](https://github.com/ratthachat/deep-rna/blob/main/deep_rna/models.py#L230). Note that in the pre-processing tutorial we allow users to generate or not **a problem specific features such as \"error-bar of the target\"**. The pretrained model does not use this feature, but in Section 2, the advanced tutorial will use this error-bar features to increase model performance.","metadata":{}},{"cell_type":"code","source":"from deep_rna.models import RNAPretrainedModel\n\nmodel = RNAPretrainedModel(weights='openvaccine', include_top=False)\nprint('The pretrained keras model is : ', model)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T14:45:23.097664Z","iopub.execute_input":"2022-02-19T14:45:23.09841Z","iopub.status.idle":"2022-02-19T14:45:33.182479Z","shell.execute_reply.started":"2022-02-19T14:45:23.098372Z","shell.execute_reply":"2022-02-19T14:45:33.181753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can then use the model to generate the **embedding vector** of each RNA. This embedding vector can be plugged in directly to any machine-learning model e.g. classification, regression or clustering from eg. keras or scikit-learn libraries. For the sake of this quick-tutorial, we use the small extracted dataset containing only 20 RNAs.\n\nIn Section 2, we will show how to extend the `RNADataset` and the `RNAModel` in any general setting easily (whether pretrained or extending the model to your context), ie. we will show how to extend `RNADataset` to extract any types of labels (e.g. AutoEncoder-labels and Pseudo-labels), and extend `RNAModel` to handle complex features such by using **keras subclass model**. We will further train the model from scratch using Kfolds with full dataset (6,000 of RNAs provided by the OpenVaccine dataset).","metadata":{}},{"cell_type":"code","source":"for x in batch_loader.load():\n    print('batch of node features: ', x[0].shape) # batch of node features: (batch, seq_len, node_features)\n    print('batch of edge features: ', x[1].shape) # batch of edge features: (batch, seq_len, seq_len, edge_features)\n    embed = model.predict(x)\n    \n    # keep this in np.array and use any machine-learning algorithms you love :heart:\n    print('batch of embedded RNA features: ',embed.shape) # (batch, seq_len, embedded_RNA_features)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T14:45:33.183936Z","iopub.execute_input":"2022-02-19T14:45:33.184198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To finetune in e.g. 100-class classification problem, we can construct the model easily as shown below. \n\nNote that you can include `class_weight` similar to an argument to standard keras model if you like to have `class_weight` during training. In the finetuning process, we need to tell `RNADataset` where to extract the labels. We illustrate 3 examples of label extraction to show that our framework is applicable to general RNA-prediction problems in the advanced tutorial on Section 2.","metadata":{}},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nmodel = RNAPretrainedModel(weights='openvaccine', include_top=True, n_labels=100, activation='softmax', class_weight=None)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean up before we finish!!\n!rm -rf {NODE_DIR}\n!rm -rf {EDGE_DIR}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Advanced Tutorial : Optimize the Model toward Specific Problem Context\n\nThis tutorial can be used to adapt the RNA model to a general setting of RNA prediction problems, whereas we will use the [OpenVaccine problem](https://www.kaggle.com/c/stanford-covid-vaccine/) (5-class regression prediction) as a case-study. The 5-class regression targets are about the \"degradation\" properties of each individual RNA where you can read [more details here](https://www.kaggle.com/c/stanford-covid-vaccine/data). This problem has many subtle details and hence good to show how to design advanced model in this complex problem. However, the general thinking process of this tutorial should be applicable to general RNA prediction problems. \n \n \n# 2.1 Big Picture of the Tutorial\n \nAccording to [top-solutions SOTA model](https://www.kaggle.com/c/stanford-covid-vaccine/discussion?sort=recent-comments), almost SOTA authors agreed that the [architecture design by MRKMAKR (one of Kaggle's top data scientists)](https://www.kaggle.com/mrkmakr/covid-ae-pretrain-gnn-attn-cnn) is fundamental to their final solutions. The design combines well-known deep-learning sequntial layers (i.e. 1D-Convolution, Self-Attention, Graph-Neural-Network, and Recurrent-Neural-Network layers) in one single model and quite complex. We also want to note that [solution of Gilles Vandewiele et. al.](https://www.kaggle.com/group16/covid-19-mrna-4th-place-solution) heavily influences the whole design of our work.\n\nIn [DeepRNA repo](https://github.com/ratthachat/deep-rna), we simplify the architecture idea while retaining model performance. The main body of DeepRNA model in the repo can be illustrated in the figure below (skip connections exist on every block but not shown):\n \n![](https://i.ibb.co/TmJ2k5S/RNABody-Model.png)\nFigure 2. Main body of DeepRNA architecture of the DeepRNA repo i.e. `RNAPretrainedModel`\n\nThe whole training process uses by SOTA models consist of several trick. This tutorial focus on what we think the most important, namely, \n\n* **Self-supervised pre-training using AutoEncoder** on all labeled and unlabeled data in the above `RNABodyModel`, \n\n* Extracting **pseudo-labels** from unlabeled data and combine them to existing training set, which can be done quite easy in DeepRNA framework\n\n* Incorporating **uncertainty information by using keras model-subclass and dynamic loss function** into the model training.\n\n<!--\nTo summarize, this tutorial will show that the following processes can be easily employed with our DeepRNA repo.\n\n\n\n* Extend RNADataset to extract labels store in a csv in any formats\n\n* Extend SOTA arch to handle error_bar and dynamic padding\n\n* Why and how Auto-encoder : RNADataset and Modeling\n\n* How to incorporate pseudo labeling easily with RNADataset\n\n* simple trick to get gold-level performance (kfolds + ensemble with low-score public + mean correction) -->","metadata":{}},{"cell_type":"markdown","source":"# 2.2 Set Things Up\n\nIn this advanced tutorial, we prepared a separated dataset containing 6,034 RNAs also from the [preprocessing tutorial](https://www.kaggle.com/ratthachat/preprocessing-deep-learning-input-from-rna-string). \n\nAs mentioned in the preprocessing tutorial, users have options to switch on/off some specific features such as \"uncertainty \"error-bar\" of the targets\" feature which is specific to OpenVaccine. If users do not have these features in their own specific RNA prediction problem, `RNAPretrainedModel` shown in Section 1 can be employed directly. However, to show advanced usage here, we choose to extract this error-bar feature and extend our DeepRNA model to `RNAErrorBarModel` using keras model-subclassing.\n\nIn any cases, let us begin by just loading libraries and setting training configs.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport json\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow.keras.layers as L\nimport tensorflow as tf\nfrom tqdm.notebook import tqdm\nimport gc\nfrom IPython.display import display\nprint('Import all basic python libraries with Tensorflow version: ', tf.__version__)\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# General config of the tutorial\nBATCH=64\nEPOCHS=99\n\nPUBLIC_PSEUDO = True # employ pseudo-labeling or not\nAE_TRAIN = True # employ autoencoder-pretraining or not\nN_FOLDS = 10 # KFolds variable, to boost model performance -- 10 folds will take around 2.5 hours\nSEED = 2020\n\n# This targets and class_weight are specific to OpenVaccine, where you have to adjust to your own problem\n# pred_cols tell us the csv columns we are predicting. Don't shuffle its sequence\npred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C']\nclass_weight = [1,1,1,0.25,0.25]\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If the previous tutorial is not yet run, so the repo is not yet cloned, lets do it!","metadata":{}},{"cell_type":"code","source":"if not os.path.exists('./deep-rna/deep_rna'):\n    !git clone https://ratthachat:{secret_value_0}@github.com/ratthachat/deep-rna.git\n    \n    !cp -rf ./deep-rna/deep_rna ./\n    !ls .","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As mentioned, the data imported below are prepared by using [preprocessing tutorial](https://www.kaggle.com/ratthachat/preprocessing-deep-learning-input-from-rna-string) and use Kaggle's Dataset capability to be available to all Kaggle users.","metadata":{}},{"cell_type":"code","source":"# Kaggle dataset unzip the dataset automatically for us, so that the paths are quite messy. \n# Nevertheless, this will not affect the tutorial\nprint('Importing node and edge features directories ...')\nNODE_DIR = '../input/rna-advanced-extracted-features/advanced_node_features/kaggle/working/advanced_node_features/'\nEDGE_DIR = '../input/rna-advanced-extracted-features/bpps/kaggle/working/bpps/'\n!ls {NODE_DIR} | wc\n!ls {EDGE_DIR} | head","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Retrieve Label Information (either real or pseudo labels)\n\nNow this is a problem-specific part. The users need to extract their own target label information. The only requirement is that the label information has to be stored in pandas dataframe in arbitrary format.\n\nIn the original OpenVaccine training data, the degradation 5-class targets were provided for a certain length of RNA bases e.g. 5 targets of 68 bases, leading to a target of shape (68, 5) for each RNA. For 2400 original training data, therefore, the target shape is (2400, 68, 5). The dataframe can store this information in arbitrary format though, for examples\n\n* OpenVaccine's `train.json` divide 5 targets into 5 columns, and each RNA on each row of the dataframe. Therefore, there are 2,400 rows and each element in the dataframe then contain a list of 68 float targets\n\n* OpenVaccine's `submission.csv` (prediction) format, on the other hand, requires each row to be just 1 base of each RNA, so suppose we will submit the prediction of training data, there will be 2400x68 = 163,200 rows with 5 columns.\n\nEach dataframe format can be supplied to `RNADataset` where the users have to implement the method `extract_label` by themselves. In the next subsections, we show 3 examples of `extract_label`.","metadata":{}},{"cell_type":"markdown","source":"First of all, let us load the label information from OpenVaccine. Users can replace this step with their own label information.","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/input/stanford-covid-vaccine/'\ntrain = pd.read_json(data_dir + 'train.json', lines=True)\n\n# This data filtering is also specific to OpenVaccine to eliminate bad-quality data\ntrain = train.query(\"signal_to_noise >= 1\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since `train.json` contains a lot of information, we only retain the label information (only RNA ids and target columns defined by `pred_cols`)","metadata":{}},{"cell_type":"code","source":"label_df = train[['id']+pred_cols]\nprint(label_df.shape)\nlabel_df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will also illustrate how to easily employ pseudo-label information (e.g. use your own best model to predict the label of unlabeled data) in DeepRNA, where the format of this prediction is different from the training data as explained above. We just need to store them in pandas dataframe in any format and we are good to go.","metadata":{}},{"cell_type":"code","source":"pseudolabel_sub_df = pd.read_csv('../input/jung-general-public-dataset/submission_4thplace_openvaccine.csv')\nprint(pseudolabel_sub_df.shape)\npseudolabel_sub_df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.3 Self-supervised AutoEncoder Pretraining\n\nAutoEncoder is one of the simplest self-supervised learning technique where targets that model need to predict is the input, i.e. `node_features`,  itself by using only the information from the embedding vector which is an output of the Body model shown in Figure 2 above.\n\nAs in Quick Tutorial in Section 1, `RNADataset` is the main tool to read the data information. In order to tell `RNADataset` to read the label information correctly, users need to subclass `RNADataset` and define the method `extract_label(self, seq_id, **kwarg)` where `seq_id` is the RNA id need to extract target in the label dataframe that user provided. In this method, users can access `self.label_df` and `kwarg` in order to extract the correct RNA id. To help make AutoEncoder easily, `kwarg[node_features]` of the corresponding `seq_id` is always sent to `extract_label` by default.\n\nTherefore, we can define `RNAAutoEncoderDataset` by the following simple subclass:\n\n```\nclass RNAAutoEncoderDataset(RNADataset):        \n    def extract_label(self, seq_id, **kwarg):\n        return kwarg['node_feature']\n```","metadata":{}},{"cell_type":"markdown","source":"Since AutoEncoder should be very useful in general contexts, we actually already implemented `RNAAutoEncoderDataset` to be ready to use.\nYou can still investigate how we implement by expanding the Jupyter's cell output below","metadata":{}},{"cell_type":"code","source":"from deep_rna.dataset import RNADataset, RNAAutoEncoderDataset\n??RNAAutoEncoderDataset","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Unlabeled Data\nThe unlabeled data we will use for AutoEncoder is the test data itself. \nIn fact, you can use any \"random\" RNA sequences which is unlabeled by definition. These random RNA sequences can be generated as much as we want, and preprocessing the features by the same [preprocessing tutorial](https://www.kaggle.com/ratthachat/preprocessing-deep-learning-input-from-rna-string).\n\nHowever, in OpenVaccine context, we simply use test data which we already had preprocessed features and these features are stored in the same directories as training data. The RNA ids of test data are stored in `test.json`. \n\nIn the case that you use the random generated RNA data, the list of RNA ids are needed to provided as well as directories which store node and edge features of the unlabeled data. ","metadata":{}},{"cell_type":"code","source":"test = pd.read_json(data_dir + 'test.json', lines=True)\n\n# OpenVaccine seperates test data into two set \"public\" and \"private\" with different length\npublic_df = test.query(\"seq_length == 107\")[['id', 'sequence']]\nprivate_df = test.query(\"seq_length == 130\")[['id', 'sequence']]\nprint(public_df.shape, private_df.shape)\npublic_df.head(3)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OpenVaccine seperates test data into two set \"public\" and \"private\" with different length. `RNADataset` can handle all these length-difference with ease.\nSince there is no label dataframe in AutoEncoder setting, we can input anything except `None` value to make `RNADataset` extract the label information.\nHere, we use `label_df = 'dummy'`, but any values can be used.","metadata":{}},{"cell_type":"code","source":"rna_ae_train = RNAAutoEncoderDataset(label_df.id.values,\n                                     node_dir=NODE_DIR,\n                                     edge_dir=EDGE_DIR,\n                                     label_df = 'dummy',\n                                     manhattan_edge_feature=True,\n                                    )\n\n# denote the symbol +=\nrna_ae_train += RNAAutoEncoderDataset(public_df.id.values,\n                                      node_dir=NODE_DIR,\n                                      edge_dir=EDGE_DIR,\n                                      label_df = 'dummy',\n                                      manhattan_edge_feature=True,\n                                     )\nrna_ae_train += RNAAutoEncoderDataset(private_df.id.values,\n                                      node_dir=NODE_DIR,\n                                      edge_dir=EDGE_DIR,\n                                      label_df = 'dummy',\n                                      manhattan_edge_feature=True,)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now just print out to see what's inside\nfor ii in range(3): # skim at the head - RNA with length 107 from train data\n    print(ii, rna_ae_train[ii])\nfor ii in range(-4,-1): # at the tail - RNA with length 130 from private test data\n    print(ii, rna_ae_train[ii])\nprint(rna_ae_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define model that can handle \"uncertain\" information!\n\nAs mentioned in the beginning, in this tutorial we decide to use optional \"error-bar\" features which is specific to OpenVaccine. If this feature is not used, the user can skip the following model subclassing since `RNAPretrainedModel` (see Section 1 - Quick Tutorial) can be used to train/finetune directly after we implement `extract_label` in `RNADataset`. \n\nThe error-bar feature contain \"uncertainty\" of the target labels indicating that the labels themselves are imperfect. The [model that get highest score in OpenVaccine competition i.e. 1st-place solution](https://www.kaggle.com/c/stanford-covid-vaccine/discussion/189620) use this uncertainty to augment data by randomly adjust target labels according to this error-bar. Also, the RNA bases having very high error-bar are \"masked out\" completely(not contribute to loss function).\n\nHere, we use error-bar in a general but simple approach where we simply divide each prediction-loss by its error-bar. By this method, the RNA bases with high error-bar will automatically contribute very minor effect to loss calculation. But to not extremely depend on few bases with very-low error-bar, we reset error-bar to 1.0 for all bases with error-bar less than 5.0. Effectively, the error-bar will affect only bases with considerably high ( > 5.0). This approach is empirically good in our experiment, but readers are free to adjust the method as they see fit.","metadata":{}},{"cell_type":"markdown","source":"The model body illustrated in Subsection 2.1 has no need to be changed. We will only subclass the model's \"prediction head\" i.e. `RNAPredictionModel` to define a new head which handle error-bar in the loss function so that we have the new class `RNAPredictionErrorBarModel`.\n\nThe error_bar input shape is the same as targets shape. Therefore its shape is `(n_rna_data, 5)` since we have 5 classes in OpenVaccine.  For pseudo-labeling target, we simply set error-bar to 1.0, but in fact we can do better by estimating the error-bar from KFolds prediction.\n\nThere are few details in the implementation below which we would like to emphasize here\n\n* By the convention of [preprocessing tutorial](https://www.kaggle.com/ratthachat/preprocessing-deep-learning-input-from-rna-string), this 5 error-bars will stay on the last 5 columns in each `node_features.csv` .\n\n* Note that if we use `RNAPretrainedModel` direcctly here, the AutoEncoder model will has to predict the error-bar feature since it is one of the input features. This does not make sense in our opinion. Even in supervised part, it's quite absurd to use error-bar as model input. Therefore, `RNAPredictionErrorBarModel` will always extract this 5-error-bars off before making inference. See `separate_error_bar()` method in the model. \n\n* These extracted error-bars, instead, will be used directly in loss calculation on training, but will be ignore in AutoEncoder.\n\n* Both loss calculation and `separate_error_bar()` also needs to deal with batch-padding for different RNA lengths. Where `BatchLoader` will add the padding mask as one final feature to `node_features`. See `dynamic_masked_mcrmse()` of how we deal this padding mask in loss calculation.\n","metadata":{}},{"cell_type":"code","source":"from deep_rna.models import RNABodyDeepModel, RNAPredictionModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RNAPredictionErrorBarModel(RNAPredictionModel):\n    ''' This model is optimized in a problem where error of each base label is given\n    e.g. as in OpenVaccine. \n    Here, we don't need to use SN_filter anymore, and dynamic_masked_mcrmse will take \n    into account error-bar automatically\n    \n    n_error_bar = 5 is defined in OpenVaccine problem\n    Note that n_labels may not equal to n_error_bar in cases of AutoEncoderPrediction\n    '''\n    def __init__(self, body_model, n_labels=5, activation='linear', class_weight = None, n_error_bar=5, auto_encoder=False):\n        super().__init__(body_model, n_labels, activation, class_weight)\n\n        self.body_model = body_model\n        self.n_labels = n_labels\n        self.final_dense = L.Dense(n_labels, activation)\n        self.class_weight = class_weight\n        self.n_error_bar = n_error_bar\n        self.err_bar = None\n        self.auto_encoder=auto_encoder\n        self.mask = None\n\n    def dynamic_masked_mcrmse(self,y_true, y_pred):\n\n        # self.mask needs to be dynamically updated for each batch\n        # here, we provide two possible losses\n        def mcrmse(y_true, y_pred):\n            if self.auto_encoder:\n                y_true = y_true[:,:,:-(self.n_error_bar)]\n                loss_square = tf.square(y_true - y_pred)\n            else:\n                loss_square = tf.square(y_true - y_pred)/self.error_bar\n            if self.mask is not None:\n                mask = tf.cast(self.mask,tf.float32)\n                loss_square *= tf.expand_dims(mask,axis=-1)\n            colwise_mse = tf.reduce_mean(loss_square, axis=(0, 1))\n            if self.class_weight is not None:\n                colwise_mse *= self.class_weight\n            \n            mask_shape = tf.shape(mask)\n            padded_total = tf.cast(mask_shape[0]*mask_shape[1], tf.float32)\n            normalized = padded_total/tf.math.reduce_sum(mask)\n\n            # counter-effect the effect of padded-zero making loss function too small\n            return tf.reduce_mean(tf.sqrt(colwise_mse), axis=-1)*normalized\n\n        return mcrmse(y_true, y_pred)\n    \n    def separate_error_bar(self, node_feat):\n        '''By convention, assuming that error_bar are in the idx:(-n_labels-1) to idx:(-2) \n        attributes of node_features\n        \n        Note node_feats are of dim (BATCH, Seq_len, n_features)\n        \n        see code for exact concept\n        \n        '''\n        \n        error_bar = node_feat[:,:,(-self.n_error_bar-1):-1]\n        error_bar = tf.where(error_bar<5.0, 1.0, error_bar)\n        error_bar = tf.cast(error_bar, node_feat.dtype)\n        \n        graph_mask_feat = node_feat[:,:,-1]\n        node_feat_pure = tf.concat([ node_feat[:,:,:(-self.n_error_bar-1)] , graph_mask_feat[...,None]],axis=-1)\n        \n        nf_shape = tf.shape(node_feat)\n        error_bar = tf.ensure_shape(error_bar, [None, None, self.n_error_bar])\n#         node_feat_pure = tf.ensure_shape(node_feat_pure, [nf_shape[0], nf_shape[1], nf_shape[2] - self.n_error_bar])\n        \n        return node_feat_pure, error_bar\n    \n    def call(self, x):\n        self.mask = self.body_model.graphmask.compute_mask(x[0])\n        \n        node_feat, edge_feat = x\n        node_feat_pure, self.error_bar = self.separate_error_bar(node_feat)\n        \n        node_embed = self.body_model([node_feat_pure, edge_feat])\n        out = self.final_dense(node_embed)\n        return out\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After we finish the hard work implementing the new `RNAPredictionErrorBarModel`, we are now ready to do auto-encoding  🔥 🔥 🔥!!!","metadata":{}},{"cell_type":"code","source":"tf.keras.backend.clear_session()\n\nn_edge_features = 5\nbody_model= RNABodyDeepModel(n_edge_features = n_edge_features)\n\nn_error_bar = 5\nae_model= RNAPredictionErrorBarModel(body_model,\n                                     n_labels=rna_ae_train[0].n_labels - n_error_bar,\n                                     class_weight=None,\n                                     n_error_bar=n_error_bar,\n                                     auto_encoder=True) # extract err_bar out, but not use it in AutoEncoderTrain since autoencoder's label is 27 but n_err_bar is 5\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from deep_rna.spektral.data import BatchLoader\nloader_ae = BatchLoader(rna_ae_train, batch_size=BATCH, mask=True, shuffle=True, epochs=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ae_model.compile(tf.optimizers.Adam(), \n              loss=ae_model.dynamic_masked_mcrmse\n             )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if AE_TRAIN:\n    epochs_ae = EPOCHS//3\nelse:\n    epochs_ae = 1\n\nhistory = ae_model.fit(loader_ae.load(), \n                    steps_per_epoch=loader_ae.steps_per_epoch, \n                    epochs=epochs_ae,\n                   )\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ae_model.summary()\nae_model.body_model.save_weights('body_ae_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.4 Supervised KFolds Learning with Pseudo-Labeled Data\n\nOk now we are ready to do a conventional supervised learning. Now we have to tell `RNADataset` how to extract the label information on training data.\nWith OpenVaccine label format, we can implement `extraction_label` as shown below.","metadata":{}},{"cell_type":"code","source":"class RNATrainDataset(RNADataset):\n    def extract_label(self, seq_id, **kwarg):\n        id_df = self.label_df[self.label_df.id == seq_id]\n        col_label_list = []\n        for col in self.pred_cols:\n            col_label = np.array(id_df[col].values[0]) # complication of Kaggle's label dataframe\n            col_label_list.append(col_label)\n\n        return np.array(col_label_list).T\n\nrna_train = RNATrainDataset(label_df.id.values, \n                            node_dir=NODE_DIR, \n                            edge_dir=EDGE_DIR, \n                            pred_len=68, \n                            label_df=label_df, \n                            manhattan_edge_feature=True,\n                            pred_cols=pred_cols\n                           )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similarly, we implement `extraction_label` for pseudo-label dataset which we already implement it in DeepRNA repo. Readers can see how we implement it by expand the cell's output below.\n\n```\nclass RNAOpenVaccinePsuedoDataset(RNADataset):        \n    def extract_label(self, seq_id, **kwarg):\n        return extract_seq_pseudo_label_from_submission(submission_df=self.label_df, rna_seq_id=seq_id)\n```","metadata":{}},{"cell_type":"code","source":"from deep_rna.dataset import RNAOpenVaccinePsuedoDataset\nfrom deep_rna.openvaccine_utils import extract_seq_pseudo_label_from_submission\n\n??extract_seq_pseudo_label_from_submission # expand the output cell to see implementation","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rna_public_pseudo = RNAOpenVaccinePsuedoDataset(public_df.id.values, \n                                                node_dir=NODE_DIR, \n                                                edge_dir=EDGE_DIR, \n                                                pred_len=107, \n                                                manhattan_edge_feature=True,\n                                                label_df = pseudolabel_sub_df,\n                                                pred_cols=pred_cols\n                                               ) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PUBLIC_PSEUDO: # we can combine both datasets easily with RNADataset class!\n    rna_train.graphs = rna_train.graphs + rna_public_pseudo.graphs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I think we finished all the hard parts!! Congratulation! ☄️☄️☄️\n\nAll the rest is the standard KFolds pipeline plus a few tricks to make this tutorial get good score on the OpenVaccine metric ;)\n\nNote that we use only \"public-test-data\" pseudo labels and not private data pseudo label. \nAlthough each session run a bit differently, we expect the public test score to exceed the pseudo-score itself and make a new-high scoring (public) notebook.\n\nThe other few extra tricks are needed for good private score. If anyone interested, are explained in the next final subsection","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport numpy as np\n\nkf = KFold(n_splits=N_FOLDS, random_state=SEED, shuffle=True)\nfor fold, (tr_idx, val_idx) in enumerate(kf.split(np.arange(len(rna_train)))):\n    \n    gc.collect()\n    tf.keras.backend.clear_session()\n    \n    print(f'\\nFold - {fold}\\n')\n    \n    loader_train = BatchLoader(rna_train[tr_idx], batch_size=BATCH, mask=True, shuffle=True, epochs=None)\n    loader_val = BatchLoader(rna_train[val_idx], batch_size=BATCH, mask=True, shuffle=False, epochs=None)\n    \n    body = RNABodyDeepModel(n_edge_features = n_edge_features)\n    for x in loader_val.load():\n        node_feat_pure, error_Bar = ae_model.separate_error_bar(x[0][0]) # x[0] = [node_feat, edge_feat], x[1] = y\n        _ = body([node_feat_pure, x[0][1]]) # initiating model.build()\n        break\n    body.load_weights('body_ae_model.h5')\n    \n    model = RNAPredictionErrorBarModel(body, n_labels=5, \n                                   class_weight=class_weight,\n                                   n_error_bar=5,\n                                   auto_encoder=False)\n    \n    model.compile(tf.optimizers.Adam(), loss=model.dynamic_masked_mcrmse)\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n                                filepath=f'./model_{fold}_best.h5',save_weights_only=True,\n                                monitor='val_loss',mode='min',save_best_only=True)\n\n    history = model.fit(loader_train.load(), \n                    steps_per_epoch=loader_train.steps_per_epoch, \n                    epochs=EPOCHS,\n                    validation_data=loader_val.load(),\n                    validation_steps=loader_val.steps_per_epoch,\n                    callbacks=[model_checkpoint_callback]\n                   )\n    \n    model.save_weights(f'./model_{fold}_last.h5')","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.5 Make predictions for OpenVaccien problem\n\nThis is specific to OpenVaccine where will convert prediction into submission format and send to Kaggle server to measure the performance of our model's prediction.\nUsers may not need to read this section and proceed their own ways of deployment. Good luck!!","metadata":{}},{"cell_type":"code","source":"# Load Kaggle submission format\nsample_df = pd.read_csv(data_dir + 'sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we don't need label_df on inference process\nrna_public = RNATrainDataset(public_df.id.values, NODE_DIR, EDGE_DIR, pred_len=107, \n                              manhattan_edge_feature=True, label_df=None)\n\nrna_private = RNATrainDataset(private_df.id.values, NODE_DIR, EDGE_DIR, pred_len=130, \n                               manhattan_edge_feature=True, label_df=None)\nrna_public, rna_private","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Uncomment this cell if want to inference using last_trained weights instead of best_val_weights\n\n# public_preds_list = []\n# private_preds_list = []\n\n# for fold in range(N_FOLDS):\n#     gc.collect()\n#     model.load_weights(f'./model_{fold}_last.h5')\n    \n#     public_loader = BatchLoader(rna_public, batch_size=BATCH, epochs=1, shuffle=False, mask=True) \n#     public_preds = []\n#     for i, x in enumerate(public_loader.load()):\n#         public_preds.append(model.predict((x[0], x[1]),verbose=1))\n#     public_preds = np.vstack(public_preds)\n#     public_preds_list.append(public_preds)\n    \n#     private_loader = BatchLoader(rna_private, batch_size=BATCH, epochs=1, shuffle=False, mask=True)\n#     private_preds = []\n#     for i, x in enumerate(private_loader.load()):\n#         private_preds.append(model.predict((x[0], x[1]),verbose=1))\n\n#     private_preds = np.vstack(private_preds)\n#     private_preds_list.append(private_preds)\n    \n# public_preds = np.mean(public_preds_list, axis=0)\n# private_preds = np.mean(private_preds_list, axis=0)\n\n# public_preds_best = public_preds.copy()\n# private_preds_best = private_preds.copy()\n\n# preds_ls = []\n# gc.collect()\n# for df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n#     for i, uid in enumerate(df.id):\n#         single_pred = preds[i]\n\n#         single_df = pd.DataFrame(single_pred, columns=pred_cols)\n#         single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n#         preds_ls.append(single_df)\n\n# preds_df = pd.concat(preds_ls)\n# preds_df.head()\n\n# submission = sample_df[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\n# submission.to_csv('submission.csv', index=False)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"public_preds_list = []\nprivate_preds_list = []\n\nfor fold in range(N_FOLDS):\n    gc.collect()\n    model.load_weights(f'./model_{fold}_best.h5')\n    \n    public_loader = BatchLoader(rna_public, batch_size=BATCH, epochs=1, shuffle=False, mask=True) \n    public_preds = []\n    for i, x in enumerate(public_loader.load()):\n        public_preds.append(model.predict((x[0], x[1]),verbose=1))\n    public_preds = np.vstack(public_preds)\n    public_preds_list.append(public_preds)\n    \n    private_loader = BatchLoader(rna_private, batch_size=BATCH, epochs=1, shuffle=False, mask=True)\n    private_preds = []\n    for i, x in enumerate(private_loader.load()):\n        private_preds.append(model.predict((x[0], x[1]),verbose=1))\n\n    private_preds = np.vstack(private_preds)\n    private_preds_list.append(private_preds)\n    \npublic_preds = np.mean(public_preds_list, axis=0)\nprivate_preds = np.mean(private_preds_list, axis=0)\n\npublic_preds_std = np.std(public_preds_list, axis=0)\nprivate_preds_std = np.std(private_preds_list, axis=0)\nnp.save('public_preds_std.npy',public_preds_std)\nnp.save('private_preds_std.npy',private_preds_std)\n\npreds_ls = []\ngc.collect()\nfor df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_ls.append(single_df)\n\npreds_df = pd.concat(preds_ls)\npreds_df.head()\n\nsubmission = sample_df[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\nsubmission.to_csv('submission_best_val.csv', index=False)","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # uncomment this cell, if want to use 'snap-shot' KFolds ensemble \n\n# public_preds = np.mean([public_preds_best, public_preds], axis=0)\n# private_preds = np.mean([private_preds_best, private_preds], axis=0)\n\n# preds_ls = []\n# gc.collect()\n# for df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n#     for i, uid in enumerate(df.id):\n#         single_pred = preds[i]\n\n#         single_df = pd.DataFrame(single_pred, columns=pred_cols)\n#         single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n#         preds_ls.append(single_df)\n\n# preds_df = pd.concat(preds_ls)\n# preds_df.head()\n\n# submission = sample_df[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\n# submission.to_csv('submission_ensemble.csv', index=False)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Few trick to make good score\n\n* The first trick is a simple ensemble with a \"poor-man\" submission. We will make a simple ensemble with my own previous \"poor\" submission of this competition 2 years ago, which not even get a bronze medal. This sub contains mostly public notebooks ensemble, so it's no cheating or anything :)\n\n* Next, below we show that the mean-prediction-magnitude of private test data is significantly less than those of public test data where public and train data are RNA of the same target length (68), whereas private test data are longer target length (91). This somehow indicates magnitude mismatch and we simply fix by a factor of 1.1 which is reasonable from the mean-mismatched shown below\n\nWith 10-folds training, with a simple ensemble plus a trick to normalize the mean-magnitude of private-prediction should make this notebook private's score near gold zone. (at least in the test run of this notebook ;)","metadata":{}},{"cell_type":"code","source":"# analyze mean on each cols of pub/priv\npublic_mean_cols = np.mean(public_preds, axis=(0,1))\nprivate_mean_cols = np.mean(private_preds, axis=(0,1))\ncorrection_cols = public_mean_cols / private_mean_cols\nprint('mean mismatched among 5 columns are : ', correction_cols)\n\nprivate_preds_correction = private_preds*1.10\n\npreds_ls = []\ngc.collect()\nfor df, preds in [(public_df, public_preds), (private_df, private_preds_correction)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_ls.append(single_df)\n\npreds_df = pd.concat(preds_ls)\npreds_df.head()\n\nsubmission = sample_df[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\nsubmission.to_csv('submission_best_val_correction.csv', index=False)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# simple ensemble, with also fix mean-correction in my own 'poor' submission\n# find private rows first\ngc.collect()\nsub_base = pd.read_csv('submission_best_val.csv')\nsub_correction = pd.read_csv('submission_best_val_correction.csv')\ncols = pred_cols\n\npos_priv = np.where(sub_base[cols[0]].values != sub_correction[cols[0]].values)[0]\npos_pub = np.where(sub_base[cols[0]].values == sub_correction[cols[0]].values)[0]\npos_priv.shape, pos_pub.shape, len(pos_priv)/len(pos_pub), 3000/600","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correct the mean in private rows\nPATH = \"../input/jung-general-public-dataset/\"\npoor_sub = pd.read_csv(PATH+'submission_353_public_notebooks_openvaccine.csv')\npoor_sub = sample_df[['id_seqpos']].merge(poor_sub, on=['id_seqpos'])\n\nsub_pub = poor_sub.loc[pos_pub,:]\nsub_priv = poor_sub.loc[pos_priv,:]\nprint(sub_pub.shape, sub_priv.shape)\n\nsub_priv110 = sub_priv.copy()\nsub_priv110[cols] *= 1.10\n\npoor_sub = pd.concat([sub_priv110, sub_pub]) # reborn :)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# simple ensemble\n\ngc.collect()\nsub = poor_sub.copy()\nsub[cols] = (sub_correction[cols] +  poor_sub[cols])/2\n\ndisplay(sub.head())\nsub.to_csv('submission_final.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sub.shape)\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert len(sub) == 457953","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n!rm -rf /kaggle/working/deep-rna\n!rm -rf /kaggle/working/deep_rna","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}