{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nimport tensorflow.keras.layers as L\nimport tensorflow as tf\nimport plotly.express as px\nfrom sklearn.preprocessing import quantile_transform,StandardScaler,MinMaxScaler\nfrom transformers import BertConfig,TFBertModel,BertModel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define helper functions and useful vars"},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This will tell us the columns we are predicting\npred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = BertConfig() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config.num_attention_heads","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def MCRMSE(y_true, y_pred):\n    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n\ndef gru_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.GRU(hidden_dim, dropout=dropout, return_sequences=True))\n\ndef build_model(seq_len=107, pred_len=68, dropout=0.5, embed_dim=100, hidden_dim=128):\n    ids = L.Input(shape=(seq_len,3), dtype=tf.int32)\n    flat = L.Flatten()(ids)\n    config = BertConfig() \n    config.vocab_size = 7\n    config.num_hidden_layers = 3\n    config.num_attention_heads = 1\n    config.attention_probs_dropout_prob  = 0.5\n    config.hidden_size = 120\n    config.hidden_act= tf.sinh #tf.tanh\n    bert_model = TFBertModel(config=config)\n\n    bert_embeddings = bert_model(flat)[0]\n\n    hidden = L.AveragePooling1D(pool_size=2)(bert_embeddings)\n    # Since we are only making predictions on the first part of each sequence, we have\n    # to truncate it\n    truncated = hidden[:,:pred_len, :]\n    out = L.Dense(5, activation='linear')(truncated)\n\n    model = tf.keras.Model(inputs=ids, outputs=out)\n\n    model.compile(tf.keras.optimizers.Adam(), loss=MCRMSE)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = {\n    'sequence': {x:i for i, x in enumerate(\"A C G U\".split())},\n    'structure': {x:i for i, x in enumerate(\"( . )\".split())},\n    'predicted_loop_type': {x:i for i, x in enumerate(\"B E H I M S X\".split())},\n}\ndef preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    \n    def f(x):\n        return [vocab['sequence'][x] for x in x[0]],\\\n                [vocab['structure'][x] for x in x[1]],\\\n                [vocab['predicted_loop_type'][x] for x in x[2]],\n\n    return np.array(\n            df[cols]\n            .apply(f, axis=1)\n            .values\n            .tolist()\n        )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load and preprocess data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_json('/kaggle/input/stanford-covid-vaccine/train.json', lines=True)\ntest = pd.read_json('/kaggle/input/stanford-covid-vaccine/test.json', lines=True)\nsample_df = pd.read_csv('/kaggle/input/stanford-covid-vaccine/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.Series(list(train['structure'][0])).value_counts())\nprint(pd.Series(list(train['sequence'][0])).value_counts())\nprint(pd.Series(list(train['predicted_loop_type'][0])).value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(train['signal_to_noise'].apply(np.round).astype(int).unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.bincount(train['signal_to_noise'].apply(np.round).astype(int))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(train['SN_filter'].apply(np.round).astype(int).unique()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.bincount(train['SN_filter'].apply(np.round).astype(int))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.query(\"signal_to_noise >= 4\")\ntrain_inputs = preprocess_inputs(train)\ntrain_labels = np.array(train[pred_cols].values.tolist()).transpose((0, 2, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_inputs.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [train,test]:\n    df['Paired']=[sum([i=='(' or i==')' for i in j]) for j in df['structure']]\n    df['Unpaired']=[sum([i=='.' for i in j]) for j in df['structure']]\n    for col in ['E','S','H','I','G','A','U']:\n        if col in ['E','S','H','I']:\n            df[col]=[sum([i==col for i in j])/len(j) for j in df['predicted_loop_type']]\n        else:\n            df[col]=[sum([i==col for i in j])/len(j) for j in df['sequence']]\nfor a in [ 'G', 'A', 'C', 'U']:\n    train[a+'_position']=[np.sum([i for i in range(len(j)) if j[i]==a])/len([i for i in range(len(j)) if j[i]==a]) for j in train['sequence']]\n    test[a+'_position']=[np.sum([i for i in range(len(j)) if j[i]==a])/len([i for i in range(len(j)) if j[i]==a]) for j in test['sequence']]\nfor a in [ 'E', 'S', 'H',]:\n    train[a+'_position']=[np.sum([i for i in range(len(j)) if j[i]==a])/len([i for i in range(len(j)) if j[i]==a]) for j in train['predicted_loop_type']]\n    test[a+'_position']=[np.sum([i for i in range(len(j)) if j[i]==a])/len([i for i in range(len(j)) if j[i]==a]) for j in test['predicted_loop_type']]\nfor a in [ 'E', 'S', 'H',]:\n    train[a+'']=[np.sum([i for i in range(len(j)) if j[i]==a])/len([i for i in range(len(j)) if j[i]==a]) for j in train['predicted_loop_type']]\n    test[a+'_position']=[np.sum([i for i in range(len(j)) if j[i]==a])/len([i for i in range(len(j)) if j[i]==a]) for j in test['predicted_loop_type']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_columns = ['reactivity', 'deg_Mg_pH10','deg_pH10', 'deg_Mg_50C', 'deg_50C']\ntarget_columns.extend(['SN_filter', 'signal_to_noise'])\ntarget_columns.extend(['deg_error_pH10', 'deg_error_Mg_50C', 'deg_error_50C', 'reactivity_error', 'deg_error_Mg_pH10'] )\ntrain.drop(target_columns,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SC = MinMaxScaler(feature_range=(-1, 1))\ntrain_measurements = SC.fit_transform(pd.concat((train.select_dtypes('float64'),train.select_dtypes('int64')),axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_measurements.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.min(train_measurements),np.max(train_measurements)\n# np.min(test_measurements),np.max(test_measurements)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(train_measurements).describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build and train model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_inputs.shape,train_labels.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"public_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\npublic_test_measurements  = SC.fit_transform(pd.concat((public_df.select_dtypes('float64'),public_df.select_dtypes('int64')),axis=1))\nprivate_test_measurements  = SC.fit_transform(pd.concat((private_df.select_dtypes('float64'),private_df.select_dtypes('int64')),axis=1))\npublic_inputs = preprocess_inputs(public_df)\nprivate_inputs = preprocess_inputs(private_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Public and private sets have different sequence lengths, so we will preprocess them separately and load models of different tensor shapes."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=5,shuffle=True,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# with tf.device('/gpu'):\nwith strategy.scope():\n    model = build_model()\n    for fold,(idxT,idxV) in enumerate(kf.split(train_inputs)):\n        history = model.fit(\n            train_inputs[idxT,:,:], train_labels[idxT,:,:], \n            batch_size=64,\n            epochs=100,\n            validation_split=0.05,\n                callbacks=[\n            tf.keras.callbacks.ReduceLROnPlateau(),\n            tf.keras.callbacks.ModelCheckpoint('model'+str(fold)+'.h5',save_weights_only=True,save_best_only=True)\n        ]\n        )\n        # Caveat: The prediction format requires the output to be the same length as the input,\n        # although it's not the case for the training data.\n        model_short = build_model(seq_len=107, pred_len=107)\n        model_long = build_model(seq_len=130, pred_len=130)\n\n        model_short.load_weights('model'+str(fold)+'.h5')\n        model_long.load_weights('model'+str(fold)+'.h5')\n        \n        if fold == 0:\n            public_preds = model_short.predict([public_inputs])/5\n            private_preds = model_long.predict([private_inputs])/5\n        else:\n            public_preds += model_short.predict([public_inputs])/5\n            private_preds += model_long.predict([private_inputs])/5\n            \n        fig = px.line(\n        history.history, y=['loss', 'val_loss'], \n        labels={'index': 'epoch', 'value': 'Mean Squared Error'}, \n        title='Training History')\n        fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bert's a bitch with weights\n# model.save_weights('model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(public_preds.shape, private_preds.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Post-processing and submit"},{"metadata":{},"cell_type":"markdown","source":"For each sample, we take the predicted tensors of shape (107, 5) or (130, 5), and convert them to the long format (i.e. $629 \\times 107, 5$ or $3005 \\times 130, 5$):"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_ls = []\n\nfor df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_ls.append(single_df)\n\npreds_df = pd.concat(preds_ls)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = sample_df[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}