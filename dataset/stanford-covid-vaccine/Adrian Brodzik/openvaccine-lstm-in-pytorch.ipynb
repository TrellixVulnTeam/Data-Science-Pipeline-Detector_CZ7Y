{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm.notebook import tqdm\nimport itertools\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token2int = {x: i for i, x in enumerate(\"().AGUCSMIBHEX \")}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_json(\"../input/stanford-covid-vaccine/train.json\", lines=True)\ntest = pd.read_json(\"../input/stanford-covid-vaccine/test.json\", lines=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = np.array(train[\"sequence\"].apply(lambda seq: [token2int[x] for x in seq]).values.tolist())\nkmeans = KMeans(n_clusters=200, random_state=42).fit(temp)\ntrain[\"cluster\"] = kmeans.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_bpps_sum(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"../input/stanford-covid-vaccine/bpps/{mol_id}.npy\").max(axis=1))\n    return bpps_arr\n\n\ndef read_bpps_max(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"../input/stanford-covid-vaccine/bpps/{mol_id}.npy\").sum(axis=1))\n    return bpps_arr\n\n\ndef read_bpps_nb(df):\n    bpps_nb_mean = 0.077522\n    bpps_nb_std = 0.08914\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps = np.load(f\"../input/stanford-covid-vaccine/bpps/{mol_id}.npy\")\n        bpps_nb = (bpps > 0).sum(axis=0) / bpps.shape[0]\n        bpps_nb = (bpps_nb - bpps_nb_mean) / bpps_nb_std\n        bpps_arr.append(bpps_nb)\n    return bpps_arr\n\n\ntrain[\"bpps_sum\"] = read_bpps_sum(train)\ntrain[\"bpps_max\"] = read_bpps_max(train)\ntrain[\"bpps_nb\"] = read_bpps_nb(train)\n\ntest[\"bpps_sum\"] = read_bpps_sum(test)\ntest[\"bpps_max\"] = read_bpps_max(test)\ntest[\"bpps_nb\"] = read_bpps_nb(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = [\"reactivity\", \"deg_Mg_pH10\", \"deg_pH10\", \"deg_Mg_50C\", \"deg_50C\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"seq_masked_1\"] = train.apply(lambda x: \"\".join([y if x.structure[i] == \".\" else \" \" for i, y in enumerate(x.sequence)]), axis=1)\ntrain[\"seq_masked_2\"] = train.apply(lambda x: \"\".join([y if x.structure[i] == \"(\" else \" \" for i, y in enumerate(x.sequence)]), axis=1)\ntrain[\"seq_masked_3\"] = train.apply(lambda x: \"\".join([y if x.structure[i] == \")\" else \" \" for i, y in enumerate(x.sequence)]), axis=1)\n\ntest[\"seq_masked_1\"] = test.apply(lambda x: \"\".join([y if x.structure[i] == \".\" else \" \" for i, y in enumerate(x.sequence)]), axis=1)\ntest[\"seq_masked_2\"] = test.apply(lambda x: \"\".join([y if x.structure[i] == \"(\" else \" \" for i, y in enumerate(x.sequence)]), axis=1)\ntest[\"seq_masked_3\"] = test.apply(lambda x: \"\".join([y if x.structure[i] == \")\" else \" \" for i, y in enumerate(x.sequence)]), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, df, mode=\"train\"):\n        self.df = df.reset_index(drop=True)\n        self.mode = mode\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        x1 = np.array(self.df.loc[idx, [\"sequence\", \"structure\", \"predicted_loop_type\", \"seq_masked_1\", \"seq_masked_2\", \"seq_masked_3\"]].apply(lambda seq: [token2int[x] for x in seq]).values.tolist()).transpose(1, 0)\n        x2 = np.array(self.df.loc[idx, [\"bpps_sum\", \"bpps_max\", \"bpps_nb\"]].values.tolist()).transpose(1, 0)\n\n        if self.mode == \"train\":\n            return x1, x2, np.array(self.df.loc[idx, targets].values.tolist()).transpose(1, 0)\n        elif self.mode == \"test\":\n            return x1, x2\n        else:\n            return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyModel(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n\n        self.embedding = nn.Embedding(num_embeddings=args[\"num_embeddings\"], embedding_dim=args[\"embedding_dim\"])\n        self.lstm = nn.LSTM(input_size=args[\"in_features\"] * args[\"embedding_dim\"] + 3, hidden_size=args[\"lstm_hidden\"], num_layers=args[\"lstm_layers\"], bias=True, batch_first=True, dropout=args[\"lstm_dropout\"], bidirectional=True)\n        self.fc = nn.Linear(2 * args[\"lstm_hidden\"], args[\"out_features\"])\n\n    def forward(self, x1, x2):\n        x1 = self.embedding(x1.long()).view(x1.shape[0], x1.shape[1], -1)\n        x = torch.cat((x1, x2), dim=2)\n        x = self.lstm(x)[0][:, :x1.shape[1]]\n        x = self.fc(x)\n\n        return x\n\n    def count_parameters(self):\n        return sum(p.numel() for p in self.parameters() if p.requires_grad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lr(optimizer):\n    for p in optimizer.param_groups:\n        return p[\"lr\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def MCRMSE(y_pred, y_true):\n    colwise_mse = torch.mean((y_pred - y_true)**2, axis=1)\n    return torch.mean(torch.sqrt(colwise_mse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_MODE = False\n\nargs = {\n    \"batch_size\": 228,\n    \"device\": torch.device(\"cuda\"),\n    \"dtype\": torch.float,\n    \"embedding_dim\": 41,\n    \"epochs\": 100,\n    \"factor\": 0.40876664932540246,\n    \"folds\": 5,\n    \"in_features\": 6,\n    \"lr\": 0.000514670300651814,\n    \"lstm_dropout\": 0.3,\n    \"lstm_hidden\": 765,\n    \"lstm_layers\": 2,\n    \"min_lr\": 1e-8,\n    \"num_embeddings\": len(token2int),\n    \"out_features\": 5,\n    \"patience\": 10,\n    \"seed\": 42,\n    \"weight_decay\": 0.23884714200145296,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TRAIN_MODE:\n    seed_everything(args[\"seed\"])\n\n    kfold = GroupKFold(n_splits=args[\"folds\"])\n\n    for fold, (train_idx, dev_idx) in enumerate(kfold.split(train, groups=train[\"cluster\"])):\n        train_loader = DataLoader(MyDataset(train.iloc[train_idx]), batch_size=args[\"batch_size\"], shuffle=True, num_workers=3, pin_memory=True)\n        dev_loader = DataLoader(MyDataset(train.iloc[dev_idx]), batch_size=args[\"batch_size\"], shuffle=False, num_workers=3, pin_memory=True)\n\n        model = MyModel(args).to(device=args[\"device\"], dtype=args[\"dtype\"])\n\n        optimizer = optim.AdamW(model.parameters(), lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n        scheduler = lr.ReduceLROnPlateau(optimizer, mode=\"min\", factor=args[\"factor\"], patience=args[\"patience\"])\n\n        pbar = tqdm(range(args[\"epochs\"]))\n\n        best_loss = np.inf\n\n        for epoch in pbar:\n            model.train()\n\n            for x1, x2, y in train_loader:\n                x1 = x1.to(device=args[\"device\"], dtype=torch.long)\n                x2 = x2.to(device=args[\"device\"], dtype=args[\"dtype\"])\n                y = y.to(device=args[\"device\"], dtype=args[\"dtype\"])\n\n                y_pred = model(x1, x2)[:, :y.shape[1]]\n                loss = MCRMSE(y_pred, y)\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            model.eval()\n\n            train_loss = 0\n            dev_loss = 0\n\n            with torch.no_grad():\n                for batch, (x1, x2, y) in enumerate(train_loader):\n                    x1 = x1.to(device=args[\"device\"], dtype=torch.long)\n                    x2 = x2.to(device=args[\"device\"], dtype=args[\"dtype\"])\n                    y = y.to(device=args[\"device\"], dtype=args[\"dtype\"])\n\n                    y_pred = model(x1, x2)[:, :y.shape[1]]\n                    loss = MCRMSE(y_pred, y)\n\n                    train_loss = (batch * train_loss + loss.item()) / (batch + 1)\n\n                for batch, (x1, x2, y) in enumerate(dev_loader):\n                    x1 = x1.to(device=args[\"device\"], dtype=torch.long)\n                    x2 = x2.to(device=args[\"device\"], dtype=args[\"dtype\"])\n                    y = y.to(device=args[\"device\"], dtype=args[\"dtype\"])\n\n                    y_pred = model(x1, x2)[:, :y.shape[1]]\n                    loss = MCRMSE(y_pred, y)\n\n                    dev_loss = (batch * dev_loss + loss.item()) / (batch + 1)\n\n            scheduler.step(dev_loss)\n\n            pbar.set_description(\"train: {:.5f} dev: {:.5f}\".format(train_loss, dev_loss))\n\n            if dev_loss < best_loss:\n                best_loss = dev_loss\n                torch.save(model.state_dict(), \"model_fold_{}.pth\".format(fold))\n\n            if get_lr(optimizer) < args[\"min_lr\"]:\n                break\n\n        print(best_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"public_test = test[test[\"seq_length\"] == 107].copy()\nprivate_test = test[test[\"seq_length\"] == 130].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"public_preds = np.zeros((len(public_test), 107, len(targets)))\nprivate_preds = np.zeros((len(private_test), 130, len(targets)))\n\nfor fold in range(args[\"folds\"]):\n    model = MyModel(args).to(device=args[\"device\"], dtype=args[\"dtype\"])\n    if TRAIN_MODE:\n        model.load_state_dict(torch.load(\"model_fold_{}.pth\".format(fold)))\n    else:\n        model.load_state_dict(torch.load(\"../input/openvaccine-trained/model_fold_{}.pth\".format(fold)))\n    model = model.eval()\n\n    public_test_loader = DataLoader(MyDataset(public_test, mode=\"test\"), batch_size=args[\"batch_size\"], shuffle=False, num_workers=3, pin_memory=True)\n    private_test_loader = DataLoader(MyDataset(private_test, mode=\"test\"), batch_size=args[\"batch_size\"], shuffle=False, num_workers=3, pin_memory=True)\n\n    with torch.no_grad():\n        public_fold_preds = []\n        \n        for batch, (x1, x2) in enumerate(public_test_loader):\n            x1 = x1.to(device=args[\"device\"], dtype=torch.long)\n            x2 = x2.to(device=args[\"device\"], dtype=torch.long)\n            y_pred = model(x1, x2).cpu().detach().numpy()\n            public_fold_preds.extend(y_pred)\n\n        public_preds += np.array(public_fold_preds) / args[\"folds\"]\n\n        private_fold_preds = []\n\n        for batch, (x1, x2) in enumerate(private_test_loader):\n            x1 = x1.to(device=args[\"device\"], dtype=torch.long)\n            x2 = x2.to(device=args[\"device\"], dtype=torch.long)\n            y_pred = model(x1, x2).cpu().detach().numpy()\n            private_fold_preds.extend(y_pred)\n\n        private_preds += np.array(private_fold_preds) / args[\"folds\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = []\n\nfor df, p in [(public_test, public_preds), (private_test, private_preds)]:\n    for i, uid in enumerate(df.id):\n        single_pred = p[i]\n\n        single_df = pd.DataFrame(single_pred, columns=targets)\n        single_df[\"id_seqpos\"] = [f\"{uid}_{x}\" for x in range(single_df.shape[0])]\n\n        preds.append(single_df)\n\npreds = pd.concat(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds[[\"id_seqpos\"] + targets].to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}