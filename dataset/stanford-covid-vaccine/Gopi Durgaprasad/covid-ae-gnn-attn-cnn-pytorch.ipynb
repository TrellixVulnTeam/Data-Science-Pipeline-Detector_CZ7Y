{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip -q install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git >> quit","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os, random, glob, gc, json, time\nimport numpy as np, pandas as pd\n\nimport torch, torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\nfrom sklearn.model_selection import KFold, StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\ntrain = pd.read_json(\"/kaggle/input/stanford-covid-vaccine/train.json\",lines=True)\ntrain = train[train.signal_to_noise > 1].reset_index(drop = True)\n\ntest  = pd.read_json(\"/kaggle/input/stanford-covid-vaccine/test.json\",lines=True)\ntest_pub = test[test[\"seq_length\"] == 107]\ntest_pri = test[test[\"seq_length\"] == 130]\nsub = pd.read_csv(\"/kaggle/input/stanford-covid-vaccine/sample_submission.csv\")\n\n\nAs = []\nfor id in tqdm(train[\"id\"]):\n    a = np.load(f\"/kaggle/input/stanford-covid-vaccine/bpps/{id}.npy\")\n    As.append(a)\nAs = np.array(As)\nAs_pub = []\nfor id in tqdm(test_pub[\"id\"]):\n    a = np.load(f\"/kaggle/input/stanford-covid-vaccine/bpps/{id}.npy\")\n    As_pub.append(a)\nAs_pub = np.array(As_pub)\nAs_pri = []\nfor id in tqdm(test_pri[\"id\"]):\n    a = np.load(f\"/kaggle/input/stanford-covid-vaccine/bpps/{id}.npy\")\n    As_pri.append(a)\nAs_pri = np.array(As_pri)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = list(sub.columns[1:])\nprint(targets)\n\ny_train = []\nseq_len = train[\"seq_length\"].iloc[0]\nseq_len_target = train[\"seq_scored\"].iloc[0]\nignore = -10000\nignore_length = seq_len - seq_len_target\nfor target in targets:\n    y = np.vstack(train[target])\n    dummy = np.zeros([y.shape[0], ignore_length]) + ignore\n    y = np.hstack([y, dummy])\n    y_train.append(y)\ny = np.stack(y_train, axis = 2)\ny.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_structure_adj(train):\n    ## get adjacent matrix from structure sequence\n    \n    ## here I calculate adjacent matrix of each base pair, \n    ## but eventually ignore difference of base pair and integrate into one matrix\n    Ss = []\n    for i in tqdm(range(len(train))):\n        seq_length = train[\"seq_length\"].iloc[i]\n        structure = train[\"structure\"].iloc[i]\n        sequence = train[\"sequence\"].iloc[i]\n\n        cue = []\n        a_structures = {\n            (\"A\", \"U\") : np.zeros([seq_length, seq_length]),\n            (\"C\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"A\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"C\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"U\") : np.zeros([seq_length, seq_length]),\n        }\n        a_structure = np.zeros([seq_length, seq_length])\n        for i in range(seq_length):\n            if structure[i] == \"(\":\n                cue.append(i)\n            elif structure[i] == \")\":\n                start = cue.pop()\n#                 a_structure[start, i] = 1\n#                 a_structure[i, start] = 1\n                a_structures[(sequence[start], sequence[i])][start, i] = 1\n                a_structures[(sequence[i], sequence[start])][i, start] = 1\n        \n        a_strc = np.stack([a for a in a_structures.values()], axis = 2)\n        a_strc = np.sum(a_strc, axis = 2, keepdims = True)\n        Ss.append(a_strc)\n    \n    Ss = np.array(Ss)\n    print(Ss.shape)\n    return Ss\nSs = get_structure_adj(train)\nSs_pub = get_structure_adj(test_pub)\nSs_pri = get_structure_adj(test_pri)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_distance_matrix(As):\n    ## adjacent matrix based on distance on the sequence\n    ## D[i, j] = 1 / (abs(i - j) + 1) ** pow, pow = 1, 2, 4\n    \n    idx = np.arange(As.shape[1])\n    Ds = []\n    for i in range(len(idx)):\n        d = np.abs(idx[i] - idx)\n        Ds.append(d)\n\n    Ds = np.array(Ds) + 1\n    Ds = 1/Ds\n    Ds = Ds[None, :,:]\n    Ds = np.repeat(Ds, len(As), axis = 0)\n    \n    Dss = []\n    for i in [1, 2, 4]: \n        Dss.append(Ds ** i)\n    Ds = np.stack(Dss, axis = 3)\n    print(Ds.shape)\n    return Ds\n\nDs = get_distance_matrix(As)\nDs_pub = get_distance_matrix(As_pub)\nDs_pri = get_distance_matrix(As_pri)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## concat adjecent\nAs = np.concatenate([As[:,:,:,None], Ss, Ds], axis = 3).astype(np.float32)\nAs_pub = np.concatenate([As_pub[:,:,:,None], Ss_pub, Ds_pub], axis = 3).astype(np.float32)\nAs_pri = np.concatenate([As_pri[:,:,:,None], Ss_pri, Ds_pri], axis = 3).astype(np.float32)\ndel Ss, Ds, Ss_pub, Ds_pub, Ss_pri, Ds_pri\nAs.shape, As_pub.shape, As_pri.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## sequence\ndef return_ohe(n, i):\n    tmp = [0] * n\n    tmp[i] = 1\n    return tmp\n\ndef get_input(train):\n    ## get node features, which is one hot encoded\n    mapping = {}\n    vocab = [\"A\", \"G\", \"C\", \"U\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_node = np.stack(train[\"sequence\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n\n    mapping = {}\n    vocab = [\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_loop = np.stack(train[\"predicted_loop_type\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    \n    mapping = {}\n    vocab = [\".\", \"(\", \")\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_structure = np.stack(train[\"structure\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    \n    \n    X_node = np.concatenate([X_node, X_loop], axis = 2)\n    \n    ## interaction\n    a = np.sum(X_node * (2 ** np.arange(X_node.shape[2])[None, None, :]), axis = 2)\n    vocab = sorted(set(a.flatten()))\n    print(vocab)\n    ohes = []\n    for v in vocab:\n        ohes.append(a == v)\n    ohes = np.stack(ohes, axis = 2)\n    X_node = np.concatenate([X_node, ohes], axis = 2).astype(np.float32)\n    \n    \n    print(X_node.shape)\n    return X_node\n\nX_node = get_input(train)\nX_node_pub = get_input(test_pub)\nX_node_pri = get_input(test_pri)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, in_channels, n_factor):\n        super().__init__()\n\n        self.n_factor = n_factor\n\n        self.x_Q = nn.Conv1d(in_channels=in_channels, out_channels=self.n_factor, kernel_size=1, padding=1//2)\n        self.x_K = nn.Conv1d(in_channels=in_channels, out_channels=self.n_factor, kernel_size=1, padding=1//2)\n        self.x_V = nn.Conv1d(in_channels=in_channels, out_channels=self.n_factor, kernel_size=1, padding=1//2)\n\n\n    def forward(self, x_inner, x_outer):\n\n        x_Q = self.x_Q(x_inner)  # (N, value_len, heads, head_dim)\n        x_K = self.x_K(x_outer)  # (N, key_len, heads, head_dim)\n        x_V = self.x_V(x_outer)  # (N, query_len, heads, heads_dim)\n\n        # Einsum does matrix mult. for query*keys for each training example\n        # with every other training example, don't be confused by einsum\n        # it's just how I like doing matrix multiplication & bmm\n\n        x_Q = x_Q.permute(0, 2, 1)\n        x_K = x_K.permute(0, 2, 1)\n        x_V = x_V.permute(0, 2, 1)\n\n        #print(x_Q.shape, x_K.shape)\n        res = torch.einsum(\"nqd,nkd->nqk\", [x_Q, x_K])\n        #print(res.shape)\n\n        # Normalize energy values similarly to seq2seq + attention\n        # so that they sum to 1. Also divide by scaling factor for\n        # better stability\n        attention = torch.softmax(res / (self.n_factor ** (1 / 2)), dim=2)\n\n        #print(attention.shape)\n\n        attention = torch.einsum(\"nql,nld->nqd\", [attention, x_V])\n        #.reshape(\n        #    N, query_len, self.n_factor\n        #)\n        \n        return attention\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_factor, n_head, dropout):\n        super().__init__()\n        #self.attention = Attention(in_channels, n_factor)\n        self.norm1 = nn.LayerNorm(n_factor)\n\n        self.n_factor_head = n_factor // n_head\n        self.heads = nn.ModuleList([\n            Attention(n_factor, self.n_factor_head) for _ in range(n_head)\n        ])\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, y):\n        \n        att_out = []\n        for head in self.heads:\n            out = head(x, y)\n            att_out.append(out)\n\n        att = torch.cat(att_out, dim=2)\n        x = x.permute(0, 2, 1)\n        #print(x.shape, att.shape)\n        x = x + att\n        x = self.norm1(x)\n        x = self.dropout(x)\n\n        return x\n\nclass  AdjAttn(nn.Module):\n    def __init__(self, in_unit, out_unit, n=2, rate=0.1):\n        super().__init__()\n        self.n = n\n        \n        self.f1 = Forward(in_unit=in_unit, out_unit=out_unit, kernel=3, rate=rate)\n        self.f2 = Forward(in_unit=out_unit, out_unit=out_unit, kernel=3, rate=rate)\n        self.f3 = Forward(in_unit=out_unit * self.n, out_unit=out_unit, kernel=3, rate=rate)\n\n    def forward(self, x, adj):\n        x_a = x\n        x_as = []\n        for i in range(self.n):\n            if i == 0:\n                x_a = self.f1(x_a)\n            else:\n                x_a = self.f2(x_a)\n            x_a = x_a.permute(0, 2, 1)\n            #print(x_a.shape, adj.shape)\n            x_a = torch.matmul(adj, x_a) ## aggregate neighborhods\n            #print(x_a.shape)\n            x_a = x_a.permute(0, 2, 1)\n            #print(x_a.shape)\n            x_as.append(x_a)\n        \n        if self.n == 1:\n            x_a = x_as[0]\n        else:\n            x_a = torch.cat(x_as, dim=1)\n        #print(x_a.shape)\n        x_a = self.f3(x_a)\n        return x_a\n\nclass Res(nn.Module):\n    def __init__(self, in_unit, out_unit, kernel, rate=0.1):\n        super().__init__()\n\n        self.cnn = nn.Conv1d(in_channels=in_unit, out_channels=out_unit, kernel_size=kernel, stride=1, padding=kernel//2)\n        self.norm = nn.LayerNorm(out_unit)\n        self.dropout = nn.Dropout(p=rate)\n\n    def forward(self, x):\n        h = self.cnn(x)\n        h = h.permute(0, 2, 1)\n        h = self.norm(h)\n        h = h.permute(0, 2, 1)\n        h = F.leaky_relu(h)\n        h = self.dropout(h)\n        return x + h\n\nclass Forward(nn.Module):\n    def __init__(self, in_unit, out_unit, kernel, rate=0.1):\n        super().__init__()\n\n        self.cnn = nn.Conv1d(in_channels=in_unit, out_channels=out_unit, kernel_size=kernel, padding=kernel//2)\n        self.norm = nn.LayerNorm(out_unit)\n        self.dropout = nn.Dropout(p=rate)\n\n        self.res = Res(out_unit, out_unit, kernel, rate)\n\n    def forward(self, x):\n        x = self.cnn(x)\n        x = x.permute(0, 2, 1)\n        x = self.norm(x)\n        x = x.permute(0, 2, 1)\n        x = self.dropout(x)\n        x = F.leaky_relu(x)\n        x = self.res(x)\n        return x\n\nclass BaseModel(nn.Module):\n    def __init__(self, in_dim, out_unit = 128, rate=0.0):\n        super().__init__()\n\n        self.adj_learned = nn.Linear(in_features=in_dim, out_features=1)\n        \n        self.f1 = Forward(in_unit=39, out_unit=128, kernel=3, rate=rate)\n        self.f2 = Forward(in_unit=128, out_unit=64, kernel=7, rate=rate)\n        self.f3 = Forward(in_unit=64, out_unit=32, kernel=17, rate=rate)\n        self.f4 = Forward(in_unit=32, out_unit=16, kernel=31, rate=rate)\n\n        self.f5_64 = Forward(in_unit=240, out_unit=64, kernel=31, rate=rate)\n        self.f5_32 = Forward(in_unit=64, out_unit=32, kernel=31, rate=rate)\n        \n        self.f6_64 = Forward(in_unit=448, out_unit=64, kernel=3, rate=rate)\n        self.f6_32 = Forward(in_unit=224, out_unit=32, kernel=3, rate=rate)\n\n        self.adj_attn_64 = AdjAttn(in_unit=240, out_unit=64, n=2)\n        self.adj_attn_32 = AdjAttn(in_unit=64, out_unit=32, n=2)\n        \n        self.multi_head_attention_64 = MultiHeadAttention(n_factor=64, n_head=4, dropout=0.0)\n        self.multi_head_attention_32 = MultiHeadAttention(n_factor=32, n_head=4, dropout=0.0)\n    \n    def forward(self, node, adj):\n\n        node = node.permute(0, 2, 1)\n        #print(node.shape, adj.shape)\n        # node -> [BS, 107, 39] -> channel first -> [BS, 39, 107]\n        # adj -> [BS, 107, 107, 5]\n\n        adj_learned = F.relu(self.adj_learned(adj)) \n        adj_all = torch.cat([adj, adj_learned], dim=3)\n        # adj_learned -> [BS, 107, 107, 1]\n        # adj_all -> [BS, 107, 107, 6]\n        #print(adj_learned.shape, adj_all.shape)\n\n        xs = []\n        xs.append(node)\n        \n        x1 = self.f1(node)\n        x2 = self.f2(x1)\n        x3 = self.f3(x2)\n        x4 = self.f4(x3)\n        #print(x1.shape, x2.shape, x3.shape, x4.shape)\n        x = torch.cat([x1, x2, x3, x4], dim=1)\n        # x -> [BS, 240, 107]\n        #print(x.shape)\n\n        for unit in [64, 32]:\n            x_as = []\n            for i in range(adj_all.shape[3]):\n                if unit == 64:\n                    #print(\"64 ----> \", x.shape)\n                    x_a = self.adj_attn_64(x, adj_all[:, :, :, i])\n                    x_as.append(x_a)\n                    x_c = self.f5_64(x)\n                if unit == 32:\n                    #print(\"32 ----> \" ,x.shape)\n                    x_a = self.adj_attn_32(x, adj_all[:, :, :, i])\n                    #print(x_a.shape)\n                    x_as.append(x_a)\n                    x_c = self.f5_32(x)\n                    \n            if unit == 64:\n                x = torch.cat(x_as + [x_c], dim=1)\n                x = self.f6_64(x)\n                x = self.multi_head_attention_64(x, x)\n                x = x.permute(0,2,1)\n                xs.append(x)\n            if unit == 32:\n                #print(\"32 ----> \" ,x.shape)\n                x = torch.cat(x_as + [x_c], dim=1)\n                #print(x.shape)\n                x = self.f6_32(x)\n                x = self.multi_head_attention_32(x, x)\n                x = x.permute(0,2,1)\n                xs.append(x)\n        \n        x = torch.cat(xs, dim=1)\n\n        return x\n\nclass AutoEncoder(nn.Module):\n    \"\"\"\n    Denoising auto encoder part\n    Node, Adj --> Middle features --> None\n    \"\"\"\n    def __init__(self, in_dim, rate=0.0):\n        super().__init__()\n\n        self.base = BaseModel(in_dim=in_dim)\n        self.f1 = Forward(in_unit=135, out_unit=16, kernel=31, rate=rate)\n\n        self.fc1 = nn.Linear(in_features=16, out_features=39)\n\n    def forward(self, node, adj):\n        x = self.base(node, adj)\n        #print(x.shape)\n        x = self.f1(x)\n        x = x.permute(0, 2, 1)\n        #print(x.shape)\n        p = torch.sigmoid(self.fc1(x))\n        \n        #loss = torch.mean(20 * node * torch.log(p + 1e-4) + (1 - node) * torch.log(1 - p + 1e-4))\n\n        return p\n\nclass FinalModel(nn.Module):\n    \"\"\"\n    Regression part\n    Node, Adj --> Middle Feature --> Prediction of targets\n    \"\"\"\n    def __init__(self, in_dim, rate=0.0):\n        super().__init__()\n\n        self.base = BaseModel(in_dim=in_dim)\n        self.f2 = Forward(in_unit=135, out_unit=128, kernel=31, rate=rate)\n\n        self.fc2 = nn.Linear(in_features=128, out_features=5)\n\n    \n    def forward(self, node, adj):\n        x = self.base(node, adj)\n        #print(x.shape)\n        x = self.f2(x)\n        x = x.permute(0, 2, 1)\n        #print(x.shape)\n        x = self.fc2(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def MCRMSE(y_true, y_pred):\n    y_true = y_true[:, :68, :]\n    y_pred = y_pred[:, :68, :]\n    colwise_mse = torch.mean(torch.square(y_true - y_pred), dim=1)\n    return torch.mean(torch.sqrt(colwise_mse), dim=1).mean()\n\nclass MCRMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n    \n    def rmse(self, y_actual, y_pred):\n        mse = self.mse(y_actual, y_pred)\n        return torch.sqrt(mse)\n    \n    def forward(self, y_actual, y_pred, num_scored=None):\n        if num_scored == None:\n            num_scored = y_actual.shape[-1]\n        score = 0\n        for i in range(num_scored):\n            score += self.rmse(y_actual[:, :68, i], y_pred[:, :68, i]) / num_scored\n        return score\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epcoh(model, loader, optimizer, criterion, device, epoch):\n    losses = AverageMeter()\n\n    model.train()\n    t = tqdm(loader)\n    for i, (node, adj, y) in enumerate(t):\n\n        #print(d)\n\n        node = node.to(device)\n        adj = adj.to(device)\n        y = y.to(device)\n\n        pred_y = model(node, adj)\n        \n        #print(y.shape, pred_y.shape)\n\n        loss = criterion(y, pred_y)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        bs = y.size(0)\n        losses.update(loss.item(), bs)\n\n        t.set_description(f\"Train E:{epoch} - Loss:{losses.avg:0.5f}\")\n    \n    t.close()\n    return losses.avg\n\ndef valid_epoch(model, loader, criterion, device, epoch):\n    losses = AverageMeter()\n\n    model.eval()\n\n    with torch.no_grad():\n        t = tqdm(loader)\n        for i, (node, adj, y) in enumerate(t):\n\n            #print(d)\n\n            node = node.to(device)\n            adj = adj.to(device)\n            y = y.to(device)\n\n            pred_y = model(node, adj)\n            \n            #print(y.shape, pred_y.shape)\n            \n            loss = criterion(y, pred_y)\n\n            bs = y.size(0)\n            losses.update(loss.item(), bs)\n\n            t.set_description(f\"Valid E:{epoch} - Loss:{losses.avg:0.5f}\")\n        \n    t.close()\n    return losses.avg\n\ndef test_predic(model, loader, device):\n    \n    predicts = []\n    \n    model.eval()\n    \n    with torch.no_grad():\n        t = tqdm(loader)\n        for i, (node, adj) in enumerate(t):\n            \n            node = node.to(device)\n            adj = adj.to(device)\n            \n            outs = model(node, adj).cpu().detach().numpy().tolist()\n            \n            predicts.extend(outs)\n    \n    return predicts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.optim.lr_scheduler import CosineAnnealingLR\n# Fix Warmup Bug\nfrom warmup_scheduler import GradualWarmupScheduler  # https://github.com/ildoonet/pytorch-gradual-warmup-lr\n\n\nclass GradualWarmupSchedulerV2(GradualWarmupScheduler):\n    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n    def get_lr(self):\n        if self.last_epoch > self.total_epoch:\n            if self.after_scheduler:\n                if not self.finished:\n                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                    self.finished = True\n                return self.after_scheduler.get_lr()\n            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n        if self.multiplier == 1.0:\n            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n        else:\n            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nAE_model = AutoEncoder(in_dim=5)\nAE_model = AE_model.to(device)\noptimizer = torch.optim.Adam(AE_model.parameters(), lr=0.001)\ncriterion = nn.MSELoss()\n\ntrain_dataset = TensorDataset(torch.tensor(X_node, dtype=torch.float), torch.tensor(As,  dtype=torch.float), torch.tensor(X_node,  dtype=torch.float))\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\npublic_dataset = TensorDataset(torch.tensor(X_node_pub, dtype=torch.float), torch.tensor(As_pub,  dtype=torch.float), torch.tensor(X_node_pub,  dtype=torch.float))\npublic_loader = DataLoader(public_dataset, batch_size=64, shuffle=True)\n\nprivate_dataset = TensorDataset(torch.tensor(X_node_pri, dtype=torch.float), torch.tensor(As_pri,  dtype=torch.float), torch.tensor(X_node_pri,  dtype=torch.float))\nprivate_loader = DataLoader(private_dataset, batch_size=64, shuffle=True)\n\nfor epoch in range(10):\n    print(\"####### Epoch : \", epoch)\n    print(\"####### Train\")\n    train_loss = train_epcoh(AE_model, train_loader, optimizer, criterion, device, epoch)\n    print(\"####### Public\")\n    public_loss = train_epcoh(AE_model, public_loader, optimizer, criterion, device, epoch)\n    print(\"####### Private\")\n    private_loss = train_epcoh(AE_model, private_loader, optimizer, criterion, device, epoch)\n\ntorch.save(AE_model.state_dict(), \"AE_model.bin\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class args:\n\n    exp_name = \"demo\"\n    output_dir = \"\"\n    sub_name = \"demo\"\n\n    # Training parameters\n    lr = 0.001\n    seed = 42\n    epochs = 100\n    n_folds = 10\n    batch_size = 64","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"args.save_path = os.path.join(args.output_dir, args.exp_name)\nos.makedirs(args.save_path, exist_ok=True)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\npublic_predictions = [] \npublic_ids         = []\n\nprivate_predictions = []\nprivate_ids         = []\n\npublic_dataset = TensorDataset(torch.tensor(X_node_pub, dtype=torch.float), torch.tensor(As_pub,  dtype=torch.float))\npublic_loader = DataLoader(public_dataset, batch_size=64, shuffle=False, drop_last=False)\n\nprivate_dataset = TensorDataset(torch.tensor(X_node_pri, dtype=torch.float), torch.tensor(As_pri,  dtype=torch.float))\nprivate_loader = DataLoader(private_dataset, batch_size=64, shuffle=False, drop_last=False)\n\nskf = KFold(args.n_folds, shuffle=True, random_state=42)\n\nfor i, (train_index, valid_index) in enumerate(skf.split(X_node, As)):\n    print(\"#\"*20)\n    print(f\"##### Fold : {i}\")\n    \n    args.fold = i\n\n    X_node_tr = X_node[train_index]\n    X_node_va = X_node[valid_index]\n    As_tr = As[train_index]\n    As_va = As[valid_index]\n    y_tr = y[train_index]\n    y_va = y[valid_index]\n    \n    train_dataset = TensorDataset(torch.tensor(X_node_tr, dtype=torch.float), torch.tensor(As_tr,  dtype=torch.float), torch.tensor(y_tr,  dtype=torch.float))\n    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n    \n    valid_dataset = TensorDataset(torch.tensor(X_node_va, dtype=torch.float), torch.tensor(As_va,  dtype=torch.float), torch.tensor(y_va,  dtype=torch.float))\n    valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n    \n    model = FinalModel(in_dim=5)\n    model.load_state_dict(torch.load(\"AE_model.bin\"), strict=False)\n    model = model.to(device)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n    criterion = MCRMSE #MCRMSELoss()\n    \n    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.epochs)\n    scheduler_warmup = GradualWarmupSchedulerV2(optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n    \n    best_loss = 99999\n\n    for epoch in range(args.epochs):\n\n        train_loss = train_epcoh(model, train_loader, optimizer, criterion, device, epoch)\n        valid_loss = valid_epoch(model, valid_loader, criterion, device, epoch)\n        \n        scheduler_warmup.step()    \n        if epoch==2: scheduler_warmup.step() # bug workaround \n        \n        content = f\"\"\"\n            {time.ctime()} \\n\n            Fold:{args.fold}, Epoch:{epoch}, lr:{optimizer.param_groups[0]['lr']:.7}, \\n\n            Train Loss:{train_loss:0.4f} - Valid Loss:{valid_loss:0.4f} \\n\n        \"\"\"\n        print(content)\n\n        with open(f'{args.save_path}/log_{args.exp_name}.txt', 'a') as appender:\n            appender.write(content + '\\n')\n\n        if valid_loss < best_loss:\n            print(f\"######### >>>>>>> Model Improved from {best_loss} -----> {valid_loss}\")\n            torch.save(model.state_dict(), os.path.join(args.save_path, f\"fold-{args.fold}.bin\"))\n            best_loss = valid_loss\n\n        torch.save(model.state_dict(), os.path.join(args.save_path, f\"last-fold-{args.fold}.bin\")) \n    \n    public_model = FinalModel(in_dim=5).to(device)\n    public_model.load_state_dict(torch.load(os.path.join(args.save_path, f\"fold-{args.fold}.bin\")))\n    \n    public_pred = test_predic(public_model, public_loader, device)\n    private_pred = test_predic(public_model, private_loader, device)\n    \n    public_predictions.append(np.array(public_pred).reshape(629 * 107 , 5))\n    private_predictions.append(np.array(private_pred).reshape(3005 * 130, 5))\n    \n    public_ids.append(test_pub.id.values)\n    private_ids.append(test_pri.id.values)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n\npublic_ids1 = [f\"{id}_{i}\" for id in public_ids[0] for i in range(107)]\nprivate_ids1 = [f\"{id}_{i}\" for id in private_ids[0] for i in range(130)]\n\npublic_preds = np.mean(public_predictions, axis=0)\nprivate_preds = np.mean(private_predictions, axis=0)\n\npublic_pred_df = pd.DataFrame(public_preds, columns=target_cols)\npublic_pred_df[\"id_seqpos\"] = public_ids1\n\nprivate_pred_df = pd.DataFrame(private_preds, columns=target_cols)\nprivate_pred_df[\"id_seqpos\"] = private_ids1\n\npred_sub_df = public_pred_df.append(private_pred_df)\n\npred_sub_df.to_csv(os.path.join(args.save_path, f\"{args.sub_name}_submission.csv\"), index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}