{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\n\n#the basics\nimport pandas as pd, numpy as np, seaborn as sns\nimport math, json\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\n\n#for model evaluation\nfrom sklearn.model_selection import train_test_split, KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading data"},{"metadata":{},"cell_type":"markdown","source":"My Basis for this notebook is based on this notebook https://www.kaggle.com/hiroshun/pytorch-implementation-gru-lstm"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#get comp data\ntrain = pd.read_json('/kaggle/input/stanford-covid-vaccine/train.json', lines=True)\ntest = pd.read_json('/kaggle/input/stanford-covid-vaccine/test.json', lines=True)\nsample_sub = pd.read_csv('/kaggle/input/stanford-covid-vaccine/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I. Brief EDA\n\n**From the data [description tab](https://www.kaggle.com/c/stanford-covid-vaccine/data), we must predict multiple ground truths in this competition, 5 to be exact. While the submission requires all 5, only 3 are scored: `reactivity`, `deg_Mg_pH10` and `deg_Mg_50C`. It might be interesting to see how performance differs when training for all 5 predictors vs. just the 3 that are scored.**\n\n**The training features we are given are as follows:**\n\n* **id** - An arbitrary identifier for each sample.\n* **seq_scored** - (68 in Train and Public Test, 91 in Private Test) Integer value denoting the number of positions used in scoring with predicted values. This should match the length of `reactivity`, `deg_*` and `*_error_*` columns. Note that molecules used for the Private Test will be longer than those in the Train and Public Test data, so the size of this vector will be different.\n* **seq_length** - (107 in Train and Public Test, 130 in Private Test) Integer values, denotes the length of `sequence`. Note that molecules used for the Private Test will be longer than those in the Train and Public Test data, so the size of this vector will be different.\n* **sequence** - (1x107 string in Train and Public Test, 130 in Private Test) Describes the RNA sequence, a combination of `A`, `G`, `U`, and `C` for each sample. Should be 107 characters long, and the first 68 bases should correspond to the 68 positions specified in `seq_scored` (note: indexed starting at 0).\n* **structure** - (1x107 string in Train and Public Test, 130 in Private Test) An array of `(`, `)`, and `.` characters that describe whether a base is estimated to be paired or unpaired. Paired bases are denoted by opening and closing parentheses e.g. (....) means that base 0 is paired to base 5, and bases 1-4 are unpaired.\n* **reactivity** - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in `sequence`, and used to determine the likely secondary structure of the RNA sample.\n* **deg_pH10** - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in `sequence`, and used to determine the likelihood of degradation at the base/linkage after incubating without magnesium at high pH (pH 10).\n* **deg_Mg_pH10** - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in `sequence`, and used to determine the likelihood of degradation at the base/linkage after incubating with magnesium in high pH (pH 10).\n* **deg_50C** - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in `sequence`, and used to determine the likelihood of degradation at the base/linkage after incubating without magnesium at high temperature (50 degrees Celsius).\n* **deg_Mg_50C** - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in `sequence`, and used to determine the likelihood of degradation at the base/linkage after incubating with magnesium at high temperature (50 degrees Celsius).\n* **`*_error_*`** - An array of floating point numbers, should have the same length as the corresponding `reactivity` or `deg_*` columns, calculated errors in experimental values obtained in `reactivity` and `deg_*` columns.\n* **predicted_loop_type** - (1x107 string) Describes the structural context (also referred to as 'loop type')of each character in `sequence`. Loop types assigned by bpRNA from Vienna RNAfold 2 structure. From the bpRNA_documentation: S: paired \"Stem\" M: Multiloop I: Internal loop B: Bulge H: Hairpin loop E: dangling End X: eXternal loop"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(train.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It seems we also have a `signal_to_noise` and `SN_filter` column. These columns control the 'quality' of samples, and as such are important training hyperparameters. We will explore them shortly:**"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"\nprint(train.shape)\nif ~ train.isnull().values.any(): print('No missing values')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#sneak peak\nprint(test.shape)\nif ~ test.isnull().values.any(): print('No missing values')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#sneak peak\nprint(sample_sub.shape)\nif ~ sample_sub.isnull().values.any(): print('No missing values')\nsample_sub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we explore `signal_to_noise` and `SN_filter` distributions. As per the data tab of this competition the samples in `test.json` have been filtered in the following way:**\n\n1. Minimum value across all 5 conditions must be greater than -0.5.\n2. Mean signal/noise across all 5 conditions must be greater than 1.0. [Signal/noise is defined as mean( measurement value over 68 nts )/mean( statistical error in measurement value over 68 nts)]\n3. To help ensure sequence diversity, the resulting sequences were clustered into clusters with less than 50% sequence similarity, and the 629 test set sequences were chosen from clusters with 3 or fewer members. That is, any sequence in the test set should be sequence similar to at most 2 other sequences.\n\n**But these filters have not been applied to the samples in `train.json` or the private test set of this competition, so we will likely see public leaderboard improvement by filtering `train.json` the same way that `test.json` was filtered.**\n\n**We can do 2) by setting `train['signal_to_noise'] > 0`. Now, I am not sure how `SN_filter` is related to this, so let's explore a bit further:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize = (15, 5))\nsns.kdeplot(train['signal_to_noise'], shade = True, ax = ax[0])\nsns.countplot(train['SN_filter'], ax = ax[1])\n\nax[0].set_title('Signal/Noise Distribution')\nax[1].set_title('Signal/Noise Filter Distribution');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Samples with signal_to_noise greater than 1: {len(train.loc[(train['signal_to_noise'] >= 1 )])}\")\nprint(f\"Samples with SN_filter = 1: {len(train.loc[(train['SN_filter'] == 1 )])}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It seems that `SN_filter = 1` filters more than just `signal_to_noise > 0`, so my guess it that `SN_filter = 1` performs all the above filters that were applied to `test.json`.**"},{"metadata":{},"cell_type":"markdown","source":"# Approach"},{"metadata":{},"cell_type":"markdown","source":"<a href=\"https://bbcode0.com\" target=\"_blank\"><img src=\"https://cdn1.bbcode0.com/uploads/2020/9/23/0d786a5279e1e53d8c2a0f9ffc897eb5-full.png\" border=\"0\"/></a>"},{"metadata":{},"cell_type":"markdown","source":"# II. Processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#target columns\ntarget_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we tokenize our RNA sequences so we can feed it to our model:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    return np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we just set `SN_filter = 1` to train on only samples with similar statistics to those in `test.json`. Be careful using this though, as only the public leaderboard test set has filtered samples whereas the private one does not (more [here](https://www.kaggle.com/c/stanford-covid-vaccine/data)). So it might improve your public LB score, but we do not know if it helps (it could even hurt) performance on the private test set.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_filtered = train.loc[train.SN_filter == 1]\ntrain_inputs = torch.tensor(preprocess_inputs(train_filtered)).to(device)\nprint(\"input shape: \", train_inputs.shape)\ntrain_labels = torch.tensor(\n    np.array(train_filtered[target_cols].values.tolist()).transpose(0, 2, 1)\n).float().to(device)\nprint(\"output shape: \", train_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# III. Model\n\n### **Here We will be trying to combine the results produced by both lstm and gru and feed that details again to a linear neural network** \n\n**Note that for submission, the output must be the same length as the input, which is 107 for `train.json` and `test.json` and 130 for the private test set. However, this is not true for training, so training prediction sequences only need to be 68 long**\n\n**So we actually build 3 different models: one for training, one for predicting public test, and one for predicting private test set, each with different sequence lengths and prediction lengths. Luckily, we only need to train one model, save its weights, and load these weights into the other models.**\n\n**The last thing to set is the size of the embedding layer. In the context of NLP, the input dimension size of an embedding layer is the size of the vocabulary, which in our case is `len(token2int)` (14). The output dimension is typically the length of the pre-trained vectors you are using, like the GloVe vectors or Word2Vec vectors, which we don't have in this case, so we are free to experiment with different sizes. Let's use 100 for now as a starting point:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(token2int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loss Function \n**Here we are using mcrmse loss function which is an average of all the losses of the 5 outputs.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"mse_loss = nn.MSELoss()\n\ndef mcrmse(y_actual, y_pred, num_scored=5):\n    score = 0\n    for i in range(num_scored):\n        score += mse_loss(y_actual[:, :, i], y_pred[:, :, i]) / num_scored\n    return score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For clear Understanding of these topics I highly recommend to go through these links for:\n* RNN: https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\n* LSTM: https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/ \n* GRU: https://blog.floydhub.com/gru-with-pytorch/\n\nI am sure these will help"},{"metadata":{},"cell_type":"markdown","source":"Embedding layer is the one which help us to provide the inputs to the gru and LSTM"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class GRU_model(nn.Module):\n    def __init__(\n        self, seq_len=107, pred_len=68, dropout=0.5, embed_dim=100, hidden_dim=128, hidden_layers=3\n    ):\n        super(GRU_model, self).__init__()\n        #super(LSTM_model, self).__init__()\n        self.pred_len = pred_len\n\n        self.embeding = nn.Embedding(num_embeddings=len(token2int), embedding_dim=embed_dim)\n        self.gru = nn.GRU(\n            input_size=embed_dim * 3,\n            hidden_size=hidden_dim,\n            num_layers=hidden_layers,\n            dropout=dropout,\n            bidirectional=True,\n            batch_first=True,\n        )\n        self.lstm = nn.LSTM(\n            input_size=embed_dim * 3,\n            hidden_size=hidden_dim,\n            num_layers=hidden_layers,\n            dropout=dropout,\n            bidirectional=True,\n            batch_first=True,\n        )\n        \n        self.linear1 = nn.Linear(hidden_dim * 2, 128)\n        #self.act = nn.Tanh()\n        self.linear2 =nn.Linear(256, 256)        \n        self.linear3 = nn.Linear(256, 5)\n\n    def forward(self, seqs):\n        embed = self.embeding(seqs)\n        reshaped = torch.reshape(embed, (-1, embed.shape[1], embed.shape[2] * embed.shape[3]))\n        output_gru, hidden = self.gru(reshaped)\n        truncated_gru = output_gru[:, : self.pred_len, :]\n        output_lstm, hidden = self.lstm(reshaped)\n        truncated_lstm = output_lstm[:, : self.pred_len, :]\n        \n        out1 = self.linear1(truncated_gru)\n        out2 = self.linear1(truncated_lstm)\n        \n        combined=torch.cat((out1,out2), dim=2)\n        #combined_tan1 = self.act(combined)  \n        combined_tan1 = self.linear2(combined)\n        out = self.linear3(combined_tan1)\n        \n        #print(out.size())\n        return out\n    \nclass LSTM_model(nn.Module):\n    def __init__(\n        self, seq_len=107, pred_len=68, dropout=0.5, embed_dim=100, hidden_dim=128, hidden_layers=3\n    ):\n        super(LSTM_model, self).__init__()\n        self.pred_len = pred_len\n\n        self.embeding = nn.Embedding(num_embeddings=len(token2int), embedding_dim=embed_dim)\n        self.lstm = nn.LSTM(\n            input_size=embed_dim * 3,\n            hidden_size=hidden_dim,\n            num_layers=hidden_layers,\n            dropout=dropout,\n            bidirectional=True,\n            batch_first=True,\n        )\n        self.linear = nn.Linear(hidden_dim * 2, 5)\n\n    def forward(self, seqs):\n        embed = self.embeding(seqs)\n        reshaped = torch.reshape(embed, (-1, embed.shape[1], embed.shape[2] * embed.shape[3]))\n        output, hidden = self.lstm(reshaped)\n        truncated = output[:, : self.pred_len, :]\n        out = self.linear(truncated)\n        return out\n    \nmse_loss = nn.MSELoss()\ndef compute_loss(batch_X, batch_Y, model, optimizer=None, is_train=True):\n    model.train(is_train)\n\n    pred_Y = model(batch_X)\n\n    loss = mcrmse(pred_Y, batch_Y)\n\n    if is_train:\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    return loss.item()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# IV. KFold Training and Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"#basic training configuration\nFOLDS = 4\nEPOCHS = 90\nBATCH_SIZE = 64\nVERBOSE = 2\nLR = 0.01","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get different test sets and process each\npublic_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\n\npublic_inputs = torch.tensor(preprocess_inputs(public_df)).to(device)\nprivate_inputs = torch.tensor(preprocess_inputs(private_df)).to(device)\n\npublic_loader = DataLoader(TensorDataset(public_inputs), shuffle=False, batch_size=BATCH_SIZE)\nprivate_loader = DataLoader(TensorDataset(private_inputs), shuffle=False, batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We will add a simple learning rate callback for now:**"},{"metadata":{},"cell_type":"markdown","source":"### 1. GRU"},{"metadata":{"trusted":true},"cell_type":"code","source":"gru_histories = []\ngru_private_preds = np.zeros((private_df.shape[0], 130, 5))\ngru_public_preds = np.zeros((public_df.shape[0], 107, 5))\n\nkfold = KFold(FOLDS, shuffle=True, random_state=2020)\n\nfor k, (train_index, val_index) in enumerate(kfold.split(train_inputs)):\n    train_dataset = TensorDataset(train_inputs[train_index], train_labels[train_index])\n    val_dataset = TensorDataset(train_inputs[val_index], train_labels[val_index])\n\n    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n    val_loader = DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE)\n\n    model = GRU_model().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=LR)\n\n    train_losses = []\n    val_losses = []\n    for epoch in tqdm(range(EPOCHS)):\n        train_losses_batch = []\n        val_losses_batch = []\n        for (batch_X, batch_Y) in train_loader:\n            train_loss = compute_loss(batch_X, batch_Y, model, optimizer=optimizer, is_train=True)\n            train_losses_batch.append(train_loss)\n        for (batch_X, batch_Y) in val_loader:\n            val_loss = compute_loss(batch_X, batch_Y, model, optimizer=optimizer, is_train=False)\n            val_losses_batch.append(val_loss)\n        train_losses.append(sum(train_losses_batch) / len(train_losses_batch))\n        val_losses.append(sum(val_losses_batch) / len(val_losses_batch))\n    model_state = model.state_dict()\n    del model\n            \n    gru_histories.append({'train_loss': train_losses, 'val_loss': val_losses})\n\n\n    gru_short = GRU_model(seq_len=107, pred_len=107).to(device)\n    gru_short.load_state_dict(model_state)\n    gru_short.eval()\n    gru_public_pred = np.ndarray((0, 107, 5))\n    for batch in public_loader:\n        batch_X = batch[0]\n        pred = gru_short(batch_X).detach().cpu().numpy()\n        gru_public_pred = np.concatenate([gru_public_pred, pred], axis=0)\n    gru_public_preds += gru_public_pred / FOLDS\n\n    gru_long = GRU_model(seq_len=130, pred_len=130).to(device)\n    gru_long.load_state_dict(model_state)\n    gru_long.eval()\n    gru_private_pred = np.ndarray((0, 130, 5))\n    for batch in private_loader:\n        batch_X = batch[0]\n        pred = gru_long(batch_X).detach().cpu().numpy()\n        gru_private_pred = np.concatenate([gru_private_pred, pred], axis=0)\n    gru_private_preds += gru_private_pred / FOLDS\n    \n    del gru_short, gru_long","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\" GRU mean fold validation loss: {np.mean([min(history['val_loss']) for history in gru_histories])}\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize = (20, 10))\n\nfor history in gru_histories:\n    ax[0].plot(history['train_loss'], 'b')\n    ax[0].plot(history['val_loss'], 'r')\n\n\nax[0].set_title('GRU')\n# ax[1].set_title('LSTM')\n\nax[0].legend(['train', 'validation'], loc = 'upper right')\n\nax[0].set_ylabel('Loss')\nax[0].set_xlabel('Epoch')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# V. Submission\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"public_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\n\npublic_inputs = preprocess_inputs(public_df)\nprivate_inputs = preprocess_inputs(private_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we just need to change the shape of each sample to long format:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_gru = []\n\nfor df, preds in [(public_df, gru_public_preds), (private_df, gru_private_preds)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=target_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_gru.append(single_df)\n\npreds_gru_df = pd.concat(preds_gru)\npreds_gru_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blend_preds_df = pd.DataFrame()\nblend_preds_df['id_seqpos'] = preds_gru_df['id_seqpos']\nblend_preds_df['reactivity'] = preds_gru_df['reactivity']\nblend_preds_df['deg_Mg_pH10'] = preds_gru_df['deg_Mg_pH10'] \nblend_preds_df['deg_pH10'] = preds_gru_df['deg_pH10'] \nblend_preds_df['deg_Mg_50C'] = preds_gru_df['deg_Mg_50C'] \nblend_preds_df['deg_50C'] = preds_gru_df['deg_50C'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = sample_sub[['id_seqpos']].merge(blend_preds_df, on=['id_seqpos'])\n\n#sanity check\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission5.csv', index=False)\nprint('Submission saved')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.chdir(r'./')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'./submission5.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}