{"cells":[{"metadata":{},"cell_type":"markdown","source":"This kernel is a mere copy from https://www.kaggle.com/tuckerarrants/openvaccine-gru-lstm, my only contribution is to show how much the scoring is affected by training only on \"signal_to_noise\" levels higher than 1."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n#the basics\nimport pandas as pd, numpy as np\nimport math, json, gc, random, os, sys\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\n\n#tensorflow deep learning basics\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\n\n#for model evaluation\nfrom sklearn.model_selection import train_test_split, KFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Competition Overview\n\n**In this [new competition](https://www.kaggle.com/c/stanford-covid-vaccine/overview) we are helping to fight against the worldwide pandemic COVID-19. mRNA vaccines are the fastest vaccine candidates to treat COVID-19 but they currently facing several limitations. In particular, it is a challenge to design stable messenger RNA molecules. Typical vaccines are packaged in syringes and shipped under refrigeration around the world, but that is not possible for mRNA vaccines (currently).**\n\n**Researches have noticed that RNA molecules tend to spontaneously degrade, which is highly problematic because a single cut can render mRNA vaccines useless. Not much is known about which part of the backbone of a particular RNA is most susceptible to being damaged.**\n\n**Without this knowledge, the current mRNA vaccines are shopped under intense refrigeration and are unlikely to reach enough humans unless they can be stabilized. This is our task as Kagglers: we must create a model to predict the most likely degradation rates at each base of an RNA molecule.**\n\n**We are given a subset of an Eterna dataset comprised of over 3000 RNA molecules and their degradation rates at each position. Our models are then tested on the new generation of RNA sequences that were just created by Eterna players for COVID-19 mRNA vaccines**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#get comp data\ntrain = pd.read_json('/kaggle/input/stanford-covid-vaccine/train.json', lines=True)\ntest = pd.read_json('/kaggle/input/stanford-covid-vaccine/test.json', lines=True)\nsample_sub = pd.read_csv('/kaggle/input/stanford-covid-vaccine/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Brief EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#sneak peak\nprint(train.shape)\nif ~ train.isnull().values.any(): print('No missing values')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sneak peak\nprint(test.shape)\nif ~ test.isnull().values.any(): print('No missing values')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sneak peak\nprint(sample_sub.shape)\nif ~ sample_sub.isnull().values.any(): print('No missing values')\nsample_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"sequence\"][0], train[\"structure\"][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"sequence\"] = train[\"sequence\"] + \"0\" * 23\ntrain[\"structure\"] += \"0\" * 23\ntrain[\"predicted_loop_type\"] += \"0\" * 23","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"struct = []\nseq = []\nplt = []\nfor idx, val in test.iterrows():\n    if val[\"seq_length\"] == 107:\n        struct.append(val[\"structure\"] + \"0\" * 23)\n        seq.append(val[\"sequence\"] + \"0\" * 23)\n        plt.append(val[\"predicted_loop_type\"] + \"0\" * 23)\n    else:\n        struct.append(val[\"structure\"])\n        seq.append(val[\"sequence\"])\n        plt.append(val[\"predicted_loop_type\"])\n        \ntest[\"sequence\"] = seq\ntest[\"structure\"] = struct\ntest[\"predicted_loop_type\"] = plt\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#target columns\ntarget_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we create a dictionary to help us map `sequence`, `structure`, and `predicted_loop_type` to columns we can feed a model:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX0')}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    return np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_inputs = preprocess_inputs(train[train.signal_to_noise > 1])\ntrain_labels = np.array(train[train.signal_to_noise > 1][target_cols].values.tolist()).transpose((0, 2, 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model\n\n**We begin with a simple GRU model taken from the one and only [Xhlulu](https://www.kaggle.com/xhlulu)'s notebook [here](https://www.kaggle.com/xhlulu/openvaccine-simple-gru-model)**\n\n**From the documentation of this competition, you can read that due to technical reasons, measurements cannot be carried out on the final bases of the RNA sequences we have just have experimental data (as ground truths) in 5 conditions for the first 68 bases. This means we must truncate the output of our model:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import layers\nclass MultiHeadSelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        if embed_dim % num_heads != 0:\n            raise ValueError(\n                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n            )\n        self.projection_dim = embed_dim // num_heads\n        self.query_dense = layers.Dense(embed_dim)\n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        self.combine_heads = layers.Dense(embed_dim)\n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"projection_dim\": self.projection_dim,\n            \"query_dense\" : self.query_dense,\n            \"key_dense\" : self.key_dense,\n            \"value_dense\" : self.value_dense,\n            \"conbine_heads\" : self.combine_heads\n        })\n        return config\n\n    def attention(self, query, key, value):\n        score = tf.matmul(query, key, transpose_b=True)\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n        scaled_score = score / tf.math.sqrt(dim_key)\n        weights = tf.nn.softmax(scaled_score, axis=-1)\n        output = tf.matmul(weights, value)\n        return output, weights\n\n    def separate_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, inputs):\n        # x.shape = [batch_size, seq_len, embedding_dim]\n        batch_size = tf.shape(inputs)[0]\n        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        query = self.separate_heads(\n            query, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        key = self.separate_heads(\n            key, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        value = self.separate_heads(\n            value, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        attention, weights = self.attention(query, key, value)\n        attention = tf.transpose(\n            attention, perm=[0, 2, 1, 3]\n        )  # (batch_size, seq_len, num_heads, projection_dim)\n        concat_attention = tf.reshape(\n            attention, (batch_size, -1, self.embed_dim)\n        )  # (batch_size, seq_len, embed_dim)\n        output = self.combine_heads(\n            concat_attention\n        )  # (batch_size, seq_len, embed_dim)\n        return output\n    \nclass TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            \"token_emb\" : self.token_emb,\n            \"pos_emd\" : self.pos_emb\n        })\n        return config\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def MCRMSE(y_true, y_pred):\n    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n\ndef gru_layer(hidden_dim, dropout):\n    return tf.keras.layers.Bidirectional(\n                                tf.keras.layers.GRU(hidden_dim,\n                                dropout=dropout,\n                                return_sequences=True,\n                                kernel_initializer = 'orthogonal'))\n\ndef lstm_layer(hidden_dim, dropout):\n    return tf.keras.layers.Bidirectional(\n                                tf.keras.layers.LSTM(hidden_dim,\n                                dropout=dropout,\n                                return_sequences=True,\n                                kernel_initializer = 'orthogonal'))\n\ndef build_model(gru=False,seq_len=130, pred_len=68, dropout=0.4,\n                embed_dim=100, hidden_dim=168, atten_head = 5):\n    \n    x = tf.keras.layers.Input(shape=(seq_len, 3))\n    tfkl = tf.keras.layers\n#     x0 = tfkl.Lambda(lambda x : x[..., 0])(x)\n#     x1 = tfkl.Lambda(lambda x : x[..., 1])(x)\n#     x2 = tfkl.Lambda(lambda x : x[..., 2])(x)\n    \n#     x0 = TokenAndPositionEmbedding(seq_len, len(token2int), embed_dim)(x0)\n#     x1 = TokenAndPositionEmbedding(seq_len, len(token2int), embed_dim)(x1)\n#     x2 = TokenAndPositionEmbedding(seq_len, len(token2int), embed_dim)(x2)\n    x0 = TokenAndPositionEmbedding(seq_len, len(token2int), embed_dim)(x)\n    \n#     reshaped0 = tf.keras.layers.Embedding(input_dim=len(token2int), output_dim=embed_dim)(x0)\n# #     reshaped0 = tf.reshape(\n# #         embed0, shape=(-1, embed0.shape[1],  embed0.shape[2] * embed0.shape[3]))\n    \n#     reshaped1 = tf.keras.layers.Embedding(input_dim=len(token2int), output_dim=embed_dim)(x1)\n# #     reshaped1 = tf.reshape(\n# #         embed1, shape=(-1, embed1.shape[1],  embed1.shape[2] * embed1.shape[3]))\n    \n#     reshaped2 = tf.keras.layers.Embedding(input_dim=len(token2int), output_dim=embed_dim)(x2)\n# #     reshaped2 = tf.reshape(\n# #         embed2, shape=(-1, embed2.shape[1],  embed2.shape[2] * embed2.shape[3]))\n    \n#     atten = []\n#     for i in range(atten_head):\n#         att1 = tf.keras.layers.Attention()([reshaped0, reshaped1])\n#         att2 = tfkl.Attention()([reshaped1, reshaped2])\n#         att3 = tfkl.Attention()([reshaped0, reshaped2])\n#         atten.append(att1)\n#         atten.append(att2)\n#         atten.append(att3)\n    att = MultiHeadSelfAttention(32)(x0)\n#     att2 = MultiHeadSelfAttention(32)(x)\n#     att3 = MultiHeadSelfAttention(32)(x2)\n        \n#     o = tfkl.Concatenate()([att1, att2, att3])\n    o = tfkl.LayerNormalization()(att)\n    \n\n\n#     embed = tf.keras.layers.Embedding(input_dim=len(token2int), output_dim=embed_dim)(inputs)\n#     reshaped = tf.reshape(\n#         embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n    \n    reshaped = tf.keras.layers.SpatialDropout1D(.2)(o)\n    \n    if gru:\n        hidden = gru_layer(hidden_dim, dropout)(reshaped)\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n        \n    else:\n        hidden = lstm_layer(hidden_dim, dropout)(reshaped)\n        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n    \n    #only making predictions on the first part of each sequence\n    truncated = hidden[:, :pred_len]\n    \n    out = tf.keras.layers.Dense(5, activation='linear')(truncated)\n\n    model = tf.keras.Model(inputs=x, outputs=out)\n\n    #some optimizers\n    adam = tf.optimizers.Adam()\n    radam = tfa.optimizers.RectifiedAdam()\n    lookahead = tfa.optimizers.Lookahead(adam, sync_period=6)\n    ranger = tfa.optimizers.Lookahead(radam, sync_period=6)\n    \n    model.compile(optimizer = ranger, loss=MCRMSE)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training\n\n**Create train/val split now so both models are trained and evaluated on the same samples:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_inputs, val_inputs, train_labels, val_labels = train_test_split(train_inputs, train_labels,\n                                                                     test_size=.1, random_state=34)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if tf.config.list_physical_devices('GPU') is not None:\n    print('Training on GPU')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We will use a simple learning rate callback for now:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_callback = tf.keras.callbacks.ReduceLROnPlateau()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. GRU"},{"metadata":{"trusted":true},"cell_type":"code","source":"gru = build_model(gru=True)\nsv_gru = tf.keras.callbacks.ModelCheckpoint('model_gru.h5', save_best_only=True)\n\nhistory_gru = gru.fit(\n    train_inputs, train_labels, \n    validation_data=(val_inputs,val_labels),\n#     batch_size=64,\n    epochs=150,\n    callbacks=[lr_callback,sv_gru],\n    verbose = 2\n)\n\nprint(f\"Min training loss={min(history_gru.history['loss'])}, min validation loss={min(history_gru.history['val_loss'])}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm = build_model(gru=False)\nsv_lstm = tf.keras.callbacks.ModelCheckpoint('model_lstm.h5', save_best_only=True)\n\nhistory_lstm = lstm.fit(\n    train_inputs, train_labels, \n    validation_data=(val_inputs,val_labels),\n#     batch_size=64,\n    epochs=150,\n    callbacks=[lr_callback,sv_lstm],\n    verbose = 2\n)\n\nprint(f\"Min training loss={min(history_lstm.history['loss'])}, min validation loss={min(history_lstm.history['val_loss'])}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib\n\nfig, ax = matplotlib.pyplot.subplots(1, 2, figsize = (20, 10))\n\nax[0].plot(history_gru.history['loss'])\nax[0].plot(history_gru.history['val_loss'])\n\nax[1].plot(history_lstm.history['loss'])\nax[1].plot(history_lstm.history['val_loss'])\n\nax[0].set_title('GRU')\nax[1].set_title('LSTM')\n\nax[0].legend(['train', 'validation'], loc = 'upper right')\nax[1].legend(['train', 'validation'], loc = 'upper right')\n\nax[0].set_ylabel('Loss')\nax[0].set_xlabel('Epoch')\nax[1].set_ylabel('Loss')\nax[1].set_xlabel('Epoch');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference and Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"public_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\n\npublic_inputs = preprocess_inputs(public_df)\nprivate_inputs = preprocess_inputs(private_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Predict twice, one for the public leaderboard, the other for the private leaderboard:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#build all models\ngru_short = build_model(gru=True, seq_len=130, pred_len=107)\ngru_long = build_model(gru=True, seq_len=130, pred_len=130)\nlstm_short = build_model(gru=False, seq_len=130, pred_len=107)\nlstm_long = build_model(gru=False, seq_len=130, pred_len=130)\n\n#load pre-trained model weights\ngru_short.load_weights('model_gru.h5')\ngru_long.load_weights('model_gru.h5')\nlstm_short.load_weights('model_lstm.h5')\nlstm_long.load_weights('model_lstm.h5')\n\n#and predict\ngru_public_preds = gru_short.predict(public_inputs)\ngru_private_preds = gru_long.predict(private_inputs)\nlstm_public_preds = lstm_short.predict(public_inputs)\nlstm_private_preds = lstm_long.predict(private_inputs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we just need to change the shape of each sample to the long format:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_gru = []\n\nfor df, preds in [(public_df, gru_public_preds), (private_df, gru_private_preds)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=target_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_gru.append(single_df)\n\npreds_gru_df = pd.concat(preds_gru)\npreds_gru_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we do the same for the LSTM model so we can blend their predictions:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_lstm = []\n\nfor df, preds in [(public_df, lstm_public_preds), (private_df, lstm_private_preds)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=target_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_lstm.append(single_df)\n\npreds_lstm_df = pd.concat(preds_lstm)\npreds_lstm_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**And now we blend:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"blend_preds_df = pd.DataFrame()\nblend_preds_df['id_seqpos'] = preds_gru_df['id_seqpos']\nblend_preds_df['reactivity'] = .5*preds_gru_df['reactivity'] + .5*preds_lstm_df['reactivity']\nblend_preds_df['deg_Mg_pH10'] = .5*preds_gru_df['deg_Mg_pH10'] + .5*preds_lstm_df['deg_Mg_pH10']\nblend_preds_df['deg_pH10'] = .5*preds_gru_df['deg_pH10'] + .5*preds_lstm_df['deg_pH10']\nblend_preds_df['deg_Mg_50C'] = .5*preds_gru_df['deg_Mg_50C'] + .5*preds_lstm_df['deg_Mg_50C']\nblend_preds_df['deg_50C'] = .5*preds_gru_df['deg_50C'] + .5*preds_lstm_df['deg_50C']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = sample_sub[['id_seqpos']].merge(blend_preds_df, on=['id_seqpos'])\n\n#sanity check\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nprint('Submission saved')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !kaggle competitions submit -c stanford-covid-vaccine -f submission.csv -m \"Message\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}