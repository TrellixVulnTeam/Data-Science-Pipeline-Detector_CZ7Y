{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom numba import jit\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\nfrom sklearn import metrics\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport dask.dataframe as dd\n\nfrom tqdm import tqdm_notebook\n\nfrom itertools import product\n\nfrom IPython.display import HTML\nimport json\n\nimport glob\nimport scipy\nimport altair as alt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric(df, preds):\n    df[\"prediction\"] = preds\n    maes = []\n    for t in df.type.unique():\n        y_true = df[df.type==t].scalar_coupling_constant.values\n        y_pred = df[df.type==t].prediction.values\n        mae = np.log(metrics.mean_absolute_error(y_true, y_pred))\n        maes.append(mae)\n    return np.mean(maes)\n\ndef give_mass(atom_val):\n    \n    if atom_val == \"H\":\n        mass = mass_dict[\"H\"]\n    elif atom_val == \"C\":\n        mass = mass_dict[\"C\"]\n    elif atom_val == \"F\":\n        mass = mass_dict[\"F\"]\n    elif atom_val ==\"O\":\n        mass = mass_dict[\"O\"]\n    elif atom_val ==\"N\":\n        mass= mass_dict[\"N\"]\n    return mass\n\n\ndef map_atom_info(df, atom_idx):\n    df = pd.merge(df, structures, how = 'left',\n                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n                  right_on = ['molecule_name',  'atom_index'])\n    \n    df = df.drop('atom_index', axis=1)\n    df = df.rename(columns={'atom': f'atom_{atom_idx}',\n                            'x': f'x_{atom_idx}',\n                            'y': f'y_{atom_idx}',\n                            'z': f'z_{atom_idx}'})\n    return df\n\n\ndef create_features(df):\n    df['molecule_couples'] = df.groupby('molecule_name')['id'].transform('count')\n    df['molecule_dist_mean'] = df.groupby('molecule_name')['dist'].transform('mean')\n    df['molecule_inv_dist_mean'] = df.groupby('molecule_name')['inv_dist'].transform('mean')\n    df['molecule_dist_min'] = df.groupby('molecule_name')['dist'].transform('min')\n    df['molecule_inv_dist_min'] = df.groupby('molecule_name')['inv_dist'].transform('min')\n    df['molecule_dist_max'] = df.groupby('molecule_name')['dist'].transform('max')\n    df['molecule_inv_dist_max'] = df.groupby('molecule_name')['inv_dist'].transform('max')\n    df['atom_0_couples_count'] = df.groupby(['molecule_name', 'atom_index_0'])['id'].transform('count')\n    df['atom_1_couples_count'] = df.groupby(['molecule_name', 'atom_index_1'])['id'].transform('count')\n    \n    \n    df[f'molecule_atom_index_0_x_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['x_1'].transform('std')\n    df[f'molecule_atom_index_0_y_1_mean'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('mean')\n    df[f'molecule_atom_index_0_y_1_mean_diff'] = df[f'molecule_atom_index_0_y_1_mean'] - df['y_1']\n    df[f'molecule_atom_index_0_y_1_mean_div'] = df[f'molecule_atom_index_0_y_1_mean'] / df['y_1']\n    df[f'molecule_atom_index_0_y_1_max'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('max')\n    df[f'molecule_atom_index_0_y_1_max_diff'] = df[f'molecule_atom_index_0_y_1_max'] - df['y_1']\n    df[f'molecule_atom_index_0_y_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('std')\n    df[f'molecule_atom_index_0_z_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['z_1'].transform('std')\n    df[f'molecule_atom_index_0_dist_mean'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('mean')\n    df[f'molecule_atom_index_0_dist_mean_diff'] = df[f'molecule_atom_index_0_dist_mean'] - df['dist']\n    df[f'molecule_atom_index_0_dist_mean_div'] = df[f'molecule_atom_index_0_dist_mean'] / df['dist']\n    df[f'molecule_atom_index_0_dist_max'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('max')\n    df[f'molecule_atom_index_0_dist_max_diff'] = df[f'molecule_atom_index_0_dist_max'] - df['dist']\n    df[f'molecule_atom_index_0_dist_max_div'] = df[f'molecule_atom_index_0_dist_max'] / df['dist']\n    df[f'molecule_atom_index_0_dist_min'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('min')\n    df[f'molecule_atom_index_0_dist_min_diff'] = df[f'molecule_atom_index_0_dist_min'] - df['dist']\n    df[f'molecule_atom_index_0_dist_min_div'] = df[f'molecule_atom_index_0_dist_min'] / df['dist']\n    df[f'molecule_atom_index_0_dist_std'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('std')\n    df[f'molecule_atom_index_0_dist_std_diff'] = df[f'molecule_atom_index_0_dist_std'] - df['dist']\n    df[f'molecule_atom_index_0_dist_std_div'] = df[f'molecule_atom_index_0_dist_std'] / df['dist']\n    df[f'molecule_atom_index_1_dist_mean'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('mean')\n    df[f'molecule_atom_index_1_dist_mean_diff'] = df[f'molecule_atom_index_1_dist_mean'] - df['dist']\n    df[f'molecule_atom_index_1_dist_mean_div'] = df[f'molecule_atom_index_1_dist_mean'] / df['dist']\n    df[f'molecule_atom_index_1_dist_max'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('max')\n    df[f'molecule_atom_index_1_dist_max_diff'] = df[f'molecule_atom_index_1_dist_max'] - df['dist']\n    df[f'molecule_atom_index_1_dist_max_div'] = df[f'molecule_atom_index_1_dist_max'] / df['dist']\n    df[f'molecule_atom_index_1_dist_min'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('min')\n    df[f'molecule_atom_index_1_dist_min_diff'] = df[f'molecule_atom_index_1_dist_min'] - df['dist']\n    df[f'molecule_atom_index_1_dist_min_div'] = df[f'molecule_atom_index_1_dist_min'] / df['dist']\n    df[f'molecule_atom_index_1_dist_std'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('std')\n    df[f'molecule_atom_index_1_dist_std_diff'] = df[f'molecule_atom_index_1_dist_std'] - df['dist']\n    df[f'molecule_atom_index_1_dist_std_div'] = df[f'molecule_atom_index_1_dist_std'] / df['dist']\n    \n    \n    df[f'molecule_atom_1_dist_mean'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('mean')\n    df[f'molecule_atom_1_inv_dist_mean'] = df.groupby(['molecule_name', 'atom_1'])['inv_dist'].transform('mean')\n\n    df[f'molecule_atom_1_dist_min'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('min')\n    df[f'molecule_atom_1_inv_dist_min'] = df.groupby(['molecule_name', 'atom_1'])['inv_dist'].transform('min')\n\n    df[f'molecule_atom_1_dist_min_diff'] = df[f'molecule_atom_1_dist_min'] - df['dist']\n    df[f'molecule_atom_1_inv_dist_min_diff'] = df[f'molecule_atom_1_inv_dist_min'] - df['inv_dist']\n\n    df[f'molecule_atom_1_dist_min_div'] = df[f'molecule_atom_1_dist_min'] / df['dist']\n    df[f'molecule_atom_1_dist_inv_min_div'] = df[f'molecule_atom_1_inv_dist_min'] / df['inv_dist']\n\n    df[f'molecule_atom_1_dist_std'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('std')\n    df[f'molecule_atom_1_inv_dist_std'] = df.groupby(['molecule_name', 'atom_1'])['inv_dist'].transform('std')\n\n    df[f'molecule_atom_1_dist_std_diff'] = df[f'molecule_atom_1_dist_std'] - df['dist']\n    df[f'molecule_atom_1_inv_dist_std_diff'] = df[f'molecule_atom_1_inv_dist_std'] - df['inv_dist']\n\n    df[f'molecule_type_0_dist_std'] = df.groupby(['molecule_name', 'type_0'])['dist'].transform('std')\n    df[f'molecule_type_0_inv_dist_std'] = df.groupby(['molecule_name', 'type_0'])['inv_dist'].transform('std')\n\n    df[f'molecule_type_0_dist_std_diff'] = df[f'molecule_type_0_dist_std'] - df['dist']\n    df[f'molecule_type_0_inv_dist_std_diff'] = df[f'molecule_type_0_inv_dist_std'] - df['inv_dist']\n\n    df[f'molecule_type_dist_mean'] = df.groupby(['molecule_name', 'type'])['dist'].transform('mean')\n    df[f'molecule_type_inv_dist_mean'] = df.groupby(['molecule_name', 'type'])['inv_dist'].transform('mean')\n\n    df[f'molecule_type_dist_mean_diff'] = df[f'molecule_type_dist_mean'] - df['dist']\n    df[f'molecule_type_inv_dist_mean_diff'] = df[f'molecule_type_inv_dist_mean'] - df['inv_dist']\n\n    df[f'molecule_type_dist_mean_div'] = df[f'molecule_type_dist_mean'] / df['dist']\n    df[f'molecule_type_inv_dist_mean_div'] = df[f'molecule_type_inv_dist_mean'] / df['inv_dist']\n\n    df[f'molecule_type_dist_max'] = df.groupby(['molecule_name', 'type'])['dist'].transform('max')\n    df[f'molecule_type_inv_dist_max'] = df.groupby(['molecule_name', 'type'])['inv_dist'].transform('max')\n\n    df[f'molecule_type_dist_min'] = df.groupby(['molecule_name', 'type'])['dist'].transform('min')\n    df[f'molecule_type_inv_dist_min'] = df.groupby(['molecule_name', 'type'])['inv_dist'].transform('min')\n\n    df[f'molecule_type_dist_std'] = df.groupby(['molecule_name', 'type'])['dist'].transform('std')\n    df[f'molecule_type_inv_dist_std'] = df.groupby(['molecule_name', 'type'])['inv_dist'].transform('std')\n\n    df[f'molecule_type_dist_std_diff'] = df[f'molecule_type_dist_std'] - df['dist']\n    df[f'molecule_type_inv_dist_std_diff'] = df[f'molecule_type_inv_dist_std'] - df['inv_dist']\n\n    df = reduce_mem_usage(df)\n    return df\n\n#now the boltzmann factor \n#e^-beta(dist_speedup)\n#e^-beta(dist_speedup^2)\ndef boltzmann_ns(dist):\n    \n    boltz = np.exp(-beta*dist)\n    return boltz\n\ndef boltzmann_sq(dist):\n    boltz= np.exp(-beta*(dist**2))\n    return boltz\n\n#UGLY CONSTANT\n\n#ATOMIC Mass\nmass_dict = {\"H\":1.00794, \"C\":12.0107, \"F\":18.9984032, \"O\":15.9994, \"N\":14.0067}\n#ATOMIC RADII\nradii_dict = {\"H\":0.23, \"C\":0.68, \"F\":0.64, \"O\":0.68, \"N\":0.68}\n#BOLTZMANN\nkT = 0.59248490241  #k * 298.15  kcal/mol\nbeta = 1./kT","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"structures = pd.read_csv(\"../input/create-structure-file/structures_done.csv\")\nstructures.head()\nstructures = reduce_mem_usage(structures)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#structures = dd.read_csv(\"../input/feature-engineering-1/structures_angles.csv\")\n#structures = pd.read_csv(\"../input/feature-engineering-1/structures_angles.csv\")\n#structures = reduce_mem_usage(structures)\n#train = dd.read_csv(\"../input/champs-scalar-coupling/train.csv\")\ntrain = pd.read_csv(\"../input/champs-scalar-coupling/train.csv\")\n#train = reduce_mem_usage(train)\n#test = dd.read_csv(\"../input/champs-scalar-coupling/test.csv\")\ntest = pd.read_csv(\"../input/champs-scalar-coupling/test.csv\")\n#test  = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#select the types \ntrain.type.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.loc[train[\"type\"]==\"1JHC\"]\ntest = test.loc[test[\"type\"]==\"1JHC\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sd_train = pd.read_csv(\"../input/modelling-types-1-metafeatures-sd/1JHC_sd_oof.csv\")\nsd_test  = pd.read_csv(\"../input/modelling-types-1-metafeatures-sd/1JHC_sd_pred.csv\")\n\nsd_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dso_train = pd.read_csv(\"../input/modelling-types-1-metafeatures-dso/1JHC_dso_oof.csv\")\ndso_test  = pd.read_csv(\"../input/modelling-types-1-metafeatures-dso/1JHC_dso_pred.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fc_train = pd.read_csv(\"../input/modelling-types-1-metafeatures-fc/1JHC_dso_oof.csv\")\nfc_test  = pd.read_csv(\"../input/modelling-types-1-metafeatures-fc/1JHC_dso_pred.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fc_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"sd\"] = sd_train[\"target\"]\ntest[\"sd\"] = sd_test[\"prediction\"]\ntrain[\"dso\"]= dso_train[\"target\"]\ntest[\"dso\"]=dso_test[\"prediction\"]\ntrain[\"fc\"]= fc_train[\"target\"]\ntest[\"fc\"]=fc_test[\"prediction\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = map_atom_info(train, 0)\ntrain = map_atom_info(train, 1)\n\ntest = map_atom_info(test, 0)\ntest = map_atom_info(test, 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = [\"type\",\"atom_0\",\"max_dist_type_x\", \"id_max_overlap_types_x\",\"min_dist_type_x\",\"id_min_overlap_types_x\",\\\n            \"id_inv_max_overlap_types_x\",\"max_inv_dist_type_x\",\"min_inv_dist_type_x\",\"id_inv_min_overlap_types_x\",\\\n           \"atom_1\",\"max_dist_type_y\", \"id_max_overlap_types_y\",\"min_dist_type_y\",\"id_min_overlap_types_y\",\\\n            \"id_inv_max_overlap_types_y\",\"max_inv_dist_type_yx\",\"min_inv_dist_type_y\",\"id_inv_min_overlap_types_y\",\\\n           \"max_inv_dist_type_y\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_p_0 = train[['x_0', 'y_0', 'z_0']].values\ntrain_p_1 = train[['x_1', 'y_1', 'z_1']].values\ntest_p_0 = test[['x_0', 'y_0', 'z_0']].values\ntest_p_1 = test[['x_1', 'y_1', 'z_1']].values\n\ntrain['dist'] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\ntrain['inv_dist'] = 1.0/np.linalg.norm(train_p_0 - train_p_1, axis=1)\ntest['dist'] = np.linalg.norm(test_p_0 - test_p_1, axis=1)\ntest['inv_dist'] = 1.0/np.linalg.norm(test_p_0 - test_p_1, axis=1)\ntrain['dist_x'] = (train['x_0'] - train['x_1']) ** 2\ntrain['inv_x'] = 1.0/(train['x_0'] - train['x_1']) ** 2\ntest['dist_x'] = (test['x_0'] - test['x_1']) ** 2\ntest['inv_x'] = 1.0/(test['x_0'] - test['x_1']) ** 2\ntrain['dist_y'] = (train['y_0'] - train['y_1']) ** 2\ntrain['inv_y'] = 1.0/(train['y_0'] - train['y_1']) ** 2\ntest['dist_y'] = (test['y_0'] - test['y_1']) ** 2\ntest['inv_y'] = 1.0/(test['y_0'] - test['y_1']) ** 2\ntrain['dist_z'] = (train['z_0'] - train['z_1']) ** 2\ntrain['inv_z'] = 1.0/(train['z_0'] - train['z_1']) ** 2\ntest['dist_z'] = (test['z_0'] - test['z_1']) ** 2\ntest['inv_z'] = 1.0/(test['z_0'] - test['z_1']) ** 2\n\ntrain['type_0'] = train['type'].apply(lambda x: x[0])\ntest['type_0'] = test['type'].apply(lambda x: x[0])\n\ntrain = create_features(train)\ntest = create_features(test)\n\n#test  = test.merge(max_at_id,how=\"inner\",on=\"molecule_name\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest  = reduce_mem_usage(test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.fillna(0, inplace=True)\ntest.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del structures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train[\"atomic_mass_0\"]=train.atom_0.apply(give_mass)\n#train[\"atomic_mass_1\"]= train.atom_1.apply(give_mass)\ntrain[\"boltzmann_ns\"]=train.dist.apply(boltzmann_ns)\ntrain[\"boltzmann_sq\"]=train.dist.apply(boltzmann_sq)\ntrain[\"boltzmann_x_ns\"]=train.dist_x.apply(boltzmann_ns)\ntrain[\"boltzmann_x_sq\"]=train.dist_x.apply(boltzmann_sq)\ntrain[\"boltzmann_y_ns\"]=train.dist_y.apply(boltzmann_ns)\ntrain[\"boltzmann_y_sq\"]=train.dist_y.apply(boltzmann_sq)\ntrain[\"boltzmann_z_ns\"]=train.dist_y.apply(boltzmann_ns)\ntrain[\"boltzmann_z_sq\"]=train.dist_y.apply(boltzmann_sq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test[\"atomic_mass_0\"]=test.atom_0.apply(give_mass)\n#test[\"atomic_mass_1\"]= test.atom_1.apply(give_mass)\ntest[\"boltzmann_ns\"]=test.dist.apply(boltzmann_ns)\ntest[\"boltzmann_sq\"]=test.dist.apply(boltzmann_sq)\ntest[\"boltzmann_x_ns\"]=test.dist_x.apply(boltzmann_ns)\ntest[\"boltzmann_x_sq\"]=test.dist_x.apply(boltzmann_sq)\ntest[\"boltzmann_y_ns\"]=test.dist_y.apply(boltzmann_ns)\ntest[\"boltzmann_y_sq\"]=test.dist_y.apply(boltzmann_sq)\ntest[\"boltzmann_z_ns\"]=test.dist_y.apply(boltzmann_ns)\ntest[\"boltzmann_z_sq\"]=test.dist_y.apply(boltzmann_sq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#append the angle info \nangles = pd.read_csv(\"../input/feature-engineering-1/structures_angles.csv\")\nangles = reduce_mem_usage(angles)\nangles.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"angles = angles.drop(labels=[\"x\",\"y\",\"z\"],axis=1)\nangles.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(train, angles, how=\"left\", left_on=[\"molecule_name\",\"atom_index_0\"], right_on=[\"molecule_name\",\"atom_index\"])\ntest = pd.merge(test,angles, how=\"left\",left_on=[\"molecule_name\",\"atom_index_0\"],right_on=[\"molecule_name\",\"atom_index\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#log_e(avg_summ_elec_atom_x *   max_dist_x) and \n# * max_overlaps_x, min_dist_x, min_overlaps_x, avg_dist_x, avg_overlaps_x\ntrain[\"log_elec_maxoverlap_x\"]=( train[\"avg_summ_elec_atom_x\"]*train[\"max_overlaps_x\"])\ntrain[\"log_elec_minoverlap_x\"]=( train[\"avg_summ_elec_atom_x\"]*train[\"min_overlaps_x\"])\ntrain[\"log_elec_avgdist_x\"]=( train[\"avg_summ_elec_atom_x\"]*train[\"avg_dist_x\"])\ntrain[\"log_elec_avgoverlaps_x\"]=( train[\"avg_summ_elec_atom_x\"]*train[\"avg_overlaps_x\"])\n\ntrain[\"log_elec_maxoverlap_y\"]=( train[\"avg_summ_elec_atom_y\"]*train[\"max_overlaps_y\"])\ntrain[\"log_elec_minoverlap_y\"]=( train[\"avg_summ_elec_atom_y\"]*train[\"min_overlaps_y\"])\ntrain[\"log_elec_avgdist_y\"]=( train[\"avg_summ_elec_atom_y\"]*train[\"avg_dist_y\"])\ntrain[\"log_elec_avgoverlaps_y\"]=( train[\"avg_summ_elec_atom_y\"]*train[\"avg_overlaps_y\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#log_e(avg_summ_elec_atom_x *   max_dist_x) and \n# * max_overlaps_x, min_dist_x, min_overlaps_x, avg_dist_x, avg_overlaps_x\ntest[\"log_elec_maxoverlap_x\"]=( test[\"avg_summ_elec_atom_x\"]*test[\"max_overlaps_x\"])\ntest[\"log_elec_minoverlap_x\"]=( test[\"avg_summ_elec_atom_x\"]*test[\"min_overlaps_x\"])\ntest[\"log_elec_avgdist_x\"]=( test[\"avg_summ_elec_atom_x\"]*test[\"avg_dist_x\"])\ntest[\"log_elec_avgoverlaps_x\"]=( test[\"avg_summ_elec_atom_x\"]*test[\"avg_overlaps_x\"])\n\ntest[\"log_elec_maxoverlap_y\"]=( test[\"avg_summ_elec_atom_y\"]*test[\"max_overlaps_y\"])\ntest[\"log_elec_minoverlap_y\"]=( test[\"avg_summ_elec_atom_y\"]*test[\"min_overlaps_y\"])\ntest[\"log_elec_avgdist_y\"]=( test[\"avg_summ_elec_atom_y\"]*test[\"avg_dist_y\"])\ntest[\"log_elec_avgoverlaps_y\"]=( test[\"avg_summ_elec_atom_y\"]*test[\"avg_overlaps_y\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"diff_elec_maxoverlap\"]=(train[\"log_elec_maxoverlap_x\"]- train[\"log_elec_maxoverlap_y\"])\ntrain[\"diff_elec_minoverlap\"]=(train[\"log_elec_minoverlap_x\"]- train[\"log_elec_minoverlap_y\"])\ntrain[\"diff_elec_avgdist\"]=(train[\"log_elec_avgdist_x\"]- train[\"log_elec_avgdist_y\"])\ntrain[\"diff_elecavgoverlaps\"]=(train[\"log_elec_avgoverlaps_x\"]- train[\"log_elec_avgoverlaps_y\"])\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"diff_elec_maxoverlap\"]=(test[\"log_elec_maxoverlap_x\"]- test[\"log_elec_maxoverlap_y\"])\ntest[\"diff_elec_minoverlap\"]=(test[\"log_elec_minoverlap_x\"]- test[\"log_elec_minoverlap_y\"])\ntest[\"diff_elec_avgdist\"]=(test[\"log_elec_avgdist_x\"]- test[\"log_elec_avgdist_y\"])\ntest[\"diff_elecavgoverlaps\"]=(test[\"log_elec_avgoverlaps_x\"]- test[\"log_elec_avgoverlaps_y\"])\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#log_e(avg_summ_elec_atom_x *   max_dist_x) and \n# * max_overlaps_x, min_dist_x, min_overlaps_x, avg_dist_x, avg_overlaps_x\ntrain[\"ratio_elec_maxoverlap_x\"]=( train[\"avg_summ_elec_atom_x\"]/train[\"max_overlaps_x\"])\ntrain[\"ratio_elec_minoverlap_x\"]=( train[\"avg_summ_elec_atom_x\"]/train[\"min_overlaps_x\"])\ntrain[\"ratio_elec_avgdist_x\"]=( train[\"avg_summ_elec_atom_x\"]/train[\"avg_dist_x\"])\ntrain[\"ratio_elec_avgoverlaps_x\"]=( train[\"avg_summ_elec_atom_x\"]/train[\"avg_overlaps_x\"])\n\ntrain[\"ratio_elec_maxoverlap_y\"]=( train[\"avg_summ_elec_atom_y\"]/train[\"max_overlaps_y\"])\ntrain[\"ratio_elec_minoverlap_y\"]=( train[\"avg_summ_elec_atom_y\"]/train[\"min_overlaps_y\"])\ntrain[\"ratio_elec_avgdist_y\"]=( train[\"avg_summ_elec_atom_y\"]/train[\"avg_dist_y\"])\ntrain[\"ratio_elec_avgoverlaps_y\"]=( train[\"avg_summ_elec_atom_y\"]/train[\"avg_overlaps_y\"])\n\ntrain[\"diff_ratio_elec_maxoverlap\"]=(train[\"ratio_elec_maxoverlap_x\"]- train[\"ratio_elec_maxoverlap_y\"])\ntrain[\"diff_ratio_elec_minoverlap\"]=(train[\"ratio_elec_minoverlap_x\"]- train[\"ratio_elec_minoverlap_y\"])\ntrain[\"diff_ratio_elec_avgdist\"]=(train[\"ratio_elec_avgdist_x\"]- train[\"ratio_elec_avgdist_y\"])\ntrain[\"diff_ratio_elecavgoverlaps\"]=(train[\"ratio_elec_avgoverlaps_x\"]- train[\"ratio_elec_avgoverlaps_y\"])\ntrain.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#log_e(avg_summ_elec_atom_x *   max_dist_x) and \n# * max_overlaps_x, min_dist_x, min_overlaps_x, avg_dist_x, avg_overlaps_x\ntest[\"ratio_elec_maxoverlap_x\"]=( test[\"avg_summ_elec_atom_x\"]/test[\"max_overlaps_x\"])\ntest[\"ratio_elec_minoverlap_x\"]=( test[\"avg_summ_elec_atom_x\"]/test[\"min_overlaps_x\"])\ntest[\"ratio_elec_avgdist_x\"]=( test[\"avg_summ_elec_atom_x\"]/test[\"avg_dist_x\"])\ntest[\"ratio_elec_avgoverlaps_x\"]=( test[\"avg_summ_elec_atom_x\"]/test[\"avg_overlaps_x\"])\n\ntest[\"ratio_elec_maxoverlap_y\"]=( test[\"avg_summ_elec_atom_y\"]/test[\"max_overlaps_y\"])\ntest[\"ratio_elec_minoverlap_y\"]=( test[\"avg_summ_elec_atom_y\"]/test[\"min_overlaps_y\"])\ntest[\"ratio_elec_avgdist_y\"]=( test[\"avg_summ_elec_atom_y\"]/test[\"avg_dist_y\"])\ntest[\"ratio_elec_avgoverlaps_y\"]=( test[\"avg_summ_elec_atom_y\"]/test[\"avg_overlaps_y\"])\n\ntest[\"diff_ratio_elec_maxoverlap\"]=(test[\"ratio_elec_maxoverlap_x\"]- test[\"ratio_elec_maxoverlap_y\"])\ntest[\"diff_ratio_elec_minoverlap\"]=(test[\"ratio_elec_minoverlap_x\"]- test[\"ratio_elec_minoverlap_y\"])\ntest[\"diff_ratio_elec_avgdist\"]=(test[\"ratio_elec_avgdist_x\"]- test[\"ratio_elec_avgdist_y\"])\ntest[\"diff_ratio_elecavgoverlaps\"]=(test[\"ratio_elec_avgoverlaps_x\"]- test[\"ratio_elec_avgoverlaps_y\"])\ntest.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#try yo drop the coords \ntrain.drop(labels=[\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\"], axis=1, inplace=True)\ntest.drop(labels=[\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in cat_cols:\n    if f in train.columns:\n        lbl = LabelEncoder()\n        lbl.fit(list(train[f].values) + list(test[f].values))\n        train[f] = lbl.transform(list(train[f].values))\n        test[f] = lbl.transform(list(test[f].values))\n        \ntrain.fillna(0, inplace=True)\ntest.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = [\"dist_z\", \"std_inv_overlaps_x\", \"max_angle\",\"diff_elec_maxoverlap\",\\\n\"mean_dihedral\", \"max_dihedral\", \"std_boltz_dihedral\", \"std_dihedral\",\\\n\"dist_y\", \"mean_boltz_dihedral\", \"std_inv_dist_x\", \"std_angle\", \"mean_angle\",\\\n\"min_angle\", \"dist_x\", \"diff_elecavgoverlaps\",\"std_inv_dist_y\",\"std_inv_overlaps_y\",\\\n\"min_dihedral\",\"diff_elec_avgdist\",\"std_overlaps_x\",\"diff_ratio_elec_maxoverlap\",\n\"mean_noboltz_dihedral\",\"diff_ratio_elec_avgdist\",\"std_devs_x\",\"diff_elec_minoverlap\",\\\n\"std_noboltz_dihedral\",\"mean_boltz_angle\",\"molecule_atom_1_dist_std\",\n\"std_boltz_angle\",\"molecule_atom_1_inv_dist_std\",\"std_noboltz_angle\",\"molecule_atom_index_1_dist_std_div\",\\\n\"diff_ratio_elec_minoverlap\",\"std_devs_y\",\"std_overlaps_y\",\"boltzmann_y_ns\",\"avg_inv_dist_x\",\\\n\"fc\",\"sd\",\"dso\",\"id\",\"molecule_name\", \"scalar_coupling_constant\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train[cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_df.drop(['id', 'molecule_name', 'scalar_coupling_constant'],axis=1)#,'atom',\"inv_x\",\"inv_y\",\"inv_z\"], axis=1)\ny = train_df['scalar_coupling_constant']\n\n#X_test = test.drop(['id', 'molecule_name'],axis=1)#,'atom',\"inv_x\",\"inv_y\",\"inv_z\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport tensorflow as tf\nfrom keras.layers import Dense, Input, Activation\nfrom keras.layers import BatchNormalization,Add,Dropout\nfrom keras.optimizers import Adam\nfrom keras.models import Model, load_model\nfrom keras import callbacks\nfrom keras import backend as K\nfrom keras.layers.advanced_activations import LeakyReLU\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(action=\"ignore\",category=DeprecationWarning)\nwarnings.filterwarnings(action=\"ignore\",category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_history(history, label):\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Loss for %s' % label)\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    _= plt.legend(['Train','Validation'], loc='upper left')\n    plt.show()\n\ndef create_nn_model(input_shape):\n    inp = Input(shape=(input_shape,))\n    x = Dense(256)(inp)\n    x = BatchNormalization()(x)\n    x = LeakyReLU(alpha=0.05)(x)\n    x = Dropout(0.4)(x)\n    x = Dense(1024)(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU(alpha=0.05)(x)\n    x = Dropout(0.2)(x)\n    x = Dense(1024)(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU(alpha=0.05)(x)\n    x = Dropout(0.2)(x)\n    x = Dense(512)(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU(alpha=0.05)(x)\n    x = Dropout(0.4)(x)\n    x = Dense(512)(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU(alpha=0.05)(x)\n    #x = Dropout(0.4)(x)\n    x = Dense(256)(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU(alpha=0.05)(x)\n    x = Dropout(0.4)(x)\n    out2 = Dense(1, activation=\"linear\")(x)#1 vector scalar coupling\n    model = Model(inputs=inp, outputs=[out2])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score=[]\ncv_score_total=0\nepoch_n = 300\nverbose = 0\nbatch_size = 2048\n    \n# Set to True if we want to train from scratch.  False will reuse saved models as a starting point.\nretrain =True\n\n\n# Set up GPU preferences\nconfig = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 2} ) \nconfig.gpu_options.allow_growth = True\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.6\nsess = tf.Session(config=config) \nK.set_session(sess)\n\n\n# import pickle\n# pickle.dump(df_train, open('train_ds.pkl', 'wb'))\n\nX.fillna(0, inplace=True)\n# Standard Scaler from sklearn does seem to work better here than other Scalers\ninput_data = StandardScaler().fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntarget_data=y.values\n\n#following parameters should be adjusted to control the loss function\n#if all parameters are zero, attractors do not work. (-> simple neural network)\n\n# Simple split to provide us a validation set to do our CV checks with\ntrain_index, cv_index = train_test_split(np.arange(len(X)),random_state=111, test_size=0.1)\n\n# Split all our input and targets by train and cv indexes\ntrain_input=input_data[train_index]\ncv_input=input_data[cv_index]\ntrain_target_1=target_data[train_index]\ncv_target_1=target_data[cv_index]\n\n# Build the Neural Net\nnn_model=create_nn_model(train_input.shape[1])\n\n# If retrain==False, then we load a previous saved model as a starting point.\nif not retrain:\n    nn_model = load_model(model_name_rd)\n\nnn_model.compile(loss='mae', optimizer=Adam())#, metrics=[auc])\n\n# Callback for Early Stopping... May want to raise the min_delta for small numbers of epochs\nes = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=8,verbose=1, mode='auto', restore_best_weights=True)\n# Callback for Reducing the Learning Rate... when the monitor levels out for 'patience' epochs, then the LR is reduced\nrlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,patience=7, min_lr=1e-6, mode='auto', verbose=1)\n# Save the best value of the model for future use\n\nhistory = nn_model.fit(train_input,[train_target_1], \n        validation_data=(cv_input,[cv_target_1]), \n        callbacks=[es, rlr], epochs=epoch_n, batch_size=batch_size, verbose=1)\n\ncv_predict=nn_model.predict(cv_input)\nplot_history(history, mol_type)\n\naccuracy=np.mean(np.abs(cv_target_1-cv_predict[:,0]))\ncv_score.append(np.log(accuracy))\ncv_score_total+=np.log(accuracy)\n\n# Predict on the test data set using our trained model\ntest_predict=nn_model.predict(test_input)\n\n# for each molecule type we'll grab the predicted values\n#test_prediction[df_test[\"type\"]==mol_type]=test_predict[0][:,0]\nK.clear_session()\n\n#cv_score_total/=len(mol_types)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\naccuracy=np.mean(np.abs(cv_target_1-cv_predict[:,0]))\ncv_score.append(np.log(accuracy))\ncv_score_total+=np.log(accuracy)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}