{"cells":[{"metadata":{},"cell_type":"markdown","source":"I guess most people prefer gradient boosting models to neural networks in this conpetition. But I don't have enough domain knowledge, I relied on the neural network model first. In my model, the additional data(mulliken charge and tensors) are used as the output of the middle of neural network model. It slightly improved the score."},{"metadata":{},"cell_type":"markdown","source":"This is my first kernel and I am a beginner of both data science and programming. Any comments and advices are welcome!"},{"metadata":{},"cell_type":"markdown","source":"## Import libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf\nfrom keras.layers import Dense, Input, Activation\nfrom keras.layers import BatchNormalization,Add\nfrom keras.optimizers import Adam\nfrom keras.models import Model, load_model\nfrom keras import callbacks\nfrom keras import backend as K\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(action=\"ignore\",category=DeprecationWarning)\nwarnings.filterwarnings(action=\"ignore\",category=FutureWarning)\n","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## Load files\nload the additional data as well."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train=pd.read_csv('../input/train.csv')\ndf_test=pd.read_csv('../input/test.csv')\ndf_struct=pd.read_csv('../input/structures.csv')\n\n#df_train_sub_potential=pd.read_csv('../input/potential_energy.csv')\n#df_train_sub_moment=pd.read_csv('../input/dipole_moments.csv')\ndf_train_sub_charge=pd.read_csv('../input/mulliken_charges.csv')\ndf_train_sub_tensor=pd.read_csv('../input/magnetic_shielding_tensors.csv')\n","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\nLet's merge all data and create features. The additional features are only used for training session.It will come up later."},{"metadata":{},"cell_type":"markdown","source":"I use this great kernel. \nhttps://www.kaggle.com/seriousran/just-speed-up-calculate-distance-from-benchmark"},{"metadata":{"trusted":true},"cell_type":"code","source":"def map_atom_info(df_1,df_2, atom_idx):\n    df = pd.merge(df_1, df_2, how = 'left',\n                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n                  right_on = ['molecule_name',  'atom_index'])\n    df = df.drop('atom_index', axis=1)\n\n    return df\n\n\n\nfor atom_idx in [0,1]:\n    df_train = map_atom_info(df_train,df_struct, atom_idx)\n    df_train = map_atom_info(df_train,df_train_sub_charge, atom_idx)\n    df_train = map_atom_info(df_train,df_train_sub_tensor, atom_idx)\n    df_train = df_train.rename(columns={'atom': f'atom_{atom_idx}',\n                                        'x': f'x_{atom_idx}',\n                                        'y': f'y_{atom_idx}',\n                                        'z': f'z_{atom_idx}',\n                                        'mulliken_charge': f'charge_{atom_idx}',\n                                        'XX': f'XX_{atom_idx}',\n                                        'YX': f'YX_{atom_idx}',\n                                        'ZX': f'ZX_{atom_idx}',\n                                        'XY': f'XY_{atom_idx}',\n                                        'YY': f'YY_{atom_idx}',\n                                        'ZY': f'ZY_{atom_idx}',\n                                        'XZ': f'XZ_{atom_idx}',\n                                        'YZ': f'YZ_{atom_idx}',\n                                        'ZZ': f'ZZ_{atom_idx}',})\n    df_test = map_atom_info(df_test,df_struct, atom_idx)\n    df_test = df_test.rename(columns={'atom': f'atom_{atom_idx}',\n                                'x': f'x_{atom_idx}',\n                                'y': f'y_{atom_idx}',\n                                'z': f'z_{atom_idx}'})\n    #add some features\n    df_struct['c_x']=df_struct.groupby('molecule_name')['x'].transform('mean')\n    df_struct['c_y']=df_struct.groupby('molecule_name')['y'].transform('mean')\n    df_struct['c_z']=df_struct.groupby('molecule_name')['z'].transform('mean')\n    df_struct['atom_n']=df_struct.groupby('molecule_name')['atom_index'].transform('max')\n\n","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distance is very effective feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_features(df):\n    df['dx']=df['x_1']-df['x_0']\n    df['dy']=df['y_1']-df['y_0']\n    df['dz']=df['z_1']-df['z_0']\n    df['distance']=(df['dx']**2+df['dy']**2+df['dz']**2)**(1/2)\n    return df\ndf_train=make_features(df_train)\ndf_test=make_features(df_test) ","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Cosine angles also works.\nhttps://www.kaggle.com/kmat2019/effective-feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_closest_farthest(df):\n    df_temp=df.loc[:,[\"molecule_name\",\"atom_index_0\",\"atom_index_1\",\"distance\",\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\"]].copy()\n    df_temp_=df_temp.copy()\n    df_temp_= df_temp_.rename(columns={'atom_index_0': 'atom_index_1',\n                                       'atom_index_1': 'atom_index_0',\n                                       'x_0': 'x_1',\n                                       'y_0': 'y_1',\n                                       'z_0': 'z_1',\n                                       'x_1': 'x_0',\n                                       'y_1': 'y_0',\n                                       'z_1': 'z_0'})\n    df_temp_all=pd.concat((df_temp,df_temp_),axis=0)\n\n    df_temp_all[\"min_distance\"]=df_temp_all.groupby(['molecule_name', 'atom_index_0'])['distance'].transform('min')\n    df_temp_all[\"max_distance\"]=df_temp_all.groupby(['molecule_name', 'atom_index_0'])['distance'].transform('max')\n    \n    df_temp= df_temp_all[df_temp_all[\"min_distance\"]==df_temp_all[\"distance\"]].copy()\n    df_temp=df_temp.drop(['x_0','y_0','z_0','min_distance'], axis=1)\n    df_temp= df_temp.rename(columns={'atom_index_0': 'atom_index',\n                                         'atom_index_1': 'atom_index_closest',\n                                         'distance': 'distance_closest',\n                                         'x_1': 'x_closest',\n                                         'y_1': 'y_closest',\n                                         'z_1': 'z_closest'})\n\n    for atom_idx in [0,1]:\n        df = map_atom_info(df,df_temp, atom_idx)\n        df = df.rename(columns={'atom_index_closest': f'atom_index_closest_{atom_idx}',\n                                        'distance_closest': f'distance_closest_{atom_idx}',\n                                        'x_closest': f'x_closest_{atom_idx}',\n                                        'y_closest': f'y_closest_{atom_idx}',\n                                        'z_closest': f'z_closest_{atom_idx}'})\n\n    df_temp= df_temp_all[df_temp_all[\"max_distance\"]==df_temp_all[\"distance\"]].copy()\n    df_temp=df_temp.drop(['x_0','y_0','z_0','max_distance'], axis=1)\n    df_temp= df_temp.rename(columns={'atom_index_0': 'atom_index',\n                                         'atom_index_1': 'atom_index_farthest',\n                                         'distance': 'distance_farthest',\n                                         'x_1': 'x_farthest',\n                                         'y_1': 'y_farthest',\n                                         'z_1': 'z_farthest'})\n\n    for atom_idx in [0,1]:\n        df = map_atom_info(df,df_temp, atom_idx)\n        df = df.rename(columns={'atom_index_farthest': f'atom_index_farthest_{atom_idx}',\n                                        'distance_farthest': f'distance_farthest_{atom_idx}',\n                                        'x_farthest': f'x_farthest_{atom_idx}',\n                                        'y_farthest': f'y_farthest_{atom_idx}',\n                                        'z_farthest': f'z_farthest_{atom_idx}'})\n    return df\n    \ndf_train=get_closest_farthest(df_train)    \ndf_test=get_closest_farthest(df_test)    ","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_cos_features(df):\n    df[\"distance_center0\"]=((df['x_0']-df['c_x'])**2+(df['y_0']-df['c_y'])**2+(df['z_0']-df['c_z'])**2)**(1/2)\n    df[\"distance_center1\"]=((df['x_0']-df['c_x'])**2+(df['y_0']-df['c_y'])**2+(df['z_0']-df['c_z'])**2)**(1/2)\n    df[\"distance_c0\"]=((df['x_0']-df['x_closest_0'])**2+(df['y_0']-df['y_closest_0'])**2+(df['z_0']-df['z_closest_0'])**2)**(1/2)\n    df[\"distance_c1\"]=((df['x_1']-df['x_closest_1'])**2+(df['y_1']-df['y_closest_1'])**2+(df['z_1']-df['z_closest_1'])**2)**(1/2)\n    df[\"distance_f0\"]=((df['x_0']-df['x_farthest_0'])**2+(df['y_0']-df['y_farthest_0'])**2+(df['z_0']-df['z_farthest_0'])**2)**(1/2)\n    df[\"distance_f1\"]=((df['x_1']-df['x_farthest_1'])**2+(df['y_1']-df['y_farthest_1'])**2+(df['z_1']-df['z_farthest_1'])**2)**(1/2)\n    df[\"vec_center0_x\"]=(df['x_0']-df['c_x'])/(df[\"distance_center0\"]+1e-10)\n    df[\"vec_center0_y\"]=(df['y_0']-df['c_y'])/(df[\"distance_center0\"]+1e-10)\n    df[\"vec_center0_z\"]=(df['z_0']-df['c_z'])/(df[\"distance_center0\"]+1e-10)\n    df[\"vec_center1_x\"]=(df['x_1']-df['c_x'])/(df[\"distance_center1\"]+1e-10)\n    df[\"vec_center1_y\"]=(df['y_1']-df['c_y'])/(df[\"distance_center1\"]+1e-10)\n    df[\"vec_center1_z\"]=(df['z_1']-df['c_z'])/(df[\"distance_center1\"]+1e-10)\n    df[\"vec_c0_x\"]=(df['x_0']-df['x_closest_0'])/(df[\"distance_c0\"]+1e-10)\n    df[\"vec_c0_y\"]=(df['y_0']-df['y_closest_0'])/(df[\"distance_c0\"]+1e-10)\n    df[\"vec_c0_z\"]=(df['z_0']-df['z_closest_0'])/(df[\"distance_c0\"]+1e-10)\n    df[\"vec_c1_x\"]=(df['x_1']-df['x_closest_1'])/(df[\"distance_c1\"]+1e-10)\n    df[\"vec_c1_y\"]=(df['y_1']-df['y_closest_1'])/(df[\"distance_c1\"]+1e-10)\n    df[\"vec_c1_z\"]=(df['z_1']-df['z_closest_1'])/(df[\"distance_c1\"]+1e-10)\n    df[\"vec_f0_x\"]=(df['x_0']-df['x_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n    df[\"vec_f0_y\"]=(df['y_0']-df['y_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n    df[\"vec_f0_z\"]=(df['z_0']-df['z_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n    df[\"vec_f1_x\"]=(df['x_1']-df['x_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n    df[\"vec_f1_y\"]=(df['y_1']-df['y_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n    df[\"vec_f1_z\"]=(df['z_1']-df['z_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n    df[\"vec_x\"]=(df['x_1']-df['x_0'])/df[\"distance\"]\n    df[\"vec_y\"]=(df['y_1']-df['y_0'])/df[\"distance\"]\n    df[\"vec_z\"]=(df['z_1']-df['z_0'])/df[\"distance\"]\n    df[\"cos_c0_c1\"]=df[\"vec_c0_x\"]*df[\"vec_c1_x\"]+df[\"vec_c0_y\"]*df[\"vec_c1_y\"]+df[\"vec_c0_z\"]*df[\"vec_c1_z\"]\n    df[\"cos_f0_f1\"]=df[\"vec_f0_x\"]*df[\"vec_f1_x\"]+df[\"vec_f0_y\"]*df[\"vec_f1_y\"]+df[\"vec_f0_z\"]*df[\"vec_f1_z\"]\n    df[\"cos_center0_center1\"]=df[\"vec_center0_x\"]*df[\"vec_center1_x\"]+df[\"vec_center0_y\"]*df[\"vec_center1_y\"]+df[\"vec_center0_z\"]*df[\"vec_center1_z\"]\n    df[\"cos_c0\"]=df[\"vec_c0_x\"]*df[\"vec_x\"]+df[\"vec_c0_y\"]*df[\"vec_y\"]+df[\"vec_c0_z\"]*df[\"vec_z\"]\n    df[\"cos_c1\"]=df[\"vec_c1_x\"]*df[\"vec_x\"]+df[\"vec_c1_y\"]*df[\"vec_y\"]+df[\"vec_c1_z\"]*df[\"vec_z\"]\n    df[\"cos_f0\"]=df[\"vec_f0_x\"]*df[\"vec_x\"]+df[\"vec_f0_y\"]*df[\"vec_y\"]+df[\"vec_f0_z\"]*df[\"vec_z\"]\n    df[\"cos_f1\"]=df[\"vec_f1_x\"]*df[\"vec_x\"]+df[\"vec_f1_y\"]*df[\"vec_y\"]+df[\"vec_f1_z\"]*df[\"vec_z\"]\n    df[\"cos_center0\"]=df[\"vec_center0_x\"]*df[\"vec_x\"]+df[\"vec_center0_y\"]*df[\"vec_y\"]+df[\"vec_center0_z\"]*df[\"vec_z\"]\n    df[\"cos_center1\"]=df[\"vec_center1_x\"]*df[\"vec_x\"]+df[\"vec_center1_y\"]*df[\"vec_y\"]+df[\"vec_center1_z\"]*df[\"vec_z\"]\n    df=df.drop(['vec_c0_x','vec_c0_y','vec_c0_z','vec_c1_x','vec_c1_y','vec_c1_z',\n                'vec_f0_x','vec_f0_y','vec_f0_z','vec_f1_x','vec_f1_y','vec_f1_z',\n                'vec_center0_x','vec_center0_y','vec_center0_z','vec_center1_x','vec_center1_y','vec_center1_z',\n                'vec_x','vec_y','vec_z'], axis=1)\n    return df\n    \ndf_train=add_cos_features(df_train)\ndf_test=add_cos_features(df_test)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"   id     molecule_name     ...       cos_center0  cos_center1\n0   0  dsgdb9nsd_000001     ...         -1.000000     0.000007\n1   1  dsgdb9nsd_000001     ...         -0.816487     0.816488\n2   2  dsgdb9nsd_000001     ...         -0.816496     0.816505\n3   3  dsgdb9nsd_000001     ...         -0.816500     0.816509\n4   4  dsgdb9nsd_000001     ...         -1.000000     0.000005\n\n[5 rows x 81 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>molecule_name</th>\n      <th>atom_index_0</th>\n      <th>atom_index_1</th>\n      <th>type</th>\n      <th>scalar_coupling_constant</th>\n      <th>atom_0</th>\n      <th>x_0</th>\n      <th>y_0</th>\n      <th>z_0</th>\n      <th>charge_0</th>\n      <th>XX_0</th>\n      <th>YX_0</th>\n      <th>ZX_0</th>\n      <th>XY_0</th>\n      <th>YY_0</th>\n      <th>ZY_0</th>\n      <th>XZ_0</th>\n      <th>YZ_0</th>\n      <th>ZZ_0</th>\n      <th>atom_1</th>\n      <th>x_1</th>\n      <th>y_1</th>\n      <th>z_1</th>\n      <th>c_x</th>\n      <th>c_y</th>\n      <th>c_z</th>\n      <th>atom_n</th>\n      <th>charge_1</th>\n      <th>XX_1</th>\n      <th>YX_1</th>\n      <th>ZX_1</th>\n      <th>XY_1</th>\n      <th>YY_1</th>\n      <th>ZY_1</th>\n      <th>XZ_1</th>\n      <th>YZ_1</th>\n      <th>ZZ_1</th>\n      <th>dx</th>\n      <th>dy</th>\n      <th>...</th>\n      <th>distance</th>\n      <th>atom_index_closest_0</th>\n      <th>distance_closest_0</th>\n      <th>x_closest_0</th>\n      <th>y_closest_0</th>\n      <th>z_closest_0</th>\n      <th>max_distance_x</th>\n      <th>atom_index_closest_1</th>\n      <th>distance_closest_1</th>\n      <th>x_closest_1</th>\n      <th>y_closest_1</th>\n      <th>z_closest_1</th>\n      <th>max_distance_y</th>\n      <th>atom_index_farthest_0</th>\n      <th>distance_farthest_0</th>\n      <th>x_farthest_0</th>\n      <th>y_farthest_0</th>\n      <th>z_farthest_0</th>\n      <th>min_distance_x</th>\n      <th>atom_index_farthest_1</th>\n      <th>distance_farthest_1</th>\n      <th>x_farthest_1</th>\n      <th>y_farthest_1</th>\n      <th>z_farthest_1</th>\n      <th>min_distance_y</th>\n      <th>distance_center0</th>\n      <th>distance_center1</th>\n      <th>distance_c0</th>\n      <th>distance_c1</th>\n      <th>distance_f0</th>\n      <th>distance_f1</th>\n      <th>cos_c0_c1</th>\n      <th>cos_f0_f1</th>\n      <th>cos_center0_center1</th>\n      <th>cos_c0</th>\n      <th>cos_c1</th>\n      <th>cos_f0</th>\n      <th>cos_f1</th>\n      <th>cos_center0</th>\n      <th>cos_center1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>dsgdb9nsd_000001</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1JHC</td>\n      <td>84.8076</td>\n      <td>H</td>\n      <td>0.002150</td>\n      <td>-0.006031</td>\n      <td>0.001976</td>\n      <td>0.133921</td>\n      <td>31.3410</td>\n      <td>-1.2317</td>\n      <td>4.0544</td>\n      <td>-1.2317</td>\n      <td>28.9546</td>\n      <td>-1.7173</td>\n      <td>4.0546</td>\n      <td>-1.7173</td>\n      <td>34.0861</td>\n      <td>C</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>-0.012689</td>\n      <td>1.085797</td>\n      <td>0.008001</td>\n      <td>4</td>\n      <td>-0.535689</td>\n      <td>195.3150</td>\n      <td>0.0000</td>\n      <td>-0.0001</td>\n      <td>0.0000</td>\n      <td>195.3170</td>\n      <td>0.0007</td>\n      <td>-0.0001</td>\n      <td>0.0007</td>\n      <td>195.3170</td>\n      <td>-0.014849</td>\n      <td>1.091835</td>\n      <td>...</td>\n      <td>1.091953</td>\n      <td>0</td>\n      <td>1.091953</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>1.783157</td>\n      <td>3</td>\n      <td>1.091946</td>\n      <td>-0.540815</td>\n      <td>1.447527</td>\n      <td>-0.876644</td>\n      <td>1.091953</td>\n      <td>4</td>\n      <td>1.783157</td>\n      <td>-0.523814</td>\n      <td>1.437933</td>\n      <td>0.906397</td>\n      <td>1.091953</td>\n      <td>1</td>\n      <td>1.091953</td>\n      <td>0.002150</td>\n      <td>-0.006031</td>\n      <td>0.001976</td>\n      <td>1.091946</td>\n      <td>1.091945</td>\n      <td>1.091945</td>\n      <td>1.091953</td>\n      <td>1.091946</td>\n      <td>1.783157</td>\n      <td>1.091953</td>\n      <td>0.333335</td>\n      <td>-0.816502</td>\n      <td>-0.000007</td>\n      <td>-1.000000</td>\n      <td>-0.333335</td>\n      <td>-0.816502</td>\n      <td>1.000000</td>\n      <td>-1.000000</td>\n      <td>0.000007</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>dsgdb9nsd_000001</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2JHH</td>\n      <td>-11.2570</td>\n      <td>H</td>\n      <td>0.002150</td>\n      <td>-0.006031</td>\n      <td>0.001976</td>\n      <td>0.133921</td>\n      <td>31.3410</td>\n      <td>-1.2317</td>\n      <td>4.0544</td>\n      <td>-1.2317</td>\n      <td>28.9546</td>\n      <td>-1.7173</td>\n      <td>4.0546</td>\n      <td>-1.7173</td>\n      <td>34.0861</td>\n      <td>H</td>\n      <td>1.011731</td>\n      <td>1.463751</td>\n      <td>0.000277</td>\n      <td>-0.012689</td>\n      <td>1.085797</td>\n      <td>0.008001</td>\n      <td>4</td>\n      <td>0.133922</td>\n      <td>31.5814</td>\n      <td>1.2173</td>\n      <td>-4.1474</td>\n      <td>1.2173</td>\n      <td>28.9036</td>\n      <td>-1.6036</td>\n      <td>-4.1476</td>\n      <td>-1.6036</td>\n      <td>33.8967</td>\n      <td>1.009580</td>\n      <td>1.469782</td>\n      <td>...</td>\n      <td>1.783120</td>\n      <td>0</td>\n      <td>1.091953</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>1.783157</td>\n      <td>0</td>\n      <td>1.091952</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>1.783158</td>\n      <td>4</td>\n      <td>1.783157</td>\n      <td>-0.523814</td>\n      <td>1.437933</td>\n      <td>0.906397</td>\n      <td>1.091953</td>\n      <td>3</td>\n      <td>1.783158</td>\n      <td>-0.540815</td>\n      <td>1.447527</td>\n      <td>-0.876644</td>\n      <td>1.091952</td>\n      <td>1.091945</td>\n      <td>1.091945</td>\n      <td>1.091953</td>\n      <td>1.091952</td>\n      <td>1.783157</td>\n      <td>1.783158</td>\n      <td>-0.333287</td>\n      <td>0.000016</td>\n      <td>-0.333304</td>\n      <td>-0.816483</td>\n      <td>0.816482</td>\n      <td>-0.499994</td>\n      <td>0.499995</td>\n      <td>-0.816487</td>\n      <td>0.816488</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>dsgdb9nsd_000001</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2JHH</td>\n      <td>-11.2548</td>\n      <td>H</td>\n      <td>0.002150</td>\n      <td>-0.006031</td>\n      <td>0.001976</td>\n      <td>0.133921</td>\n      <td>31.3410</td>\n      <td>-1.2317</td>\n      <td>4.0544</td>\n      <td>-1.2317</td>\n      <td>28.9546</td>\n      <td>-1.7173</td>\n      <td>4.0546</td>\n      <td>-1.7173</td>\n      <td>34.0861</td>\n      <td>H</td>\n      <td>-0.540815</td>\n      <td>1.447527</td>\n      <td>-0.876644</td>\n      <td>-0.012689</td>\n      <td>1.085797</td>\n      <td>0.008001</td>\n      <td>4</td>\n      <td>0.133923</td>\n      <td>31.5172</td>\n      <td>4.1086</td>\n      <td>1.2723</td>\n      <td>4.1088</td>\n      <td>33.9068</td>\n      <td>1.6950</td>\n      <td>1.2724</td>\n      <td>1.6951</td>\n      <td>28.9579</td>\n      <td>-0.542965</td>\n      <td>1.453558</td>\n      <td>...</td>\n      <td>1.783147</td>\n      <td>0</td>\n      <td>1.091953</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>1.783157</td>\n      <td>0</td>\n      <td>1.091946</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>1.783158</td>\n      <td>4</td>\n      <td>1.783157</td>\n      <td>-0.523814</td>\n      <td>1.437933</td>\n      <td>0.906397</td>\n      <td>1.091953</td>\n      <td>2</td>\n      <td>1.783158</td>\n      <td>1.011731</td>\n      <td>1.463751</td>\n      <td>0.000277</td>\n      <td>1.091946</td>\n      <td>1.091945</td>\n      <td>1.091945</td>\n      <td>1.091953</td>\n      <td>1.091946</td>\n      <td>1.783157</td>\n      <td>1.783158</td>\n      <td>-0.333335</td>\n      <td>-0.000016</td>\n      <td>-0.333338</td>\n      <td>-0.816498</td>\n      <td>0.816496</td>\n      <td>-0.500002</td>\n      <td>0.500018</td>\n      <td>-0.816496</td>\n      <td>0.816505</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>dsgdb9nsd_000001</td>\n      <td>1</td>\n      <td>4</td>\n      <td>2JHH</td>\n      <td>-11.2543</td>\n      <td>H</td>\n      <td>0.002150</td>\n      <td>-0.006031</td>\n      <td>0.001976</td>\n      <td>0.133921</td>\n      <td>31.3410</td>\n      <td>-1.2317</td>\n      <td>4.0544</td>\n      <td>-1.2317</td>\n      <td>28.9546</td>\n      <td>-1.7173</td>\n      <td>4.0546</td>\n      <td>-1.7173</td>\n      <td>34.0861</td>\n      <td>H</td>\n      <td>-0.523814</td>\n      <td>1.437933</td>\n      <td>0.906397</td>\n      <td>-0.012689</td>\n      <td>1.085797</td>\n      <td>0.008001</td>\n      <td>4</td>\n      <td>0.133923</td>\n      <td>31.4029</td>\n      <td>-4.0942</td>\n      <td>-1.1793</td>\n      <td>-4.0944</td>\n      <td>34.0776</td>\n      <td>1.6259</td>\n      <td>-1.1795</td>\n      <td>1.6260</td>\n      <td>28.9013</td>\n      <td>-0.525964</td>\n      <td>1.443964</td>\n      <td>...</td>\n      <td>1.783157</td>\n      <td>0</td>\n      <td>1.091953</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>1.783157</td>\n      <td>0</td>\n      <td>1.091948</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>1.783157</td>\n      <td>4</td>\n      <td>1.783157</td>\n      <td>-0.523814</td>\n      <td>1.437933</td>\n      <td>0.906397</td>\n      <td>1.091953</td>\n      <td>1</td>\n      <td>1.783157</td>\n      <td>0.002150</td>\n      <td>-0.006031</td>\n      <td>0.001976</td>\n      <td>1.091948</td>\n      <td>1.091945</td>\n      <td>1.091945</td>\n      <td>1.091953</td>\n      <td>1.091948</td>\n      <td>1.783157</td>\n      <td>1.783157</td>\n      <td>-0.333347</td>\n      <td>-1.000000</td>\n      <td>-0.333351</td>\n      <td>-0.816502</td>\n      <td>0.816500</td>\n      <td>-1.000000</td>\n      <td>1.000000</td>\n      <td>-0.816500</td>\n      <td>0.816509</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>dsgdb9nsd_000001</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1JHC</td>\n      <td>84.8074</td>\n      <td>H</td>\n      <td>1.011731</td>\n      <td>1.463751</td>\n      <td>0.000277</td>\n      <td>0.133922</td>\n      <td>31.5814</td>\n      <td>1.2173</td>\n      <td>-4.1474</td>\n      <td>1.2173</td>\n      <td>28.9036</td>\n      <td>-1.6036</td>\n      <td>-4.1476</td>\n      <td>-1.6036</td>\n      <td>33.8967</td>\n      <td>C</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>-0.012689</td>\n      <td>1.085797</td>\n      <td>0.008001</td>\n      <td>4</td>\n      <td>-0.535689</td>\n      <td>195.3150</td>\n      <td>0.0000</td>\n      <td>-0.0001</td>\n      <td>0.0000</td>\n      <td>195.3170</td>\n      <td>0.0007</td>\n      <td>-0.0001</td>\n      <td>0.0007</td>\n      <td>195.3170</td>\n      <td>-1.024429</td>\n      <td>-0.377947</td>\n      <td>...</td>\n      <td>1.091952</td>\n      <td>0</td>\n      <td>1.091952</td>\n      <td>-0.012698</td>\n      <td>1.085804</td>\n      <td>0.008001</td>\n      <td>1.783158</td>\n      <td>3</td>\n      <td>1.091946</td>\n      <td>-0.540815</td>\n      <td>1.447527</td>\n      <td>-0.876644</td>\n      <td>1.091953</td>\n      <td>3</td>\n      <td>1.783158</td>\n      <td>-0.540815</td>\n      <td>1.447527</td>\n      <td>-0.876644</td>\n      <td>1.091952</td>\n      <td>1</td>\n      <td>1.091953</td>\n      <td>0.002150</td>\n      <td>-0.006031</td>\n      <td>0.001976</td>\n      <td>1.091946</td>\n      <td>1.091946</td>\n      <td>1.091946</td>\n      <td>1.091952</td>\n      <td>1.091946</td>\n      <td>1.783158</td>\n      <td>1.091953</td>\n      <td>0.333352</td>\n      <td>-0.000028</td>\n      <td>-0.000005</td>\n      <td>-1.000000</td>\n      <td>-0.333352</td>\n      <td>-0.816503</td>\n      <td>-0.333287</td>\n      <td>-1.000000</td>\n      <td>0.000005</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Create Neural Network Model\nI use the additional data (mulliken charge and tensor) as the output of the middle of neural network model. I am too lazy to make the pipeline model. I hope this works as the kind of attractors.*(I don't know how to call this, so let it call as attractors in this kernel)*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_nn_model(input_shape):\n    inp = Input(shape=(input_shape,))\n    x = Dense(256, activation=\"relu\")(inp)\n    x = BatchNormalization()(x)\n    x = Dense(256, activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    x = Dense(256, activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    x = Dense(128, activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    out1 = Dense(2, activation=\"linear\")(x)#mulliken charge 2\n    out2 = Dense(6, activation=\"linear\")(x)#tensor 6(xx,yy,zz)\n    out3 = Dense(12, activation=\"linear\")(x)#tensor 12(others) \n    x = Dense(64, activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    out = Dense(1, activation=\"linear\")(x)#scalar_coupling_constant    \n    model = Model(inputs=inp, outputs=[out,out1,out2,out3])\n    return model","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"create neural networks for each molecule type. (calculate 8 models in total) "},{"metadata":{"trusted":true},"cell_type":"code","source":"mol_types=df_train[\"type\"].unique()\ncv_score=[]\ncv_score_total=0\ntest_prediction=np.zeros(len(df_test))\n\nfor mol_type in mol_types:\n    df_train_=df_train[df_train[\"type\"]==mol_type]\n    df_test_=df_test[df_test[\"type\"]==mol_type]\n    input_features=[\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\",\"c_x\",\"c_y\",\"c_z\",\n                    'x_closest_0','y_closest_0','z_closest_0','x_closest_1','y_closest_1','z_closest_1',\n                    \"distance\",\"distance_center0\",\"distance_center1\",\"distance_c0\",\"distance_c1\",\"distance_f0\",\"distance_f1\",\n                    \"cos_c0_c1\",\"cos_f0_f1\",\"cos_center0_center1\",\"cos_c0\",\"cos_c1\",\"cos_f0\",\"cos_f1\",\"cos_center0\",\"cos_center1\",\n                    \"atom_n\"\n                   ]\n    input_data=StandardScaler().fit_transform(pd.concat([df_train_.loc[:,input_features],df_test_.loc[:,input_features]]))\n    target_data=df_train_.loc[:,\"scalar_coupling_constant\"].values\n    target_data_1=df_train_.loc[:,[\"charge_0\",\"charge_1\"]]\n    target_data_2=df_train_.loc[:,[\"XX_0\",\"YY_0\",\"ZZ_0\",\"XX_1\",\"YY_1\",\"ZZ_1\"]]\n    target_data_3=df_train_.loc[:,[\"YX_0\",\"ZX_0\",\"XY_0\",\"ZY_0\",\"XZ_0\",\"YZ_0\",\"YX_1\",\"ZX_1\",\"XY_1\",\"ZY_1\",\"XZ_1\",\"YZ_1\"]]\n    \n    #following parameters should be adjusted to control the loss function\n    #if all parameters are zero, attractors do not work. (-> simple neural network)\n    m1=1\n    m2=4\n    m3=1\n    target_data_1=m1*(StandardScaler().fit_transform(target_data_1))\n    target_data_2=m2*(StandardScaler().fit_transform(target_data_2))\n    target_data_3=m3*(StandardScaler().fit_transform(target_data_3))\n    \n    train_index, cv_index = train_test_split(np.arange(len(df_train_)),random_state=111, test_size=0.2)\n    \n    train_input=input_data[train_index]\n    cv_input=input_data[cv_index]\n    train_target=target_data[train_index]\n    cv_target=target_data[cv_index]\n    train_target_1=target_data_1[train_index]\n    cv_target_1=target_data_1[cv_index]\n    train_target_2=target_data_2[train_index]\n    cv_target_2=target_data_2[cv_index]\n    train_target_3=target_data_3[train_index]\n    cv_target_3=target_data_3[cv_index]\n    test_input=input_data[len(df_train_):,:]\n\n    epoch_n=12#as trial. Longer epoch improve the score\n    verbose=1\n    nn_model=create_nn_model(train_input.shape[1])\n    nn_model.compile(loss='mae', optimizer=Adam(lr=1e-3))#, metrics=[auc])\n    es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0., patience=12,#val_auc\n                                 verbose=verbose, mode='min', baseline=None, restore_best_weights=True)\n    rlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3,\n                                      patience=4, min_lr=2e-6, mode='min', verbose=verbose)\n    nn_model.fit(train_input,[train_target,train_target_1,train_target_2,train_target_3], \n            validation_data=(cv_input,[cv_target,cv_target_1,cv_target_2,cv_target_3]), \n            callbacks=[es, rlr], epochs=epoch_n, batch_size=64, verbose=verbose)\n    cv_predict=nn_model.predict(cv_input)\n    \n    accuracy=np.mean(np.abs(cv_target-cv_predict[0][:,0]))\n    cv_score.append(np.log(accuracy))\n    cv_score_total+=np.log(accuracy)\n    #print(mol_type,\": log score is \",np.log(accuracy))    \n    test_predict=nn_model.predict(test_input)\n    test_prediction[df_test[\"type\"]==mol_type]=test_predict[0][:,0]\n    K.clear_session()\n\ncv_score_total/=len(mol_types)\n#print(\"total score is\",cv_score_total)","execution_count":12,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nTrain on 567536 samples, validate on 141885 samples\nEpoch 1/10\n567536/567536 [==============================] - 68s 119us/step - loss: 12.1848 - dense_10_loss: 8.9959 - dense_5_loss: 0.4113 - dense_6_loss: 2.0541 - dense_7_loss: 0.7234 - val_loss: 6.0881 - val_dense_10_loss: 3.1220 - val_dense_5_loss: 0.3556 - val_dense_6_loss: 1.8933 - val_dense_7_loss: 0.7172\nEpoch 2/10\n567536/567536 [==============================] - 63s 110us/step - loss: 6.6334 - dense_10_loss: 3.6353 - dense_5_loss: 0.3660 - dense_6_loss: 1.9162 - dense_7_loss: 0.7159 - val_loss: 5.8269 - val_dense_10_loss: 2.9411 - val_dense_5_loss: 0.3398 - val_dense_6_loss: 1.8296 - val_dense_7_loss: 0.7164\nEpoch 3/10\n567536/567536 [==============================] - 65s 115us/step - loss: 6.3914 - dense_10_loss: 3.4618 - dense_5_loss: 0.3512 - dense_6_loss: 1.8629 - dense_7_loss: 0.7155 - val_loss: 5.6961 - val_dense_10_loss: 2.8794 - val_dense_5_loss: 0.3290 - val_dense_6_loss: 1.7722 - val_dense_7_loss: 0.7156\nEpoch 4/10\n567536/567536 [==============================] - 63s 112us/step - loss: 6.2496 - dense_10_loss: 3.3624 - dense_5_loss: 0.3429 - dense_6_loss: 1.8289 - dense_7_loss: 0.7153 - val_loss: 5.4654 - val_dense_10_loss: 2.6911 - val_dense_5_loss: 0.3135 - val_dense_6_loss: 1.7444 - val_dense_7_loss: 0.7164\nEpoch 5/10\n567536/567536 [==============================] - 64s 113us/step - loss: 6.1460 - dense_10_loss: 3.2886 - dense_5_loss: 0.3372 - dense_6_loss: 1.8051 - dense_7_loss: 0.7151 - val_loss: 5.3420 - val_dense_10_loss: 2.5966 - val_dense_5_loss: 0.3071 - val_dense_6_loss: 1.7227 - val_dense_7_loss: 0.7155\nEpoch 6/10\n567536/567536 [==============================] - 61s 108us/step - loss: 6.0894 - dense_10_loss: 3.2542 - dense_5_loss: 0.3328 - dense_6_loss: 1.7875 - dense_7_loss: 0.7150 - val_loss: 5.2751 - val_dense_10_loss: 2.5436 - val_dense_5_loss: 0.3034 - val_dense_6_loss: 1.7121 - val_dense_7_loss: 0.7160\nEpoch 7/10\n567536/567536 [==============================] - 62s 109us/step - loss: 6.0201 - dense_10_loss: 3.2053 - dense_5_loss: 0.3287 - dense_6_loss: 1.7711 - dense_7_loss: 0.7150 - val_loss: 5.2073 - val_dense_10_loss: 2.4819 - val_dense_5_loss: 0.3050 - val_dense_6_loss: 1.7046 - val_dense_7_loss: 0.7157\nEpoch 8/10\n567536/567536 [==============================] - 62s 110us/step - loss: 5.9732 - dense_10_loss: 3.1745 - dense_5_loss: 0.3251 - dense_6_loss: 1.7588 - dense_7_loss: 0.7148 - val_loss: 5.1700 - val_dense_10_loss: 2.4736 - val_dense_5_loss: 0.2980 - val_dense_6_loss: 1.6830 - val_dense_7_loss: 0.7154\nEpoch 9/10\n567536/567536 [==============================] - 63s 112us/step - loss: 5.9121 - dense_10_loss: 3.1267 - dense_5_loss: 0.3230 - dense_6_loss: 1.7477 - dense_7_loss: 0.7147 - val_loss: 5.1416 - val_dense_10_loss: 2.4670 - val_dense_5_loss: 0.2943 - val_dense_6_loss: 1.6647 - val_dense_7_loss: 0.7157\nEpoch 10/10\n567536/567536 [==============================] - 64s 113us/step - loss: 5.8816 - dense_10_loss: 3.1078 - dense_5_loss: 0.3208 - dense_6_loss: 1.7384 - dense_7_loss: 0.7147 - val_loss: 5.1123 - val_dense_10_loss: 2.4392 - val_dense_5_loss: 0.2950 - val_dense_6_loss: 1.6627 - val_dense_7_loss: 0.7155\nTrain on 302428 samples, validate on 75608 samples\nEpoch 1/10\n302428/302428 [==============================] - 35s 115us/step - loss: 4.7966 - dense_10_loss: 1.1822 - dense_5_loss: 0.3202 - dense_6_loss: 2.4819 - dense_7_loss: 0.8122 - val_loss: 3.9728 - val_dense_10_loss: 0.6017 - val_dense_5_loss: 0.2635 - val_dense_6_loss: 2.3053 - val_dense_7_loss: 0.8024\nEpoch 2/10\n302428/302428 [==============================] - 33s 108us/step - loss: 4.1211 - dense_10_loss: 0.7433 - dense_5_loss: 0.2777 - dense_6_loss: 2.2997 - dense_7_loss: 0.8005 - val_loss: 3.7883 - val_dense_10_loss: 0.5371 - val_dense_5_loss: 0.2433 - val_dense_6_loss: 2.2060 - val_dense_7_loss: 0.8020\nEpoch 3/10\n302428/302428 [==============================] - 33s 108us/step - loss: 3.9919 - dense_10_loss: 0.7038 - dense_5_loss: 0.2660 - dense_6_loss: 2.2227 - dense_7_loss: 0.7995 - val_loss: 3.6928 - val_dense_10_loss: 0.5150 - val_dense_5_loss: 0.2385 - val_dense_6_loss: 2.1383 - val_dense_7_loss: 0.8010\nEpoch 4/10\n302428/302428 [==============================] - 32s 107us/step - loss: 3.9100 - dense_10_loss: 0.6781 - dense_5_loss: 0.2595 - dense_6_loss: 2.1734 - dense_7_loss: 0.7990 - val_loss: 3.6819 - val_dense_10_loss: 0.5487 - val_dense_5_loss: 0.2277 - val_dense_6_loss: 2.1051 - val_dense_7_loss: 0.8005\nEpoch 5/10\n302428/302428 [==============================] - 32s 107us/step - loss: 3.8435 - dense_10_loss: 0.6565 - dense_5_loss: 0.2541 - dense_6_loss: 2.1344 - dense_7_loss: 0.7985 - val_loss: 3.6161 - val_dense_10_loss: 0.5186 - val_dense_5_loss: 0.2259 - val_dense_6_loss: 2.0716 - val_dense_7_loss: 0.8000\nEpoch 6/10\n302428/302428 [==============================] - 33s 109us/step - loss: 3.7956 - dense_10_loss: 0.6445 - dense_5_loss: 0.2505 - dense_6_loss: 2.1025 - dense_7_loss: 0.7981 - val_loss: 3.5051 - val_dense_10_loss: 0.4546 - val_dense_5_loss: 0.2203 - val_dense_6_loss: 2.0311 - val_dense_7_loss: 0.7990\nEpoch 7/10\n302428/302428 [==============================] - 33s 107us/step - loss: 3.7563 - dense_10_loss: 0.6341 - dense_5_loss: 0.2472 - dense_6_loss: 2.0772 - dense_7_loss: 0.7978 - val_loss: 3.5294 - val_dense_10_loss: 0.4967 - val_dense_5_loss: 0.2183 - val_dense_6_loss: 2.0155 - val_dense_7_loss: 0.7988\nEpoch 8/10\n302428/302428 [==============================] - 33s 108us/step - loss: 3.7299 - dense_10_loss: 0.6315 - dense_5_loss: 0.2459 - dense_6_loss: 2.0551 - dense_7_loss: 0.7973 - val_loss: 3.4559 - val_dense_10_loss: 0.4487 - val_dense_5_loss: 0.2149 - val_dense_6_loss: 1.9937 - val_dense_7_loss: 0.7986\nEpoch 9/10\n302428/302428 [==============================] - 34s 113us/step - loss: 3.6998 - dense_10_loss: 0.6216 - dense_5_loss: 0.2437 - dense_6_loss: 2.0375 - dense_7_loss: 0.7971 - val_loss: 3.4135 - val_dense_10_loss: 0.4336 - val_dense_5_loss: 0.2151 - val_dense_6_loss: 1.9666 - val_dense_7_loss: 0.7983\nEpoch 10/10\n302428/302428 [==============================] - 33s 109us/step - loss: 3.6758 - dense_10_loss: 0.6161 - dense_5_loss: 0.2422 - dense_6_loss: 2.0207 - dense_7_loss: 0.7968 - val_loss: 3.4060 - val_dense_10_loss: 0.4427 - val_dense_5_loss: 0.2136 - val_dense_6_loss: 1.9518 - val_dense_7_loss: 0.7979\nTrain on 34690 samples, validate on 8673 samples\nEpoch 1/10\n34690/34690 [==============================] - 6s 161us/step - loss: 33.7203 - dense_10_loss: 30.4540 - dense_5_loss: 0.4338 - dense_6_loss: 2.0866 - dense_7_loss: 0.7460 - val_loss: 14.5767 - val_dense_10_loss: 11.2642 - val_dense_5_loss: 0.4860 - val_dense_6_loss: 2.1016 - val_dense_7_loss: 0.7249\nEpoch 2/10\n34690/34690 [==============================] - 4s 111us/step - loss: 5.2141 - dense_10_loss: 2.3097 - dense_5_loss: 0.3491 - dense_6_loss: 1.8626 - dense_7_loss: 0.6927 - val_loss: 4.4491 - val_dense_10_loss: 1.7188 - val_dense_5_loss: 0.3150 - val_dense_6_loss: 1.7309 - val_dense_7_loss: 0.6845\nEpoch 3/10\n34690/34690 [==============================] - 4s 108us/step - loss: 4.8085 - dense_10_loss: 2.0504 - dense_5_loss: 0.3196 - dense_6_loss: 1.7523 - dense_7_loss: 0.6862 - val_loss: 4.4397 - val_dense_10_loss: 1.7998 - val_dense_5_loss: 0.3177 - val_dense_6_loss: 1.6415 - val_dense_7_loss: 0.6807\nEpoch 4/10\n34690/34690 [==============================] - 4s 105us/step - loss: 4.6250 - dense_10_loss: 1.9485 - dense_5_loss: 0.3049 - dense_6_loss: 1.6891 - dense_7_loss: 0.6826 - val_loss: 4.1278 - val_dense_10_loss: 1.5580 - val_dense_5_loss: 0.2770 - val_dense_6_loss: 1.6148 - val_dense_7_loss: 0.6780\nEpoch 5/10\n","name":"stdout"},{"output_type":"stream","text":"34690/34690 [==============================] - 4s 104us/step - loss: 4.5835 - dense_10_loss: 1.9424 - dense_5_loss: 0.2988 - dense_6_loss: 1.6618 - dense_7_loss: 0.6805 - val_loss: 4.0778 - val_dense_10_loss: 1.5491 - val_dense_5_loss: 0.2806 - val_dense_6_loss: 1.5726 - val_dense_7_loss: 0.6756\nEpoch 6/10\n34690/34690 [==============================] - 4s 106us/step - loss: 4.4759 - dense_10_loss: 1.8854 - dense_5_loss: 0.2916 - dense_6_loss: 1.6200 - dense_7_loss: 0.6789 - val_loss: 4.0056 - val_dense_10_loss: 1.5195 - val_dense_5_loss: 0.2582 - val_dense_6_loss: 1.5523 - val_dense_7_loss: 0.6756\nEpoch 7/10\n34690/34690 [==============================] - 4s 109us/step - loss: 4.3782 - dense_10_loss: 1.8308 - dense_5_loss: 0.2826 - dense_6_loss: 1.5878 - dense_7_loss: 0.6770 - val_loss: 3.8475 - val_dense_10_loss: 1.4015 - val_dense_5_loss: 0.2552 - val_dense_6_loss: 1.5179 - val_dense_7_loss: 0.6729\nEpoch 8/10\n34690/34690 [==============================] - 4s 107us/step - loss: 4.3251 - dense_10_loss: 1.8043 - dense_5_loss: 0.2797 - dense_6_loss: 1.5653 - dense_7_loss: 0.6759 - val_loss: 3.7784 - val_dense_10_loss: 1.3675 - val_dense_5_loss: 0.2572 - val_dense_6_loss: 1.4811 - val_dense_7_loss: 0.6727\nEpoch 9/10\n34690/34690 [==============================] - 4s 104us/step - loss: 4.2633 - dense_10_loss: 1.7753 - dense_5_loss: 0.2742 - dense_6_loss: 1.5387 - dense_7_loss: 0.6751 - val_loss: 3.6853 - val_dense_10_loss: 1.2871 - val_dense_5_loss: 0.2462 - val_dense_6_loss: 1.4798 - val_dense_7_loss: 0.6722\nEpoch 10/10\n34690/34690 [==============================] - 4s 106us/step - loss: 4.1959 - dense_10_loss: 1.7256 - dense_5_loss: 0.2708 - dense_6_loss: 1.5248 - dense_7_loss: 0.6747 - val_loss: 3.6541 - val_dense_10_loss: 1.2598 - val_dense_5_loss: 0.2518 - val_dense_6_loss: 1.4703 - val_dense_7_loss: 0.6722\nTrain on 95405 samples, validate on 23852 samples\nEpoch 1/10\n95405/95405 [==============================] - 13s 133us/step - loss: 4.3374 - dense_10_loss: 1.1203 - dense_5_loss: 0.3840 - dense_6_loss: 2.0773 - dense_7_loss: 0.7558 - val_loss: 3.5039 - val_dense_10_loss: 0.6504 - val_dense_5_loss: 0.2810 - val_dense_6_loss: 1.8540 - val_dense_7_loss: 0.7185\nEpoch 2/10\n95405/95405 [==============================] - 10s 109us/step - loss: 3.6159 - dense_10_loss: 0.7506 - dense_5_loss: 0.3011 - dense_6_loss: 1.8448 - dense_7_loss: 0.7195 - val_loss: 3.2779 - val_dense_10_loss: 0.5539 - val_dense_5_loss: 0.2644 - val_dense_6_loss: 1.7457 - val_dense_7_loss: 0.7140\nEpoch 3/10\n95405/95405 [==============================] - 10s 108us/step - loss: 3.4236 - dense_10_loss: 0.6699 - dense_5_loss: 0.2806 - dense_6_loss: 1.7572 - dense_7_loss: 0.7159 - val_loss: 3.0984 - val_dense_10_loss: 0.4765 - val_dense_5_loss: 0.2472 - val_dense_6_loss: 1.6632 - val_dense_7_loss: 0.7115\nEpoch 4/10\n95405/95405 [==============================] - 10s 109us/step - loss: 3.3257 - dense_10_loss: 0.6374 - dense_5_loss: 0.2677 - dense_6_loss: 1.7060 - dense_7_loss: 0.7145 - val_loss: 3.0305 - val_dense_10_loss: 0.4722 - val_dense_5_loss: 0.2311 - val_dense_6_loss: 1.6166 - val_dense_7_loss: 0.7105\nEpoch 5/10\n95405/95405 [==============================] - 11s 112us/step - loss: 3.2511 - dense_10_loss: 0.6168 - dense_5_loss: 0.2586 - dense_6_loss: 1.6620 - dense_7_loss: 0.7137 - val_loss: 2.9345 - val_dense_10_loss: 0.4230 - val_dense_5_loss: 0.2246 - val_dense_6_loss: 1.5773 - val_dense_7_loss: 0.7096\nEpoch 6/10\n95405/95405 [==============================] - 10s 109us/step - loss: 3.1894 - dense_10_loss: 0.5954 - dense_5_loss: 0.2520 - dense_6_loss: 1.6287 - dense_7_loss: 0.7133 - val_loss: 2.9339 - val_dense_10_loss: 0.4405 - val_dense_5_loss: 0.2192 - val_dense_6_loss: 1.5635 - val_dense_7_loss: 0.7108\nEpoch 7/10\n95405/95405 [==============================] - 11s 111us/step - loss: 3.1491 - dense_10_loss: 0.5864 - dense_5_loss: 0.2473 - dense_6_loss: 1.6024 - dense_7_loss: 0.7131 - val_loss: 2.8640 - val_dense_10_loss: 0.4077 - val_dense_5_loss: 0.2125 - val_dense_6_loss: 1.5345 - val_dense_7_loss: 0.7093\nEpoch 8/10\n95405/95405 [==============================] - 10s 110us/step - loss: 3.0962 - dense_10_loss: 0.5640 - dense_5_loss: 0.2417 - dense_6_loss: 1.5776 - dense_7_loss: 0.7128 - val_loss: 2.8210 - val_dense_10_loss: 0.3896 - val_dense_5_loss: 0.2100 - val_dense_6_loss: 1.5118 - val_dense_7_loss: 0.7096\nEpoch 9/10\n95405/95405 [==============================] - 10s 109us/step - loss: 3.0765 - dense_10_loss: 0.5653 - dense_5_loss: 0.2379 - dense_6_loss: 1.5606 - dense_7_loss: 0.7127 - val_loss: 2.8047 - val_dense_10_loss: 0.3907 - val_dense_5_loss: 0.2073 - val_dense_6_loss: 1.4969 - val_dense_7_loss: 0.7098\nEpoch 10/10\n95405/95405 [==============================] - 11s 112us/step - loss: 3.0350 - dense_10_loss: 0.5464 - dense_5_loss: 0.2345 - dense_6_loss: 1.5416 - dense_7_loss: 0.7125 - val_loss: 2.7763 - val_dense_10_loss: 0.3830 - val_dense_5_loss: 0.2027 - val_dense_6_loss: 1.4807 - val_dense_7_loss: 0.7099\nTrain on 912543 samples, validate on 228136 samples\nEpoch 1/10\n912543/912543 [==============================] - 105s 115us/step - loss: 4.4600 - dense_10_loss: 1.4152 - dense_5_loss: 0.2904 - dense_6_loss: 2.0527 - dense_7_loss: 0.7017 - val_loss: 3.8491 - val_dense_10_loss: 1.0666 - val_dense_5_loss: 0.2238 - val_dense_6_loss: 1.8621 - val_dense_7_loss: 0.6967\nEpoch 2/10\n912543/912543 [==============================] - 102s 112us/step - loss: 3.9588 - dense_10_loss: 1.1455 - dense_5_loss: 0.2493 - dense_6_loss: 1.8678 - dense_7_loss: 0.6962 - val_loss: 3.6468 - val_dense_10_loss: 0.9729 - val_dense_5_loss: 0.2124 - val_dense_6_loss: 1.7652 - val_dense_7_loss: 0.6963\nEpoch 3/10\n912543/912543 [==============================] - 102s 111us/step - loss: 3.8223 - dense_10_loss: 1.0792 - dense_5_loss: 0.2388 - dense_6_loss: 1.8085 - dense_7_loss: 0.6959 - val_loss: 3.5079 - val_dense_10_loss: 0.9013 - val_dense_5_loss: 0.2008 - val_dense_6_loss: 1.7094 - val_dense_7_loss: 0.6964\nEpoch 4/10\n912543/912543 [==============================] - 102s 111us/step - loss: 3.7377 - dense_10_loss: 1.0386 - dense_5_loss: 0.2326 - dense_6_loss: 1.7709 - dense_7_loss: 0.6956 - val_loss: 3.4426 - val_dense_10_loss: 0.8626 - val_dense_5_loss: 0.2015 - val_dense_6_loss: 1.6822 - val_dense_7_loss: 0.6962\nEpoch 5/10\n912543/912543 [==============================] - 100s 110us/step - loss: 3.6770 - dense_10_loss: 1.0110 - dense_5_loss: 0.2276 - dense_6_loss: 1.7430 - dense_7_loss: 0.6954 - val_loss: 3.3784 - val_dense_10_loss: 0.8339 - val_dense_5_loss: 0.1923 - val_dense_6_loss: 1.6562 - val_dense_7_loss: 0.6959\nEpoch 6/10\n912543/912543 [==============================] - 104s 114us/step - loss: 3.6278 - dense_10_loss: 0.9873 - dense_5_loss: 0.2239 - dense_6_loss: 1.7214 - dense_7_loss: 0.6952 - val_loss: 3.3297 - val_dense_10_loss: 0.8181 - val_dense_5_loss: 0.1864 - val_dense_6_loss: 1.6294 - val_dense_7_loss: 0.6958\nEpoch 7/10\n912543/912543 [==============================] - 105s 115us/step - loss: 3.5921 - dense_10_loss: 0.9717 - dense_5_loss: 0.2213 - dense_6_loss: 1.7040 - dense_7_loss: 0.6951 - val_loss: 3.2858 - val_dense_10_loss: 0.7945 - val_dense_5_loss: 0.1846 - val_dense_6_loss: 1.6111 - val_dense_7_loss: 0.6955\nEpoch 8/10\n912543/912543 [==============================] - 104s 114us/step - loss: 3.5590 - dense_10_loss: 0.9559 - dense_5_loss: 0.2195 - dense_6_loss: 1.6887 - dense_7_loss: 0.6950 - val_loss: 3.2695 - val_dense_10_loss: 0.7875 - val_dense_5_loss: 0.1864 - val_dense_6_loss: 1.6003 - val_dense_7_loss: 0.6954\nEpoch 9/10\n912543/912543 [==============================] - 104s 114us/step - loss: 3.5369 - dense_10_loss: 0.9473 - dense_5_loss: 0.2175 - dense_6_loss: 1.6772 - dense_7_loss: 0.6948 - val_loss: 3.2245 - val_dense_10_loss: 0.7612 - val_dense_5_loss: 0.1804 - val_dense_6_loss: 1.5872 - val_dense_7_loss: 0.6956\nEpoch 10/10\n912543/912543 [==============================] - 104s 114us/step - loss: 3.5146 - dense_10_loss: 0.9364 - dense_5_loss: 0.2164 - dense_6_loss: 1.6670 - dense_7_loss: 0.6948 - val_loss: 3.2234 - val_dense_10_loss: 0.7714 - val_dense_5_loss: 0.1796 - val_dense_6_loss: 1.5770 - val_dense_7_loss: 0.6954\nTrain on 472493 samples, validate on 118124 samples\nEpoch 1/10\n","name":"stdout"},{"output_type":"stream","text":"472493/472493 [==============================] - 55s 117us/step - loss: 4.5510 - dense_10_loss: 0.9909 - dense_5_loss: 0.3064 - dense_6_loss: 2.4899 - dense_7_loss: 0.7638 - val_loss: 3.9175 - val_dense_10_loss: 0.6516 - val_dense_5_loss: 0.2316 - val_dense_6_loss: 2.2785 - val_dense_7_loss: 0.7557\nEpoch 2/10\n472493/472493 [==============================] - 53s 113us/step - loss: 4.0124 - dense_10_loss: 0.7364 - dense_5_loss: 0.2512 - dense_6_loss: 2.2701 - dense_7_loss: 0.7547 - val_loss: 3.7149 - val_dense_10_loss: 0.5638 - val_dense_5_loss: 0.2111 - val_dense_6_loss: 2.1865 - val_dense_7_loss: 0.7536\nEpoch 3/10\n472493/472493 [==============================] - 52s 110us/step - loss: 3.8690 - dense_10_loss: 0.6865 - dense_5_loss: 0.2389 - dense_6_loss: 2.1895 - dense_7_loss: 0.7542 - val_loss: 3.5873 - val_dense_10_loss: 0.5285 - val_dense_5_loss: 0.2011 - val_dense_6_loss: 2.1038 - val_dense_7_loss: 0.7539\nEpoch 4/10\n472493/472493 [==============================] - 52s 110us/step - loss: 3.7822 - dense_10_loss: 0.6587 - dense_5_loss: 0.2321 - dense_6_loss: 2.1375 - dense_7_loss: 0.7539 - val_loss: 3.4932 - val_dense_10_loss: 0.4890 - val_dense_5_loss: 0.1972 - val_dense_6_loss: 2.0536 - val_dense_7_loss: 0.7533\nEpoch 5/10\n472493/472493 [==============================] - 53s 112us/step - loss: 3.7202 - dense_10_loss: 0.6401 - dense_5_loss: 0.2275 - dense_6_loss: 2.0990 - dense_7_loss: 0.7536 - val_loss: 3.4776 - val_dense_10_loss: 0.5048 - val_dense_5_loss: 0.1920 - val_dense_6_loss: 2.0276 - val_dense_7_loss: 0.7532\nEpoch 6/10\n472493/472493 [==============================] - 53s 111us/step - loss: 3.6742 - dense_10_loss: 0.6282 - dense_5_loss: 0.2244 - dense_6_loss: 2.0681 - dense_7_loss: 0.7535 - val_loss: 3.3895 - val_dense_10_loss: 0.4575 - val_dense_5_loss: 0.1903 - val_dense_6_loss: 1.9890 - val_dense_7_loss: 0.7529\nEpoch 7/10\n472493/472493 [==============================] - 53s 111us/step - loss: 3.6351 - dense_10_loss: 0.6186 - dense_5_loss: 0.2208 - dense_6_loss: 2.0423 - dense_7_loss: 0.7533 - val_loss: 3.3522 - val_dense_10_loss: 0.4436 - val_dense_5_loss: 0.1849 - val_dense_6_loss: 1.9702 - val_dense_7_loss: 0.7536\nEpoch 8/10\n472493/472493 [==============================] - 53s 112us/step - loss: 3.5999 - dense_10_loss: 0.6080 - dense_5_loss: 0.2186 - dense_6_loss: 2.0201 - dense_7_loss: 0.7532 - val_loss: 3.3476 - val_dense_10_loss: 0.4711 - val_dense_5_loss: 0.1845 - val_dense_6_loss: 1.9391 - val_dense_7_loss: 0.7529\nEpoch 9/10\n472493/472493 [==============================] - 52s 110us/step - loss: 3.5744 - dense_10_loss: 0.6027 - dense_5_loss: 0.2172 - dense_6_loss: 2.0014 - dense_7_loss: 0.7530 - val_loss: 3.3145 - val_dense_10_loss: 0.4457 - val_dense_5_loss: 0.1838 - val_dense_6_loss: 1.9325 - val_dense_7_loss: 0.7525\nEpoch 10/10\n472493/472493 [==============================] - 51s 109us/step - loss: 3.5457 - dense_10_loss: 0.5922 - dense_5_loss: 0.2161 - dense_6_loss: 1.9844 - dense_7_loss: 0.7530 - val_loss: 3.2853 - val_dense_10_loss: 0.4468 - val_dense_5_loss: 0.1823 - val_dense_6_loss: 1.9038 - val_dense_7_loss: 0.7524\nTrain on 1208310 samples, validate on 302078 samples\nEpoch 1/10\n1208310/1208310 [==============================] - 135s 112us/step - loss: 4.4247 - dense_10_loss: 1.2173 - dense_5_loss: 0.3248 - dense_6_loss: 2.1805 - dense_7_loss: 0.7021 - val_loss: 3.9001 - val_dense_10_loss: 0.9597 - val_dense_5_loss: 0.2643 - val_dense_6_loss: 1.9803 - val_dense_7_loss: 0.6959\nEpoch 2/10\n1208310/1208310 [==============================] - 133s 110us/step - loss: 3.9773 - dense_10_loss: 0.9932 - dense_5_loss: 0.2841 - dense_6_loss: 2.0022 - dense_7_loss: 0.6978 - val_loss: 3.6981 - val_dense_10_loss: 0.8600 - val_dense_5_loss: 0.2490 - val_dense_6_loss: 1.8936 - val_dense_7_loss: 0.6954\nEpoch 3/10\n1208310/1208310 [==============================] - 134s 111us/step - loss: 3.8282 - dense_10_loss: 0.9336 - dense_5_loss: 0.2707 - dense_6_loss: 1.9265 - dense_7_loss: 0.6974 - val_loss: 3.5828 - val_dense_10_loss: 0.8279 - val_dense_5_loss: 0.2418 - val_dense_6_loss: 1.8174 - val_dense_7_loss: 0.6957\nEpoch 4/10\n1208310/1208310 [==============================] - 135s 112us/step - loss: 3.7182 - dense_10_loss: 0.8926 - dense_5_loss: 0.2604 - dense_6_loss: 1.8681 - dense_7_loss: 0.6971 - val_loss: 3.4739 - val_dense_10_loss: 0.7794 - val_dense_5_loss: 0.2255 - val_dense_6_loss: 1.7733 - val_dense_7_loss: 0.6957\nEpoch 5/10\n1208310/1208310 [==============================] - 131s 109us/step - loss: 3.6522 - dense_10_loss: 0.8667 - dense_5_loss: 0.2541 - dense_6_loss: 1.8345 - dense_7_loss: 0.6969 - val_loss: 3.4168 - val_dense_10_loss: 0.7579 - val_dense_5_loss: 0.2220 - val_dense_6_loss: 1.7416 - val_dense_7_loss: 0.6953\nEpoch 6/10\n1208310/1208310 [==============================] - 133s 110us/step - loss: 3.6042 - dense_10_loss: 0.8473 - dense_5_loss: 0.2498 - dense_6_loss: 1.8104 - dense_7_loss: 0.6968 - val_loss: 3.3817 - val_dense_10_loss: 0.7483 - val_dense_5_loss: 0.2171 - val_dense_6_loss: 1.7209 - val_dense_7_loss: 0.6954\nEpoch 7/10\n1208310/1208310 [==============================] - 138s 114us/step - loss: 3.5681 - dense_10_loss: 0.8321 - dense_5_loss: 0.2470 - dense_6_loss: 1.7924 - dense_7_loss: 0.6966 - val_loss: 3.3439 - val_dense_10_loss: 0.7295 - val_dense_5_loss: 0.2157 - val_dense_6_loss: 1.7035 - val_dense_7_loss: 0.6952\nEpoch 8/10\n1208310/1208310 [==============================] - 137s 113us/step - loss: 3.5366 - dense_10_loss: 0.8194 - dense_5_loss: 0.2441 - dense_6_loss: 1.7765 - dense_7_loss: 0.6966 - val_loss: 3.3275 - val_dense_10_loss: 0.7192 - val_dense_5_loss: 0.2147 - val_dense_6_loss: 1.6987 - val_dense_7_loss: 0.6949\nEpoch 9/10\n1208310/1208310 [==============================] - 133s 110us/step - loss: 3.5131 - dense_10_loss: 0.8088 - dense_5_loss: 0.2426 - dense_6_loss: 1.7652 - dense_7_loss: 0.6965 - val_loss: 3.2810 - val_dense_10_loss: 0.7043 - val_dense_5_loss: 0.2094 - val_dense_6_loss: 1.6722 - val_dense_7_loss: 0.6951\nEpoch 10/10\n1208310/1208310 [==============================] - 131s 108us/step - loss: 3.4929 - dense_10_loss: 0.8005 - dense_5_loss: 0.2410 - dense_6_loss: 1.7549 - dense_7_loss: 0.6964 - val_loss: 3.2528 - val_dense_10_loss: 0.6903 - val_dense_5_loss: 0.2082 - val_dense_6_loss: 1.6592 - val_dense_7_loss: 0.6951\nTrain on 133136 samples, validate on 33285 samples\nEpoch 1/10\n133136/133136 [==============================] - 16s 122us/step - loss: 3.9039 - dense_10_loss: 0.6074 - dense_5_loss: 0.3879 - dense_6_loss: 2.1772 - dense_7_loss: 0.7315 - val_loss: 3.4389 - val_dense_10_loss: 0.4838 - val_dense_5_loss: 0.2949 - val_dense_6_loss: 1.9553 - val_dense_7_loss: 0.7048\nEpoch 2/10\n133136/133136 [==============================] - 14s 104us/step - loss: 3.4043 - dense_10_loss: 0.4644 - dense_5_loss: 0.3087 - dense_6_loss: 1.9301 - dense_7_loss: 0.7010 - val_loss: 3.1848 - val_dense_10_loss: 0.3945 - val_dense_5_loss: 0.2689 - val_dense_6_loss: 1.8214 - val_dense_7_loss: 0.7000\nEpoch 3/10\n133136/133136 [==============================] - 14s 103us/step - loss: 3.2286 - dense_10_loss: 0.4146 - dense_5_loss: 0.2884 - dense_6_loss: 1.8268 - dense_7_loss: 0.6988 - val_loss: 3.0585 - val_dense_10_loss: 0.3613 - val_dense_5_loss: 0.2513 - val_dense_6_loss: 1.7476 - val_dense_7_loss: 0.6984\nEpoch 4/10\n133136/133136 [==============================] - 13s 101us/step - loss: 3.1249 - dense_10_loss: 0.3885 - dense_5_loss: 0.2746 - dense_6_loss: 1.7635 - dense_7_loss: 0.6983 - val_loss: 2.9441 - val_dense_10_loss: 0.3257 - val_dense_5_loss: 0.2371 - val_dense_6_loss: 1.6827 - val_dense_7_loss: 0.6986\nEpoch 5/10\n133136/133136 [==============================] - 14s 103us/step - loss: 3.0482 - dense_10_loss: 0.3687 - dense_5_loss: 0.2648 - dense_6_loss: 1.7167 - dense_7_loss: 0.6980 - val_loss: 2.8956 - val_dense_10_loss: 0.3269 - val_dense_5_loss: 0.2340 - val_dense_6_loss: 1.6368 - val_dense_7_loss: 0.6979\nEpoch 6/10\n133136/133136 [==============================] - 14s 106us/step - loss: 2.9928 - dense_10_loss: 0.3542 - dense_5_loss: 0.2579 - dense_6_loss: 1.6829 - dense_7_loss: 0.6978 - val_loss: 2.8444 - val_dense_10_loss: 0.3048 - val_dense_5_loss: 0.2234 - val_dense_6_loss: 1.6182 - val_dense_7_loss: 0.6981\n","name":"stdout"},{"output_type":"stream","text":"Epoch 7/10\n133136/133136 [==============================] - 14s 103us/step - loss: 2.9503 - dense_10_loss: 0.3461 - dense_5_loss: 0.2532 - dense_6_loss: 1.6531 - dense_7_loss: 0.6978 - val_loss: 2.7823 - val_dense_10_loss: 0.2904 - val_dense_5_loss: 0.2186 - val_dense_6_loss: 1.5756 - val_dense_7_loss: 0.6977\nEpoch 8/10\n133136/133136 [==============================] - 14s 106us/step - loss: 2.9124 - dense_10_loss: 0.3369 - dense_5_loss: 0.2482 - dense_6_loss: 1.6295 - dense_7_loss: 0.6977 - val_loss: 2.7600 - val_dense_10_loss: 0.2836 - val_dense_5_loss: 0.2173 - val_dense_6_loss: 1.5608 - val_dense_7_loss: 0.6983\nEpoch 9/10\n133136/133136 [==============================] - 14s 107us/step - loss: 2.8799 - dense_10_loss: 0.3306 - dense_5_loss: 0.2455 - dense_6_loss: 1.6063 - dense_7_loss: 0.6975 - val_loss: 2.7320 - val_dense_10_loss: 0.2806 - val_dense_5_loss: 0.2119 - val_dense_6_loss: 1.5415 - val_dense_7_loss: 0.6980\nEpoch 10/10\n133136/133136 [==============================] - 15s 110us/step - loss: 2.8542 - dense_10_loss: 0.3264 - dense_5_loss: 0.2423 - dense_6_loss: 1.5880 - dense_7_loss: 0.6975 - val_loss: 2.7164 - val_dense_10_loss: 0.2821 - val_dense_5_loss: 0.2099 - val_dense_6_loss: 1.5265 - val_dense_7_loss: 0.6979\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"If you need higher score, you can increase the number of epochs and adjust the hyper parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\nfor mol_type in mol_types: \n    print(mol_type,\": cv score is \",cv_score[i])\n    i+=1\nprint(\"total cv score is\",cv_score_total)","execution_count":13,"outputs":[{"output_type":"stream","text":"1JHC : cv score is  0.8916530837139433\n2JHH : cv score is  -0.8149072033859551\n1JHN : cv score is  0.23093738254476512\n2JHN : cv score is  -0.9596804349972338\n2JHC : cv score is  -0.2595848701337643\n3JHH : cv score is  -0.805650843151497\n3JHC : cv score is  -0.370646211453285\n3JHN : cv score is  -1.265383915345859\ntotal cv score is -0.41915787652611075\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submit(predictions):\n    submit = pd.read_csv('../input/sample_submission.csv')\n    submit[\"scalar_coupling_constant\"] = predictions\n    submit.to_csv(\"submission.csv\", index=False)\n\nsubmit(test_prediction)","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check the effect of middle outputs\nNow, let's compare with the model without attractors by changing parameter m1,m2,m3 to zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"mol_types=df_train[\"type\"].unique()\ncv_score=[]\ncv_score_total=0\n#test_prediction=np.zeros(len(df_test))\n\nfor mol_type in mol_types:\n    df_train_=df_train[df_train[\"type\"]==mol_type]\n    df_test_=df_test[df_test[\"type\"]==mol_type]\n    input_features=[\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\",\"c_x\",\"c_y\",\"c_z\",\n                    'x_closest_0','y_closest_0','z_closest_0','x_closest_1','y_closest_1','z_closest_1',\n                    \"distance\",\"distance_center0\",\"distance_center1\",\"distance_c0\",\"distance_c1\",\"distance_f0\",\"distance_f1\",\n                    \"cos_c0_c1\",\"cos_f0_f1\",\"cos_center0_center1\",\"cos_c0\",\"cos_c1\",\"cos_f0\",\"cos_f1\",\"cos_center0\",\"cos_center1\",\n                    \"atom_n\"\n                   ]\n    input_data=StandardScaler().fit_transform(pd.concat([df_train_.loc[:,input_features],df_test_.loc[:,input_features]]))\n    target_data=df_train_.loc[:,\"scalar_coupling_constant\"].values\n    target_data_1=df_train_.loc[:,[\"charge_0\",\"charge_1\"]]\n    target_data_2=df_train_.loc[:,[\"XX_0\",\"YY_0\",\"ZZ_0\",\"XX_1\",\"YY_1\",\"ZZ_1\"]]\n    target_data_3=df_train_.loc[:,[\"YX_0\",\"ZX_0\",\"XY_0\",\"ZY_0\",\"XZ_0\",\"YZ_0\",\"YX_1\",\"ZX_1\",\"XY_1\",\"ZY_1\",\"XZ_1\",\"YZ_1\"]]\n    \n    #following parameters should be adjusted to control the loss function\n    #if all parameters are zero, attractors do not work. (-> simple neural network)\n    m1=0\n    m2=0\n    m3=0\n    target_data_1=m1*(StandardScaler().fit_transform(target_data_1))\n    target_data_2=m2*(StandardScaler().fit_transform(target_data_2))\n    target_data_3=m3*(StandardScaler().fit_transform(target_data_3))\n    \n    train_index, cv_index = train_test_split(np.arange(len(df_train_)),random_state=111, test_size=0.2)\n    \n    train_input=input_data[train_index]\n    cv_input=input_data[cv_index]\n    train_target=target_data[train_index]\n    cv_target=target_data[cv_index]\n    train_target_1=target_data_1[train_index]\n    cv_target_1=target_data_1[cv_index]\n    train_target_2=target_data_2[train_index]\n    cv_target_2=target_data_2[cv_index]\n    train_target_3=target_data_3[train_index]\n    cv_target_3=target_data_3[cv_index]\n    test_input=input_data[len(df_train_):,:]\n\n    epoch_n=12#for trial. Longer epoch improve the score\n    verbose=1\n    nn_model=create_nn_model(train_input.shape[1])\n    nn_model.compile(loss='mae', optimizer=Adam(lr=1e-3))#, metrics=[auc])\n    es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0., patience=12,#val_auc\n                                 verbose=verbose, mode='min', baseline=None, restore_best_weights=True)\n    rlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3,\n                                      patience=4, min_lr=2e-6, mode='min', verbose=verbose)\n    nn_model.fit(train_input,[train_target,train_target_1,train_target_2,train_target_3], \n            validation_data=(cv_input,[cv_target,cv_target_1,cv_target_2,cv_target_3]), \n            callbacks=[es, rlr], epochs=epoch_n, batch_size=64, verbose=verbose)\n    cv_predict=nn_model.predict(cv_input)\n    \n    accuracy=np.mean(np.abs(cv_target-cv_predict[0][:,0]))\n    cv_score.append(np.log(accuracy))\n    cv_score_total+=np.log(accuracy)\n    #print(mol_type,\": log score is \",np.log(accuracy))    \n    #test_predict=nn_model.predict(test_input)\n    #test_prediction[df_test[\"type\"]==mol_type]=test_predict[0][:,0]\n    K.clear_session()\n\ncv_score_total/=len(mol_types)\n#print(\"total score is\",cv_score_total)","execution_count":15,"outputs":[{"output_type":"stream","text":"Train on 567536 samples, validate on 141885 samples\nEpoch 1/10\n567536/567536 [==============================] - 61s 107us/step - loss: 9.3691 - dense_10_loss: 9.3151 - dense_5_loss: 0.0171 - dense_6_loss: 0.0176 - dense_7_loss: 0.0193 - val_loss: 3.5532 - val_dense_10_loss: 3.5285 - val_dense_5_loss: 0.0085 - val_dense_6_loss: 0.0085 - val_dense_7_loss: 0.0077\nEpoch 2/10\n567536/567536 [==============================] - 61s 107us/step - loss: 3.8390 - dense_10_loss: 3.8183 - dense_5_loss: 0.0069 - dense_6_loss: 0.0069 - dense_7_loss: 0.0070 - val_loss: 3.4073 - val_dense_10_loss: 3.3854 - val_dense_5_loss: 0.0065 - val_dense_6_loss: 0.0083 - val_dense_7_loss: 0.0071\nEpoch 3/10\n567536/567536 [==============================] - 60s 106us/step - loss: 3.6611 - dense_10_loss: 3.6452 - dense_5_loss: 0.0055 - dense_6_loss: 0.0053 - dense_7_loss: 0.0052 - val_loss: 3.4060 - val_dense_10_loss: 3.3910 - val_dense_5_loss: 0.0062 - val_dense_6_loss: 0.0043 - val_dense_7_loss: 0.0044\nEpoch 4/10\n567536/567536 [==============================] - 58s 103us/step - loss: 3.5670 - dense_10_loss: 3.5540 - dense_5_loss: 0.0046 - dense_6_loss: 0.0043 - dense_7_loss: 0.0042 - val_loss: 2.9257 - val_dense_10_loss: 2.9137 - val_dense_5_loss: 0.0052 - val_dense_6_loss: 0.0035 - val_dense_7_loss: 0.0034\nEpoch 5/10\n567536/567536 [==============================] - 58s 102us/step - loss: 3.4885 - dense_10_loss: 3.4770 - dense_5_loss: 0.0041 - dense_6_loss: 0.0038 - dense_7_loss: 0.0037 - val_loss: 2.9098 - val_dense_10_loss: 2.8999 - val_dense_5_loss: 0.0030 - val_dense_6_loss: 0.0037 - val_dense_7_loss: 0.0032\nEpoch 6/10\n567536/567536 [==============================] - 57s 101us/step - loss: 3.4170 - dense_10_loss: 3.4067 - dense_5_loss: 0.0036 - dense_6_loss: 0.0034 - dense_7_loss: 0.0033 - val_loss: 3.0142 - val_dense_10_loss: 3.0051 - val_dense_5_loss: 0.0035 - val_dense_6_loss: 0.0026 - val_dense_7_loss: 0.0029\nEpoch 7/10\n567536/567536 [==============================] - 59s 104us/step - loss: 3.3812 - dense_10_loss: 3.3719 - dense_5_loss: 0.0032 - dense_6_loss: 0.0031 - dense_7_loss: 0.0030 - val_loss: 2.8790 - val_dense_10_loss: 2.8694 - val_dense_5_loss: 0.0040 - val_dense_6_loss: 0.0032 - val_dense_7_loss: 0.0023\nEpoch 8/10\n567536/567536 [==============================] - 59s 104us/step - loss: 3.3224 - dense_10_loss: 3.3139 - dense_5_loss: 0.0029 - dense_6_loss: 0.0028 - dense_7_loss: 0.0028 - val_loss: 2.7913 - val_dense_10_loss: 2.7827 - val_dense_5_loss: 0.0036 - val_dense_6_loss: 0.0025 - val_dense_7_loss: 0.0024\nEpoch 9/10\n567536/567536 [==============================] - 58s 103us/step - loss: 3.3106 - dense_10_loss: 3.3028 - dense_5_loss: 0.0027 - dense_6_loss: 0.0026 - dense_7_loss: 0.0026 - val_loss: 2.9139 - val_dense_10_loss: 2.9069 - val_dense_5_loss: 0.0025 - val_dense_6_loss: 0.0021 - val_dense_7_loss: 0.0024\nEpoch 10/10\n567536/567536 [==============================] - 59s 103us/step - loss: 3.2660 - dense_10_loss: 3.2585 - dense_5_loss: 0.0025 - dense_6_loss: 0.0025 - dense_7_loss: 0.0024 - val_loss: 2.8401 - val_dense_10_loss: 2.8338 - val_dense_5_loss: 0.0020 - val_dense_6_loss: 0.0026 - val_dense_7_loss: 0.0016\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-b4d95893093f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0;34m\"atom_n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                    ]\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0minput_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_train_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_test_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtarget_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_train_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"scalar_coupling_constant\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtarget_data_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_train_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"charge_0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"charge_1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples_seen_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                     _incremental_mean_and_var(X, self.mean_, self.var_,\n\u001b[0;32m--> 734\u001b[0;31m                                               self.n_samples_seen_)\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;31m# for backward-compatibility, reduce n_samples_seen_ to an integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36m_incremental_mean_and_var\u001b[0;34m(X, last_mean, last_variance, last_sample_count)\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mupdated_variance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m         \u001b[0mnew_unnormalized_variance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnew_sample_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m         \u001b[0mlast_unnormalized_variance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_variance\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlast_sample_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/nanfunctions.py\u001b[0m in \u001b[0;36mnanvar\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m     \"\"\"\n\u001b[0;32m-> 1476\u001b[0;31m     \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_replace_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m         return np.var(arr, axis=axis, dtype=dtype, out=out, ddof=ddof,\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/nanfunctions.py\u001b[0m in \u001b[0;36m_replace_nan\u001b[0;34m(a, val)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\nfor mol_type in mol_types: \n    print(mol_type,\": cv score is \",cv_score[i])\n    i+=1\nprint(\"total cv score is\",cv_score_total)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The score with attractors is slightly better than without attractors."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}