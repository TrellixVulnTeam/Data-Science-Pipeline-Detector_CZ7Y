{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"dipole_moments_csv=pd.read_csv(\"../input/dipole_moments.csv\")\nmagnetic_shielding_tensors_csv=pd.read_csv(\"../input/magnetic_shielding_tensors.csv\")\nmulliken_charges_csv=pd.read_csv(\"../input/mulliken_charges.csv\")\npotential_energy_csv=pd.read_csv(\"../input/potential_energy.csv\")\nsample_submission_csv=pd.read_csv(\"../input/sample_submission.csv\")\nscalar_coupling_contributions_csv=pd.read_csv(\"../input/scalar_coupling_contributions.csv\")\nstructures_csv=pd.read_csv(\"../input/structures.csv\")\ntest_csv=pd.read_csv(\"../input/test.csv\")\ntrain_csv=pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"dipole_moments.shape:\",dipole_moments_csv.shape)\nprint(\"magnetic_shielding_tensors.shape:\",magnetic_shielding_tensors_csv.shape)\nprint(\"mulliken_charges.shape:\",mulliken_charges_csv.shape)\nprint(\"potential_energy:\",potential_energy_csv.shape)\nprint(\"sample_submission:\",sample_submission_csv.shape)\nprint(\"scalar_coupling_contributions.shape:\",scalar_coupling_contributions_csv.shape)\nprint(\"structures.shape:\",structures_csv.shape)\nprint(\"test_csv.shape:\",test_csv.shape)\nprint(\"train_csv.shape:\",train_csv.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modification of structures_csv\n\n#from tqdm import tqdm_notebook as tqdm\natomic_radius = {'H':0.38, 'C':0.77, 'N':0.75, 'O':0.73, 'F':0.71} # Without fudge factor\n\nfudge_factor = 0.05\natomic_radius = {k:v + fudge_factor for k,v in atomic_radius.items()}\n\n\nelectronegativity = {'H':2.2, 'C':2.55, 'N':3.04, 'O':3.44, 'F':3.98}\natomic_num = {'H':1, 'C':6, 'N':7, 'O':8, 'F':9}\n\natoms = structures_csv['atom'].values\natoms_en = [electronegativity[x] for x in atoms]\natoms_rad = [atomic_radius[x] for x in atoms]\natomic_number = [atomic_num[x] for x in atoms]\n\nstructures_csv['Atomic_EN'] = atoms_en\nstructures_csv['Atomic_radius'] = atoms_rad\nstructures_csv['Atomic_mass'] = atomic_number\n\nstructures_csv['Tot_atoms_molecule']= structures_csv.groupby(['molecule_name'])['molecule_name'].transform('count')\n\n# Center of gravity. Begining\n\nstructures_csv['mass_x'] = structures_csv['Atomic_mass'] * structures_csv['x']\nstructures_csv['mass_y'] = structures_csv['Atomic_mass'] * structures_csv['y']\nstructures_csv['mass_z'] = structures_csv['Atomic_mass'] * structures_csv['z']\nstructures_csv['Molecular_mass']= structures_csv.groupby(['molecule_name'])['Atomic_mass'].transform('sum')\n\nstructures_csv['sum_mass_x']= structures_csv.groupby(['molecule_name'])['mass_x'].transform('sum')\nstructures_csv['sum_mass_y']= structures_csv.groupby(['molecule_name'])['mass_y'].transform('sum')\nstructures_csv['sum_mass_z']= structures_csv.groupby(['molecule_name'])['mass_z'].transform('sum')\n\nstructures_csv['XG']= structures_csv['sum_mass_x'] / structures_csv['Molecular_mass']\nstructures_csv['YG']= structures_csv['sum_mass_y'] / structures_csv['Molecular_mass']\nstructures_csv['ZG']= structures_csv['sum_mass_z'] / structures_csv['Molecular_mass']\n\n# Center of gravity. Begining\n\nstructures_csv= structures_csv.drop({'mass_x','mass_y','mass_z','sum_mass_x','sum_mass_y','sum_mass_z'}, axis=1)\n\nstructures_csv['Dist_XG_to_x']= np.abs(structures_csv['XG']-structures_csv['x'])\nstructures_csv['Dist_YG_to_y']= np.abs(structures_csv['YG']-structures_csv['y'])\nstructures_csv['Dist_ZG_to_z']= np.abs(structures_csv['ZG']-structures_csv['z'])\n\nstructures_csv = pd.concat([structures_csv,pd.get_dummies(structures_csv['atom'],prefix='Atom_type')], axis=1)\n\n# Merge train_csv with scalar_coupling contributions\ntrain_csv = pd.merge(train_csv, scalar_coupling_contributions_csv, how = 'left',\n                  left_on  = ['molecule_name', 'atom_index_0', 'atom_index_1', 'type'],\n                  right_on = ['molecule_name', 'atom_index_0', 'atom_index_1', 'type'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge train_csv with dipole moments and potential energy\ntrain_csv = pd.merge(train_csv,dipole_moments_csv,left_on='molecule_name', right_on='molecule_name')\ntrain_csv = pd.merge(train_csv,potential_energy_csv,left_on='molecule_name', right_on='molecule_name')\ndipole=np.sqrt(np.square(train_csv['X'])+np.square(train_csv['Y'])+np.square(train_csv['Z']))\ntrain_csv['Dipole']=dipole","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge train_csv with Mulliken charges\n\ndef map_mulliken_charge(database,atom_idx) :\n    database = pd.merge(database,mulliken_charges_csv,how = 'left',\n                 left_on = ['molecule_name',f'atom_index_{atom_idx}'],\n                 right_on = ['molecule_name','atom_index']\n                 )\n    database = database.rename(columns={'mulliken_charge': f'mulliken_charge_{atom_idx}'}\n                  )\n    database = database.drop('atom_index',axis = 1)\n    return database","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv = map_mulliken_charge(train_csv,0)\ntrain_csv = map_mulliken_charge(train_csv,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate number each type of atoms in a molecule\ndef number_each_atoms_molecule(df,df1,index):\n    df['number_each_at_mol'] = df.groupby(['molecule_name','atom'])['molecule_name'].transform('count')\n    df1 = pd.merge(df1, df, how = 'left',\n                  left_on  = ['molecule_name', f'atom_index_{index}'],\n                  right_on = ['molecule_name',  'atom_index'])\n    df1=df1.rename(columns={'x':f'x{index}','y':f'y{index}','z':f'z{index}','atom':f'atom{index}'})\n\n    return df1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge train_csv and test_csv with structures_csv\n\ntrain_csv = number_each_atoms_molecule(structures_csv, train_csv,0)\ntrain_csv = number_each_atoms_molecule(structures_csv, train_csv,1)\ntrain_csv = train_csv.rename(columns={'x_x':'x0','y_x':'y0','z_x':'z0','x_y':'x1','y_y':'y1','z_y':'z1','atom_x':'atom_0','atom_y':'atom_1','number_each_at_mol_x':'number_each_at_mol_0','number_each_at_mol_y':'number_each_at_mol_1'})\n\ntest_csv = number_each_atoms_molecule(structures_csv, test_csv,0)\ntest_csv = number_each_atoms_molecule(structures_csv, test_csv,1)\ntest_csv = test_csv.rename(columns={'x_x':'x0','y_x':'y0','z_x':'z0','x_y':'x1','y_y':'y1','z_y':'z1','atom_x':'atom_0','atom_y':'atom_1','number_each_at_mol_x':'number_each_at_mol_0','number_each_at_mol_y':'number_each_at_mol_1'})\n\ntrain_csv = reduce_mem_usage(train_csv)\ntest_csv = reduce_mem_usage(test_csv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######### Operations with train_csv and test_csv ########################################\n\n# distances train\ndistances_train=np.sqrt(np.square(train_csv['x0']-train_csv['x1'])+np.square(train_csv['y0']-train_csv['y1'])+np.square(train_csv['z0']-train_csv['z1']))\ndistances_train_x =np.abs(train_csv['x0']-train_csv['x1'])\ndistances_train_y =np.abs(train_csv['y0']-train_csv['y1'])\ndistances_train_z =np.abs(train_csv['z0']-train_csv['z1'])\n\n\ntrain_csv['At_dist']=distances_train\ntrain_csv['At_dist_x']=distances_train_x\ntrain_csv['At_dist_y']=distances_train_y\ntrain_csv['At_dist_z']=distances_train_z\n\ntrain_csv['Molec_dist_mean']=train_csv.groupby('molecule_name')['At_dist'].transform('mean')\ntrain_csv['Molec_dist_max']=train_csv.groupby('molecule_name')['At_dist'].transform('max')\ntrain_csv['Molec_dist_min']=train_csv.groupby('molecule_name')['At_dist'].transform('min')\n#train_csv['Molec_dist_std']=train_csv.groupby('molecule_name')['At_dist'].transform('std')\n\ntrain_csv['Molec_type_dist_mean']=train_csv.groupby(['molecule_name', 'type'])['At_dist'].transform('mean')\ntrain_csv['Molec_type_dist_max']=train_csv.groupby(['molecule_name', 'type'])['At_dist'].transform('max')\ntrain_csv['Molec_type_dist_min']=train_csv.groupby(['molecule_name', 'type'])['At_dist'].transform('min')\n\ntrain_csv['Num_couplings']=train_csv.groupby('molecule_name')['molecule_name'].transform('count')\ntrain_csv['Atom_0_couples_count'] = train_csv.groupby(['molecule_name', 'atom_index_0'])['id'].transform('count')\ntrain_csv['Atom_1_couples_count'] = train_csv.groupby(['molecule_name', 'atom_index_1'])['id'].transform('count')\n\ntrain_csv['Relative_dist_x']= train_csv['At_dist_x'] / (train_csv['At_dist'] + 1e-5)\ntrain_csv['Relative_dist_y']= train_csv['At_dist_y'] / (train_csv['At_dist'] + 1e-5)\ntrain_csv['Relative_dist_z']= train_csv['At_dist_z'] / (train_csv['At_dist'] + 1e-5)\n\ndistances_0_CG_train= np.sqrt(np.square(train_csv['Dist_XG_to_x_x'])+np.square(train_csv['Dist_YG_to_y_x'])+np.square(train_csv['Dist_ZG_to_z_x']))\ndistances_1_CG_train= np.sqrt(np.square(train_csv['Dist_XG_to_x_y'])+np.square(train_csv['Dist_YG_to_y_y'])+np.square(train_csv['Dist_ZG_to_z_y']))\ntrain_csv['Dist_CG_x0']= distances_0_CG_train\ntrain_csv['Dist_CG_x1']= distances_1_CG_train\n\n\ntrain_csv['Relative_XG_dist_x0']= train_csv['Dist_XG_to_x_x'] / (train_csv['Dist_CG_x0'] + 1e-5) \ntrain_csv['Relative_YG_dist_x0']= train_csv['Dist_YG_to_y_x'] / (train_csv['Dist_CG_x0'] + 1e-5)\ntrain_csv['Relative_ZG_dist_x0']= train_csv['Dist_ZG_to_z_x'] / (train_csv['Dist_CG_x0'] + 1e-5)\n\ntrain_csv['Relative_XG_dist_x1']= train_csv['Dist_XG_to_x_y'] / (train_csv['Dist_CG_x1'] + 1e-5) \ntrain_csv['Relative_YG_dist_x1']= train_csv['Dist_YG_to_y_y'] / (train_csv['Dist_CG_x1'] + 1e-5)\ntrain_csv['Relative_ZG_dist_x1']= train_csv['Dist_ZG_to_z_y'] / (train_csv['Dist_CG_x1'] + 1e-5)\n\ntrain_csv['Molecule_atom_index_0_dist_mean'] = train_csv.groupby(['molecule_name', 'atom_index_0'])['At_dist'].transform('mean')\ntrain_csv['Molecule_atom_index_0_dist_max'] = train_csv.groupby(['molecule_name', 'atom_index_0'])['At_dist'].transform('max')\ntrain_csv['Molecule_atom_index_0_dist_min'] = train_csv.groupby(['molecule_name', 'atom_index_0'])['At_dist'].transform('min')\n#train_csv['Molecule_atom_index_0_dist_std'] = train_csv.groupby(['molecule_name', 'atom_index_0'])['At_dist'].transform('std') #### ojito con este\n\ntrain_csv['Molecule_atom_index_1_dist_mean'] = train_csv.groupby(['molecule_name', 'atom_index_1'])['At_dist'].transform('mean')\ntrain_csv['Molecule_atom_index_1_dist_max'] = train_csv.groupby(['molecule_name', 'atom_index_1'])['At_dist'].transform('max')\ntrain_csv['Molecule_atom_index_1_dist_min'] = train_csv.groupby(['molecule_name', 'atom_index_1'])['At_dist'].transform('min')\n#train_csv['Molecule_atom_index_1_dist_std'] = train_csv.groupby(['molecule_name', 'atom_index_1'])['At_dist'].transform('std') #### ojito con este\n\n#train_csv['links'] = train_csv['type'].apply(lambda x: x[0])\n#train_csv = pd.concat([train_csv,pd.get_dummies(train_csv['links'],prefix='Number_links')], axis=1)\n#train_csv = pd.concat([train_csv,pd.get_dummies(train_csv['type'])], axis=1)\ntrain_csv = pd.concat([train_csv,pd.get_dummies(train_csv['atom0'],prefix='Coupling_atom_0_type')], axis=1)\ntrain_csv = pd.concat([train_csv,pd.get_dummies(train_csv['atom1'],prefix='Coupling_atom_1_type')], axis=1)\n\ntrain_csv = train_csv.rename(columns={'Atomic_EN_x':'Atomic_EN_x0'})\ntrain_csv = train_csv.rename(columns={'Atomic_EN_y':'Atomic_EN_x1'}) \ntrain_csv = train_csv.rename(columns={'Atomic_radius_x':'Atomic_radius_x0'})\ntrain_csv = train_csv.rename(columns={'Atomic_radius_y':'Atomic_radius_x1'})\ntrain_csv = train_csv.rename(columns={'Atomic_mass_x':'Atomic_mass_x0'})\ntrain_csv = train_csv.rename(columns={'Atomic_mass_y':'Atomic_mass_x1'})\n\ntrain_csv = train_csv.rename(columns={'Tot_atoms_molecule_x':'Tot_atoms_molecule'})\ntrain_csv = train_csv.rename(columns={'Atom_type_C_x':'Atom_type_C_0'})  \ntrain_csv = train_csv.rename(columns={'Atom_type_F_x':'Atom_type_F_0'})\ntrain_csv = train_csv.rename(columns={'Atom_type_H_x':'Atom_type_H_0'}) \ntrain_csv = train_csv.rename(columns={'Atom_type_N_x':'Atom_type_N_0'}) \ntrain_csv = train_csv.rename(columns={'Atom_type_O_x':'Atom_type_O_0'}) \ntrain_csv = train_csv.rename(columns={'Atom_type_C_y':'Atom_type_C_1'})  \ntrain_csv = train_csv.rename(columns={'Atom_type_F_y':'Atom_type_F_1'})\ntrain_csv = train_csv.rename(columns={'Atom_type_H_y':'Atom_type_H_1'}) \ntrain_csv = train_csv.rename(columns={'Atom_type_N_y':'Atom_type_N_1'}) \ntrain_csv = train_csv.rename(columns={'Atom_type_O_y':'Atom_type_O_1'}) \ntrain_csv = train_csv.rename(columns={'Molecular_mass_x':'Molecular_mass'})\n#cosine\ndx= train_csv['x0']-train_csv['x1']\ndy= train_csv['y0']-train_csv['y1']\ndz= train_csv['z0']-train_csv['z1']\n\nuve_0_x = train_csv['XG_x']-train_csv['x0']\nuve_0_y = train_csv['YG_x']-train_csv['y0']\nuve_0_z = train_csv['ZG_x']-train_csv['z0']\n\nuve_1_x = train_csv['XG_x']-train_csv['x1']\nuve_1_y = train_csv['YG_x']-train_csv['y1']\nuve_1_z = train_csv['ZG_x']-train_csv['z1']\n\ntrain_csv['cos_G_x0'] = ((uve_0_x * dx)+(uve_0_y * dy)+(uve_0_z * dz))/((abs(distances_train)*abs(distances_0_CG_train))+ 1e-2)\ntrain_csv['cos_G_x1'] = ((uve_1_x * dx)+(uve_1_y * dy)+(uve_1_z * dz))/((abs(distances_train)*abs(distances_1_CG_train))+ 1e-2)\n\ndist_xy = np.sqrt(np.square(dx)+np.square(dy))\ndist_xz = np.sqrt(np.square(dx)+np.square(dz))\ndist_yz = np.sqrt(np.square(dy)+np.square(dz))\n\nuve_0_xy = np.sqrt(np.square(uve_0_x)+np.square(uve_0_y))\nuve_0_xz = np.sqrt(np.square(uve_0_x)+np.square(uve_0_z))\nuve_0_yz = np.sqrt(np.square(uve_0_y)+np.square(uve_0_z))\nuve_1_xy = np.sqrt(np.square(uve_1_x)+np.square(uve_1_y))\nuve_1_xz = np.sqrt(np.square(uve_1_x)+np.square(uve_1_z))\nuve_1_yz = np.sqrt(np.square(uve_1_y)+np.square(uve_1_z))\n\ntrain_csv['cos_G_x0_plane_xy'] = ((dx*uve_0_x)+(dy*uve_0_y))/((abs(dist_xy)*abs(uve_0_xy))+1e-2)\ntrain_csv['cos_G_x0_plane_xz'] = ((dx*uve_0_x)+(dz*uve_0_z))/((abs(dist_xz)*abs(uve_0_xz))+1e-2)\ntrain_csv['cos_G_x0_plane_yz'] = ((dy*uve_0_y)+(dz*uve_0_z))/((abs(dist_yz)*abs(uve_0_yz))+1e-2)\ntrain_csv['cos_G_x1_plane_xy'] = ((dx*uve_1_x)+(dy*uve_1_y))/((abs(dist_xy)*abs(uve_1_xy))+1e-2)\ntrain_csv['cos_G_x1_plane_xz'] = ((dx*uve_1_x)+(dz*uve_1_z))/((abs(dist_xz)*abs(uve_1_xz))+1e-2)\ntrain_csv['cos_G_x1_plane_yz'] = ((dy*uve_1_y)+(dz*uve_1_z))/((abs(dist_yz)*abs(uve_1_yz))+1e-2)\n\n\n\n#---------------------------------------------------------\ntrain_csv['Molec_x_mean']=train_csv.groupby('molecule_name')['x0'].transform('mean')\ntrain_csv['Molec_x_max']=train_csv.groupby('molecule_name')['x0'].transform('max')\ntrain_csv['Molec_x_min']=train_csv.groupby('molecule_name')['x0'].transform('min')\ntrain_csv['Molec_y_mean']=train_csv.groupby('molecule_name')['y0'].transform('mean')\ntrain_csv['Molec_y_max']=train_csv.groupby('molecule_name')['y0'].transform('max')\ntrain_csv['Molec_y_min']=train_csv.groupby('molecule_name')['y0'].transform('min')\ntrain_csv['Molec_z_mean']=train_csv.groupby('molecule_name')['z0'].transform('mean')\ntrain_csv['Molec_z_max']=train_csv.groupby('molecule_name')['z0'].transform('max')\ntrain_csv['Molec_z_min']=train_csv.groupby('molecule_name')['z0'].transform('min')\n\ndx0_mean= train_csv['x0']-train_csv['Molec_x_mean']\ndy0_mean= train_csv['y0']-train_csv['Molec_y_mean']\ndz0_mean= train_csv['z0']-train_csv['Molec_z_mean']\n\ndx0_max= train_csv['x0']-train_csv['Molec_x_max']\ndy0_max= train_csv['y0']-train_csv['Molec_y_max']\ndz0_max= train_csv['z0']-train_csv['Molec_z_max']\n\ndx0_min= train_csv['x0']-train_csv['Molec_x_min']\ndy0_min= train_csv['y0']-train_csv['Molec_y_min']\ndz0_min= train_csv['z0']-train_csv['Molec_z_min']\n\ndx1_mean= train_csv['x1']-train_csv['Molec_x_mean']\ndy1_mean= train_csv['y1']-train_csv['Molec_y_mean']\ndz1_mean= train_csv['z1']-train_csv['Molec_z_mean']\n\ndx1_max= train_csv['x1']-train_csv['Molec_x_max']\ndy1_max= train_csv['y1']-train_csv['Molec_y_max']\ndz1_max= train_csv['z1']-train_csv['Molec_z_max']\n\ndx1_min= train_csv['x1']-train_csv['Molec_x_min']\ndy1_min= train_csv['y1']-train_csv['Molec_y_min']\ndz1_min= train_csv['z1']-train_csv['Molec_z_min']\n\ndist_x0_mean_xy = np.sqrt(np.square(dx0_mean)+np.square(dy0_mean))\ndist_x0_mean_xz = np.sqrt(np.square(dx0_mean)+np.square(dz0_mean))\ndist_x0_mean_yz = np.sqrt(np.square(dy0_mean)+np.square(dz0_mean))\ntrain_csv['cos_Mean_x0_plane_xy'] = ((dx*dx0_mean)+(dy*dy0_mean))/((abs(dist_xy)*abs(dist_x0_mean_xy))+1e-2)\ntrain_csv['cos_Mean_x0_plane_xz'] = ((dx*dx0_mean)+(dz*dz0_mean))/((abs(dist_xz)*abs(dist_x0_mean_xz))+1e-2)\ntrain_csv['cos_Mean_x0_plane_yz'] = ((dy*dy0_mean)+(dz*dz0_mean))/((abs(dist_yz)*abs(dist_x0_mean_yz))+1e-2)\n\ndist_x0_max_xy = np.sqrt(np.square(dx0_max)+np.square(dy0_max))\ndist_x0_max_xz = np.sqrt(np.square(dx0_max)+np.square(dz0_max))\ndist_x0_max_yz = np.sqrt(np.square(dy0_max)+np.square(dz0_max))\ntrain_csv['cos_Max_x0_plane_xy'] = ((dx*dx0_max)+(dy*dy0_max))/((abs(dist_xy)*abs(dist_x0_max_xy))+1e-2)\ntrain_csv['cos_Max_x0_plane_xz'] = ((dx*dx0_max)+(dz*dz0_max))/((abs(dist_xz)*abs(dist_x0_max_xz))+1e-2)\ntrain_csv['cos_Max_x0_plane_yz'] = ((dy*dy0_max)+(dz*dz0_max))/((abs(dist_yz)*abs(dist_x0_max_yz))+1e-2)\n\ndist_x0_min_xy = np.sqrt(np.square(dx0_min)+np.square(dy0_min))\ndist_x0_min_xz = np.sqrt(np.square(dx0_min)+np.square(dz0_min))\ndist_x0_min_yz = np.sqrt(np.square(dy0_min)+np.square(dz0_min))\ntrain_csv['cos_Min_x0_plane_xy'] = ((dx*dx0_min)+(dy*dy0_min))/((abs(dist_xy)*abs(dist_x0_min_xy))+1e-2)\ntrain_csv['cos_Min_x0_plane_xz'] = ((dx*dx0_min)+(dz*dz0_min))/((abs(dist_xz)*abs(dist_x0_min_xz))+1e-2)\ntrain_csv['cos_Min_x0_plane_yz'] = ((dy*dy0_min)+(dz*dz0_min))/((abs(dist_yz)*abs(dist_x0_min_yz))+1e-2)\n\ndist_x0_mean = np.sqrt(np.square(dx0_mean)+np.square(dy0_mean)+np.square(dz0_mean))\ndist_x0_max = np.sqrt(np.square(dx0_max)+np.square(dy0_max)+np.square(dz0_max))\ndist_x0_min = np.sqrt(np.square(dx0_min)+np.square(dy0_min)+np.square(dz0_min))\ntrain_csv['cos_Mean_x0'] = ((dx*dx0_mean)+(dy*dy0_mean)+(dy*dy0_mean))/((abs(distances_train)*abs(dist_x0_mean))+1e-2)\ntrain_csv['cos_Max_x0'] = ((dx*dx0_max)+(dy*dy0_max)+(dy*dy0_max))/((abs(distances_train)*abs(dist_x0_max))+1e-2)\ntrain_csv['cos_Min_x0'] = ((dx*dx0_min)+(dy*dy0_min)+(dy*dy0_min))/((abs(distances_train)*abs(dist_x0_min))+1e-2)\n\ndist_x1_mean_xy = np.sqrt(np.square(dx1_mean)+np.square(dy1_mean))\ndist_x1_mean_xz = np.sqrt(np.square(dx1_mean)+np.square(dz1_mean))\ndist_x1_mean_yz = np.sqrt(np.square(dy1_mean)+np.square(dz1_mean))\ntrain_csv['cos_Mean_x1_plane_xy'] = ((dx*dx1_mean)+(dy*dy1_mean))/((abs(dist_xy)*abs(dist_x1_mean_xy))+1e-2)\ntrain_csv['cos_Mean_x1_plane_xz'] = ((dx*dx1_mean)+(dz*dz1_mean))/((abs(dist_xz)*abs(dist_x1_mean_xz))+1e-2)\ntrain_csv['cos_Mean_x1_plane_yz'] = ((dy*dy1_mean)+(dz*dz1_mean))/((abs(dist_yz)*abs(dist_x1_mean_yz))+1e-2)\n\ndist_x1_max_xy = np.sqrt(np.square(dx1_max)+np.square(dy1_max))\ndist_x1_max_xz = np.sqrt(np.square(dx1_max)+np.square(dz1_max))\ndist_x1_max_yz = np.sqrt(np.square(dy1_max)+np.square(dz1_max))\ntrain_csv['cos_Max_x1_plane_xy'] = ((dx*dx1_max)+(dy*dy1_max))/((abs(dist_xy)*abs(dist_x1_max_xy))+1e-2)\ntrain_csv['cos_Max_x1_plane_xz'] = ((dx*dx1_max)+(dz*dz1_max))/((abs(dist_xz)*abs(dist_x1_max_xz))+1e-2)\ntrain_csv['cos_Max_x1_plane_yz'] = ((dy*dy1_max)+(dz*dz1_max))/((abs(dist_yz)*abs(dist_x1_max_yz))+1e-2)\n\ndist_x1_min_xy = np.sqrt(np.square(dx1_min)+np.square(dy1_min))\ndist_x1_min_xz = np.sqrt(np.square(dx1_min)+np.square(dz1_min))\ndist_x1_min_yz = np.sqrt(np.square(dy1_min)+np.square(dz1_min))\ntrain_csv['cos_Min_x1_plane_xy'] = ((dx*dx1_min)+(dy*dy1_min))/((abs(dist_xy)*abs(dist_x1_min_xy))+1e-2)\ntrain_csv['cos_Min_x1_plane_xz'] = ((dx*dx1_min)+(dz*dz1_min))/((abs(dist_xz)*abs(dist_x1_min_xz))+1e-2)\ntrain_csv['cos_Min_x1_plane_yz'] = ((dy*dy1_min)+(dz*dz1_min))/((abs(dist_yz)*abs(dist_x1_min_yz))+1e-2)\n\ndist_x1_mean = np.sqrt(np.square(dx1_mean)+np.square(dy1_mean)+np.square(dz1_mean))\ndist_x1_max = np.sqrt(np.square(dx1_max)+np.square(dy1_max)+np.square(dz1_max))\ndist_x1_min = np.sqrt(np.square(dx1_min)+np.square(dy1_min)+np.square(dz1_min))\ntrain_csv['cos_Mean_x1'] = ((dx*dx1_mean)+(dy*dy1_mean)+(dz*dz1_mean))/((abs(distances_train)*abs(dist_x1_mean))+1e-2)\ntrain_csv['cos_Max_x1'] = ((dx*dx1_max)+(dy*dy1_max)+(dz*dz1_max))/((abs(distances_train)*abs(dist_x1_max))+1e-2)\ntrain_csv['cos_Min_x1'] = ((dx*dx1_min)+(dy*dy1_min)+(dz*dz1_min))/((abs(distances_train)*abs(dist_x1_min))+1e-2)\n\n#-----------------------\n\n#  Test\ndistances_test=np.sqrt(np.square(test_csv['x0']-test_csv['x1'])+np.square(test_csv['y0']-test_csv['y1'])+np.square(test_csv['z0']-test_csv['z1']))\ndistances_test_x =np.abs(test_csv['x0']-test_csv['x1'])\ndistances_test_y =np.abs(test_csv['y0']-test_csv['y1'])\ndistances_test_z =np.abs(test_csv['z0']-test_csv['z1'])\n\ntest_csv['At_dist']=distances_test\ntest_csv['At_dist_x']=distances_test_x\ntest_csv['At_dist_y']=distances_test_y\ntest_csv['At_dist_z']=distances_test_z\n\ntest_csv['Molec_dist_mean']=test_csv.groupby('molecule_name')['At_dist'].transform('mean')\ntest_csv['Molec_dist_max']=test_csv.groupby('molecule_name')['At_dist'].transform('max')\ntest_csv['Molec_dist_min']=test_csv.groupby('molecule_name')['At_dist'].transform('min')\n#test_csv['Molec_dist_std']=test_csv.groupby('molecule_name')['At_dist'].transform('std')\n\ntest_csv['Molec_type_dist_mean']=test_csv.groupby(['molecule_name', 'type'])['At_dist'].transform('mean')\ntest_csv['Molec_type_dist_max']=test_csv.groupby(['molecule_name', 'type'])['At_dist'].transform('max')\ntest_csv['Molec_type_dist_min']=test_csv.groupby(['molecule_name', 'type'])['At_dist'].transform('min')\n\ntest_csv['Num_couplings']=test_csv.groupby('molecule_name')['molecule_name'].transform('count')\ntest_csv['Atom_0_couples_count'] = test_csv.groupby(['molecule_name', 'atom_index_0'])['id'].transform('count')\ntest_csv['Atom_1_couples_count'] = test_csv.groupby(['molecule_name', 'atom_index_1'])['id'].transform('count')\n\ntest_csv['Relative_dist_x']= test_csv['At_dist_x'] / (test_csv['At_dist'] + 1e-5)\ntest_csv['Relative_dist_y']= test_csv['At_dist_y'] / (test_csv['At_dist'] + 1e-5)\ntest_csv['Relative_dist_z']= test_csv['At_dist_z'] / (test_csv['At_dist'] + 1e-5)\n\ndistances_0_CG_test=np.sqrt(np.square(test_csv['Dist_XG_to_x_x'])+np.square(test_csv['Dist_YG_to_y_x'])+np.square(test_csv['Dist_ZG_to_z_x']))\ndistances_1_CG_test=np.sqrt(np.square(test_csv['Dist_XG_to_x_y'])+np.square(test_csv['Dist_YG_to_y_y'])+np.square(test_csv['Dist_ZG_to_z_y']))\ntest_csv['Dist_CG_x0']= distances_0_CG_test\ntest_csv['Dist_CG_x1']= distances_1_CG_test\n\ntest_csv['Relative_XG_dist_x0']= test_csv['Dist_XG_to_x_x'] / (test_csv['Dist_CG_x0'] + 1e-5)\ntest_csv['Relative_YG_dist_x0']= test_csv['Dist_YG_to_y_x'] / (test_csv['Dist_CG_x0'] + 1e-5)\ntest_csv['Relative_ZG_dist_x0']= test_csv['Dist_ZG_to_z_x'] / (test_csv['Dist_CG_x0'] + 1e-5)\n\ntest_csv['Relative_XG_dist_x1']= test_csv['Dist_XG_to_x_y'] / (test_csv['Dist_CG_x1'] + 1e-5)\ntest_csv['Relative_YG_dist_x1']= test_csv['Dist_YG_to_y_y'] / (test_csv['Dist_CG_x1'] + 1e-5)\ntest_csv['Relative_ZG_dist_x1']= test_csv['Dist_ZG_to_z_y'] / (test_csv['Dist_CG_x1'] + 1e-5)\n\ntest_csv['Molecule_atom_index_0_dist_mean'] = test_csv.groupby(['molecule_name', 'atom_index_0'])['At_dist'].transform('mean')\ntest_csv['Molecule_atom_index_0_dist_max'] = test_csv.groupby(['molecule_name', 'atom_index_0'])['At_dist'].transform('max')\ntest_csv['Molecule_atom_index_0_dist_min'] = test_csv.groupby(['molecule_name', 'atom_index_0'])['At_dist'].transform('min')\n#test_csv['Molecule_atom_index_0_dist_std'] = test_csv.groupby(['molecule_name', 'atom_index_0'])['At_dist'].transform('std')\n\ntest_csv['Molecule_atom_index_1_dist_mean'] = test_csv.groupby(['molecule_name', 'atom_index_1'])['At_dist'].transform('mean')\ntest_csv['Molecule_atom_index_1_dist_max'] = test_csv.groupby(['molecule_name', 'atom_index_1'])['At_dist'].transform('max')\ntest_csv['Molecule_atom_index_1_dist_min'] = test_csv.groupby(['molecule_name', 'atom_index_1'])['At_dist'].transform('min')\n#test_csv['Molecule_atom_index_1_dist_std'] = test_csv.groupby(['molecule_name', 'atom_index_1'])['At_dist'].transform('std')\n\n#test_csv['links'] = test_csv['type'].apply(lambda x: x[0])\n#test_csv = pd.concat([test_csv,pd.get_dummies(test_csv['links'],prefix='Number_links')], axis=1)\n#test_csv = pd.concat([test_csv,pd.get_dummies(test_csv['type'])], axis=1)\ntest_csv = pd.concat([test_csv,pd.get_dummies(test_csv['atom0'],prefix='Coupling_atom_0_type')], axis=1)\ntest_csv = pd.concat([test_csv,pd.get_dummies(test_csv['atom1'],prefix='Coupling_atom_1_type')], axis=1)\n\ntest_csv = test_csv.rename(columns={'Atomic_EN_x':'Atomic_EN_x0'})\ntest_csv = test_csv.rename(columns={'Atomic_EN_y':'Atomic_EN_x1'}) \ntest_csv = test_csv.rename(columns={'Atomic_radius_x':'Atomic_radius_x0'})\ntest_csv = test_csv.rename(columns={'Atomic_radius_y':'Atomic_radius_x1'})\ntest_csv = test_csv.rename(columns={'Atomic_mass_x':'Atomic_mass_x0'})\ntest_csv = test_csv.rename(columns={'Atomic_mass_y':'Atomic_mass_x1'})\n\ntest_csv = test_csv.rename(columns={'Tot_atoms_molecule_x':'Tot_atoms_molecule'})\ntest_csv = test_csv.rename(columns={'Atom_type_C_x':'Atom_type_C_0'})  \ntest_csv = test_csv.rename(columns={'Atom_type_F_x':'Atom_type_F_0'})\ntest_csv = test_csv.rename(columns={'Atom_type_H_x':'Atom_type_H_0'}) \ntest_csv = test_csv.rename(columns={'Atom_type_N_x':'Atom_type_N_0'}) \ntest_csv = test_csv.rename(columns={'Atom_type_O_x':'Atom_type_O_0'}) \ntest_csv = test_csv.rename(columns={'Atom_type_C_y':'Atom_type_C_1'})  \ntest_csv = test_csv.rename(columns={'Atom_type_F_y':'Atom_type_F_1'})\ntest_csv = test_csv.rename(columns={'Atom_type_H_y':'Atom_type_H_1'}) \ntest_csv = test_csv.rename(columns={'Atom_type_N_y':'Atom_type_N_1'}) \ntest_csv = test_csv.rename(columns={'Atom_type_O_y':'Atom_type_O_1'})\ntest_csv = test_csv.rename(columns={'Molecular_mass_x':'Molecular_mass'})\n#cosine\ntdx= test_csv['x0']-test_csv['x1']\ntdy= test_csv['y0']-test_csv['y1']\ntdz= test_csv['z0']-test_csv['z1']\n\ntuve_0_x = test_csv['XG_x']-test_csv['x0']\ntuve_0_y = test_csv['YG_x']-test_csv['y0']\ntuve_0_z = test_csv['ZG_x']-test_csv['z0']\n\ntuve_1_x = test_csv['XG_x']-test_csv['x1']\ntuve_1_y = test_csv['YG_x']-test_csv['y1']\ntuve_1_z = test_csv['ZG_x']-test_csv['z1']\n\ntest_csv['cos_G_x0'] = ((tuve_0_x * tdx)+(tuve_0_y * tdy)+(tuve_0_z * tdz))/((abs(distances_test)*abs(distances_0_CG_test))+ 1e-2)\ntest_csv['cos_G_x1'] = ((tuve_1_x * tdx)+(tuve_1_y * tdy)+(tuve_1_z * tdz))/((abs(distances_test)*abs(distances_1_CG_test))+ 1e-2)\n\ntdist_xy = np.sqrt(np.square(tdx)+np.square(tdy))\ntdist_xz = np.sqrt(np.square(tdx)+np.square(tdz))\ntdist_yz = np.sqrt(np.square(tdy)+np.square(tdz))\n\ntuve_0_xy = np.sqrt(np.square(tuve_0_x)+np.square(tuve_0_y))\ntuve_0_xz = np.sqrt(np.square(tuve_0_x)+np.square(tuve_0_z))\ntuve_0_yz = np.sqrt(np.square(tuve_0_y)+np.square(tuve_0_z))\ntuve_1_xy = np.sqrt(np.square(tuve_1_x)+np.square(tuve_1_y))\ntuve_1_xz = np.sqrt(np.square(tuve_1_x)+np.square(tuve_1_z))\ntuve_1_yz = np.sqrt(np.square(tuve_1_y)+np.square(tuve_1_z))\n\ntest_csv['cos_G_x0_plane_xy'] = ((tdx*tuve_0_x)+(tdy*tuve_0_y))/((abs(tdist_xy)*abs(tuve_0_xy))+1e-2)\ntest_csv['cos_G_x0_plane_xz'] = ((tdx*tuve_0_x)+(tdz*tuve_0_z))/((abs(tdist_xz)*abs(tuve_0_xz))+1e-2)\ntest_csv['cos_G_x0_plane_yz'] = ((tdy*tuve_0_y)+(tdz*tuve_0_z))/((abs(tdist_yz)*abs(tuve_0_yz))+1e-2)\ntest_csv['cos_G_x1_plane_xy'] = ((tdx*tuve_1_x)+(tdy*tuve_1_y))/((abs(tdist_xy)*abs(tuve_1_xy))+1e-2)\ntest_csv['cos_G_x1_plane_xz'] = ((tdx*tuve_1_x)+(tdz*tuve_1_z))/((abs(tdist_xz)*abs(tuve_1_xz))+1e-2)\ntest_csv['cos_G_x1_plane_yz'] = ((tdy*tuve_1_y)+(tdz*tuve_1_z))/((abs(tdist_yz)*abs(tuve_1_yz))+1e-2)\n\n#---------------------------------------------------------\ntest_csv['Molec_x_mean']=test_csv.groupby('molecule_name')['x0'].transform('mean')\ntest_csv['Molec_x_max']=test_csv.groupby('molecule_name')['x0'].transform('max')\ntest_csv['Molec_x_min']=test_csv.groupby('molecule_name')['x0'].transform('min')\ntest_csv['Molec_y_mean']=test_csv.groupby('molecule_name')['y0'].transform('mean')\ntest_csv['Molec_y_max']=test_csv.groupby('molecule_name')['y0'].transform('max')\ntest_csv['Molec_y_min']=test_csv.groupby('molecule_name')['y0'].transform('min')\ntest_csv['Molec_z_mean']=test_csv.groupby('molecule_name')['z0'].transform('mean')\ntest_csv['Molec_z_max']=test_csv.groupby('molecule_name')['z0'].transform('max')\ntest_csv['Molec_z_min']=test_csv.groupby('molecule_name')['z0'].transform('min')\n\ndx0_mean= test_csv['x0']-test_csv['Molec_x_mean']\ndy0_mean= test_csv['y0']-test_csv['Molec_y_mean']\ndz0_mean= test_csv['z0']-test_csv['Molec_z_mean']\n\ndx0_max= test_csv['x0']-test_csv['Molec_x_max']\ndy0_max= test_csv['y0']-test_csv['Molec_y_max']\ndz0_max= test_csv['z0']-test_csv['Molec_z_max']\n\ndx0_min= test_csv['x0']-test_csv['Molec_x_min']\ndy0_min= test_csv['y0']-test_csv['Molec_y_min']\ndz0_min= test_csv['z0']-test_csv['Molec_z_min']\n\ndx1_mean= test_csv['x1']-test_csv['Molec_x_mean']\ndy1_mean= test_csv['y1']-test_csv['Molec_y_mean']\ndz1_mean= test_csv['z1']-test_csv['Molec_z_mean']\n\ndx1_max= test_csv['x1']-test_csv['Molec_x_max']\ndy1_max= test_csv['y1']-test_csv['Molec_y_max']\ndz1_max= test_csv['z1']-test_csv['Molec_z_max']\n\ndx1_min= test_csv['x1']-test_csv['Molec_x_min']\ndy1_min= test_csv['y1']-test_csv['Molec_y_min']\ndz1_min= test_csv['z1']-test_csv['Molec_z_min']\n\ndist_x0_mean_xy = np.sqrt(np.square(dx0_mean)+np.square(dy0_mean))\ndist_x0_mean_xz = np.sqrt(np.square(dx0_mean)+np.square(dz0_mean))\ndist_x0_mean_yz = np.sqrt(np.square(dy0_mean)+np.square(dz0_mean))\ntest_csv['cos_Mean_x0_plane_xy'] = ((tdx*dx0_mean)+(tdy*dy0_mean))/((abs(tdist_xy)*abs(dist_x0_mean_xy))+1e-2)\ntest_csv['cos_Mean_x0_plane_xz'] = ((tdx*dx0_mean)+(tdz*dz0_mean))/((abs(tdist_xz)*abs(dist_x0_mean_xz))+1e-2)\ntest_csv['cos_Mean_x0_plane_yz'] = ((tdy*dy0_mean)+(tdz*dz0_mean))/((abs(tdist_yz)*abs(dist_x0_mean_yz))+1e-2)\n\ndist_x0_max_xy = np.sqrt(np.square(dx0_max)+np.square(dy0_max))\ndist_x0_max_xz = np.sqrt(np.square(dx0_max)+np.square(dz0_max))\ndist_x0_max_yz = np.sqrt(np.square(dy0_max)+np.square(dz0_max))\ntest_csv['cos_Max_x0_plane_xy'] = ((tdx*dx0_max)+(tdy*dy0_max))/((abs(tdist_xy)*abs(dist_x0_max_xy))+1e-2)\ntest_csv['cos_Max_x0_plane_xz'] = ((tdx*dx0_max)+(tdz*dz0_max))/((abs(tdist_xz)*abs(dist_x0_max_xz))+1e-2)\ntest_csv['cos_Max_x0_plane_yz'] = ((tdy*dy0_max)+(tdz*dz0_max))/((abs(tdist_yz)*abs(dist_x0_max_yz))+1e-2)\n\ndist_x0_min_xy = np.sqrt(np.square(dx0_min)+np.square(dy0_min))\ndist_x0_min_xz = np.sqrt(np.square(dx0_min)+np.square(dz0_min))\ndist_x0_min_yz = np.sqrt(np.square(dy0_min)+np.square(dz0_min))\ntest_csv['cos_Min_x0_plane_xy'] = ((tdx*dx0_min)+(tdy*dy0_min))/((abs(tdist_xy)*abs(dist_x0_min_xy))+1e-2)\ntest_csv['cos_Min_x0_plane_xz'] = ((tdx*dx0_min)+(tdz*dz0_min))/((abs(tdist_xz)*abs(dist_x0_min_xz))+1e-2)\ntest_csv['cos_Min_x0_plane_yz'] = ((tdy*dy0_min)+(tdz*dz0_min))/((abs(tdist_yz)*abs(dist_x0_min_yz))+1e-2)\n\ndist_x0_mean = np.sqrt(np.square(dx0_mean)+np.square(dy0_mean)+np.square(dz0_mean))\ndist_x0_max = np.sqrt(np.square(dx0_max)+np.square(dy0_max)+np.square(dz0_max))\ndist_x0_min = np.sqrt(np.square(dx0_min)+np.square(dy0_min)+np.square(dz0_min))\ntest_csv['cos_Mean_x0'] = ((tdx*dx0_mean)+(tdy*dy0_mean)+(tdz*dz0_mean))/((abs(distances_test)*abs(dist_x0_mean))+1e-2)\ntest_csv['cos_Max_x0'] = ((tdx*dx0_max)+(tdy*dy0_max)+(tdz*dz0_max))/((abs(distances_test)*abs(dist_x0_max))+1e-2)\ntest_csv['cos_Min_x0'] = ((tdx*dx0_min)+(tdy*dy0_min)+(tdz*dz0_min))/((abs(distances_test)*abs(dist_x0_min))+1e-2)\n\ndist_x1_mean_xy = np.sqrt(np.square(dx1_mean)+np.square(dy1_mean))\ndist_x1_mean_xz = np.sqrt(np.square(dx1_mean)+np.square(dz1_mean))\ndist_x1_mean_yz = np.sqrt(np.square(dy1_mean)+np.square(dz1_mean))\ntest_csv['cos_Mean_x1_plane_xy'] = ((tdx*dx1_mean)+(tdy*dy1_mean))/((abs(tdist_xy)*abs(dist_x1_mean_xy))+1e-2)\ntest_csv['cos_Mean_x1_plane_xz'] = ((tdx*dx1_mean)+(tdz*dz1_mean))/((abs(tdist_xz)*abs(dist_x1_mean_xz))+1e-2)\ntest_csv['cos_Mean_x1_plane_yz'] = ((tdy*dy1_mean)+(tdz*dz1_mean))/((abs(tdist_yz)*abs(dist_x1_mean_yz))+1e-2)\n\ndist_x1_max_xy = np.sqrt(np.square(dx1_max)+np.square(dy1_max))\ndist_x1_max_xz = np.sqrt(np.square(dx1_max)+np.square(dz1_max))\ndist_x1_max_yz = np.sqrt(np.square(dy1_max)+np.square(dz1_max))\ntest_csv['cos_Max_x1_plane_xy'] = ((tdx*dx1_max)+(tdy*dy1_max))/((abs(tdist_xy)*abs(dist_x1_max_xy))+1e-2)\ntest_csv['cos_Max_x1_plane_xz'] = ((tdx*dx1_max)+(tdz*dz1_max))/((abs(tdist_xz)*abs(dist_x1_max_xz))+1e-2)\ntest_csv['cos_Max_x1_plane_yz'] = ((tdy*dy1_max)+(tdz*dz1_max))/((abs(tdist_yz)*abs(dist_x1_max_yz))+1e-2)\n\ndist_x1_min_xy = np.sqrt(np.square(dx1_min)+np.square(dy1_min))\ndist_x1_min_xz = np.sqrt(np.square(dx1_min)+np.square(dz1_min))\ndist_x1_min_yz = np.sqrt(np.square(dy1_min)+np.square(dz1_min))\ntest_csv['cos_Min_x1_plane_xy'] = ((tdx*dx1_min)+(tdy*dy1_min))/((abs(tdist_xy)*abs(dist_x1_min_xy))+1e-2)\ntest_csv['cos_Min_x1_plane_xz'] = ((tdx*dx1_min)+(tdz*dz1_min))/((abs(tdist_xz)*abs(dist_x1_min_xz))+1e-2)\ntest_csv['cos_Min_x1_plane_yz'] = ((tdy*dy1_min)+(tdz*dz1_min))/((abs(tdist_yz)*abs(dist_x1_min_yz))+1e-2)\n\ndist_x1_mean = np.sqrt(np.square(dx1_mean)+np.square(dy1_mean)+np.square(dz1_mean))\ndist_x1_max = np.sqrt(np.square(dx1_max)+np.square(dy1_max)+np.square(dz1_max))\ndist_x1_min = np.sqrt(np.square(dx1_min)+np.square(dy1_min)+np.square(dz1_min))\ntest_csv['cos_Mean_x1'] = ((tdx*dx1_mean)+(tdy*dy1_mean)+(tdz*dz1_mean))/((abs(distances_test)*abs(dist_x1_mean))+1e-2)\ntest_csv['cos_Max_x1'] = ((tdx*dx1_max)+(tdy*dy1_max)+(tdz*dz1_max))/((abs(distances_test)*abs(dist_x1_max))+1e-2)\ntest_csv['cos_Min_x1'] = ((tdx*dx1_min)+(tdy*dy1_min)+(tdz*dz1_min))/((abs(distances_test)*abs(dist_x1_min))+1e-2)\n\n#-----------------------\n\n# Drop columns\ntrain_csv = train_csv.drop({'X','Y','Z'}, axis=1)\n\ntrain_csv = train_csv.drop(['atom_index_x','atom_index_y'], axis=1)\ntrain_csv = train_csv.drop(['XG_y','YG_y','ZG_y'], axis=1)\ntrain_csv = train_csv.drop(['XG_x','YG_x','ZG_x'], axis=1)\ntrain_csv = train_csv.drop({'Molecular_mass_y'}, axis=1)\ntrain_csv = train_csv.drop(['atom0','atom1'], axis=1)\n#train_csv = train_csv.drop(['links'], axis=1)\n#train_csv = train_csv.drop(['id','molecule_name'], axis=1)\ntrain_csv = train_csv.drop({'Tot_atoms_molecule_y'}, axis=1)\ntrain_csv = train_csv.drop({'x0','x1','y1','z0','z1'}, axis=1)\ntrain_csv = train_csv.drop({'y0'}, axis=1)\ntrain_csv = train_csv.drop({'Molec_x_mean','Molec_x_max','Molec_x_min'}, axis=1)\ntrain_csv = train_csv.drop({'Molec_y_mean','Molec_y_max','Molec_y_min'}, axis=1)\ntrain_csv = train_csv.drop({'Molec_z_mean','Molec_z_max','Molec_z_min'}, axis=1)\n#------------------------------------------------------------------------------\ntest_csv = test_csv.drop(['atom_index_x','atom_index_y'], axis=1)\ntest_csv = test_csv.drop(['XG_y','YG_y','ZG_y'], axis=1)\ntest_csv = test_csv.drop(['XG_x','YG_x','ZG_x'], axis=1)\ntest_csv = test_csv.drop(['Molecular_mass_y'], axis=1)\ntest_csv = test_csv.drop(['atom0','atom1'], axis=1)\n#test_csv = test_csv.drop(['links'], axis=1)\n#test_csv = test_csv.drop(['id','molecule_name'], axis=1)\ntest_csv = test_csv.drop({'Tot_atoms_molecule_y'}, axis=1)\ntest_csv = test_csv.drop({'x0','x1','y1','z0','z1'}, axis=1)\ntest_csv = test_csv.drop({'y0'}, axis=1)\ntest_csv = test_csv.drop({'Molec_x_mean','Molec_x_max','Molec_x_min'}, axis=1)\ntest_csv = test_csv.drop({'Molec_y_mean','Molec_y_max','Molec_y_min'}, axis=1)\ntest_csv = test_csv.drop({'Molec_z_mean','Molec_z_max','Molec_z_min'}, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv = reduce_mem_usage(train_csv)\ntest_csv = reduce_mem_usage(test_csv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Intermediate databases:\ntrain_csv = train_csv.drop(['id','molecule_name'], axis=1)\ntrain_csv = train_csv.drop(['Atomic_EN_x0','Atomic_radius_x0','Atomic_mass_x0','Atom_type_C_0','Atom_type_F_0','Atom_type_H_0','Atom_type_N_0','Atom_type_O_0','Coupling_atom_0_type_H'], axis=1)\ntrain_csv = train_csv.drop(['Atomic_EN_x1','Atomic_radius_x1','Atomic_mass_x1','Atom_type_C_1','Atom_type_F_1','Atom_type_H_1','Atom_type_N_1','Atom_type_O_1'], axis=1)\ntrain_csv = train_csv.drop(['Coupling_atom_1_type_C','Coupling_atom_1_type_H','Coupling_atom_1_type_N'], axis=1)\n\ntest_csv = test_csv.drop(['Atomic_EN_x0','Atomic_radius_x0','Atomic_mass_x0','Atom_type_C_0','Atom_type_F_0','Atom_type_H_0','Atom_type_N_0','Atom_type_O_0','Coupling_atom_0_type_H'], axis=1)\ntest_csv = test_csv.drop(['Atomic_EN_x1','Atomic_radius_x1','Atomic_mass_x1','Atom_type_C_1','Atom_type_F_1','Atom_type_H_1','Atom_type_N_1','Atom_type_O_1'], axis=1)\ntest_csv = test_csv.drop(['Coupling_atom_1_type_C','Coupling_atom_1_type_H','Coupling_atom_1_type_N'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Threshold for removing correlated variables\n# https://www.kaggle.com/adrianoavelar/gridsearch-for-eachtype-lb-1-0\n\nthreshold = 0.95\n\n# Absolute value correlation matrix\n\n\ncorr_matrix = train_csv.corr().abs()\n\n# Getting the upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d columns to remove.' % (len(to_drop)))\nprint('The columns are: ', to_drop )\n\n\ntrain_csv = train_csv.drop(to_drop[1:],axis=1)\ntest_csv = test_csv.drop(to_drop[1:],axis=1)\ntrain_csv = reduce_mem_usage(train_csv)\ntest_csv = reduce_mem_usage(test_csv)\n\nprint('Training shape: ', train_csv.shape)\nprint('Testing shape: ', test_csv.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## DEEP NEURAL NETWORK \n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization \nfrom keras.layers.pooling import MaxPooling2D, AveragePooling2D\nfrom keras.layers.merge import Concatenate\nfrom keras.layers.core import Lambda, Flatten, Dense, Dropout\nfrom keras.initializers import glorot_uniform\nfrom keras.engine.topology import Layer\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import LeakyReLU\nfrom sklearn import metrics\nfrom keras import regularizers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def database_type(db,index):\n    db_type = db[db['type'] == index]\n    \n    coupling= db_type[['scalar_coupling_constant']].copy()\n    dipole = db_type[['Dipole']].copy()\n    potential = db_type[['potential_energy']].copy()\n    fermi = db_type[['fc']].copy()\n    spin_dipolar = db_type[['sd']].copy()\n    paramagnetic_spin = db_type[['pso']].copy()\n    diamagnetic_spin = db_type[['dso']].copy()\n    mulliken_0 = db_type[['mulliken_charge_0']].copy()\n    mulliken_1 = db_type[['mulliken_charge_1']].copy()\n    \n    db_type = db_type.drop(['type', 'scalar_coupling_constant','Dipole','potential_energy','fc','sd','pso','dso','mulliken_charge_0','mulliken_charge_1'], axis=1)\n    db_type = reduce_mem_usage(db_type)\n    \n    return [db_type, coupling, dipole, potential, fermi, spin_dipolar,paramagnetic_spin,diamagnetic_spin,mulliken_0,mulliken_1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric_mae(df, preds):\n    df[\"prediction\"] = preds\n    maes = []\n    y_true = df.scalar_coupling_constant.values\n    y_pred = df.prediction.values\n    mae = np.log(metrics.mean_absolute_error(y_true, y_pred))\n    maes.append(mae)\n    \n    return np.mean(maes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split train_csv in several dataframes, each type of coupling\ntrain_csv, val_csv = train_test_split(train_csv, test_size=0.10, random_state=111)\n\ntrain_1JHN = database_type(train_csv,'1JHN')\ntrain_1JHC = database_type(train_csv,'1JHC')\ntrain_2JHH = database_type(train_csv,'2JHH')\ntrain_2JHN = database_type(train_csv,'2JHN')\ntrain_2JHC = database_type(train_csv,'2JHC')\ntrain_3JHH = database_type(train_csv,'3JHH')\ntrain_3JHC = database_type(train_csv,'3JHC')\ntrain_3JHN = database_type(train_csv,'3JHN')\n\nval_1JHN = database_type(val_csv,'1JHN')\nval_1JHC = database_type(val_csv,'1JHC')\nval_2JHH = database_type(val_csv,'2JHH')\nval_2JHN = database_type(val_csv,'2JHN')\nval_2JHC = database_type(val_csv,'2JHC')\nval_3JHH = database_type(val_csv,'3JHH')\nval_3JHC = database_type(val_csv,'3JHC')\nval_3JHN = database_type(val_csv,'3JHN')\n\ntest_1JHN = test_csv[test_csv['type'] == '1JHN']\ntest_1JHC = test_csv[test_csv['type'] == '1JHC']\ntest_2JHH = test_csv[test_csv['type'] == '2JHH']\ntest_2JHN = test_csv[test_csv['type'] == '2JHN']\ntest_2JHC = test_csv[test_csv['type'] == '2JHC']\ntest_3JHH = test_csv[test_csv['type'] == '3JHH']\ntest_3JHC = test_csv[test_csv['type'] == '3JHC']\ntest_3JHN = test_csv[test_csv['type'] == '3JHN']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # TRAINING:","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HYPERPARAMETERS\nepochs_1JHN= 100\nbatch_1JHN = 32\n\nepochs_2JHH= 50\nbatch_2JHH = 128\n\nepochs_2JHN= 50\nbatch_2JHN = 128\n\nepochs_3JHN= 50\nbatch_3JHN = 128\n\nepochs_1JHC= 50\nbatch_1JHC = 128\n\nepochs_2JHC= 50\nbatch_2JHC = 1024\n\nepochs_3JHH= 50\nbatch_3JHH = 512\n\nepochs_3JHC= 50\nbatch_3JHC = 1024\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### 1) \ndef model_coupling_constant_1JHN(X):\n    X_input = Input(shape = (X.shape[1],))\n                    \n  \n    #concat1 = concatenate([concat, x2_output])    \n    \n    # Fermi coupling\n    xfc = BatchNormalization()(X_input)\n    xfc = Dense(256, activation='elu')(xfc)\n    xfc = Dense(128, activation='elu')(xfc)\n    xfc = Dense(64, activation='elu')(xfc)\n    xfc = Dense(32, activation='elu')(xfc)\n    xfc = Dense(16, activation='elu')(xfc)\n    xfc = Dense(8, activation='elu')(xfc)\n    x3_output  = Dense(1, activation='linear', name = 'fermi_coupling')(xfc)\n    \n    concat = concatenate([X_input, x3_output])\n    \n    # Scalar Coupling Constant\n    \n    xsc = BatchNormalization()(concat)\n    xsc = Dense(256, activation='elu')(xsc) \n    xsc = Dense(128, activation='elu')(xsc)\n    xsc = Dense(64, activation='elu')(xsc)\n    xsc = Dense(32, activation='elu')(xsc)\n    xsc = Dense(16, activation='elu')(xsc)\n    xsc = Dense(8, activation='elu')(xsc)\n    x9_output  = Dense(1, activation='linear', name = 'scalar_coupling')(xsc)\n\n    model= Model(inputs = [X_input] , outputs = [x9_output])\n\n    model.summary()\n    return model\n\nModel_Coupling_1JHN = model_coupling_constant_1JHN(train_1JHN[0]) # R1\nModel_Coupling_1JHN.compile(loss='mae', optimizer='Adam')#, metrics=['accuracy'])\nhistory_1JHN = Model_Coupling_1JHN.fit(train_1JHN[0],{'scalar_coupling': train_1JHN[1].values,\n                            'fermi_coupling': train_1JHN[4].values},validation_data=(val_1JHN[0],{'scalar_coupling': val_1JHN[1].values,\n                            'fermi_coupling': val_1JHN[4].values}),epochs=epochs_1JHN,verbose=1,batch_size = batch_1JHN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history_1JHN.history['loss'])\nplt.plot(history_1JHN.history['val_loss'])\nplt.title('loss 1JHN')\nplt.ylabel('Loss 1JHN')\nplt.xlabel('Epoch')\n_= plt.legend(['Train','Validation'], loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### 1JHN\n\nmodel_check_1JHN = Model_Coupling_1JHN.predict(val_1JHN[0])\nlog_MAE_1JHN = metric_mae(val_1JHN[1], model_check_1JHN)\nval_1JHN[1] = val_1JHN[1].drop('prediction',axis=1)\n\nprint(\"log MAE_1JHN=\", log_MAE_1JHN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inter_1JHN = pd.DataFrame(test_1JHN['id']) \ninter_1JHN['type'] = test_1JHN['type']\ntest_1JHN_bis = test_1JHN.drop(['id','molecule_name','type'], axis=1)\n\nmodel_predict_1JHN = Model_Coupling_1JHN.predict(test_1JHN_bis)\ninter_1JHN['pred_values'] = model_predict_1JHN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### 2) \ndef model_coupling_constant_2JHH(X):  \n    X_input = Input(shape = (X.shape[1],))\n                    \n    # Fermi coupling\n    xfc = BatchNormalization()(X_input)\n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dense(1024, activation='elu')(xfc)\n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(256, activation='elu')(xfc) \n    xfc = Dense(64, activation='elu')(xfc)\n    xfc = Dense(32, activation='elu')(xfc)\n    xfc = Dense(16, activation='elu')(xfc)\n    xfc = Dense(8, activation='elu')(xfc)\n    x3_output  = Dense(1, activation='linear', name = 'fermi_coupling')(xfc)\n    \n    concat = concatenate([X_input, x3_output])\n    \n    # Scalar Coupling Constant\n    \n    xsc = BatchNormalization()(concat)\n    #xsc = Dense(256, activation='relu')(xsc) \n    #xsc = Dense(128, activation='relu')(xsc)\n    #xsc = Dense(64, activation='relu')(xsc)\n    #xsc = Dense(32, activation='relu')(xsc)\n    #xsc = Dense(16, activation='relu')(xsc)\n    xsc = Dense(8, activation='elu')(xsc)\n    x9_output  = Dense(1, activation='linear', name = 'scalar_coupling')(xsc)\n\n    model= Model(inputs = [X_input] , outputs = [x9_output])\n\n    model.summary()\n    return model\n\nModel_Coupling_2JHH = model_coupling_constant_2JHH(train_2JHH[0]) # R1\nModel_Coupling_2JHH.compile(loss='mae', optimizer='Adam')#, metrics=['accuracy'])\nhistory_2JHH = Model_Coupling_2JHH.fit(train_2JHH[0],{'scalar_coupling': train_2JHH[1].values,\n                            'fermi_coupling': train_2JHH[4].values},validation_data=(val_2JHH[0],{'scalar_coupling': val_2JHH[1].values,\n                            'fermi_coupling': val_2JHH[4].values}),epochs=epochs_2JHH,verbose=1,batch_size = batch_2JHH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history_2JHH.history['loss'])\nplt.plot(history_2JHH.history['val_loss'])\nplt.title('loss 2JHH')\nplt.ylabel('Loss 2JHH')\nplt.xlabel('Epoch')\n_= plt.legend(['Train','Validation'], loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### 2JHH\n\nmodel_check_2JHH = Model_Coupling_2JHH.predict(val_2JHH[0])\nlog_MAE_2JHH = metric_mae(val_2JHH[1], model_check_2JHH)\nval_2JHH[1] = val_2JHH[1].drop('prediction',axis=1)\n\nprint(\"log MAE_2JHH=\", log_MAE_2JHH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inter_2JHH = pd.DataFrame(test_2JHH['id']) \ninter_2JHH['type'] = test_2JHH['type']\ntest_2JHH_bis = test_2JHH.drop(['id','molecule_name','type'], axis=1)\n\nmodel_predict_2JHH = Model_Coupling_2JHH.predict(test_2JHH_bis)\ninter_2JHH['pred_values'] = model_predict_2JHH","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### 3) \ndef model_coupling_constant_2JHN(X):\n    X_input = Input(shape = (X.shape[1],))\n                    \n    # Fermi coupling\n    xfc = BatchNormalization()(X_input)\n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dense(1024, activation='elu')(xfc)\n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(1024, activation='elu')(xfc)\n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(256, activation='elu')(xfc) \n    xfc = Dense(64, activation='elu')(xfc)\n    xfc = Dense(32, activation='elu')(xfc)\n    xfc = Dense(16, activation='elu')(xfc)\n    xfc = Dense(8, activation='elu')(xfc)\n    x3_output  = Dense(1, activation='linear', name = 'fermi_coupling')(xfc)\n    \n    concat = concatenate([X_input, x3_output])\n    \n    # Scalar Coupling Constant\n    \n    xsc = BatchNormalization()(concat)\n    #xsc = Dense(256, activation='relu')(xsc) \n    #xsc = Dense(128, activation='relu')(xsc)\n    #xsc = Dense(64, activation='relu')(xsc)\n    #xsc = Dense(32, activation='relu')(xsc)\n    #xsc = Dense(16, activation='relu')(xsc)\n    xsc = Dense(8, activation='elu')(xsc)\n    x9_output  = Dense(1, activation='linear', name = 'scalar_coupling')(xsc)\n\n    model= Model(inputs = [X_input] , outputs = [x9_output])\n\n    model.summary()\n    return model\n\nModel_Coupling_2JHN = model_coupling_constant_2JHN(train_2JHN[0]) # R1\nModel_Coupling_2JHN.compile(loss='mae', optimizer='Adam')#, metrics=['accuracy'])\nhistory_2JHN = Model_Coupling_2JHN.fit(train_2JHN[0],{'scalar_coupling': train_2JHN[1].values,\n                            'fermi_coupling': train_2JHN[4].values},validation_data=(val_2JHN[0],{'scalar_coupling': val_2JHN[1].values,\n                            'fermi_coupling': val_2JHN[4].values}),epochs=epochs_2JHN,verbose=1,batch_size = batch_2JHN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history_2JHN.history['loss'])\nplt.plot(history_2JHN.history['val_loss'])\nplt.title('loss 2JHN')\nplt.ylabel('Loss 2JHN')\nplt.xlabel('Epoch')\n_= plt.legend(['Train','Validation'], loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### 2JHN\n\nmodel_check_2JHN = Model_Coupling_2JHN.predict(val_2JHN[0])\nlog_MAE_2JHN = metric_mae(val_2JHN[1], model_check_2JHN)\nval_2JHN[1] = val_2JHN[1].drop('prediction',axis=1)\n\nprint(\"log MAE_2JHN=\", log_MAE_2JHN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inter_2JHN = pd.DataFrame(test_2JHN['id']) \ninter_2JHN['type'] = test_2JHN['type']\ntest_2JHN_bis = test_2JHN.drop(['id','molecule_name','type'], axis=1)\n\nmodel_predict_2JHN = Model_Coupling_2JHN.predict(test_2JHN_bis)\ninter_2JHN['pred_values'] = model_predict_2JHN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### 4)\ndef model_coupling_constant_3JHN(X):\n    X_input = Input(shape = (X.shape[1],))\n                    \n    # Fermi coupling\n    xfc = BatchNormalization()(X_input)\n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dense(1024, activation='elu')(xfc)\n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(1024, activation='elu')(xfc) \n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(256, activation='elu')(xfc) \n    xfc = Dense(64, activation='elu')(xfc)\n    xfc = Dense(32, activation='elu')(xfc)\n    xfc = Dense(16, activation='elu')(xfc)\n    xfc = Dense(8, activation='elu')(xfc)\n    x3_output  = Dense(1, activation='linear', name = 'fermi_coupling')(xfc)\n    \n    concat = concatenate([X_input, x3_output])\n    \n    # Scalar Coupling Constant\n    \n    xsc = BatchNormalization()(concat)\n    #xsc = Dense(256, activation='relu')(xsc) \n    #xsc = Dense(128, activation='relu')(xsc)\n    #xsc = Dense(64, activation='relu')(xsc)\n    #xsc = Dense(32, activation='relu')(xsc)\n    #xsc = Dense(16, activation='relu')(xsc)\n    xsc = Dense(8, activation='elu')(xsc)\n    x9_output  = Dense(1, activation='linear', name = 'scalar_coupling')(xsc)\n\n    model= Model(inputs = [X_input] , outputs = [x9_output])\n\n    model.summary()\n    return model\n\nModel_Coupling_3JHN = model_coupling_constant_3JHN(train_3JHN[0]) # R1\nModel_Coupling_3JHN.compile(loss='mae', optimizer='Adam')#, metrics=['accuracy'])\nhistory_3JHN = Model_Coupling_3JHN.fit(train_3JHN[0],{'scalar_coupling': train_3JHN[1].values,\n                            'fermi_coupling': train_3JHN[4].values},validation_data=(val_3JHN[0],{'scalar_coupling': val_3JHN[1].values,\n                            'fermi_coupling': val_3JHN[4].values}),epochs=epochs_3JHN,verbose=1,batch_size = batch_3JHN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history_3JHN.history['loss'])\nplt.plot(history_3JHN.history['val_loss'])\nplt.title('loss 3JHN')\nplt.ylabel('Loss 3JHN')\nplt.xlabel('Epoch')\n_= plt.legend(['Train','Validation'], loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### 3JHN\n\nmodel_check_3JHN = Model_Coupling_3JHN.predict(val_3JHN[0])\nlog_MAE_3JHN = metric_mae(val_3JHN[1], model_check_3JHN)\nval_3JHN[1] = val_3JHN[1].drop('prediction',axis=1)\n\nprint(\"log MAE_3JHN=\", log_MAE_3JHN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inter_3JHN = pd.DataFrame(test_3JHN['id']) \ninter_3JHN['type'] = test_3JHN['type']\ntest_3JHN_bis = test_3JHN.drop(['id','molecule_name','type'], axis=1)\n\nmodel_predict_3JHN = Model_Coupling_3JHN.predict(test_3JHN_bis)\ninter_3JHN['pred_values'] = model_predict_3JHN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################ B) 1JHC , 2JHC, 3JHH , 3JHC  #############################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### 5)  \ndef model_coupling_constant_1JHC(X):\n    X_input = Input(shape = (X.shape[1],))\n    act= LeakyReLU(alpha=0.1)\n    dr=0.4\n    \n    X_input = Input(shape = (X.shape[1],))   \n    \n    #concat1 = concatenate([concat, x2_output])    \n    \n    # Fermi coupling\n    xfc = BatchNormalization()(X_input)\n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dense(1024, activation='elu')(xfc)\n    xfc = Dense(1024, activation='elu')(xfc)\n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(256, activation='elu')(xfc) \n    xfc = Dense(64, activation='elu')(xfc)\n    xfc = Dense(32, activation='elu')(xfc)\n    xfc = Dense(16, activation='elu')(xfc)\n    xfc = Dense(8, activation='elu')(xfc)\n    x3_output  = Dense(1, activation='linear', name = 'fermi_coupling')(xfc)\n    \n    concat = concatenate([X_input, x3_output])\n    \n    # Scalar Coupling Constant\n    \n    xsc = BatchNormalization()(concat)\n    #xsc = Dense(256, activation='relu')(xsc) \n    #xsc = Dense(128, activation='relu')(xsc)\n    #xsc = Dense(64, activation='relu')(xsc)\n    #xsc = Dense(32, activation='relu')(xsc)\n    #xsc = Dense(16, activation='relu')(xsc)\n    xsc = Dense(8, activation='elu')(xsc)\n    x9_output  = Dense(1, activation='linear', name = 'scalar_coupling')(xsc)\n\n    model= Model(inputs = [X_input] , outputs = [x9_output])\n\n    model.summary()\n    return model\n\nModel_Coupling_1JHC = model_coupling_constant_1JHC(train_1JHC[0]) # R1\nModel_Coupling_1JHC.compile(loss='mae', optimizer='Adam')#, metrics=['accuracy'])\nhistory_1JHC = Model_Coupling_1JHC.fit(train_1JHC[0],{'scalar_coupling': train_1JHC[1].values,\n                            'fermi_coupling': train_1JHC[4].values},validation_data=(val_1JHC[0],{'scalar_coupling': val_1JHC[1].values,\n                            'fermi_coupling': val_1JHC[4].values}),epochs=epochs_1JHC,verbose=1,batch_size = batch_1JHC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history_1JHC.history['loss'])\nplt.plot(history_1JHC.history['val_loss'])\nplt.title('loss 1JHC')\nplt.ylabel('Loss 1JHC')\nplt.xlabel('Epoch')\n_= plt.legend(['Train','Validation'], loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### 1JHC\n\nmodel_check_1JHC = Model_Coupling_1JHC.predict(val_1JHC[0])\nlog_MAE_1JHC = metric_mae(val_1JHC[1], model_check_1JHC)\nval_1JHC[1] = val_1JHC[1].drop('prediction',axis=1)\n\nprint(\"log MAE_1JHC=\", log_MAE_1JHC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inter_1JHC = pd.DataFrame(test_1JHC['id']) \ninter_1JHC['type'] = test_1JHC['type']\ntest_1JHC_bis = test_1JHC.drop(['id','molecule_name','type'], axis=1)\n\nmodel_predict_1JHC = Model_Coupling_1JHC.predict(test_1JHC_bis)\ninter_1JHC['pred_values'] = model_predict_1JHC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 6)\ndef model_coupling_constant_2JHC(X):\n    X_input = Input(shape = (X.shape[1],))\n    act= LeakyReLU(alpha=0.1)\n    dr=0.4\n    \n    X_input = Input(shape = (X.shape[1],))\n                    \n  \n    #concat1 = concatenate([concat, x2_output])    \n    \n    # Fermi coupling\n    xfc = BatchNormalization()(X_input)\n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dense(1024, activation='elu')(xfc)\n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(256, activation='elu')(xfc) \n    xfc = Dense(64, activation='elu')(xfc)\n    xfc = Dense(32, activation='elu')(xfc)\n    xfc = Dense(16, activation='elu')(xfc)\n    xfc = Dense(8, activation='elu')(xfc)\n    x3_output  = Dense(1, activation='linear', name = 'fermi_coupling')(xfc)\n    \n    concat = concatenate([X_input, x3_output])\n    \n    # Scalar Coupling Constant\n    \n    xsc = BatchNormalization()(concat)\n    #xsc = Dense(256, activation='relu')(xsc) \n    #xsc = Dense(128, activation='relu')(xsc)\n    #xsc = Dense(64, activation='relu')(xsc)\n    #xsc = Dense(32, activation='relu')(xsc)\n    #xsc = Dense(16, activation='relu')(xsc)\n    xsc = Dense(8, activation='elu')(xsc)\n    x9_output  = Dense(1, activation='linear', name = 'scalar_coupling')(xsc)\n\n    model= Model(inputs = [X_input] , outputs = [x9_output])\n\n    model.summary()\n    return model\n\nModel_Coupling_2JHC = model_coupling_constant_2JHC(train_2JHC[0]) # R1\nModel_Coupling_2JHC.compile(loss='mae', optimizer='Adam')#, metrics=['accuracy'])\nhistory_2JHC = Model_Coupling_2JHC.fit(train_2JHC[0],{'scalar_coupling': train_2JHC[1].values,\n                            'fermi_coupling': train_2JHC[4].values},validation_data=(val_2JHC[0],{'scalar_coupling': val_2JHC[1].values,\n                            'fermi_coupling': val_2JHC[4].values}),epochs=epochs_2JHC,verbose=1,batch_size = batch_2JHC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history_2JHC.history['loss'])\nplt.plot(history_2JHC.history['val_loss'])\nplt.title('loss 2JHC')\nplt.ylabel('Loss 2JHC')\nplt.xlabel('Epoch')\n_= plt.legend(['Train','Validation'], loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### 2JHC\n\nmodel_check_2JHC = Model_Coupling_2JHC.predict(val_2JHC[0])\nlog_MAE_2JHC = metric_mae(val_2JHC[1], model_check_2JHC)\nval_2JHC[1] = val_2JHC[1].drop('prediction',axis=1)\n\nprint(\"log MAE_2JHC=\", log_MAE_2JHC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inter_2JHC = pd.DataFrame(test_2JHC['id']) \ninter_2JHC['type'] = test_2JHC['type']\ntest_2JHC_bis = test_2JHC.drop(['id','molecule_name','type'], axis=1)\n\nmodel_predict_2JHC = Model_Coupling_2JHC.predict(test_2JHC_bis)\ninter_2JHC['pred_values'] = model_predict_2JHC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### 7)\ndef model_coupling_constant_3JHH(X):\n\n    X_input = Input(shape = (X.shape[1],))\n                    \n    # Fermi coupling\n    xfc = BatchNormalization()(X_input)\n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dense(1024, activation='elu')(xfc)\n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(256, activation='elu')(xfc) \n    xfc = Dense(64, activation='elu')(xfc)\n    xfc = Dense(32, activation='elu')(xfc)\n    xfc = Dense(16, activation='elu')(xfc)\n    xfc = Dense(8, activation='elu')(xfc)\n    x3_output  = Dense(1, activation='linear', name = 'fermi_coupling')(xfc)\n    \n    concat = concatenate([X_input, x3_output])\n    \n    # Scalar Coupling Constant\n    \n    xsc = BatchNormalization()(concat)\n    #xsc = Dense(256, activation='relu')(xsc) \n    #xsc = Dense(128, activation='relu')(xsc)\n    #xsc = Dense(64, activation='relu')(xsc)\n    #xsc = Dense(32, activation='relu')(xsc)\n    #xsc = Dense(16, activation='relu')(xsc)\n    xsc = Dense(8, activation='elu')(xsc)\n    x9_output  = Dense(1, activation='linear', name = 'scalar_coupling')(xsc)\n\n    model= Model(inputs = [X_input] , outputs = [x9_output])\n\n    model.summary()\n    return model\n\nModel_Coupling_3JHH = model_coupling_constant_3JHH(train_3JHH[0]) # R1\nModel_Coupling_3JHH.compile(loss='mae', optimizer='Adam')#, metrics=['accuracy'])\nhistory_3JHH = Model_Coupling_3JHH.fit(train_3JHH[0],{'scalar_coupling': train_3JHH[1].values,\n                            'fermi_coupling': train_3JHH[4].values},validation_data=(val_3JHH[0],{'scalar_coupling': val_3JHH[1].values,\n                            'fermi_coupling': val_3JHH[4].values}),epochs=epochs_3JHH,verbose=1,batch_size = batch_3JHH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history_3JHH.history['loss'])\nplt.plot(history_3JHH.history['val_loss'])\nplt.title('loss 3JHH')\nplt.ylabel('Loss 3JHH')\nplt.xlabel('Epoch')\n_= plt.legend(['Train','Validation'], loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### 3JHH\n\nmodel_check_3JHH = Model_Coupling_3JHH.predict(val_3JHH[0])\nlog_MAE_3JHH = metric_mae(val_3JHH[1], model_check_3JHH)\nval_3JHH[1] = val_3JHH[1].drop('prediction',axis=1)\n\nprint(\"log MAE_3JHH=\", log_MAE_3JHH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inter_3JHH = pd.DataFrame(test_3JHH['id']) \ninter_3JHH['type'] = test_3JHH['type']\ntest_3JHH_bis = test_3JHH.drop(['id','molecule_name','type'], axis=1)\n\nmodel_predict_3JHH = Model_Coupling_3JHH.predict(test_3JHH_bis)\ninter_3JHH['pred_values'] = model_predict_3JHH","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### 8)\ndef model_coupling_constant_3JHC(X):\n\n    X_input = Input(shape = (X.shape[1],))\n    act= LeakyReLU(alpha=0.1)\n    dr=0.4\n    \n    X_input = Input(shape = (X.shape[1],))\n                    \n    #concat1 = concatenate([concat, x2_output])    \n    \n    # Fermi coupling\n    xfc = BatchNormalization()(X_input)\n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dense(1024, activation='elu')(xfc)\n    xfc = Dropout(0.1)(xfc) #0,1\n    xfc = Dense(512, activation='elu')(xfc) \n    xfc = Dropout(0.1)(xfc) \n    xfc = Dense(64, activation='elu')(xfc)\n    xfc = Dense(32, activation='elu')(xfc)\n    xfc = Dense(16, activation='elu')(xfc)\n    xfc = Dense(8, activation='elu')(xfc)\n    x3_output  = Dense(1, activation='linear', name = 'fermi_coupling')(xfc)\n    \n    concat = concatenate([X_input, x3_output])\n    \n    # Scalar Coupling Constant\n    \n    xsc = BatchNormalization()(concat)\n    #xsc = Dense(256, activation='relu')(xsc) \n    #xsc = Dense(128, activation='elu')(xsc) #esta\n    #xsc = Dense(64, activation='relu')(xsc)\n    #xsc = Dense(32, activation='relu')(xsc)\n    #xsc = Dense(16, activation='elu')(xsc) #esta\n    xsc = Dense(8, activation='elu')(xsc)\n    x9_output  = Dense(1, activation='linear', name = 'scalar_coupling')(xsc)\n\n    model= Model(inputs = [X_input] , outputs = [x9_output])\n\n    model.summary()\n    return model\n\nModel_Coupling_3JHC = model_coupling_constant_3JHC(train_3JHC[0]) # R1\nModel_Coupling_3JHC.compile(loss='mae', optimizer='Adam')#, metrics=['accuracy'])\nhistory_3JHC = Model_Coupling_3JHC.fit(train_3JHC[0],{'scalar_coupling': train_3JHC[1].values,\n                            'fermi_coupling': train_3JHC[4].values},validation_data=(val_3JHC[0],{'scalar_coupling': val_3JHC[1].values,\n                            'fermi_coupling': val_3JHC[4].values}),epochs=epochs_3JHC,verbose=1,batch_size = batch_3JHC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history_3JHC.history['loss'])\nplt.plot(history_3JHC.history['val_loss'])\nplt.title('loss 3JHC')\nplt.ylabel('Loss 3JHC')\nplt.xlabel('Epoch')\n_= plt.legend(['Train','Validation'], loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### 3JHC\n\nmodel_check_3JHC = Model_Coupling_3JHC.predict(val_3JHC[0])\nlog_MAE_3JHC = metric_mae(val_3JHC[1], model_check_3JHC)\nval_3JHC[1] = val_3JHC[1].drop('prediction',axis=1)\n\nprint(\"log MAE_3JHC=\", log_MAE_3JHC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inter_3JHC = pd.DataFrame(test_3JHC['id']) \ninter_3JHC['type'] = test_3JHC['type']\ntest_3JHC_bis = test_3JHC.drop(['id','molecule_name','type'], axis=1)\n\nmodel_predict_3JHC = Model_Coupling_3JHC.predict(test_3JHC_bis)\ninter_3JHC['pred_values'] = model_predict_3JHC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenate all predictions' model\npred = pd.concat([inter_1JHN, inter_1JHC, inter_2JHN, inter_3JHN, inter_2JHC, inter_2JHH, inter_3JHH, inter_3JHC])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = pred.sort_values(by=['id'], ascending=[True])\npred = pred['pred_values']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # SUBMISSION:\npredictions = sample_submission_csv.copy()\npredictions['scalar_coupling_constant'] = pred\npredictions.to_csv('submission_MOLECULAR.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CHECKING THE MODEL WITH MAE METRICS\n\nprint(\"log MAE_1JHN=\", log_MAE_1JHN)\nprint(\"log MAE_2JHH=\", log_MAE_2JHH)\nprint(\"log MAE_2JHN=\", log_MAE_2JHN)\nprint(\"log MAE_3JHN=\", log_MAE_3JHN)\nprint(\"log MAE_1JHC=\", log_MAE_1JHC)\nprint(\"log MAE_2JHC=\", log_MAE_2JHC)\nprint(\"log MAE_3JHH=\", log_MAE_3JHH)\nprint(\"log MAE_3JHC=\", log_MAE_3JHC)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = (log_MAE_1JHN+log_MAE_2JHH+log_MAE_2JHN+log_MAE_3JHN+log_MAE_1JHC+log_MAE_2JHC+log_MAE_3JHH+log_MAE_3JHC)/8\nprint('SCORE:', score)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}