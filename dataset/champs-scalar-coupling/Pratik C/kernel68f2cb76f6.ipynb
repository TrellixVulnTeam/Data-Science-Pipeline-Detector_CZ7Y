{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n#print(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\nprint(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['molecule_name'].nunique())\nprint(train['scalar_coupling_constant'].nunique())\nprint(train['atom_index_0'].nunique())\nprint(train['atom_index_1'].nunique())\nprint(train['type'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pd.to_numeric(train['scalar_coupling_constant'],downcast='integer')\n#train['scalar_coupling_constant'].astype(int).nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train['scalar_coupling_constant']=train['scalar_coupling_constant'].astype(int)\n# train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"structure = pd.read_csv(\"../input/structures.csv\")\nprint(structure.shape)\nstructure.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\n#train_sample=train.values[::100]\n#plt.plot(train_sample.atom_index_0,train_sample.scalar_coupling_constant)\n#train_sample[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from numpy import array\n# train_sample=array([[int(x[0]),x[1],int(x[2]),int(x[3]),x[4],int(x[5])] for x in train_sample])\n# #plt.plot(train_sample[2],train_sample[5])\n# type(train_sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.plot(train_sample[:,2],train_sample[:,5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.tree import DecisionTreeRegressor\n# from sklearn.model_selection import cross_val_score\n# reg=DecisionTreeRegressor()\n# cross_val_score(reg,train_sample[:,2:4],train_sample[:,5])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  params = {'boosting': 'gbdt', 'colsample_bytree': 1, \n#               'learning_rate': 0.1, 'max_depth': 40, 'metric': 'mae',\n#               'min_child_samples': 50, 'num_leaves': 500, \n#               'objective': 'regression', 'reg_alpha': 0.5, \n#               'reg_lambda': 0.8, 'subsample': 0.5}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#     X_train = X_train.drop(['id', 'atom_0', 'type', 'atom_1','molecule_name'], axis=1).values\n#     y_train = y_train.values\n#     X_val = X_val.drop(['id', 'atom_0', 'type', 'atom_1','molecule_name'], axis=1).values\n#     y_val = y_val.values\n# train['type'] = train['type'].astype('category')\n# train['atom_index_0'] = train['atom_index_0'].astype('category')\n# train['atom_index_0'] = train['atom_index_0'].astype('category')\n# lgtrain = lgb.Dataset(train.drop(['id','molecule_name'], axis=1))\n\n#lgtrain.set_categorical_feature(categorical_feature=4)\n#     lgval = lightgbm.Dataset(X_val, label=y_val)\n \n#model_lgb = lgb.train(params, lgtrain, 5000, verbose_eval=500, categorical_feature=['atom_index_0', 'type', 'atom_index_1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/test.csv\")\n#test=test.drop(['id', 'molecule_name'], axis=1)\n#test['type'] = test['type'].astype('category')\n#test['atom_index_0'] = test['atom_index_0'].astype('category')\n#test['atom_index_0'] = test['atom_index_0'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def map_atom_info(df, atom_idx):\n    df = pd.merge(df, structure, how = 'left',\n                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n                  right_on = ['molecule_name',  'atom_index'])\n    \n    df = df.drop('atom_index', axis=1)\n    df = df.rename(columns={'atom': f'atom_{atom_idx}',\n                            'x': f'x_{atom_idx}',\n                            'y': f'y_{atom_idx}',\n                            'z': f'z_{atom_idx}'})\n    return df\n\ntrain = map_atom_info(train, 0)\ntrain = map_atom_info(train, 1)\n\ntest = map_atom_info(test, 0)\ntest = map_atom_info(test, 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"str(train['id'][1]).isnumeric()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_p_0 = train[['x_0', 'y_0', 'z_0']].values\ntrain_p_1 = train[['x_1', 'y_1', 'z_1']].values\ntest_p_0 = test[['x_0', 'y_0', 'z_0']].values\ntest_p_1 = test[['x_1', 'y_1', 'z_1']].values\n\ntrain['dist'] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\ntest['dist'] = np.linalg.norm(test_p_0 - test_p_1, axis=1)\ntrain['dist_x'] = (train['x_0'] - train['x_1']) ** 2\ntest['dist_x'] = (test['x_0'] - test['x_1']) ** 2\ntrain['dist_y'] = (train['y_0'] - train['y_1']) ** 2\ntest['dist_y'] = (test['y_0'] - test['y_1']) ** 2\ntrain['dist_z'] = (train['z_0'] - train['z_1']) ** 2\ntest['dist_z'] = (test['z_0'] - test['z_1']) ** 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['type_0'] = train['type'].apply(lambda x: x[0])\ntest['type_0'] = test['type'].apply(lambda x: x[0])\ntrain['type_1'] = train['type'].apply(lambda x: x[1:])\ntest['type_1'] = test['type'].apply(lambda x: x[1:])\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['dist_to_type_mean'] = train['dist'] / train.groupby('type')['dist'].transform('mean')\ntest['dist_to_type_mean'] = test['dist'] / test.groupby('type')['dist'].transform('mean')\n\ntrain['dist_to_type_0_mean'] = train['dist'] / train.groupby('type_0')['dist'].transform('mean')\ntest['dist_to_type_0_mean'] = test['dist'] / test.groupby('type_0')['dist'].transform('mean')\n\ntrain['dist_to_type_1_mean'] = train['dist'] / train.groupby('type_1')['dist'].transform('mean')\ntest['dist_to_type_1_mean'] = test['dist'] / test.groupby('type_1')['dist'].transform('mean')\n\ntrain[f'molecule_type_dist_mean'] = train.groupby(['molecule_name', 'type'])['dist'].transform('mean')\ntest[f'molecule_type_dist_mean'] = test.groupby(['molecule_name', 'type'])['dist'].transform('mean')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import networkx as nx\nfig, ax = plt.subplots(figsize = (20, 12))\nfor i, t in enumerate(train['type'].unique()):\n    train_type = train.loc[train['type'] == t]\n    \n    bad_atoms_0 = list(train_type['atom_index_0'].value_counts(normalize=True)[train_type['atom_index_0'].value_counts(normalize=True) < 0.01].index)\n    bad_atoms_1 = list(train_type['atom_index_1'].value_counts(normalize=True)[train_type['atom_index_1'].value_counts(normalize=True) < 0.01].index)\n    bad_atoms = list(set(bad_atoms_0 + bad_atoms_1))\n    train_type = train_type.loc[(train_type['atom_index_0'].isin(bad_atoms_0) == False) & (train_type['atom_index_1'].isin(bad_atoms_1) == False)]\n    \n    G = nx.from_pandas_edgelist(train_type, 'atom_index_0', 'atom_index_1', ['scalar_coupling_constant'])\n    plt.subplot(2, 4, i + 1);\n    nx.draw(G, with_labels=True);\n    plt.title(f'Graph for type {t}')\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing #import LableEncoder\n\nfor f in ['atom_0', 'atom_1', 'type_0', 'type_1', 'type']:\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(train[f].values) + list(test[f].values))\n    train[f] = lbl.transform(list(train[f].values))\n    test[f] = lbl.transform(list(test[f].values))\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_fold = 5\nfrom sklearn.model_selection import KFold\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=11)\nparams = {'num_leaves': 128,\n          'min_child_samples': 79,\n          'objective': 'regression',\n          'max_depth': 11,\n          'learning_rate': 0.2,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 1,\n          \"subsample\": 0.9,\n          \"bagging_seed\": 11,\n          \"metric\": 'mae',\n          \"verbosity\": -1,\n          'reg_alpha': 0.1,\n          'reg_lambda': 0.3,\n          'colsample_bytree': 1.0\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for t,val in folds.split(train):\nfrom sklearn.model_selection import train_test_split\nt,val=train_test_split(train,test_size=0.2)\nX = t.drop(['id', 'molecule_name', 'scalar_coupling_constant'], axis=1)\nY = t['scalar_coupling_constant']\nlgb_train = lgb.Dataset(X, Y)\nX_val = val.drop(['id', 'molecule_name','scalar_coupling_constant'], axis=1)\nY_val = val['scalar_coupling_constant']\nlgb_eval = lgb.Dataset(X_val, Y_val, reference=lgb_train)\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=20,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=5)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=gbm.predict(X_val)\n#print(pred)\n#print(Y_val)\nmaes = (Y_val-pred).abs()\nprint(np.log(maes.map(lambda x:x).mean()))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores = []\n# feature_importance = pd.DataFrame()\n# fold_importance = pd.DataFrame()\n#             fold_importance[\"feature\"] = columns\n#             fold_importance[\"importance\"] = model.feature_importances_\n#             fold_importance[\"fold\"] = fold_n + 1\n#             feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\nimport seaborn as sns\nscores=gbm.feature_importance()\nfeatures=gbm.feature_name()\nfeature_importance = pd.DataFrame()\nfeature_importance[\"feature\"] = features#train.columns\nfeature_importance[\"importance\"] = scores#gbm.feature_importance()\n#f=sns.load_dataset(feature_importance).sort_value(by=\"importance\", ascending=False)\nfeature_importance.sort_values(by=[\"importance\"], ascending=False, inplace=True)\nplt.figure(figsize=(16, 12))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_val = pd.DataFrame({\"type\":X_val[\"type\"]})\ndf_val['scalar_coupling_constant'] = Y_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric(df, preds):\n    df['diff'] = (df['scalar_coupling_constant'] - preds).abs()\n    return np.log(df.groupby('type')['diff'].mean().map(lambda x: max(x, 1e-9))).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_metric(params):\n    \n    model_lgb = lgb.train(params, lgb_train, 500, \n                          valid_sets=[lgb_train, lgb_eval], early_stopping_rounds=20, \n                          verbose_eval=500)\n\n    pred = model_lgb.predict(X_val)\n    #pred=gbm.predict(X_val)\n    #print(pred)\n    #print(Y_val)\n    #maes = (Y_val-pred).abs()\n    #print(np.log(maes.map(lambda x:x).mean()))\n\n    score = metric(df_val, pred)\n    \n    print(score)\n \n    return {\n        'loss': score,\n        'status': STATUS_OK,\n        'stats_running': STATUS_RUNNING\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyper_space = {'objective': 'regression',\n               'metric':'mae',\n               'boosting':'gbdt',\n               #'n_estimators': hp.choice('n_estimators', [25, 40, 50, 75, 100, 250, 500]),\n               'max_depth':  hp.choice('max_depth', [5, 8, 10, 12, 15]),\n               'num_leaves': hp.choice('num_leaves', [100, 250, 500, 650, 750, 1000,1300]),\n               'subsample': hp.choice('subsample', [.3, .5, .7, .8, 1]),\n               'colsample_bytree': hp.choice('colsample_bytree', [ .6, .7, .8, .9, 1]),\n               'learning_rate': hp.choice('learning_rate', [.1, .2, .3]),\n               'reg_alpha': hp.choice('reg_alpha', [.1, .2, .3, .4, .5, .6]),\n               'reg_lambda':  hp.choice('reg_lambda', [.1, .2, .3, .4, .5, .6]),               \n               'min_child_samples': hp.choice('min_child_samples', [20, 45, 70, 100])}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from functools import partial\ntrials = Trials()\n\n# Set algoritm parameters\nalgo = partial(tpe.suggest, \n               n_startup_jobs=-1)\n\n# Seting the number of Evals\nMAX_EVALS= 5\n#MAX_EVALS=15\n# Fit Tree Parzen Estimator\nbest_vals = fmin(evaluate_metric, space=hyper_space, verbose=1,\n                 algo=algo, max_evals=MAX_EVALS, trials=trials)\n\n# Print best parameters\nbest_params = space_eval(hyper_space, best_vals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbm = lgb.train(best_params,\n                lgb_train,\n                num_boost_round=20,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=5)\npred=gbm.predict(X_val)\n#print(pred)\n#print(Y_val)\nmaes = (Y_val-pred).abs()\nprint(np.log(maes.map(lambda x:x).mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.drop(['molecule_name'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test.drop(['molecule_name'], axis=1,inplace=True)\ny_preds = gbm.predict(test)\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='id')\npredictions = sample_submission.copy()\npredictions['scalar_coupling_constant'] = y_preds[0]\npredictions.to_csv('submission.csv')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}