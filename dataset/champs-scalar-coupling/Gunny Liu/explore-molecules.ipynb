{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DATA_PATH = \"../input/\"\ntrain = pd.read_csv(DATA_PATH+\"train.csv\")\ncontributions = pd.read_csv(DATA_PATH+\"scalar_coupling_contributions.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_contribs = train.merge(contributions, on=list(train.columns)[1:4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert(train_contribs.shape[0] == train.shape[0])\nassert(contributions.shape[0] == train_contribs.shape[0])\nassert(train_contribs.shape[1] == train.shape[1] + contributions.shape[1] - 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_contribs.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calc cov between each individual comp and scc\nSAMPLING_RATE = 0.01\nsample_idx = np.random.choice(np.arange(train_contribs.shape[0]), size=int(train_contribs.shape[0]*SAMPLING_RATE))\ncov = np.cov(train_contribs[[\"scalar_coupling_constant\", \"fc\", \"sd\", \"dso\", \"pso\"]].iloc[sample_idx].values.transpose())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(cov)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = np.corrcoef(train_contribs[[\"scalar_coupling_constant\", \"fc\", \"sd\", \"dso\", \"pso\"]].iloc[sample_idx].values.transpose())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(corr, cmap=\"YlGnBu\")\n# fc, then dso best","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc = preprocessing.LabelEncoder()\nenc.fit_transform(train.type)\nsns.set(style=\"whitegrid\")\nsns.violinplot(x=\"type\", y=\"scalar_coupling_constant\", data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit a 1-NN based on sample mean across bond types\nSAMPLE_SIZE = 100\nnp.random.seed(2019)\ntype_sample_mean = {}\nfor t in train.type.unique():\n    sample_idx = np.random.choice(train.loc[train[\"type\"] == t].id, SAMPLE_SIZE)\n    type_sample_mean[t] = np.mean(train.iloc[sample_idx].scalar_coupling_constant)\nprint(\"Lookup dict for type-based sample mean is:\", type_sample_mean)\ntest = pd.read_csv(DATA_PATH+\"test.csv\")\ntest[\"scalar_coupling_constant\"] = test.type.apply(lambda x: type_sample_mean[x])\ntest[[\"id\", \"scalar_coupling_constant\"]].to_csv(\"tsm_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_contribs.type_x = enc.fit_transform(train_contribs.type_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# example uses 3 layers, 200 each for 500 data points. overfit. need 10e4x params each W to get same overfit on mine.\nNUNITS = 2000\nNCOVARIATES = 5\nCOVARIATES = [\"type_x\", \"fc\", \"sd\", \"dso\", \"pso\"]\nNLAYERS = 2\nNEPOCHS = 1\nP_SETOUT = 0.2\nSEED = 2019\nX, y = train_contribs[COVARIATES].values, train_contribs.scalar_coupling_constant.values\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=P_SETOUT, random_state=SEED)\nm = Sequential()\nm.add(Dense(NUNITS, activation='relu', input_shape=(NCOVARIATES,)))\nfor _ in range(NLAYERS):\n    m.add(Dense(NUNITS, activation='relu'))\nm.add(Dense(1))\nm.compile(optimizer='adam', loss='mean_squared_error')\nm.fit(Xtrain, ytrain, \n      validation_split=P_SETOUT, epochs=NEPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(m.metrics_names)\nprint(m.evaluate(Xtest, ytest, verbose=1))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}