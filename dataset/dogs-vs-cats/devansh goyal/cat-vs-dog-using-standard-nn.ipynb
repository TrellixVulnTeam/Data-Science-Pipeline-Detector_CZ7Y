{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport zipfile\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport cv2\nimport random\nimport os\nprint(os.listdir(\"../input/dogs-vs-cats\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"IMAGE_WIDTH=64\nIMAGE_HEIGHT=64\nIMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\nIMAGE_CHANNELS=3\nTRAIN_DIRECTORY=\"/kaggle/working/train/\"\nTEST_DIRECTORY=\"/kaggle/working/test1\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_files(source_path, target_path):\n    zip_ref = zipfile.ZipFile(source_path,'r')\n    zip_ref.extractall(target_path)\n    zip_ref.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extraction():\n    extract_files('/kaggle/input/dogs-vs-cats/test1.zip','/kaggle/working/')\n    extract_files('/kaggle/input/dogs-vs-cats/train.zip','/kaggle/working/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extraction()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_filenames(directory):\n    filenames=(os.listdir(directory))\n    return filenames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make input and output data\ndef load_data(filenames,directory):\n#     i=500   #for testing purpose\n    i=len(filenames)\n    X=[]\n    y=[]\n    for name in filenames:\n        img=mpimg.imread(os.path.join(directory,name))\n        X.append(cv2.resize(img,IMAGE_SIZE))\n        cat=name.split('.')[0]\n        if(cat=='dog'):\n            y.append(0)\n        else:\n            y.append(1)\n        i-=1\n        if(i<=0):\n            break\n    return X,y\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading train data\nfilenames=get_filenames(TRAIN_DIRECTORY)\nX,y=load_data(filenames,TRAIN_DIRECTORY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading test data\ntest_filenames=get_filenames(TEST_DIRECTORY)\nX_test,y_test=load_data(test_filenames,TEST_DIRECTORY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# showing a sample image\ndef show_image(filenames,directory):\n    sample = random.choice(filenames)\n    print(sample)\n    plt.imshow(mpimg.imread(directory+sample))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_image(filenames,TRAIN_DIRECTORY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making a image into 1D array\ndef refine_data(X,y):\n    X=np.array(X)\n    X=X.reshape(X.shape[0],-1)\n    X=X.T\n    X=X/255\n    y=np.array(y)\n    y=y.reshape((1,y.shape[0]))\n    return X,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X,y=refine_data(X,y)\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining layer dimensions of NN\nlayer_dims=[X.shape[0],500,100,50,25,1]\nlayer_dims","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining different methods used in NN\n\n# initialize parameters\ndef initialize_parameters(layer_dims):\n    np.random.seed(1)\n    parameters={}\n    L=len(layer_dims)\n    for l in range(1,L):\n        parameters['W'+str(l)]=np.random.randn(layer_dims[l],layer_dims[l-1])/ np.sqrt(layer_dims[l-1]/2)#*0.01\n        parameters['b'+str(l)]=np.zeros((layer_dims[l],1))\n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters=initialize_parameters(layer_dims)\nfor param in parameters:\n    print(param+\" : \"+str(parameters[param].shape))\nparameters['W1'][:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining linear forward function\ndef linear_fwd(A,W,b):\n    Z=np.dot(W,A)+b\n    cache=(A,W,b)\n    return Z,cache","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Z,cache=linear_fwd(X,parameters['W1'],parameters['b1'])\nZ.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining different activation function\ndef sigmoid(Z):\n    A=1/(1+np.exp(-Z))\n    cache=Z\n    return A,cache\ndef relu(Z):\n    A=np.maximum(Z,0)\n    cache=Z\n    return A,cache","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sigmoid(np.array([0,2]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relu(np.array([-50,50]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def linear activation function\ndef linear_fwd_activation(A_prev,W,b,activation):\n    Z,linear_cache=linear_fwd(A_prev,W,b)\n    if activation=='relu':\n        A,activation_cache=relu(Z)\n    elif activation=='sigmoid':\n        A,activation_cache=sigmoid(Z)\n    cache=(linear_cache,activation_cache)\n    return A,cache\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"A,cache=linear_fwd_activation(X,parameters['W1'],parameters['b1'],'relu')\nprint(A.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making forward function\ndef forward(X,parameters):\n    caches=[]\n    A=X\n    L=len(parameters)//2\n    for l in range(1,L):\n        A_prev=A\n        A,cache=linear_fwd_activation(A_prev,parameters['W'+str(l)],parameters['b'+str(l)],'relu')\n        caches.append(cache)\n    AL,cache=linear_fwd_activation(A,parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n    caches.append(cache)\n    assert(AL.shape == (1,X.shape[1]))\n    return AL,caches","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AL,cache=forward(X,parameters)\nprint(AL.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_cost(AL,y):\n    m=y.shape[1]\n    cost=-np.sum(np.dot(y,np.log(AL).T)+np.dot(1-y,np.log(1-AL).T))/m\n    cost=np.squeeze(cost)\n    assert(cost.shape == ())\n    return cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compute_cost(AL,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making methods of backward propogation\ndef linear_backward(dz,cache):\n    A_prev,W,b=cache\n    m=A_prev.shape[1]\n    dA_prev=np.dot(W.T,dz)\n    dW=np.dot(dz,A_prev.T)/m\n    db=np.sum(dz,keepdims=True,axis=1)/m\n    return dA_prev,dW,db","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def relu_backward(dA,activation_cache):\n    Z=activation_cache\n    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n    \n    # When z <= 0, you should set dz to 0 as well. \n    dZ[Z <= 0] = 0\n    \n    \n    return dZ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid_backward(dA, cache):\n    Z = cache\n    s = 1/(1+np.exp(-Z))\n    dZ = dA * s * (1-s)\n    return dZ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef linear_activation_backward(dA, cache, activation):\n    linear_cache, activation_cache = cache\n    \n    if activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n        \n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    \n    return dA_prev, dW, db","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# backward propogation\ndef backward(AL,y,caches):\n    grads={}\n    L=len(caches)\n    m = AL.shape[1]\n    y = y.reshape(AL.shape)\n    dAL=-np.divide(y,AL)+np.divide(1-y,1-AL)\n    current_cache=caches[L-1]\n    grads['dA'+str(L-1)],grads['dW'+str(L)],grads['db'+str(L)]=linear_activation_backward(dAL,current_cache,'sigmoid')\n    for i in reversed(range(L-1)):\n        grads['dA'+str(i)],grads['dW'+str(i+1)],grads['db'+str(i+1)]=linear_activation_backward(grads['dA'+str(i+1)],caches[i],'relu')\n    return grads","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initial_adam_optimisation(parameters):\n    L=len(parameters)//2\n    optimise={}\n    for i in range(1,L+1):\n        optimise['Vdw'+str(i)]=np.zeros_like(parameters['W'+str(i)])\n        optimise['Vdb'+str(i)]=np.zeros_like(parameters['b'+str(i)])\n        optimise['Sdw'+str(i)]=np.zeros_like(parameters['W'+str(i)])\n        optimise['Sdb'+str(i)]=np.zeros_like(parameters['b'+str(i)])\n    return optimise","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to update parameters\ndef update_parameters(parameters,grads,learning_rate,optimise,beta1,beta2,epsilon):\n#     print(learning_rate)\n#     print(beta1,beta2)\n    L=len(parameters)//2\n#     print(parameters[\"W\" + str(L)].shape,grads[\"dW\" + str(L)].shape)\n    for i in range(1,L+1):\n        optimise['Vdw'+str(i)]=beta1*optimise['Vdw'+str(i)]+(1-beta1)*grads['dW'+str(i)]\n        optimise['Vdb'+str(i)]=beta1*optimise['Vdb'+str(i)]+(1-beta1)*grads['db'+str(i)]\n        optimise['Sdw'+str(i)]=beta2*optimise['Sdw'+str(i)]+(1-beta2)*np.power(grads['dW'+str(i)],2)\n        optimise['Sdb'+str(i)]=beta2*optimise['Sdb'+str(i)]+(1-beta2)*np.power(grads['db'+str(i)],2)\n#         print(np.sqrt(optimise['Sdw'+str(i)]+epsilon))\n#         if(np.sqrt(optimise['Sdw'+str(i)]+epsilon).any==0):\n#             print('hlo')\n        print('Sdw'+str(i))\n        print(np.sqrt(optimise['Sdw'+str(i)]+epsilon))\n        print('dw'+str(i))\n        print(grads['dW'+str(i)])\n#         print('Hlo')\n#         parameters['W'+str(i)]-=learning_rate*optimise['Vdw'+str(i)]/np.sqrt(optimise['Sdw'+str(i)]+epsilon)\n#         parameters['b'+str(i)]-=learning_rate*optimise['Vdb'+str(i)]#/np.sqrt(optimise['Sdb'+str(i)]+epsilon)\n        parameters['W'+str(i)]-=learning_rate*grads['dW'+str(i)]\n        parameters['b'+str(i)]-=learning_rate*grads['db'+str(i)]\n    return parameters,optimise","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# code for mini batches\ndef random_mini_batch(X,Y,batch_size=512):\n    batches=[]\n    m=X.shape[1]\n    num_batch=m//batch_size\n#     shuffling x and y\n    permutation=list(np.random.permutation(m))\n    shuffeled_X=X[:,permutation]\n    shuffeled_Y=Y[:,permutation].reshape((1,m))\n    for i in range(0,num_batch):\n        mini_X=X[:,i*batch_size:(i+1)*batch_size]\n        mini_Y=Y[:,i*batch_size:(i+1)*batch_size]\n        batches.append((mini_X,mini_Y))\n    i=num_batch\n    rest_X=X[:,i*batch_size:(i+1)*batch_size]\n    rest_Y=Y[:,i*batch_size:(i+1)*batch_size]\n    batches.append((rest_X,rest_Y))\n    return batches","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now that all the helper function are made,lets make our model\ndef model(X,Y,layer_dims,learning_rate=0.0075,beta1=0.9,beta2=0.999,epsilon=1e-8,num_iterations=3000,print_cost=False):\n    np.random.seed(1)\n    print(layer_dims)\n    costs=[]\n    m=X.shape[1]\n    parameters=initialize_parameters(layer_dims)\n    optimise=initial_adam_optimisation(parameters)\n    mini_batches=random_mini_batch(X,Y,batch_size=512)\n    for i in range(0,num_iterations):\n        cost=0.\n        for mini_X,mini_Y in mini_batches:\n            AL,caches=forward(mini_X,parameters)\n            cost+=compute_cost(AL,mini_Y)\n            grads=backward(AL,mini_Y,caches)\n            parameters,optimise=update_parameters(parameters,grads,learning_rate,optimise,beta1,beta2,epsilon)\n        avg_cost=cost/len(mini_batches)\n        costs.append(avg_cost)\n        if i%10==0 and print_cost==True:\n            print (\"Cost after iteration %i: %f\" %(i, avg_cost))\n    plt.plot(range(num_iterations),costs)\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    print(avg_cost)\n    plt.show()\n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"layer_dims=[X.shape[0],2,1]\nparameters=model(X,y,layer_dims,learning_rate=0.0075,num_iterations=1,print_cost=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lr=0.0055\n# while(lr<=0.01):\n#     parameters=model(X,y,layer_dims,learning_rate=lr,num_iterations=2500,print_cost=False)\n#     lr+=0.0005","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef predict(X, y, parameters):\n    m = X.shape[1]\n    n = len(parameters) // 2 # number of layers in the neural network\n    p = np.zeros((1,m))\n    \n    # Forward propagation\n    probas, caches =forward(X, parameters)\n\n    \n    # convert probabilities to 0/1 predictions\n    for i in range(0, probas.shape[1]):\n        if probas[0,i] > 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n    \n    #print results\n#     print (\"predictions: \" + str(p))\n#     print (\"true labels: \" + str(y))\n    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n        \n    return p\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p=predict(X,y,parameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_test,y_test=refine_data(X_test,y_test)\nprint(X_test.shape)\nprint(y_test.shape)\np_test=predict(X_test,y_test,parameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}