{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#This kernel aims to build a CNN model to solve the Kaggle Dogs vs. Cats image classification problem","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%xmode Plain\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns;sns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import RMSprop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def result_visualisation(loss,acc,val_loss,val_acc):\n    #Result visualisation\n    epochs = np.arange(1,len(loss)+1)\n    fig,ax = plt.subplots(1,2,figsize=(20,5))\n    ax[0].plot(epochs,loss,label='loss')\n    ax[0].plot(epochs,val_loss,label='val_loss')\n    ax[0].set_title('Loss')\n    ax[0].set_xlabel('Epochs')\n    ax[0].legend(['loss','val_loss'])\n    ax[1].plot(epochs,acc,label='acc')\n    ax[1].plot(epochs,val_acc,label='val_acc')\n    ax[1].set_title('Accuracy')\n    ax[1].set_xlabel('Epochs')\n    ax[1].legend(['acc','val_acc'])\n    plt.tight_layout()\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_val_acc_comparison(val_acc_list,smooth=False):\n    legend = []\n    if smooth:\n        smooth_list = []\n        for i,val_acc in enumerate(val_acc_list):\n            smooth_list.append(smoothing(val_acc))\n        data_list = smooth_list\n    else:\n        data_list = val_acc_list\n    for i,val_acc in enumerate(data_list):\n        plt.plot(val_acc)\n        legend.append('val_acc'+str(i+1))\n    plt.title('Validation Accuracy across all networks')\n    plt.xlabel('Epochs')\n    plt.legend(legend)\n    plt.tight_layout()\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_val_loss_comparison(val_loss_list,smooth=False):\n    legend = []\n    if smooth:\n        smooth_list = []\n        for i,val_loss in enumerate(val_loss_list):\n            smooth_list.append(smoothing(val_loss))\n        data_list = smooth_list\n    else:\n        data_list = val_loss_list\n    for i,val_loss in enumerate(data_list):\n        plt.plot(val_loss)\n        legend.append('val_loss'+str(i+1))\n    plt.title('Validation Loss across all networks')\n    plt.xlabel('Epochs')\n    plt.legend(legend)\n    plt.tight_layout()\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def smoothing(data,factor=0.4):\n    smooth = []\n    for item in data:\n        if smooth:\n            smooth.append(factor*smooth[-1]+(1-factor)*item)\n        else:\n            smooth.append(item)\n    return smooth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Directories of data\ntrain_dir = '../input/dogs-vs-cats/train/train'\ntest_dir = '../input/dogs-vs-cats/test1/test1'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create labels for the images\nfilenames = os.listdir(train_dir)\nlabels = []\nfor filename in filenames:\n    if filename.startswith('dog'):\n        labels.append(1)\n    else:\n        labels.append(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import the filenames and labels into a dataframe\n#Only 5000 images would be used for a shorter training time\ndf_train = pd.DataFrame({'Filenames':filenames,'Labels':labels})\n#Stratified sampling\ndf_train = df_train.groupby('Labels').apply(lambda x:x.sample(frac=0.2,random_state=100))\n#Read data from the directory\nX_filenames = df_train['Filenames'].values\ny = df_train['Labels'].values\nX = [image.load_img(os.path.join(train_dir,filename),target_size=(150,150)) for filename in X_filenames]\nX = np.array([image.img_to_array(item) for item in X])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Standardization of training data\nX = X/255.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the data into train and val set\nXtrain,Xval,ytrain,yval = train_test_split(X,y,stratify=y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Constrcut the CNN model\nCNN1 = Sequential()\nCNN1.add(Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)))\nCNN1.add(Conv2D(32,(3,3),activation='relu'))\nCNN1.add(MaxPooling2D((2,2)))\nCNN1.add(Conv2D(64,(3,3),activation='relu'))\nCNN1.add(Conv2D(64,(3,3),activation='relu'))\nCNN1.add(MaxPooling2D((2,2)))\nCNN1.add(Flatten())\nCNN1.add(Dense(512,activation='relu'))\nCNN1.add(Dense(1,activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define the optimizer and compile the CNN\noptimizer = RMSprop(lr=1e-4)\nCNN1.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history1 = CNN1.fit(Xtrain,ytrain,validation_data=(Xval,yval),batch_size=128,epochs=40,verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extract loss and acc for visualisation\nloss1 = history1.history['loss']\nacc1 = history1.history['acc']\nval_loss1= history1.history['val_loss']\nval_acc1 = history1.history['val_acc']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Result visualisation\nresult_visualisation(loss1,acc1,val_loss1,val_acc1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Overfitting problem is quite prominent\n#Try  to introduce a dropout layer to improve the model performance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Constrcut the CNN model with Dropout Layer\nCNN2 = Sequential()\nCNN2.add(Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)))\nCNN2.add(MaxPooling2D((2,2)))\nCNN2.add(Conv2D(32,(3,3),activation='relu'))\nCNN2.add(MaxPooling2D((2,2)))\nCNN2.add(Conv2D(64,(3,3),activation='relu'))\nCNN2.add(MaxPooling2D((2,2)))\nCNN2.add(Conv2D(128,(3,3),activation='relu'))\nCNN2.add(MaxPooling2D((2,2)))\nCNN2.add(Flatten())\nCNN2.add(Dropout(0.5))\nCNN2.add(Dense(512,activation='relu'))\nCNN2.add(Dense(1,activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CNN2.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['acc'])\nhistory2 = CNN2.fit(Xtrain,ytrain,validation_data=(Xval,yval),batch_size=128,epochs=100,verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extract loss and acc for visualisation\nloss2 = history2.history['loss']\nacc2 = history2.history['acc']\nval_loss2= history2.history['val_loss']\nval_acc2 = history2.history['val_acc']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Result visualisation\nresult_visualisation(loss2,acc2,val_loss2,val_acc2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#After introducing the dropout layer, the validation accuracy has improved\n#In the following, we are going to introduce data augmentation and see if this further improve the performance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Augmentation\nimggen = ImageDataGenerator(rotation_range=30,width_shift_range=0.2,height_shift_range=0.2,\n                            shear_range=0.2,horizontal_flip=True,fill_mode='nearest')\ntrain_gen = imggen.flow(Xtrain,ytrain,batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Constrcut the CNN model with Dropout Layer\nCNN3 = Sequential()\nCNN3.add(Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)))\nCNN3.add(MaxPooling2D((2,2)))\nCNN3.add(Conv2D(32,(3,3),activation='relu'))\nCNN3.add(MaxPooling2D((2,2)))\nCNN3.add(Conv2D(64,(3,3),activation='relu'))\nCNN3.add(MaxPooling2D((2,2)))\nCNN3.add(Conv2D(128,(3,3),activation='relu'))\nCNN3.add(MaxPooling2D((2,2)))\nCNN3.add(Flatten())\nCNN3.add(Dropout(0.5))\nCNN3.add(Dense(512,activation='relu'))\nCNN3.add(Dense(1,activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CNN3.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['acc'])\nhistory3 = CNN3.fit_generator(train_gen,steps_per_epoch=80,validation_data=(Xval,yval),epochs=100,verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extract loss and acc for visualisation\nloss3 = history3.history['loss']\nacc3 = history3.history['acc']\nval_loss3= history3.history['val_loss']\nval_acc3 = history3.history['val_acc']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Result visualisation\nresult_visualisation(loss3,acc3,val_loss3,val_acc3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compare the validation accuracy across all the networks\nval_acc_list = [val_acc1,val_acc2,val_acc3]\nplot_val_acc_comparison(val_acc_list,smooth=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compare the validation loss across all the networks\nval_loss_list = [val_loss1,val_loss2,val_loss3]\nplot_val_loss_comparison(val_loss_list,smooth=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#By introducing data augmentation, the performance of the network on the validation set has greatly improved\n#If the full training dataset has been used, it is believed that the performance of the network would be even better","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}