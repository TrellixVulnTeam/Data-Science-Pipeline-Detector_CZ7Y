{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PART 1: Using randomly generated Data\n\n### Walkthrough for code BY ORIGINAL AUTHOR found below.\n\nhttps://towardsdatascience.com/a-practical-guide-to-tfrecords-584536bc786c\n\n### The author also implemented this code in Colabs. Link found here:\n\nhttps://colab.research.google.com/drive/1xU_MJ3R8oj8YYYi-VI_WJTU3hD1OpAB7?usp=sharing\n\n### Original Author using it in one of their projects found here:\nhttps://towardsdatascience.com/custom-audio-classification-with-tensorflow-af8c16c38689","metadata":{"execution":{"iopub.status.busy":"2022-04-14T01:27:52.519352Z","iopub.execute_input":"2022-04-14T01:27:52.520126Z","iopub.status.idle":"2022-04-14T01:27:52.523936Z","shell.execute_reply.started":"2022-04-14T01:27:52.520073Z","shell.execute_reply":"2022-04-14T01:27:52.522952Z"}}},{"cell_type":"markdown","source":"# How to do a single tfrec using images","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-14T03:22:50.419563Z","iopub.execute_input":"2022-04-14T03:22:50.420217Z","iopub.status.idle":"2022-04-14T03:22:50.42869Z","shell.execute_reply.started":"2022-04-14T03:22:50.420176Z","shell.execute_reply":"2022-04-14T03:22:50.427954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _bytes_feature(value):\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))): # if value ist tensor\n        value = value.numpy() # get value of tensor\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _float_feature(value):\n  \"\"\"Returns a floast_list from a float / double.\"\"\"\n  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\ndef _int64_feature(value):\n  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef serialize_array(array):\n  array = tf.io.serialize_tensor(array)\n  return array","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:22:50.447786Z","iopub.execute_input":"2022-04-14T03:22:50.448918Z","iopub.status.idle":"2022-04-14T03:22:50.456226Z","shell.execute_reply.started":"2022-04-14T03:22:50.448848Z","shell.execute_reply":"2022-04-14T03:22:50.455113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimage_small_shape = (250,250,3)\nnumber_of_images_small = 100\n\nimages_small = np.random.randint(low=0, high=256, size=(number_of_images_small, *image_small_shape), dtype=np.int16)\nprint(images_small.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:22:50.48153Z","iopub.execute_input":"2022-04-14T03:22:50.482015Z","iopub.status.idle":"2022-04-14T03:22:50.57426Z","shell.execute_reply.started":"2022-04-14T03:22:50.481974Z","shell.execute_reply":"2022-04-14T03:22:50.573064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_small = np.random.randint(low=0, high=5, size=(number_of_images_small, 1))\nprint(labels_small.shape)\nprint(labels_small[:10])","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:22:50.576282Z","iopub.execute_input":"2022-04-14T03:22:50.576573Z","iopub.status.idle":"2022-04-14T03:22:50.5841Z","shell.execute_reply.started":"2022-04-14T03:22:50.576539Z","shell.execute_reply":"2022-04-14T03:22:50.582839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_single_image(image, label):\n  \n  #define the dictionary -- the structure -- of our single example\n  data = {\n        'height' : _int64_feature(image.shape[0]),\n        'width' : _int64_feature(image.shape[1]),\n        'depth' : _int64_feature(image.shape[2]),\n        'raw_image' : _bytes_feature(serialize_array(image)),\n        'label' : _int64_feature(label)\n    }\n  #create an Example, wrapping the single features\n  out = tf.train.Example(features=tf.train.Features(feature=data))\n\n  return out","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:22:50.585756Z","iopub.execute_input":"2022-04-14T03:22:50.586052Z","iopub.status.idle":"2022-04-14T03:22:50.598076Z","shell.execute_reply.started":"2022-04-14T03:22:50.586018Z","shell.execute_reply":"2022-04-14T03:22:50.596901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_images_to_tfr_short(images, labels, filename:str=\"images\"):\n  filename= filename+\".tfrecords\"\n  writer = tf.io.TFRecordWriter(filename) #create a writer that'll store our data to disk\n  count = 0\n\n  for index in range(len(images)):\n\n    #get the data we want to write\n    current_image = images[index] \n    current_label = labels[index]\n\n    out = parse_single_image(image=current_image, label=current_label)\n    writer.write(out.SerializeToString())\n    count += 1\n\n  writer.close()\n  print(f\"Wrote {count} elements to TFRecord\")\n  return count","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:22:50.60008Z","iopub.execute_input":"2022-04-14T03:22:50.600463Z","iopub.status.idle":"2022-04-14T03:22:50.618257Z","shell.execute_reply.started":"2022-04-14T03:22:50.60043Z","shell.execute_reply":"2022-04-14T03:22:50.6171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncount = write_images_to_tfr_short(images_small, labels_small, filename=\"small_images\")","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:22:50.620085Z","iopub.execute_input":"2022-04-14T03:22:50.620573Z","iopub.status.idle":"2022-04-14T03:22:50.780912Z","shell.execute_reply.started":"2022-04-14T03:22:50.620538Z","shell.execute_reply":"2022-04-14T03:22:50.780136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_tfr_element(element):\n  #use the same structure as above; it's kinda an outline of the structure we now want to create\n  data = {\n      'height': tf.io.FixedLenFeature([], tf.int64),\n      'width':tf.io.FixedLenFeature([], tf.int64),\n      'label':tf.io.FixedLenFeature([], tf.int64),\n      'raw_image' : tf.io.FixedLenFeature([], tf.string),\n      'depth':tf.io.FixedLenFeature([], tf.int64),\n    }\n\n    \n  content = tf.io.parse_single_example(element, data)\n  \n  height = content['height']\n  width = content['width']\n  depth = content['depth']\n  label = content['label']\n  raw_image = content['raw_image']\n  \n  \n  #get our 'feature'-- our image -- and reshape it appropriately\n  feature = tf.io.parse_tensor(raw_image, out_type=tf.int16)\n  feature = tf.reshape(feature, shape=[height,width,depth])\n  return (feature, label)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:22:50.782544Z","iopub.execute_input":"2022-04-14T03:22:50.783031Z","iopub.status.idle":"2022-04-14T03:22:50.793358Z","shell.execute_reply.started":"2022-04-14T03:22:50.782988Z","shell.execute_reply":"2022-04-14T03:22:50.792263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef get_dataset_small(filename):\n  #create the dataset\n  dataset = tf.data.TFRecordDataset(filename)\n\n  #pass every single feature through our mapping function\n  dataset = dataset.map(\n      parse_tfr_element\n  )\n    \n  return dataset","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:22:50.794727Z","iopub.execute_input":"2022-04-14T03:22:50.795269Z","iopub.status.idle":"2022-04-14T03:22:50.807291Z","shell.execute_reply.started":"2022-04-14T03:22:50.795228Z","shell.execute_reply":"2022-04-14T03:22:50.8063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_small = get_dataset_small(\"./small_images.tfrecords\")\n\nfor sample in dataset_small.take(1):\n  print(sample[0].shape)\n  print(sample[1].shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:22:50.809597Z","iopub.execute_input":"2022-04-14T03:22:50.810515Z","iopub.status.idle":"2022-04-14T03:22:51.002034Z","shell.execute_reply.started":"2022-04-14T03:22:50.810459Z","shell.execute_reply":"2022-04-14T03:22:51.000796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now using a larger image datset it is then sharded into multiple files","metadata":{}},{"cell_type":"code","source":"image_large_shape = (400,750,3)\nnumber_of_images_large = 500 #constraining to 500 files here, to not outgrow RAM capacities\n\nimages_large = np.random.randint(low=0, high=256, size=(number_of_images_large, *image_large_shape), dtype=np.int16)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:22:51.003553Z","iopub.execute_input":"2022-04-14T03:22:51.003939Z","iopub.status.idle":"2022-04-14T03:22:53.068077Z","shell.execute_reply.started":"2022-04-14T03:22:51.003899Z","shell.execute_reply":"2022-04-14T03:22:53.067264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlabels_large = np.random.randint(low=0, high=5, size=(number_of_images_large, 1))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:22:53.069286Z","iopub.execute_input":"2022-04-14T03:22:53.069542Z","iopub.status.idle":"2022-04-14T03:22:53.073835Z","shell.execute_reply.started":"2022-04-14T03:22:53.069506Z","shell.execute_reply":"2022-04-14T03:22:53.073218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tqdm\ndef write_images_to_tfr_long(images, labels, filename:str=\"large_images\", max_files:int=10, out_dir:str=\"./\"):\n\n    #determine the number of shards (single TFRecord files) we need:\n    splits = (len(images)//max_files) + 1 #determine how many tfr shards are needed\n    if len(images)%max_files == 0:\n        splits-=1\n    print(f\"\\nUsing {splits} shard(s) for {len(images)} files, with up to {max_files} samples per shard\")\n\n    file_count = 0\n    for i in tqdm.tqdm(range(splits)):\n        current_shard_name = \"{}{}_{}{}.tfrecords\".format(out_dir, i+1, splits, filename)\n        writer = tf.io.TFRecordWriter(current_shard_name)\n\n        current_shard_count = 0\n        while current_shard_count < max_files: #as long as our shard is not full\n            #get the index of the file that we want to parse now\n            index = i*max_files+current_shard_count\n            if index == len(images): #when we have consumed the whole data, preempt generation\n                break\n            current_image = images[index]\n            current_label = labels[index]\n\n            #create the required Example representation\n            out = parse_single_image(image=current_image, label=current_label)\n\n            writer.write(out.SerializeToString())\n            current_shard_count+=1\n            file_count += 1\n\n        writer.close()\n    print(f\"\\nWrote {file_count} elements to TFRecord\")\n    return file_count","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:22:53.075553Z","iopub.execute_input":"2022-04-14T03:22:53.076208Z","iopub.status.idle":"2022-04-14T03:22:53.087147Z","shell.execute_reply.started":"2022-04-14T03:22:53.076172Z","shell.execute_reply":"2022-04-14T03:22:53.08615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write_images_to_tfr_long(images_large, labels_large, max_files=30)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:22:53.088709Z","iopub.execute_input":"2022-04-14T03:22:53.089025Z","iopub.status.idle":"2022-04-14T03:22:58.854429Z","shell.execute_reply.started":"2022-04-14T03:22:53.088993Z","shell.execute_reply":"2022-04-14T03:22:58.853377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataset_large(tfr_dir:str=\"./\", pattern:str=\"*large_images.tfrecords\"):\n    files = glob.glob(tfr_dir+pattern, recursive=False)\n\n    #create the dataset\n    dataset = tf.data.TFRecordDataset(files)\n\n    #pass every single feature through our mapping function\n    dataset = dataset.map(\n        parse_tfr_element\n    )\n    \n    return dataset\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:22:58.856129Z","iopub.execute_input":"2022-04-14T03:22:58.856494Z","iopub.status.idle":"2022-04-14T03:22:58.86353Z","shell.execute_reply.started":"2022-04-14T03:22:58.856449Z","shell.execute_reply":"2022-04-14T03:22:58.862409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\ndataset_large = get_dataset_large()\n\nfor sample in dataset_large.take(1):\n  print(sample[0].shape)\n  print(sample[1].shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:22:58.865179Z","iopub.execute_input":"2022-04-14T03:22:58.865516Z","iopub.status.idle":"2022-04-14T03:22:59.302517Z","shell.execute_reply.started":"2022-04-14T03:22:58.865473Z","shell.execute_reply":"2022-04-14T03:22:59.301799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now we make a tfrec using Audiofiles","metadata":{}},{"cell_type":"code","source":"import librosa\n\ndef create_dummy_audio_dataset():\n    files = []\n    labels = []\n\n    for i in range(100):\n        if i %2==0:\n            filename = librosa.ex('fishin')\n            labels.append(0)\n        if i %3==0:\n            filename = librosa.ex('brahms')\n            labels.append(1)\n        if i %5==0:\n            filename = librosa.ex('nutcracker')\n            labels.append(2)\n        if i %7==0:\n            filename = librosa.ex('trumpet')\n            labels.append(3)\n        else:\n            filename = librosa.ex('vibeace')\n            labels.append(4)\n\n        #The audio samples are of different length. But that's not of concern, TFRecords naturally support this case.\n        y, sr = librosa.load(filename)\n        files.append([y, sr])\n\n    return files, labels\n\n#get the audio dataset\naudios, labels = create_dummy_audio_dataset()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:22:59.303981Z","iopub.execute_input":"2022-04-14T03:22:59.304325Z","iopub.status.idle":"2022-04-14T03:23:03.786212Z","shell.execute_reply.started":"2022-04-14T03:22:59.304296Z","shell.execute_reply":"2022-04-14T03:23:03.785224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_single_audio_file(audio, label):\n\n    data = {\n    'sr' : _int64_feature(audio[1]),\n    'len' : _int64_feature(len(audio[0])),\n    'y' : _bytes_feature(serialize_array(audio[0])),\n    'label' : _int64_feature(label)\n    }\n\n    out = tf.train.Example(features=tf.train.Features(feature=data))\n\n    return out","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:03.787649Z","iopub.execute_input":"2022-04-14T03:23:03.787998Z","iopub.status.idle":"2022-04-14T03:23:03.793913Z","shell.execute_reply.started":"2022-04-14T03:23:03.787919Z","shell.execute_reply":"2022-04-14T03:23:03.793234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_audio_to_tfr(audios, labels, filename:str=\"audio\"):\n    filename= filename+\".tfrecords\"\n    writer = tf.io.TFRecordWriter(filename) #create a writer that'll store our audio data to disk\n    count = 0\n\n    for index in range(len(audios)):\n\n        #get the data we want to write\n        current_audio = audios[index] \n        current_label = labels[index]\n\n        #get a singe Example object\n        out = parse_single_audio_file(audio=current_audio, label=current_label)\n        #write the single Example to disk\n        writer.write(out.SerializeToString())\n        count += 1\n\n    writer.close()\n    print(f\"Wrote {count} elements to TFRecord\")\n    return count","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:03.795024Z","iopub.execute_input":"2022-04-14T03:23:03.795342Z","iopub.status.idle":"2022-04-14T03:23:03.806733Z","shell.execute_reply.started":"2022-04-14T03:23:03.795315Z","shell.execute_reply":"2022-04-14T03:23:03.806051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write_audio_to_tfr(audios, labels)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:03.80818Z","iopub.execute_input":"2022-04-14T03:23:03.808506Z","iopub.status.idle":"2022-04-14T03:23:06.035365Z","shell.execute_reply.started":"2022-04-14T03:23:03.80846Z","shell.execute_reply":"2022-04-14T03:23:06.0347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_tfr_audio_element(element):\n\n    #use the same structure as before as a placeholder\n    data = {\n      'sr': tf.io.FixedLenFeature([], tf.int64),\n      'len':tf.io.FixedLenFeature([], tf.int64),\n      'y' : tf.io.FixedLenFeature([], tf.string),\n      'label':tf.io.FixedLenFeature([], tf.int64),\n\n    }\n\n    content = tf.io.parse_single_example(element, data)\n\n    sr = content['sr']\n    len = content['len']\n    y = content['y']\n    label = content['label']\n\n\n    #get our 'feature'-- our audio file -- and reshape it appropriately\n    feature = tf.io.parse_tensor(y, out_type=tf.float32) #note that we change the data type to float32\n    feature = tf.reshape(feature, shape=[len])\n\n    return (feature, label)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:06.038361Z","iopub.execute_input":"2022-04-14T03:23:06.038867Z","iopub.status.idle":"2022-04-14T03:23:06.045644Z","shell.execute_reply.started":"2022-04-14T03:23:06.038818Z","shell.execute_reply":"2022-04-14T03:23:06.044947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef get_audio_dataset(filename):\n\n    #create the dataset\n    dataset = tf.data.TFRecordDataset(filename)\n\n    #pass every single Example through our audio parsing function\n    dataset = dataset.map(\n      parse_tfr_audio_element\n    )\n\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:06.0471Z","iopub.execute_input":"2022-04-14T03:23:06.047354Z","iopub.status.idle":"2022-04-14T03:23:06.081142Z","shell.execute_reply.started":"2022-04-14T03:23:06.047325Z","shell.execute_reply":"2022-04-14T03:23:06.08035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_audio = get_audio_dataset(\"./audio.tfrecords\")\n\nfor sample in dataset_audio.take(1):\n  print(sample[0].shape) #the audio data\n  print(sample[1]) #the label","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:06.08307Z","iopub.execute_input":"2022-04-14T03:23:06.083522Z","iopub.status.idle":"2022-04-14T03:23:06.19353Z","shell.execute_reply.started":"2022-04-14T03:23:06.083474Z","shell.execute_reply":"2022-04-14T03:23:06.191924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now this is how to do it with Text Data!","metadata":{}},{"cell_type":"code","source":"def create_dummy_text_dataset(size:int=100):\n    text_data = []\n    labels = []\n\n    for i in range(size):\n        if i % 2 == 0:\n            text = \"Hey, this is a sample text. We can use many different symbols.\"\n            label = 0\n        else:\n            text = \"A point is exactly what the folks think of it; after Gauss.\"\n            label = 1\n        text_data.append(text)\n        labels.append(label)\n\n    return text_data, labels","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:06.195238Z","iopub.execute_input":"2022-04-14T03:23:06.195484Z","iopub.status.idle":"2022-04-14T03:23:06.200551Z","shell.execute_reply.started":"2022-04-14T03:23:06.195454Z","shell.execute_reply":"2022-04-14T03:23:06.199872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text, labels = create_dummy_text_dataset()\ntext[:5]","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:06.201689Z","iopub.execute_input":"2022-04-14T03:23:06.202372Z","iopub.status.idle":"2022-04-14T03:23:06.216377Z","shell.execute_reply.started":"2022-04-14T03:23:06.202336Z","shell.execute_reply":"2022-04-14T03:23:06.215719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_single_text_data(text, label):\n    data = {\n        'text' : _bytes_feature(serialize_array(text)),\n        'label' : _int64_feature(label)\n    }\n\n    out = tf.train.Example(features=tf.train.Features(feature=data))\n\n    return out","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:06.217627Z","iopub.execute_input":"2022-04-14T03:23:06.218433Z","iopub.status.idle":"2022-04-14T03:23:06.227276Z","shell.execute_reply.started":"2022-04-14T03:23:06.218383Z","shell.execute_reply":"2022-04-14T03:23:06.226529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef write_text_to_tfr(text_data, labels, filename:str=\"text\"):\n    filename= filename+\".tfrecords\"\n    writer = tf.io.TFRecordWriter(filename) #create a writer that'll store our text data to disk\n    count = 0\n\n    for index in range(len(text_data)):\n\n        #get the data we want to write\n        current_text = text_data[index] \n        current_label = labels[index]\n\n        #define the dictionary -- the structure -- of our single example\n        out = parse_single_text_data(text=current_text, label=current_label)\n        writer.write(out.SerializeToString())\n        count += 1\n\n    writer.close()\n    print(f\"Wrote {count} elements to TFRecord\")\n    return count","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:06.228639Z","iopub.execute_input":"2022-04-14T03:23:06.229441Z","iopub.status.idle":"2022-04-14T03:23:06.241019Z","shell.execute_reply.started":"2022-04-14T03:23:06.229394Z","shell.execute_reply":"2022-04-14T03:23:06.240046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write_text_to_tfr(text_data=text, labels=labels)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:06.242725Z","iopub.execute_input":"2022-04-14T03:23:06.243841Z","iopub.status.idle":"2022-04-14T03:23:06.276459Z","shell.execute_reply.started":"2022-04-14T03:23:06.243793Z","shell.execute_reply":"2022-04-14T03:23:06.275544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_tfr_text_element(element):\n    #use the same structure as above; it's kinda an outline of the structure we now want to create\n    data = {\n      'text' : tf.io.FixedLenFeature([], tf.string),\n      'label':tf.io.FixedLenFeature([], tf.int64),\n\n    }\n\n    content = tf.io.parse_single_example(element, data)\n\n    text = content['text']\n    label = content['label']\n\n    #get our 'feature', our text data\n    feature = tf.io.parse_tensor(text, out_type=tf.string)\n\n    return (feature, label)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:06.278321Z","iopub.execute_input":"2022-04-14T03:23:06.278948Z","iopub.status.idle":"2022-04-14T03:23:06.286792Z","shell.execute_reply.started":"2022-04-14T03:23:06.2789Z","shell.execute_reply":"2022-04-14T03:23:06.2858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_text_dataset(filename):\n    #create the dataset\n    dataset = tf.data.TFRecordDataset(filename)\n\n    #pass every single feature through our mapping function\n    dataset = dataset.map(\n      parse_tfr_text_element\n    )\n\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:06.288236Z","iopub.execute_input":"2022-04-14T03:23:06.288489Z","iopub.status.idle":"2022-04-14T03:23:06.300936Z","shell.execute_reply.started":"2022-04-14T03:23:06.28845Z","shell.execute_reply":"2022-04-14T03:23:06.299808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_dataset = get_text_dataset(\"./text.tfrecords\")\n\nfor sample in text_dataset.take(2):\n    print(sample[0].numpy()) #the text data\n    print(sample[1]) #the label","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:06.302138Z","iopub.execute_input":"2022-04-14T03:23:06.302378Z","iopub.status.idle":"2022-04-14T03:23:06.359275Z","shell.execute_reply.started":"2022-04-14T03:23:06.302349Z","shell.execute_reply":"2022-04-14T03:23:06.358345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now for multiple data types","metadata":{}},{"cell_type":"code","source":"images_shape = (256, 256, 3)\nsize = 100\nimages_combined = np.random.randint(low=0, high=256, size=(100, *images_shape), dtype=np.int16)\nprint(images_combined.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:06.360691Z","iopub.execute_input":"2022-04-14T03:23:06.361441Z","iopub.status.idle":"2022-04-14T03:23:06.457951Z","shell.execute_reply.started":"2022-04-14T03:23:06.361407Z","shell.execute_reply":"2022-04-14T03:23:06.456921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dummy_text_dataset_combined(size:int=100):\n    text_data = []\n    labels = []\n\n    for i in range(size):\n        if i %2==0:\n            text = \"This image shows a wooden bridge. It connects South Darmian with the norther parts of Frenklund.\"\n            label = 0\n        if i %3==0:\n            text = \"This image shows a sun flower. It's leaves are green, the petals are of strong yellow\"\n            label = 1\n        if i %5==0:\n            text = \"This image shows five children playing in the sandbox. They are laughing\"\n            label = 2\n        if i %7==0:\n            text = \"This image shows a house on a cliff. The house is painted in red and brown tones.\"\n            label = 3\n        else:\n            text = \"This image shows a horse and a zebra. They come from a CycleGAN.\"\n            label = 4\n\n        text_data.append(text)\n        labels.append(label)\n\n    return text_data, labels\n\n#get the text dataset and the labels\ntext, text_labels = create_dummy_text_dataset_combined()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:06.459179Z","iopub.execute_input":"2022-04-14T03:23:06.459424Z","iopub.status.idle":"2022-04-14T03:23:06.467662Z","shell.execute_reply.started":"2022-04-14T03:23:06.459395Z","shell.execute_reply":"2022-04-14T03:23:06.466575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dummy_audio_dataset(size:int=100):\n    files = []\n    labels = []\n\n    for i in range(size):\n        if i %2==0:\n            filename = librosa.ex('fishin')\n            labels.append(0)\n        if i %3==0:\n            filename = librosa.ex('brahms')\n            labels.append(1)\n        if i %5==0:\n            filename = librosa.ex('nutcracker')\n            labels.append(2)\n        if i %7==0:\n            filename = librosa.ex('trumpet')\n            labels.append(3)\n        else:\n            filename = librosa.ex('vibeace')\n            labels.append(4)\n\n        y, sr = librosa.load(filename)\n        files.append([y, sr])\n    return files, labels\n\n#get audio dataset\naudio, audio_labels = create_dummy_audio_dataset()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:06.469073Z","iopub.execute_input":"2022-04-14T03:23:06.469317Z","iopub.status.idle":"2022-04-14T03:23:11.204378Z","shell.execute_reply.started":"2022-04-14T03:23:06.469289Z","shell.execute_reply":"2022-04-14T03:23:11.203234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_combined_data(image, text, text_label, audio, audio_label):\n    \n    data = {\n        #for the image\n        'height' : _int64_feature(image.shape[0]),\n        'width' : _int64_feature(image.shape[1]),\n        'depth' : _int64_feature(image.shape[2]),\n        'raw_image' : _bytes_feature(serialize_array(image)),\n        #for the text\n        'text' : _bytes_feature(serialize_array(text)),\n        'text_label' : _int64_feature(text_label),\n        #for the audio\n        'sr' : _int64_feature(audio[1]),\n        'len' : _int64_feature(len(audio[0])),\n        'y' : _bytes_feature(serialize_array(audio[0])),\n        'audio_label' : _int64_feature(audio_label)\n    }\n\n    out = tf.train.Example(features=tf.train.Features(feature=data))\n    return out","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:11.206006Z","iopub.execute_input":"2022-04-14T03:23:11.207004Z","iopub.status.idle":"2022-04-14T03:23:11.216406Z","shell.execute_reply.started":"2022-04-14T03:23:11.206955Z","shell.execute_reply":"2022-04-14T03:23:11.21569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_combined_data_to_tfr(images, text_data, text_labels, audio_data, audio_labels, filename:str=\"combined\"):\n    filename= filename+\".tfrecords\"\n    writer = tf.io.TFRecordWriter(filename) #create a writer that'll store our combined data to disk\n    count = 0\n\n    for index in range(len(images)):\n\n        #get the image data\n        current_image = images[index]\n\n        #get the text data\n        current_text = text_data[index] \n        current_text_label = text_labels[index]\n\n        #get the audio data\n        current_audio = audio_data[index]\n        current_audio_label = audio_labels[index]\n\n        out = parse_combined_data(image=current_image, text=current_text, text_label=current_text_label, audio=current_audio, audio_label=current_audio_label)\n        writer.write(out.SerializeToString())\n        count += 1\n\n    writer.close()\n    print(f\"Wrote {count} elements to TFRecord\")\n\n    return count","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:11.217708Z","iopub.execute_input":"2022-04-14T03:23:11.218927Z","iopub.status.idle":"2022-04-14T03:23:11.235727Z","shell.execute_reply.started":"2022-04-14T03:23:11.218849Z","shell.execute_reply":"2022-04-14T03:23:11.23482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write_combined_data_to_tfr(images=images_combined, text_data=text, text_labels=text_labels, audio_data=audio, audio_labels=audio_labels)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:11.237293Z","iopub.execute_input":"2022-04-14T03:23:11.237537Z","iopub.status.idle":"2022-04-14T03:23:13.45279Z","shell.execute_reply.started":"2022-04-14T03:23:11.237509Z","shell.execute_reply":"2022-04-14T03:23:13.451797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_combined_tfr_element(element):\n  \n    data = {\n        #for the images\n        'height': tf.io.FixedLenFeature([], tf.int64),\n        'width':tf.io.FixedLenFeature([], tf.int64),\n        'raw_image' : tf.io.FixedLenFeature([], tf.string),\n        'depth':tf.io.FixedLenFeature([], tf.int64),\n        #for the text\n        'text' : tf.io.FixedLenFeature([], tf.string),\n        'text_label':tf.io.FixedLenFeature([], tf.int64),\n        #for the audio\n        'sr': tf.io.FixedLenFeature([], tf.int64),\n        'len':tf.io.FixedLenFeature([], tf.int64),\n        'y' : tf.io.FixedLenFeature([], tf.string),\n        'audio_label':tf.io.FixedLenFeature([], tf.int64),\n\n    }\n\n    content = tf.io.parse_single_example(element, data)\n\n    #image data\n    height = content['height']\n    width = content['width']\n    depth = content['depth']\n    raw_image = content['raw_image']\n\n    image_feature = tf.io.parse_tensor(raw_image, out_type=tf.int16)\n    image_feature = tf.reshape(image_feature, shape=[height,width,depth])\n\n    #audio data\n    sr = content['sr']\n    len = content['len']\n    y = content['y']\n    audio_label = content['audio_label']\n\n    audio_feature = tf.io.parse_tensor(y, out_type=tf.float32)\n    audio_feature = tf.reshape(audio_feature, shape=[len])\n\n\n    #text data\n    text = content['text']\n    text_label = content['text_label']\n\n    text_feature = tf.io.parse_tensor(text, out_type=tf.string)\n\n\n    return image_feature, text_feature, text_label, audio_feature, audio_label","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:13.454717Z","iopub.execute_input":"2022-04-14T03:23:13.455405Z","iopub.status.idle":"2022-04-14T03:23:13.469321Z","shell.execute_reply.started":"2022-04-14T03:23:13.455359Z","shell.execute_reply":"2022-04-14T03:23:13.467929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_combined_dataset(filename):\n    #create the dataset\n    dataset = tf.data.TFRecordDataset(filename)\n\n    #pass every single feature through our mapping function\n    dataset = dataset.map(\n          parse_combined_tfr_element\n    )\n\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:13.47062Z","iopub.execute_input":"2022-04-14T03:23:13.471031Z","iopub.status.idle":"2022-04-14T03:23:13.495043Z","shell.execute_reply.started":"2022-04-14T03:23:13.470898Z","shell.execute_reply":"2022-04-14T03:23:13.494048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nds = get_combined_dataset(\"./combined.tfrecords\")\nnext(iter(ds))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:13.496835Z","iopub.execute_input":"2022-04-14T03:23:13.497893Z","iopub.status.idle":"2022-04-14T03:23:13.592475Z","shell.execute_reply.started":"2022-04-14T03:23:13.497823Z","shell.execute_reply":"2022-04-14T03:23:13.591596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Regardless of the actual content, the procedure is always as follows:\n\n* Define a dictionary for the data that gets stored in the TFRecord file\n* Reconstruct the data by replicating this dictionary when parsing the data\n* Map every element to the parsing function\n\nSlight modifications are only required when you are dealing with large datasets.\n\nIn this case, you have to write your data to multiple TFRecord files,\nwhich we have covered in the section on dealing with large image data.","metadata":{}},{"cell_type":"markdown","source":"# Part 2: Using Cats/Dogs Dataset\n## WARNING THIS IS USING TFV1, so no guarantee everything is as tidy as it could be.\n\n### Walkthrough for code BY ORIGINAL AUTHOR found below.\n\nhttps://ai.plainenglish.io/a-quick-and-simple-guide-to-tfrecord-c421337a6562","metadata":{"execution":{"iopub.status.busy":"2022-04-14T01:28:19.031329Z","iopub.execute_input":"2022-04-14T01:28:19.031646Z","iopub.status.idle":"2022-04-14T01:28:19.03575Z","shell.execute_reply.started":"2022-04-14T01:28:19.031613Z","shell.execute_reply":"2022-04-14T01:28:19.035129Z"}}},{"cell_type":"code","source":"!echo 'N' | unzip -q ../input/dogs-vs-cats/train.zip\n!echo 'N' | unzip -q ../input/dogs-vs-cats/test1.zip\n\n# Importing required libraries\nimport tensorflow as tf\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.compat.v1.Session(config=config)\nimport numpy as np\nimport os\nfrom PIL import Image\nimport random\n# Setup the train and test imgae directories\ntrain_dir=r'train'\ntest_dir=r'test1'\n#Setting up Image dimension\nIMG_HEIGHT=100\nIMG_WIDTH=100\n# setup train and test TFRecord file\ntrain_tfrecord='train_data.tfrecords'\ntest_tfrecord = 'test_data.tfrecords'\n# Define the classes\n#List all train and test image path\ntrain_image_path=[]\ntest_image_path=[]\nfor file in os.listdir(train_dir):\n    train_image_path.append(os.path.join(train_dir, file))\nfor file in os.listdir(test_dir):\n    test_image_path.append( os.path.join(test_dir, file))\n#Shuffle the image paths for better accuracy and precision\nrandom.seed(0)\nrandom.shuffle(train_image_path)\nrandom.shuffle(test_image_path)\n# create train and test lables for shuffled image paths\ntest_labels=[]\ntrain_labels=[]\nfor i in range(len(train_image_path)):\n    if os.path.basename(train_image_path[i])[6:9]=='cat':\n        train_labels.append(0)\n    else:\n        train_labels.append(1)\n# for i in range(len(test_image_path)):\n    \n#     if os.path.basename(test_image_path[i])[:3]=='cat':\n#         test_labels.append(0)\n#     else:\n#         test_labels.append(1)\ntest_labels=train_labels","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:13.593969Z","iopub.execute_input":"2022-04-14T03:23:13.594496Z","iopub.status.idle":"2022-04-14T03:23:15.941902Z","shell.execute_reply.started":"2022-04-14T03:23:13.59446Z","shell.execute_reply":"2022-04-14T03:23:15.940862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _bytes_feature(value):\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"    \n    if isinstance(value, type(tf.constant(0))):\n        value = value.numpy() \n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\ndef _float_feature(value):\n    \"\"\"Returns a float_list from a float / double.\"\"\"\n    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\ndef _int64_feature(value):\n    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:15.94372Z","iopub.execute_input":"2022-04-14T03:23:15.944993Z","iopub.status.idle":"2022-04-14T03:23:15.95243Z","shell.execute_reply.started":"2022-04-14T03:23:15.944945Z","shell.execute_reply":"2022-04-14T03:23:15.951299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def serialize_example(image, label):\n    ## Create a dictionary with features for images and their target labels\n    feature = {\n        'image': _bytes_feature(image),\n        'label': _int64_feature(label),\n        \n    }\n    #  Create a Features message using tf.train.Example.\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n    #serializes the message and returns it as a string. Note that the bytes are binary\n    return example_proto.SerializeToString()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:15.95385Z","iopub.execute_input":"2022-04-14T03:23:15.954192Z","iopub.status.idle":"2022-04-14T03:23:15.96998Z","shell.execute_reply.started":"2022-04-14T03:23:15.954161Z","shell.execute_reply":"2022-04-14T03:23:15.968973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_TFRecord(image_path, label):\n    img=tf.keras.preprocessing.image.load_img(image_path, target_size=(IMG_HEIGHT, IMG_WIDTH))        \n    img_array= tf.keras.preprocessing.image.img_to_array(img)\n    img_bytes= tf.io.serialize_tensor(img_array)\n    example= serialize_example(img_bytes, label)\n    return example\n#Write Train TFRecord file\nwith tf.io.TFRecordWriter(train_tfrecord) as writer:\n    for image_path, label in zip(train_image_path, train_labels):\n        writer.write(write_TFRecord(image_path, int(label)))\n#Write Test TFRecord file\nwith tf.io.TFRecordWriter(test_tfrecord) as writer:\n    for image_path, label in zip(test_image_path, test_labels):\n         writer.write(write_TFRecord(image_path, int(label)))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:23:15.971575Z","iopub.execute_input":"2022-04-14T03:23:15.972362Z","iopub.status.idle":"2022-04-14T03:25:40.561348Z","shell.execute_reply.started":"2022-04-14T03:23:15.972315Z","shell.execute_reply":"2022-04-14T03:25:40.558334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initilizaing the TFRecordDataset for train and test TFRecord file\ntrain_tfrecord_dataset=tf.data.TFRecordDataset(train_tfrecord)\ntest_tfrecord_dataset=tf.data.TFRecordDataset(test_tfrecord)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:25:40.567608Z","iopub.execute_input":"2022-04-14T03:25:40.568521Z","iopub.status.idle":"2022-04-14T03:25:40.59115Z","shell.execute_reply.started":"2022-04-14T03:25:40.568465Z","shell.execute_reply":"2022-04-14T03:25:40.590087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_tfrecord(serialized_example):\n    feature_description={\n        'image': tf.io.FixedLenFeature((), tf.string),\n        'label':tf.io.FixedLenFeature((), tf.int64)                \n    }\n    example= tf.io.parse_single_example(serialized_example, feature_description)\n    image=tf.io.parse_tensor(example['image'], out_type=float)\n    image = tf.reshape(image, [IMG_HEIGHT, IMG_WIDTH,3])\n    \n    return image, example['label']","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:25:40.592915Z","iopub.execute_input":"2022-04-14T03:25:40.593154Z","iopub.status.idle":"2022-04-14T03:25:40.60227Z","shell.execute_reply.started":"2022-04-14T03:25:40.593127Z","shell.execute_reply":"2022-04-14T03:25:40.601278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_dataset=train_tfrecord_dataset.map(read_tfrecord)\ntest_dataset=test_tfrecord_dataset.map(read_tfrecord)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:25:40.603653Z","iopub.execute_input":"2022-04-14T03:25:40.603925Z","iopub.status.idle":"2022-04-14T03:25:40.639454Z","shell.execute_reply.started":"2022-04-14T03:25:40.603891Z","shell.execute_reply":"2022-04-14T03:25:40.638475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\ntrain_dataset = train_dataset.shuffle(True)\ntrain_dataset = train_dataset.batch(10)\ntest_dataset = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)\ntest_dataset = test_dataset.batch(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:25:40.640991Z","iopub.execute_input":"2022-04-14T03:25:40.641336Z","iopub.status.idle":"2022-04-14T03:25:40.650158Z","shell.execute_reply.started":"2022-04-14T03:25:40.641303Z","shell.execute_reply":"2022-04-14T03:25:40.649172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(IMG_HEIGHT)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:25:40.651462Z","iopub.execute_input":"2022-04-14T03:25:40.651797Z","iopub.status.idle":"2022-04-14T03:25:40.660225Z","shell.execute_reply.started":"2022-04-14T03:25:40.651755Z","shell.execute_reply":"2022-04-14T03:25:40.659356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_model():\n    base_model = tf.keras.applications.Xception(input_shape=(IMG_WIDTH, IMG_HEIGHT,3), include_top=False, weights='imagenet')\n    base_model.trainable = False\n    inputs = tf.keras.layers.Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n    x = tf.keras.applications.xception.preprocess_input(inputs)\n    x = base_model(x)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(8, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.7)(x)\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n    loss='binary_crossentropy',\n    metrics=tf.keras.metrics.AUC(name='auc'))\n    return model\n\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(0.001, decay_steps=20, decay_rate=0.96, staircase=True)\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint('model_cat_n_dog.h5', save_best_only=True)\nmodel=make_model()\n# Train the model on data extrcated from TFRecord file\n\nhistory = model.fit(x=train_dataset,epochs=20, callbacks=[checkpoint_cb])","metadata":{"execution":{"iopub.status.busy":"2022-04-14T03:28:29.732299Z","iopub.execute_input":"2022-04-14T03:28:29.732622Z"},"trusted":true},"execution_count":null,"outputs":[]}]}