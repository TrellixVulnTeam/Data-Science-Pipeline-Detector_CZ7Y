{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport keras\nfrom keras import models,layers,optimizers\nfrom keras.layers import BatchNormalization,Dropout\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator,load_img\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n\nimport os\n#print(os.listdir(\"../input/train/train\"))\n\n# Any results you write to the current directory are saved as output.\nfilenames = os.listdir(\"../input/train/train\")\ncategories = []\nfor file in filenames:\n    if file.split(\".\")[0]=='dog':\n        categories.append('Dog')\n    else:\n        categories.append('Cat')\n\n#print(categories[:10])\ndata = pd.DataFrame(\n        {'filename':filenames,\n        'category':categories})\n#data.head()\ndata['category'].value_counts().plot.bar()\n\n","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f45e21cbf60>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model =models.Sequential()\nmodel.add(layers.Conv2D(32,(3,3),activation = 'relu',input_shape = (128,128,3)))\nmodel.add(layers.MaxPooling2D(2,2))\nmodel.add(layers.Conv2D(64,(3,3),activation = 'relu'))\nmodel.add(layers.MaxPooling2D(2,2))\nmodel.add(layers.Conv2D(128,(3,3),activation = 'relu'))\nmodel.add(layers.MaxPooling2D(2,2))\nmodel.add(layers.Conv2D(128,(3,3),activation = 'relu'))\nmodel.add(layers.MaxPooling2D(2,2))\nmodel.add(Dropout(0.25))                            #Adding regularization\nmodel.add(layers.Flatten())\nmodel.add(BatchNormalization())\nmodel.add(layers.Dense(512,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(layers.Dense(1,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=optimizers.RMSprop(lr=1e-4),\n              metrics=['accuracy'])\n\nmodel.summary()\n\n","execution_count":2,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 126, 126, 32)      896       \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 63, 63, 32)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 61, 61, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 30, 30, 64)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 28, 28, 128)       73856     \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 14, 14, 128)       0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 12, 12, 128)       147584    \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 6, 6, 128)         0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 6, 6, 128)         0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 4608)              0         \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 4608)              18432     \n_________________________________________________________________\ndense_1 (Dense)              (None, 512)               2359808   \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 513       \n=================================================================\nTotal params: 2,619,585\nTrainable params: 2,610,369\nNon-trainable params: 9,216\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, val_X = train_test_split(data, test_size=0.20, random_state=42)\ntrain_X = train_X.reset_index(drop=True)\nval_X = val_X.reset_index(drop=True)\ntotal_train = train_X.shape[0]\nprint(total_train)\ntotal_val = val_X.shape[0]\nbatch_size=40\nprint(train_X.shape)\n\n","execution_count":3,"outputs":[{"output_type":"stream","text":"20000\n(20000, 2)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,)\n#test_datagen = ImageDataGenerator(rescale = 1./255)\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    train_X,\n    \"../input/train/train/\",\n    x_col = 'filename',\n    y_col = 'category',\n    target_size = (128,128),\n    class_mode = 'binary',\n    batch_size = batch_size)\n\n\n","execution_count":4,"outputs":[{"output_type":"stream","text":"Found 20000 images belonging to 2 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_datagen = ImageDataGenerator(rescale = 1./255)\nval_generator = val_datagen.flow_from_dataframe(\n    val_X,\n    \"../input/train/train/\",\n    x_col = 'filename',\n    y_col = 'category',\n    target_size = (128,128),\n    class_mode = 'binary',\n    batch_size = batch_size)\n\n","execution_count":5,"outputs":[{"output_type":"stream","text":"Found 5000 images belonging to 2 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ReduceLROnPlateau\nearlystop = EarlyStopping(patience=10)\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\ncallbacks = [earlystop, learning_rate_reduction]\n\n\nhistory = model.fit_generator(\ntrain_generator,\nsteps_per_epoch=total_train//batch_size,\nepochs=50,\nvalidation_data=val_generator,\nvalidation_steps=total_val//batch_size,\ncallbacks=callbacks)\n","execution_count":null,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('catndogs_wo_pretrain_nw.h5')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"acc=history.history['acc']\nval_acc = history.history['val_acc']\nloss=history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1,len(acc)+1)\n\nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_files = os.listdir(\"../input/test1/test1\")\ntest_X = pd.DataFrame({\n    'filename': test_files\n})\n\nsamples = test_X.shape[0]\nprint(samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_gen = ImageDataGenerator(rescale=1./255)\ntest_generator = test_gen.flow_from_dataframe(\n    test_X, \n    \"../input/test1/test1/\", \n    x_col='filename',\n    y_col=None,\n    class_mode=None,\n    target_size=(128,128),\n    batch_size=40,\n    shuffle=False\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = model.predict_generator(test_generator, steps=np.ceil(samples/batch_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 0.5\ntest_X['probability'] = predict\ntest_X['category'] = np.where(test_X['probability'] > threshold, '1','0')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import load_img\nsample_test = test_X.head(15)\nsample_test.head()\nplt.figure(figsize=(15, 30))\nfor index, row in sample_test.iterrows():\n    filename = row['filename']\n    category = row['category']\n    img = load_img(\"../input/test1/test1/\"+filename, target_size=(128,128))\n    plt.subplot(5, 5, index+1)\n    plt.imshow(img)\n    plt.xlabel(filename + '(' + \"{}\".format(category) +')')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = test_X.copy()\nsubmission_df['id'] = submission_df['filename'].str.split('.').str[0]\nsubmission_df['label'] = submission_df['category']\nsubmission_df.drop(['filename', 'category'], axis=1, inplace=True)\nsubmission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}