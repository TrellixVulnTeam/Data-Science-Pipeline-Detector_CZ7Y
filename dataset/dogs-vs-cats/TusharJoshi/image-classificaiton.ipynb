{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport math\nimport time\nimport itertools\nimport torch.nn as nn\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nimport matplotlib.pyplot as plt\n\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\nimport pandas as pd\nimport psutil\nprint(os.listdir(\"../input\"))\n\nnp.random.seed(10)\nUSE_GPU = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_sudo_data(directory_location, print_random=False, size=(100,100)):\n    files = glob.glob(directory_location)\n    img_file_name, img_category = [], []\n    \n    for file in files:\n        img_category.append(file.split(\"/\")[-1].split(\".\")[0])\n        img_file_name.append(file)\n    \n    df = pd.DataFrame({\"Img_filename\": img_file_name, \"Img_Category\": img_category})\n    \n    if print_random:\n        v = np.random.randint(0,len(files))\n        image = cv2.resize(cv2.imread(df.Img_filename[v], cv2.IMREAD_GRAYSCALE), size)\n        image_cat = df.Img_Category[v]\n        \n        plt.imshow(image, cmap=\"gray\")\n        plt.title(image_cat)\n    \n    df = shuffle(df, random_state=10)\n    df.reset_index(drop=True, inplace=True)\n    return df\n\ndf = load_sudo_data(\"../input/train/train/*\", print_random=True)\ndf_train, df_val = train_test_split(df, test_size=0.05, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (df_train.shape)\nprint (df_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_batch(df, batch_number, batch_size=100, size=(100, 100)):\n    \n    batch_start_index = batch_number * batch_size\n    if batch_start_index + batch_size >= len(df):\n        batch_end_index = len(df)\n    else:\n        batch_end_index = batch_start_index + batch_size\n        \n    X, y = [], [] \n    category_to_index = {'cat': 0, 'dog': 1}\n    index_to_category = {0: 'cat', 1: 'dog'}\n    \n    for index, dp in df[batch_start_index:batch_end_index].iterrows():\n        X.append(cv2.resize(cv2.imread(dp.Img_filename, cv2.IMREAD_GRAYSCALE), size))\n        y.append(category_to_index[dp.Img_Category])\n            \n    return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_mem(itnum,bnum): \n    pid = os.getpid()\n    py = psutil.Process(pid)\n    memoryUse = py.memory_info()[0]/2.**20\n    return 'iteration: {} batchnum {} memory use: {}MB'.format(itnum, bnum, memoryUse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImageClassifier2layer(nn.Module):\n    def __init__(self, input_channels, conv_l1_channels, conv_l2_channels, \n                 conv_l1_kernelsize, conv_l2_kernelsize, \n                 conv_l1_padding, conv_l2_padding, \n                 conv_l1_stride, conv_l2_stride,\n                 conv_l1_pool, conv_l2_pool, \n                 size, dropout=0.2):\n        super(ImageClassifier2layer, self).__init__()\n        self.size = size\n        self.conv_layer1 = nn.Sequential(\n            nn.Conv2d(input_channels, conv_l1_channels, kernel_size=conv_l1_kernelsize, padding=conv_l1_padding, stride=conv_l1_stride),\n            nn.BatchNorm2d(conv_l1_channels),\n            nn.ReLU(),\n            nn.MaxPool2d(conv_l1_pool))\n             \n        self.conv_layer2 = nn.Sequential(\n            nn.Conv2d(conv_l1_channels, conv_l2_channels, kernel_size=conv_l2_kernelsize, padding=conv_l2_padding, stride=conv_l2_stride),\n            nn.BatchNorm2d(conv_l2_channels),\n            nn.ReLU(),\n            nn.MaxPool2d(conv_l2_pool)\n        )\n        self.fc1 = nn.Linear(400, 10)\n        self.dropout1 = nn.Dropout(dropout)\n        self.fc2 = nn.Linear(10, 1)\n        self.dropout2 = nn.Dropout(dropout)\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, x, batch_size):\n        x = x.view(batch_size, 1, self.size[0], self.size[1])\n        out = self.conv_layer1(x)\n        out = self.conv_layer2(out)\n        \n        out = out.view(batch_size, -1)\n        out = self.fc1(out)\n        out = self.dropout1(out)\n        \n        out = self.fc2(out)\n        out = self.sigmoid(out)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(classifier, criterion, optimizer, batch_size, data, size, iter_num, calc_accuracy=False):\n    iteration_loss = 0.0\n    correct_predicted = 0.0\n    \n    classifier = classifier.train()\n    \n    for batch_number in range(0, math.ceil(len(data)/batch_size)):\n#         print (print_mem(iter_num, batch_number))\n        input, output = load_batch(data, batch_number, batch_size=batch_size, size=size)\n        current_batch_size = len(input)\n        \n        if USE_GPU and torch.cuda.is_available():\n            input_tensor = torch.tensor(input, dtype=torch.float).cuda()\n            output_tensor = torch.tensor(output, dtype=torch.float).view(-1, 1).cuda()\n        else:\n            input_tensor = torch.tensor(input, dtype=torch.float)\n            output_tensor = torch.tensor(output, dtype=torch.float).view(-1, 1)\n            \n        optimizer.zero_grad()\n        output = classifier(input_tensor, current_batch_size)\n        \n        loss = criterion(output, output_tensor)\n        loss.backward()\n        \n        optimizer.step()\n        \n        iteration_loss += loss.item() * current_batch_size\n        correct_predicted += get_accuracy(output_tensor, output) * current_batch_size\n        torch.cuda.empty_cache()\n        \n    return iteration_loss/float(len(data)), correct_predicted/float(len(data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validate(classifier, criterion, data, size):\n    classifier = classifier.eval()\n    \n    input, output = load_batch(data, 0, batch_size=len(data), size=size)\n    \n    if USE_GPU and torch.cuda.is_available():\n        input_tensor = torch.tensor(input, dtype=torch.float).cuda()\n        output_tensor = torch.tensor(output, dtype=torch.float).view(-1, 1).cuda()\n    else:\n        input_tensor = torch.tensor(input, dtype=torch.float)\n        output_tensor = torch.tensor(output, dtype=torch.float).view(-1, 1)\n    \n    output = classifier(input_tensor, len(data))\n    \n    loss = criterion(output, output_tensor)\n    acc = get_accuracy(output_tensor, output)\n    \n    return loss.item(), acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_accuracy(true, predicted):\n    true = true.tolist()\n    pred = predicted.round().tolist()\n    \n    return accuracy_score(true, pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trainiter(classifier, criterion, optimizer, n_iters, batch_size, size, print_every=100):\n    start = time.time()\n    training_error, validation_error, training_accuracy, validation_accuracy = [], [], [], []\n    for iter in range(1, n_iters + 1):\n        train_error, train_acc = train(classifier, criterion, optimizer, batch_size, df_train, size, iter)\n        val_error, val_acc = validate(classifier, criterion, df_val, size)\n        \n        training_error.append(train_error)\n        training_accuracy.append(train_acc)\n        validation_error.append(val_error)\n        validation_accuracy.append(val_acc)\n        \n        if iter % print_every == 0:\n            print('%s (%d %d%%) Train Error = %.4f Train Acc = %.4f Val Error = %.4f Val Acc = %.4f' \n                  % (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, train_error, train_acc, val_error, val_acc))\n\n    return training_error, training_accuracy, validation_error, validation_accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INP_CHANNELS = 1\nCONV1_CHANNELS = 32\nCONV2_CHANNELS = 16\nCONV1_KERNEL_SIZE = 5\nCONV2_KERNEL_SIZE = 3\nCONV1_PADDING = 2\nCONV2_PADDING = 1\nCONV1_STRIDE = 3\nCONV2_STRIDE = 2\nCONV1_POOL = 2\nCONV2_POOL = 2\nBATCH_SIZE = 1000\nSIZE = (128, 128)\nDROPOUT = 0.2\nLEARNING_RATE = 0.001\n\nclf = ImageClassifier2layer(INP_CHANNELS, CONV1_CHANNELS, CONV2_CHANNELS, CONV1_KERNEL_SIZE, CONV2_KERNEL_SIZE, \n                            CONV1_PADDING, CONV2_PADDING, CONV1_STRIDE, CONV2_STRIDE, CONV1_POOL, CONV2_POOL, \n                            SIZE, DROPOUT)\n\nif USE_GPU and torch.cuda.is_available():\n    clf.cuda()\nelse:\n    pass\n\ncriterion = nn.BCELoss();\noptimizer = torch.optim.Adam(clf.parameters(), lr=LEARNING_RATE);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_error, training_accuracy, validation_error, validation_accuracy = trainiter(clf, criterion, optimizer, 200, BATCH_SIZE, SIZE, print_every=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_test_data(directory_location, size=(100,100)):\n    files = glob.glob(directory_location)\n    \n    X_test = []\n    id = []\n    for file in files:\n        X_test.append(cv2.resize(cv2.imread(file, cv2.IMREAD_GRAYSCALE), size))\n        id.append(file.split(\"/\")[-1].split(\".\")[0])\n        \n    return X_test, id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(classifier, X):\n    classifier = classifier.eval()\n    \n    if USE_GPU and torch.cuda.is_available():\n        X_tensor = torch.tensor(X, dtype=torch.float).cuda()\n    else:\n        X_tensor = torch.tensor(X, dtype=torch.float)\n    \n    predicted = classifier(X_tensor, len(X))\n    \n    return predicted.round().tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test, ids = load_test_data(\"../input/test1/test1/*\", size=SIZE)\npredicted = predict(clf, X_test)\npred = list(itertools.chain(*predicted))\n\ndf = pd.DataFrame({\"id\": ids, \"label\": pred})\ndf.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(0, len(training_error)), training_error)\nplt.plot(range(0, len(validation_error)), validation_error)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}