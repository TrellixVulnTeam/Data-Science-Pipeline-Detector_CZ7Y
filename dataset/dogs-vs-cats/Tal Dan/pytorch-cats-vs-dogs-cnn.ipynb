{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-15T07:11:26.629864Z","iopub.execute_input":"2022-02-15T07:11:26.630457Z","iopub.status.idle":"2022-02-15T07:11:28.200011Z","shell.execute_reply.started":"2022-02-15T07:11:26.630372Z","shell.execute_reply":"2022-02-15T07:11:28.19919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the data","metadata":{}},{"cell_type":"code","source":"# unzip the dataset\nimport zipfile\n\nwith zipfile.ZipFile(\"../input/dogs-vs-cats/train.zip\",\"r\") as z:\n    z.extractall(\".\")\nwith zipfile.ZipFile(\"../input/dogs-vs-cats/test1.zip\", \"r\") as z:\n    z.extractall(\".\")\n    \n# save the directories of the datasets to variables\ntrain_dir = \"/kaggle/working/train/\"\ntest_dir = \"/kaggle/working/test1/\"\n\n# check the format of the data\ntrain_image_names = os.listdir(train_dir)\ntest_image_names = os.listdir(test_dir)\n\nprint(train_image_names[:3])\nprint(test_image_names[:3])","metadata":{"execution":{"iopub.status.busy":"2022-02-15T07:11:28.202203Z","iopub.execute_input":"2022-02-15T07:11:28.202426Z","iopub.status.idle":"2022-02-15T07:11:44.618556Z","shell.execute_reply.started":"2022-02-15T07:11:28.202394Z","shell.execute_reply":"2022-02-15T07:11:44.617707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make lists of dataset images with their full path\nimport glob\n\ntrain_images = glob.glob(os.path.join(train_dir, \"*.jpg\"))\ntest_images = glob.glob(os.path.join(test_dir, \"*.jpg\" ))\n\nprint(train_images[:3])\nprint(len(train_images))\nprint(test_images[:3])\nprint(len(test_images))","metadata":{"execution":{"iopub.status.busy":"2022-02-15T07:11:44.619972Z","iopub.execute_input":"2022-02-15T07:11:44.620437Z","iopub.status.idle":"2022-02-15T07:11:44.749196Z","shell.execute_reply.started":"2022-02-15T07:11:44.620394Z","shell.execute_reply":"2022-02-15T07:11:44.748333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split the labeled train data into training and validation sets","metadata":{"execution":{"iopub.status.busy":"2022-02-12T09:41:30.581167Z","iopub.execute_input":"2022-02-12T09:41:30.581453Z","iopub.status.idle":"2022-02-12T09:41:30.589401Z","shell.execute_reply.started":"2022-02-12T09:41:30.581425Z","shell.execute_reply":"2022-02-12T09:41:30.588321Z"}}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_list, val_list = train_test_split(train_images, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T07:11:48.509317Z","iopub.execute_input":"2022-02-15T07:11:48.509692Z","iopub.status.idle":"2022-02-15T07:11:49.286593Z","shell.execute_reply.started":"2022-02-15T07:11:48.509654Z","shell.execute_reply":"2022-02-15T07:11:49.285856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define a pytorch class for retrieving the images in the dataset","metadata":{}},{"cell_type":"code","source":"# make a dataset class\nclass CatDogDataset(Dataset):\n    \n    def __init__(self, images_list, mode=\"train\", transform=None):\n        self.images_list = images_list\n        self.mode = mode\n        self.transform = transform\n        \n    # dataset length\n    def __len__(self):\n        self.dataset_len = len(self.images_list)\n        return self.dataset_len\n    \n    # load an image\n    def __getitem__(self, idx):\n        image_name = self.images_list[idx] \n        image = Image.open(image_name)\n        image = image.resize((224,224)) # this is important when feeding into a pretrained model\n        transformed_image = self.transform(image)\n        image_category = image_name.split(\"/\")[-1].split(\".\")[0]\n        \n        if self.mode == \"train\" or self.mode == \"val\":\n            if image_category == \"cat\":\n                label = 0 \n            else:\n                label = 1              \n            return transformed_image, label\n        else:\n            image_id = int(image_name.split('/')[-1].split('.')[0])\n            return transformed_image, image_id","metadata":{"execution":{"iopub.status.busy":"2022-02-15T07:11:50.522916Z","iopub.execute_input":"2022-02-15T07:11:50.523661Z","iopub.status.idle":"2022-02-15T07:11:50.533187Z","shell.execute_reply.started":"2022-02-15T07:11:50.523621Z","shell.execute_reply":"2022-02-15T07:11:50.532391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define transformations to normalize/augment the images","metadata":{}},{"cell_type":"code","source":"# define transformations for the train, test and holdout images\ntrain_transforms = transforms.Compose([\n        transforms.RandomHorizontalFlip(p=0.5), \n        transforms.RandomRotation(15),\n        transforms.RandomResizedCrop(224, scale=(0.8,1.0),ratio=(1.0,1.0)),\n        transforms.ToTensor(),\n        transforms.Normalize((0, 0, 0),(1, 1, 1))\n    ])\n\n# for validation we only need to normalize the data\nval_transforms = transforms.Compose([ \n        transforms.ToTensor(),\n        transforms.Normalize((0, 0, 0),(1, 1, 1))\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-02-15T07:11:52.336734Z","iopub.execute_input":"2022-02-15T07:11:52.337234Z","iopub.status.idle":"2022-02-15T07:11:52.343434Z","shell.execute_reply.started":"2022-02-15T07:11:52.337197Z","shell.execute_reply":"2022-02-15T07:11:52.342426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define model hyperparameters\n","metadata":{}},{"cell_type":"code","source":"batch_size = 128\nnum_epochs = 100\nlearning_rate = 0.001","metadata":{"execution":{"iopub.status.busy":"2022-02-15T07:11:55.755888Z","iopub.execute_input":"2022-02-15T07:11:55.756405Z","iopub.status.idle":"2022-02-15T07:11:55.762659Z","shell.execute_reply.started":"2022-02-15T07:11:55.756368Z","shell.execute_reply":"2022-02-15T07:11:55.761776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the datasets for training","metadata":{}},{"cell_type":"code","source":"# create dataset objects\ntrain_dataset = CatDogDataset(train_list, transform=train_transforms)\nval_dataset = CatDogDataset(val_list, mode=\"val\", transform=val_transforms)\ntest_dataset = CatDogDataset(test_images, mode=\"test\", transform=val_transforms)\n\n# create dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T07:11:58.711235Z","iopub.execute_input":"2022-02-15T07:11:58.711492Z","iopub.status.idle":"2022-02-15T07:11:58.717981Z","shell.execute_reply.started":"2022-02-15T07:11:58.711464Z","shell.execute_reply":"2022-02-15T07:11:58.717095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize several images\nfor images, labels in train_dataloader:\n    \n    images = images[:16,:,:,:]\n    fig, ax = plt.subplots(figsize = (10, 10))\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.imshow(torchvision.utils.make_grid(images, nrow=4).permute(1,2,0))\n    break","metadata":{"execution":{"iopub.status.busy":"2022-02-15T07:13:51.705609Z","iopub.execute_input":"2022-02-15T07:13:51.705912Z","iopub.status.idle":"2022-02-15T07:13:55.138699Z","shell.execute_reply.started":"2022-02-15T07:13:51.705875Z","shell.execute_reply":"2022-02-15T07:13:55.137935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note on **.permute(1,2,0)** above:\nThe dimensions of grid_img are [# color channels x image height x image width]. Conversely, the input to matplotlib.pyplot.imshow() needs to be [image heigth x image width x # color channels] (i.e., the shape needs to be [518, 1292, 3]). The .permute(1, 2, 0) action is a Torch-specific function that permutes the axes of the original in exactly that order: [axis 1 x axis 2 x axis 0] = [image heigth x image width x # color channels]. ","metadata":{}},{"cell_type":"markdown","source":"## Check if we have GPU available","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T08:26:59.220625Z","iopub.execute_input":"2022-02-14T08:26:59.220895Z","iopub.status.idle":"2022-02-14T08:26:59.230792Z","shell.execute_reply.started":"2022-02-14T08:26:59.220865Z","shell.execute_reply":"2022-02-14T08:26:59.229972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build a model class","metadata":{}},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.cnn_layers = nn.Sequential(\n            # convolutional layer 1\n            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, padding=0, stride=1),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2), \n            \n            # convolutional layer 2\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, padding=0, stride=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2), \n            \n            # convolutional layer 3\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=0, stride=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2) \n        )\n        \n        self.linear_layers = nn.Sequential(\n            #nn.Linear(in_features=64 * 24 * 24, out_features=10),\n            nn.Linear(in_features=64 * 24 * 24, out_features=1024),\n            nn.ReLU(),\n            #nn.Dropout(0.3),\n            nn.BatchNorm1d(1024),\n            nn.Linear(in_features=1024, out_features=2)\n        )\n    \n    def forward(self, x):\n        out = self.cnn_layers(x)\n        #print(out.shape)\n        out = out.view(-1, 64 * 24 * 24) # flatten \n        out = self.linear_layers(out)\n        return out\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-14T08:26:59.232549Z","iopub.execute_input":"2022-02-14T08:26:59.232913Z","iopub.status.idle":"2022-02-14T08:26:59.245476Z","shell.execute_reply.started":"2022-02-14T08:26:59.232874Z","shell.execute_reply":"2022-02-14T08:26:59.244601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# instantiate the model\nmodel = CNN()\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T08:26:59.247345Z","iopub.execute_input":"2022-02-14T08:26:59.248218Z","iopub.status.idle":"2022-02-14T08:26:59.616125Z","shell.execute_reply.started":"2022-02-14T08:26:59.248177Z","shell.execute_reply":"2022-02-14T08:26:59.615374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the loss function and optimizer","metadata":{"execution":{"iopub.status.busy":"2022-02-12T15:22:32.341756Z","iopub.execute_input":"2022-02-12T15:22:32.342225Z","iopub.status.idle":"2022-02-12T15:22:32.362021Z","shell.execute_reply.started":"2022-02-12T15:22:32.342186Z","shell.execute_reply":"2022-02-12T15:22:32.361167Z"}}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss() # applies log_softmax and then NLLLoss cost function\noptimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)\n#optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n# Creating LR scheduler\n#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T08:26:59.618993Z","iopub.execute_input":"2022-02-14T08:26:59.61951Z","iopub.status.idle":"2022-02-14T08:26:59.624387Z","shell.execute_reply.started":"2022-02-14T08:26:59.61947Z","shell.execute_reply":"2022-02-14T08:26:59.623646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the model","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\ntrain_losses = []\nval_losses = []\naccuracy_list = []\n\nfor epoch in range(num_epochs):\n    \n    # perform training on train set\n    model.train()\n    running_loss = 0\n    \n    for images, labels in tqdm(train_dataloader):\n        \n        # load to gpu\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        running_loss += loss.item()\n        \n        # backprop and update model params\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        #scheduler.step() # for LR scheduler\n        \n    # calculate training loss for the epoch\n    train_losses.append(running_loss / len(train_dataloader))\n    \n    # calculate loss accuracy on validation set\n    model.eval()\n    running_loss = 0\n    num_correct = 0\n    num_predictions = 0\n    \n    with torch.no_grad():  \n        for images, labels in tqdm(val_dataloader):\n            \n            # load to gpu\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            # forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n            \n            # calculate accuracy for batch\n            _, predicted = torch.max(outputs.data, 1)\n            num_correct += (predicted == labels).sum().item()\n            num_predictions += labels.size(0)\n            \n    # calculate val loss for epoch\n    val_losses.append(running_loss / len(val_dataloader))\n    \n    # calculate accuracy for epoch\n    accuracy = num_correct / num_predictions * 100\n    accuracy_list.append(accuracy)\n    \n    print(\"[Epoch: %d / %d],  [Train loss: %.4f],  [Test loss: %.4f],  [Acc: %.2f]\" \\\n          %(epoch+1, num_epochs, train_losses[-1], val_losses[-1], accuracy))\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-02-14T08:27:10.080809Z","iopub.execute_input":"2022-02-14T08:27:10.081347Z","iopub.status.idle":"2022-02-14T12:15:22.314236Z","shell.execute_reply.started":"2022-02-14T08:27:10.081309Z","shell.execute_reply":"2022-02-14T12:15:22.313373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"id_list = []\npred_list = []\n\nwith torch.no_grad():\n    model.eval()\n    for images, ids in tqdm(test_dataloader):\n        \n        # load to gpu\n        images = images.to(device)\n        ids = ids.to(device)\n        \n        # forward pass\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        \n        \n        id_list.append([id for id in ids])\n        pred_list.append([pred for pred in predicted])","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:48:51.494786Z","iopub.execute_input":"2022-02-14T12:48:51.49522Z","iopub.status.idle":"2022-02-14T12:49:56.499054Z","shell.execute_reply.started":"2022-02-14T12:48:51.495184Z","shell.execute_reply":"2022-02-14T12:49:56.498154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\n\nflat_id_list = list(itertools.chain(*id_list))\nflat_pred_list = list(itertools.chain(*pred_list))\n\nid_list_np = [id.cpu().data.numpy() for id in flat_id_list]\npred_list_np = [pred.cpu().data.numpy() for pred in flat_pred_list]","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:56:29.000342Z","iopub.execute_input":"2022-02-14T12:56:29.000597Z","iopub.status.idle":"2022-02-14T12:56:29.428625Z","shell.execute_reply.started":"2022-02-14T12:56:29.000569Z","shell.execute_reply":"2022-02-14T12:56:29.427888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'id': id_list_np, 'label': pred_list_np})\n\nsubmission.sort_values(by='id', inplace=True)\nsubmission.reset_index(drop=True, inplace=True)\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T12:56:31.430193Z","iopub.execute_input":"2022-02-14T12:56:31.430451Z","iopub.status.idle":"2022-02-14T12:56:31.709656Z","shell.execute_reply.started":"2022-02-14T12:56:31.430424Z","shell.execute_reply":"2022-02-14T12:56:31.708948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}