{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport zipfile\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport cv2\nimport random\nimport os\nprint(os.listdir(\"../input/dogs-vs-cats\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_WIDTH=64\nIMAGE_HEIGHT=64\nIMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\nIMAGE_CHANNELS=3\nTRAIN_DIRECTORY=\"/kaggle/working/train/\"\nTEST_DIRECTORY=\"/kaggle/working/test1\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_files(source_path, target_path):\n    zip_ref = zipfile.ZipFile(source_path,'r')\n    zip_ref.extractall(target_path)\n    zip_ref.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extraction():\n    extract_files('/kaggle/input/dogs-vs-cats/test1.zip','/kaggle/working/')\n    extract_files('/kaggle/input/dogs-vs-cats/train.zip','/kaggle/working/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extraction()\n\ndef get_filenames(directory):\n    filenames=(os.listdir(directory))\n    return filenames\n\n\ndef load_data(filenames,directory):\n#     i=500   #for testing purpose\n    i=len(filenames);\n    X=[]\n    y=[]\n    for name in filenames:\n        img=mpimg.imread(os.path.join(directory,name))\n        X.append(cv2.resize(img,IMAGE_SIZE))\n        cat=name.split('.')[0]\n        if(cat=='dog'):\n            y.append(0)\n        else:\n            y.append(1)\n        i-=1\n        if(i<=0):\n            break\n    return X,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filenames=get_filenames(TRAIN_DIRECTORY)\nX,Y=load_data(filenames,TRAIN_DIRECTORY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_x=np.array(X)\ntemp_y=np.array(Y).reshape(len(Y),1);\nx_train,x_test,y_train,y_test=train_test_split(temp_x,temp_y);\nx_train=x_train.reshape(x_train.shape[0],-1);\nx_test=x_test.reshape(x_test.shape[0],-1);\nx_train=x_train.T;\nx_test=x_test.T;\nx_train=x_train/255;\nx_test=x_test/255;\ny_train=y_train.T;\ny_test=y_test.T;\nprint(x_train.shape,x_test.shape,y_train.shape,y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_mini_batches(X,Y,mini_batch_size=64):\n    m=X.shape[1];\n    mini_batches=[];\n    permutation=list(np.random.permutation(m))\n    X=X[:,permutation];\n    Y=Y[:,permutation];\n    for i in range(m//mini_batch_size):\n        mini_batch_x=X[:, i*mini_batch_size:(i+1)*mini_batch_size ];\n        mini_batch_y=Y[:, i*mini_batch_size:(i+1)*mini_batch_size ];\n        mini_batches.append((mini_batch_x,mini_batch_y));\n        \n    if m%mini_batch_size!=0:\n        mini_batch_x=X[:, (m//mini_batch_size)*mini_batch_size: ];\n        mini_batch_y=Y[:, (m//mini_batch_size)*mini_batch_size: ];\n        mini_batches.append((mini_batch_x,mini_batch_y));\n    \n    return mini_batches;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"layer_dims=[x_train.shape[0],20,20,7,5,1];\ndef initialize_parameters():\n    parameters={};\n    np.random.seed(1);\n    for i in range(1,len(layer_dims)):\n        parameters['W'+str(i)]=np.random.randn(layer_dims[i],layer_dims[i-1])/np.sqrt(layer_dims[i-1]);\n        parameters['b'+str(i)]=np.zeros((layer_dims[i],1));\n    return parameters;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_forward(A_prev,W,b):\n#     print('W',W);\n#     print('b',b);\n#     print('Input_to_layer',A_prev);\n    Z=np.dot(W,A_prev)+b;\n    linear_cache=(W,b,A_prev);\n    return (Z,linear_cache);\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_activation_forward(A_prev,W,b,activation):\n    Z,linear_cache=linear_forward(A_prev,W,b);\n#     print(\"Z\",Z);\n    A=None;\n    if(activation==\"sigmoid\"):\n        A=1/(1+np.exp(-Z));\n    else:\n        A=np.maximum(0,Z);\n    cache=(linear_cache,Z);\n    return A,cache;\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def L_model_forward(X,parameters):\n    caches=[];\n    A_prev=X;\n    for l in range(1,len(layer_dims)-1):\n        A,temp_cache=linear_activation_forward(A_prev,parameters[\"W\"+str(l)],parameters[\"b\"+str(l)], \"relu\");\n        A_prev=A;\n        caches.append(temp_cache);\n    A,temp_cache=linear_activation_forward(A_prev,parameters[\"W\"+str(len(layer_dims)-1)],parameters[\"b\"+str(len(layer_dims)-1)], \"sigmoid\");\n    caches.append(temp_cache);\n    return A,caches;\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_cost(AL,Y):\n    m = Y.shape[1]\n    cost = -np.sum((Y*np.log(AL) + (1-Y)*np.log(1-AL)),keepdims=True)/m;\n    cost = np.squeeze(cost)\n    return cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_backward(dz,cache):\n    W,b,A_prev=cache;\n    m = A_prev.shape[1]\n    \n#     print(\"W.T\",W.T);\n#     print(\"dz\",dz);\n#     print(\"A_prev.T\",A_prev.T);\n    dA_prev=np.dot(W.T,dz);\n    dw=np.dot(dz,A_prev.T)/m;\n    db=np.sum(dz,axis=1,keepdims=True)/m;\n    return dA_prev,dw,db;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_activation_backward(dA,cache,activation):\n    linear_cache,activation_cache=cache;\n    dz=None;\n    if(activation==\"sigmoid\"):\n#         print(\"Activation_cache\",activation_cache);\n        A=1/(1+np.exp(-activation_cache));\n#         print(\"dA\",dA);\n#         print(\"A\",A);\n        dz=dA*A*(1-A);\n    else:\n        z=activation_cache;\n#         print(\"Activation_cache\",activation_cache);\n        dz=dA.copy();\n        dz[z<=0]=0;\n    return linear_backward(dz,linear_cache);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def L_model_backward(AL,Y,Caches):\n    L=len(layer_dims)-1; # no of layers=3;\n    grads={};\n#     print(\"Y\",Y);\n#     print(\"AL\",AL);\n    dAL=-Y/AL + (1-Y)/(1-AL);\n    current_cache = Caches[L-1];\n    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL,current_cache,\"sigmoid\");\n    for l in reversed(range(L-1)):\n        current_cache = Caches[l];\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+1)],current_cache,\"relu\");\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n    return grads;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_parameters(parameters, grads, learning_rate):\n    L=len(layer_dims)-1; # no of layers=3;\n    for l in range(L):\n        parameters[\"W\" + str(l+1)] =  parameters[\"W\" + str(l+1)]- learning_rate*grads[\"dW\" + str(l+1)]\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def L_Layer_model(X,Y,learning_rate=0.07,epochs=1000,print_cost=False):\n    costs=[];\n    parameters=initialize_parameters();\n    mini_batches=random_mini_batches(X,Y,64);\n    for i in range(0,epochs+1):\n        cost_total=0;\n        for mini_batch in mini_batches:\n            mini_batch_x,mini_batch_y=mini_batch;\n            AL, caches = L_model_forward(mini_batch_x,parameters);\n            cost_total =cost_total+ compute_cost(AL, mini_batch_y)\n            grads = L_model_backward(AL, mini_batch_y, caches);\n            parameters = update_parameters(parameters, grads, learning_rate)\n        cost_total=cost_total/X.shape[1];\n        if(i%100==0):\n            print(\"Cost at\",i,\"epoch\",cost_total);\n        costs.append(cost_total);\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    return parameters;\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = L_Layer_model(x_train, y_train,epochs = 500, print_cost = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(X, y, parameters):\n    \"\"\"\n    This function is used to predict the results of a  L-layer neural network.\n    \n    Arguments:\n    X -- data set of examples you would like to label\n    parameters -- parameters of the trained model\n    \n    Returns:\n    p -- predictions for the given dataset X\n    \"\"\"\n    \n    m = X.shape[1]\n    n = len(parameters) // 2 # number of layers in the neural network\n    p = np.zeros((1,m))\n    \n    # Forward propagation\n    probas, caches = L_model_forward(X, parameters)\n\n    \n    # convert probas to 0/1 predictions\n    for i in range(0, probas.shape[1]):\n        if probas[0,i] > 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n    \n    #print results\n    #print (\"predictions: \" + str(p))\n    #print (\"true labels: \" + str(y))\n    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n        \n    return p","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train = predict(x_train, y_train, parameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_train = predict(x_test, y_test, parameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}