{"cells":[{"metadata":{"id":"Rx-bYbyYZyi2","trusted":false},"cell_type":"code","source":"!pip install -q kaggle","execution_count":null,"outputs":[]},{"metadata":{"id":"-6aiT0rwaFd4","outputId":"4c329a8e-7edb-4dd5-cf0f-f4e8c5a2dbf1","trusted":false},"cell_type":"code","source":"! mkdir ~/.kaggle","execution_count":null,"outputs":[]},{"metadata":{"id":"udR3Q5NVaPuY","trusted":false},"cell_type":"code","source":"! cp kaggle.json ~/.kaggle/","execution_count":null,"outputs":[]},{"metadata":{"id":"BS_9ryGeaUls","trusted":false},"cell_type":"code","source":" !chmod 600 ~/.kaggle/kaggle.json","execution_count":null,"outputs":[]},{"metadata":{"id":"7tralA94aYHG","outputId":"ce11af00-80bd-4d50-edff-4d7ac7bc4549","trusted":false},"cell_type":"code","source":"!kaggle competitions download -c dogs-vs-cats","execution_count":null,"outputs":[]},{"metadata":{"id":"YWUZQXLyabIC","outputId":"e86350be-95dc-4986-9cdc-885c182aa77a","trusted":false},"cell_type":"code","source":"!unzip /content/train.zip\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"id":"Xu0mciBgfS3k","outputId":"d7bfe21f-f04d-4c67-aa1c-90fd03d2941d","trusted":false},"cell_type":"code","source":"%mkdir images","execution_count":null,"outputs":[]},{"metadata":{"id":"Kk-gjWrqfZhF","trusted":false},"cell_type":"code","source":"%mv train images","execution_count":null,"outputs":[]},{"metadata":{"id":"Nz1WeGdSzG0c","trusted":false},"cell_type":"code","source":"class CatDogDataset(torch.utils.data.Dataset):                             #data generating class\n    def __init__(self, file_list, dir, mode='train', transform = None):\n        self.file_list = file_list\n        self.dir = dir\n        self.mode= mode\n        self.transform = transform\n        if self.mode == 'train':\n            if 'dog' in self.file_list[0]:\n                self.label = 1\n            else:\n                self.label = 0\n            \n    def __len__(self):\n        return len(self.file_list)\n    \n    def __getitem__(self, idx):\n        img = Image.open(os.path.join(self.dir, self.file_list[idx]))\n        if self.transform:\n            img = self.transform(img)\n        if self.mode == 'train':\n            img = img.numpy()\n            return img.astype('float32'), self.label\n        else:\n            img = img.numpy()\n            return img.astype('float32'), self.file_list[idx]\n\ntransform1 = transform.Compose([\n    transform.RandomResizedCrop(224),  \n    transform.ToTensor(),\n    transform.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n    ])","execution_count":null,"outputs":[]},{"metadata":{"id":"vNNDdhsxzW_U","trusted":false},"cell_type":"code","source":"train_dir = 'images/train'\ncat_files = []\ndog_files = []\n\ncat_files= [i for i in os.listdir('images/train') if 'cat' in i]\ndog_files = [i for i in os.listdir('images/train') if 'dog' in i]\n\ncats = CatDogDataset(cat_files, train_dir, transform = transform1)\ndogs = CatDogDataset(dog_files, train_dir, transform = transform1)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ZuUPvuWJz5dQ","trusted":false},"cell_type":"code","source":"catdogs = torch.utils.data.ConcatDataset([cats,dogs])","execution_count":null,"outputs":[]},{"metadata":{"id":"LERdeJH44kQz","trusted":false},"cell_type":"code","source":"catdogs\ndataloader = torch.utils.data.DataLoader(catdogs, batch_size = 32, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"2_fdPsNG4t6K","trusted":false},"cell_type":"code","source":"images, labels = iter(dataloader).next()","execution_count":null,"outputs":[]},{"metadata":{"id":"_KENnLKB8NDy","trusted":false},"cell_type":"code","source":"classes = {}\nclasses[1] = 'dog'\nclasses[0] = 'cat'","execution_count":null,"outputs":[]},{"metadata":{"id":"LGZFXSCY8-Mk","outputId":"119533ee-03de-4a2f-f9bc-0de45cc3512b","trusted":false},"cell_type":"code","source":"npimg = images[4].numpy()\nnpimg = np.transpose(npimg,(1,2,0))\n\nstd_corr = np.asarray([0.229, 0.224, 0.225])\nmean = np.asarray([0.485, 0.456, 0.406])\n\nnpimg = np.multiply(npimg,std_corr) + mean\nplt.imshow(npimg)\nprint(classes[labels[4].item()])\nprint(images[1].shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"jCEJQABebA1v"},"cell_type":"markdown","source":"Importing libraries"},{"metadata":{"id":"5P2rKv9ubDsc","trusted":false},"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transform\nimport torchvision.datasets as datasets\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom PIL import Image","execution_count":null,"outputs":[]},{"metadata":{"id":"21IT6SYFh90o","trusted":false},"cell_type":"code","source":"def imshow(images,title):\n  npimg = images.numpy()\n  \n  std_corr = np.asarray((0.229, 0.224, 0.225)).reshape(3,1,1)\n  mean = np.asarray((0.485, 0.456, 0.406)).reshape(3,1,1)\n  \n  npimg = np.multiply(npimg , std_corr) + mean\n  npimg = np.transpose(npimg, (1,2,0))\n\n  plt.figure(figsize=(batchsize*4,4))\n  plt.imshow(npimg)\n  plt.title(title)\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"ultjXkakiPbJ","trusted":false},"cell_type":"code","source":"batchsize=4\ndef show_batch():\n  dataloader = torch.utils.data.DataLoader(catdogs,batch_size=batchsize,shuffle=True)\n  dataiter = iter(dataloader)\n  \n  images,labels = next(dataiter)\n\n  output = model.forward(images)\n  a, pred = torch.max(output.data,1)\n  print(a)\n  imshow(torchvision.utils.make_grid(images),[classes[x.item()] for x in pred])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ERJpCcU1-PDh","trusted":false},"cell_type":"code","source":"rest = torchvision.models.alexnet(pretrained=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"KC9l4-g9B6Nc","outputId":"20bf0ee4-eca7-464c-c31e-3294d4e54854","trusted":false},"cell_type":"code","source":"rest","execution_count":null,"outputs":[]},{"metadata":{"id":"HebVZnpmA5KO","trusted":false},"cell_type":"code","source":"in_features = rest.classifier[6].in_features\nrest.classifier  = nn.Sequential(\n    nn.Dropout(p=0.5,inplace=False),\n    nn.Linear(in_features=9216, out_features=4096, bias=True),\n    nn.ReLU(inplace=True),\n    nn.Dropout(p=0.5, inplace=False),\n    nn.Linear(in_features=4096, out_features=4096, bias=True),\n    nn.ReLU(inplace=True),\n    nn.Linear(in_features=4096, out_features=2, bias=True),\n    nn.Softmax(dim=1)\n)\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"5da6CV7mYxOV","outputId":"22054752-60f7-4442-db5c-20c4e2ebe897","trusted":false},"cell_type":"code","source":"rest","execution_count":null,"outputs":[]},{"metadata":{"id":"yI5luHkQ_Cdy","outputId":"1776870f-ee24-4c2c-9d04-23373da22bed","trusted":false},"cell_type":"code","source":"for params in rest.parameters():\n  if params.requires_grad:\n    print(params.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"QZN-vtaNp8ZH","trusted":false},"cell_type":"code","source":"batch_size = 1\ndef evaluation(dataloader, model):\n    total, correct = 0, 0\n    for data in dataloader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        outputs = nn.functional.softmax(outputs, dim=1)\n        _, pred = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (pred == labels).sum().item()\n    return 100 * correct / total\n","execution_count":null,"outputs":[]},{"metadata":{"id":"hh-2WyDTrRH5","trusted":false},"cell_type":"code","source":"import copy\ndevice = torch.device('cuda:0')\nloss_epc=[]\ndef fit(model,trainloader,opt,loss_fn,epochs):\n  \n  min_loss = 1000\n  for j in tqdm_notebook(range(epochs),total=epochs ,unit= 'epoch'): \n    for i, data in enumerate(trainloader, 0):\n\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        opt.zero_grad()\n\n        outputs = model.forward(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        opt.step()\n        \n      \n        \n        del inputs, labels, outputs\n        torch.cuda.empty_cache()\n        \n        if i%200 == 0:\n          print('For epoch [{}/{}]: Training error after {} iterations is {}'.format(j,epochs,i,loss.item()))\n          \n          if min_loss > loss.item():\n            min_loss = loss.item()\n            best_model = copy.deepcopy(vgg.state_dict())\n            print('Min loss %0.2f' % min_loss)\n\n    print('Loss after {} epochs is {}'.format(j,loss.item()))    \n    loss_epc.append(loss.item())\n        \n    print('The training accuracy is, {}'.format(evaluation(trainloader,model)))\n  \n  plt.plot(loss_epc,'red')\n  plt.xlabel('epochs')\n  plt.ylabel('error')","execution_count":null,"outputs":[]},{"metadata":{"id":"AV1IpLUs8T6e","trusted":false},"cell_type":"code","source":"rest = rest.to(device)\nloss_fn  = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(rest.parameters(),lr=0.001)","execution_count":null,"outputs":[]},{"metadata":{"id":"M4rx8Bby888T","outputId":"87836701-b460-4491-9d1e-897d88de7d40","trusted":false},"cell_type":"code","source":"from tqdm.notebook import tqdm_notebook \nfit(rest,dataloader,optimizer,loss_fn,10)","execution_count":null,"outputs":[]},{"metadata":{"id":"fdxSOlM7ChgD","outputId":"96266533-cecd-48ee-96fa-1b75ee429edc","trusted":false},"cell_type":"code","source":"!unzip /content/test1.zip","execution_count":null,"outputs":[]},{"metadata":{"id":"7zhfnte0NHds","trusted":false},"cell_type":"code","source":"%mkdir images_test\n%mv test1 images_test","execution_count":null,"outputs":[]},{"metadata":{"id":"ykermKF8eDc5","trusted":false},"cell_type":"code","source":"filename_pth = 'ckpt_densenet121_catdog.pth'\ntorch.save(rest.state_dict(), filename_pth)","execution_count":null,"outputs":[]},{"metadata":{"id":"w21ZUe3AeYf5","trusted":false},"cell_type":"code","source":"test_dir = 'images_test/test1'\ntest_files = os.listdir(test_dir)\ntest_transform = transform.Compose([\n    transform.RandomResizedCrop(224),\n    transform.ToTensor()\n])","execution_count":null,"outputs":[]},{"metadata":{"id":"dihT3f39ekDX","trusted":false},"cell_type":"code","source":"testset = CatDogDataset(test_files,test_dir,mode ='test',transform=test_transform)","execution_count":null,"outputs":[]},{"metadata":{"id":"fXdD8FwIfDV-","trusted":false},"cell_type":"code","source":"test_loader = torch.utils.data.DataLoader(testset,batch_size=32,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"iUSBGErGfNCH","trusted":false},"cell_type":"code","source":"rest.eval()\nfn_list,pred_list = [], []\nfor x,fn in test_loader:\n  with torch.no_grad():\n    x = x.to(device)\n    out = rest.forward(x)\n    pred = torch.argmax(out.data,1)\n    fn_list += [n[:-4] for n in fn]\n    pred_list += [p.item() for p in pred]","execution_count":null,"outputs":[]},{"metadata":{"id":"ivaLAbn-gF3r","trusted":false},"cell_type":"code","source":"import pandas as pd\nsubmission = pd.DataFrame({'id':fn_list,'label':pred_list})\n","execution_count":null,"outputs":[]},{"metadata":{"id":"NT9017EZgHVO","trusted":false},"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"4L500iEqibCN","trusted":false},"cell_type":"code","source":"/cat\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}