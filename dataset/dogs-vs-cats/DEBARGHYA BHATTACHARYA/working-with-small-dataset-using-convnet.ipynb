{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os, shutil\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unziping the train zip and test zip file after copying them to the working folder \n# remember in kaggle input folder have been given only read only acess by the admin\n# hence we copy in the working folder and then complete our job\n!rm -rf ./train ./cats_and_dogs_small ./train.zip ./cats_and_dogs_small_2.h5 ./cats_and_dogs_small_1.h5\n# making the copy of the test and train dir from the input dir since it is read-only\n!ls -lrt\n# !cp /kaggle/input/dogs-vs-cats/test1.zip /kaggle/working\n# !unzip /kaggle/working/test1.zip \n!cp /kaggle/input/dogs-vs-cats/train.zip /kaggle/working\n!unzip /kaggle/working/train.zip\n!cd /kaggle/working\n# use this block wisely","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the contents of my working directory\n# use this wisely\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n!ls -lrt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First lets make the small dataset we will be working with. Because here our goal is to work with a smaller dataset  create a new\ndataset containing three subsets: a training set with 1,000 samples of each class, a validation set with 500 samples of each class, and a test set with 500 samples of each class. The original datset consists 25,000 images of dogs and cats (12,500 from each class) and is 543 MB (compressed). But since our goal is to use a smaller dataset we will resist the temptation of using it.","metadata":{}},{"cell_type":"code","source":"!cd /kaggle/working\n!ls -lrt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first lets set the base paths and make the necessary directories for working out our problem\noriginal_dataset_dir = '/kaggle/working'\nbase_dir = '/kaggle/working/cats_and_dogs_small'\nos.mkdir(base_dir)\ntrain_dir = os.path.join(base_dir,'train')\nos.mkdir(train_dir)\nvalidation_dir = os.path.join(base_dir, 'validation')\nos.mkdir(validation_dir)\ntest_dir = os.path.join(base_dir, 'test')\nos.mkdir(test_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check using the console the directories at this stage.\n\nDo a 'cd /cats_and_dogs_small' and then run the next block.","metadata":{}},{"cell_type":"code","source":"# gives you the content of the newly made folder\n!ls -lrt\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now go to console opnce again and then do a 'cd ..'","metadata":{}},{"cell_type":"code","source":"# now lets make the cats and dogs directories in them for housing the cats and the dogs images\ntrain_cats_dir = os.path.join(train_dir, 'cats')\nos.mkdir(train_cats_dir)\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')\nos.mkdir(train_dogs_dir)\nvalidation_cats_dir = os.path.join(validation_dir, 'cats')\nos.mkdir(validation_cats_dir)\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')\nos.mkdir(validation_dogs_dir)\ntest_cats_dir = os.path.join(test_dir, 'cats')\nos.mkdir(test_cats_dir)\ntest_dogs_dir = os.path.join(test_dir, 'dogs')\nos.mkdir(test_dogs_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now go and check from the console if the directories are made. How ? \nIf you know unix you know how.","metadata":{}},{"cell_type":"code","source":"# Now we will be copying the actual data from the base directory to the working directories\nfnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\nfor fname in fnames:\n    src = os.path.join(os.path.join(original_dataset_dir,'train'), fname)\n    dst = os.path.join(train_cats_dir, fname)\n    shutil.copyfile(src, dst)\nfnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\nfor fname in fnames:\n    src = os.path.join(os.path.join(original_dataset_dir,'train'), fname)\n    dst = os.path.join(validation_cats_dir, fname)\n    shutil.copyfile(src, dst)\nfnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\nfor fname in fnames:\n    src = os.path.join(os.path.join(original_dataset_dir,'train'), fname)\n    dst = os.path.join(test_cats_dir, fname)\n    shutil.copyfile(src, dst)\nfnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\nfor fname in fnames:\n    src = os.path.join(os.path.join(original_dataset_dir,'train'), fname)\n    dst = os.path.join(train_dogs_dir, fname)\n    shutil.copyfile(src, dst)\nfnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\nfor fname in fnames:\n    src = os.path.join(os.path.join(original_dataset_dir,'train'), fname)\n    dst = os.path.join(validation_dogs_dir, fname)\n    shutil.copyfile(src, dst)\nfnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\nfor fname in fnames:\n    src = os.path.join(os.path.join(original_dataset_dir,'train'), fname)\n    dst = os.path.join(test_dogs_dir, fname)\n    shutil.copyfile(src, dst)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets check the length of the dat if they are being properly transferred.","metadata":{}},{"cell_type":"code","source":"# checking the that data have been properly copied or not\nprint('total training cat images:', len(os.listdir(train_cats_dir)))\nprint('total training dog images:', len(os.listdir(train_dogs_dir)))\nprint('total validation cat images:', len(os.listdir(validation_cats_dir)))\nprint('total validation dog images:', len(os.listdir(validation_dogs_dir)))\nprint('total test cat images:', len(os.listdir(test_cats_dir)))\nprint('total test dog images:', len(os.listdir(test_dogs_dir)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now for ease of use I will do something\n# since the train folder contains too many images which we don't need I will delete it.\n!rm -rf ./train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf ./train.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay Now we have the required dataset. Now I will start with the actual chapter. And before doing that I will actual save the version because saving the file commits the file in kaggle/working. And if you don't their is good chance you can loose the files.","metadata":{}},{"cell_type":"markdown","source":"# DOG vs CAT classifier using a ConvNet\n\nWe previously built a small ConvNet for MNIST dataset. We wil go ahead with a similar structure for our networkover here too only since we are dealing with a bigger images and a more complex problem , so we will make our network larger than last time by just adding another pair of Conv2D and MaxPooling2D layers. This serves both to augment the capacity of the network and to further reduce the size of the feature maps so they aren’t overly large when you reach the Flatten layer. Here, because you start from inputs of size 150 × 150 (a somewhat arbitrary choice), you end up with feature maps of size 7 × 7 just before the Flatten layer. \n\nAlso Note The depth of the feature maps progressively increases in the network(from 32 to 128), whereas the size of the feature maps decreases (from 148 × 148 to 7 × 7). This is a pattern you’ll see in almost all convnets.\n\nBecause you’re attacking a binary-classification problem, you’ll end the network with a single unit (a Dense layer of size 1) and a sigmoid activation. This unit will encode the probability that the network is looking at one class or the other.\n\nOkay now lets start with defining the network","metadata":{}},{"cell_type":"code","source":"# defining the network\n# importing the layers and models sub- module from keras library\nfrom keras import layers,models\n\nmodel = models.Sequential() # using a sequentail class to make the model\n\n# defining the first layer which will be a Convulational layer with a feature map size 32 (filters) over a filter of 3 x 3 \n# and the input shape will be that of size (150,150,3) which encodes the length breadth and channels of the input image.\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(150, 150, 3)))\n# just like last time the second layer will be a maxpooling layer of size (2,2) which will agressively downsample \n# the output of the conv layer\nmodel.add(layers.MaxPooling2D((2, 2)))\n# we will be repeating the same thing for a few times with a increasing \n# feature map learning new features from existing features\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\n# downsampling again in the fourth layer\nmodel.add(layers.MaxPooling2D((2, 2)))\n# repeating same two steps\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\n# and one final time\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\n\n# Now lets define the classifier\n# and we start with a flatten layer transforming the output of the base from a 3D Tensor to 1D tensor\nmodel.add(layers.Flatten())\n# followed by a dense layer with 512 nodes \nmodel.add(layers.Dense(512, activation='relu'))\n# ending with sigmoid activated 1 noded classifing final layer\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\n# checking the model Summary Now\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the compilation step, you’ll go with the RMSprop optimizer, as usual. Because you ended the network with a single sigmoid unit, you’ll use binary crossentropy as the loss .","metadata":{}},{"cell_type":"code","source":"# compiling the network\n\nfrom keras import optimizers\nmodel.compile(loss='binary_crossentropy',\n              optimizer=optimizers.RMSprop(lr=1e-4),\n              metrics=['acc'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing\n\nAs you know by now, data should be formatted into appropriately preprocessed floatingpoint tensors before being fed into the network. Currently, the data sits on a drive as JPG files, so the steps for getting it into the network are roughly as follows:\n1.  Read the picture files.\n2.  Decode the JPEG content to RGB grids of pixels.\n3.  Convert these into floating-point tensors.\n4.  Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know, neural networks prefer to deal with small input values).\n\n\nIt may seem a bit daunting, but fortunately Keras has utilities to take care of these steps automatically. Keras has a module with image-processing helper tools, located at keras.preprocessing.image. In particular, it contains the class ImageDataGenerator, which lets you quickly set up Python generators that can automatically turn image files on disk into batches of preprocessed tensors. This is what you’ll use here. ","metadata":{}},{"cell_type":"code","source":"# Using ImageDataGenerator to read images from directories\n# if you don't know about generators go check it in the generators in python documentation\n\nfrom keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    target_size=(150, 150),\n                                                    batch_size=20,\n                                                    class_mode='binary')\nvalidation_generator = test_datagen.flow_from_directory(validation_dir,\n                                                        target_size=(150, 150),\n                                                        batch_size=20,\n                                                        class_mode='binary')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let’s look at the output of one of these generators: it yields batches of 150 × 150 RGB images (shape (20, 150, 150, 3)) and binary labels (shape (20,)). There are 20 samples in each batch (the batch size). Note that the generator yields these batches indefinitely: it loops endlessly over the images in the target folder. For this reason, you need to break the iteration loop at some point.","metadata":{}},{"cell_type":"code","source":"# checking the shape of the output from this generators\nfor data_batch, labels_batch in train_generator:\n    print('data batch shape:', data_batch.shape)\n    print('labels batch shape:', labels_batch.shape)\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let’s fit the model to the data using the generator. You do so using the fit_generator method, the equivalent of fit for data generators like this one. It expects as its first argument a Python generator that will yield batches of inputs and targets indefinitely, like this one does. Because the data is being generated endlessly, the Keras model needs to know how many samples to draw from the generator before declaring an epoch over. This is the role of the steps_per_epoch argument: after having drawn steps_per_epoch batches from the generator—that is, after having run for steps_per_epoch gradient descent steps—the fitting process will go to the next epoch. \n\nJust remember it is nothing but the number of batches and batch size is the number of samples in the batches. The relationship actually goes as train_length // batch_size\n\nIn this case, batches are 20 samples, so it will take 100 batches until you see your target of 2,000 samples.\n\nWhen using fit_generator, you can pass a validation_data argument, much as with the fit method. It’s important to note that this argument is allowed to be a data generator, but it could also be a tuple of Numpy arrays. If you pass a generator as validation_data, then this generator is expected to yield batches of validation data endlessly; thus you should also specify the validation_steps argument, which tells the process how many batches to draw from the validation generator for evaluation. ","metadata":{}},{"cell_type":"code","source":"# Fitting the model using a batch generator\nhistory = model.fit_generator(train_generator,\n                              steps_per_epoch=100,\n                              epochs=30,\n                              validation_data=validation_generator,\n                              validation_steps=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we will be doing something we haven't done previously\n# its a simple practice but it is a good practice.\n# So lets save our model\nmodel.save('cats_and_dogs_small_1.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let’s plot the loss and accuracy of the model over the training and validation data during training.","metadata":{}},{"cell_type":"code","source":"# Displaying curves of loss and accuracy during training\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These plots are characteristic of overfitting. The training accuracy increases linearly over time, until it reaches nearly 100%, whereas the validation accuracy stalls at 70–72%. The validation loss reaches its minimum after only five epochs and then stalls, whereas the training loss keeps decreasing linearly until it reaches nearly 0. Because you have relatively few training samples (2,000), overfitting will be your number-one concern. You already know about a number of techniques that can help mitigate overfitting, such as dropout and weight decay (L2 regularization). We’re now going to work with a new one, specific to computer vision and used almost universally when processing images with deep-learning models: data augmentation. ","metadata":{}},{"cell_type":"markdown","source":"# Using data augmentation\nOverfitting is caused by having too few samples to learn from, rendering you unable to train a model that can generalize to new data. Given infinite data, your mode would be exposed to every possible aspect of the data distribution at hand: you would\nnever overfit. Data augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data and generalize better. In Keras, this can be done by configuring a number of random transformations to be performed on the images read by the ImageDataGenerator instance. Let’s get started with an example. ","metadata":{}},{"cell_type":"code","source":"datagen = ImageDataGenerator(rotation_range=40,\n                             width_shift_range=0.2,\n                             height_shift_range=0.2,\n                             shear_range=0.2,\n                             zoom_range=0.2,\n                             horizontal_flip=True,\n                             fill_mode='nearest')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These are just a few of the options available (for more, see the Keras documentation).\n\n* rotation_range is a value in degrees (0–180), a range within which to randomly rotate pictures.\n* width_shift and height_shift are ranges (as a fraction of total width orheight) within which to randomly translate pictures vertically or horizontally.\n* shear_range is for randomly applying shearing transformations.\n* zoom_range is for randomly zooming inside pictures.\n* horizontal_flip is for randomly flipping half the images horizontally—relevant when there are no assumptions of horizontal asymmetry (for example,real-world pictures).\n* fill_mode is the strategy used for filling in newly created pixels, which canappear after a rotation or a width/height shift.\n\nLet’s look at the augmented images\n","metadata":{}},{"cell_type":"code","source":"# checking the augmented images by plotting them using matplotlib\n# after converting them into tensors and then augmenting them\nfrom keras.preprocessing import image\n\nfnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n\nimg_path = fnames[20]\nimg = image.load_img(img_path, target_size=(150, 150))\n\nx = image.img_to_array(img)\nx = x.reshape((1,) + x.shape)\n\ni = 0\nfor batch in datagen.flow(x, batch_size=1):\n    plt.figure(i)\n    imgplot = plt.imshow(image.array_to_img(batch[0]))\n    i += 1\n    if i % 4 == 0:\n        break\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I guess now we all understand what data augmentation stands for.\n\nNow if we train a new network using this data-augmentation configuration, the network will never see the same input twice. But the inputs it sees are still heavily intercorrelated, because they come from a small number of original images—you can’t produce new information, you can only remix existing information. As such, this may not be enough to completely get rid of overfitting. To further fight overfitting, you’ll also add a Dropout layer to your model, right before the densely connected classifier.\n\nSo come on lets configure a new network again with data augmentation.","metadata":{}},{"cell_type":"code","source":"# defining the new network\n\nmodel = models.Sequential() # calling the sequential class\n\n# making the convnet just as before\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu',\n                        input_shape=(150, 150, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\n# model.add(layers.BatchNormalization())\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\n\n# defining the classifier same as before\nmodel.add(layers.Flatten())\n# adding the dropout layer as discussed \nmodel.add(layers.Dropout(0.5))\n# adding a batch normalization layer to normalize\n# the flactuations in the output analysis curve\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',\n              optimizer=optimizers.RMSprop(lr=1e-4),\n              metrics=['acc'])\n\n# checking in the model summary\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let’s train the network using data augmentation and dropout.","metadata":{}},{"cell_type":"code","source":"import math\nBATCH_SIZE=40\n\nTRAINING_SIZE = 2000\n\nVALIDATION_SIZE = 1000\n\n# We take the ceiling because we do not drop the remainder of the batch\ncompute_steps_per_epoch = lambda x: int(math.ceil(1. * x / BATCH_SIZE))\n\nsteps_per_epoch = compute_steps_per_epoch(TRAINING_SIZE)\nval_steps = compute_steps_per_epoch(VALIDATION_SIZE)\nprint(steps_per_epoch,val_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating the data augmentation generator classes\n\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n                                   rotation_range=40,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   horizontal_flip=True,)\n\n# note that we don't apply data augmentation to the test because that will be useless\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# and defining the training generator\n\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    target_size=(150, 150),\n                                                    batch_size=32,\n                                                    class_mode='binary')\n\n# and defining the validation generator\n\nvalidation_generator = test_datagen.flow_from_directory(validation_dir,\n                                                        target_size=(150, 150),\n                                                        batch_size=32,\n                                                        class_mode='binary')\n\n# training the new model\n\nhistory = model.fit_generator(train_generator,\n                              steps_per_epoch=50,\n                              epochs=100,\n                              validation_data=validation_generator,\n                              validation_steps=25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let’s save the model","metadata":{}},{"cell_type":"code","source":"# saving the second model\n\nmodel.save('cats_and_dogs_small_2.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And let’s plot the results again. ","metadata":{}},{"cell_type":"code","source":"# ploting the graphs \n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By using regularization techniques even further, and by tuning the network’s parameters (such as the number of filters per convolution layer, or the number of layers in the network), you may be able to get an even better accuracy, likely up to 86% or 87%. So keep trying, also try using batch normalization, it may prove good in order to reduce the fluctuations in the analysis curve.","metadata":{}}]}