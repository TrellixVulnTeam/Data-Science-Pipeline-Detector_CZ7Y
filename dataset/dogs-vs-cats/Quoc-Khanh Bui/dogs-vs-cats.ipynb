{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\ndef list_files(path):\n    files = []\n    for dirname, _, filenames in os.walk(path):\n        for filename in filenames:\n            name = os.path.join(dirname, filename)\n            files.append(name)\n    return files","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nlist_files('/kaggle/input')\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import zipfile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unzip(zip_path, unzip_path):\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(unzip_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pwd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_name = 'dogs-vs-cats'\ntest_zip_path = f'/kaggle/input/{dataset_name}/test1.zip'\ntrain_zip_path = f'/kaggle/input/{dataset_name}/train.zip'\ntest_unzip_path = '.' #unzip to working space\ntrain_unzip_path = '.' #unzip to working space\nunzip(test_zip_path, test_unzip_path)\nunzip(train_zip_path, train_unzip_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_files('.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label = pd.read_csv(f'/kaggle/input/{dataset_name}/sampleSubmission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_label = label.to_numpy()\nid_label.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"label = 1 for dog and label = 0 for cat","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image_size = (64, 64)\ndef read_data(path):\n    data = np.array([])\n    Y = np.array([])\n    from PIL import Image\n    from numpy import asarray\n    file_names = list_files(path)\n    for file_name in file_names[:100]:\n        img = Image.open(file_name)\n        img = img.resize(image_size) #shape (64, 64, 3)\n        img = asarray(img)\n        img = np.expand_dims(img, axis = 0) #shape (1, 64, 64, 3)\n\n        if not data.any():\n            data = img\n        else:\n            data = np.append(data, img, axis=0)\n        l = 1 # default is dog\n        if file_name.find('cat') > 0:\n            l = 0\n        print(file_name, ' ', l)\n        Y = np.append(Y, [l])\n\n    X = data.reshape(data.shape[0], -1).T    \n    return data, X, Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train, X_train, Y_train = read_data('./train')\ndata_test, X_test, Y_test = read_data('./test1')\nprint(data_train.shape)\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ni =36\n\n\n\nplt.imshow(X_train[:,i].reshape(64,64,3))\nprint(Y_train[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(Z):\n    \"\"\"\n    Z can be a scalar, vector or matrix\n    \"\"\"\n    result = 1/(1 + np.exp(-Z))\n    return result, Z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid_backward(Z):\n    A, _ = sigmoid(Z)\n    return A*(1-A)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def relu(Z):\n    new_Z = np.maximum(Z, 0)\n    return new_Z, Z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def relu_backward(Z):\n    A, _ = relu(Z)\n    A[A > 0] = 1\n    return A","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_parameters(layer_dims):\n    \"\"\"\n    layer_dims contains the number of unit for each layer in network, start with input layer\n    \"\"\"\n    np.random.seed(3)\n    parameters = {}\n    L = len(layer_dims)\n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ninitialize_parameters([3,4,5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_forward(A, W, b):\n    Z = W.dot(A) + b\n    cache = (A, W, b)\n    assert(Z.shape == (W.shape[0], A.shape[1]))\n    return Z, cache","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_forward_activation(A_prev, W, b, activation):\n    \"\"\"\n    A_prev is vector A of last layer\n    \n    activation has two types: relu or sigmoid\n    \"\"\"\n    if activation == 'relu':\n        Z, linear_cache = linear_forward(A_prev, W, b) # linear_cache contains A,W, b\n        A, activation_cache = relu(Z) # activation_cache contains Z\n    if activation == 'sigmoid':\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n    cache = (linear_cache, activation_cache)    \n    return A, cache","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def L_model_forward(X, parameters):\n    A = X\n    L = len(parameters) // 2 # number of layer. because each layer except input layer has two params, W and b\n    caches = [] # list contains cache for each layer\n    # from layer 1 to layer L-1 (relu layer)\n    for l in range(1, L):\n        A_prev = A\n        W = parameters['W' + str(l)]\n        b = parameters['b' + str(l)]\n        A, cache = linear_forward_activation(A_prev, W, b, 'relu')\n        caches.append(cache)\n    # for last layer (sigmoid layers)\n    W = parameters['W' + str(L)]\n    b = parameters['b' + str(L)]\n    AL, cache = linear_forward_activation(A, W, b, 'sigmoid')\n    caches.append(cache)\n    assert(AL.shape == (1, X.shape[1]))\n    return AL, caches","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_forward_model():\n    X = np.array([[1,2,3,4],[4,5,6,7],[7,8,9, 10]])\n    Y = np.array([[1,0,1,0]])\n    parameters = initialize_parameters([3,4,1])\n    print(parameters)\n    AL, caches = L_model_forward(X, parameters)\n    print(AL)\ntest_forward_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_cost(AL, Y):\n    \"\"\"\n    AL is column vector of A in last layer\n    \"\"\"\n    m = Y.shape[0] # number of data point\n    cost = -1/m*np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL))\n    cost = cost.squeeze()\n    assert(cost.shape == ())\n    return cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( Y_train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(compute_cost(np.array([[0.50008811,0.50014391,0.50019507,0.50024622]]), np.array([1,0,1,1])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_backward(dZ, cache):\n    \"\"\"\n    with a specific layer,\n    dZ is derrivative of loss function (J) respect to Z of this layer\n    cache is linear_cache which contains A_prev, W, b\n    \"\"\"\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n    \n    dW = 1/m*dZ.dot(A_prev.T)\n    db = 1/m*np.sum(dZ, axis=1, keepdims=True)\n    dA_prev = W.T.dot(dZ)\n    \n    assert (dA_prev.shape == A_prev.shape)\n    assert (dW.shape == W.shape)\n    assert (db.shape == b.shape)\n    \n    return dA_prev, dW, db","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_linear_backward():\n    X = np.array([[1,2,3,4],[4,5,6,7],[7,8,9, 10]])\n    Y = np.array([[1,0,1,0]])\n    parameters = initialize_parameters([3,4,1])\n    AL, caches = L_model_forward(X, parameters)\n    \n    linear, activation = caches[1]\n    A, W, b = linear\n    Z = activation\n    dA_prev, dW, db = linear_backward(Z, linear)\n    assert(dA_prev.shape == A.shape)\n    assert(dW.shape == W.shape)\n    assert(db.shape == b.shape)\ntest_linear_backward()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_backward_activation(dA, cache, activation):\n    \"\"\"\n    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n    \n    Arguments:\n    dA -- post-activation gradient for current layer l \n    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n    \n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    linear_cache, activation_cache = cache\n    Z = activation_cache\n    A, W, b = linear_cache\n    if activation == 'relu':\n        dZ = dA*relu_backward(Z)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    if activation == 'sigmoid':\n        dZ = dA*sigmoid_backward(Z)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    #print(dA.shape)\n    #print(dZ.shape)\n    #print(Z.shape)\n    \n    \n    return dA_prev, dW, db","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_linear_backward_activation():\n    X = np.array([[1,2,3,4,5,6,7,8,9,0],[1,2,3,4,5,6,7,8,9,0]])\n    Y = np.array([[1,0,1,0,1,0,0,0,1,1]])\n    parameters = initialize_parameters([2,5,3,1])\n    AL, caches = L_model_forward(X, parameters)\n    \n    linear, activation = caches[0]\n    A_prev, W, b = linear\n    Z = activation\n    dA_prev, dW, db = linear_backward_activation(sigmoid(Z)[0], caches[0], 'sigmoid')\n    assert(dA_prev.shape == A_prev.shape)\n    assert(dW.shape == W.shape)\n    assert(db.shape == b.shape)\n    assert(True == False)\ntest_linear_backward_activation()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def L_model_backward(AL, Y, caches):\n    \n    L = len(caches)\n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape)\n    grads = {} # contains dW, db for each layer\n    #derrivative of loss function respects to A of last layer\n    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n    #last layer - sigmoid layer\n    dA_prev, dW, db = linear_backward_activation(dAL, caches[L-1], 'sigmoid')\n    grads['dA' + str(L-1)] = dA_prev\n    grads['dW' + str(L)] = dW\n    grads['db' + str(L)] = db\n    \n    #from L-1 layer back to layer 1\n    for l in reversed(range(L-1)):\n        dA_prev, dW, db = linear_backward_activation(grads['dA' + str(l+1)], caches[l], 'relu')\n        grads['dA' + str(l)] = dA_prev\n        grads['dW' + str(l+1)] = dW\n        grads['db' + str(l+1)] = db\n    return grads","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_paramenters(parameters, grads, learning_rate):\n    L = len(parameters) // 2\n    for l in range(1, L+1):\n        parameters['W' + str(l)] -= learning_rate*grads['dW' + str(l)]\n        parameters['b' + str(l)] -= learning_rate*grads['db' + str(l)]\n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 64x64x3=12288\n\n\ndef L_layer_model(X, Y, layer_dims, num_iterations=100, learing_rate=0.01):\n    np.random.seed(1)\n    costs = []\n    parameters = initialize_parameters(layer_dims)\n    for i in range(epochs):\n        AL, caches = L_model_forward(X, parameters)\n        cost = compute_cost(AL, Y)\n        grads = L_model_backward(AL, Y, caches)\n        parameters = update_paramenters(parameters, grads, learning_rate)\n        \n        if i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n        if i % 100 == 0:\n            costs.append(cost)\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_nums = image_size[0] * image_size[1] * 3\nlayer_dims = [feature_nums,20, 7, 5, 1] #  4-layer model]\nlearning_rate = 0.0075\nepochs = 3000\nX_train = X_train / 255\nX_test = X_test / 255\nparameters = L_layer_model(X_train[:,:], Y_train[:], layer_dims, epochs, learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(X, Y, parameters):\n    L = len(parameters) // 2\n    AL, caches = L_model_forward(X, parameters)\n    AL[AL > 0.5] = 1\n    AL[AL <= 0.5] = 0\n    print(AL)\n    print(Y)\n    accurary = np.sum(AL == Y)/len(Y)\n    return accurary\nprint(test(X_train, Y_train, parameters))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}