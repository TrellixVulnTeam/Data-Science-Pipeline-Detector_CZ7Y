{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from io import StringIO\n\nNvals = 4\nFrac = 0.05\n\n# Tells notebook to display all rows or columns instead of truncating\npd.options.display.max_rows = 100\npd.options.display.max_columns = 100\n\n# read_random()\n#\n# Quickly read a dataframe from a file.  Assumes that there is a header.\n# Reading into a buffer, then read_csv from the buffer, is much faster than reading directly from the file\n# https://stackoverflow.com/questions/38233719/reading-random-rows-of-a-large-csv-file-python-pandas\n#\n# writeheader = should ordinarily be set to True.  A case where you might not want to\n#      write the header is if you have already recorded it, and you are starting from the middle of the file.\n# minrow = first row to read.  (0 = start from beginning.)\n# maxrow = max number of rows to read from csv file (rows are from 0 to maxrow - 1)\n#          it's okay if the file contains fewer than maxrow rows.\n# prob = probability of including each row\n# num_reports = number of times to report progress\n# kwargs = to be passed to read_csv() function that parses the csv data.\n#\ndef read_random(filename, writeheader = True, minrow = 0, maxrow = 100, prob = 0.12345, num_reports = 10, **kwargs):\n\n    rowcount = -1\n    rowcount_div = 0\n    start = True\n    s_buf = StringIO()\n    \n    with open(filename) as file:\n        for line in file:\n            if rowcount == -1: # get header\n                rowcount += 1\n                if writeheader:\n                    s_buf.write(line)\n            elif rowcount < minrow: # skip all rows before minrow\n                rowcount += 1\n                continue\n            elif rowcount >= maxrow: # stop at maxrow\n                break\n            else: # in between minrow and maxrow\n                rowcount += 1\n                if np.random.rand() < prob:\n                    s_buf.write(line)\n                if rowcount_div >= (maxrow - minrow) // num_reports - 1:\n                    print(\"At row: {}\".format(rowcount))\n                    rowcount_div = 0\n                else:\n                    rowcount_div += 1\n    print(\"Done!  Rowcount is: {}\".format(rowcount))\n    s_buf.seek(0)\n    return pd.read_csv(s_buf, **kwargs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"3b52770d558e71d5366ee8ef3f1dbd7b4b1affae"},"cell_type":"code","source":"from collections import OrderedDict\ndef make_dummies_from_df_train(df_train, Nvals = 5):\n\n    cols = list()\n    dm = None\n    cat_columns = OrderedDict()\n    expand_list = ('EngineVersion', 'AppVersion', 'AvSigVersion', 'OsVer', 'OsBuildLab', 'Census_OSVersion')\n    # TEST expand_list = ('A')\n    \n    for col in df_train.columns:\n        if col in expand_list:\n            df_train = pd.concat((df_train, col_split(df_train, col)), axis=1)\n        \n    # This loop goes through all columns in df_train of types \"object\"\n    # In ech column, it sets cat_columns[col] to a list of the\n    # Nvals most common values (or less, if less values exist.)\n    # It then gets the dummies for each column for those NVals values and concatenates them.\n    # The return value is the concatenated dummy values.\n    for col in df_train.columns:\n        if col in expand_list:\n            continue\n        if df_train.dtypes[col] == 'object':\n            # We actually process each column twice:\n            # once to get the value counts to see which are the three most common values\n            # and once to make a get_dummies version of it.  cat_columns has to store\n            # the ordering produced by the latter.\n            s = df_train[col].value_counts()\n            if s.size > 1000:\n                print(\"Column {} is long; skipping it.\".format(col))\n                continue\n            if s.size > Nvals:\n                s_temp = df_train[col].replace(list(s.iloc[Nvals:].index), np.nan, inplace=False)\n            else:\n                s_temp = df_train[col]\n            dmcol = pd.get_dummies(s_temp, prefix = col, prefix_sep=\"_\")\n            vals = [x[len(col) + 1:] for x in list(dmcol.columns)]\n            cat_columns[col] = vals\n            if dm is None:\n                dm = dmcol\n            else:\n                dm = pd.concat([dm, dmcol], axis=1)\n            del dmcol\n    return dm, cat_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a561bf60756b8f8595cc4c7bd807539beb42390","scrolled":true},"cell_type":"code","source":"# Pipeline: scale and impute data\nfrom sklearn.preprocessing import StandardScaler\n\n# Set stdscale to true if you want to standardize the data and fill nans with zero.\n# Set stdscale to false if you just want to fill nans with -1 and leave the scaling alone.\ndef ScaleImpute(X, dummy_frame, stdscale = True):\n    # numer_columns = list(X.columns)\n    # X_num is a numerical only version of X, where categorical columns have been converted\n    # to dummies.  It may be scaled or unscaled.\n    if stdscale:\n        scaler = StandardScaler()\n        X_num = np.nan_to_num(scaler.fit_transform(pd.concat([X.astype(float), dummy_frame.astype(float)], axis = 1)))\n    else:\n        X_num = pd.concat([X.astype(float).fillna(-1), dummy_frame.astype(float)], axis = 1)\n    return X_num\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\ndef test_roc_auc(X_num, y, clf):\n    X_train, X_test, y_train, y_test = train_test_split(X_num, y, test_size=0.33, random_state=42)\n\n    #sgd_clf = SGDClassifier(loss = 'log', penalty='l1')\n    #sgd_clf.fit(X_num, y)\n\n    print(\"About to fit.\")\n    clf.fit(X_train, y_train)\n    print(\"About to predict_proba()\")\n    y_predict = clf.predict_proba(X_test)\n    print(\"About to compute roc_auc\")\n    print(\"roc_auc:{}\".format(roc_auc_score(y_test, y_predict[:,1])))\n    return(clf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8305b5be56fd9c74aed232db86a8bf8489cad50a"},"cell_type":"code","source":"def getnth(x, n):\n    try:\n        return x.split('.')[n]\n    except Exception:\n        print(\"Warning: {} cannot be split; part #{} will be NULL.\".format(x, n))\n        return \"NULL\"\n\ndef col_split(df, col):\n    lst = list()\n    names = list()\n    numcols = len(df[col].iloc[0].split('.'))\n    for n in range(0, numcols):\n        lst.append(df[col].apply(lambda x: getnth(x, n)).astype('object').rename(col + str(n)))\n        names.append(col + str(n))\n    return pd.concat(lst, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65047d802407cf948d0678fb0eb6af8518858258"},"cell_type":"code","source":"os.system('echo read_random')\ndf_train = read_random(\"../input/train.csv\", minrow = 0, maxrow = 20000000, prob = Frac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e72303b724860c534899693efab28e30eb349d5"},"cell_type":"code","source":"# TEST df_train = pd.DataFrame({'A': ['1.2.3', '4.5.6', '7.8.9'], 'B': [4, 5, 6], 'HasDetections': [1, 1, 0]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9b8a86eee0be5fd8fbc1aad277112fcc957645c"},"cell_type":"code","source":"os.system('echo make_dummies_from_df_train')\ndummy_frame, cat_columns = make_dummies_from_df_train(df_train, Nvals = Nvals)\n#for k, v in cat_columns.items():\n#    print(k, v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08410437902ee3ae231c6136ff575b408b1a8966"},"cell_type":"code","source":"#dummy_frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d707c5cc36d5fa35579ee22427e483edfa1bbf1d"},"cell_type":"code","source":"cat_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39c5c29e5829457aedebac7badd19d3192b63e6d"},"cell_type":"code","source":"X = df_train.drop('HasDetections', axis = 1)\nnumer_columns = X.select_dtypes(exclude=['object']).columns\ny = df_train.HasDetections","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a93e5de8159624c57fb1fd4e8c4648323fdda1f6"},"cell_type":"code","source":"numer_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"caca934920d2ddeb6ee7fbfe9399989fb46effa1"},"cell_type":"code","source":"X_num = ScaleImpute(X.select_dtypes(exclude=['object']), dummy_frame, stdscale = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"134e1fef4fd89e5afac1fd8c5c0f6e113da25845"},"cell_type":"code","source":"X_num","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72ecfaa30b89fb016437aa089cc19cc83d1de09e"},"cell_type":"code","source":"import scipy.stats\n\nlst = list()\ntot = 0\n\n# This code could be used to test whether columns M and N in the dummies\n# correspond to the same feature, i.e.\n# get_feature_number(div_columns, N) == get_feature_number(div_columns, M)\n# I don't think it's necessary to check this, however.  These products aren't\n# \"cheating\" in any sense, and there are relatively few of them.\n#div_columns = [1 for x in range(len(numer_columns))] + [len(x) for x in cat_columns.values()]\n#def get_feature_number(div_columns, N):\n#    dc_cpy = div_columns.copy()\n#    total = 0\n#    feature_number = 0\n#    while total < N:\n#        total += dc_cpy.pop(0)\n#        feature_number += 1\n#    return feature_number\n\nos.system('echo compute correlations')\n# start: 2:38 PM\n# Go through all pairs (N, M) with N >= M\n# Multiply feature N by feature M elementwise to get the interaction term columns\n# Find the interaction term column's correlation with HasDetections\nfor N in range(0, X_num.shape[1]):\n    for M in range(0, N + 1):\n        c = scipy.stats.pearsonr(X_num[:,N] * X_num[:,M], df_train.HasDetections)[0]\n        if pd.notna(c):\n            #tot += 1\n            # if tot > 1000:\n            #    break\n            lst.append([N, M, c])\n    #if tot > 1000:\n    #    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d104e454a5e3b47fc77348a7b5eacd6546c22545"},"cell_type":"code","source":"os.system('echo concatenate correlated columns')\nN_pairs = 100\n# TEST N_pairs = 2\npct = np.percentile(np.array([np.abs(x[2]) for x in lst]), 100 * (1 - N_pairs / len(lst)))\n# TEST: should be >= pct\ninteraction_triples = [x for x in lst if np.abs(x[2]) > pct]\n#X_num_cpy = X_num.copy()\nfor x in interaction_triples:\n    X_num = np.concatenate((X_num, np.reshape(X_num[:, x[0]] * X_num[:, x[1]], (-1, 1))), axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"477780f500c3ab52e164e564d3f61755b785fc8e"},"cell_type":"code","source":"X_num","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1152c48b1571f0f09b7daa6d77607f873aa888d"},"cell_type":"code","source":"interaction_triples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21e76e57619f0aa9708ab2592c208726715f2f36","scrolled":true},"cell_type":"code","source":"#from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n#parameters = {'C':2.0**-(np.arange(1))}\n#logreg = LogisticRegression(penalty='l1', solver='liblinear')\n#clf = GridSearchCV(logreg, parameters, cv=2)\n#clf.fit(X_num, y)\nos.system('echo LogisticRegression')\n# TEMP clf = LogisticRegression(penalty='l1', solver='liblinear', C=2**(-7))\nclf = RandomForestClassifier(n_estimators = 300)\n\nclf = test_roc_auc(X_num, y, clf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dbda0f77209b3c1e5e05763e0b746bf286b8ca3"},"cell_type":"code","source":"from joblib import dump, load\ndump(clf, 'clf.joblib')\ndump(numer_columns, 'numer_columns.joblib')\ndump(cat_columns, 'cat_columns.joblib')\ndump(interaction_triples, 'interaction_triples.joblib')\n# clf = load('filename.joblib') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d9b40bad5718a8c0733b04355cba3f64e8dd761"},"cell_type":"code","source":"#print(interaction_triples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"980edeb97b8c464b2ef71ff23fc92237f9c9b686"},"cell_type":"code","source":"# You can't use make_dummies_from_df to process X_test, because make_dummies_from_df\n# expects to take a df in a format read directly from the file.  X_test has already been\n# processed into numerical and categorical / dummy columns.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61c6d13a39d65a2b1f6d90e9ac64ee9563ca3132"},"cell_type":"code","source":"#from sklearn.ensemble import RandomForestClassifier\n#rf_clf = RandomForestClassifier()\n#rf_clf.fit(X_train, y_train)\n#y_rf_predict = rf_clf.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1fc4a93548fecef0196b957b170e76393904dd6"},"cell_type":"code","source":"#from sklearn.ensemble import RandomForestClassifier\n#rf_clf = RandomForestClassifier()\n#rf_clf.fit(X_num, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c632d03ad69ae38c2fd8f407653eb2a315da7ec"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf672f1ef5377b224cba395e13ff47bc2d4a160b"},"cell_type":"code","source":"#print(rf_clf.classes_) # If [0 1], use column 1 for HasDetections probability\n# otherwise, use column 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d55cc94fb106f9d7ccdb9547ea7afe8ccd5575be"},"cell_type":"code","source":"#decide_test_file(\"../input/test.csv\", \"out_sm.csv\", sgd_clf, numer_columns, cat_columns, num_reports = 5, numrows=200000, stdscale=stdscale)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3c48caa246493c258db8ca92ec89d85e25635c5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3aabe1f95b5127f1e4f6cc221daea7f14d469dee"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ee4f572bb2a0cf255819e09c03853ca202bf10e"},"cell_type":"markdown","source":"# "},{"metadata":{"trusted":true,"_uuid":"4c4f828b592e8734a673f3c791f6bc3fe6f2e53a"},"cell_type":"code","source":"#7853254","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}