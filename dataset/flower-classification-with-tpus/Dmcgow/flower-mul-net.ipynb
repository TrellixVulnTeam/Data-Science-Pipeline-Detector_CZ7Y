{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"!pip install -q efficientnet\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\nimport matplotlib.pyplot as plt\nimport glob,os\nimport numpy as np\nimport pandas as pd\nfrom kaggle_datasets import KaggleDatasets\nkeras = tf.keras\nlayers = keras.layers\nEPOCH = 20\nDATASET_SIZE = 16465\nTRAIN_SIZE = 12753\nVAL_SIZE = 3712\nTEST_SIZE = 7382\nLR_START = 1e-3\nAUTO = tf.data.experimental.AUTOTUNE\nignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU',tpu.master())\nexcept ValueError:\n    tpu = None\n    \nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nprint(f'tensorflow version : {tf.__version__}')\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataset_filenames = tf.io.gfile.glob(r'gs://kds-b2e6cdbc4af76dcf0363776c09c12fe46872cab211d1de9f60ec7aec/tfrecords-jpeg-512x512/*a*/*.tfrec')\ntrain_filenames = tf.io.gfile.glob(r'gs://kds-b2e6cdbc4af76dcf0363776c09c12fe46872cab211d1de9f60ec7aec/tfrecords-jpeg-512x512/train/*.tfrec')\nval_filenames = tf.io.gfile.glob(r'gs://kds-b2e6cdbc4af76dcf0363776c09c12fe46872cab211d1de9f60ec7aec/tfrecords-jpeg-512x512/val/*.tfrec')\ntest_filenames = tf.io.gfile.glob(r'gs://kds-b2e6cdbc4af76dcf0363776c09c12fe46872cab211d1de9f60ec7aec/tfrecords-jpeg-512x512/test/*.tfrec')\n#count_dataset = np.sum([int(path.split('-')[-1].split('.')[0]) for path in dataset_filenames])\n#count_train = np.sum([int(path.split('-')[-1].split('.')[0]) for path in train_filenames])\n#count_val = np.sum([int(path.split('-')[-1].split('.')[0]) for path in val_filenames])\n#print(count_dataset)\n#print(count_train)\n#print(count_val)\n#dataset = tf.data.TFRecordDataset(dataset_filenames)\n#dataset = dataset.with_options(ignore_order)\ndataset_train = tf.data.TFRecordDataset(train_filenames)\ndataset_train = dataset_train.with_options(ignore_order)\ndataset_val = tf.data.TFRecordDataset(val_filenames)\ndataset_val = dataset_val.with_options(ignore_order)\ndataset_test = tf.data.TFRecordDataset(test_filenames)\ndataset_test = dataset_test.with_options(ignore_order)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_description = {\n    'class':tf.io.FixedLenFeature([],tf.int64),\n    'image':tf.io.FixedLenFeature([],tf.string)\n}\ntest_feature_description = {\n    'id':tf.io.FixedLenFeature([],tf.string),\n    'image':tf.io.FixedLenFeature([],tf.string)\n}\ndef dataset_decode(data):\n    decode_data = tf.io.parse_single_example(data,feature_description)\n    label = decode_data['class']\n    image = tf.image.decode_jpeg(decode_data['image'],channels=3)\n    image = tf.reshape(image,[512,512,3])\n    image = tf.image.random_flip_left_right(image)\n    image = tf.cast(image,tf.float32)\n    image = (image - 127.5) / 127.5\n    return image,label\ndef test_dataset_decode(data):\n    decode_data = tf.io.parse_single_example(data,test_feature_description)\n    ID = decode_data['id']\n    image = tf.image.decode_jpeg(decode_data['image'],channels=3)\n    image = tf.reshape(image,[512,512,3])\n    image = tf.cast(image,tf.float32)\n    image = (image - 127.5) / 127.5\n    return ID,image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataset = dataset.map(dataset_decode)\ndataset_train = dataset_train.map(dataset_decode)\ndataset_val = dataset_val.map(dataset_decode)\ndataset_test = dataset_test.map(test_dataset_decode)\n#dataset = dataset.shuffle(DATASET_SIZE).repeat().batch(BATCH_SIZE).prefetch(AUTO)\ndataset_train = dataset_train.shuffle(DATASET_SIZE).repeat().batch(BATCH_SIZE).prefetch(AUTO)\ndataset_val = dataset_val.batch(BATCH_SIZE).prefetch(AUTO)\ndataset_test = dataset_test.batch(BATCH_SIZE).prefetch(AUTO)\n#print(dataset)\nprint(dataset_train)\nprint(dataset_val)\nprint(dataset_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = keras.optimizers.Adam(LR_START)\nloss = keras.losses.SparseCategoricalCrossentropy()\nmetrics = keras.metrics.SparseCategoricalAccuracy()\nlr_callback = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.8,patience=3,min_lr=1e-6)\nwith strategy.scope():\n    base_network_1 = efn.EfficientNetB7(include_top=False,input_shape=(512,512,3),weights='imagenet')\n    #base_network_2 = keras.applications.InceptionResNetV2(include_top=False,input_shape=[512,512,3],weights='imagenet')\n    base_network_3 = keras.applications.DenseNet201(include_top=False,input_shape=[512,512,3],weights='imagenet')\n    network_1 = keras.Sequential()\n    network_1.add(base_network_1)\n    network_1.add(layers.MaxPooling2D())\n    network_1.add(layers.Conv2D(2560,3,padding='same'))\n    network_1.add(layers.BatchNormalization())\n    network_1.add(layers.ReLU())\n    network_1.add(layers.GlobalAveragePooling2D())\n    network_1.add(layers.Dense(1024))\n    network_1.add(layers.BatchNormalization())\n    network_1.add(layers.LeakyReLU())\n    network_1.add(layers.Dense(512))\n    network_1.add(layers.BatchNormalization())\n    network_1.add(layers.LeakyReLU())\n    network_1.add(layers.Dense(104,activation='softmax'))\n    network_1.compile(optimizer=optimizer,loss=loss,metrics=[metrics])\n    \"\"\"\n    network_2 = keras.Sequential()\n    network_2.add(base_network_2)\n    network_2.add(layers.MaxPooling2D())\n    network_2.add(layers.Conv2D(2560,3,padding='same'))\n    network_2.add(layers.BatchNormalization())\n    network_2.add(layers.ReLU())\n    network_2.add(layers.GlobalAveragePooling2D())\n    network_2.add(layers.Dense(1024))\n    network_2.add(layers.BatchNormalization())\n    network_2.add(layers.LeakyReLU())\n    network_2.add(layers.Dense(512))\n    network_2.add(layers.BatchNormalization())\n    network_2.add(layers.LeakyReLU())\n    network_2.add(layers.Dense(104,activation='softmax'))\n    network_2.compile(optimizer=optimizer,loss=loss,metrics=[metrics])\n    \"\"\"\n    network_3 = keras.Sequential()\n    network_3.add(base_network_3)\n    network_3.add(layers.MaxPooling2D())\n    network_3.add(layers.Conv2D(2560,3,padding='same'))\n    network_3.add(layers.BatchNormalization())\n    network_3.add(layers.ReLU())\n    network_3.add(layers.GlobalAveragePooling2D())\n    network_3.add(layers.Dense(1024))\n    network_3.add(layers.BatchNormalization())\n    network_3.add(layers.LeakyReLU())\n    network_3.add(layers.Dense(512))\n    network_3.add(layers.BatchNormalization())\n    network_3.add(layers.LeakyReLU())\n    network_3.add(layers.Dense(104,activation='softmax'))\n    network_3.compile(optimizer=optimizer,loss=loss,metrics=[metrics])\nnetwork_1.summary()\nnetwork_1.fit(dataset_train,\n              epochs=EPOCH,\n              steps_per_epoch=TRAIN_SIZE//BATCH_SIZE,\n              validation_data=dataset_val,\n              validation_steps=VAL_SIZE//BATCH_SIZE,\n              callbacks=[lr_callback])\n\"\"\"\nnetwork_2.summary()\nnetwork_2.fit(dataset_train,\n              epochs=EPOCH,\n              steps_per_epoch=TRAIN_SIZE//BATCH_SIZE,\n              validation_data=dataset_val,\n              validation_steps=VAL_SIZE//BATCH_SIZE,\n              callbacks=[lr_callback])\n\"\"\"\nnetwork_3.summary()\nnetwork_3.fit(dataset_train,\n              epochs=EPOCH,\n              steps_per_epoch=TRAIN_SIZE//BATCH_SIZE,\n              validation_data=dataset_val,\n              validation_steps=VAL_SIZE//BATCH_SIZE,\n              callbacks=[lr_callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_csv = []\nfor ID,image in dataset_test:\n    prediction_1 = network_1.predict(image)\n    #prediction_2 = network_2.predict(image)\n    prediction_3 = network_3.predict(image)\n    prediction = prediction_1 + prediction_3# + prediction_2\n    label = tf.argmax(prediction,axis=1).numpy()\n    ID = [item.numpy().decode('utf-8') for item in ID]\n    for i in range(len(ID)):\n        predict_csv.append([ID[i],label[i]])\n    print('*',end='')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csv_name = ['id','label']\ncsv_data = pd.DataFrame(columns=csv_name,data=predict_csv)\ncsv_data.to_csv(r'submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('DONE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}