{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Special Thanks To @dlibenzi (github) for all his help;"},{"metadata":{"trusted":true},"cell_type":"code","source":"!echo $TPU_NAME","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!env","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os; os.environ[\"XRT_TPU_CONFIG\"] = \"tpu_worker;0;10.0.0.2:8470\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"527d9b53-12bf-4aec-a7ed-d87761598b35","_cell_guid":"abda0b10-e1f2-49eb-abe4-29d729c10c30","trusted":true},"cell_type":"code","source":"import collections\nfrom datetime import datetime, timedelta\nimport os\nimport tensorflow as tf\nimport numpy as np\nimport requests, threading\n\n_VersionConfig = collections.namedtuple('_VersionConfig', 'wheels,server')\nVERSION = \"torch_xla==nightly\"\nCONFIG = {\n    'torch_xla==nightly': _VersionConfig('nightly', 'XRT-dev{}'.format(\n        (datetime.today() - timedelta(1)).strftime('%Y%m%d'))),\n}[VERSION]\n\nDIST_BUCKET = 'gs://tpu-pytorch/wheels'\nTORCH_WHEEL = 'torch-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\nTORCH_XLA_WHEEL = 'torch_xla-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\nTORCHVISION_WHEEL = 'torchvision-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CONFIG.wheels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"527d9b53-12bf-4aec-a7ed-d87761598b35","_cell_guid":"abda0b10-e1f2-49eb-abe4-29d729c10c30","trusted":true},"cell_type":"code","source":"!export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH\n!apt-get install libomp5 -y\n!apt-get install libopenblas-dev -y\n\n# Install COLAB TPU compat PyTorch/TPU wheels and dependencies\n!pip uninstall -y torch torchvision\n!gsutil cp \"$DIST_BUCKET/$TORCH_WHEEL\" .\n!gsutil cp \"$DIST_BUCKET/$TORCH_XLA_WHEEL\" .\n!gsutil cp \"$DIST_BUCKET/$TORCHVISION_WHEEL\" .\n!pip install \"$TORCH_WHEEL\"\n!pip install \"$TORCH_XLA_WHEEL\"\n!pip install \"$TORCHVISION_WHEEL\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"527d9b53-12bf-4aec-a7ed-d87761598b35","_cell_guid":"abda0b10-e1f2-49eb-abe4-29d729c10c30","trusted":true},"cell_type":"code","source":"import torch_xla.core.xla_model as xm\nimport torch_xla.distributed.data_parallel as dp # http://pytorch.org/xla/index.html#running-on-multiple-xla-devices-with-multithreading\nimport torch_xla.distributed.xla_multiprocessing as xmp # http://pytorch.org/xla/index.html#running-on-multiple-xla-devices-with-multiprocessing\nimport torch_xla.distributed.parallel_loader as pl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_datasets import KaggleDatasets\n\n# Data access\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\n\n# Configuration\nIMAGE_SIZE = [512, 512]\nEPOCHS = 20\nBATCH_SIZE = 16 * 1\n\nGCS_PATH_SELECT = { # available image sizes\n    192: GCS_DS_PATH + '/tfrecords-jpeg-192x192',\n    224: GCS_DS_PATH + '/tfrecords-jpeg-224x224',\n    331: GCS_DS_PATH + '/tfrecords-jpeg-331x331',\n    512: GCS_DS_PATH + '/tfrecords-jpeg-512x512'\n}\nGCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n\nTRAINING_FILENAMES   = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')\nVALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')\nTEST_FILENAMES       = tf.io.gfile.glob(GCS_PATH + '/test/*.tfrec')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAINING_FILENAMES","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the 100 flowers dataset, the format of each TFRecord of labeled data is:\n    - \"image\": list of bytestrings containing 1 bytestring (the JPEG-ecoded image bytes)\n    - \"label\": list of int64 containing 1 int64"},{"metadata":{"trusted":true},"cell_type":"code","source":"# REFERENCE https://gist.githubusercontent.com/dlibenzi/c9868a1090f6f8ef9d79d2cfcbadd8ab/raw/947fbec325cbdeda91bd53acb5e126caa4115348/more_tf_stuff.py\n# Thanks A Lot For Your Help!!!\n\nfrom PIL import Image\nimport numpy as np\nimport hashlib\nimport os\nimport sys\nimport torch\nimport torch_xla.utils.tf_record_reader as tfrr\n\na = \"\"\"\nimage/class/label       tensor([82])\nimage/class/synset      n01796340\nimage/channels  tensor([3])\nimage/object/bbox/label tensor([], dtype=torch.int64)\nimage/width     tensor([900])\nimage/format    JPEG\nimage/height    tensor([600])\nimage/class/text        ptarmigan\nimage/object/bbox/ymin  tensor([])\nimage/encoded   tensor([ -1, -40,  -1,  ..., -30,  -1, -39], dtype=torch.int8)\nimage/object/bbox/ymax  tensor([])\nimage/object/bbox/xmin  tensor([])\nimage/filename  n01796340_812.JPEG\nimage/object/bbox/xmax  tensor([])\nimage/colorspace        RGB\n\"\"\"\n\ndef decode(ex):\n\n    w = 512 # ex['image/width'].item()\n    h = 512 # ex['image/height'].item()\n    imgb = ex['image'].numpy().tobytes()\n    \n    # m = hashlib.md5()\n    # m.update(imgb)\n    # print('HASH = {}'.format(m.hexdigest()))\n    \n    image = Image.frombytes(\"RGB\", (w, h), imgb,\n                            \"JPEG\".lower(),\n                            'RGB', None\n                           )\n    npa = np.asarray(image)\n    return torch.from_numpy(npa), image\n\n\ndef readem(path, img_path=None):\n    count = 0\n    transforms = {}  \n    r = tfrr.TfRecordReader(path, compression='', transforms=transforms)\n    while True:\n        ex = r.read_example()\n        if not ex: break\n        # print('\\n')\n        # for lbl, data in ex.items():\n            # print('{}\\t{}'.format(lbl, data))\n        img_tensor, image = decode(ex)\n        if img_path:\n            image.save(os.path.join(img_path, str(count) + '.jpg'))\n        count += 1\n    print('\\n\\nDecoded {} samples'.format(count))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls && pwd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nimport os;\nfor idx, file in enumerate(TRAINING_FILENAMES):\n    img_path = f\"/kaggle/working/flower_images_{idx}\"\n    os.makedirs(img_path, exist_ok=True)\n    print(file)\n    readem(path = file, img_path = img_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n# https://stackoverflow.com/questions/11159436/multiple-figures-in-a-single-window\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n\ndef plot_figures(figures, nrows = 1, ncols=1):\n    \"\"\"\n    Plot a dictionary of figures.\n    Parameters\n    ----------\n    figures : <title, figure> dictionary\n    ncols : number of columns of subplots wanted in the display\n    nrows : number of rows of subplots wanted in the figure\n    \"\"\"\n    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows, figsize=(20,20))\n    for ind,title in zip(range(len(figures)), figures):\n        axeslist.ravel()[ind].imshow(figures[title], cmap=plt.jet())\n        # axeslist.ravel()[ind].set_title(title)\n        axeslist.ravel()[ind].set_axis_off()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generation of a dictionary of (title, images)\nw, h = 10, 10\nnumber_of_im = w*h\n\nfigures = {'im'+str(i): Image.open(f\"./flower_images_0/{i}.jpg\") for i in range(number_of_im)}\n\n# plot of the images in a figure, with 5 rows and 4 columns\nplot_figures(figures, w, h)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generation of a dictionary of (title, images)\nw, h = 10, 10\nnumber_of_im = w*h\n\nfigures = {'im'+str(i): Image.open(f\"./flower_images_1/{i}.jpg\") for i in range(number_of_im)}\n\nplot_figures(figures, w, h)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generation of a dictionary of (title, images)\nw, h = 10, 10\nnumber_of_im = w*h\n\nfigures = {'im'+str(i): Image.open(f\"./flower_images_2/{i}.jpg\") for i in range(number_of_im)}\n\nplot_figures(figures, w, h)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generation of a dictionary of (title, images)\nw, h = 10, 10\nnumber_of_im = w*h\n\nfigures = {'im'+str(i): Image.open(f\"./flower_images_3/{i}.jpg\") for i in range(number_of_im)}\n\nplot_figures(figures, w, h)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generation of a dictionary of (title, images)\nw, h = 10, 10\nnumber_of_im = w*h\n\nfigures = {'im'+str(i): Image.open(f\"./flower_images_10/{i}.jpg\") for i in range(number_of_im)}\n\nplot_figures(figures, w, h)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}