{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Baby steps towards a Halite Reinforcement Learning agent\n\nThis kernel contains the first steps I have taken in creating a reinforcement learning agent for Halite. Being a beginner in reinforcement learning, I have worked my way up in baby steps using the tf-agents package. \n\nAt this point, here are the steps that I have taken : \n1. Create an environment and teach an agent to control a single ship, with no opponents. Reward is collected halite. Agent learns to maximise collected halite, by mining the highest containing halite cells of the board. \n2. Add a single opponent ship, moving randomly without collecting any halite. Reward is still collected halite but with a high penalty if the agent ship is destroyed. Agent learns to maximise collected halite while avoiding the opponent ship. \n3. Use the agent trained at step 2 as an opponent to train a second agent. Reward is now the difference in collected halite between the agent and his opponent. Agent must learn to optimise his halite collection better than his opponent. \n\nAll these steps are applied on a simplified version of the game : The board is 7 * 7 and the game lasts 20 turns (instead of 21 * 21 and 400 turns). \n\nThe main difficulties I still have to overcome to train an agent on the full game are : \n1. How can you enable the agent to control multiple ships ? You can see that in this code the agent observation is defined as relative to the ship the agent must control. My goal is to call the agent as many times as there are ships, giving it each time the specific observation for this ship. For this, the order in which the ships must be submitted to the agent needs to be defined. Adapting the reward is also a challenge. \n2. How to implement \"league training\" ? Having the agent train against multiple versions of himself in a virtual league in order to make sure the agent does not overspecialize in defeating a particular strategy and still manages to defeat earlier versions of himself. \n\nUnfortunately the Kaggle kernel does not yet include tf-agents, so it is not possible to submit a bot to the competition using this package. I am hoping they will add it for future Halite competitions ! \n\nYou can find a jupyter notebook version of this code on my GitHub. It will probably be more up to date then this kernel so you are welcome to check it out : https://github.com/alexandersmedley/halite_rl\n\nAny feedback or comment will be greatly appreciated ! Like I said, I am beginning in Reinforcement Learning so there are probably loads of ways this code can be improved. \n\n\n# Code description\nThe code is based on the tf-agents DQN tutorial for the cartpole gym environment : https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial?hl=en\n\n## Environment \nTo adapt the tf-agents cartpole tutorial to Halite, the main difficulty was to create a compatible environment. In the tutorial, the cartpole gym environment already provides everything needed for tf-agents to interact with an agent. For Halite, a custom environment compatible with tf-agents must be created. \n\nI created this environment by following a tic tac toc tutorial (https://towardsdatascience.com/creating-a-custom-environment-for-tensorflow-agent-tic-tac-toe-example-b66902f73059). Basically, we need to create a python environment class containing the following functions : \n* init : Create a board instance and initialize internal variables. \n* _step : Apply the action chosen by the agent to the board and calculate reward. Return observation and reward in a time_step. \n* _reset : Reset board at the end of a game. \n\nI also added the following functions to the environment (I guess they could be placed elsewhere, in a helper class for example):\n* get_observation : Get agent observation from the current board state. \n* set_opponent_behaviour and opponent_agent : Define opponent behaviour. \n\n## Observation\nThe observation is defined relative to the current ship the agent must control. For now, the agent has two \"radars\", one containing the surrounding halite, the other one containing the ally and enemy ships. For ships, I have distinguished ally ships (we don't want to collide with those), enemy ships with more cargo (we want to collide with those) and enemy ships with less cargo (we absolutely don't want to collide with those). \n\n## Reward \nThe reward is the difference in halite collected between the agent ship and the opponent ship. If the opponent's behaviour is defined as randomly moving, then its collected halite will always be 0. \n\n## Neural network \nThe agent neural network is a Conv2D layer followed by two Dense layers. Conv2D is used as a feature extractor. \n\nI have not tested any alternative architecture. This simple neural network gave good results on the simplified version of the game. "},{"metadata":{},"cell_type":"markdown","source":"# Install tf-agents"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install tensorflow_probability==0.11.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install tf_agents==0.6.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf \n\nfrom kaggle_environments import evaluate, make\nfrom kaggle_environments.envs.halite.helpers import *\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.networks import categorical_q_network\nfrom tf_agents.agents.categorical_dqn import categorical_dqn_agent\nfrom tf_agents.utils import common\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.policies.policy_saver import PolicySaver\nfrom tf_agents.specs import BoundedArraySpec\n\nfrom random import randint","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_iterations = 20000\n\nnum_atoms = 51 # number of atoms for Categorical Q-Network\nmin_q_value = -1000\nmax_q_value = 1000 \nepsilon_greedy = 0.1\n\ninitial_collect_steps = 400 \ncollect_steps_per_iteration = 10\nreplay_buffer_max_length = 100000\n\nbatch_size = 64\nlearning_rate = 1e-3\nlog_interval = 200\n\nnum_eval_episodes = 10\neval_interval = 1000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Environment"},{"metadata":{"trusted":true},"cell_type":"code","source":"def time_step_ok(time_step_nok):\n    # to convert an time_step created from observation into a agent policy compatible time_step\n    tensors = []\n    for array in time_step_nok:\n        new_array = np.expand_dims(array, axis=0)\n        tensor = tf.constant(new_array)\n        tensors.append(tensor)\n        \n    # step_type\n    # reward\n    # discount\n    # observation\n    \n    return ts.TimeStep(tensors[0], tensors[1], tensors[2], tensors[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class HaliteWrapper(py_environment.PyEnvironment):\n    def __init__(self):\n        \n        # Create environment and board \n        self._board_size = 7\n        self._agent_count = 2\n        self._max_turns = 20\n        self._starting_halite = 2500\n        self.env = make(\"halite\", configuration={'episodeSteps':self._max_turns, 'size':self._board_size, \n                                                 'startingHalite':self._starting_halite})\n        self.state = self.env.reset(num_agents=self._agent_count)\n        \n        self.obs = self.state[0].observation\n        self.config = self.env.configuration\n        self.max_cell_halite = self.config.maxCellHalite\n        self.board = Board(self.obs, self.config)\n        \n        # Define observation and action specs \n        self.observation_shape = self.get_observation(self.board).shape\n        self._observation_spec = BoundedArraySpec(\n            shape=self.observation_shape, \n            dtype=np.int64, maximum=None, minimum=None)\n        \n        self._action_def = {0: None,\n                            1: ShipAction.NORTH,\n                            2: ShipAction.EAST,\n                            3: ShipAction.SOUTH,\n                            4: ShipAction.WEST,\n#                             5: ShipAction.CONVERT\n                           }\n        self._action_spec = BoundedArraySpec(shape=(), dtype=np.int32, maximum=len(self._action_def)-1, minimum=0)\n        \n        # Define opponent behaviour \n        self.opponent_behaviour = 'static'\n        self.opponent_policy = None\n        \n        # Define counters \n        self.turn_counter = 1 # turn that will be solved on next step, not last solved turn\n        self.previous_cargo_halite = 0\n        self.opponent_previous_cargo_halite = 0\n        self.game_over = False\n        \n    def get_observation(self, board, position=(0,0), player_id=0):\n        \"\"\"\n        Agent observation. Halite and ship channels with set radius around input position. \n        \"\"\"\n        size = board.configuration.size\n\n        # Halite distribution\n        halite = np.array(board.observation['halite']).reshape(size, size)\n        halite = np.tile(halite, (3,3))\n\n        # convert Halite SDK position format to np matrix \n        row = size - 1 - position[1] + size\n        col = position[0] + size\n        radius = 4\n        halite_radius = halite[row-radius+1:row+radius, col-radius+1:col+radius]\n        halite_radius = np.expand_dims(halite_radius, axis=-1)\n\n        # Ship distribution\n        if board[position].ship:\n            ref_halite = board[position].ship.halite\n            ref_ship_id = board[position].ship.id\n        else:\n            ref_halite = 0\n            ref_ship_id = None \n\n        ships = np.zeros((size, size))\n        for ship in board.ships.values():\n            if ship.player_id == player_id: # ally\n                if ship.id == ref_ship_id:\n                    value = 0\n                else:\n                    value = -1\n            elif ship.halite > ref_halite: # opponent with more halite\n                value = 1\n            else: # opponent with equal or less halite \n                value = -2\n            ships[size-1-ship.position[1], ship.position[0]] = value\n\n        ships = np.tile(ships, (3,3))\n        ships_radius = ships[row-radius+1:row+radius, col-radius+1:col+radius]\n        ships_radius = np.expand_dims(ships_radius, axis=-1)\n\n        # Concatenate channels\n        observation = np.concatenate([halite_radius, ships_radius], axis=-1)\n        observation = np.array(observation, dtype='int64')\n        return observation\n        \n    def observation_spec(self):\n        return self._observation_spec\n    \n    def action_spec(self):\n        return self._action_spec\n    \n    def _reset(self):\n        # reset environment and board\n        self.env = make(\"halite\", configuration={'episodeSteps':self._max_turns, 'size':self._board_size, \n                                                 'startingHalite':self._starting_halite})\n        self.state = self.env.reset(num_agents=self._agent_count)\n        \n        self.obs = self.state[0].observation\n        self.config = self.env.configuration\n        self.max_cell_halite = self.config.maxCellHalite\n        self.board = Board(self.obs, self.config)\n        \n        # reset counters\n        self.turn_counter = 1\n        self.previous_cargo_halite = 0\n        self.opponent_previous_cargo_halite = 0\n        self.game_over = False\n        \n        observation = self.get_observation(self.board, position=self.board.players[0].ships[0].position, player_id=0)\n        \n        return_object = ts.restart(observation)\n        \n        return return_object\n    \n    def _step(self, action):\n        \n        if self.game_over:\n            return self._reset()\n        \n        # Agent action\n        action = int(action)\n        self.board.ships['0-1'].next_action = self._action_def[action]\n        \n        # Opponent action\n        self.board.ships['0-2'].next_action = self.opponent_agent()\n        \n        self.board = self.board.next()\n        \n        # Calculate reward\n        if len(self.board.players[0].ships) == 0: # ship destroyed\n            reward = -10000\n        elif len(self.board.players[1].ships) == 0: # enemy ship destroyed\n            reward = 10000\n        else:\n            reward = (self.board.ships['0-1'].halite - self.previous_cargo_halite \n                      - (self.board.ships['0-2'].halite - self.opponent_previous_cargo_halite))\n        \n        # Update counters \n        self.turn_counter += 1\n        if len(self.board.players[0].ships) > 0:\n            self.previous_cargo_halite = self.board.ships['0-1'].halite\n        else:\n            self.previous_cargo_halite = 0\n        \n        if len(self.board.players[1].ships) > 0:\n            self.opponent_previous_cargo_halite = self.board.ships['0-2'].halite\n        else:\n            self.opponent_previous_cargo_halite = 0\n            \n        if self.turn_counter >= self._max_turns:\n            self.game_over = True\n        elif len(self.board.players[0].ships) == 0 or len(self.board.players[1].ships) == 0:\n            self.game_over = True\n            \n        if len(self.board.players[0].ships) > 0:\n            observation = self.get_observation(self.board, position=self.board.players[0].ships[0].position, player_id=0)\n        else:\n            observation = self.get_observation(self.board, position=(0,0), player_id=0)\n        \n        # Return \n        if self.game_over:\n            return_object = ts.termination(observation, reward)\n            return return_object\n        else:\n            return_object = ts.transition(observation, reward, discount=1.0)\n            return return_object\n        \n    def set_opponent_behaviour(self, behaviour, policy_file=None):\n        \"\"\"\n        Set opponent behaviour internal variables. \n        If behaviour is load_policy then policy_file must be specified. \n        \"\"\"\n        self.opponent_behaviour = behaviour\n        if policy_file:\n            self.opponent_policy = tf.saved_model.load(policy_file)\n            \n    def opponent_agent(self):\n        \"\"\"\n        Return opponent action based on opponent behaviour. \n        \"\"\"\n        \n        if self.opponent_behaviour == 'static':\n            action = None\n            \n        elif self.opponent_behaviour == 'random_moving':\n            action = self._action_def[randint(1,4)]\n            \n        elif self.opponent_behaviour == 'random_collecting':\n            action = self._action_def[randint(0,4)]\n            \n        elif self.opponent_behaviour == 'load_policy':\n            me = self.board.opponents[0]\n            observation = self.get_observation(self.board, position=me.ships[0].position, player_id=me.id)\n            \n            if self.obs.step == 0:\n                time_step = ts.restart(observation)\n            elif self.obs.step >= self.config.episodeSteps-1:\n                time_step = ts.termination(observation, reward=0)\n            else:\n                time_step = ts.transition(observation, reward=0)\n            \n            time_step = time_step_ok(time_step)\n            \n            action_int = int(self.opponent_policy.action(time_step).action)\n            action = self._action_def[action_int]\n\n        return action","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_py_env = HaliteWrapper()\neval_py_env = HaliteWrapper()\n\ntrain_py_env.set_opponent_behaviour(behaviour='random_moving', policy_file=None)\neval_py_env.set_opponent_behaviour(behaviour='random_moving', policy_file=None)\n\ntrain_env = tf_py_environment.TFPyEnvironment(train_py_env)\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessing_layers = tf.keras.models.Sequential()\npreprocessing_layers.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), input_shape=train_py_env.observation_shape, \n                                                padding='same', activation='relu'))\npreprocessing_layers.add(tf.keras.layers.Flatten())\n\nfc_layer_params = (128,128)\n\nq_net = categorical_q_network.CategoricalQNetwork(\n    input_tensor_spec=train_env.observation_spec(),\n    action_spec=train_env.action_spec(),\n    preprocessing_layers=preprocessing_layers,\n    num_atoms=num_atoms, \n    fc_layer_params=fc_layer_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n\ntrain_step_counter = tf.Variable(0)\n\nagent = categorical_dqn_agent.CategoricalDqnAgent(\n    train_env.time_step_spec(),\n    train_env.action_spec(),\n    categorical_q_network=q_net,\n    optimizer=optimizer,\n    min_q_value=min_q_value, \n    max_q_value=max_q_value,\n    epsilon_greedy=epsilon_greedy,\n    td_errors_loss_fn=common.element_wise_squared_loss,\n    train_step_counter=train_step_counter)\n\nagent.initialize()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_policy = agent.policy\ncollect_policy = agent.collect_policy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n                                                train_env.action_spec())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_avg_return(environment, policy, num_episodes=10):\n\n    total_return = 0.0\n    for _ in range(num_episodes):\n        time_step = environment.reset()\n        episode_return = 0.0\n\n        while not time_step.is_last():\n            action_step = policy.action(time_step)\n            time_step = environment.step(action_step.action)\n            episode_return += time_step.reward\n        total_return += episode_return\n\n    avg_return = total_return / num_episodes\n    return avg_return.numpy()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compute_avg_return(eval_env, random_policy, num_eval_episodes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Collect data"},{"metadata":{"trusted":true},"cell_type":"code","source":"replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n    data_spec=agent.collect_data_spec,\n    batch_size=train_env.batch_size,\n    max_length=replay_buffer_max_length)\n\ndef collect_step(environment, policy, buffer):\n    time_step = environment.current_time_step()\n    action_step = policy.action(time_step)\n    next_time_step = environment.step(action_step.action)\n    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n\n    # Add trajectory to the replay buffer\n    buffer.add_batch(traj)\n\ndef collect_data(env, policy, buffer, steps):\n    for _ in range(steps):\n        collect_step(env, policy, buffer)\n\ncollect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n\n# Dataset generates trajectories with shape [Bx2x...]\ndataset = replay_buffer.as_dataset(\n    num_parallel_calls=3, \n    sample_batch_size=batch_size, \n    num_steps=2).prefetch(3)\n\ndataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iterator = iter(dataset)\n\nprint(iterator)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# (Optional) Optimize by wrapping some of the code in a graph using TF function.\nagent.train = common.function(agent.train)\n\n# Reset the train step\nagent.train_step_counter.assign(0)\n\n# Evaluate the agent's policy once before training.\navg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\nreturns = [avg_return]\n\nfor _ in range(num_iterations):\n\n    # Collect a few steps using collect_policy and save to the replay buffer.\n    collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n\n    # Sample a batch of data from the buffer and update the agent's network.\n    experience, unused_info = next(iterator)\n    train_loss = agent.train(experience).loss\n\n    step = agent.train_step_counter.numpy()\n\n    if step % log_interval == 0:\n        print('step = {0}: loss = {1}'.format(step, train_loss))\n\n    if step % eval_interval == 0:\n        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n        returns.append(avg_return)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iterations = range(0, num_iterations + 1, eval_interval)\nplt.plot(iterations, returns)\nplt.ylabel('Average Return')\nplt.xlabel('Iterations')\nplt.ylim(top=6000)\n\n# plt.savefig('return_per_iteration.png', bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Watch our agent play the game !"},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_rl_agent(obs, config):\n    board = Board(obs, config)\n    me = board.current_player\n    \n    observation = train_py_env.get_observation(board, position=me.ships[0].position, player_id=me.id)\n    \n    if obs.step == 0:\n        time_step = ts.restart(observation)\n    elif obs.step >= config.episodeSteps-1:\n        time_step = ts.termination(observation, reward=0)\n    else:\n        time_step = ts.transition(observation, reward=0)\n    \n    time_step = time_step_ok(time_step)\n    \n    action_int = int(eval_policy.action(time_step).action)\n    action = train_py_env._action_def[action_int]\n    \n    for ship in me.ships:\n        ship.next_action = action\n\n    return me.next_actions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_ship_agent(obs, config):\n    board = Board(obs, config)\n    me = board.current_player\n    \n    for ship in me.ships:\n        ship.next_action = train_py_env._action_def[randint(0,4)]\n    \n    return me.next_actions\n\ndef static_ship_agent(obs, config):\n    board = Board(obs, config)\n    me = board.current_player\n    \n    return me.next_actions\n\ndef random_moving_ship_agent(obs, config):\n    board = Board(obs, config)\n    me = board.current_player\n    \n    for ship in me.ships:\n        ship.next_action = train_py_env._action_def[randint(1,4)]\n    \n    return me.next_actions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_py_env._reset()\nenv = train_py_env.env\nenv.run([my_rl_agent, random_moving_ship_agent])\nenv.render(mode='ipython', width=400, height=300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save the agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use this to save the agent policy and load it as an opponent \nsaver = PolicySaver(agent.policy, batch_size=None)\n# saver.save('agent_policy') \n\n# You can then use is as an opponent to train a second agent by using : \n# train_py_env.set_opponent_behaviour(behaviour='load_policy', policy_file='my_policy')\n# eval_py_env.set_opponent_behaviour(behaviour='load_policy', policy_file='my_policy')\n# instead of the random_moving behaviour defined here","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}