{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ubiquant Market Prediction with cumsum dataset\n\nThis notebook makes an assumption that the provided 300 features are actually asset returns of some investment. There are about 1500 investments which could be some different subfunds or different traiders within some fund. The different invesments are independent of each other and should be trained separately. The best choise is to train them as separate models.\nThus I desided to combine feature values with cumsum grouped by investment_id. The same way there were combined investment targets.\nThe tests showed amazing correlation of 0.9 but in live we went into some issues that actually made this model a bad shoice:\n- there are investments that are completely missing in production as well as having as low as 2 trade activities.\n- when we train model with loss function to minimize a Pearson correlation the obtained result has a very high RMSE which makes problematic calculating the next step data. Normalizing targets helps us but not too much.\n- because of the data becoming an obviously time-series we cannot shuffle it and have to split it by `TimeSeriesSplit`. Because of that sometimes the last fold fails to train.","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom tensorflow.python.keras import backend as K\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nimport tensorflow_addons as tfa\n\nfrom scipy import stats\nfrom tqdm import tqdm\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import StratifiedKFold, ShuffleSplit, StratifiedShuffleSplit, TimeSeriesSplit\nfrom scipy import stats\nfrom datetime import datetime\nimport timeit\n\ntf.config.optimizer.set_jit(True)\n\n#from tensorflow.keras.mixed_precision import experimental as mixed_precision\n#mixed_precision.set_policy('mixed_bfloat16')\n#DATA_DTYPE = tf.bfloat16\n\ntf.config.list_physical_devices('GPU')\n\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\nfeatures_target = features + ['target']","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-16T13:23:34.185383Z","iopub.execute_input":"2022-04-16T13:23:34.186016Z","iopub.status.idle":"2022-04-16T13:23:40.247252Z","shell.execute_reply.started":"2022-04-16T13:23:34.185918Z","shell.execute_reply":"2022-04-16T13:23:40.246489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating cumsum datasets","metadata":{}},{"cell_type":"code","source":"%%script echo skipping\n# import original dataset for debug\n\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-16T13:23:40.248755Z","iopub.execute_input":"2022-04-16T13:23:40.249464Z","iopub.status.idle":"2022-04-16T13:23:40.273967Z","shell.execute_reply.started":"2022-04-16T13:23:40.249423Z","shell.execute_reply":"2022-04-16T13:23:40.273093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%script echo skipping\n# create cumsum dataset\n\ninvestments = train.investment_id.unique()\n#investments = investments[investments<50]\n#train_orig = train.copy()\n\ntrain[features_target] = train[features_target].astype(np.float32)\nfor investment_id in tqdm(investments):\n    train.loc[train.investment_id==investment_id,features_target] = train[train.investment_id==investment_id][features_target].cumsum()\n\n# alternative way of summing data but is less convenient than the simple loop, because there is no tqdm\n#train[features_target] = train[features_target].groupby('investment_id').apply(lambda group: group.astype(np.float32).cumsum()).astype(np.float16)\n\ntrain[features_target] = train[features_target].astype(np.float16)\ntrain.to_pickle('ubiquant-market-prediction-cumsum-f16.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-04-16T13:23:40.275393Z","iopub.execute_input":"2022-04-16T13:23:40.27578Z","iopub.status.idle":"2022-04-16T13:23:40.297219Z","shell.execute_reply.started":"2022-04-16T13:23:40.27574Z","shell.execute_reply":"2022-04-16T13:23:40.296407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%script echo skipping\n# create normalized targets\n\ntrain.set_index('investment_id', inplace=True)\nnormalize_ds = train['target'].groupby('investment_id').apply(lambda group: (group.mean(), group.std()))\n\n#train['target'] = train['target'].groupby('investment_id').apply(lambda group: (group-group.mean())/group.std())\nfor investment_id,gr in tqdm(train['target'].groupby('investment_id')):\n    train.loc[investment_id,'target'] = (gr-gr.mean())/gr.std()\n    \ntrain.reset_index(inplace=True)\n\ntrain = train.copy()\nprint(train['target'].max(), train['target'].min())\n\ntrain['target'].to_pickle('ubiquant-cumsum-nomalized-target.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-04-16T13:23:40.29954Z","iopub.execute_input":"2022-04-16T13:23:40.299883Z","iopub.status.idle":"2022-04-16T13:23:40.321178Z","shell.execute_reply.started":"2022-04-16T13:23:40.299846Z","shell.execute_reply":"2022-04-16T13:23:40.32037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%script echo skipping\n# create cumsum last row\n\ndef get_last_row(train):\n    last_row = pd.DataFrame(columns=train.columns)\n    for x in train.investment_id.unique():\n        if not train[train.investment_id==x].shape[0]: continue\n        row = train[train.investment_id==x].iloc[-1]\n        last_row = last_row.append(row)\n\n    last_row['time_id'] = last_row['time_id'].astype('int')\n    last_row['investment_id'] = last_row['investment_id'].astype('int')\n    last_row = last_row.set_index('investment_id').sort_index()\n    return last_row\n\n#train_dev = train[train.time_id<500]\nlast_row = get_last_row(train)\nlast_row.to_pickle('ubiquant-cumsum-nomalized-lastrow.pkl')\nlast_row","metadata":{"execution":{"iopub.status.busy":"2022-04-16T13:23:40.322606Z","iopub.execute_input":"2022-04-16T13:23:40.322858Z","iopub.status.idle":"2022-04-16T13:23:40.344773Z","shell.execute_reply.started":"2022-04-16T13:23:40.322823Z","shell.execute_reply":"2022-04-16T13:23:40.344058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading cumsum datasets","metadata":{}},{"cell_type":"code","source":"# load cumsum dataset\n\ntrain = pd.read_pickle('../input/ubiquant-cumsum-data-f16/ubiquant-market-prediction-cumsum-f16.pkl')\ntrain[train.time_id==1000].head()","metadata":{"execution":{"iopub.status.busy":"2022-04-16T13:23:40.346998Z","iopub.execute_input":"2022-04-16T13:23:40.347473Z","iopub.status.idle":"2022-04-16T13:23:56.401986Z","shell.execute_reply.started":"2022-04-16T13:23:40.347422Z","shell.execute_reply":"2022-04-16T13:23:56.401113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('max: ', train['target'].max(), 'min: ', train['target'].min())","metadata":{"execution":{"iopub.status.busy":"2022-04-16T13:23:56.403272Z","iopub.execute_input":"2022-04-16T13:23:56.405636Z","iopub.status.idle":"2022-04-16T13:23:56.424891Z","shell.execute_reply.started":"2022-04-16T13:23:56.405593Z","shell.execute_reply":"2022-04-16T13:23:56.424157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the special case of investment where on 2 activities exist\n\n#train.loc[pd.IndexSlice[:, 1415], 'f_5']\ntrain[train.investment_id==1415][['time_id','f_5']]","metadata":{"execution":{"iopub.status.busy":"2022-04-16T13:23:56.426162Z","iopub.execute_input":"2022-04-16T13:23:56.42642Z","iopub.status.idle":"2022-04-16T13:23:56.441152Z","shell.execute_reply.started":"2022-04-16T13:23:56.426383Z","shell.execute_reply":"2022-04-16T13:23:56.440361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('time_id.unique:', len(train.time_id.unique()))\ntrain[train.investment_id<10][['time_id','investment_id','target']].astype({'target':np.float32}).pivot(index='time_id',columns='investment_id',values='target').plot()","metadata":{"execution":{"iopub.status.busy":"2022-04-16T13:23:56.442421Z","iopub.execute_input":"2022-04-16T13:23:56.442755Z","iopub.status.idle":"2022-04-16T13:23:57.074749Z","shell.execute_reply.started":"2022-04-16T13:23:56.442719Z","shell.execute_reply":"2022-04-16T13:23:57.07404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the normalized targets and the last row\n\ntrain.set_index('investment_id', inplace=True)\nnormalize_ds = pd.DataFrame(index=train.index.unique())\nnormalize_ds[['mean','std']] = train['target'].groupby('investment_id').apply(lambda group: (group.mean(), group.std())).to_list()\ntrain.reset_index(inplace=True)\n\ntrain['target'] = pd.read_pickle('../input/ubiquant-cumsum-data-sources-2/ubiquant-cumsum-nomalized-target.pkl')\nlast_row = pd.read_pickle('../input/ubiquant-cumsum-data-sources-2/ubiquant-cumsum-nomalized-lastrow.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-04-16T13:23:57.079953Z","iopub.execute_input":"2022-04-16T13:23:57.082137Z","iopub.status.idle":"2022-04-16T13:23:58.659852Z","shell.execute_reply.started":"2022-04-16T13:23:57.08209Z","shell.execute_reply":"2022-04-16T13:23:58.659042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing and Modeling","metadata":{}},{"cell_type":"code","source":"investment_ids = train[\"investment_id\"]\ny = train[\"target\"]","metadata":{"execution":{"iopub.status.busy":"2022-04-16T13:23:58.662259Z","iopub.execute_input":"2022-04-16T13:23:58.662941Z","iopub.status.idle":"2022-04-16T13:23:58.667307Z","shell.execute_reply.started":"2022-04-16T13:23:58.662897Z","shell.execute_reply":"2022-04-16T13:23:58.666605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model_rmse(investment_ids): \n    investment_ids = list(investment_ids.unique())\n    investment_id_size = len(investment_ids) + 1\n    \n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    \n    fc = tf.feature_column.categorical_column_with_vocabulary_list(key='investment_id', vocabulary_list=investment_ids)\n    fc = tf.feature_column.embedding_column(categorical_column=fc, dimension=64)\n    investment_id_x = tf.keras.layers.DenseFeatures([fc])({'investment_id':investment_id_inputs})\n    #investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_inputs)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)    \n    investment_id_x = layers.Dropout(0.2)(investment_id_x)\n   \n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    feature_x = layers.BatchNormalization()(features_inputs)\n    feature_x = layers.GaussianNoise(0.1)(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.2)(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    #x = layers.LayerNormalization()(x)\n    output = layers.Dense(1)(x)\n    \n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.01), loss=rmse, metrics=[rmse,pearson_r])\n    return model\n\ndef preprocess(X, y):\n    return X, y\n\ndef make_dataset(feature, investment_id, y, batch_size, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n    ds = ds.map(preprocess)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\n\ndef make_test_dataset(feature, investment_id, batch_size):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)\n\ndef pearson_r(x, y):\n    axis = 1\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xvar * yvar)\n    return -corr","metadata":{"execution":{"iopub.status.busy":"2022-04-16T13:23:58.668943Z","iopub.execute_input":"2022-04-16T13:23:58.669308Z","iopub.status.idle":"2022-04-16T13:23:58.691838Z","shell.execute_reply.started":"2022-04-16T13:23:58.669251Z","shell.execute_reply":"2022-04-16T13:23:58.690994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%script echo skipping\n# train model with sample data - DEBUG\n\nres_compare = {}\nres_real, res_pred = [], []\n\ndev_ds = train[(train.time_id<500)&(train.investment_id<100)]\ndev_ds.set_index('investment_id', inplace=True)\nnormalize_ds = pd.DataFrame(index=dev_ds.index.unique())\nnormalize_ds[['mean','std']] = dev_ds['target'].groupby('investment_id').apply(lambda group: (group.mean(), group.std())).to_list()\ndev_ds.reset_index(inplace=True)\n\nlast_row = get_last_row(dev_ds)\niter_test = train_orig[(train_orig.time_id>=500)&(train_orig.time_id<600)&(train_orig.investment_id<100)]# & (train_orig.time_id<600)]\n\niter_test['row_id'] = iter_test.index\niter_test['time_id'] = iter_test['time_id'].astype('int')\niter_test['investment_id'] = iter_test['investment_id'].astype('int')\niter_test = [x[1] for x in iter_test.groupby('time_id')]\n\nfor i, test_df in tqdm(enumerate(iter_test)):\n    test_df = test_df.set_index('investment_id').sort_index()\n    pred_df = pd.DataFrame(index=test_df.index, data={'target':0.0, 'row_id':test_df.row_id})\n    #print(i, len(test_df.index))\n    \n    test_investment_ids = test_df.index.values\n    intersect = list(set(last_row.index) & set(test_df.index))\n    last_row.loc[intersect,features] = last_row.loc[intersect,features] + test_df.loc[intersect,features]\n    new_investments = test_df[~test_df.index.isin(last_row.index)]\n    if new_investments.shape[0] > 0:\n        last_row = last_row.append(new_investments)\n        normalize_ds = normalize_ds.append(pd.DataFrame(index=new_investments.index, data={'mean':0, 'std':1}))\n    \n    #print(i, 'step-2')\n    ds = make_test_dataset(last_row.loc[test_investment_ids, features].values, last_row.loc[test_investment_ids].index.values, BATCH_SIZE)\n    y_pred = inference(models, ds)\n    y_pred = pd.Series(index=test_investment_ids, data=y_pred.squeeze())\n    #print(last_row.loc[test_investment_ids, 'target'])\n    #print(y_pred)\n    \n    #print(i, 'step-3')\n    for _, row in test_df.iterrows():\n        if pred_df[pred_df.row_id==row.row_id].shape[0] == 0: \n            continue\n            \n        y_pred.loc[row.name] = y_pred.loc[row.name] * normalize_ds.loc[row.name, 'std'] + normalize_ds.loc[row.name, 'mean']\n        pred_df.loc[pred_df.row_id==row.row_id, 'target'] = y_pred.loc[row.name] - last_row.loc[row.name,'target']\n        last_row.loc[row.name,'target'] = y_pred.loc[row.name]\n    \n    #print(y_pred)\n    \n    #print(i, 'step-4')\n    res_real = res_real + test_df['target'].values.tolist()\n    res_pred = res_pred + pred_df['target'].values.tolist()\n    for _, row in test_df.iterrows():\n        investment_id = int(row.name)\n        if not investment_id in res_compare: \n            res_compare[investment_id] = {'real':[], 'pred':[]}\n        res_compare[investment_id]['real'].append(row.target)\n        res_compare[investment_id]['pred'].append(pred_df.loc[investment_id].values[0])\n\n\nres = [stats.pearsonr(res_compare[i]['real'], res_compare[i]['pred'])[0] for i in res_compare if len(res_compare[i]['real'])>2]\nprint('pearson by investment: ', np.array(res)[~np.isnan(res)].mean())\nprint('pearson total: ', stats.pearsonr(res_real, res_pred)[0])","metadata":{"execution":{"iopub.status.busy":"2022-04-16T13:23:58.693237Z","iopub.execute_input":"2022-04-16T13:23:58.69374Z","iopub.status.idle":"2022-04-16T13:23:58.73525Z","shell.execute_reply.started":"2022-04-16T13:23:58.693703Z","shell.execute_reply":"2022-04-16T13:23:58.734526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%script echo skipping\n# training the models\n\nBATCH_SIZE = 512\ndev_ds = train\nmodels = []\nkfold = TimeSeriesSplit(n_splits=5) \n#kfold = StratifiedKFold(5, random_state=42)\nfor index, (train_indices, valid_indices) in enumerate(kfold.split(dev_ds, investment_ids)):\n    #if index < 4: continue\n        \n    X_train, X_val = dev_ds.loc[train_indices, features], dev_ds.loc[valid_indices, features]\n    y_train, y_val = y.loc[train_indices], y.loc[valid_indices]\n    investment_id_train, investment_id_val = investment_ids.loc[train_indices], investment_ids.loc[valid_indices]\n    \n    train_ds = make_dataset(X_train, investment_id_train, y_train, BATCH_SIZE)\n    valid_ds = make_dataset(X_val, investment_id_val, y_val, BATCH_SIZE, mode=\"valid\")\n    model = get_model_rmse(investment_ids)\n    \n    reduce_lr  = keras.callbacks.ReduceLROnPlateau(monitor='val_pearson_r', factor=0.25, min_lr=0.0001, patience=5, mode='min', verbose=1)\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{index}.h5\", monitor='val_pearson_r', mode='min', save_best_only=True, save_weights_only=True)\n    early_stop = keras.callbacks.EarlyStopping(monitor=\"val_pearson_r\", mode='min', patience=7)\n    history = model.fit(train_ds, epochs=50, validation_data=valid_ds, callbacks=[reduce_lr, checkpoint, early_stop])\n    #plt.plot(np.array(history.history['val_pearson_r'])-history.history['val_pearson_r'][0], label='Val Metrics')\n    #plt.plot(np.array(history.history['val_loss'])-history.history['val_loss'][0], label='Val Loss')\n    #plt.legend(loc=\"upper right\")\n    #plt.show()\n\n    model.load_weights(f\"model_{index}.h5\")#, custom_objects={\"pearson_r\": pearson_r})\n    models.append(model)\n    \n    y_pred = model.predict(valid_ds).ravel()\n    print(y_pred[:10], y_pred[-10:])\n    print(y_val.values[:10], y_val.values[-10:])\n    print('Pearson:', stats.pearsonr(y_pred, y_val.values)[0])\n    print('RMSE:', np.sqrt(mean_squared_error(y_pred, y_val.values)))\n    \n    del investment_id_train\n    del investment_id_val\n    del X_train\n    del X_val\n    del y_train\n    del y_val\n    del train_ds\n    del valid_ds\n    tf.keras.backend.clear_session()\n    gc.collect()\n    #break","metadata":{"execution":{"iopub.status.busy":"2022-04-16T13:23:58.736705Z","iopub.execute_input":"2022-04-16T13:23:58.738235Z","iopub.status.idle":"2022-04-16T13:23:58.770228Z","shell.execute_reply.started":"2022-04-16T13:23:58.738201Z","shell.execute_reply":"2022-04-16T13:23:58.769498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 512\nmodels = []\n\nfor index in range(4):\n    model = get_model_rmse(investment_ids)\n    #model.load_weights(f\"../input/ubiquant-5folds-cumsum/model_{index}.h5\")\n    model.load_weights(f\"../input/ubiquant-4folds-cumsum/model_{index}.h5\")\n    models.append(model)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T13:23:58.771747Z","iopub.execute_input":"2022-04-16T13:23:58.772089Z","iopub.status.idle":"2022-04-16T13:24:01.678561Z","shell.execute_reply.started":"2022-04-16T13:23:58.77205Z","shell.execute_reply":"2022-04-16T13:24:01.677708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nimport sys\nenv = ubiquant.make_env()\niter_test = env.iter_test() \n\nfor i, (test_df, pred_df) in enumerate(iter_test):\n    test_df = test_df.set_index('investment_id').sort_index()\n    \n    # update last row with new data\n    test_investment_ids = test_df.index.values\n    intersect = list(set(last_row.index) & set(test_df.index))\n    last_row.loc[intersect,features] = last_row.loc[intersect,features] + test_df.loc[intersect,features]\n    new_investments = test_df[~test_df.index.isin(last_row.index)]\n    \n    # update the last row if it didn't exist yes because of the new investment_id\n    if new_investments.shape[0] > 0:\n        last_row = last_row.append(new_investments)\n        normalize_ds = normalize_ds.append(pd.DataFrame(index=new_investments.index, data={'mean':0, 'std':1}))\n        \n    # predict new values based on existing \"last row\".\n    # here we obtain the real absolute values and not the deltas that we go on with\n    ds = make_test_dataset(last_row.loc[test_investment_ids, features].values, last_row.loc[test_investment_ids].index.values, BATCH_SIZE)\n    y_pred = inference(models, ds)\n    y_pred = pd.Series(index=test_investment_ids, data=y_pred.squeeze())\n    \n    for _, row in test_df.iterrows():\n        if pred_df[pred_df.row_id==row.row_id].shape[0] == 0: \n            continue\n            \n        # reverse-normalizing the predicted values\n        y_pred.loc[row.name] = y_pred.loc[row.name] * normalize_ds.loc[row.name, 'std'] + normalize_ds.loc[row.name, 'mean']\n        \n        # calulate the target deltas because we obrained the real absolute values \n        pred_df.loc[pred_df.row_id==row.row_id, 'target'] = y_pred.loc[row.name] - last_row.loc[row.name,'target']\n        last_row.loc[row.name,'target'] = y_pred.loc[row.name]\n    \n    env.predict(pred_df.fillna(0)) ","metadata":{"execution":{"iopub.status.busy":"2022-04-16T13:24:01.680805Z","iopub.execute_input":"2022-04-16T13:24:01.681038Z","iopub.status.idle":"2022-04-16T13:24:06.582094Z","shell.execute_reply.started":"2022-04-16T13:24:01.68101Z","shell.execute_reply":"2022-04-16T13:24:06.581281Z"},"trusted":true},"execution_count":null,"outputs":[]}]}