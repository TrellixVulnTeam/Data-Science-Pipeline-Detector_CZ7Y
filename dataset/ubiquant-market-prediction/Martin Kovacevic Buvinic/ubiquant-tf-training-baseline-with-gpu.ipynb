{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom scipy.stats import pearsonr\nfrom sklearn.model_selection import GroupKFold\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import backend as K\nfrom tqdm.notebook import tqdm\nimport random\nimport warnings\nimport gc\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 100)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T13:56:13.16789Z","iopub.execute_input":"2022-01-31T13:56:13.168405Z","iopub.status.idle":"2022-01-31T13:56:20.161387Z","shell.execute_reply.started":"2022-01-31T13:56:13.168269Z","shell.execute_reply":"2022-01-31T13:56:20.160324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration\nEPOCHS = 20\nBATCH_SIZE = 256\n# Model Seed \nMODEL_SEED = 42\n# Learning rate\nLR = 0.001\n# Folds\nFOLDS = 5\n# Verbosity\nVERBOSE = 2","metadata":{"execution":{"iopub.status.busy":"2022-01-31T13:56:55.883417Z","iopub.execute_input":"2022-01-31T13:56:55.884028Z","iopub.status.idle":"2022-01-31T13:56:55.889766Z","shell.execute_reply.started":"2022-01-31T13:56:55.883965Z","shell.execute_reply":"2022-01-31T13:56:55.88862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to seed everything\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n    \ndef correlationLoss(x, y, axis = -2):\n    \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n    while trying to have the same mean and variance\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = tf.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis = axis)\n    ysum = tf.reduce_sum(y, axis = axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xsqsum = tf.reduce_sum(tf.math.squared_difference(x, xmean), axis = axis)\n    ysqsum = tf.reduce_sum(tf.math.squared_difference(y, ymean), axis = axis)\n    cov = tf.reduce_sum((x - xmean) * (y - ymean), axis = axis)\n    corr = cov / tf.sqrt(xsqsum * ysqsum)\n    sqdif = tf.reduce_sum(tf.math.squared_difference(x, y), axis = axis) / n / tf.sqrt(ysqsum / n)\n    return tf.convert_to_tensor(K.mean(tf.constant(1.0, dtype = x.dtype) - corr + (0.01 * sqdif)) , dtype = tf.float32 )\n\n# Function to build our model\ndef build_model(shape, steps):\n    def fc_block(x, units, dropout):\n        x = tf.keras.layers.Dropout(dropout)(x)\n        x = tf.keras.layers.Dense(units, activation = 'swish')(x)\n        return x\n    # Input layer\n    inp = tf.keras.layers.Input(shape = (shape))\n    # Encoder block\n    encoder = tf.keras.layers.GaussianNoise(0.015)(inp)\n    encoder = tf.keras.layers.Dense(96)(encoder)\n    encoder = tf.keras.layers.Activation('swish')(encoder)\n    # Decoder block to predict the input to generate more features\n    decoder = tf.keras.layers.Dropout(0.03)(encoder)\n    decoder = tf.keras.layers.Dense(shape, activation = 'linear', name = 'decoder')(decoder)\n    # Autoencoder\n    autoencoder = tf.keras.layers.Dense(96)(decoder)\n    autoencoder = tf.keras.layers.Activation('swish')(autoencoder)\n    autoencoder = tf.keras.layers.Dropout(0.40)(autoencoder)\n    out_autoencoder = tf.keras.layers.Dense(1, activation = 'linear', name = 'autoencoder')(autoencoder)\n    # Concatenate input and encoder output for extra features\n    x = tf.keras.layers.Concatenate()([inp, encoder])\n    x = fc_block(x, units = 1024, dropout = 0.4)\n    x = fc_block(x, units = 512, dropout = 0.4)\n    x = fc_block(x, units = 256, dropout = 0.4)\n    output = tf.keras.layers.Dense(1, activation = 'linear', name = 'mlp')(x)\n    model = tf.keras.models.Model(inputs = [inp], outputs = [decoder, out_autoencoder, output])\n    scheduler = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate = LR, decay_steps = steps, end_learning_rate = 0.00001)\n    opt = tf.keras.optimizers.Adam(learning_rate = scheduler)\n    model.compile(\n        optimizer = opt,\n        loss = [tf.keras.losses.MeanSquaredError(), tf.keras.losses.MeanSquaredError(), tf.keras.losses.MeanSquaredError()],\n    )\n    return model\n\n# Calculate pearson correlation coefficient\ndef pearson_coef(data):\n    return data.corr()['target']['prediction']\n\n# Calculate mean pearson correlation coefficient\ndef comp_metric(valid_df):\n    return np.mean(valid_df.groupby(['time_id']).apply(pearson_coef))\n\n# Function to train and evaluate\ndef train_and_evaluate():\n    # Seed everything\n    seed_everything(MODEL_SEED)\n    # Read data\n    train = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\n    # Feature list\n    features = [col for col in train.columns if col not in ['row_id', 'time_id', 'investment_id', 'target']]\n    # Some feature engineering\n    # Get the correlations with the target to encode time_id\n    corr1 = train[features[0:100] + ['target']].corr()['target'].reset_index()\n    corr2 = train[features[100:200] + ['target']].corr()['target'].reset_index()\n    corr3 = train[features[200:] + ['target']].corr()['target'].reset_index()\n    corr = pd.concat([corr1, corr2, corr3], axis = 0, ignore_index = True)\n    corr['target'] = abs(corr['target'])\n    corr.sort_values('target', ascending = False, inplace = True)\n    best_corr = corr.iloc[3:53, 0].to_list()\n    del corr1, corr2, corr3, corr\n    # Add time id related features (market general features to relate time_ids)\n    time_id_features = []\n    for col in tqdm(best_corr):\n        mapper = train.groupby(['time_id'])[col].mean().to_dict()\n        train[f'time_id_{col}'] = train['time_id'].map(mapper)\n        train[f'time_id_{col}'] = train[f'time_id_{col}'].astype(np.float16)\n        time_id_features.append(f'time_id_{col}')\n    print(f'We added {len(time_id_features)} features related to time_id')\n    # Update feature list\n    features += time_id_features\n    np.save('features.npy', np.array(features))\n    np.save('best_corr.npy', np.array(best_corr))\n    # Store out of folds predictions\n    oof_predictions = np.zeros(len(train))\n    # Initiate GroupKFold\n    kfold = GroupKFold(n_splits = FOLDS)\n    # Create groups based on time_id\n    train.loc[(train['time_id'] >= 0) & (train['time_id'] < 280), 'group'] = 0\n    train.loc[(train['time_id'] >= 280) & (train['time_id'] < 585), 'group'] = 1\n    train.loc[(train['time_id'] >= 585) & (train['time_id'] < 825), 'group'] = 2\n    train.loc[(train['time_id'] >= 825) & (train['time_id'] < 1030), 'group'] = 3\n    train.loc[(train['time_id'] >= 1030) & (train['time_id'] < 1400), 'group'] = 4\n    train['group'] = train['group'].astype(np.int16)\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, groups = train['group'])):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = train[features].loc[trn_ind], train[features].loc[val_ind]\n        y_train, y_val = train['target'].loc[trn_ind], train['target'].loc[val_ind]\n        # Reset keras session\n        K.clear_session()\n        n_training_rows = x_train.shape[0]\n        n_validation_rows = x_val.shape[0]\n        STEPS_PER_EPOCH = n_training_rows  // BATCH_SIZE\n        # Build simple fc model\n        print('Building model...')\n        model = build_model(len(features), STEPS_PER_EPOCH * EPOCHS)\n        print(f'Training with {n_training_rows} rows')\n        print(f'Validating with {n_validation_rows} rows')\n        print(f'Training model with {len(features)} features...')\n        # Callbacks\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n            f'simple_fc_dnn_{fold + 1}.h5', \n            monitor = 'val_mlp_loss', \n            verbose = VERBOSE, \n            save_best_only = True,\n            save_weights_only = True, \n            mode = 'min', \n            save_freq = 'epoch'\n        )\n        # Train and evaluate\n        history = model.fit(\n            x = x_train,\n            y = (x_train, y_train, y_train),\n            batch_size = BATCH_SIZE,\n            epochs = EPOCHS,\n            verbose = VERBOSE,\n            callbacks = [checkpoint],\n            validation_data = (x_val, (x_val, y_val, y_val)),\n        )\n        # Load best weights\n        model.load_weights(f'simple_fc_dnn_{fold + 1}.h5')\n        # Predict validation set\n        val_pred = model.predict(x_val, batch_size = BATCH_SIZE)[2].reshape(-1)\n        # Add validation prediction to out of folds array\n        oof_predictions[val_ind] = val_pred\n        del x_train, x_val, y_train, y_val\n        gc.collect()\n    # Compute out of folds Pearson Correlation Coefficient (for each time_id)\n    oof_df = pd.DataFrame({'time_id': train['time_id'], 'target': train['target'], 'prediction': oof_predictions})\n    # Save out of folds csv for blending\n    oof_df.to_csv('simple_fc_dnn.csv', index = False)\n    score = comp_metric(oof_df)\n    print(f'Our out of folds mean pearson correlation coefficient is {score}')\n    \ntrain_and_evaluate()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T13:56:20.175348Z","iopub.execute_input":"2022-01-31T13:56:20.175629Z","iopub.status.idle":"2022-01-31T13:56:20.227267Z","shell.execute_reply.started":"2022-01-31T13:56:20.175589Z","shell.execute_reply":"2022-01-31T13:56:20.226142Z"},"trusted":true},"execution_count":null,"outputs":[]}]}