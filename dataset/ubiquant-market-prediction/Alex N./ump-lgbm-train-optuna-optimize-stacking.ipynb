{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Thanks to [valley](http://https://www.kaggle.com/valleyzw) for his wonderful [notebook](https://www.kaggle.com/code/valleyzw/ubiquant-lgbm-baseline?scriptVersionId=89116394). I used it as a baseline. In the final version (ver. 47) I didn't use any additional data and supplemental training. CV/LB of the final version is 0.147/0.1437\n\n# Prepare data","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport joblib\nimport random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom datetime import datetime\nfrom argparse import Namespace\nfrom collections import defaultdict\nfrom scipy.signal import find_peaks\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, GroupKFold, train_test_split, KFold\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\n\nimport matplotlib.style as style\nstyle.use('fivethirtyeight') \n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_rows = 100\npd.options.display.max_columns = 100\n\ndef seed_everything(seed: int = 42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","scrolled":true,"execution":{"iopub.status.busy":"2022-04-12T18:12:34.72423Z","iopub.execute_input":"2022-04-12T18:12:34.727043Z","iopub.status.idle":"2022-04-12T18:12:37.05307Z","shell.execute_reply.started":"2022-04-12T18:12:34.726583Z","shell.execute_reply":"2022-04-12T18:12:37.052152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Configuration","metadata":{}},{"cell_type":"code","source":"args = Namespace(\n    train=False,\n    optimize=False,\n    train_l2=False,\n    inference=True,\n    seed=21,\n    folds=5,\n    workers=4,\n    min_time_id=None, \n    holdout=False,\n    cv_method=\"single\",\n    num_bins=16,\n    holdout_size=100,\n    outlier_threshold=0.005,\n    trading_days_per_year=250,   # chinese stock market trading days per year (roughly)\n    add_investment_id_model=False,\n    data_path=Path(\"../input/ubiquant-parquet\"),\n    just_eda=True,\n)\nseed_everything(args.seed)\n\nassert args.cv_method in {\"single\", \"kfold\", \"group\", \"stratified\", \"time\", \"group_time\", \"time_range\"}, \"unknown cv method\"\nassert args.data_path.exists(), \"data_path not exists\"","metadata":{"execution":{"iopub.status.busy":"2022-04-12T18:12:37.054635Z","iopub.execute_input":"2022-04-12T18:12:37.054901Z","iopub.status.idle":"2022-04-12T18:12:37.063499Z","shell.execute_reply.started":"2022-04-12T18:12:37.054865Z","shell.execute_reply":"2022-04-12T18:12:37.06252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the data","metadata":{}},{"cell_type":"code","source":"%%time\ntrain = pd.read_parquet(args.data_path.joinpath(\"train_low_mem.parquet\"))\n# assert train.isnull().any().sum() == 0, \"null exists.\"\n# assert train.row_id.str.extract(r\"(?P<time_id>\\d+)_(?P<investment_id>\\d+)\").astype(train.time_id.dtype).equals(train[[\"time_id\", \"investment_id\"]]), \"row_id!=time_id_investment_id\"\n# assert train.time_id.is_monotonic_increasing, \"time_id not monotonic increasing\"","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-12T18:12:37.064911Z","iopub.execute_input":"2022-04-12T18:12:37.065253Z","iopub.status.idle":"2022-04-12T18:13:02.099707Z","shell.execute_reply.started":"2022-04-12T18:12:37.065216Z","shell.execute_reply":"2022-04-12T18:13:02.09875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features EDA + processing\n\n### Possible outliers","metadata":{}},{"cell_type":"code","source":"df = train[[\"investment_id\", \"target\"]].groupby(\"investment_id\").target.mean()\nupper_bound, lower_bound = df.quantile([1-args.outlier_threshold, args.outlier_threshold])\ndisplay(upper_bound, lower_bound)\nax = df.plot(figsize=(16, 8))\nax.axhspan(lower_bound, upper_bound, fill=False, linestyle=\"--\", color=\"k\")\nplt.show()\n\noutlier_investments = df.loc[(df>upper_bound)|(df<lower_bound)|(df==0)].index\n_=pd.pivot(\n    train.loc[train.investment_id.isin(outlier_investments), [\"investment_id\", \"time_id\", \"target\"]],\n    index='time_id', columns='investment_id', values='target'\n).plot(figsize=(16,12), subplots=True, sharex=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T18:13:02.101059Z","iopub.execute_input":"2022-04-12T18:13:02.101789Z","iopub.status.idle":"2022-04-12T18:13:08.419069Z","shell.execute_reply.started":"2022-04-12T18:13:02.101753Z","shell.execute_reply":"2022-04-12T18:13:08.418385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop indexes with feature outliers: [notebook](https://www.kaggle.com/junjitakeshima/ubiquant-simple-lgbm-removing-outliers-en-jp)","metadata":{}},{"cell_type":"code","source":"outlier_list = []\noutlier_col = []\n\nfor col in (f\"f_{i}\" for i in range(300)):\n    _mean, _std = train[col].mean(), train[col].std()\n    \n    temp_df = train.loc[(train[col] > _mean + _std * 70) | (train[col] < _mean - _std * 70)]\n    temp2_df = train.loc[(train[col] > _mean + _std * 35) | (train[col] < _mean - _std * 35)]\n    if len(temp_df) >0 : \n        outliers = temp_df.index.to_list()\n        outlier_list.extend(outliers)\n        outlier_col.append(col)\n        print(col, len(temp_df))\n    elif len(temp2_df)>0 and len(temp2_df) <6 :\n        outliers = temp2_df.index.to_list()\n        outlier_list.extend(outliers)\n        outlier_col.append(col)\n        print(col, len(temp2_df))\n\noutlier_list = list(set(outlier_list))\ntrain.drop(train.index[outlier_list], inplace=True)\nprint(len(outlier_col), len(outlier_list), train.shape)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-12T18:13:08.420689Z","iopub.execute_input":"2022-04-12T18:13:08.420926Z","iopub.status.idle":"2022-04-12T18:13:20.37518Z","shell.execute_reply.started":"2022-04-12T18:13:08.420888Z","shell.execute_reply":"2022-04-12T18:13:20.37424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop outliers from train","metadata":{}},{"cell_type":"code","source":"if args.min_time_id is not None:\n    train = train.query(\"time_id>=@args.min_time_id\").reset_index(drop=True)\n    gc.collect()\n    \ntrain=train.loc[~train.investment_id.isin(outlier_investments)].reset_index(drop=True)\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-12T18:13:20.376386Z","iopub.execute_input":"2022-04-12T18:13:20.376605Z","iopub.status.idle":"2022-04-12T18:13:29.3799Z","shell.execute_reply.started":"2022-04-12T18:13:20.376579Z","shell.execute_reply":"2022-04-12T18:13:29.378965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Strange behaviour of some features according to time id\n\n### f_1","metadata":{}},{"cell_type":"code","source":"def get_unique_total_values_ratio(df, feature, plot=False, return_result=False):\n    df_date_group = df.groupby('time_id').agg({feature: [lambda x: len(x.unique()), 'count']})\n    df_date_group.columns.set_levels(['len_unique','count'], level=1,inplace=True)\n    df_date_group['unique_count_ratio'] = df_date_group[feature]['len_unique']/df_date_group[feature]['count']\n    df_date_group['num_or_cat'] = df_date_group['unique_count_ratio'].apply(lambda x: 1 if x > 0.5 else 0)\n    if plot:\n        fig = plt.figure(figsize=(14, 6))\n        ax = fig.add_subplot(111)\n        plt.plot(df_date_group.index, df_date_group['unique_count_ratio'] , label=\"Number of unique investments\")\n    if return_result:\n        return df_date_group['num_or_cat']\n\nget_unique_total_values_ratio(train, 'f_1', plot=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T18:13:29.381559Z","iopub.execute_input":"2022-04-12T18:13:29.382268Z","iopub.status.idle":"2022-04-12T18:13:29.845588Z","shell.execute_reply.started":"2022-04-12T18:13:29.382225Z","shell.execute_reply":"2022-04-12T18:13:29.844621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### f_7","metadata":{}},{"cell_type":"code","source":"num_or_cat = get_unique_total_values_ratio(train, 'f_7', plot=True, return_result=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T18:13:29.846574Z","iopub.execute_input":"2022-04-12T18:13:29.846779Z","iopub.status.idle":"2022-04-12T18:13:30.285028Z","shell.execute_reply.started":"2022-04-12T18:13:29.846755Z","shell.execute_reply":"2022-04-12T18:13:30.284148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add time group feature","metadata":{}},{"cell_type":"code","source":"train.loc[(train['time_id'] < 910), 'group'] = 0\ntrain.loc[(train['time_id'] >= 910) & (train['time_id'] < 970), 'group'] = 1\ntrain.loc[(train['time_id'] >= 970) & (train['time_id'] < 1030), 'group'] = 2\ntrain.loc[(train['time_id'] >= 1030) & (train['time_id'] < 1090), 'group'] = 3\ntrain.loc[(train['time_id'] >= 1090) & (train['time_id'] < 1150), 'group'] = 4\ntrain.loc[(train['time_id'] >= 1150), 'group'] = 5\ntrain['group'] = train['group'].astype(np.int16)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T18:13:30.334365Z","iopub.execute_input":"2022-04-12T18:13:30.334915Z","iopub.status.idle":"2022-04-12T18:13:30.509238Z","shell.execute_reply.started":"2022-04-12T18:13:30.334878Z","shell.execute_reply":"2022-04-12T18:13:30.508248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add feature that shows if some features are categorical or numerical for the current time_id","metadata":{}},{"cell_type":"code","source":"# add numerical/categorical flag\ntrain = train.merge(num_or_cat, how='left', on='time_id')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T18:13:30.513038Z","iopub.execute_input":"2022-04-12T18:13:30.513711Z","iopub.status.idle":"2022-04-12T18:13:33.721145Z","shell.execute_reply.started":"2022-04-12T18:13:30.513658Z","shell.execute_reply":"2022-04-12T18:13:33.720333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Some features are not stationary. Let's make a rank transformation by time_id to make them stationary","metadata":{}},{"cell_type":"code","source":"train['f_74']  = train[['time_id', 'f_74']].groupby('time_id').rank(pct=True)\ntrain['f_142']  = train[['time_id', 'f_142']].groupby('time_id').rank(pct=True)\ntrain['f_63']  = train[['time_id', 'f_63']].groupby('time_id').rank(pct=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T18:13:33.722397Z","iopub.execute_input":"2022-04-12T18:13:33.723046Z","iopub.status.idle":"2022-04-12T18:13:39.816335Z","shell.execute_reply.started":"2022-04-12T18:13:33.723Z","shell.execute_reply":"2022-04-12T18:13:39.815508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make combinations for some features and drop not necessary features.","metadata":{}},{"cell_type":"code","source":"num_features = [c for c in train.columns if c.startswith('f_')]\ncat_features = ['num_or_cat'] #, 'sub_section', 'main_section']\nfeatures = num_features + cat_features + ['time_id'] #, 'year', 'quarter', 'market_value']\n\n# make feature combinations\ncombination_features = [\"f_231-f_250\", \"f_118-f_280\", \"f_155-f_297\", \"f_25-f_237\", \"f_179-f_265\", \"f_119-f_270\", \"f_71-f_197\", \"f_21-f_65\"]\nfor f in combination_features:\n    f1, f2 = f.split(\"-\")\n    train[f] = train[f1] + train[f2]\nfeatures += combination_features\n\n# drop unnecessary features\nto_drop = [\"f_148\", \"f_72\", \"f_49\", \"f_205\", \"f_228\", \"f_97\", \"f_262\", \"f_258\"]\nfeatures = list(sorted(set(features).difference(set(to_drop))))\ntrain = train.drop(columns=[\"row_id\"] + to_drop)\n\nfeatures_backup = features.copy()\ngc.collect()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-12T18:13:39.845948Z","iopub.execute_input":"2022-04-12T18:13:39.846283Z","iopub.status.idle":"2022-04-12T18:13:45.470845Z","shell.execute_reply.started":"2022-04-12T18:13:39.846249Z","shell.execute_reply":"2022-04-12T18:13:45.469883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reduce memory usage.","metadata":{}},{"cell_type":"code","source":"%%time\n\ntrain = reduce_mem_usage(train)\ntrain[[\"investment_id\", \"time_id\"]] = train[[\"investment_id\", \"time_id\"]].astype(np.uint16)\ngc.collect()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-12T18:13:45.472016Z","iopub.execute_input":"2022-04-12T18:13:45.472248Z","iopub.status.idle":"2022-04-12T18:16:54.679479Z","shell.execute_reply.started":"2022-04-12T18:13:45.472221Z","shell.execute_reply":"2022-04-12T18:16:54.67855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Standard scale numerical features","metadata":{}},{"cell_type":"code","source":"%%time\n\nsscaler = StandardScaler()\nfeatures_to_scale = [c for c in train.columns if c.startswith('f_')]\ntrain[features_to_scale] = sscaler.fit_transform(train[features_to_scale])","metadata":{"execution":{"iopub.status.busy":"2022-04-12T18:16:54.681007Z","iopub.execute_input":"2022-04-12T18:16:54.681322Z","iopub.status.idle":"2022-04-12T18:18:10.046563Z","shell.execute_reply.started":"2022-04-12T18:16:54.68128Z","shell.execute_reply":"2022-04-12T18:18:10.045589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train\n\n### Set scoring and training functions","metadata":{}},{"cell_type":"code","source":"def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\n# TODO: replace with feval_pearsonr\ndef feval_rmse(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'rmse', rmse(y_true, y_pred), False\n\n# https://www.kaggle.com/c/ubiquant-market-prediction/discussion/302480\ndef feval_pearsonr(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'pearsonr', pearsonr(y_true, y_pred)[0], True\n\n# https://www.kaggle.com/gogo827jz/jane-street-supervised-autoencoder-mlp?scriptVersionId=73762661\n# weighted average as per Donate et al.'s formula\n# https://doi.org/10.1016/j.neucom.2012.02.053\n# [0.0625, 0.0625, 0.125, 0.25, 0.5] for 5 fold\ndef weighted_average(a):\n    w = []\n    n = len(a)\n    for j in range(1, n + 1):\n        j = 2 if j == 1 else j\n        w.append(1 / (2**(n + 1 - j)))\n    return np.average(a, weights = w)\n\ndef run(info):    \n    # hyperparams from: https://www.kaggle.com/valleyzw/ubiquant-lgbm-optimization\n    params = {\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0,\n        'learning_rate':0.05,\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        'boosting_type': \"gbdt\",\n        'verbosity': 0,\n        'n_jobs': -1, \n        'seed': 42,\n#         'lambda_l1': 0.03627602394442367, \n#         'lambda_l2': 0.43523855951142926, \n#         'num_leaves': 114, \n#         'feature_fraction': 0.9505625064462319, \n#         'bagging_fraction': 0.9785558707339647, \n#         'bagging_freq': 7, \n#         'max_depth': -1, \n#         'max_bin': 501, \n#         'min_data_in_leaf': 374,\n        'n_estimators': 1000, \n    }\n    \n    y = train['target']\n    train['preds'] = -1000\n    scores = defaultdict(list)\n    features_importance= []\n    \n    def run_lgbm(trn_ind, val_ind):\n        train_dataset = lgb.Dataset(train.loc[trn_ind, features], y.loc[trn_ind], categorical_feature=cat_features)\n        valid_dataset = lgb.Dataset(train.loc[val_ind, features], y.loc[val_ind], categorical_feature=cat_features)\n        model = lgb.train(\n            params,\n            train_set = train_dataset, \n            valid_sets = [train_dataset, valid_dataset], \n            callbacks=[lgb.log_evaluation(50), lgb.early_stopping(stopping_rounds=50)],\n            feval = feval_pearsonr\n        )\n        joblib.dump(model, f'lgbm_seed_42_{info}.pkl')\n        preds = model.predict(train.loc[val_ind, features])\n        train.loc[val_ind, \"preds\"] = preds\n        rmse_score = rmse(y.loc[val_ind], preds)\n        pearsonr_score = pearsonr(y.loc[val_ind], preds)[0]\n        features_importance = pd.DataFrame({'feature': features, 'importance': model.feature_importance()})\n        del train_dataset, valid_dataset\n        gc.collect()\n        return rmse_score, pearsonr_score, features_importance\n        \n    time_thresh = round(train.time_id.max() * 0.9)\n    trn_ind, val_ind = train.time_id < time_thresh, train.time_id >= time_thresh\n    print(f\"train length: {trn_ind.sum()}, valid length: {val_ind.sum()}\")\n    rmse_score, pearsonr_score, features_importance = run_lgbm(trn_ind, val_ind)\n    \n#     train.filter(regex=r\"^(?!f_).*\").to_csv(f\"preds_{info}.csv\", index=False)\n    return features_importance, scores","metadata":{"execution":{"iopub.status.busy":"2022-04-12T18:18:10.048086Z","iopub.execute_input":"2022-04-12T18:18:10.048311Z","iopub.status.idle":"2022-04-12T18:18:10.063406Z","shell.execute_reply.started":"2022-04-12T18:18:10.048284Z","shell.execute_reply":"2022-04-12T18:18:10.062329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train with deafault params","metadata":{}},{"cell_type":"code","source":"%%time\n\nif args.train:\n    info = \"\"\n    features_importance, scores = run(info=info)\n    df = train[[\"target\", \"preds\", \"time_id\"]].query(\"preds!=-1000\")\n    pearsonr_by_time_id = df.groupby(\"time_id\").apply(lambda x: pearsonr(x.target, x.preds)[0]).mean()\n    print(f\"lgbm rmse: {rmse(df.target, df.preds):.4f}, pearsonr: {pearsonr(df.target, df.preds)[0]:.4f}, pearsonr by time_id: {pearsonr_by_time_id:.4f}\")\n\n#     features_importance.to_csv(f\"features_importance_{info}.csv\", index=False)\n\n    plt.figure(figsize=(16, 10))\n    plt.subplot(1,2,1)\n    sns.barplot(x=\"importance\", y=\"feature\", data=features_importance.sort_values('importance', ascending=False).head(50))\n    plt.title(f'Head LightGBM Features {info}')\n    plt.subplot(1,2,2)\n    sns.barplot(x=\"importance\", y=\"feature\", data=features_importance.sort_values('importance', ascending=False).tail(50))\n    plt.title(f'Tail LightGBM Features {info}')\n    plt.tight_layout()\n    plt.show()\n    del df\n\n# del train\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T18:18:10.065017Z","iopub.execute_input":"2022-04-12T18:18:10.065351Z","iopub.status.idle":"2022-04-12T18:18:10.085292Z","shell.execute_reply.started":"2022-04-12T18:18:10.065309Z","shell.execute_reply":"2022-04-12T18:18:10.084334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimize LGBM with Optuna","metadata":{}},{"cell_type":"code","source":"import optuna\nimport warnings\nwarnings.filterwarnings(\"ignore\", module=\"lightgbm\")\nfrom sklearn.metrics import mean_squared_error\n\n# FYI: Objective functions can take additional arguments\n# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\ndef objective(trial):\n    # https://www.kaggle.com/c/ubiquant-market-prediction/discussion/302480\n    def feval_pearsonr(y_pred, lgb_train):\n        y_true = lgb_train.get_label()\n        return 'pearsonr', pearsonr(y_true, y_pred)[0], True\n    \n    time_thresh = round(train.time_id.max() * 0.9)\n    trn_ind, val_ind = train.time_id < time_thresh, train.time_id >= time_thresh\n    \n    X_train, y_train = train.loc[trn_ind, features], train.loc[trn_ind, 'target']\n    X_val, y_val = train.loc[val_ind, features], train.loc[val_ind, 'target']\n    \n    dtrain = lgb.Dataset(X_train, y_train, categorical_feature=cat_features)\n    dvalid = lgb.Dataset(X_val, y_val, categorical_feature=cat_features)\n\n    param = {\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0,\n        'objective': 'regression',\n        'metric': 'rmse',\n        'seed': 42,\n        'verbosity': 0,\n        'boosting_type': 'gbdt', # other options: rf, dart, goss\n        'force_col_wise': False, # Use only with CPU devices\n       \n        'subsample_for_bin': 300000, # Number of data that sampled to construct feature discrete bins; setting this \n                                     # to larger value will give better training result but may increase train time\n        'n_estimators': trial.suggest_int('n_estimators', 500, 1200),      \n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-2, 1e-1),\n        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256), # Max number of leaves in one tree\n        'max_bin': trial.suggest_int('max_bin', 32, 255), # Max number of bins that feature values will be \n                                                           # bucketed in. small number of bins may reduce training \n                                                           # accuracy but may deal with overfitting\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0), # Randomly select a subset of features \n                                                                               # if feature_fraction < 1.0\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0), # Randomly select part of data without \n                                                                               # resampling if bagging_fraction < 1.0\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7), # Perform bagging at every k iteration\n        'min_data_in_leaf': trial.suggest_int('min_child_samples', 5, 100), # Minimal number of data in one leaf\n                                                                            # aliases: min_child_samples, \n        'min_sum_hessian_in_leaf': trial.suggest_float('min_sum_hessian_in_leaf', 1e-4, 1e-1), # Stop trying to split \n                                                                                               # leave if sum of it's\n                                                                                               # hessian less than k\n        'cat_smooth': trial.suggest_float('cat_smooth', 10.0, 100.0), # this can reduce the effect of noises in \n                                                                      # categorical features, especially for \n                                                                      # categories with few data\n    }\n\n    # Add a callback for pruning.\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, 'pearsonr', valid_name='valid_1')\n    gbm = lgb.train(\n        param, \n        train_set = dtrain, \n        valid_sets = [dtrain, dvalid], \n        feval = feval_pearsonr,\n        callbacks = [lgb.log_evaluation(100),\n                     pruning_callback]\n    )\n\n    y_pred = gbm.predict(X_val)\n    corr = pearsonr(y_val, y_pred)[0]\n    return corr\n\n\nif args.optimize is True:\n    study = optuna.create_study(\n        pruner=optuna.pruners.MedianPruner(n_warmup_steps=100), direction=\"maximize\"\n    )\n    study.optimize(objective, timeout=10*3600)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-12T18:18:10.138726Z","iopub.execute_input":"2022-04-12T18:18:10.13944Z","iopub.status.idle":"2022-04-12T18:18:10.767291Z","shell.execute_reply.started":"2022-04-12T18:18:10.139393Z","shell.execute_reply":"2022-04-12T18:18:10.76642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stacking best LGBM models\n\n### Get best parameters","metadata":{}},{"cell_type":"code","source":"if args.train_l2 is True:\n    lgb_params = study.trials_dataframe()\n    lgb_params.to_pickle('LGBM_Optuna_params.pkl')\n    lgb_params = pd.read_pickle('LGBM_Optuna_params.pkl').sort_values('value', ascending=False).head(5)\n\n    param_cols = [c for c in lgb_params.columns if c.startswith('params_')]\n    lgb_params = lgb_params[param_cols]\n\n    best_params = list()\n\n    for idx, row in lgb_params.iterrows():\n        row_dict = {k[7:]: v for k, v in row.items()}\n        row_dict['device'] = 'gpu'\n        row_dict['gpu_platform_id'] = 0\n        row_dict['gpu_device_id'] = 0\n        row_dict['verbosity'] = 0\n        row_dict['max_bin'] = int(row_dict['max_bin'])\n        row_dict['bagging_freq'] = int(row_dict['bagging_freq'])\n        row_dict['min_child_samples'] = int(row_dict['min_child_samples'])\n        row_dict['n_estimators'] = int(row_dict['n_estimators'])\n        row_dict['num_leaves'] = int(row_dict['num_leaves'])\n        best_params.append(row_dict)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-12T18:18:10.768441Z","iopub.execute_input":"2022-04-12T18:18:10.769057Z","iopub.status.idle":"2022-04-12T18:18:10.776966Z","shell.execute_reply.started":"2022-04-12T18:18:10.769021Z","shell.execute_reply":"2022-04-12T18:18:10.776077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create metafeatures for the train set","metadata":{}},{"cell_type":"code","source":"if args.train_l2 is True:\n    groups_level2 = [1, 2, 3, 4, 5]\n\n    # That is how we get target for the 2nd level dataset\n    y_train_level2 = train.loc[train.group.isin(groups_level2), 'target']\n\n    # Create 1st level feature matrix\n    X_train, y_train = train[features], train['target']\n    \n    # And here we create 2nd level feature matrix, init it with zeros first\n    X_train_level2 = np.zeros([y_train_level2.shape[0], len(best_params)+1])\n    X_train_level2[:, len(best_params)] = y_train_level2\n\n    meta_index_begin = 0\n    meta_index_end = 0\n\n    # Now fill `X_train_level2` with metafeatures\n    for current_group in tqdm(groups_level2):\n        # split data\n        train_index = X_train.loc[train.group <  current_group].index\n        test_index  = X_train.loc[train.group == current_group].index\n\n        X_train_l2 = X_train.loc[train_index, :]\n        X_test_l2 =  X_train.loc[test_index, :]\n\n        y_train_l2 = y_train[train_index]\n        y_test_l2 =  y_train[test_index]\n\n        meta_index_end += y_test_l2.shape[0]\n\n        print(f\"===================== time group: {current_group} =====================\")\n        \n        # predict metafeatures for each of LGBM regressors\n        for i, params in enumerate(tqdm(best_params)):\n            print(f\"===================== model: {i} =====================\")\n            train_dataset = lgb.Dataset(X_train_l2, y_train_l2, categorical_feature=cat_features)\n            model = lgb.train(\n                params,\n                train_set = train_dataset, \n                valid_sets = [train_dataset], \n                callbacks = [lgb.log_evaluation(100)],\n                feval = feval_pearsonr\n            )\n#             joblib.dump(model, f'lgbr_{i+1}.pkl')\n            \n            pred = model.predict(X_test_l2)\n            X_train_level2[meta_index_begin:meta_index_end, i] = pred\n\n            del train_dataset, model, pred\n            gc.collect()\n\n        meta_index_begin = meta_index_end\n\n    X_train_level2 = pd.DataFrame(X_train_level2, columns=[f'lgbr_{i+1}' for i in range(len(best_params))]+['target'])\n    X_train_level2.to_pickle('LGBM_X_train_level2.pkl')\n    \n    # train every LGB model on the full dataset and save it\n    for i, params in enumerate(tqdm(best_params)):\n        print(f\"===================== model: {i} =====================\")\n        train_dataset = lgb.Dataset(X_train, y_train, categorical_feature=cat_features)\n        model = lgb.train(\n            params,\n            train_set = train_dataset, \n            valid_sets = [train_dataset], \n            callbacks = [lgb.log_evaluation(100)],\n            feval = feval_pearsonr\n        )\n        joblib.dump(model, f'lgbr_{i+1}.pkl')\n\n    del X_train, y_train\n    gc.collect()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-12T18:18:10.778298Z","iopub.execute_input":"2022-04-12T18:18:10.778771Z","iopub.status.idle":"2022-04-12T18:18:10.794504Z","shell.execute_reply.started":"2022-04-12T18:18:10.778729Z","shell.execute_reply":"2022-04-12T18:18:10.793812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fit LinearRegression for the second layer","metadata":{}},{"cell_type":"code","source":"if args.train_l2 is True:\n#     X_train_level2 = joblib.load('../input/ubiquant-lgbm-models/LGBM_X_train_level2.pkl')\n    X_train_level2 = joblib.load('LGBM_X_train_level2.pkl')\n\n    X = X_train_level2[[c for c in X_train_level2.columns if c != 'target']]\n    y = X_train_level2['target']\n\n    lr = LinearRegression()\n    lr.fit(X, y)\n\n    joblib.dump(lr, 'lr.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T18:18:10.795933Z","iopub.execute_input":"2022-04-12T18:18:10.796353Z","iopub.status.idle":"2022-04-12T18:18:10.813894Z","shell.execute_reply.started":"2022-04-12T18:18:10.796323Z","shell.execute_reply":"2022-04-12T18:18:10.812861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"import ubiquant\n\ntime_id = 1220\n\nif args.inference:\n    env = ubiquant.make_env()  \n    iter_test = env.iter_test()\n\n    lr_model = joblib.load('../input/ubiquant-lgbm-models/lr.pkl')\n    lgbr_models = list(map(joblib.load, sorted(Path(\"../input/ubiquant-lgbm-models\").glob(\"lgbr_*.pkl\"))))\n\n    # https://www.kaggle.com/c/ubiquant-market-prediction/discussion/305353 \n    # When making predictions check if the investment_id was in the train set\n    for (test_df, sample_prediction_df) in iter_test:\n        # get feature combinations\n        for f in combination_features:\n            f1, f2 = f.split(\"-\")\n            test_df[f] = test_df[f1] + test_df[f2]\n\n        try:\n            test_time_id = int(test_df['row_id'].values[0].split('_')[0]) # extract time_id from row_id\n            test_df[\"time_id\"] = test_time_id\n        except:\n            test_df[\"time_id\"] = time_id\n            test_time_id = 0\n        # in case of error just increase time_id on 1\n        if test_time_id:\n            time_id = test_time_id + 1\n        else:\n            test_df[\"time_id\"] = time_id\n            time_id += 1\n        \n        # create num_or_cat feature for the test_df\n        unqiues = len(test_df['f_7'].unique())\n        if unqiues/len(test_df) > 0.5:\n            test_df['num_or_cat'] = 0\n        else:\n            test_df['num_or_cat'] = 1\n        \n        # standard scale numeric features\n        test_df[features_to_scale] = sscaler.transform(test_df[features_to_scale])\n        \n        # predict\n        test_df[\"preds\"] = lr_model.predict(np.stack([model.predict(test_df[features_backup]) for model in lgbr_models], axis=1))\n        sample_prediction_df['target'] = test_df[\"preds\"]\n        env.predict(sample_prediction_df) \n        display(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T18:20:25.083792Z","iopub.execute_input":"2022-04-12T18:20:25.084103Z","iopub.status.idle":"2022-04-12T18:20:26.638531Z","shell.execute_reply.started":"2022-04-12T18:20:25.084073Z","shell.execute_reply":"2022-04-12T18:20:26.637503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Further ideas\n\n\n- f_165, f_197, f_76, f_237, f_206 - correlate with target lag 1\n- f_259, f_69, f_21, f_130, f_81   - correlate with target lag 2 (f_259 has the highes corr)\n- f_149, f_233, f_110, f_42, f_105 - correlate with target lag 3\n\n\n- try custom loss function (for DNN) (https://www.kaggle.com/c/ubiquant-market-prediction/discussion/302181)","metadata":{}},{"cell_type":"markdown","source":"# What works for LGBM CV\n\n- define if feature is numerical of categorical at this time period (num_or_cat)\n- clipping of target (at 0.5% min/max)","metadata":{}}]}