{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install PySpark","metadata":{}},{"cell_type":"code","source":"#Install Pyspark\n!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2022-02-08T15:15:27.383868Z","iopub.execute_input":"2022-02-08T15:15:27.384535Z","iopub.status.idle":"2022-02-08T15:16:17.74016Z","shell.execute_reply.started":"2022-02-08T15:15:27.384412Z","shell.execute_reply":"2022-02-08T15:16:17.739249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\nimport seaborn as sns\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport plotly.express as px\nimport pyspark.pandas as ps\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.sql import SparkSession,SQLContext\nfrom pyspark import SparkContext\nfrom pyspark import SparkConf\nimport pyspark.pandas as ps\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml import Pipeline","metadata":{"execution":{"iopub.status.busy":"2022-02-08T15:16:17.742542Z","iopub.execute_input":"2022-02-08T15:16:17.742864Z","iopub.status.idle":"2022-02-08T15:16:20.533481Z","shell.execute_reply.started":"2022-02-08T15:16:17.742821Z","shell.execute_reply":"2022-02-08T15:16:20.532545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initiate Spark","metadata":{}},{"cell_type":"code","source":"spark = (\n    SparkSession\n    .builder\n    .appName(\"Ubiquant EDA\")\n    .config(\"spark.sql.execution.arrow.pyspark.enabled\", True)\n    .config(\"spark.executor.memory\", \"5g\")\n    .config(\"spark.driver.maxResultSize\",\"40g\")\n    .getOrCreate())","metadata":{"execution":{"iopub.status.busy":"2022-02-08T15:16:20.534877Z","iopub.execute_input":"2022-02-08T15:16:20.535149Z","iopub.status.idle":"2022-02-08T15:16:26.796694Z","shell.execute_reply.started":"2022-02-08T15:16:20.535117Z","shell.execute_reply":"2022-02-08T15:16:26.795609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Dataset","metadata":{}},{"cell_type":"code","source":"start_time = datetime.now()\ndf=spark.read.csv(\"/kaggle/input/ubiquant-market-prediction/train.csv\", inferSchema=True, header=True)\nend_time = datetime.now()\nprint('Duration: {}'.format(end_time - start_time))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T15:16:26.799358Z","iopub.execute_input":"2022-02-08T15:16:26.799643Z","iopub.status.idle":"2022-02-08T15:21:04.562247Z","shell.execute_reply.started":"2022-02-08T15:16:26.799602Z","shell.execute_reply":"2022-02-08T15:21:04.561145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of rows=>\",df.count(),\"\\nNumber of columns=>\",len(df.columns))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T15:21:04.567051Z","iopub.execute_input":"2022-02-08T15:21:04.567399Z","iopub.status.idle":"2022-02-08T15:22:14.408383Z","shell.execute_reply.started":"2022-02-08T15:21:04.567354Z","shell.execute_reply":"2022-02-08T15:22:14.40743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Null Check","metadata":{}},{"cell_type":"markdown","source":"3.1 mn rows has taken less than 5 min to read in spark","metadata":{}},{"cell_type":"code","source":"#Check for number of nulls in each column\ndf.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).show()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T15:22:14.409658Z","iopub.execute_input":"2022-02-08T15:22:14.409862Z","iopub.status.idle":"2022-02-08T15:27:17.011746Z","shell.execute_reply.started":"2022-02-08T15:22:14.409837Z","shell.execute_reply":"2022-02-08T15:27:17.010465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Datatype of Columns","metadata":{}},{"cell_type":"markdown","source":"**Finding:** No nulls found in the dataset","metadata":{}},{"cell_type":"code","source":"#Data type of each column\ndf.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-02-08T15:27:17.014735Z","iopub.execute_input":"2022-02-08T15:27:17.015223Z","iopub.status.idle":"2022-02-08T15:27:17.045912Z","shell.execute_reply.started":"2022-02-08T15:27:17.015172Z","shell.execute_reply":"2022-02-08T15:27:17.044854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conversion of Spakdataframe to View for easy SQL","metadata":{}},{"cell_type":"code","source":"#Convert spark dataframe to View\ndf.createTempView(\"train\")","metadata":{"execution":{"iopub.status.busy":"2022-02-08T15:27:17.047776Z","iopub.execute_input":"2022-02-08T15:27:17.04852Z","iopub.status.idle":"2022-02-08T15:27:17.125469Z","shell.execute_reply.started":"2022-02-08T15:27:17.048475Z","shell.execute_reply":"2022-02-08T15:27:17.124456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Minimum and Maximum Time_id","metadata":{}},{"cell_type":"code","source":"#Minimum and Maximum Time id\nspark.sql(\"select min(time_id) as min_time_id,max(time_id) as max_time_id from train \").show()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T15:27:17.127052Z","iopub.execute_input":"2022-02-08T15:27:17.127363Z","iopub.status.idle":"2022-02-08T15:28:35.875586Z","shell.execute_reply.started":"2022-02-08T15:27:17.127322Z","shell.execute_reply":"2022-02-08T15:28:35.873223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Time Id Frequency","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"fig = px.bar(spark.sql(\"select time_id,count(*) as count from train group by time_id \").toPandas().sort_values(by=\"time_id\"), x=\"time_id\", y=\"count\" ,title='Number of records from each time_id')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T15:28:35.877838Z","iopub.execute_input":"2022-02-08T15:28:35.878963Z","iopub.status.idle":"2022-02-08T15:29:55.939939Z","shell.execute_reply.started":"2022-02-08T15:28:35.878913Z","shell.execute_reply":"2022-02-08T15:29:55.939088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Finding:**No time_ids in the range 361,368 to 372,382","metadata":{}},{"cell_type":"code","source":"#Top 20 investment ids present most of the time \nspark.sql(\"\"\"select investment_id,count(*) as count from train group by investment_id order by count desc \"\"\").show()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T15:29:55.94167Z","iopub.execute_input":"2022-02-08T15:29:55.942141Z","iopub.status.idle":"2022-02-08T15:31:16.335552Z","shell.execute_reply.started":"2022-02-08T15:29:55.942099Z","shell.execute_reply":"2022-02-08T15:31:16.334543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Time ID vs Avg Return","metadata":{}},{"cell_type":"code","source":"#Average return for each time_id\nfig = px.line(spark.sql(\"select time_id,avg(target) as avg_return from train group by time_id  \").toPandas().sort_values(by=\"time_id\"), x=\"time_id\", y=\"avg_return\" ,title='Time_id vs Avg Return')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T15:31:16.336962Z","iopub.execute_input":"2022-02-08T15:31:16.337282Z","iopub.status.idle":"2022-02-08T15:32:36.589195Z","shell.execute_reply.started":"2022-02-08T15:31:16.337235Z","shell.execute_reply":"2022-02-08T15:32:36.58545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Exclude columns 'row_id','time_id','investment_id'\nexcl_cols=['row_id','time_id','investment_id']\ncols = [col for col in df.columns if col not in excl_cols]","metadata":{"execution":{"iopub.status.busy":"2022-02-08T15:32:36.594336Z","iopub.execute_input":"2022-02-08T15:32:36.596642Z","iopub.status.idle":"2022-02-08T15:32:36.618191Z","shell.execute_reply.started":"2022-02-08T15:32:36.59639Z","shell.execute_reply":"2022-02-08T15:32:36.615517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert to pandas on spark\npdsprk_df=df.to_pandas_on_spark()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T15:32:36.6329Z","iopub.execute_input":"2022-02-08T15:32:36.639726Z","iopub.status.idle":"2022-02-08T15:32:39.100568Z","shell.execute_reply.started":"2022-02-08T15:32:36.63941Z","shell.execute_reply":"2022-02-08T15:32:39.097306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation Plot","metadata":{}},{"cell_type":"code","source":"corr = pdsprk_df[cols].corr()\ncorr.style.background_gradient(cmap='Blues',axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T15:32:39.104117Z","iopub.execute_input":"2022-02-08T15:32:39.104964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PySpark Machine Learning","metadata":{}},{"cell_type":"code","source":"#Remove the columns containing ids\ntraindf=df.drop('time_id','investment_id','row_id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Feature preprocessing\nfeature_list = []\nfor col in traindf.columns:\n    if col == 'target':\n        continue\n    else:\n        feature_list.append(col)\nassembler = VectorAssembler(inputCols=feature_list, outputCol=\"features\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Assign Random Forest Regressor\nrf = RandomForestRegressor(labelCol=\"target\", featuresCol=\"features\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Build pipeline\npipeline = Pipeline(stages=[assembler, rf])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train the model\n\nrf_model=pipeline.fit(traindf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Feature Importance\n\nva = rf_model.stages[-2]\ntree = rf_model.stages[-1]\nzipped=zip(va.getInputCols(), tree.featureImportances)\nfea=sorted(zipped, key=lambda x: x[1],reverse=True)\nfeature_importance = pd.DataFrame(fea, columns =['features', 'importance'])\n\n#Store the columns having importance>0\nimp_cols=[]\nfor i in range(0,len(feature_importance)):\n    #print(feature_importance.features[i])\n    if feature_importance['importance'][i]>0:\n        imp_cols.append(feature_importance.features[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Top 10 features based on Base Random Forest Regressor Model","metadata":{}},{"cell_type":"code","source":"feature_importance.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Stop Spark Session\nspark.stop()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}