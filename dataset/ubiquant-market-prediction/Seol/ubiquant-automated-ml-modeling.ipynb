{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This notebooks is test for automated machine learning library\n#####  1. Feature Engineering  : featuretools\n#####  2. Hyperparameter Tuning : optuna\n#####  3. Base algorithms : XGBRegressor\n#####    ※ featuretools is not working in kaggle environment. If you want to test this code, download and test in local pc\n#####    : woodwork related error","metadata":{}},{"cell_type":"markdown","source":"### 1. Load library and Setting config","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nimport joblib\nwarnings.filterwarnings(action='ignore')\n\nimport featuretools as ft\n\nfrom featuretools.selection import (\n    remove_highly_correlated_features,\n    remove_highly_null_features,\n    remove_single_value_features,\n)\n\n\nfrom sklearn.model_selection import GridSearchCV,KFold\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\n\nfrom xgboost import XGBRegressor\n\nimport optuna \nfrom optuna import Trial, visualization\nfrom optuna.samplers import TPESampler","metadata":{"execution":{"iopub.status.busy":"2022-04-02T03:08:30.122187Z","iopub.execute_input":"2022-04-02T03:08:30.122466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Config\nPCA_N_COMP = 50\nK_FOLD_SPLIT = 4\nSTUDY_TRIAL = 30\nCORR_THRESHOLD = 0.85","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:04:36.896548Z","iopub.execute_input":"2022-04-01T00:04:36.896819Z","iopub.status.idle":"2022-04-01T00:04:36.901355Z","shell.execute_reply.started":"2022-04-01T00:04:36.896788Z","shell.execute_reply":"2022-04-01T00:04:36.900435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Convert CSV -> Parquet format, for reducing Memory usage ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/ubiquant-market-prediction/train.csv')\ndf.to_parquet('./train.parquet')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_df = pd.read_parquet('./train.parquet')\ntrain_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_x = train_df.iloc[:, train_df.columns.str.contains('f_')]\ntrain_x.reset_index(inplace=True)\ntrain_y = train_df['target']\ndel train_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Automated feature engineering using 'featuretools'","metadata":{}},{"cell_type":"code","source":"es = ft.EntitySet('Ubiquant')\nes.add_dataframe(dataframe=train_x, dataframe_name='train_x', index='index')\nfm, features = ft.dfs(entityset=es, target_dataframe_name=\"train_x\",\n                      trans_primitives=['negate'], agg_primitives=[], max_depth=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Remove high corr features","metadata":{}},{"cell_type":"code","source":"new_fm, new_features = remove_highly_correlated_features(fm, features=features, pct_corr_threshold=CORR_THRESHOLD)\nnew_fm.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(new_fm.columns).to_csv('column.csv')\ntrain_x = new_fm.copy()\ndel new_fm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pc = PCA(n_components=PCA_N_COMP)\npc.fit(train_x)\njoblib.dump(pc, './pca_model.pkl')\ntrain_x = pc.transform(train_x)\n\nrc = RobustScaler()\nrc.fit(train_x)\njoblib.dump(rc, './robust_model.pkl')\ntrain_x = rc.transform(train_x)\ntrain_x = pd.DataFrame(data=train_x)\ntrain_x.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Hyperparameter tuning using 'optuna'","metadata":{}},{"cell_type":"code","source":"def objectiveXGB(trial, X, y):\n    \n    params = {'n_estimators':1000,\n              # trail.suggest_unifrom() allows to pick out any value between the given range, values will be continuous and\n              # not just integers.\n              'learning_rate':trial.suggest_uniform('learning_rate', 0.005, 0.01),\n              \n              # trial.suggest_categorical() allows only the passed categorical values to be suggested.\n              'subsample':trial.suggest_categorical('subsample', [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n              \n              # trial.suggest_int() will suggest integer values within the integer range. \n              'max_depth':trial.suggest_int('max_depth', 3, 11),\n              \n              'colsample_bylevel':trial.suggest_categorical('colsample_bylevel', [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n              \n              # trail.suggest_loguniform() is used when the range of values have different scales.\n              'reg_lambda':trial.suggest_loguniform('reg_lambda', 1e-3, 100),\n              'reg_alpha':trial.suggest_loguniform('reg_alpha', 1e-3, 100),\n              'n_jobs':-1,\n              'tree_method': 'gpu_hist',\n              'predictor': 'gpu_predictor'\n             }\n    \n    model = XGBRegressor(**params)\n    \n    split = KFold(n_splits=K_FOLD_SPLIT)\n    train_scores = []\n    test_scores = []\n    for train_idx, val_idx in split.split(X):\n        X_tr = X.iloc[train_idx]\n        X_val = X.iloc[val_idx]\n        y_tr = y.iloc[train_idx]\n        y_val = y.iloc[val_idx]\n        \n        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n                  eval_metric=['rmse'],\n                  early_stopping_rounds=30, verbose=0,\n                  # optuna allows us to pass pruning callback to xgboost callbacks, so any trial which does not seem to be \n                  # better or not qualify a given threshold of loss reduciton after some iterations will get pruned, that is\n                  # stopped in between hence saving time, we will see it in action below.\n                  callbacks=[optuna.integration.XGBoostPruningCallback(trial, observation_key=\"validation_0-rmse\")]\n                 )\n    \n        train_score = np.round(np.sqrt(mean_squared_error(y_tr, model.predict(X_tr))), 4)\n        test_score = np.round(np.sqrt(mean_squared_error(y_val, model.predict(X_val))), 4)\n        train_scores.append(train_score)\n        test_scores.append(test_score)\n        \n    \n    print(f'train score : {train_scores}')\n    print(f'test score : {test_scores}')\n    train_score = np.round(np.mean(train_scores), 4)\n    test_score = np.round(np.mean(test_scores), 4)\n    \n    print(f'TRAIN RMSE : {train_score} || TEST RMSE : {test_score}')\n    \n    # you can make this function as bespoke as possible... you can return any kind of modified value using the return function\n    # optuna will try to optimize it!!\n    \n    return test_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# direction : score 값을 최대 또는 최소로 하는 방향으로 지정 \nstudy = optuna.create_study(direction='minimize')\n\n# n_trials : 시도 횟수 (미 입력시 Key interrupt가 있을 때까지 무한 반복)\nstudy.optimize(lambda trial : objectiveXGB(trial, train_x,  train_y), n_trials=STUDY_TRIAL)\nprint('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trial = study.best_trial\ntrial_params = trial.params\nprint(f'Best Trial : score {trial.value}, \\nparams {trial_params}')\nXGBR_model = XGBRegressor(**trial_params, tree_method = 'gpu_hist', predictor = 'gpu_predictor')\nXGBR_model.fit(train_x,train_y)\njoblib.dump(XGBR_model,'./XGBR_model.pkl')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Load save pkl file and submission","metadata":{}},{"cell_type":"code","source":"pc = joblib.load('./pca_model.pkl')\nxgbr_model = joblib.load('./XGBR_model.pkl')\nrc = joblib.load('./robust_model.pkl')\ncol = pd.read_csv('./column.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env=ubiquant.make_env()\niter_test = env.iter_test()\n\nfor (test_df, sample_prediction_df) in iter_test:\n    test_x = test_df.loc[:,list(col.iloc[:,1])]\n    test_x = pc.transform(test_x)\n    test_x = rc.transform(test_x)\n    test_x = pd.DataFrame(data=test_x)\n    sample_prediction_df['target'] = xgbr_model.predict(test_x)\n    env.predict(sample_prediction_df)","metadata":{},"execution_count":null,"outputs":[]}]}