{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is the code for training and uploading to the dataset assuming that Google Colaboratory is used.  \nIn Google Colaboratory, you can use more memory than Kaggle Notebook by setting the high-memory.  \n\nThis pipeline was very referenced from [this notebook](https://www.kaggle.com/mst8823/19th-place-best-single-model-resbilstm)  created by [@mst8823](https://www.kaggle.com/mst8823).\n\n\n## Setting\n\nThe directory structure is assumed to be as follows.\n\n```\nMyDrive  \n├UbiquantMarketPredictionDrive  \n│ └[Author Name]\n│    └Notebook  \n│       └[This Nootbook]\n└kaggle.json\n```\n\nAnd when you run everything, the directory structure will look like this, for example.\n\n```\nMyDrive  \n├UbiquantMarketPredictionDrive  \n│ └colum2131\n│    ├Notebook  \n│    │   └UMP-Exp001-ColabTraining.ipnyb\n│    ├Input\n│    │   ├ubiquant\n│    │   ├ubiquant-market-prediction.zip\n│    │   ├train.csv\n│    │   ├example_test.csv\n│    │   └example_sample_submission.csv\n│    ├Output\n│    │   └UMP-Exp001-ColabTraining\n│    │      ├preds\n│    │      ├model\n│    │      └fig\n│    ├Dataset\n│    │   └ubiquant-parquet\n│    └Submission\n└kaggle.json\n```\nYou also need to rewrite Config appropriately if you run this code.\n\n## Inference\n\n* [[Ubiquant] Inference: Google Colaboratory Training](https://www.kaggle.com/columbia2131/ubiquant-inference-google-colaboratory-training) - LB Score: 0.132\n\n## Reference\n\n* [19th Place Best Single Model [ResBiLSTM]](https://www.kaggle.com/mst8823/19th-place-best-single-model-resbilstm) created by [@mst8823](https://www.kaggle.com/mst8823)\n* [⏫ Fast Data Loading and Low Mem with Parquet Files](https://www.kaggle.com/robikscube/fast-data-loading-and-low-mem-with-parquet-files) created by [@robikscube](https://www.kaggle.com/robikscube)\n* [Ubiquant Competition Data in Parquet Format](https://www.kaggle.com/robikscube/ubiquant-parquet) created by [@robikscube](https://www.kaggle.com/robikscube)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    author = \"colum2131\" # Your name\n    competition = \"ubiquant-market-prediction\"\n    name = \"UMP-Exp001-ColabTraining\" # The name of the Dataset\n    upload_from_colab = True # If True, the model uploads to the Kaggle Dataset\n    \n    colab_dir = \"/content/drive/Shareddrives/UbiquantMarketPredictionDrive\" # Your own directory\n    drive_path = colab_dir + f\"/{author}\"\n    api_path = \"/content/drive/MyDrive/kaggle.json\" # Your own api-path\n    \n    dataset_path = ['robikscube/ubiquant-parquet'] # The dataset you want to download\n\n    n_fold = 5\n    trn_fold = [0, 1, 2, 3, 4]\n    seed = 42\n    max_epochs = 100","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport json\nimport pickle\nimport shutil\nimport random\nimport joblib\nimport requests\nimport itertools\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\nfrom tqdm.auto import tqdm\n\nfrom sklearn.preprocessing import (\n    StandardScaler,\n    MinMaxScaler,\n    RobustScaler,\n)\nfrom sklearn.model_selection import (\n    KFold,\n    StratifiedKFold,\n    GroupKFold\n)\nfrom sklearn.linear_model import (\n    Ridge,\n)\nfrom sklearn.metrics import (\n    accuracy_score,\n    f1_score,\n    log_loss,\n    roc_auc_score,\n    mean_absolute_error,\n    mean_squared_error,\n)\n\nimport lightgbm as lgbm\nimport torch\nimport tensorflow as tf\n\n\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\ndef MAE(y_true, y_pred):\n    return mean_absolute_error(y_true, y_pred)\n\ndef MSE(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred)\n\ndef RMSE(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred, squared=False)\n\ndef get_kfold(train, n_splits, seed):\n    fold_series = []\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    kf_generator = kf.split(train)\n    for fold, (idx_train, idx_valid) in enumerate(kf_generator):\n        fold_series.append(pd.Series(fold, index=idx_valid))\n    fold_series = pd.concat(fold_series).sort_index()\n    return fold_series\n\ndef get_stratifiedkfold(train, target_col, n_splits, seed):\n    fold_series = []\n    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    kf_generator = kf.split(train, train[target_col])\n    for fold, (idx_train, idx_valid) in enumerate(kf_generator):\n        fold_series.append(pd.Series(fold, index=idx_valid))\n    fold_series = pd.concat(fold_series).sort_index()\n    return fold_series\n\ndef get_groupkfold(train, target_col, group_col, n_splits):\n    fold_series = []\n    kf = GroupKFold(n_splits=n_splits)\n    kf_generator = kf.split(train, train[target_col], train[group_col])\n    for fold, (idx_train, idx_valid) in enumerate(kf_generator):\n        fold_series.append(pd.Series(fold, index=idx_valid))\n    fold_series = pd.concat(fold_series).sort_index()\n    return fold_series","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def setup(cfg):\n    cfg.COLAB = 'google.colab' in sys.modules\n    if cfg.COLAB:\n        print('This environment is Google Colab')\n        \n        # mount\n        from google.colab import drive\n        if not os.path.isdir('/content/drive'):\n            drive.mount('/content/drive') \n        \n        # import library\n        ! pip install --quiet tensorflow-addons\n\n        # use kaggle api (need kaggle token)\n        f = open(cfg.api_path, 'r')\n        json_data = json.load(f) \n        os.environ['KAGGLE_USERNAME'] = json_data['username']\n        os.environ['KAGGLE_KEY'] = json_data['key']\n\n        # set dirs\n        cfg.DRIVE = cfg.drive_path\n        cfg.EXP = (cfg.name if cfg.name is not None \n            else requests.get('http://172.28.0.2:9000/api/sessions').json()[0]['name'][:-6]\n        )\n        cfg.INPUT = os.path.join(cfg.DRIVE, 'Input')\n        cfg.OUTPUT = os.path.join(cfg.DRIVE, 'Output')\n        cfg.SUBMISSION = os.path.join(cfg.DRIVE, 'Submission')\n        cfg.DATASET = os.path.join(cfg.DRIVE, 'Dataset')\n\n        cfg.OUTPUT_EXP = os.path.join(cfg.OUTPUT, cfg.EXP) \n        cfg.EXP_MODEL = os.path.join(cfg.OUTPUT_EXP, 'model')\n        cfg.EXP_FIG = os.path.join(cfg.OUTPUT_EXP, 'fig')\n        cfg.EXP_PREDS = os.path.join(cfg.OUTPUT_EXP, 'preds')\n\n        # make dirs\n        for d in [cfg.INPUT, cfg.SUBMISSION, cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n            os.makedirs(d, exist_ok=True)\n        \n        if not os.path.isfile(os.path.join(cfg.INPUT, 'train.csv')):\n            # load dataset\n            ! pip install --upgrade --force-reinstall --no-deps kaggle\n            ! kaggle competitions download -c $cfg.competition -p $cfg.INPUT\n            filepath = os.path.join(cfg.INPUT,cfg.competition+'.zip')\n            ! unzip -d $cfg.INPUT $filepath\n            \n        \n        for path in cfg.dataset_path:\n            datasetpath = os.path.join(cfg.DATASET,  path.split('/')[1])\n            if not os.path.exists(datasetpath):\n                os.makedirs(datasetpath, exist_ok=True)\n                ! kaggle datasets download $path -p $datasetpath\n                filepath = os.path.join(datasetpath, path.split(\"/\")[1]+'.zip')\n                ! unzip -d $datasetpath $filepath\n\n    \n    else:\n        print('This environment is Kaggle Kernel')\n\n        # set dirs\n        cfg.INPUT = f'../input/{cfg.competition}'\n        cfg.EXP = cfg.name\n        cfg.OUTPUT_EXP = cfg.name\n        cfg.SUBMISSION = './'\n        cfg.DATASET = '../input/'\n        \n        cfg.EXP_MODEL = os.path.join(cfg.EXP, 'model')\n        cfg.EXP_FIG = os.path.join(cfg.EXP, 'fig')\n        cfg.EXP_PREDS = os.path.join(cfg.EXP, 'preds')\n\n        # make dirs\n        for d in [cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n            os.makedirs(d, exist_ok=True)\n\n    seed_everything(cfg.seed)\n    return cfg\n\n\ndef dataset_create_new(dataset_name, upload_dir):\n    dataset_metadata = {}\n    dataset_metadata['id'] = f'{os.environ[\"KAGGLE_USERNAME\"]}/{dataset_name}'\n    dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n    dataset_metadata['title'] = dataset_name\n    with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n        json.dump(dataset_metadata, f, indent=4)\n    api = KaggleApi()\n    api.authenticate()\n    api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_lightgbm(cfg, X, y, params, folds, add_suffix=''):\n    oof_pred = np.zeros(len(y), dtype=np.float64)\n\n    for fold in cfg.trn_fold:\n        idx_train = (folds!=fold)\n        idx_valid = (folds==fold)\n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n        \n        lgbm_train = lgbm.Dataset(x_train, y_train)\n        lgbm_valid = lgbm.Dataset(x_valid, y_valid, reference=lgbm_train)\n        \n        model = lgbm.train(\n            params=params,\n            train_set=lgbm_train,\n            valid_sets=[lgbm_train, lgbm_valid],\n            num_boost_round=cfg.max_epochs,\n            verbose_eval=100,\n            early_stopping_rounds=100,\n        )\n        \n        # save model\n        tmp_path = os.path.join(Config.EXP_MODEL, f'lgbm_fold{fold}{add_suffix}.pkl')\n        pickle.dump(model, open(tmp_path, 'wb'))\n        # save oof-pred\n        pred_i = model.predict(x_valid, num_iteration=model.best_iteration)\n        oof_pred[x_valid.index] = pred_i\n        tmp_path = os.path.join(Config.EXP_PREDS, f'lgbm_fold{fold}{add_suffix}.npy')\n        np.save(tmp_path, pred_i)\n        \n        score = round(RMSE(y_valid, pred_i), 5)\n        print(f'Performance of the prediction: {score}')\n\n    # save oof-pred\n    tmp_path = os.path.join(Config.EXP_PREDS, f'lgbm_foldall{add_suffix}.npy')\n    np.save(tmp_path, oof_pred)\n\n    score = round(RMSE(y, oof_pred), 5)\n    print(f'All Performance of the prediction: {score}')\n    return oof_pred\n\n\ndef pred_lightgbm(cfg, X, add_suffix=''):\n    models = glob(os.path.join(cfg.EXP_MODEL, f'lgbm*{add_suffix}.pkl'))\n    models = [pickle.load(open(model, 'rb')) for model in models]\n    preds = np.array([model.predict(X) for model in models])\n    preds = np.mean(preds, axis=0)\n    return preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =========================\n# SetUp\n# =========================\nConfig = setup(Config)\n\n# 2nd import\nimport tensorflow_addons as tfa","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =========================\n# Pre-Processing\n# =========================\ntrain = pd.read_parquet(os.path.join(Config.DATASET, 'ubiquant-parquet/train_low_mem.parquet'))\ntrain_time = train['time_id']\ntrain_investment = train['investment_id']\n\nfeature_cols = [f'f_{i}' for i in range(300)]\ntrain_X = train[['investment_id'] + feature_cols]\ntrain_y = train['target']\n\nfolds = get_groupkfold(train, 'target', 'time_id', Config.n_fold)\n\ndel train\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# =========================\n# Training & Upload\n# =========================\nlgbm_params = {\n    'objective': 'rmse',\n    'metric': 'rmse',\n    'boosting_type': 'gbdt', \n    'learning_rate': 0.1, \n    'num_leaves': 31, \n}\noof_base_pred = fit_lightgbm(Config, train_X, train_y, lgbm_params, folds, '_base')\n\n\n# upload output folder to kaggle dataset\nif Config.upload_from_colab:\n    from kaggle.api.kaggle_api_extended import KaggleApi\n    dataset_create_new(dataset_name=Config.EXP, upload_dir=Config.OUTPUT_EXP)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}