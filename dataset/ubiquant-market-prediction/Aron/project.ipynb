{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import data\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\nfrom scipy.stats import probplot, pearsonr\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain_dtypes = {f'f_{i}': np.float32 for i in range(300)}\ntrain_dtypes['investment_id'] = np.uint16\ntrain_dtypes['time_id'] = np.uint16\ntrain_dtypes['target'] = np.float32\n\ndf_train = pd.read_csv('../input/ubiquant-market-prediction/train.csv', usecols=list(train_dtypes.keys()), dtype=train_dtypes)\nprint(f'Training Set Shape: {df_train.shape} - Memory Usage: {df_train.memory_usage().sum() / 1024 ** 2:.2f} MB')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-06T04:05:52.108254Z","iopub.execute_input":"2022-03-06T04:05:52.108669Z","iopub.status.idle":"2022-03-06T04:12:40.963935Z","shell.execute_reply.started":"2022-03-06T04:05:52.10858Z","shell.execute_reply":"2022-03-06T04:12:40.962854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reduce memory usage of dataset","metadata":{}},{"cell_type":"code","source":"#reduce memory usage of data\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\nfrom collections import defaultdict\nfrom argparse import Namespace\nimport lightgbm as lgb\nargs = Namespace(\n    seed=21,\n    folds=5,\n    workers=4,\n    samples=2500000,\n)\n    \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-06T04:12:40.966726Z","iopub.execute_input":"2022-03-06T04:12:40.967087Z","iopub.status.idle":"2022-03-06T04:12:42.272212Z","shell.execute_reply.started":"2022-03-06T04:12:40.967044Z","shell.execute_reply":"2022-03-06T04:12:42.27126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing data","metadata":{}},{"cell_type":"code","source":"#preprocessing data\ndf_train.drop('f_124', axis=1, inplace=True) # since the variance of it is much lower than 0.1\ndf_train.shape\n\n#delete row with time_ID from 350 to 550 since its oscillation part from EDA\ndf_train = df_train.drop(df_train[(df_train['time_id'] < 551) & (df_train['time_id'] > 349)].index)\n\nnum_features = list(df_train.filter(like=\"f_\").columns)\nfeatures = num_features\ndf_train = reduce_mem_usage(df_train)\nlen(features)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-06T04:12:42.273594Z","iopub.execute_input":"2022-03-06T04:12:42.273815Z","iopub.status.idle":"2022-03-06T04:15:20.877285Z","shell.execute_reply.started":"2022-03-06T04:12:42.273789Z","shell.execute_reply":"2022-03-06T04:15:20.876284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## correlation coefficient","metadata":{}},{"cell_type":"code","source":"# find the correlation coefficient between features and target\ndf=df_train\ncor_t=df[features].apply(lambda x: x.corr(df['target']))\ncr=pd.DataFrame(columns = ['feature','correlation'],)\ncr['feature'] = pd.Series(features)\ncr['correlation'] = pd.Series(list(abs(cor_t)))\ntemp=cr.sort_values(by=\"correlation\", ascending=False ).head(100)\nprint(temp.head(20))\nfeatures_first100=list(temp['feature'])\ndf_train_filtered=df_train[features_first100]\ndel df\ndf_train_filtered.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-06T04:15:20.880244Z","iopub.execute_input":"2022-03-06T04:15:20.880791Z","iopub.status.idle":"2022-03-06T04:15:45.886433Z","shell.execute_reply.started":"2022-03-06T04:15:20.880709Z","shell.execute_reply":"2022-03-06T04:15:45.885413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#isomap embedding \n\n#from sklearn import manifold\n\n#iso = manifold.Isomap(n_neighbors=80, n_components=50)\n#df_features_ISO= iso.fit_transform(df_features_filtered.head(10000))\n\n#iso.reconstruction_error()\n\n#LLEembeding=manifold.LocallyLinearEmbedding(n_neighbors=100, n_components=2)\n#df_features_LLE=LLEembeding.fit_transform(df_features_filtered.head(10000))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-06T04:15:45.887866Z","iopub.execute_input":"2022-03-06T04:15:45.888184Z","iopub.status.idle":"2022-03-06T04:15:45.892801Z","shell.execute_reply.started":"2022-03-06T04:15:45.888142Z","shell.execute_reply":"2022-03-06T04:15:45.891701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBRegressor","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom matplotlib import pyplot as plt\n##X = df_train_filtered[features_first100]\nX = df_train[features]\ny = df_train['target']\n\n# creating the training and validation set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=42)\ndel X,y\nfrom xgboost.sklearn import XGBRegressor\nxgb_model = XGBRegressor(objective='reg:squarederror',n_estimators=100, max_depth=8, learning_rate=0.1, random_state=0)\nxgb_model.fit(X_train, y_train)\ny_predict = xgb_model.predict(X_test)\n\nscore = xgb_model.score(X_train, y_train)  \nprint(\"Training score: \", score)\n\nmse = mean_squared_error(y_test,y_predict)\nprint(\"MSE: \", mse)\n#scores = cross_val_score(xgb_model, X_train, y_train)\n#print(\"Mean cross-validation score: %.2f\" % scores.mean())\n\n#kfold = KFold(n_splits=5, shuffle=True)\n#kf_cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=kfold )\n#print(\"K-fold CV average score: %.2f\" % kf_cv_scores.mean())\n\n#sorted_idx = xgb_model.feature_importances_.argsort()\n#plt.barh(X_train.columns[sorted_idx], xgb_model.feature_importances_[sorted_idx])\n#plt.xlabel(\"Xgboost Feature Importance\")\n\n\nfeatures_importance_df= pd.DataFrame({'feature': features, 'importance': xgb_model.feature_importances_}).sort_values(by=\"importance\", ascending=False)\nplt.figure(figsize=(14, 8))\nplt.subplot(1,2,1)\nsns.barplot(x=\"importance\", y=\"feature\", data=features_importance_df.head(25))\nplt.title(f'50 Head XGBRegressor Features ')\nplt.subplot(1,2,2)\nsns.barplot(x=\"importance\", y=\"feature\", data=features_importance_df.tail(25))\nplt.title(f'50 Tail XGBRegressor Features ')\nplt.tight_layout()\nplt.show()                                     \n","metadata":{"execution":{"iopub.status.busy":"2022-03-06T04:15:45.893945Z","iopub.execute_input":"2022-03-06T04:15:45.89414Z","iopub.status.idle":"2022-03-06T05:46:21.191126Z","shell.execute_reply.started":"2022-03-06T04:15:45.894116Z","shell.execute_reply":"2022-03-06T05:46:21.187242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.kernel_ridge import KernelRidge\n#from sklearn.metrics.pairwise import rbf_kernel\n#KR = KernelRidge(kernel ='rbf', alpha=1.0,gamma=0.8)\n\n#KR.fit(X_train,y_train)\n#preds = KR.predict(X_test)\n#mse = np.sqrt(mean_squared_error(y_test,preds))\n#print(\"MSE: \", mse)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T05:46:21.192533Z","iopub.execute_input":"2022-03-06T05:46:21.192753Z","iopub.status.idle":"2022-03-06T05:46:21.198404Z","shell.execute_reply.started":"2022-03-06T05:46:21.192728Z","shell.execute_reply":"2022-03-06T05:46:21.197038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import shap\n#shap.initjs()\n#explainer = shap.TreeExplainer(xgb_model)\n#shap_values = explainer.shap_values(X_train)\n\n#shap.summary_plot(shap_values, features=X_train, feature_names=X_train.columns)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T05:46:21.199881Z","iopub.execute_input":"2022-03-06T05:46:21.200257Z","iopub.status.idle":"2022-03-06T05:46:21.216872Z","shell.execute_reply.started":"2022-03-06T05:46:21.200183Z","shell.execute_reply":"2022-03-06T05:46:21.21584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()  \niter_test = env.iter_test()\n\nfor (test_df, sample_prediction_df) in iter_test:\n    final_pred =xgb_model.predict(test_df[features]) \n    sample_prediction_df['target'] = final_pred\n    env.predict(sample_prediction_df) \n    display(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T05:46:21.218523Z","iopub.execute_input":"2022-03-06T05:46:21.21915Z","iopub.status.idle":"2022-03-06T05:46:21.39186Z","shell.execute_reply.started":"2022-03-06T05:46:21.219101Z","shell.execute_reply":"2022-03-06T05:46:21.391136Z"},"trusted":true},"execution_count":null,"outputs":[]}]}