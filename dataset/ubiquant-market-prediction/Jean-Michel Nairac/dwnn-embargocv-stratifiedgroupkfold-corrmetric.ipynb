{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Ubiquant Market Prediction with DNN","metadata":{"papermill":{"duration":0.016475,"end_time":"2022-01-25T15:39:01.467349","exception":false,"start_time":"2022-01-25T15:39:01.450874","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### References:\n\n[Ubiquant Market Prediction with DNN](https://www.kaggle.com/lonnieqin/ubiquant-market-prediction-with-dnn)","metadata":{}},{"cell_type":"markdown","source":"## Import Packages","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import backend as K\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cluster import KMeans\nfrom scipy import stats\nfrom pathlib import Path\nimport ubiquant\nimport pickle\nimport math\nimport time\nimport umap\nimport random\nfrom collections import Counter, defaultdict\nfrom tqdm import tqdm\n# !pip install \"../input/faisscpu/faiss_cpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\"\n# import faiss","metadata":{"papermill":{"duration":7.781864,"end_time":"2022-01-25T15:39:09.265726","exception":false,"start_time":"2022-01-25T15:39:01.483862","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import dataset","metadata":{"papermill":{"duration":0.015291,"end_time":"2022-01-25T15:39:09.296817","exception":false,"start_time":"2022-01-25T15:39:09.281526","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\ntrain.info()\ntrain.head()","metadata":{"papermill":{"duration":16.88418,"end_time":"2022-01-25T15:39:26.19638","exception":false,"start_time":"2022-01-25T15:39:09.3122","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Integer Lookup for Investment IDs","metadata":{}},{"cell_type":"markdown","source":"#### Note that since we are likely to see new investment ids in future, it might be better to group similar investment ids as opposed to using them directly in the model. I attempted this but thus far it did not produce good results...","metadata":{}},{"cell_type":"code","source":"# since we will likely see new investment ids, its best to add OOV indices\ninvestment_ids = list(train.investment_id.unique())\ninvestment_id_size = len(investment_ids) + 150\ninvestment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size, num_oov_indices=150)\ninvestment_id_lookup_layer.adapt(pd.DataFrame({\"investment_ids\":investment_ids}))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make Tensorflow dataset","metadata":{"papermill":{"duration":0.018846,"end_time":"2022-01-25T15:39:30.567495","exception":false,"start_time":"2022-01-25T15:39:30.548649","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def preprocess(X, y):\n    return X, y\ndef make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(4096)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds","metadata":{"papermill":{"duration":0.02858,"end_time":"2022-01-25T15:39:30.614302","exception":false,"start_time":"2022-01-25T15:39:30.585722","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{"papermill":{"duration":0.017564,"end_time":"2022-01-25T15:39:30.64918","exception":false,"start_time":"2022-01-25T15:39:30.631616","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### We implement 2 models for 2 types of cross validation. (1) The first is Stratified Group KFold. Here the objective is for the model to be trained on all investment IDs, in order to minimize the number of unknown investment IDs in the test set. On the other hand, we want to maximize the number of unkown time id's in the test set, since we know that time ids will be new in the test set. (2) The 2nd is EmbargoCV, adapted from the numerai tournament, which is a form of time series cross validation where we exclude the initial time ids from the validation folds to minimize time leakage... we combine the models at the end by averaging...","metadata":{}},{"cell_type":"code","source":"def stratified_group_k_fold(X, y, groups, k, seed=None):\n    \"\"\" https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation \"\"\"\n    labels_num = np.max(y) + 1\n    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n    y_distr = Counter()\n    for label, g in zip(y, groups):\n        y_counts_per_group[g][label] += 1\n        y_distr[label] += 1\n\n    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n    groups_per_fold = defaultdict(set)\n\n    def eval_y_counts_per_fold(y_counts, fold):\n        y_counts_per_fold[fold] += y_counts\n        std_per_label = []\n        for label in range(labels_num):\n            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n            std_per_label.append(label_std)\n        y_counts_per_fold[fold] -= y_counts\n        return np.mean(std_per_label)\n    \n    groups_and_y_counts = list(y_counts_per_group.items())\n    random.Random(seed).shuffle(groups_and_y_counts)\n\n    for g, y_counts in tqdm(sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])), total=len(groups_and_y_counts)):\n        best_fold = None\n        min_eval = None\n        for i in range(k):\n            fold_eval = eval_y_counts_per_fold(y_counts, i)\n            if min_eval is None or fold_eval < min_eval:\n                min_eval = fold_eval\n                best_fold = i\n        y_counts_per_fold[best_fold] += y_counts\n        groups_per_fold[best_fold].add(g)\n\n    all_groups = set(groups)\n    for i in range(k):\n        train_groups = all_groups - groups_per_fold[i]\n        test_groups = groups_per_fold[i]\n\n        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n        yield train_indices, test_indices\n        \n# Embargo CV adapted from Numerai tournament...\ndef get_time_series_cross_val_splits(data, cv = 5, embargo = 50):\n    all_train_time_ids = data.time_id.unique()\n    len_split = len(all_train_time_ids) // cv\n    test_splits = [all_train_time_ids[i * len_split:(i + 1) * len_split] for i in range(cv)]\n    # fix the last test split to have all the last time_ids, in case the number of time_ids wasn't divisible by cv\n    rem = len(all_train_time_ids) - len_split*cv\n    if rem>0:\n        test_splits[-1] = np.append(test_splits[-1], all_train_time_ids[-rem:])\n\n    train_splits = []\n    for test_split in test_splits:\n        test_split_max = int(np.max(test_split))\n        test_split_min = int(np.min(test_split))\n        # get all of the time_ids that aren't in the test split\n        train_split_not_embargoed = [e for e in all_train_time_ids if not (test_split_min <= int(e) <= test_split_max)]\n        train_split = [e for e in train_split_not_embargoed if\n                       abs(int(e) - test_split_max) > embargo and abs(int(e) - test_split_min) > embargo]\n        train_splits.append(train_split)\n\n    # convenient way to iterate over train and test splits\n    train_test_zip = zip(train_splits, test_splits)\n    return train_test_zip\n\ndef load_model(name):\n    path = Path(f\"{name}/saved_model.pb\")\n    if path.is_file():\n        model = keras.models.load_model(name, custom_objects={'corr_eval': corr_eval})\n    else:\n        model = False\n    return model\n\n# early stopping and learning rate reduction on plateau\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_corr_eval', patience=7, verbose=1,\n                                              mode='min', restore_best_weights=True)\nplateau = keras.callbacks.ReduceLROnPlateau(monitor='val_corr_eval', factor=0.1, patience=3, \n                                           verbose=1, mode='min')\n\n## Learning Rate schedular obtained from book: Hands On Scikit-Learn, Tensorflow by A Geron\nclass OneCycleScheduler(keras.callbacks.Callback):\n    def __init__(self, iterations, max_rate, start_rate=None,\n                 last_iterations=None, last_rate=None):\n        self.iterations = iterations\n        self.max_rate = max_rate\n        self.start_rate = start_rate or max_rate / 10\n        self.last_iterations = last_iterations or iterations // 10 + 1\n        self.half_iteration = (iterations - self.last_iterations) // 2\n        self.last_rate = last_rate or self.start_rate / 1000\n        self.iteration = 0\n    def _interpolate(self, iter1, iter2, rate1, rate2):\n        return ((rate2 - rate1) * (self.iteration - iter1)\n                / (iter2 - iter1) + rate1)\n    def on_batch_begin(self, batch, logs):\n        if self.iteration < self.half_iteration:\n            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n        elif self.iteration < 2 * self.half_iteration:\n            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n                                     self.max_rate, self.start_rate)\n        else:\n            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n                                     self.start_rate, self.last_rate)\n        self.iteration += 1\n        K.set_value(self.model.optimizer.learning_rate, rate)\n        \n\n# using correlation as metric\ndef corr_eval(y_true, y_pred):\n    x = tf.cast(y_true, tf.float32)\n    y = tf.cast(y_pred, tf.float32)\n    mx = K.mean(x)\n    my = K.mean(y)\n    xm, ym = x-mx, y-my\n    r_num = K.sum(tf.multiply(xm,ym))\n    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n    r = r_num / r_den\n    r = K.maximum(K.minimum(r, 1.0), -1.0)\n    return - r","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The below model is Deep and Wide Neural Network. We split the lower layers into two, the deep model which will learn high dimensional features and the wide model to learn lower dimensional features. Why do we do this? Ubiquant have already given us engineered features which may already have some predictability to the target. Therefore, we must include this in the wide model...","metadata":{}},{"cell_type":"code","source":"def get_model(num_inputs):\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((num_inputs, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x2 = layers.Dense(32, activation='relu', kernel_regularizer=\"l2\")(investment_id_x)\n    \n    feature_x = layers.Dense(64, activation='relu', kernel_regularizer=\"l2\")(features_inputs)\n    feature_wide = layers.Dense(128, activation='relu', kernel_regularizer=\"l2\")(features_inputs)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x2, feature_x])\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.2)(x)\n    x = layers.Dense(64, activation='relu', kernel_regularizer=\"l2\")(x)\n    x = layers.BatchNormalization()(x)\n#     x = layers.GaussianNoise(0.1)(x)\n    x = layers.Dropout(0.2)(x)\n    x = layers.Dense(64, activation='relu', kernel_regularizer=\"l2\")(x)\n    x = layers.BatchNormalization()(x)\n#     x = layers.GaussianNoise(0.1)(x)\n    x = layers.Dropout(0.2)(x)\n    x = layers.Dense(32, activation='relu', kernel_regularizer=\"l2\")(x)\n    x = layers.Concatenate(axis=1)([investment_id_x, feature_wide, x])\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.2)(x)\n    x = layers.Dense(128, activation='relu', kernel_regularizer=\"l2\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.GaussianNoise(0.1)(x)\n    x = layers.Dropout(0.4)(x)\n    x = layers.Dense(128, activation='relu', kernel_regularizer=\"l2\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.GaussianNoise(0.1)(x)\n    x = layers.Dropout(0.4)(x)\n    x = layers.Dense(32, activation='relu', kernel_regularizer=\"l2\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.GaussianNoise(0.1)(x)\n    x = layers.Dropout(0.2)(x)\n    x = layers.Dense(16, activation='relu', kernel_regularizer=\"l2\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.GaussianNoise(0.1)(x)\n    x = layers.Dropout(0.2)(x)\n    x = layers.Dense(8, activation='relu', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(1, activation='linear')(x)\n    model = keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[x])\n    model.compile(optimizer=tf.optimizers.Nadam(0.0001), loss='mse', metrics=[corr_eval])\n    return model","metadata":{"papermill":{"duration":0.033569,"end_time":"2022-01-25T15:39:30.700649","exception":false,"start_time":"2022-01-25T15:39:30.66708","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model(300)\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)","metadata":{"papermill":{"duration":0.209912,"end_time":"2022-01-25T15:39:30.929112","exception":false,"start_time":"2022-01-25T15:39:30.7192","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model\nK.clear_session()\ntf.compat.v1.reset_default_graph()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntf.random.set_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\nepoch_stop = 0\nn_epochs = 40\ncv = 5\nif not load_model('model_4_skf'):\n    skf = stratified_group_k_fold(X=train, y=train['investment_id'].astype('category').cat.codes.values, \n                                  groups=np.array(train['time_id'].astype('category').cat.codes.values), k=cv, seed=42)\n    for split, (train_split_index, test_split_index) in enumerate(skf):\n           \n        X_train = train.loc[train_split_index].copy()\n        X_val = train.loc[test_split_index].copy()\n        \n        train_ds = make_dataset(X_train[features], train.loc[train_split_index, 'investment_id'], train.loc[train_split_index, 'target'])\n        valid_ds = make_dataset(X_val[features], train.loc[test_split_index, 'investment_id'], train.loc[test_split_index, 'target'], mode=\"valid\")        \n        del X_train\n        del X_val\n        \n        gc.collect()\n        \n        model = get_model(len(features))\n#         onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * n_epochs, max_rate=0.0025, start_rate=0.00025, last_rate=0.00001)\n#         history = model.fit(train_ds, epochs=n_epochs, validation_data=valid_ds, callbacks=[onecycle])\n        history = model.fit(train_ds, epochs=n_epochs, validation_data=valid_ds, callbacks=[early_stop, plateau])\n        epoch_stop += (np.argmin(history.history['val_corr_eval']) + 1) / cv\n        model.save(f\"model_{split}_skf\")\n        pearson_score = stats.pearsonr(model.predict(valid_ds).ravel(), train.loc[test_split_index, 'target'].values)[0]\n        print('Pearson:', pearson_score)\n        pd.DataFrame(history.history, columns=[\"loss\", \"val_loss\"]).plot()\n        plt.title(\"MSE\")\n        plt.show()\n        pd.DataFrame(history.history, columns=[\"corr_eval\", \"val_corr_eval\"]).plot()\n        plt.title(\"CORR\")\n        plt.show()\n        \n\n        del model\n        del history\n        del train_ds\n        del valid_ds\n        K.clear_session()\n        tf.compat.v1.reset_default_graph()\n        plt.close()\n        gc.collect()\n        # 0.143 0.8347","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n\n# tf.random.set_seed(42)\n# np.random.seed(42)\n# random.seed(42)\n\n# epoch_stop = 0\n# n_epochs = 40\n# cv = 5\n# if not load_model('model_4_ts'):\n#     train_test_zip = get_time_series_cross_val_splits(train, cv = cv)\n#     for split, (train_split, test_split) in enumerate(train_test_zip):\n#         train_split_index = train.time_id.isin(train_split)\n#         test_split_index = train.time_id.isin(test_split)\n        \n#         X_train = train.loc[train_split_index].copy()\n#         X_val = train.loc[test_split_index].copy()\n        \n#         train_ds = make_dataset(X_train[features], train.loc[train_split_index, 'investment_id'], train.loc[train_split_index, 'target'])\n#         valid_ds = make_dataset(X_val[features], train.loc[test_split_index, 'investment_id'], train.loc[test_split_index, 'target'], mode=\"valid\")\n        \n#         del X_train\n#         del X_val\n#         gc.collect()\n        \n#         model = get_model(len(features))\n# #         onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * n_epochs, max_rate=0.0025, start_rate=0.00025, last_rate=0.00001)\n# #         history = model.fit(train_ds, epochs=n_epochs, validation_data=valid_ds, callbacks=[onecycle])\n#         history = model.fit(train_ds, epochs=n_epochs, validation_data=valid_ds, callbacks=[early_stop, plateau])\n#         epoch_stop += (np.argmin(history.history['val_corr_eval']) + 1) / cv\n#         model.save(f\"model_{split}_ts\")\n#         pearson_score = stats.pearsonr(model.predict(valid_ds).ravel(), train.loc[test_split_index, 'target'].values)[0]\n#         print('Pearson:', pearson_score)\n#         pd.DataFrame(history.history, columns=[\"loss\", \"val_loss\"]).plot()\n#         plt.title(\"MSE\")\n#         plt.show()\n#         pd.DataFrame(history.history, columns=[\"corr_eval\", \"val_corr_eval\"]).plot()\n#         plt.title(\"CORR\")\n#         plt.show()\n        \n#         K.clear_session()\n#         tf.compat.v1.reset_default_graph()\n#         del model\n#         del history\n#         del train_ds\n#         del valid_ds\n#         plt.close()\n#         gc.collect()","metadata":{"papermill":{"duration":471.71946,"end_time":"2022-01-25T15:47:23.749584","exception":false,"start_time":"2022-01-25T15:39:32.030124","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{"papermill":{"duration":2.11441,"end_time":"2022-01-25T15:47:27.649127","exception":false,"start_time":"2022-01-25T15:47:25.534717","status":"completed"},"tags":[]}},{"cell_type":"code","source":"models = {}\nsplits = 5\nfor split in range(splits):\n    models[f\"model_{split}_skf\"] = load_model(f\"model_{split}_skf\")\n#     models[f\"model_{split}_ts\"] = load_model(f\"model_{split}_ts\")\n\ndef preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds","metadata":{"papermill":{"duration":1.80736,"end_time":"2022-01-25T15:47:31.24096","exception":false,"start_time":"2022-01-25T15:47:29.4336","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = ubiquant.make_env()\niter_test = env.iter_test()\n\nfor (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df['target'] = 0\n    for split in range(splits):\n        ds = make_test_dataset(test_df[features], test_df['investment_id'])\n#         sample_prediction_df['target'] += (0.8*models[f\"model_{split}_skf\"].predict(ds).ravel()+0.2*models[f\"model_{split}_ts\"].predict(ds).ravel()) / splits\n        sample_prediction_df['target'] += models[f\"model_{split}_skf\"].predict(ds).ravel() / splits\n\n    env.predict(sample_prediction_df) ","metadata":{"papermill":{"duration":2.234161,"end_time":"2022-01-25T15:47:35.24214","exception":false,"start_time":"2022-01-25T15:47:33.007979","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}