{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom scipy import stats\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.keras import backend as K\nfrom ipywidgets import interactive_output\nfrom ipywidgets import Text, HBox, VBox, Select, fixed\nimport math\nimport random\nimport lightgbm\nfrom scipy.stats import probplot, pearsonr\n\nimport seaborn as sns\n\n### add random seed\ntf.random.set_seed(3)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-23T11:07:38.982572Z","iopub.execute_input":"2022-03-23T11:07:38.983432Z","iopub.status.idle":"2022-03-23T11:07:39.0552Z","shell.execute_reply.started":"2022-03-23T11:07:38.983389Z","shell.execute_reply":"2022-03-23T11:07:39.054493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_parquet('../input/ubiquant-parquet-low-mem/train_low_mem.parquet')\ntrain_data = train_data.drop('row_id', axis=1)\ntrain_data.loc[:, ['time_id', 'investment_id']] = train_data.loc[:, ['time_id', 'investment_id']].astype(np.int16)\ntrain_data = train_data.sort_values(['time_id', 'investment_id'], ascending=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:06:10.152512Z","iopub.execute_input":"2022-03-23T11:06:10.152999Z","iopub.status.idle":"2022-03-23T11:06:55.287507Z","shell.execute_reply.started":"2022-03-23T11:06:10.152961Z","shell.execute_reply":"2022-03-23T11:06:55.28655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Rows and Columns in train dataset:', train_data.shape)\nprint('Missing values in train dataset:', sum(train_data.isnull().sum()))","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:11:33.863839Z","iopub.execute_input":"2022-03-23T11:11:33.864534Z","iopub.status.idle":"2022-03-23T11:11:35.983266Z","shell.execute_reply.started":"2022-03-23T11:11:33.864492Z","shell.execute_reply":"2022-03-23T11:11:35.982319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\ncolors = sns.color_palette('Wistia', 20)\nmean_target = train_data.groupby(['investment_id'])['target'].mean()\nfig, axes = plt.subplots(3, figsize=(20, 10), sharex=True)\n\ntrain_data.groupby('time_id')['investment_id'].nunique().plot(color=random.choice(colors), ax=axes[0])\n\n\ntrain_data.groupby('time_id')['target'].mean().plot(color=random.choice(colors), ax=axes[1])\n\naxes[1].axhline(y=np.mean(mean_target), color='black', linestyle='--', label=\"mean\")\n\n\ntrain_data.groupby('time_id')['target'].std().plot(color=random.choice(colors), ax=axes[2])\n\naxes[2].axhline(y=np.mean(train_data.groupby('time_id')['target'].std()), color='black', linestyle='--', label=\"mean\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:14:18.037651Z","iopub.execute_input":"2022-03-23T11:14:18.038416Z","iopub.status.idle":"2022-03-23T11:14:19.348331Z","shell.execute_reply.started":"2022-03-23T11:14:18.038372Z","shell.execute_reply":"2022-03-23T11:14:19.347386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scatter_hist(plot_data, col_x, col_y, hue=None, title=None):\n    fig, axes = plt.subplots(2, 2, figsize=(10, 10), facecolor='white',gridspec_kw={'width_ratios': [9, 1],'height_ratios': [1, 9]})\n    sns.scatterplot(data=plot_data, x=col_x, y=col_y, ax=axes[1, 0], hue=hue)\n    sns.histplot(data=plot_data, x=col_x, ax=axes[0, 0], hue=hue, kde=True)\n    sns.histplot(data=plot_data, y=col_y,ax=axes[1, 1], hue=hue, kde=True)\n                             \n    axes = axes.ravel()\n    for i in range(len(axes)):\n        axes[i].grid(True)\n        if i != 2:\n            axes[i].tick_params(length=0)\n            axes[i].xaxis.set_visible(False)\n            axes[i].yaxis.set_visible(False)\n            for loc in ['top', 'bottom', 'left', 'right']:\n                axes[i].spines[loc].set_visible(False)\n    if type(title) is str:\n        fig.suptitle(title)\n    fig.tight_layout(rect=[0, 0, 0.96, 1])\n    plt.close()\n    return fig\n\n\ndef scatter_groupby(dataframe, col_x, agg_x, col_y, agg_y, groupby):\n    target_meam_by_time_id = dataframe.loc[:, ['time_id', 'investment_id', col_y]].groupby(groupby).agg(agg_y).copy()\n    feats_nuni_by_time_id = dataframe.loc[:, ['time_id', 'investment_id', col_x]].groupby(groupby).agg(agg_x).copy()\n    plot_data = pd.concat([target_meam_by_time_id, feats_nuni_by_time_id], axis=1)\n    title = f'X: {col_x}({agg_x}) Y: {col_y}({agg_y}) / groupby: {groupby}'\n    fig = scatter_hist(plot_data, col_x, col_y, title=title)\n    display(fig)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:06:55.289419Z","iopub.execute_input":"2022-03-23T11:06:55.289687Z","iopub.status.idle":"2022-03-23T11:06:55.303732Z","shell.execute_reply.started":"2022-03-23T11:06:55.289651Z","shell.execute_reply":"2022-03-23T11:06:55.302746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = fixed(train_data)\ns_x = Select(description='X', options=train_data.columns[2:], value='f_62', rows=4,)\ns_y = Select(description='Y', options=train_data.columns[2:], value='target', rows=4,)\ns_aggx = Select(description='X_aggregate', options=['mean', 'std', 'nunique', 'min', 'max'], value='nunique', rows=4,)   \ns_aggy = Select(description='Y_aggregate', options=['mean', 'std', 'nunique', 'min', 'max'], value='mean', rows=4,)   \ns_gby = Select(description='groupby', options=['time_id', 'investment_id'], value='time_id', rows=2,) \n\nselector = HBox([s_x, s_aggx, s_y, s_aggy, s_gby])\nplot_output = interactive_output(scatter_groupby, dict(dataframe=data, col_x=s_x, agg_x=s_aggx, col_y=s_y, agg_y=s_aggy, groupby=s_gby))\ndisplay(selector, plot_output)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:07:42.864183Z","iopub.execute_input":"2022-03-23T11:07:42.865009Z","iopub.status.idle":"2022-03-23T11:07:46.362237Z","shell.execute_reply.started":"2022-03-23T11:07:42.864958Z","shell.execute_reply":"2022-03-23T11:07:46.3614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n#     is_training = False\n    is_training = True\n    tf_record_dataset_path = \"../input/ump-combinatorialpurgedgroupkfold-tf-record/\"\n    output_dataset_path = \"../input/ubiquant-market-prediction-with-dnn-output/\"\nconfig = Config()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T19:29:59.391888Z","iopub.execute_input":"2022-03-22T19:29:59.392107Z","iopub.status.idle":"2022-03-22T19:29:59.399293Z","shell.execute_reply.started":"2022-03-22T19:29:59.392074Z","shell.execute_reply":"2022-03-22T19:29:59.398619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ninvestment_ids = pd.read_csv(\"../input/ump-combinatorialpurgedgroupkfold-tf-record/investment_ids.csv\")\ninvestment_id_size = len(investment_ids) + 1\nwith tf.device(\"cpu\"):\n    investment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\n    investment_id_lookup_layer.adapt(investment_ids)","metadata":{"execution":{"iopub.status.busy":"2022-03-22T19:29:59.400752Z","iopub.execute_input":"2022-03-22T19:29:59.40105Z","iopub.status.idle":"2022-03-22T19:30:01.602669Z","shell.execute_reply.started":"2022-03-22T19:29:59.400967Z","shell.execute_reply":"2022-03-22T19:30:01.601928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_function(record_bytes):\n  return tf.io.parse_single_example(\n      # Data\n      record_bytes,\n      # Schema\n      {\n          \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n          \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"investment_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n      }\n  )\n\ndef preprocess(item):\n    return (item[\"features\"]), item[\"target\"]\ndef make_dataset(file_paths, batch_size=4096, mode=\"train\"):\n    ds = tf.data.TFRecordDataset(file_paths)\n    ds = ds.map(decode_function)\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(batch_size * 4)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-03-22T19:30:01.605254Z","iopub.execute_input":"2022-03-22T19:30:01.605881Z","iopub.status.idle":"2022-03-22T19:30:01.615612Z","shell.execute_reply.started":"2022-03-22T19:30:01.605839Z","shell.execute_reply":"2022-03-22T19:30:01.614867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def correlation(x, y, axis=-2):\n    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    \n    \n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xvar * yvar)\n    return tf.constant(1.0, dtype=x.dtype) - corr\n\ndef get_model():\n\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    \n    ## feature ##\n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.1)(feature_x)\n    ## convolution 1 ##\n    feature_x = layers.Reshape((-1,1))(feature_x)\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 2 ##\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 3 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 4 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 5 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 6 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=2, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## flatten ##\n    feature_x = layers.Flatten()(feature_x)\n    \n\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n    \n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse, correlation])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-22T19:30:01.617707Z","iopub.execute_input":"2022-03-22T19:30:01.618359Z","iopub.status.idle":"2022-03-22T19:30:01.642245Z","shell.execute_reply.started":"2022-03-22T19:30:01.61832Z","shell.execute_reply":"2022-03-22T19:30:01.641186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model()\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-22T19:30:01.644206Z","iopub.execute_input":"2022-03-22T19:30:01.644635Z","iopub.status.idle":"2022-03-22T19:30:03.071536Z","shell.execute_reply.started":"2022-03-22T19:30:01.644593Z","shell.execute_reply":"2022-03-22T19:30:03.070353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodels = []\nfor i in range(1):\n    train_path = f\"{config.tf_record_dataset_path}fold_{i}_train.tfrecords\"\n    valid_path = f\"{config.tf_record_dataset_path}fold_{i}_test.tfrecords\"\n    valid_ds = make_dataset([valid_path], mode=\"valid\")\n    print(valid_ds)\n    model = get_model()\n    if config.is_training:\n        train_ds = make_dataset([train_path])\n        checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{i}.tf\", monitor=\"val_correlation\", mode=\"min\", save_best_only=True, save_weights_only=True)\n        early_stop = keras.callbacks.EarlyStopping(patience=10)\n        history = model.fit(train_ds, epochs=40, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n        model.load_weights(f\"model_{i}.tf\")\n        for metric in [\"loss\", \"mae\", \"mape\", \"rmse\", \"correlation\"]:\n            pd.DataFrame(history.history, columns=[metric, f\"val_{metric}\"]).plot()\n            plt.title(metric.upper())\n            plt.show()\n    else:\n        model.load_weights(f\"{config.output_dataset_path}model_{i}.tf\")\n    y_vals = []\n    for _, y in valid_ds:\n        y_vals += list(y.numpy().reshape(-1))\n    y_val = np.array(y_vals)\n    pearson_score = stats.pearsonr(model.predict(valid_ds).reshape(-1), y_val)[0]\n    models.append(model)\n    print(f\"Pearson Score: {pearson_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-03-22T19:30:03.072919Z","iopub.execute_input":"2022-03-22T19:30:03.073193Z","iopub.status.idle":"2022-03-22T19:43:37.981729Z","shell.execute_reply.started":"2022-03-22T19:30:03.073158Z","shell.execute_reply":"2022-03-22T19:43:37.978672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_test_dataset(feature, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((feature)))\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds\n\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-22T19:43:37.983109Z","iopub.execute_input":"2022-03-22T19:43:37.986766Z","iopub.status.idle":"2022-03-22T19:43:37.996838Z","shell.execute_reply.started":"2022-03-22T19:43:37.986714Z","shell.execute_reply":"2022-03-22T19:43:37.995718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfeatures = [f\"f_{i}\" for i in range(300)]\nfor (test_df, sample_prediction_df) in iter_test:\n    ds = make_test_dataset(test_df[features])\n    sample_prediction_df['target'] = inference(models, ds)\n    env.predict(sample_prediction_df) ","metadata":{"execution":{"iopub.status.busy":"2022-03-22T19:45:41.697141Z","iopub.execute_input":"2022-03-22T19:45:41.697507Z","iopub.status.idle":"2022-03-22T19:45:42.060467Z","shell.execute_reply.started":"2022-03-22T19:45:41.697461Z","shell.execute_reply":"2022-03-22T19:45:42.059722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}