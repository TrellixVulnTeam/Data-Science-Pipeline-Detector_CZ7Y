{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Ubiquant Market Prediction with DNN\nIn this notebook, I will build a Ubiquant Market Prediction Model with TensorFlow DNN. \nI will train the Model using CombinatorialPurgedGroupKFold CV strategy. You can find more detials in this [discussion](https://www.kaggle.com/c/ubiquant-market-prediction/discussion/305118).\nIn other to train the Model for 5 folds in a single run and avoid OOM error, I made a TF-record dataset in notebook [UMP TF-Record: CombinatorialPurgedGroupKFold](https://www.kaggle.com/lonnieqin/ump-tf-record-combinatorialpurgedgroupkfold).\n## Import Packages","metadata":{"papermill":{"duration":0.016475,"end_time":"2022-01-25T15:39:01.467349","exception":false,"start_time":"2022-01-25T15:39:01.450874","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom scipy import stats\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.keras import backend as K","metadata":{"papermill":{"duration":7.781864,"end_time":"2022-01-25T15:39:09.265726","exception":false,"start_time":"2022-01-25T15:39:01.483862","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-26T08:10:10.24191Z","iopub.execute_input":"2022-03-26T08:10:10.242225Z","iopub.status.idle":"2022-03-26T08:10:10.248979Z","shell.execute_reply.started":"2022-03-26T08:10:10.242192Z","shell.execute_reply":"2022-03-26T08:10:10.247539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"class Config:\n    is_training = False\n    tf_record_dataset_path = \"../input/ump-combinatorialpurgedgroupkfold-tf-record/\"\n    output_dataset_path = \"../input/ubiquant-market-prediction-with-dnn-output/\"\nconfig = Config()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T08:10:12.608841Z","iopub.execute_input":"2022-03-26T08:10:12.609198Z","iopub.status.idle":"2022-03-26T08:10:12.615403Z","shell.execute_reply.started":"2022-03-26T08:10:12.609166Z","shell.execute_reply":"2022-03-26T08:10:12.614022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create an IntegerLookup layer for investment_id input","metadata":{"papermill":{"duration":0.016633,"end_time":"2022-01-25T15:39:26.408785","exception":false,"start_time":"2022-01-25T15:39:26.392152","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\ninvestment_ids = pd.read_csv(\"../input/ump-combinatorialpurgedgroupkfold-tf-record/investment_ids.csv\")\ninvestment_id_size = len(investment_ids) + 1\nwith tf.device(\"cpu\"):\n    investment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\n    investment_id_lookup_layer.adapt(investment_ids)","metadata":{"papermill":{"duration":4.105382,"end_time":"2022-01-25T15:39:30.530653","exception":false,"start_time":"2022-01-25T15:39:26.425271","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-26T08:10:14.900879Z","iopub.execute_input":"2022-03-26T08:10:14.90146Z","iopub.status.idle":"2022-03-26T08:10:17.75473Z","shell.execute_reply.started":"2022-03-26T08:10:14.901424Z","shell.execute_reply":"2022-03-26T08:10:17.753724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make Tensorflow dataset","metadata":{"papermill":{"duration":0.018846,"end_time":"2022-01-25T15:39:30.567495","exception":false,"start_time":"2022-01-25T15:39:30.548649","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def decode_function(record_bytes):\n  return tf.io.parse_single_example(\n      # Data\n      record_bytes,\n      # Schema\n      {\n          \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n          \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"investment_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n      }\n  )\ndef preprocess(item):\n    return (item[\"investment_id\"], item[\"features\"]), item[\"target\"]\ndef make_dataset(file_paths, batch_size=4096, mode=\"train\"):\n    ds = tf.data.TFRecordDataset(file_paths)\n    ds = ds.map(decode_function)\n    if mode != \"train\":\n        time_ids = ds.map(lambda item: item[\"time_id\"])\n        time_ids = time_ids.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(batch_size * 4) \n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    if mode == \"train\":\n        return ds\n    else:\n        return ds, time_ids","metadata":{"papermill":{"duration":0.02858,"end_time":"2022-01-25T15:39:30.614302","exception":false,"start_time":"2022-01-25T15:39:30.585722","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-26T08:11:43.972042Z","iopub.execute_input":"2022-03-26T08:11:43.972347Z","iopub.status.idle":"2022-03-26T08:11:43.986493Z","shell.execute_reply.started":"2022-03-26T08:11:43.972314Z","shell.execute_reply":"2022-03-26T08:11:43.982916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{"papermill":{"duration":0.017564,"end_time":"2022-01-25T15:39:30.64918","exception":false,"start_time":"2022-01-25T15:39:30.631616","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def correlation(x, y, axis=-2):\n    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xvar * yvar)\n    return tf.constant(1.0, dtype=x.dtype) - corr\n\ndef get_model():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.1)(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.1)(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.1)(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.1)(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.1)(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.1)(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse, correlation])\n    return model","metadata":{"papermill":{"duration":0.033569,"end_time":"2022-01-25T15:39:30.700649","exception":false,"start_time":"2022-01-25T15:39:30.66708","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-26T08:11:48.772472Z","iopub.execute_input":"2022-03-26T08:11:48.77277Z","iopub.status.idle":"2022-03-26T08:11:48.793756Z","shell.execute_reply.started":"2022-03-26T08:11:48.77274Z","shell.execute_reply":"2022-03-26T08:11:48.791908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at this Model's architecture.","metadata":{}},{"cell_type":"code","source":"model = get_model()\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)","metadata":{"papermill":{"duration":0.209912,"end_time":"2022-01-25T15:39:30.929112","exception":false,"start_time":"2022-01-25T15:39:30.7192","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-26T08:11:52.9204Z","iopub.execute_input":"2022-03-26T08:11:52.92067Z","iopub.status.idle":"2022-03-26T08:11:54.409469Z","shell.execute_reply.started":"2022-03-26T08:11:52.920641Z","shell.execute_reply":"2022-03-26T08:11:54.407478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"%%time\nmodels = []\nfor i in range(5):\n    train_path = f\"{config.tf_record_dataset_path}fold_{i}_train.tfrecords\"\n    valid_path = f\"{config.tf_record_dataset_path}fold_{i}_test.tfrecords\"\n    valid_ds, time_id_ds = make_dataset([valid_path], mode=\"valid\")\n    model = get_model()\n    if config.is_training:\n        train_ds = make_dataset([train_path])\n        checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{i}.tf\", monitor=\"val_correlation\", mode=\"min\", save_best_only=True, save_weights_only=True)\n        early_stop = keras.callbacks.EarlyStopping(patience=10)\n        history = model.fit(train_ds, epochs=30, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n        model.load_weights(f\"model_{i}.tf\")\n        for metric in [\"loss\", \"mae\", \"mape\", \"rmse\", \"correlation\"]:\n            pd.DataFrame(history.history, columns=[metric, f\"val_{metric}\"]).plot()\n            plt.title(metric.upper())\n            plt.show()\n    else:\n        model.load_weights(f\"{config.output_dataset_path}model_{i}.tf\")\n    y_vals = []\n    time_ids = []\n    for (_, y), time_id in zip(valid_ds, time_id_ds):\n        y_vals += list(y.numpy().reshape(-1))\n        time_ids += list(time_id.numpy().reshape(-1))\n    y_pred = model.predict(valid_ds).reshape(-1)\n    df = pd.DataFrame({\"time_id\": time_ids, \"y\": y_vals, \"y_predict\": y_pred})\n    pearson_score = df.groupby(\"time_id\").apply(lambda item: stats.pearsonr(item.y, item.y_predict)[0]).mean()\n    print(f\"Pearson Score: {pearson_score}\")\n    models.append(model)","metadata":{"papermill":{"duration":471.71946,"end_time":"2022-01-25T15:47:23.749584","exception":false,"start_time":"2022-01-25T15:39:32.030124","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-26T08:12:58.39699Z","iopub.execute_input":"2022-03-26T08:12:58.397304Z","iopub.status.idle":"2022-03-26T08:18:44.887452Z","shell.execute_reply.started":"2022-03-26T08:12:58.397271Z","shell.execute_reply":"2022-03-26T08:18:44.886056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{"papermill":{"duration":2.11441,"end_time":"2022-01-25T15:47:27.649127","exception":false,"start_time":"2022-01-25T15:47:25.534717","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{"papermill":{"duration":1.80736,"end_time":"2022-01-25T15:47:31.24096","exception":false,"start_time":"2022-01-25T15:47:29.4336","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-27T16:23:29.779097Z","iopub.execute_input":"2022-02-27T16:23:29.779531Z","iopub.status.idle":"2022-02-27T16:23:29.794517Z","shell.execute_reply.started":"2022-02-27T16:23:29.779467Z","shell.execute_reply":"2022-02-27T16:23:29.793065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfeatures = [f\"f_{i}\" for i in range(300)]\nfor (test_df, sample_prediction_df) in iter_test:\n    ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n    sample_prediction_df['target'] = inference(models[0:4], ds)\n    env.predict(sample_prediction_df) ","metadata":{"papermill":{"duration":2.234161,"end_time":"2022-01-25T15:47:35.24214","exception":false,"start_time":"2022-01-25T15:47:33.007979","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-27T16:23:32.513471Z","iopub.execute_input":"2022-02-27T16:23:32.513771Z","iopub.status.idle":"2022-02-27T16:23:34.026536Z","shell.execute_reply.started":"2022-02-27T16:23:32.513741Z","shell.execute_reply":"2022-02-27T16:23:34.025509Z"},"trusted":true},"execution_count":null,"outputs":[]}]}