{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Create TF-Record for UMP dataset\n\nIn this notebook, I am going to create TF-Record for UMP dataset. I am going to divide the dataset to 10 folds by using Time Series Split. You may experiment other types of Data Spliting method.","metadata":{"papermill":{"duration":0.016475,"end_time":"2022-01-25T15:39:01.467349","exception":false,"start_time":"2022-01-25T15:39:01.450874","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Import Packages","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt76de7\nimport tensorflow as tf\nimport json","metadata":{"papermill":{"duration":7.781864,"end_time":"2022-01-25T15:39:09.265726","exception":false,"start_time":"2022-01-25T15:39:01.483862","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T02:33:37.897304Z","iopub.execute_input":"2022-01-29T02:33:37.897588Z","iopub.status.idle":"2022-01-29T02:33:37.90285Z","shell.execute_reply.started":"2022-01-29T02:33:37.897556Z","shell.execute_reply":"2022-01-29T02:33:37.901921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import dataset","metadata":{"papermill":{"duration":0.015291,"end_time":"2022-01-25T15:39:09.296817","exception":false,"start_time":"2022-01-25T15:39:09.281526","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\ntrain.head()","metadata":{"papermill":{"duration":16.88418,"end_time":"2022-01-25T15:39:26.19638","exception":false,"start_time":"2022-01-25T15:39:09.3122","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T02:33:41.969997Z","iopub.execute_input":"2022-01-29T02:33:41.970414Z","iopub.status.idle":"2022-01-29T02:33:44.989166Z","shell.execute_reply.started":"2022-01-29T02:33:41.970385Z","shell.execute_reply":"2022-01-29T02:33:44.988574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment_id = train.pop(\"investment_id\")\ninvestment_id.head()","metadata":{"papermill":{"duration":0.041856,"end_time":"2022-01-25T15:39:26.254473","exception":false,"start_time":"2022-01-25T15:39:26.212617","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T02:34:05.536804Z","iopub.execute_input":"2022-01-29T02:34:05.537495Z","iopub.status.idle":"2022-01-29T02:34:05.551433Z","shell.execute_reply.started":"2022-01-29T02:34:05.537453Z","shell.execute_reply":"2022-01-29T02:34:05.550467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_id = train.pop(\"time_id\")","metadata":{"papermill":{"duration":0.045271,"end_time":"2022-01-25T15:39:26.316843","exception":false,"start_time":"2022-01-25T15:39:26.271572","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T02:15:47.319441Z","iopub.execute_input":"2022-01-29T02:15:47.319802Z","iopub.status.idle":"2022-01-29T02:15:47.332473Z","shell.execute_reply.started":"2022-01-29T02:15:47.319743Z","shell.execute_reply":"2022-01-29T02:15:47.331536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train.pop(\"target\")\ny.head()","metadata":{"papermill":{"duration":0.04224,"end_time":"2022-01-25T15:39:26.375506","exception":false,"start_time":"2022-01-25T15:39:26.333266","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-29T02:15:57.478365Z","iopub.execute_input":"2022-01-29T02:15:57.478651Z","iopub.status.idle":"2022-01-29T02:15:57.493589Z","shell.execute_reply.started":"2022-01-29T02:15:57.478621Z","shell.execute_reply":"2022-01-29T02:15:57.492943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create TF-Record","metadata":{"papermill":{"duration":0.017564,"end_time":"2022-01-25T15:39:30.64918","exception":false,"start_time":"2022-01-25T15:39:30.631616","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def create_record(i):\n    dic = {}\n    dic[f\"features\"] = tf.train.Feature(float_list=tf.train.FloatList(value=list(train.iloc[i])))\n    dic[\"time_id\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[time_id.iloc[i]]))\n    dic[\"investment_id\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[investment_id.iloc[i]]))\n    dic[\"target\"] = tf.train.Feature(float_list=tf.train.FloatList(value=[y.iloc[i]]))\n    record_bytes = tf.train.Example(features=tf.train.Features(feature=dic)).SerializeToString()\n    return record_bytes\n    \ndef decode_function(record_bytes):\n  return tf.io.parse_single_example(\n      # Data\n      record_bytes,\n      # Schema\n      {\n          \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n          \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"investment_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n      }\n  )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now separate dataset to 10 folds and make sure samples with same time_id appear only in the same fold. So the sample count for each fold are slightly different. In the same time, I also output some useful fold information for convenience.","metadata":{}},{"cell_type":"code","source":"folds = 10\ntime_id_value_counts = dict(time_id.value_counts())\ntime_ids = sorted(time_id.unique())\nsample_per_fold = train.shape[0] // folds\nfold = 0\nsample_count_for_fold = 0\ntime_ids_for_fold = []\nfold_info = []\ntotal_count = 0\nfor i in range(len(time_ids)):\n    identifier = time_ids[i]\n    sample_count_for_fold += time_id_value_counts[identifier]\n    time_ids_for_fold.append(identifier)\n    if sample_count_for_fold >= sample_per_fold or i == len(time_ids) - 1:\n        print(f\"Sample Count for Fold {fold}\", sample_count_for_fold)\n        fold_info.append({\"sample_count\": sample_count_for_fold, \"time_ids\": time_ids_for_fold, \"start_position\": total_count, \"end_position\": total_count + sample_count_for_fold - 1, \"file_name\": f\"fold_{fold}.tfrecords\"})\n        total_count += sample_count_for_fold\n        sample_count_for_fold = 0\n        time_ids_for_current_fold = []\n        fold += 1\ninfo = pd.DataFrame(fold_info)\ninfo.to_csv(\"info.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T02:17:05.424708Z","iopub.execute_input":"2022-01-29T02:17:05.425031Z","iopub.status.idle":"2022-01-29T02:17:05.485946Z","shell.execute_reply.started":"2022-01-29T02:17:05.425001Z","shell.execute_reply":"2022-01-29T02:17:05.484972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see a simple sample of creating and reading TF-Record.","metadata":{}},{"cell_type":"code","source":"%%time\nsave_path = \"sample.tfrecords\"\nwith tf.io.TFRecordWriter(save_path) as file_writer:\n    for i in range(10000):\n        record_bytes = create_record(i)\n        file_writer.write(record_bytes)\ndataset = tf.data.TFRecordDataset([save_path])\ndataset = dataset.map(decode_function).batch(32)\nfor item in dataset.take(1):\n    print(item)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T18:55:58.873418Z","iopub.execute_input":"2022-01-28T18:55:58.873907Z","iopub.status.idle":"2022-01-28T18:56:01.593555Z","shell.execute_reply.started":"2022-01-28T18:55:58.87386Z","shell.execute_reply":"2022-01-28T18:56:01.592942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now create the whole dataset, it will take more than 1 hour.","metadata":{}},{"cell_type":"code","source":"%%time\nimport time\nfor i, info in enumerate(fold_info):\n    begin = time.time()\n    save_path = f\"fold_{i}.tfrecords\"\n    print(f\"Create {save_path}\")\n    print(f\"Begin position: %d, End Position: %d\"%(info[\"start_position\"], info[\"end_position\"]))\n    save_path = f\"fold_{i}.tfrecords\"\n    with tf.io.TFRecordWriter(save_path) as file_writer:\n        for i in range(info[\"start_position\"], info[\"end_position\"]):\n            record_bytes = create_record(i)\n            file_writer.write(record_bytes)\n    print(\"Elapsed time: %.2f\"%(time.time() - begin))","metadata":{"execution":{"iopub.status.busy":"2022-01-28T19:10:26.846113Z","iopub.execute_input":"2022-01-28T19:10:26.846489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Write unique Investment Ids","metadata":{}},{"cell_type":"code","source":"investment_ids = investment_id.unique()\ninvestment_id_df = pd.DataFrame({\"investment_id\": investment_ids})\ninvestment_id_df.to_csv(\"investment_ids.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}