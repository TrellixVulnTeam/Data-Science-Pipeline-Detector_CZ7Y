{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.cluster import AgglomerativeClustering\n!pip install hdbscan\nimport umap, hdbscan\nimport pickle\nfrom scipy.stats import pearsonr\nfrom typing import Tuple\nimport seaborn as sns\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"ee93e926-f1ac-4dd4-9105-10e1ff0156f6","_cell_guid":"a80f36dd-15b8-4576-8a48-4d55b21aae02","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-05T01:09:49.242613Z","iopub.execute_input":"2022-04-05T01:09:49.242894Z","iopub.status.idle":"2022-04-05T01:11:27.288406Z","shell.execute_reply.started":"2022-04-05T01:09:49.242824Z","shell.execute_reply":"2022-04-05T01:11:27.287454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Overview of the data","metadata":{"_uuid":"d30a094d-912d-47d0-9c09-ef10f04226b3","_cell_guid":"ce8beeed-ccc1-48af-9609-1a31c1e42da7","trusted":true}},{"cell_type":"code","source":"train    = pd.read_pickle('/kaggle/input/low-memory-pickle/train.pkl')\n\nfeatures = [col for col in train.columns if col.startswith('f_')]\ntarget   = 'target'","metadata":{"_uuid":"ac86170e-28e6-421a-9c08-f97801217cbb","_cell_guid":"366c0c69-ecc7-49f2-aee2-265afa4a7260","collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-05T01:11:27.290074Z","iopub.execute_input":"2022-04-05T01:11:27.290241Z","iopub.status.idle":"2022-04-05T01:11:43.844669Z","shell.execute_reply.started":"2022-04-05T01:11:27.29022Z","shell.execute_reply":"2022-04-05T01:11:43.844088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"_uuid":"ed6abcb8-7416-407f-911e-9c367985a418","_cell_guid":"22bdefe1-f84f-457f-be19-531802fdc925","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-05T01:11:43.881566Z","iopub.execute_input":"2022-04-05T01:11:43.882336Z","iopub.status.idle":"2022-04-05T01:11:43.931813Z","shell.execute_reply.started":"2022-04-05T01:11:43.882309Z","shell.execute_reply":"2022-04-05T01:11:43.930931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-05T01:11:43.933057Z","iopub.execute_input":"2022-04-05T01:11:43.933218Z","iopub.status.idle":"2022-04-05T01:11:43.959437Z","shell.execute_reply.started":"2022-04-05T01:11:43.933198Z","shell.execute_reply":"2022-04-05T01:11:43.958574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum().sum()","metadata":{"_uuid":"f0b79692-07a4-44ac-94a1-e4bfe88691c9","_cell_guid":"1d217a03-aede-428d-a887-e52029f82110","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T00:02:41.950985Z","iopub.execute_input":"2022-03-26T00:02:41.951915Z","iopub.status.idle":"2022-03-26T00:02:48.396721Z","shell.execute_reply.started":"2022-03-26T00:02:41.951843Z","shell.execute_reply":"2022-03-26T00:02:48.395777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many time_ids are there?\nlen(train.index.unique())","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:02:48.39793Z","iopub.execute_input":"2022-03-26T00:02:48.398205Z","iopub.status.idle":"2022-03-26T00:02:48.458489Z","shell.execute_reply.started":"2022-03-26T00:02:48.398164Z","shell.execute_reply":"2022-03-26T00:02:48.457604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many investment_ids are there? \nlen(train.investment_id.unique())","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:02:48.460092Z","iopub.execute_input":"2022-03-26T00:02:48.460615Z","iopub.status.idle":"2022-03-26T00:02:48.494633Z","shell.execute_reply.started":"2022-03-26T00:02:48.46056Z","shell.execute_reply":"2022-03-26T00:02:48.493867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we have a panel of 3579 different `investment_id`s across 1211 time periods, jointly indexing a target variable (presumably some sort of forward return) and 300 features. We don't know what the `time_id`corresponds to, nor what any of the features are. ","metadata":{}},{"cell_type":"markdown","source":"# Exploration of the target variable\n\nLet's take a look at the overall distribution of the target variable:","metadata":{}},{"cell_type":"code","source":"(train[target]\n .plot(kind='hist', \n       bins = 1000, \n       figsize = (20,10), \n       title='Distribution of target variable', \n       ylabel='Frequency'))","metadata":{"_uuid":"35a5aa99-9380-4dff-8b7f-99c28061b4a9","_cell_guid":"2a32d0f7-a25a-46a6-9fb4-80645b7f435a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T00:02:48.495972Z","iopub.execute_input":"2022-03-26T00:02:48.496388Z","iopub.status.idle":"2022-03-26T00:02:52.563235Z","shell.execute_reply.started":"2022-03-26T00:02:48.496352Z","shell.execute_reply":"2022-03-26T00:02:52.562493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The general picture is pretty normal, if skewed somewhat to the right.\n\nOne thing that immediately jumps out is a discontinuity at 0. Let's first look into whether that's a data quality issue or not:","metadata":{"_uuid":"7dd78636-34f4-49a3-b2dc-22df9750db2b","_cell_guid":"87cb8f8a-70f3-41f8-9c6f-25cfb38d46b5","trusted":true}},{"cell_type":"code","source":"# Graph a narrow bandwidth around 0 just to confirm\n(train[target]\n .groupby(pd.cut(train[target], bins=np.arange(-0.4,0.4,0.02)))\n .count()\n .plot(rot=45, \n       title='Does the target variable bunch around 0?', \n       ylabel='Frequency', \n       xlabel='Target (binned)',\n       xticks=np.arange(18,22),\n       grid=True,\n       figsize=(20, 10)))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:02:52.564494Z","iopub.execute_input":"2022-03-26T00:02:52.564835Z","iopub.status.idle":"2022-03-26T00:02:58.573151Z","shell.execute_reply.started":"2022-03-26T00:02:52.564801Z","shell.execute_reply":"2022-03-26T00:02:58.57187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's a clump of data at 0 -- 0s are about 10% more frequent than values in the neighborhood. \n\nIs this a problem with the data? Maybe not: I could see a world in which financial data is truly discontinuous at 0, e.g. if our target is a forward return and between two periods no one trades the underlying asset, then its price won't change and the return will be zero. If so, then there's no need to worry about this. I'd be curious, though, to see if the frequency of 0s varies by `time_id`or `investment_id`, which might reflect artifacts in the data that we'd want to cut out of our training set. \n\nLet's plot the distribution of the share of zeros, first by `investment_id`, then by time_id:","metadata":{}},{"cell_type":"code","source":"train['temp'] = train[target].between(-0.02, 0.02)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:02:58.574826Z","iopub.execute_input":"2022-03-26T00:02:58.575372Z","iopub.status.idle":"2022-03-26T00:02:58.695328Z","shell.execute_reply.started":"2022-03-26T00:02:58.575319Z","shell.execute_reply":"2022-03-26T00:02:58.694225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(train\n .groupby('investment_id')['temp']\n .mean()\n .plot(kind='hist',\n       bins=100,\n       figsize=(20, 10)))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:02:58.696574Z","iopub.execute_input":"2022-03-26T00:02:58.696799Z","iopub.status.idle":"2022-03-26T00:02:59.272604Z","shell.execute_reply.started":"2022-03-26T00:02:58.696771Z","shell.execute_reply":"2022-03-26T00:02:59.271656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The left-hand side of this plot shows nothing out of the ordinary. Let's see what those outliers are:","metadata":{}},{"cell_type":"code","source":"\n(train\n .groupby('investment_id')['temp']\n .aggregate(['mean','count'])\n .sort_values('mean', ascending=False)[:30])\n","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:02:59.27437Z","iopub.execute_input":"2022-03-26T00:02:59.275393Z","iopub.status.idle":"2022-03-26T00:02:59.488366Z","shell.execute_reply.started":"2022-03-26T00:02:59.27534Z","shell.execute_reply":"2022-03-26T00:02:59.487463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, investments that only show up a handful of times in the data are the outliers -- although this doesn't tell us much, as it's consistent even with the null hypothesis that the true fraction of zeros per `investment_id` is constant (i.e. this could just reflect sampling variation). That being said it seems sensible to throw out the most sparse `investment_id`s, because they basically have no variation in the target variable and are unlikely to show up in the evaluation data. \n\nDoes the fraction of zeros vary across time?","metadata":{}},{"cell_type":"code","source":"(train\n .groupby('time_id')['temp']\n .mean()\n .plot(kind='hist',\n       bins=100,\n       figsize=(20, 10)))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:02:59.489878Z","iopub.execute_input":"2022-03-26T00:02:59.490177Z","iopub.status.idle":"2022-03-26T00:03:00.042084Z","shell.execute_reply.started":"2022-03-26T00:02:59.490128Z","shell.execute_reply":"2022-03-26T00:03:00.041158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, nothing really jumps out as an obvious data problem. Let's see where the outliers are.\n\nBelow is a list of time_ids in descending order of fraction of zeros:","metadata":{}},{"cell_type":"code","source":"(train\n .groupby('time_id')['temp']\n .aggregate(['mean','count'])\n .sort_values('mean', ascending=False)[:30])","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:03:00.046013Z","iopub.execute_input":"2022-03-26T00:03:00.046312Z","iopub.status.idle":"2022-03-26T00:03:00.180032Z","shell.execute_reply.started":"2022-03-26T00:03:00.046278Z","shell.execute_reply":"2022-03-26T00:03:00.179211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's an issue with time_id==367 -- notice how the mean discontinuously jumps? However, quantitatively this isn't enough to drive the +10,000 excess zeros we see in the distribution of the target variable. I conclude that the zeros are valid data that we should train on, but also that there might be something strange going on with the time_ids, especially around time_id==367. ","metadata":{}},{"cell_type":"code","source":"del train['temp']","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:03:00.181344Z","iopub.execute_input":"2022-03-26T00:03:00.181612Z","iopub.status.idle":"2022-03-26T00:03:00.186461Z","shell.execute_reply.started":"2022-03-26T00:03:00.181578Z","shell.execute_reply":"2022-03-26T00:03:00.18561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lastly, here, let's see how the target variable was standardized. I'd guess that it's something like z-scores across `investment_id` within time_id, or maybe vice-versa. Let's see:","metadata":{}},{"cell_type":"code","source":"(train\n .groupby('investment_id')['target']\n .aggregate(['mean', 'std'])\n .plot(kind='hist', \n       bins=50, \n       figsize = (20,10), \n       title='Distribution of target\\'s mean and standard deviation within investment_id across time_id'))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:03:00.188059Z","iopub.execute_input":"2022-03-26T00:03:00.188365Z","iopub.status.idle":"2022-03-26T00:03:00.92947Z","shell.execute_reply.started":"2022-03-26T00:03:00.188333Z","shell.execute_reply":"2022-03-26T00:03:00.926809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(train\n .groupby('time_id')['target']\n .aggregate(['mean', 'std'])\n .plot(kind='hist', \n       bins=50, \n       figsize = (20,10), \n       title='Distribution of target\\'s mean and standard deviation within `time_id`across investment_id'))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:03:00.930919Z","iopub.execute_input":"2022-03-26T00:03:00.931235Z","iopub.status.idle":"2022-03-26T00:03:01.643154Z","shell.execute_reply.started":"2022-03-26T00:03:00.931198Z","shell.execute_reply":"2022-03-26T00:03:01.642179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generally it looks like it's standardized within-time_id, although not perfectly -- perhaps they enforce standard rolling means and variances over some time. Regardless I think it's pretty clear that we don't need to do much or any additional work cleaning the target variable, at least for now.","metadata":{}},{"cell_type":"markdown","source":"# Exploration of the time dimension","metadata":{}},{"cell_type":"markdown","source":"We already know that the panel isn't balanced, so let's look into that. Below we have descriptive stats of the count of time_ids across `investment_id`s -- clearly there is a wide range of time periods covered in each investment.","metadata":{}},{"cell_type":"code","source":"(train\n .groupby('investment_id')['target']\n .count()\n .describe())","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:03:01.644636Z","iopub.execute_input":"2022-03-26T00:03:01.645372Z","iopub.status.idle":"2022-03-26T00:03:01.8214Z","shell.execute_reply.started":"2022-03-26T00:03:01.645324Z","shell.execute_reply":"2022-03-26T00:03:01.820251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's get a sense for what these investments actually look like by picking 10 randomly and plotting the target for each of them over time:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfor k in np.random.choice(train['investment_id'].unique(), 10):\n    d = train[train['investment_id']==k]\n    d['target'].plot(figsize=(20, 5),\n                     title=f'investment_id {k}',\n                     style='.-',\n                     yticks=np.arange(-6, 9, 2),\n                     xticks=np.arange(0, 1300, 200),\n                     ylabel='Target')\n    plt.show()","metadata":{"_uuid":"9195fa84-3d5d-48ce-9428-4233754980a0","_cell_guid":"39b3cf50-ebdf-4150-a0c3-aa32c35f9a47","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T00:03:01.824615Z","iopub.execute_input":"2022-03-26T00:03:01.825073Z","iopub.status.idle":"2022-03-26T00:03:04.263807Z","shell.execute_reply.started":"2022-03-26T00:03:01.82502Z","shell.execute_reply":"2022-03-26T00:03:04.26317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These series look mostly like noise, although presumably with a degree of mean reversion. The main thing that stands out to me is the missing data -- there are gaps in the time series of varying lengths, sometimes long, sometimes short. \n\nBelow is a quick way to visualize time coverage in the aggregate: a scatter plot with `investment_id` on the y-axis and `time_id`on the x-axis, with each dot reflecting coverage for that (x,y) pair. ","metadata":{"_uuid":"c6e4aa72-082a-40bc-868e-128ed2fc936d","_cell_guid":"31d44b9a-8a93-48ac-b772-c5e8cfff03ce","trusted":true}},{"cell_type":"code","source":"train['time_id'] = train.index\n(train\n .plot\n .scatter(y='investment_id', \n          x='time_id', \n          figsize = (20, 20), \n          s=0.01))","metadata":{"_uuid":"78167465-b9de-4eee-bed4-668df00aa0b3","_cell_guid":"1c5527f5-b1af-42f0-81ae-a23957a15822","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T00:03:04.265002Z","iopub.execute_input":"2022-03-26T00:03:04.265795Z","iopub.status.idle":"2022-03-26T00:03:17.568947Z","shell.execute_reply.started":"2022-03-26T00:03:04.265757Z","shell.execute_reply":"2022-03-26T00:03:17.568119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The vertical white streak around `time_id`==370 is interesting -- note that this is where we saw a preponderance of 0s. I wonder if it corresponds to the market crash in early March 2020. If so, then the market dynamics before and after that vertical band are presumably quite different, and we should try training a version of the model with that early period excluded. \n\nThe tab below shows us that `time_id`is missing between 367 and 373:","metadata":{}},{"cell_type":"code","source":"(train\n .groupby(by=train.index)\n .count()['target'][350:375])","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:03:17.570083Z","iopub.execute_input":"2022-03-26T00:03:17.57092Z","iopub.status.idle":"2022-03-26T00:03:23.783585Z","shell.execute_reply.started":"2022-03-26T00:03:17.570876Z","shell.execute_reply":"2022-03-26T00:03:23.782752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One way to infer whether or not that period around `time_id`==370 is the Covid market crash (or at least some sort of unique financial event) would be to look for substantially heightened volatility around that time, because we know that the target isn't perfectly standardized within-`time_id`. Plotting the cross-sectional standard deviation in the target variable over time, we find exactly that:","metadata":{}},{"cell_type":"code","source":"(train\n .groupby(by=train.index)['target']\n .std()\n .plot(figsize=(20,10),\n       title=\"Standard deviation of the target variable across investments over time\"))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:03:23.784969Z","iopub.execute_input":"2022-03-26T00:03:23.785817Z","iopub.status.idle":"2022-03-26T00:03:24.194728Z","shell.execute_reply.started":"2022-03-26T00:03:23.785765Z","shell.execute_reply":"2022-03-26T00:03:24.194123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's unusual volatility around this period of missing data -- clearly we should run versions of our models in which we've excluded data from this time period (and potentially from before it as well) from the training set, as it's unlikely that whatever was going on then is a good guide to the evaluation period in spring 2022. \n\nWe saw earlier that the series for some of these investments don't span the entire observation window. The white horizontal streaks are missing `time_id`s -- they're all concentrated on the lefthand side of the plot, whereas the righthand side is pretty blue. I'm thinking that we should try cutting some of the investments that we only observe for a sliver of time or that go missing well before the most recent `time_id`, because they're unlikely to correlate well with future data. Let's plot the min/max `time_id`by `investment_id` to get a sense for the prevalence of this particular issue:","metadata":{}},{"cell_type":"code","source":"temp = (train\n        .groupby('investment_id')['time_id']\n        .aggregate(['min', 'max'])\n        .sort_values(['max', 'min']))\n\ntemp['y'] = np.arange(len(temp))/len(temp)*100\n\nax1 = (temp\n      .plot\n      .scatter(y='y',\n               x='max',\n               figsize = (20, 10), \n               s=0.3, \n               c='blue', \n               title='Last time_id observed (blue) versus first time_id observed (red), by investment_id', \n               xlabel='time_id',\n               ylabel='Percentile of investments'))\n\n(temp\n .plot\n .scatter(y='y',\n          x='min', \n          s=0.3,  \n          c='red', \n          ax=ax1, \n          xlabel='time_id',\n          ylabel='Percentile of investments'))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:03:24.195768Z","iopub.execute_input":"2022-03-26T00:03:24.196426Z","iopub.status.idle":"2022-03-26T00:03:24.598608Z","shell.execute_reply.started":"2022-03-26T00:03:24.196386Z","shell.execute_reply":"2022-03-26T00:03:24.596747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some observations:\n1. ~60% of the investments have full time coverage (although this says nothing of how sparse they are)\n2. Relatively few (~2%) of the investments do not have any data extending to the most recent time period -- among these, virtually all end after `time_id`==800 (i.e. it's not like they end at the very beginning of the observation window)\n3. A solid chunk (~20%) of the investments only begin to have coverage halfway through the observation window\n\nI think the principled way to deal with this is simply to drop the few investments that go missing before the end of the observation window because they're unlikely to show up in the evaluation set.\n\nLet's look at the autocorrelations of the target for a subset of investments:","metadata":{}},{"cell_type":"code","source":"sample_tickers = np.random.choice(train['investment_id'].unique(), 10)\n(train\n .query('investment_id in @sample_tickers')\n .set_index(['time_id', 'investment_id'])['target']\n .astype('float64')\n .unstack()\n .rolling(90)\n .apply(lambda x: x.autocorr(), raw=False)\n .plot(alpha=0.4,\n       title='Target autocorrelation for 10 random investments (rolling)',\n       figsize=(20,10))\n .legend(loc='lower center',\n         ncol=len(sample_tickers)));\nplt.axhline(0);","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:03:24.599718Z","iopub.execute_input":"2022-03-26T00:03:24.6003Z","iopub.status.idle":"2022-03-26T00:03:26.065778Z","shell.execute_reply.started":"2022-03-26T00:03:24.600251Z","shell.execute_reply":"2022-03-26T00:03:26.064784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's no clear pattern in the autocorrelations -- perhaps surprisingly it's more common for them to be positive than negative (I would have expected mean-reversion to be dominant here). \n\nHowever, I now think that time-series analysis is a no-go in this problem because of this line from the competition data page: \n\n> The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set.\n\nThis precludes the use of lags as predictors -- if the people running the competition reduce the gap between successive time periods, then any lags in our models will be totally misspecified. So I think our best bet is to train a model that predicts purely based on cross-sectional variation. \n\nWith that being said, let's look into the cross-sections: ","metadata":{}},{"cell_type":"markdown","source":"Drop investments that disappear before the end of the sample period and sparse investments:","metadata":{}},{"cell_type":"code","source":"train['time_max']   = (train\n                       .groupby('investment_id')['time_id']\n                       .transform('max'))\ntrain['time_count'] = (train\n                       .groupby('investment_id')['time_id']\n                       .transform('count'))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:03:26.066937Z","iopub.execute_input":"2022-03-26T00:03:26.067191Z","iopub.status.idle":"2022-03-26T00:03:26.239137Z","shell.execute_reply.started":"2022-03-26T00:03:26.067163Z","shell.execute_reply":"2022-03-26T00:03:26.238171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train[(train['time_max']>1200) & (train['time_count']>10)]","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:03:26.240529Z","iopub.execute_input":"2022-03-26T00:03:26.240763Z","iopub.status.idle":"2022-03-26T00:03:34.426977Z","shell.execute_reply.started":"2022-03-26T00:03:26.240734Z","shell.execute_reply":"2022-03-26T00:03:34.426081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train['time_id']\ndel train['time_max']\ndel train['time_count']\ndel temp","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:03:34.428637Z","iopub.execute_input":"2022-03-26T00:03:34.428971Z","iopub.status.idle":"2022-03-26T00:03:34.44108Z","shell.execute_reply.started":"2022-03-26T00:03:34.428926Z","shell.execute_reply":"2022-03-26T00:03:34.440003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What do the features look like?\n\nLet's get a sense for how the features relate to the target and to each other:\n\nAt a high level these things look very clean -- presumably they've already been standardized to mean zero and standard deviation 1 in some manner.\n\n","metadata":{"_uuid":"3e005bde-5f41-4e65-ab06-21901a17749e","_cell_guid":"33cfd85a-d1a2-4720-8639-a3cab321ed4d","trusted":true}},{"cell_type":"code","source":"# Speed things up by taking a 10% sample\nsample = train.sample(frac=.10)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:03:34.442532Z","iopub.execute_input":"2022-03-26T00:03:34.44277Z","iopub.status.idle":"2022-03-26T00:03:37.282518Z","shell.execute_reply.started":"2022-03-26T00:03:34.442741Z","shell.execute_reply":"2022-03-26T00:03:37.281627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Need to cast to float64 because the standard deviation function in groupby.aggregate() seems broken for float16 data\n(train[features]\n .astype('float64')\n .aggregate(['mean', 'std'])\n .T\n .plot(kind='hist', \n       bins=50, \n       figsize = (20,10), \n       title='Distribution of unconditional mean and standard deviation across features')\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:03:37.283939Z","iopub.execute_input":"2022-03-26T00:03:37.28421Z","iopub.status.idle":"2022-03-26T00:03:55.195128Z","shell.execute_reply.started":"2022-03-26T00:03:37.284175Z","shell.execute_reply":"2022-03-26T00:03:55.193912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = (train[features]\n .astype('float64')\n .aggregate(['mean', 'std'])\n .T)","metadata":{"_uuid":"292935c3-8284-4194-84cc-e82422bed212","_cell_guid":"0c9ad6d8-2b9e-42f0-b41a-718c16c4df13","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T00:03:55.196856Z","iopub.execute_input":"2022-03-26T00:03:55.197543Z","iopub.status.idle":"2022-03-26T00:04:08.472214Z","shell.execute_reply.started":"2022-03-26T00:03:55.197489Z","shell.execute_reply":"2022-03-26T00:04:08.470991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp.sort_values(by='mean')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:04:08.473874Z","iopub.execute_input":"2022-03-26T00:04:08.474268Z","iopub.status.idle":"2022-03-26T00:04:08.490308Z","shell.execute_reply.started":"2022-03-26T00:04:08.474221Z","shell.execute_reply":"2022-03-26T00:04:08.489318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[list(temp.sort_values(by='mean').iloc[:10].index)]","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:04:08.492Z","iopub.execute_input":"2022-03-26T00:04:08.492376Z","iopub.status.idle":"2022-03-26T00:04:08.637681Z","shell.execute_reply.started":"2022-03-26T00:04:08.492328Z","shell.execute_reply":"2022-03-26T00:04:08.636651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['f_170'].plot(kind='hist', bins=200, figsize=(20,10))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:04:08.639649Z","iopub.execute_input":"2022-03-26T00:04:08.639993Z","iopub.status.idle":"2022-03-26T00:04:10.278288Z","shell.execute_reply.started":"2022-03-26T00:04:08.639945Z","shell.execute_reply":"2022-03-26T00:04:10.276594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[list(temp.sort_values(by='mean', ascending=False).iloc[:10].index)]","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:04:10.280391Z","iopub.execute_input":"2022-03-26T00:04:10.28131Z","iopub.status.idle":"2022-03-26T00:04:10.414302Z","shell.execute_reply.started":"2022-03-26T00:04:10.281252Z","shell.execute_reply":"2022-03-26T00:04:10.413215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['f_246'].astype('float64')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:04:10.415816Z","iopub.execute_input":"2022-03-26T00:04:10.416056Z","iopub.status.idle":"2022-03-26T00:04:10.440813Z","shell.execute_reply.started":"2022-03-26T00:04:10.416026Z","shell.execute_reply":"2022-03-26T00:04:10.439798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[list(temp.sort_values(by='std').iloc[:10].index)]","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:04:10.442357Z","iopub.execute_input":"2022-03-26T00:04:10.442616Z","iopub.status.idle":"2022-03-26T00:04:10.576738Z","shell.execute_reply.started":"2022-03-26T00:04:10.442583Z","shell.execute_reply":"2022-03-26T00:04:10.575797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's sort the descriptive stats by the standard deviation:","metadata":{}},{"cell_type":"code","source":"temp.sort_values(by='std')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:04:10.577813Z","iopub.execute_input":"2022-03-26T00:04:10.578074Z","iopub.status.idle":"2022-03-26T00:04:10.592001Z","shell.execute_reply.started":"2022-03-26T00:04:10.578041Z","shell.execute_reply":"2022-03-26T00:04:10.590809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The outlier is f_124:","metadata":{}},{"cell_type":"code","source":"train['f_124'].plot(kind='hist', bins=200, figsize=(20,10))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:04:10.593853Z","iopub.execute_input":"2022-03-26T00:04:10.594232Z","iopub.status.idle":"2022-03-26T00:04:12.253643Z","shell.execute_reply.started":"2022-03-26T00:04:10.594182Z","shell.execute_reply":"2022-03-26T00:04:12.252478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['f_124'].astype('float64').describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:04:12.25525Z","iopub.execute_input":"2022-03-26T00:04:12.255507Z","iopub.status.idle":"2022-03-26T00:04:12.390518Z","shell.execute_reply.started":"2022-03-26T00:04:12.255475Z","shell.execute_reply":"2022-03-26T00:04:12.389768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['f_124'].corr(train['target'])","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:04:12.392185Z","iopub.execute_input":"2022-03-26T00:04:12.392546Z","iopub.status.idle":"2022-03-26T00:04:12.496197Z","shell.execute_reply.started":"2022-03-26T00:04:12.392487Z","shell.execute_reply":"2022-03-26T00:04:12.494963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This data is bad. We can try dropping it from the training set -- let's see whether it's the only feature that looks like this. ","metadata":{}},{"cell_type":"code","source":"train['f_170'].astype('float64').describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:04:12.50202Z","iopub.execute_input":"2022-03-26T00:04:12.502364Z","iopub.status.idle":"2022-03-26T00:04:12.616492Z","shell.execute_reply.started":"2022-03-26T00:04:12.502329Z","shell.execute_reply":"2022-03-26T00:04:12.615401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['f_182'].astype('float64').describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:04:12.618017Z","iopub.execute_input":"2022-03-26T00:04:12.618809Z","iopub.status.idle":"2022-03-26T00:04:12.727884Z","shell.execute_reply.started":"2022-03-26T00:04:12.618744Z","shell.execute_reply":"2022-03-26T00:04:12.727188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The rest of these look broadly fine -- the lower standard deviation seems to be a normal outcome of combining a categorical variable (the 0s/1s/-1s) with a continuous variable. We might want to try splitting these features into their categorical and continuous parts to make things a bit easier on our models. \n\nLet's look at the full correlation matrix. Here's the distribution of feature correlations with the target:","metadata":{}},{"cell_type":"code","source":"correlation = sample[[target] + features].corr()\n(correlation[target]\n .iloc[1:]\n .plot(kind='hist',\n       bins = 50, \n       figsize = (20,10),\n       title='Distribution of feature correlations with target'))","metadata":{"_uuid":"519ca9e1-b71b-48a0-afa6-d32a94ddf3e1","_cell_guid":"3a7686c3-0d00-41a5-9955-f05aa369e788","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T00:04:12.729444Z","iopub.execute_input":"2022-03-26T00:04:12.729705Z","iopub.status.idle":"2022-03-26T00:05:28.894512Z","shell.execute_reply.started":"2022-03-26T00:04:12.729671Z","shell.execute_reply":"2022-03-26T00:05:28.893388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's very little (linear) signal in any of these individual features.\n\nLet's see if there are any correlated clusters of the features that we could use to reduce the dimensionality of the feature space or derive new features from:","metadata":{}},{"cell_type":"code","source":"sns.clustermap(correlation, figsize=(10,10))","metadata":{"_uuid":"9eaf09c4-603d-446e-b9ab-3a2c749b85f0","_cell_guid":"3fc9ec01-c85d-4090-8e81-770cc337fd77","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T00:05:28.895997Z","iopub.execute_input":"2022-03-26T00:05:28.896563Z","iopub.status.idle":"2022-03-26T00:05:30.342385Z","shell.execute_reply.started":"2022-03-26T00:05:28.896515Z","shell.execute_reply":"2022-03-26T00:05:30.341287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At a high level, I would say that about half of the data belongs to 2-3 fairly tight clusters of features, while the other half are more or less mutually orthogonal. I'll make some hierarchical clusters because that's what's visualized here, and then try some other clustering methods at a later date. \n\n#### A note on using `investment_id` for feature engineering: \n\nI don't think we can directly use `investment_id` in the model. According to the competition description, there's no guarantee that any of the `investment_id`s in the training set will be present in the evaluation set -- that's not ideal, because you would think there would be some important information contained in the interplay between movements in different investments (e.g. when investment X and Y are both up in time $t$, then investment Z will often be down in time $t+1$). \n\nOne way to exploit some of the information contained in the `investment_id`s without overfitting to the particular set of them contained in the training data would be to train a clustering algorithm on the set of investments that seem most likely to show up in the evaluation set (e.g. the investments that always show up in the training set) and then assign any new investments we encounter to one of those clusters before computing new features within- or cross-clusters. This procedure would have a lot of assumptions baked into it, however, about the validity of the clustering, so for now I'll avoid it, instead focusing on clustering like groups of features. ","metadata":{}},{"cell_type":"markdown","source":"Below I write a function that uses UMAP to non-linearly project the feature space into two dimensions before clustering these two-dimensional representations with HDBSCAN, plotting the resulting clusters. ","metadata":{}},{"cell_type":"code","source":"def cluster(data, plot=True, tag=''):\n    reduced_dim_data = (umap\n                        .UMAP(n_neighbors=5,\n                              min_dist=0.15,\n                              n_components=2,\n                              random_state=42)\n                        .fit_transform(data))\n    clusters = (hdbscan\n                .HDBSCAN(min_samples=15,\n                         min_cluster_size=35)\n                .fit_predict(reduced_dim_data))\n    if plot:\n        clustered = (clusters >= 0)\n        \n        fig = plt.figure(figsize=(15,15))\n        ax  = plt.gca()\n\n        ax.scatter(\n            reduced_dim_data[clustered, 0],\n            reduced_dim_data[clustered, 1],\n            c=clusters[clustered]\n        )\n\n        ax.scatter(\n            reduced_dim_data[~clustered, 0],\n            reduced_dim_data[~clustered, 1],\n            c='grey',\n            alpha=0.5\n        )\n\n        ax.set_title(f'Two-dimensional UMAP projections of features, clustered with HDBSCAN \\n Unclustered features in grey \\n {tag}');\n    \n    # Return a dictionary indexed by the cluster label that contains the list of column names for each cluster\n    return {x: pd.Series(data.index, index=clusters)[x].to_list() for x in set(clusters)}","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:05:30.344031Z","iopub.execute_input":"2022-03-26T00:05:30.34436Z","iopub.status.idle":"2022-03-26T00:05:30.358131Z","shell.execute_reply.started":"2022-03-26T00:05:30.34432Z","shell.execute_reply":"2022-03-26T00:05:30.357189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's run this clustering algorithm on two samples:\n\n1. Full training data\n2. Training data with `time_id`<400 excluded (corresponding to that large temporal break in the data)","metadata":{}},{"cell_type":"code","source":"clusters_full_sample = cluster(train[features].T, tag='Sample: full time period')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:05:30.359736Z","iopub.execute_input":"2022-03-26T00:05:30.360072Z","iopub.status.idle":"2022-03-26T00:08:09.369977Z","shell.execute_reply.started":"2022-03-26T00:05:30.360027Z","shell.execute_reply":"2022-03-26T00:08:09.368829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clusters_late_sample = cluster(train.loc[400:, features].T, tag='Sample: late time period')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:08:09.371397Z","iopub.execute_input":"2022-03-26T00:08:09.371756Z","iopub.status.idle":"2022-03-26T00:09:59.393663Z","shell.execute_reply.started":"2022-03-26T00:08:09.371697Z","shell.execute_reply":"2022-03-26T00:09:59.392225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at how much overlap there is in the cluster assignments between the two samples:","metadata":{}},{"cell_type":"code","source":"len(set(clusters_late_sample[1]).intersection(clusters_full_sample[0]))/len(clusters_late_sample[1])","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:09:59.395137Z","iopub.execute_input":"2022-03-26T00:09:59.395445Z","iopub.status.idle":"2022-03-26T00:09:59.403083Z","shell.execute_reply.started":"2022-03-26T00:09:59.395382Z","shell.execute_reply":"2022-03-26T00:09:59.402258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(set(clusters_late_sample[0]).intersection(clusters_full_sample[1]))/len(clusters_late_sample[0])","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:09:59.404765Z","iopub.execute_input":"2022-03-26T00:09:59.405022Z","iopub.status.idle":"2022-03-26T00:09:59.420927Z","shell.execute_reply.started":"2022-03-26T00:09:59.40499Z","shell.execute_reply":"2022-03-26T00:09:59.420256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the clustering algorithm (which I've tuned somewhat) produces very similar clusters when run on either the full sample or the late sample (dropping the data from before the big time break), which is good evidence of its robustness to time. In the future, I'll experiment with reducing the min_cluster_size parameter to see if there's any extra signal in additional clusters, but for now I'll err on the side of caution with larger/fewer clusters. In the next notebook I'll engineer some features based on the cross-sectional distributions within- and cross-clusters. ","metadata":{}},{"cell_type":"code","source":"# Save our feature clusters\nwith open('feature_clusters.pickle', 'wb') as handle:\n    pickle.dump(clusters_late_sample, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:09:59.422524Z","iopub.execute_input":"2022-03-26T00:09:59.423124Z","iopub.status.idle":"2022-03-26T00:09:59.435021Z","shell.execute_reply.started":"2022-03-26T00:09:59.423049Z","shell.execute_reply":"2022-03-26T00:09:59.434203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-validation strategy\n\nIt's easy to rule out a couple of simple CV strategies:\n\n1. Splitting the data randomly into folds will train the model on data collected *after* the holdout set, which we definitely don't want\n2. Splitting the data by `investment_id` may introduce leakage because some of the anonymous features are calculated as aggregates of the values of other investments\n\nThis means that we must split by time, training our model on data from times $j<t_i$ and evaluating performance at time $t_i$ for $i=1,...,k$, for $k$ folds.\n\nThe competition data page tells us to \"Expect to see roughly one million rows in the test set.\" I'd estimate that the number of time_ids to be predicted is: $1,000,000 / 3,579 = ~300$, where 3,579 is the number of `investment_id`s in the training set. Therefore I'll make my holdout sets $t_i$ of size ~300. ","metadata":{"_uuid":"6ea1362f-0329-4817-a3b8-67fc35813182","_cell_guid":"ade6d899-b1b7-4dff-b63a-93ad3283e93b","_kg_hide-output":true,"trusted":true}},{"cell_type":"markdown","source":"# Iterating on models\n\nBelow I plot the importance of the various features after running two 895-estimator LGBM models (with hyperparameters tuned locally according to the above 5-fold, `time_id`-based cross-validation strategy):\n1. In the first I've thrown in the cluster-derived features in addition to the 300 original features\n2. The second has only added features derived purely from cross-sectional distributional statistics taken over the 300 original features (i.e. no clustering involved) \n\nYou can see that clustering doesn't help at all, and essentially none of the other smorgasbord of derived features help our model, so I will not pursue this kind of thing any further -- you'd expect that if the approach worked *at all*, then we'd see *some* improvement...","metadata":{}},{"cell_type":"code","source":"version = 1\nscores, models, importance = pickle.load(open(f'../input/version{version}/results{version}.pkl', 'rb'))\n(importance\n .groupby('feature')['importance']\n .aggregate(['mean','std'])\n .sort_values(by='mean')\n .plot(kind='barh',\n       y='mean',\n       xerr='std',\n       grid=True,\n       figsize=(20,80),\n       title='Feature importance (gain): mean and standard deviation across 5 folds\\n' \n       'Model includes features derived within- and across-clusters'))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:09:59.436885Z","iopub.execute_input":"2022-03-26T00:09:59.437263Z","iopub.status.idle":"2022-03-26T00:10:07.517396Z","shell.execute_reply.started":"2022-03-26T00:09:59.437214Z","shell.execute_reply":"2022-03-26T00:10:07.516589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"version = 2\nscores, models, importance = pickle.load(open(f'../input/version{version}/results{version}.pkl', 'rb'))\n(importance\n .groupby('feature')['importance']\n .aggregate(['mean','std'])\n .sort_values(by='mean')\n .plot(kind='barh',\n       y='mean',\n       xerr='std',\n       grid=True,\n       figsize=(20,80),\n       title='Feature importance (gain): mean and standard deviation across 5 folds\\n' \n       'Model includes features derived from cross-sectional distr. of original features'))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:10:07.518714Z","iopub.execute_input":"2022-03-26T00:10:07.519133Z","iopub.status.idle":"2022-03-26T00:10:14.954296Z","shell.execute_reply.started":"2022-03-26T00:10:07.51907Z","shell.execute_reply":"2022-03-26T00:10:14.953289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submissions\n\nNotes:\n* Used correlation as our early stopping rule (this is the metric Kaggle will score submissions with)\n* Used MSE as the loss function -- practically this seems to learn better than the correlation, which has a stranger-shaped gradient function\n* Found a wide range of plausible hyperparameters with a grid search, then fine-tuned with a Bayesian search\n* Fine-tuned and trained all models on a Google Cloud virtual machine, then uploaded final weights to Kaggle\n* Clustering algorithms all failed to produce any gains\n* All models perform worse when trimming the earlier time periods\n\n#### Version 1: LGBM (CV score: 0.1397, test score: 0.1356)\n\nFine-tuned LGBM trained only on the original features (no clustering)\n\n#### Version 2: DNN (score: 0.1472, test score: 1302)\n\nStill fine-tuning this -- clearly I'm overfitting somewhat. \n\n#### Version 3: 50/50 ensemble of LGBM and DNN (test score: 0.1377)","metadata":{}},{"cell_type":"markdown","source":"# Discussion\n\nI feel pretty good about the score of 0.138 -- I'd note that the leaderboard is almost entirely scores of 0.15, but I've looked at the code and these models are all way overfitted to the leaderboard data because they use `time_id` and `investment_id` extensively, e.g. directly using either as a feature or including lags or clusters of investments. I'm not *sure*, but my belief is that these models will be way off during the evaluation phase due to the increased time-frequency of observations already discussed as well as the shuffled set of `investment_id`s. \n\nThere are only two ways I think my score could be substantively improved while avoiding any of the above issues:\n1. Find a better way to cluster the features\n2. Use `investment_id` in some conservative way\n\n(1) might work if I can infer the \"speed\" of the individual features, e.g. whether they're 30-day rolling averages versus intraday volatilities, and then include relevant statistics as features in the LGBM/DNN ensemble -- this might allow for a little more separation between long-term and short-term signals. I really doubt this is going to do much, though -- surely my previous attempt at clustering features would have picked up on this somewhat, but in practice it contributed nothing. \n\n(2) might allow me to cluster some `investment_id`s that are sure to show up, so that I'm no longer throwing away signal in the correlations between investments -- e.g. right now, I don't have anything that says \"when some tech stocks go up, then the others are likely to go up as well.\" \n\nMore on (2), which seems most promising (if somewhat risky):\n\nI'm going to remake the plot from earlier in this notebook that shows the coverage of `time_id` by `investment_id`, but this time I'll plot also the share of the `time_id`s that are present in between the first and last ones. I want to see how common it is for an investment to virtually always be in the data, because then (2) could be based solely on those, which I feel comfortable doing.\n\nYou can see straightaway in the plot below that very, very few values of `investment_id` have full time-coverage. So there won't be a simple \"core\" set of investments that we can use as the bedrock for a clustering strategy. \n\nI think the most principled approach would be something like this:\n1. Pull out the, say, top 1/2 of `investment_id` by time coverage\n2. Cluster those somehow\n3. Re-run LGBM/DNN models with statistics derived from those clusters, where we've first imputed clusters to \"new\" `investment_id`s either randomly or by some rule based on the observed values of their features","metadata":{}},{"cell_type":"markdown","source":"# To-do\n\n1. Fine-tune the neural net further\n2. Implement some of the above clustering techniques\n3. Train an autoencoder to derive new features\n4. Reduce prediction variance by training final models on three different seeds and average the results","metadata":{}},{"cell_type":"code","source":"\ntemp = (train\n        .reset_index()\n        .groupby('investment_id')['time_id']\n        .aggregate(['min', 'max', 'count'])\n        .sort_values(['max', 'min', 'count']))\n\ntemp['y']     = np.arange(len(temp))/len(temp)*100\ntemp['share'] = temp['min'] + temp['count']\n\nax1 = (temp\n      .plot\n      .scatter(y='y',\n               x='max',\n               figsize = (20, 10), \n               s=0.3, \n               c='blue', \n               title='Last time_id observed (blue) versus first time_id observed (red), along with share of time_id coverage (green), by investment_id', \n               xlabel='time_id',\n               ylabel='Percentile of investments'))\n\nax2 = (temp\n      .plot\n      .scatter(y='y',\n               x='share',\n               figsize = (20, 10), \n               s=0.3, \n               c='green',\n               ax=ax1,\n               xlabel='time_id',\n               ylabel='Percentile of investments'))\n\n(temp\n .plot\n .scatter(y='y',\n          x='min', \n          s=0.3,  \n          c='red', \n          ax=ax2, \n          xlabel='time_id',\n          ylabel='Percentile of investments'))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T00:10:14.955638Z","iopub.execute_input":"2022-03-26T00:10:14.955955Z","iopub.status.idle":"2022-03-26T00:10:16.513368Z","shell.execute_reply.started":"2022-03-26T00:10:14.955907Z","shell.execute_reply":"2022-03-26T00:10:16.512654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission notebook\n\n[www.kaggle.com/danielreuter/ubiquant-model-testing](https://www.kaggle.com/danielreuter/ubiquant-model-testing)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}