{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intorduction\nThis notebook is EDA and training by using tabnet.  \nRAPIDS(cuDF) is GPU DataFrame library for loading, aggregating, filtering, and otherwise manipulating data.  (<a href='https://docs.rapids.ai/api/cudf/stable/'>Ref</a>)  <br>\n  \nver.14: cuDF dataframe is used in EDA.  \nver.18: Remove outliers (public score 0.135->0.137)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Packages","metadata":{}},{"cell_type":"code","source":"!pip -q install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\n\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom pytorch_tabnet.metrics import Metric","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:47:30.95878Z","iopub.execute_input":"2022-03-10T06:47:30.959119Z","iopub.status.idle":"2022-03-10T06:48:01.256015Z","shell.execute_reply.started":"2022-03-10T06:47:30.959018Z","shell.execute_reply":"2022-03-10T06:48:01.255271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\nimport gc\nimport glob\nimport os\nimport pickle\nimport random\n\nimport argparse\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import AutoMinorLocator\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nimport torch\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom tqdm.auto import tqdm\n","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:48:01.257751Z","iopub.execute_input":"2022-03-10T06:48:01.257994Z","iopub.status.idle":"2022-03-10T06:48:01.336903Z","shell.execute_reply.started":"2022-03-10T06:48:01.257961Z","shell.execute_reply":"2022-03-10T06:48:01.336302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parameters","metadata":{}},{"cell_type":"code","source":"# Paths\nINPUT_PATH = '../input/ubiquant-market-prediction'\nTRAIN_PICKLE = '../input/fast-read-data-ubiquant/train_reduced.pkl' # train.csv reduced memory usage\n\n# Train parematers\nDEBUG = False\n'''\nIf DEBUG==True, \nreduced sampling is performed for train data,\nand the fold calculation is stopped at the first fold.  \n'''\nargs = argparse.Namespace(\n    seed = 2022,\n    patience = 20,\n    batch_size = 1024*20, \n    virtual_batch_size = 128*20,\n    drop_last = True,\n    reduced_sampling = None if not DEBUG else 0.10, # if DEBUG, 10% samples are used.\n    max_epochs = 200 if not DEBUG else 5,\n    n_folds = 5, # if DEBUG, one fold is only calculated.\n    n_steps = 2, # equals to the number of masks\n    n_workers = 2,\n    n_bins = 16\n)\n\n# Random seed\ndef seed_everything(seed: int) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nseed_everything(args.seed)\n\n# Matplotlib style\nplt.style.use('ggplot')\nplt.rcParams['axes.grid'] = True\nplt.rcParams['grid.color'] = 'gray'\nplt.rcParams['grid.alpha'] = 0.5\nplt.rcParams['grid.linestyle'] = '--'","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:48:01.338531Z","iopub.execute_input":"2022-03-10T06:48:01.338724Z","iopub.status.idle":"2022-03-10T06:48:01.349647Z","shell.execute_reply.started":"2022-03-10T06:48:01.338699Z","shell.execute_reply":"2022-03-10T06:48:01.348975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Input data","metadata":{}},{"cell_type":"code","source":"%%time\ndf_train = pd.read_pickle(TRAIN_PICKLE)\ndisplay(df_train.head())","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:48:01.351175Z","iopub.execute_input":"2022-03-10T06:48:01.351458Z","iopub.status.idle":"2022-03-10T06:48:17.984812Z","shell.execute_reply.started":"2022-03-10T06:48:01.351423Z","shell.execute_reply":"2022-03-10T06:48:17.984127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform from pd.DataFrame to cudf.DataFrame\nimport cudf\nprint('RAPIDS version',cudf.__version__)\n\ncudf_train = cudf.DataFrame.from_pandas(df_train)\ndisplay(cudf_train.head())","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:48:17.987165Z","iopub.execute_input":"2022-03-10T06:48:17.987572Z","iopub.status.idle":"2022-03-10T06:48:41.84833Z","shell.execute_reply.started":"2022-03-10T06:48:17.987535Z","shell.execute_reply":"2022-03-10T06:48:41.847478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"class EDA():\n    \n    def __init__(self, df):\n        self.df = df\n        self.bin_color = '#66bd63'\n        \n    def base_info(self):\n        print(f'# of records: {len(self.df)}')\n        print(f'# of unique investment_id: {len(self.df[\"investment_id\"].unique())}')\n        print(f'# of unique time_id: {len(self.df[\"time_id\"].unique())}')\n        print(f'# of record with \"target==0\": {len(self.df[self.df[\"target\"]==0])}')\n        features_max = self.df.iloc[:, 4:].max()\n        features_min = self.df.iloc[:, 4:].min()\n        max_value   = features_max.max()\n        max_feature = features_max.sort_values().index[-1]\n        min_value   = features_min.min()\n        min_feature = features_min.sort_values().index[0]\n        print(f'Max feature value: {max_value:.2f} (in {max_feature})')\n        print(f'Min feature value: {min_value:.2f} (in {min_feature})')\n            \n    def plot_histgram(self, n_bins=100):\n        fig, ax = plt.subplots(1, 3, figsize=(20,4))\n        \n        ax[0].set_xlabel('target')\n        ax[0].hist(self.df['target'].to_arrow(), bins=n_bins, color=self.bin_color)\n        # .to_arrow() is requred for cudf dataframe or series\n\n        ax[1].set_xlabel('target.mean group by investment_id')\n        target_mean = self.df[['investment_id','target']]\\\n                     .groupby('investment_id', as_index=False).agg({'target':'mean'})\\\n                     ['target'].to_arrow()\n        ax[1].hist(target_mean, bins=n_bins, color=self.bin_color)\n\n        ax[2].set_xlabel('time_id.count group by investment_id')\n        time_id_count = self.df[['investment_id','time_id']]\\\n                       .groupby('investment_id', as_index=False).agg({'time_id':'count'})\\\n                       ['time_id'].to_arrow()\n        ax[2].hist(time_id_count, bins=n_bins, color=self.bin_color)\n                \n    def plot_features_mean_and_std(self):\n        fig, ax1 = plt.subplots(3, 1, figsize=(19,8)) # ax1: mean\n        feature_means = []\n        for i in range(3):\n            # Sampling data per 100 features\n            col_s = 4 + i*100\n            col_e = col_s + 100\n            labels = self.df.columns[col_s:col_e] \n            # Plot Mean\n            mean = (self.df.iloc[:,col_s:col_e]/len(self.df)).sum(axis=0)\n            ax1[i].plot(mean.to_arrow(), color='red', label='Mean')\n            ax1[i].set_xticks(np.arange(100))\n            ax1[i].set_xticklabels(labels=labels, fontsize=8, rotation=90)\n            ax1[i].tick_params(axis='y', color='red', labelcolor='red')\n            ax1[i].set_xlim(-1, 100)\n            ax1[i].set_ylim(-0.6, 0.6)\n            ax1[i].set_ylabel('Mean', color='red')\n            # Plot standard deviation\n            std = np.sqrt(((mean - self.df.iloc[:,col_s:col_e])**2/len(self.df)).sum(axis=0))\n            ax2 = ax1[i].twinx() # ax2: std\n            ax2.plot(std.to_arrow(), color='blue', label='Standard Deviation')\n            ax2.set_xticks(np.arange(100))\n            ax2.set_xticklabels(labels=labels, fontsize=8, rotation=90)\n            ax2.tick_params(axis='y', color='blue', labelcolor='blue')\n            ax2.set_xlim(-1, 100)\n            ax2.set_ylim(0, 1.2)\n            ax2.set_ylabel('Standard Deviation', color='blue')\n            # Legend\n            handle1, label1 = ax1[i].get_legend_handles_labels()\n            handle2, label2 = ax2.get_legend_handles_labels()\n            ax1[i].legend([handle2[0],handle1[0]], [label2[0],label1[0]], loc=4)\n        \n        plt.subplots_adjust(hspace=0.4)    \n        plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T06:48:41.850097Z","iopub.execute_input":"2022-03-10T06:48:41.850393Z","iopub.status.idle":"2022-03-10T06:48:41.876611Z","shell.execute_reply.started":"2022-03-10T06:48:41.850353Z","shell.execute_reply":"2022-03-10T06:48:41.875817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda = EDA(cudf_train)\neda.base_info()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:48:41.877702Z","iopub.execute_input":"2022-03-10T06:48:41.878162Z","iopub.status.idle":"2022-03-10T06:48:42.326496Z","shell.execute_reply.started":"2022-03-10T06:48:41.878122Z","shell.execute_reply":"2022-03-10T06:48:42.325728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### There are \"target==0\" data. If you use the loss function like RSMPE, the value becames inf.","metadata":{}},{"cell_type":"code","source":"eda.plot_histgram()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:48:42.327613Z","iopub.execute_input":"2022-03-10T06:48:42.328016Z","iopub.status.idle":"2022-03-10T06:48:43.31313Z","shell.execute_reply.started":"2022-03-10T06:48:42.327979Z","shell.execute_reply":"2022-03-10T06:48:43.312269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda.plot_features_mean_and_std()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:48:43.314217Z","iopub.execute_input":"2022-03-10T06:48:43.315207Z","iopub.status.idle":"2022-03-10T06:48:48.529492Z","shell.execute_reply.started":"2022-03-10T06:48:43.315166Z","shell.execute_reply":"2022-03-10T06:48:48.528182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From above figures, almost the features have mean ≃ 0.0, std ≃ 1.0 except for some features:\nSmall std  : f_124, f_170, f175, f_272  \nSmall mean : f_170, f_175  \nLarge mean : f_41, f_182, f_246  \n\n### Investigate these distributions as follows","metadata":{}},{"cell_type":"code","source":"def plot_distribution(df, features, title=None, bin_min=-100, bin_max=100, n_bins=100):\n    fig, ax = plt.subplots(1, 4, figsize=(14, 3))\n    bins = np.linspace(bin_min, bin_max, n_bins)\n    for i in range(len(features)):\n        \n        mean = (df[features[i]]/len(df)).sum(axis=0)\n        std = np.sqrt(((mean - df[features[i]])**2/len(df_train)).sum(axis=0))\n                \n        ax[i].hist(df[features[i]].to_arrow(), bins=bins, color='#66bd63', label=features[i])\n        ax[i].set_xlim(bin_min, bin_max)\n        ax[i].set_ylim(1, 10**6)\n        ax[i].set_yscale('log')\n        ax[i].text(0.99, 0.99, f'mean: {mean:.3f}',\n                   va='top', ha='right', transform=ax[i].transAxes)\n        ax[i].text(0.99, 0.89, f'std : {std:.3f}',\n                   va='top', ha='right', transform=ax[i].transAxes)\n        ax[i].set_title(features[i])\n    \n    ax[0].text(-0.5, 0.5, f'{title}:', fontsize=14,\n               va='top', ha='right', transform=ax[0].transAxes)\n    plt.show()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-03-10T06:48:48.530469Z","iopub.execute_input":"2022-03-10T06:48:48.530697Z","iopub.status.idle":"2022-03-10T06:48:48.5418Z","shell.execute_reply.started":"2022-03-10T06:48:48.530666Z","shell.execute_reply":"2022-03-10T06:48:48.541036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_features = {}\nsample_features['Reference   '] = ['f_1', 'f_2', 'f_3', 'f_4']\nsample_features['Small std   '] = ['f_124', 'f_170', 'f_175', 'f_272']\nsample_features['Small mean']   = ['f_170', 'f_175'] \nsample_features['Large mean']   = ['f_41', 'f_182', 'f_246'] \n\nfor k, v in sample_features.items():\n    plot_distribution(cudf_train, features=v, title=k, bin_min=-100, bin_max=100, n_bins=100)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:48:48.543041Z","iopub.execute_input":"2022-03-10T06:48:48.543438Z","iopub.status.idle":"2022-03-10T06:48:58.732144Z","shell.execute_reply.started":"2022-03-10T06:48:48.543401Z","shell.execute_reply":"2022-03-10T06:48:58.731421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Change the range of the histgrams between -10 and 10.¶","metadata":{}},{"cell_type":"code","source":"for k, v in sample_features.items():\n    plot_distribution(cudf_train, features=v, title=k, bin_min=-10, bin_max=10, n_bins=100)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:48:58.733489Z","iopub.execute_input":"2022-03-10T06:48:58.733919Z","iopub.status.idle":"2022-03-10T06:49:08.290864Z","shell.execute_reply.started":"2022-03-10T06:48:58.733879Z","shell.execute_reply":"2022-03-10T06:49:08.288527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From above,\n- Most of the features are not normal and some have multimodal.\n- Some singular peek exist in a distribution. Especially, f_124 is like delta functions.","metadata":{}},{"cell_type":"code","source":"del eda\ntorch.cuda.empty_cache()\n_ = gc.collect()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-03-10T06:49:08.298184Z","iopub.execute_input":"2022-03-10T06:49:08.300612Z","iopub.status.idle":"2022-03-10T06:49:08.643101Z","shell.execute_reply.started":"2022-03-10T06:49:08.300561Z","shell.execute_reply":"2022-03-10T06:49:08.642205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summary","metadata":{}},{"cell_type":"code","source":"summary = cudf_train.describe().to_pandas()\nf_xx = [f'f_{i}' for i in range(300)]  \nsummary.insert(3, 'f0-f291_mean', summary[f_xx].mean(axis='columns'))\nsummary.insert(4, 'f0-f291_std',  summary[f_xx].std(axis='columns'))\nsummary.loc['abs_mean'] = abs(summary.loc['mean'])\ndisplay(summary)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:49:08.6517Z","iopub.execute_input":"2022-03-10T06:49:08.652218Z","iopub.status.idle":"2022-03-10T06:49:15.544563Z","shell.execute_reply.started":"2022-03-10T06:49:08.652178Z","shell.execute_reply":"2022-03-10T06:49:15.543901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Sorted by abs_mean (descending)')\ndisplay(summary.sort_values(by='abs_mean', axis=1, ascending=False).\\\n        loc[['abs_mean'],:].drop(['investment_id', 'time_id'], axis=1))\n      \nprint('Sorted from large std (descending)')\ndisplay(summary.sort_values(by='std', axis=1, ascending=False).\\\n        loc[['std'],:].drop(['investment_id', 'time_id'], axis=1))","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-03-10T06:49:15.545772Z","iopub.execute_input":"2022-03-10T06:49:15.546628Z","iopub.status.idle":"2022-03-10T06:49:15.588067Z","shell.execute_reply.started":"2022-03-10T06:49:15.546584Z","shell.execute_reply":"2022-03-10T06:49:15.587388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot target-feature with large absolute\nfig, ax = plt.subplots(1, 4, figsize=(28, 7))\nfig.suptitle('Large mean (absolute values)', fontsize=24)\nfor i, f in enumerate(['f_246', 'f_175', 'f_41', 'f_182']) :\n    ax[i].scatter(cudf_train[f].to_arrow(), cudf_train['target'].to_arrow(), alpha=0.5)\n    ax[i].set_xlabel(f)\n    ax[i].set_ylabel('target')\n    ax[i].set_title(f)\nfig.tight_layout(rect=[0,0,1,0.96])\nplt.show()\n\n#Plot target-feature with large std\nfig, ax = plt.subplots(1, 4, figsize=(28, 7))\nfig.suptitle('Large  std', fontsize=24)\nfor i, f in enumerate(['f_9', 'f_294', 'f_18', 'f_176']) :\n    ax[i].scatter(cudf_train[f].to_arrow(), cudf_train['target'].to_arrow(), alpha=0.5)\n    ax[i].set_xlabel(f)\n    ax[i].set_ylabel('target')\n    ax[i].set_title(f)\nfig.tight_layout(rect=[0,0,1,0.96])\nplt.show","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T06:49:15.589295Z","iopub.execute_input":"2022-03-10T06:49:15.590106Z","iopub.status.idle":"2022-03-10T06:50:18.227551Z","shell.execute_reply.started":"2022-03-10T06:49:15.590067Z","shell.execute_reply":"2022-03-10T06:50:18.226916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Some data exist far from the center of distribution. These points will be removed as follows.","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Define outliers\nJudge the record as outlier if its features is out of mean ± 20*std   ","metadata":{}},{"cell_type":"code","source":"def get_outliers(df):\n    outliers = []\n    features = []\n    for f in f_xx :\n        mean = summary.loc['mean', f]\n        std  = summary.loc['std', f]\n        df_temp1 = cudf_train[(cudf_train[f] > mean + 50*std) | (cudf_train[f] < mean - 50*std)]\n        df_temp2 = cudf_train[(cudf_train[f] > mean + 20*std) | (cudf_train[f] < mean - 20*std)]\n\n        if len(df_temp1)> 0 : \n            outliers.extend(df_temp1.index.to_arrow().to_pylist())\n            features.extend([f for _ in range(len(df_temp1))])\n\n        elif len(df_temp2)>0 and len(df_temp2) < 20:\n            outliers.extend(df_temp2.index.to_arrow().to_pylist())\n            features.extend([f for _ in range(len(df_temp2))])\n    \n    df_outliers = pd.DataFrame({'idx': outliers, 'features': features})\n    df_outliers =  df_outliers.drop_duplicates(subset='idx')    \n    \n    return df_outliers","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-03-10T06:50:18.228806Z","iopub.execute_input":"2022-03-10T06:50:18.231474Z","iopub.status.idle":"2022-03-10T06:50:18.241037Z","shell.execute_reply.started":"2022-03-10T06:50:18.231432Z","shell.execute_reply":"2022-03-10T06:50:18.240211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_outliers = get_outliers(cudf)\nprint('Number of outliers:', len(df_outliers))\ndisplay(df_outliers.groupby('features').agg('count')\\\n        .sort_values('idx', ascending=False).head())","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:50:18.242506Z","iopub.execute_input":"2022-03-10T06:50:18.242913Z","iopub.status.idle":"2022-03-10T06:50:30.649507Z","shell.execute_reply.started":"2022-03-10T06:50:18.242875Z","shell.execute_reply":"2022-03-10T06:50:30.648795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove the outliers","metadata":{}},{"cell_type":"code","source":"cudf_train = cudf_train.drop(cudf_train.index[df_outliers['idx']])\nprint(f'len(cudf_train): {len(cudf_train)} ({len(df_outliers)} outliers are removed.)')","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:50:30.650958Z","iopub.execute_input":"2022-03-10T06:50:30.651451Z","iopub.status.idle":"2022-03-10T06:50:31.453364Z","shell.execute_reply.started":"2022-03-10T06:50:30.651414Z","shell.execute_reply":"2022-03-10T06:50:31.452617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del summary, df_outliers\ntorch.cuda.empty_cache()\n_ = gc.collect()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T06:50:31.454648Z","iopub.execute_input":"2022-03-10T06:50:31.455085Z","iopub.status.idle":"2022-03-10T06:50:31.605876Z","shell.execute_reply.started":"2022-03-10T06:50:31.455029Z","shell.execute_reply":"2022-03-10T06:50:31.604857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### KFold","metadata":{}},{"cell_type":"code","source":"def add_fold_column(df):\n    # Create 'time_span' column, which is used for stratified KFold.\n    df_time = (df.loc[:,['time_id', 'investment_id']]\n               .groupby('investment_id')\n               .agg({'time_id': ['min', 'max']})\n               .reset_index())\n    df_time['time_span'] = df_time['time_id']['max'] - df_time['time_id']['min']\n    display(df_time.head())\n    \n    # Merge 'time_span' to df\n    df_time = pd.DataFrame(df_time.to_pandas()\n                           .droplevel(level=1, axis=1)\n                           .drop('time_id' ,axis=1))\n    df_time = cudf.DataFrame.from_pandas(df_time)\n    df = df.merge(df_time, on=['investment_id'])\n   \n    # Holdout\n    _target = cudf.cut(df['time_span'], args.n_bins, labels=False)\n    _train, _valid = train_test_split(_target,\n                                      stratify=_target.values.get(),\n                                      random_state=args.seed)\n    print(f'Number of holdout records: {len(_valid)}')\n    df = df.iloc[_train.index].sort_values(by=['time_id', 'investment_id'])\\\n           .reset_index(drop=True)\n    \n    # StratifiedKFold\n    df[\"fold\"] = -1\n    _target = cudf.cut(df['time_span'], args.n_bins, labels=False) \n    skfold = StratifiedKFold(n_splits=args.n_folds)\n    for fold, (train_idx, valid_idx) in enumerate(skfold.split(_target, _target.values.get())):\n        df.loc[valid_idx, 'fold'] = fold\n       \n    return df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T06:50:31.607309Z","iopub.execute_input":"2022-03-10T06:50:31.607538Z","iopub.status.idle":"2022-03-10T06:50:31.619754Z","shell.execute_reply.started":"2022-03-10T06:50:31.607506Z","shell.execute_reply":"2022-03-10T06:50:31.619026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cudf_train = add_fold_column(cudf_train)\ndisplay(cudf_train.head())","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:50:31.621015Z","iopub.execute_input":"2022-03-10T06:50:31.621711Z","iopub.status.idle":"2022-03-10T06:50:44.235618Z","shell.execute_reply.started":"2022-03-10T06:50:31.621671Z","shell.execute_reply":"2022-03-10T06:50:44.2349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create features\nref. https://www.kaggle.com/valleyzw/ubiquant-lgbm-baseline","metadata":{}},{"cell_type":"code","source":"def create_features(df):\n    cat_features = ['investment_id']\n    num_features = [f'f_{i}' for i in range(300)] + ['time_id']\n    features = num_features + cat_features\n\n    combination_features = ['f_231-f_250', 'f_118-f_280', 'f_155-f_297','f_25-f_237',\n                            'f_179-f_265', 'f_119-f_270', 'f_71-f_197', 'f_21-f_65']\n    for f in combination_features:\n        f1, f2 = f.split('-')\n        df[f] = df[f1] + df[f2]\n    \n    features += combination_features\n    drop_features = ['f_148', 'f_72', 'f_49', 'f_205', 'f_228', 'f_97', 'f_262', 'f_258']\n    features = list(sorted(set(features).difference(set(drop_features))))\n    df = df.drop(drop_features, axis=1)\n                     \n    return df, features","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T06:50:44.236981Z","iopub.execute_input":"2022-03-10T06:50:44.237804Z","iopub.status.idle":"2022-03-10T06:50:44.245266Z","shell.execute_reply.started":"2022-03-10T06:50:44.237762Z","shell.execute_reply":"2022-03-10T06:50:44.244485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cudf_train, features = create_features(cudf_train)\ncudf_train = cudf_train.drop(['time_span'], axis=1)\nprint('len(features):', len(features))\nprint(features)\ndisplay(cudf_train.head())","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:50:44.246572Z","iopub.execute_input":"2022-03-10T06:50:44.246884Z","iopub.status.idle":"2022-03-10T06:50:53.483745Z","shell.execute_reply.started":"2022-03-10T06:50:44.246843Z","shell.execute_reply":"2022-03-10T06:50:53.482993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Metric\nclass PearsonCorrelation(Metric):\n    def __init__(self):\n        self._name = 'pearson_corr'\n        self._maximize = True\n\n    def __call__(self, x, y):\n        x = x.squeeze()\n        y = y.squeeze()\n        x_diff = x - np.mean(x)\n        y_diff = y - np.mean(y)\n        return np.dot(x_diff, y_diff)/(np.sqrt(sum(x_diff**2))*np.sqrt(sum(y_diff**2)))\n\n# Train run\ndef train(df_train, features, args, debug=DEBUG):\n    \n    # Tabnet parameters    \n    tabnet_params = dict(\n                cat_idxs = [i for i, f in enumerate(features) if f in ['investment_id']],\n                cat_emb_dim = 1,\n                n_d = 16,\n                n_a = 16,\n                n_steps = args.n_steps,\n                gamma = 2,\n                n_independent = 2,\n                n_shared = 2,\n                lambda_sparse = 0,\n                optimizer_fn = Adam,\n                optimizer_params = dict(lr = (2e-2)),\n                mask_type = 'entmax',\n                scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, \n                                        last_epoch=-1, verbose=False),\n                scheduler_fn = CosineAnnealingWarmRestarts,\n                seed = args.seed,\n                verbose = 10\n            )\n    \n    if debug: # Reduced sampling\n        print('Run as DEBUG')\n        n_samples = len(df_train)\n        sample_idx = df_train.sample(int(n_samples*args.reduced_sampling), random_state=args.seed).index\n        df_train = df_train.iloc[sample_idx].reset_index(drop=True)\n        print('len(df_train):', len(df_train))\n        del sample_idx\n        _ = gc.collect()\n    \n    ###### Outputs ######\n    histories = {}\n    oof_predictions = pd.DataFrame({'row_id': df_train['row_id'].to_pandas()}) # predictions for validatation data\n    feature_importances = pd.DataFrame({'features': features})\n#     masks = {}\n#     explain_matrices = {}\n    ####################\n\n    for fold in tqdm(range(args.n_folds)):\n\n        print(f\"\\n{'>'*15} Fold {fold} {'<'*15}\")\n        \n        # Preapare train and valid data\n        train_idx = df_train['fold']!=fold\n        valid_idx = df_train['fold']==fold\n        X_train = df_train.loc[train_idx, features].values.get()\n        X_valid = df_train.loc[valid_idx, features].values.get()\n        y_train = df_train.loc[train_idx,'target'].values.get().reshape(-1,1)\n        y_valid = df_train.loc[valid_idx,'target'].values.get().reshape(-1,1)\n        print(f'len(X_train): {len(X_train)}, len(X_valid): {len(X_valid)},')\n        \n        # Model initialize\n        model = TabNetRegressor(**tabnet_params)\n        \n        # Train\n        model.fit(\n            X_train, y_train,\n            eval_set = [(X_valid, y_valid)],\n            max_epochs = args.max_epochs,\n            patience = args.patience,\n            batch_size = args.batch_size, \n            virtual_batch_size = args.virtual_batch_size,\n            num_workers = args.n_workers,\n            drop_last = args.drop_last,\n            eval_metric = [PearsonCorrelation],\n            loss_fn = torch.nn.MSELoss() # torch.nn.L1Loss()\n        )\n\n        # Save model\n        model.save_model(f'./tabnet_fold{fold}')\n        # Scores\n        histories[f'fold{fold}'] = model.history\n        # Predict for validation data\n        oof_predictions.loc[valid_idx.values.get(), 'pred'] = model.predict(X_valid).astype('float16')\n        # Feature importances\n        feature_importances[f'importance_fold{fold}'] = model.feature_importances_\n        # Explain matrices and Masks\n#         explain_matrices_, masks_ = model.explain(X_valid)\n#         explain_matrices[f'fold{fold}'] = explain_matrices_\n#         masks[f'fold{fold}'] = masks_\n\n        del model, X_train, X_valid, y_train, y_valid\n        torch.cuda.empty_cache()\n        _ = gc.collect()\n        \n        if debug:\n            print('Debug: stop calculation at first epoch')\n            break\n        \n    outputs = dict(\n        histories = histories,\n        feature_importances = feature_importances,\n        oof_predictions = oof_predictions,\n#         masks = masks,\n#         explain_matrices = explain_matrices,\n    )\n    \n    return outputs","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:50:53.484955Z","iopub.execute_input":"2022-03-10T06:50:53.485235Z","iopub.status.idle":"2022-03-10T06:50:53.505245Z","shell.execute_reply.started":"2022-03-10T06:50:53.485204Z","shell.execute_reply":"2022-03-10T06:50:53.504045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = train(cudf_train, features, args)\nprint('outputs:')\nfor k, v in outputs.items():\n    print(f'  {k}: {type(v)}')","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:50:53.506443Z","iopub.execute_input":"2022-03-10T06:50:53.507065Z","iopub.status.idle":"2022-03-10T06:51:18.081849Z","shell.execute_reply.started":"2022-03-10T06:50:53.507015Z","shell.execute_reply":"2022-03-10T06:51:18.081023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"markdown","source":"## Score","metadata":{}},{"cell_type":"code","source":"def plot_scores(histories, metrics):\n    \n    fig, ax = plt.subplots(len(metrics), 1, figsize=(12,4*len(metrics)))\n    for i, metric in enumerate(metrics):\n        for fold, history in histories.items():\n            \n            if len(history[metric])==0:\n                print(f'{metric} is not in history. (fold{fold})')\n                return\n\n            # Set linestyle\n            if 'val' in metric:\n                linestyle = 'solid'\n            else:\n                linestyle = 'dashed'\n                \n            ax[i].plot(history[metric], label=metric + f' {fold}', linestyle=linestyle)\n            ax[i].set_xlabel('Epoch', fontsize=14)\n            ax[i].set_ylabel(metric, fontsize=14)\n            ax[i].legend()\n            \n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T06:51:18.083655Z","iopub.execute_input":"2022-03-10T06:51:18.084001Z","iopub.status.idle":"2022-03-10T06:51:18.091959Z","shell.execute_reply.started":"2022-03-10T06:51:18.083959Z","shell.execute_reply":"2022-03-10T06:51:18.090946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_scores(outputs['histories'], metrics=['loss', 'val_0_pearson_corr'])","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:51:18.093407Z","iopub.execute_input":"2022-03-10T06:51:18.093696Z","iopub.status.idle":"2022-03-10T06:51:18.446408Z","shell.execute_reply.started":"2022-03-10T06:51:18.093658Z","shell.execute_reply":"2022-03-10T06:51:18.445667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature importance (Top50)","metadata":{}},{"cell_type":"code","source":"def sort_by_importance(df):\n    df_sorted = df.copy() \n    df_sorted['mean_importance'] = df.drop('features', axis=1).mean(axis=1)\n    df_sorted.sort_values(by='mean_importance', ascending=False, inplace=True)\n    return df_sorted\n\ndef plot_feature_importance_top50(df):\n    df = sort_by_importance(df)\n    fig = plt.figure(figsize=(10,14))\n    sns.barplot(y=df['features'][:50], x=df['mean_importance'][:50], palette='autumn')\n    plt.xlabel('')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Top50 (mean by folds)')\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T06:51:18.44771Z","iopub.execute_input":"2022-03-10T06:51:18.448108Z","iopub.status.idle":"2022-03-10T06:51:18.45573Z","shell.execute_reply.started":"2022-03-10T06:51:18.448047Z","shell.execute_reply":"2022-03-10T06:51:18.454986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(sort_by_importance(outputs['feature_importances']).head())\nplot_feature_importance_top50(outputs['feature_importances'])","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:51:18.458214Z","iopub.execute_input":"2022-03-10T06:51:18.458744Z","iopub.status.idle":"2022-03-10T06:51:19.139613Z","shell.execute_reply.started":"2022-03-10T06:51:18.4587Z","shell.execute_reply":"2022-03-10T06:51:19.138924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mask","metadata":{}},{"cell_type":"code","source":"def plot_mask(masks, fold):\n    fig, ax = plt.subplots(args.n_steps, 1, figsize=(18, 3*args.n_steps))\n    for i, mask in masks.items():\n        ax[i].set_title(f'Mask{i+1} ({fold})')\n        sns.heatmap(mask[:100, :], ax=ax[i], cmap='inferno') # sampling first 100 records\n        x_ticks = sort_by_importance(outputs['feature_importances'])[:5].index # Top 10 features\n        ax[i].set_xticks(x_ticks)\n        ax[i].set_xticklabels(labels=df_train.columns[x_ticks], fontsize=7, rotation=90)\n        ax[i].xaxis.grid(True, which='minor')\n        ax[i].xaxis.set_minor_locator(AutoMinorLocator())\n        ax[i].set_ylabel('rows (first 100 records)')\n        \n    plt.subplots_adjust(hspace=0.5)    \n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T06:51:19.140869Z","iopub.execute_input":"2022-03-10T06:51:19.141761Z","iopub.status.idle":"2022-03-10T06:51:19.150238Z","shell.execute_reply.started":"2022-03-10T06:51:19.141722Z","shell.execute_reply":"2022-03-10T06:51:19.149423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for fold, masks in outputs['masks'].items():\n#     plot_mask(masks, fold)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:51:19.151532Z","iopub.execute_input":"2022-03-10T06:51:19.151977Z","iopub.status.idle":"2022-03-10T06:51:19.162786Z","shell.execute_reply.started":"2022-03-10T06:51:19.15194Z","shell.execute_reply":"2022-03-10T06:51:19.162078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explain matrix","metadata":{}},{"cell_type":"code","source":"def plot_explain_matrices(explain_matrix, fold):\n    fig, ax = plt.subplots(1, 1, figsize=(18, 2.5))\n    ax.set_title(f'Explain Matrix ({fold})')\n    sns.heatmap(explain_matrix[:100], ax=ax, cmap='inferno')\n    x_ticks = sort_by_importance(outputs['feature_importances'])[:5].index # Top 10 features\n    ax.set_xticks(x_ticks)\n    ax.set_xticklabels(labels=df_train.columns[x_ticks], fontsize=7, rotation=90)\n    ax.xaxis.grid(True, which='minor')\n    ax.xaxis.set_minor_locator(AutoMinorLocator())\n    ax.set_ylabel('rows (first 100 records)')\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T06:51:19.165136Z","iopub.execute_input":"2022-03-10T06:51:19.165385Z","iopub.status.idle":"2022-03-10T06:51:19.173481Z","shell.execute_reply.started":"2022-03-10T06:51:19.165352Z","shell.execute_reply":"2022-03-10T06:51:19.172731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for fold, explain_matrix in outputs['explain_matrices'].items():\n#     plot_explain_matrices(explain_matrix, fold)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:51:19.174752Z","iopub.execute_input":"2022-03-10T06:51:19.17545Z","iopub.status.idle":"2022-03-10T06:51:19.181754Z","shell.execute_reply.started":"2022-03-10T06:51:19.175412Z","shell.execute_reply":"2022-03-10T06:51:19.181099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## True-Prediction correlation","metadata":{}},{"cell_type":"code","source":"def plot_pred_true(pred, true):\n    fig, ax = plt.subplots(1, 1, figsize=(5,5))\n    ax.scatter(pred, true, s=2)\n    ax.plot([-10,10], [-10,10], c='gray', linestyle='dashed')\n    ax.set_xlabel('Prediction')\n    ax.set_ylabel('True target')\n    ax.set_xlim(-10,10)\n    ax.set_ylim(-10,10)\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T06:51:19.183603Z","iopub.execute_input":"2022-03-10T06:51:19.184379Z","iopub.status.idle":"2022-03-10T06:51:19.191423Z","shell.execute_reply.started":"2022-03-10T06:51:19.184342Z","shell.execute_reply":"2022-03-10T06:51:19.190586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_corr = outputs['oof_predictions'].dropna().sort_values(by='row_id').reset_index(drop=True)\ndf_corr = df_corr.merge(df_train.loc[:, ['row_id', 'target']], on='row_id', how='left')\nplot_pred_true(df_corr['pred'], df_corr['target'])","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:51:19.192513Z","iopub.execute_input":"2022-03-10T06:51:19.193241Z","iopub.status.idle":"2022-03-10T06:51:24.987673Z","shell.execute_reply.started":"2022-03-10T06:51:19.193206Z","shell.execute_reply":"2022-03-10T06:51:24.986928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"def predict(models, df_test, features):\n    # df_test['time_id'] = df_test.row_id.str.extract(r'(\\d+)_.*').astype(np.uint16) # extract time_id from row_id # remove.ver2\n    preds = []\n    for model in models:\n        pred = model.predict(df_test[features].values)\n        preds.append(pred)\n        \n    mean_pred_by_folds = np.mean(np.stack(preds), axis=0)\n    return mean_pred_by_folds\n\ndef load_trained_models():\n    \n    tabnet_params = dict(\n                cat_idxs = [i for i, f in enumerate(features) if f in ['investment_id']],\n                cat_emb_dim = 1,\n                n_d = 16,\n                n_a = 16,\n                n_steps = args.n_steps,\n                gamma = 2,\n                n_independent = 2,\n                n_shared = 2,\n                lambda_sparse = 0,\n                optimizer_fn = Adam,\n                optimizer_params = dict(lr = (2e-2)),\n                mask_type = 'entmax',\n                scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, \n                                        last_epoch=-1, verbose=False),\n                scheduler_fn = CosineAnnealingWarmRestarts,\n                seed = args.seed,\n                verbose = 10\n            )\n    \n    model_paths = glob.glob('./tabnet_fold*.zip')\n    model =  TabNetRegressor(**tabnet_params)\n    models = []\n    for model_path in model_paths:    \n        model.load_model(model_path)\n        model_ = copy.deepcopy(model)\n        models.append(model_)\n    \n    return models","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T06:51:24.989006Z","iopub.execute_input":"2022-03-10T06:51:24.989362Z","iopub.status.idle":"2022-03-10T06:51:24.999525Z","shell.execute_reply.started":"2022-03-10T06:51:24.989321Z","shell.execute_reply":"2022-03-10T06:51:24.99843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load model\nmodels = load_trained_models()\n\n# Make submission\nimport ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test()\nfor (df_test, df_submission) in iter_test:\n    # Extract 'time_id' from 'row_id'\n    df_test['time_id'] = df_test.row_id.str.extract(r'(\\d+)_.*').astype(np.uint16)\n    # Create features same as df_train\n    df_test, features = create_features(df_test)\n    df_submission['target'] = predict(models, df_test, features)\n    env.predict(df_submission) ","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:51:25.001101Z","iopub.execute_input":"2022-03-10T06:51:25.001394Z","iopub.status.idle":"2022-03-10T06:51:25.191729Z","shell.execute_reply.started":"2022-03-10T06:51:25.001353Z","shell.execute_reply":"2022-03-10T06:51:25.191038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Please upvoke, if useful for you","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}