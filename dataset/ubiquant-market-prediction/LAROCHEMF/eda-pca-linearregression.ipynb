{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ubiquant market prediction : EDA, PCA and Linear Regression\nThis is a notebook dedicated to :\n- analysis of the dataset of Ubiquant market prediction Kaggle competition,\n- PCA on a sample of the dataset\n- Linear Regression used for predictions.\n\nSeveral ideas are picked up from this kernel https://www.kaggle.com/code/bastiendelaval/analyse-oc such as correlations and PCA.","metadata":{}},{"cell_type":"markdown","source":"## Librairies","metadata":{}},{"cell_type":"code","source":"# Data Manipulation\nimport numpy as np\nimport pandas as pd\nimport random\n\n# Get files content\nimport os\nimport joblib\n\n# Data Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\n\nimport warnings\n\nwarnings.filterwarnings(action=\"ignore\")\n\n# scipy tools\nfrom scipy.stats.stats import pearsonr\n\n# sklearn tools\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (\n    mean_absolute_error,\n    mean_squared_error,\n    make_scorer,\n)\nfrom sklearn.model_selection import (\n    learning_curve,\n    cross_validate,\n    KFold,\n    TimeSeriesSplit,\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:07:02.780789Z","iopub.execute_input":"2022-04-06T13:07:02.781049Z","iopub.status.idle":"2022-04-06T13:07:02.788703Z","shell.execute_reply.started":"2022-04-06T13:07:02.781023Z","shell.execute_reply":"2022-04-06T13:07:02.78777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data importation","metadata":{}},{"cell_type":"markdown","source":"We use parquet version of the dataset thanks to this kernel https://www.kaggle.com/code/camilomx/parquet-format-quickstart.","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Import dataset\ndf = pd.read_parquet(\"../input/ubiquant-parquet-low-mem/train_low_mem.parquet\")","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:12:28.560829Z","iopub.execute_input":"2022-04-06T13:12:28.561422Z","iopub.status.idle":"2022-04-06T13:13:14.437073Z","shell.execute_reply.started":"2022-04-06T13:12:28.561387Z","shell.execute_reply":"2022-04-06T13:13:14.436182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## First look on the dataset","metadata":{}},{"cell_type":"code","source":"# Display first rows\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:13:53.473853Z","iopub.execute_input":"2022-04-06T13:13:53.474222Z","iopub.status.idle":"2022-04-06T13:13:53.520584Z","shell.execute_reply.started":"2022-04-06T13:13:53.474187Z","shell.execute_reply":"2022-04-06T13:13:53.519967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 300 features named \"f_i\" for i in (0, 300). \n\nThere is the target named \"target\".\n\nRow_id is indexed on investment_id time_id.\n\nFor each column time_id value, there are several investment_id.","metadata":{}},{"cell_type":"code","source":"# Dimension\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:13:58.047898Z","iopub.execute_input":"2022-04-06T13:13:58.048207Z","iopub.status.idle":"2022-04-06T13:13:58.053578Z","shell.execute_reply.started":"2022-04-06T13:13:58.048176Z","shell.execute_reply":"2022-04-06T13:13:58.053008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Info about data\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:14:01.372958Z","iopub.execute_input":"2022-04-06T13:14:01.373654Z","iopub.status.idle":"2022-04-06T13:14:01.407453Z","shell.execute_reply.started":"2022-04-06T13:14:01.37362Z","shell.execute_reply":"2022-04-06T13:14:01.406622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"304 columns and more than 3M of rows.\n\nColumn row_id is dtype object.","metadata":{}},{"cell_type":"code","source":"print(\"Columns of dtype uint16 : \")\nfor col in df.select_dtypes(\"uint16\"):\n    print(col)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:14:05.587081Z","iopub.execute_input":"2022-04-06T13:14:05.587728Z","iopub.status.idle":"2022-04-06T13:14:05.621057Z","shell.execute_reply.started":"2022-04-06T13:14:05.587683Z","shell.execute_reply":"2022-04-06T13:14:05.620248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data summary\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:14:08.799954Z","iopub.execute_input":"2022-04-06T13:14:08.800447Z","iopub.status.idle":"2022-04-06T13:14:41.602025Z","shell.execute_reply.started":"2022-04-06T13:14:08.800413Z","shell.execute_reply":"2022-04-06T13:14:41.601007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Features seem to have low values (< 100) and can get be negative, even for the target.\n\nAs the means are very close to zero, we can consider that features f had already been standardized.","metadata":{}},{"cell_type":"code","source":"# Check if there are missing values\ndf.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:15:14.271992Z","iopub.execute_input":"2022-04-06T13:15:14.27256Z","iopub.status.idle":"2022-04-06T13:15:16.326899Z","shell.execute_reply.started":"2022-04-06T13:15:14.272524Z","shell.execute_reply":"2022-04-06T13:15:16.326059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no missing value.","metadata":{}},{"cell_type":"markdown","source":"### Reduce memory usage of the dataset\nMany kernels use this function to reduce the memory usage of the dataset (to avoid Memory-over error). I didn't find the original kernel that introduce this.\n\nBe careful it takes a long time.\n\nEDIT : it seems that there is a lost of information, especially when we get the describe information. I am not sure if it is a good idea.\n\nI wrote a notebook about it https://www.kaggle.com/code/larochemf/ubiquant-low-memory-use-be-careful. It seems that some lines have to be changed. Nevertheless, I finally did not use this function.","metadata":{}},{"cell_type":"code","source":"%%time\n\ndef reduce_mem_usage(df):\n\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype(\"category\")\n\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n\n    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\n\n\ndf_1 = reduce_mem_usage(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data summary\ndf_1.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features analysis\n\nWe are going to analyse features, with some points taken from this kernel https://www.kaggle.com/code/jiahauc/ubiqunt-eda-linearregression\n### Investment","metadata":{}},{"cell_type":"code","source":"investments = df[\"investment_id\"].nunique()\nprint(\"Number of unique investiment_id : \", investments)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:15:23.10427Z","iopub.execute_input":"2022-04-06T13:15:23.104583Z","iopub.status.idle":"2022-04-06T13:15:23.130267Z","shell.execute_reply.started":"2022-04-06T13:15:23.10455Z","shell.execute_reply":"2022-04-06T13:15:23.129662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"investment_id\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:15:26.595715Z","iopub.execute_input":"2022-04-06T13:15:26.596249Z","iopub.status.idle":"2022-04-06T13:15:26.62018Z","shell.execute_reply.started":"2022-04-06T13:15:26.596201Z","shell.execute_reply":"2022-04-06T13:15:26.619523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that several investments have low frequency. Let's have a look at investment_id = 905.","metadata":{}},{"cell_type":"code","source":"df.loc[df[\"investment_id\"] == 905]","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:15:29.836557Z","iopub.execute_input":"2022-04-06T13:15:29.836874Z","iopub.status.idle":"2022-04-06T13:15:29.873461Z","shell.execute_reply.started":"2022-04-06T13:15:29.836839Z","shell.execute_reply":"2022-04-06T13:15:29.872466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This investment is only present at this end of the dataset.","metadata":{}},{"cell_type":"code","source":"# Let's group by investment_id and see distribution\nobs_by_investments = df.groupby([\"investment_id\"])[\"target\"].count()\n\nobs_by_investments.plot(kind=\"hist\", bins=100)\nplt.title(\"Target by investment distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:15:55.562455Z","iopub.execute_input":"2022-04-06T13:15:55.563043Z","iopub.status.idle":"2022-04-06T13:15:56.203414Z","shell.execute_reply.started":"2022-04-06T13:15:55.562998Z","shell.execute_reply":"2022-04-06T13:15:56.20267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are more targets with investment_id with high values count.","metadata":{}},{"cell_type":"code","source":"# Get mean values of the target when groupping by investment_id\nmean_targets = df.groupby([\"investment_id\"])[\"target\"].mean()\nmean_targets","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:16:00.580651Z","iopub.execute_input":"2022-04-06T13:16:00.580946Z","iopub.status.idle":"2022-04-06T13:16:00.741943Z","shell.execute_reply.started":"2022-04-06T13:16:00.580911Z","shell.execute_reply":"2022-04-06T13:16:00.741304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot these means distributions\nmean_targets.plot(kind=\"hist\", bins=100)\nplt.title(\"target mean distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:16:04.172696Z","iopub.execute_input":"2022-04-06T13:16:04.173212Z","iopub.status.idle":"2022-04-06T13:16:04.836164Z","shell.execute_reply.started":"2022-04-06T13:16:04.173163Z","shell.execute_reply":"2022-04-06T13:16:04.835286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Target mean distribution is close to normal distribution.","metadata":{}},{"cell_type":"code","source":"ax = sns.jointplot(\n    x=obs_by_investments,\n    y=mean_targets,\n    kind=\"reg\",\n    height=10,\n    joint_kws={\"line_kws\": {\"color\": \"red\"}},\n)\nax.ax_joint.set_xlabel(\"observations\")\nax.ax_joint.set_ylabel(\"mean of target\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:16:11.193128Z","iopub.execute_input":"2022-04-06T13:16:11.193445Z","iopub.status.idle":"2022-04-06T13:16:12.635372Z","shell.execute_reply.started":"2022-04-06T13:16:11.193412Z","shell.execute_reply":"2022-04-06T13:16:12.634681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Through this joint plot of observations in each investment and mean target value in each investment, it shows there is a growing trend when the observations increase. Also, the dispersion of target values is more apparent when the number of recorded investments is relatively low.","metadata":{}},{"cell_type":"markdown","source":"### time_id","metadata":{}},{"cell_type":"code","source":"timestamps = df[\"time_id\"].nunique()\nprint(\"Number of unique time_id : \", timestamps)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:16:17.710312Z","iopub.execute_input":"2022-04-06T13:16:17.710603Z","iopub.status.idle":"2022-04-06T13:16:17.733839Z","shell.execute_reply.started":"2022-04-06T13:16:17.710574Z","shell.execute_reply":"2022-04-06T13:16:17.7328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"time_id\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:16:20.830204Z","iopub.execute_input":"2022-04-06T13:16:20.830547Z","iopub.status.idle":"2022-04-06T13:16:20.85395Z","shell.execute_reply.started":"2022-04-06T13:16:20.830511Z","shell.execute_reply":"2022-04-06T13:16:20.85305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(30, 8))\ndf[\"time_id\"].value_counts().plot(kind=\"bar\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:16:24.98829Z","iopub.execute_input":"2022-04-06T13:16:24.988573Z","iopub.status.idle":"2022-04-06T13:16:50.907757Z","shell.execute_reply.started":"2022-04-06T13:16:24.988545Z","shell.execute_reply":"2022-04-06T13:16:50.906732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\n    \"Percent of time_id value_counts >= 2000 : {}%\".format(\n        round(\n            (df[\"time_id\"].value_counts() >= 2000).sum()\n            / len(df[\"time_id\"].value_counts())\n            * 100,\n            1,\n        )\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:16:57.774303Z","iopub.execute_input":"2022-04-06T13:16:57.774583Z","iopub.status.idle":"2022-04-06T13:16:57.814522Z","shell.execute_reply.started":"2022-04-06T13:16:57.774554Z","shell.execute_reply":"2022-04-06T13:16:57.813777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's plot investment_id and time_id together\ndf[[\"time_id\", \"investment_id\"]].plot(\n    kind=\"scatter\", x=\"time_id\", y=\"investment_id\", figsize=(20, 30), s=0.5\n)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:17:03.430535Z","iopub.execute_input":"2022-04-06T13:17:03.430851Z","iopub.status.idle":"2022-04-06T13:17:05.380359Z","shell.execute_reply.started":"2022-04-06T13:17:03.430813Z","shell.execute_reply":"2022-04-06T13:17:05.379562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that investment_id are more present with high time_id.","metadata":{}},{"cell_type":"code","source":"# Let's see what's happenning around 300-400 time_id.\ndf[[\"time_id\", \"investment_id\"]].plot(\n    kind=\"scatter\", x=\"time_id\", y=\"investment_id\", figsize=(20, 30), s=0.5\n)\nplt.xlim(300, 400)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:17:17.984502Z","iopub.execute_input":"2022-04-06T13:17:17.984803Z","iopub.status.idle":"2022-04-06T13:17:18.720203Z","shell.execute_reply.started":"2022-04-06T13:17:17.984769Z","shell.execute_reply":"2022-04-06T13:17:18.71935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are some missing time_id.","metadata":{}},{"cell_type":"markdown","source":"### Features f_i\nA histogram of all features is available at this kernel https://www.kaggle.com/code/mk1001/eda-f-0-299-histogram/notebook.\n\nLet's see randomly six of them with boxplot :","metadata":{}},{"cell_type":"code","source":"np.random.seed(1)\n\n# Plot randomly 6 histograms and boxplots of features f_\nfor f in np.random.choice(range(0, 300), 6):\n    \n    # Initiate plot\n    fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n    plt.suptitle(\"Distribution of feature f_{}\".format(f), size=14)\n    \n    # Target histogram\n    df[\"f_{}\".format(f)].hist(bins=50, ax=axes[0])\n\n    # Target Boxplot\n    sns.boxplot(x=\"f_{}\".format(f), data=df, ax=axes[1])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:17:43.581119Z","iopub.execute_input":"2022-04-06T13:17:43.581468Z","iopub.status.idle":"2022-04-06T13:17:48.887622Z","shell.execute_reply.started":"2022-04-06T13:17:43.581434Z","shell.execute_reply":"2022-04-06T13:17:48.886602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some features are centered in zero.\n\nSome of them get outliers as the distribution is not centered. So maybe, in the future we could consider to normalize data with a Robust Scaler in order to limit the influence of outliers.","metadata":{}},{"cell_type":"code","source":"# List of features columns\nfeatures = [f\"f_{i}\" for i in range(0, 300)]","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:18:10.235147Z","iopub.execute_input":"2022-04-06T13:18:10.235465Z","iopub.status.idle":"2022-04-06T13:18:10.240643Z","shell.execute_reply.started":"2022-04-06T13:18:10.235433Z","shell.execute_reply":"2022-04-06T13:18:10.239672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target\nLet's see the target distribution.","metadata":{}},{"cell_type":"code","source":"# Initiate plot\nf, axes = plt.subplots(2, 1, figsize=(15, 8))\n\nplt.suptitle(\"Distribution of the target\", size=14)\n\n# Target histogram\ndf[\"target\"].hist(bins=50, ax=axes[0])\n\n# Target Boxplot\nsns.boxplot(x=\"target\", data=df, ax=axes[1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:18:14.595797Z","iopub.execute_input":"2022-04-06T13:18:14.596124Z","iopub.status.idle":"2022-04-06T13:18:15.597936Z","shell.execute_reply.started":"2022-04-06T13:18:14.596089Z","shell.execute_reply":"2022-04-06T13:18:15.597115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_target = df[\"target\"].mean()\nstd_target = df[\"target\"].std()\nprint(\"Target mean value : \", mean_target)\nprint(\"Target std value : \", std_target)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:18:21.870867Z","iopub.execute_input":"2022-04-06T13:18:21.871165Z","iopub.status.idle":"2022-04-06T13:18:21.892022Z","shell.execute_reply.started":"2022-04-06T13:18:21.871132Z","shell.execute_reply":"2022-04-06T13:18:21.891345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution seems to be gaussian.\n\nLet's plot the target distribution for some investment_id :","metadata":{}},{"cell_type":"code","source":"np.random.seed(1)\n\n# Initiate counter\ni = 1\n\n# Initiate plot\nplt.figure(figsize=(15, 12))\nplt.suptitle(\"Target distribution for 6 random investment_id\", size=16)\n\n# Plot randomly 6 histograms of the target\nfor j in np.random.choice(df[\"investment_id\"].unique(), 6):\n    plt.subplot(2, 3, i)\n    df[df[\"investment_id\"] == j][\"target\"].hist(bins=50)\n    plt.title(\"Target distribution\\nfor investment_id {}\".format(j), size=14)\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:18:28.158029Z","iopub.execute_input":"2022-04-06T13:18:28.158467Z","iopub.status.idle":"2022-04-06T13:18:30.246367Z","shell.execute_reply.started":"2022-04-06T13:18:28.158438Z","shell.execute_reply":"2022-04-06T13:18:30.245477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For individual investment_id, target distribution seems to be less gaussian. Some values are high for values being at the \"tail of the distribution\" (e.g. investment_id 2441, 1337).","metadata":{}},{"cell_type":"markdown","source":"## Bidimensional analysis\n### Get a sample dataset\nLet's take a sample of the data.","metadata":{}},{"cell_type":"code","source":"sample_df = df.sample(frac=0.05, random_state=1)\nsample_df","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:18:37.760594Z","iopub.execute_input":"2022-04-06T13:18:37.761554Z","iopub.status.idle":"2022-04-06T13:18:38.815117Z","shell.execute_reply.started":"2022-04-06T13:18:37.761487Z","shell.execute_reply":"2022-04-06T13:18:38.814192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort by time_id and investment_id to get data in order \n# and reset index\nsample_df = sample_df.sort_values(\n    [\"time_id\", \"investment_id\"], ascending=[True, True]\n).reset_index(drop=True)\nsample_df","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:19:04.217901Z","iopub.execute_input":"2022-04-06T13:19:04.218245Z","iopub.status.idle":"2022-04-06T13:19:04.598502Z","shell.execute_reply.started":"2022-04-06T13:19:04.2182Z","shell.execute_reply":"2022-04-06T13:19:04.597639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataframe information\nsample_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:19:13.90028Z","iopub.execute_input":"2022-04-06T13:19:13.900596Z","iopub.status.idle":"2022-04-06T13:19:13.924581Z","shell.execute_reply.started":"2022-04-06T13:19:13.900565Z","shell.execute_reply":"2022-04-06T13:19:13.923932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the target distribution","metadata":{}},{"cell_type":"code","source":"# Initiate plot\nf, axes = plt.subplots(2, 1, figsize=(15, 8))\n\nplt.suptitle(\"Distribution of the target\", size=14)\n\n# Target histogram\nsample_df[\"target\"].hist(bins=50, ax=axes[0])\n\n# Target Boxplot\nsns.boxplot(x=\"target\", data=sample_df, ax=axes[1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:19:20.706695Z","iopub.execute_input":"2022-04-06T13:19:20.707278Z","iopub.status.idle":"2022-04-06T13:19:21.480979Z","shell.execute_reply.started":"2022-04-06T13:19:20.7072Z","shell.execute_reply":"2022-04-06T13:19:21.48015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution is close to the one in the full dataset, but we don't have outliers above 10 and less than 8 as there are in the full dataset.\n\n\n### Correlation\n#### Target vs features\nLet's see if the target is correlated to the features f_i.","metadata":{}},{"cell_type":"code","source":"correlation = sample_df[[\"target\"] + features].corr()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:19:27.272206Z","iopub.execute_input":"2022-04-06T13:19:27.27301Z","iopub.status.idle":"2022-04-06T13:20:05.658507Z","shell.execute_reply.started":"2022-04-06T13:19:27.272965Z","shell.execute_reply":"2022-04-06T13:20:05.657442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot correlation values between target and features f_i\nplt.figure(figsize=(7, 7))\ncorrelation[\"target\"].iloc[1:].hist(bins=20)\nplt.title(\"Correlation between target and features f_i\", size=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:20:18.801984Z","iopub.execute_input":"2022-04-06T13:20:18.802312Z","iopub.status.idle":"2022-04-06T13:20:19.074755Z","shell.execute_reply.started":"2022-04-06T13:20:18.802277Z","shell.execute_reply":"2022-04-06T13:20:19.073925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlation values are very low, so it means that target is not linked to features.\n#### Between features\nLet's see the correlation between the features f_i. As there are 300 features, it is difficult to see all correlations.","metadata":{}},{"cell_type":"code","source":"def mat_corr(df):\n    \"\"\"\n    Function to plot correlation matrix heatmap between columns of a dataframe\n    \n    Arguments :\n    - dataframe df\n    \n    Display :\n    - correlation matrix as heatmap\n    \"\"\"\n\n    # Compute correlation\n    corr = df.corr()\n\n    # Mask to display only lower part of the heatmap\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Plot initialization\n    f, ax = plt.subplots(figsize=(30, 30))\n\n    # Color mapping\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Heatmap\n    sns.heatmap(\n        corr,\n        mask=mask,\n        cmap=cmap,\n        # vmax=1,\n        center=0,\n        square=True,\n        linewidths=0.5,\n        cbar=True,\n        # annot=True, # do not display correlation values\n    )\n    plt.title(\"Correlation Matrix\", size=20)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:20:25.652723Z","iopub.execute_input":"2022-04-06T13:20:25.653028Z","iopub.status.idle":"2022-04-06T13:20:25.66134Z","shell.execute_reply.started":"2022-04-06T13:20:25.652995Z","shell.execute_reply":"2022-04-06T13:20:25.660293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display heatmap\nmat_corr(sample_df[features])","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:20:36.136286Z","iopub.execute_input":"2022-04-06T13:20:36.136646Z","iopub.status.idle":"2022-04-06T13:21:19.519188Z","shell.execute_reply.started":"2022-04-06T13:20:36.13661Z","shell.execute_reply":"2022-04-06T13:21:19.51847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of correlations are low. \n\nWe are going to see the highest correlations. Generally, it is considered that high correlation is above 0.8.","metadata":{}},{"cell_type":"code","source":"# Compute correlation matrix with absolute values\ncorr_matrix = sample_df[features].corr().abs()\n\n# Keep high correlations\nhigh_corr_var = np.where(corr_matrix >= 0.80)\n\n# Get pairs of features with high correlations\nhigh_corr_var = [\n    (corr_matrix.columns[x], corr_matrix.columns[y])\n    for x, y in zip(*high_corr_var)\n    if x != y and x < y\n]\nhigh_corr_var","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:21:28.690832Z","iopub.execute_input":"2022-04-06T13:21:28.692738Z","iopub.status.idle":"2022-04-06T13:22:06.865899Z","shell.execute_reply.started":"2022-04-06T13:21:28.692679Z","shell.execute_reply":"2022-04-06T13:22:06.865042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that several features are correlated to more than one feature, such as f_4, f_228, f_41, f_95, f_97...","metadata":{}},{"cell_type":"code","source":"# Select the lower triangle of the correlation matrix\nlower = corr_matrix.where(np.tril(np.ones(corr_matrix.shape), k=-1).astype(np.bool))\n# k = -1 to remove values on diagonal\nlower","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:22:11.60662Z","iopub.execute_input":"2022-04-06T13:22:11.607151Z","iopub.status.idle":"2022-04-06T13:22:11.641403Z","shell.execute_reply.started":"2022-04-06T13:22:11.607108Z","shell.execute_reply":"2022-04-06T13:22:11.640803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the upper triangle of the correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# k = 1 to remove values on diagonal\nupper","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:22:21.946598Z","iopub.execute_input":"2022-04-06T13:22:21.947711Z","iopub.status.idle":"2022-04-06T13:22:21.984925Z","shell.execute_reply.started":"2022-04-06T13:22:21.947666Z","shell.execute_reply":"2022-04-06T13:22:21.984298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find features with correlation greater than 0.80 in lower matrix\nto_drop_low = [column for column in lower.columns if any(lower[column] >= 0.8)]\nprint(\"{} features with high correlation (>=0.8)\".format(len(to_drop_low)))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:22:26.70183Z","iopub.execute_input":"2022-04-06T13:22:26.702101Z","iopub.status.idle":"2022-04-06T13:22:26.763311Z","shell.execute_reply.started":"2022-04-06T13:22:26.702073Z","shell.execute_reply":"2022-04-06T13:22:26.762657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find features with correlation greater than 0.80 in upper matrix\nto_drop_up = [column for column in upper.columns if any(upper[column] >= 0.8)]\nprint(\"{} features with high correlation (>=0.8)\".format(len(to_drop_up)))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:22:30.088884Z","iopub.execute_input":"2022-04-06T13:22:30.089766Z","iopub.status.idle":"2022-04-06T13:22:30.152286Z","shell.execute_reply.started":"2022-04-06T13:22:30.089726Z","shell.execute_reply":"2022-04-06T13:22:30.15138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see which features are both in drop lists\nfeat_common = [f for f in to_drop_low if f in to_drop_up]\nfeat_common","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:22:33.816165Z","iopub.execute_input":"2022-04-06T13:22:33.816667Z","iopub.status.idle":"2022-04-06T13:22:33.823418Z","shell.execute_reply.started":"2022-04-06T13:22:33.81663Z","shell.execute_reply":"2022-04-06T13:22:33.822406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 5 features in common for the drop lists. Otherwise, 15 features are different considering upper or lower part of the matrix. Maybe this could have an incidence for the modelisation. \n\nLet's have a look of their distribution.","metadata":{}},{"cell_type":"code","source":"# Plot histograms and boxplot of these features f_\nfor f in feat_common:\n    \n    # Initiate plot\n    fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n    plt.suptitle(\"Distribution of feature {}\".format(f), size=14)\n    \n    # Target histogram\n    sample_df[\"{}\".format(f)].hist(bins=50, ax=axes[0])\n\n    # Target Boxplot\n    sns.boxplot(x=\"{}\".format(f), data=sample_df, ax=axes[1])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:22:40.348069Z","iopub.execute_input":"2022-04-06T13:22:40.349014Z","iopub.status.idle":"2022-04-06T13:22:43.044104Z","shell.execute_reply.started":"2022-04-06T13:22:40.348973Z","shell.execute_reply":"2022-04-06T13:22:43.043409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution are not all centered in zero. Many outliers.\n\nSo we are going to compare 2 possibilities : upper matrix and lower matrix.","metadata":{}},{"cell_type":"code","source":"# Drop these features\nsample_df_up = sample_df.drop(to_drop_up, axis=1)\nsample_df_low = sample_df.drop(to_drop_low, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:22:49.333286Z","iopub.execute_input":"2022-04-06T13:22:49.333591Z","iopub.status.idle":"2022-04-06T13:22:49.473276Z","shell.execute_reply.started":"2022-04-06T13:22:49.333557Z","shell.execute_reply":"2022-04-06T13:22:49.472355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"sample_df_up shape : \", sample_df_up.shape)\nprint(\"sample_df_low shape : \", sample_df_low.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:22:52.055388Z","iopub.execute_input":"2022-04-06T13:22:52.055998Z","iopub.status.idle":"2022-04-06T13:22:52.062887Z","shell.execute_reply.started":"2022-04-06T13:22:52.055939Z","shell.execute_reply":"2022-04-06T13:22:52.061868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove others columns that are not \"features\"\nothers = [\"row_id\", \"time_id\", \"investment_id\", \"target\"]\n\nfeatures_up = list(sample_df_up.columns)\nfeatures_low = list(sample_df_low.columns)\n\nfor x in others:\n    features_up.remove(x)\n    features_low.remove(x)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:22:59.323369Z","iopub.execute_input":"2022-04-06T13:22:59.323951Z","iopub.status.idle":"2022-04-06T13:22:59.33006Z","shell.execute_reply.started":"2022-04-06T13:22:59.323901Z","shell.execute_reply":"2022-04-06T13:22:59.329304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(features_up)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:23:03.221902Z","iopub.execute_input":"2022-04-06T13:23:03.222424Z","iopub.status.idle":"2022-04-06T13:23:03.228799Z","shell.execute_reply.started":"2022-04-06T13:23:03.222374Z","shell.execute_reply":"2022-04-06T13:23:03.227976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(features_low)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:23:05.678568Z","iopub.execute_input":"2022-04-06T13:23:05.679119Z","iopub.status.idle":"2022-04-06T13:23:05.685917Z","shell.execute_reply.started":"2022-04-06T13:23:05.679085Z","shell.execute_reply":"2022-04-06T13:23:05.685089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlated features have been removed.","metadata":{}},{"cell_type":"code","source":"# Let's have a look at correlation matrix\nmat_corr(sample_df[features_up])","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:23:10.116855Z","iopub.execute_input":"2022-04-06T13:23:10.117768Z","iopub.status.idle":"2022-04-06T13:23:48.249738Z","shell.execute_reply.started":"2022-04-06T13:23:10.117724Z","shell.execute_reply":"2022-04-06T13:23:48.249076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It remains some correlation above 0.6 in absolute value.","metadata":{}},{"cell_type":"markdown","source":"## Split data\nWe are going to split data now in order that the test part is not influenced by operations done on the train part.","metadata":{}},{"cell_type":"code","source":"# Define X and y\nX = sample_df[features].values\nX_up = sample_df_up.drop(others, axis=1).values\nX_low = sample_df_low.drop(others, axis=1).values\ny = sample_df_up[\"target\"].values","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:23:58.868321Z","iopub.execute_input":"2022-04-06T13:23:58.868885Z","iopub.status.idle":"2022-04-06T13:23:59.068822Z","shell.execute_reply.started":"2022-04-06T13:23:58.868849Z","shell.execute_reply":"2022-04-06T13:23:59.067834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"X shape : \", X.shape)\nprint(\"X_up shape : \", X_up.shape)\nprint(\"X_low shape : \", X_low.shape)\nprint(\"y shape : \", y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:24:03.223564Z","iopub.execute_input":"2022-04-06T13:24:03.223883Z","iopub.status.idle":"2022-04-06T13:24:03.230936Z","shell.execute_reply.started":"2022-04-06T13:24:03.223848Z","shell.execute_reply":"2022-04-06T13:24:03.230057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The test part has to be the end of the dataset as it is \"the future\" observations (remember that our data are ordered by time_id).","metadata":{}},{"cell_type":"code","source":"# Split data\nX_train = X[:140000]\nX_test = X[140000:]\n\nX_up_train = X_up[:140000]\nX_up_test = X_up[140000:]\n\nX_low_train = X_low[:140000]\nX_low_test = X_low[140000:]\n\ny_train = y[:140000]\ny_test = y[140000:]\n\nprint(\"X_train shape : \", X_train.shape)\nprint(\"X_test shape : \", X_test.shape)\nprint(\"X_up_train shape : \", X_up_train.shape)\nprint(\"X_up_test shape : \", X_up_test.shape)\nprint(\"X_low_train shape : \", X_low_train.shape)\nprint(\"X_low_test shape : \", X_low_test.shape)\nprint(\"y_train shape : \", y_train.shape)\nprint(\"y_test shape : \", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:24:07.910875Z","iopub.execute_input":"2022-04-06T13:24:07.911381Z","iopub.status.idle":"2022-04-06T13:24:07.923249Z","shell.execute_reply.started":"2022-04-06T13:24:07.911347Z","shell.execute_reply":"2022-04-06T13:24:07.922085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"perc_test = round(len(X_up_test) / len(X_up) * 100, 1)\nprint(\"Percent of data in test set : {}%\".format(perc_test))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:24:13.292884Z","iopub.execute_input":"2022-04-06T13:24:13.29343Z","iopub.status.idle":"2022-04-06T13:24:13.299067Z","shell.execute_reply.started":"2022-04-06T13:24:13.293381Z","shell.execute_reply":"2022-04-06T13:24:13.298075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see information about the first line of testset\nsample_df.loc[140000]","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:24:26.163281Z","iopub.execute_input":"2022-04-06T13:24:26.164174Z","iopub.status.idle":"2022-04-06T13:24:26.171875Z","shell.execute_reply.started":"2022-04-06T13:24:26.164131Z","shell.execute_reply":"2022-04-06T13:24:26.171287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test set contains data with time_id above 1116.","metadata":{}},{"cell_type":"code","source":"sample_df.loc[139999]","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:24:30.902147Z","iopub.execute_input":"2022-04-06T13:24:30.902601Z","iopub.status.idle":"2022-04-06T13:24:30.91194Z","shell.execute_reply.started":"2022-04-06T13:24:30.902571Z","shell.execute_reply":"2022-04-06T13:24:30.910961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_df.loc[140001]","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:24:34.14833Z","iopub.execute_input":"2022-04-06T13:24:34.149251Z","iopub.status.idle":"2022-04-06T13:24:34.157108Z","shell.execute_reply.started":"2022-04-06T13:24:34.149178Z","shell.execute_reply":"2022-04-06T13:24:34.156457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing\nWe are going to consider PCA in order to decrease the number of features.","metadata":{}},{"cell_type":"markdown","source":"We are going to compare normalized data and unnormalized data. As mentioned above, we are going to use Robust Scaler for normalization.\n\n### Scale","metadata":{}},{"cell_type":"code","source":"# X\nrobust_scal = RobustScaler().fit(X_train)\nX_scaled = robust_scal.transform(X_train)\nX_scaled.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:24:49.582248Z","iopub.execute_input":"2022-04-06T13:24:49.582972Z","iopub.status.idle":"2022-04-06T13:24:51.387875Z","shell.execute_reply.started":"2022-04-06T13:24:49.582926Z","shell.execute_reply":"2022-04-06T13:24:51.386958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Up\nrobust_scal_up = RobustScaler().fit(X_up_train)\nX_up_scaled = robust_scal_up.transform(X_up_train)\nX_up_scaled.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:24:54.378488Z","iopub.execute_input":"2022-04-06T13:24:54.378784Z","iopub.status.idle":"2022-04-06T13:24:55.987159Z","shell.execute_reply.started":"2022-04-06T13:24:54.378755Z","shell.execute_reply":"2022-04-06T13:24:55.986301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Low\nrobust_scal_low = RobustScaler().fit(X_low_train)\nX_low_scaled = robust_scal_low.transform(X_low_train)\nX_low_scaled.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:24:59.786458Z","iopub.execute_input":"2022-04-06T13:24:59.786768Z","iopub.status.idle":"2022-04-06T13:25:01.345888Z","shell.execute_reply.started":"2022-04-06T13:24:59.786734Z","shell.execute_reply":"2022-04-06T13:25:01.345005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PCA","metadata":{}},{"cell_type":"code","source":"# PCA X\npca = PCA(random_state=0)\npca.fit(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:25:05.070797Z","iopub.execute_input":"2022-04-06T13:25:05.071465Z","iopub.status.idle":"2022-04-06T13:25:07.453603Z","shell.execute_reply.started":"2022-04-06T13:25:05.071422Z","shell.execute_reply":"2022-04-06T13:25:07.452843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_scree_plot(pca, data):\n\n    \"\"\" Function to display eigenvalues scree of pca\n        \n    - Arguments :\n        - pca : pca model fitted\n        - data : data on which PCA has been fitted (string)\n    \n    - Display :\n        - barplot for each pca component\n        - cumulated inertie percent (variance explained by pca) \n    \"\"\"\n    \n    # Initiate plot\n    plt.figure(figsize=(12, 8))\n    \n    # Get explained_variance_ratio_\n    scree = pca.explained_variance_ratio_ * 100\n\n    # Barplot for each component\n    plt.bar(np.arange(len(scree)) + 1, scree)\n\n    # Cumulative sum\n    plt.plot(np.arange(len(scree)) + 1, scree.cumsum(), c=\"red\", marker=\"o\")\n\n    plt.xlabel(\"rank of the axis of inertia\", size=13)\n    plt.ylabel(\"Inertie percent\", size=13)\n    plt.title(\"Eigenvalues scree of pca for {}\".format(data), size=14)\n    plt.show(block=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:25:13.124795Z","iopub.execute_input":"2022-04-06T13:25:13.125104Z","iopub.status.idle":"2022-04-06T13:25:13.134177Z","shell.execute_reply.started":"2022-04-06T13:25:13.125075Z","shell.execute_reply":"2022-04-06T13:25:13.132989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = \"X\"\ndisplay_scree_plot(pca, data)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:25:18.244576Z","iopub.execute_input":"2022-04-06T13:25:18.244855Z","iopub.status.idle":"2022-04-06T13:25:19.076734Z","shell.execute_reply.started":"2022-04-06T13:25:18.244827Z","shell.execute_reply":"2022-04-06T13:25:19.075652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA 0.85 X\npca_85 = PCA(n_components=0.85, random_state=0)\npca_85.fit(X_train)\nX_pca85 = pca_85.transform(X_train)\nX_pca85.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:25:23.737741Z","iopub.execute_input":"2022-04-06T13:25:23.738131Z","iopub.status.idle":"2022-04-06T13:25:26.356946Z","shell.execute_reply.started":"2022-04-06T13:25:23.738083Z","shell.execute_reply":"2022-04-06T13:25:26.355835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The features number decreases of 58%.","metadata":{}},{"cell_type":"code","source":"# PCA X_up\npca_up = PCA(random_state=0)\npca_up.fit(X_up_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:25:30.419823Z","iopub.execute_input":"2022-04-06T13:25:30.42017Z","iopub.status.idle":"2022-04-06T13:25:32.668279Z","shell.execute_reply.started":"2022-04-06T13:25:30.420133Z","shell.execute_reply":"2022-04-06T13:25:32.667343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = \"X_up\"\ndisplay_scree_plot(pca_up, data)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:25:35.889854Z","iopub.execute_input":"2022-04-06T13:25:35.890145Z","iopub.status.idle":"2022-04-06T13:25:36.641443Z","shell.execute_reply.started":"2022-04-06T13:25:35.89011Z","shell.execute_reply":"2022-04-06T13:25:36.640526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's keep 85% of explained variance.","metadata":{}},{"cell_type":"code","source":"# PCA 0.85 X_up\npca_up_85 = PCA(n_components=0.85, random_state=0)\npca_up_85.fit(X_up_train)\nX_up_pca85 = pca_up_85.transform(X_up_train)\nX_up_pca85.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:25:41.462363Z","iopub.execute_input":"2022-04-06T13:25:41.462687Z","iopub.status.idle":"2022-04-06T13:25:43.909084Z","shell.execute_reply.started":"2022-04-06T13:25:41.462646Z","shell.execute_reply":"2022-04-06T13:25:43.908095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The features number decreases of 55%.","metadata":{}},{"cell_type":"code","source":"# PCA X_low\npca_low = PCA(random_state=0)\npca_low.fit(X_low_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:25:52.654463Z","iopub.execute_input":"2022-04-06T13:25:52.655017Z","iopub.status.idle":"2022-04-06T13:25:54.842616Z","shell.execute_reply.started":"2022-04-06T13:25:52.654981Z","shell.execute_reply":"2022-04-06T13:25:54.841657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = \"X_low\"\ndisplay_scree_plot(pca_low, data)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:25:58.031256Z","iopub.execute_input":"2022-04-06T13:25:58.031572Z","iopub.status.idle":"2022-04-06T13:25:58.779963Z","shell.execute_reply.started":"2022-04-06T13:25:58.031536Z","shell.execute_reply":"2022-04-06T13:25:58.779103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA 0.85 X_low\npca_low_85 = PCA(n_components=0.85, random_state=0)\npca_low_85.fit(X_low_train)\nX_low_pca85 = pca_low_85.transform(X_low_train)\nX_low_pca85.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:26:03.19946Z","iopub.execute_input":"2022-04-06T13:26:03.199844Z","iopub.status.idle":"2022-04-06T13:26:05.648056Z","shell.execute_reply.started":"2022-04-06T13:26:03.199804Z","shell.execute_reply":"2022-04-06T13:26:05.647051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The features number decreases of 55.4%.","metadata":{}},{"cell_type":"code","source":"# PCA X_scaled\npca_scal = PCA(random_state=0)\npca_scal.fit(X_scaled)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:26:09.078652Z","iopub.execute_input":"2022-04-06T13:26:09.079256Z","iopub.status.idle":"2022-04-06T13:26:12.078031Z","shell.execute_reply.started":"2022-04-06T13:26:09.079189Z","shell.execute_reply":"2022-04-06T13:26:12.077152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = \"X_scaled\"\ndisplay_scree_plot(pca_scal, data)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:26:15.965474Z","iopub.execute_input":"2022-04-06T13:26:15.965774Z","iopub.status.idle":"2022-04-06T13:26:16.978847Z","shell.execute_reply.started":"2022-04-06T13:26:15.965739Z","shell.execute_reply":"2022-04-06T13:26:16.977897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA 0.85 X_scaled\npca_scal_85 = PCA(n_components=0.85, random_state=0)\npca_scal_85.fit(X_scaled)\nX_scal_pca85 = pca_scal_85.transform(X_scaled)\nX_scal_pca85.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:26:21.288897Z","iopub.execute_input":"2022-04-06T13:26:21.289173Z","iopub.status.idle":"2022-04-06T13:26:23.721815Z","shell.execute_reply.started":"2022-04-06T13:26:21.289144Z","shell.execute_reply":"2022-04-06T13:26:23.720756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The features number decreases of 96.7% !","metadata":{}},{"cell_type":"code","source":"# PCA X_up_scaled\npca_up_scal = PCA(random_state=0)\npca_up_scal.fit(X_up_scaled)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:26:27.497167Z","iopub.execute_input":"2022-04-06T13:26:27.49779Z","iopub.status.idle":"2022-04-06T13:26:29.692646Z","shell.execute_reply.started":"2022-04-06T13:26:27.497739Z","shell.execute_reply":"2022-04-06T13:26:29.691764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = \"X_up_scaled\"\ndisplay_scree_plot(pca_up_scal, data)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:26:33.19404Z","iopub.execute_input":"2022-04-06T13:26:33.194766Z","iopub.status.idle":"2022-04-06T13:26:33.941877Z","shell.execute_reply.started":"2022-04-06T13:26:33.194726Z","shell.execute_reply":"2022-04-06T13:26:33.941098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA 0.85 X_up_scaled\npca_up_scal_85 = PCA(n_components=0.85, random_state=0)\npca_up_scal_85.fit(X_up_scaled)\nX_up_scal_pca85 = pca_up_scal_85.transform(X_up_scaled)\nX_up_scal_pca85.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:26:38.19479Z","iopub.execute_input":"2022-04-06T13:26:38.195245Z","iopub.status.idle":"2022-04-06T13:26:40.579835Z","shell.execute_reply.started":"2022-04-06T13:26:38.195198Z","shell.execute_reply":"2022-04-06T13:26:40.578581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The features number decreases of 96.8% !","metadata":{}},{"cell_type":"code","source":"# PCA X_low_scaled\npca_low_scal = PCA(random_state=0)\npca_low_scal.fit(X_low_scaled)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:26:44.586044Z","iopub.execute_input":"2022-04-06T13:26:44.586684Z","iopub.status.idle":"2022-04-06T13:26:46.83112Z","shell.execute_reply.started":"2022-04-06T13:26:44.58664Z","shell.execute_reply":"2022-04-06T13:26:46.830455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = \"X_low_scaled\"\ndisplay_scree_plot(pca_low_scal, data)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:26:49.538366Z","iopub.execute_input":"2022-04-06T13:26:49.538917Z","iopub.status.idle":"2022-04-06T13:26:50.296317Z","shell.execute_reply.started":"2022-04-06T13:26:49.538869Z","shell.execute_reply":"2022-04-06T13:26:50.295575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA 0.85 X_low_scaled\npca_low_scal_85 = PCA(n_components=0.85, random_state=0)\npca_low_scal_85.fit(X_low_scaled)\nX_low_scal_pca85 = pca_low_scal_85.transform(X_low_scaled)\nX_low_scal_pca85.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:26:54.959922Z","iopub.execute_input":"2022-04-06T13:26:54.960353Z","iopub.status.idle":"2022-04-06T13:26:57.295796Z","shell.execute_reply.started":"2022-04-06T13:26:54.96032Z","shell.execute_reply":"2022-04-06T13:26:57.294969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The features number decreases of 96.8% !","metadata":{}},{"cell_type":"markdown","source":"## Linear Regression\n### 1st Try \nWe are going to see on a simple linear regression how models perform with our different data (PCA, up, down, scaled..).","metadata":{}},{"cell_type":"code","source":"def display_learning_curve(model, X_train, y_train, name_model, name_X):\n\n    \"\"\" Function to display learning curve for a model\n    \n    - Arguments : \n        - model : model to train\n        - X_train : data to fit\n        - y_train : data to compare\n        - name_model : name of the model (string)\n        - name_X : name of X_data (string)\n    \n    - Display :\n        - Learning curve with training score and validation score\n    \n    \"\"\"\n    N, train_score, val_score = learning_curve(\n        model,\n        X_train,\n        y_train,\n        cv=n_folds,\n        scoring=\"neg_root_mean_squared_error\",\n        train_sizes=np.linspace(0.1, 1, 10),\n    )\n\n    # Plot learning-curve\n    plt.figure(figsize=(6, 6))\n    plt.plot(N, -train_score.mean(axis=1), label=\"train_score\")\n    plt.plot(N, -val_score.mean(axis=1), label=\"validation_score\")\n    plt.xlabel(\"Dataset size\", size=14)\n    plt.ylabel(\"Mean RMSE\", size=14)\n    # plt.xlim([50,680])\n    # plt.ylim([y_min, y_max])\n\n    plt.title(\"Learning curve for {} with {}\".format(name_model, name_X), size=14)\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:27:24.822917Z","iopub.execute_input":"2022-04-06T13:27:24.823248Z","iopub.status.idle":"2022-04-06T13:27:24.833077Z","shell.execute_reply.started":"2022-04-06T13:27:24.823202Z","shell.execute_reply":"2022-04-06T13:27:24.832051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_scorer(X, y):\n    \"\"\"\n    Function to get Pearson correlation coefficient between X and y\n    \n    Arguments :\n        - X\n        _ y\n    \n    Returns :\n        - Pearson correlation coefficient computed with \n        scipy.stats module\n    \"\"\"\n    pearson = pearsonr(X, y)[0]\n    return pearson\n\n# Let's transform my_scorer has a scorer\nmy_pearson = make_scorer(my_scorer, greater_is_better=True)\n\n# Dictionnay of scores\nscoring = {\n    \"neg_root_mean_squared_error\": \"neg_root_mean_squared_error\",\n    \"neg_mean_absolute_error\": \"neg_mean_absolute_error\",\n    \"my_pearson\": my_pearson,\n}","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:27:29.316402Z","iopub.execute_input":"2022-04-06T13:27:29.316838Z","iopub.status.idle":"2022-04-06T13:27:29.32183Z","shell.execute_reply.started":"2022-04-06T13:27:29.316808Z","shell.execute_reply":"2022-04-06T13:27:29.321153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cross_val(model, X_train, y_train, name_model, name_X):\n\n    \"\"\" Function to do cross-validation on a model and get scores in\n    a dataframe \n        \n    - Arguments :\n        - model : model to test\n        - X_train : X data \n        - y_train : X data\n        - name_model : name of the model (string)\n        - name_X : name given to the X data (string)\n    \n    - Return :\n        - dataframe with name_model, name_X and scoring : RMSE, MAE, R2\n    \"\"\"\n\n    # Cross validation\n    scores = cross_validate(model, X_train, y_train, cv=n_folds, scoring=scoring,)\n\n    # Get mean scores\n    RMSE = -scores[\"test_neg_root_mean_squared_error\"].mean()\n    MAE = -scores[\"test_neg_mean_absolute_error\"].mean()\n    pearson = scores[\"test_my_pearson\"].mean()\n\n    # Dataframe creation for results\n    df_model = pd.DataFrame(\n        [[name_model, name_X, RMSE, MAE, pearson]],\n        columns=[\"model\", \"X_data\", \"RMSE\", \"MAE\", \"Pearson_coef\"],\n    )\n\n    return df_model","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:27:34.31972Z","iopub.execute_input":"2022-04-06T13:27:34.320142Z","iopub.status.idle":"2022-04-06T13:27:34.327547Z","shell.execute_reply.started":"2022-04-06T13:27:34.320112Z","shell.execute_reply":"2022-04-06T13:27:34.326552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dico of X_data\ndico_X = {\n    \"X_train\": X_train,\n    \"X_scaled\": X_scaled,\n    \"X_pca85\": X_pca85,\n    \"X_scal_pca85\": X_scal_pca85,\n    \"X_up_pca85\": X_up_pca85,\n    \"X_up_scal_pca85\": X_up_scal_pca85,\n    \"X_low_pca85\": X_low_pca85,\n    \"X_low_scal_pca85\": X_low_scal_pca85,\n}","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:27:48.165316Z","iopub.execute_input":"2022-04-06T13:27:48.165635Z","iopub.status.idle":"2022-04-06T13:27:48.172531Z","shell.execute_reply.started":"2022-04-06T13:27:48.1656Z","shell.execute_reply":"2022-04-06T13:27:48.171897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel = LinearRegression()\nn_folds = 5\n\n# Dataframe for results\nlr_results = pd.DataFrame(columns=[\n            \"model\", \"X_data\", \"RMSE\", \"MAE\", \"Pearson_coef\",\n        ])\n\n# for each kind of X data\nfor name_X, X_data in dico_X.items() :\n    \n    # Learning curve\n    display_learning_curve(model, X_data, y_train, \"LinearRegression\", name_X)\n                           \n    # Get results\n    df_lr = cross_val(model, X_data, y_train, \"LinearRegression\", name_X)\n    lr_results = pd.concat([lr_results, df_lr], axis = 0)\n\nlr_results","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:27:52.855365Z","iopub.execute_input":"2022-04-06T13:27:52.855824Z","iopub.status.idle":"2022-04-06T13:30:37.345616Z","shell.execute_reply.started":"2022-04-06T13:27:52.855783Z","shell.execute_reply":"2022-04-06T13:30:37.344682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_results = lr_results.reset_index(drop=True)\nlr_results.sort_values(by = \"RMSE\")","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:30:56.55641Z","iopub.execute_input":"2022-04-06T13:30:56.556734Z","iopub.status.idle":"2022-04-06T13:30:56.57339Z","shell.execute_reply.started":"2022-04-06T13:30:56.556699Z","shell.execute_reply":"2022-04-06T13:30:56.572477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_results.sort_values(by = \"MAE\")","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:31:01.300151Z","iopub.execute_input":"2022-04-06T13:31:01.300456Z","iopub.status.idle":"2022-04-06T13:31:01.314908Z","shell.execute_reply.started":"2022-04-06T13:31:01.300425Z","shell.execute_reply":"2022-04-06T13:31:01.313828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_results.sort_values(by = \"Pearson_coef\", ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:31:05.030485Z","iopub.execute_input":"2022-04-06T13:31:05.030789Z","iopub.status.idle":"2022-04-06T13:31:05.047356Z","shell.execute_reply.started":"2022-04-06T13:31:05.030759Z","shell.execute_reply":"2022-04-06T13:31:05.046425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Metrics are close to each other.\nWe can see that top3 is the same for each metric.","metadata":{}},{"cell_type":"markdown","source":"### Using TimeSeriesSplit\nWe are going to compare our different model of LinearRegression using TimeSeriesSplit : we are going to split data in cross validation with this.","metadata":{}},{"cell_type":"code","source":"# TimeSeriesSplit\nts_cv = TimeSeriesSplit(n_splits=5, test_size = 20000)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:31:10.735212Z","iopub.execute_input":"2022-04-06T13:31:10.73581Z","iopub.status.idle":"2022-04-06T13:31:10.740332Z","shell.execute_reply.started":"2022-04-06T13:31:10.73576Z","shell.execute_reply":"2022-04-06T13:31:10.739392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initiate counter\ni = 1\n\n# Get number of samples in each fold\nfor train_index, val_index in ts_cv.split(X_pca85):\n    print(\n        \"Split \",\n        i,\n        \"\\nTrain nb of samples :\",\n        len(train_index),\n        \"Validation nb of samples :\",\n        len(val_index),\n        \"\\n\",\n    )\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:31:14.790272Z","iopub.execute_input":"2022-04-06T13:31:14.790603Z","iopub.status.idle":"2022-04-06T13:31:14.802257Z","shell.execute_reply.started":"2022-04-06T13:31:14.79057Z","shell.execute_reply":"2022-04-06T13:31:14.799276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model training","metadata":{}},{"cell_type":"code","source":"%%time\n\nmodel = LinearRegression()\nn_folds = ts_cv\n\n# Dataframe for results\nlr_results_ts_cv = pd.DataFrame(columns=[\n            \"model\", \"X_data\", \"RMSE\", \"MAE\", \"Pearson_coef\",\n        ])\n\n# for each kind of X data\nfor name_X, X_data in dico_X.items() :\n                           \n    # Get results\n    df_lr = cross_val(model, X_data, y_train, \"LinearRegression\", name_X)\n    lr_results_ts_cv = pd.concat([lr_results_ts_cv, df_lr], axis = 0).reset_index(drop=True)\n\nlr_results_ts_cv","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:31:25.858279Z","iopub.execute_input":"2022-04-06T13:31:25.858584Z","iopub.status.idle":"2022-04-06T13:31:43.066617Z","shell.execute_reply.started":"2022-04-06T13:31:25.858554Z","shell.execute_reply":"2022-04-06T13:31:43.065792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_results_ts_cv.sort_values(by=\"RMSE\")","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:31:47.855241Z","iopub.execute_input":"2022-04-06T13:31:47.855507Z","iopub.status.idle":"2022-04-06T13:31:47.86959Z","shell.execute_reply.started":"2022-04-06T13:31:47.85548Z","shell.execute_reply":"2022-04-06T13:31:47.868424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Top 3 is the same, RMSE ans MAE are better, but Pearson_coef is worse (except when trained on all data).\n\nWe choose X_up_pca85 as best pre processed data and we are going to evaluate this model. ","metadata":{}},{"cell_type":"markdown","source":"#### Model evaluation","metadata":{}},{"cell_type":"code","source":"# Create Pipeline\npipeline_lr = Pipeline(\n    [(\"pca\", PCA(n_components=0.85, random_state=0)), (\"lr\", LinearRegression()),]\n)\n\n# Model training\npipeline_lr.fit(X_up_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:32:31.799002Z","iopub.execute_input":"2022-04-06T13:32:31.799333Z","iopub.status.idle":"2022-04-06T13:32:34.775515Z","shell.execute_reply.started":"2022-04-06T13:32:31.799297Z","shell.execute_reply":"2022-04-06T13:32:34.774281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Pipeline\npipeline_lr = Pipeline(\n    [(\"pca\", PCA(n_components=0.85, random_state=0)), (\"lr\", LinearRegression()),]\n)\n\n# Model training\npipeline_lr.fit(X_up_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:32:39.053802Z","iopub.execute_input":"2022-04-06T13:32:39.054094Z","iopub.status.idle":"2022-04-06T13:32:42.627938Z","shell.execute_reply.started":"2022-04-06T13:32:39.054063Z","shell.execute_reply":"2022-04-06T13:32:42.626817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_scores(model, name_model, name_X, X_test, y_test):\n\n    \"\"\"\n    Function to get target predictions and dataframe with\n    metrics\n    \n    Arguments : \n    - model : model to evaluate\n    - name_model : name of the model (string)\n    - name_X : name of the data (string)\n    - X_test : data to get predictions\n    - y_test : actual target values\n    \n    Return :\n    - y_pred : array of predictions values\n    - results : dataframe with name_model, name_X, RMSE and\n    Pearson_coef\n    \n    \"\"\"\n\n    # get predictions\n    y_pred = model.predict(X_test)\n\n    # Get scores\n    RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n    pearson = pearsonr(y_pred, y_test)[0]\n\n    # Data frame for results\n    results = pd.DataFrame(\n        [[name_model, name_X, RMSE, pearson]],\n        columns=[\"model\", \"X_data\", \"RMSE\", \"Pearson_coef\"],\n    )\n\n    return y_pred, results","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:32:47.01432Z","iopub.execute_input":"2022-04-06T13:32:47.015349Z","iopub.status.idle":"2022-04-06T13:32:47.021984Z","shell.execute_reply.started":"2022-04-06T13:32:47.015308Z","shell.execute_reply":"2022-04-06T13:32:47.021198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model evaluation\ny_pred, lr_final = get_scores(\n    pipeline_lr, \"LinearRegression\", \"X_up_pca85\", X_up_test, y_test\n)\nlr_final","metadata":{"execution":{"iopub.status.busy":"2022-04-06T13:32:51.380353Z","iopub.execute_input":"2022-04-06T13:32:51.380693Z","iopub.status.idle":"2022-04-06T13:32:51.437478Z","shell.execute_reply.started":"2022-04-06T13:32:51.38066Z","shell.execute_reply":"2022-04-06T13:32:51.436528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}