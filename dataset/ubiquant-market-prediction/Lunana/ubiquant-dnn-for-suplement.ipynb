{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"lunana  \nlast update 2022 04 26  \nゆっくりしていってね！","metadata":{}},{"cell_type":"markdown","source":"version 6 +Trends","metadata":{}},{"cell_type":"markdown","source":"# Trends  \nUbiquantコンペは、Optiverコンペに似ています。  \nhttps://www.kaggle.com/c/optiver-realized-volatility-prediction  \ntraining timelineとforcasting timelineで訓練データが変わるかもしれない。  \nnotebookが実行されるごとに、モデル訓練が必要かもしれません。  \nそこで、効果検証用のnotebookと最終提出用のnotebook２つを作っていきます。  \n\nThe Ubiquant competition is similar to the Optiver competition.  \nhttps://www.kaggle.com/c/optiver-realized-volatility-prediction  \nTraining data may change between training timeline and forcasting timeline.  \nModel training may be required each time the notebook is run.  \nTherefore, we will create two notebooks, one for effect verification and the other for final submission.  \n\n## EDA  \nhttps://www.kaggle.com/lunapandachan/ubiquant-eda-english\n\n## Add test  \n* 🔥Ubiquant DNN chiranjeev  \nversion 5　seed=41, 5folds,LB=0.148  \nversion 7　seed=41, 10folds, LB=0.149  \nversion 8　seed=41, 10folds, LB=0.149  \n","metadata":{}},{"cell_type":"markdown","source":"# Score  \nversion 1　val_correlation LB=0.1522  \nversion 2　val_mae LB=0.1496  \nversion 3　val_loss LB=0.1481  \nversion 4　val_mape LB=0.1495  \nversion 5　val_rmse LB=0.1492  \n\n検証した結果、version1のval_correlationが一番高いスコアでした。","metadata":{}},{"cell_type":"markdown","source":"**霊夢:今日はLonnieさんのコードを追試するよ。**\n\n**Reimu: Today I'll retest Lonnie's code.**  \n\nhttps://www.kaggle.com/lonnieqin/ubiquant-market-prediction-with-dnn","metadata":{}},{"cell_type":"markdown","source":"現時点では、supplemental_train.csvを使用すると、Submission Scoring Errorが出ます。これは、ノイズデータのためと考えます。4月18日以降は正しいsuplemental_train.csvになるようなので、これに期待して、スイッチをつけることにしました。 4月17日以降に提出するときには、注意してください。","metadata":{}},{"cell_type":"code","source":"import datetime\ndt_now = datetime.datetime.now()\nIS_SUP=False\nif dt_now.month>5 or dt_now.day>16:\n    IS_SUP=True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CFG","metadata":{}},{"cell_type":"code","source":"class CFG:\n    n_folds=5\n    train_filename='../input/ubiquant-market-prediction-half-precision-pickle/train.pkl'\n    s_filename='../input/ubiquant-market-prediction/supplemental_train.csv'\n    train_dataset_path='../input/ubiquant-tfrecords/'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## import","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom scipy import stats\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import *\nimport warnings\n\nimport time\nimport pickle\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.keras import backend as K","metadata":{"papermill":{"duration":7.781864,"end_time":"2022-01-25T15:39:09.265726","exception":false,"start_time":"2022-01-25T15:39:01.483862","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-21T14:15:05.231054Z","iopub.execute_input":"2022-03-21T14:15:05.23183Z","iopub.status.idle":"2022-03-21T14:15:10.548349Z","shell.execute_reply.started":"2022-03-21T14:15:05.231609Z","shell.execute_reply":"2022-03-21T14:15:10.547529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\ndtype_features={'time_id':int,'investment_id':int,'target':np.float32}\nusecols=['time_id','investment_id','target']\nfor feature in features:\n    dtype_features[feature]= np.float32\n    usecols.append(feature)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_SUP:\n    f=open(CFG.s_filename,'r')\n    strain_file=open('s_train.csv', 'w')\n    flg_first=True\n    index_f=None\n    count =0\n    rows=[]\n    p_count=0\n    for row in f:\n        if flg_first:\n            index_f=row\n            strain_file.write(index_f)\n            flg_first=False\n        else:\n            if count<200000:\n                count+=1\n                strain_file.write(row)\n            else:\n                count=0\n                strain_file.write(row)\n                strain_file.close()\n                ## procedure s_train to df\n                s_train=pd.read_csv(f's_train.csv',usecols=usecols,dtype=dtype_features)\n                s_train.to_parquet(f's_train.parquet{p_count}')\n                p_count+=1                   \n                !rm s_train.csv\n                del s_train\n                gc.collect()\n                ! free -m\n                strain_file=open('s_train.csv', 'w')\n                strain_file.write(index_f)            \n    f.close()\n    strain_file.close()\n    df=pd.read_csv(f's_train.csv',usecols=usecols,dtype=dtype_features)\n    !rm s_train.csv\n    for idx in range(p_count):\n        s_train = pd.read_parquet(f's_train.parquet{idx}')\n        df=pd.concat([df,s_train])\n        del s_train\n        !rm s_train.parquet{idx}\n    gc.collect()\n    !free -m\n    df = df.reset_index(drop=True)\n    df.to_pickle(f's_train.pkl')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport json\nimport numpy as np\nfrom scipy.special import comb\nfrom itertools import combinations\n\nclass CombinatorialPurgedGroupKFold():\n    def __init__(self, n_splits = 6, n_test_splits = 2, purge = 1, pctEmbargo = 0.01, **kwargs):\n        self.n_splits = n_splits\n        self.n_test_splits = n_test_splits\n        self.purge = purge\n        self.pctEmbargo = pctEmbargo\n        \n    def split(self, X, y = None, groups = None):\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n            \n        u, ind = np.unique(groups, return_index = True)\n        unique_groups = u[np.argsort(ind)]\n        n_groups = len(unique_groups)\n        group_dict = {}\n        for idx in range(len(X)):\n            if groups[idx] in group_dict:\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n                \n        n_folds = comb(self.n_splits, self.n_test_splits, exact = True)\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n            \n        mbrg = int(n_groups * self.pctEmbargo)\n        if mbrg < 0:\n            raise ValueError(\n                \"The number of 'embargoed' groups should not be negative\")\n        \n        split_dict = {}\n        group_test_size = n_groups // self.n_splits\n        for split in range(self.n_splits):\n            if split == self.n_splits - 1:\n                split_dict[split] = unique_groups[int(split * group_test_size):].tolist()\n            else:\n                split_dict[split] = unique_groups[int(split * group_test_size):int((split + 1) * group_test_size)].tolist()\n        \n        for test_splits in combinations(range(self.n_splits), self.n_test_splits):\n            test_groups = []\n            banned_groups = []\n            for split in test_splits:\n                test_groups += split_dict[split]\n                banned_groups += unique_groups[split_dict[split][0] - self.purge:split_dict[split][0]].tolist()\n                banned_groups += unique_groups[split_dict[split][-1] + 1:split_dict[split][-1] + self.purge + mbrg + 1].tolist()\n            train_groups = [i for i in unique_groups if (i not in banned_groups) and (i not in test_groups)]\n\n            train_idx = []\n            test_idx = []\n            for train_group in train_groups:\n                train_idx += group_dict[train_group]\n            for test_group in test_groups:\n                test_idx += group_dict[test_group]\n            yield train_idx, test_idx","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_SUP:\n    investment_id = df.pop(\"investment_id\")\n    time_id = df.pop(\"time_id\")\n    investment_id.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_SUP:\n    y = df.pop(\"target\")\n    y.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_record(i):\n    dic = {}\n    dic[f\"features\"] = tf.train.Feature(float_list=tf.train.FloatList(value=list(df.iloc[i])))\n    dic[\"time_id\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[time_id.iloc[i]]))\n    dic[\"investment_id\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[investment_id.iloc[i]]))\n    dic[\"target\"] = tf.train.Feature(float_list=tf.train.FloatList(value=[y.iloc[i]]))\n    record_bytes = tf.train.Example(features=tf.train.Features(feature=dic)).SerializeToString()\n    return record_bytes\n    \ndef decode_function(record_bytes):\n  return tf.io.parse_single_example(\n      # Data\n      record_bytes,\n      # Schema\n      {\n          \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n          \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"investment_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n      }\n  )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nif IS_SUP:\n    import time\n    n_splits = CFG.n_folds\n    n_test_splits = 1\n    kfold = CombinatorialPurgedGroupKFold(n_splits, n_test_splits)\n    for fold, (train_indices, test_indices) in enumerate(kfold.split(df, groups=time_id)):\n        print(\"=\" * 100)\n        print(f\"Fold {fold}\")\n        print(\"=\" * 100)\n        print(\"Train Sample size:\", len(test_indices))\n        train_save_path = f\"s_train_fold{fold}.tfrecords\"\n        begin = time.time()\n        print(f\"Creating {train_save_path}\")\n        with tf.io.TFRecordWriter(train_save_path) as file_writer:\n            for i in test_indices:\n                file_writer.write(create_record(i))\n        print(\"Elapsed time: %.2f\"%(time.time() - begin))\n    del df,time_id,y\n    gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!free -m","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model DNN","metadata":{}},{"cell_type":"code","source":"train = pd.read_pickle(CFG.train_filename)\nif IS_SUP:\n    investment_id_2 = train.pop(\"investment_id\")\n    investment_id=pd.concat([investment_id,investment_id_2])\nelse:\n    investment_id = train.pop(\"investment_id\")    \ndel train\ngc.collect()\ninvestment_id.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ninvestment_id=investment_id.astype(np.int16)\ninvestment_ids = list(investment_id.unique())\ninvestment_id_size = len(investment_ids) + 1\nwith tf.device(\"cpu\"):\n    investment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\n    investment_id_lookup_layer.adapt(pd.DataFrame({\"investment_ids\":investment_ids}))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make Tensorflow dataset","metadata":{"papermill":{"duration":0.018846,"end_time":"2022-01-25T15:39:30.567495","exception":false,"start_time":"2022-01-25T15:39:30.548649","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def decode_function(record_bytes):\n  return tf.io.parse_single_example(\n      # Data\n      record_bytes,\n      # Schema\n      {\n          \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n          \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"investment_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n      }\n  )\ndef preprocess(item):\n    return (item[\"investment_id\"], item[\"features\"]), item[\"target\"]\ndef make_dataset(file_paths, batch_size=4096, mode=\"train\"):\n    ds = tf.data.TFRecordDataset(file_paths)\n    ds = ds.map(decode_function)\n    ds = ds.map(preprocess)\n    #if mode == \"train\":\n     #   ds = ds.shuffle(batch_size * 4)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds","metadata":{"papermill":{"duration":0.02858,"end_time":"2022-01-25T15:39:30.614302","exception":false,"start_time":"2022-01-25T15:39:30.585722","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-21T14:15:12.982927Z","iopub.execute_input":"2022-03-21T14:15:12.98333Z","iopub.status.idle":"2022-03-21T14:15:12.992256Z","shell.execute_reply.started":"2022-03-21T14:15:12.983289Z","shell.execute_reply":"2022-03-21T14:15:12.99158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{"papermill":{"duration":0.017564,"end_time":"2022-01-25T15:39:30.64918","exception":false,"start_time":"2022-01-25T15:39:30.631616","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def correlation(x, y, axis=-2):\n    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xvar * yvar)\n    return tf.constant(1.0, dtype=x.dtype) - corr\n\ndef get_model():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.1)(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.1)(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.1)(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.1)(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.1)(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.1)(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse, correlation])\n    return model","metadata":{"papermill":{"duration":0.033569,"end_time":"2022-01-25T15:39:30.700649","exception":false,"start_time":"2022-01-25T15:39:30.66708","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-21T14:15:12.994568Z","iopub.execute_input":"2022-03-21T14:15:12.994903Z","iopub.status.idle":"2022-03-21T14:15:13.01192Z","shell.execute_reply.started":"2022-03-21T14:15:12.994869Z","shell.execute_reply":"2022-03-21T14:15:13.01117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at this Model's architecture.","metadata":{}},{"cell_type":"code","source":"model = get_model()\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)","metadata":{"papermill":{"duration":0.209912,"end_time":"2022-01-25T15:39:30.929112","exception":false,"start_time":"2022-01-25T15:39:30.7192","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-21T14:15:13.013083Z","iopub.execute_input":"2022-03-21T14:15:13.013489Z","iopub.status.idle":"2022-03-21T14:15:14.276268Z","shell.execute_reply.started":"2022-03-21T14:15:13.013454Z","shell.execute_reply":"2022-03-21T14:15:14.275452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"markdown","source":"**魔理沙:ここからがtrainingだぜ。**  \n**Marisa: Training starts here.**","metadata":{}},{"cell_type":"code","source":"import time","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fold(index):\n    train_path = []\n    valid_path = []\n    for i in range(CFG.n_folds):\n        if i==index:\n            valid_path.append(f\"{CFG.train_dataset_path}train_fold{i}.tfrecords\")\n            if IS_SUP:\n                valid_path.append(f\"s_train_fold{i}.tfrecords\")\n        else:\n            train_path.append(f\"{CFG.train_dataset_path}train_fold{i}.tfrecords\")\n            if IS_SUP:\n                train_path.append(f\"s_train_fold{i}.tfrecords\")\n\n    train_ds = make_dataset(train_path)\n    valid_ds = make_dataset(valid_path, mode=\"valid\")\n    print(f' fold{index}  ')\n    model = get_model()\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{index}.tf\", monitor=\"val_correlation\", mode=\"min\", save_best_only=True, save_weights_only=True)\n    early_stop = keras.callbacks.EarlyStopping(patience=10)\n    history = model.fit(train_ds, epochs=30, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n    model.load_weights(f\"model_{index}.tf\")\n    model.save(f'model_{index}')\n    #for metric in [\"loss\", \"mae\", \"mape\", \"rmse\", \"correlation\"]:\n    #    pd.DataFrame(history.history, columns=[metric, f\"val_{metric}\"]).plot()\n    #    plt.title(metric.upper())\n    #    plt.show()\n    #y_vals = []\n    #for _, y in valid_ds:\n    #    y_vals += list(y.numpy().reshape(-1))\n    #y_val = np.array(y_vals)\n    #pearson_score = stats.pearsonr(model.predict(valid_ds).reshape(-1), y_val)[0]\n    #print(f\"Pearson Score: {pearson_score}　fold{index} \")\n    del model,train_ds,valid_ds\n    K.clear_session()\n    gc.collect()\n    #time.sleep(10)\n    !free -m","metadata":{"papermill":{"duration":471.71946,"end_time":"2022-01-25T15:47:23.749584","exception":false,"start_time":"2022-01-25T15:39:32.030124","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-21T14:15:14.277684Z","iopub.execute_input":"2022-03-21T14:15:14.277962Z","iopub.status.idle":"2022-03-21T14:20:53.079395Z","shell.execute_reply.started":"2022-03-21T14:15:14.277927Z","shell.execute_reply":"2022-03-21T14:20:53.078671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index in range(CFG.n_folds):\n    train_fold(index)\n\nprint(\"load all models\")\nmodels = []\n\nfor index in range(CFG.n_folds):\n    model = get_model()\n    model.load_weights(f\"model_{index}\")\n    models.append(model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{"papermill":{"duration":2.11441,"end_time":"2022-01-25T15:47:27.649127","exception":false,"start_time":"2022-01-25T15:47:25.534717","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfeatures = [f\"f_{i}\" for i in range(300)]\nfor (test_df, sample_prediction_df) in iter_test:\n    ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n    sample_prediction_df['target'] = inference(models, ds)\n    env.predict(sample_prediction_df) ","metadata":{"papermill":{"duration":2.234161,"end_time":"2022-01-25T15:47:35.24214","exception":false,"start_time":"2022-01-25T15:47:33.007979","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-21T14:20:53.099328Z","iopub.execute_input":"2022-03-21T14:20:53.099666Z","iopub.status.idle":"2022-03-21T14:20:53.8352Z","shell.execute_reply.started":"2022-03-21T14:20:53.099633Z","shell.execute_reply":"2022-03-21T14:20:53.834479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission=pd.read_csv(\"./submission.csv\")\nsubmission","metadata":{},"execution_count":null,"outputs":[]}]}