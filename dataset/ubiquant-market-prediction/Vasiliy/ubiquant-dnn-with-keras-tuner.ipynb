{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Ubiquant Market Prediction with DNN and Keras Tuner","metadata":{"papermill":{"duration":0.029801,"end_time":"2022-01-26T03:06:09.375462","exception":false,"start_time":"2022-01-26T03:06:09.345661","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Based on this great notebook [Ubiquant Market Prediction with DNN](https://www.kaggle.com/lonnieqin/ubiquant-market-prediction-with-dnn/notebook), so please upvote it. \n\n[Keras Tuner](https://www.tensorflow.org/tutorials/keras/keras_tuner) is added to find best hyperparameters of the DNN model.","metadata":{}},{"cell_type":"code","source":"DEBUG = False","metadata":{"execution":{"iopub.status.busy":"2022-02-15T08:24:23.499875Z","iopub.execute_input":"2022-02-15T08:24:23.500202Z","iopub.status.idle":"2022-02-15T08:24:23.528594Z","shell.execute_reply.started":"2022-02-15T08:24:23.500111Z","shell.execute_reply":"2022-02-15T08:24:23.527929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nif not DEBUG:\n    warnings.filterwarnings('ignore')\nimport os\nimport gc\nimport json\nimport time\nimport random\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom scipy import stats\nimport kerastuner as kt\nfrom tqdm.auto import tqdm\nprint('tensorflow version:', tf.__version__)\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\nif gpu_devices:\n    for gpu_device in gpu_devices:\n        print('device available:', gpu_device)\npd.set_option('display.max_columns', None)","metadata":{"papermill":{"duration":6.4223,"end_time":"2022-01-26T03:06:15.827014","exception":false,"start_time":"2022-01-26T03:06:09.404714","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-15T08:24:23.530255Z","iopub.execute_input":"2022-02-15T08:24:23.530519Z","iopub.status.idle":"2022-02-15T08:24:29.174733Z","shell.execute_reply.started":"2022-02-15T08:24:23.530485Z","shell.execute_reply":"2022-02-15T08:24:29.173968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"VER = 'v1'\nCONFIG = {\n    'version': VER,\n    'folds': 5,\n    'epochs': 4 if DEBUG else 20,\n    'patience': 2 if DEBUG else 4,\n    'decay': False,\n    'batch_size': 1024,\n    'seed': 2021,\n    'lr': .001,\n    'max_trials': 3 if DEBUG else 8,\n    'skf': True,\n    'comments': ''\n}\nDATA_PATH = '../input/ubiquant-market-prediction-half-precision-pickle'\nMDLS_PATH = f'./models_{VER}'\nif not os.path.exists(MDLS_PATH):\n    os.mkdir(MDLS_PATH)\nwith open(f'{MDLS_PATH}/config.json', 'w') as file:\n    json.dump(CONFIG, file)\n    \ndef seed_all(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_all(CONFIG['seed'])\nstart_time = time.time()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T08:24:29.176284Z","iopub.execute_input":"2022-02-15T08:24:29.1768Z","iopub.status.idle":"2022-02-15T08:24:29.19063Z","shell.execute_reply.started":"2022-02-15T08:24:29.176762Z","shell.execute_reply":"2022-02-15T08:24:29.189362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load data","metadata":{"papermill":{"duration":0.015333,"end_time":"2022-01-26T03:06:15.858386","exception":false,"start_time":"2022-01-26T03:06:15.843053","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\ntrain = pd.read_pickle(f'{DATA_PATH}/train.pkl')\nif DEBUG:\n    train = train.loc[train.investment_id >= 3000, :]\n    train.reset_index(inplace=True)\n    del train['index']\nelse:\n    # limit the train dataset due to Kaggle memory issue\n    train = train.loc[train.investment_id >= 2000, :]\n    train.reset_index(inplace=True)\n    del train['index']\nprint(train.shape)\ndisplay(train.head())\n\ninvestment_id = train.pop('investment_id')\ninvestment_ids = list(investment_id.unique())\n_ = train.pop('time_id')\ny = train.pop('target')","metadata":{"papermill":{"duration":16.852695,"end_time":"2022-01-26T03:06:32.726165","exception":false,"start_time":"2022-01-26T03:06:15.87347","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-15T08:24:29.193947Z","iopub.execute_input":"2022-02-15T08:24:29.196834Z","iopub.status.idle":"2022-02-15T08:24:49.425772Z","shell.execute_reply.started":"2022-02-15T08:24:29.196787Z","shell.execute_reply":"2022-02-15T08:24:49.424967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{"papermill":{"duration":0.016753,"end_time":"2022-01-26T03:06:32.9157","exception":false,"start_time":"2022-01-26T03:06:32.898947","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def preprocess(X, y):\n    return X, y\n\ndef make_dataset(feature, investment_id, y,\n                 batch_size, mode='train'):\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (\n            (\n                investment_id,\n                feature,\n            ), \n            y\n        )\n    )\n    dataset = dataset.map(preprocess)\n    if mode == 'train':\n        dataset = dataset.shuffle(4096)\n    dataset = dataset.batch(batch_size).cache().prefetch(\n        tf.data.experimental.AUTOTUNE\n    )\n    return dataset\n\ndef metric_plot(history, cols, title):\n    plt.plot(pd.DataFrame(history.history).loc[:, cols])\n    plt.title(title)\n\ndef datasets(trn_ind_folds, val_ind_folds, fold, train, investment_id):\n    trn_ind = trn_ind_folds[fold]\n    val_ind = val_ind_folds[fold]\n    train_ds = make_dataset(\n        train.iloc[trn_ind], \n        investment_id[trn_ind], \n        y.iloc[trn_ind],\n        batch_size=CONFIG['batch_size'],\n        mode='train'\n    )\n    val_ds = make_dataset(\n        train.iloc[val_ind], \n        investment_id[val_ind], \n        y.iloc[val_ind], \n        batch_size=CONFIG['batch_size'],\n        mode='val'\n    )\n    return train_ds, val_ds, y.iloc[val_ind]    \n    \ndef show_results(model, history, val_ds, y_val):\n    pearson_score = stats.pearsonr(model.predict(val_ds).ravel(), y_val.values)[0]\n    print('pearson:', pearson_score)\n    plt.figure(figsize=(16, 3))\n    plt.subplot(1, 3, 1)\n    metric_plot(history, ['mse', 'val_mse'], 'MSE')\n    plt.subplot(1, 3, 2)\n    metric_plot(history, ['mae', 'val_mae'], 'MAE')\n    plt.subplot(1, 3, 3)\n    metric_plot(history, ['rmse', 'val_rmse'], 'RMSE')\n    plt.show()","metadata":{"papermill":{"duration":0.029123,"end_time":"2022-01-26T03:06:36.447216","exception":false,"start_time":"2022-01-26T03:06:36.418093","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-15T08:24:49.42714Z","iopub.execute_input":"2022-02-15T08:24:49.427781Z","iopub.status.idle":"2022-02-15T08:24:49.440182Z","shell.execute_reply.started":"2022-02-15T08:24:49.427754Z","shell.execute_reply":"2022-02-15T08:24:49.439531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train with Keras Tuner","metadata":{"papermill":{"duration":0.018137,"end_time":"2022-01-26T03:06:36.483185","exception":false,"start_time":"2022-01-26T03:06:36.465048","status":"completed"},"tags":[]}},{"cell_type":"code","source":"investment_id_size = len(investment_ids) + 1\ninvestment_id_lookup_layer = layers.IntegerLookup(\n    max_tokens=investment_id_size\n)\ninvestment_id_lookup_layer.adapt(\n    pd.DataFrame(\n        {'investment_ids': investment_ids}\n    )\n)\n\ndef tune_model(hp, features_num=300, lr=.001):\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((features_num, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(\n        investment_id_size, \n        hp.Int('investment_embedding_size', 16, 64),\n        input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    for n_emb_hidden in range(hp.Int('emb_num_layers', 1, 4)):\n        investment_id_x = layers.Dense(\n            hp.Int(f'emb_num_units_{n_emb_hidden}', 16, 512),\n            activation='swish')(investment_id_x)\n        investment_id_x = layers.BatchNormalization()(investment_id_x)\n        investment_id_x = layers.Dropout(\n            hp.Float(f'emb_dropout_{n_emb_hidden}', .0, .5))(investment_id_x)\n    \n    features_x = layers.Dense(\n        hp.Int('features_enter_size', 16, 512), \n        activation='swish')(features_inputs)\n    for n_feats_hidden in range(hp.Int('feats_num_layers', 1, 4)):\n        features_x = layers.Dense(\n            hp.Int(f'feats_num_units_{n_feats_hidden}', 16, 512),\n            activation='swish')(features_x)\n        features_x = layers.BatchNormalization()(features_x)\n        features_x = layers.Dropout(\n            hp.Float(f'feats_dropout_{n_feats_hidden}', .0, .5))(features_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, features_x])\n    for n_hidden in range(hp.Int('num_layers', 1, 4)):\n        x = layers.Dense(\n            hp.Int(f'num_units_{n_hidden}', 16, 512),  \n            activation='swish', \n            kernel_regularizer='l2')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Dropout(hp.Float(f'dropout_{n_hidden}', .0, .5))(x)\n    output = layers.Dense(1)(x)\n    \n    rmse = tf.keras.metrics.RootMeanSquaredError(name='rmse')\n    model = tf.keras.Model(\n        inputs=[investment_id_inputs, features_inputs], \n        outputs=[output]\n    )\n    model.compile(\n        optimizer=tf.optimizers.Adam(\n            hp.Float('lr', .00001, .05, default=lr)\n        ), \n        loss='mse', \n        metrics=['mse', 'mae', 'mape', rmse]\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-15T08:24:49.441715Z","iopub.execute_input":"2022-02-15T08:24:49.442074Z","iopub.status.idle":"2022-02-15T08:24:51.8621Z","shell.execute_reply.started":"2022-02-15T08:24:49.442036Z","shell.execute_reply":"2022-02-15T08:24:51.861292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopper = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', \n    patience=CONFIG['patience'], \n    verbose=1,\n    mode='min',\n    restore_best_weights=True\n)\nplateau_reducer = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', \n    factor=.1, \n    patience=CONFIG['patience'] / 2, \n    verbose=1,\n    mode='min'\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T08:24:51.863612Z","iopub.execute_input":"2022-02-15T08:24:51.863861Z","iopub.status.idle":"2022-02-15T08:24:51.869858Z","shell.execute_reply.started":"2022-02-15T08:24:51.863829Z","shell.execute_reply":"2022-02-15T08:24:51.869154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CVTuner(kt.engine.tuner.Tuner):\n    \n    def run_trial(self, trial, train, n_folds, batch_size, \n                  epochs, obj, mode, callbacks):\n        trn_ind_folds, val_ind_folds = [], []\n        if CONFIG['skf']:\n            print('stratified kfold split')\n            kfold = StratifiedKFold(\n                n_splits=n_folds, \n                shuffle=True, \n                random_state=CONFIG['seed']\n            ).split(train, investment_id)\n            for fold, (trn_ind, val_ind) in enumerate(kfold):\n                trn_ind_folds.append(trn_ind)\n                val_ind_folds.append(val_ind)\n        val_losses = []\n        for counter, fold in enumerate(range(n_folds)):\n            print('CV {}/{}'.format(counter + 1, n_folds))\n            if CONFIG['skf']:\n                train_ds, val_ds, y_val = datasets(\n                    trn_ind_folds,\n                    val_ind_folds, \n                    fold, train, \n                    investment_id\n                )\n            model = self.hypermodel.build(trial.hyperparameters)\n            history = model.fit(\n                train_ds, \n                epochs=epochs, \n                validation_data=val_ds, \n                callbacks=callbacks,\n                verbose=1\n            )\n            val_losses.append(\n                min(history.history[obj]) \n                if mode == 'min' else max(history.history[obj])\n            )\n            show_results(model, history, val_ds, y_val)\n            del train_ds, val_ds, y_val; gc.collect()\n        self.oracle.update_trial(\n            trial.trial_id, \n            {obj: np.mean(val_losses, axis=0)}\n        )\n\nmodel_fn = lambda hp: tune_model(\n    hp, \n    features_num=300, \n    lr=CONFIG['lr']\n)\nif CONFIG['max_trials']:       \n    tuner = CVTuner(\n        hypermodel=model_fn,\n        oracle=kt.oracles.BayesianOptimization(\n            objective= kt.Objective('val_loss', direction='min'),\n            num_initial_points=1,\n            max_trials=CONFIG['max_trials']\n        ),\n        project_name=f'tuner_{VER}'\n    )\n    print('=' * 10, f'TUNER max trials={CONFIG[\"max_trials\"]}', '=' * 10)\n    tuner.search(\n        train, \n        n_folds=CONFIG['folds'], \n        batch_size=CONFIG['batch_size'], \n        epochs=CONFIG['epochs'], \n        obj='val_loss', \n        mode='min',\n        callbacks=[early_stopper, plateau_reducer]\n    )\n    hp = tuner.get_best_hyperparameters(1)[0]\n    pd.to_pickle(hp, f'{MDLS_PATH}/best_hp.pkl', protocol=4)\n    del tuner; gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T08:24:51.871707Z","iopub.execute_input":"2022-02-15T08:24:51.872272Z","iopub.status.idle":"2022-02-15T08:32:20.802627Z","shell.execute_reply.started":"2022-02-15T08:24:51.872234Z","shell.execute_reply":"2022-02-15T08:32:20.801908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hp = pd.read_pickle(f'{MDLS_PATH}/best_hp.pkl')\nprint('hp params loaded:', hp.values)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T08:32:20.804451Z","iopub.execute_input":"2022-02-15T08:32:20.804956Z","iopub.status.idle":"2022-02-15T08:32:20.810891Z","shell.execute_reply.started":"2022-02-15T08:32:20.80492Z","shell.execute_reply":"2022-02-15T08:32:20.810172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn_ind_folds, val_ind_folds = [], []\nif CONFIG['skf']:\n    print('stratified kfold split')\n    kfold = StratifiedKFold(\n        n_splits=CONFIG['folds'], \n        shuffle=True, \n        random_state=CONFIG['seed']\n    ).split(train, investment_id)\n    for fold, (trn_ind, val_ind) in enumerate(kfold):\n        trn_ind_folds.append(trn_ind)\n        val_ind_folds.append(val_ind)\n                \nfor counter, fold in enumerate(range(CONFIG['folds'])):\n    print('========== CV {}/{} =========='.format(\n        counter + 1, \n        CONFIG['folds']\n    ))\n    ch_path = f'{MDLS_PATH}/model_f{fold}'\n    if CONFIG['skf']:\n        train_ds, val_ds, y_val = datasets(\n            trn_ind_folds,\n            val_ind_folds, \n            fold, train, \n            investment_id\n        )\n    model = model_fn(hp)\n    history = model.fit(\n        train_ds, \n        epochs=CONFIG['epochs'], \n        validation_data=val_ds, \n        callbacks=[\n            early_stopper, \n            plateau_reducer, \n            tf.keras.callbacks.ModelCheckpoint(\n                ch_path,\n                monitor='val_loss',\n                verbose=1, \n                save_best_only=True,\n                mode='min'\n            )\n        ],\n        verbose=1\n    )\n    show_results(model, history, val_ds, y_val)\n    del model, train_ds, val_ds, y_val; gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T08:32:20.813846Z","iopub.execute_input":"2022-02-15T08:32:20.8145Z","iopub.status.idle":"2022-02-15T08:35:45.13895Z","shell.execute_reply.started":"2022-02-15T08:32:20.814453Z","shell.execute_reply":"2022-02-15T08:35:45.138253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submit to competition","metadata":{}},{"cell_type":"code","source":"%%time\nmodels = []\nfor fold in range(CONFIG['folds']):\n    ch_path = f'{MDLS_PATH}/model_f{fold}'\n    models.append(tf.keras.models.load_model(ch_path))\n    print('model loaded:', ch_path)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T08:37:19.803954Z","iopub.execute_input":"2022-02-15T08:37:19.804215Z","iopub.status.idle":"2022-02-15T08:37:27.872137Z","shell.execute_reply.started":"2022-02-15T08:37:19.804187Z","shell.execute_reply":"2022-02-15T08:37:27.871372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\n\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    dataset = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    dataset = dataset.map(preprocess_test)\n    dataset = dataset.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return dataset\n\ndef inference(models, dataset):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(dataset)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T08:37:27.880732Z","iopub.execute_input":"2022-02-15T08:37:27.880939Z","iopub.status.idle":"2022-02-15T08:37:27.888821Z","shell.execute_reply.started":"2022-02-15T08:37:27.880913Z","shell.execute_reply":"2022-02-15T08:37:27.88725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\n\nenv = ubiquant.make_env()\niter_test = env.iter_test() \n\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\nfor (test_df, sample_pred_df) in iter_test:\n    dataset = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n    sample_pred_df['target'] = inference(models, dataset)\n    env.predict(sample_pred_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T08:37:27.89105Z","iopub.execute_input":"2022-02-15T08:37:27.891665Z","iopub.status.idle":"2022-02-15T08:37:28.200735Z","shell.execute_reply.started":"2022-02-15T08:37:27.891627Z","shell.execute_reply":"2022-02-15T08:37:28.199606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_pred_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T08:37:28.201909Z","iopub.status.idle":"2022-02-15T08:37:28.202502Z","shell.execute_reply.started":"2022-02-15T08:37:28.202234Z","shell.execute_reply":"2022-02-15T08:37:28.202261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}