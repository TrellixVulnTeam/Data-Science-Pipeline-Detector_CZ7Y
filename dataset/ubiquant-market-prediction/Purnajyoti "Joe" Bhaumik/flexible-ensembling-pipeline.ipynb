{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# revised ensembling from https://www.kaggle.com/viktorbarbarich/linear-regression-baseline\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-03T18:56:23.196753Z","iopub.execute_input":"2022-03-03T18:56:23.19706Z","iopub.status.idle":"2022-03-03T18:56:24.249077Z","shell.execute_reply.started":"2022-03-03T18:56:23.197016Z","shell.execute_reply":"2022-03-03T18:56:24.248273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import dependencies and begin ETL","metadata":{}},{"cell_type":"code","source":"%%time\ndata = pd.read_pickle('../input/ump-train-picklefile/train.pkl')\ndata.drop(columns = ['row_id'], inplace = True) #row id is the time_id and investment_id joined using '-'","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:56:24.251143Z","iopub.execute_input":"2022-03-03T18:56:24.251399Z","iopub.status.idle":"2022-03-03T18:57:06.294458Z","shell.execute_reply.started":"2022-03-03T18:56:24.251362Z","shell.execute_reply":"2022-03-03T18:57:06.292587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's plot the trajectory of some investments","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\nfor investment in np.random.choice(pd.unique(data['investment_id']), 20):\n    data[data['investment_id']==investment].plot('time_id', 'target')","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:57:06.295895Z","iopub.execute_input":"2022-03-03T18:57:06.296279Z","iopub.status.idle":"2022-03-03T18:57:10.832376Z","shell.execute_reply.started":"2022-03-03T18:57:06.296238Z","shell.execute_reply":"2022-03-03T18:57:10.831647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pre-processing","metadata":{}},{"cell_type":"code","source":"y = data.pop('target')\nX = data\nX.describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:57:10.833748Z","iopub.execute_input":"2022-03-03T18:57:10.834013Z","iopub.status.idle":"2022-03-03T18:57:40.219656Z","shell.execute_reply.started":"2022-03-03T18:57:10.833975Z","shell.execute_reply":"2022-03-03T18:57:40.218896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features appear already scaled with mean ~ 0 and std dev ~ 1, so there is no need to scale them.","metadata":{}},{"cell_type":"code","source":"X['time_id'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:57:40.222229Z","iopub.execute_input":"2022-03-03T18:57:40.222713Z","iopub.status.idle":"2022-03-03T18:57:40.247013Z","shell.execute_reply.started":"2022-03-03T18:57:40.222671Z","shell.execute_reply":"2022-03-03T18:57:40.246059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# There are 1211 different time steps with varying numbers of investments per time step.","metadata":{}},{"cell_type":"code","source":"X['investment_id'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:57:40.248653Z","iopub.execute_input":"2022-03-03T18:57:40.248962Z","iopub.status.idle":"2022-03-03T18:57:40.27082Z","shell.execute_reply.started":"2022-03-03T18:57:40.248924Z","shell.execute_reply":"2022-03-03T18:57:40.269906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# There are 3579 different investments","metadata":{}},{"cell_type":"code","source":"# from sklearn.decomposition import IncrementalPCA\n\n# transformer = IncrementalPCA(n_components=10)\n# transformer.partial_fit(X)\n# X = transformer.transform(X)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:57:40.272178Z","iopub.execute_input":"2022-03-03T18:57:40.272556Z","iopub.status.idle":"2022-03-03T18:57:40.277907Z","shell.execute_reply.started":"2022-03-03T18:57:40.272515Z","shell.execute_reply":"2022-03-03T18:57:40.276992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neither PCA nor incremental PCA works within the memory limits of this kernel. Unsupervised dimensionality reduction may be an option later... \n\n# Let's train models for ensembling using Stratified K folding to maintain the distribution of time steps per sample.","metadata":{}},{"cell_type":"code","source":"CNT_MODELS = 30 # 30 linear models\n\nmodels = []\n\nfor i in range(0, CNT_MODELS):\n    X_s = X.loc[i::CNT_MODELS]\n    y_s = y.loc[X_s.index]\n    models.append(LinearRegression().fit(X_s, y_s))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-03T18:57:40.279608Z","iopub.execute_input":"2022-03-03T18:57:40.27991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's stack the models creating a meta model.","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(columns=np.arange(len(models)))\n\nfor i, model in enumerate(models):\n    df[i]=model.predict(X)\n\nregr = LinearRegression().fit(df.values, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict values. First predict from each model in the ensemble. Finally, use those predictions as input into the meta model.","metadata":{}},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test()\ni = 0\n\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df.reset_index(inplace = True)\n    test_df.pop('row_id')\n    test_df.rename(columns={'index':'time_id'}, inplace = True)\n    test_df['time_id'] = i\n    df = pd.DataFrame(columns=np.arange(len(models)))\n    for i, model in enumerate(models):\n        df[i] = model.predict(test_df.values)\n    sample_prediction_df['target'] = regr.predict(df)\n    env.predict(sample_prediction_df)\n    i += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{},"execution_count":null,"outputs":[]}]}