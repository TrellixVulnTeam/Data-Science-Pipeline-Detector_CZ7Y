{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport gc\nimport os\nfrom pathlib import Path\n#from argparse import Namespace\n#from collections import defaultdict\nimport matplotlib as plt\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\n#from scipy import stats\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reduce Memory Size","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                #    df[col] = df[col].astype(np.float16)\n                #elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                #    df[col] = df[col].astype(np.float32)\n                #else:\n                df[col] = df[col].astype(np.float16)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import data","metadata":{}},{"cell_type":"code","source":"#train = pd.read_csv('../input/ubiquant-market-prediction/train.csv')\ntrain = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')\ntrain =train[0:1000000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import gc\ngc.collect()\ntest_cols = ['time_id','investment_id']\nfeature = [c for c in train.columns if \"f_\" in c]\nfeatures = ['investment_id','time_id'] + feature","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"len(feature)","metadata":{}},{"cell_type":"code","source":"train['time_id']=pd.to_numeric(train['time_id'])\ntrain['time_id']=train['time_id'].astype(np.float16)\ntrain['investment_id']=pd.to_numeric(train['investment_id'])\ntrain['investment_id']=train['investment_id'].astype(np.float16)\ntrain['investment_id2']=train['investment_id']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\nscaler.fit(train[test_cols])\ntrain[test_cols]=scaler.transform(train[test_cols])\ntrain[test_cols]=train[test_cols].astype(np.float16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_=train.pop('row_id')\ninvestment_id2=train.pop('investment_id2')\ny=train.pop('target')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ndef preprocess(X, y):\n    return X, y\ndef make_dataset(feature, y, batch_size=1024, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices(((feature), y))\n    \n\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(4096)\n    \n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    #time_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    #investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((302, ), dtype=tf.float16)\n    \n    feature_x = layers.Dense(512, activation='swish')(features_inputs)\n    feature_x = layers.Dense(512, activation='swish')(feature_x)\n    feature_x = layers.Dense(512, activation='swish')(feature_x)\n    \n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n    #x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    #x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    #x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x) \n    #x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x) \n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.0002), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model()\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(2, shuffle=True, random_state=42)\nmodels = []\ngc.collect()\nfor index, (train_indices, valid_indices) in enumerate(kfold.split(train[features], investment_id2)):\n    \n    X_train, X_val = train.iloc[train_indices], train.iloc[valid_indices]\n    #time_id_train = time_id[train_indices]\n    #investment_id_train = time_id[train_indices]\n    y_train, y_val = y.iloc[train_indices], y.iloc[valid_indices]\n    #investment_id_val = time_id[valid_indices]\n    #time_id_val = time_id[valid_indices]\n    train_ds = make_dataset(X_train, y_train)\n    valid_ds = make_dataset(X_val, y_val, mode=\"valid\")\n    model = get_model()\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{index}\", save_best_only=True)\n    early_stop = keras.callbacks.EarlyStopping(patience=3)\n    model.fit(train_ds, epochs=50, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n    model = keras.models.load_model(f\"model_{index}\")\n    models.append(model)\n    #pearson_score = stats.pearsonr(model.predict(valid_ds).ravel(), y_val.values)[0]\n    #print('Pearson:', pearson_score)\n    #pd.DataFrame(history.history, columns=[\"mse\", \"val_mse\"]).plot()\n    #plt.title(\"MSE\")\n    #plt.show()\n    #pd.DataFrame(history.history, columns=[\"mae\", \"val_mae\"]).plot()\n    #plt.title(\"MAE\")\n    #plt.show()\n    #pd.DataFrame(history.history, columns=[\"rmse\", \"val_rmse\"]).plot()\n    #plt.title(\"RMSE\")\n    #plt.show()\n\n    del X_train\n    del X_val\n    del y_train\n    del y_val\n    del train_ds\n    del valid_ds\n    gc.collect()\n    break\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_test(feature):\n    return (feature), 0\ndef make_test_dataset(feature, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((feature)))\n    ds = ds.map(preprocess_test)\n    #ds = ds.batch(batch_size)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfor (test_df, sample_prediction_df) in iter_test:\n    \n    test_df['time_id']=test_df['row_id'].apply(lambda x:x[0:x.find('_')])\n    test_df['time_id']=pd.to_numeric(test_df['time_id'], downcast='float')\n    test_df['time_id']=test_df['time_id'].astype(np.float16)\n    test_df['investment_id']=pd.to_numeric(test_df['investment_id'], downcast='float')\n    test_df['investment_id']=test_df['investment_id'].astype(np.float16)\n    test_df[test_cols]=scaler.transform(test_df[test_cols])\n    ds = make_test_dataset(test_df[features])\n    #print(test_df[features])\n    sample_prediction_df['target'] = inference(models, ds)\n    #print(sample_prediction_df)\n    env.predict(sample_prediction_df) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}