{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TabNet Baseline\n\n- TabNet is widely used in table-based competitions. This notebook is just the most basic attempt based on LightGMB's baseline. I hope you will like it and give me some more suggestions.\n- 大家好，这是我第一次上传我的notebook，TabNet是表格类数据比赛中常用的手法之一。这个笔记本是根据别人开源的LightGBM稍微更改了一点的笔记本。希望大家多给一些意见。\n- 今回は初めて自分のノートブックをアップロードしました。TabNetは表データのコンペでよく使われている手法です。このノートブックはLightGBMのベースラインからTabNetが使えるように少し変更したものです。みんなさんからアドバイスを頂けたら嬉しいです。\n\n- [Paper](https://arxiv.org/abs/1908.07442v5)\n\n# What I want to try next.\n- Version 1 :Baseline\n- Version 2 :Feature filtering (based on importance, etc.), change the validation method\n- Version 3 :Optuna tuning parameters\n- Version 4 :Ensable with LightGBM","metadata":{}},{"cell_type":"markdown","source":"## INSTALL","metadata":{}},{"cell_type":"code","source":"!pip -q install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\n!pip -q install ../input/talib-binary/talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2022-02-22T22:46:35.223338Z","iopub.execute_input":"2022-02-22T22:46:35.223652Z","iopub.status.idle":"2022-02-22T22:47:31.341596Z","shell.execute_reply.started":"2022-02-22T22:46:35.22357Z","shell.execute_reply":"2022-02-22T22:47:31.340749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom pytorch_tabnet.pretraining import TabNetPretrainer","metadata":{"execution":{"iopub.status.busy":"2022-02-22T22:47:31.344048Z","iopub.execute_input":"2022-02-22T22:47:31.34426Z","iopub.status.idle":"2022-02-22T22:47:33.569638Z","shell.execute_reply.started":"2022-02-22T22:47:31.344235Z","shell.execute_reply":"2022-02-22T22:47:33.568891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import","metadata":{}},{"cell_type":"markdown","source":"## config","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport joblib\nimport random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom argparse import Namespace\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, train_test_split\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 64)\n\ndef seed_everything(seed: int = 42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-22T22:47:33.570919Z","iopub.execute_input":"2022-02-22T22:47:33.571749Z","iopub.status.idle":"2022-02-22T22:47:33.602448Z","shell.execute_reply.started":"2022-02-22T22:47:33.571694Z","shell.execute_reply":"2022-02-22T22:47:33.601811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = Namespace(\n    INFER=True,\n    debug=False,\n    seed=21,\n    folds=5,\n    workers=4,\n    min_time_id=None, \n    holdout=True,\n    num_bins=16,\n    data_path=Path(\"../input/ubiquant-parquet/\"),\n)\nseed_everything(args.seed)\n\nif args.debug:\n    setattr(args, 'min_time_id', 1100)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T22:47:33.604257Z","iopub.execute_input":"2022-02-22T22:47:33.60452Z","iopub.status.idle":"2022-02-22T22:47:33.609824Z","shell.execute_reply.started":"2022-02-22T22:47:33.604485Z","shell.execute_reply":"2022-02-22T22:47:33.608927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain = pd.read_parquet(args.data_path.joinpath(\"train_low_mem.parquet\"))\nassert train.isnull().any().sum() == 0, \"null exists.\"\nassert train.row_id.str.extract(r\"(?P<time_id>\\d+)_(?P<investment_id>\\d+)\").astype(train.time_id.dtype).equals(train[[\"time_id\", \"investment_id\"]]), \"row_id!=time_id_investment_id\"\n\nif args.min_time_id is not None:\n    train = train.query(\"time_id>=@args.min_time_id\").reset_index(drop=True)\n    gc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-22T22:47:33.611054Z","iopub.execute_input":"2022-02-22T22:47:33.611743Z","iopub.status.idle":"2022-02-22T22:48:24.946601Z","shell.execute_reply.started":"2022-02-22T22:47:33.611691Z","shell.execute_reply":"2022-02-22T22:48:24.944762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# StratifiedKFold by time_span","metadata":{}},{"cell_type":"code","source":"cat_features = [\"investment_id\"]\ncat_idxs = [ i for i, f in enumerate(train.columns.tolist()) if f in cat_features]\ndel train","metadata":{"execution":{"iopub.status.busy":"2022-02-22T22:48:24.948151Z","iopub.execute_input":"2022-02-22T22:48:24.948529Z","iopub.status.idle":"2022-02-22T22:48:24.95498Z","shell.execute_reply.started":"2022-02-22T22:48:24.948485Z","shell.execute_reply":"2022-02-22T22:48:24.953751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\nimport torch\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\nfrom sklearn.metrics import mean_squared_error\n","metadata":{"execution":{"iopub.status.busy":"2022-02-22T22:48:24.956714Z","iopub.execute_input":"2022-02-22T22:48:24.957029Z","iopub.status.idle":"2022-02-22T22:48:24.965063Z","shell.execute_reply.started":"2022-02-22T22:48:24.956989Z","shell.execute_reply":"2022-02-22T22:48:24.96425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds = 1","metadata":{"execution":{"iopub.status.busy":"2022-02-22T22:48:24.96652Z","iopub.execute_input":"2022-02-22T22:48:24.967095Z","iopub.status.idle":"2022-02-22T22:48:24.972993Z","shell.execute_reply.started":"2022-02-22T22:48:24.967058Z","shell.execute_reply":"2022-02-22T22:48:24.972268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport zipfile\n \ndef zipDir(dirpath, outFullName):\n\n    zip = zipfile.ZipFile(outFullName, \"w\", zipfile.ZIP_DEFLATED)\n    for path, dirnames, filenames in os.walk(dirpath):\n\n        fpath = path.replace(dirpath, '')\n\n        for filename in filenames:\n            zip.write(os.path.join(path, filename), os.path.join(fpath, filename))\n    zip.close()\n    \n\nfor fold in range(folds):\n    input_path =f'../input/tabnet-res-fold1/'\n    output_path = f\"./fold{fold}.zip\"\n    zipDir(input_path, output_path)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-22T22:48:24.974573Z","iopub.execute_input":"2022-02-22T22:48:24.975152Z","iopub.status.idle":"2022-02-22T22:48:25.021607Z","shell.execute_reply.started":"2022-02-22T22:48:24.975114Z","shell.execute_reply":"2022-02-22T22:48:25.020933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Infer","metadata":{}},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()  \niter_test = env.iter_test()\ntabnet_params = dict(\n        cat_idxs=cat_idxs,\n        cat_emb_dim=1,\n        n_d = 16,\n        n_a = 16,\n        n_steps = 2,\n        gamma =1.4690246460970766,\n        n_independent = 9,\n        n_shared = 4,\n        lambda_sparse = 0,\n        optimizer_fn = Adam,\n        optimizer_params = dict(lr = (0.024907164557092944)),\n        mask_type = \"entmax\",\n        scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n        scheduler_fn = CosineAnnealingWarmRestarts,\n        seed = 42,\n        verbose = 10, \n    )    \n\n\n\nimport copy\nclf =  TabNetRegressor(**tabnet_params)\nmodels = []\nfor fold in range(folds):\n    clf.load_model(f\"./fold0.zip\")\n    model=copy.deepcopy(clf)\n    models.append(model)\n    \n\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df.drop([\"row_id\", \"row_id\"], axis=1, inplace=True)\n    final_pred = [models[fold].predict(test_df.values) for fold in range(folds)]\n    sample_prediction_df['target'] = np.mean(np.stack(final_pred), axis=0)\n    env.predict(sample_prediction_df) ","metadata":{"execution":{"iopub.status.busy":"2022-02-22T22:48:25.025352Z","iopub.execute_input":"2022-02-22T22:48:25.025548Z","iopub.status.idle":"2022-02-22T22:48:29.415683Z","shell.execute_reply.started":"2022-02-22T22:48:25.025524Z","shell.execute_reply":"2022-02-22T22:48:29.414769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}