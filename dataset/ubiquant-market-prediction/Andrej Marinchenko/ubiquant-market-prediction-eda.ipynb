{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ubiquant Market Prediction\nTwitch Stream EDA.\n\n1. This notebook was create during a live coding session on twitch. follow for past and future broadcasts here: [here](https://www.twitch.tv/medallionstallion_) ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom itertools import cycle\nimport gc\n\nplt.style.use(\"ggplot\")\ncolor_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ncolor_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:13:35.225703Z","iopub.execute_input":"2022-01-23T16:13:35.226201Z","iopub.status.idle":"2022-01-23T16:13:36.028554Z","shell.execute_reply.started":"2022-01-23T16:13:35.226129Z","shell.execute_reply":"2022-01-23T16:13:36.027476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Data\nNote that the training data is roughly 18.55 Gb in size. This is too large to load into memory directly in kaggle notebook.\n\nSome things to note when exploring the entire dataset on a local machine:\n- There are 3579 unique `investment_id`s\n- There are 1211 unique `time_id`s - we are told these are not equally spaced and could be different in the test set.\n- The features columns are mostly normalized with a mean value close to 0 and standard deviation of ~1.\n","metadata":{}},{"cell_type":"code","source":"!ls -GFlash ../input/ubiquant-market-prediction/","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:13:36.034133Z","iopub.execute_input":"2022-01-23T16:13:36.034485Z","iopub.status.idle":"2022-01-23T16:13:36.884245Z","shell.execute_reply.started":"2022-01-23T16:13:36.034354Z","shell.execute_reply":"2022-01-23T16:13:36.883078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading the Parquet Version\nReading in csvs can be slow. Instead read from the parquet version here:\n- https://www.kaggle.com/robikscube/ubiquant-parquet","metadata":{}},{"cell_type":"code","source":"train = pd.read_parquet('../input/ubiquant-parquet/train.parquet',\n               columns=['time_id','investment_id','target','f_1','f_2','f_3'])\ntest = pd.read_parquet('../input/ubiquant-parquet/example_test.parquet')\nss = pd.read_parquet('../input/ubiquant-parquet/example_sample_submission.parquet')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:13:36.886102Z","iopub.execute_input":"2022-01-23T16:13:36.88738Z","iopub.status.idle":"2022-01-23T16:13:38.188605Z","shell.execute_reply.started":"2022-01-23T16:13:36.887316Z","shell.execute_reply":"2022-01-23T16:13:38.187422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Getting an idea of how many observations, assets and time steps'''\n\nobs = train.shape[0]\nprint(f\"Number of observations: {obs}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:13:38.191297Z","iopub.execute_input":"2022-01-23T16:13:38.191535Z","iopub.status.idle":"2022-01-23T16:13:38.203641Z","shell.execute_reply.started":"2022-01-23T16:13:38.191507Z","shell.execute_reply":"2022-01-23T16:13:38.201718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_time_ids = train['time_id'].nunique()\nunique_inv_ids = train['investment_id'].nunique()\n\nprint(f'There are {unique_inv_ids} unique investment ids and {unique_time_ids} unique time ids')\n\nprint(f\"Number of assets: {unique_time_ids} (range from {train.investment_id.min()} to {train.investment_id.max()})\")","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:13:38.205094Z","iopub.execute_input":"2022-01-23T16:13:38.205339Z","iopub.status.idle":"2022-01-23T16:13:38.341291Z","shell.execute_reply.started":"2022-01-23T16:13:38.20531Z","shell.execute_reply":"2022-01-23T16:13:38.340002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read in a single invesment_id","metadata":{}},{"cell_type":"code","source":"example = pd.read_parquet('../input/ubiquant-parquet/investment_ids/1.parquet')\nexample.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:13:38.343041Z","iopub.execute_input":"2022-01-23T16:13:38.34331Z","iopub.status.idle":"2022-01-23T16:13:38.455706Z","shell.execute_reply.started":"2022-01-23T16:13:38.343265Z","shell.execute_reply":"2022-01-23T16:13:38.454553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Data Fields\n\ntl;dr - we have time series data but don't know the exact time periods being provided. We also have investment_ids that are not unique. Everything is anonymized so it's not easy to create features.\n\n- `row_id` - A unique identifier for the row.\n- `time_id` - The ID code for the time the data was gathered. The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set.\n- `investment_id` - The ID code for an investment. Not all investment have data in all time IDs.\n- `target` - The target.\n- `features` - [f_0:f_299] - Anonymized features generated from market data.","metadata":{}},{"cell_type":"markdown","source":"# Example of Features for a Single Investment ID\n- We are only looking at 3 of the features.","metadata":{}},{"cell_type":"code","source":"example_id = train.query('investment_id == 529')\nsns.pairplot(example_id,\n             vars=['f_1','f_2','f_3','target'],\n            hue='time_id')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:13:40.116556Z","iopub.execute_input":"2022-01-23T16:13:40.116848Z","iopub.status.idle":"2022-01-23T16:13:57.100678Z","shell.execute_reply.started":"2022-01-23T16:13:40.116814Z","shell.execute_reply":"2022-01-23T16:13:57.099444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target analysis","metadata":{}},{"cell_type":"code","source":"'''The target: investment return rate (IRR)'''\nplt.figure(figsize = (12,5))\nax = sns.distplot(train['target'], bins=1000)\nplt.xlim(-3,3)\nplt.xlabel(\"Histogram of the IRR values\", size=18)\nplt.show();\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:13:57.102586Z","iopub.execute_input":"2022-01-23T16:13:57.103208Z","iopub.status.idle":"2022-01-23T16:14:10.808521Z","shell.execute_reply.started":"2022-01-23T16:13:57.103162Z","shell.execute_reply":"2022-01-23T16:14:10.807124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target values look quite normal without any outliers or long tails. We should not have any problems working with it. \n\n\nAssuming an uniform investment (all investment have the same weight), the overall investment is in loss ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\n\nfor i in range(5):\n    plt.subplot(5,1,i+1)\n    cumReturn = train.loc[train['investment_id']==i,'target'].cumsum()\n    time_id = train.loc[train['investment_id']==i,'time_id']\n    plt.plot(time_id, cumReturn, color='green', lw=2);\n    plt.ylabel (f'investment_id {i}', fontsize=18);\n    plt.title(f'investment_id {i}  time dependency', size=18)\n\nplt.xlabel ('Time_id', fontsize=18)\n\ndel cumReturn, time_id\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:14:10.810347Z","iopub.execute_input":"2022-01-23T16:14:10.810681Z","iopub.status.idle":"2022-01-23T16:14:11.93222Z","shell.execute_reply.started":"2022-01-23T16:14:10.810637Z","shell.execute_reply":"2022-01-23T16:14:11.93112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for investment_id in range(5):\n    d = train.query('investment_id == @investment_id')\n    d.set_index('time_id')['target'] \\\n        .plot(figsize=(15, 5),\n              title=f'Investment_id {investment_id}',\n              color=next(color_cycle),\n              style='.-')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:37:29.653081Z","iopub.execute_input":"2022-01-23T16:37:29.653451Z","iopub.status.idle":"2022-01-23T16:37:30.938223Z","shell.execute_reply.started":"2022-01-23T16:37:29.653418Z","shell.execute_reply":"2022-01-23T16:37:30.937036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selection = train.groupby(\"investment_id\").time_id.max()\noutlier_inv_ids = selection[selection != 1219].index.values\n\nplt.figure(figsize=(20,5))\nfor n in range(10):\n    plt.plot(train[train.investment_id == outlier_inv_ids[n]].time_id,\n               train[train.investment_id == outlier_inv_ids[n]].target.cumsum(), '.')\n    plt.xlim([0,1220])\n    plt.title(\"Return/target cumsum for outlier investments\")\n    plt.xlabel(\"time_id\")\n    plt.ylabel(\"cumsum return\");","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:36:31.210461Z","iopub.execute_input":"2022-01-23T16:36:31.210905Z","iopub.status.idle":"2022-01-23T16:36:31.709148Z","shell.execute_reply.started":"2022-01-23T16:36:31.210867Z","shell.execute_reply":"2022-01-23T16:36:31.707959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Have you noticed that some of the data we have is missing, this can be judged by the long smooth connected areas without hesitation.\n\n- We can clearly see that some investments miss parts of their timeseries or end earlier.\n- Looking back into the competition description, we find: \"The ID code for an investment. Not all investment have data in all time IDs.\"","metadata":{}},{"cell_type":"code","source":"print('timestamps in our data', train.time_id.unique())\nprint('the total number of timestamps in our data = ', len(train.time_id.unique()))\n\nmissing_time_ids = []\nfor t in range(1220):\n    if t not in train.time_id.unique():\n        missing_time_ids.append(t)\n        \nprint('Missing time_ids: ', missing_time_ids)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:14:11.933719Z","iopub.execute_input":"2022-01-23T16:14:11.934291Z","iopub.status.idle":"2022-01-23T16:14:34.111573Z","shell.execute_reply.started":"2022-01-23T16:14:11.934249Z","shell.execute_reply":"2022-01-23T16:14:34.110336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"time_id: The ID code for the time the data was gathered. The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set.\n\nYes the IDs are in order from 0-1219 with 8 missing (?) time_ids.\n\nOne time id may belong to 1st Jan 2:00 IST, the next one can be 4th Jan 12:00 IST, the other one 5th Jan 16:00 IST and so on.\n\nClearly the number of data points (rows) in each time_id is not constant.\n\nThe following time_ids are not present. I don't think it should be an issue since we anyway don't have a constant gap between consecutive time_ids.","metadata":{}},{"cell_type":"code","source":"obs_by_asset = train.groupby(['investment_id'])['target'].count()\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nobs_by_asset.plot.hist(bins=60)\nplt.title(\"target by asset distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:14:34.113096Z","iopub.execute_input":"2022-01-23T16:14:34.114164Z","iopub.status.idle":"2022-01-23T16:14:34.614029Z","shell.execute_reply.started":"2022-01-23T16:14:34.114115Z","shell.execute_reply":"2022-01-23T16:14:34.613053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Assets are distributed in a different way, there are assets that are actually more frequently observed and others that are not. A good cv and modelling strategy should keep this into account (stratify if you are working with subsamples).","metadata":{}},{"cell_type":"code","source":"mean_target = train.groupby(['investment_id'])['target'].mean()\nmean_mean_target = np.mean(mean_target)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nmean_target.plot.hist(bins=60)\nplt.title(\"mean target distribution\")\nplt.show()\n\nprint(f\"Mean of mean target: {mean_mean_target: 0.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:14:34.617836Z","iopub.execute_input":"2022-01-23T16:14:34.618136Z","iopub.status.idle":"2022-01-23T16:14:35.093888Z","shell.execute_reply.started":"2022-01-23T16:14:34.6181Z","shell.execute_reply":"2022-01-23T16:14:35.092984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The average of mean target by asset show a bell-shaped distribution, beware that there are outliers, anyway, because there are some assets with quite negative average target (-0.4 area) and some quite positive ones (+0.8 area). Overall the average mean target by asset is slightly negative (-0.0231)","metadata":{}},{"cell_type":"code","source":"sts_target = train.groupby(['investment_id'])['target'].std()\nmean_std_target = np.mean(sts_target)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nsts_target.plot.hist(bins=60)\nplt.title(\"standard deviation of target distribution\")\nplt.show()\n\nprint(f\"Mean of std target: {mean_std_target: 0.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:14:35.095233Z","iopub.execute_input":"2022-01-23T16:14:35.095482Z","iopub.status.idle":"2022-01-23T16:14:35.561343Z","shell.execute_reply.started":"2022-01-23T16:14:35.095449Z","shell.execute_reply":"2022-01-23T16:14:35.560781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also the average of mean standard deviation (std) by asset presents some interesting patterns. First of all, it is skewed toward the right, with some assets having more std (up to 2.5). On the other side there are also some few assets with std almost at zero.","metadata":{}},{"cell_type":"code","source":"ax = sns.jointplot(x=obs_by_asset, y=mean_target, kind=\"reg\", \n                   height=8, joint_kws={'line_kws':{'color':'blue'}})\nax.ax_joint.set_xlabel('observations')\nax.ax_joint.set_ylabel('mean target')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:14:35.562418Z","iopub.execute_input":"2022-01-23T16:14:35.562819Z","iopub.status.idle":"2022-01-23T16:14:36.886058Z","shell.execute_reply.started":"2022-01-23T16:14:35.562774Z","shell.execute_reply":"2022-01-23T16:14:36.885224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By jointly plotting the distribution of observartions by asset and the mean target value by asset, we may notice that the target value slightly reduces proportionally to the number of observation. The dispersion of values tends to grow with less observations, hence we need to re-plot the scatterplot this time using the standard deviation.","metadata":{}},{"cell_type":"code","source":"qx = sns.jointplot(x=obs_by_asset.values, y=sts_target, kind=\"reg\", \n                   height=8, joint_kws={'line_kws':{'color':'blue'}})\nax.ax_joint.set_xlabel('observations')\nax.ax_joint.set_ylabel('std target')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:14:36.887481Z","iopub.execute_input":"2022-01-23T16:14:36.887833Z","iopub.status.idle":"2022-01-23T16:14:38.046355Z","shell.execute_reply.started":"2022-01-23T16:14:36.887788Z","shell.execute_reply":"2022-01-23T16:14:38.045299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The new scatterplot reveals that the less the observations, imply a much more uncertainty in the mean target. ","metadata":{}},{"cell_type":"markdown","source":"Strategy: in training you need to control this effect by expliciting the number of observations because this is predictive of the uncertainty of the predictions. In the test phase, instead, when you are working with an asset that you don't know about, you need to impute an average number of observations, thus expecting an average dispersion of predictions for that asset.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12, 6))\ntrain.groupby('time_id')['investment_id'].nunique().plot()\nplt.title(\"number of unique assets by time\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:14:38.04793Z","iopub.execute_input":"2022-01-23T16:14:38.048168Z","iopub.status.idle":"2022-01-23T16:14:38.86312Z","shell.execute_reply.started":"2022-01-23T16:14:38.048138Z","shell.execute_reply":"2022-01-23T16:14:38.861957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_investments_per_time_id = train.groupby(\"time_id\").investment_id.nunique()\n\nplt.figure(figsize=(20,5))\nplt.plot(num_investments_per_time_id.index, num_investments_per_time_id.values, 'o')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:42:11.598657Z","iopub.execute_input":"2022-01-23T16:42:11.599066Z","iopub.status.idle":"2022-01-23T16:42:12.481196Z","shell.execute_reply.started":"2022-01-23T16:42:11.59903Z","shell.execute_reply":"2022-01-23T16:42:12.48017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we have reasoned how the investments with less observations seem more risky, we notice how the number of the assets present at each time step is quite different and also highly oscillating. By the end of the avaliable time, the number of assets has grown by one third. We can see that the number of investments given the time id varies especially around the id 400.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12, 6))\ntrain.groupby('time_id')['investment_id'].nunique().plot()\nplt.title(\"number of unique assets by time\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:14:38.864732Z","iopub.execute_input":"2022-01-23T16:14:38.865321Z","iopub.status.idle":"2022-01-23T16:14:39.636634Z","shell.execute_reply.started":"2022-01-23T16:14:38.865284Z","shell.execute_reply":"2022-01-23T16:14:39.635811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12, 6))\ntrain.groupby('time_id')['target'].mean().plot()\nplt.title(\"average target by time\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:14:39.63806Z","iopub.execute_input":"2022-01-23T16:14:39.639984Z","iopub.status.idle":"2022-01-23T16:14:39.952388Z","shell.execute_reply.started":"2022-01-23T16:14:39.63993Z","shell.execute_reply":"2022-01-23T16:14:39.951529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12, 6))\ntrain.groupby('time_id')['target'].std().plot()\nplt.title(\"average target by time\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:14:39.953939Z","iopub.execute_input":"2022-01-23T16:14:39.954419Z","iopub.status.idle":"2022-01-23T16:14:40.274418Z","shell.execute_reply.started":"2022-01-23T16:14:39.954374Z","shell.execute_reply":"2022-01-23T16:14:40.273764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r = np.corrcoef(train.groupby('time_id')['investment_id'].nunique(), train.groupby('time_id')['target'].mean())[0][1]\nprint(f\"Correlation of number of assets by target: {r:0.3f}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:14:40.275522Z","iopub.execute_input":"2022-01-23T16:14:40.275971Z","iopub.status.idle":"2022-01-23T16:14:40.907309Z","shell.execute_reply.started":"2022-01-23T16:14:40.275923Z","shell.execute_reply":"2022-01-23T16:14:40.906168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(20,5))\nax[0].plot(train[train.investment_id==4].target.cumsum())\nax[1].plot(train[train.investment_id==4].f_3.cumsum())","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:43:30.917859Z","iopub.execute_input":"2022-01-23T16:43:30.918284Z","iopub.status.idle":"2022-01-23T16:43:31.286585Z","shell.execute_reply.started":"2022-01-23T16:43:30.918245Z","shell.execute_reply":"2022-01-23T16:43:31.285552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we plot the number of assets by time alongside the average target by time, it becomes evident that when there are less assets, the target oscillates more with prevalently higher targets. The correlation of assets number and target is negative, in fact. ","metadata":{}},{"cell_type":"markdown","source":"### Features interaction\nWe will do analysis on a smaller random 1% samle of the dataset to speed up the process.","metadata":{}},{"cell_type":"code","source":"data_types_dict = {\n    'time_id': 'int32',\n    'investment_id': 'int16',\n    \"target\": 'float16',\n}\n\nfeatures = [f'f_{i}' for i in range(300)]\n\nfor f in features:\n    data_types_dict[f] = 'float16'\n    \ntarget = 'target'\n\ntrain_df = pd.read_csv('/kaggle/input/ubiquant-market-prediction/train.csv', \n                       usecols = data_types_dict.keys(),\n                       dtype=data_types_dict, \n                       index_col = 0)\n\nsample_df = train_df.sample(frac = 0.01)\nsample_df","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:27:03.586801Z","iopub.execute_input":"2022-01-23T16:27:03.587215Z","iopub.status.idle":"2022-01-23T16:34:05.053549Z","shell.execute_reply.started":"2022-01-23T16:27:03.587162Z","shell.execute_reply":"2022-01-23T16:34:05.052053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation = sample_df[[target] + features].corr()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:34:05.055463Z","iopub.execute_input":"2022-01-23T16:34:05.055785Z","iopub.status.idle":"2022-01-23T16:34:12.946068Z","shell.execute_reply.started":"2022-01-23T16:34:05.055739Z","shell.execute_reply":"2022-01-23T16:34:12.945034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation['target'].iloc[1:].hist(bins = 20, figsize = (20,10))","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:34:12.948617Z","iopub.execute_input":"2022-01-23T16:34:12.948938Z","iopub.status.idle":"2022-01-23T16:34:13.293467Z","shell.execute_reply.started":"2022-01-23T16:34:12.948899Z","shell.execute_reply":"2022-01-23T16:34:13.292128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.clustermap(correlation, figsize=(20, 20))","metadata":{"execution":{"iopub.status.busy":"2022-01-23T16:34:13.295272Z","iopub.execute_input":"2022-01-23T16:34:13.295549Z","iopub.status.idle":"2022-01-23T16:34:17.03781Z","shell.execute_reply.started":"2022-01-23T16:34:13.295518Z","shell.execute_reply":"2022-01-23T16:34:17.036519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are definitely some clusters of highly correlated features that can be later analyzed together.","metadata":{}}]}