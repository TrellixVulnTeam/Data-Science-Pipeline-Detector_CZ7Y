{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Infer Transformer\n\n\nThe training code is [here](https://www.kaggle.com/takamichitoda/ump-train-transformer-on-tpu?scriptVersionId=86363969), and standerd scaler model is [here](https://www.kaggle.com/takamichitoda/ump-npy-dataset).","metadata":{}},{"cell_type":"code","source":"import gc\nimport pickle\nimport numpy as np\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport ubiquant\n\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-07T06:12:52.423589Z","iopub.execute_input":"2022-02-07T06:12:52.424063Z","iopub.status.idle":"2022-02-07T06:12:59.746188Z","shell.execute_reply.started":"2022-02-07T06:12:52.42397Z","shell.execute_reply":"2022-02-07T06:12:59.74505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GCF:\n    MODEL_ROOT = \"/kaggle/input/ump-train-transformer-on-tpu\"\n    N_FOLDS = 5\n    FEAT_COLS = [f\"f_{i}\" for i in range(300)]\n    SCALER_PATH = \"/kaggle/input/ump-npy-dataset/std_scaler.pkl\"\n    \n    # Transformer Parameters\n    EMBED_DIM = 64//2\n    N_HEAD = 8\n    FF_DIM = 128//2\n    DROPOUT = 0.0\n    N_BLOCK = 4","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:12:59.747784Z","iopub.execute_input":"2022-02-07T06:12:59.748506Z","iopub.status.idle":"2022-02-07T06:12:59.76012Z","shell.execute_reply.started":"2022-02-07T06:12:59.748458Z","shell.execute_reply":"2022-02-07T06:12:59.756031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_dim = 300\n# https://www.kaggle.com/pratikskarnik/riiid-keras-transformer-starter\nclass MultiHeadSelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8, **kwargs):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        if embed_dim % num_heads != 0:\n            raise ValueError(\n                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n            )\n        self.projection_dim = embed_dim // num_heads\n        self.query_dense = layers.Dense(embed_dim)\n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        self.combine_heads = layers.Dense(embed_dim)\n\n    def attention(self, query, key, value):\n        score = tf.matmul(query, key, transpose_b=True)\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n        scaled_score = score / tf.math.sqrt(dim_key)\n        weights = tf.nn.softmax(scaled_score, axis=-1)\n        output = tf.matmul(weights, value)\n        return output, weights\n\n    def separate_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, inputs):\n        # x.shape = [batch_size, seq_len, embedding_dim]\n        batch_size = tf.shape(inputs)[0]\n        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        query = self.separate_heads(\n            query, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        key = self.separate_heads(\n            key, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        value = self.separate_heads(\n            value, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        attention, weights = self.attention(query, key, value)\n        attention = tf.transpose(\n            attention, perm=[0, 2, 1, 3]\n        )  # (batch_size, seq_len, num_heads, projection_dim)\n        concat_attention = tf.reshape(\n            attention, (batch_size, -1, self.embed_dim)\n        )  # (batch_size, seq_len, embed_dim)\n        output = self.combine_heads(\n            concat_attention\n        )  # (batch_size, seq_len, embed_dim)\n        return output\n    \n    def get_config(self):\n        config = {\n            \"embed_dim\" : self.embed_dim,\n            \"num_heads\" : self.num_heads,\n            \"projection_dim\" : self.projection_dim,\n            \"query_dense\" : self.query_dense,\n            \"key_dense\" : self.key_dense,\n            \"value_dense\" : self.value_dense,\n            \"combine_heads\" : self.combine_heads,\n        }\n        base_config = super(MultiHeadSelfAttention, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))  \n\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim=GCF.EMBED_DIM, feat_dim=feat_dim, num_heads=GCF.N_HEAD, ff_dim=GCF.FF_DIM, rate=GCF.DROPOUT, **kwargs):\n        super(TransformerBlock, self).__init__()\n        #self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.att = MultiHeadSelfAttention(num_heads=num_heads, embed_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            #[layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(feat_dim),]\n            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        #attn_output = self.att(inputs, inputs)\n        attn_output = self.att(inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n    \n    def get_config(self):\n        config = {\n            \"att\" : self.att,\n            \"ffn\" : self.ffn,\n            \"layernorm1\" : self.layernorm1,\n            \"layernorm2\" : self.layernorm2,\n            \"dropout1\" : self.dropout1,\n            \"dropout2\" : self.dropout2,\n        }\n        base_config = super(TransformerBlock, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:12:59.763894Z","iopub.execute_input":"2022-02-07T06:12:59.76436Z","iopub.status.idle":"2022-02-07T06:12:59.789385Z","shell.execute_reply.started":"2022-02-07T06:12:59.764324Z","shell.execute_reply":"2022-02-07T06:12:59.788663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.load_model(f\"{GCF.MODEL_ROOT}/ump_transformer_all_train.h5\", compile=False,\n                                   custom_objects={\n                                       \"MultiHeadSelfAttention\": MultiHeadSelfAttention,\n                                       \"TransformerBlock\": TransformerBlock,\n                                   })\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:12:59.792868Z","iopub.execute_input":"2022-02-07T06:12:59.793741Z","iopub.status.idle":"2022-02-07T06:13:01.326026Z","shell.execute_reply.started":"2022-02-07T06:12:59.793703Z","shell.execute_reply":"2022-02-07T06:13:01.325301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = pickle.load(open(GCF.SCALER_PATH, \"rb\"))\nscaler","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:13:01.32714Z","iopub.execute_input":"2022-02-07T06:13:01.327552Z","iopub.status.idle":"2022-02-07T06:13:01.34346Z","shell.execute_reply.started":"2022-02-07T06:13:01.327504Z","shell.execute_reply":"2022-02-07T06:13:01.342844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nenv = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\nfor (test_df, sample_prediction_df) in iter_test:\n    x = scaler.transform(test_df[GCF.FEAT_COLS].values)\n    \n    with tf.device('/GPU:0'):\n        #pred = model.predict(np.expand_dims(x, axis=1))\n        pred = np.stack([model(np.expand_dims(x, axis=1), training=True).numpy() for _ in range(100)]).mean(0)[:, 0]\n    \n    sample_prediction_df['target'] = pred  # make your predictions here\n    env.predict(sample_prediction_df)   # register your predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:13:01.344731Z","iopub.execute_input":"2022-02-07T06:13:01.344978Z","iopub.status.idle":"2022-02-07T06:13:01.676162Z","shell.execute_reply.started":"2022-02-07T06:13:01.344945Z","shell.execute_reply":"2022-02-07T06:13:01.675496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_prediction_df","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:13:01.677534Z","iopub.execute_input":"2022-02-07T06:13:01.677805Z","iopub.status.idle":"2022-02-07T06:13:01.690937Z","shell.execute_reply.started":"2022-02-07T06:13:01.677769Z","shell.execute_reply":"2022-02-07T06:13:01.690284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}