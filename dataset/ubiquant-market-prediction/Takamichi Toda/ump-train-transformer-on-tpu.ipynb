{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Train Transformer on TPU\n\nThis code train Transformer model.\n\nTo save time and memory, I converted train.csv to a numpy array beforehand. ([dataset link](https://www.kaggle.com/takamichitoda/ump-npy-dataset))\n\nThis dataset made from [this notebook](https://www.kaggle.com/takamichitoda/ump-train-csv-to-npy).  \n\nThe model architecture was based on [Mr.Chris](https://www.kaggle.com/cdeotte)'s notebook.  \nhttps://www.kaggle.com/cdeotte/tensorflow-transformer-0-112\n\nThanks:)\n\nupdate:\n- Version 2: dropout=0.2\n- Version 3: time series holdout\n- Version 7: ALL_TRAIN_ADD_EPOCH=5\n- Version 8: ALL_TRAIN_ADD_EPOCH=7\n- Version 9: ALL_TRAIN_ADD_EPOCH=3\n- Version 10: harf params","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport random\nfrom matplotlib import pyplot as plt\n\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import TimeSeriesSplit\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.keras import backend as K\n\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nprint('Running on TPU ', tpu.master())\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:17:32.02274Z","iopub.execute_input":"2022-02-07T06:17:32.023708Z","iopub.status.idle":"2022-02-07T06:17:43.509443Z","shell.execute_reply.started":"2022-02-07T06:17:32.023582Z","shell.execute_reply":"2022-02-07T06:17:43.508473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GCF:\n    INPUT_ROOT = \"/kaggle/input/ump-npy-dataset/\"\n    LAG_FEATURES = \"/kaggle/input/ump-lag-freatures/target_shift_1.npy\"\n    SEED = 0\n    \n    N_EPOCHS = 1000\n    BATCH_SIZE = 4096\n    EARLY_STOPPING_PATIENCE = 10\n    EARLY_STOPPING_MIN_DELTA = 1e-3\n    ALL_TRAIN_ADD_EPOCH = 3\n    \n    # Transformer Parameters\n    EMBED_DIM = 64//2\n    N_HEAD = 8\n    FF_DIM = 128//2\n    DROPOUT = 0.0\n    N_BLOCK = 4","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:37:50.083346Z","iopub.execute_input":"2022-02-07T06:37:50.083845Z","iopub.status.idle":"2022-02-07T06:37:50.090099Z","shell.execute_reply.started":"2022-02-07T06:37:50.08381Z","shell.execute_reply":"2022-02-07T06:37:50.089473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=GCF.SEED):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:27:32.369165Z","iopub.execute_input":"2022-02-07T06:27:32.369481Z","iopub.status.idle":"2022-02-07T06:27:32.37487Z","shell.execute_reply.started":"2022-02-07T06:27:32.369451Z","shell.execute_reply":"2022-02-07T06:27:32.373903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nX = np.load(f\"{GCF.INPUT_ROOT}/features_std_scaled.npy\")\ny = np.load(f\"{GCF.INPUT_ROOT}/targets.npy\")\ntime_id = np.load(f\"{GCF.INPUT_ROOT}/time_id.npy\")","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:27:33.453913Z","iopub.execute_input":"2022-02-07T06:27:33.454289Z","iopub.status.idle":"2022-02-07T06:28:00.95324Z","shell.execute_reply.started":"2022-02-07T06:27:33.454244Z","shell.execute_reply":"2022-02-07T06:28:00.952365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"is_train = np.where((time_id <= 1066) & (time_id > 700), True, False)\nis_test = time_id > 1066\n\nsum(is_train), sum(is_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:28:00.954525Z","iopub.execute_input":"2022-02-07T06:28:00.954956Z","iopub.status.idle":"2022-02-07T06:28:24.34557Z","shell.execute_reply.started":"2022-02-07T06:28:00.954915Z","shell.execute_reply":"2022-02-07T06:28:24.344635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/c/ubiquant-market-prediction/discussion/302977\n\ndef correlationMetric(x, y, axis=-2):\n    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xvar * yvar)\n    return tf.constant(1.0, dtype=x.dtype) - corr\n\n\ndef correlationLoss(x,y, axis=-2):\n    \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n    while trying to have the same mean and variance\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xsqsum * ysqsum)\n    sqdif = tf.reduce_sum(tf.math.squared_difference(x, y), axis=axis) / n / tf.sqrt(ysqsum / n)\n    return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr + (0.01 * sqdif)) , dtype=tf.float32 )\n\n\n#ã€€https://www.kaggle.com/c/ubiquant-market-prediction/discussion/301987\ndef pearson_coef(data):\n    return data.corr()['target']['preds']\n\ndef comp_metric(time_id, y, pred):\n    return np.mean(\n        pd.DataFrame(np.stack([time_id, y, pred]).T, columns=['time_id', 'target', 'preds']\n    ).groupby('time_id').apply(pearson_coef))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:28:24.347332Z","iopub.execute_input":"2022-02-07T06:28:24.347574Z","iopub.status.idle":"2022-02-07T06:28:24.366194Z","shell.execute_reply.started":"2022-02-07T06:28:24.347549Z","shell.execute_reply":"2022-02-07T06:28:24.365356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_dim = X.shape[-1]\n\n\n# https://www.kaggle.com/pratikskarnik/riiid-keras-transformer-starter\nclass MultiHeadSelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        if embed_dim % num_heads != 0:\n            raise ValueError(\n                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n            )\n        self.projection_dim = embed_dim // num_heads\n        self.query_dense = layers.Dense(embed_dim)\n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        self.combine_heads = layers.Dense(embed_dim)\n\n    def attention(self, query, key, value):\n        score = tf.matmul(query, key, transpose_b=True)\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n        scaled_score = score / tf.math.sqrt(dim_key)\n        weights = tf.nn.softmax(scaled_score, axis=-1)\n        output = tf.matmul(weights, value)\n        return output, weights\n\n    def separate_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, inputs):\n        # x.shape = [batch_size, seq_len, embedding_dim]\n        batch_size = tf.shape(inputs)[0]\n        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        query = self.separate_heads(\n            query, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        key = self.separate_heads(\n            key, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        value = self.separate_heads(\n            value, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        attention, weights = self.attention(query, key, value)\n        attention = tf.transpose(\n            attention, perm=[0, 2, 1, 3]\n        )  # (batch_size, seq_len, num_heads, projection_dim)\n        concat_attention = tf.reshape(\n            attention, (batch_size, -1, self.embed_dim)\n        )  # (batch_size, seq_len, embed_dim)\n        output = self.combine_heads(\n            concat_attention\n        )  # (batch_size, seq_len, embed_dim)\n        return output\n    \n    def get_config(self):\n        config = {\n            \"embed_dim\" : self.embed_dim,\n            \"num_heads\" : self.num_heads,\n            \"projection_dim\" : self.projection_dim,\n            \"query_dense\" : self.query_dense,\n            \"key_dense\" : self.key_dense,\n            \"value_dense\" : self.value_dense,\n            \"combine_heads\" : self.combine_heads,\n        }\n        base_config = super(MultiHeadSelfAttention, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))  \n\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim=GCF.EMBED_DIM, feat_dim=feat_dim, num_heads=GCF.N_HEAD, ff_dim=GCF.FF_DIM, rate=GCF.DROPOUT, **kwargs):\n        super(TransformerBlock, self).__init__()\n        self.att = MultiHeadSelfAttention(num_heads=num_heads, embed_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n    \n    def get_config(self):\n        config = {\n            \"att\" : self.att,\n            \"ffn\" : self.ffn,\n            \"layernorm1\" : self.layernorm1,\n            \"layernorm2\" : self.layernorm2,\n            \"dropout1\" : self.dropout1,\n            \"dropout2\" : self.dropout2,\n        }\n        base_config = super(TransformerBlock, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:28:24.3677Z","iopub.execute_input":"2022-02-07T06:28:24.368213Z","iopub.status.idle":"2022-02-07T06:28:24.396123Z","shell.execute_reply.started":"2022-02-07T06:28:24.368183Z","shell.execute_reply":"2022-02-07T06:28:24.395458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/cdeotte/tensorflow-transformer-0-112\n\ndef create_model():\n    inputs = layers.Input(shape=(1, feat_dim))\n    \n    # \"EMBEDDING LAYER\"\n    x = layers.Dense(GCF.EMBED_DIM)(inputs)\n    x = layers.LayerNormalization(epsilon=1e-6)(x)\n    \n    # TRANSFORMER BLOCKS\n    for k in range(GCF.N_BLOCK):\n        #x_old = x\n        transformer_block = TransformerBlock(GCF.EMBED_DIM, feat_dim, GCF.N_HEAD, GCF.FF_DIM, GCF.DROPOUT)\n        x = transformer_block(x)\n        #x = 0.7*x + 0.3*x_old # SKIP CONNECTION\n    \n    x = layers.GlobalAveragePooling1D()(x)\n    #x = layers.Dropout(0.2)(x)\n    x = layers.Dense(20, activation=\"relu\")(x)\n    #x = layers.Dropout(0.2)(x)\n    \n    # REGRESSION HEAD\n    outputs = layers.Dense(1, activation=\"linear\")(x)\n    \n    model = keras.Model(inputs=inputs, outputs=outputs)\n    \n    model.compile(\n        optimizer=tf.optimizers.Adam(1e-4),\n        loss='mse',\n        #loss=correlationLoss,\n        metrics=[keras.metrics.RootMeanSquaredError(), correlationMetric]\n    )\n    return model\n\ncreate_model().summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:37:55.841522Z","iopub.execute_input":"2022-02-07T06:37:55.841871Z","iopub.status.idle":"2022-02-07T06:37:56.836436Z","shell.execute_reply.started":"2022-02-07T06:37:55.841837Z","shell.execute_reply":"2022-02-07T06:37:56.834575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything()\n\nwith strategy.scope():\n    model = create_model()\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_correlationMetric',\n    patience=GCF.EARLY_STOPPING_PATIENCE,\n    min_delta=GCF.EARLY_STOPPING_MIN_DELTA,\n    restore_best_weights=True,\n)\nreduce_lr = ReduceLROnPlateau(\n                    monitor='val_correlationMetric',\n                    factor=0.5,\n                    patience=3,\n                    min_lr=1e-5,\n                    verbose=1\n)\n\nhistory = model.fit(\n    np.expand_dims(X[is_train, :], axis=1), y[is_train],\n    validation_data=(np.expand_dims(X[is_test, :], axis=1), y[is_test]),\n    batch_size=GCF.BATCH_SIZE,\n    epochs=GCF.N_EPOCHS,\n    callbacks=[early_stopping, reduce_lr],\n)\n\nmodel.save(f\"ump_transformer_holdout.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:37:59.168488Z","iopub.execute_input":"2022-02-07T06:37:59.168792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = [h.replace(\"val_\", \"\") for h in history.history.keys() if 'val' in h]\nfor c in cols:\n    pd.DataFrame(history.history)[[c, \"val_\"+c]].plot()\n    plt.title(c)\n    plt.show()\n    \npd.DataFrame(history.history)['lr'].plot()\nplt.title('lr')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:33:25.466586Z","iopub.execute_input":"2022-02-07T06:33:25.467019Z","iopub.status.idle":"2022-02-07T06:33:26.371079Z","shell.execute_reply.started":"2022-02-07T06:33:25.466986Z","shell.execute_reply":"2022-02-07T06:33:26.370138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_pred = model.predict(np.expand_dims(X[is_test, :], 1))\nvalid_pred = valid_pred[:, 0]\n\nrmse = mean_squared_error(y[is_test], valid_pred, squared=False)\nscore = comp_metric(time_id[is_test], y[is_test], valid_pred)\nprint(f'RMSE={rmse}, SCORE={score}')","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:36:48.721565Z","iopub.execute_input":"2022-02-07T06:36:48.722191Z","iopub.status.idle":"2022-02-07T06:36:48.854412Z","shell.execute_reply.started":"2022-02-07T06:36:48.722147Z","shell.execute_reply":"2022-02-07T06:36:48.85362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_epoch = len(history.history['loss'])\nbest_epoch = run_epoch - GCF.EARLY_STOPPING_PATIENCE\nprint(f\"best epoch is {best_epoch}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:25:56.552105Z","iopub.status.idle":"2022-02-07T06:25:56.552438Z","shell.execute_reply.started":"2022-02-07T06:25:56.552268Z","shell.execute_reply":"2022-02-07T06:25:56.552288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything()\n\nwith strategy.scope():\n    model = create_model()\n\nreduce_lr = ReduceLROnPlateau(\n                    monitor='val_loss',\n                    factor=0.5,\n                    patience=3,\n                    min_lr=1e-5,\n                    verbose=1\n)\n\nhistory = model.fit(\n    np.expand_dims(X[is_train + is_test, :], axis=1), y[is_train + is_test],\n    batch_size=GCF.BATCH_SIZE,\n    epochs=best_epoch + GCF.ALL_TRAIN_ADD_EPOCH,\n    callbacks=[reduce_lr],\n)\n\nmodel.save(f\"ump_transformer_all_train.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:25:56.553847Z","iopub.status.idle":"2022-02-07T06:25:56.554887Z","shell.execute_reply.started":"2022-02-07T06:25:56.554484Z","shell.execute_reply":"2022-02-07T06:25:56.554564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = [h.replace(\"val_\", \"\") for h in history.history.keys() if 'val' in h]\nfor c in cols:\n    pd.DataFrame(history.history)[[c, \"val_\"+c]].plot()\n    plt.title(c)\n    plt.show()\n    \npd.DataFrame(history.history)['lr'].plot()\nplt.title('lr')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:25:56.556094Z","iopub.status.idle":"2022-02-07T06:25:56.556505Z","shell.execute_reply.started":"2022-02-07T06:25:56.556261Z","shell.execute_reply":"2022-02-07T06:25:56.556282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2022-02-07T06:25:56.5579Z","iopub.status.idle":"2022-02-07T06:25:56.558636Z","shell.execute_reply.started":"2022-02-07T06:25:56.55837Z","shell.execute_reply":"2022-02-07T06:25:56.558397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}