{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Prediction with DNN(PRELU)\nI have used deep neural network with sequential arhitecture for ubiquant data.\n\nThe key features are:\n\nSelu activation\n\nInitial learning rate of .001\n\nDecayed learning rate in steps\n\n6 layers each with around 200 neurons\n\nAll features are scaled.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport sklearn\nfrom tensorflow import keras\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport gc","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:59:27.4419Z","iopub.execute_input":"2022-03-26T13:59:27.442238Z","iopub.status.idle":"2022-03-26T13:59:33.358413Z","shell.execute_reply.started":"2022-03-26T13:59:27.442154Z","shell.execute_reply":"2022-03-26T13:59:33.357176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This data is very vast with around 300 columns and 3 million rows. This version of data is available in parquet format which helps reading the data fast. To access it all you need to do is click on the add data and type ubiquant on search option. Once you locate the data just click on Add data.","metadata":{}},{"cell_type":"code","source":"train = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T13:59:33.360176Z","iopub.execute_input":"2022-03-26T13:59:33.360479Z","iopub.status.idle":"2022-03-26T14:00:09.018833Z","shell.execute_reply.started":"2022-03-26T13:59:33.360437Z","shell.execute_reply":"2022-03-26T14:00:09.017731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I found this function useful as it converts this data into a format that makes working around it easier.","metadata":{}},{"cell_type":"code","source":"# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    #start_mem = df.memory_usage().sum() / 1024**2\n    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    #end_mem = df.memory_usage().sum() / 1024**2\n    #print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    #print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\n\ntrain = reduce_mem_usage(train)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:00:09.020294Z","iopub.execute_input":"2022-03-26T14:00:09.020509Z","iopub.status.idle":"2022-03-26T14:03:13.762101Z","shell.execute_reply.started":"2022-03-26T14:00:09.020483Z","shell.execute_reply":"2022-03-26T14:03:13.76045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TRAIN TEST SPLIT","metadata":{}},{"cell_type":"markdown","source":"\nI have tried to split the data according to investment_ids and time_ids in order to get a better representation of data.","metadata":{}},{"cell_type":"code","source":"# Split investment id in 2 lots\nfirst_lot=train['investment_id'].unique()\nfirst_lot=first_lot[0:len(first_lot)//2]\nsecond_lot=first_lot[len(first_lot)//2:len(first_lot)+1]\n\n# Identify the lot in the Dataframe\ntrain['inv_lot']=train['investment_id'].apply(lambda x: 1 if x in first_lot else 2)\n\n# Fix a point in Time_id to split. I have used the quantile method and arbitrarily selected the 75th percentile.\ntile=train['time_id'].quantile(q=0.75, interpolation='lower')\n\n# Creating various splits as per investment_id and time_id\ntrain1=train[(train['inv_lot'] ==1) & (train['time_id']< (tile))]\ntrain2=train[(train['inv_lot']==2) & (train['time_id']>= tile)]\ntrain3=train[(train['inv_lot'] ==1) & (train['time_id']>= tile)]\ntrain4=train[(train['inv_lot'] ==2) & (train['time_id']< tile)]\ntrain5=train[(train['inv_lot'] ==2)]\n\n# Take a look at the length in each dataframe\nprint(len(train1)/len(train),len(train2)/len(train),len(train3)/len(train),len(train4)/len(train),len(train5)/len(train) )\n\n# Creation of test and train  sets of features\ntrain=pd.concat([train1,train5], axis=0)\ntest=pd.concat([train2,train3], axis=0)\n# Creation of test and train  sets of target variable\ny_train=train[['target']]\ny_test=test[['target']]","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:03:13.764972Z","iopub.execute_input":"2022-03-26T14:03:13.765441Z","iopub.status.idle":"2022-03-26T14:04:18.701103Z","shell.execute_reply.started":"2022-03-26T14:03:13.765404Z","shell.execute_reply":"2022-03-26T14:04:18.70006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_cols=[col for col in train.columns if 'f_' in col]\nf_cols=f_cols+[ 'investment_id']\nX_train=train[f_cols]\nX_test=test[f_cols]\nprint('The test set is {}% of the train set'.format((len(X_test)/len(train))*100))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:04:18.702717Z","iopub.execute_input":"2022-03-26T14:04:18.703042Z","iopub.status.idle":"2022-03-26T14:04:22.35843Z","shell.execute_reply.started":"2022-03-26T14:04:18.703Z","shell.execute_reply":"2022-03-26T14:04:22.357546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train,\ndel train1\ndel train2\ndel train3\ndel train4\ndel train5\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:04:22.359925Z","iopub.execute_input":"2022-03-26T14:04:22.360331Z","iopub.status.idle":"2022-03-26T14:04:22.72868Z","shell.execute_reply.started":"2022-03-26T14:04:22.360284Z","shell.execute_reply":"2022-03-26T14:04:22.727752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SCALING","metadata":{}},{"cell_type":"code","source":"scaler= StandardScaler()\nscaler.fit(X_train.values)\n\ndef scale_dataset(df):\n    scaled_features=scaler.transform(df.values)\n    scaled_features_df = pd.DataFrame(scaled_features, index=df.index, columns=df.columns)\n    return scaled_features_df\n\nX_train=scale_dataset(X_train)\nX_test=scale_dataset(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:04:22.730623Z","iopub.execute_input":"2022-03-26T14:04:22.73096Z","iopub.status.idle":"2022-03-26T14:05:03.946556Z","shell.execute_reply.started":"2022-03-26T14:04:22.730919Z","shell.execute_reply":"2022-03-26T14:05:03.945399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:05:03.948259Z","iopub.execute_input":"2022-03-26T14:05:03.9487Z","iopub.status.idle":"2022-03-26T14:05:04.13586Z","shell.execute_reply.started":"2022-03-26T14:05:03.948641Z","shell.execute_reply":"2022-03-26T14:05:04.135222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MODEL ","metadata":{}},{"cell_type":"markdown","source":"Deep Neural Network(Sequential architecture)\nI have created a network of 6 layers, excluding the input layer.\n\nDropout layers are added.\n\nAdam optimizer is selected and with exponentially reducing lrearning rate.\n\nLoss is Mean Squared error and metrics is Root mean squared error.","metadata":{}},{"cell_type":"code","source":"learning_sch = tf.keras.optimizers.schedules.ExponentialDecay( initial_learning_rate = 0.001, decay_steps = 4000, decay_rate = 0.98)\n\n# Function to create a sequential model\ndef build_seqmodel(input_shape, layer_params=[(250,'relu'), (200, 'relu'), (150,'relu')], epochs_num=10, prelu=False):\n  model=keras.models.Sequential()\n  #define layers\n  model.add(keras.layers.Input(shape=input_shape))\n  for params in layer_params:\n    if prelu==True:\n        model.add(keras.layers.Dense(units=params[0], kernel_initializer='he_normal'))\n        model.add(keras.layers.PReLU())\n        model.add(keras.layers.Dropout(.20))\n        model.add(keras.layers.BatchNormalization())\n    else: \n        model.add(keras.layers.Dense(units=params[0], activation=params[1]))\n        model.add(keras.layers.BatchNormalization())\n    \n  model.add(keras.layers.Dense(1))\n  \n  # Select optimizer, loss and metrics\n  optimizer=keras.optimizers.Adam(learning_rate=learning_sch)\n  loss=keras.losses.MeanSquaredError()\n  metrics = keras.metrics.RootMeanSquaredError()\n  model.compile(loss= loss, metrics=metrics, optimizer= optimizer)\n  return model","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:05:04.137306Z","iopub.execute_input":"2022-03-26T14:05:04.137556Z","iopub.status.idle":"2022-03-26T14:05:05.261981Z","shell.execute_reply.started":"2022-03-26T14:05:04.137526Z","shell.execute_reply":"2022-03-26T14:05:05.260627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the arguments for the model function\ninput_shape=X_train.shape[1:]\nlayer_params=[(250,'prelu'), (200, 'prelu'), (150,'prelu'),(200, 'prelu'),(200, 'prelu'),(200, 'prelu') ]\n\nepochs_num=10\n\nmodel=build_seqmodel(input_shape, layer_params=layer_params,   epochs_num=epochs_num, prelu=True)\n# Create checkpoint to save best model and creat early stopping criteria\ncheck_point_best= keras.callbacks.ModelCheckpoint('keras_model.h5',save_best_only = True)\nearly_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n\ntf_train = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\ntf_val = tf.data.Dataset.from_tensor_slices((X_test, y_test)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n\nhistory=model.fit(tf_train, epochs=epochs_num, batch_size=128, validation_data=(tf_val), callbacks= [check_point_best,early_stopping], shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:05:05.264041Z","iopub.execute_input":"2022-03-26T14:05:05.264277Z","iopub.status.idle":"2022-03-26T14:36:07.123645Z","shell.execute_reply.started":"2022-03-26T14:05:05.264251Z","shell.execute_reply":"2022-03-26T14:36:07.122088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot(figsize=(12,6))\nplt.grid(True)\nplt.show()\nplt.close()\nplt.clf()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:36:07.127312Z","iopub.execute_input":"2022-03-26T14:36:07.12804Z","iopub.status.idle":"2022-03-26T14:36:07.477533Z","shell.execute_reply.started":"2022-03-26T14:36:07.127992Z","shell.execute_reply":"2022-03-26T14:36:07.47648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nbest_model = keras.models.load_model('keras_model.h5')\nenv = ubiquant.make_env()   \niter_test = env.iter_test()    \nfor (test_df, sample_prediction_df) in iter_test:\n    test_df=test_df[f_cols]\n    test_df = scale_dataset(test_df)\n    #test_df = tf.data.Dataset.from_tensor_slices((test_df)).batch(1024, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n    sample_prediction_df['target'] = best_model.predict(test_df)  \n    env.predict(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:36:07.479275Z","iopub.execute_input":"2022-03-26T14:36:07.479646Z","iopub.status.idle":"2022-03-26T14:36:08.58598Z","shell.execute_reply.started":"2022-03-26T14:36:07.479601Z","shell.execute_reply":"2022-03-26T14:36:08.585071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:36:08.587304Z","iopub.execute_input":"2022-03-26T14:36:08.587523Z","iopub.status.idle":"2022-03-26T14:36:09.04798Z","shell.execute_reply.started":"2022-03-26T14:36:08.587496Z","shell.execute_reply":"2022-03-26T14:36:09.046869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks! for going through my notebook.\n\nKindly upvote, if you liked it.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}