{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Prediction with DNN","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport sklearn\nfrom tensorflow import keras\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:08:24.767351Z","iopub.execute_input":"2022-02-24T13:08:24.767632Z","iopub.status.idle":"2022-02-24T13:08:24.774439Z","shell.execute_reply.started":"2022-02-24T13:08:24.767601Z","shell.execute_reply":"2022-02-24T13:08:24.773818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:08.088551Z","iopub.execute_input":"2022-02-24T13:01:08.08884Z","iopub.status.idle":"2022-02-24T13:01:40.548414Z","shell.execute_reply.started":"2022-02-24T13:01:08.088811Z","shell.execute_reply":"2022-02-24T13:01:40.54781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:43.22964Z","iopub.execute_input":"2022-02-24T13:01:43.230275Z","iopub.status.idle":"2022-02-24T13:01:43.260161Z","shell.execute_reply.started":"2022-02-24T13:01:43.230234Z","shell.execute_reply":"2022-02-24T13:01:43.259605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    #start_mem = df.memory_usage().sum() / 1024**2\n    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    #end_mem = df.memory_usage().sum() / 1024**2\n    #print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    #print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:46.449Z","iopub.execute_input":"2022-02-24T13:01:46.449253Z","iopub.status.idle":"2022-02-24T13:01:46.461655Z","shell.execute_reply.started":"2022-02-24T13:01:46.449226Z","shell.execute_reply":"2022-02-24T13:01:46.460798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = reduce_mem_usage(train)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:51.951358Z","iopub.execute_input":"2022-02-24T13:01:51.951911Z","iopub.status.idle":"2022-02-24T13:04:56.581183Z","shell.execute_reply.started":"2022-02-24T13:01:51.951871Z","shell.execute_reply":"2022-02-24T13:04:56.579717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to generate histogram of target variable\ndef target_distriution(target, df):\n  plt.figure(figsize=(14,6))\n     \n  df[target].plot.hist(bins=50)\n  plt.axvline(df[target].mean(), color='lightgreen', linewidth=3, label='Mean')\n  plt.axvline(np.percentile(df[target],25), color='brown', linewidth=3, label='Quartiles')\n  plt.axvline(np.percentile(df[target],75), color='brown', linewidth=3)\n  plt.xlabel(target,fontsize=14)\n  plt.ylabel('Frequency',fontsize=14)\n  plt.title('Histogram of ' + target,fontsize=14)\n  plt.legend(fontsize=14)\n  plt.show()\n  \n  plt.close()\n  plt.clf()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:05:11.567206Z","iopub.execute_input":"2022-02-24T13:05:11.568115Z","iopub.status.idle":"2022-02-24T13:05:11.57668Z","shell.execute_reply.started":"2022-02-24T13:05:11.568076Z","shell.execute_reply":"2022-02-24T13:05:11.575775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target='target'\ndf=train\ntarget_distriution(target, df)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:05:14.49874Z","iopub.execute_input":"2022-02-24T13:05:14.499486Z","iopub.status.idle":"2022-02-24T13:05:16.367194Z","shell.execute_reply.started":"2022-02-24T13:05:14.499448Z","shell.execute_reply":"2022-02-24T13:05:16.366417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:05:16.368562Z","iopub.execute_input":"2022-02-24T13:05:16.368781Z","iopub.status.idle":"2022-02-24T13:05:16.373447Z","shell.execute_reply.started":"2022-02-24T13:05:16.368738Z","shell.execute_reply":"2022-02-24T13:05:16.372658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Scatter Plot\nWe will take a look at how some features appear vs the target variable.","metadata":{}},{"cell_type":"code","source":"features=train.columns.to_list()[4:10]","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:05:38.386661Z","iopub.execute_input":"2022-02-24T13:05:38.387152Z","iopub.status.idle":"2022-02-24T13:05:38.392408Z","shell.execute_reply.started":"2022-02-24T13:05:38.387115Z","shell.execute_reply":"2022-02-24T13:05:38.391602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6,6))\nfor feature in features:\n    plt.scatter(train[feature], train['target'])\n    plt.xlabel(feature,fontsize=14)\n    plt.ylabel('Target',fontsize=14)\n    plt.show()\n    plt.close()\n    plt.clf()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:05:39.748857Z","iopub.execute_input":"2022-02-24T13:05:39.749513Z","iopub.status.idle":"2022-02-24T13:06:13.509572Z","shell.execute_reply.started":"2022-02-24T13:05:39.749467Z","shell.execute_reply":"2022-02-24T13:06:13.508545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Scatterplot of Time_id vs target variable\nLet's also look at the target vs the time_id for a single investment_id.","metadata":{}},{"cell_type":"code","source":"investment=train['investment_id'].unique()\n#Pick an investment ID\ninvestment1=investment[0]\ninvestment2=investment[1]","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:06:13.511004Z","iopub.execute_input":"2022-02-24T13:06:13.511213Z","iopub.status.idle":"2022-02-24T13:06:13.561377Z","shell.execute_reply.started":"2022-02-24T13:06:13.511188Z","shell.execute_reply":"2022-02-24T13:06:13.560669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_investment1=train[train['investment_id']==investment1]","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:06:13.562515Z","iopub.execute_input":"2022-02-24T13:06:13.562831Z","iopub.status.idle":"2022-02-24T13:06:14.810304Z","shell.execute_reply.started":"2022-02-24T13:06:13.562792Z","shell.execute_reply":"2022-02-24T13:06:14.809565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Line plot\nplt.figure()\nplt.plot(df_investment1['time_id'], df_investment1['target'])\nplt.xlabel('Time_id',fontsize=14)\nplt.ylabel('Target',fontsize=14)\nplt.show()\nplt.close()\nplt.clf()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:06:14.811827Z","iopub.execute_input":"2022-02-24T13:06:14.812059Z","iopub.status.idle":"2022-02-24T13:06:14.93955Z","shell.execute_reply.started":"2022-02-24T13:06:14.812032Z","shell.execute_reply":"2022-02-24T13:06:14.938531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_investment1","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:06:14.941132Z","iopub.execute_input":"2022-02-24T13:06:14.941736Z","iopub.status.idle":"2022-02-24T13:06:14.946145Z","shell.execute_reply.started":"2022-02-24T13:06:14.941692Z","shell.execute_reply":"2022-02-24T13:06:14.945207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train Test Split\nI have tried to split the data in the following manner instead of the usual Train-test-split. This is done in order to preserve all the aspects related with Investment_ids and time_ids.","metadata":{}},{"cell_type":"code","source":"# Split investment id in 2 lots\nfirst_lot=train['investment_id'].unique()\nfirst_lot=first_lot[0:len(first_lot)//2]\nsecond_lot=first_lot[len(first_lot)//2:len(first_lot)+1]","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:06:20.086593Z","iopub.execute_input":"2022-02-24T13:06:20.086877Z","iopub.status.idle":"2022-02-24T13:06:20.136152Z","shell.execute_reply.started":"2022-02-24T13:06:20.086846Z","shell.execute_reply":"2022-02-24T13:06:20.135441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify the lot in the Dataframe\ntrain['inv_lot']=train['investment_id'].apply(lambda x: 1 if x in first_lot else 2)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:06:22.340734Z","iopub.execute_input":"2022-02-24T13:06:22.341015Z","iopub.status.idle":"2022-02-24T13:07:12.8114Z","shell.execute_reply.started":"2022-02-24T13:06:22.340986Z","shell.execute_reply":"2022-02-24T13:07:12.81046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fix a point in Time_id to split. I have used the quantile method and arbitrarily selected the 75th percentile.\ntile=train['time_id'].quantile(q=0.75, interpolation='lower')","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:07:12.813155Z","iopub.execute_input":"2022-02-24T13:07:12.813534Z","iopub.status.idle":"2022-02-24T13:07:12.858607Z","shell.execute_reply.started":"2022-02-24T13:07:12.813493Z","shell.execute_reply":"2022-02-24T13:07:12.857768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating various splits as per investment_id and time_id\ntrain1=train[(train['inv_lot'] ==1) & (train['time_id']< (tile))]\ntrain2=train[(train['inv_lot']==2) & (train['time_id']>= tile)]\ntrain3=train[(train['inv_lot'] ==1) & (train['time_id']>= tile)]\ntrain4=train[(train['inv_lot'] ==2) & (train['time_id']< tile)]\ntrain5=train[(train['inv_lot'] ==2)]","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:07:12.859684Z","iopub.execute_input":"2022-02-24T13:07:12.859929Z","iopub.status.idle":"2022-02-24T13:07:25.346761Z","shell.execute_reply.started":"2022-02-24T13:07:12.859902Z","shell.execute_reply":"2022-02-24T13:07:25.345919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take a look at the length in each dataframe\nprint(len(train1)/len(train),len(train2)/len(train),len(train3)/len(train),len(train4)/len(train),len(train5)/len(train) )","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:07:25.34867Z","iopub.execute_input":"2022-02-24T13:07:25.349312Z","iopub.status.idle":"2022-02-24T13:07:25.355655Z","shell.execute_reply.started":"2022-02-24T13:07:25.349278Z","shell.execute_reply":"2022-02-24T13:07:25.354879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creation of test and train  sets of features\nX_train=pd.concat([train1,train5], axis=0)\nX_test=pd.concat([train2,train3], axis=0)\n# Creation of test and train  sets of target variable\ny_train=X_train[['target']]\ny_test=X_test[['target']]\n# Remove columns that are not to be included in training data\nX_train=X_train.drop(['row_id', 'target', 'inv_lot'], axis=1)\nX_test=X_test.drop(['row_id', 'target', 'inv_lot'], axis=1)\n\n\nprint('The test set is {}% of the train set'.format((len(X_test)/len(train))*100))","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:07:25.356975Z","iopub.execute_input":"2022-02-24T13:07:25.357353Z","iopub.status.idle":"2022-02-24T13:07:32.047085Z","shell.execute_reply.started":"2022-02-24T13:07:25.35731Z","shell.execute_reply":"2022-02-24T13:07:32.046352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train\ndel train1\ndel train2\ndel train3\ndel train4\ndel train5","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:07:32.048468Z","iopub.execute_input":"2022-02-24T13:07:32.04891Z","iopub.status.idle":"2022-02-24T13:07:32.177805Z","shell.execute_reply.started":"2022-02-24T13:07:32.048879Z","shell.execute_reply":"2022-02-24T13:07:32.177018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Scaling\nWe will create a Function to scale the features. Note: Don't forget to use the same function to scale the actual test data","metadata":{}},{"cell_type":"code","source":"scaler= StandardScaler()\nscaler.fit(X_train.values)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:10:51.20502Z","iopub.execute_input":"2022-02-24T13:10:51.206025Z","iopub.status.idle":"2022-02-24T13:11:36.714189Z","shell.execute_reply.started":"2022-02-24T13:10:51.205968Z","shell.execute_reply":"2022-02-24T13:11:36.713325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scale_dataset(df):\n    scaled_features=scaler.transform(df.values)\n    scaled_features_df = pd.DataFrame(scaled_features, index=df.index, columns=df.columns)\n    return scaled_features_df","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:13:48.325475Z","iopub.execute_input":"2022-02-24T13:13:48.325962Z","iopub.status.idle":"2022-02-24T13:13:48.331116Z","shell.execute_reply.started":"2022-02-24T13:13:48.325913Z","shell.execute_reply":"2022-02-24T13:13:48.330159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=scale_dataset(X_train)\nX_test=scale_dataset(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:14:05.343405Z","iopub.execute_input":"2022-02-24T13:14:05.344491Z","iopub.status.idle":"2022-02-24T13:14:29.757227Z","shell.execute_reply.started":"2022-02-24T13:14:05.344449Z","shell.execute_reply":"2022-02-24T13:14:29.756307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:14:42.80982Z","iopub.execute_input":"2022-02-24T13:14:42.810435Z","iopub.status.idle":"2022-02-24T13:14:42.817304Z","shell.execute_reply.started":"2022-02-24T13:14:42.810381Z","shell.execute_reply":"2022-02-24T13:14:42.816255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:14:45.778626Z","iopub.execute_input":"2022-02-24T13:14:45.778948Z","iopub.status.idle":"2022-02-24T13:14:45.784547Z","shell.execute_reply.started":"2022-02-24T13:14:45.778911Z","shell.execute_reply":"2022-02-24T13:14:45.783738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Deep Neural Network(Sequential architecture)\nI have created a network of 7 layers, excluding the input layer.\n\nAdam optimizer is selected and a function lr_ is created to take the advantage of reducing learning rate.\n\nLoss is Mean Squared error and metrics is Root mean squared error.","metadata":{}},{"cell_type":"code","source":"# Function for decaying Learning rate\ndef lr_(init_lr, epoch_num, decay_rate):\n    learning_rate = (1/ (1+  (epoch_num* decay_rate)))* init_lr\n     \n    return learning_rate","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:15:03.230345Z","iopub.execute_input":"2022-02-24T13:15:03.23107Z","iopub.status.idle":"2022-02-24T13:15:03.23549Z","shell.execute_reply.started":"2022-02-24T13:15:03.231027Z","shell.execute_reply":"2022-02-24T13:15:03.234528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to create a sequential model\ndef build_seqmodel(input_shape, layer_params=[(250,'relu'), (200, 'relu'), (150,'relu')], learning_rate= .001, epochs_num=5, decay_rate=1):\n  model=keras.models.Sequential()\n  #define layers\n  model.add(keras.layers.Input(shape=input_shape))\n  for params in layer_params:\n    model.add(keras.layers.Dense(units=params[0], activation=params[1]))\n    model.add(keras.layers.BatchNormalization())\n    \n  model.add(keras.layers.Dense(1))\n  \n  # Select optimizer, loss and metrics\n  optimizer=keras.optimizers.Adam(learning_rate=lr_(learning_rate,epochs_num, decay_rate ))\n  loss=keras.losses.MeanSquaredError()\n  metrics = keras.metrics.RootMeanSquaredError()\n  model.compile(loss= loss, metrics=metrics, optimizer= optimizer)\n  return model","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:15:05.470167Z","iopub.execute_input":"2022-02-24T13:15:05.47108Z","iopub.status.idle":"2022-02-24T13:15:05.561117Z","shell.execute_reply.started":"2022-02-24T13:15:05.471034Z","shell.execute_reply":"2022-02-24T13:15:05.560209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the arguments for the model function\ninput_shape=X_train.shape[1:]\nlayer_params=[(250,'relu'), (200, 'relu'), (150,'relu'),(200, 'relu'),(200, 'relu'),(200, 'relu') ]\nlearning_rate=.01\nepochs_num=10\ndecay_rate=1\nmodel=build_seqmodel(input_shape, layer_params=layer_params, learning_rate= learning_rate, epochs_num=epochs_num, decay_rate=decay_rate)\n# Create checkpoint to save best model and creat early stopping criteria\ncheck_point_best= keras.callbacks.ModelCheckpoint('keras_model.h5',save_best_only = True)\nearly_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n\nhistory=model.fit(X_train, y_train, epochs=epochs_num, batch_size=128, validation_data=(X_test, y_test), callbacks= [check_point_best,early_stopping], shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:15:09.327733Z","iopub.execute_input":"2022-02-24T13:15:09.32831Z","iopub.status.idle":"2022-02-24T13:49:52.540495Z","shell.execute_reply.started":"2022-02-24T13:15:09.32826Z","shell.execute_reply":"2022-02-24T13:49:52.539819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X_train\ndel X_test\ndel model","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:50:02.78513Z","iopub.execute_input":"2022-02-24T13:50:02.785604Z","iopub.status.idle":"2022-02-24T13:50:02.796122Z","shell.execute_reply.started":"2022-02-24T13:50:02.785554Z","shell.execute_reply":"2022-02-24T13:50:02.795527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets view the losses and metrics\npd.DataFrame(history.history).plot(figsize=(12,6))\nplt.grid(True)\nplt.show()\nplt.close()\nplt.clf()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:50:04.903905Z","iopub.execute_input":"2022-02-24T13:50:04.904407Z","iopub.status.idle":"2022-02-24T13:50:05.09324Z","shell.execute_reply.started":"2022-02-24T13:50:04.904357Z","shell.execute_reply":"2022-02-24T13:50:05.092248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nbest_model = keras.models.load_model('keras_model.h5')\nenv = ubiquant.make_env()   \niter_test = env.iter_test()    \nfor (test_df, sample_prediction_df) in iter_test:\n    test_df = scale_dataset(test_df)\n    sample_prediction_df['target'] = best_model.predict(test_df)  \n    env.predict(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:50:09.649169Z","iopub.execute_input":"2022-02-24T13:50:09.649449Z","iopub.status.idle":"2022-02-24T13:50:10.419184Z","shell.execute_reply.started":"2022-02-24T13:50:09.649418Z","shell.execute_reply":"2022-02-24T13:50:10.418593Z"},"trusted":true},"execution_count":null,"outputs":[]}]}