{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# A straightforward DNN implemented with PyTorch Lightning on GPU. Current features include: early stopping based on the mean daily correlation of the output with target (the competition metric), 10x cross validation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\n\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nimport torch.nn.functional as F\nfrom pytorch_lightning.callbacks import Callback\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset, Subset\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2022-03-03T08:26:39.363147Z","iopub.execute_input":"2022-03-03T08:26:39.363719Z","iopub.status.idle":"2022-03-03T08:26:43.56962Z","shell.execute_reply.started":"2022-03-03T08:26:39.36365Z","shell.execute_reply":"2022-03-03T08:26:43.568717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the data, using a low memory parquet file\ntrain = pd.read_parquet(\"../input/train-small/train_small.parquet\")\n# Creation of th parquet file follows https://www.kaggle.com/robikscube/fast-data-loading-and-low-mem-with-parquet-files\n# it can be found here https://www.kaggle.com/leonweninger/train-small\n\n# Treat the features differently from target, time_id, investment_id\nfloat_feature_names = train.drop(['target', 'row_id', 'time_id', 'investment_id'], axis=1).columns\nfloat_input = train[float_feature_names].values\ninvestment_id = train[['investment_id']].values.astype(int)\ntime_id = train[['time_id']].values.astype(int)\ntargets = train[['target']].values\n\ndel train\n\n# everything as torch tensors\nfloat_input = torch.FloatTensor(float_input)\ninvestment_id = torch.LongTensor(investment_id)\ntime_id = torch.LongTensor(time_id)\ntarget = torch.FloatTensor(targets)\ndataset = TensorDataset(time_id, investment_id, float_input, target)\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T08:26:43.571801Z","iopub.execute_input":"2022-03-03T08:26:43.572086Z","iopub.status.idle":"2022-03-03T08:27:23.107939Z","shell.execute_reply.started":"2022-03-03T08:26:43.572046Z","shell.execute_reply":"2022-03-03T08:27:23.107294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UbiquantRegressor(pl.LightningModule):\n    def __init__(self):\n        super(UbiquantRegressor, self).__init__()\n\n        # Embedding of investment_id to 11 float features.\n        # As the number of unseen investment_ids is unknown, a large margin is selected (10000). \n        self.id_embedding = nn.Embedding(10000,11)\n\n        # credits to sahil112: https://www.kaggle.com/sahil112/whyonlykeras-easy-pytorch-competitive-dnn for this architecture\n        self.layers = nn.Sequential(nn.Linear(311, 64),\n                                    nn.BatchNorm1d(64),\n                                    nn.SiLU(),\n                                    nn.Dropout(0.4),\n\n                                    nn.Linear(64, 128),\n                                    nn.BatchNorm1d(128),\n                                    nn.SiLU(),\n                                    nn.Dropout(0.4),\n\n                                    nn.Linear(128, 256),\n                                    nn.BatchNorm1d(256),\n                                    nn.SiLU(),\n                                    nn.Dropout(0.4),\n\n                                    nn.Linear(256, 512),\n                                    nn.BatchNorm1d(512),\n                                    nn.SiLU(0.1),\n                                    nn.Dropout(0.4),\n\n                                    nn.Linear(512, 256),\n                                    nn.BatchNorm1d(256),\n                                    nn.SiLU(),\n                                    nn.Dropout(0.4),\n\n                                    nn.Linear(256, 128),\n                                    nn.BatchNorm1d(128),\n                                    nn.SiLU(0.1),\n                                    nn.Dropout(0.4),\n\n                                    nn.Linear(128, 8),\n                                    nn.BatchNorm1d(8),\n                                    nn.SiLU(),\n                                    nn.Dropout(0.4),\n\n                                    nn.Linear(8, 1))\n\n    def forward(self, time_id, investment_id, f_features):\n        # Embedding of the investment_id\n        invest_embedding = self.id_embedding(investment_id).squeeze(dim=1)\n        # Concat embedding and features.\n        # Open question: should the network have access to the time_id?\n        # The final test set will consist of time_id never seen in the train set \n        # Nevertheless, it can be easily added here...\n        #dnn_input = torch.cat((invest_embedding, time_id, f_features), axis=-1)\n        dnn_input = torch.cat((invest_embedding, f_features), axis=-1)\n        return self.layers(dnn_input)\n\n    def training_step(self, batch, batch_nb):\n        time_id, investment_id, float_input, target = batch\n\n        out = self(time_id, investment_id, float_input)\n        loss = F.mse_loss(out, target)\n\n        self.log('train_loss', loss)\n        return loss\n\n    def validation_step(self, batch, batch_nb):\n        time_id, investment_id, float_input, target = batch\n\n        result = self(time_id, investment_id, float_input)\n        loss = F.mse_loss(result, target)\n\n        dict = {'val_loss': loss,\n                'result': result,\n                'target': target,\n                'time_id': time_id,\n                'investment_id': investment_id,\n                }\n        return dict\n\n    def validation_epoch_end(self, outputs):\n        val_losses = [x['val_loss'] for x in outputs]\n        result = torch.cat([x['result'] for x in outputs])\n        target = torch.cat([x['target'] for x in outputs])\n        time_ids = torch.cat([x['time_id'] for x in outputs])\n        investment_ids = torch.cat([x['investment_id'] for x in outputs])\n\n        corrs = []\n        for t in torch.unique(time_ids):\n            t_results = result[time_ids == t]\n            t_target = target[time_ids == t]\n            # corr = torch.corrcoef(torch.stack((t_results, t_target)))[0,1] # use this when pytorch>=1.10\n            corr = np.corrcoef(torch.stack((t_results, t_target)).cpu().numpy())[0, 1]\n            corrs.append(corr)\n\n        # mean_corr = torch.mean(torch.stack(corrs)) # use this when pytorch>=1.10\n        mean_corr = np.nanmean(corrs)\n        epoch_loss = torch.stack(val_losses).mean()  # Combine losses\n\n        self.log('val_loss', epoch_loss, prog_bar=True)\n        self.log('mean_corr', mean_corr, prog_bar=True)\n\n        dict = {'val_loss': epoch_loss,\n                'corrs': mean_corr}\n        return dict\n\n    def epoch_end(self, epoch, result):\n        pass\n\n    def test_step(self, batch, batch_nb):\n        pass\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.001)\n\nclass MetricTracker(Callback):\n    def __init__(self):\n        self.val_losses = []\n        self.corrs = []\n\n    def on_validation_epoch_end(self, trainer, module):\n        self.val_losses.append(trainer._results['validation_epoch_end.val_loss'].value.cpu().numpy()) # track them\n        self.corrs.append(trainer._results['validation_epoch_end.mean_corr'].value.cpu().numpy()) # track them\n        if 0: #index==1:\n            # live plotting of results during training, switched off\n            ax.plot(self.val_losses, color=\"orange\")\n            ax.set_ylabel(\"Val loss\", color=\"orange\", fontsize=14)\n            ax2 = ax.twinx()\n            ax2.plot(self.corrs, color=\"blue\")\n            ax2.set_ylabel(\"Mean daily corr 2 target\", color=\"blue\", fontsize=14)\n            plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T08:27:23.10924Z","iopub.execute_input":"2022-03-03T08:27:23.109712Z","iopub.status.idle":"2022-03-03T08:27:23.212838Z","shell.execute_reply.started":"2022-03-03T08:27:23.109675Z","shell.execute_reply":"2022-03-03T08:27:23.212167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_splits=10\nkf = KFold(n_splits=n_splits, shuffle=True)\nval_losses = []\nmean_corrs = []\nmodels = [] # A list of all final models\nindex=0\nfor train_index, test_index in kf.split(dataset):\n    index+=1\n    print(\"CV run {}...\".format(index))\n    train_ds, val_ds = Subset(dataset, train_index), Subset(dataset, test_index)\n\n    train_loader = DataLoader(train_ds, 32768)\n    val_loader = DataLoader(val_ds, 32768)\n\n    uq_regressor = UbiquantRegressor()\n\n    metricTracker = MetricTracker()\n    trainer = pl.Trainer(gpus=1,\n                         callbacks=[metricTracker,\n                                    EarlyStopping(monitor=\"mean_corr\", mode=\"max\", patience=3),\n                                    ModelCheckpoint(save_top_k=1, monitor=\"mean_corr\", mode=\"max\", save_on_train_epoch_end=False)],\n                         max_epochs=21,\n                         num_sanity_val_steps=0,)\n\n    trainer.fit(uq_regressor, train_loader, val_loader)\n\n    # Load best model based on mean daily correlation with target\n    uq_regressor = UbiquantRegressor().load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n    models.append(uq_regressor)\n    \n    # Show val results\n    val_result = trainer.validate(model=uq_regressor, dataloaders=val_loader)\n    val_losses.append(val_result[0]['val_loss'])\n    mean_corrs.append(val_result[0]['mean_corr'])\n    fig, ax = plt.subplots()\n    ax.plot(metricTracker.val_losses, color=\"orange\")\n    ax.set_ylabel(\"Val loss\", color=\"orange\", fontsize=14)\n    ax2 = ax.twinx()\n    ax2.plot(metricTracker.corrs, color=\"blue\")\n    ax2.set_ylabel(\"Mean daily corr 2 target\", color=\"blue\", fontsize=14)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T08:27:39.055921Z","iopub.execute_input":"2022-03-03T08:27:39.056521Z","iopub.status.idle":"2022-03-03T08:28:22.615665Z","shell.execute_reply.started":"2022-03-03T08:27:39.05648Z","shell.execute_reply":"2022-03-03T08:28:22.614989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The submission part\nimport ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() ","metadata":{"execution":{"iopub.status.busy":"2022-03-03T08:26:08.87967Z","iopub.status.idle":"2022-03-03T08:26:08.880382Z","shell.execute_reply.started":"2022-03-03T08:26:08.88013Z","shell.execute_reply":"2022-03-03T08:26:08.880155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (test_df, sample_prediction_df) in iter_test:\n    time_id = test_df.row_id.str.split(\"_\", expand=True)[0].values.astype(int) \n    investment_id = test_df[['investment_id']].values.astype(int)\n    float_input = test_df[float_feature_names].values\n    \n    float_input = torch.FloatTensor(float_input)\n    investment_id = torch.LongTensor(investment_id)\n    time_id = torch.LongTensor(time_id).unsqueeze(-1)\n    \n    sample_prediction_df['target'] = 0\n    for uq_regressor in models:\n        uq_regressor.eval()\n        with torch.no_grad():\n            predictions = uq_regressor(time_id, investment_id, float_input).squeeze()\n        sample_prediction_df['target'] += predictions.detach().cpu().numpy()/n_splits\n    env.predict(sample_prediction_df) \n    display(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T08:21:04.312576Z","iopub.execute_input":"2022-03-03T08:21:04.312888Z","iopub.status.idle":"2022-03-03T08:21:04.502511Z","shell.execute_reply.started":"2022-03-03T08:21:04.312849Z","shell.execute_reply":"2022-03-03T08:21:04.50185Z"},"trusted":true},"execution_count":null,"outputs":[]}]}