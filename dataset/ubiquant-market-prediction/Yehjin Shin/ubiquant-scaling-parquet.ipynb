{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## UBIQUANT SCALING PARQUET\n\n### <span style=\"color:blue\">Scaling Method\n\n- **time_id** $\\rightarrow$ min-max scaling ( $X-min\\over max-min$ )\n- **f_1 ~ f_300** $\\rightarrow$ standard scaling ( $X-mean\\over std$ )\n- **investment_id** $\\rightarrow$ mean target encoding (Version 2)\n- **target** $\\rightarrow$ keep the original\n\n### <span style=\"color:blue\">Description\n\n- **scaled_df.parquet** $\\rightarrow$ full dataframe after scaling\n- **scaled_train.parquet** $\\rightarrow$ 80% datafram after scaling\n- **scaled_test.parquet** $\\rightarrow$ 20% dataframe after scaling\n- **df_describe.parquet** $\\rightarrow$ statistic of full dataframe\n- **remain_cols.parquet** $\\rightarrow$ full time_df(scaled), investment_id, target\n\n### <span style=\"color:blue\">How to Use\n\n- **Add data** (on the top right corner of kaggle notebook) > **Notebook Output File** > Search with keyword below\n- keyword: ubiquant-scaling-parquet\n- location: /kaggle/input/ubiquant-scaling-parquet","metadata":{}},{"cell_type":"markdown","source":"## IMPORT LIBRARIES","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport math\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-04T23:30:33.534847Z","iopub.execute_input":"2022-03-04T23:30:33.536066Z","iopub.status.idle":"2022-03-04T23:30:33.5644Z","shell.execute_reply.started":"2022-03-04T23:30:33.535932Z","shell.execute_reply":"2022-03-04T23:30:33.563671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LOADING DATA\n- parquet url: https://www.kaggle.com/robikscube/ubiquant-parquet?select=train_low_mem.parquet\n- location: /kaggle/input/ubiquant-parquet","metadata":{}},{"cell_type":"code","source":"%%time\n# load data and shuffle with random state 2022\ntrain = [pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet').sample(frac=1, random_state=2022)]\nprint(train[0].shape)\nprint(f'number of row_id: {len(train[0].row_id.unique())}')\nprint(f'number of time_id: {len(train[0].time_id.unique())}')\nprint(f'number of investment_id: {len(train[0].investment_id.unique())}')","metadata":{"execution":{"iopub.status.busy":"2022-03-04T23:30:36.34564Z","iopub.execute_input":"2022-03-04T23:30:36.346194Z","iopub.status.idle":"2022-03-04T23:31:43.179615Z","shell.execute_reply.started":"2022-03-04T23:30:36.346144Z","shell.execute_reply":"2022-03-04T23:31:43.178187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DATAFRAME DESCRIBE\n- includes statistic of raw data\n- used in scaling process","metadata":{}},{"cell_type":"code","source":"dfs = []\n\ndfs.append(train[0].describe())\ndfs[0].to_parquet('df_describe.parquet', engine='pyarrow')","metadata":{"execution":{"iopub.status.busy":"2022-03-04T23:31:43.182162Z","iopub.execute_input":"2022-03-04T23:31:43.185068Z","iopub.status.idle":"2022-03-04T23:32:20.644205Z","shell.execute_reply.started":"2022-03-04T23:31:43.18502Z","shell.execute_reply":"2022-03-04T23:32:20.642909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MEAN TARGET ENCODING\n- https://casa-de-feel.tistory.com/22","metadata":{}},{"cell_type":"code","source":"mean_target_encoded = train[0].groupby('investment_id')['target'].mean()\nmean_target_encoded.to_csv('mean_target_encoded.csv')\ntrain[0]['investment_id'] = train[0]['investment_id'].map(mean_target_encoded)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T23:40:40.463089Z","iopub.execute_input":"2022-03-04T23:40:40.463462Z","iopub.status.idle":"2022-03-04T23:40:40.704466Z","shell.execute_reply.started":"2022-03-04T23:40:40.463426Z","shell.execute_reply":"2022-03-04T23:40:40.703585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## STANDARD SCALING\n- downcast dataframe: https://www.kaggle.com/ljjblackpig/3-steps-to-reduce-memory-size-for-the-dataset","metadata":{}},{"cell_type":"code","source":"def downcast_df(df):\n    list_of_columns = list(df.select_dtypes(include=[\"float64\"]).columns)\n        \n    if len(list_of_columns)>=1:\n        max_string_length = max([len(col) for col in list_of_columns])\n        for col in list_of_columns:\n            df[col] = pd.to_numeric(df[col], downcast=\"float\")\n    else:\n        print(\"no columns to downcast\")\n    \n    return df\n\n    \ndef standard_scale(df, cols, desc):\n    df = df[cols]\n    desc = desc[cols]\n    df = (df-desc.loc['mean'])/desc.loc['std']\n    return downcast_df(df)\n\ndef min_max_scale(df, cols, desc):\n    df = df[cols]\n    desc = desc[cols]\n    df = (df-desc.loc['min'])/(desc.loc['max']-desc.loc['min'])\n    return downcast_df(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"remain_cols = ['time_id', 'investment_id', 'target']\ndfs.append(pd.DataFrame(train[0][remain_cols].astype('float32')))\ndfs[1]['time_id'] = min_max_scale(dfs[1], ['time_id'], dfs[0])\ndfs[1].to_parquet('remain_cols.parquet', engine='pyarrow')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = [v for v in list(train[0].columns) if v[0]=='f']\nn = len(cols)//7 + 1\n\nscaled_dfs = []\nfor i in range(7):\n    temp_col = cols[i*n:(i+1)*n]\n    scaled_dfs.append(standard_scale(train[0], temp_col, dfs[0]))\n    print('*', end='')\ntrain.pop()\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SAVE SCALED DATAFRAME AS PARQUET","metadata":{}},{"cell_type":"code","source":"temp_dfs = []\n\ntemp_dfs.append(pd.concat(dfs[1:2]+scaled_dfs[:2], axis=1))\ntemp_dfs.append(pd.concat(scaled_dfs[2:4], axis=1))\ntemp_dfs.append(pd.concat(scaled_dfs[4:], axis=1))\n\nwhile scaled_dfs:\n    scaled_dfs.pop()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled_df = pd.concat(temp_dfs, axis=1)\n\nwhile temp_dfs:\n    temp_dfs.pop()\n\nscaled_df.to_parquet('scaled_df.parquet', engine='pyarrow')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TRAIN TEST SPLIT\n- train 80 : test 20\n- shuffle was already done before","metadata":{}},{"cell_type":"code","source":"n = int(len(scaled_df)*0.2)\nscaled_df.iloc[:n].to_parquet('scaled_test.parquet', engine='pyarrow')\nscaled_df.iloc[n:].to_parquet('scaled_train.parquet', engine='pyarrow')","metadata":{},"execution_count":null,"outputs":[]}]}