{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Autoencoder Baseline for Ubiquant Market Prediction\nsome notes:\n- this notebook uses an autoencoder to obtain lower-dimensional features\n- the regular model uses this lower-dimensional representation to predict our 'target' variable\n- a 5Fold GroupTimeSeriesSplit is used for CV, and only the last model is used for inference\n- make sure to use GPU when running the code","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom fastai.tabular.all import *\nimport numpy as np\nimport gc\n# import optuna\n\nset_seed(42)\n\n# from pympler.tracker import SummaryTracker\n# tracker = SummaryTracker()\n\ncont = [f'f_{i}' for i in range(300)]\nPATH = '../input/ubiquant-trainfeather-gtss'\nsplits = pd.read_csv('../input/ubiquant-trainfeather-gtss/cv_splits.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-13T17:15:59.406581Z","iopub.execute_input":"2022-02-13T17:15:59.407174Z","iopub.status.idle":"2022-02-13T17:16:01.988921Z","shell.execute_reply.started":"2022-02-13T17:15:59.407034Z","shell.execute_reply":"2022-02-13T17:16:01.98815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class CFG_AE:\n    layers = [300, 600]\n    bottleneck = 150\n    ps = 0.025\n    bswap = 0.1\n    bs = 1024\n    \n\nclass CFG:\n    layers = [450, 600, 600, 450, 300, 150]\n    ps = 0.05\n    embed_p = 0.5\n    bs = 4096\n\ncfg_ae = CFG_AE()\ncfg = CFG()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T17:16:01.990712Z","iopub.execute_input":"2022-02-13T17:16:01.99098Z","iopub.status.idle":"2022-02-13T17:16:01.996629Z","shell.execute_reply.started":"2022-02-13T17:16:01.990944Z","shell.execute_reply":"2022-02-13T17:16:01.995951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset/Dataloader and other functions","metadata":{}},{"cell_type":"code","source":"# Credits to Slawek Biel\nclass UbiquantDataset:\n    def __init__(self, feature_tensor, targets):\n        store_attr()\n        self.n_inp = 2\n    def __getitem__(self, idx):\n        return torch.empty(0),self.feature_tensor[idx], self.targets[idx, None]\n    \n    def __len__(self):\n        return len(self.feature_tensor)\n    \nclass UbiDL(DataLoader):\n    def __iter__(self):\n        if self.shuffle:\n            self.__idxs = torch.tensor(range(0,self.n))\n        else:\n            self.__idxs = torch.tensor(range(0,self.n))\n        for batch_start in range(0, self.n, self.bs):\n            if batch_start + self.bs > self.n and self.drop_last:\n                return \n            indices = self.__idxs[batch_start:batch_start+self.bs]\n            yield self.dataset[indices]\n","metadata":{"execution":{"iopub.status.busy":"2022-02-13T17:16:01.998101Z","iopub.execute_input":"2022-02-13T17:16:01.998625Z","iopub.status.idle":"2022-02-13T17:16:02.010349Z","shell.execute_reply.started":"2022-02-13T17:16:01.998584Z","shell.execute_reply":"2022-02-13T17:16:02.009602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pearson Loss and Metric","metadata":{}},{"cell_type":"code","source":"# Mean Pearson Corr Metric\ndef pearson_coef(data):\n    return data.corr()['target']['preds']\n\nclass CompMetric(AccumMetric):\n    def __init__(self, val_df):\n        super().__init__(None)\n        self.val_df = val_df\n        \n    @property\n    def name(self):\n        return 'Pears'\n        \n    @property\n    def value(self):\n        preds = torch.cat(self.preds)\n        val_df['preds'] = preds.cpu().numpy()\n        return np.mean(self.val_df[['time_id', 'target', 'preds']].groupby('time_id').apply(pearson_coef))\n    \n# Loss Function\ndef pearson_loss(x, y):\n    xd = x - x.mean()\n    yd = y - y.mean()\n    nom = (xd*yd).sum()\n    denom = ((xd**2).sum() * (yd**2).sum()).sqrt()\n    return 1 - nom / denom","metadata":{"execution":{"iopub.status.busy":"2022-02-13T17:16:02.013397Z","iopub.execute_input":"2022-02-13T17:16:02.0136Z","iopub.status.idle":"2022-02-13T17:16:02.02284Z","shell.execute_reply.started":"2022-02-13T17:16:02.013577Z","shell.execute_reply":"2022-02-13T17:16:02.021921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AutoEncoder","metadata":{}},{"cell_type":"code","source":"class BatchSwapNoise(Module):\n    \"Swap Noise Module\"\n    def __init__(self, p): store_attr()\n\n    def forward(self, x):\n        if self.training:\n            mask = torch.rand(x.size()) > (1 - self.p)\n            l1 = torch.floor(torch.rand(x.size()) * x.size(0)).type(torch.LongTensor)\n            l2 = (mask.type(torch.LongTensor) * x.size(1))\n            res = (l1 * l2).view(-1)\n            idx = torch.arange(x.nelement()) + res\n            idx[idx>=x.nelement()] = idx[idx>=x.nelement()]-x.nelement()\n            return x.flatten()[idx].view(x.size())\n        else:\n            return x\n        \nclass TabularAE(TabularModel):\n    \"A simple AutoEncoder model\"\n    def __init__(self, emb_szs, n_cont, hidden_size, cats, layers, ps=0.05, embed_p=0.2, bswap=None):\n        super().__init__(emb_szs, n_cont, layers=layers[::-1], out_sz=hidden_size, embed_p=embed_p, ps=ps, act_cls=nn.ReLU(inplace=True))\n        \n        self.bswap = bswap\n        self.cats = cats\n        self.activation_cats = sum([v for k,v in cats.items()])\n        \n        self.layers = nn.Sequential(*L(self.layers.children())[:-1] + nn.Sequential(LinBnDrop(layers[0], hidden_size, p=ps, act=nn.ReLU(inplace=True))))\n        \n        if(bswap != None): self.noise = BatchSwapNoise(bswap)\n        decoder_layers = [LinBnDrop(hidden_size, layers[0], p=ps, act=nn.ReLU(inplace=True))]\n        for i in range(1, len(layers)):\n            decoder_layers.append(LinBnDrop(layers[i-1], layers[i], p=ps, act=nn.ReLU(inplace=True)))\n        self.decoder = nn.Sequential(*decoder_layers)\n        \n        self.decoder_cont = nn.Sequential(\n            LinBnDrop(layers[-1], n_cont, p=ps, bn=False, act=None),\n        )\n        \n    def forward(self, x_cat, x_cont=None, encode=False):\n        if(self.bswap != None):\n            x_cont = self.noise(x_cont)\n        encoded = super().forward(x_cat, x_cont)\n        if encode: \n            return encoded # return the representation\n        decoded_trunk = self.decoder(encoded)\n        decoded_conts = self.decoder_cont(decoded_trunk)\n        \n        return decoded_conts","metadata":{"execution":{"iopub.status.busy":"2022-02-13T17:16:02.02463Z","iopub.execute_input":"2022-02-13T17:16:02.024921Z","iopub.status.idle":"2022-02-13T17:16:02.042886Z","shell.execute_reply.started":"2022-02-13T17:16:02.024882Z","shell.execute_reply":"2022-02-13T17:16:02.041981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataloaders","metadata":{}},{"cell_type":"code","source":"# Dataloader for Autoencoder\ndef get_dls_ae(df, fold, SPLIT_IDX):\n    feature_tensor = torch.tensor(df[cont].to_numpy()).cuda()\n    \n    ds_train = UbiquantDataset(feature_tensor[:SPLIT_IDX], feature_tensor[:SPLIT_IDX])\n    ds_val = UbiquantDataset(feature_tensor[SPLIT_IDX:], feature_tensor[SPLIT_IDX:])\n    \n    del df\n    gc.collect()\n\n    dls = DataLoaders.from_dsets(ds_train, ds_val, bs = cfg_ae.bs, dl_type=UbiDL, num_workers=0, drop_last=False)\n\n    return dls\n\n# Dataloador for regular model\ndef get_dls(df, fold, SPLIT_IDX):     \n\n    feature_tensor = torch.tensor(df[cont_embed].to_numpy()).cuda()\n    target_tensor = torch.tensor(df.target.to_numpy()).cuda()\n    \n    ds_train = UbiquantDataset(feature_tensor[:SPLIT_IDX], target_tensor[:SPLIT_IDX])\n    ds_val = UbiquantDataset(feature_tensor[SPLIT_IDX:], target_tensor[SPLIT_IDX:])\n    \n    val_df = df.iloc[SPLIT_IDX:].copy()\n    \n    del df\n    gc.collect()\n\n    dls = DataLoaders.from_dsets(ds_train, ds_val, bs = cfg_ae.bs, dl_type=UbiDL, num_workers=0, drop_last=False)\n\n    return dls, val_df\n\n# Function to obtain autoencoder embeddings\ndef obtain_embeddings(learn, dl):\n    outs = []\n    for batch in dl:\n        with torch.no_grad():\n            learn.model.eval()\n            out = learn.model(tensor([]).cuda(), batch[1], encode=True).cpu().numpy()\n            outs.append(out)\n    outs = np.concatenate(outs)\n    return outs","metadata":{"execution":{"iopub.status.busy":"2022-02-13T17:16:02.044658Z","iopub.execute_input":"2022-02-13T17:16:02.045292Z","iopub.status.idle":"2022-02-13T17:16:02.058598Z","shell.execute_reply.started":"2022-02-13T17:16:02.045251Z","shell.execute_reply":"2022-02-13T17:16:02.057704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"oof = []\ncont_embed = [f'f_{x}' for x in range(cfg_ae.bottleneck)]    \n\nfor fold in range(5):  \n    print('#'*15)\n    print(f'Fold: {fold}')\n    print('#'*15)\n    print()\n    print('Training Autoencoder')\n    \n    df = pd.read_feather(f'{PATH}/fold_{fold}.feather')\n    SPLIT_IDX = splits.iloc[fold, 1]\n    \n    dls = get_dls_ae(df, fold, SPLIT_IDX)\n    \n    # Train Autoencoder and obtain outputs\n    autoencoder = TabularAE(emb_szs=[], n_cont=len(cont), ps=cfg_ae.ps, hidden_size=cfg_ae.bottleneck, bswap=cfg_ae.bswap, layers=cfg_ae.layers, cats={}).cuda()\n    learn_ae = Learner(dls, autoencoder, loss_func=MSELossFlat())\n    learn_ae.fit_one_cycle(50, cbs=[SaveModelCallback(monitor='valid_loss'), EarlyStoppingCallback(monitor='valid_loss', patience=3)])\n    \n    out_train = obtain_embeddings(learn_ae, learn_ae.dls.train)\n    out_valid = obtain_embeddings(learn_ae, learn_ae.dls.valid)\n    out = np.concatenate((out_train, out_valid), axis=0)    \n    \n    torch.save(learn_ae.model, f'model_AE_fold_{fold}.pkl')\n    \n    print('Training model on embeddings')\n    \n    # Create dataset with embeddings\n    df = pd.read_feather(f'{PATH}/fold_{fold}.feather', columns=['target', 'time_id'])\n    df[cont_embed] = out\n\n    del dls\n    del autoencoder\n    del learn_ae\n    del out_train\n    del out_valid\n    del out\n    gc.collect()\n        \n    # Train regular model\n    dls, val_df = get_dls(df, fold, SPLIT_IDX) \n    model = TabularModel(emb_szs=[], n_cont=len(cont_embed), layers=cfg.layers, out_sz=1,\n                         ps=cfg.ps, embed_p=cfg.embed_p, use_bn=True, act_cls=nn.ReLU(inplace=True)).cuda()\n        \n    learn = Learner(dls, model, loss_func=pearson_loss, metrics = CompMetric(val_df))   \n\n    learn.fit_one_cycle(30,  \n                        cbs=[SaveModelCallback(monitor='Pears', comp=np.greater),\n                             EarlyStoppingCallback(monitor='Pears', patience=3, comp=np.greater)])\n    \n    preds, _ = learn.get_preds(dl=learn.dls.valid)\n    oof.append([x[0] for x in preds.tolist()])\n\n    torch.save(learn.model, f'model_fold_{fold}.pkl')\n    \n    # Delete garbage\n    del dls\n    del model\n    del learn\n    del val_df\n    del df\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T17:16:02.060607Z","iopub.execute_input":"2022-02-13T17:16:02.060968Z","iopub.status.idle":"2022-02-13T17:22:46.25329Z","shell.execute_reply.started":"2022-02-13T17:16:02.060927Z","shell.execute_reply":"2022-02-13T17:22:46.250652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# OOF Score","metadata":{}},{"cell_type":"code","source":"# Save OOF\noof = [y for x in oof for y in x]\n\nidx = range(splits.iloc[0, 1], splits.iloc[0, 1]+len(oof))\ndata = list(zip(idx, oof))\noof = pd.DataFrame(data, columns=['index', 'preds'])\n\noof.to_csv('oof.csv', index=False)\n\ndel data\ndel idx\ndel oof\ngc.collect()\n\n### Get Score\ndf = pd.read_feather('../input/ubiquant-trainfeather/train.feather', columns=['time_id', 'target'])\noof = pd.read_csv('./oof.csv').set_index('index')\ndf = df.join(oof, how='inner')\n\n# Mean Pearson Coefficient\nscore_per_time = df.groupby('time_id').apply(pearson_coef)\nscore_equal = np.mean(score_per_time)\nscore_per_time = pd.DataFrame(score_per_time, columns=['preds']).reset_index()\n\nprint(f'OOF score:')\nprint(f'equal weights: {score_equal}')\n\nplt.plot(score_per_time['preds'])\n\ndel df\ndel oof\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T17:22:46.257488Z","iopub.execute_input":"2022-02-13T17:22:46.259914Z","iopub.status.idle":"2022-02-13T17:23:27.301835Z","shell.execute_reply.started":"2022-02-13T17:22:46.259866Z","shell.execute_reply":"2022-02-13T17:23:27.301064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get Predictions","metadata":{}},{"cell_type":"code","source":"# Only uses the last model (trained on most of the data)\nmodel_f4 = torch.load('./model_fold_4.pkl').cuda().eval()\nae_f4 = torch.load('./model_AE_fold_4.pkl').cuda().eval()\n\nimport ubiquant\nenv = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    data = torch.tensor(test_df[cont].to_numpy(), dtype=torch.float).cuda()\n    with torch.no_grad():        \n        out = ae_f4([], data, encode=True)\n        preds_f4 = model_f4([], out)\n    \n    sample_prediction_df['target'] = preds_f4.view(-1).cpu().numpy()  # make your predictions here\n    env.predict(sample_prediction_df)   # register your predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-13T17:23:27.303286Z","iopub.execute_input":"2022-02-13T17:23:27.303539Z","iopub.status.idle":"2022-02-13T17:23:27.542741Z","shell.execute_reply.started":"2022-02-13T17:23:27.303505Z","shell.execute_reply":"2022-02-13T17:23:27.541936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}