{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# If you copy this notebook, please upvote !!\n\n##  Market Prediction with Conv2d\nIt is assumed that there is a complex relationship between the features.\n\nThe relationship is clarified by spatial analysis.\n\nLB = 0.152\n\nDerived from Conv1d ver. The difference in performance from previous version is under verification.\n### Refrence\nSpecial thanks @Lonnie Ubiquant Market Prediction with DNN\n\n- https://www.kaggle.com/lonnieqin/ubiquant-market-prediction-with-dnn## \n\nConv1d version is here\n- https://www.kaggle.com/code/shigeeeru/prediction-including-spatial-info-with-conv1d\n\n### Note\n- delete investment feature\n- add random.set_seed","metadata":{"papermill":{"duration":0.016475,"end_time":"2022-01-25T15:39:01.467349","exception":false,"start_time":"2022-01-25T15:39:01.450874","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom scipy import stats\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.keras import backend as K\n\n### add random seed\ntf.random.set_seed(3)","metadata":{"papermill":{"duration":7.781864,"end_time":"2022-01-25T15:39:09.265726","exception":false,"start_time":"2022-01-25T15:39:01.483862","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-22T00:43:51.159913Z","iopub.execute_input":"2022-03-22T00:43:51.160192Z","iopub.status.idle":"2022-03-22T00:43:56.319246Z","shell.execute_reply.started":"2022-03-22T00:43:51.160115Z","shell.execute_reply":"2022-03-22T00:43:56.31835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration\n\nIf you want to train model, change 'is_trainig' to 'True'.","metadata":{}},{"cell_type":"code","source":"class Config:\n    is_training = False\n#     is_training = True\n    tf_record_dataset_path = \"../input/ump-combinatorialpurgedgroupkfold-tf-record/\"\n    ### change here ###\n    output_dataset_path = \"../input/ump-conv2d-fold5-outputs/\"\nconfig = Config()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T00:43:56.322926Z","iopub.execute_input":"2022-03-22T00:43:56.326354Z","iopub.status.idle":"2022-03-22T00:43:56.334925Z","shell.execute_reply.started":"2022-03-22T00:43:56.326315Z","shell.execute_reply":"2022-03-22T00:43:56.333406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create an IntegerLookup layer for investment_id input","metadata":{"papermill":{"duration":0.016633,"end_time":"2022-01-25T15:39:26.408785","exception":false,"start_time":"2022-01-25T15:39:26.392152","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\ninvestment_ids = pd.read_csv(\"../input/ump-combinatorialpurgedgroupkfold-tf-record/investment_ids.csv\")\ninvestment_id_size = len(investment_ids) + 1\nwith tf.device(\"cpu\"):\n    investment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\n    investment_id_lookup_layer.adapt(investment_ids)","metadata":{"papermill":{"duration":4.105382,"end_time":"2022-01-25T15:39:30.530653","exception":false,"start_time":"2022-01-25T15:39:26.425271","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-22T00:43:56.336984Z","iopub.execute_input":"2022-03-22T00:43:56.337252Z","iopub.status.idle":"2022-03-22T00:43:58.547196Z","shell.execute_reply.started":"2022-03-22T00:43:56.33722Z","shell.execute_reply":"2022-03-22T00:43:58.546414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make Tensorflow dataset","metadata":{"papermill":{"duration":0.018846,"end_time":"2022-01-25T15:39:30.567495","exception":false,"start_time":"2022-01-25T15:39:30.548649","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def decode_function(record_bytes):\n  return tf.io.parse_single_example(\n      # Data\n      record_bytes,\n      # Schema\n      {\n          \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n          \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"investment_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n      }\n  )\n# def preprocess(item):\n#     return (item[\"investment_id\"], item[\"features\"]), item[\"target\"]\ndef preprocess(item):\n    return (item[\"features\"]), item[\"target\"]\ndef make_dataset(file_paths, batch_size=4096, mode=\"train\"):\n    ds = tf.data.TFRecordDataset(file_paths)\n    ds = ds.map(decode_function)\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(batch_size * 4)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds","metadata":{"papermill":{"duration":0.02858,"end_time":"2022-01-25T15:39:30.614302","exception":false,"start_time":"2022-01-25T15:39:30.585722","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-22T00:43:58.54929Z","iopub.execute_input":"2022-03-22T00:43:58.549629Z","iopub.status.idle":"2022-03-22T00:43:58.55763Z","shell.execute_reply.started":"2022-03-22T00:43:58.54959Z","shell.execute_reply":"2022-03-22T00:43:58.556983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling\n\nI use layers.Conv1d. \n\n[source is here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D)\n\n","metadata":{"papermill":{"duration":0.017564,"end_time":"2022-01-25T15:39:30.64918","exception":false,"start_time":"2022-01-25T15:39:30.631616","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def correlation(x, y, axis=-2):\n    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    ###    不偏分散にしたら？？   ###\n    \n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xvar * yvar)\n    return tf.constant(1.0, dtype=x.dtype) - corr\n\ndef get_model():\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    ## Dense 1 ##\n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.1)(feature_x)\n#     ## Dense 2 ##\n#     feature_x = layers.Dense(256, activation='swish')(feature_x)\n#     feature_x = layers.Dropout(0.1)(feature_x)\n    ## convolution 1 ##\n    feature_x = layers.Reshape((-1,1))(feature_x)\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 2 ##\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 3 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    ## convolution2D 1 ##\n    feature_x = layers.Reshape((64,64,1))(feature_x)\n    feature_x = layers.Conv2D(filters=32, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution2D 2 ##\n    feature_x = layers.Conv2D(filters=32, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution2D 3 ##\n    feature_x = layers.Conv2D(filters=32, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    ## flatten ##\n    feature_x = layers.Flatten()(feature_x)\n    ## Dense 3 ##\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n    ## Dense 4 ##\n    x = layers.Dropout(0.1)(x)\n    ## Dense 5 ##    \n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    ## Dense 6 ##\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    ## Dense 7 ##\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse, correlation])\n    return model","metadata":{"papermill":{"duration":0.033569,"end_time":"2022-01-25T15:39:30.700649","exception":false,"start_time":"2022-01-25T15:39:30.66708","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-22T00:43:58.559048Z","iopub.execute_input":"2022-03-22T00:43:58.559538Z","iopub.status.idle":"2022-03-22T00:43:58.580587Z","shell.execute_reply.started":"2022-03-22T00:43:58.559501Z","shell.execute_reply":"2022-03-22T00:43:58.579886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at this Model's architecture.","metadata":{}},{"cell_type":"code","source":"model = get_model()\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)","metadata":{"papermill":{"duration":0.209912,"end_time":"2022-01-25T15:39:30.929112","exception":false,"start_time":"2022-01-25T15:39:30.7192","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-22T00:43:58.583448Z","iopub.execute_input":"2022-03-22T00:43:58.583702Z","iopub.status.idle":"2022-03-22T00:43:59.947157Z","shell.execute_reply.started":"2022-03-22T00:43:58.583671Z","shell.execute_reply":"2022-03-22T00:43:59.943422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":471.71946,"end_time":"2022-01-25T15:47:23.749584","exception":false,"start_time":"2022-01-25T15:39:32.030124","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodels = []\nfor i in range(5):\n    train_path = f\"{config.tf_record_dataset_path}fold_{i}_train.tfrecords\"\n    valid_path = f\"{config.tf_record_dataset_path}fold_{i}_test.tfrecords\"\n    valid_ds = make_dataset([valid_path], mode=\"valid\")\n    print(valid_ds)\n    model = get_model()\n    if config.is_training:\n        train_ds = make_dataset([train_path])\n        checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{i}.tf\", monitor=\"val_correlation\", mode=\"min\", save_best_only=True, save_weights_only=True)\n        early_stop = keras.callbacks.EarlyStopping(patience=10)\n        history = model.fit(train_ds, epochs=50, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n        model.save_weights(f\"model_{i}.tf\")\n        for metric in [\"loss\", \"mae\", \"mape\", \"rmse\", \"correlation\"]:\n            pd.DataFrame(history.history, columns=[metric, f\"val_{metric}\"]).plot()\n            plt.title(metric.upper())\n            plt.show()\n    else:\n        model.load_weights(f\"{config.output_dataset_path}model_{i}.tf\")\n    y_vals = []\n    for _, y in valid_ds:\n        y_vals += list(y.numpy().reshape(-1))\n    y_val = np.array(y_vals)\n    pearson_score = stats.pearsonr(model.predict(valid_ds).reshape(-1), y_val)[0]\n    models.append(model)\n    print(f\"Pearson Score: {pearson_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-03-22T00:43:59.948513Z","iopub.execute_input":"2022-03-22T00:43:59.949319Z","iopub.status.idle":"2022-03-22T00:47:19.026913Z","shell.execute_reply.started":"2022-03-22T00:43:59.949281Z","shell.execute_reply":"2022-03-22T00:47:19.026147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{"papermill":{"duration":2.11441,"end_time":"2022-01-25T15:47:27.649127","exception":false,"start_time":"2022-01-25T15:47:25.534717","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# def preprocess_test(investment_id, feature):\n#     return (investment_id, feature), 0\n\n# def make_test_dataset(feature, investment_id, batch_size=1024):\n#     ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n#     ds = ds.map(preprocess_test)\n#     ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n#     return ds\n\ndef make_test_dataset(feature, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((feature)))\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds\n\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{"papermill":{"duration":1.80736,"end_time":"2022-01-25T15:47:31.24096","exception":false,"start_time":"2022-01-25T15:47:29.4336","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-22T00:47:19.028288Z","iopub.execute_input":"2022-03-22T00:47:19.028527Z","iopub.status.idle":"2022-03-22T00:47:19.040632Z","shell.execute_reply.started":"2022-03-22T00:47:19.028492Z","shell.execute_reply":"2022-03-22T00:47:19.039684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfeatures = [f\"f_{i}\" for i in range(300)]\nfor (test_df, sample_prediction_df) in iter_test:\n    ds = make_test_dataset(test_df[features])\n    sample_prediction_df['target'] = inference(models, ds)\n    env.predict(sample_prediction_df)     \n    ","metadata":{"papermill":{"duration":2.234161,"end_time":"2022-01-25T15:47:35.24214","exception":false,"start_time":"2022-01-25T15:47:33.007979","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-22T00:47:19.041875Z","iopub.execute_input":"2022-03-22T00:47:19.042826Z","iopub.status.idle":"2022-03-22T00:47:20.228476Z","shell.execute_reply.started":"2022-03-22T00:47:19.04279Z","shell.execute_reply":"2022-03-22T00:47:20.227788Z"},"trusted":true},"execution_count":null,"outputs":[]}]}