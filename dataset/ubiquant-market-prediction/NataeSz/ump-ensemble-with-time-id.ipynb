{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# If you like my model, please upvote ‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"This notebook consists of 4 models published by other users and one [mine](https://www.kaggle.com/code/nataliasz/ump-multimodal-nn-with-time-id).<br>\n\nThank you [@ËÄÅËÇ•](https://www.kaggle.com/librauee), [@shigeeeru](https://www.kaggle.com/shigeeeru) for publishing your models and [@Lonnie](https://www.kaggle.com/lonnieqin) for publishing dataset pickle üòä\n\n- [ËÄÅËÇ•](https://www.kaggle.com/librauee) models: \n    - [https://www.kaggle.com/datasets/librauee/dnnmodelnew](https://www.kaggle.com/datasets/librauee/dnnmodelnew)\n    - [https://www.kaggle.com/code/librauee/dnn-base](https://www.kaggle.com/code/librauee/dnn-base)\n    - [https://www.kaggle.com/code/librauee/train-dnn-v2-10fold](https://www.kaggle.com/code/librauee/train-dnn-v2-10fold)\n<br>\n- [shigeeeru](https://www.kaggle.com/shigeeeru) model: \n     - [https://www.kaggle.com/code/shigeeeru/prediction-including-spatial-info-with-conv1d](https://www.kaggle.com/code/shigeeeru/prediction-including-spatial-info-with-conv1d)\n<br>\n- [NataeSz](https://www.kaggle.com/nataliasz)\n    - [https://www.kaggle.com/code/nataliasz/ump-multimodal-nn-with-time-id](https://www.kaggle.com/code/nataliasz/ump-multimodal-nn-with-time-id)\n- [Lonnie](https://www.kaggle.com/lonnieqin) dataset:\n     - [https://www.kaggle.com/datasets/lonnieqin/ubiquant-market-prediction-half-precision-pickle](https://www.kaggle.com/datasets/lonnieqin/ubiquant-market-prediction-half-precision-pickle)","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents\n1. [Load UMP data](#Load-UMP-data)\n2. [EDA](#EDA)\n3. [Preprocess](#Preprocess)\n    - [Drop Short Investments](#Drop-Short-Investments)\n    - [Make TensorFlow Dataset](#Make-TensorFlow-Dataset)\n4. [Model](#Model)\n    - [Build Models](#Build-Models)\n    - [Load models weights](#Load-models-weights)\n    - [Train the Model](#Train-the-Model)\n5. [Predict and Submit](#Predict-and-Submit)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T12:34:26.647426Z","iopub.execute_input":"2022-04-03T12:34:26.64775Z","iopub.status.idle":"2022-04-03T12:34:26.67329Z","shell.execute_reply.started":"2022-04-03T12:34:26.647669Z","shell.execute_reply":"2022-04-03T12:34:26.672246Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport gc\nimport tensorflow as tf\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2022-04-09T19:59:29.636892Z","iopub.execute_input":"2022-04-09T19:59:29.637549Z","iopub.status.idle":"2022-04-09T19:59:34.681843Z","shell.execute_reply.started":"2022-04-09T19:59:29.637451Z","shell.execute_reply":"2022-04-09T19:59:34.681042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load UMP data","metadata":{}},{"cell_type":"markdown","source":"The size of the original csv dataset is 18.55 GB.<br>\nThanks to [@Lonnie](https://www.kaggle.com/lonnieqin), we can load smaller [pickle of the dataset](https://www.kaggle.com/datasets/lonnieqin/ubiquant-market-prediction-half-precision-pickle) üìà <br>","metadata":{}},{"cell_type":"code","source":"%%time\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T19:59:34.685756Z","iopub.execute_input":"2022-04-09T19:59:34.685959Z","iopub.status.idle":"2022-04-09T19:59:49.921247Z","shell.execute_reply.started":"2022-04-09T19:59:34.685934Z","shell.execute_reply":"2022-04-09T19:59:49.92047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['target'].hist(bins = 100, figsize = (20,6));","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby(['investment_id'])['time_id'].count().hist(bins = 100, figsize = (16,6));","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby(['time_id'])['investment_id'].count().hist(bins = 100, figsize = (20,6));","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"markdown","source":"### Drop Short Investments\nLet's remove some short investments. They appear to be less representative.<br>\nStatistical methods like [IQR](https://towardsdatascience.com/why-1-5-in-iqr-method-of-outlier-detection-5d07fdc82097) don't catch all of investments that may disturb results, so I have dropped 2% of the shortest ones.","metadata":{}},{"cell_type":"code","source":"short_investments = train.groupby(['investment_id'])['time_id'].count()\nshort_investments_count = len(short_investments) *0.02\nshort_investments = short_investments[short_investments < short_investments_count].index\nshort_investments = train[train['investment_id'].isin(short_investments)].index","metadata":{"execution":{"iopub.status.busy":"2022-04-09T19:59:49.922498Z","iopub.execute_input":"2022-04-09T19:59:49.923283Z","iopub.status.idle":"2022-04-09T19:59:51.195229Z","shell.execute_reply.started":"2022-04-09T19:59:49.923241Z","shell.execute_reply":"2022-04-09T19:59:51.194522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make TensorFlow Dataset","metadata":{}},{"cell_type":"code","source":"investment_id = train.pop('investment_id')\ntime_id = train.pop(\"time_id\")\ny = train.pop(\"target\")","metadata":{"execution":{"iopub.status.busy":"2022-04-09T19:59:51.197272Z","iopub.execute_input":"2022-04-09T19:59:51.197526Z","iopub.status.idle":"2022-04-09T19:59:51.204463Z","shell.execute_reply.started":"2022-04-09T19:59:51.197492Z","shell.execute_reply":"2022-04-09T19:59:51.203765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_dataset(investment_id, feature, time_id, y=None, batch_size=1024):\n    if y is not None:\n        slices = ((investment_id, feature, time_id), y)\n    else:\n        slices = ((investment_id, feature, time_id))\n        \n    ds = tf.data.Dataset.from_tensor_slices(slices)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-04-09T19:59:51.205886Z","iopub.execute_input":"2022-04-09T19:59:51.206361Z","iopub.status.idle":"2022-04-09T19:59:51.213428Z","shell.execute_reply.started":"2022-04-09T19:59:51.206322Z","shell.execute_reply":"2022-04-09T19:59:51.21275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = make_dataset(investment_id=investment_id, feature=train, time_id=time_id, y=y)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T19:59:51.214753Z","iopub.execute_input":"2022-04-09T19:59:51.215085Z","iopub.status.idle":"2022-04-09T20:00:00.157342Z","shell.execute_reply.started":"2022-04-09T19:59:51.215017Z","shell.execute_reply":"2022-04-09T20:00:00.156626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment_ids = list(investment_id.unique())\ninvestment_id_size = len(investment_ids) + 1\ninvestment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\nwith tf.device(\"cpu\"):\n    investment_id_lookup_layer.adapt(investment_id)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T20:00:00.158775Z","iopub.execute_input":"2022-04-09T20:00:00.159025Z","iopub.status.idle":"2022-04-09T20:01:22.333947Z","shell.execute_reply.started":"2022-04-09T20:00:00.158991Z","shell.execute_reply":"2022-04-09T20:01:22.333125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"### Build Models","metadata":{}},{"cell_type":"code","source":"def get_model():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    output = layers.Dense(1)(x)\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\"])\n    return model\n\n\ndef get_model2():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)    \n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n   # investment_id_x = layers.Dropout(0.65)(investment_id_x)\n   \n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.65)(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n   # x = layers.Dropout(0.2)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n  #  x = layers.Dropout(0.4)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.75)(x)\n    output = layers.Dense(1)(x)\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\"])\n    return model\n\n\ndef get_model3():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n    investment_id_x = layers.Dense(32, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n    #investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.5)(feature_x)\n    feature_x = layers.Dense(128, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.5)(feature_x)\n    feature_x = layers.Dense(64, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.5)(x)\n    output = layers.Dense(1)(x)\n    output = tf.keras.layers.BatchNormalization(axis=1)(output)\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\"])\n    return model\n\ndef get_model5():\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    ## feature ##\n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.1)(feature_x)\n    ## convolution 1 ##\n    feature_x = layers.Reshape((-1,1))(feature_x)\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 2 ##\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 3 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 4 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 5 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=2, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## flatten ##\n    feature_x = layers.Flatten()(feature_x)\n    \n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n    \n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    output = layers.Dense(1)(x)\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\"])\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2022-04-09T20:01:22.335668Z","iopub.execute_input":"2022-04-09T20:01:22.336012Z","iopub.status.idle":"2022-04-09T20:01:22.379971Z","shell.execute_reply.started":"2022-04-09T20:01:22.335967Z","shell.execute_reply":"2022-04-09T20:01:22.379162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model6():\n    investment_id_input = tf.keras.Input(shape=(1,), dtype=tf.uint16, name='investment_id')\n    inv_x = layers.Dense(64, activation='relu')(investment_id_input)\n    inv_x = layers.Dropout(0.2)(inv_x)\n\n    features_input = tf.keras.Input(shape=(300,), dtype=tf.float16, name='features')\n    f_x = layers.Dense(512, activation='relu')(features_input)\n    f_x = layers.Dropout(0.25)(f_x)\n    f_x = layers.Dense(256, activation='relu')(f_x)\n    f_x = layers.Dropout(0.2)(f_x)\n\n    time_id_input = tf.keras.Input(shape=(1,), dtype=tf.uint16, name='time_id')\n    time_x = layers.Dense(64, activation='relu')(time_id_input)\n    time_x = layers.Dropout(0.2)(time_x)\n\n    concatenated = layers.concatenate([inv_x, f_x, time_x], axis=-1)\n    output = layers.Dense(1)(concatenated)\n\n    model = tf.keras.models.Model([investment_id_input, features_input, time_id_input], output, name='model_with_time_id')\n    \n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mse', 'mae', 'mape'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-09T20:01:22.382857Z","iopub.execute_input":"2022-04-09T20:01:22.38305Z","iopub.status.idle":"2022-04-09T20:01:22.393493Z","shell.execute_reply.started":"2022-04-09T20:01:22.383026Z","shell.execute_reply":"2022-04-09T20:01:22.392543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T20:01:22.39607Z","iopub.execute_input":"2022-04-09T20:01:22.39663Z","iopub.status.idle":"2022-04-09T20:01:22.544241Z","shell.execute_reply.started":"2022-04-09T20:01:22.396593Z","shell.execute_reply":"2022-04-09T20:01:22.543254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model6 = get_model6()\nmodel6.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T20:01:34.679386Z","iopub.execute_input":"2022-04-09T20:01:34.679718Z","iopub.status.idle":"2022-04-09T20:01:34.762937Z","shell.execute_reply.started":"2022-04-09T20:01:34.679671Z","shell.execute_reply":"2022-04-09T20:01:34.762254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model6, show_shapes=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load models weights","metadata":{}},{"cell_type":"code","source":"model6.load_weights('../input/ump-multimodal-nn-with-time-id/ns_model_with_time_id.tf')","metadata":{"execution":{"iopub.status.busy":"2022-04-09T20:01:44.578213Z","iopub.execute_input":"2022-04-09T20:01:44.578484Z","iopub.status.idle":"2022-04-09T20:01:44.664257Z","shell.execute_reply.started":"2022-04-09T20:01:44.578454Z","shell.execute_reply":"2022-04-09T20:01:44.663577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = []\n\nfor i in range(5):\n    model = get_model()\n    model.load_weights(f'../input/dnn-base/model_{i}')\n    models.append(model)\n\nfor i in range(10):\n    model = get_model2()\n    model.load_weights(f'../input/train-dnn-v2-10fold/model_{i}')\n    models.append(model)\n    \n    \nfor i in range(10):\n    model = get_model3()\n    model.load_weights(f'../input/dnnmodelnew/model_{i}')\n    models.append(model)\n    \n    \nmodels2 = []\n    \nfor i in range(5):\n    model = get_model5()\n    model.load_weights(f'../input/prediction-including-spatial-info-with-conv1d/model_{i}.tf')\n    models2.append(model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_model_dr04():\n#     features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n    \n#     feature_x = layers.Dense(256, activation='swish')(features_inputs)\n#     feature_x = layers.Dropout(0.4)(feature_x)\n#     feature_x = layers.Dense(128, activation='swish')(feature_x)\n#     feature_x = layers.Dropout(0.4)(feature_x)\n#     feature_x = layers.Dense(64, activation='swish')(feature_x)\n    \n#     x = layers.Concatenate(axis=1)([feature_x])\n#     x = layers.Dropout(0.4)(x)\n#     x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n#     x = layers.Dropout(0.4)(x)\n#     x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n#     x = layers.Dropout(0.4)(x)\n#     x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n#     x = layers.Dropout(0.4)(x)\n#     output = layers.Dense(1)(x)\n#     output = tf.keras.layers.BatchNormalization(axis=1)(output)\n#     model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n#     model.compile(optimizer=tf.optimizers.Adam(0.001),  loss = correlationLoss, metrics=[correlationMetric])\n#     return model\n\n# dr=0.3\n\n# gpus = tf.config.experimental.list_physical_devices('GPU')\n# for gpu in gpus:\n#     print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n    \n\n# def correlationMetric(x, y, axis=-2):\n#     \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n#     x = tf.convert_to_tensor(x)\n#     y = math_ops.cast(y, x.dtype)\n#     n = tf.cast(tf.shape(x)[axis], x.dtype)\n#     xsum = tf.reduce_sum(x, axis=axis)\n#     ysum = tf.reduce_sum(y, axis=axis)\n#     xmean = xsum / n\n#     ymean = ysum / n\n#     xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n#     yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n#     cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n#     corr = cov / tf.sqrt(xvar * yvar)\n#     return tf.constant(1.0, dtype=x.dtype) - corr\n\n\n# def correlationLoss(x,y, axis=-2):\n#     \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n#     while trying to have the same mean and variance\"\"\"\n#     x = tf.convert_to_tensor(x)\n#     y = math_ops.cast(y, x.dtype)\n#     n = tf.cast(tf.shape(x)[axis], x.dtype)\n#     xsum = tf.reduce_sum(x, axis=axis)\n#     ysum = tf.reduce_sum(y, axis=axis)\n#     xmean = xsum / n\n#     ymean = ysum / n\n#     xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n#     ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n#     cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n#     corr = cov / tf.sqrt(xsqsum * ysqsum)\n#     return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr ) , dtype=tf.float32 )\n\n\n# gc.collect()\n\n    \n# models3 = []\n\n# for index in range(10):\n#     model = get_model_dr04()\n#     model.load_weights(f\"../input/model10mse/model_{index}\")\n#     models3.append(model)\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the Model","metadata":{}},{"cell_type":"code","source":"# history = model6.fit(train_ds, epochs=40)  # callbacks=early_stop","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model6.save_weights(f'ns_{model6.name}.tf')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict and Submit","metadata":{}},{"cell_type":"code","source":"def inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\n\ndef preprocess_test_s(feature):\n    return (feature), 0\n\n# def preprocess_test(*args):\n#     return (args), 0\n\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef make_test_dataset2(feature, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((feature)))\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds\n\ndef make_test_dataset3(feature, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices((feature))\n    ds = ds.map(preprocess_test_s)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test()\n\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n    p1 = inference(models, ds)\n    ds2 = make_test_dataset2(test_df[features])\n    p2 = inference(models2, ds2)\n#     ds3 = make_test_dataset3(test_df[features])\n#     p3 = inference(models3, ds3)\n    \n    test_time_id = test_df['row_id'].str.split('_', expand=True).get(key=0).astype(int)\n    ds6 = make_dataset(investment_id=test_df['investment_id'], feature=test_df[features], time_id=test_time_id)\n    p6 = model6.predict([test_df['investment_id'], test_df[features], test_time_id])[:, 0]\n    \n    sample_prediction_df['target'] = p1 * 0.29 + p2 * 0.59 + p6 * 0.12\n    env.predict(sample_prediction_df) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}