{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ubiquant Market Prediction \n## Comparing ML Techniques and LightGBM Finetuning! ‚ö°\n","metadata":{"papermill":{"duration":0.04285,"end_time":"2022-02-05T13:16:48.192238","exception":false,"start_time":"2022-02-05T13:16:48.149388","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"color:turquoise;\n           display:fill;\n           border-radius:5px;\n           background-color:aquamarine;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:turquoise;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.04119,"end_time":"2022-02-05T13:16:48.275452","exception":false,"start_time":"2022-02-05T13:16:48.234262","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Introduction","metadata":{"papermill":{"duration":0.041284,"end_time":"2022-02-05T13:16:48.359996","exception":false,"start_time":"2022-02-05T13:16:48.318712","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"This notebook aims to share initial analysis on which technique may get you a nice score in the Ubiquant Market Prediction Competition. üçÄ\n\nIt does this by trying out different models and comparing their results! \n\nThe models compared are:\n\n\n* Linear Regression\n* Random Forest Regressor\n* Ridge\n* XGBoost\n* LightGBM\n* Support Vector Regressor\n\n\nThe 3 sections of this notebook includes: Import, Model Comparison and LightGBM Finetuning (As the LightGBM, seen the best performance, from our analysis).\n\nHere we go! üòÄ","metadata":{"papermill":{"duration":0.043735,"end_time":"2022-02-05T13:16:48.445502","exception":false,"start_time":"2022-02-05T13:16:48.401767","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"\n<div style=\"color:yellow;\n           display:fill;\n           border-radius:5px;\n           background-color:chartreuse;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:yellow;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.04236,"end_time":"2022-02-05T13:16:48.529834","exception":false,"start_time":"2022-02-05T13:16:48.487474","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Import","metadata":{"papermill":{"duration":0.041235,"end_time":"2022-02-05T13:16:48.613158","exception":false,"start_time":"2022-02-05T13:16:48.571923","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"First we need to import!","metadata":{"papermill":{"duration":0.042799,"end_time":"2022-02-05T13:16:48.697407","exception":false,"start_time":"2022-02-05T13:16:48.654608","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Import libraries\n\nimport datetime\nfrom datetime import datetime\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd\nimport plotly_express as px\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:16:48.795648Z","iopub.status.busy":"2022-02-05T13:16:48.794996Z","iopub.status.idle":"2022-02-05T13:16:51.481954Z","shell.execute_reply":"2022-02-05T13:16:51.480965Z","shell.execute_reply.started":"2022-02-05T11:42:52.136705Z"},"papermill":{"duration":2.743005,"end_time":"2022-02-05T13:16:51.482145","exception":false,"start_time":"2022-02-05T13:16:48.73914","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import competition data\n\ntrain = pd.read_csv('/kaggle/input/ubiquant-market-prediction/train.csv', nrows=10000)\nexample_test = pd.read_csv('/kaggle/input/ubiquant-market-prediction/example_test.csv')\nexample_sample_submission =  pd.read_csv('/kaggle/input/ubiquant-market-prediction/example_sample_submission.csv')","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:16:51.572382Z","iopub.status.busy":"2022-02-05T13:16:51.571758Z","iopub.status.idle":"2022-02-05T13:16:53.17591Z","shell.execute_reply":"2022-02-05T13:16:53.176413Z","shell.execute_reply.started":"2022-02-05T11:43:00.248813Z"},"papermill":{"duration":1.6508,"end_time":"2022-02-05T13:16:53.176595","exception":false,"start_time":"2022-02-05T13:16:51.525795","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:16:53.271468Z","iopub.status.busy":"2022-02-05T13:16:53.270634Z","iopub.status.idle":"2022-02-05T13:16:53.301838Z","shell.execute_reply":"2022-02-05T13:16:53.301243Z","shell.execute_reply.started":"2022-02-05T11:43:04.289146Z"},"papermill":{"duration":0.081418,"end_time":"2022-02-05T13:16:53.301981","exception":false,"start_time":"2022-02-05T13:16:53.220563","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Please note we only imported the first 10,000 rows as an introductory method to compare techniques. And to share the results.\n\nAs the competition progresses futher work could include attempting different import methods. \n\nThat said, lets see how the different models faired! ","metadata":{"papermill":{"duration":0.042715,"end_time":"2022-02-05T13:16:53.387494","exception":false,"start_time":"2022-02-05T13:16:53.344779","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"color:turquoise;\n           display:fill;\n           border-radius:5px;\n           background-color:turquoise;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:turquoise;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.04384,"end_time":"2022-02-05T13:16:53.474576","exception":false,"start_time":"2022-02-05T13:16:53.430736","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Data Prep","metadata":{"papermill":{"duration":0.042655,"end_time":"2022-02-05T13:16:53.560491","exception":false,"start_time":"2022-02-05T13:16:53.517836","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Data Prep for Models\n\ndf = train\n\nscaler = StandardScaler()\nX = np.array(df.drop(['row_id', 'time_id', 'investment_id', 'target'], axis = 1))\nscaler.fit(X)\nX = scaler.transform(X)\n\ny = np.array(df['target'])\n\nprint(X.shape)\nprint(y.shape)In this section we compare different techniques performance\n\nFirst need to do some data prep for the models.","metadata":{"papermill":{"duration":0.044086,"end_time":"2022-02-05T13:16:53.647375","exception":false,"start_time":"2022-02-05T13:16:53.603289","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Data Prep for Models\n\ndf = train\n\nscaler = StandardScaler()\nX = np.array(df.drop(['row_id', 'time_id', 'investment_id', 'target'], axis = 1))\nscaler.fit(X)\nX = scaler.transform(X)\n\ny = np.array(df['target'])\n\nprint(X.shape)\nprint(y.shape)","metadata":{"papermill":{"duration":1.040353,"end_time":"2022-02-05T13:16:54.73048","exception":false,"start_time":"2022-02-05T13:16:53.690127","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-06T12:32:32.28022Z","iopub.execute_input":"2022-02-06T12:32:32.28091Z","iopub.status.idle":"2022-02-06T12:32:32.373899Z","shell.execute_reply.started":"2022-02-06T12:32:32.280749Z","shell.execute_reply":"2022-02-06T12:32:32.372537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That is the inital prep complete!","metadata":{"papermill":{"duration":0.042671,"end_time":"2022-02-05T13:16:54.816484","exception":false,"start_time":"2022-02-05T13:16:54.773813","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"\n<div style=\"color:yellow;\n           display:fill;\n           border-radius:5px;\n           background-color:yellow;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:yellow;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.04319,"end_time":"2022-02-05T13:16:54.902421","exception":false,"start_time":"2022-02-05T13:16:54.859231","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Linear Regression","metadata":{"papermill":{"duration":0.04232,"end_time":"2022-02-05T13:16:54.988085","exception":false,"start_time":"2022-02-05T13:16:54.945765","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"By fitting a linear equation to observed data, linear regression seeks to model the relationship between two variables. One variable is regarded as an explanatory variable, while the other is regarded as a dependent variable. A modeller might, for example, use a linear regression model to match people's weights to their heights.\n\n(For futher details our recommended read is by Yale, linked below).\n\n\nLets try it out! üí°","metadata":{"papermill":{"duration":0.043355,"end_time":"2022-02-05T13:16:55.074883","exception":false,"start_time":"2022-02-05T13:16:55.031528","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:16:55.165966Z","iopub.status.busy":"2022-02-05T13:16:55.164075Z","iopub.status.idle":"2022-02-05T13:16:55.824127Z","shell.execute_reply":"2022-02-05T13:16:55.824671Z","shell.execute_reply.started":"2022-01-29T00:24:24.619866Z"},"papermill":{"duration":0.706305,"end_time":"2022-02-05T13:16:55.824874","exception":false,"start_time":"2022-02-05T13:16:55.118569","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(X_train, y_train)\nlr_confidence = lr.score(X_test, y_test)\nlr_confidence","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:16:55.914267Z","iopub.status.busy":"2022-02-05T13:16:55.913628Z","iopub.status.idle":"2022-02-05T13:16:56.39807Z","shell.execute_reply":"2022-02-05T13:16:56.399092Z","shell.execute_reply.started":"2022-01-29T00:24:24.961169Z"},"papermill":{"duration":0.531573,"end_time":"2022-02-05T13:16:56.399368","exception":false,"start_time":"2022-02-05T13:16:55.867795","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"color:fuchsia;\n           display:fill;\n           border-radius:5px;\n           background-color:fuchsia;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:fuchsia;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.057386,"end_time":"2022-02-05T13:16:56.53709","exception":false,"start_time":"2022-02-05T13:16:56.479704","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# RandomForestRegressor","metadata":{"papermill":{"duration":0.043058,"end_time":"2022-02-05T13:16:56.623705","exception":false,"start_time":"2022-02-05T13:16:56.580647","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"A random forest is a meta estimator that employs averaging to increase predicted accuracy and control over-fitting by fitting a number of classification decision trees on various sub-samples of the dataset. \n\nIf bootstrap=True (default), the sub-sample size is regulated by the max samples argument; otherwise, the entire dataset is utilised to create each tree.\n\n(For futher details our recommended read is by Scikit - Learn, linked below).\n\nLet's try it out! ü™Ñ","metadata":{"papermill":{"duration":0.043011,"end_time":"2022-02-05T13:16:56.710273","exception":false,"start_time":"2022-02-05T13:16:56.667262","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:16:56.802977Z","iopub.status.busy":"2022-02-05T13:16:56.802322Z","iopub.status.idle":"2022-02-05T13:16:57.542339Z","shell.execute_reply":"2022-02-05T13:16:57.541771Z","shell.execute_reply.started":"2022-01-29T00:24:25.363471Z"},"papermill":{"duration":0.788961,"end_time":"2022-02-05T13:16:57.542489","exception":false,"start_time":"2022-02-05T13:16:56.753528","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestRegressor()\nrf.fit(X_train, y_train)\nrf_confidence = rf.score(X_test, y_test)\nrf_confidence","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:16:57.835575Z","iopub.status.busy":"2022-02-05T13:16:57.834887Z","iopub.status.idle":"2022-02-05T13:20:53.913505Z","shell.execute_reply":"2022-02-05T13:20:53.914053Z","shell.execute_reply.started":"2022-01-29T00:24:25.706457Z"},"papermill":{"duration":236.327614,"end_time":"2022-02-05T13:20:53.914233","exception":false,"start_time":"2022-02-05T13:16:57.586619","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"color:orange;\n           display:fill;\n           border-radius:5px;\n           background-color:orange;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:orange;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.044211,"end_time":"2022-02-05T13:20:54.002715","exception":false,"start_time":"2022-02-05T13:20:53.958504","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Ridge","metadata":{"papermill":{"duration":0.045003,"end_time":"2022-02-05T13:20:54.093046","exception":false,"start_time":"2022-02-05T13:20:54.048043","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"This model handles a regression problem in which the loss function is the linear least squares function and the l2-norm is used for regularisation. \n\nRidge Regression or Tikhonov regularisation are other terms for the same thing. When y is a 2d-array of shape (n samples, n targets), this estimator contains built-in support for multi-variate regression. \n\n(For futher details our recommended read is by Scikit - Learn, linked below).\n\nLets give it a go! üëÄ","metadata":{"papermill":{"duration":0.044558,"end_time":"2022-02-05T13:20:54.181513","exception":false,"start_time":"2022-02-05T13:20:54.136955","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:20:54.272653Z","iopub.status.busy":"2022-02-05T13:20:54.271679Z","iopub.status.idle":"2022-02-05T13:20:55.05603Z","shell.execute_reply":"2022-02-05T13:20:55.056509Z","shell.execute_reply.started":"2022-01-29T00:26:01.313158Z"},"papermill":{"duration":0.83154,"end_time":"2022-02-05T13:20:55.056704","exception":false,"start_time":"2022-02-05T13:20:54.225164","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rg = Ridge()\nrg.fit(X_train, y_train)\nrg_confidence = rg.score(X_test, y_test)\nrg_confidence","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:20:55.148693Z","iopub.status.busy":"2022-02-05T13:20:55.147732Z","iopub.status.idle":"2022-02-05T13:20:55.540919Z","shell.execute_reply":"2022-02-05T13:20:55.541672Z","shell.execute_reply.started":"2022-01-29T00:26:01.655562Z"},"papermill":{"duration":0.441016,"end_time":"2022-02-05T13:20:55.541879","exception":false,"start_time":"2022-02-05T13:20:55.100863","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"color:blueviolet;\n           display:fill;\n           border-radius:5px;\n           background-color:blueviolet;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:blueviolet;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.055258,"end_time":"2022-02-05T13:20:55.680847","exception":false,"start_time":"2022-02-05T13:20:55.625589","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# XGBoost","metadata":{"papermill":{"duration":0.043658,"end_time":"2022-02-05T13:20:55.76849","exception":false,"start_time":"2022-02-05T13:20:55.724832","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"XGBoost is a distributed gradient boosting toolkit that has been tuned for efficiency, flexibility, and portability. It uses the Gradient Boosting framework to create machine learning algorithms. \n\nXGBoost is a parallel tree boosting (also known as GBDT, GBM) algorithm that solves a variety of data science issues quickly and accurately. The same algorithm may tackle problems with billions of examples in a distributed environment (Hadoop, SGE, MPI). \n\n(For futher details our recommended read is by Scikit - Learn, linked below).\n\nLets have a go! üëì","metadata":{"papermill":{"duration":0.044316,"end_time":"2022-02-05T13:20:55.857011","exception":false,"start_time":"2022-02-05T13:20:55.812695","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Import xgboost\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\n\n# Create the training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\n# Instantiate the XGBRegressor: xg_reg\nxg_reg = xgb.XGBRegressor(objective=\"reg:linear\", n_estimators=10, seed=123)\n\n# Fit the regressor to the training set\nxg_reg.fit(X_train, y_train)\n\n# Predict the labels of the test set: preds\npreds = xg_reg.predict(X_test)\n\n# Compute the rmse: rmse\nrmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))\n\n\nXGB_confidence = xg_reg.score(X_test, y_test)\nXGB_confidence","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:20:55.954503Z","iopub.status.busy":"2022-02-05T13:20:55.953734Z","iopub.status.idle":"2022-02-05T13:20:59.182937Z","shell.execute_reply":"2022-02-05T13:20:59.183566Z","shell.execute_reply.started":"2022-02-05T11:57:09.302692Z"},"papermill":{"duration":3.28178,"end_time":"2022-02-05T13:20:59.183831","exception":false,"start_time":"2022-02-05T13:20:55.902051","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the training and testing sets into DMatrixes: DM_train, DM_test\nDM_train = xgb.DMatrix(data=X_train,label=y_train)\nDM_test =  xgb.DMatrix(data=X_test,label=y_test)\n\n# Create the parameter dictionary: params\nparams = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\n\n# Train the model: xg_reg\nxg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)\n\n# Predict the labels of the test set: preds\npreds = xg_reg.predict(DM_test)\n\n# Compute and print the RMSE\nrmse = np.sqrt(mean_squared_error(y_test,preds))\nprint(\"RMSE: %f\" % (rmse))\n\nprint('Prediction: %.3f' % preds[0])","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:20:59.283879Z","iopub.status.busy":"2022-02-05T13:20:59.283135Z","iopub.status.idle":"2022-02-05T13:20:59.628814Z","shell.execute_reply":"2022-02-05T13:20:59.62959Z","shell.execute_reply.started":"2022-01-29T00:26:03.463718Z"},"papermill":{"duration":0.398697,"end_time":"2022-02-05T13:20:59.629844","exception":false,"start_time":"2022-02-05T13:20:59.231147","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"color:blue;\n           display:fill;\n           border-radius:5px;\n           background-color:blue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:blue;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.044784,"end_time":"2022-02-05T13:20:59.723031","exception":false,"start_time":"2022-02-05T13:20:59.678247","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# LightGBM","metadata":{"papermill":{"duration":0.044988,"end_time":"2022-02-05T13:20:59.813115","exception":false,"start_time":"2022-02-05T13:20:59.768127","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"LightGBM is a decision tree-based gradient boosting framework that improves model efficiency while reducing memory utilisation.\n\nIt employs two innovative techniques: Gradient-based One Side Sampling and Exclusive Feature Bundling (EFB), Which address the drawbacks of the histogram-based approach employed in most GBDT (Gradient Boosting Decision Tree) frameworks. The properties of LightGBM Algorithm are formed by the two methodologies of GOSS and EFB. They work together to make the model run smoothly and give it an advantage over competing GBDT frameworks. \n\n(For futher details our recommended read is by Light GBM, linked below) \n\n\nLets try it out!ü§ó","metadata":{"papermill":{"duration":0.044777,"end_time":"2022-02-05T13:20:59.902935","exception":false,"start_time":"2022-02-05T13:20:59.858158","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\n\n\n# Create the training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nLGBM = LGBMRegressor()\n\n# Fit the regressor to the training set\nLGBM.fit(X, y)\n\nLGBM_confidence = LGBM.score(X_test, y_test)\nLGBM_confidence","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:20:59.998847Z","iopub.status.busy":"2022-02-05T13:20:59.998209Z","iopub.status.idle":"2022-02-05T13:21:03.613697Z","shell.execute_reply":"2022-02-05T13:21:03.614308Z","shell.execute_reply.started":"2022-01-29T00:26:03.674022Z"},"papermill":{"duration":3.666273,"end_time":"2022-02-05T13:21:03.614517","exception":false,"start_time":"2022-02-05T13:20:59.948244","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"color:blue;\n           display:fill;\n           border-radius:5px;\n           background-color:coral;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:yellow;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.045886,"end_time":"2022-02-05T13:21:03.709602","exception":false,"start_time":"2022-02-05T13:21:03.663716","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Support Vector Regressor ","metadata":{"papermill":{"duration":0.045591,"end_time":"2022-02-05T13:21:03.802954","exception":false,"start_time":"2022-02-05T13:21:03.757363","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Support vector regression is a feature of support vector machines. In other words, there is a concept known as a support vector machine that may be used to analyse both regression and classification data.\n\nSupport vector regression (SVR), is distinguished by the use of kernels, sparse solutions, and VC control of the margin and number of support vectors. \n\n(For futher details our recommended read is by Awad & Khanna, linked below) \n\n\nLets give it a go! üí™\n","metadata":{"papermill":{"duration":0.045642,"end_time":"2022-02-05T13:21:03.894436","exception":false,"start_time":"2022-02-05T13:21:03.848794","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:21:03.991983Z","iopub.status.busy":"2022-02-05T13:21:03.991259Z","iopub.status.idle":"2022-02-05T13:21:04.730186Z","shell.execute_reply":"2022-02-05T13:21:04.730692Z","shell.execute_reply.started":"2022-01-29T00:26:06.232562Z"},"papermill":{"duration":0.790449,"end_time":"2022-02-05T13:21:04.730901","exception":false,"start_time":"2022-02-05T13:21:03.940452","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svr = SVR()\nsvr.fit(X_train, y_train)\nsvr_confidence = svr.score(X_test, y_test)\nsvr_confidence","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:21:04.827181Z","iopub.status.busy":"2022-02-05T13:21:04.826268Z","iopub.status.idle":"2022-02-05T13:21:42.60435Z","shell.execute_reply":"2022-02-05T13:21:42.604912Z","shell.execute_reply.started":"2022-01-29T00:26:06.595907Z"},"papermill":{"duration":37.827757,"end_time":"2022-02-05T13:21:42.605088","exception":false,"start_time":"2022-02-05T13:21:04.777331","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"color:blue;\n           display:fill;\n           border-radius:5px;\n           background-color:yellow;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:yellow;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.046589,"end_time":"2022-02-05T13:21:42.698592","exception":false,"start_time":"2022-02-05T13:21:42.652003","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Model Comparision ‚ú®","metadata":{"papermill":{"duration":0.046217,"end_time":"2022-02-05T13:21:42.791319","exception":false,"start_time":"2022-02-05T13:21:42.745102","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Lets compare each ML models performane üîç","metadata":{"papermill":{"duration":0.046247,"end_time":"2022-02-05T13:21:42.884641","exception":false,"start_time":"2022-02-05T13:21:42.838394","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Compare performence of each model\n\nnames = ['Linear Regression', 'Random Forest', 'Ridge', 'SVR', 'XGBoost', 'LGBM']\ncolumns = ['model', 'accuracy']\nscores = [lr_confidence, rf_confidence, rg_confidence, svr_confidence, XGB_confidence, LGBM_confidence]\nalg_vs_score = pd.DataFrame([[x, y] for x, y in zip(names, scores)], columns = columns)","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:21:42.981783Z","iopub.status.busy":"2022-02-05T13:21:42.98115Z","iopub.status.idle":"2022-02-05T13:21:42.986912Z","shell.execute_reply":"2022-02-05T13:21:42.987465Z","shell.execute_reply.started":"2022-01-29T00:26:15.333834Z"},"papermill":{"duration":0.055437,"end_time":"2022-02-05T13:21:42.987627","exception":false,"start_time":"2022-02-05T13:21:42.93219","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\nfig = px.bar(alg_vs_score, y='accuracy', x='model',\n            title=\"Performance of Different Models\", color=\"model\", hover_name=\"accuracy\",\n             color_discrete_sequence=px.colors.qualitative.Pastel\n             )\n\nfig.show()","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:21:43.085345Z","iopub.status.busy":"2022-02-05T13:21:43.084672Z","iopub.status.idle":"2022-02-05T13:21:44.072466Z","shell.execute_reply":"2022-02-05T13:21:44.072965Z","shell.execute_reply.started":"2022-01-29T00:26:15.349081Z"},"papermill":{"duration":1.038653,"end_time":"2022-02-05T13:21:44.073146","exception":false,"start_time":"2022-02-05T13:21:43.034493","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see the Light GBM performed the best! ü•á‚≠ê","metadata":{"papermill":{"duration":0.053694,"end_time":"2022-02-05T13:21:44.181975","exception":false,"start_time":"2022-02-05T13:21:44.128281","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"\n<div style=\"color:blue;\n           display:fill;\n           border-radius:5px;\n           background-color:lightgreen;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:lightgreen;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.053902,"end_time":"2022-02-05T13:21:44.289916","exception":false,"start_time":"2022-02-05T13:21:44.236014","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Fine Tuning LightGBM Hyperparameters\n","metadata":{"papermill":{"duration":0.053865,"end_time":"2022-02-05T13:21:44.397572","exception":false,"start_time":"2022-02-05T13:21:44.343707","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Lets fine tune to see if we can improve the score! üß†","metadata":{"execution":{"iopub.execute_input":"2022-02-05T12:16:48.444111Z","iopub.status.busy":"2022-02-05T12:16:48.443814Z","iopub.status.idle":"2022-02-05T12:16:48.449488Z","shell.execute_reply":"2022-02-05T12:16:48.448324Z","shell.execute_reply.started":"2022-02-05T12:16:48.444078Z"},"papermill":{"duration":0.068186,"end_time":"2022-02-05T13:21:44.522427","exception":false,"start_time":"2022-02-05T13:21:44.454241","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Tuning Number of Trees\n\nAn important hyperparameter for the LightGBM ensemble algorithm is the number of decision trees used in the ensemble.\n\nRecall that decision trees are added to the model sequentially in an effort to correct and improve upon the predictions made by prior trees. As such, more trees are often better.\n\nThe number of trees can be set via the ‚Äún_estimators‚Äù argument and defaults to 100.\n\nThe example below explores the effect of the number of trees with values between 10 to 5,000.","metadata":{"papermill":{"duration":0.053584,"end_time":"2022-02-05T13:21:44.638643","exception":false,"start_time":"2022-02-05T13:21:44.585059","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df = train\nscaler = StandardScaler()\nX = np.array(df.drop(['target'], 1))\nscaler.fit(X)\nX = scaler.transform(X)\nX = np.array(df.drop(['target'], 1))\ny = np.array(df['target'])","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:21:44.758664Z","iopub.status.busy":"2022-02-05T13:21:44.75328Z","iopub.status.idle":"2022-02-05T13:21:45.640692Z","shell.execute_reply":"2022-02-05T13:21:45.640004Z","shell.execute_reply.started":"2022-01-29T00:41:04.059927Z"},"papermill":{"duration":0.94849,"end_time":"2022-02-05T13:21:45.64087","exception":false,"start_time":"2022-02-05T13:21:44.69238","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:21:45.755183Z","iopub.status.busy":"2022-02-05T13:21:45.754244Z","iopub.status.idle":"2022-02-05T13:21:45.758155Z","shell.execute_reply":"2022-02-05T13:21:45.757599Z","shell.execute_reply.started":"2022-01-29T00:41:07.553769Z"},"papermill":{"duration":0.063243,"end_time":"2022-02-05T13:21:45.758297","exception":false,"start_time":"2022-02-05T13:21:45.695054","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:21:45.873305Z","iopub.status.busy":"2022-02-05T13:21:45.872292Z","iopub.status.idle":"2022-02-05T13:21:45.875982Z","shell.execute_reply":"2022-02-05T13:21:45.876416Z","shell.execute_reply.started":"2022-01-29T00:41:09.515345Z"},"papermill":{"duration":0.063631,"end_time":"2022-02-05T13:21:45.876581","exception":false,"start_time":"2022-02-05T13:21:45.81295","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# explore lightgbm boosting type effect on performance\nfrom numpy import arange\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold, cross_val_score\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    trees = [10, 50, 100, 500, 1000, 5000]\n    for n in trees:\n        models[str(n)] = LGBMRegressor(n_estimators=n)\n    return models\n \n# evaluate a give model using cross-validation\ndef evaluate_model(model):\n    cv = KFold(n_splits=10, random_state=1)\n    scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n    return scores\n \n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n    \n# plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:21:45.991436Z","iopub.status.busy":"2022-02-05T13:21:45.990433Z","iopub.status.idle":"2022-02-05T13:43:47.325523Z","shell.execute_reply":"2022-02-05T13:43:47.324457Z","shell.execute_reply.started":"2022-01-29T01:09:22.650011Z"},"papermill":{"duration":1321.393812,"end_time":"2022-02-05T13:43:47.325841","exception":false,"start_time":"2022-02-05T13:21:45.932029","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tuning Tree Depth\n\nVarying the depth of each tree added to the ensemble is another important hyperparameter for gradient boosting.\n\nThe tree depth controls how specialized each tree is to the training dataset: how general or overfit it might be. Trees are preferred that are not too shallow and general (like AdaBoost) and not too deep and specialized (like bootstrap aggregation).\n\nGradient boosting generally performs well with trees that have a modest depth, finding a balance between skill and generality.\n\nTree depth is controlled via the ‚Äúmax_depth‚Äù argument and defaults to an unspecified value as the default mechanism for controlling how complex trees are is to use the number of leaf nodes.\n\nThere are two main ways to control tree complexity: the max depth of the trees and the maximum number of terminal nodes (leaves) in the tree. In this case, we are exploring the number of leaves so we need to increase the number of leaves to support deeper trees by setting the ‚Äúnum_leaves‚Äù argument.\n\nThe example below explores tree depths between 1 and 10 and the effect on model performance.","metadata":{"papermill":{"duration":0.057245,"end_time":"2022-02-05T13:43:47.446858","exception":false,"start_time":"2022-02-05T13:43:47.389613","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    for i in range(1,11):\n        models[str(i)] = LGBMRegressor(max_depth=i, num_leaves=2**i)\n    return models\n \n# evaluate a give model using cross-validation\ndef evaluate_model(model):\n    cv = KFold(n_splits=10, random_state=1)\n    scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n    return scores\n \n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n# plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:43:47.574624Z","iopub.status.busy":"2022-02-05T13:43:47.573877Z","iopub.status.idle":"2022-02-05T13:48:04.236964Z","shell.execute_reply":"2022-02-05T13:48:04.236235Z","shell.execute_reply.started":"2022-01-29T01:27:25.019797Z"},"papermill":{"duration":256.73287,"end_time":"2022-02-05T13:48:04.237132","exception":false,"start_time":"2022-02-05T13:43:47.504262","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tuning Learning Rate\n\nLearning rate controls the amount of contribution that each model has on the ensemble prediction.\n\nSmaller rates may require more decision trees in the ensemble.\n\nThe learning rate can be controlled via the ‚Äúlearning_rate‚Äù argument and defaults to 0.1.\n\nThe example below explores the learning rate and compares the effect of values between 0.0001 and 1.0.","metadata":{"papermill":{"duration":0.060867,"end_time":"2022-02-05T13:48:04.360385","exception":false,"start_time":"2022-02-05T13:48:04.299518","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    rates = [0.0001, 0.001, 0.01, 0.1, 1.0]\n    for r in rates:\n        key = '%.4f' % r\n        models[key] = LGBMRegressor(learning_rate=r)\n    return models\n \n# evaluate a give model using cross-validation\ndef evaluate_model(model):\n    cv = KFold(n_splits=10, random_state=1)\n    scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n    return scores\n \n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n# plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:48:04.496Z","iopub.status.busy":"2022-02-05T13:48:04.495312Z","iopub.status.idle":"2022-02-05T13:50:33.906684Z","shell.execute_reply":"2022-02-05T13:50:33.905733Z","shell.execute_reply.started":"2022-01-29T01:30:06.420614Z"},"papermill":{"duration":149.485414,"end_time":"2022-02-05T13:50:33.906945","exception":false,"start_time":"2022-02-05T13:48:04.421531","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tuning Boosting Type\n\nA feature of LightGBM is that it supports a number of different boosting algorithms, referred to as boosting types.\n\nThe boosting type can be specified via the ‚Äúboosting_type‚Äù argument and take a string to specify the type. The options include:\n\n‚Äògbdt‚Äò: Gradient Boosting Decision Tree (GDBT).\n‚Äòdart‚Äò: Dropouts meet Multiple Additive Regression Trees (DART).\n‚Äògoss‚Äò: Gradient-based One-Side Sampling (GOSS).\nThe default is GDBT, which is the classical gradient boosting algorithm.","metadata":{"papermill":{"duration":0.063042,"end_time":"2022-02-05T13:50:34.034341","exception":false,"start_time":"2022-02-05T13:50:33.971299","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    types = ['gbdt', 'dart', 'goss']\n    for t in types:\n        models[t] = LGBMRegressor(boosting_type=t)\n    return models\n \n\ndef evaluate_model(model):\n    cv = KFold(n_splits=10, random_state=1)\n    scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n    return scores\n \n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n    \n# plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()\n","metadata":{"execution":{"iopub.execute_input":"2022-02-05T13:50:34.166245Z","iopub.status.busy":"2022-02-05T13:50:34.165596Z","iopub.status.idle":"2022-02-05T13:51:59.479289Z","shell.execute_reply":"2022-02-05T13:51:59.478445Z","shell.execute_reply.started":"2022-01-29T01:33:07.84836Z"},"papermill":{"duration":85.380027,"end_time":"2022-02-05T13:51:59.479536","exception":false,"start_time":"2022-02-05T13:50:34.099509","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:turquoise;\n           display:fill;\n           border-radius:5px;\n           background-color:deeppink;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:turquoise;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.067659,"end_time":"2022-02-05T13:51:59.635729","exception":false,"start_time":"2022-02-05T13:51:59.56807","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Conclusion","metadata":{"papermill":{"duration":0.064446,"end_time":"2022-02-05T13:51:59.765585","exception":false,"start_time":"2022-02-05T13:51:59.701139","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Thanks for reading this notebook! We wish you the best in the competition üí•\n\nPlease give it an upvote - if you found it  useful insight into ML models  you could use. And LGBM finetuning.\n\nThanks üëç","metadata":{"papermill":{"duration":0.066089,"end_time":"2022-02-05T13:51:59.896808","exception":false,"start_time":"2022-02-05T13:51:59.830719","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"color:turquoise;\n           display:fill;\n           border-radius:5px;\n           background-color:royalblue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:turquoise;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.064552,"end_time":"2022-02-05T13:52:00.026067","exception":false,"start_time":"2022-02-05T13:51:59.961515","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Recommended Reads\n","metadata":{"papermill":{"duration":0.065257,"end_time":"2022-02-05T13:52:00.158203","exception":false,"start_time":"2022-02-05T13:52:00.092946","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Here are some links to reading material we found useful and would recommend: \n- https://www.geeksforgeeks.org/lightgbm-light-gradient-boosting-machine/\n- https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n- https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/\n- https://machinelearningmastery.com/light-gradient-boosted-machine-lightgbm-ensemble/\n- http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm\n- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n- https://link.springer.com/chapter/10.1007/978-1-4302-5990-9_4\n","metadata":{"papermill":{"duration":0.064761,"end_time":"2022-02-05T13:52:00.288348","exception":false,"start_time":"2022-02-05T13:52:00.223587","status":"completed"},"tags":[]}}]}