{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nfrom scipy.stats import pearsonr\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import norm, skew, kurtosis\nfrom scipy import stats\nimport datatable as dt\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport plotly.express as px\nfrom scipy import stats\nfrom scipy.stats import norm, skew, kurtosis\nimport gc\nimport warnings\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom scipy import stats\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.keras import backend as K\nfrom scipy.special import comb\nfrom itertools import combinations\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-03-31T23:30:03.225437Z","iopub.execute_input":"2022-03-31T23:30:03.226061Z","iopub.status.idle":"2022-03-31T23:30:12.268237Z","shell.execute_reply.started":"2022-03-31T23:30:03.226008Z","shell.execute_reply":"2022-03-31T23:30:12.267481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Problem Definition: **\n\nThe aim of this competition is to predict obfuscated (unclear) metric which is relevant for making trading decisions from the features derived from real historic data from thousands of investments.\n\nThere are 3579 unique investments in training set, but investments in training set doesn't necessarily appear in public or private test set. There are also 1211 unique time IDs in training set. All of the investments doesn't necessarily appear in all time IDs.","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"**Columns of the train data-**\n\nrow_id - A unique identifier for the row. \n\ntime_id - The ID code for the time the data was gathered. The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set.\n\ninvestment_id - The ID code for an investment. Not all investment have data in all time IDs.\n\ntarget - The target.\n\n[f_0:f_299] - Anonymized features generated from market data.","metadata":{}},{"cell_type":"markdown","source":"## **The goal of this competition is to build a model that forecasts an investment's return rate.**\n\nOur submission for this project is being scored based on the mean of Pearson correlation coefficient for each time ID. We would like to measure our success by comparing our score with the scores of top performing teams on the Leaderboard. By the end of this kaggle project, we aim to score high and keep ourselves on top of the leaderboard.","metadata":{}},{"cell_type":"markdown","source":"# Reading the Data","metadata":{}},{"cell_type":"code","source":"# df = pd.read_parquet('../input/low-mem-parquet/train_low_mem.parquet')\n# train=df[:20000]\n# df2=df\n# df.drop(['row_id'],axis=1, inplace=True)\n# #test=pd.read_csv('../input/ubiquant-market-prediction/example_test.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-28T22:58:49.721868Z","iopub.execute_input":"2022-03-28T22:58:49.722126Z","iopub.status.idle":"2022-03-28T22:58:49.728795Z","shell.execute_reply.started":"2022-03-28T22:58:49.722092Z","shell.execute_reply":"2022-03-28T22:58:49.727612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class CombinatorialPurgedGroupKFold():\n#     def __init__(self, n_splits = 6, n_test_splits = 2, purge = 1, pctEmbargo = 0.01, **kwargs):\n#         self.n_splits = n_splits\n#         self.n_test_splits = n_test_splits\n#         self.purge = purge\n#         self.pctEmbargo = pctEmbargo\n        \n#     def split(self, X, y = None, groups = None):\n#         if groups is None:\n#             raise ValueError(\n#                 \"The 'groups' parameter should not be None\")\n            \n#         u, ind = np.unique(groups, return_index = True)\n#         unique_groups = u[np.argsort(ind)]\n#         n_groups = len(unique_groups)\n#         group_dict = {}\n#         for idx in range(len(X)):\n#             if groups[idx] in group_dict:\n#                 group_dict[groups[idx]].append(idx)\n#             else:\n#                 group_dict[groups[idx]] = [idx]\n                \n#         n_folds = comb(self.n_splits, self.n_test_splits, exact = True)\n#         if n_folds > n_groups:\n#             raise ValueError(\n#                 (\"Cannot have number of folds={0} greater than\"\n#                  \" the number of groups={1}\").format(n_folds,\n#                                                      n_groups))\n            \n#         mbrg = int(n_groups * self.pctEmbargo)\n#         if mbrg < 0:\n#             raise ValueError(\n#                 \"The number of 'embargoed' groups should not be negative\")\n        \n#         split_dict = {}\n#         group_test_size = n_groups // self.n_splits\n#         for split in range(self.n_splits):\n#             if split == self.n_splits - 1:\n#                 split_dict[split] = unique_groups[int(split * group_test_size):].tolist()\n#             else:\n#                 split_dict[split] = unique_groups[int(split * group_test_size):int((split + 1) * group_test_size)].tolist()\n        \n#         for test_splits in combinations(range(self.n_splits), self.n_test_splits):\n#             test_groups = []\n#             banned_groups = []\n#             for split in test_splits:\n#                 test_groups += split_dict[split]\n#                 banned_groups += unique_groups[split_dict[split][0] - self.purge:split_dict[split][0]].tolist()\n#                 banned_groups += unique_groups[split_dict[split][-1] + 1:split_dict[split][-1] + self.purge + mbrg + 1].tolist()\n#             train_groups = [i for i in unique_groups if (i not in banned_groups) and (i not in test_groups)]\n\n#             train_idx = []\n#             test_idx = []\n#             for train_group in train_groups:\n#                 train_idx += group_dict[train_group]\n#             for test_group in test_groups:\n#                 test_idx += group_dict[test_group]\n#             yield train_idx, test_idx","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.729762Z","iopub.execute_input":"2022-03-28T22:58:49.729987Z","iopub.status.idle":"2022-03-28T22:58:49.742464Z","shell.execute_reply.started":"2022-03-28T22:58:49.729955Z","shell.execute_reply":"2022-03-28T22:58:49.74176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# investment_id = train.pop(\"investment_id\")\n# time_id = train.pop(\"time_id\")\n# y = train.pop(\"target\")\n# def create_record(i):\n#     dic = {}\n#     dic[f\"features\"] = tf.train.Feature(float_list=tf.train.FloatList(value=list(train.iloc[i])))\n#     dic[\"time_id\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[time_id.iloc[i]]))\n#     dic[\"investment_id\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[investment_id.iloc[i]]))\n#     dic[\"target\"] = tf.train.Feature(float_list=tf.train.FloatList(value=[y.iloc[i]]))\n#     record_bytes = tf.train.Example(features=tf.train.Features(feature=dic)).SerializeToString()\n#     return record_bytes\n    \n# def decode_function(record_bytes):\n#   return tf.io.parse_single_example(\n#       # Data\n#       record_bytes,\n#       # Schema\n#       {\n#           \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n#           \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n#           \"investment_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n#           \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n#       }\n#   )","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.746402Z","iopub.execute_input":"2022-03-28T22:58:49.746868Z","iopub.status.idle":"2022-03-28T22:58:49.753832Z","shell.execute_reply.started":"2022-03-28T22:58:49.74683Z","shell.execute_reply":"2022-03-28T22:58:49.75314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# import time\n# n_splits = 5\n# n_test_splits = 1\n# kfold = CombinatorialPurgedGroupKFold(n_splits, n_test_splits)\n# for fold, (train_indices, test_indices) in enumerate(kfold.split(train, groups=time_id)):\n#     print(f\"Fold\")\n#     print(\"=\" * 100)\n#     print(\"Train Sample size:\", len(train_indices))\n#     print(\"Test Sample size:\", len(test_indices))\n#     train_save_path = f\"fold_{fold}_train.tfrecords\"\n#     begin = time.time()\n#     print(f\"Creating {train_save_path}\")\n#     with tf.io.TFRecordWriter(train_save_path) as file_writer:\n#         for i in train_indices:\n#             file_writer.write(create_record(i))\n#     print(\"Elapsed time: %.2f\"%(time.time() - begin))\n#     begin = time.time()\n#     print(f\"Creating {train_save_path}\")\n#     test_save_path = f\"fold_{fold}_test.tfrecords\"\n#     with tf.io.TFRecordWriter(test_save_path) as file_writer:\n#         for i in test_indices:\n#             file_writer.write(create_record(i))\n#     print(\"Elapsed time: %.2f\"%(time.time() - begin))","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.75514Z","iopub.execute_input":"2022-03-28T22:58:49.755631Z","iopub.status.idle":"2022-03-28T22:58:49.765444Z","shell.execute_reply.started":"2022-03-28T22:58:49.755597Z","shell.execute_reply":"2022-03-28T22:58:49.764835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# investment_ids = investment_id.unique()\n# investment_id_df = pd.DataFrame({\"investment_id\": investment_ids})\n# investment_id_df.to_csv(\"investment_ids.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.768162Z","iopub.execute_input":"2022-03-28T22:58:49.768974Z","iopub.status.idle":"2022-03-28T22:58:49.775333Z","shell.execute_reply.started":"2022-03-28T22:58:49.768946Z","shell.execute_reply":"2022-03-28T22:58:49.774691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test of Null Values","metadata":{}},{"cell_type":"markdown","source":"Checking null values in the dataframe. As the output of the below mentioned snippet is False, with which we can conclude that there are no null values in the dataset.","metadata":{}},{"cell_type":"code","source":"#print(df.isnull().any().any()) #No Null Values","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.776663Z","iopub.execute_input":"2022-03-28T22:58:49.77709Z","iopub.status.idle":"2022-03-28T22:58:49.783619Z","shell.execute_reply.started":"2022-03-28T22:58:49.777056Z","shell.execute_reply":"2022-03-28T22:58:49.782835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test of Outliers","metadata":{}},{"cell_type":"code","source":"#df['f_0'].plot.box(figsize=(100,100))","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.785166Z","iopub.execute_input":"2022-03-28T22:58:49.785756Z","iopub.status.idle":"2022-03-28T22:58:49.791183Z","shell.execute_reply.started":"2022-03-28T22:58:49.785719Z","shell.execute_reply":"2022-03-28T22:58:49.790539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in df.columns[3:]:       #Test of Outlier\n#     print(pd.DataFrame(df[i].describe()))","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.792472Z","iopub.execute_input":"2022-03-28T22:58:49.793007Z","iopub.status.idle":"2022-03-28T22:58:49.798822Z","shell.execute_reply.started":"2022-03-28T22:58:49.792972Z","shell.execute_reply":"2022-03-28T22:58:49.79821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking the category of Columns","metadata":{}},{"cell_type":"code","source":"# print(df.dtypes)\n# print(pd.DataFrame(df.dtypes=='float64').count() if (df.dtypes=='float64').any() else \"None\")\n# print((df.dtypes=='float64'))","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.802916Z","iopub.execute_input":"2022-03-28T22:58:49.803363Z","iopub.status.idle":"2022-03-28T22:58:49.806949Z","shell.execute_reply.started":"2022-03-28T22:58:49.803336Z","shell.execute_reply":"2022-03-28T22:58:49.806272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df['investment_id'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.808599Z","iopub.execute_input":"2022-03-28T22:58:49.809036Z","iopub.status.idle":"2022-03-28T22:58:49.814945Z","shell.execute_reply.started":"2022-03-28T22:58:49.808999Z","shell.execute_reply":"2022-03-28T22:58:49.814236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df['target'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.81652Z","iopub.execute_input":"2022-03-28T22:58:49.817659Z","iopub.status.idle":"2022-03-28T22:58:49.823276Z","shell.execute_reply.started":"2022-03-28T22:58:49.81763Z","shell.execute_reply":"2022-03-28T22:58:49.822607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features = [f'f_{i}' for i in range(300)]\n# for f in features:\n#     df[f] = df[f].astype('float16')","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.824369Z","iopub.execute_input":"2022-03-28T22:58:49.825694Z","iopub.status.idle":"2022-03-28T22:58:49.831573Z","shell.execute_reply.started":"2022-03-28T22:58:49.825665Z","shell.execute_reply":"2022-03-28T22:58:49.830886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1) Univariate Analysis ","metadata":{}},{"cell_type":"markdown","source":"## 1 a) Target and it's frequency count ","metadata":{}},{"cell_type":"code","source":"# f = plt.figure(figsize=(16, 10))\n# ax=sns.histplot(df['target'].values, kde=True, color='Red')\n# ax.set()\n# ax.set_xlabel('Target',fontsize=20)\n# ax.set_ylabel('Frequency',fontsize=20)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.832463Z","iopub.execute_input":"2022-03-28T22:58:49.833274Z","iopub.status.idle":"2022-03-28T22:58:49.839857Z","shell.execute_reply.started":"2022-03-28T22:58:49.833236Z","shell.execute_reply":"2022-03-28T22:58:49.838945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1 b) Investment ID and it's frequency count ","metadata":{}},{"cell_type":"code","source":"# f = plt.figure(figsize=(16, 10))\n# ax = sns.histplot(x=\"investment_id\", data=df, color='gray')\n# ax.set_xlabel('Investment ID',fontsize=20)\n# ax.set_ylabel('Frequency',fontsize=20)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.84135Z","iopub.execute_input":"2022-03-28T22:58:49.841896Z","iopub.status.idle":"2022-03-28T22:58:49.846915Z","shell.execute_reply.started":"2022-03-28T22:58:49.84186Z","shell.execute_reply":"2022-03-28T22:58:49.846128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1 c) features and it's frequency count which doesn't follow Gaussian distibution","metadata":{}},{"cell_type":"code","source":"# def feat_dist(df, cols, rows=3, columns=3, title=None, figsize=(30, 25)):\n    \n#     '''A function for displaying skew feat distribution'''\n    \n#     fig, axes = plt.subplots(rows, columns, figsize=figsize, constrained_layout=True)\n#     axes = axes.flatten()\n\n#     for i, j in zip(cols, axes):\n#         sns.distplot(\n#                     df[i],\n#                     ax=j,\n#                     fit=norm,\n#                     hist=False,\n#                     kde_kws={'linewidth':3}\n#         )   \n        \n#         (mu, sigma) = norm.fit(df[i])\n#         j.set_title('Dist of {0} Norm Fit: $\\mu=${1:.2g}, $\\sigma=${2:.2f}'.format(i, mu, sigma), weight='bold')\n#         j.legend(labels=[f'{i}', 'Normal Dist'])\n#         fig.suptitle(f'{title}', fontsize=24, weight='bold')","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.848745Z","iopub.execute_input":"2022-03-28T22:58:49.848929Z","iopub.status.idle":"2022-03-28T22:58:49.856674Z","shell.execute_reply.started":"2022-03-28T22:58:49.848907Z","shell.execute_reply":"2022-03-28T22:58:49.855927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sampled_df = df.sample(frac=0.05, random_state=42)\n# features_std = sampled_df.iloc[:,4:].apply(lambda x: np.std(x)).sort_values(\n#     ascending=False)\n# f_std = sampled_df[features_std.iloc[:25].index.tolist()]\n\n# with pd.option_context('mode.use_inf_as_na', True):\n#     features_skew = np.abs(sampled_df.iloc[:,4:].apply(lambda x: np.abs(skew(x))).sort_values(\n#         ascending=False)).dropna()\n# skewed = sampled_df[features_skew.iloc[:25].index.tolist()]\n\n# with pd.option_context('mode.use_inf_as_na', True):\n#     features_kurt = np.abs(sampled_df.iloc[:,4:].apply(lambda x: np.abs(kurtosis(x))).sort_values(\n#         ascending=False)).dropna()\n# kurt_f = sampled_df[features_kurt.iloc[:25].index.tolist()]\n\n\n# feat_dist(sampled_df, f_std.columns.tolist(), rows=1, columns=2, title='Distribution of High Std Features', figsize=(30, 8))","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.857803Z","iopub.execute_input":"2022-03-28T22:58:49.858443Z","iopub.status.idle":"2022-03-28T22:58:49.865032Z","shell.execute_reply.started":"2022-03-28T22:58:49.858407Z","shell.execute_reply":"2022-03-28T22:58:49.864249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feat_dist(sampled_df, skewed.columns.tolist(), rows=5, columns=5, title='Distribution of Skewed Features')","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.866415Z","iopub.execute_input":"2022-03-28T22:58:49.866839Z","iopub.status.idle":"2022-03-28T22:58:49.873749Z","shell.execute_reply.started":"2022-03-28T22:58:49.866804Z","shell.execute_reply":"2022-03-28T22:58:49.873058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1d) Highly Correlated features","metadata":{}},{"cell_type":"code","source":"# correlations = sampled_df.corrwith(sampled_df['target']).iloc[:-1].to_frame()\n# correlations['Abs Corr'] = correlations[0].abs()\n# sorted_correlations = correlations.sort_values('Abs Corr', ascending=False)['Abs Corr']\n# fig, ax = plt.subplots(figsize=(15,15))\n# sns.heatmap(sorted_correlations.iloc[1:].to_frame()[sorted_correlations>=.046], cmap='BuPu', annot=True, vmin=0.046, vmax=0.067, ax=ax)\n# plt.title('Feature Correlations With Target')\n# plt.savefig('correlation.jpg')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.875115Z","iopub.execute_input":"2022-03-28T22:58:49.875402Z","iopub.status.idle":"2022-03-28T22:58:49.881777Z","shell.execute_reply.started":"2022-03-28T22:58:49.875329Z","shell.execute_reply":"2022-03-28T22:58:49.880964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) HeatMap","metadata":{}},{"cell_type":"code","source":"# df_temp = df\n# corr = df_temp.corr()\n# f = plt.figure(figsize=(100, 100))\n# sns.heatmap(corr)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.882624Z","iopub.execute_input":"2022-03-28T22:58:49.885012Z","iopub.status.idle":"2022-03-28T22:58:49.891829Z","shell.execute_reply.started":"2022-03-28T22:58:49.884972Z","shell.execute_reply":"2022-03-28T22:58:49.891097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3) Bivariate Analysis","metadata":{}},{"cell_type":"markdown","source":"## 3 a) Investment_ID vs Target","metadata":{}},{"cell_type":"code","source":"# import pylab as pl\n# df_temp=df.set_index('time_id')\n# for f in np.random.choice(df['investment_id'].unique(),25):\n#     ax=df[df['investment_id'] == f]['target'].hist(bins = 100, alpha = 0.2, figsize = (20,10))\n#     ax.set_xlabel(\"Target\", fontsize=20)\n#     ax.set_ylabel(\"Investment_id\", fontsize=20)\n#     ax.tick_params(axis='both', which='major', labelsize=15)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.892769Z","iopub.execute_input":"2022-03-28T22:58:49.893391Z","iopub.status.idle":"2022-03-28T22:58:49.90065Z","shell.execute_reply.started":"2022-03-28T22:58:49.893357Z","shell.execute_reply":"2022-03-28T22:58:49.899917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3 b) Increase in Investement along the time horizon","metadata":{}},{"cell_type":"code","source":"# fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n# df.groupby('time_id')['investment_id'].nunique().plot(color='magenta')\n# plt.title(\"number of unique assets by time\")\n# plt.xlabel('Time_ID', size=20)\n# plt.ylabel('Frequency', size=20)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.901949Z","iopub.execute_input":"2022-03-28T22:58:49.902409Z","iopub.status.idle":"2022-03-28T22:58:49.90847Z","shell.execute_reply.started":"2022-03-28T22:58:49.90231Z","shell.execute_reply":"2022-03-28T22:58:49.907702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3 c) Plot of Time ID vs Target for set of investment ID","metadata":{}},{"cell_type":"code","source":"# df_temp=df[['investment_id','time_id','target']]\n# for f in np.random.choice(df_temp['investment_id'].unique(),5):\n#     d = df_temp[df_temp['investment_id'] == f]\n#     d.set_index('time_id')['target'].plot(figsize=(15, 5),style='.-',label='investment_id'+str(f))\n#     plt.xlabel('Time_ID', size=20)\n#     plt.ylabel('Target', size=20)\n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.909408Z","iopub.execute_input":"2022-03-28T22:58:49.909611Z","iopub.status.idle":"2022-03-28T22:58:49.916906Z","shell.execute_reply.started":"2022-03-28T22:58:49.909589Z","shell.execute_reply":"2022-03-28T22:58:49.916196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Investment Count vs target dispersion","metadata":{}},{"cell_type":"code","source":"# obs_by_asset = df.groupby(['investment_id'])['target'].count()\n# mean_target = df.groupby(['investment_id'])['target'].mean()\n# ax = sns.jointplot(x=obs_by_asset, y=mean_target, kind=\"reg\", \n#                    height=8, joint_kws={'line_kws':{'color':'green'}})\n# ax.ax_joint.set_xlabel('Count by investment ID', size=20)\n# ax.ax_joint.set_ylabel('Target by mean', size=20)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.91806Z","iopub.execute_input":"2022-03-28T22:58:49.918417Z","iopub.status.idle":"2022-03-28T22:58:49.924735Z","shell.execute_reply.started":"2022-03-28T22:58:49.918382Z","shell.execute_reply":"2022-03-28T22:58:49.923945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Investment Count vs target dispersion using Standard Deviation","metadata":{}},{"cell_type":"code","source":"# sts_target = df.groupby(['investment_id'])['target'].std()\n# mean_std_target = np.mean(sts_target)\n# ax = sns.jointplot(x=obs_by_asset.values, y=sts_target, kind=\"reg\",height=8, joint_kws={'line_kws':{'color':'green'}})\n# ax.ax_joint.set_xlabel('observations', size=20)\n# ax.ax_joint.set_ylabel('std target', size=20)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.926259Z","iopub.execute_input":"2022-03-28T22:58:49.926773Z","iopub.status.idle":"2022-03-28T22:58:49.933117Z","shell.execute_reply.started":"2022-03-28T22:58:49.92674Z","shell.execute_reply":"2022-03-28T22:58:49.932178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Grid of scatter plot for each features","metadata":{}},{"cell_type":"code","source":"# df1=df.iloc[:,2:27]\n# g = sns.PairGrid(df1)\n# g.map(sns.scatterplot)\n# g.savefig('pairplot.jpg')","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.935739Z","iopub.execute_input":"2022-03-28T22:58:49.936761Z","iopub.status.idle":"2022-03-28T22:58:49.942066Z","shell.execute_reply.started":"2022-03-28T22:58:49.93673Z","shell.execute_reply":"2022-03-28T22:58:49.941442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(20,20))\n# for i in df1.columns:\n#     sns.scatterplot(data=df,x=i,y='target',palette=\"deep\")\n#     g.map(sns.scatterplot)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.943077Z","iopub.execute_input":"2022-03-28T22:58:49.943824Z","iopub.status.idle":"2022-03-28T22:58:49.950073Z","shell.execute_reply.started":"2022-03-28T22:58:49.943794Z","shell.execute_reply":"2022-03-28T22:58:49.94913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling using light GBM","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"## Train Test Split","metadata":{}},{"cell_type":"code","source":"# df1=df\n# Y=df1.pop('target')\n# X=df[['f_231', 'f_119', 'f_179', 'f_297', 'f_76']]\n# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42,shuffle=False)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-28T22:58:49.954147Z","iopub.execute_input":"2022-03-28T22:58:49.9564Z","iopub.status.idle":"2022-03-28T22:58:49.959315Z","shell.execute_reply.started":"2022-03-28T22:58:49.956373Z","shell.execute_reply":"2022-03-28T22:58:49.958578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implementing lightGBM","metadata":{}},{"cell_type":"code","source":"# selection = [\"time_id\", \"investment_id\"]\n# feature_names = df2.drop(['row_id',\"time_id\", \"investment_id\", \"target\"], axis=1).columns.values\n# N = np.random.choice(np.arange(0, len(feature_names)), replace=False, size=10)\n# my_features = [\"f_{}\".format(n) for n in N]\n# selection.extend(my_features)\n# X = df2[selection].copy()\n# Y = df2.target\n# for f in my_features:\n#     X[f + \"_shift1\"] = X.groupby(\"investment_id\")[f].shift(1).fillna(0)\n# features = [\"f_{}_shift1\".format(n) for n in N]\n# features.extend([\"f_{}\".format(n) for n in N])\n# V = 0.9\n# x_train, x_dev = X[0:int(V*X.shape[0])][features], X[int(V*X.shape[0])::][features]\n# y_train, y_dev = Y[0:int(V*X.shape[0])], Y[int(V*X.shape[0])::]\n","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.960449Z","iopub.execute_input":"2022-03-28T22:58:49.962214Z","iopub.status.idle":"2022-03-28T22:58:49.968122Z","shell.execute_reply.started":"2022-03-28T22:58:49.962184Z","shell.execute_reply":"2022-03-28T22:58:49.967471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lgb_train = lgb.Dataset(x_train.values, y_train.values)\n# lgb_eval = lgb.Dataset(x_dev.values, y_dev.values, reference=lgb_train)\n# params = {'boosting_type': 'gbdt',\n#               'objective': 'regression',\n#               'num_leaves': 5,\n#               'learning_rate': 0.1,\n#               'feature_fraction': 0.9\n#               }\n# lm = lgb.train(params,\n#     lgb_train,\n#     num_boost_round=10,\n#     valid_sets=[lgb_train, lgb_eval],\n#     valid_names=['train','valid'],\n#     callbacks= [lgb.early_stopping(10)]\n#    )\n# y_pred = lm.predict(x_dev)\n# pearsonr(y_dev, y_pred)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-28T22:58:49.969796Z","iopub.execute_input":"2022-03-28T22:58:49.971721Z","iopub.status.idle":"2022-03-28T22:58:49.976808Z","shell.execute_reply.started":"2022-03-28T22:58:49.971681Z","shell.execute_reply":"2022-03-28T22:58:49.976119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DNN Implementation","metadata":{}},{"cell_type":"code","source":"# class Config:\n#     is_training = False\n#     tf_record_dataset_path = \"../input/ump-tf-record-combinatorialpurgedgroupkfold/\"\n#     output_dataset_path = \"../input/ubiquant-market-prediction-with-dnn-output/\"\n# config = Config()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.979057Z","iopub.execute_input":"2022-03-28T22:58:49.979243Z","iopub.status.idle":"2022-03-28T22:58:49.986798Z","shell.execute_reply.started":"2022-03-28T22:58:49.979221Z","shell.execute_reply":"2022-03-28T22:58:49.985942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# investment_ids = pd.read_csv('../input/ump-tf-record-combinatorialpurgedgroupkfold/investment_ids.csv')\n# investment_id_size = len(investment_ids) + 1\n# with tf.device(\"cpu\"):\n#     investment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\n#     investment_id_lookup_layer.adapt(investment_ids)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:49.987887Z","iopub.execute_input":"2022-03-28T22:58:49.988835Z","iopub.status.idle":"2022-03-28T22:58:52.350952Z","shell.execute_reply.started":"2022-03-28T22:58:49.988745Z","shell.execute_reply":"2022-03-28T22:58:52.350223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def decode_function(record_bytes):\n#   return tf.io.parse_single_example(\n#       # Data\n#       record_bytes,\n#       # Schema\n#       {\n#           \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n#           \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n#           \"investment_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n#           \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n#       }\n#   )\n# def preprocess(item):\n#     return (item[\"investment_id\"], item[\"features\"]), item[\"target\"]\n# def make_dataset(file_paths, batch_size=4096, mode=\"train\"):\n#     ds = tf.data.TFRecordDataset(file_paths)\n#     ds = ds.map(decode_function)\n#     ds = ds.map(preprocess)\n#     if mode == \"train\":\n#         ds = ds.shuffle(batch_size * 4)\n#     ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n#     return ds","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:52.352436Z","iopub.execute_input":"2022-03-28T22:58:52.352687Z","iopub.status.idle":"2022-03-28T22:58:52.362172Z","shell.execute_reply.started":"2022-03-28T22:58:52.352652Z","shell.execute_reply":"2022-03-28T22:58:52.360952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def correlation(x, y, axis=-2):\n#     x = tf.convert_to_tensor(x)\n#     y = math_ops.cast(y, x.dtype)\n#     n = tf.cast(tf.shape(x)[axis], x.dtype)\n#     xsum = tf.reduce_sum(x, axis=axis)\n#     ysum = tf.reduce_sum(y, axis=axis)\n#     xmean = xsum / n\n#     ymean = ysum / n\n#     xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n#     yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n#     cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n#     corr = cov / tf.sqrt(xvar * yvar)\n#     return tf.constant(1.0, dtype=x.dtype) - corr\n\n# def get_model():\n#     investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n#     features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n#     investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n#     investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n#     investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n#     investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n#     investment_id_x = layers.Dropout(0.1)(investment_id_x)\n#     investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n#     investment_id_x = layers.Dropout(0.1)(investment_id_x)\n#     investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n#     investment_id_x = layers.Dropout(0.1)(investment_id_x)\n    \n#     feature_x = layers.Dense(256, activation='swish')(features_inputs)\n#     feature_x = layers.Dropout(0.1)(feature_x)\n#     feature_x = layers.Dense(256, activation='swish')(feature_x)\n#     feature_x = layers.Dropout(0.1)(feature_x)\n#     feature_x = layers.Dense(256, activation='swish')(feature_x)\n#     feature_x = layers.Dropout(0.1)(feature_x)\n    \n#     x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n#     x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n#     x = layers.Dropout(0.1)(x)\n#     x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n#     x = layers.Dropout(0.1)(x)\n#     x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n#     x = layers.Dropout(0.1)(x)\n#     output = layers.Dense(1)(x)\n#     rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n#     model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n#     model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse, correlation])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:52.364479Z","iopub.execute_input":"2022-03-28T22:58:52.365009Z","iopub.status.idle":"2022-03-28T22:58:52.382201Z","shell.execute_reply.started":"2022-03-28T22:58:52.364953Z","shell.execute_reply":"2022-03-28T22:58:52.381565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = get_model()\n# keras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T23:01:22.006881Z","iopub.execute_input":"2022-03-28T23:01:22.007362Z","iopub.status.idle":"2022-03-28T23:01:23.031947Z","shell.execute_reply.started":"2022-03-28T23:01:22.007322Z","shell.execute_reply":"2022-03-28T23:01:23.029997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# models = []\n# for i in range(5):\n#     train_path = f\"{config.tf_record_dataset_path}fold_{i}_train.tfrecords\"\n#     valid_path = f\"{config.tf_record_dataset_path}fold_{i}_test.tfrecords\"\n#     valid_ds = make_dataset([valid_path], mode=\"valid\")\n#     model = get_model()\n#     if config.is_training:\n#         train_ds = make_dataset([train_path])\n#         checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{i}.tf\", monitor=\"val_correlation\", mode=\"min\", save_best_only=True, save_weights_only=True)\n#         early_stop = keras.callbacks.EarlyStopping(patience=10)\n#         history = model.fit(train_ds, epochs=30, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n#         model.load_weights(f\"model_{i}.tf\")\n#         for metric in [\"loss\", \"mae\", \"mape\", \"rmse\", \"correlation\"]:\n#             pd.DataFrame(history.history, columns=[metric, f\"val_{metric}\"]).plot()\n#             plt.title(metric.upper())\n#             plt.show()\n#     else:\n#         model.load_weights(f\"{config.output_dataset_path}model_{i}.tf\")\n#     y_vals = []\n#     for _, y in valid_ds:\n#         y_vals += list(y.numpy().reshape(-1))\n#     y_val = np.array(y_vals)\n#     pearson_score = stats.pearsonr(model.predict(valid_ds).reshape(-1), y_val)[0]\n#     models.append(model)\n#     print(f\"Pearson Score: {pearson_score}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:53.179675Z","iopub.status.idle":"2022-03-28T22:58:53.180298Z","shell.execute_reply.started":"2022-03-28T22:58:53.180037Z","shell.execute_reply":"2022-03-28T22:58:53.180068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### model ensemble","metadata":{}},{"cell_type":"code","source":"%%time\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\nfeature_columns = ['investment_id', 'time_id'] + features\ntrain = pd.read_parquet('../input/low-mem-parquet/train_low_mem.parquet')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T23:31:40.575426Z","iopub.execute_input":"2022-03-31T23:31:40.576131Z","iopub.status.idle":"2022-03-31T23:32:30.690676Z","shell.execute_reply.started":"2022-03-31T23:31:40.576093Z","shell.execute_reply":"2022-03-31T23:32:30.689985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment_id = train.pop(\"investment_id\")\n_ = train.pop(\"time_id\")\ny = train.pop(\"target\")","metadata":{"execution":{"iopub.status.busy":"2022-03-31T23:34:12.211759Z","iopub.execute_input":"2022-03-31T23:34:12.212242Z","iopub.status.idle":"2022-03-31T23:34:12.222839Z","shell.execute_reply.started":"2022-03-31T23:34:12.212201Z","shell.execute_reply":"2022-03-31T23:34:12.222095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment_ids = list(investment_id.unique())\ninvestment_id_size = len(investment_ids) + 1\ninvestment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\nwith tf.device(\"cpu\"):\n    investment_id_lookup_layer.adapt(investment_id)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T23:34:13.757423Z","iopub.execute_input":"2022-03-31T23:34:13.757878Z","iopub.status.idle":"2022-03-31T23:35:38.445413Z","shell.execute_reply.started":"2022-03-31T23:34:13.75784Z","shell.execute_reply":"2022-03-31T23:35:38.444589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(X, y):\n    print(X)\n    print(y)\n    return X, y\ndef make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(256)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-03-31T23:40:49.228628Z","iopub.execute_input":"2022-03-31T23:40:49.228893Z","iopub.status.idle":"2022-03-31T23:40:49.235774Z","shell.execute_reply.started":"2022-03-31T23:40:49.228864Z","shell.execute_reply":"2022-03-31T23:40:49.235097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\n\ndef get_model2():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)    \n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n   # investment_id_x = layers.Dropout(0.65)(investment_id_x)\n   \n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.65)(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n   # x = layers.Dropout(0.2)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n  #  x = layers.Dropout(0.4)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.75)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\n\ndef get_model3():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n    investment_id_x = layers.Dense(32, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n    #investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.5)(feature_x)\n    feature_x = layers.Dense(128, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.5)(feature_x)\n    feature_x = layers.Dense(64, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.5)(x)\n    output = layers.Dense(1)(x)\n    output = tf.keras.layers.BatchNormalization(axis=1)(output)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\ndef get_model5():\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    ## feature ##\n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.1)(feature_x)\n    ## convolution 1 ##\n    feature_x = layers.Reshape((-1,1))(feature_x)\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 2 ##\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 3 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 4 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 5 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=2, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## flatten ##\n    feature_x = layers.Flatten()(feature_x)\n    \n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n    \n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\ndel train\ndel investment_id\ndel y\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T23:40:50.586496Z","iopub.execute_input":"2022-03-31T23:40:50.587058Z","iopub.status.idle":"2022-03-31T23:40:50.834525Z","shell.execute_reply.started":"2022-03-31T23:40:50.586982Z","shell.execute_reply":"2022-03-31T23:40:50.833679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# models = []\n\n# for i in range(5):\n#     model = get_model()\n#     model.load_weights(f'../input/dnn-base/model_{i}')\n#     models.append(model)\n\n# for i in range(10):\n#     model = get_model2()\n#     model.load_weights(f'../input/train-dnn-v2-10fold/model_{i}')\n#     models.append(model)\n    \n    \n# for i in range(10):\n#     model = get_model3()\n#     model.load_weights(f'../input/dnnmodelnew/model_{i}')\n#     models.append(model)\n    \n    \n# models2 = []\n    \n# for i in range(5):\n#     model = get_model5()\n#     model.load_weights(f'../input/prediction-including-spatial-info-with-conv1d/model_{i}.tf')\n#     models2.append(model)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T23:41:03.343805Z","iopub.execute_input":"2022-03-31T23:41:03.344076Z","iopub.status.idle":"2022-03-31T23:41:03.511983Z","shell.execute_reply.started":"2022-03-31T23:41:03.344025Z","shell.execute_reply":"2022-03-31T23:41:03.510966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\n\ndef preprocess_test_s(feature):\n    return (feature), 0\n\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef make_test_dataset2(feature, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((feature)))\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds\n\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=1)\n\n\ndef make_test_dataset3(feature, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices((feature))\n    ds = ds.map(preprocess_test_s)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef infer(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append((y_pred-y_pred.mean())/y_pred.std())\n    \n    return np.mean(y_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T23:41:51.698912Z","iopub.execute_input":"2022-03-31T23:41:51.699501Z","iopub.status.idle":"2022-03-31T23:41:51.709891Z","shell.execute_reply.started":"2022-03-31T23:41:51.69946Z","shell.execute_reply":"2022-03-31T23:41:51.70921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfor (test_df, sample_prediction_df) in iter_test:\n    ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n    p1 = inference(models, ds)\n    ds2 = make_test_dataset2(test_df[features])\n    p2 = inference(models2, ds2)\n    ds3 = make_test_dataset3(test_df[features])\n    p3 = infer(models3, ds3)\n    sample_prediction_df['target'] = p1 * 0.29 + p2 * 0.64 + p3 * 0.07\n    env.predict(sample_prediction_df) \n    display(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T23:57:20.647389Z","iopub.execute_input":"2022-03-31T23:57:20.64764Z","iopub.status.idle":"2022-03-31T23:57:20.653994Z","shell.execute_reply.started":"2022-03-31T23:57:20.647611Z","shell.execute_reply":"2022-03-31T23:57:20.653333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"# def preprocess_test(investment_id, feature):\n#     return (investment_id, feature), 0\n# def make_test_dataset(feature, investment_id, batch_size=1024):\n#     ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n#     ds = ds.map(preprocess_test)\n#     ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n#     return ds\n# def inference(models, ds):\n#     y_preds = []\n#     for model in models:\n#         y_pred = model.predict(ds)\n#         y_preds.append(y_pred)\n#     return np.mean(y_preds, axis=0)\n# import ubiquant\n# env = ubiquant.make_env()\n# iter_test = env.iter_test() \n# features = [f\"f_{i}\" for i in range(300)]\n# for (test_df, sample_prediction_df) in iter_test:\n#     ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n#     sample_prediction_df['target'] = inference(models, ds)\n#     env.predict(sample_prediction_df) ","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:53.18159Z","iopub.status.idle":"2022-03-28T22:58:53.182364Z","shell.execute_reply.started":"2022-03-28T22:58:53.182108Z","shell.execute_reply":"2022-03-28T22:58:53.182136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import ubiquant\n# env = ubiquant.make_env()   # initialize the environment\n# iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\n\n# previous_test_df = df2[df2.time_id == df2.time_id.max()].iloc[0:int(V*df2.shape[0])]\n# for (test_df, sample_prediction_df) in iter_test:\n    \n#     for f in my_features:\n#         test_df.loc[:, f+\"_shift1\"] = 0\n#         already_known = previous_test_df[previous_test_df.investment_id.isin(test_df.investment_id)]\n#         test_df.loc[test_df.investment_id.isin(already_known.investment_id), f+\"_shift1\"] = already_known[f]\n#         test_df.loc[:, f+\"_shift1\"] = test_df.loc[:, f+\"_shift1\"].fillna(0)\n        \n#         #test_df.loc[:, f+\"_diff\"] = test_df[f] - test_df[f+\"_shift1\"]\n#         #test_df.loc[:, f+\"_diff\"] = test_df.loc[:, f+\"_diff\"].fillna(0)\n    \n#     pred = lm.predict(test_df[features])\n#     sample_prediction_df['target'] = pred  # make your predictions here\n#     env.predict(sample_prediction_df)   # register your predictions\n#     previous_test_df = test_df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T22:58:53.18361Z","iopub.status.idle":"2022-03-28T22:58:53.184403Z","shell.execute_reply.started":"2022-03-28T22:58:53.184124Z","shell.execute_reply":"2022-03-28T22:58:53.184157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Submissions are scored on the mean of Pearson correlation coefficient for each time ID. Pearson correlation coefficient (Pearson's r) is a measure of linear correlation between two sets of values. It can be denoted as\n\nr=n∑xy−(∑x)(∑y)[n∑x2−(∑x)2][n∑y2−(∑y)2−−−−−−−−−−−−−−−−−−−−−−−−−−√] \nr  = Pearson Correlation Coefficient\nn  = Number of samples\nx  = First set of values\ny  = Second set of values\nMean of Pearson correlation coefficient across time IDs can be denoted as\n\nMean r=1T∑Ti=1tir \nT  = Number of time IDs\ntir  = ith time ID's Pearson correlation coefficient","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}