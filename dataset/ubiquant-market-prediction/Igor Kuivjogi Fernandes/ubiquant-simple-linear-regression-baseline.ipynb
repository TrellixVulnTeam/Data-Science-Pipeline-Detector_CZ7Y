{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nfrom tqdm.notebook import tqdm\n\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = [9, 9]\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 350)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-31T20:10:03.812053Z","iopub.execute_input":"2022-01-31T20:10:03.812402Z","iopub.status.idle":"2022-01-31T20:10:03.907805Z","shell.execute_reply.started":"2022-01-31T20:10:03.812306Z","shell.execute_reply":"2022-01-31T20:10:03.906946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Update\n\n- Changed the Pearson correlation calculation, from [here](https://www.kaggle.com/c/ubiquant-market-prediction/discussion/303627)\n- Changed GroupKFold to GroupTimeSeriesSplit from [here](https://www.kaggle.com/c/ubiquant-market-prediction/discussion/304036)\n- Using only 80% of most frequent investment ids due memory error  \n\n---\n\n- Using GroupKFold: \n    - CV: 0.12937332275514163\n    - LB: 0.108\n- Using GroupTimeSeriesSplit:\n    - CV: 0.11928179218531812\n    - LB: ?","metadata":{}},{"cell_type":"code","source":"PATH = '../input/ubiquant-market-prediction-half-precision-pickle'","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:10:03.909427Z","iopub.execute_input":"2022-01-31T20:10:03.909651Z","iopub.status.idle":"2022-01-31T20:10:03.913528Z","shell.execute_reply.started":"2022-01-31T20:10:03.909625Z","shell.execute_reply":"2022-01-31T20:10:03.912998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir(PATH)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:10:03.914602Z","iopub.execute_input":"2022-01-31T20:10:03.914859Z","iopub.status.idle":"2022-01-31T20:10:03.936568Z","shell.execute_reply.started":"2022-01-31T20:10:03.914828Z","shell.execute_reply":"2022-01-31T20:10:03.935747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntrain = pd.read_pickle(f'{PATH}/train.pkl')\n\nfor col in ['time_id', 'investment_id']:\n    train[col] = train[col].astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:10:03.937895Z","iopub.execute_input":"2022-01-31T20:10:03.938503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_ROWS = len(train)\nfeatures = [f'f_{i}' for i in range(300)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# no NAs at all\ntrain.isnull().sum().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['investment_id'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_investment_ids = train['investment_id'].unique()\nprint(unique_investment_ids)\nprint(unique_investment_ids.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for x in unique_investment_ids[:20]:\n#     train[train['investment_id'] == x].plot('time_id', 'target', title=f'investment_id = {x}', figsize=(6, 6));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### On the target","metadata":{}},{"cell_type":"code","source":"train['target'].plot.hist();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[['target']].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.sample(int(N_ROWS * 0.02), random_state=1)[features].describe().T['mean'].plot.hist(title='0 mean');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.sample(int(N_ROWS * 0.02), random_state=1)[features].describe().T['std'].plot.hist('1 std');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n\n# many high correlated features, so some type of models can struggle with this\n# train.sample(int(N_ROWS * 0.001), random_state=1)[['target'] + features].corr().style.background_gradient(axis=None)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.sample(int(N_ROWS * 0.02), random_state=1)[['f_109', 'target']].plot.scatter('f_109', 'target');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.sample(int(N_ROWS * 0.02), random_state=1)[['f_108', 'target']].plot.scatter('f_108', 'target');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling","metadata":{"execution":{"iopub.status.busy":"2022-01-22T17:06:33.7463Z","iopub.execute_input":"2022-01-22T17:06:33.746981Z","iopub.status.idle":"2022-01-22T17:06:33.965393Z","shell.execute_reply.started":"2022-01-22T17:06:33.746913Z","shell.execute_reply":"2022-01-22T17:06:33.964341Z"}}},{"cell_type":"code","source":"# take some investment ids \nN = int(len(unique_investment_ids) * 0.8)\ninvestments_to_use = train['investment_id'].value_counts()[:N]\nprint('Selecting:', N)\nprint('Before:', train.shape)\ntrain = train[train['investment_id'].isin(investments_to_use)].reset_index(drop=True)\nprint('After:', train.shape)\n\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import linear_model\nfrom sklearn import metrics\nfrom scipy.stats import pearsonr\nfrom sklearn.preprocessing import StandardScaler\nimport lightgbm as lgbm\nfrom sklearn import svm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Tuple\n\n\nclass GroupTimeSeriesSplit:\n    \"\"\"\n    From: https://www.kaggle.com/c/ubiquant-market-prediction/discussion/304036\n    Custom class to create a Group Time Series Split. We ensure\n    that the time id values that are in the testing data are not a part\n    of the training data & the splits are temporal\n    \"\"\"\n    def __init__(self, n_folds: int, holdout_size: int, groups: str) -> None:\n        self.n_folds = n_folds\n        self.holdout_size = holdout_size\n        self.groups = groups\n\n    def split(self, X) -> Tuple[np.array, np.array]:\n        # Take the group column and get the unique values\n        unique_time_ids = np.unique(self.groups.values)\n\n        # Split the time ids into the length of the holdout size\n        # and reverse so we work backwards in time. Also, makes\n        # it easier to get the correct time_id values per\n        # split\n        array_split_time_ids = np.array_split(\n            unique_time_ids, len(unique_time_ids) // self.holdout_size\n        )[::-1]\n\n        # Get the first n_folds values\n        array_split_time_ids = array_split_time_ids[:self.n_folds]\n\n        for time_ids in array_split_time_ids:\n            # Get test index - time id values that are in the time_ids\n            test_condition = X['time_id'].isin(time_ids)\n            test_index = X.loc[test_condition].index\n\n            # Get train index - The train index will be the time\n            # id values right up until the minimum value in the test\n            # data - we can also add a gap to this step by\n            # time id < (min - gap)\n            train_condition = X['time_id'] < (np.min(time_ids))\n            train_index = X.loc[train_condition].index\n\n            yield train_index, test_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nFEATS = features + ['investment_id', 'time_id']\n\npearsons = []\nmodels = []\nscalers = []\n\nFOLDS = 5\ngtss = GroupTimeSeriesSplit(n_folds=FOLDS, holdout_size=20, groups=train['time_id'])\nfor fold, (tr, val) in enumerate(gtss.split(train)):\n    print('FOLD:', fold)\n    \n    # use a fraction to training\n    X_train = train.loc[tr, FEATS]\n    y_train = train.loc[tr, 'target']\n    del tr\n    gc.collect()\n    \n    X_val = train.loc[val, FEATS]\n    y_val = train.loc[val, 'target']\n    del val\n    gc.collect()\n    \n    print('Train time_id range:', X_train['time_id'].min(), '->', X_train['time_id'].max())\n    print('Val time_id range:', X_val['time_id'].min(), '->', X_val['time_id'].max())\n    \n    # store time_id to calculate Pearson correlation\n    time_ids_val = X_val['time_id'].values\n    \n    # standardize\n#     scaler = StandardScaler()\n#     X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n#     X_val = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)\n#     scalers.append(scaler)\n    \n#     model = lgbm.LGBMRegressor(\n#         random_state=1,\n#         max_depth=3\n#     )\n#     model = svm.LinearSVR(\n#         random_state=1,\n#         loss='squared_epsilon_insensitive',\n#         dual=False # when n_samples > n_features\n#     )\n\n    # fit\n    model = linear_model.LinearRegression(\n        n_jobs=-1\n    )\n    model.fit(X_train.drop(['investment_id', 'time_id'], axis=1), y_train)\n    models.append(model)\n    \n    del X_train, y_train\n    gc.collect()\n    \n    # metrics\n    # submissions are evaluated on the mean of the Pearson correlation coefficient for each time ID\n    X_val['y_pred'] = model.predict(X_val.drop(['investment_id', 'time_id'], axis=1))\n    X_val['y_true'] = y_val.values\n    X_val['time_id'] = time_ids_val\n    \n    del y_val, time_ids_val\n    gc.collect()\n    \n    pearson = X_val[['time_id', 'y_true', 'y_pred']].groupby('time_id').apply(lambda x: pearsonr(x['y_true'], x['y_pred'])[0]).mean()\n    print('Pearson:', pearson)\n    print()\n    pearsons.append(pearson)\n    \n    del X_val\n    gc.collect()\n    \nprint('-' * 30)\nprint('Mean:', np.mean(pearsons))\nprint('Std:', np.std(pearsons))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BEST_FOLD = np.argmax(pearsons)\nprint(BEST_FOLD)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Linear Regression (GroupKFold) -> LB: 0.108\n# Mean: 0.12937332275514163\n# Std: 0.006988315782118249\n\n# Linear Regression (GroupTimeSeriesSplit) -> LB: 0.102\n# Mean: 0.11933183135552154\n# Std: 0.0416659840190710","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"import ubiquant\n\nenv = ubiquant.make_env()\niter_test = env.iter_test()\n\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    # time_id is not present in test set \n    test_df['time_id'] = test_df['row_id'].apply(lambda x: int(x.split('_')[0]))\n        \n    # predict using each model\n    final_pred = models[BEST_FOLD].predict(test_df[features])\n    \n    # average\n    sample_prediction_df['target'] = final_pred\n    \n    env.predict(sample_prediction_df)\n    display(sample_prediction_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}