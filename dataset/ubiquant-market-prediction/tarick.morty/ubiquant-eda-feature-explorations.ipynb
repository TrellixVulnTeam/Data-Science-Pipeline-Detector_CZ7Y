{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- This notebook contains an initial EDA of train dataset for the [Ubiquant Market Prediction competition](https://www.kaggle.com/c/ubiquant-market-prediction) built on https://www.kaggle.com/ilialar/ubiquant-eda-and-baseline and https://www.kaggle.com/gpreda/santander-eda-and-prediction\n- Main objective is to find some Features of interest and group features based on similarity","metadata":{}},{"cell_type":"markdown","source":"#### Load packages ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd        \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\nimport os\nimport logging\nimport datetime\nimport warnings\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 310)\npd.set_option('max_rows', 200)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-26T15:18:25.077864Z","iopub.execute_input":"2022-01-26T15:18:25.078188Z","iopub.status.idle":"2022-01-26T15:18:26.04665Z","shell.execute_reply.started":"2022-01-26T15:18:25.078155Z","shell.execute_reply":"2022-01-26T15:18:26.04537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load data\n- Let's see what all files are provided by Ubiquant","metadata":{}},{"cell_type":"code","source":"PATH=\"../input/ubiquant-market-prediction/\"\nos.listdir(PATH)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-26T15:18:26.049248Z","iopub.execute_input":"2022-01-26T15:18:26.049766Z","iopub.status.idle":"2022-01-26T15:18:26.060194Z","shell.execute_reply.started":"2022-01-26T15:18:26.049715Z","shell.execute_reply":"2022-01-26T15:18:26.059334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `example_sample_submission.csv` sample submission file (Not for EDA)\n- `ubiquant` API wheel for loading test data (Not for EDA)\n- `example_test.csv` sample test file (Not for EDA)\n- `train.csv` This is our little treasure - TRAINING DATA","metadata":{}},{"cell_type":"markdown","source":"Let's load the train file.","metadata":{}},{"cell_type":"code","source":"data_types_dict = {\n    'time_id': 'int32',\n    'investment_id': 'int16',\n    \"target\": 'float16'}\n\nfeatures = [f'f_{i}' for i in range(300)]\n\nfor f in features:\n    data_types_dict[f] = 'float16'\n    \ntarget = 'target'","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-26T15:18:26.061947Z","iopub.execute_input":"2022-01-26T15:18:26.062223Z","iopub.status.idle":"2022-01-26T15:18:26.073457Z","shell.execute_reply.started":"2022-01-26T15:18:26.062189Z","shell.execute_reply":"2022-01-26T15:18:26.07235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_df = pd.read_csv('/kaggle/input/ubiquant-market-prediction/train.csv', \n                       usecols = data_types_dict.keys(),\n                       dtype=data_types_dict, \n                       index_col = 0)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-26T15:18:26.07573Z","iopub.execute_input":"2022-01-26T15:18:26.076035Z","iopub.status.idle":"2022-01-26T15:25:49.67364Z","shell.execute_reply.started":"2022-01-26T15:18:26.076002Z","shell.execute_reply":"2022-01-26T15:25:49.67264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data exploration\nLet's check the train set.","metadata":{}},{"cell_type":"code","source":"train_df.shape","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-26T15:25:49.676624Z","iopub.execute_input":"2022-01-26T15:25:49.677642Z","iopub.status.idle":"2022-01-26T15:25:49.684787Z","shell.execute_reply.started":"2022-01-26T15:25:49.67759Z","shell.execute_reply":"2022-01-26T15:25:49.683613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a glimpse of train dataset.","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-26T15:25:49.686641Z","iopub.execute_input":"2022-01-26T15:25:49.686962Z","iopub.status.idle":"2022-01-26T15:25:49.953762Z","shell.execute_reply.started":"2022-01-26T15:25:49.68692Z","shell.execute_reply":"2022-01-26T15:25:49.952641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our dataset contains 300 anonymous features that don't have any description, `investment_id,` and target that is also some anonymous float value.\n\n**Train contains:**\n- time_id (int)\n- investment_id (int)\n- target (float)\n- 300 numerical features (float), from f_0 to f_299","metadata":{}},{"cell_type":"markdown","source":"Let's check if we have any missing data.","metadata":{}},{"cell_type":"code","source":"def missing_data(data):\n    \n    total = data.isnull().sum()\n    percent = (data.isnull().sum()/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    \n    return(np.transpose(tt))\n\nmissing_data(train_df)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-26T15:25:49.955255Z","iopub.execute_input":"2022-01-26T15:25:49.955495Z","iopub.status.idle":"2022-01-26T15:25:59.884605Z","shell.execute_reply.started":"2022-01-26T15:25:49.955467Z","shell.execute_reply":"2022-01-26T15:25:59.883677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no missing data present in the train data. Let's check the numerical value distribution.","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-26T15:25:59.885845Z","iopub.execute_input":"2022-01-26T15:25:59.886089Z","iopub.status.idle":"2022-01-26T15:28:31.707582Z","shell.execute_reply.started":"2022-01-26T15:25:59.886058Z","shell.execute_reply":"2022-01-26T15:28:31.706498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feats = []\nfor i in range(0,300):\n    if train_df['f_'+str(i)].std()!=0:\n        feats.append('f_'+str(i))\n\nprint('Features with Non Zero Standard Deviation: {}'.format(feats))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-26T15:28:31.708994Z","iopub.execute_input":"2022-01-26T15:28:31.709251Z","iopub.status.idle":"2022-01-26T15:28:31.819023Z","shell.execute_reply.started":"2022-01-26T15:28:31.709218Z","shell.execute_reply":"2022-01-26T15:28:31.81642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's interesting to see here that:\n- Majority of features have 0 standard deviations, seems like they are deidentified by some transformation.\n- Only `f_124` have a non-zero standard deviations (std = 0.04837)","metadata":{}},{"cell_type":"markdown","source":"Let's check the **target** distribution in train dataset.","metadata":{}},{"cell_type":"code","source":"train_df['target'].hist(bins = 100, figsize = (10,6))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-26T15:29:07.994299Z","iopub.execute_input":"2022-01-26T15:29:07.994624Z","iopub.status.idle":"2022-01-26T15:29:08.844835Z","shell.execute_reply.started":"2022-01-26T15:29:07.994592Z","shell.execute_reply":"2022-01-26T15:29:08.843837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Target values look quite normal without any outliers or long tails. Let's also plot distributions of targets with few random features.","metadata":{}},{"cell_type":"code","source":"for f in np.random.choice(train_df['investment_id'].unique(), 10):\n    train_df[train_df['investment_id'] == f]['target'].hist(bins=100, alpha=0.2,figsize=(10,6))","metadata":{"execution":{"iopub.status.busy":"2022-01-26T15:29:10.666675Z","iopub.execute_input":"2022-01-26T15:29:10.666958Z","iopub.status.idle":"2022-01-26T15:29:13.379437Z","shell.execute_reply.started":"2022-01-26T15:29:10.666928Z","shell.execute_reply":"2022-01-26T15:29:13.378486Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On a high-level target for each investment_id also looks ok.","metadata":{}},{"cell_type":"markdown","source":"### Feature plots with Target\nLet's see the density plot of variables in train dataset.","metadata":{}},{"cell_type":"code","source":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(20,24))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","metadata":{"execution":{"iopub.status.busy":"2022-01-26T15:29:13.381546Z","iopub.execute_input":"2022-01-26T15:29:13.382057Z","iopub.status.idle":"2022-01-26T15:29:13.391422Z","shell.execute_reply.started":"2022-01-26T15:29:13.382009Z","shell.execute_reply":"2022-01-26T15:29:13.390632Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t0 = train_df.loc[train_df['target'] > 0]\nt1 = train_df.loc[train_df['target'] < 0]\nfeatures = train_df.columns.values[2:102]\nplot_feature_distribution(t0, t1, '>0', '<0', features)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T15:29:44.074165Z","iopub.execute_input":"2022-01-26T15:29:44.074925Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = train_df.columns.values[102:202]\nplot_feature_distribution(t0, t1, '>0', '<0', features)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T15:28:31.826592Z","iopub.status.idle":"2022-01-26T15:28:31.826927Z","shell.execute_reply.started":"2022-01-26T15:28:31.826752Z","shell.execute_reply":"2022-01-26T15:28:31.826768Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = train_df.columns.values[202:]\nplot_feature_distribution(t0, t1, '>0', '<0', features)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T15:28:31.828073Z","iopub.status.idle":"2022-01-26T15:28:31.828657Z","shell.execute_reply.started":"2022-01-26T15:28:31.828452Z","shell.execute_reply":"2022-01-26T15:28:31.828471Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Some Observations based on the First 100 features density plot:\n- Features with significant spikes: `['f_23','f_39','f_42','f_45','f_55','f_64','f_68','f_71','f_77','f_79','f_80','f_92']`\n- Features with High Skewness: `['f_63','f_87','f_59','f_3','f_67','f_34','f_7','f_57','f_98','f_74','f_56','f_26','f_33','f_30','f_8','f_58']`\n\nWe can use this information to group features and do feature engineering using them for our prediction model.","metadata":{}},{"cell_type":"markdown","source":"### More to be Added later...","metadata":{}}]}