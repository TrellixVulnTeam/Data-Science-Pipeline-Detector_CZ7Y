{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is continuation of the training notebook here: https://www.kaggle.com/slawekbiel/fast-fastai-training/\n\nThe only difference is that I train on the full data rather than the first 80%, see [discussion here](https://www.kaggle.com/c/ubiquant-market-prediction/discussion/302286) for why that matters for LB score.","metadata":{}},{"cell_type":"code","source":"from fastai.tabular.all import *\nimport ubiquant","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:08:27.185203Z","iopub.execute_input":"2022-01-29T10:08:27.185511Z","iopub.status.idle":"2022-01-29T10:08:28.047383Z","shell.execute_reply.started":"2022-01-29T10:08:27.185456Z","shell.execute_reply":"2022-01-29T10:08:28.046334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torch.load('../input/ubiquant-models-public/fullmodel60.pth').cuda().eval()\nftrs = [f'f_{i}' for i in range(300)]\nenv = ubiquant.make_env()  \niter_test = env.iter_test()\nfor (test_df, sub_df) in iter_test:\n    data = torch.tensor(test_df[ftrs].to_numpy(), dtype=torch.float).cuda()\n    with torch.no_grad():\n        preds = model([], data)\n    sub_df['target'] = preds.view(-1).cpu().numpy()\n    env.predict(sub_df) ","metadata":{"execution":{"iopub.status.busy":"2022-01-29T10:08:28.053848Z","iopub.execute_input":"2022-01-29T10:08:28.054115Z","iopub.status.idle":"2022-01-29T10:08:30.064304Z","shell.execute_reply.started":"2022-01-29T10:08:28.054079Z","shell.execute_reply":"2022-01-29T10:08:30.06352Z"},"trusted":true},"execution_count":null,"outputs":[]}]}