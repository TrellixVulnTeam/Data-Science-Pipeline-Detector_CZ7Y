{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom pathlib import Path\nfrom argparse import Namespace\nfrom collections import defaultdict\nimport matplotlib as plt\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom scipy import stats\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport xgboost as xgb\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-07T14:00:37.855105Z","iopub.execute_input":"2022-03-07T14:00:37.856159Z","iopub.status.idle":"2022-03-07T14:00:43.027932Z","shell.execute_reply.started":"2022-03-07T14:00:37.856033Z","shell.execute_reply":"2022-03-07T14:00:43.027181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                #    df[col] = df[col].astype(np.float16)\n                #elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                #    df[col] = df[col].astype(np.float32)\n                #else:\n                df[col] = df[col].astype(np.float16)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:00:55.377275Z","iopub.execute_input":"2022-03-07T14:00:55.377572Z","iopub.status.idle":"2022-03-07T14:00:55.390545Z","shell.execute_reply.started":"2022-03-07T14:00:55.377533Z","shell.execute_reply":"2022-03-07T14:00:55.389484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#train = pd.read_csv('../input/ubiquant-market-prediction/train.csv')\ntrain = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:01:01.71912Z","iopub.execute_input":"2022-03-07T14:01:01.719386Z","iopub.status.idle":"2022-03-07T14:01:41.270316Z","shell.execute_reply.started":"2022-03-07T14:01:01.719357Z","shell.execute_reply":"2022-03-07T14:01:41.269547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = reduce_mem_usage(train)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:01:44.967505Z","iopub.execute_input":"2022-03-07T14:01:44.968147Z","iopub.status.idle":"2022-03-07T14:04:49.724287Z","shell.execute_reply.started":"2022-03-07T14:01:44.968106Z","shell.execute_reply":"2022-03-07T14:04:49.723484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"l=100\ntrain=trains[trains['investment_id']<=l]\ntest=trains[(trains['investment_id']>l) & (trains['investment_id']<l+20)]\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:57:54.327646Z","iopub.execute_input":"2022-02-17T17:57:54.327931Z","iopub.status.idle":"2022-02-17T17:57:54.724505Z","shell.execute_reply.started":"2022-02-17T17:57:54.327901Z","shell.execute_reply":"2022-02-17T17:57:54.723804Z"}}},{"cell_type":"code","source":"import gc\ngc.collect()\ntest_cols = ['investment_id']\nfeature = [c for c in train.columns if \"f_\" in c]\nfeatures = test_cols + feature\n","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:05:43.812004Z","iopub.execute_input":"2022-03-07T14:05:43.812569Z","iopub.status.idle":"2022-03-07T14:05:43.99645Z","shell.execute_reply.started":"2022-03-07T14:05:43.812511Z","shell.execute_reply":"2022-03-07T14:05:43.995592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = train.pop(\"row_id\")\n_ = train.pop(\"time_id\")","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:05:49.890325Z","iopub.execute_input":"2022-03-07T14:05:49.89111Z","iopub.status.idle":"2022-03-07T14:05:50.013995Z","shell.execute_reply.started":"2022-03-07T14:05:49.891062Z","shell.execute_reply":"2022-03-07T14:05:50.012816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:05:57.501688Z","iopub.execute_input":"2022-03-07T14:05:57.502154Z","iopub.status.idle":"2022-03-07T14:05:57.539315Z","shell.execute_reply.started":"2022-03-07T14:05:57.502115Z","shell.execute_reply":"2022-03-07T14:05:57.538495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"df=test\ndf['row_id']=((df['row_id']).str.strip())\n#df['len']=pd.to_numeric((df['row_id']).str.len()-2)\ndf['row_id']=df['row_id'].apply(lambda x:x[0:x.find('_')])\ndf['time_id']=pd.to_numeric(df['row_id'])\ntest=df\ntest.drop('row_id',axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T17:59:12.260771Z","iopub.execute_input":"2022-02-17T17:59:12.261299Z","iopub.status.idle":"2022-02-17T17:59:13.55776Z","shell.execute_reply.started":"2022-02-17T17:59:12.261262Z","shell.execute_reply":"2022-02-17T17:59:13.557016Z"}}},{"cell_type":"code","source":"\ny = train.pop(\"target\")\ninvestment_id = train.pop(\"investment_id\")","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:10:27.540835Z","iopub.execute_input":"2022-03-07T14:10:27.54143Z","iopub.status.idle":"2022-03-07T14:10:27.886961Z","shell.execute_reply.started":"2022-03-07T14:10:27.541389Z","shell.execute_reply":"2022-03-07T14:10:27.885893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ninvestment_ids = list(investment_id.unique())\ninvestment_id_size = len(investment_ids) + 1\ninvestment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\ninvestment_id_lookup_layer.adapt(pd.DataFrame({\"investment_ids\":investment_ids}))","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:10:37.918562Z","iopub.execute_input":"2022-03-07T14:10:37.919134Z","iopub.status.idle":"2022-03-07T14:10:40.708623Z","shell.execute_reply.started":"2022-03-07T14:10:37.919086Z","shell.execute_reply":"2022-03-07T14:10:40.707881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ndef preprocess(X, y):\n    return X, y\ndef make_dataset(feature, y, batch_size=1024, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices(((feature), y))\n    \n\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(4096)\n    \n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:10:47.873388Z","iopub.execute_input":"2022-03-07T14:10:47.874139Z","iopub.status.idle":"2022-03-07T14:10:47.880263Z","shell.execute_reply.started":"2022-03-07T14:10:47.8741Z","shell.execute_reply":"2022-03-07T14:10:47.879187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    #investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    #investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    #investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    #investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    #investment_id_x = layers.Dense(64, activation='swis')(investment_id_x)\n    #investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    #investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(512, activation='swish')(features_inputs)\n    feature_x = layers.Dense(512, activation='swish')(feature_x)\n    feature_x = layers.Dense(515, activation='swish')(feature_x)\n    \n    #x = layers.Concatenate(axis=1)([feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(256, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x) \n    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x) \n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.0001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:10:54.289455Z","iopub.execute_input":"2022-03-07T14:10:54.289994Z","iopub.status.idle":"2022-03-07T14:10:54.302301Z","shell.execute_reply.started":"2022-03-07T14:10:54.289943Z","shell.execute_reply":"2022-03-07T14:10:54.301486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model()\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:11:20.029277Z","iopub.execute_input":"2022-03-07T14:11:20.029671Z","iopub.status.idle":"2022-03-07T14:11:21.025094Z","shell.execute_reply.started":"2022-03-07T14:11:20.029631Z","shell.execute_reply":"2022-03-07T14:11:21.024235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:11:26.870108Z","iopub.execute_input":"2022-03-07T14:11:26.8704Z","iopub.status.idle":"2022-03-07T14:11:27.080662Z","shell.execute_reply.started":"2022-03-07T14:11:26.870364Z","shell.execute_reply":"2022-03-07T14:11:27.079637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(5, shuffle=True, random_state=42)\nmodels = []\nfor index, (train_indices, valid_indices) in enumerate(kfold.split(train,investment_id)):\n    X_train, X_val = train.iloc[train_indices], train.iloc[valid_indices]\n    #investment_id_train = investment_id[train_indices]\n    y_train, y_val = y.iloc[train_indices], y.iloc[valid_indices]\n    #investment_id_val = investment_id[valid_indices]\n    train_ds = make_dataset(X_train, y_train)\n    valid_ds = make_dataset(X_val, y_val, mode=\"valid\")\n    model = get_model()\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{index}\", save_best_only=True)\n    early_stop = keras.callbacks.EarlyStopping(patience=10)\n    history = model.fit(train_ds, epochs=25, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n    model = keras.models.load_model(f\"model_{index}\")\n    models.append(model)\n    #pearson_score = stats.pearsonr(model.predict(valid_ds).ravel(), y_val.values)[0]\n    #print('Pearson:', pearson_score)\n    #pd.DataFrame(history.history, columns=[\"mse\", \"val_mse\"]).plot()\n    #plt.title(\"MSE\")\n    #plt.show()\n    #pd.DataFrame(history.history, columns=[\"mae\", \"val_mae\"]).plot()\n    #plt.title(\"MAE\")\n    #plt.show()\n    #pd.DataFrame(history.history, columns=[\"rmse\", \"val_rmse\"]).plot()\n    #plt.title(\"RMSE\")\n    #plt.show()\n    #del investment_id_train\n    #del investment_id_val\n    del X_train\n    del X_val\n    del y_train\n    del y_val\n    del train_ds\n    del valid_ds\n    gc.collect()\n    break","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:11:30.73808Z","iopub.execute_input":"2022-03-07T14:11:30.738911Z","iopub.status.idle":"2022-03-07T14:20:09.777615Z","shell.execute_reply.started":"2022-03-07T14:11:30.738854Z","shell.execute_reply":"2022-03-07T14:20:09.774833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_test(feature):\n    return (feature), 0\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((feature)))\n    ds = ds.map(preprocess_test)\n    #ds = ds.batch(batch_size)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:25:47.3657Z","iopub.execute_input":"2022-03-07T14:25:47.366216Z","iopub.status.idle":"2022-03-07T14:25:47.377259Z","shell.execute_reply.started":"2022-03-07T14:25:47.366173Z","shell.execute_reply":"2022-03-07T14:25:47.376135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfor (test_df, sample_prediction_df) in iter_test:\n    ds = make_test_dataset(test_df[feature], test_df[\"investment_id\"])\n    sample_prediction_df['target'] = inference(models, ds)\n    env.predict(sample_prediction_df) ","metadata":{"execution":{"iopub.status.busy":"2022-03-07T14:25:52.9788Z","iopub.execute_input":"2022-03-07T14:25:52.979075Z","iopub.status.idle":"2022-03-07T14:25:53.362422Z","shell.execute_reply.started":"2022-03-07T14:25:52.979044Z","shell.execute_reply":"2022-03-07T14:25:53.361698Z"},"trusted":true},"execution_count":null,"outputs":[]}]}