{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# api是否能本地调用 A：不可以\n# 测试集数据能否拉出来 A：不可以\n# 看下测试集每个investiment预测长度H A：不可以\n\n# 每个investiment的匿名特征是否为单一属性值，还是时序变化？A：随时序变化,但有个别特征比较奇怪，例如很多都为0，待深入\n# 选择递归预测还是直接预测？\n# 使用lgb\n# 采用不同的交叉验证方式，看何种方法跟线上更基本保持一致\n\n# 加入统计特征\n# 声明categorical features\n# 采用mlp\n# 借鉴deepar方式，引入协变量\n\n# 处理time_id不等距问题\n\n# 在线学习，随着测试集的引入，不断训练再预测","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:01:45.911859Z","iopub.execute_input":"2022-02-06T12:01:45.912491Z","iopub.status.idle":"2022-02-06T12:01:45.932599Z","shell.execute_reply.started":"2022-02-06T12:01:45.91238Z","shell.execute_reply":"2022-02-06T12:01:45.931943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport gc\nimport random\nimport copy\nimport joblib\nfrom collections import Counter, defaultdict\n\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:38:31.238391Z","iopub.execute_input":"2022-02-11T13:38:31.239232Z","iopub.status.idle":"2022-02-11T13:38:34.07324Z","shell.execute_reply.started":"2022-02-11T13:38:31.239188Z","shell.execute_reply":"2022-02-11T13:38:34.072502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_CSV_PATH = '../input/ubiquant-market-prediction/train.csv'\nTRAIN_PKL_PATH = '../input/ump-train-picklefile/train.pkl'\nEXP_TEST_PATH = '../input/ubiquant-market-prediction/example_test.csv'\nEXP_SUB_PATH = '../input/ubiquant-market-prediction/example_sample_submission.csv'\nUSE_COLS = ['investment_id', ] + [f'f_{i}' for i in range(300)]\nCAT_COLS = ['investment_id']\nTARGET_COLS = 'target'","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:38:34.074817Z","iopub.execute_input":"2022-02-11T13:38:34.075056Z","iopub.status.idle":"2022-02-11T13:38:34.081123Z","shell.execute_reply.started":"2022-02-11T13:38:34.075023Z","shell.execute_reply":"2022-02-11T13:38:34.080374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_csv2pickle(csv_path, pkl_path):\n    '''将CSV转为PKL文件'''\n    basecols = ['row_id', 'time_id', 'investment_id', 'target']\n    features = [f'f_{i}' for i in range(300)]\n    usecols = basecols+features\n    \n    dtypes = {\n        'row_id': 'str',\n        'time_id': 'uint16',\n        'investment_id': 'uint16',\n        'target': 'float32',\n    }\n    for col in features:\n        dtypes[col] = 'float32'\n\n    train = pd.read_csv(csv_path, usecols=usecols, dtype=dtypes)\n    train.to_pickle(pkl_path)\n\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df      \n    \ndef fix_seed(seed = 2022):\n    '''固定种子数'''\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:38:34.331722Z","iopub.execute_input":"2022-02-11T13:38:34.331952Z","iopub.status.idle":"2022-02-11T13:38:34.348905Z","shell.execute_reply.started":"2022-02-11T13:38:34.331927Z","shell.execute_reply":"2022-02-11T13:38:34.348206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stratified_group_k_fold(y, groups, k, seed=None):\n    '''\n    将 具备相似bin label样本分布 的timeids收集起来放入fold的测试集中。\n\n    stratified是保证fold内的每个class下的样本分布尽可能一致/相似。\n    stratified_group_kfold是保证fold内的每个timeids下的样本分布尽可能一致/相似。\n\n    params:\n        X: 自变量特征\n        y: target的bin label\n        group: time_id\n        k: k折数量\n    '''\n    labels_num = np.max(y) + 1 # bin label的数量\n    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n    y_distr = Counter()\n    for label, g in zip(y, groups):\n        y_counts_per_group[g][label] += 1 # 每个timeid下，给定bin label下，样本数量\n        y_distr[label] += 1 # 每个timeid下的样本数量\n\n    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n    groups_per_fold = defaultdict(set)\n\n    \n    def eval_y_counts_per_fold(y_counts, fold):\n        y_counts_per_fold[fold] += y_counts # {fold0:[bin1_num, bin2_num, ...],...}\n        std_per_label = [] # [fold0的bin label样本分布的标准差,..]\n        for label in range(labels_num):\n            # 求fold下，各bin的样本占比，然后求std，得到每个fold的bin label样本分布的标准差\n            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)]) \n            std_per_label.append(label_std)\n        y_counts_per_fold[fold] -= y_counts # 还原成初始值\n        return np.mean(std_per_label) # 求各个fold下，bin label样本分布的平均标准差\n    \n    groups_and_y_counts = list(y_counts_per_group.items()) # [(timeid,[bin1_num, bin2_num, ...]), (timeid,[bin1_num, bin2_num, ...]), ...]，内部每个list对应一个timeid的各bin样本数\n    random.Random(seed).shuffle(groups_and_y_counts) # 打乱timeid\n\n    # 整个目的：保证每个fold里的timeids们，都有相似的bin label样本分布！！！\n    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])): # 某timeid内的target分布越离散，-std越小，排序靠上\n        # g为timeid, y_counts为[bin1_num, bin2_num, ...]\n        best_fold = None\n        min_eval = None\n        for i in range(k):\n            fold_eval = eval_y_counts_per_fold(y_counts, i) # 求各个fold下，bin label样本分布的平均标准差\n            if min_eval is None or fold_eval < min_eval:\n                min_eval = fold_eval\n                best_fold = i\n        y_counts_per_fold[best_fold] += y_counts # 将bin label样本分布std最小的fold下，[bin1_num, bin2_num, ...]追加进去\n        groups_per_fold[best_fold].add(g) # {最好的fold: (timeid, timeid, ...)}\n\n    all_groups = set(groups)\n    for i in range(k):\n        # 获取每个fold的训练和测试集\n        train_groups = all_groups - groups_per_fold[i]\n        test_groups = groups_per_fold[i]\n\n        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n        yield train_indices, test_indices","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:01:48.893621Z","iopub.execute_input":"2022-02-06T12:01:48.893935Z","iopub.status.idle":"2022-02-06T12:01:48.909317Z","shell.execute_reply.started":"2022-02-06T12:01:48.893899Z","shell.execute_reply":"2022-02-06T12:01:48.908443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_train_valid(data, method=1):\n    '''划分数据集->训练集和验证集\n    每个investment_id下最近20%的数据作为验证集'''\n    if method == 1:\n        # 取每条时序的后面部分作为验证集\n        train_idxs, valid_idxs = [], []\n        data = data.sort_values(by=['investment_id', 'time_id'], ascending=(True, True))\n        valid_ratio = 0.2\n        for investment_id in tqdm(data['investment_id'].unique()):\n            tgt_data = data[data['investment_id'] == investment_id]\n            valid_num = int(valid_ratio * len(tgt_data))\n            train_idxs.extend(tgt_data.iloc[:-valid_num,:].index.to_list())\n            valid_idxs.extend(tgt_data.iloc[-valid_num:,:].index.to_list())\n        train = data.iloc[train_idxs,:]\n        valid = data.iloc[valid_idxs,:]\n    return train, valid\n\n# # 划分数据集\n# train_df, valid_df = split_train_valid(trn_df, method=1)\n# print(trn_df.shape, train_df.shape, valid_df.shape)\n# del trn_df # 释放内存，否则会报错","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:01:48.912054Z","iopub.execute_input":"2022-02-06T12:01:48.912777Z","iopub.status.idle":"2022-02-06T12:01:48.922201Z","shell.execute_reply.started":"2022-02-06T12:01:48.912733Z","shell.execute_reply":"2022-02-06T12:01:48.921401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pearson_coef(data):\n    return data.corr()['target']['preds']\n\ndef comp_metric(valid_df):\n    # 对每个time_id求pearson系数再平均\n    return np.mean(valid_df[['time_id', 'target', 'preds']].groupby('time_id').apply(pearson_coef))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:01:48.923528Z","iopub.execute_input":"2022-02-06T12:01:48.923859Z","iopub.status.idle":"2022-02-06T12:01:48.934295Z","shell.execute_reply.started":"2022-02-06T12:01:48.92382Z","shell.execute_reply":"2022-02-06T12:01:48.933505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_params = {\n    'metric':'rmse',\n    'objective':'regression',\n    'learning_rate':0.1,\n    'seed':2020,\n    'boosting_type':'gbdt', # 也可用其他的，但dart不支持early stopping\n    'early_stopping_round':30,\n    'subsample':0.8,\n    'colsample_bytree':0.8,\n    'subsample': 0.8,\n    'lambda_l1': 0.014647978740193088, \n    'lambda_l2': 1.0739045659323434e-05,\n    'num_leaves': 88, \n    'colsample_bytree': 0.5658895874313851, \n    'subsample': 0.8999680059621733, \n    'bagging_freq': 3, \n    'max_depth': 13, \n    'max_bin': 442, \n    'min_data_in_leaf': 316,\n    'n_jobs': -1,\n    'verbose':-1\n}","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:01:48.936981Z","iopub.execute_input":"2022-02-06T12:01:48.937892Z","iopub.status.idle":"2022-02-06T12:01:48.944665Z","shell.execute_reply.started":"2022-02-06T12:01:48.937836Z","shell.execute_reply":"2022-02-06T12:01:48.943599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn_df = pd.read_pickle(TRAIN_PKL_PATH)\nprint(len(trn_df['investment_id'].unique()))\ntrn_df = reduce_mem_usage(trn_df)\nprint(len(trn_df['investment_id'].unique()))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:38:38.481771Z","iopub.execute_input":"2022-02-11T13:38:38.482039Z","iopub.status.idle":"2022-02-11T13:42:18.034229Z","shell.execute_reply.started":"2022-02-11T13:38:38.482009Z","shell.execute_reply":"2022-02-11T13:42:18.033375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transform_csv2pickle(TRAIN_CSV_PATH, TRAIN_PKL_PATH)\n\n# 读取数据集\ntrn_df = pd.read_pickle(TRAIN_PKL_PATH)\ntrn_df = reduce_mem_usage(trn_df)\ndisplay(trn_df.info())\ndisplay(trn_df.head())\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:01:48.946405Z","iopub.execute_input":"2022-02-06T12:01:48.946812Z","iopub.status.idle":"2022-02-06T12:05:32.285052Z","shell.execute_reply.started":"2022-02-06T12:01:48.946734Z","shell.execute_reply":"2022-02-06T12:05:32.284295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stratified_group_k_fold：将具备相似bin label样本分布的timeids收集起来放入fold的验证集中\nnum_bins = 12\ntrn_df['bins'] = pd.cut(trn_df['target'], bins=num_bins, labels=False)\ntrn_y = trn_df[['target']]\ntrn_df = trn_df.drop(['target'], axis=1)\ngroups = np.array(trn_df['time_id'].values)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:05:32.286199Z","iopub.execute_input":"2022-02-06T12:05:32.286535Z","iopub.status.idle":"2022-02-06T12:05:35.010584Z","shell.execute_reply.started":"2022-02-06T12:05:32.2865Z","shell.execute_reply":"2022-02-06T12:05:35.009752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# investment过多，不适用单独建模\n\ntrn_df['preds'] = np.zeros(len(trn_df))\nfold_num = 0\npred_valid_scores = []\n\nfor trn_ind, val_ind in stratified_group_k_fold(trn_df['bins'].values, groups, k=5, seed=2020):\n    fold_num += 1\n    print('-'*20, 'Fold '+str(fold_num), '-'*20)\n    \n    # 创建LGBM数据集\n    train_matrix = lgb.Dataset(trn_df[USE_COLS].iloc[trn_ind,:], label=trn_y[TARGET_COLS].iloc[trn_ind], categorical_feature=CAT_COLS)\n    valid_matrix = lgb.Dataset(trn_df[USE_COLS].iloc[val_ind,:], label=trn_y[TARGET_COLS].iloc[val_ind], categorical_feature=CAT_COLS)\n    \n    # 训练模型\n    lgb_model = lgb.train(model_params,\n                            train_matrix,\n                            num_boost_round=1000,\n                            valid_sets=[valid_matrix], \n                            categorical_feature=[],\n                            verbose_eval=250)\n\n    # 预测划分后验证集 和 输出评估分数\n    pred_valid = lgb_model.predict(trn_df[USE_COLS].iloc[val_ind,:], num_iteration =  lgb_model.best_iteration)\n    pred_valid_score = lgb_model.best_score['valid_0']['rmse']\n    pred_valid_scores.append(pred_valid_score)\n    trn_df['preds'].iloc[val_ind] = pred_valid\n    \n    # 保存模型\n    lgb_model.save_model(f'lgbm_{fold_num}.txt', num_iteration=lgb_model.best_iteration)\n#     joblib.dump(lgb_model, f'lgbm_{fold_num}.pkl')\n    \n    # 清空变量和内存\n    del train_matrix, valid_matrix, lgb_model\n    gc.collect()\n    \n    \ntrn_df['target'] = trn_y\nvalid_pearson_coef = comp_metric(trn_df)\nprint('='*20)\nprint('The valid dataset | %s is %0.4f and pearson coef. is %.4f' % (model_params['metric'],\n                                                                     np.mean(pred_valid_scores),\n                                                                     valid_pearson_coef))\ndel trn_df, trn_y\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T12:05:35.011857Z","iopub.execute_input":"2022-02-06T12:05:35.01221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 读取模型\n# models = [joblib.load(f'lgbm_{fold}.pkl') for fold in range(1,6)]\nmodels = [lgb.Booster(model_file=f'lgbm_{fold}.txt') for fold in range(1,6)]\n\nimport ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test()\nfor (test_df, sample_prediction_df) in iter_test:\n    pred = 0 \n    for lgb_model in models:\n        pred += lgb_model.predict(test_df[USE_COLS])\n    sample_prediction_df['target'] = pred / len(models)\n    env.predict(sample_prediction_df) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}