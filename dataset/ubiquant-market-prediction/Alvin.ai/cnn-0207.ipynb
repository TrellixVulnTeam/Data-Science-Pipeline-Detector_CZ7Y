{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# https://www.kaggle.com/lonnieqin/ubiquant-market-prediction-with-dnn/notebook","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport gc\nimport random\nimport copy\nimport gc\nfrom collections import Counter, defaultdict\nfrom scipy import stats\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.metrics import mean_squared_error\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-07T14:55:54.755723Z","iopub.execute_input":"2022-02-07T14:55:54.755993Z","iopub.status.idle":"2022-02-07T14:56:00.509429Z","shell.execute_reply.started":"2022-02-07T14:55:54.75594Z","shell.execute_reply":"2022-02-07T14:56:00.508604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_CSV_PATH = '../input/ubiquant-market-prediction/train.csv'\nTRAIN_PKL_PATH = '../input/ump-train-picklefile/train.pkl'\nEXP_TEST_PATH = '../input/ubiquant-market-prediction/example_test.csv'\nEXP_SUB_PATH = '../input/ubiquant-market-prediction/example_sample_submission.csv'\nUSE_COLS = ['investment_id', ] + [f'f_{i}' for i in range(300)]\nCAT_COLS = ['investment_id']\nTARGET_COLS = 'target'","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:56:02.046481Z","iopub.execute_input":"2022-02-07T14:56:02.047261Z","iopub.status.idle":"2022-02-07T14:56:02.054116Z","shell.execute_reply.started":"2022-02-07T14:56:02.047209Z","shell.execute_reply":"2022-02-07T14:56:02.052016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_csv2pickle(csv_path, pkl_path):\n    '''将CSV转为PKL文件'''\n    basecols = ['row_id', 'time_id', 'investment_id', 'target']\n    features = [f'f_{i}' for i in range(300)]\n    usecols = basecols+features\n    \n    dtypes = {\n        'row_id': 'str',\n        'time_id': 'uint16',\n        'investment_id': 'uint16',\n        'target': 'float32',\n    }\n    for col in features:\n        dtypes[col] = 'float32'\n\n    train = pd.read_csv(csv_path, usecols=usecols, dtype=dtypes)\n    train.to_pickle(pkl_path)\n\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    return df    \n    \ndef fix_seed(seed = 2022):\n    '''固定种子数'''\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:56:02.880725Z","iopub.execute_input":"2022-02-07T14:56:02.881108Z","iopub.status.idle":"2022-02-07T14:56:02.898281Z","shell.execute_reply.started":"2022-02-07T14:56:02.881071Z","shell.execute_reply":"2022-02-07T14:56:02.897599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stratified_group_k_fold(y, groups, k, seed=None):\n    '''\n    将 具备相似bin label样本分布 的timeids收集起来放入fold的测试集中。\n\n    stratified是保证fold内的每个class下的样本分布尽可能一致/相似。\n    stratified_group_kfold是保证fold内的每个timeids下的样本分布尽可能一致/相似。\n\n    params:\n        y: target的bin label\n        group: time_id\n        k: k折数量\n    '''\n    labels_num = np.max(y) + 1 # bin label的数量\n    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n    y_distr = Counter()\n    for label, g in zip(y, groups):\n        y_counts_per_group[g][label] += 1 # 每个timeid下，给定bin label下，样本数量\n        y_distr[label] += 1 # 每个timeid下的样本数量\n\n    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n    groups_per_fold = defaultdict(set)\n\n    \n    def eval_y_counts_per_fold(y_counts, fold):\n        y_counts_per_fold[fold] += y_counts # {fold0:[bin1_num, bin2_num, ...],...}\n        std_per_label = [] # [fold0的bin label样本分布的标准差,..]\n        for label in range(labels_num):\n            # 求fold下，各bin的样本占比，然后求std，得到每个fold的bin label样本分布的标准差\n            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)]) \n            std_per_label.append(label_std)\n        y_counts_per_fold[fold] -= y_counts # 还原成初始值\n        return np.mean(std_per_label) # 求各个fold下，bin label样本分布的平均标准差\n    \n    groups_and_y_counts = list(y_counts_per_group.items()) # [(timeid,[bin1_num, bin2_num, ...]), (timeid,[bin1_num, bin2_num, ...]), ...]，内部每个list对应一个timeid的各bin样本数\n    random.Random(seed).shuffle(groups_and_y_counts) # 打乱timeid\n\n    # 整个目的：保证每个fold里的timeids们，都有相似的bin label样本分布！！！\n    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])): # 某timeid内的target分布越离散，-std越小，排序靠上\n        # g为timeid, y_counts为[bin1_num, bin2_num, ...]\n        best_fold = None\n        min_eval = None\n        for i in range(k):\n            fold_eval = eval_y_counts_per_fold(y_counts, i) # 求各个fold下，bin label样本分布的平均标准差\n            if min_eval is None or fold_eval < min_eval:\n                min_eval = fold_eval\n                best_fold = i\n        y_counts_per_fold[best_fold] += y_counts # 将bin label样本分布std最小的fold下，[bin1_num, bin2_num, ...]追加进去\n        groups_per_fold[best_fold].add(g) # {最好的fold: (timeid, timeid, ...)}\n\n    all_groups = set(groups)\n    for i in range(k):\n        # 获取每个fold的训练和测试集\n        train_groups = all_groups - groups_per_fold[i]\n        test_groups = groups_per_fold[i]\n\n        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n        yield train_indices, test_indices","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:56:03.614646Z","iopub.execute_input":"2022-02-07T14:56:03.615328Z","iopub.status.idle":"2022-02-07T14:56:03.630698Z","shell.execute_reply.started":"2022-02-07T14:56:03.615281Z","shell.execute_reply":"2022-02-07T14:56:03.629992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pearson_coef(data):\n    return data.corr()['target']['preds']\n\ndef comp_metric(valid_df):\n    # 对每个time_id求pearson系数再平均\n    return np.mean(valid_df[['time_id', 'target', 'preds']].groupby('time_id').apply(pearson_coef))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:56:04.332535Z","iopub.execute_input":"2022-02-07T14:56:04.333095Z","iopub.status.idle":"2022-02-07T14:56:04.338135Z","shell.execute_reply.started":"2022-02-07T14:56:04.333045Z","shell.execute_reply":"2022-02-07T14:56:04.337164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 读取数据集\ntrn_df = pd.read_pickle(TRAIN_PKL_PATH)\ntrn_df = reduce_mem_usage(trn_df)\ndisplay(trn_df.info())\ndisplay(trn_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:56:06.507758Z","iopub.execute_input":"2022-02-07T14:56:06.508101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_intlookup_layer(ts_id_df):\n    '''针对时序标识特征，初始化tf的IntegerLookup层'''\n    ts_ids = list(ts_id_df.unique())\n    ts_id_size = len(ts_ids) + 1\n    ts_id_lookup_layer = layers.IntegerLookup(max_tokens=ts_id_size)\n    ts_id_lookup_layer.adapt(ts_id_df)\n    return ts_id_lookup_layer\n\ndef preprocess(X, y):\n    '''数据预处理'''\n    return X, y\n\ndef make_dataset(feature, investment_id, y, batch_size=800, mode=\"train\"):\n    '''准备数据集'''\n    # 将multi-D的tensor -> 1d tensor\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        # 避免内存不够，每次选buffer_size个数据进行shuffle\n        ds = ds.shuffle(buffer_size=4096, seed=2020)\n    # 提升训练流程，batch放入内存，在GPU训练的同时，CPU在准备下一次训练用的数据\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE) \n    return ds\n\ndef get_model(id_size, id_df, feat_num=300):\n    '''构建DNN模型\n    params:\n        id_size(int):时序id的数量\n        id_df(pd.DataFrame):时序id的枚举值dataframe\n        feat_num(int):特征数量\n        \n    '''\n    id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((feat_num, ), dtype=tf.float16)\n    \n    # 构建id的索引层\n    id_lookup_layer = layers.IntegerLookup(max_tokens=id_size)\n    id_lookup_layer.adapt(id_df)\n    \n    id_x = id_lookup_layer(id_inputs)\n    id_x = layers.Embedding(id_size, 32, input_length=1)(id_x) # 对时序id做embedding\n    id_x = layers.Reshape((-1, ))(id_x)\n    id_x = layers.Dense(64, activation='swish')(id_x)\n    id_x = layers.Dense(64, activation='swish')(id_x)\n    id_x = layers.Dense(64, activation='swish')(id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\ndef pearson_coef(data):\n    return data.corr()['target']['preds']\n\ndef comp_metric(valid_df):\n    # 对每个time_id求pearson系数再平均\n    return np.mean(valid_df[['time_id', 'target', 'preds']].groupby('time_id').apply(pearson_coef))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment_id_df = pd.DataFrame({'investment_ids':list(trn_df['investment_id'].unique())})\nfeat_num = 300 # 特征数量\nid_size = len(investment_id_df) + 1 # 时序id的数量+1\nUSE_COLS = [f'f_{i}' for i in range(300)]\nCAT_COLS = ['investment_id']\n\n# 展示模型\nmodel = get_model(id_size, investment_id_df, feat_num)\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)\ndel model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stratified_group_k_fold：将具备相似bin label样本分布的timeids收集起来放入fold的验证集中\nnum_bins = 12\ntrn_df['bins'] = pd.cut(trn_df['target'], bins=num_bins, labels=False)\ntrn_y = trn_df[[\"target\"]]\ntrn_df = trn_df.drop(['target'], axis=1)\ngroups = np.array(trn_df['time_id'].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 模型训练和验证\nfold_num = 0\n\ntrn_df['preds'] = np.zeros(len(trn_df))\npred_valid_scores = []\nsgkf = stratified_group_k_fold(trn_df['bins'].values, groups, k=5, seed=2020)\ndel groups\n\nfor trn_ind, val_ind in sgkf:\n    fold_num += 1\n    print('-'*20, 'Fold '+str(fold_num), '-'*20)\n    \n    # 准备数据集\n    X_train, X_val = trn_df[USE_COLS].iloc[trn_ind,:], trn_df[USE_COLS].iloc[val_ind,:]\n    y_train, y_val = trn_y[TARGET_COLS].iloc[trn_ind], trn_y[TARGET_COLS].iloc[val_ind]\n    investment_id_train, investment_id_val = trn_df[['investment_id']].iloc[trn_ind,:], trn_df[['investment_id']].iloc[val_ind,:]\n    train_ds = make_dataset(X_train, investment_id_train, y_train)\n    valid_ds = make_dataset(X_val, investment_id_val, y_val, mode=\"valid\")\n    del X_train, y_train, investment_id_train, investment_id_val\n    \n    # 获取model\n    model = get_model(id_size, investment_id_df, feat_num)\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{fold_num}\", save_best_only=True)\n    early_stop = keras.callbacks.EarlyStopping(patience=10)\n    history = model.fit(train_ds, epochs=30, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n    model = keras.models.load_model(f\"model_{fold_num}\")\n    \n    \n    # 预测\n    pred_valid = model.predict(valid_ds).ravel()\n    trn_df['preds'].iloc[val_ind] = pred_valid\n    pred_valid_scores.append(np.min(history.history[\"val_mse\"]))\n    pd.DataFrame(history.history, columns=[\"mse\", \"val_mse\"]).plot()\n    plt.title(\"MSE\")\n    plt.show()\n    pd.DataFrame(history.history, columns=[\"mae\", \"val_mae\"]).plot()\n    plt.title(\"MAE\")\n    plt.show()\n    pd.DataFrame(history.history, columns=[\"rmse\", \"val_rmse\"]).plot()\n    plt.title(\"RMSE\")\n    plt.show()\n    \n    del X_val, y_val, train_ds, valid_ds, model, history\n    gc.collect()\n\ntrn_df['target'] = trn_y\nvalid_pearson_coef = comp_metric(trn_df)\nprint('='*20)\nprint('The valid dataset | %s is %0.4f and pearson coef. is %.4f' % (model_params['metric'],\n                                                                     np.mean(pred_valid_scores),\n                                                                     valid_pearson_coef))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_test(investment_id, feature):\n    '''数据预处理'''\n    return (investment_id, feature), 0\n            \ndef make_test_dataset(feature, investment_id, batch_size=800):\n    '''准备数据集'''\n    # 将multi-D的tensor -> 1d tensor\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    # 提升训练流程，batch放入内存，在GPU训练的同时，CPU在准备下一次训练用的数据\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE) \n    return ds\n\ndef inference(models, ds):\n    y_preds = []\n    for fold_num in range(1,6):\n        model = keras.models.load_model(f\"model_{fold_num}\")\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n        del model\n    return np.mean(y_preds, axis=0)    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfor (test_df, sample_prediction_df) in iter_test:\n    ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n    sample_prediction_df['target'] = inference(models, ds)\n    env.predict(sample_prediction_df) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}