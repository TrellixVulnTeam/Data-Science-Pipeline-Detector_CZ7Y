{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ubiquant Market Prediction \n## Comparing ML Techniques and LightGBM Finetuning! ‚ö°\n","metadata":{"papermill":{"duration":0.04285,"end_time":"2022-02-05T13:16:48.192238","exception":false,"start_time":"2022-02-05T13:16:48.149388","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"color:turquoise;\n           display:fill;\n           border-radius:5px;\n           background-color:aquamarine;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:turquoise;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.04119,"end_time":"2022-02-05T13:16:48.275452","exception":false,"start_time":"2022-02-05T13:16:48.234262","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Introduction","metadata":{"papermill":{"duration":0.041284,"end_time":"2022-02-05T13:16:48.359996","exception":false,"start_time":"2022-02-05T13:16:48.318712","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"This notebook aims to share initial analysis on which technique may get you a nice score in the Ubiquant Market Prediction Competition. üçÄ\n\nIt does this by trying out different models and comparing their results! \n\nThe models compared are:\n\n\n* Linear Regression\n* Random Forest Regressor\n* Ridge\n* XGBoost\n* LightGBM\n* Support Vector Regressor\n\n\nThe 3 sections of this notebook includes: Import, Model Comparison and LightGBM Finetuning (As the LightGBM, seen the best performance, from our analysis).\n\nHere we go! üòÄ","metadata":{"papermill":{"duration":0.043735,"end_time":"2022-02-05T13:16:48.445502","exception":false,"start_time":"2022-02-05T13:16:48.401767","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"\n<div style=\"color:yellow;\n           display:fill;\n           border-radius:5px;\n           background-color:chartreuse;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:yellow;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.04236,"end_time":"2022-02-05T13:16:48.529834","exception":false,"start_time":"2022-02-05T13:16:48.487474","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Import","metadata":{"papermill":{"duration":0.041235,"end_time":"2022-02-05T13:16:48.613158","exception":false,"start_time":"2022-02-05T13:16:48.571923","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"First we need to import!","metadata":{"papermill":{"duration":0.042799,"end_time":"2022-02-05T13:16:48.697407","exception":false,"start_time":"2022-02-05T13:16:48.654608","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Import libraries\nimport pandas as pd\nimport pickle\nimport lightgbm as lgb\nimport datetime\nfrom datetime import datetime\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd\nimport plotly_express as px\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"papermill":{"duration":2.743005,"end_time":"2022-02-05T13:16:51.482145","exception":false,"start_time":"2022-02-05T13:16:48.73914","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-03T12:20:46.4464Z","iopub.execute_input":"2022-03-03T12:20:46.446859Z","iopub.status.idle":"2022-03-03T12:20:50.056144Z","shell.execute_reply.started":"2022-03-03T12:20:46.44682Z","shell.execute_reply":"2022-03-03T12:20:50.055418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing pickle files\nwith open('../input/ubiquant-how-to-make-pickle-file/train.pickle', 'rb') as f:\n    train = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:20:51.759879Z","iopub.execute_input":"2022-03-03T12:20:51.760929Z","iopub.status.idle":"2022-03-03T12:21:09.131295Z","shell.execute_reply.started":"2022-03-03T12:20:51.760852Z","shell.execute_reply":"2022-03-03T12:21:09.130562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"papermill":{"duration":0.081418,"end_time":"2022-02-05T13:16:53.301981","exception":false,"start_time":"2022-02-05T13:16:53.220563","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-03T12:21:09.135772Z","iopub.execute_input":"2022-03-03T12:21:09.138933Z","iopub.status.idle":"2022-03-03T12:21:09.199311Z","shell.execute_reply.started":"2022-03-03T12:21:09.138879Z","shell.execute_reply":"2022-03-03T12:21:09.198554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:21:09.20356Z","iopub.execute_input":"2022-03-03T12:21:09.206747Z","iopub.status.idle":"2022-03-03T12:21:11.189321Z","shell.execute_reply.started":"2022-03-03T12:21:09.206704Z","shell.execute_reply":"2022-03-03T12:21:11.188583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Please note we only imported the first 10,000 rows as an introductory method to compare techniques. And to share the results.\n\nAs the competition progresses futher work could include attempting different import methods. \n\nThat said, lets see how the different models faired! ","metadata":{"papermill":{"duration":0.042715,"end_time":"2022-02-05T13:16:53.387494","exception":false,"start_time":"2022-02-05T13:16:53.344779","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"color:turquoise;\n           display:fill;\n           border-radius:5px;\n           background-color:turquoise;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:turquoise;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.04384,"end_time":"2022-02-05T13:16:53.474576","exception":false,"start_time":"2022-02-05T13:16:53.430736","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Data Prep","metadata":{"papermill":{"duration":0.042655,"end_time":"2022-02-05T13:16:53.560491","exception":false,"start_time":"2022-02-05T13:16:53.517836","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Data Prep for Models\n\ndf = train\n\nscaler = StandardScaler()\nX = np.array(df.drop(['row_id', 'time_id', 'investment_id', 'target'], axis = 1))\nscaler.fit(X)\nX = scaler.transform(X)\n\ny = np.array(df['target'])\n\nprint(X.shape)\nprint(y.shape)In this section we compare different techniques performance\n\nFirst need to do some data prep for the models.","metadata":{"papermill":{"duration":0.044086,"end_time":"2022-02-05T13:16:53.647375","exception":false,"start_time":"2022-02-05T13:16:53.603289","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# # Data Prep for Models\n\n# df = train\n\n# scaler = StandardScaler()\n# X = np.array(df.drop(['row_id', 'time_id', 'investment_id', 'target'], axis = 1))\n# scaler.fit(X)\n# X = scaler.transform(X)\n\n# y = np.array(df['target'])\n\n# print(X.shape)\n# print(y.shape)","metadata":{"papermill":{"duration":1.040353,"end_time":"2022-02-05T13:16:54.73048","exception":false,"start_time":"2022-02-05T13:16:53.690127","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-03T12:21:11.191397Z","iopub.execute_input":"2022-03-03T12:21:11.191654Z","iopub.status.idle":"2022-03-03T12:21:11.195134Z","shell.execute_reply.started":"2022-03-03T12:21:11.191619Z","shell.execute_reply":"2022-03-03T12:21:11.19422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lim  = int(train.shape[0]*0.75) # the row number at which we split the train & test dataset (here we take the first 75% of the data-df)\nx_train = train.iloc[0:lim,4:] # select x_train as the first 75% of the features dataset\nx_test   = train.iloc[lim:,4:] # select x_test as the last 25% of the features dataset\ny_train = train.target.iloc[0:lim] # select y_train as the first 75% of the target dataset\ny_test   = train.target.iloc[lim:] # select y_test as the last 25% of the target dataset\n\nprint(np.shape(x_train), np.shape(x_test), np.shape(y_train), np.shape(y_test))","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:21:11.196412Z","iopub.execute_input":"2022-03-03T12:21:11.196826Z","iopub.status.idle":"2022-03-03T12:21:18.963353Z","shell.execute_reply.started":"2022-03-03T12:21:11.196791Z","shell.execute_reply":"2022-03-03T12:21:18.962591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:21:18.966258Z","iopub.execute_input":"2022-03-03T12:21:18.966458Z","iopub.status.idle":"2022-03-03T12:21:18.999119Z","shell.execute_reply.started":"2022-03-03T12:21:18.966433Z","shell.execute_reply":"2022-03-03T12:21:18.998454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:21:19.00045Z","iopub.execute_input":"2022-03-03T12:21:19.000703Z","iopub.status.idle":"2022-03-03T12:21:19.008299Z","shell.execute_reply.started":"2022-03-03T12:21:19.000669Z","shell.execute_reply":"2022-03-03T12:21:19.007468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That is the inital prep complete!","metadata":{"papermill":{"duration":0.042671,"end_time":"2022-02-05T13:16:54.816484","exception":false,"start_time":"2022-02-05T13:16:54.773813","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"\n<div style=\"color:blue;\n           display:fill;\n           border-radius:5px;\n           background-color:blue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:blue;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.044784,"end_time":"2022-02-05T13:20:59.723031","exception":false,"start_time":"2022-02-05T13:20:59.678247","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# LightGBM","metadata":{"papermill":{"duration":0.044988,"end_time":"2022-02-05T13:20:59.813115","exception":false,"start_time":"2022-02-05T13:20:59.768127","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"LightGBM is a decision tree-based gradient boosting framework that improves model efficiency while reducing memory utilisation.\n\nIt employs two innovative techniques: Gradient-based One Side Sampling and Exclusive Feature Bundling (EFB), Which address the drawbacks of the histogram-based approach employed in most GBDT (Gradient Boosting Decision Tree) frameworks. The properties of LightGBM Algorithm are formed by the two methodologies of GOSS and EFB. They work together to make the model run smoothly and give it an advantage over competing GBDT frameworks. \n\n(For futher details our recommended read is by Light GBM, linked below) \n\n\nLets try it out!ü§ó","metadata":{"papermill":{"duration":0.044777,"end_time":"2022-02-05T13:20:59.902935","exception":false,"start_time":"2022-02-05T13:20:59.858158","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**Defaults (Base Params)**","metadata":{}},{"cell_type":"code","source":"model=lgb.LGBMRegressor(random_state=0, num_threads=4)\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\nmodel.get_params()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:40:54.764894Z","iopub.execute_input":"2022-03-03T12:40:54.765438Z","iopub.status.idle":"2022-03-03T12:40:54.773094Z","shell.execute_reply.started":"2022-03-03T12:40:54.765403Z","shell.execute_reply":"2022-03-03T12:40:54.772228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train,y_train,eval_set=[(x_test,y_test),(x_train,y_train)],verbose=50 , eval_metric='logloss') ","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:41:16.491347Z","iopub.execute_input":"2022-03-03T12:41:16.491595Z","iopub.status.idle":"2022-03-03T12:46:57.910488Z","shell.execute_reply.started":"2022-03-03T12:41:16.491568Z","shell.execute_reply":"2022-03-03T12:46:57.909644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training accuracy {:.4f}'.format(model.score(x_train,y_train)))\nprint('Testing accuracy {:.4f}'.format(model.score(x_test,y_test)))","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:46:57.91222Z","iopub.execute_input":"2022-03-03T12:46:57.914888Z","iopub.status.idle":"2022-03-03T12:47:25.293557Z","shell.execute_reply.started":"2022-03-03T12:46:57.914857Z","shell.execute_reply":"2022-03-03T12:47:25.292598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb.plot_metric(model)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:47:25.294867Z","iopub.execute_input":"2022-03-03T12:47:25.295167Z","iopub.status.idle":"2022-03-03T12:47:25.556182Z","shell.execute_reply.started":"2022-03-03T12:47:25.295123Z","shell.execute_reply":"2022-03-03T12:47:25.555429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Tuning Parameters\n \n -Trees = [10, 50, 100(default), 500, 1000, 5000]\n \n -Tuning Tree Depth = 1 ....10\n \n -Learning rates = [0.0001, 0.001, 0.01, 0.1, 1.0]\n \n -Boosting types = ['gbdt', 'dart', 'goss']","metadata":{}},{"cell_type":"markdown","source":"**Number of trees / n_estimator=600**","metadata":{}},{"cell_type":"code","source":"model=lgb.LGBMRegressor(random_state=0, n_estimators=600, num_threads=4)\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\nmodel.get_params()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:06:57.502796Z","iopub.execute_input":"2022-03-03T13:06:57.503423Z","iopub.status.idle":"2022-03-03T13:06:57.511453Z","shell.execute_reply.started":"2022-03-03T13:06:57.503383Z","shell.execute_reply":"2022-03-03T13:06:57.510614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train,y_train,eval_set=[(x_test,y_test),(x_train,y_train)],verbose=50 , eval_metric='logloss') \n\n#objective='rmse', boosting_type='gbdt', num_leaves=100, n_jobs=-1, learning_rate=0.1, feature_fraction=0.8, bagging_fraction=0.8, num_threads=2) ","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:07:19.531281Z","iopub.execute_input":"2022-03-03T13:07:19.531968Z","iopub.status.idle":"2022-03-03T13:25:34.594353Z","shell.execute_reply.started":"2022-03-03T13:07:19.531922Z","shell.execute_reply":"2022-03-03T13:25:34.593653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training accuracy {:.4f}'.format(model.score(x_train,y_train)))\nprint('Testing accuracy {:.4f}'.format(model.score(x_test,y_test)))\nlgb.plot_metric(model)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:25:42.033525Z","iopub.execute_input":"2022-03-03T13:25:42.033784Z","iopub.status.idle":"2022-03-03T13:27:03.742686Z","shell.execute_reply.started":"2022-03-03T13:25:42.033758Z","shell.execute_reply":"2022-03-03T13:27:03.741039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Boosting Type = goss**","metadata":{}},{"cell_type":"code","source":"model=lgb.LGBMRegressor(random_state=0, boosting_type='goss', num_threads=4)\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\nmodel.get_params()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:32:41.675059Z","iopub.execute_input":"2022-03-03T13:32:41.67562Z","iopub.status.idle":"2022-03-03T13:32:41.683425Z","shell.execute_reply.started":"2022-03-03T13:32:41.675582Z","shell.execute_reply":"2022-03-03T13:32:41.682752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train,y_train,eval_set=[(x_test,y_test),(x_train,y_train)],verbose=50 , eval_metric='logloss') \n\n#objective='rmse', boosting_type='gbdt', num_leaves=100, n_jobs=-1, learning_rate=0.1, feature_fraction=0.8, bagging_fraction=0.8, num_threads=2) ","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:33:03.96559Z","iopub.execute_input":"2022-03-03T13:33:03.965894Z","iopub.status.idle":"2022-03-03T13:37:00.091165Z","shell.execute_reply.started":"2022-03-03T13:33:03.965863Z","shell.execute_reply":"2022-03-03T13:37:00.090459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training accuracy {:.4f}'.format(model.score(x_train,y_train)))\nprint('Testing accuracy {:.4f}'.format(model.score(x_test,y_test)))\nlgb.plot_metric(model)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:37:00.095404Z","iopub.execute_input":"2022-03-03T13:37:00.097292Z","iopub.status.idle":"2022-03-03T13:37:28.143117Z","shell.execute_reply.started":"2022-03-03T13:37:00.097255Z","shell.execute_reply":"2022-03-03T13:37:28.142377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Learning rates = 1.0**","metadata":{}},{"cell_type":"code","source":"model=lgb.LGBMRegressor(random_state=0, learning_rate=1.0, num_threads=4)\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\nmodel.get_params()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:38:39.539528Z","iopub.execute_input":"2022-03-03T13:38:39.539826Z","iopub.status.idle":"2022-03-03T13:38:39.549533Z","shell.execute_reply.started":"2022-03-03T13:38:39.539796Z","shell.execute_reply":"2022-03-03T13:38:39.548726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train,y_train,eval_set=[(x_test,y_test),(x_train,y_train)],verbose=50 , eval_metric='logloss') \n\n#objective='rmse', boosting_type='gbdt', num_leaves=100, n_jobs=-1, learning_rate=0.1, feature_fraction=0.8, bagging_fraction=0.8, num_threads=2) ","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:38:50.232594Z","iopub.execute_input":"2022-03-03T13:38:50.232874Z","iopub.status.idle":"2022-03-03T13:43:06.459388Z","shell.execute_reply.started":"2022-03-03T13:38:50.232843Z","shell.execute_reply":"2022-03-03T13:43:06.458721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training accuracy {:.4f}'.format(model.score(x_train,y_train)))\nprint('Testing accuracy {:.4f}'.format(model.score(x_test,y_test)))\nlgb.plot_metric(model)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:43:06.463799Z","iopub.execute_input":"2022-03-03T13:43:06.465703Z","iopub.status.idle":"2022-03-03T13:43:28.076238Z","shell.execute_reply.started":"2022-03-03T13:43:06.465661Z","shell.execute_reply":"2022-03-03T13:43:28.075503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Learning rates = 0.0001****","metadata":{}},{"cell_type":"code","source":"model=lgb.LGBMRegressor(random_state=0, learning_rate=0.0001, num_threads=4)\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\nmodel.get_params()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:46:12.42872Z","iopub.execute_input":"2022-03-03T13:46:12.42901Z","iopub.status.idle":"2022-03-03T13:46:12.436932Z","shell.execute_reply.started":"2022-03-03T13:46:12.428977Z","shell.execute_reply":"2022-03-03T13:46:12.436078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train,y_train,eval_set=[(x_test,y_test),(x_train,y_train)],verbose=50 , eval_metric='logloss') \n\n#objective='rmse', boosting_type='gbdt', num_leaves=100, n_jobs=-1, learning_rate=0.1, feature_fraction=0.8, bagging_fraction=0.8, num_threads=2) ","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:46:14.279654Z","iopub.execute_input":"2022-03-03T13:46:14.280011Z","iopub.status.idle":"2022-03-03T13:52:48.29403Z","shell.execute_reply.started":"2022-03-03T13:46:14.27997Z","shell.execute_reply":"2022-03-03T13:52:48.293233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training accuracy {:.4f}'.format(model.score(x_train,y_train)))\nprint('Testing accuracy {:.4f}'.format(model.score(x_test,y_test)))\nlgb.plot_metric(model)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:52:48.295761Z","iopub.execute_input":"2022-03-03T13:52:48.296025Z","iopub.status.idle":"2022-03-03T13:53:07.512636Z","shell.execute_reply.started":"2022-03-03T13:52:48.295988Z","shell.execute_reply":"2022-03-03T13:53:07.511925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Learning rates = 0.05****","metadata":{}},{"cell_type":"code","source":"model=lgb.LGBMRegressor(random_state=0, learning_rate=0.05, num_threads=4)\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\nmodel.get_params()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:55:38.325514Z","iopub.execute_input":"2022-03-03T13:55:38.325791Z","iopub.status.idle":"2022-03-03T13:55:38.335202Z","shell.execute_reply.started":"2022-03-03T13:55:38.325761Z","shell.execute_reply":"2022-03-03T13:55:38.334179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train,y_train,eval_set=[(x_test,y_test),(x_train,y_train)],verbose=50 , eval_metric='logloss') \n\n#objective='rmse', boosting_type='gbdt', num_leaves=100, n_jobs=-1, learning_rate=0.1, feature_fraction=0.8, bagging_fraction=0.8, num_threads=2) ","metadata":{"execution":{"iopub.status.busy":"2022-03-03T13:55:41.066174Z","iopub.execute_input":"2022-03-03T13:55:41.066854Z","iopub.status.idle":"2022-03-03T14:01:42.561494Z","shell.execute_reply.started":"2022-03-03T13:55:41.066816Z","shell.execute_reply":"2022-03-03T14:01:42.560751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training accuracy {:.4f}'.format(model.score(x_train,y_train)))\nprint('Testing accuracy {:.4f}'.format(model.score(x_test,y_test)))\nlgb.plot_metric(model)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:01:42.563353Z","iopub.execute_input":"2022-03-03T14:01:42.563616Z","iopub.status.idle":"2022-03-03T14:02:09.661028Z","shell.execute_reply.started":"2022-03-03T14:01:42.563579Z","shell.execute_reply":"2022-03-03T14:02:09.660325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Max depth = 20**","metadata":{}},{"cell_type":"code","source":"model=lgb.LGBMRegressor(random_state=0, max_depth=20, num_threads=4)\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\nmodel.get_params()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:02:22.275773Z","iopub.execute_input":"2022-03-03T14:02:22.276667Z","iopub.status.idle":"2022-03-03T14:02:22.289567Z","shell.execute_reply.started":"2022-03-03T14:02:22.276536Z","shell.execute_reply":"2022-03-03T14:02:22.288763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train,y_train,eval_set=[(x_test,y_test),(x_train,y_train)],verbose=50 , eval_metric='logloss') \n\n#objective='rmse', boosting_type='gbdt', num_leaves=100, n_jobs=-1, learning_rate=0.1, feature_fraction=0.8, bagging_fraction=0.8, num_threads=2) ","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:02:22.964236Z","iopub.execute_input":"2022-03-03T14:02:22.96504Z","iopub.status.idle":"2022-03-03T14:08:03.260451Z","shell.execute_reply.started":"2022-03-03T14:02:22.964995Z","shell.execute_reply":"2022-03-03T14:08:03.259724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training accuracy {:.4f}'.format(model.score(x_train,y_train)))\nprint('Testing accuracy {:.4f}'.format(model.score(x_test,y_test)))\nlgb.plot_metric(model)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:08:03.262057Z","iopub.execute_input":"2022-03-03T14:08:03.263073Z","iopub.status.idle":"2022-03-03T14:08:30.751329Z","shell.execute_reply.started":"2022-03-03T14:08:03.263032Z","shell.execute_reply":"2022-03-03T14:08:30.750576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Max bin = 2000**","metadata":{}},{"cell_type":"code","source":"model=lgb.LGBMRegressor(random_state=0, max_bin=2000, num_threads=4)\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\nmodel.get_params()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:09:23.193233Z","iopub.execute_input":"2022-03-03T14:09:23.19369Z","iopub.status.idle":"2022-03-03T14:09:23.201199Z","shell.execute_reply.started":"2022-03-03T14:09:23.193652Z","shell.execute_reply":"2022-03-03T14:09:23.200297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train,y_train,eval_set=[(x_test,y_test),(x_train,y_train)],verbose=50 , eval_metric='logloss') \n\n#objective='rmse', boosting_type='gbdt', num_leaves=100, n_jobs=-1, learning_rate=0.1, feature_fraction=0.8, bagging_fraction=0.8, num_threads=2) ","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:09:24.869707Z","iopub.execute_input":"2022-03-03T14:09:24.870447Z","iopub.status.idle":"2022-03-03T14:17:14.330645Z","shell.execute_reply.started":"2022-03-03T14:09:24.870408Z","shell.execute_reply":"2022-03-03T14:17:14.329688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training accuracy {:.4f}'.format(model.score(x_train,y_train)))\nprint('Testing accuracy {:.4f}'.format(model.score(x_test,y_test)))\nlgb.plot_metric(model)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:17:14.332668Z","iopub.execute_input":"2022-03-03T14:17:14.332952Z","iopub.status.idle":"2022-03-03T14:17:43.275716Z","shell.execute_reply.started":"2022-03-03T14:17:14.332915Z","shell.execute_reply":"2022-03-03T14:17:43.27486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=lgb.LGBMRegressor(random_state=0, objective='rmse', boosting_type='gbdt', num_leaves=100, \n                        n_jobs=-1, learning_rate=0.05, feature_fraction=0.8, num_threads=4)\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\nmodel.get_params()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:21:19.010339Z","iopub.execute_input":"2022-03-03T12:21:19.010621Z","iopub.status.idle":"2022-03-03T12:21:19.021205Z","shell.execute_reply.started":"2022-03-03T12:21:19.010573Z","shell.execute_reply":"2022-03-03T12:21:19.020235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train,y_train,eval_set=[(x_test,y_test),(x_train,y_train)],verbose=50 , eval_metric='logloss') \n\n#objective='rmse', boosting_type='gbdt', num_leaves=100, n_jobs=-1, learning_rate=0.1, feature_fraction=0.8, bagging_fraction=0.8, num_threads=2) ","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:21:19.023027Z","iopub.execute_input":"2022-03-03T12:21:19.023356Z","iopub.status.idle":"2022-03-03T12:27:51.970004Z","shell.execute_reply.started":"2022-03-03T12:21:19.023323Z","shell.execute_reply":"2022-03-03T12:27:51.969242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LGBM_confidence = model.score(x_test, y_test)\nLGBM_confidence","metadata":{"papermill":{"duration":3.666273,"end_time":"2022-02-05T13:21:03.614517","exception":false,"start_time":"2022-02-05T13:20:59.948244","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-03T12:27:51.972508Z","iopub.execute_input":"2022-03-03T12:27:51.973305Z","iopub.status.idle":"2022-03-03T12:27:59.82654Z","shell.execute_reply.started":"2022-03-03T12:27:51.973265Z","shell.execute_reply":"2022-03-03T12:27:59.825829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training accuracy {:.4f}'.format(model.score(x_train,y_train)))\nprint('Testing accuracy {:.4f}'.format(model.score(x_test,y_test)))","metadata":{"execution":{"iopub.status.busy":"2022-03-03T12:27:59.827985Z","iopub.execute_input":"2022-03-03T12:27:59.82824Z","iopub.status.idle":"2022-03-03T12:28:31.594341Z","shell.execute_reply.started":"2022-03-03T12:27:59.828205Z","shell.execute_reply":"2022-03-03T12:28:31.593593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"color:blue;\n           display:fill;\n           border-radius:5px;\n           background-color:yellow;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:yellow;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.046589,"end_time":"2022-02-05T13:21:42.698592","exception":false,"start_time":"2022-02-05T13:21:42.652003","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Fine Tuning LightGBM Hyperparameters\n","metadata":{"papermill":{"duration":0.053865,"end_time":"2022-02-05T13:21:44.397572","exception":false,"start_time":"2022-02-05T13:21:44.343707","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Lets fine tune to see if we can improve the score! üß†","metadata":{"execution":{"iopub.execute_input":"2022-02-05T12:16:48.444111Z","iopub.status.busy":"2022-02-05T12:16:48.443814Z","iopub.status.idle":"2022-02-05T12:16:48.449488Z","shell.execute_reply":"2022-02-05T12:16:48.448324Z","shell.execute_reply.started":"2022-02-05T12:16:48.444078Z"},"papermill":{"duration":0.068186,"end_time":"2022-02-05T13:21:44.522427","exception":false,"start_time":"2022-02-05T13:21:44.454241","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Tuning Number of Trees\n\nAn important hyperparameter for the LightGBM ensemble algorithm is the number of decision trees used in the ensemble.\n\nRecall that decision trees are added to the model sequentially in an effort to correct and improve upon the predictions made by prior trees. As such, more trees are often better.\n\nThe number of trees can be set via the ‚Äún_estimators‚Äù argument and defaults to 100.\n\nThe example below explores the effect of the number of trees with values between 10 to 5,000.","metadata":{"papermill":{"duration":0.053584,"end_time":"2022-02-05T13:21:44.638643","exception":false,"start_time":"2022-02-05T13:21:44.585059","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# df = train\n# scaler = StandardScaler()\n# X = np.array(df.drop(['target'], 1))\n# scaler.fit(X)\n# X = scaler.transform(X)\n# X = np.array(df.drop(['target'], 1))\n# y = np.array(df['target'])","metadata":{"papermill":{"duration":0.94849,"end_time":"2022-02-05T13:21:45.64087","exception":false,"start_time":"2022-02-05T13:21:44.69238","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-03T12:28:31.595395Z","iopub.execute_input":"2022-03-03T12:28:31.595654Z","iopub.status.idle":"2022-03-03T12:28:31.599397Z","shell.execute_reply.started":"2022-03-03T12:28:31.595601Z","shell.execute_reply":"2022-03-03T12:28:31.598745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X.shape","metadata":{"papermill":{"duration":0.063243,"end_time":"2022-02-05T13:21:45.758297","exception":false,"start_time":"2022-02-05T13:21:45.695054","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-03T12:28:31.600617Z","iopub.execute_input":"2022-03-03T12:28:31.601108Z","iopub.status.idle":"2022-03-03T12:28:31.609363Z","shell.execute_reply.started":"2022-03-03T12:28:31.601073Z","shell.execute_reply":"2022-03-03T12:28:31.608671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y.shape\n\n","metadata":{"papermill":{"duration":0.063631,"end_time":"2022-02-05T13:21:45.876581","exception":false,"start_time":"2022-02-05T13:21:45.81295","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-03T12:28:31.610739Z","iopub.execute_input":"2022-03-03T12:28:31.611074Z","iopub.status.idle":"2022-03-03T12:28:31.618867Z","shell.execute_reply.started":"2022-03-03T12:28:31.611037Z","shell.execute_reply":"2022-03-03T12:28:31.617941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# explore lightgbm boosting type effect on performance\nfrom numpy import arange\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold, cross_val_score\n\n#model=lgb.LGBMRegressor(random_state=0)\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    trees = [10, 50, 100, 200]\n    for n in trees:\n        models[str(n)] = lgb.LGBMRegressor(n_estimators=n, num_threads=4)\n    return models\n \n# evaluate a give model using cross-validation\ndef evaluate_model(model):\n    cv = KFold(n_splits=10, random_state=1)\n    scores = cross_val_score(model, x_train, y_train, cv=cv, n_jobs=-1)\n    return scores\n \n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n    \n# plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()","metadata":{"papermill":{"duration":1321.393812,"end_time":"2022-02-05T13:43:47.325841","exception":false,"start_time":"2022-02-05T13:21:45.932029","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tuning Tree Depth\n\nVarying the depth of each tree added to the ensemble is another important hyperparameter for gradient boosting.\n\nThe tree depth controls how specialized each tree is to the training dataset: how general or overfit it might be. Trees are preferred that are not too shallow and general (like AdaBoost) and not too deep and specialized (like bootstrap aggregation).\n\nGradient boosting generally performs well with trees that have a modest depth, finding a balance between skill and generality.\n\nTree depth is controlled via the ‚Äúmax_depth‚Äù argument and defaults to an unspecified value as the default mechanism for controlling how complex trees are is to use the number of leaf nodes.\n\nThere are two main ways to control tree complexity: the max depth of the trees and the maximum number of terminal nodes (leaves) in the tree. In this case, we are exploring the number of leaves so we need to increase the number of leaves to support deeper trees by setting the ‚Äúnum_leaves‚Äù argument.\n\nThe example below explores tree depths between 1 and 10 and the effect on model performance.","metadata":{"papermill":{"duration":0.057245,"end_time":"2022-02-05T13:43:47.446858","exception":false,"start_time":"2022-02-05T13:43:47.389613","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    for i in range(1,11):\n        models[str(i)] = LGBMRegressor(max_depth=i, num_leaves=2**i)\n    return models\n \n# evaluate a give model using cross-validation\ndef evaluate_model(model):\n    cv = KFold(n_splits=10, random_state=1)\n    scores = cross_val_score(model, x_train, y_train, cv=cv, n_jobs=-1)\n    return scores\n \n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n# plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()","metadata":{"papermill":{"duration":256.73287,"end_time":"2022-02-05T13:48:04.237132","exception":false,"start_time":"2022-02-05T13:43:47.504262","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tuning Learning Rate\n\nLearning rate controls the amount of contribution that each model has on the ensemble prediction.\n\nSmaller rates may require more decision trees in the ensemble.\n\nThe learning rate can be controlled via the ‚Äúlearning_rate‚Äù argument and defaults to 0.1.\n\nThe example below explores the learning rate and compares the effect of values between 0.0001 and 1.0.","metadata":{"papermill":{"duration":0.060867,"end_time":"2022-02-05T13:48:04.360385","exception":false,"start_time":"2022-02-05T13:48:04.299518","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    rates = [0.0001, 0.001, 0.01, 0.1, 1.0]\n    for r in rates:\n        key = '%.4f' % r\n        models[key] = LGBMRegressor(learning_rate=r)\n    return models\n \n# evaluate a give model using cross-validation\ndef evaluate_model(model):\n    cv = KFold(n_splits=10, random_state=1)\n    scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n    return scores\n \n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n# plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()","metadata":{"papermill":{"duration":149.485414,"end_time":"2022-02-05T13:50:33.906945","exception":false,"start_time":"2022-02-05T13:48:04.421531","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-02T04:18:41.145959Z","iopub.execute_input":"2022-03-02T04:18:41.146328Z","iopub.status.idle":"2022-03-02T04:21:05.013856Z","shell.execute_reply.started":"2022-03-02T04:18:41.14629Z","shell.execute_reply":"2022-03-02T04:21:05.01318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tuning Boosting Type\n\nA feature of LightGBM is that it supports a number of different boosting algorithms, referred to as boosting types.\n\nThe boosting type can be specified via the ‚Äúboosting_type‚Äù argument and take a string to specify the type. The options include:\n\n‚Äògbdt‚Äò: Gradient Boosting Decision Tree (GDBT).\n‚Äòdart‚Äò: Dropouts meet Multiple Additive Regression Trees (DART).\n‚Äògoss‚Äò: Gradient-based One-Side Sampling (GOSS).\nThe default is GDBT, which is the classical gradient boosting algorithm.","metadata":{"papermill":{"duration":0.063042,"end_time":"2022-02-05T13:50:34.034341","exception":false,"start_time":"2022-02-05T13:50:33.971299","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    types = ['gbdt', 'dart', 'goss']\n    for t in types:\n        models[t] = LGBMRegressor(boosting_type=t)\n    return models\n \n\ndef evaluate_model(model):\n    cv = KFold(n_splits=10, random_state=1)\n    scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n    return scores\n \n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n    \n# plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()\n","metadata":{"papermill":{"duration":85.380027,"end_time":"2022-02-05T13:51:59.479536","exception":false,"start_time":"2022-02-05T13:50:34.099509","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-02T04:21:05.01601Z","iopub.execute_input":"2022-03-02T04:21:05.016454Z","iopub.status.idle":"2022-03-02T04:22:27.967728Z","shell.execute_reply.started":"2022-03-02T04:21:05.016415Z","shell.execute_reply":"2022-03-02T04:22:27.967055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:turquoise;\n           display:fill;\n           border-radius:5px;\n           background-color:deeppink;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:turquoise;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.067659,"end_time":"2022-02-05T13:51:59.635729","exception":false,"start_time":"2022-02-05T13:51:59.56807","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Conclusion","metadata":{"papermill":{"duration":0.064446,"end_time":"2022-02-05T13:51:59.765585","exception":false,"start_time":"2022-02-05T13:51:59.701139","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Thanks for reading this notebook! We wish you the best in the competition üí•\n\nPlease give it an upvote - if you found it  useful insight into ML models  you could use. And LGBM finetuning.\n\nThanks üëç","metadata":{"papermill":{"duration":0.066089,"end_time":"2022-02-05T13:51:59.896808","exception":false,"start_time":"2022-02-05T13:51:59.830719","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"color:turquoise;\n           display:fill;\n           border-radius:5px;\n           background-color:royalblue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:turquoise;\">\n\n</p>\n</div>","metadata":{"papermill":{"duration":0.064552,"end_time":"2022-02-05T13:52:00.026067","exception":false,"start_time":"2022-02-05T13:51:59.961515","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Recommended Reads\n","metadata":{"papermill":{"duration":0.065257,"end_time":"2022-02-05T13:52:00.158203","exception":false,"start_time":"2022-02-05T13:52:00.092946","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Here are some links to reading material we found useful and would recommend: \n- https://www.geeksforgeeks.org/lightgbm-light-gradient-boosting-machine/\n- https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n- https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/\n- https://machinelearningmastery.com/light-gradient-boosted-machine-lightgbm-ensemble/\n- http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm\n- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n- https://link.springer.com/chapter/10.1007/978-1-4302-5990-9_4\n","metadata":{"papermill":{"duration":0.064761,"end_time":"2022-02-05T13:52:00.288348","exception":false,"start_time":"2022-02-05T13:52:00.223587","status":"completed"},"tags":[]}}]}