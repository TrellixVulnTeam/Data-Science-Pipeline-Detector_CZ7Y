{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Description\n    This dataset contains features derived from real historic data from thousands of investments. Your challenge is to predict the value of an obfuscated metric relevant for making trading decisions.\n\n# About the data from the competition\n\n    In this competition, we are given a large number of cross-sectional alphas and a training label which represents a transformed forward return for a given investment_id. Cross-sectional means that for a given time_id, an alpha is meant to be able to sort effectively the universe of invesments for that time_id. \"Sort effectively\" means to create a vector which is, as best as possible, directly proportionate the the target. There is quite a bit on information in the public domain which describes this style of investing in detail. The paper 101 Formulaic Alphas describes in detail how, generally, the features f_0 to f_299 are generated. The book Finding Alphas is also specificially about the style of investing in this competition. That paper and book will give you a very good sense of how the features were designed. It is our task to combine the alphas into a master alpha...also known as designing a meta model.\n\n#### train.csv\n\n1. row_id - A unique identifier for the row.\n2. time_id - The ID code for the time the data was gathered. The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set.\n\n3. investment_id - The ID code for an investment. Not all investment have data in all time IDs.\n    target - The target.\n4. [f_0:f_299] - Anonymized features generated from market data.\n   \n#### example_test.csv - Random data provided to demonstrate what shape and format of data the API will deliver to your notebook when you submit.\n\n#### example_sample_submission.csv - An example submission file provided so the publicly accessible copy of the API provides the correct data shape and format.\n\n5. ubiquant/ - The image delivery API that will serve the test set. You may need Python 3.7 and a Linux environment to run the example test set through the API offline without errors.\n\n   ","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom scipy import stats\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import *\nimport warnings\n\nimport pickle\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:31:17.274484Z","iopub.execute_input":"2022-02-17T04:31:17.274873Z","iopub.status.idle":"2022-02-17T04:31:25.180917Z","shell.execute_reply.started":"2022-02-17T04:31:17.274788Z","shell.execute_reply":"2022-02-17T04:31:25.180193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:31:25.18274Z","iopub.execute_input":"2022-02-17T04:31:25.182999Z","iopub.status.idle":"2022-02-17T04:31:40.458603Z","shell.execute_reply.started":"2022-02-17T04:31:25.182955Z","shell.execute_reply":"2022-02-17T04:31:40.457374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:31:40.459876Z","iopub.execute_input":"2022-02-17T04:31:40.461404Z","iopub.status.idle":"2022-02-17T04:31:40.468846Z","shell.execute_reply.started":"2022-02-17T04:31:40.461365Z","shell.execute_reply":"2022-02-17T04:31:40.467961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:31:40.470982Z","iopub.execute_input":"2022-02-17T04:31:40.471359Z","iopub.status.idle":"2022-02-17T04:31:40.498843Z","shell.execute_reply.started":"2022-02-17T04:31:40.471322Z","shell.execute_reply":"2022-02-17T04:31:40.49807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model DNN","metadata":{}},{"cell_type":"code","source":"investment_id = train.pop(\"investment_id\")\ninvestment_id.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:31:40.500168Z","iopub.execute_input":"2022-02-17T04:31:40.500409Z","iopub.status.idle":"2022-02-17T04:31:40.515986Z","shell.execute_reply.started":"2022-02-17T04:31:40.500376Z","shell.execute_reply":"2022-02-17T04:31:40.515219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = train.pop(\"time_id\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:31:40.517188Z","iopub.execute_input":"2022-02-17T04:31:40.517495Z","iopub.status.idle":"2022-02-17T04:31:40.529435Z","shell.execute_reply.started":"2022-02-17T04:31:40.51746Z","shell.execute_reply":"2022-02-17T04:31:40.528798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train.pop(\"target\")\ny.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:31:40.530648Z","iopub.execute_input":"2022-02-17T04:31:40.530892Z","iopub.status.idle":"2022-02-17T04:31:40.543186Z","shell.execute_reply.started":"2022-02-17T04:31:40.53086Z","shell.execute_reply":"2022-02-17T04:31:40.542229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"IntegerLookup layer for investment_id input\n\n","metadata":{}},{"cell_type":"code","source":"%%time\ninvestment_ids = list(investment_id.unique())\ninvestment_id_size = len(investment_ids) + 1\ninvestment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\ninvestment_id_lookup_layer.adapt(pd.DataFrame({\"investment_ids\":investment_ids}))","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:31:40.544675Z","iopub.execute_input":"2022-02-17T04:31:40.545266Z","iopub.status.idle":"2022-02-17T04:31:43.409512Z","shell.execute_reply.started":"2022-02-17T04:31:40.54523Z","shell.execute_reply":"2022-02-17T04:31:43.408078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making Tesorflow dataset\nimport tensorflow as tf\ndef preprocess(X, y):\n    return X, y\ndef make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(4096)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:31:43.411099Z","iopub.execute_input":"2022-02-17T04:31:43.411372Z","iopub.status.idle":"2022-02-17T04:31:43.417553Z","shell.execute_reply.started":"2022-02-17T04:31:43.411331Z","shell.execute_reply":"2022-02-17T04:31:43.416696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n\n\n\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:31:43.420528Z","iopub.execute_input":"2022-02-17T04:31:43.421073Z","iopub.status.idle":"2022-02-17T04:31:43.43447Z","shell.execute_reply.started":"2022-02-17T04:31:43.42101Z","shell.execute_reply":"2022-02-17T04:31:43.433519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model()\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:31:43.435618Z","iopub.execute_input":"2022-02-17T04:31:43.436066Z","iopub.status.idle":"2022-02-17T04:31:44.522897Z","shell.execute_reply.started":"2022-02-17T04:31:43.435949Z","shell.execute_reply":"2022-02-17T04:31:44.52205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(5, shuffle=True, random_state=42)\nmodels = []\nfor index, (train_indices, valid_indices) in enumerate(kfold.split(train, investment_id)):\n    X_train, X_val = train.iloc[train_indices], train.iloc[valid_indices]\n    investment_id_train = investment_id[train_indices]\n    y_train, y_val = y.iloc[train_indices], y.iloc[valid_indices]\n    investment_id_val = investment_id[valid_indices]\n    train_ds = make_dataset(X_train, investment_id_train, y_train)\n    valid_ds = make_dataset(X_val, investment_id_val, y_val, mode=\"valid\")\n    model = get_model()\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{index}\", save_best_only=True)\n    early_stop = keras.callbacks.EarlyStopping(patience=10)\n    history = model.fit(train_ds, epochs=50, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n    model = keras.models.load_model(f\"model_{index}\")\n    models.append(model)\n    \n    pearson_score = stats.pearsonr(model.predict(valid_ds).ravel(), y_val.values)[0]\n    print('Pearson:', pearson_score)\n    pd.DataFrame(history.history, columns=[\"mse\", \"val_mse\"]).plot()\n    plt.title(\"MSE\")\n    plt.show()\n    pd.DataFrame(history.history, columns=[\"mae\", \"val_mae\"]).plot()\n    plt.title(\"MAE\")\n    plt.show()\n    pd.DataFrame(history.history, columns=[\"rmse\", \"val_rmse\"]).plot()\n    plt.title(\"RMSE\")\n    plt.show()\n    del investment_id_train\n    del investment_id_val\n    del X_train\n    del X_val\n    del y_train\n    del y_val\n    del train_ds\n    del valid_ds\n    gc.collect()\n    break","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:31:44.524274Z","iopub.execute_input":"2022-02-17T04:31:44.524522Z","iopub.status.idle":"2022-02-17T04:37:12.591297Z","shell.execute_reply.started":"2022-02-17T04:31:44.524488Z","shell.execute_reply":"2022-02-17T04:37:12.590457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:37:12.593667Z","iopub.execute_input":"2022-02-17T04:37:12.594195Z","iopub.status.idle":"2022-02-17T04:37:12.603012Z","shell.execute_reply.started":"2022-02-17T04:37:12.594143Z","shell.execute_reply":"2022-02-17T04:37:12.601976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfor (test_df, sample_prediction_df) in iter_test:\n    ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n    sample_prediction_df['target'] = inference(models, ds)\n    env.predict(sample_prediction_df) ","metadata":{"execution":{"iopub.status.busy":"2022-02-17T04:37:12.604439Z","iopub.execute_input":"2022-02-17T04:37:12.605602Z","iopub.status.idle":"2022-02-17T04:37:13.205503Z","shell.execute_reply.started":"2022-02-17T04:37:12.605564Z","shell.execute_reply":"2022-02-17T04:37:13.204602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}