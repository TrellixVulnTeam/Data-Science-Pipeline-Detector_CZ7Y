{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hi, Welcome.\n\n**Today, we will examine the Ubiquant Market Prediction dataset.**\n\n**I will explain feature values and make visualizations. In this notebook, I will use the highly efficient parquet data format.**","metadata":{}},{"cell_type":"markdown","source":"# We Import relevant libraries here\n\nWhich are useful.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\nimport plotly.express as px\nimport random\nfrom sklearn.linear_model import LinearRegression\nfrom scipy.stats import pearsonr\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-28T10:14:13.743353Z","iopub.execute_input":"2022-03-28T10:14:13.744173Z","iopub.status.idle":"2022-03-28T10:14:16.724127Z","shell.execute_reply.started":"2022-03-28T10:14:13.744068Z","shell.execute_reply":"2022-03-28T10:14:16.72306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" **I had imported the dataset in parquet format, you can check this [notebook](https://www.kaggle.com/code/robikscube/fast-data-loading-and-low-mem-with-parquet-files) for details. Its main purpose is to reduce the file size, memory usage and prevent crashes due to massive file size.**","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_df = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')\ntrain_df.memory_usage().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:14:16.730292Z","iopub.execute_input":"2022-03-28T10:14:16.730608Z","iopub.status.idle":"2022-03-28T10:14:52.35006Z","shell.execute_reply.started":"2022-03-28T10:14:16.730559Z","shell.execute_reply":"2022-03-28T10:14:52.349345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:14:52.351412Z","iopub.execute_input":"2022-03-28T10:14:52.351884Z","iopub.status.idle":"2022-03-28T10:14:52.386433Z","shell.execute_reply.started":"2022-03-28T10:14:52.351846Z","shell.execute_reply":"2022-03-28T10:14:52.385485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's check our features**","metadata":{}},{"cell_type":"code","source":"for idx, i in enumerate(train_df.columns):\n    idx += 1\n    seq = \"th\"\n    if idx <= 5: # for first 5 rows\n        if idx == 1: \n            seq = \"st\"\n        elif idx == 2:\n            seq = \"nd\"\n        elif idx == 3:\n            seq = \"rd\"\n        else:\n            seq=\"th\"\n        print(f'{idx}{seq} column is {i}')\n    elif idx >= (train_df.columns.shape[0] - 4): # for last 5 rows\n        print(f'{idx}{seq} column is {i}')\n    elif idx % 20 == 0: # print every 20th column for less confusion.\n        print(f'{idx}{seq} column is {i}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-28T10:14:52.38946Z","iopub.execute_input":"2022-03-28T10:14:52.390018Z","iopub.status.idle":"2022-03-28T10:14:52.403361Z","shell.execute_reply.started":"2022-03-28T10:14:52.389969Z","shell.execute_reply":"2022-03-28T10:14:52.40227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Apparently, we have 304 different columns.**\n\n**First four columns are with respect to:**\n\n**row_id** = A unique identifier for every single row.\n\n**time_id** = The ID code for the time the data was gathered. The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set.\n\n**investment_id** = The ID code for an investment. Not all investment have data in all time IDs.\n\n**target** = The target value.\n\n**f_0 - f_299** = Anonymized features generated from market data. (my guess is they are particular financial calculations, to find the correlations.)","metadata":{}},{"cell_type":"markdown","source":"# Check how many observations we have, to understand the size of dataset intuitively.","metadata":{}},{"cell_type":"code","source":"total_obs = train_df.shape[0]\nprint(f\"WOW! There are {total_obs} observations exist.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-28T10:14:52.405284Z","iopub.execute_input":"2022-03-28T10:14:52.405852Z","iopub.status.idle":"2022-03-28T10:14:52.41406Z","shell.execute_reply.started":"2022-03-28T10:14:52.405804Z","shell.execute_reply":"2022-03-28T10:14:52.412958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(train_df, x=\"target\", nbins=50)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-28T10:14:52.415366Z","iopub.execute_input":"2022-03-28T10:14:52.415776Z","iopub.status.idle":"2022-03-28T10:15:08.872593Z","shell.execute_reply.started":"2022-03-28T10:14:52.415718Z","shell.execute_reply":"2022-03-28T10:15:08.871372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Our target values look normal, It is roughly between -2.5 and 3 values. Highest population is in -0.5 and 0 interval.**\n\n**Now, let's inspect time_id and investment id**","metadata":{}},{"cell_type":"code","source":"tmp = train_df.groupby(\"time_id\").investment_id.nunique()\nfig = px.scatter(x=tmp.index, y=tmp.values, labels={'x':'time_id', 'y':'investment_id'})#, title=\"time_id and investment_id\")\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-28T10:15:08.873719Z","iopub.execute_input":"2022-03-28T10:15:08.874018Z","iopub.status.idle":"2022-03-28T10:15:09.532525Z","shell.execute_reply.started":"2022-03-28T10:15:08.873984Z","shell.execute_reply":"2022-03-28T10:15:09.531838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There was a break in the area roughly corresponding to time_id 400, except that one we don't have remarkable outliers.**\n","metadata":{}},{"cell_type":"markdown","source":"# SUBMISSION API?\n\n****\n**Now, I will create a very simple model to show how to submit.**\n\n**Submitting via API is not a common concept, it took me a while to understand.**","metadata":{}},{"cell_type":"markdown","source":"That's what host says:\n\n> Submissions are evaluated on the mean of the Pearson correlation coefficient for each time ID.\n\n**What is Pearson Correlation coefficient?**\n\nPearson Correlation coefficient is simply a measure of linear correlation between two sets of data.\n\n**Formula:**\n\n![](https://www.gstatic.com/education/formulas2/397133473/en/correlation_coefficient_formula.svg)\n\n* r = correlation coefficient\n\n* xᵢ = values of the x-variable in a sample\n\n* x̄ = mean of the values of the x variable\n\n* yᵢ = values of the y-variable in a sample\n\n* ȳ = mean of the values of the y variable\n\n> definition from [statisticshowto.com](https://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/)\n\nFor detailed [definition](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient).","metadata":{}},{"cell_type":"markdown","source":"**In order to submit our predicts, we must do following steps:**\n\n![](https://cdn.discordapp.com/attachments/928151364524183565/957917677836460032/unknown.png)\n\n# Let's create our model now!\n\n**I will be using Linear Regression for sake of simplicity.** For information you can check [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n\n**First of all, let's reduce the row size and columns of our example.**","metadata":{}},{"cell_type":"code","source":"# Train\nfeatures = [f'f_{i}' for i in range(300)]\n\nX = train_df[features]\ny = train_df['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nX_test","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:15:09.533836Z","iopub.execute_input":"2022-03-28T10:15:09.534249Z","iopub.status.idle":"2022-03-28T10:15:24.221126Z","shell.execute_reply.started":"2022-03-28T10:15:09.534197Z","shell.execute_reply":"2022-03-28T10:15:24.220037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num = 100\ncon = 500000\nfeatures = [f'f_{i}' for i in range(num)]\n\nreduced_X_train = train_df[features][:con]\nreduced_y_train = train_df['target'][:con]","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:15:24.222477Z","iopub.execute_input":"2022-03-28T10:15:24.222723Z","iopub.status.idle":"2022-03-28T10:15:25.713369Z","shell.execute_reply.started":"2022-03-28T10:15:24.222693Z","shell.execute_reply":"2022-03-28T10:15:25.712211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# linear regression model\nreg = LinearRegression().fit(reduced_X_train, reduced_y_train)\n# our predict function\ndef predict(df):\n    predict = df[reduced_X_train.columns]\n    prediction = reg.predict(predict)\n    return prediction","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:15:59.038605Z","iopub.execute_input":"2022-03-28T10:15:59.038912Z","iopub.status.idle":"2022-03-28T10:16:00.808106Z","shell.execute_reply.started":"2022-03-28T10:15:59.038882Z","shell.execute_reply":"2022-03-28T10:16:00.806664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = predict(X_test)\nprint(\"Result:\", pearsonr(y_test, res)[0]) # pearson correlation result","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:16:02.226116Z","iopub.execute_input":"2022-03-28T10:16:02.227131Z","iopub.status.idle":"2022-03-28T10:16:02.615483Z","shell.execute_reply.started":"2022-03-28T10:16:02.22707Z","shell.execute_reply":"2022-03-28T10:16:02.61248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant # host's lib\nenv = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\nfor (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df['target'] = predict(test_df)  # make your predictions here\n    env.predict(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T10:16:04.114065Z","iopub.execute_input":"2022-03-28T10:16:04.114383Z","iopub.status.idle":"2022-03-28T10:16:04.222329Z","shell.execute_reply.started":"2022-03-28T10:16:04.114348Z","shell.execute_reply":"2022-03-28T10:16:04.221504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n**Thanks for reading my notebook.**\n\n**Feel free to ask in comments section.**\n\n**Upvotes are all appreciated.**","metadata":{}}]}