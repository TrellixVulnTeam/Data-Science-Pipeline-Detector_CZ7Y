{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# UMP Pytorch ResNet Training & Inference\n\nTreat the challenge as a tabular data problem and use the [rtdl](https://github.com/Yura52/rtdl) package to implement the ResNet model described in the paper [Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko, \"Revisiting Deep Learning Models for Tabular Data‚Äù, 2021\"](https://github.com/Yura52/tabular-dl-revisiting-models).\n\nThe preprocessing is taken from [columbia2131](https://www.kaggle.com/columbia2131)'s [Speed Up Reading (csv-to-pickle)](https://www.kaggle.com/code/columbia2131/speed-up-reading-csv-to-pickle/notebook) and [Takamichi Toda](https://www.kaggle.com/takamichitoda)'s [UMP Train Transformer on TPU](https://www.kaggle.com/code/takamichitoda/ump-train-transformer-on-tpu/) and [UMP train.csv to npy](https://www.kaggle.com/takamichitoda/ump-train-csv-to-npy).\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"lr = 1e-3 #1e-4 #0.001\nweight_decay = 0.0001\nbatch_size = 1024*6\n\nclass CFG:\n    CHECKPOINT = 'resnet_chkpt_20220418a'\n    TRAIN = False\n    INFER = True\n    SPLITS = 7\n    D_IN = 300\n    D_MAIN = 256\n    D_HIDDEN = 256\n    DROPOUT_FIRST = 0.4\n    DROPOUT_SECOND = 0.4\n    N_BLOCKS = 8\n    D_OUT = 1\n\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport numpy as np\nimport pickle\nimport random\nimport sklearn.datasets\nimport sklearn.metrics\nimport sklearn.model_selection\nimport torch\nimport torch.nn.functional as F\n\nfrom scipy.stats import pearsonr\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\nimport ubiquant\nimport time\nimport datetime\nimport logging.handlers\nfrom torch.utils.tensorboard import SummaryWriter\n\nif os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost') != 'Localhost':\n    # !pip install rtdl\n    !pip install ../input/transwork003data/rtdl-0.0.13-py3-none-any.whl\nimport rtdl\nif os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost') == 'Localhost':\n    import zero\n    zero.hardware.free_memory()\n\ndevice = torch.device('cuda')\nloss_fn = F.mse_loss\nprint(f'Num GPUs Available: {torch.cuda.device_count()}')\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Based on code in the book Deep Learning with PyTorch by Eli Stevens,\n# Luca Antiga, and Thomas Viehmann, published by Manning Publications.\n# https://www.manning.com/books/deep-learning-with-pytorch\ndef enumerateWithEstimate(_iter, desc_str, print_ndx=4,):\n    iter_len = len(_iter)\n    backoff = 2\n    while backoff ** 7 < iter_len:\n        backoff *= 2\n\n    log.warning(\"{} ----/{}, starting\".format(\n        desc_str, iter_len,\n    ))\n    start_ts = time.time()\n    for (current_ndx, item) in enumerate(_iter):\n        yield (current_ndx, item)\n        if current_ndx == print_ndx:\n            duration_sec = ((time.time() - start_ts)\n                            / (current_ndx - 1) * (iter_len)\n                            )\n            done_dt = datetime.datetime.fromtimestamp(start_ts + duration_sec)\n            done_td = datetime.timedelta(seconds=duration_sec)\n            log.info(\"{} {:-4}/{}, done at {}, {}\".format(\n                desc_str,\n                current_ndx,\n                iter_len,\n                str(done_dt).rsplit('.', 1)[0],\n                str(done_td).rsplit('.', 1)[0],\n            ))\n            print_ndx *= backoff\n\n        if current_ndx + 1 == 0:\n            start_ts = time.time()\n\n    log.warning(\"{} ----/{}, done at {}\".format(\n        desc_str, iter_len,\n        str(datetime.datetime.now()).rsplit('.', 1)[0],\n    ))\n\nfeatures = [f'f_{i}' for i in range(300)]\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n# https://www.kaggle.com/competitions/ubiquant-market-prediction/discussion/302480\n\ndef apply_model(model, device, x_num):\n    if isinstance(model, rtdl.FTTransformer):\n        return model(x_num.to(device), None)\n    elif isinstance(model, (rtdl.MLP, rtdl.ResNet)):\n        return model(x_num.to(device))\n    else:\n        raise NotImplementedError(\n            f'{type(model)} not implemented.'\n        )\n\n@torch.no_grad()\ndef evaluate(model, part):\n    model.eval()\n    prediction = []\n    for batch in zero.iter_batches(X[part], batch_size):\n        prediction.append(apply_model(model, device, batch))\n    prediction = torch.cat(prediction).squeeze(1).cpu().numpy()\n    target = y[part].cpu().numpy()\n    score = sklearn.metrics.mean_squared_error(target, prediction) ** 0.5 * 1.0\n    return score\n\n@torch.no_grad()\ndef evaluate_pearson(model, part):\n    model.eval()\n    prediction = []\n    for batch in zero.iter_batches(X[part], batch_size):\n        prediction.append(apply_model(model, device, batch))\n    prediction = torch.cat(prediction).squeeze(1).cpu().numpy()\n    target = y[part].cpu().numpy()\n    score = pearsonr(target, prediction)[0]\n    return score\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/columbia2131/speed-up-reading-csv-to-pickle\n\nif CFG.TRAIN and (not os.path.isfile('train.pkl')):\n    def transform_csv2pickle(path, usecols, dtypes):\n        train = pd.read_csv(\n            path,\n            usecols=usecols,\n            dtype=dtypes\n        )\n        train.to_pickle('train.pkl')\n\n    path = '../input/ubiquant-market-prediction/train.csv'\n    basecols = ['row_id', 'time_id', 'investment_id', 'target']\n    dtypes = {\n        'row_id': 'str',\n        'time_id': 'uint16',\n        'investment_id': 'uint16',\n        'target': 'float32',\n    }\n    for col in features:\n        dtypes[col] = 'float32'\n\n    transform_csv2pickle(path, basecols+features, dtypes)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/takamichitoda/ump-train-csv-to-npy\n\nif CFG.TRAIN and (\\\n        (not os.path.isfile('targets.npy')) or \\\n        (not os.path.isfile('time_id.npy')) or \\\n        (not os.path.isfile('investment_id.npy')) or \\\n        (not os.path.isfile('std_scaler.pkl')) or \\\n        (not os.path.isfile('robust_scaler.pkl')) or\n        (not os.path.isfile('quantile_transformer.pkl')) or\n        (not os.path.isfile('features_std_scaled.npy')) or\n        (not os.path.isfile('features_robust_scaled.npy')) or\n        (not os.path.isfile('features_quantile_transformer.npy'))):\n    train_df = pd.read_pickle('train.pkl')\n\n    y = train_df['target'].values\n    time_id = train_df['time_id'].values\n    investment_id = train_df['investment_id'].values\n    del train_df['row_id'], train_df['time_id'], train_df['investment_id'], train_df['target']\n    gc.collect()\n\n    np.save('targets.npy', y)\n    np.save('time_id.npy', time_id)\n    np.save('investment_id.npy', investment_id)\n\n    X = train_df[features].values\n\n    del train_df\n    gc.collect()\n\n    std_scaler = StandardScaler()\n    _X = std_scaler.fit_transform(X)\n    pickle.dump(std_scaler, open(\"std_scaler.pkl\", \"wb\"))\n    np.save('features_std_scaled.npy', _X)\n\n    robust_scaler = RobustScaler()\n    _X = robust_scaler.fit_transform(X)\n    pickle.dump(robust_scaler, open(\"robust_scaler.pkl\", \"wb\"))\n    np.save('features_robust_scaled.npy', _X)\n\n    quantile_transformer = QuantileTransformer(\n            output_distribution='normal',\n            n_quantiles=max(min(X.shape[0] // 30, 1000), 10), # n_quantiles=100, # 1000\n            subsample=1e9,\n            random_state=42,\n        )\n    _X = quantile_transformer.fit_transform(X)\n    pickle.dump(quantile_transformer, open(\"quantile_transformer.pkl\", \"wb\"))\n    np.save('features_quantile_transformer.npy', _X)\n\n    del std_scaler, robust_scaler, quantile_transformer, _X\n    gc.collect()\n\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.TRAIN:\n    X_all = np.load('features_std_scaled.npy')\n    # X_all = np.load('features_robust_scaled.npy')\n    # X_all = np.load('features_quantile_transformer.npy')\n    investment_id_all = np.load('investment_id.npy')\n    y_all = np.load('targets.npy')\n    time_id_all = np.load('time_id.npy')\n    zero.hardware.free_memory()\n    seed_everything(42)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.TRAIN:\n    root_logger = logging.getLogger()\n    root_logger.setLevel(logging.INFO)\n    for handler in list(root_logger.handlers):\n        root_logger.removeHandler(handler)\n    logfmt_str = \"%(asctime)s %(levelname)-8s pid:%(process)d %(name)s:%(lineno)03d:%(funcName)s %(message)s\"\n    formatter = logging.Formatter(logfmt_str)\n    streamHandler = logging.StreamHandler()\n    streamHandler.setFormatter(formatter)\n    streamHandler.setLevel(logging.DEBUG)\n    fileHandler = logging.FileHandler(\"log.txt\")\n    fileHandler.setLevel(logging.DEBUG)\n    root_logger.addHandler(streamHandler)\n    root_logger.addHandler(fileHandler)\n    log = logging.getLogger(__name__)\n    # Docs: https://yura52.github.io/zero/0.0.4/reference/api/zero.improve_reproducibility.html\n    zero.improve_reproducibility(seed=123456)\n    kfold = sklearn.model_selection.GroupKFold(n_splits=CFG.SPLITS)\n\n    fold_scores = []\n    models = []\n\n    X = {}\n    y = {}\n    for fold, (trn_idx, val_idx) in enumerate(kfold.split(X_all, y_all, groups=investment_id_all)):\n        X['train'], y['train'] = torch.from_numpy(X_all[trn_idx]).clone(), torch.from_numpy(y_all[trn_idx]).clone()\n        X['val'], y['val'] = torch.from_numpy(X_all[val_idx]).clone(), torch.from_numpy(y_all[val_idx]).clone()\n        model = rtdl.ResNet.make_baseline(\n            d_in=CFG.D_IN,\n            d_main=CFG.D_MAIN,\n            d_hidden=CFG.D_HIDDEN,\n            dropout_first=CFG.DROPOUT_FIRST,\n            dropout_second=CFG.DROPOUT_SECOND,\n            n_blocks=CFG.N_BLOCKS,\n            d_out=CFG.D_OUT,\n        )\n        model.to(device)\n        optimizer = (\n            model.make_default_optimizer()\n            if isinstance(model, rtdl.FTTransformer)\n            else torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n        )\n        # Create a dataloader for batches of indices\n        # Docs: https://yura52.github.io/zero/reference/api/zero.data.IndexLoader.html\n        # train_loader = zero.data.IndexLoader(len(X['train']), batch_size, device=device)\n        train_loader = zero.data.IndexLoader(len(X['train']), batch_size, device='cpu')\n\n        # Create a progress tracker for early stopping\n        # Docs: https://yura52.github.io/zero/reference/api/zero.ProgressTracker.html\n        # progress = zero.ProgressTracker(patience=100)\n        progress = zero.ProgressTracker(patience=30)\n        # print(f'Test score before training: {evaluate(model, \"val\"):.4f}')\n\n        writer = SummaryWriter(flush_secs=30)\n        checkpoint_file = CFG.CHECKPOINT + '_fold' + str(fold) + '.pt'\n        zero.hardware.free_memory()\n        n_epochs = 1000\n        report_frequency = len(X['train']) // batch_size\n        best_epoch = 1\n        for epoch in range(1, n_epochs + 1):\n            # def enumerateWithEstimate(_iter, desc_str, print_ndx=4,):\n            # for iteration, batch_idx in enumerate(train_loader):\n            for iteration, batch_idx in enumerateWithEstimate(train_loader, 'train'):\n                model.train()\n                optimizer.zero_grad()\n                x_batch = X['train'][batch_idx]\n                y_batch = y['train'][batch_idx]\n                loss = loss_fn(apply_model(model, device, x_batch).squeeze(1), y_batch.to(device))\n                loss.backward()\n                optimizer.step()\n            writer.add_scalar(\"train_loss\", loss.item(), epoch)\n\n            val_score = evaluate(model, 'val')\n            val_pearson = evaluate_pearson(model, 'val')\n            print(f'Epoch {epoch:03d} | val_score: {val_score:.4f} | val_pearson: {val_pearson:.4f}', end='')\n            writer.add_scalar(\"val_score\", val_score, epoch)\n            writer.add_scalar(\"pearsonr\", val_pearson, epoch)\n            progress.update(-1 * val_score)\n            # progress.update(val_pearson)\n            if progress.success:\n                best_epoch = epoch\n                print(' <<< BEST VALIDATION EPOCH', end='')\n                torch.save(\n                    { 'model': model.state_dict(),\n                      'optimizer': optimizer.state_dict(),\n                      'random_state': zero.random.get_state(),\n                      },\n                    checkpoint_file,\n                )\n            print()\n            if progress.fail:\n                models.append(model)\n                print(f\"best epoch is {best_epoch}\")\n                break\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.INFER:\n    env = ubiquant.make_env()   # initialize the environment\n    iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\n\n    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost') != 'Localhost':\n        DATA_PATH = os.path.join('..', 'input', 'transwork003data')\n    else:\n        DATA_PATH = os.path.join('.')\n    normalizer = pickle.load(open(os.path.join(DATA_PATH, 'std_scaler.pkl'), 'rb'))\n    # normalizer = pickle.load(open(os.path.join(DATA_PATH, 'quantile_transformer.pkl'), 'rb'))\n    models = []\n    for fold in range(CFG.SPLITS):\n        model = rtdl.ResNet.make_baseline(\n            d_in=CFG.D_IN,\n            d_main=CFG.D_MAIN,\n            d_hidden=CFG.D_HIDDEN,\n            dropout_first=CFG.DROPOUT_FIRST,\n            dropout_second=CFG.DROPOUT_SECOND,\n            n_blocks=CFG.N_BLOCKS,\n            d_out=CFG.D_OUT,\n        )\n        # batch_size = 128 if isinstance(model, rtdl.FTTransformer) else 1024*6\n        model.to(device)\n        checkpoint_file = CFG.CHECKPOINT + '_fold' + str(fold) + '.pt'\n        checkpoint = torch.load(os.path.join(DATA_PATH, checkpoint_file))\n        model.load_state_dict(checkpoint['model'])\n        device = torch.device('cuda')\n        model.to(device)\n        model.eval()\n        models.append(model)\n    print('Resuming from the checkpoint.\\n')\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.INFER:\n    for (test_df, sample_prediction_df) in iter_test:\n        x = normalizer.transform(test_df[features].values)\n        preds = None\n        ####\n        with torch.no_grad():\n            _x = torch.from_numpy(x.astype(np.float32)).clone().to(device)\n            for model in models:\n                if preds is None:\n                    preds = apply_model(model, device, _x).reshape(-1, 1).cpu().numpy()\n                else:\n                    preds += apply_model(model, device, _x).reshape(-1, 1).cpu().numpy()\n        pred = preds / CFG.SPLITS\n        sample_prediction_df['target'] = pred  # make your predictions here\n        env.predict(sample_prediction_df)   # register your predictions","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]}]}