{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-27T05:15:42.427999Z","iopub.execute_input":"2022-02-27T05:15:42.428999Z","iopub.status.idle":"2022-02-27T05:15:42.439514Z","shell.execute_reply.started":"2022-02-27T05:15:42.428953Z","shell.execute_reply":"2022-02-27T05:15:42.438469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This notebook demonstrates the simple approach that trains LightGBM model that uses pearson correlation as a customized evaluation metric in hyperparameter tuning.\n\nThe notebook also shows data pre-processing and EDA to provide high-level understanding of target and feature distributions as well as their pairwise correlations.\n\nSome of works are inspired by popular notebooks in the competition.\n\nEDA:\n\n- https://www.kaggle.com/ilialar/ubiquant-eda-and-baseline\n- https://www.kaggle.com/lucamassaron/eda-target-analysis#Target-analysis\n- https://www.kaggle.com/marketneutral/ubiquant-feature-exploration#Thinking-(and-Trading)-Fast-and-Slow","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n\nimport plotly.express as px\n\nimport lightgbm as lgb\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.base import clone\nfrom sklearn.metrics import (\n    roc_auc_score,\n    mean_squared_error\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T05:15:45.435617Z","iopub.execute_input":"2022-02-27T05:15:45.436412Z","iopub.status.idle":"2022-02-27T05:15:45.442799Z","shell.execute_reply.started":"2022-02-27T05:15:45.436373Z","shell.execute_reply":"2022-02-27T05:15:45.441767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_types_dict = {\n    'time_id': 'int32',\n    'investment_id': 'int16',\n    \"target\": 'float16',\n}\n\nfeatures = [f'f_{i}' for i in range(300)]\n\nfor f in features:\n    data_types_dict[f] = 'float16'\n    \ntarget = 'target'\n\ndef load_data(data_folder, file_name, data_types_dict):\n    return pd.read_csv(data_folder + '/' + file_name + '.csv'\n                       , usecols = data_types_dict.keys()\n                       , dtype=data_types_dict\n                       , index_col = 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T04:58:59.516582Z","iopub.execute_input":"2022-02-27T04:58:59.51687Z","iopub.status.idle":"2022-02-27T04:58:59.524211Z","shell.execute_reply.started":"2022-02-27T04:58:59.516838Z","shell.execute_reply":"2022-02-27T04:58:59.523138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = load_data('/kaggle/input/ubiquant-market-prediction', 'train', data_types_dict)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA of feature and target","metadata":{}},{"cell_type":"code","source":"# Overall Standard deviation\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nstd_target = train_data.groupby(['investment_id'])['target'].std()\nstd_target.plot.hist(bins=60)\nplt.title(\"standard deviation of target distribution\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.subplot(3, 1, 1)\ntrain_data.groupby(['time_id'])['target'].mean().plot()\nplt.axhline(y= np.mean(train_data.groupby(['time_id'])['target'].mean()), color='r', linestyle='--', label=\"mean\")\nplt.title(\"Mean of target by time\")\nplt.show()\n\nplt.subplot(3, 1, 2)\ntrain_data.groupby(['time_id'])['target'].std().plot()\nplt.axhline(y= np.mean(train_data.groupby(['time_id'])['target'].std()), color='r', linestyle='--', label=\"mean\")\nplt.title(\"STD of target by time\")\nplt.show()\n\nplt.subplot(3, 1, 2)\ntrain_data.groupby(['time_id'])['investment_id'].nunique().plot()\nplt.title(\"Number of investment_id by time\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_target_by_time = train_data.groupby(['time_id'])['target'].mean()\nstd_target_by_time = train_data.groupby(['time_id'])['target'].std()\n\ntrain_data['target_from_mean'] = train_data['target'] - train_data.groupby(['time_id'])['target'].transform(np.mean)\ntrain_data['std_target_at_time'] = train_data.groupby(['time_id'])['target'].transform(np.std)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nplt.plot(mean_target_by_time.index, mean_target_by_time, \"o-\", color=\"r\")\nplt.fill_between(\n        mean_target_by_time.index,\n        mean_target_by_time - std_target_by_time,\n        mean_target_by_time + std_target_by_time,\n        alpha=0.2,\n        color=\"r\")\nplt.axhline(y= np.mean(mean_target_by_time), color='g', linestyle='--', label=\"mean\")\nax.set_ylabel(\"target\")\nax.set_xlabel(\"time\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['abs_target_from_mean'] = np.abs(train_data['target_from_mean'])\ntrain_data['abs_z_score'] = np.abs(train_data['target_from_mean'])/train_data['std_target_at_time']\n\ntrain_data.groupby(['investment_id']).agg({\n    'abs_target_from_mean': ['mean', 'std']\n    , 'abs_z_score': ['mean', 'std']\n}).reset_index()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Time-series autocorrelation","metadata":{}},{"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pacf(mean_target_by_time,lags=25,title=\"Partial Autocorrelation chart: (Mean Target)\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pacf(std_target_by_time,lags=25,title=\"Partial Autocorrelation chart: (Std Target)\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm wondering if temporal diffence of historical targets correlates with the target","metadata":{}},{"cell_type":"code","source":"train_data['target_lag1'] = train_data.sort_values(by=['time_id'], ascending=True)\\\n.groupby(['investment_id'])['target'].shift(1)\n\ntrain_data['target_lag2'] = train_data.sort_values(by=['time_id'], ascending=True)\\\n.groupby(['investment_id'])['target'].shift(2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['diff_target_lag1'] = train_data['target'] - \\\nnp.where(train_data['target_lag1'].isna(), 0,  train_data['target_lag1'])\n\ntrain_data['diff_target_lag1_2'] = np.where(train_data['target_lag1'].isna(), 0,  train_data['target_lag1']) - \\\nnp.where(train_data['target_lag2'].isna(), 0,  train_data['target_lag2'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.corrcoef(train_data['diff_target_lag1_2'], train_data['target'])[0][1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.corrcoef(train_data['diff_target_lag1'], train_data['diff_target_lag1_2'])[0][1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As correlation between $target_{t-1} - target_{t-2}$ and $target$ is low, this temporal difference feature in target doesn't appear to be a great predictor.","metadata":{}},{"cell_type":"markdown","source":"# Correlation between features and target","metadata":{}},{"cell_type":"code","source":"# Correlation between the target and each feature by time_id\ncorr_lists = list()\nfor feature in features:\n    corr_lists.append(train_data.groupby('time_id')[['target', feature]].corr().unstack().iloc[:,1])\n\ncorr_data = np.stack(corr_lists, axis = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_dataframe = pd.DataFrame(corr_data, columns = features).set_axis(np.unique(train_data.index), axis='index')\ncorr_dataframe.index.name = 'time_id'\ncorr_dataframe","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.subplot(3, 1, 1)\nplt.axhline(y= np.mean(corr_dataframe['f_164']), color='r', linestyle='--', label=\"mean\")\nplt.plot(corr_dataframe.index, corr_dataframe['f_164'])\nplt.title(\"Correaltion of between target and f_164 by time\")\nplt.show()\n\nplt.subplot(3, 1, 2)\nplt.axhline(y= np.mean(corr_dataframe['f_7']), color='r', linestyle='--', label=\"mean\")\nplt.plot(corr_dataframe.index, corr_dataframe['f_180'])\nplt.title(\"Correaltion of between target and f_180 by time\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Standard deviation of correlations over time depends on the number of investments","metadata":{}},{"cell_type":"code","source":"def top_correlated_features(data, features, target):\n    corrs = list()\n    for feature in features:\n        corr = np.corrcoef(train_data[target], train_data[feature])[0][1]\n        corrs.append(corr)\n    feature_corrs = pd.Series(np.abs(corrs), index = features)\n    return feature_corrs\n\ntop_features_series = top_correlated_features(train_data, features, target).nlargest(20)\ntop_features_series.plot(kind='barh', figsize=(12, 6)).invert_yaxis()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Features highly correlated with the target might be correlated with each other. Adding all of them together wouldn't be so informative to the model. Let's try removing those with high correlation with each other.","metadata":{}},{"cell_type":"code","source":"top_feature_names = np.unique(top_features_series.nlargest(20).index).tolist()\ntop_20_correlations = train_data[top_feature_names].corr()\nsns.clustermap(top_20_correlations, figsize=(20, 20), cmap=\"mako\", vmin = -1, vmax = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correaltion clustering map shows f_270, f_119, and f_76 have strong negative correlation. f_25, f_71, and f_155 also have strong positive correlation. Of 2 groups, I select the features most correlated with the targets. This step can be automated later in modeling training, but I show a simple single step.","metadata":{}},{"cell_type":"code","source":"remove_list = ['f_76', 'f_270', 'f_71', 'f_155', 'f_119']\n\nfeatures_left = [f'f_{i}' for i in range(300)]\nfor f in remove_list:\n    features_left.remove(f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_features_series_2 = top_correlated_features(train_data, features_left, target).nlargest(20)\ntop_features_series_2.plot(kind='barh', figsize=(12, 6)).invert_yaxis()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_feature_names_2 = np.unique(top_features_series_2.index).tolist()\ntop_20_correlations_2 = train_data[top_feature_names_2].corr()\nsns.clustermap(top_20_correlations_2, figsize=(20, 20), cmap=\"mako\", vmin = -1, vmax = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the previous section, we see that the data has the consistent mean of target over time, seemingly consisting of investment types in a wide range of risks. Outperforming investment would have higher target than the mean, and vice versa. I wonder if the set of features correlated with the target will vary upon investment performance. In other words, does feature importance investment performance?\n\nIf this is the case, we could train different models with important features in each investment class. By removing unecessary features, a model would suffer less from noises.","metadata":{}},{"cell_type":"code","source":"underperform_investments = train_data[train_data['abs_z_score'] < 0.20]\noutperform_investments = train_data[train_data['abs_z_score'] > 0.80]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"underperform_features = top_correlated_features(underperform_investments, features, target).nlargest(20)\noutperform_features = top_correlated_features(outperform_investments, features, target).nlargest(20)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize = (16, 6))\nunderperform_features.plot(kind='barh', title='Most Predictive Features for low-risk investment'\\\n                       , legend=False, ax=ax[0]).invert_yaxis()\nax[0].set_xlabel(\"Pearson Corr with Target\")\n\noutperform_features.plot(kind='barh', title='Most Predictive Features for high-risk investment'\\\n                       , legend=False, ax=ax[1]).invert_yaxis()\nax[1].set_xlabel(\"Pearson Corr with Target\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The graph shows the features importance is independent on the investment performance.\n","metadata":{}},{"cell_type":"markdown","source":"# Training model","metadata":{}},{"cell_type":"markdown","source":"We train the model with a set of the features determined in the previous step. Assuming this time-series data has `time_id` properly chronologically ranked, we should split the training and validation set to avoid spillover effects from leakge future information.","metadata":{}},{"cell_type":"code","source":"def corr(a, b, w):\n    cov = lambda x, y: np.sum(w * (x - np.average(x, weights=w)) * (y - np.average(y, weights=w))) / np.sum(w)\n    return cov(a, b) / np.sqrt(cov(a, a) * cov(b, b))\n\ndef corr_metric(labels, preds):\n    return 'corr', corr(labels, preds, np.ones(len(labels))), True\n\ndef corr_eval(preds, dataset):\n    labels = dataset.get_label()\n    return 'corr', np.corrcoef(labels, labels)[0][1], True","metadata":{"execution":{"iopub.status.busy":"2022-02-27T05:08:04.155124Z","iopub.execute_input":"2022-02-27T05:08:04.155753Z","iopub.status.idle":"2022-02-27T05:08:04.165967Z","shell.execute_reply.started":"2022-02-27T05:08:04.155701Z","shell.execute_reply":"2022-02-27T05:08:04.165239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Manual random search for hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import ParameterGrid\n\nn_round = 10\ndicts = list()\n\nfor i in range(n_round):\n    random_params = {\n    'num_leaves': 2 ** np.random.randint(3, 8),\n    'learning_rate': 10 ** (-np.random.uniform(0.1,2)),\n    'min_data_in_leaf': np.random.randint(50, 1000), \n    'bagging_fraction': 0.5,\n    'feature_pre_filter': False\n    }\n    dicts = np.append(dicts, random_params)\n\nmerged_random_params = {k: [d[k] for d in dicts] for k in dicts[0]}    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_train_data = lgb.Dataset(data=train_data.loc[:, top_feature_names_2], label=train_data.target)\n\ndef corr_eval(preds, dataset):\n    labels = dataset.get_label()\n    return 'corr', np.corrcoef(labels, labels)[0][1], True\n\ndef lgbCV(lgb_train_data, params, num_boost_round, early_stopping_rounds):\n    eval_hist = lgb.cv(params,\n                       lgb_train_data,\n                       nfold=5,\n                       num_boost_round=num_boost_round,\n                       early_stopping_rounds=early_stopping_rounds,\n                       verbose_eval=50,\n                       seed=112,\n                       feval=corr_eval,\n                       stratified=False,\n                       show_stdv=True)\n    return eval_hist","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# report_params = [(params, lgbCV(lgb_train_data, params,  num_boost_round = 2000, early_stopping_rounds = 100)) for params in ParameterGrid(merged_random_params)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Time-based cross-validation","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit\nimport lightgbm\nfrom lightgbm import LGBMRegressor\n\ndef time_split_cross_validation(train_data, features, target):\n    models = dict()\n    corr_scores = dict()\n    \n    tscv = TimeSeriesSplit(max_train_size=None, n_splits=10)\n    \n    for fold, (train_index, test_index) in enumerate(tscv.split(train_data)):\n        \n        train = train_data.iloc[train_index]\n        valid = train_data.iloc[test_index]\n        \n        lgbm = LGBMRegressor(\n            num_leaves=2 ** np.random.randint(3, 8),\n            learning_rate = 10 ** (-np.random.uniform(0.1,2)),\n            n_estimators = 2000,\n            min_child_samples = 1000, \n            subsample=np.random.uniform(0.5,1.0), \n            subsample_freq=1    \n        )\n        \n        lgbm.fit(train[features], train[target]\n                 , eval_set = (valid[features], valid[target])\n                 , eval_metric = corr_metric\n                 , early_stopping_rounds = 100)\n        \n        preds = lgbm.predict(valid[features])\n        \n        models[fold] = lgbm\n        corr_scores[fold] = np.corrcoef(valid[target], preds)[0][1]\n                    \n    return models, corr_scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models, scores = time_split_cross_validation(train_data, top_feature_names_2, target = 'target')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When making predictions, we can use average of predictions from multiple models to mitigate overfitting.","metadata":{}},{"cell_type":"code","source":"def apply_model(models, df, features):\n    for model in models.values:\n        df['target'] += model.predict(df[features])\n    \n    df['target']/len(models)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T05:16:00.828432Z","iopub.execute_input":"2022-02-27T05:16:00.828828Z","iopub.status.idle":"2022-02-27T05:16:00.878551Z","shell.execute_reply.started":"2022-02-27T05:16:00.828787Z","shell.execute_reply":"2022-02-27T05:16:00.87747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In progress, let's do\n\n* Time-span stratified cross-validation: stratifying the groups of count(time_id) * count(investment_id) to ensure equal number of target corresponding to each (time_id, investment_id)\n* Hyperparameter optimization with Optuna","metadata":{}},{"cell_type":"code","source":"# import ubiquant\n# env = ubiquant.make_env()   # initialize the environment\n\n# iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\n\n# for (test_df, sample_prediction_df) in iter_test:\n#     apply_model(models, sample_prediction_df, top_feature_names_2)  # make your predictions here\n#     env.predict(sample_prediction_df)   # register your predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-27T05:15:50.686538Z","iopub.execute_input":"2022-02-27T05:15:50.687013Z","iopub.status.idle":"2022-02-27T05:15:50.69175Z","shell.execute_reply.started":"2022-02-27T05:15:50.686976Z","shell.execute_reply":"2022-02-27T05:15:50.69074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Other concerns and ideas","metadata":{}},{"cell_type":"markdown","source":"- Concerns\n1. It's unclear what target is. Target could be the investment performance such as log-return, alpha where each value is overlapping. For example, target at time t derives from target from time t+1 or t+2, representing the next 2-period return. If this is the case, time-series split for cross-validation should have an appropriate gap for to validate the result a out-of-train-sample period.\n2. There obviously are unequal number of investments over time, but I rarely leverage the information about missing investments in some periods. Number of investments could be a good signal to targets. For instance, the lower `count(investment_id)` is, the more target likely deviates from its $\\mathbb{E}_{investment}[target]$\n\n- Further ideas\n1. We can conduct stationarity test on the target to investigate the long-term behavior. The plot of mean and standard deviation over time signifies overall target is mean-reverting. However, the long-term behavior of each investment_id can be different. We can compute p-value of unit-root testing from `adfuller(train_data.loc[train_data.investment_id == X])`.\n2. I'd want to experiment transforming $target$ to be $\\Delta(target)$ or $\\Delta(target) - \\Delta\\mathbb{E}_{investment}[target]$ where $\\Delta$ refers to the temporal difference of values. This should capture the derivative of target better than directly predicting the target. However, it doesn't guarantee the highest correlation (the evaluation metric of this competition).","metadata":{}}]}