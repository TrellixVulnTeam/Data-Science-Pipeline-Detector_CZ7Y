{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<table>\n    <thead>\n        <tr>\n            <th colspan=\"2\"><br>\n<font size=\"6\" color=orange><div style=\"text-align:center;\"> <span style=\"border: 3px outset black;\">OPENCLASSROOMS</span></div></font></th>\n            <p>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td><font size = \"5\"><strong><div style=\"text-align:center;\">\n<br>Projet 6\n</div></strong>\n\n<font size = \"6\"><strong><div style=\"text-align:center;\">\n<font color=blue> Compétition Kaggle\n</div></strong>\n    </tbody>\n</table>\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk(r'C:\\Users\\bdelaval\\Downloads\\train'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=red> Analyse exploratoire </font><a id='partie1.5'> </a>","metadata":{}},{"cell_type":"code","source":"data_types_dict = {\n                   'time_id': 'int32',\n                   'investment_id': 'int16',\n                   'target': 'float16',\n                  }\n\nfeatures = [f'f_{i}' for i in range(300)]\n\nfor f in features:\n    data_types_dict[f] = 'float16'\n    \ntarget = 'target'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(r'C:\\Users\\bdelaval\\Downloads\\train.csv', # nrows=5 * 10**4,\n                       usecols = data_types_dict.keys(),\n                       dtype=data_types_dict, \n                       index_col = 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Les features sont déjà standardisées.","metadata":{}},{"cell_type":"markdown","source":"## <font color=blue> Target </font><a id='partie1.5'> </a>","metadata":{}},{"cell_type":"code","source":"train_df['target'].hist(bins = 100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### La distribution semble gaussienne, pas d'outlier. Cela ne devrait pas poser de problème de travailler avec.","metadata":{}},{"cell_type":"code","source":"# Traçons également les distributions des cibles de quelques entités aléatoires :\n\nfor f in np.random.choice(train_df['investment_id'].unique(), 10):\n    train_df[train_df['investment_id'] == f]['target'].hist(bins = 100, alpha = 0.2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Chaque investment_id semble correct aussi.","metadata":{}},{"cell_type":"markdown","source":"## <font color=blue> Investment </font><a id='partie1.5'> </a>","metadata":{}},{"cell_type":"code","source":"train_df['investment_id'].nunique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_df['investment_id'].value_counts().plot(kind = 'bar')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Nous avons 3579 investissements différents, et la plupart d'entre eux ont une quantité suffisante de points de données, aucun filtrage n'est nécessaire.","metadata":{}},{"cell_type":"markdown","source":"## <font color=blue> Features </font><a id='partie1.5'> </a>","metadata":{}},{"cell_type":"markdown","source":"#### Avec les 300 features, c'est très compliqué de toutes les observer. Voici quelques distributions graphiques en focus.","metadata":{}},{"cell_type":"code","source":"f = 'f_67'\ntrain_df[f].hist(bins = 100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = 'f_109'\ntrain_df[f].hist(bins = 100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = 'f_234'\ntrain_df[f].hist(bins = 100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = 'f_164'\ntrain_df[f].hist(bins = 100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[features].nunique().hist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=red> Features interaction </font><a id='partie1.5'> </a>","metadata":{}},{"cell_type":"code","source":"# analyse d'un échantillon de la population\n\nsample_df = train_df.head(50000)\nsample_df.to_csv(r'C:\\Users\\bdelaval\\Downloads\\sample_df.csv')\nsample_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nsample_df = pd.read_csv(r'C:\\Users\\bdelaval\\Downloads\\sample_df.csv')\n\n# ici je m'assure que le time_id dans l'ordre chronologique\nsample_df = sample_df.sort_values(by=['time_id'])\nsample_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_types_dict = {\n                   'time_id': 'int32',\n                   'investment_id': 'int16',\n                   'target': 'float16',\n                  }\n\nfeatures = [f'f_{i}' for i in range(300)]\n\nfor f in features:\n    data_types_dict[f] = 'float16'\n    \ntarget = 'target'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation = sample_df[[target] + features].corr()\ncorrelation['target'].iloc[1:].hist(bins = 20)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Aucune feature n'est très corrélée avec la cible, vérifions maintenant si il y a des variables très corrélées entre elles.","metadata":{}},{"cell_type":"code","source":"import numpy as np","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Je cherche qu'elles sont les paires corrélées à plus de 0.95\n\ncorr_matrix = sample_df.corr().abs()\nhigh_corr_var = np.where(corr_matrix > 0.95)\nhigh_corr_var = [(corr_matrix.columns[x],corr_matrix.columns[y]) for x, y in zip(*high_corr_var) if x != y and x < y]\nhigh_corr_var","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Regard sur les fortes corrélations:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(sample_df[\"f_4\"], sample_df[\"f_228\"], 'o', alpha = 0.8)\nplt.xlabel(\"f_4\")\nplt.ylabel(\"f_228\")\nplt.show()\n\nimport scipy.stats as st\nprint(st.pearsonr(sample_df[\"f_4\"], sample_df[\"f_228\"])[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(sample_df[\"f_14\"], sample_df[\"f_18\"], 'o', alpha = 0.8)\nplt.xlabel(\"f_14\")\nplt.ylabel(\"f_18\")\nplt.show()\n\nprint(st.pearsonr(sample_df[\"f_14\"], sample_df[\"f_18\"])[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ce code sélectionne les corrélations supérieures à 0.95 et les supprime du df directement\n\ncorr_matrix = sample_df.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find features with correlation greater than 0.90\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n\n# Drop features \nsample_df.drop(to_drop, axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check s'il y a bien 9 colonnes de moins\nsample_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sn\n\nplt.figure(figsize = (20, 20))\ncorrMatrix = sample_df.corr().abs()\nsn.heatmap(corrMatrix, annot = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=red> PCA </font><a id='partie1.5'> </a>\n","metadata":{}},{"cell_type":"markdown","source":"#### L'objectif est de réduire le nombre de variables mais en conservant une variance expliquée à 95%.","metadata":{}},{"cell_type":"code","source":"X = sample_df.drop(['target','investment_id'], axis=1).values\ny = sample_df['target'].values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape)\nprint(y.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca_294 = PCA(n_components=294)\npca_294.fit(X)\nx_pca_294 = pca_294.transform(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Variance explained by all 294 principal components =\", \n     sum(pca_294.explained_variance_ratio_* 100))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_294.explained_variance_ratio_* 100","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.cumsum(pca_294.explained_variance_ratio_* 100)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=(14,8))\nplt.plot(np.cumsum(pca_294.explained_variance_ratio_))\nplt.xlabel('Number of components')\nplt.ylabel('Explained variance')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calcul du nombre de variables dont on a besoin pour expliquer 95% de la variance\n\npca_95 = PCA(n_components = 0.95)\npca_95.fit(X)\nX_pca_95 = pca_95.transform(X)\nX_pca_95.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vérif du résultat précédent\n\nprint('Variance explained by the first X principal components =',\n     np.cumsum(pca_294.explained_variance_ratio_* 100)[143])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_df.drop(['target','investment_id'], axis=1).var()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best correlations variables-target in absolute values\n\ncorr_matrix = sample_df.drop(['investment_id'], axis=1).corr()\ncorrel = corr_matrix[\"target\"].sort_values(ascending=False)\ncorrel_abs = correl.abs()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correl_abs.sort_values(ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sélection des variables les plus influentes pour le machine learning\n\ndf_correl_abs = pd.DataFrame(correl_abs.sort_values(ascending=False))\ndf_correl_abs_best_variance = df_correl_abs[:-150]\ndf_correl_abs_best_variance","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=red> Analyse des fortes corrélations </font><a id='partie1.5'> </a>\n","metadata":{}},{"cell_type":"code","source":"plt.plot(sample_df[\"f_67\"], sample_df[\"target\"], 'o', alpha = 0.8)\nplt.xlabel(\"f_67\")\nplt.ylabel(\"variable cible\")\nplt.show()\n\nimport scipy.stats as st\nprint(st.pearsonr(sample_df[\"f_67\"], sample_df[\"target\"])[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(sample_df[\"f_134\"], sample_df[\"target\"], 'o', alpha = 0.8)\nplt.xlabel(\"f_134\")\nplt.ylabel(\"variable cible\")\nplt.show()\n\nimport scipy.stats as st\nprint(st.pearsonr(sample_df[\"f_134\"], sample_df[\"target\"])[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(sample_df[\"f_254\"], sample_df[\"target\"], 'o', alpha = 0.8)\nplt.xlabel(\"f_254\")\nplt.ylabel(\"variable cible\")\nplt.show()\n\nimport scipy.stats as st\nprint(st.pearsonr(sample_df[\"f_254\"], sample_df[\"target\"])[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(sample_df[\"f_211\"], sample_df[\"target\"], 'o', alpha = 0.8)\nplt.xlabel(\"f_211\")\nplt.ylabel(\"variable cible\")\nplt.show()\n\nimport scipy.stats as st\nprint(st.pearsonr(sample_df[\"f_211\"], sample_df[\"target\"])[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Aucune variable ne montre une belle corrélation avec la cible.","metadata":{}},{"cell_type":"code","source":"df_correl_abs_best_variance_transposed = df_correl_abs_best_variance.T\ndf_correl_abs_best_variance_transposed","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_df.head(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keep only best explained col of the 2 df\n\ndf_ml = pd.concat([df_correl_abs_best_variance_transposed[:-1], sample_df], axis=0)\ndf_ml = df_ml[df_ml.columns[:-151]]\ndf_ml.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment = sample_df[['investment_id', 'target']]\ninvestment.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fetch col investment_id\n\ndf_ml2 = pd.concat([investment, df_ml], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=red> Model training </font><a id='partie1.5'> </a>\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nX = df_ml.drop(['target'], axis=1)\ny = df_ml[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n# data_split_shuffle = False, so that we do not use \"future\" observations to predict \"past\" observations","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=blue> Model baseline, linear regression </font><a id='partie1.5'> </a>","metadata":{}},{"cell_type":"markdown","source":"#### Pour être en accord avec la métrique de soumission de la compétition Kaggle, je choisis l'indice de Pearson.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats.stats import pearsonr\nfrom sklearn.metrics import make_scorer\n\ndef my_scorer(X, y):\n    correlation = pearsonr(X, y)[0]\n    return correlation\n\nmy_func = make_scorer(my_scorer, greater_is_better=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nparameters = {}\n\nlr = GridSearchCV(LinearRegression(), \n                  parameters, \n                  scoring=my_func, \n                  cv=5, \n                  n_jobs=-1, \n                  return_train_score=True)\n\nlr.fit(X_train, y_train)\n\nlr.cv_results_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_lr_results = pd.DataFrame(lr.cv_results_)\ndf_lr_results = df_lr_results[['mean_fit_time','mean_score_time','mean_train_score','mean_test_score']]\ndf_lr_results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=blue> ElasticNet </font><a id='partie1.5'> </a>","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\n\nparameters = {'alpha': [1e-4, 1e-2, 1, 1e2, 1e4],\n              'tol': [1e-6, 1e-4, 1e-2],\n              'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]}\n\nelasticnet = GridSearchCV(ElasticNet(), \n                          parameters, \n                          scoring=my_func, \n                          return_train_score=True,\n                          cv=5, \n                          n_jobs=-1) #n_jobs=-1 pour utiliser tous les coeurs\n\nelasticnet.fit(X_train, y_train)\n\nprint(elasticnet.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model to disk\n\nimport pickle\n\nfilename = 'elasticnet_model.sav'\npickle.dump(elasticnet, open(filename, 'wb'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_en_results = pd.DataFrame(elasticnet.cv_results_)\ndf_en_results = df_en_results[['mean_fit_time','mean_score_time','mean_train_score','mean_test_score']]\nbest_results_en = df_en_results.sort_values(by=['mean_test_score'], ascending=False)\n\nbest_results_en = best_results_en.head(1)\nbest_results_en","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the model from disk\n\n# elasticnet = pickle.load(open(filename, 'rb'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=blue> K Nearest Neighbors </font><a id='partie1.5'> </a>","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\n\nparameters = {'n_neighbors':[1, 3, 5, 7],\n              'p': [2], # p = 1, is to using manhattan_distance (l1), and euclidean_distance (l2)\n              'weights': ['distance'] # si distance:les points proches ont plus d'influence que les points éloignés\n             }                       \n\nknn = GridSearchCV(KNeighborsRegressor(), \n                          parameters, \n                          scoring=my_func, \n                          return_train_score=True,\n                          cv=5, \n                          n_jobs=-1)\n\nknn.fit(X_train, y_train)\n\nprint(knn.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model to disk\n\nfilename = 'knn_model.sav'\npickle.dump(knn, open(filename, 'wb'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_knn_results = pd.DataFrame(knn.cv_results_)\ndf_knn_results = df_knn_results[['mean_fit_time','mean_score_time','mean_train_score','mean_test_score']]\nbest_results_knn = df_knn_results.sort_values(by=['mean_test_score'], ascending=False)\n\nbest_results_knn = best_results_knn.head(1)\nbest_results_knn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=blue> Random Forest Regressor </font><a id='partie1.5'> </a>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nparameters = {'max_features': ['auto', 'sqrt', 'log2'],\n              'n_estimators': [50,100,300]}    \n\nrf = GridSearchCV(RandomForestRegressor(), \n                          parameters, \n                          scoring=my_func, \n                          return_train_score=True,\n                          cv=5, \n                          n_jobs=-1)\n\nrf.fit(X_train, y_train)\n\nprint(rf.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model to disk\n\nfilename = 'rf_model.sav'\npickle.dump(knn, open(filename, 'wb'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_rf_results = pd.DataFrame(rf.cv_results_)\ndf_rf_results = df_rf_results[['mean_fit_time','mean_score_time','mean_train_score','mean_test_score']]\nbest_results_rf = df_rf_results.sort_values(by=['mean_test_score'], ascending=False)\n\nbest_results_rf = best_results_rf.head(1)\nbest_results_rf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Gros overfitting. Il faudra jouer sur la profondeur des arbres.","metadata":{}},{"cell_type":"markdown","source":"## <font color=blue> Gradient Boosting Regressor </font><a id='partie1.5'> </a>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\nparameters = {'alpha': [1e-4, 1e-3, 1e-2, 1e-1],\n              'tol': [1e-4, 1e-3, 1e-2],\n              'n_estimators': [10, 20]} \n\ngbr = GridSearchCV(GradientBoostingRegressor(), \n                          parameters, \n                          scoring=my_func, \n                          return_train_score=True,\n                          cv=5, \n                          n_jobs=-1)\n\ngbr.fit(X_train, y_train)\n\nprint(gbr.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model to disk\n\nfilename = 'gb_model.sav'\npickle.dump(knn, open(filename, 'wb'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_gbr_results = pd.DataFrame(gbr.cv_results_)\ndf_gbr_results = df_gbr_results[['mean_fit_time','mean_score_time','mean_train_score','mean_test_score']]\nbest_results_gbr = df_gbr_results.sort_values(by=['mean_test_score'], ascending=False)\n\nbest_results_gbr = best_results_gbr.head(1)\nbest_results_gbr","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=blue> Adaboost </font><a id='partie1.5'> </a>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\n\nparameters = {'loss': ['linear', 'square', 'exponential'], \n              'learning_rate': [0.1, 1.0, 10], \n              'n_estimators': [10, 50, 200]}\n\nab = GridSearchCV(AdaBoostRegressor(), \n                          parameters, \n                          scoring=my_func, \n                          return_train_score=True,\n                          cv=5, \n                          n_jobs=-1)\n\nab.fit(X_train, y_train)\n\nprint(ab.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model to disk\n\nfilename = 'ad_model.sav'\npickle.dump(knn, open(filename, 'wb'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ab_results = pd.DataFrame(ab.cv_results_)\ndf_ab_results = df_ab_results[['mean_fit_time','mean_score_time','mean_train_score','mean_test_score']]\nbest_results_ab = df_ab_results.sort_values(by=['mean_test_score'], ascending=False)\n\nbest_results_ab = best_results_ab.head(1)\nbest_results_ab","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=blue> LGBM Regressor </font><a id='partie1.5'> </a>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\n\nparameters = {'learning_rate': [0.01, 0.1, 1.0], \n              'n_estimators': [10, 100, 500]}\n\nlgbm = GridSearchCV(AdaBoostRegressor(), \n                          parameters, \n                          scoring=my_func, \n                          return_train_score=True,\n                          cv=5, \n                          n_jobs=-1)\n\nlgbm.fit(X_train, y_train)\n\nprint(lgbm.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model to disk\n\nfilename = 'lgbm_model.sav'\npickle.dump(knn, open(filename, 'wb'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_lgbm_results = pd.DataFrame(lgbm.cv_results_)\ndf_lgbm_results = df_lgbm_results[['mean_fit_time','mean_score_time','mean_train_score','mean_test_score']]\nbest_results_lgbm = df_lgbm_results.sort_values(by=['mean_test_score'], ascending=False)\n\nbest_results_lgbm = best_results_lgbm.head(1)\nbest_results_lgbm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=blue> BayesianRidge </font><a id='partie1.5'> </a>","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import BayesianRidge\n\nparameters = {'n_iter': [10,300,1000],\n              'lambda_1': [1e-8, 1e-6, 1e-3],\n              'lambda_2': [1e-8, 1e-6, 1e-3]}\n\nbr = GridSearchCV(BayesianRidge(), \n                          parameters, \n                          scoring=my_func, \n                          return_train_score=True,\n                          cv=5, \n                          n_jobs=-1)\n                  \nbr.fit(X_train, y_train)\n\nprint(br.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model to disk\n\nfilename = 'br_model.sav'\npickle.dump(br, open(filename, 'wb'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_br_results = pd.DataFrame(br.cv_results_)\ndf_br_results = df_br_results[['mean_fit_time','mean_score_time','mean_train_score','mean_test_score']]\nbest_results_br = df_br_results.sort_values(by=['mean_test_score'], ascending=False)\n\nbest_results_br = best_results_br.head(1)\nbest_results_br","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=red> Visualisation des résultats des modèles sur un graphique </font><a id='partie1.5'> </a>","metadata":{}},{"cell_type":"code","source":"df_compare = pd.concat([df_lr_results,best_results_en,best_results_knn,best_results_rf,best_results_gbr,best_results_ab,\n                       best_results_lgbm,best_results_br], axis = 0)\n\nmodel = ['linear_regression','elasticnet','KNeighborsRegressor','random_forest','gradient_boosting','adaboost','lgbm','bayesian_ridge']\ndf_compare['model'] = model\n\ndf_compare","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12, 7))\nplt.rcParams.update({'font.size': 14})\nbarWidth = 0.25\n\ny1 = df_compare['mean_train_score']\ny2 = df_compare['mean_test_score']\nx = df_compare['model']\nr1 = range(len(y1))\nr2 = [x + barWidth for x in r1]\n\nplt.bar(r1, y1, width = barWidth, color = ['springgreen' for i in y1],\n           edgecolor = ['black' for i in y1], linewidth = 1, label = 'mean_train_score')\nplt.bar(r2, y2, width = barWidth, color = ['hotpink' for i in y1], \n           edgecolor = ['black' for i in y2], linewidth = 1, label = 'mean_test_score')\nplt.xticks([r + barWidth / 2 for r in range(len(y1))], df_compare['model'], rotation = 45)\n\nplt.xlabel(\"Models\")\nplt.ylabel(\"Pearsonr\")\n\nplt.title(\"Coefficient de corrélation avec la target\")\nplt.legend(bbox_to_anchor = (0.85, 0.9))\nplt.grid()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### On remarque que 2 modèles, KNN et RF font un gros overfitting. Malgré cela, le résultat de Random Forest est le meilleur.","metadata":{}},{"cell_type":"code","source":"# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.bar.html\n\n\nax = df_compare.plot.bar(x='model', y='mean_test_score', rot=45, figsize=(12, 9))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importances = pd.Series(test.coef_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importances = pd.Series(test.coef_, index=X.columns)\nfeat_importances.nsmallest(10).plot(kind='barh')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color=blue> Model stacking </font><a id='partie1.5'> </a>","metadata":{}},{"cell_type":"markdown","source":"#### En récupérant les meilleurs modèles, je construis un modèle empilé.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingRegressor\n\nestimators = [\n    ('en', ElasticNet()),\n    ('gbr', GradientBoostingRegressor())\n    ]\n\nreg_stack = StackingRegressor(estimators = estimators,\n                        final_estimator = RandomForestRegressor())\n\nparams = {'en__alpha': [1e-2],\n          'en__tol': [1e-2],\n          'en__l1_ratio': [0.3],\n          'gbr__alpha': [1e-2],\n          'gbr__tol': [1e-2],\n          'gbr__n_estimators': [20]}\n\ngrid = GridSearchCV(estimator = reg_stack, param_grid = params, cv=5, n_jobs=-1, return_train_score=True)\ngrid.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(grid.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_grid_results = pd.DataFrame(grid.cv_results_)\ndf_grid_results = df_grid_results[['mean_fit_time','mean_score_time','mean_train_score','mean_test_score']]\nbest_results_grid = df_grid_results.sort_values(by=['mean_test_score'], ascending=False)\n\nbest_results_grid = best_results_grid.head(1)\nbest_results_grid","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Le modèle empilé ne fait pas mieux que le meilleur modèle qui est ElasticNet.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport ubiquant\n\nenv = ubiquant.make_env()  \niter_test = env.iter_test()\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df.drop(['row_id'], axis=1, inplace=True)\n    pred = model.predict(test_df)\n    sample_prediction_df['target'] = pred\n    env.predict(sample_prediction_df) \n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}