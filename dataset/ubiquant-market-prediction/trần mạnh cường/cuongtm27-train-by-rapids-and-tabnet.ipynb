{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intorduction\nThis notebook is EDA and training by using tabnet.  \nRAPIDS(cuDF) is GPU DataFrame library for loading, aggregating, filtering, and otherwise manipulating data.  (<a href='https://docs.rapids.ai/api/cudf/stable/'>Ref</a>)  <br>\n  \nver.14: cuDF dataframe is used in EDA.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Package","metadata":{}},{"cell_type":"code","source":"!pip -q install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\n\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom pytorch_tabnet.metrics import Metric","metadata":{"execution":{"iopub.status.busy":"2022-03-08T16:28:02.249229Z","iopub.execute_input":"2022-03-08T16:28:02.250161Z","iopub.status.idle":"2022-03-08T16:28:32.855597Z","shell.execute_reply.started":"2022-03-08T16:28:02.250042Z","shell.execute_reply":"2022-03-08T16:28:32.854755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\nimport gc\nimport glob\nimport os\nimport pickle\nimport random\n\nimport argparse\nimport cudf\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import AutoMinorLocator\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nimport torch\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom tqdm.auto import tqdm\n\nimport cudf\nprint('RAPIDS version',cudf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-03-08T16:28:32.857453Z","iopub.execute_input":"2022-03-08T16:28:32.857716Z","iopub.status.idle":"2022-03-08T16:28:35.429262Z","shell.execute_reply.started":"2022-03-08T16:28:32.857682Z","shell.execute_reply":"2022-03-08T16:28:35.427727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Paths\nINPUT_PATH= '../input/ubiquant-market-prediction'\nTRAIN_PICKLE = '../input/fast-read-data-ubiquant/train_reduced.pkl' # train.csv reduced memory usage\n\n# Train parematers\nDEBUG = False \n'''\nIf DEBUG==True, \nreduced sampling is performed for train data,\nand the fold calculation is stopped at the first fold.  \n'''\nargs = argparse.Namespace(\n    seed = 2022,\n    patience = 20,\n    batch_size = 1024*20, \n    virtual_batch_size = 128*20,\n    drop_last = True,\n    reduced_sampling = None if not DEBUG else 0.10, # if DEBUG, 10% samples are used.\n    max_epochs = 200 if not DEBUG else 5,\n    n_folds = 5, # if DEBUG, one fold is only calculated.\n    n_steps = 2, # equals to the number of masks\n    n_workers = 2,\n    n_bins = 16\n)\n\n# Random seed\ndef seed_everything(seed: int) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nseed_everything(args.seed)\n\n# Matplotlib style\nplt.style.use('ggplot')\nplt.rcParams['axes.grid'] = True\nplt.rcParams['grid.color'] = 'gray'\nplt.rcParams['grid.alpha'] = 0.5\nplt.rcParams['grid.linestyle'] = '--'","metadata":{"execution":{"iopub.status.busy":"2022-03-08T16:28:35.430594Z","iopub.execute_input":"2022-03-08T16:28:35.430842Z","iopub.status.idle":"2022-03-08T16:28:35.441512Z","shell.execute_reply.started":"2022-03-08T16:28:35.430809Z","shell.execute_reply":"2022-03-08T16:28:35.439795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Input data","metadata":{}},{"cell_type":"code","source":"%%time\ndf_train = pd.read_pickle(TRAIN_PICKLE)\ndisplay(df_train.head())","metadata":{"execution":{"iopub.status.busy":"2022-03-08T16:28:35.443323Z","iopub.execute_input":"2022-03-08T16:28:35.443772Z","iopub.status.idle":"2022-03-08T16:28:53.369213Z","shell.execute_reply.started":"2022-03-08T16:28:35.443647Z","shell.execute_reply":"2022-03-08T16:28:53.368507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Short EDA","metadata":{}},{"cell_type":"markdown","source":"### There are \"target==0\" data. If you use the loss function like RSMPE, the value becames inf.","metadata":{}},{"cell_type":"markdown","source":"### From above figures, almost the features have mean ≃ 0.0, std ≃ 1.0 except for some features:\nSmall std  : f_124, f_170, f175, f_272  \nSmall mean : f_170, f_175  \nLarge mean : f_41, f_182, f_246  \n\n### Investigate these distributions as follows","metadata":{}},{"cell_type":"markdown","source":"## Change the range of the histgrams between -10 and 10.¶","metadata":{}},{"cell_type":"markdown","source":"### From above,\n- Most of the features are not normal and some have multimodal.\n- Some singular peek exist in a distribution. Especially, f_124 is like delta functions.","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### KFold","metadata":{}},{"cell_type":"code","source":"def add_fold_column(df):\n    # Create 'time_span' column, which is used for stratified KFold.\n    df_time = (df.loc[:,['time_id', 'investment_id']]\n               .groupby('investment_id')\n               .agg({'time_id': ['min', 'max']})\n               .reset_index())\n    df_time['time_span'] = df_time['time_id']['max'] - df_time['time_id']['min']\n    display(df_time.head())\n    \n    # Merge 'time_span' to df\n    df_time = pd.DataFrame(df_time.to_pandas()\n                           .droplevel(level=1, axis=1)\n                           .drop('time_id' ,axis=1))\n    df_time = cudf.DataFrame.from_pandas(df_time)\n    df = df.merge(df_time, on=['investment_id'])\n   \n    # Holdout\n    _target = cudf.cut(df['time_span'], args.n_bins, labels=False)\n    _train, _valid = train_test_split(_target,\n                                      stratify=_target.values.get(),\n                                      random_state=args.seed)\n    print(f'Number of holdout records: {len(_valid)}')\n    df = df.iloc[_train.index].sort_values(by=['time_id', 'investment_id'])\\\n           .reset_index(drop=True)\n    \n    # StratifiedKFold\n    df[\"fold\"] = -1\n    _target = cudf.cut(df['time_span'], args.n_bins, labels=False) \n    skfold = StratifiedKFold(n_splits=args.n_folds)\n    for fold, (train_idx, valid_idx) in enumerate(skfold.split(_target, _target.values.get())):\n        df.loc[valid_idx, 'fold'] = fold\n       \n    return df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-08T16:29:24.041757Z","iopub.execute_input":"2022-03-08T16:29:24.042028Z","iopub.status.idle":"2022-03-08T16:29:24.052052Z","shell.execute_reply.started":"2022-03-08T16:29:24.042001Z","shell.execute_reply":"2022-03-08T16:29:24.051416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cudf_train = add_fold_column(cudf_train)\ndisplay(cudf_train.head())","metadata":{"execution":{"iopub.status.busy":"2022-03-08T16:29:26.969923Z","iopub.execute_input":"2022-03-08T16:29:26.970397Z","iopub.status.idle":"2022-03-08T16:29:40.590214Z","shell.execute_reply.started":"2022-03-08T16:29:26.970361Z","shell.execute_reply":"2022-03-08T16:29:40.589444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create features\nref. https://www.kaggle.com/valleyzw/ubiquant-lgbm-baseline","metadata":{}},{"cell_type":"code","source":"def create_features(df):\n    cat_features = ['investment_id']\n    num_features = [f'f_{i}' for i in range(300)]\n    features = num_features + cat_features\n\n    combination_features = ['f_231-f_250', 'f_118-f_280', 'f_155-f_297','f_25-f_237',\n                            'f_179-f_265', 'f_119-f_270', 'f_71-f_197', 'f_21-f_65']\n    for f in combination_features:\n        f1, f2 = f.split('-')\n        df[f] = df[f1] + df[f2]\n    \n    features += combination_features\n    drop_features = ['f_148', 'f_72', 'f_49', 'f_205', 'f_228', 'f_97', 'f_262', 'f_258']\n    features = list(sorted(set(features).difference(set(drop_features))))\n    df = df.drop(drop_features, axis=1)\n                     \n    return df, features","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-08T16:29:40.592032Z","iopub.execute_input":"2022-03-08T16:29:40.592311Z","iopub.status.idle":"2022-03-08T16:29:40.599198Z","shell.execute_reply.started":"2022-03-08T16:29:40.592275Z","shell.execute_reply":"2022-03-08T16:29:40.598426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cudf_train, features = create_features(cudf_train)\ncudf_train = cudf_train.drop(['time_id', 'time_span'], axis=1)\nprint('len(features):', len(features))\nprint(features)\ndisplay(cudf_train.head())","metadata":{"execution":{"iopub.status.busy":"2022-03-08T16:29:40.601091Z","iopub.execute_input":"2022-03-08T16:29:40.601429Z","iopub.status.idle":"2022-03-08T16:29:50.221213Z","shell.execute_reply.started":"2022-03-08T16:29:40.601392Z","shell.execute_reply":"2022-03-08T16:29:50.22045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Tabnet parameters    \ntabnet_params = dict(\n            cat_idxs = [i for i, f in enumerate(df_train.columns.tolist()) if f in ['investment_id']],\n            cat_emb_dim = 1,\n            n_d = 16,\n            n_a = 16,\n            n_steps = args.n_steps,\n            gamma = 2,\n            n_independent = 2,\n            n_shared = 2,\n            lambda_sparse = 0,\n            optimizer_fn = Adam,\n            optimizer_params = dict(lr = (2e-2)),\n            mask_type = 'entmax',\n            scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, \n                                    last_epoch=-1, verbose=False),\n            scheduler_fn = CosineAnnealingWarmRestarts,\n            seed = args.seed,\n            verbose = 10\n        )\n\n# Metric\nclass PearsonCorrelation(Metric):\n    def __init__(self):\n        self._name = 'pearson_corr'\n        self._maximize = True\n\n    def __call__(self, x, y):\n        x = x.squeeze()\n        y = y.squeeze()\n        x_diff = x - np.mean(x)\n        y_diff = y - np.mean(y)\n        return np.dot(x_diff, y_diff)/(np.sqrt(sum(x_diff**2))*np.sqrt(sum(y_diff**2)))\n\n# Train run\ndef train(df_train, features, args, tabnet_params, debug=DEBUG):\n\n    if debug: # Reduced sampling\n        print('Run as DEBUG')\n        n_samples = len(df_train)\n        sample_idx = df_train.sample(int(n_samples*args.reduced_sampling), random_state=args.seed).index\n        df_train = df_train.iloc[sample_idx].reset_index(drop=True)\n        print('len(df_train):', len(df_train))\n        del sample_idx\n        _ = gc.collect()\n    \n    ###### Outputs ######\n    histories = {}\n    oof_predictions = pd.DataFrame({'row_id': df_train['row_id'].to_pandas()}) # predictions for validatation data\n    feature_importances = pd.DataFrame({'features': features})\n    masks = {}\n#     explain_matrices = {}\n    ####################\n\n    for fold in tqdm(range(args.n_folds)):\n\n        print(f\"\\n{'>'*15} Fold {fold} {'<'*15}\")\n        \n        # Preapare train and valid data\n        train_idx = df_train['fold']!=fold\n        valid_idx = df_train['fold']==fold\n        X_train = df_train.loc[train_idx, features].values.get()\n        X_valid = df_train.loc[valid_idx, features].values.get()\n        y_train = df_train.loc[train_idx,'target'].values.get().reshape(-1,1)\n        y_valid = df_train.loc[valid_idx,'target'].values.get().reshape(-1,1)\n        print(f'len(X_train): {len(X_train)}, len(X_valid): {len(X_valid)},')\n        \n        # Model initialize\n        model = TabNetRegressor(**tabnet_params)\n        \n        # Train\n        model.fit(\n            X_train, y_train,\n            eval_set = [(X_valid, y_valid)],\n            max_epochs = args.max_epochs,\n            patience = args.patience,\n            batch_size = args.batch_size, \n            virtual_batch_size = args.virtual_batch_size,\n            num_workers = args.n_workers,\n            drop_last = args.drop_last,\n            eval_metric = [PearsonCorrelation],\n            loss_fn = torch.nn.MSELoss() # torch.nn.L1Loss()\n        )\n\n        # Save model\n        model.save_model(f'./tabnet_fold{fold}')\n        # Scores\n        histories[f'fold{fold}'] = model.history\n        # Predict for validation data\n        oof_predictions.loc[valid_idx.values.get(), 'pred'] = model.predict(X_valid).astype('float16')\n        # Feature importances\n        feature_importances[f'importance_fold{fold}'] = model.feature_importances_\n        # Explain matrices and Masks\n#         explain_matrices_, masks_ = model.explain(X_valid)\n#         explain_matrices[f'fold{fold}'] = explain_matrices_\n#         masks[f'fold{fold}'] = masks_\n\n        del model, X_train, X_valid, y_train, y_valid\n        torch.cuda.empty_cache()\n        _ = gc.collect()\n        \n        if debug:\n            print('Debug: stop calculation at first epoch')\n            break\n        \n    outputs = dict(\n        histories = histories,\n        feature_importances = feature_importances,\n        oof_predictions = oof_predictions,\n#         masks = masks,\n#         explain_matrices = explain_matrices,\n    )\n    \n    return outputs","metadata":{"execution":{"iopub.status.busy":"2022-03-08T16:29:50.223081Z","iopub.execute_input":"2022-03-08T16:29:50.223495Z","iopub.status.idle":"2022-03-08T16:29:50.243809Z","shell.execute_reply.started":"2022-03-08T16:29:50.223457Z","shell.execute_reply":"2022-03-08T16:29:50.24291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = train(cudf_train, features, args, tabnet_params)\nprint('outputs:')\nfor k, v in outputs.items():\n    print(f'  {k}: {type(v)}')","metadata":{"execution":{"iopub.status.busy":"2022-03-08T16:29:50.245112Z","iopub.execute_input":"2022-03-08T16:29:50.24536Z","iopub.status.idle":"2022-03-08T18:06:30.562303Z","shell.execute_reply.started":"2022-03-08T16:29:50.245324Z","shell.execute_reply":"2022-03-08T18:06:30.561539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"markdown","source":"## Score","metadata":{}},{"cell_type":"markdown","source":"## Feature importance (Top50)","metadata":{}},{"cell_type":"markdown","source":"## Mask","metadata":{}},{"cell_type":"markdown","source":"## Explain matrix","metadata":{}},{"cell_type":"markdown","source":"## True-Prediction correlation","metadata":{}},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# def predict(models, df_test, features):\n#     # df_test['time_id'] = df_test.row_id.str.extract(r'(\\d+)_.*').astype(np.uint16) # extract time_id from row_id # remove.ver2\n#     preds = []\n#     for model in models:\n#         pred = model.predict(df_test[features].values)\n#         preds.append(pred)\n        \n#     mean_pred_by_folds = np.mean(np.stack(preds), axis=0)\n#     return mean_pred_by_folds\n\n# def load_trained_models():\n#     model_paths = glob.glob('./tabnet_fold*.zip')\n#     model =  TabNetRegressor(**tabnet_params)\n#     models = []\n#     for model_path in model_paths:    \n#         model.load_model(model_path)\n#         model_ = copy.deepcopy(model)\n#         models.append(model_)\n    \n#     return models","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-07T16:46:06.652629Z","iopub.execute_input":"2022-03-07T16:46:06.653237Z","iopub.status.idle":"2022-03-07T16:46:06.660672Z","shell.execute_reply.started":"2022-03-07T16:46:06.653193Z","shell.execute_reply":"2022-03-07T16:46:06.659496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Load model\n# models = load_trained_models()\n\n# # Make submission\n# import ubiquant\n# # env = ubiquant.make_env()\n# # iter_test = env.iter_test()\n# for (df_test, df_submission) in iter_test:\n#     df_test, features = create_features(df_test)\n#     df_submission['target'] = predict(models, df_test, features)\n#     env.predict(df_submission) ","metadata":{"execution":{"iopub.status.busy":"2022-03-07T17:05:56.937516Z","iopub.execute_input":"2022-03-07T17:05:56.938271Z","iopub.status.idle":"2022-03-07T17:05:57.190252Z","shell.execute_reply.started":"2022-03-07T17:05:56.938219Z","shell.execute_reply":"2022-03-07T17:05:57.189403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model =  TabNetRegressor()\nbest_model.load_model('./tabnet_fold1.zip')","metadata":{"execution":{"iopub.status.busy":"2022-03-08T18:36:16.052252Z","iopub.execute_input":"2022-03-08T18:36:16.052513Z","iopub.status.idle":"2022-03-08T18:36:16.104351Z","shell.execute_reply.started":"2022-03-08T18:36:16.052486Z","shell.execute_reply":"2022-03-08T18:36:16.103487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test()\nfor (df_test, df_submission) in iter_test:\n    df_test, features = create_features(df_test)\n    pred = best_model.predict(df_test[features].values)\n    df_submission['target']=pred\n    env.predict(df_submission) ","metadata":{"execution":{"iopub.status.busy":"2022-03-08T18:36:30.704074Z","iopub.execute_input":"2022-03-08T18:36:30.704329Z","iopub.status.idle":"2022-03-08T18:36:30.849625Z","shell.execute_reply.started":"2022-03-08T18:36:30.7043Z","shell.execute_reply":"2022-03-08T18:36:30.848884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub=pd.read_csv('./submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-08T18:47:09.815956Z","iopub.execute_input":"2022-03-08T18:47:09.816781Z","iopub.status.idle":"2022-03-08T18:47:09.825005Z","shell.execute_reply.started":"2022-03-08T18:47:09.816742Z","shell.execute_reply":"2022-03-08T18:47:09.824093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-08T18:47:20.484019Z","iopub.execute_input":"2022-03-08T18:47:20.484289Z","iopub.status.idle":"2022-03-08T18:47:20.500225Z","shell.execute_reply.started":"2022-03-08T18:47:20.484261Z","shell.execute_reply":"2022-03-08T18:47:20.4995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"execution":{"iopub.status.busy":"2022-03-08T18:47:31.106596Z","iopub.execute_input":"2022-03-08T18:47:31.106889Z","iopub.status.idle":"2022-03-08T18:47:31.116347Z","shell.execute_reply.started":"2022-03-08T18:47:31.106857Z","shell.execute_reply":"2022-03-08T18:47:31.115535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Please upvoke, if useful for you","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}