{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nplt.rcParams['figure.figsize'] = (10, 9)\nplt.style.use('seaborn-darkgrid')\nsns.set_style('darkgrid')\n\nfrom scipy.stats import pearsonr\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\n\nfrom warnings import filterwarnings, simplefilter\nfilterwarnings('ignore')\nsimplefilter('ignore')\n\nfrom tqdm.auto import tqdm\nfrom tqdm.keras import TqdmCallback\n\nimport gc\ngc.enable()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try :\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver().connect()\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept :\n    strategy = tf.distribute.get_strategy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_dataset(x, y = None, batch_size = 512, shuffle = False) :\n    def preprocess(x, y) :\n        return x, y\n    feat = [col for col in x.columns if col.startswith('f_')]\n    feature = x[feat].values\n    dataset = tf.data.Dataset.from_tensor_slices((\n        feature, y\n    )).map(preprocess).batch(batch_size = batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    if shuffle :\n        dataset = dataset.shuffle(500, seed = 50)\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def correlationMetric(x, y, axis=-2):\n    from tensorflow.python.ops import math_ops\n    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xvar * yvar)\n    return corr\n\n\ndef correlationLoss(x,y, axis=-2):\n    from tensorflow.python.ops import math_ops\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xsqsum * ysqsum)\n    return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr ) , dtype=tf.float32 )\n\ndef build_model() :\n    with strategy.scope() :\n        inputs = keras.layers.Input(shape = (300, ), dtype = tf.float16)\n        x = keras.layers.Dense(256, activation = 'swish')(inputs)\n        x = keras.layers.Dropout(.1)(x)\n\n        x = keras.layers.Reshape((-1, 1))(x)\n        x = keras.layers.Conv1D(16, 4, strides = 1, padding = 'same')(x)\n        x = keras.layers.MaxPool1D()(x)\n        x = keras.layers.BatchNormalization()(x)\n\n        x = keras.layers.Conv1D(32, 4, padding = 'same')(x)\n        x = keras.layers.MaxPool1D()(x)\n        x = keras.layers.BatchNormalization()(x)\n\n        x = keras.layers.Conv1D(64, 4, padding = 'same')(x)\n        x = keras.layers.MaxPool1D()(x)\n        x = keras.layers.BatchNormalization()(x)\n\n        x = keras.layers.Conv1D(128, 4, padding = 'same')(x)\n        x = keras.layers.MaxPool1D()(x)\n        x = keras.layers.BatchNormalization()(x)\n\n        x = keras.layers.Conv1D(128, 4, padding = 'same')(x)\n        x = keras.layers.MaxPool1D()(x)\n        x = keras.layers.BatchNormalization()(x)\n\n        x = keras.layers.Flatten()(x)\n\n        x1 = keras.layers.Dense(256, activation = 'swish', kernel_regularizer = 'l2')(x)\n        x2 = keras.layers.Dense(128, activation = 'swish', kernel_regularizer=  'l2')(x1)\n\n        x = keras.layers.Concatenate(axis = -1)([x1, x2])\n        x = keras.layers.Dense(64, activation = 'swish', kernel_regularizer = 'l2')(x)\n        out = keras.layers.Dense(1, activation = 'linear')(x)\n\n        model = keras.models.Model(\n            inputs = inputs,\n            outputs = out\n        )\n        model.compile(\n            optimizer = keras.optimizers.Adam(learning_rate = 7e-4),\n            loss = correlationLoss,\n            metrics = [correlationMetric, keras.metrics.RootMeanSquaredError(name = 'rmse')]\n        )\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model()\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_index = [\n    (\n        train.loc[(train.time_id > 800) & (train.time_id <= 1000)].index.values,\n        train.loc[(train.time_id > 1000)].index.values\n    ),\n    (\n        train.loc[(train.time_id > 900) & (train.time_id <= 1100)].index.values,\n        train.loc[(train.time_id > 1100)].index.values\n    ),\n    (\n        train.loc[(train.time_id > 1000) & (train.time_id <= 1200)].index.values,\n        train.loc[(train.time_id > 1200)].index.values\n    ),\n    (\n        train.loc[(train.time_id > 800) & (train.time_id <= 1200)].index.values,\n        train.loc[(train.time_id > 1200)].index.values\n    )\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train.pop('target')\ny","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dir = \"./mnist_model\"\n\nlocalhost_save_option = tf.saved_model.SaveOptions(experimental_io_device=\"/job:localhost\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, (t, v) in enumerate(cv_index) :\n    xtrain = train.iloc[t, :]\n    xval = train.iloc[v, :]\n    ytrain = y.iloc[t]\n    yval = y.iloc[v]\n    \n    gc.collect()\n    K.clear_session()\n    model = build_model()\n    cb = [\n        keras.callbacks.EarlyStopping(patience = 14, restore_best_weights = True),\n        keras.callbacks.ReduceLROnPlateau(patience = 4, factor = .3, min_lr = 1e-5),\n        TqdmCallback(verbose = 1)\n    ]\n    train_ds = make_dataset(xtrain, ytrain, batch_size = 512, shuffle = True)\n    val_ds = make_dataset(xval, yval, batch_size = 512, shuffle = False)\n    del xtrain, ytrain\n    history = model.fit(\n        train_ds, validation_data = val_ds,\n        callbacks = cb, epochs = 250, verbose = 0\n    )\n    history = pd.DataFrame(history.history).loc[2:, ['val_loss', 'loss']].plot.line(figsize = (10, 9))\n    plt.show()\n    yhat = model.predict(val_ds).ravel()\n    score, p = pearsonr(yval, yhat)\n    model.save_weights(f'cnn_model_fold_{i}', options=localhost_save_option)\n    print(f'Pearson : {score}')\n    print(f'p-value : {p}')\n    del xval, yval, history, model\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}