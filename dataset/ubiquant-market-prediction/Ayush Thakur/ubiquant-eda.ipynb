{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is a quick stab at understanding the dataset and might be useful for folks who are starting out with this competition, are new to time-series (like me) or want a quick look at the fundamentals of the data.","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-01-19T22:22:43.350603Z","iopub.execute_input":"2022-01-19T22:22:43.351214Z","iopub.status.idle":"2022-01-19T22:22:43.378528Z","shell.execute_reply.started":"2022-01-19T22:22:43.351062Z","shell.execute_reply":"2022-01-19T22:22:43.377608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Dataset\n\nUsing parquet format of the dataset allows for fast loading with lower memory footprint. Thanks to Rob and check out his kernel here: https://www.kaggle.com/robikscube/fast-data-loading-and-low-mem-with-parquet-files","metadata":{}},{"cell_type":"code","source":"df = pd.read_parquet('../input/ubiquant-parquet/train.parquet')\ndf","metadata":{"execution":{"iopub.status.busy":"2022-01-19T22:23:37.836926Z","iopub.execute_input":"2022-01-19T22:23:37.837594Z","iopub.status.idle":"2022-01-19T22:24:47.975144Z","shell.execute_reply.started":"2022-01-19T22:23:37.837536Z","shell.execute_reply":"2022-01-19T22:24:47.973068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> It takes time to load the `train.csv` file and usually the kernel crashes in the process of doing so.","metadata":{}},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"print('Number of rows in the train.csv file: ', len(df))","metadata":{"execution":{"iopub.status.busy":"2022-01-19T22:26:41.887922Z","iopub.execute_input":"2022-01-19T22:26:41.888788Z","iopub.status.idle":"2022-01-19T22:26:41.897449Z","shell.execute_reply.started":"2022-01-19T22:26:41.888736Z","shell.execute_reply":"2022-01-19T22:26:41.896419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `time_id`","metadata":{}},{"cell_type":"code","source":"df.time_id.unique()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T22:26:44.760857Z","iopub.execute_input":"2022-01-19T22:26:44.761433Z","iopub.status.idle":"2022-01-19T22:26:44.794965Z","shell.execute_reply.started":"2022-01-19T22:26:44.761391Z","shell.execute_reply":"2022-01-19T22:26:44.794332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df.time_id.unique())","metadata":{"execution":{"iopub.status.busy":"2022-01-19T22:26:46.016484Z","iopub.execute_input":"2022-01-19T22:26:46.017143Z","iopub.status.idle":"2022-01-19T22:26:46.047562Z","shell.execute_reply.started":"2022-01-19T22:26:46.017087Z","shell.execute_reply":"2022-01-19T22:26:46.046449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> `time_id`: The ID code for the time the data was gathered. The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set. \n\n> Yes the IDs are in order from 0-1219 with 8 missing (?) time_ids.\n\n> One time id may belong to 1st Jan 2:00 IST, the next one can be 4th Jan 12:00 IST, the other one 5th Jan 16:00 IST and so on.","metadata":{}},{"cell_type":"code","source":"df.time_id.value_counts().sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T22:26:48.677972Z","iopub.execute_input":"2022-01-19T22:26:48.678803Z","iopub.status.idle":"2022-01-19T22:26:48.716043Z","shell.execute_reply.started":"2022-01-19T22:26:48.678756Z","shell.execute_reply":"2022-01-19T22:26:48.714959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Clearly the number of data points (rows) in each `time_id` is not constant. ","metadata":{}},{"cell_type":"code","source":"missing_time_ids = []\nfor t in range(1220):\n    if t not in df.time_id.unique():\n        missing_time_ids.append(t)\n        \nprint('Missing time_ids: ', missing_time_ids)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T22:26:52.998976Z","iopub.execute_input":"2022-01-19T22:26:52.999291Z","iopub.status.idle":"2022-01-19T22:27:15.182255Z","shell.execute_reply.started":"2022-01-19T22:26:52.999258Z","shell.execute_reply":"2022-01-19T22:27:15.181109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The following `time_id`s are not present. I don't think it should be an issue since we anyway don't have a constant gap between consecutive `time_id`s. ","metadata":{}},{"cell_type":"markdown","source":"### `row_id`","metadata":{}},{"cell_type":"code","source":"len(df.row_id.unique()) == len(df)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T22:27:19.022369Z","iopub.execute_input":"2022-01-19T22:27:19.022693Z","iopub.status.idle":"2022-01-19T22:27:20.420537Z","shell.execute_reply.started":"2022-01-19T22:27:19.02266Z","shell.execute_reply":"2022-01-19T22:27:20.419851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> `row_id`: It's a unique identifier for each row. The id is in the format of `x_y` where `x` is the unique `time_id` and `y` is the unique `investment_id`.","metadata":{}},{"cell_type":"markdown","source":"### `investment_id`","metadata":{}},{"cell_type":"code","source":"unique_investments = sorted(df.investment_id.unique())\nprint('Number of investment ids: ', len(unique_investments))","metadata":{"execution":{"iopub.status.busy":"2022-01-19T22:27:26.716416Z","iopub.execute_input":"2022-01-19T22:27:26.716857Z","iopub.status.idle":"2022-01-19T22:27:26.767709Z","shell.execute_reply.started":"2022-01-19T22:27:26.716815Z","shell.execute_reply":"2022-01-19T22:27:26.766647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.investment_id.value_counts().sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T22:27:29.54783Z","iopub.execute_input":"2022-01-19T22:27:29.548737Z","iopub.status.idle":"2022-01-19T22:27:29.66294Z","shell.execute_reply.started":"2022-01-19T22:27:29.548684Z","shell.execute_reply":"2022-01-19T22:27:29.661745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Total number of unique investments are 3579 while the last `investment_id` is 3773. There must be missing `investment_id`s. \n\n> I don't think this to be an issue as well. ","metadata":{}},{"cell_type":"code","source":"missing_investment_ids = []\nfor iid in range(3774):\n    if iid not in df.investment_id.unique():\n        missing_investment_ids.append(iid)\n        \nprint('Missing investment_ids: ', missing_investment_ids)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T22:27:32.646061Z","iopub.execute_input":"2022-01-19T22:27:32.64643Z","iopub.status.idle":"2022-01-19T22:29:42.341183Z","shell.execute_reply.started":"2022-01-19T22:27:32.646392Z","shell.execute_reply":"2022-01-19T22:29:42.339992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The following `investment_id`s are not present.","metadata":{}},{"cell_type":"code","source":"df.groupby('time_id')['investment_id'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T22:33:41.592089Z","iopub.execute_input":"2022-01-19T22:33:41.592607Z","iopub.status.idle":"2022-01-19T22:33:41.899994Z","shell.execute_reply.started":"2022-01-19T22:33:41.59256Z","shell.execute_reply":"2022-01-19T22:33:41.899005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> We can see that not all investment have data in all time IDs.","metadata":{}},{"cell_type":"markdown","source":"## Let's look at all the `investment_id`s in a single `time_id`. \n\nNote that few `investment_id`s may be missing in a given `time_id`.","metadata":{}},{"cell_type":"code","source":"sample_time_id = 0\nassert sample_time_id not in missing_time_ids\n\nsample_df = df[df.time_id == sample_time_id]\nsample_df","metadata":{"execution":{"iopub.status.busy":"2022-01-19T22:33:44.849468Z","iopub.execute_input":"2022-01-19T22:33:44.849816Z","iopub.status.idle":"2022-01-19T22:33:44.899826Z","shell.execute_reply.started":"2022-01-19T22:33:44.849775Z","shell.execute_reply":"2022-01-19T22:33:44.898959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_df.investment_id.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T22:33:49.520431Z","iopub.execute_input":"2022-01-19T22:33:49.520968Z","iopub.status.idle":"2022-01-19T22:33:49.53112Z","shell.execute_reply.started":"2022-01-19T22:33:49.520931Z","shell.execute_reply":"2022-01-19T22:33:49.53025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> There's one `investment_id` per `time_id`.","metadata":{}},{"cell_type":"markdown","source":"## Let's look at a single `investment_id` across `time_id`s. ","metadata":{}},{"cell_type":"code","source":"sample_investment_id = 30\nassert sample_investment_id not in missing_investment_ids\n\nsample_df = df[df.investment_id == sample_investment_id]\nsample_df","metadata":{"execution":{"iopub.status.busy":"2022-01-19T22:33:53.521035Z","iopub.execute_input":"2022-01-19T22:33:53.522146Z","iopub.status.idle":"2022-01-19T22:33:53.787265Z","shell.execute_reply.started":"2022-01-19T22:33:53.522098Z","shell.execute_reply":"2022-01-19T22:33:53.786326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,6));\nsample_df.set_index('time_id').target.plot();","metadata":{"execution":{"iopub.status.busy":"2022-01-19T22:33:58.28786Z","iopub.execute_input":"2022-01-19T22:33:58.288149Z","iopub.status.idle":"2022-01-19T22:33:58.695147Z","shell.execute_reply.started":"2022-01-19T22:33:58.288118Z","shell.execute_reply":"2022-01-19T22:33:58.693936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Clearly there is a time series trend when we look at an `investment_id` across time. \n\n> There are missing `time_id`s which is needed to be handled. \n\n> Clearly the `target` values are not scaled but we will be using LightGBM so scaling the data is not crucial. ","metadata":{}},{"cell_type":"markdown","source":"# Using Time Series API","metadata":{}},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\nfor (test_df, sample_prediction_df) in iter_test:\n    print(test_df)\n    sample_prediction_df['target'] = 0  # make your predictions here\n    env.predict(sample_prediction_df)   # register your predictions","metadata":{"execution":{"iopub.status.busy":"2022-01-19T22:34:08.339311Z","iopub.execute_input":"2022-01-19T22:34:08.339866Z","iopub.status.idle":"2022-01-19T22:34:08.524837Z","shell.execute_reply.started":"2022-01-19T22:34:08.339818Z","shell.execute_reply":"2022-01-19T22:34:08.523976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> We get dataframes with shape `(n row x 302 columns)` where `n rows` are`row_id`s. Each `row_id` belong to the same `time_id` so at each iteration we get data for different `investment_id`s. So we need to predict the targets for each `investment_id`s for the given `time_id`. ","metadata":{}},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"> Features `f_0` to `f_299` are features for the model per `time_id`. \n\n> `investment_id` can be a feature, feature with extra weightage, handled seperately by individual models (but then there will be a lot of models) or part of the feature vector for the same model. ","metadata":{}}]}