{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This competition is challenging because of plethora of features and for the simple fact that it's financial data. There can be multiple ways of approaching this problem statement but there are two top-level approaches.\n\n* Treat it like a regular regression task. Here features (f_0 - f_299 + investment_id) are not time bound. \n* Treat it literally like a forecasting problem where temporal relationship between features should be modeled along side. \n\nOne popular choice is going to be [LightGBM](https://lightgbm.readthedocs.io/en/latest/). Since there are multiple permutation and combination (P&C) of hyperparameters we can try, this kernel is about how [Weights and Biases](https://lightgbm.readthedocs.io/en/latest/) can be useful to keep sanity while juggling with so many hyperparameters.\n\nIn this kernel, we approach the problem as a regular regression task and will use LightGBM to model the data distribution. ","metadata":{}},{"cell_type":"markdown","source":"# Imports and Setup\nLet's get the latest version of W&B. ","metadata":{}},{"cell_type":"code","source":"!pip -qq install --upgrade wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-24T07:51:44.52896Z","iopub.execute_input":"2022-01-24T07:51:44.529342Z","iopub.status.idle":"2022-01-24T07:51:53.976878Z","shell.execute_reply.started":"2022-01-24T07:51:44.529245Z","shell.execute_reply":"2022-01-24T07:51:53.975765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\n# W&B\nimport wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    anonymous = None\nexcept:\n    anonymous = \"must\"\n    print('To use your W&B account,\\nGo to Add-ons -> Secrets and provide your W&B access token. Use the Label name as WANDB. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"execution":{"iopub.status.busy":"2022-01-24T08:11:07.765504Z","iopub.execute_input":"2022-01-24T08:11:07.766277Z","iopub.status.idle":"2022-01-24T08:11:09.038725Z","shell.execute_reply.started":"2022-01-24T08:11:07.766235Z","shell.execute_reply":"2022-01-24T08:11:09.038128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we have imported the functions that allows us to log a bunch of useful things (hyperparameters, training and validation metrics, etc.). We can get the most out of LightGBM with these two functions. ","metadata":{}},{"cell_type":"code","source":"from wandb.integration.lightgbm import log_summary, wandb_callback","metadata":{"execution":{"iopub.status.busy":"2022-01-24T08:09:48.924063Z","iopub.execute_input":"2022-01-24T08:09:48.924355Z","iopub.status.idle":"2022-01-24T08:09:48.931355Z","shell.execute_reply.started":"2022-01-24T08:09:48.924313Z","shell.execute_reply":"2022-01-24T08:09:48.930655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset\n\nWe will be using Rob Mulla's highly useful [parquet dataset](https://www.kaggle.com/robikscube/ubiquant-parquet). ","metadata":{}},{"cell_type":"code","source":"df = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T08:09:58.386518Z","iopub.execute_input":"2022-01-24T08:09:58.387352Z","iopub.status.idle":"2022-01-24T08:10:40.421911Z","shell.execute_reply.started":"2022-01-24T08:09:58.387301Z","shell.execute_reply":"2022-01-24T08:10:40.420783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just to simplify things we will be looking at one `investment_id`. Note that for a given `investment_id` not all `time_id`s are available. But since we ain't concerned about time (temporal relationship, for now), we can proceed without second thoughts. ","metadata":{}},{"cell_type":"code","source":"investment_id = 100\ninvestment_df = df[df.investment_id == investment_id]\nprint(f'Number of rows: {len(investment_df)}')\ninvestment_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T08:11:16.666444Z","iopub.execute_input":"2022-01-24T08:11:16.66712Z","iopub.status.idle":"2022-01-24T08:11:16.748284Z","shell.execute_reply.started":"2022-01-24T08:11:16.667076Z","shell.execute_reply":"2022-01-24T08:11:16.747502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple Model\n\nIn this section, we will not use any fancy stratified K fold training. Since the purpose of this kernel is to also show-off the LightGBM x W&B functionalities, it's best if we train a LightGBM Regressor on a standard train-validation split. \n\nNote that if you are training on the entire dataset, `investment_id` can be a feature.","metadata":{}},{"cell_type":"markdown","source":"### Create train-validation split\n\nHere we are not creating a random split. For now we are trying to not look at the future. You will see in few momemt, why I did this even though we are not concerned about temporal dependence. :)","metadata":{}},{"cell_type":"code","source":"features = [feat for feat in investment_df.columns if 'f' in feat]\n\nval_split = 0.2\nval_split_index = len(investment_df) - int(len(investment_df)*val_split)\nprint(f'Number of rows: {len(investment_df)}, number of train rows: {val_split_index}')\n\ntrain_df, valid_df = investment_df[:val_split_index], investment_df[val_split_index:]\ntrain_X, train_y = train_df[features].values, train_df.target.values\nvalid_X, valid_y = valid_df[features].values, valid_df.target.values","metadata":{"execution":{"iopub.status.busy":"2022-01-24T08:11:19.990854Z","iopub.execute_input":"2022-01-24T08:11:19.991515Z","iopub.status.idle":"2022-01-24T08:11:20.001566Z","shell.execute_reply.started":"2022-01-24T08:11:19.99147Z","shell.execute_reply":"2022-01-24T08:11:20.000957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LightGBM dataset","metadata":{}},{"cell_type":"code","source":"lgb_train = lgb.Dataset(train_X, train_y)\nlgb_valid = lgb.Dataset(valid_X, valid_y, reference=lgb_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T08:11:59.717295Z","iopub.execute_input":"2022-01-24T08:11:59.717631Z","iopub.status.idle":"2022-01-24T08:11:59.722277Z","shell.execute_reply.started":"2022-01-24T08:11:59.717595Z","shell.execute_reply":"2022-01-24T08:11:59.721434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train \n\n1. Pass `wandb_callback` to the `callbacks` argument of `fit`. This will:\n    - log params passed to lightgbm.train as W&B config.\n    - log evaluation metrics collected by LightGBM, such as rmse, accuracy etc to Weights & Biases\n    - Capture the best metrics.\n\n\n2. Once the model is trained, use `log_summary` to:\n    - log `best_iteration` and `best_score`.\n    - log feature importance plot.\n    - save and upload your best trained model to Weights & Biases Artifacts (when `save_model_checkpoint = True`)\n\n\nNotes: \n> You can use `LGBMRegressor` and it will work the same way. \n\n> The parameters used in the cell below may/may not be optimal. \n","metadata":{}},{"cell_type":"code","source":"# Define the parameters for LightGBM\nparams = {\n     'boosting_type': 'gbdt',\n     'objective': 'regression',\n     'metric': ['rmse', 'l2', 'l1', 'mae'],\n     'num_leaves': 8,\n     'learning_rate': 0.1,\n     'feature_fraction': 0.7,\n     'bagging_fraction': 0.8,\n     'bagging_freq': 5,\n}\n\n# 1️⃣ Initialize a new wandb project\nrun = wandb.init(project='ubiquant-lgb', job_type=f'train_{investment_id}')\n\n# 2️⃣ Train with `wandb_callback`.\ngbm = lgb.train(params,\n             lgb_train,\n             num_boost_round=100,\n             valid_sets=[lgb_train, lgb_valid],\n             valid_names=('validation'),\n             callbacks=[wandb_callback()],\n             early_stopping_rounds = 10)\n\n# 3️⃣ Use `log_summary` to get feature importance, best score, etc.\nlog_summary(gbm, save_model_checkpoint=False)\n\n# 4️⃣ End the run (needed in Jupyter sessions)\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T08:12:11.779427Z","iopub.execute_input":"2022-01-24T08:12:11.780183Z","iopub.status.idle":"2022-01-24T08:12:32.988022Z","shell.execute_reply.started":"2022-01-24T08:12:11.780143Z","shell.execute_reply":"2022-01-24T08:12:32.987036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What do you get by using W&B?\n\n1. The hyperparameters used to trained your model is saved as W&B Config. \n![img](https://i.imgur.com/fGeGj5T.png)\n\n2. Track train/val/test metrics. \n![img](https://i.imgur.com/1cdlUIT.png)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WORK IN PROGRESS","metadata":{}}]}