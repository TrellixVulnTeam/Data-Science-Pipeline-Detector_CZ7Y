{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this kernel, I have compared **three of the best baseline kernels** we have seen in this competition so far. \n\nBeing a beginner in modeling time-series data, I decided to learn from the experienced and setup a useful training+inference workflow in the process. Thus most of the credits for the code goes to the authors of those kernels. \n\nI have used the following baseline kernels:\n* [Ubiquant Market Prediction with DNN](https://www.kaggle.com/lonnieqin/ubiquant-market-prediction-with-dnn/notebook) by [Lonnie](https://www.kaggle.com/lonnieqin).\n* [Ubiquant RNN - Training Pipeline and Inference](https://www.kaggle.com/ravishah1/ubiquant-rnn-training-pipeline-and-inference/notebook) by  [Ravi Shah](https://www.kaggle.com/ravishah1).\n* [Ubiquant LGBM Baseline](https://www.kaggle.com/valleyzw/ubiquant-lgbm-baseline) by [valley](https://www.kaggle.com/valleyzw).\n\nThis notebook uses [Weights and Biases](https://wandb.ai/site) to compare the three modeling techniques. If you are using these baselines, this kernel can be a good place to learn how W&B can be used.","metadata":{}},{"cell_type":"markdown","source":"# Imports and Setup","metadata":{}},{"cell_type":"code","source":"!pip install -qq --upgrade wandb","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:12:54.453165Z","iopub.execute_input":"2022-01-31T19:12:54.453911Z","iopub.status.idle":"2022-01-31T19:13:03.550494Z","shell.execute_reply.started":"2022-01-31T19:12:54.453817Z","shell.execute_reply":"2022-01-31T19:13:03.549602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pathlib import Path\nfrom argparse import Namespace\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow import keras\n\nfrom scipy import stats\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold\n\nimport lightgbm as lgb\n\nimport wandb\nfrom wandb.lightgbm import log_summary, wandb_callback\nfrom wandb.keras import WandbCallback","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:13:03.55301Z","iopub.execute_input":"2022-01-31T19:13:03.553296Z","iopub.status.idle":"2022-01-31T19:13:11.739895Z","shell.execute_reply.started":"2022-01-31T19:13:03.553236Z","shell.execute_reply":"2022-01-31T19:13:11.739066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=secret_value_0)\n\n    anony=None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:13:11.741416Z","iopub.execute_input":"2022-01-31T19:13:11.741666Z","iopub.status.idle":"2022-01-31T19:13:14.653337Z","shell.execute_reply.started":"2022-01-31T19:13:11.741624Z","shell.execute_reply":"2022-01-31T19:13:14.652642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:13:14.655368Z","iopub.execute_input":"2022-01-31T19:13:14.655559Z","iopub.status.idle":"2022-01-31T19:13:16.671355Z","shell.execute_reply.started":"2022-01-31T19:13:14.655534Z","shell.execute_reply":"2022-01-31T19:13:16.668057Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset\n\nWe will be using the [Ubiquant Market Prediction half precision Pickle](https://www.kaggle.com/lonnieqin/ubiquant-market-prediction-half-precision-pickle) by Lonnie. You can find more information about the creation of this dataset in this [kernel](https://www.kaggle.com/lonnieqin/reduce-the-dataset-to-1-8g).","metadata":{}},{"cell_type":"code","source":"df = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:13:16.672904Z","iopub.execute_input":"2022-01-31T19:13:16.673229Z","iopub.status.idle":"2022-01-31T19:13:32.573094Z","shell.execute_reply.started":"2022-01-31T19:13:16.673187Z","shell.execute_reply":"2022-01-31T19:13:32.572458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [f'f_{i}' for i in range(300)]\ntarget = 'target'\nEPOCHS = 100","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:13:32.57534Z","iopub.execute_input":"2022-01-31T19:13:32.57573Z","iopub.status.idle":"2022-01-31T19:13:32.580019Z","shell.execute_reply.started":"2022-01-31T19:13:32.575694Z","shell.execute_reply":"2022-01-31T19:13:32.579149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DNN\n\nSource: https://www.kaggle.com/lonnieqin/ubiquant-market-prediction-with-dnn/notebook","metadata":{}},{"cell_type":"code","source":"def make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n    if mode == \"train\":\n        ds = ds.shuffle(buffer_size=batch_size*8)\n    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:13:32.581426Z","iopub.execute_input":"2022-01-31T19:13:32.58171Z","iopub.status.idle":"2022-01-31T19:13:32.589845Z","shell.execute_reply.started":"2022-01-31T19:13:32.581675Z","shell.execute_reply":"2022-01-31T19:13:32.589188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_investment_lookup(investment_id_df):\n    investment_ids = list(investment_id_df.unique())\n    investment_id_size = len(investment_ids) + 1\n    investment_id_lookup_layer = IntegerLookup(max_tokens=investment_id_size)\n    investment_id_lookup_layer.adapt(pd.DataFrame({\"investment_ids\": investment_ids}))\n    \n    return investment_id_lookup_layer, investment_id_size\n\ndef get_model(investment_id_df):\n    investment_id_inputs = Input((1, ), dtype=tf.uint16)\n    features_inputs = Input((300, ), dtype=tf.float16)\n    \n    investment_id_lookup_layer, investment_id_size = get_investment_lookup(investment_id_df)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = Reshape((-1, ))(investment_id_x)\n    investment_id_x = Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = Dense(64, activation='swish')(investment_id_x)\n    \n    feature_x = Dense(256, activation='swish')(features_inputs)\n    feature_x = Dense(256, activation='swish')(feature_x)\n    feature_x = Dense(256, activation='swish')(feature_x)\n    \n    x = Concatenate(axis=1)([investment_id_x, feature_x])\n    x = Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    output = Dense(1)(x)\n    model = Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    \n    rmse = tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', rmse])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:13:32.591043Z","iopub.execute_input":"2022-01-31T19:13:32.59147Z","iopub.status.idle":"2022-01-31T19:13:32.605405Z","shell.execute_reply.started":"2022-01-31T19:13:32.591433Z","shell.execute_reply":"2022-01-31T19:13:32.604643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nmodel = get_model(df[\"investment_id\"])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:13:32.606718Z","iopub.execute_input":"2022-01-31T19:13:32.607009Z","iopub.status.idle":"2022-01-31T19:13:33.137757Z","shell.execute_reply.started":"2022-01-31T19:13:32.606972Z","shell.execute_reply":"2022-01-31T19:13:33.137054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = StratifiedKFold(5, shuffle=True, random_state=42)\nmodels = []\n\nearly_stop = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n\nfor fold, (train_indices, valid_indices) in enumerate(kfold.split(df[features], df[\"investment_id\"])):\n    # Prepare dataset\n    X_train, y_train = df[features].iloc[train_indices], df[\"target\"].iloc[train_indices]\n    X_val, y_val = df[features].iloc[valid_indices], df[\"target\"].iloc[valid_indices]\n    invest_train, invest_val = df[\"investment_id\"][train_indices], df[\"investment_id\"][valid_indices]\n    \n    # Get Dataloaders\n    train_ds = make_dataset(X_train, invest_train, y_train)\n    valid_ds = make_dataset(X_val, invest_val, y_val, mode=\"valid\")\n\n    # Get Model\n    model = get_model(df[\"investment_id\"])\n    \n    # Initialize W&B run\n    run = wandb.init(project='ubiquant_kaggle', group='DNN', job_type='train')\n    \n    # Train model\n    _ = model.fit(train_ds,\n                  epochs=EPOCHS,\n                  validation_data=valid_ds,\n                  callbacks=[WandbCallback(save_model=False), early_stop])\n    \n    # Evaluate\n    preds = model.predict(valid_ds)\n    \n    # Save the model\n    model.save(f'DNN/models/model_{fold}')\n    \n    # Get rmse score\n    rmse_score = np.sqrt(mean_squared_error(y_val.values, preds.ravel()))\n    wandb.log({'oof_rmse': rmse_score})\n\n    # Get pearson score\n    pearson_score = stats.pearsonr(preds.ravel(), y_val.values)[0]\n    wandb.log({'oof_pearsonr': pearson_score})\n    \n    # Clear W&B run\n    wandb.finish()\n    \n    del invest_train, invest_val, X_train, X_val, y_train, y_val, train_ds, valid_ds\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:13:33.139971Z","iopub.execute_input":"2022-01-31T19:13:33.140307Z","iopub.status.idle":"2022-01-31T19:18:46.124552Z","shell.execute_reply.started":"2022-01-31T19:13:33.140253Z","shell.execute_reply":"2022-01-31T19:18:46.123766Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run = wandb.init(project='ubiquant_kaggle', group='DNN', job_type='save_model')\nmodel_artifact = wandb.Artifact(name='Baseline_DNN', type='dnn_model')\nmodel_artifact.add_dir('DNN/models')\nrun.log_artifact(model_artifact)\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T17:39:46.025235Z","iopub.execute_input":"2022-01-31T17:39:46.025522Z","iopub.status.idle":"2022-01-31T17:39:58.244348Z","shell.execute_reply.started":"2022-01-31T17:39:46.025486Z","shell.execute_reply":"2022-01-31T17:39:58.243602Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RNN\n\nSource: https://www.kaggle.com/ravishah1/ubiquant-rnn-training-pipeline-and-inference/notebook","metadata":{}},{"cell_type":"code","source":"def setup_cv(df, X, y, groups, splits=5):\n    kf = GroupKFold(n_splits=splits)\n    for f, (t_, v_) in enumerate(kf.split(X=X, y=y, groups=groups)):\n            df.loc[v_, 'fold'] = f\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:24:44.366878Z","iopub.execute_input":"2022-01-31T19:24:44.36713Z","iopub.status.idle":"2022-01-31T19:24:44.371943Z","shell.execute_reply.started":"2022-01-31T19:24:44.367101Z","shell.execute_reply":"2022-01-31T19:24:44.37128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_rnn_v2():\n    f300_in = Input(shape=(300,), name='300 feature input')\n    x = BatchNormalization(name='batch_norm1')(f300_in)\n    x = Dense(256, activation='swish', name='dense1')(x)\n    x = Dropout(0.1, name='dropout1')(x)\n    x = Reshape((1, -1), name='reshape1')(x)\n    x = BatchNormalization(name='batch_norm2')(x)\n    x = LSTM(128, dropout=0.3, recurrent_dropout=0.3, return_sequences=True, activation='relu', name='lstm1')(x)\n    x = LSTM(16, dropout=0.1, return_sequences=False, activation='relu', name='lstm2')(x)\n    output_layer = Dense(1, name='output')(x)\n\n    model = Model([f300_in], \n                    [output_layer])\n\n    rmse = tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                  loss='mse', metrics=['mse', rmse])\n\n    return model\n\nclass UbiquantRNNV2:\n    def __init__(self, df: pd.DataFrame, feature_cols: list=None, target: str='target'):\n\n        self.model = get_rnn_v2()\n\n        self.df = df\n\n        if feature_cols is not None:\n            self.feature_cols = feature_cols\n        else:\n            self.feature_cols = [f\"f_{i}\" for i in range(300)]\n\n        self.target_col = target\n\n    def train_one_fold(self, f: int, max_epochs=10, log_wandb=True):\n        X_train = self.df[self.df.fold!=f][self.feature_cols]\n        X_valid = self.df[self.df.fold==f][self.feature_cols]\n\n        y_train = self.df[self.df.fold!=f][self.target_col]\n        y_valid = self.df[self.df.fold==f][self.target_col]\n        \n        if log_wandb:\n            run = wandb.init(project='ubiquant_kaggle', group='RNN')\n\n        self.model.fit(X_train, y_train,\n                       validation_data=(X_valid, y_valid),\n                       batch_size=512, epochs=EPOCHS,\n                       callbacks=[\n                         WandbCallback(save_model=False),\n                         tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_delta=1e-4, mode='min'),\n                         tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, mode='min', baseline=None, restore_best_weights=True)\n            ])\n\n        oof = self.model.predict(X_valid)\n        \n        if log_wandb:\n            wandb.log({'oof_rmse': self.compute_rmse(y_valid, oof)})\n            wandb.log({'oof_pearsonr': self.compute_pearsonr(oof.ravel(), y_valid.values)})\n            wandb.finish()\n            \n        del X_train, X_valid, y_train, y_valid\n        _ = gc.collect()\n\n    def predict(self, X: np.ndarray):\n        preds = self.model.predict(X)\n        return preds\n\n    def save(self, path: str, f: int):\n        self.model.save(f'{path}/model_{f}.h5')\n        \n    def oof_save(self):\n        self.df[['target', 'preds']].to_csv('rnn_oof.csv', index=False)\n        \n    def compute_rmse(self, y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n    \n    def compute_pearsonr(self, y_true, y_pred):\n        return pearsonr(y_pred, y_true)[0]","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:25:08.657233Z","iopub.execute_input":"2022-01-31T19:25:08.657685Z","iopub.status.idle":"2022-01-31T19:25:08.677767Z","shell.execute_reply.started":"2022-01-31T19:25:08.657648Z","shell.execute_reply":"2022-01-31T19:25:08.677077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uneven_group = np.sort(np.random.randint(0, 1000, len(df)))\nfold_df = setup_cv(df, df[features], df[\"investment_id\"], uneven_group)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:25:11.764517Z","iopub.execute_input":"2022-01-31T19:25:11.765094Z","iopub.status.idle":"2022-01-31T19:25:15.521215Z","shell.execute_reply.started":"2022-01-31T19:25:11.765054Z","shell.execute_reply":"2022-01-31T19:25:15.520445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ntrain_ubiquant_rnn = UbiquantRNNV2(df)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:25:18.633892Z","iopub.execute_input":"2022-01-31T19:25:18.634505Z","iopub.status.idle":"2022-01-31T19:25:18.875782Z","shell.execute_reply.started":"2022-01-31T19:25:18.634463Z","shell.execute_reply":"2022-01-31T19:25:18.87492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(5):\n    train_ubiquant_rnn.train_one_fold(fold)\n    train_ubiquant_rnn.save('RNN/models', fold)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:25:21.096799Z","iopub.execute_input":"2022-01-31T19:25:21.097544Z","iopub.status.idle":"2022-01-31T19:38:15.320879Z","shell.execute_reply.started":"2022-01-31T19:25:21.097504Z","shell.execute_reply":"2022-01-31T19:38:15.320146Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run = wandb.init(project='ubiquant_kaggle', group='RNN', job_type='save_model')\nmodel_artifact = wandb.Artifact(name='Baseline_RNN', type='rnn_model')\nmodel_artifact.add_dir('RNN/models')\nrun.log_artifact(model_artifact)\nwandb.finish()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LightGBM","metadata":{}},{"cell_type":"code","source":"args = Namespace(\n    debug=False,\n    seed=21,\n    folds=5,\n    workers=4,\n    min_time_id=None, \n    num_bins=16,\n    data_path=Path(\"parquets\"),\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:43:41.686443Z","iopub.execute_input":"2022-01-31T19:43:41.687076Z","iopub.status.idle":"2022-01-31T19:43:41.691646Z","shell.execute_reply.started":"2022-01-31T19:43:41.687032Z","shell.execute_reply":"2022-01-31T19:43:41.690673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_id_df = (\n    df.filter(regex=r\"^(?!f_).*\")\n    .groupby(\"investment_id\")\n    .agg({\"time_id\": [\"min\", \"max\"]})\n    .reset_index()\n)\ntime_id_df[\"time_span\"] = time_id_df[\"time_id\"].diff(axis=1)[\"max\"]\n\ntrain = df.merge(time_id_df.drop(columns=\"time_id\").droplevel(level=1, axis=1), on=\"investment_id\")\ntrain.time_span.hist(bins=args.num_bins, figsize=(16,8))\ndel time_id_df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:43:44.937848Z","iopub.execute_input":"2022-01-31T19:43:44.938104Z","iopub.status.idle":"2022-01-31T19:43:57.155828Z","shell.execute_reply.started":"2022-01-31T19:43:44.938073Z","shell.execute_reply":"2022-01-31T19:43:57.155094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"fold\"] = -1\n_target = pd.cut(train.time_span, args.num_bins, labels=False)\nskf = StratifiedKFold(n_splits=args.folds)\nfor fold, (train_index, valid_index) in enumerate(skf.split(_target, _target)):\n    train.loc[valid_index, 'fold'] = fold\n    \nfig, axs = plt.subplots(nrows=args.folds, ncols=1, sharex=True, figsize=(16,8), tight_layout=True)\nfor ax, (fold, df) in zip(axs, train[[\"fold\", \"time_span\"]].groupby(\"fold\")):\n    ax.hist(df.time_span, bins=args.num_bins)\n    ax.text(0, 40000, f\"fold: {fold}, count: {len(df)}\", fontsize=16)\nplt.show()\ndel _target, train_index, valid_index\n_=gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:44:09.126653Z","iopub.execute_input":"2022-01-31T19:44:09.127402Z","iopub.status.idle":"2022-01-31T19:44:11.015153Z","shell.execute_reply.started":"2022-01-31T19:44:09.127362Z","shell.execute_reply":"2022-01-31T19:44:11.014339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_features = [\"investment_id\"]\nnum_features = list(train.filter(like=\"f_\").columns)\nfeatures = num_features + cat_features\n\ntrain = train.drop(columns=\"time_span\")\ntrain[[\"investment_id\", \"time_id\"]] = train[[\"investment_id\", \"time_id\"]].astype(np.uint16)\ntrain[\"fold\"] = train[\"fold\"].astype(np.uint8)\ngc.collect()\nfeatures += [\"time_id\"] # https://www.kaggle.com/c/ubiquant-market-prediction/discussion/302429\nlen(features)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:44:12.120869Z","iopub.execute_input":"2022-01-31T19:44:12.121509Z","iopub.status.idle":"2022-01-31T19:44:18.587829Z","shell.execute_reply.started":"2022-01-31T19:44:12.121462Z","shell.execute_reply":"2022-01-31T19:44:18.587115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\ndef feval_rmse(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'rmse', rmse(y_true, y_pred), False\n\n# https://www.kaggle.com/c/ubiquant-market-prediction/discussion/302480\ndef feval_pearsonr(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'pearsonr', pearsonr(y_true, y_pred)[0], True\n\ndef run():    \n    params = {\n        'learning_rate':0.05,\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        'boosting_type': \"gbdt\",\n        'verbosity': -1,\n        'n_jobs': -1, \n        'seed': args.seed,\n        'lambda_l1': 2.7223413643193285e-08, \n        'lambda_l2': 0.009462714717237544, \n        'num_leaves': 108, \n        'feature_fraction': 0.5298125662824026, \n        'bagging_fraction': 0.7279540797730281, \n        'bagging_freq': 6, \n        'max_depth': 10, \n        'max_bin': 487, \n        'min_data_in_leaf': 158,\n        'n_estimators': 1000, \n    }\n    \n    y = train['target']\n    train['preds'] = -1000\n    scores = defaultdict(list)\n    features_importance= pd.DataFrame()\n    \n    for fold in range(args.folds):\n        print(f\"=====================fold: {fold}=====================\")\n        trn_ind, val_ind = train.fold!=fold, train.fold==fold\n        print(f\"train length: {trn_ind.sum()}, valid length: {val_ind.sum()}\")\n        train_dataset = lgb.Dataset(train.loc[trn_ind, features], y.loc[trn_ind], categorical_feature=cat_features)\n        valid_dataset = lgb.Dataset(train.loc[val_ind, features], y.loc[val_ind], categorical_feature=cat_features)\n        \n        run = wandb.init(project='ubiquant_kaggle', group='LGBM')\n        \n        model = lgb.train(\n            params,\n            train_set = train_dataset, \n            valid_sets = [train_dataset, valid_dataset], \n            verbose_eval=100,\n            early_stopping_rounds=50,\n            feval = feval_pearsonr,\n            callbacks=[wandb_callback()]\n        )\n        joblib.dump(model, f'LGBM/lgbm_seed{args.seed}_{fold}.pkl')\n\n        preds = model.predict(train.loc[val_ind, features])\n        train.loc[val_ind, \"preds\"] = preds\n        \n        scores[\"rmse\"].append(rmse(y.loc[val_ind], preds))\n        scores[\"pearsonr\"].append(pearsonr(y.loc[val_ind], preds)[0])\n        \n        fold_importance_df= pd.DataFrame({'feature': features, 'importance': model.feature_importance(), 'fold': fold})\n        features_importance = pd.concat([features_importance, fold_importance_df], axis=0)\n        \n        wandb.log({f'oof_rmse': rmse(y.loc[val_ind], preds),\n                   f'oof_pearsonr': pearsonr(y.loc[val_ind], preds)[0]})\n        \n        del train_dataset, valid_dataset, model\n        gc.collect()\n    print(f\"lgbm {args.folds} folds mean rmse: {np.mean(scores['rmse'])}, mean pearsonr: {np.mean(scores['pearsonr'])}\")\n    train.filter(regex=r\"^(?!f_).*\").to_csv(\"preds.csv\", index=False)\n    return features_importance","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:44:29.807855Z","iopub.execute_input":"2022-01-31T19:44:29.80813Z","iopub.status.idle":"2022-01-31T19:44:29.826404Z","shell.execute_reply.started":"2022-01-31T19:44:29.808099Z","shell.execute_reply":"2022-01-31T19:44:29.82562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_importance = run()\ndf = train[[\"target\", \"preds\"]].query(\"preds!=-1000\")\nprint(f\"lgbm {args.folds} folds mean rmse: {rmse(df.target, df.preds)}, mean pearsonr: {pearsonr(df.target, df.preds)[0]}\")\ndel df, train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T19:44:42.867818Z","iopub.execute_input":"2022-01-31T19:44:42.868071Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run = wandb.init(project='ubiquant_kaggle', group='LGBM', job_type='save_model')\nmodel_artifact = wandb.Artifact(name='Baseline_LGBM', type='lgbm_model')\nmodel_artifact.add_dir('LGBM/')\nrun.log_artifact(model_artifact)\nwandb.finish()","metadata":{},"execution_count":null,"outputs":[]}]}