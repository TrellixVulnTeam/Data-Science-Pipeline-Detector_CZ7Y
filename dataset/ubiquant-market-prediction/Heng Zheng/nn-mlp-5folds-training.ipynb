{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')\n\nimport gc\n\nimport numpy as np\nimport pandas as pd\n\npd.set_option('max_columns', None)\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GroupKFold\n\nimport joblib\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntrain = pd.read_csv('../input/ubiquant-market-prediction/train.csv')\nprint(train.shape)\ntrain.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\ntrain[[f'f_{i}' for i in range(300)]] = scaler.fit_transform(train[[f'f_{i}' for i in range(300)]])\n\ntrain.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UBIQUANT_DATASET(Dataset):\n    def __init__(self, df_data, mode='train'):\n        self.mode = mode\n        self.ids = np.array(df_data['investment_id'].values.tolist(), dtype=np.int64)\n        self.vals = np.array(df_data.iloc[:, 4:].values.tolist(), dtype=np.float64)\n        if self.mode != 'test':\n            self.targets = np.array(df_data['target'].values, dtype=np.float64)\n        self.len = df_data.shape[0]\n        \n    def __len__(self):\n        return self.len\n    \n    def __getitem__(self, index):\n        ids_out = self.ids[index]\n        vals_out = self.vals[index]\n        if self.mode != 'test':\n            targets_out = self.targets[index]\n            return ids_out, vals_out, targets_out\n        else:\n            return ids_out, vals_out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# copy from: https://www.kaggle.com/elcaiseri/pytorch-optiver-realized-volatility-baseline\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nclass SimpleMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.emb = nn.Embedding(3774, 64)\n        self.emb_drop = nn.Dropout(0.1)\n        \n        self.bn1 = nn.BatchNorm1d(300)\n        self.lin1 = nn.Linear(64+300, 32)\n        self.lin2 = nn.Linear(32, 128)\n        self.lin3 = nn.Linear(128, 64)\n        self.lin4 = nn.Linear(64, 32)\n        self.lin_drop = nn.Dropout(0.25)\n        self.lin5 = nn.Linear(32, 1)    \n\n    def forward(self, x_cat, x_cont):\n        x1 = self.emb(x_cat)\n        x1 = self.emb_drop(x1)\n        \n        x2 = self.bn1(x_cont)\n\n        x = torch.cat([x1, x2], 1)\n        x = swish(self.lin1(x))\n        x = swish(self.lin2(x))\n        x = swish(self.lin3(x))\n        x = swish(self.lin4(x))\n        x = self.lin5(x)\n        \n        return x\n    \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(dataloaders, fold_id):\n    \n    model = SimpleMLP().to(device)\n    loss_fn = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), \n                           lr=1e-3)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n                                                     factor=0.1, \n                                                     patience=1, \n                                                     mode='min')\n    \n    epochs = 8\n    num_train_examples = len(dataloaders['train'])\n    num_valid_examples = len(dataloaders['valid'])\n\n    losses = []\n    best_loss = np.inf\n\n    for e in range(epochs):\n        # train\n        model.train()\n        train_loss = 0\n        for i, (ids, vals, targets) in enumerate(dataloaders['train']):\n            ids = ids.to(device)\n            vals = vals.to(device=device, dtype=torch.float)\n            targets = targets.unsqueeze(1).to(device, dtype=torch.float)\n\n            yhat = model(ids, vals)\n            loss = loss_fn(yhat, targets)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        train_epoch_loss = train_loss / num_train_examples\n\n        # valid\n        model.eval()\n        valid_preds = list()\n        valid_loss = 0\n        with torch.no_grad():\n            for i, (ids, vals, targets) in enumerate(dataloaders['valid']):\n                ids = ids.to(device)\n                vals = vals.to(device=device, dtype=torch.float)\n                targets = targets.unsqueeze(1).to(device, dtype=torch.float)\n\n                yhat = model(ids, vals)\n                val_loss = loss_fn(yhat, targets)\n                valid_loss += val_loss.item()\n                valid_preds.extend(yhat.detach().cpu().numpy().flatten())\n        valid_epoch_loss = valid_loss / num_valid_examples\n\n        # change lr\n        scheduler.step(valid_epoch_loss)\n\n        # oof\n        oof = df_valid[['target']].copy()\n        oof['pred'] = valid_preds\n        score = oof['pred'].corr(oof['target'])\n\n        # print score\n        print(f\"Epoch {e}, LR: {optimizer.param_groups[0]['lr']}\")\n        print(f\"train loss: {train_epoch_loss:.8f}, valid loss {valid_epoch_loss:.8f}, pearson score: {score:.6f}\")\n        losses.append((train_epoch_loss, valid_epoch_loss))\n\n        # save model\n        if best_loss > valid_epoch_loss:\n            torch.save(model.state_dict(), f'simple_mlp_model_{fold_id}.pth')\n            print(f'-- loss from {best_loss:.8f} to {valid_epoch_loss:.8f}, model saved')\n            best_loss = valid_epoch_loss\n        print()\n        \n    return losses, oof","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_list = list()\n\nkfold = GroupKFold(n_splits=5)\nfor fold_id, (trn_idx, val_idx) in enumerate(kfold.split(train, train['target'], train['time_id'])):\n    \n    print(f'Training Fold: {fold_id}\\n')\n    \n    df_train = train.iloc[trn_idx]\n    df_valid = train.iloc[val_idx]\n    \n    train_set = UBIQUANT_DATASET(df_train, mode='train')\n    valid_set = UBIQUANT_DATASET(df_valid, mode='valid')\n    dataloaders = {\n        'train': DataLoader(train_set, batch_size=1024, num_workers=4, pin_memory=True, shuffle=True),\n        'valid': DataLoader(valid_set, batch_size=1024, num_workers=4, pin_memory=True, shuffle=False)\n    }\n    \n    _, oof = train_fn(dataloaders, fold_id)\n    oof_list.append(oof)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof = pd.concat(oof_list)\nprint('oof pearson score:', oof['pred'].corr(oof['target']))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joblib.dump(scaler, 'minmaxscaler.pkl')","metadata":{},"execution_count":null,"outputs":[]}]}