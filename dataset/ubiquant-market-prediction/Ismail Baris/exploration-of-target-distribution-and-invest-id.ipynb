{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install openturns","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T21:20:36.629079Z","iopub.execute_input":"2022-02-10T21:20:36.629603Z","iopub.status.idle":"2022-02-10T21:20:50.774692Z","shell.execute_reply.started":"2022-02-10T21:20:36.629561Z","shell.execute_reply":"2022-02-10T21:20:50.773498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploration of Target Distribution\nIn this notebook we will try to explore the distribution of the `target` feature. Since there are a lot of investment IDs and not always 'more data' has positive effects (e.g. on training time), we try to aggregate some investments. Because the assumption is that many investments correlate with each other or correspond to the same investment category, which should lead to the same underlying distribution. The goal is to cluster similar values in order to merge different investment id'.","metadata":{}},{"cell_type":"code","source":"import os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport openturns as ot\nimport pandas as pd\nimport scipy\nimport seaborn as sns\n\nsns.set_style(\"ticks\", {'axes.grid': True})\n\nmarket_data_path = r\"../input/ubiquant-parquet\"\nmodel_data_path = r\"../input/distribution-data-targets-raw\"","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T21:20:52.915941Z","iopub.execute_input":"2022-02-10T21:20:52.916227Z","iopub.status.idle":"2022-02-10T21:20:52.922683Z","shell.execute_reply.started":"2022-02-10T21:20:52.916197Z","shell.execute_reply":"2022-02-10T21:20:52.921982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will create a function in order to ease the data read routine","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def get_data(path: str, investment_id: int = None, columns: list = None):\n    \"\"\"\n    Get the market Data.\n\n    Parameters\n    ----------\n    investment_id : int, optional\n        An investment ID between 0 - 3773. If None (Default) you will get the whole training data.\n    columns : list\n        Specify columns to import. If None (default) all columns will be imported.\n    \"\"\"\n    if investment_id is None:\n        train_path = os.path.join(path, \"train.parquet\")\n\n        return pd.read_parquet(train_path, columns=columns)\n    else:\n        id_path = os.path.join(path, \"investment_ids\", \"{0}.parquet\")\n\n        return pd.read_parquet(id_path.format(investment_id), columns=columns)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T21:18:16.032437Z","iopub.execute_input":"2022-02-10T21:18:16.03268Z","iopub.status.idle":"2022-02-10T21:18:16.040004Z","shell.execute_reply.started":"2022-02-10T21:18:16.032651Z","shell.execute_reply":"2022-02-10T21:18:16.038842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To retrieve the data we use the custom `GetData` class, where we can define to load individual investment `id`Â´.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"target = get_data(market_data_path, columns=[\"target\"])","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T21:18:54.27474Z","iopub.execute_input":"2022-02-10T21:18:54.275035Z","iopub.status.idle":"2022-02-10T21:18:55.215464Z","shell.execute_reply.started":"2022-02-10T21:18:54.275001Z","shell.execute_reply":"2022-02-10T21:18:55.214811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Firstly, we look at the plain `target` data and look at its histogram.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"bins_width = int(180 / 2)\nsns.histplot(target, color='darkblue', stat='density', bins=bins_width, alpha=0.50, label=\"Target\")\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T21:18:57.99341Z","iopub.execute_input":"2022-02-10T21:18:57.993699Z","iopub.status.idle":"2022-02-10T21:19:04.785833Z","shell.execute_reply.started":"2022-02-10T21:18:57.993664Z","shell.execute_reply":"2022-02-10T21:19:04.785044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we will try to fit a `normal distribution`,","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"_, bins = np.histogram(target, bins=bins_width)\n\nparams = scipy.stats.norm.fit(target)\nnorm_fit_line = scipy.stats.norm.pdf(bins, *params)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T21:19:06.752609Z","iopub.execute_input":"2022-02-10T21:19:06.752919Z","iopub.status.idle":"2022-02-10T21:19:06.82448Z","shell.execute_reply.started":"2022-02-10T21:19:06.752882Z","shell.execute_reply":"2022-02-10T21:19:06.823517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"and plot its results:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"sns.histplot(target, color='darkblue', stat='density', bins=bins_width, alpha=0.50, label=\"Target\")\nplt.plot(bins, norm_fit_line, color=\"red\", label=\"Normal Dist.\")\nplt.legend()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T21:19:09.89497Z","iopub.execute_input":"2022-02-10T21:19:09.895508Z","iopub.status.idle":"2022-02-10T21:19:16.613844Z","shell.execute_reply.started":"2022-02-10T21:19:09.895471Z","shell.execute_reply":"2022-02-10T21:19:16.613036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see, that this distribution is far away from a normal distribution. Moreover, the distribution is not symmetrical, which can cause some performance issues in the machine learning algorithm. Maybe, after removing the outlier, the distribution looks more symmetric.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"## Log Returns\nAs the description states, `targets` are `return rates`. These `return rates` ($r_\\alpha$) are well suited for most uses, but there are some characteristics that complicate the use of arithmetic `return rates` in some academic and valuation setting. Therefore, we will try to logarithmize them (referred as `log returns` and denoted as $r_l$) to then assess whether the `targets` then fit more closely to a normal distribution. We can transform the `return rates` into `log returns` with the relation:\n$$r_l=\\ln{\\left(r_\\alpha + 1\\right)}.$$\nThe back-transformation is then defined as:\n$$r_\\alpha = \\exp{\\left(r_l\\right)}-1.$$","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"log_target = np.log(target/100 + 1)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T21:19:19.591261Z","iopub.execute_input":"2022-02-10T21:19:19.591566Z","iopub.status.idle":"2022-02-10T21:19:19.628668Z","shell.execute_reply.started":"2022-02-10T21:19:19.591535Z","shell.execute_reply":"2022-02-10T21:19:19.627815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us have a look at its distribution now:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"_, log_bins = np.histogram(log_target, bins=bins_width)\n\nlog_params = scipy.stats.norm.fit(log_target)\nlog_norm_fit_line = scipy.stats.norm.pdf(log_bins, *log_params)\n\nsns.histplot(log_target, color='darkblue', stat='density', bins=bins_width, alpha=0.50, label=\"Log Return\")\nplt.plot(log_bins, log_norm_fit_line, color=\"red\", label=\"Normal Dist.\")\nplt.legend()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T21:19:22.791196Z","iopub.execute_input":"2022-02-10T21:19:22.791509Z","iopub.status.idle":"2022-02-10T21:19:29.569358Z","shell.execute_reply.started":"2022-02-10T21:19:22.791477Z","shell.execute_reply":"2022-02-10T21:19:29.568804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As one can see, the `log returns` seems to look and fit better the normal curve. Moreover, the maximum density of the data is now placed around the middle, which makes the distribution more symmetrical.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"## Distribution of the Investment IDs\nWe can still neglect any normal distribution. On possible solution to retrieve the underlying distribution is to use the `openturns` package. It tests many distributions and in the end returns the distribution that performs best on a criterion of choice. Our criterion of choice is the [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion). Furthermore, we will do the test for every `investment id`.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Since the target is very big, it takes alot of time to compute this. Thus, I precompute this and saved the result.\n# target_distribution = pd.DataFrame(columns=[\"ID\", \"Model\", \"Parameter\", \"BIC\"])\n# target_distribution.index.name = \"Index\"\n# total_ids = 3773\n#\n# for ids in range(0, total_ids):\n#     try:\n#         print(\"\\r>Processing ID (Total: {0}) \".format(total_ids), end=str(ids))\n#\n#         target = get_data(market_data_path, investment_id=ids, columns=[\"target\"]).values.flatten()\n#         target = np.log(target/100 + 1)\n#\n#         sample = ot.Sample([[x] for x in target])\n#         tested_factories = ot.DistributionFactory.GetContinuousUniVariateFactories()\n#         best_model_bic, best_bic = ot.FittingTest.BestModelBIC(sample, tested_factories)\n#         split = str(best_model_bic).split(\"(\")\n#\n#         target_distribution.loc[ids] = [ids, split[0], split[-1][:-1], best_bic]\n#\n#     except Exception:\n#         print(\"\\r> Could not calculate ID .\", end=str(ids))\n#\n# target_distribution.to_csv(os.path.join(model_data_path, \"distribution_data_targets.csv\"))","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now group the determined distributions:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"target_distribution = pd.read_csv(os.path.join(model_data_path, \"distribution_data_targets_raw.csv\"))\n\nx = target_distribution['Model'].value_counts()\nlabel = target_distribution['Model'].unique()\n\nplt.figure(figsize=(5, 5))\nplt.pie(x, labels=label, autopct='%1.1f%%')\nplt.tight_layout()\nplt.show()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T21:19:58.543464Z","iopub.execute_input":"2022-02-10T21:19:58.543846Z","iopub.status.idle":"2022-02-10T21:19:58.826718Z","shell.execute_reply.started":"2022-02-10T21:19:58.543799Z","shell.execute_reply":"2022-02-10T21:19:58.825735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, over the half of the underlying distribution of the `targets` could follow a `Laplace` and 34% a `Logistic Distribution`. On the next step, we will try to merge the minor `distribution` classes. In order to cluster the distribution of the `other` classes, we look at its distribution as a whole. Firstly, we filter the `other` class:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"other = target_distribution[(target_distribution[\"Model\"] != \"Student\") & (target_distribution[\"Model\"] != \"Logistic\") & (\n        target_distribution[\"Model\"] != \"Laplace\")]\nother.head()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T21:20:10.446796Z","iopub.execute_input":"2022-02-10T21:20:10.4471Z","iopub.status.idle":"2022-02-10T21:20:10.467547Z","shell.execute_reply.started":"2022-02-10T21:20:10.447064Z","shell.execute_reply":"2022-02-10T21:20:10.466767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Secondly, we join all the `target` values of the corresponding `ID` and look at its distribution","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"ids = other[\"ID\"].values\n\nother_targets = [get_data(market_data_path, investment_id=item, columns=[\"target\"]).values.tolist() for item in ids]\nother_targets = np.array([val[0] for sublist in other_targets for val in sublist])\nother_targets = np.log(other_targets/100 + 1)\n\nbins_width = int(180 / 2)\nsns.histplot(other_targets, color='darkblue', stat='density', bins=bins_width, alpha=0.50, label=\"Target\")","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T21:20:15.282406Z","iopub.execute_input":"2022-02-10T21:20:15.282998Z","iopub.status.idle":"2022-02-10T21:20:17.747593Z","shell.execute_reply.started":"2022-02-10T21:20:15.28296Z","shell.execute_reply":"2022-02-10T21:20:17.745881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next step is to determine the best fitted distribution with the `openturns` package:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"sample = ot.Sample([[x] for x in other_targets])\ntested_factories = ot.DistributionFactory.GetContinuousUniVariateFactories()\nbest_model_bic, best_bic = ot.FittingTest.BestModelBIC(sample, tested_factories)\n\nprint(best_model_bic)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T21:21:00.878263Z","iopub.execute_input":"2022-02-10T21:21:00.878553Z","iopub.status.idle":"2022-02-10T21:21:06.94884Z","shell.execute_reply.started":"2022-02-10T21:21:00.878515Z","shell.execute_reply":"2022-02-10T21:21:06.947864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It says, that a `Student t Distribution` matches best with the data. Let us have a look:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"_, bins = np.histogram(other_targets, bins=bins_width)\n\nt_params = scipy.stats.t.fit(other_targets)\nt_fit_line = scipy.stats.t.pdf(bins, *t_params)\n\nsns.histplot(other_targets, color='darkblue', stat='density', bins=bins_width, alpha=0.50, label=\"Log Return\")\nplt.plot(bins, t_fit_line, color=\"red\", label=\"Students t Dist.\")\nplt.legend()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T21:21:20.614626Z","iopub.execute_input":"2022-02-10T21:21:20.614939Z","iopub.status.idle":"2022-02-10T21:21:21.438653Z","shell.execute_reply.started":"2022-02-10T21:21:20.614911Z","shell.execute_reply":"2022-02-10T21:21:21.437803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In fact, the `Students t Distribution` fits very well with the data. Thus, we can cluster the data as the `Student t Distribution` and add the parameter to them:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"target_distribution.loc[(target_distribution[\"Model\"] != \"Student\") & (target_distribution[\"Model\"] != \"Logistic\") & (\n        target_distribution[\"Model\"] != \"Laplace\"), 'Parameter'] = \"nu = 4.75649, mu = -3.49823e-05, sigma = 0.00661145\"\ntarget_distribution.loc[(target_distribution[\"Model\"] != \"Student\") & (target_distribution[\"Model\"] != \"Logistic\") & (\n        target_distribution[\"Model\"] != \"Laplace\"), 'BIC'] = best_bic\ntarget_distribution.loc[(target_distribution[\"Model\"] != \"Student\") & (target_distribution[\"Model\"] != \"Logistic\") & (\n        target_distribution[\"Model\"] != \"Laplace\"), 'Model'] = \"Student\"","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T21:22:03.650341Z","iopub.execute_input":"2022-02-10T21:22:03.650634Z","iopub.status.idle":"2022-02-10T21:22:03.668828Z","shell.execute_reply.started":"2022-02-10T21:22:03.650601Z","shell.execute_reply":"2022-02-10T21:22:03.667744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us plot the new percentages of the three distributions:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"x = target_distribution['Model'].value_counts()\nlabel = target_distribution['Model'].unique()\n\nplt.figure(figsize=(5, 5))\nplt.pie(x, labels=label, autopct='%1.1f%%')\nplt.tight_layout()\nplt.show()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-10T21:22:08.762133Z","iopub.execute_input":"2022-02-10T21:22:08.762612Z","iopub.status.idle":"2022-02-10T21:22:08.907298Z","shell.execute_reply.started":"2022-02-10T21:22:08.762573Z","shell.execute_reply":"2022-02-10T21:22:08.906298Z"},"trusted":true},"execution_count":null,"outputs":[]}]}