{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# I copied this notebook from here: https://www.kaggle.com/code/shigeeeru/prediction-including-spatial-info-with-conv1d/notebook\n# I just del the investment_id Embedding\n--------------------------------------------------------\n# If you copy this notebook, please upvote !!\n\n##  Market Prediction with Conv1d\nIt is assumed that there is a complex relationship between the features.\n\nThe relationship is clarified by spatial analysis.\n### Refrence\nSpecial thanks @Lonnie Ubiquant Market Prediction with DNN\nhttps://www.kaggle.com/lonnieqin/ubiquant-market-prediction-with-dnn## ","metadata":{"papermill":{"duration":0.016475,"end_time":"2022-01-25T15:39:01.467349","exception":false,"start_time":"2022-01-25T15:39:01.450874","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom scipy import stats\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.keras import backend as K","metadata":{"papermill":{"duration":7.781864,"end_time":"2022-01-25T15:39:09.265726","exception":false,"start_time":"2022-01-25T15:39:01.483862","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-17T09:18:48.924124Z","iopub.execute_input":"2022-03-17T09:18:48.924419Z","iopub.status.idle":"2022-03-17T09:18:54.623405Z","shell.execute_reply.started":"2022-03-17T09:18:48.924338Z","shell.execute_reply":"2022-03-17T09:18:54.622692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"class Config:\n#     is_training = False\n    is_training = True\n    tf_record_dataset_path = \"../input/ump-combinatorialpurgedgroupkfold-tf-record/\"\n    output_dataset_path = \"../input/ubiquant-market-prediction-with-dnn-output/\"\nconfig = Config()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T09:18:54.624877Z","iopub.execute_input":"2022-03-17T09:18:54.625111Z","iopub.status.idle":"2022-03-17T09:18:54.633929Z","shell.execute_reply.started":"2022-03-17T09:18:54.625081Z","shell.execute_reply":"2022-03-17T09:18:54.630626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create an IntegerLookup layer for investment_id input","metadata":{"papermill":{"duration":0.016633,"end_time":"2022-01-25T15:39:26.408785","exception":false,"start_time":"2022-01-25T15:39:26.392152","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\ninvestment_ids = pd.read_csv(\"../input/ump-combinatorialpurgedgroupkfold-tf-record/investment_ids.csv\")\ninvestment_id_size = len(investment_ids) + 1\nwith tf.device(\"cpu\"):\n    investment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\n    investment_id_lookup_layer.adapt(investment_ids)","metadata":{"papermill":{"duration":4.105382,"end_time":"2022-01-25T15:39:30.530653","exception":false,"start_time":"2022-01-25T15:39:26.425271","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-17T09:18:54.635277Z","iopub.execute_input":"2022-03-17T09:18:54.63554Z","iopub.status.idle":"2022-03-17T09:18:57.053279Z","shell.execute_reply.started":"2022-03-17T09:18:54.635505Z","shell.execute_reply":"2022-03-17T09:18:57.052499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make Tensorflow dataset","metadata":{"papermill":{"duration":0.018846,"end_time":"2022-01-25T15:39:30.567495","exception":false,"start_time":"2022-01-25T15:39:30.548649","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def decode_function(record_bytes):\n  return tf.io.parse_single_example(\n      # Data\n      record_bytes,\n      # Schema\n      {\n          \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n          \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"investment_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n      }\n  )\ndef preprocess(item):\n    return (item[\"features\"]), item[\"target\"]\ndef make_dataset(file_paths, batch_size=4096, mode=\"train\"):\n    ds = tf.data.TFRecordDataset(file_paths)\n    ds = ds.map(decode_function)\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(batch_size * 4)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds","metadata":{"papermill":{"duration":0.02858,"end_time":"2022-01-25T15:39:30.614302","exception":false,"start_time":"2022-01-25T15:39:30.585722","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-17T09:18:57.055472Z","iopub.execute_input":"2022-03-17T09:18:57.056229Z","iopub.status.idle":"2022-03-17T09:18:57.064328Z","shell.execute_reply.started":"2022-03-17T09:18:57.05619Z","shell.execute_reply":"2022-03-17T09:18:57.063669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling\n\nI use layers.Conv1d. \n\n[source is here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D)\n\n","metadata":{"papermill":{"duration":0.017564,"end_time":"2022-01-25T15:39:30.64918","exception":false,"start_time":"2022-01-25T15:39:30.631616","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def correlation(x, y, axis=-2):\n    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    ###    不偏分散にしたら？？   ###\n    \n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xvar * yvar)\n    return tf.constant(1.0, dtype=x.dtype) - corr\n\ndef get_model():\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    ## feature ##\n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.1)(feature_x)\n    ## convolution 1 ##\n    feature_x = layers.Reshape((-1,1))(feature_x)\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 2 ##\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 3 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 4 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 5 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=2, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## flatten ##\n    feature_x = layers.Flatten()(feature_x)\n    \n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n    \n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse, correlation])\n    return model","metadata":{"papermill":{"duration":0.033569,"end_time":"2022-01-25T15:39:30.700649","exception":false,"start_time":"2022-01-25T15:39:30.66708","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-17T09:18:57.065404Z","iopub.execute_input":"2022-03-17T09:18:57.065616Z","iopub.status.idle":"2022-03-17T09:18:57.092384Z","shell.execute_reply.started":"2022-03-17T09:18:57.065586Z","shell.execute_reply":"2022-03-17T09:18:57.091585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at this Model's architecture.","metadata":{}},{"cell_type":"code","source":"model = get_model()\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)","metadata":{"papermill":{"duration":0.209912,"end_time":"2022-01-25T15:39:30.929112","exception":false,"start_time":"2022-01-25T15:39:30.7192","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-17T09:18:57.095821Z","iopub.execute_input":"2022-03-17T09:18:57.096033Z","iopub.status.idle":"2022-03-17T09:18:58.461862Z","shell.execute_reply.started":"2022-03-17T09:18:57.096008Z","shell.execute_reply":"2022-03-17T09:18:58.460353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":471.71946,"end_time":"2022-01-25T15:47:23.749584","exception":false,"start_time":"2022-01-25T15:39:32.030124","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodels = []\nfor i in range(1):\n    train_path = f\"{config.tf_record_dataset_path}fold_{i}_train.tfrecords\"\n    valid_path = f\"{config.tf_record_dataset_path}fold_{i}_test.tfrecords\"\n    valid_ds = make_dataset([valid_path], mode=\"valid\")\n    print(valid_ds)\n    model = get_model()\n    if config.is_training:\n        train_ds = make_dataset([train_path])\n        checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{i}.tf\", monitor=\"val_correlation\", mode=\"min\", save_best_only=True, save_weights_only=True)\n        early_stop = keras.callbacks.EarlyStopping(patience=10)\n        history = model.fit(train_ds, epochs=30, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n        model.load_weights(f\"model_{i}.tf\")\n        for metric in [\"loss\", \"mae\", \"mape\", \"rmse\", \"correlation\"]:\n            pd.DataFrame(history.history, columns=[metric, f\"val_{metric}\"]).plot()\n            plt.title(metric.upper())\n            plt.show()\n    else:\n        model.load_weights(f\"{config.output_dataset_path}model_{i}.tf\")\n    y_vals = []\n    for _, y in valid_ds:\n        y_vals += list(y.numpy().reshape(-1))\n    y_val = np.array(y_vals)\n    pearson_score = stats.pearsonr(model.predict(valid_ds).reshape(-1), y_val)[0]\n    models.append(model)\n    print(f\"Pearson Score: {pearson_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-03-17T09:18:58.463139Z","iopub.execute_input":"2022-03-17T09:18:58.463726Z","iopub.status.idle":"2022-03-17T09:37:27.638938Z","shell.execute_reply.started":"2022-03-17T09:18:58.463681Z","shell.execute_reply":"2022-03-17T09:37:27.637608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{"papermill":{"duration":2.11441,"end_time":"2022-01-25T15:47:27.649127","exception":false,"start_time":"2022-01-25T15:47:25.534717","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\ndef make_test_dataset(feature, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((feature)))\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{"papermill":{"duration":1.80736,"end_time":"2022-01-25T15:47:31.24096","exception":false,"start_time":"2022-01-25T15:47:29.4336","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-17T09:37:27.641858Z","iopub.execute_input":"2022-03-17T09:37:27.644253Z","iopub.status.idle":"2022-03-17T09:37:27.658682Z","shell.execute_reply.started":"2022-03-17T09:37:27.644203Z","shell.execute_reply":"2022-03-17T09:37:27.657592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfeatures = [f\"f_{i}\" for i in range(300)]\nfor (test_df, sample_prediction_df) in iter_test:\n    ds = make_test_dataset(test_df[features])\n    sample_prediction_df['target'] = inference(models, ds)\n    env.predict(sample_prediction_df) ","metadata":{"papermill":{"duration":2.234161,"end_time":"2022-01-25T15:47:35.24214","exception":false,"start_time":"2022-01-25T15:47:33.007979","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-17T09:37:27.661115Z","iopub.execute_input":"2022-03-17T09:37:27.661381Z","iopub.status.idle":"2022-03-17T09:37:28.978415Z","shell.execute_reply.started":"2022-03-17T09:37:27.661345Z","shell.execute_reply":"2022-03-17T09:37:28.977625Z"},"trusted":true},"execution_count":null,"outputs":[]}]}