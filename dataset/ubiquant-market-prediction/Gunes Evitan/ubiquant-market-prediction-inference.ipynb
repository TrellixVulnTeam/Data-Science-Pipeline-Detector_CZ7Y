{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from glob import glob\nfrom tqdm.notebook import tqdm\nimport yaml\nimport pickle\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\nimport lightgbm as lgb\nimport xgboost as xgb\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, SequentialSampler\nimport ubiquant","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-02T06:59:22.933139Z","iopub.execute_input":"2022-03-02T06:59:22.934087Z","iopub.status.idle":"2022-03-02T06:59:27.384674Z","shell.execute_reply.started":"2022-03-02T06:59:22.933984Z","shell.execute_reply":"2022-03-02T06:59:27.383928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_pickle('../input/ubiquant-market-prediction-eda/train.pkl')\nprint(f'Training Set Shape: {df_train.shape} - Memory Usage: {df_train.memory_usage().sum() / 1024 ** 2:.2f} MB')","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:59:27.388084Z","iopub.execute_input":"2022-03-02T06:59:27.388284Z","iopub.status.idle":"2022-03-02T06:59:58.964292Z","shell.execute_reply.started":"2022-03-02T06:59:27.38826Z","shell.execute_reply":"2022-03-02T06:59:58.963541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Model","metadata":{}},{"cell_type":"code","source":"def load_linear_model(path):\n    \n    config = yaml.load(open(f'{path}/config.yaml', 'r'), Loader=yaml.FullLoader)\n    with open(f'{path}/model', 'rb') as f:\n        model = pickle.load(f)\n\n    return config, model\n\n\n#linear_model_config, linear_model = load_linear_model('../input/ubiquant-market-prediction-dataset/linear_model/no_split')\n#linear_model","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:59:58.965561Z","iopub.execute_input":"2022-03-02T06:59:58.96598Z","iopub.status.idle":"2022-03-02T06:59:58.971377Z","shell.execute_reply.started":"2022-03-02T06:59:58.965929Z","shell.execute_reply":"2022-03-02T06:59:58.970704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM","metadata":{}},{"cell_type":"code","source":"def load_lgb_model(path):\n    \n    \"\"\"\n    Load LightGBM models and config file from the given path\n\n    Parameters\n    ----------\n    path (str): Path of the model directory\n    \n    Returns\n    -------\n    config (dict): Dictionary of model configurations\n    models (list): List of trained LightGBM models\n    \"\"\"\n    \n    config = yaml.load(open(f'{path}/config.yaml', 'r'), Loader=yaml.FullLoader)\n    models = [lgb.Booster(model_file=model_path) for model_path in glob(f'{path}/model*.txt')]\n\n    return config, models\n\n\nlgb_config, lgb_models = load_lgb_model('../input/ubiquant-market-prediction-dataset/lightgbm_regression/no_split')","metadata":{"execution":{"iopub.status.busy":"2022-03-02T06:59:58.973328Z","iopub.execute_input":"2022-03-02T06:59:58.973603Z","iopub.status.idle":"2022-03-02T07:00:03.520932Z","shell.execute_reply.started":"2022-03-02T06:59:58.973567Z","shell.execute_reply":"2022-03-02T07:00:03.520273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"def load_xgb_model(path):\n    \n    \"\"\"\n    Load XGBoost models and config file from the given path\n\n    Parameters\n    ----------\n    path (str): Path of the model directory\n    \n    Returns\n    -------\n    config (dict): Dictionary of model configurations\n    models (list): List of trained XGBoost models\n    \"\"\"\n    \n    config = yaml.load(open(f'{path}/config.yaml', 'r'), Loader=yaml.FullLoader)\n    models = [xgb.Booster(model_file=model_path) for model_path in glob(f'{path}/model*.json')]\n\n    return config, models\n\n\nxgb_config, xgb_models = load_xgb_model('../input/ubiquant-market-prediction-dataset/xgboost_regression/no_split')","metadata":{"execution":{"iopub.status.busy":"2022-03-02T07:00:03.522319Z","iopub.execute_input":"2022-03-02T07:00:03.522593Z","iopub.status.idle":"2022-03-02T07:00:10.009548Z","shell.execute_reply.started":"2022-03-02T07:00:03.52256Z","shell.execute_reply":"2022-03-02T07:00:10.008815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PyTorch Datasets","metadata":{}},{"cell_type":"code","source":"class TabularDataset(Dataset):\n\n    def __init__(self, features, labels=None):\n\n        self.features = features\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n\n        \"\"\"\n        Get the idxth element in the dataset\n\n        Parameters\n        ----------\n        idx (int): Index of the sample (0 <= idx < len(self.features))\n\n        Returns\n        -------\n        features [torch.FloatTensor of shape (1, n_features)]: Features\n        label [torch.FloatTensor of shape (1)]: Label\n        \"\"\"\n\n        features = self.features[idx]\n        features = torch.tensor(features, dtype=torch.float)\n\n        if self.labels is not None:\n            label = self.labels[idx]\n            label = torch.tensor(label, dtype=torch.float)\n            return features, label\n        else:\n            return features\n","metadata":{"execution":{"iopub.status.busy":"2022-03-02T07:00:50.437531Z","iopub.execute_input":"2022-03-02T07:00:50.437786Z","iopub.status.idle":"2022-03-02T07:00:50.444703Z","shell.execute_reply.started":"2022-03-02T07:00:50.437758Z","shell.execute_reply":"2022-03-02T07:00:50.443716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multilayer Perceptron","metadata":{}},{"cell_type":"code","source":"def init_weights(module,\n                 linear_weight_init_type, linear_weight_init_args, linear_bias_init_type, linear_bias_init_args,\n                 batch_normalization_weight_init_type=None, batch_normalization_weight_init_args=None,\n                 batch_normalization_bias_init_type=None, batch_normalization_bias_init_args=None):\n\n    \"\"\"\n    Initialize weights and biases of given layers with specified configurations\n\n    Parameters\n    ----------\n    module (torch.nn.Module): Layer\n    linear_weight_init_type (str): Weight initialization method of the linear layer\n    linear_weight_init_args (dict): Weight initialization arguments of the linear layer\n    linear_bias_init_type (str): Bias initialization method of the linear layer\n    linear_bias_init_args (dict): Bias initialization arguments of the linear layer\n    batch_normalization_weight_init_type (str): Weight initialization method of the batch normalization layer\n    batch_normalization_weight_init_args (dict): Weight initialization arguments of the batch normalization layer\n    batch_normalization_bias_init_type (str): Bias initialization method of the batch normalization layer\n    batch_normalization_bias_init_args (dict): Bias initialization arguments of the batch normalization layer\n    \"\"\"\n\n    if isinstance(module, nn.Linear):\n        # Initialize weights of linear layer\n        if linear_weight_init_type == 'uniform':\n            nn.init.uniform_(\n                module.weight,\n                a=linear_weight_init_args['a'],\n                b=linear_weight_init_args['b']\n            )\n        elif linear_weight_init_type == 'normal':\n            nn.init.normal_(\n                module.weight,\n                mean=linear_weight_init_args['mean'],\n                std=linear_weight_init_args['std']\n            )\n        elif linear_weight_init_type == 'xavier_uniform':\n            nn.init.xavier_uniform_(\n                module.weight,\n                gain=nn.init.calculate_gain(\n                    nonlinearity=linear_weight_init_args['nonlinearity'],\n                    param=linear_weight_init_args['nonlinearity_param']\n                )\n            )\n        elif linear_weight_init_type == 'xavier_normal':\n            nn.init.xavier_normal_(\n                module.weight,\n                gain=nn.init.calculate_gain(\n                    nonlinearity=linear_weight_init_args['nonlinearity'],\n                    param=linear_weight_init_args['nonlinearity_param']\n                )\n            )\n        elif linear_weight_init_type == 'kaiming_uniform':\n            nn.init.kaiming_uniform_(\n                module.weight,\n                a=linear_weight_init_args['nonlinearity_param'],\n                mode=linear_weight_init_args['mode'],\n                nonlinearity=linear_weight_init_args['nonlinearity']\n            )\n        elif linear_weight_init_type == 'kaiming_normal':\n            nn.init.kaiming_normal_(\n                module.weight,\n                a=linear_weight_init_args['nonlinearity_param'],\n                mode=linear_weight_init_args['mode'],\n                nonlinearity=linear_weight_init_args['nonlinearity']\n            )\n        elif linear_weight_init_type == 'orthogonal':\n            nn.init.orthogonal_(\n                module.weight,\n                gain=nn.init.calculate_gain(\n                    nonlinearity=linear_weight_init_args['nonlinearity'],\n                    param=linear_weight_init_args['nonlinearity_param']\n                )\n            )\n        elif linear_weight_init_type == 'sparse':\n            nn.init.sparse_(\n                module.weight,\n                sparsity=linear_weight_init_args['sparsity'],\n                std=linear_weight_init_args['std']\n            )\n        # Initialize biases of Linear layer\n        if module.bias is not None:\n            if linear_bias_init_type == 'uniform':\n                nn.init.uniform_(\n                    module.bias,\n                    a=linear_bias_init_args['a'],\n                    b=linear_bias_init_args['b']\n                )\n            elif linear_bias_init_type == 'normal':\n                nn.init.normal_(\n                    module.bias,\n                    mean=linear_bias_init_args['mean'],\n                    std=linear_bias_init_args['std']\n                )\n\n    elif isinstance(module, nn.BatchNorm1d):\n        # Initialize weights of batch normalization layer\n        if batch_normalization_weight_init_type is not None:\n            if batch_normalization_weight_init_type == 'uniform':\n                nn.init.uniform_(\n                    module.weight,\n                    a=batch_normalization_weight_init_args['a'],\n                    b=batch_normalization_weight_init_args['b']\n                )\n            elif batch_normalization_weight_init_type == 'normal':\n                nn.init.normal_(\n                    module.weight,\n                    mean=batch_normalization_weight_init_args['mean'],\n                    std=batch_normalization_weight_init_args['std']\n                )\n            elif batch_normalization_weight_init_type == 'constant':\n                nn.init.constant_(\n                    module.weight,\n                    val=batch_normalization_weight_init_args['val'],\n                )\n        # Initialize biases of batch normalization layer\n        if batch_normalization_bias_init_type is not None:\n            if batch_normalization_bias_init_type == 'uniform':\n                nn.init.uniform_(\n                    module.bias,\n                    a=batch_normalization_bias_init_args['a'],\n                    b=batch_normalization_bias_init_args['b']\n                )\n            elif batch_normalization_bias_init_type == 'normal':\n                nn.init.normal_(\n                    module.bias,\n                    mean=batch_normalization_bias_init_args['mean'],\n                    std=batch_normalization_bias_init_args['std']\n                )\n            elif batch_normalization_bias_init_type == 'constant':\n                nn.init.constant_(\n                    module.bias,\n                    val=batch_normalization_bias_init_args['val'],\n                )\n\n\nclass DenseBlock(nn.Module):\n\n    def __init__(self, input_dim, output_dim, batch_normalization, weight_normalization, dropout_probability, activation, activation_args, init_args):\n\n        \"\"\"\n        Vanilla dense block (Linear -> Batch Normalization -> Dropout -> Activation)\n\n        Parameters\n        ----------\n        input_dim (int): Number of input dimensions of the dense layer\n        output_dim (int): Number of output dimensions of the dense layer\n        batch_normalization (bool): Whether to add batch normalization or not\n        weight_normalization (bool): Whether to add weight normalization or not\n        dropout_probability (int): Probability of the dropout\n        activation (str): Class name of the activation function\n        activation_args (dict): Class arguments of the activation function\n        init_args (dict): Weight and bias initialization arguments of the layers\n        \"\"\"\n\n        super(DenseBlock, self).__init__()\n\n        if weight_normalization:\n            self.linear = nn.utils.weight_norm(nn.Linear(in_features=input_dim, out_features=output_dim, bias=True))\n        else:\n            self.linear = nn.Linear(in_features=input_dim, out_features=output_dim, bias=True)\n\n        if batch_normalization:\n            self.batch_normalization = nn.BatchNorm1d(num_features=output_dim)\n        else:\n            self.batch_normalization = nn.Identity()\n\n        if dropout_probability > 0.0:\n            self.dropout = nn.Dropout(p=dropout_probability)\n        else:\n            self.dropout = nn.Identity()\n\n        if activation is not None and activation != 'Swish':\n            self.activation = getattr(nn, activation)(**activation_args)\n        elif activation == 'Swish':\n            self.activation = Swish()\n        else:\n            self.activation = nn.Identity()\n\n        if init_args is not None:\n            init_weights(self.linear, **init_args)\n            if batch_normalization:\n                init_weights(self.batch_normalization, **init_args)\n\n    def forward(self, x):\n\n        x = self.linear(x)\n        x = self.batch_normalization(x)\n        x = self.dropout(x)\n        x = self.activation(x)\n\n        return x\n\n\nclass MultiLayerPerceptron(nn.Module):\n\n    def __init__(self, input_dim, batch_normalization, weight_normalization, dropout_probability, activation, activation_args, init_args):\n\n        \"\"\"\n        MLP (Multi Layer Perceptron) with multiple dense blocks\n\n        Parameters\n        ----------\n        input_dim (int): Number of input dimensions of the first dense block\n        batch_normalization (bool): Whether to add batch normalization to dense blocks or not\n        weight_normalization (bool): Whether to add weight normalization to dense blocks or not\n        dropout_probability (int): Probability of the dropout in dense blocks\n        activation (str): Class name of the activation function in dense blocks\n        activation_args (dict): Class arguments of the activation function in dense blocks\n        init_args (dict): Weight and bias initialization arguments of the layers in dense blocks\n        \"\"\"\n\n        super(MultiLayerPerceptron, self).__init__()\n\n        self.dense_block1 = DenseBlock(\n            input_dim=input_dim,\n            output_dim=512,\n            batch_normalization=batch_normalization,\n            weight_normalization=weight_normalization,\n            dropout_probability=dropout_probability,\n            activation=activation,\n            activation_args=activation_args,\n            init_args=init_args\n        )\n        self.dense_block2 = DenseBlock(\n            input_dim=512,\n            output_dim=768,\n            batch_normalization=batch_normalization,\n            weight_normalization=weight_normalization,\n            dropout_probability=dropout_probability,\n            activation=activation,\n            activation_args=activation_args,\n            init_args=init_args\n        )\n        self.dense_block3 = DenseBlock(\n            input_dim=768,\n            output_dim=1024,\n            batch_normalization=batch_normalization,\n            weight_normalization=weight_normalization,\n            dropout_probability=dropout_probability,\n            activation=activation,\n            activation_args=activation_args,\n            init_args=init_args\n        )\n        self.dense_block4 = DenseBlock(\n            input_dim=1024,\n            output_dim=768,\n            batch_normalization=batch_normalization,\n            weight_normalization=weight_normalization,\n            dropout_probability=dropout_probability,\n            activation=activation,\n            activation_args=activation_args,\n            init_args=init_args\n        )\n        self.dense_block5 = DenseBlock(\n            input_dim=768,\n            output_dim=512,\n            batch_normalization=batch_normalization,\n            weight_normalization=weight_normalization,\n            dropout_probability=dropout_probability,\n            activation=activation,\n            activation_args=activation_args,\n            init_args=init_args\n        )\n        self.dense_block6 = DenseBlock(\n            input_dim=512,\n            output_dim=256,\n            batch_normalization=batch_normalization,\n            weight_normalization=weight_normalization,\n            dropout_probability=dropout_probability,\n            activation=activation,\n            activation_args=activation_args,\n            init_args=init_args\n        )\n        self.dense_block7 = DenseBlock(\n            input_dim=256,\n            output_dim=128,\n            batch_normalization=batch_normalization,\n            weight_normalization=weight_normalization,\n            dropout_probability=dropout_probability,\n            activation=activation,\n            activation_args=activation_args,\n            init_args=init_args\n        )\n        self.head = DenseBlock(\n            input_dim=128,\n            output_dim=1,\n            batch_normalization=False,\n            weight_normalization=False,\n            dropout_probability=0.0,\n            activation=None,\n            activation_args=None,\n            init_args=init_args\n        )\n\n    def forward(self, x):\n\n        x = self.dense_block1(x)\n        x = self.dense_block2(x)\n        x = self.dense_block3(x)\n        x = self.dense_block4(x)\n        x = self.dense_block5(x)\n        x = self.dense_block6(x)\n        x = self.dense_block7(x)\n        output = self.head(x)\n\n        return output.view(-1)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-02T07:02:40.726622Z","iopub.execute_input":"2022-03-02T07:02:40.726898Z","iopub.status.idle":"2022-03-02T07:02:40.766108Z","shell.execute_reply.started":"2022-03-02T07:02:40.726867Z","shell.execute_reply":"2022-03-02T07:02:40.765195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_mlp_model(path):\n    \n    \"\"\"\n    Load Multilayer Perceptron models and config file from the given path\n\n    Parameters\n    ----------\n    path (str): Path of the model directory\n    \n    Returns\n    -------\n    config (dict): Dictionary of model configurations\n    models (list): List of trained Multilayer Perceptron models\n    \"\"\"\n    \n    config = yaml.load(open(f'{path}/config.yaml', 'r'), Loader=yaml.FullLoader)\n    models = []\n    \n    for model_path in glob(f'{path}/model*.pt'):\n        \n        model = MultiLayerPerceptron(\n            input_dim=300,\n            batch_normalization=True,\n            weight_normalization=False,\n            dropout_probability=0.2,\n            activation='LeakyReLU',\n            activation_args={\n                'negative_slope': 0.1\n            },\n            init_args={\n                'linear_weight_init_type': 'kaiming_normal',\n                'linear_weight_init_args': {\n                    'mode': 'fan_out',\n                    'nonlinearity': 'leaky_relu',\n                    'nonlinearity_param': 0.1\n                },\n                'linear_bias_init_type': 'normal',\n                'linear_bias_init_args': {\n                    'mean': 0.0,\n                    'std': 1.0\n                },\n                'batch_normalization_weight_init_type': None,\n                'batch_normalization_weight_init_args': None,\n                'batch_normalization_bias_init_type': None,\n                'batch_normalization_bias_init_args': None\n            }\n\n        )\n        model.load_state_dict(torch.load(model_path))\n        model.eval()\n        model.to(torch.device('cuda'))\n        models.append(model)\n\n    return config, models\n\n\nmlp_config, mlp_models = load_mlp_model('../input/ubiquant-market-prediction-dataset/mlp_regression/no_split')","metadata":{"execution":{"iopub.status.busy":"2022-03-02T07:44:32.25947Z","iopub.execute_input":"2022-03-02T07:44:32.26005Z","iopub.status.idle":"2022-03-02T07:44:33.587444Z","shell.execute_reply.started":"2022-03-02T07:44:32.260013Z","shell.execute_reply":"2022-03-02T07:44:33.586716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Post-processing","metadata":{}},{"cell_type":"code","source":"def _pearson_correlation_coefficient(df, col_x, col_y):\n\n    \"\"\"\n    Calculate Pearson correlation coefficient between two columns\n\n    Parameters\n    ----------\n    df [pandas.DataFrame of shape (n_samples, 2)]: Training set with two columns\n    col_x (str): Name of the first column\n    col_y (str): Name of the second column\n    \"\"\"\n\n    return df.corr()[col_x][col_y]\n\n\ndef mean_pearson_correlation_coefficient(df, col_x, col_y):\n\n    \"\"\"\n    Calculate Pearson correlation coefficient between two columns for every time_id and average their results\n\n    Parameters\n    ----------\n    df [pandas.DataFrame of shape (n_samples, 3)]: Training set with time_id and two columns\n    col_x (str): Name of the first column\n    col_y (str): Name of the second column\n    \"\"\"\n\n    return np.mean(df[['time_id', col_x, col_y]].groupby('time_id').apply(_pearson_correlation_coefficient, col_x, col_y))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-02T07:05:54.163431Z","iopub.execute_input":"2022-03-02T07:05:54.163693Z","iopub.status.idle":"2022-03-02T07:05:54.16967Z","shell.execute_reply.started":"2022-03-02T07:05:54.163664Z","shell.execute_reply":"2022-03-02T07:05:54.16875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_lgb_regression_predictions = pd.read_csv('../input/ubiquant-market-prediction-dataset/lightgbm_regression/single_split/predictions.csv')\ndf_xgb_regression_predictions = pd.read_csv('../input/ubiquant-market-prediction-dataset/xgboost_regression/single_split/predictions.csv')\ndf_mlp_regression_predictions = pd.read_csv('../input/ubiquant-market-prediction-dataset/mlp_regression/single_split/predictions.csv')\n\ndf_train['lgb_regression_predictions'] = df_lgb_regression_predictions['predictions'].values\ndf_train['xgb_regression_predictions'] = df_xgb_regression_predictions['predictions'].values\ndf_train['mlp_regression_predictions'] = df_mlp_regression_predictions['predictions'].values\n\nmodels_and_prediction_columns = {\n    'LightGBM Regression': 'lgb_regression_predictions',\n    'XGBoost Regression': 'xgb_regression_predictions',\n    'Multilayer Perceptron': 'mlp_regression_predictions'\n}\n\nfor model_name, prediction_column in models_and_prediction_columns.items():\n    score = mean_pearson_correlation_coefficient(df_train, col_x='target', col_y=prediction_column)\n    print(f'{model_name} - Validation Mean Pearson Correlation Coefficient: {score:.6f}')","metadata":{"execution":{"iopub.status.busy":"2022-03-02T07:05:54.751608Z","iopub.execute_input":"2022-03-02T07:05:54.75242Z","iopub.status.idle":"2022-03-02T07:05:58.189468Z","shell.execute_reply.started":"2022-03-02T07:05:54.752372Z","shell.execute_reply":"2022-03-02T07:05:58.188633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['blend_predictions'] = (df_train['lgb_regression_predictions'] * 1) + (df_train['xgb_regression_predictions'] * 1) + (df_train['mlp_regression_predictions'] / 1000)\nscore = mean_pearson_correlation_coefficient(df_train, col_x='target', col_y='blend_predictions')\nprint(f'Blend - Validation Mean Pearson Correlation Coefficient: {score:.6f}')","metadata":{"execution":{"iopub.status.busy":"2022-03-02T07:05:58.191122Z","iopub.execute_input":"2022-03-02T07:05:58.191373Z","iopub.status.idle":"2022-03-02T07:05:58.961954Z","shell.execute_reply.started":"2022-03-02T07:05:58.191339Z","shell.execute_reply":"2022-03-02T07:05:58.961244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"env = ubiquant.make_env()\niter_test = env.iter_test()\n\nfor df_test, df_sample_submission in iter_test:\n    \n    '''# LightGBM Inference\n    lgb_predictions = np.zeros(len(df_test))\n    for lgb_model in lgb_models:\n        lgb_predictions += (lgb_model.predict(df_test.loc[:, lgb_config['features']]) / len(lgb_models))\n    \n    # XGBoost Inference\n    xgb_predictions = np.zeros(len(df_test))\n    for xgb_model in xgb_models:\n        xgb_predictions += (xgb_model.predict(xgb.DMatrix(df_test.loc[:, xgb_config['features']])) / len(xgb_models))'''\n        \n    # PyTorch Dataset and DataLoader\n    test_dataset = TabularDataset(features=df_test.loc[:, mlp_config['features']].values)\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=len(test_dataset),\n        sampler=SequentialSampler(test_dataset),\n        pin_memory=True,\n        drop_last=False,\n        num_workers=0\n    )\n    \n    # Multilayer Perceptron Inference\n    mlp_predictions = np.zeros(len(df_test))\n    with torch.no_grad():\n        for features in test_loader:\n            for model in mlp_models:\n                outputs = model(features.to(torch.device('cuda')))\n                mlp_predictions += (outputs.detach().cpu().numpy() / len(mlp_models))\n    \n    df_sample_submission['target'] = mlp_predictions\n    \n    env.predict(df_sample_submission)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}