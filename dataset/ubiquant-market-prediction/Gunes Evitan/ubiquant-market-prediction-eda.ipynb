{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\nfrom scipy.stats import probplot, pearsonr\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-01T08:35:56.363526Z","iopub.execute_input":"2022-02-01T08:35:56.363878Z","iopub.status.idle":"2022-02-01T08:35:57.368171Z","shell.execute_reply.started":"2022-02-01T08:35:56.363788Z","shell.execute_reply":"2022-02-01T08:35:57.367409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ubiquant Market Prediction","metadata":{}},{"cell_type":"markdown","source":"## 1. Introduction\n\nThis competition's objective is predicting value of an obfuscated metric which is relevant for making trading decisions. Dataset contains anonymized features extracted from thousands of investments and an anonymized target. There are 3579 unique investments in training set, but investments in training set doesn't necessarily appear in public or private test set. There are also 1211 unique time IDs in training set.  All of the investments doesn't necessarily appear in all time IDs.\n\nThere are 303 columns in training and test set after removing row_id. row_id can be removed safely since it is combination of time_id and investment_id columns, and it doesn't contain any additional information. The mentioned columns are:\n\n* `time_id`: Unique ID of the time bucket\n* `investment_id`: Unique ID of the investment\n* `target`: Anonymized target\n* `f0` - `f299`: Anonymized features","metadata":{}},{"cell_type":"code","source":"train_dtypes = {f'f_{i}': np.float32 for i in range(300)}\ntrain_dtypes['investment_id'] = np.uint16\ntrain_dtypes['time_id'] = np.uint16\ntrain_dtypes['target'] = np.float32\n\ndf_train = pd.read_csv('../input/ubiquant-market-prediction/train.csv', usecols=list(train_dtypes.keys()), dtype=train_dtypes)\nprint(f'Training Set Shape: {df_train.shape} - Memory Usage: {df_train.memory_usage().sum() / 1024 ** 2:.2f} MB')","metadata":{"execution":{"iopub.status.busy":"2022-02-01T08:36:57.228158Z","iopub.execute_input":"2022-02-01T08:36:57.228472Z","iopub.status.idle":"2022-02-01T08:44:21.996149Z","shell.execute_reply.started":"2022-02-01T08:36:57.228426Z","shell.execute_reply":"2022-02-01T08:44:21.994931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the training set is quite large, it takes more than 8 minutes to read it. Training set can be written as a pickle file so it can be read faster later.","metadata":{}},{"cell_type":"code","source":"df_train.to_pickle('train.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:26:24.974803Z","iopub.execute_input":"2022-01-30T09:26:24.975194Z","iopub.status.idle":"2022-01-30T09:26:56.912429Z","shell.execute_reply.started":"2022-01-30T09:26:24.975147Z","shell.execute_reply":"2022-01-30T09:26:56.911584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Evaluation\n\nSubmissions are scored on the mean of Pearson correlation coefficient for each time ID. Pearson correlation coefficient (Pearson's r) is a measure of linear correlation between two sets of values. It can be denoted as\n\n$\\huge \\text{r} = \\frac{n\\sum{xy}-(\\sum{x})(\\sum{y})}{\\sqrt{ [n \\sum{x^2}-(\\sum{x})^2 ][n \\sum{y^2}-(\\sum{y})^2 }]}$\n\n* $r$ = Pearson Correlation Coefficient\n* $n$ = Number of samples\n* $x$ = First set of values\n* $y$ = Second set of values\n\nMean of Pearson correlation coefficient across time IDs can be denoted as\n\n$\\huge \\text{Mean r} = {\\frac{1}{T} \\sum_{i=1}^{T}} t_i r$\n\n* $T$ = Number of time IDs\n* $t_i r$ = ith time ID's Pearson correlation coefficient\n","metadata":{}},{"cell_type":"markdown","source":"Fastest way of calculating mean Pearson correlation coefficient is utilizing `groupby` method of `pandas.DataFrame`, however it is not very flexible. The implementations below require predictions column named as _predictions_ and they don't output Pearson correlation coefficient of every time ID separately.","metadata":{}},{"cell_type":"code","source":"def pearson_correlation_coefficient(df):\n    return df.corr()['target']['predictions']\n\ndef mean_pearson_correlation_coefficient(df):\n    return np.mean(df[['time_id', 'target', 'predictions']].groupby('time_id').apply(pearson_correlation_coefficient))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:26:56.91364Z","iopub.execute_input":"2022-01-30T09:26:56.914238Z","iopub.status.idle":"2022-01-30T09:26:56.920929Z","shell.execute_reply.started":"2022-01-30T09:26:56.914194Z","shell.execute_reply":"2022-01-30T09:26:56.920008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pearson correlation coefficient is the ratio between covariances of two sets of values and the product of their standard deviations. That means Pearson correlation coefficient won't change when standard deviations and order of values are kept same for time IDs. Therefore, actual values of predictions doesn't matter and it is similar to a ranking metric in that sense.","metadata":{}},{"cell_type":"code","source":"df_train['predictions'] = np.random.rand(len(df_train))\nscore = mean_pearson_correlation_coefficient(df_train)\nprint(f'Pearson correlation coefficient: {score:.6f} - (Predictions mean: {df_train[\"predictions\"].mean():.4f} std: {df_train[\"predictions\"].std():.4f})')\ndf_train['predictions'] += 999999\nscore = mean_pearson_correlation_coefficient(df_train)\nprint(f'Pearson correlation coefficient: {score:.6f} - (Predictions mean: {df_train[\"predictions\"].mean():.4f} std: {df_train[\"predictions\"].std():.4f})')\n\ndf_train.drop(columns=['predictions'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T09:26:56.923182Z","iopub.execute_input":"2022-01-30T09:26:56.923602Z","iopub.status.idle":"2022-01-30T09:27:05.198643Z","shell.execute_reply.started":"2022-01-30T09:26:56.923557Z","shell.execute_reply":"2022-01-30T09:27:05.197453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Target\n\nTarget is anonymized and defined as an obfuscated metric relevant for making trading decisions. Target is mean centric and follows a very symmetrical normal distribution which are strong evidences of standardization. Target has very long tails on both ends so distribution truncation or trimming can be quite useful for dealing with outliers.","metadata":{}},{"cell_type":"code","source":"def visualize_target(df, target):\n    \n    print(f'{target}\\n{\"-\" * len(target)}')\n        \n    print(f'Mean: {df[target].mean():.4f}  -  Median: {df[target].median():.4f}  -  Std: {df[target].std():.4f}')\n    print(f'Min: {df[target].min():.4f}  -  25%: {df[target].quantile(0.25):.4f}  -  50%: {df[target].quantile(0.5):.4f}  -  75%: {df[target].quantile(0.75):.4f}  -  Max: {df[target].max():.4f}')\n    print(f'Skew: {df[target].skew():.4f}  -  Kurtosis: {df[target].kurtosis():.4f}')\n    missing_count = df[df[target].isnull()].shape[0]\n    total_count = df.shape[0]\n    print(f'Missing Values: {missing_count}/{total_count} ({missing_count * 100 / total_count:.4f}%)')\n\n    fig, axes = plt.subplots(ncols=2, figsize=(24, 6), dpi=100)\n\n    sns.kdeplot(df[target], label=target, fill=True, ax=axes[0])\n    axes[0].axvline(df[target].mean(), label='Mean', color='r', linewidth=2, linestyle='--')\n    axes[0].axvline(df[target].median(), label='Median', color='b', linewidth=2, linestyle='--')\n    axes[0].legend(prop={'size': 15})\n    probplot(df[target], plot=axes[1])\n    \n    for i in range(2):\n        axes[i].tick_params(axis='x', labelsize=12.5)\n        axes[i].tick_params(axis='y', labelsize=12.5)\n        axes[i].set_xlabel('')\n        axes[i].set_ylabel('')\n    axes[0].set_title(f'{target} Distribution', fontsize=15, pad=12)\n    axes[1].set_title(f'{target} Probability Plot', fontsize=15, pad=12)\n    \n    plt.show()\n\nvisualize_target(df_train, 'target')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-30T09:27:05.200441Z","iopub.execute_input":"2022-01-30T09:27:05.200903Z","iopub.status.idle":"2022-01-30T09:27:27.606008Z","shell.execute_reply.started":"2022-01-30T09:27:05.200863Z","shell.execute_reply":"2022-01-30T09:27:27.604928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Time IDs\n\ntime_id column is the ID code for the time the data was gathered. Time IDs are in chronological order, but the real time gap between time IDs is not constant and will be shorter for the final private test set than in the training set. As mentioned before, there are 1211 unique time IDs in training set and it will be less in private test set.","metadata":{}},{"cell_type":"code","source":"def visualize_time_ids(df, column):\n    \n    print(f'{column}\\n{\"-\" * len(column)}')\n    print(f'Mean: {df[column].mean():.4f}  -  Median: {df[column].median():.4f}  -  Std: {df[column].std():.4f}')\n    print(f'Min: {df[column].min():.4f}  -  25%: {df[column].quantile(0.25):.4f}  -  50%: {df[column].quantile(0.5):.4f}  -  75%: {df[column].quantile(0.75):.4f}  -  Max: {df[column].max():.4f}')\n    print(f'Skew: {df[column].skew():.4f}  -  Kurtosis: {df[column].kurtosis():.4f}')\n    missing_count = df[df[column].isnull()].shape[0]\n    total_count = df.shape[0]\n    print(f'Missing Values: {missing_count}/{total_count} ({missing_count * 100 / total_count:.4f}%)')\n\n    fig, axes = plt.subplots(ncols=2, figsize=(24, 6), dpi=100)\n\n    sns.kdeplot(df[column], label=column, fill=True, ax=axes[0])\n    axes[0].axvline(df[column].mean(), label='Mean', color='r', linewidth=2, linestyle='--')\n    axes[0].axvline(df[column].median(), label='Median', color='b', linewidth=2, linestyle='--')\n    axes[0].legend(prop={'size': 15})\n    axes[1].plot(df.set_index('time_id')[column], label=column)\n    \n    for i in range(2):\n        axes[i].tick_params(axis='x', labelsize=12.5)\n        axes[i].tick_params(axis='y', labelsize=12.5)\n        axes[i].set_ylabel('')\n    axes[0].set_xlabel('')\n    axes[1].set_xlabel('time_id', fontsize=12.5)\n    axes[0].set_title(f'{column} Distribution', fontsize=15, pad=12)\n    axes[1].set_title(f'{column} as a Function of Time', fontsize=15, pad=12)\n    \n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-26T08:09:49.124126Z","iopub.execute_input":"2022-01-26T08:09:49.124462Z","iopub.status.idle":"2022-01-26T08:09:49.420914Z","shell.execute_reply.started":"2022-01-26T08:09:49.124427Z","shell.execute_reply":"2022-01-26T08:09:49.419992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Number of samples in time IDs are quite different because every investment doesn't necessarily appear in every time ID. Time IDs with less than 2000 samples look like outliers. More than 75% of time IDs have more than 2000 samples. All of the time IDs with less than 2000 samples are observed between time_id 350 and 550. There are couple outliers between time_id 1100 and 1200 as well.","metadata":{}},{"cell_type":"code","source":"df = df_train.groupby('time_id')['target'].count().reset_index().rename(columns={'target': 'sample_counts_in_time_ids'})\nvisualize_time_ids(df, 'sample_counts_in_time_ids')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-26T08:09:53.108265Z","iopub.execute_input":"2022-01-26T08:09:53.109422Z","iopub.status.idle":"2022-01-26T08:09:53.63544Z","shell.execute_reply.started":"2022-01-26T08:09:53.109369Z","shell.execute_reply":"2022-01-26T08:09:53.633815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Target means in time IDs are centered around 0 and they are quite balanced even though there are some outliers, but outliers look very natural. Time IDs with low number of samples match time IDs with high target mean. Target might be correlated with number of samples but it is hard to tell which causes which. Very high and very low target mean values are also observed in the same period (between time_id 350 and 550).","metadata":{}},{"cell_type":"code","source":"df = df_train.groupby('time_id')['target'].mean().reset_index().rename(columns={'target': 'target_means_in_time_ids'})\nvisualize_time_ids(df, 'target_means_in_time_ids')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-25T06:06:49.226243Z","iopub.execute_input":"2022-01-25T06:06:49.226569Z","iopub.status.idle":"2022-01-25T06:06:49.776707Z","shell.execute_reply.started":"2022-01-25T06:06:49.226526Z","shell.execute_reply":"2022-01-25T06:06:49.775782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Target standard deviations in time IDs are centered around 0.9 and they are quite balanced as well, except one of them. One of the time IDs have very low target standard deviation (0.45) and it skews the distribution to left. That outlier doesn't look very natural and it probably is an anomaly. Target standard deviation outliers are also observed in the same period but they are not affected as much as target means and sample counts.","metadata":{}},{"cell_type":"code","source":"df = df_train.groupby('time_id')['target'].std().reset_index().rename(columns={'target': 'target_stds_in_time_ids'})\nvisualize_time_ids(df, 'target_stds_in_time_ids')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-25T06:57:51.657353Z","iopub.execute_input":"2022-01-25T06:57:51.657636Z","iopub.status.idle":"2022-01-25T06:57:52.135145Z","shell.execute_reply.started":"2022-01-25T06:57:51.657591Z","shell.execute_reply":"2022-01-25T06:57:52.133941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Investments\n\ninvestment_id column is the ID code for an investment. There are 3579 unique investments in training set and private test set will include new unseen investments. Investments appear only once in time IDs, so samples are time_id-investment_id combinations. In that case, visualization of sample counts as a function of time, is identical with unique number of investments in time IDs.","metadata":{}},{"cell_type":"code","source":"def visualize_investment_ids(df, column):\n    \n    print(f'{column}\\n{\"-\" * len(column)}')\n    print(f'Mean: {df[column].mean():.4f}  -  Median: {df[column].median():.4f}  -  Std: {df[column].std():.4f}')\n    print(f'Min: {df[column].min():.4f}  -  25%: {df[column].quantile(0.25):.4f}  -  50%: {df[column].quantile(0.5):.4f}  -  75%: {df[column].quantile(0.75):.4f}  -  Max: {df[column].max():.4f}')\n    print(f'Skew: {df[column].skew():.4f}  -  Kurtosis: {df[column].kurtosis():.4f}')\n    missing_count = df[df[column].isnull()].shape[0]\n    total_count = df.shape[0]\n    print(f'Missing Values: {missing_count}/{total_count} ({missing_count * 100 / total_count:.4f}%)')\n\n    fig, axes = plt.subplots(ncols=2, figsize=(24, 6), dpi=100)\n\n    sns.kdeplot(df[column], label=column, fill=True, ax=axes[0])\n    axes[0].axvline(df[column].mean(), label='Mean', color='r', linewidth=2, linestyle='--')\n    axes[0].axvline(df[column].median(), label='Median', color='b', linewidth=2, linestyle='--')\n    axes[0].legend(prop={'size': 15})\n    axes[1].plot(df.set_index('investment_id')[column], label=column)\n    \n    for i in range(2):\n        axes[i].tick_params(axis='x', labelsize=12.5)\n        axes[i].tick_params(axis='y', labelsize=12.5)\n        axes[i].set_ylabel('')\n    axes[0].set_xlabel('')\n    axes[1].set_xlabel('investment_id', fontsize=12.5)\n    axes[0].set_title(f'{column} Distribution', fontsize=15, pad=12)\n    axes[1].set_title(f'{column} as a Function of Investment', fontsize=15, pad=12)\n    \n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-26T08:09:22.714298Z","iopub.execute_input":"2022-01-26T08:09:22.714765Z","iopub.status.idle":"2022-01-26T08:09:22.731279Z","shell.execute_reply.started":"2022-01-26T08:09:22.714698Z","shell.execute_reply":"2022-01-26T08:09:22.730516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Number of samples per investment is quite different as expected. This is related to the period between time IDs 350 and 550 at when most of the investments are not observed. Number of samples per investment forms a bimodal distribution. Investments with low number of samples are centered around 400 sample count, and investments with high number of samples are centered around 1100 sample count.","metadata":{}},{"cell_type":"code","source":"df = df_train.groupby('investment_id')['target'].count().reset_index().rename(columns={'target': 'sample_counts_in_investment_ids'})\nvisualize_investment_ids(df, 'sample_counts_in_investment_ids')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-26T08:09:23.497785Z","iopub.execute_input":"2022-01-26T08:09:23.498317Z","iopub.status.idle":"2022-01-26T08:09:24.177494Z","shell.execute_reply.started":"2022-01-26T08:09:23.498268Z","shell.execute_reply":"2022-01-26T08:09:24.176208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Target means in investments are centered around 0 just like target means in time IDs. It has a very symmetrical and balanced distribution with some small peaks. One investment has a very high target mean (0.795) which could be an outlier. That investment might have higher number of samples between time IDs 350 and 550 because target means are higher at that period. ","metadata":{}},{"cell_type":"code","source":"df = df_train.groupby('investment_id')['target'].mean().reset_index().rename(columns={'target': 'target_means_in_investment_ids'})\nvisualize_investment_ids(df, 'target_means_in_investment_ids')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-26T07:29:36.512532Z","iopub.execute_input":"2022-01-26T07:29:36.512935Z","iopub.status.idle":"2022-01-26T07:29:37.134992Z","shell.execute_reply.started":"2022-01-26T07:29:36.512893Z","shell.execute_reply":"2022-01-26T07:29:37.133799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Target standard deviations in investments are more stable than target means except one investment. That investment has 0 target standard deviation and it has only two samples. That investment's low target standard deviation is related to its low sample count so it might not be an outlier.","metadata":{}},{"cell_type":"code","source":"df = df_train.groupby('investment_id')['target'].std().reset_index().rename(columns={'target': 'target_stds_in_investment_ids'})\nvisualize_investment_ids(df, 'target_stds_in_investment_ids')","metadata":{"execution":{"iopub.status.busy":"2022-01-26T07:29:40.408523Z","iopub.execute_input":"2022-01-26T07:29:40.408853Z","iopub.status.idle":"2022-01-26T07:29:40.975024Z","shell.execute_reply.started":"2022-01-26T07:29:40.408817Z","shell.execute_reply":"2022-01-26T07:29:40.97348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Features\n\nThere are 300 anonymized continuous features in dataset and they are named from f_0 to f_299. All of the feature distributions, target interactions, feature means and standard deviations along time and investment axis are visualized. There are little summaries of statistical properties displayed before the visualizations.","metadata":{}},{"cell_type":"code","source":"def visualize_feature(df, column):\n    \n    print(f'{column}\\n{\"-\" * len(column)}')\n    print(f'Mean: {df[column].mean():.4f}  -  Median: {df[column].median():.4f}  -  Std: {df[column].std():.4f}')\n    print(f'Min: {df[column].min():.4f}  -  25%: {df[column].quantile(0.25):.4f}  -  50%: {df[column].quantile(0.5):.4f}  -  75%: {df[column].quantile(0.75):.4f}  -  Max: {df[column].max():.4f}')\n    print(f'Skew: {df[column].skew():.4f}  -  Kurtosis: {df[column].kurtosis():.4f}')\n    missing_count = df[df[column].isnull()].shape[0]\n    total_count = df.shape[0]\n    print(f'Missing Values: {missing_count}/{total_count} ({missing_count * 100 / total_count:.4f}%)')\n\n    fig, axes = plt.subplots(ncols=2, nrows=3, figsize=(24, 22), dpi=100)\n\n    sns.kdeplot(df[column], label=column, fill=True, ax=axes[0][0])\n    axes[0][0].axvline(df[column].mean(), label='Mean', color='r', linewidth=2, linestyle='--')\n    axes[0][0].axvline(df[column].median(), label='Median', color='b', linewidth=2, linestyle='--')\n    axes[0][0].legend(prop={'size': 15})\n    sns.scatterplot(x=df[column], y=df['target'], ax=axes[0][1])\n    \n    df_feature_means_in_time_ids = df_train.groupby('time_id')[column].mean().reset_index().rename(columns={column: f'{column}_means_in_time_ids'})\n    axes[1][0].plot(df_feature_means_in_time_ids.set_index('time_id')[f'{column}_means_in_time_ids'], label=f'{column}_means_in_time_ids')\n    df_feature_stds_in_time_ids = df_train.groupby('time_id')[column].std().reset_index().rename(columns={column: f'{column}_stds_in_time_ids'})\n    axes[1][1].plot(df_feature_stds_in_time_ids.set_index('time_id')[f'{column}_stds_in_time_ids'], label=f'{column}_stds_in_time_ids')\n    \n    df_feature_means_in_investment_ids = df_train.groupby('investment_id')[column].mean().reset_index().rename(columns={column: f'{column}_means_in_investment_ids'})\n    axes[2][0].plot(df_feature_means_in_investment_ids.set_index('investment_id')[f'{column}_means_in_investment_ids'], label=f'{column}_means_in_investment_ids')\n    df_feature_stds_in_investment_ids = df_train.groupby('investment_id')[column].std().reset_index().rename(columns={column: f'{column}_stds_in_investment_ids'})\n    axes[2][1].plot(df_feature_stds_in_investment_ids.set_index('investment_id')[f'{column}_stds_in_investment_ids'], label=f'{column}_stds_in_investment_ids')\n\n    for i in range(3):\n        for j in range(2):\n            axes[i][j].tick_params(axis='x', labelsize=12.5)\n            axes[i][j].tick_params(axis='y', labelsize=12.5)\n            axes[i][j].set_ylabel('')\n            \n    axes[0][0].set_xlabel('')\n    axes[0][1].set_xlabel(column, fontsize=12.5)\n    axes[0][1].set_ylabel('target', fontsize=12.5)\n    \n    for i in range(2):\n        axes[1][i].set_xlabel('time_id', fontsize=12.5)\n        axes[1][i].set_ylabel(column, fontsize=12.5)\n        \n    for i in range(2):\n        axes[2][i].set_xlabel('investment_id', fontsize=12.5)\n        axes[2][i].set_ylabel(column, fontsize=12.5)\n        \n    axes[0][0].set_title(f'{column} Distribution', fontsize=15, pad=12)\n    axes[0][1].set_title(f'{column} vs Target', fontsize=15, pad=12)\n    axes[1][0].set_title(f'{column} Means as a Function of Time', fontsize=15, pad=12)\n    axes[1][1].set_title(f'{column} Stds as a Function of Time', fontsize=15, pad=12)\n    axes[2][0].set_title(f'{column} Means as a Function of Investment', fontsize=15, pad=12)\n    axes[2][1].set_title(f'{column} Stds as a Function of Investment', fontsize=15, pad=12)\n    \n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-26T08:10:47.340561Z","iopub.execute_input":"2022-01-26T08:10:47.341797Z","iopub.status.idle":"2022-01-26T08:10:47.369512Z","shell.execute_reply.started":"2022-01-26T08:10:47.341736Z","shell.execute_reply":"2022-01-26T08:10:47.368275Z"},"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All of the features are zero-centered and they have standard deviation of one since they are standardized during the anonymization process. Most of the features have symmetrical normal distributions but some of them have very extreme outliers which are skewing their distributions.\n\nFeature means and standard deviations vary between different time IDs and investments. It looks like feature means and standard deviations are dependent to time. They make sharp transitions on some periods. Feature standard deviations are more likely to make sharp transitions on different periods however feature mean outliers are observed in the same period most of the time. Feature means and standard deviations per investment looks randomly distributed among investments because it is related to those investment's time IDs.","metadata":{}},{"cell_type":"code","source":"for i in range(300):\n    visualize_feature(df_train, f'f_{i}')","metadata":{"execution":{"iopub.status.busy":"2022-01-26T08:12:47.253673Z","iopub.execute_input":"2022-01-26T08:12:47.254026Z","iopub.status.idle":"2022-01-26T08:20:09.785014Z","shell.execute_reply.started":"2022-01-26T08:12:47.253991Z","shell.execute_reply":"2022-01-26T08:20:09.783544Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Time Series\n\nTraining set is already in time series format as it is sorted by time_id and investment_id columns. Time series of an investment can be selected by directly indexing the investment_id. However, it might be risky to use time series models for predicting target for couple reasons. First, there are lots missing time IDs in different investments. Second, gap between time IDs is not constant and will be shorter in final private test set. Third, some of the investments have very few samples. Those things makes it hard to utilize time property of data.","metadata":{}},{"cell_type":"code","source":"def visualize_time_series(df, investment_id):\n    \n    df_investment = df.loc[df['investment_id'] == investment_id]\n\n    fig, ax = plt.subplots(figsize=(24, 8), dpi=100)\n    ax.plot(df_investment.set_index('time_id')['target'], label='target', linewidth=3)\n    for i in range(300):\n        ax.plot(df_investment.set_index('time_id')[f'f_{i}'], alpha=0.05)\n\n    ax.set_xlabel('time_id', fontsize=12.5)\n    ax.set_ylabel('Features', fontsize=12.5)\n    ax.tick_params(axis='x', labelsize=12.5)\n    ax.tick_params(axis='y', labelsize=12.5)\n    ax.set_title(f'Investment {investment_id} - Features and Target Along Time Axis', fontsize=15, pad=12.5)\n    ax.legend(prop={'size': 15})\n    \n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:10:04.921359Z","iopub.execute_input":"2022-02-01T10:10:04.921792Z","iopub.status.idle":"2022-02-01T10:10:04.932184Z","shell.execute_reply.started":"2022-02-01T10:10:04.92175Z","shell.execute_reply":"2022-02-01T10:10:04.931008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_time_series(df_train, 0)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-02-01T10:10:05.399895Z","iopub.execute_input":"2022-02-01T10:10:05.400248Z","iopub.status.idle":"2022-02-01T10:10:07.128724Z","shell.execute_reply.started":"2022-02-01T10:10:05.400206Z","shell.execute_reply":"2022-02-01T10:10:07.127561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Feature Sequences\n\nEven though there is no order in features, feature sequences can show some insights about target. Samples with lowest and highest target value are visualized below. Their feature sequences are quite different from each other. Some of the peaks and pits are almost occured in the same places but those occurences don't necessarily suggest that features are sequential. It might be related to their time IDs being really close to each other since features were mostly dependent to time IDs.","metadata":{}},{"cell_type":"code","source":"def visualize_feature_sequence(df, idx):\n    \n    sample = df.loc[idx, [f'f_{i}' for i in range(300)]].reset_index(drop=True)\n    target = df.loc[idx, 'target']\n    investment_id = df.loc[idx, 'investment_id']\n    time_id = df.loc[idx, 'time_id']\n\n    fig, ax = plt.subplots(figsize=(24, 8), dpi=100)\n    ax.plot(sample, linewidth=3)\n    \n    ax.set_xlabel('Features (f_#)', fontsize=12.5)\n    ax.set_ylabel('Values', fontsize=12.5)\n    ax.tick_params(axis='x', labelsize=12.5)\n    ax.tick_params(axis='y', labelsize=12.5)\n    ax.set_title(f'Investment {investment_id} - Time ID {time_id} - Target {target:.6f}', fontsize=15, pad=12.5)\n    ax.legend(prop={'size': 15})\n    \n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:10:16.595357Z","iopub.execute_input":"2022-02-01T10:10:16.59619Z","iopub.status.idle":"2022-02-01T10:10:16.605219Z","shell.execute_reply.started":"2022-02-01T10:10:16.596138Z","shell.execute_reply":"2022-02-01T10:10:16.604425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_feature_sequence(df_train, 1621688)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:10:16.979453Z","iopub.execute_input":"2022-02-01T10:10:16.979755Z","iopub.status.idle":"2022-02-01T10:10:17.295548Z","shell.execute_reply.started":"2022-02-01T10:10:16.979724Z","shell.execute_reply":"2022-02-01T10:10:17.293842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_feature_sequence(df_train, 1639094)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:10:17.297437Z","iopub.execute_input":"2022-02-01T10:10:17.297755Z","iopub.status.idle":"2022-02-01T10:10:17.634832Z","shell.execute_reply.started":"2022-02-01T10:10:17.297704Z","shell.execute_reply":"2022-02-01T10:10:17.633651Z"},"trusted":true},"execution_count":null,"outputs":[]}]}