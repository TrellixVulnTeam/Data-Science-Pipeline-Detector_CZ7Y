{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport lightgbm as lgbm\nfrom lightgbm import *","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-14T04:41:25.189209Z","iopub.execute_input":"2022-02-14T04:41:25.189658Z","iopub.status.idle":"2022-02-14T04:41:25.202032Z","shell.execute_reply.started":"2022-02-14T04:41:25.189611Z","shell.execute_reply":"2022-02-14T04:41:25.201423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One of the most commonly used terms in financial market data analysis is \"regime\". A regime refers to a market environment. In general, the market environment is known to vary depending on the time of year, and can be divided into markets with a continuing uptrend, markets with little fluctuation, and so on. When this regime is known, it is useful for forecasting, as it allows us to infer that a bull market is more likely to rise than the usual market environment.\n\nThere are various methods to estimate the market regime, but in this study, I used clustering by the k-means method, which is one of the unsupervised learning methods, to estimate the regime.\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T04:24:30.140278Z","iopub.execute_input":"2022-02-14T04:24:30.140587Z","iopub.status.idle":"2022-02-14T04:25:09.764634Z","shell.execute_reply.started":"2022-02-14T04:24:30.140557Z","shell.execute_reply":"2022-02-14T04:25:09.763831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n  \n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    \n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n     \n    return df\n\ndf = reduce_mem_usage(df)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T04:25:09.766Z","iopub.execute_input":"2022-02-14T04:25:09.76637Z","iopub.status.idle":"2022-02-14T04:28:14.50649Z","shell.execute_reply.started":"2022-02-14T04:25:09.766336Z","shell.execute_reply":"2022-02-14T04:28:14.505755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Divide the data frame into train, val, and test data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold, train_test_split\n\n\nfeatures = [f'f_{i}' for i in range(300)]\ntarget = 'target'\n\ndf = df.drop(range(0,10001)) \n\ndf_features = df[features]\n\nX_train, X, Y_train, Y = train_test_split(df_features, df[target], train_size=0.6, shuffle=False)\n\ndf = [[]]\ndf_features = [[]]\n\nX_val, X_test, Y_val, Y_test = train_test_split(X, Y, train_size=0.5, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T04:28:14.508758Z","iopub.execute_input":"2022-02-14T04:28:14.509228Z","iopub.status.idle":"2022-02-14T04:28:34.747857Z","shell.execute_reply.started":"2022-02-14T04:28:14.509189Z","shell.execute_reply":"2022-02-14T04:28:34.7471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Find the optimal number of clusters for k-means.","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.cluster import KMeans\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\n# Elbow Method\nwcss = []\n\nfor i in range(1, 10):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 30, random_state = 0)\n    kmeans.fit(X_train.head(10000))\n    wcss.append(kmeans.inertia_)\n\n\nplt.plot(range(1, 10), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS') \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T04:28:34.748799Z","iopub.execute_input":"2022-02-14T04:28:34.748985Z","iopub.status.idle":"2022-02-14T04:31:14.269252Z","shell.execute_reply.started":"2022-02-14T04:28:34.748962Z","shell.execute_reply":"2022-02-14T04:31:14.268434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the elbow method, the best cluster is the one where the slope is gentle. In this case, the optimal number of clusters is 5.\n\nNow that we know that the optimal number of clusters is 5, I model with n_clusters=5 and add the result to the feature set.","metadata":{}},{"cell_type":"code","source":"# modeling\nclf = KMeans(n_clusters=5, random_state=0)\nclf.fit(X_train.head(10000))\n\nclf.labels_\n\n\n\n\ny_pred_train = clf.predict(X_train)\nX_train['k-means'] = y_pred_train\ny_pred_val = clf.predict(X_val)\nX_val['k-means'] = y_pred_val\ny_pred_test = clf.predict(X_test)\nX_test['k-means'] = y_pred_test","metadata":{"execution":{"iopub.status.busy":"2022-02-14T04:31:14.270818Z","iopub.execute_input":"2022-02-14T04:31:14.271434Z","iopub.status.idle":"2022-02-14T04:31:38.149974Z","shell.execute_reply.started":"2022-02-14T04:31:14.2714Z","shell.execute_reply":"2022-02-14T04:31:38.149268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nimport numpy as np\nimport lightgbm as lgb\nfrom scipy.stats import pearsonr\n\nwarnings.simplefilter('ignore')\n\nlgb_train = lgb.Dataset(X_train, Y_train)\nlgb_eval = lgb.Dataset(X_val, Y_val, reference=lgb_train)\n\nparams = {'seed': 1,\n          'verbose' : -1,\n           'objective': \"regression\",\n           'learning_rate': 0.02,\n           'bagging_fraction': 0.1,\n           'bagging_freq': 1,\n           'feature_fraction': 0.1,\n           'max_depth': 6,\n           'min_child_samples': 50,\n           'num_leaves': 64}\n        \n        \ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=1100,\n                valid_sets=lgb_eval,\n                verbose_eval=False,\n                early_stopping_rounds=10,\n                )\n\n\nY_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n\nscore_tuple = pearsonr(Y_test, Y_pred)\nscore = score_tuple[0]\nprint(f\"Validation Pearsonr score : {score:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-14T04:31:38.151504Z","iopub.execute_input":"2022-02-14T04:31:38.151952Z","iopub.status.idle":"2022-02-14T04:33:27.810727Z","shell.execute_reply.started":"2022-02-14T04:31:38.15192Z","shell.execute_reply":"2022-02-14T04:33:27.80993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n\nfeature = gbm.feature_importance(importance_type='gain')\n\n\nf = pd.DataFrame({'number': range(0, len(feature)),\n             'feature': feature[:]})\nf2 = f.sort_values('feature',ascending=False)\n\n#features' name\nlabel = X_train.columns[0:]\n\n#feature rank\nindices = np.argsort(feature)[::-1]\n\nfor i in range(len(feature)):\n    print(str(i + 1) + \"   \" + str(label[indices[i]]) + \"   \" + str(feature[indices[i]]))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-14T04:33:27.812223Z","iopub.execute_input":"2022-02-14T04:33:27.81304Z","iopub.status.idle":"2022-02-14T04:33:27.970047Z","shell.execute_reply.started":"2022-02-14T04:33:27.813003Z","shell.execute_reply":"2022-02-14T04:33:27.969469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we check the 282th position of the feature importance, we can see that \"k-means\" is indeed effective.\n\n","metadata":{}}]}