{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I have previously posted the idea that clustering using the k-means method could be used to identify market regimes. (Link [here](https://www.kaggle.com/code/tmrtj9999/regime-classification-by-k-means/notebook))\n\nTo summarize the content of this article, each row is clustered by the k-means method as it is, using the given 300 features, and the classification result is considered as a regime.\n\nWhen I published this code, someone commented to me that it might be possible to classify regimes by aggregating features for each time_id and clustering each time_id using the aggregate features.\n\nTherefore, I would like to take the average of each feature for each time_id and use the aggregate features for clustering.","metadata":{}},{"cell_type":"markdown","source":"First, train data is read into a DataFrame.","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport lightgbm as lgbm\nfrom lightgbm import *","metadata":{"execution":{"iopub.status.busy":"2022-03-22T04:54:51.871587Z","iopub.execute_input":"2022-03-22T04:54:51.871838Z","iopub.status.idle":"2022-03-22T04:54:51.884208Z","shell.execute_reply.started":"2022-03-22T04:54:51.871809Z","shell.execute_reply":"2022-03-22T04:54:51.883378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')","metadata":{"execution":{"iopub.status.busy":"2022-03-22T04:54:51.895335Z","iopub.execute_input":"2022-03-22T04:54:51.895621Z","iopub.status.idle":"2022-03-22T04:54:56.126546Z","shell.execute_reply.started":"2022-03-22T04:54:51.895594Z","shell.execute_reply":"2022-03-22T04:54:56.125835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In addition, I would like to visualize the results of the regime estimation by means of a graph.\nIn doing so, the number of investment_ids for each time_id will be displayed for comparison.\n\nIt is the following graph.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nnum_investment = df.groupby('time_id').count()\n\ntime_id = num_investment.index\nnum = num_investment['investment_id']\n\nplt.plot(time_id, num)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T04:54:56.131312Z","iopub.execute_input":"2022-03-22T04:54:56.133282Z","iopub.status.idle":"2022-03-22T04:55:02.407182Z","shell.execute_reply.started":"2022-03-22T04:54:56.133244Z","shell.execute_reply":"2022-03-22T04:55:02.406159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We do the necessary prep work.\n\nMemory is saved and unnecessary columns are removed.","metadata":{}},{"cell_type":"code","source":"del df['row_id']\ndel df['investment_id']\ndel df['target']","metadata":{"execution":{"iopub.status.busy":"2022-03-22T04:55:02.408403Z","iopub.execute_input":"2022-03-22T04:55:02.408631Z","iopub.status.idle":"2022-03-22T04:55:02.477898Z","shell.execute_reply.started":"2022-03-22T04:55:02.408603Z","shell.execute_reply":"2022-03-22T04:55:02.476904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n  \n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    \n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n     \n    return df\n\ndf = reduce_mem_usage(df)","metadata":{"execution":{"iopub.status.busy":"2022-03-22T04:55:02.479044Z","iopub.execute_input":"2022-03-22T04:55:02.479932Z","iopub.status.idle":"2022-03-22T04:56:34.954366Z","shell.execute_reply.started":"2022-03-22T04:55:02.479902Z","shell.execute_reply":"2022-03-22T04:56:34.953461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Take an average for each time_id.","metadata":{}},{"cell_type":"code","source":"df_agg = df.groupby('time_id').mean()\n\ndf = [[]]","metadata":{"execution":{"iopub.status.busy":"2022-03-22T04:56:34.955535Z","iopub.execute_input":"2022-03-22T04:56:34.955707Z","iopub.status.idle":"2022-03-22T04:56:42.616704Z","shell.execute_reply.started":"2022-03-22T04:56:34.955686Z","shell.execute_reply":"2022-03-22T04:56:42.615568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Determine the optimal number of clusters using the elbow method.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\n# Elbow Method\nwcss = []\n\nfor i in range(1, 10):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 30, random_state = 0)\n    kmeans.fit(df_agg)\n    wcss.append(kmeans.inertia_)\n\n\nplt.plot(range(1, 10), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS') \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T04:56:42.617862Z","iopub.execute_input":"2022-03-22T04:56:42.618064Z","iopub.status.idle":"2022-03-22T04:57:23.167039Z","shell.execute_reply.started":"2022-03-22T04:56:42.618036Z","shell.execute_reply":"2022-03-22T04:57:23.165816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The elbow method considers the optimal cluster to be the one where the slope is smooth. In this case, the optimal number of clusters is 4.\n\nSince I know that the optimal number of clusters is 4, I model with n_clusters=4.","metadata":{}},{"cell_type":"code","source":"clf = KMeans(n_clusters=4, random_state=0)\nclf.fit(df_agg)\n\ny_clustering = clf.predict(df_agg)\ndf_clustering = pd.DataFrame({'time_id': df_agg.index, 'cluster' : y_clustering})","metadata":{"execution":{"iopub.status.busy":"2022-03-22T04:57:23.168353Z","iopub.execute_input":"2022-03-22T04:57:23.168568Z","iopub.status.idle":"2022-03-22T04:57:24.578927Z","shell.execute_reply.started":"2022-03-22T04:57:23.168541Z","shell.execute_reply":"2022-03-22T04:57:24.578409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Separate DataFrames for each cluster.","metadata":{}},{"cell_type":"code","source":"df_0 = df_clustering[df_clustering['cluster'] == 0]\ndf_1 = df_clustering[df_clustering['cluster'] == 1]\ndf_2 = df_clustering[df_clustering['cluster'] == 2]\ndf_3 = df_clustering[df_clustering['cluster'] == 3]","metadata":{"execution":{"iopub.status.busy":"2022-03-22T04:57:24.582801Z","iopub.execute_input":"2022-03-22T04:57:24.58345Z","iopub.status.idle":"2022-03-22T04:57:24.594158Z","shell.execute_reply.started":"2022-03-22T04:57:24.583422Z","shell.execute_reply":"2022-03-22T04:57:24.593211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I'm ready to go.\nI will now visualize the clustering results.\n\nThe background of the time_ids classified in each cluster is visualized with a color.\nAlso, the blue line displayed is the number of investment_ids per time_id.","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport pandas as pd\n\n\n\n\nplt.plot(time_id, num)\n\n\n\n\n\nfor i in df_0['time_id'].values:\n    start_datetime0 = i - 0.01\n    end_datetime0 = i + 0.01\n    plt.axvspan(start_datetime0, end_datetime0, color=\"red\")\n\n\n\n\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-22T04:58:27.246455Z","iopub.execute_input":"2022-03-22T04:58:27.247214Z","iopub.status.idle":"2022-03-22T04:58:27.88799Z","shell.execute_reply.started":"2022-03-22T04:58:27.247186Z","shell.execute_reply":"2022-03-22T04:58:27.887286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although cluster 0 appears in full, we can see that it is particularly dense in the areas where the number of investment_ids has dropped significantly. From this, we can infer that this is probably the cluster corresponding to the crash.","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\nplt.plot(time_id, num)\n\n\n\n\n\n\nfor i in df_1['time_id'].values:\n    start_datetime1 = i - 0.01\n    end_datetime1 = i + 0.01\n    plt.axvspan(start_datetime1, end_datetime1, color=\"green\")\n    \n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T04:59:02.451724Z","iopub.execute_input":"2022-03-22T04:59:02.452186Z","iopub.status.idle":"2022-03-22T04:59:02.808786Z","shell.execute_reply.started":"2022-03-22T04:59:02.452144Z","shell.execute_reply":"2022-03-22T04:59:02.807874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cluster 1 seems to be concentrated in the second half of the year, not so much in the first half. given the trend in the number of investment_ids, I wonder if this cluster corresponds to a rising market.","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n\n\nplt.plot(time_id, num)\n\n\n\nfor i in df_2['time_id'].values:\n    start_datetime2 = i - 0.01\n    end_datetime2 = i + 0.01\n    plt.axvspan(start_datetime2, end_datetime2, color=\"yellow\")\n    \n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T04:59:20.688524Z","iopub.execute_input":"2022-03-22T04:59:20.68878Z","iopub.status.idle":"2022-03-22T04:59:21.091986Z","shell.execute_reply.started":"2022-03-22T04:59:20.688756Z","shell.execute_reply":"2022-03-22T04:59:21.090855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I think cluster 2 is unique in that it does not appear where the number of investment_ids decreases significantly.\nGiven this, we can assume that it is a cluster that corresponds to a time when the market is not very volatile.","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n\n\n\nplt.plot(time_id, num)\n\n\n\nfor i in df_3['time_id'].values:\n    start_datetime3 = i - 0.01\n    end_datetime3 = i + 0.01\n    plt.axvspan(start_datetime3, end_datetime3, color=\"aqua\")\n    \n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-22T04:59:38.266338Z","iopub.execute_input":"2022-03-22T04:59:38.26659Z","iopub.status.idle":"2022-03-22T04:59:39.086818Z","shell.execute_reply.started":"2022-03-22T04:59:38.266567Z","shell.execute_reply":"2022-03-22T04:59:39.085649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cluster 3 is found to appear in full. It may be a cluster that corresponds to a market with high volatility by elimination　method.","metadata":{}}]}