{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport lightgbm as lgbm\nfrom lightgbm import *","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In general, it is known that financial market data is prone to overfitting. Therefore, when applying machine learning to financial market prediction, it is necessary to focus on countermeasures against overfitting.\n\nFor example, in the Two Sigma Financial Modeling Challenge held by Kaggle in 2016, the top winners used Extra Trees. This is a method similar to Random Forest, but instead of combining multiple decision trees to make predictions, it creates trees with randomly selected features and makes predictions by bagging them. Since the selection of features in each tree is random, this method is less prone to overfitting compared to Random Forest.\n\nIn this code, I will introduce DAE (Denosing Autoencoder) as one of the methods to prevent overfitting. There are various types of DAE, but in this case, I will use Gaussian noise, which is noise that follows a normal distribution of values, as the input.This is a method of feature extraction by adding noise to the Autoencoder, and unlike the conventional Autoencoder, it is expected to be able to extract more robust features since noise is added.\n\nThere are various types of DAE, but in this article, we will introduce a DAE that adds Gaussian noise to the input, a noise that follows a normal distribution.\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n  \n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    \n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n     \n    return df\n\ndf = reduce_mem_usage(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The train data is divided into the feature data (X_0, X_1, . , X_4), and target data (Y_0, Y_1, ..., Y_4) in time series order.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold, train_test_split\n\n\nfeatures = [f'f_{i}' for i in range(300)]\ntarget = 'target'\n\nX_012, X_34, Y_012, Y_34 = train_test_split(df[features], df[target], train_size=0.6, shuffle=False)\n\nX_3, X_4, Y_3, Y_4 = train_test_split(X_34, Y_34, train_size=0.5, shuffle=False)\n\nX_0, X_12, Y_0, Y_12 = train_test_split(X_012, Y_012, train_size=0.33, shuffle=False)\n\nX_1, X_2, Y_1, Y_2 = train_test_split(X_12, Y_12, train_size=0.5, shuffle=False)\n\ndf = [[]]\n\nX_0 = X_0.reset_index()\nX_1 = X_1.reset_index()\nX_2 = X_2.reset_index()\nX_3 = X_3.reset_index()\nX_4 = X_4.reset_index()\n\nY_0 = Y_0.reset_index()\nY_1 = Y_1.reset_index()\nY_2 = Y_2.reset_index()\nY_3 = Y_3.reset_index()\nY_4 = Y_4.reset_index()\n\ndel X_0['index']\ndel X_1['index']\ndel X_2['index']\ndel X_3['index']\ndel X_4['index']\ndel Y_0['index']\ndel Y_1['index']\ndel Y_2['index']\ndel Y_3['index']\ndel Y_4['index']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, I will train the DAE with X_0 and Y_0, and use X_1 and Y_1 as validation data.","metadata":{}},{"cell_type":"code","source":"import keras\nfrom keras import layers\nfrom keras.layers import Input, Dense, BatchNormalization\nfrom keras.layers.noise import GaussianNoise\nfrom keras.models import Model, load_model\nfrom keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n\n# EaelyStopping\nearly_stopping =  EarlyStopping(\n                            monitor='val_loss',\n                            min_delta=0.0,\n                            patience=3,\n)\n\ninput_dim = X_0.shape[1]\nencoding_dim = 100\n\ninput_layer = Input(shape=(input_dim, ))\nencoder = GaussianNoise(0.1)(input_layer)\nencoder = Dense(encoding_dim, activation=\"tanh\")(encoder)\nencoder = BatchNormalization()(encoder)\nencoder = Dense(10, activation=\"relu\")(encoder)\n\ndecoder = Dense(10, activation='tanh')(encoder)\ndecoder = Dense(encoding_dim, activation='relu')(decoder)\noutput_layer = Dense(1, activation='tanh')(decoder)\n\nautoencoder = Model(inputs=input_layer, outputs=output_layer)\n\nautoencoder.compile(optimizer=\"adam\", loss='mean_squared_error', metrics=[\"mse\"])\n\nmodelCheckpoint = ModelCheckpoint(filepath = 'XXX.h5',\n                                  monitor='val_loss',\n                                  verbose=1,\n                                  save_best_only=True,\n                                  save_weights_only=False,\n                                  mode='min',\n                                  save_freq=1)\n\n\nautoencoder.fit(X_0, Y_0, batch_size=1440, epochs=11890,\n          validation_data=(X_1, Y_1),\n          callbacks=[early_stopping] # CallBacks\n            )","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, when the training is finished, apply the trained DAE to X_2~X_4 to create new features.","metadata":{}},{"cell_type":"code","source":"X_0 = [[]]\nX_1 = [[]]\nY_0 = []\nY_1 = []\ndf = [[]]\n\n\ndae= Model(input_layer, encoder)\n\nencoding_X_2 = dae.predict(X_2)\ndae_X_2 = pd.DataFrame(encoding_X_2)\nencoding_X_3 = dae.predict(X_3)\ndae_X_3 = pd.DataFrame(encoding_X_3)\nencoding_X_4 = dae.predict(X_4)\ndae_X_4 = pd.DataFrame(encoding_X_4)\n\nX_2 = pd.concat([X_2, dae_X_2], axis=1)\nX_3 = pd.concat([X_3, dae_X_3], axis=1)\nX_4 = pd.concat([X_4, dae_X_4], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For example, X_2 looks like this after using DAE.","metadata":{}},{"cell_type":"code","source":"print(X_2.head(4))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, I will use LightGBM.\nI will use X_2 and Y_2 for train data, X_3 and Y_3 for val data, and X_4 and Y_4 for test data.","metadata":{}},{"cell_type":"code","source":"import warnings\nimport numpy as np\nimport lightgbm as lgb\nfrom scipy.stats import pearsonr\n\nwarnings.simplefilter('ignore')\n\nlgb_train = lgb.Dataset(X_2, Y_2)\nlgb_eval = lgb.Dataset(X_3, Y_3, reference=lgb_train)\n\nparams = {'seed': 1,\n          'verbose' : -1,\n           'objective': \"regression\",\n           'learning_rate': 0.02,\n           'bagging_fraction': 0.2,\n           'bagging_freq': 1,\n           'feature_fraction': 0.3,\n           'max_depth': 5,\n           'min_child_samples': 50,\n           'num_leaves': 64}\n        \n        \ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=1000,\n                valid_sets=lgb_eval,\n                verbose_eval=False,\n                early_stopping_rounds=10,\n                )\n\n\nY_pred = gbm.predict(X_4, num_iteration=gbm.best_iteration)\nl_4 = Y_4['target'].values.tolist()\nscore_tuple = pearsonr(l_4, Y_pred)\nscore = score_tuple[0]\nprint(f\"Validation Pearsonr score : {score:.4f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When the LightGBM training is finished, display the feature importance.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n\nfeature = gbm.feature_importance(importance_type='gain')\n\n\nf = pd.DataFrame({'number': range(0, len(feature)),\n             'feature': feature[:]})\nf2 = f.sort_values('feature',ascending=False)\n\n#features' name\nlabel = X_2.columns[0:]\n\n#feature rank\nindices = np.argsort(feature)[::-1]\n\nfor i in range(len(feature)):\n    print(str(i + 1) + \"   \" + str(label[indices[i]]) + \"   \" + str(feature[indices[i]]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These '0'~'9' are the new features added by DAE.\nWhen we check the feature importance, we can see that the features added by DAE are indeed effective.","metadata":{}}]}