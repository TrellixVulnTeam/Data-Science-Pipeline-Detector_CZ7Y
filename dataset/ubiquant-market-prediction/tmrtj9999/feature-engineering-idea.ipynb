{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport lightgbm as lgbm\nfrom lightgbm import *","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:07:09.996487Z","iopub.execute_input":"2022-02-14T02:07:09.996743Z","iopub.status.idle":"2022-02-14T02:07:10.00944Z","shell.execute_reply.started":"2022-02-14T02:07:09.996713Z","shell.execute_reply":"2022-02-14T02:07:10.008644Z"},"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:07:10.010738Z","iopub.execute_input":"2022-02-14T02:07:10.011446Z","iopub.status.idle":"2022-02-14T02:07:22.721052Z","shell.execute_reply.started":"2022-02-14T02:07:10.011405Z","shell.execute_reply":"2022-02-14T02:07:22.720096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n  \n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    \n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n     \n    return df\n\ndf = reduce_mem_usage(df)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:07:22.723018Z","iopub.execute_input":"2022-02-14T02:07:22.723492Z","iopub.status.idle":"2022-02-14T02:10:41.751272Z","shell.execute_reply.started":"2022-02-14T02:07:22.723448Z","shell.execute_reply":"2022-02-14T02:10:41.750279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As EDA, many people are checking the distribution of features. For example, you can make a histogram like this. This is a histogram of 'f14'.","metadata":{}},{"cell_type":"code","source":"df['f_14'].hist(bins = 100)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:10:41.754585Z","iopub.execute_input":"2022-02-14T02:10:41.755257Z","iopub.status.idle":"2022-02-14T02:10:42.727384Z","shell.execute_reply.started":"2022-02-14T02:10:41.755208Z","shell.execute_reply":"2022-02-14T02:10:42.726508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the distributions of these feature values, some distributions are beautifully symmetrical, while others are asymmetrical. I don't know the actual calculation formulas, but it seems that these characteristics appear due to the way the feature values are calculated. Therefore, I thought of incorporating how much the values of these features are out of the middle as a feature value. The specific formula for this is\n\nabs(feature value - median value of feature value) \n\n","metadata":{}},{"cell_type":"markdown","source":"I will implement the code and test the hypothesis in the following.","metadata":{}},{"cell_type":"markdown","source":"show 'f_146' 's histogram","metadata":{}},{"cell_type":"code","source":"df['f_146'].hist(bins = 100)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:10:42.729017Z","iopub.execute_input":"2022-02-14T02:10:42.729334Z","iopub.status.idle":"2022-02-14T02:10:43.838697Z","shell.execute_reply.started":"2022-02-14T02:10:42.729293Z","shell.execute_reply":"2022-02-14T02:10:43.837819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, this is a beautiful symmetrical distribution.\nNext, let's process the features.\nThis time, we will see if the processing of 'f146' is working, but since we are here, we will calculate the difference from the median from the median all together.","metadata":{}},{"cell_type":"code","source":"features = [f'f_{i}' for i in range(300)]\ndf_median = df[features].head(30000).median()\nprint(df_median)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:10:43.840247Z","iopub.execute_input":"2022-02-14T02:10:43.840782Z","iopub.status.idle":"2022-02-14T02:10:46.98024Z","shell.execute_reply.started":"2022-02-14T02:10:43.840746Z","shell.execute_reply":"2022-02-14T02:10:46.979361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(300):\n    df[f'f_median_{i}'] = abs(df[f'f_{i}']-df_median[f'f_{i}'])\n\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:10:46.981434Z","iopub.execute_input":"2022-02-14T02:10:46.981749Z","iopub.status.idle":"2022-02-14T02:11:04.315634Z","shell.execute_reply.started":"2022-02-14T02:10:46.981717Z","shell.execute_reply":"2022-02-14T02:11:04.314703Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To check if this feature is actually working, we will use LightGBM to check the feature importance.\n\nWe will split the data into train, val, and test data, making sure that the rows used for the median calculation are not included in the training data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold, train_test_split\n\n#\ndf = df.tail(3101410)\nfeatures = [f'f_{i}' for i in range(300)] + [f'f_median_146']\ntarget = 'target'\n\ndf_features = df[features]\n\nX_train, X, Y_train, Y = train_test_split(df_features, df[target], train_size=0.6, shuffle=False)\n\ndf = [[]]\ndf_features = [[]]\n\nX_val, X_test, Y_val, Y_test = train_test_split(X, Y, train_size=0.5, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:11:04.31706Z","iopub.execute_input":"2022-02-14T02:11:04.31764Z","iopub.status.idle":"2022-02-14T02:11:20.06932Z","shell.execute_reply.started":"2022-02-14T02:11:04.317591Z","shell.execute_reply":"2022-02-14T02:11:20.068488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import warnings\nimport numpy as np\nimport lightgbm as lgb\nfrom scipy.stats import pearsonr\n\nwarnings.simplefilter('ignore')\n\nlgb_train = lgb.Dataset(X_train, Y_train)\nlgb_eval = lgb.Dataset(X_val, Y_val, reference=lgb_train)\n\n\nparams = {'seed': 1,\n          'verbose' : -1,\n           'objective': \"regression\",\n           'learning_rate': 0.02,\n           'bagging_fraction': 0.2,\n           'bagging_freq': 1,\n           'feature_fraction': 0.3,\n           'max_depth': 5,\n           'min_child_samples': 50,\n           'num_leaves': 64}\n\n\n        \n        \ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=1000,\n                valid_sets=lgb_eval,\n                verbose_eval=False,\n                early_stopping_rounds=5,\n                )\n\n\nY_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n\nscore_tuple = pearsonr(Y_test, Y_pred)\nscore = score_tuple[0]\nprint(f\"Validation Pearsonr score : {score:.4f}\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-14T03:07:56.584043Z","iopub.execute_input":"2022-02-14T03:07:56.584433Z","iopub.status.idle":"2022-02-14T03:07:58.601748Z","shell.execute_reply.started":"2022-02-14T03:07:56.584339Z","shell.execute_reply":"2022-02-14T03:07:58.600623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n\n\n\nfeature = gbm.feature_importance(importance_type='gain')\n\n\nf = pd.DataFrame({'number': range(0, len(feature)),\n             'feature': feature[:]})\nf2 = f.sort_values('feature',ascending=False)\n\n\nlabel = X_train.columns[0:]\n\n\nindices = np.argsort(feature)[::-1]\n\nfor i in range(len(feature)):\n   print(str(i + 1) + \"   \" + str(label[indices[i]]) + \"   \" + str(feature[indices[i]]))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:14:54.741506Z","iopub.execute_input":"2022-02-14T02:14:54.741913Z","iopub.status.idle":"2022-02-14T02:14:54.799254Z","shell.execute_reply.started":"2022-02-14T02:14:54.741861Z","shell.execute_reply":"2022-02-14T02:14:54.798225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we check the 246th position of the feature importance, we can see that \"f_median_146\" is indeed effective.\n\nThere are other features with symmetrical distributions, so it is likely that there are features that can be processed in a similar way.","metadata":{}}]}