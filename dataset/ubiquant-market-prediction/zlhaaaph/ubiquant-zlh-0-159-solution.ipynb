{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nseed = 69\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n# os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\" # fix\n# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \nos.environ[\"PYTHONHASHSEED\"] = str(seed)\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport gc\nimport joblib\nimport pickle\nimport subprocess\nimport logging\nimport datetime as dtm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\nfrom xgboost.sklearn import XGBRegressor\nfrom catboost import CatBoostRegressor\n# from autogluon.tabular import TabularPredictor\nfrom sklearn.metrics import accuracy_score\nfrom scipy import stats\nimport tensorflow as tf\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\nfrom tensorflow.keras.layers import Lambda\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# set seed\ntf.random.set_seed(seed)\nrandom.seed(seed)\nnp.random.seed(seed)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:00:35.267134Z","iopub.execute_input":"2022-04-18T05:00:35.267443Z","iopub.status.idle":"2022-04-18T05:00:43.041747Z","shell.execute_reply.started":"2022-04-18T05:00:35.267358Z","shell.execute_reply":"2022-04-18T05:00:43.041004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weighted_average(a):\n    w = []\n    n = len(a)\n    for j in range(1, n + 1):\n        j = 2 if j == 1 else j\n        w.append(1 / (2**(n + 1 - j)))\n    return np.average(a, weights = w)\n\n\n            \nlogging.basicConfig(\n    format=\n    \"%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s\",\n    datefmt=\"[%Y-%m-%d %H:%M:%S]\",\n    level=logging.INFO)\nlogger = logging.getLogger(__name__)\ndef reduce_mem(df, use_float16=False):\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    tm_cols = df.select_dtypes('datetime').columns\n    for col in df.columns:\n        if col in tm_cols:\n            continue\n        col_type = df[col].dtypes\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(\n                        np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(\n                        np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(\n                        np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(\n                        np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if use_float16 and c_min > np.finfo(\n                        np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(\n                        np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(\n        start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndef mkdirs(dir2make):\n    if isinstance(dir2make, list):\n        for i_dir in dir2make:\n            if not os.path.exists(i_dir):\n                os.makedirs(i_dir)\n    elif isinstance(dir2make, str):\n        if not os.path.exists(dir2make):\n            os.makedirs(dir2make)\n    else:\n        raise ValueError(\"dir2make should be string or list type.\")\n\nBASE_DIR_cwd = os.getcwd()\nBASE_DIR = BASE_DIR_cwd\n\n\nclass Cache():\n    @staticmethod\n    def cache_data(data, nm_marker=None, dt_format='%Y%m%d_%Hh'):\n        mkdirs(os.path.join(BASE_DIR, 'cached_data'))\n        name_ = dtm.datetime.now().strftime(dt_format)\n        if nm_marker is not None:\n            name_ = nm_marker\n        path_ = os.path.join(BASE_DIR, f'cached_data/CACHE_{name_}.pkl')\n        with open(path_, 'wb') as file:\n            pickle.dump(data, file, protocol=4)\n        logger.info(f'Cache Successfully! File name: {path_}')\n\n    @staticmethod\n    def reload_cache(file_nm,\n                     pure_nm=False,\n                     base_dir=None,\n                     prefix='CACHE_',\n                     postfix='.pkl'):\n        if pure_nm:\n            file_nm = prefix + file_nm + postfix\n        if base_dir is None:\n            base_dir = os.path.join(BASE_DIR, 'cached_data')\n        file_path = os.path.join(base_dir, file_nm)\n        with open(file_path, 'rb') as file:\n            data = pickle.load(file)\n        logger.info(f'Successfully Reload: {file_path}')\n        return data","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:00:43.043357Z","iopub.execute_input":"2022-04-18T05:00:43.043587Z","iopub.status.idle":"2022-04-18T05:00:43.068114Z","shell.execute_reply.started":"2022-04-18T05:00:43.043555Z","shell.execute_reply":"2022-04-18T05:00:43.06743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\n\ndata_types_dict = {\n    'time_id': 'int32',\n    'investment_id': 'int32',\n    \"target\": 'float16',\n}\n\nfor f in features:\n    data_types_dict[f] = 'float16'","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:00:43.06926Z","iopub.execute_input":"2022-04-18T05:00:43.069696Z","iopub.status.idle":"2022-04-18T05:00:43.089642Z","shell.execute_reply.started":"2022-04-18T05:00:43.069647Z","shell.execute_reply":"2022-04-18T05:00:43.088916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npath = \"/kaggle/input/ubiquant-market-prediction\"\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\nwith open('../input/ubiquant-time-id-mapping/mapping_dict_all_2014_2022.pkl', 'rb') as file:\n    mapping_dict = pickle.load(file)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:01:02.949941Z","iopub.execute_input":"2022-04-18T05:01:02.950204Z","iopub.status.idle":"2022-04-18T05:01:16.766217Z","shell.execute_reply.started":"2022-04-18T05:01:02.950174Z","shell.execute_reply":"2022-04-18T05:01:16.765378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add time feature\nimport datetime as dt\ndf_time = train[[\"time_id\"]].drop_duplicates()\ndf_time[\"trade_date\"] = df_time[\"time_id\"].map(dict(zip(mapping_dict.values(),mapping_dict.keys())))\ndf_time[\"trade_date\"] = df_time[\"trade_date\"].map(lambda x:dt.datetime.strptime(str(x),\"%Y%m%d\"))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:01:29.6276Z","iopub.execute_input":"2022-04-18T05:01:29.62809Z","iopub.status.idle":"2022-04-18T05:01:30.88978Z","shell.execute_reply.started":"2022-04-18T05:01:29.62805Z","shell.execute_reply":"2022-04-18T05:01:30.888877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import calendar\n\ndef week_of_month(tgtdate):\n    days_this_month = calendar.mdays[tgtdate.month]\n    for i in range(1, days_this_month):\n        d = dt.datetime(tgtdate.year, tgtdate.month, i)\n        if d.day - d.weekday() > 0:\n            startdate = d\n            break\n    # now we canuse the modulo 7 appraoch\n    return (tgtdate - startdate).days //7 + 1","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:01:30.891192Z","iopub.execute_input":"2022-04-18T05:01:30.89142Z","iopub.status.idle":"2022-04-18T05:01:30.89804Z","shell.execute_reply.started":"2022-04-18T05:01:30.891387Z","shell.execute_reply":"2022-04-18T05:01:30.897392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_time_feature(df_time):\n    df_time[\"trade_date_year\"]=df_time[\"trade_date\"].map(lambda x:x.year)\n    df_time[\"trade_date_month\"]=df_time[\"trade_date\"].map(lambda x:x.month)\n    df_time[\"trade_date_weekday\"]=df_time[\"trade_date\"].map(lambda x:x.isoweekday())\n    df_time[\"trade_date_day\"]=df_time[\"trade_date\"].map(lambda x:x.day)\n    df_time[\"trade_date_year_rank\"]=df_time[\"time_id\"].groupby(df_time[\"trade_date_year\"]).rank()\n    df_time[\"week_of_month\"] = df_time[\"trade_date\"].map(lambda x:week_of_month(x))\n    return df_time\ndf_time = get_time_feature(df_time)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:01:33.757772Z","iopub.execute_input":"2022-04-18T05:01:33.758348Z","iopub.status.idle":"2022-04-18T05:01:33.822813Z","shell.execute_reply.started":"2022-04-18T05:01:33.758302Z","shell.execute_reply":"2022-04-18T05:01:33.82218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.merge(df_time[[\"time_id\",\"trade_date_year\",\"trade_date_month\",\"trade_date_weekday\",\"trade_date_day\",\"trade_date_year_rank\",\"week_of_month\"]]\n                    ,on=\"time_id\",how=\"left\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:01:35.247384Z","iopub.execute_input":"2022-04-18T05:01:35.247625Z","iopub.status.idle":"2022-04-18T05:01:42.741206Z","shell.execute_reply.started":"2022-04-18T05:01:35.247598Z","shell.execute_reply":"2022-04-18T05:01:42.740505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nclass TimeCache:\n    def __init__(self):\n        self.lasttime = {}  # 上次出现的time_id\n        self.lastfelist = {}\n        \n    def set_time_value(self, key, value):\n        self.lasttime[key] = value\n\n    def get_time_value(self, key, value_now):\n        if key in self.lasttime:\n            value_last = self.lasttime[key]\n            self.lasttime[key] = value_now\n            return value_last\n        else:\n            self.lasttime[key] = value_now\n            return None\n        \n    def get_fe_value(self, key, fe, value_now):\n        if key in self.lastfelist:\n            value_last = self.lastfelist[key][fe]\n            self.lastfelist[key][fe] = value_now\n            return value_last\n        else:\n            return value_now\n        \n    def set_fe_value(self, key, fe, value_now):\n        if key in self.lastfelist:\n            self.lastfelist[key][fe]=value_now\n        else:\n            self.lastfelist[key]={fe: value_now}\n        \n        \n        \ntc = TimeCache()\n# last occur time\nlast_times = train.groupby(\"investment_id\")[\"time_id\"].apply(lambda x:x.iloc[-1]).rename(\"time_id\").reset_index()\n# add import fes lag\nn_features = 100\nfelist=[f'f_{i+100}' for i in range(n_features)]\nfor i, values in tqdm(last_times.iterrows()):\n    last_time = int(values[\"time_id\"])\n    key = int(values[\"investment_id\"])\n    tc.set_time_value(key, last_time)\nlast_df = train.groupby(\"investment_id\").apply(lambda x: x.tail(1)).reset_index(drop=True)\nfor i, values in tqdm(last_df.iterrows()):\n    key = int(values[\"investment_id\"])\n    for fe in felist:\n        last_value = values[fe]\n        tc.set_fe_value(key, fe, last_value)\n# train\ntrain[\"lagging_occur_time_diff\"] = train.groupby(\"investment_id\")[\"time_id\"].diff()-1 # 最低0天 上次出现time_id\nmax_fill = train[\"lagging_occur_time_diff\"].max()+1\ntrain[\"lagging_occur_time_diff\"] = train[\"lagging_occur_time_diff\"].fillna(max_fill)\nfor fe in tqdm(felist):\n    train[f\"lagging_{fe}_rate\"] = train.groupby(\"investment_id\")[fe].diff() # 上次出现value\n    train[f\"lagging_{fe}_rate\"] = train[f\"lagging_{fe}_rate\"].fillna(0).astype(np.float16)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:01:42.747349Z","iopub.execute_input":"2022-04-18T05:01:42.749477Z","iopub.status.idle":"2022-04-18T05:04:41.523451Z","shell.execute_reply.started":"2022-04-18T05:01:42.74943Z","shell.execute_reply":"2022-04-18T05:04:41.522668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del last_times, last_df\njoblib.dump(tc,\"tc.pkl\")\ndel tc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:04:41.524748Z","iopub.execute_input":"2022-04-18T05:04:41.525332Z","iopub.status.idle":"2022-04-18T05:04:51.865191Z","shell.execute_reply.started":"2022-04-18T05:04:41.525285Z","shell.execute_reply":"2022-04-18T05:04:51.864376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def modify_features(x, mode=\"train\", col_status={}):\n    # time\n    for col in [\"trade_date_year\",\"trade_date_month\",\"trade_date_weekday\",\"trade_date_day\",\"trade_date_year_rank\",\n               \"lagging_occur_time_diff\",\"week_of_month\"]+[f'lagging_f_{i+100}_rate' for i in range(n_features)]:\n        if mode==\"train\":\n            col_status[col]=(x[col].min(), x[col].max())\n        x[col] = (x[col]-col_status[col][0])/(col_status[col][1]-col_status[col][0])\n        x[col] = x[col].astype(np.float16)\n    return x, col_status\ntrain, col_status = modify_features(train, mode=\"train\")\nfeatures += [\"trade_date_year\",\"trade_date_month\",\"trade_date_weekday\",\"trade_date_day\",\"trade_date_year_rank\",\n            \"lagging_occur_time_diff\",\"week_of_month\"]\nfeatures += [f'lagging_f_{i+100}_rate' for i in range(n_features)]\ntrain = train.loc[~train[\"investment_id\"].isin([85, 905, 2558, 3662, 2800, 1415])].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:04:51.867208Z","iopub.execute_input":"2022-04-18T05:04:51.868852Z","iopub.status.idle":"2022-04-18T05:05:23.140523Z","shell.execute_reply.started":"2022-04-18T05:04:51.868806Z","shell.execute_reply":"2022-04-18T05:05:23.139621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:05:23.142005Z","iopub.execute_input":"2022-04-18T05:05:23.142262Z","iopub.status.idle":"2022-04-18T05:05:23.329975Z","shell.execute_reply.started":"2022-04-18T05:05:23.142222Z","shell.execute_reply":"2022-04-18T05:05:23.329181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment_id = train.pop(\"investment_id\")\ninvestment_id.head()\n_ = train.pop(\"time_id\")\ny = train.pop(\"target\")\ny.mean()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:05:23.331528Z","iopub.execute_input":"2022-04-18T05:05:23.332098Z","iopub.status.idle":"2022-04-18T05:05:23.399144Z","shell.execute_reply.started":"2022-04-18T05:05:23.332061Z","shell.execute_reply":"2022-04-18T05:05:23.3985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making Tesorflow dataset\ndef preprocess(X, y):\n    return X, y\n\ndef make_dataset(feature, y, batch_size=1024, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices(((feature), (y, y)))\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(4096)\n    ds = ds.batch(batch_size)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:10:05.156773Z","iopub.execute_input":"2022-04-18T05:10:05.157046Z","iopub.status.idle":"2022-04-18T05:10:05.162747Z","shell.execute_reply.started":"2022-04-18T05:10:05.157016Z","shell.execute_reply":"2022-04-18T05:10:05.162006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CosineLayer():\n \n    def __call__(self, x1, x2):\n \n        def _cosine(x):\n            dot1 = K.batch_dot(x[0], x[1], axes=1)\n            dot2 = K.batch_dot(x[0], x[0], axes=1)\n            dot3 = K.batch_dot(x[1], x[1], axes=1)\n            max_ = K.maximum(K.sqrt(dot2 * dot3), K.epsilon())\n            return dot1 / max_\n \n        output_shape = (1,)\n        value = Lambda(\n            _cosine,\n            output_shape=output_shape)([x1, x2])\n        return value\n\ndef my_cor(y_true, y_pred):\n    y_true = y_true - K.mean(y_true)\n    y_pred = y_pred - K.mean(y_pred)\n    t_m1 = K.sqrt(K.sum(y_pred ** 2))# + 0.00001\n    t_m2 = K.sqrt(K.sum(y_true ** 2))# + 0.00001\n\n    correlation = K.sum(y_pred*y_true) / (t_m1 * t_m2 + 0.00001)\n    return correlation\n\ndef spearmancorrelationloss(temp=1.0):\n    # 有用\n    def loss_fn(y_true, y_pred):\n        cosine = CosineLayer()\n        similarity = temp - cosine(tf.reshape(y_true,(1, -1)), tf.reshape(y_pred,(1, -1)))\n        return similarity\n    return loss_fn\n\ndef get_model():\n    features_inputs = tf.keras.Input((len(features), ), dtype=tf.float16)\n\n    \n    feature_x = tf.keras.layers.Dense(512, activation='swish')(features_inputs)\n    feature_x = tf.keras.layers.Dropout(0.5)(feature_x)\n    feature_x = tf.keras.layers.Dense(256, activation='swish')(feature_x)\n    feature_x = tf.keras.layers.Dense(256, activation='swish')(feature_x)\n    feature_x = tf.keras.layers.Dense(256, activation='swish')(feature_x)\n    \n    # x = tf.keras.layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = tf.keras.layers.Dense(256, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n    x = tf.keras.layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = tf.keras.layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n\n\n\n    output_0 = tf.keras.layers.Dense(1,name = 'regression')(x)\n    output_1 = tf.keras.layers.Dense(1,name = 'cor')(x)\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output_0, output_1])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss={'regression': tf.keras.losses.MeanSquaredError(),\n                                                             'cor':spearmancorrelationloss(temp=1.0)}, \n                  metrics = {'regression': tf.keras.metrics.MeanAbsoluteError(name = 'MAE'), \n                             'cor': my_cor\n                            }, )\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:05:23.407062Z","iopub.execute_input":"2022-04-18T05:05:23.407513Z","iopub.status.idle":"2022-04-18T05:05:23.425138Z","shell.execute_reply.started":"2022-04-18T05:05:23.407477Z","shell.execute_reply":"2022-04-18T05:05:23.424419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:05:23.426439Z","iopub.execute_input":"2022-04-18T05:05:23.426707Z","iopub.status.idle":"2022-04-18T05:05:23.612953Z","shell.execute_reply.started":"2022-04-18T05:05:23.426656Z","shell.execute_reply":"2022-04-18T05:05:23.612217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(5, shuffle=False)\nfor index, (train_indices, valid_indices) in enumerate(kfold.split(train, investment_id)):\n    print(f\"start save model data fold {index}\")\n    X_train= train[features].iloc[train_indices]\n    Cache.cache_data(X_train, f'X_train_{index}')\n    del X_train\n    gc.collect()\n    X_val = train[features].iloc[valid_indices]\n    Cache.cache_data(X_val, f'X_val_{index}')\n    del X_val\n    gc.collect()\n    investment_id_train = investment_id[train_indices]\n    Cache.cache_data(investment_id_train, f'investment_id_train_{index}')\n    del investment_id_train\n    gc.collect()\n    y_train = y.iloc[train_indices]\n    Cache.cache_data(y_train, f'y_train_{index}')\n    del y_train\n    gc.collect()\n    y_val = y.iloc[valid_indices]\n    Cache.cache_data(y_val, f'y_val_{index}')\n    del y_val\n    gc.collect()\n    investment_id_val = investment_id[valid_indices]\n    Cache.cache_data(investment_id_val, f'investment_id_val_{index}')\n    del investment_id_val\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:05:23.61717Z","iopub.execute_input":"2022-04-18T05:05:23.617358Z","iopub.status.idle":"2022-04-18T05:08:02.426893Z","shell.execute_reply.started":"2022-04-18T05:05:23.617335Z","shell.execute_reply":"2022-04-18T05:08:02.425267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:08:02.428054Z","iopub.execute_input":"2022-04-18T05:08:02.429875Z","iopub.status.idle":"2022-04-18T05:08:02.607602Z","shell.execute_reply.started":"2022-04-18T05:08:02.429829Z","shell.execute_reply":"2022-04-18T05:08:02.606661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train,y,investment_id,df_time\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:08:02.609958Z","iopub.execute_input":"2022-04-18T05:08:02.610288Z","iopub.status.idle":"2022-04-18T05:08:02.817465Z","shell.execute_reply.started":"2022-04-18T05:08:02.610246Z","shell.execute_reply":"2022-04-18T05:08:02.8166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_callbacks(index):\n    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_cor_my_cor', patience=4, min_delta = 1e-4, mode = 'max', \n                           baseline = None, restore_best_weights = True, verbose = 0)\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(f\"model_0159_{index}\", save_best_only=True)\n    callbacks = [early_stop_callback,  checkpoint]\n    return callbacks ","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:08:02.819097Z","iopub.execute_input":"2022-04-18T05:08:02.819502Z","iopub.status.idle":"2022-04-18T05:08:02.827098Z","shell.execute_reply.started":"2022-04-18T05:08:02.819465Z","shell.execute_reply":"2022-04-18T05:08:02.826316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:08:02.828174Z","iopub.execute_input":"2022-04-18T05:08:02.828363Z","iopub.status.idle":"2022-04-18T05:08:03.025519Z","shell.execute_reply.started":"2022-04-18T05:08:02.82834Z","shell.execute_reply":"2022-04-18T05:08:03.024549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodels = []\nscores = []\nfor index in range(5):\n    X_train = Cache.reload_cache(f\"CACHE_X_train_{index}.pkl\")\n    y_train = Cache.reload_cache(f\"CACHE_y_train_{index}.pkl\")\n    investment_id_train = Cache.reload_cache(f\"CACHE_investment_id_train_{index}.pkl\")\n    train_ds = make_dataset(X_train, y_train)\n    del X_train, y_train, investment_id_train\n    gc.collect()\n    X_val = Cache.reload_cache(f\"CACHE_X_val_{index}.pkl\")\n    y_val = Cache.reload_cache(f\"CACHE_y_val_{index}.pkl\")\n    investment_id_val = Cache.reload_cache(f\"CACHE_investment_id_val_{index}.pkl\")\n    valid_ds = make_dataset(X_val, y_val, mode=\"valid\")\n    del X_val, investment_id_val\n    gc.collect()\n    model = get_model()\n    early_stop = tf.keras.callbacks.EarlyStopping(monitor = 'val_cor_my_cor', patience=4,min_delta = 1e-4, mode = 'max', \n                           baseline = None, restore_best_weights = True, verbose = 0)\n    model.fit(train_ds, epochs=50, validation_data=valid_ds, callbacks=get_callbacks(index))\n    del train_ds\n    gc.collect()\n    models.append(model)\n    y_hats = model.predict(valid_ds)\n    y_hats = (y_hats[0].ravel() *0.8 +y_hats[1].ravel()*0.2)\n    pearson_score = stats.pearsonr(y_hats, y_val.values)[0]\n    print('Pearson:', pearson_score)\n    scores.append(pearson_score)\n    del y_hats, y_val\n    del valid_ds\n    K.clear_session()\n    del model\n    gc.collect()\nprint(\"Valid Scores:\", scores)\nprint(\"Valid Scores Mean:\", np.mean(scores))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:10:08.183767Z","iopub.execute_input":"2022-04-18T05:10:08.184716Z","iopub.status.idle":"2022-04-18T06:18:31.146216Z","shell.execute_reply.started":"2022-04-18T05:10:08.184641Z","shell.execute_reply":"2022-04-18T06:18:31.1444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_test(feature):\n    return (feature), 0\n\ndef make_test_dataset(feature, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\nweights = [0.2,0.2,0.2,0.2,0.2]\ndef inference(models, ds):\n    y_preds = []\n    for index, model in enumerate(models):\n        res = model.predict(ds)\n        y_pred = res[0]*0.8+res[1]*0.2\n        y_preds.append(y_pred * weights[index])\n    return np.sum(y_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:24:48.350232Z","iopub.execute_input":"2022-04-18T06:24:48.351277Z","iopub.status.idle":"2022-04-18T06:24:48.359945Z","shell.execute_reply.started":"2022-04-18T06:24:48.351229Z","shell.execute_reply":"2022-04-18T06:24:48.35897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \ntc = joblib.load(\"tc.pkl\")\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df[\"time_id\"]=test_df[\"row_id\"].map(lambda x:int(x.split(\"_\")[0]))# make time_id\n    df_time = test_df[[\"time_id\"]].drop_duplicates()\n    df_time[\"trade_date\"] = df_time[\"time_id\"].map(dict(zip(mapping_dict.values(),mapping_dict.keys())))\n    df_time[\"trade_date\"] = df_time[\"trade_date\"].map(lambda x:dt.datetime.strptime(str(x),\"%Y%m%d\"))\n    df_time = get_time_feature(df_time)\n    test_df[\"lagging_occur_time_diff\"]=test_df.apply(lambda x: tc.get_time_value(x[\"investment_id\"], x[\"time_id\"]),axis=1)\n    test_df[\"lagging_occur_time_diff\"]=test_df[\"time_id\"]-test_df[\"lagging_occur_time_diff\"]\n    test_df[\"lagging_occur_time_diff\"]=test_df[\"lagging_occur_time_diff\"].fillna(max_fill)\n    test_df = test_df.merge(df_time[[\"time_id\",\"trade_date_year\",\"trade_date_month\",\"trade_date_weekday\",\"trade_date_day\",\"trade_date_year_rank\",\"week_of_month\"]]\n                    ,on=\"time_id\",how=\"left\")\n    for fe in felist:\n        test_df[f\"lagging_{fe}_rate\"] = test_df.apply(lambda x: tc.get_fe_value(x[\"investment_id\"], fe, x[fe]),axis=1)\n        test_df[f\"lagging_{fe}_rate\"] = test_df[fe] - test_df[f\"lagging_{fe}_rate\"] \n    test_df, _ = modify_features(test_df, mode=\"test\", col_status=col_status)\n    ds = make_test_dataset(test_df[features])\n    sample_prediction_df['target'] = inference(models, ds)\n    env.predict(sample_prediction_df) ","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:24:49.827214Z","iopub.execute_input":"2022-04-18T06:24:49.827731Z","iopub.status.idle":"2022-04-18T06:24:56.182638Z","shell.execute_reply.started":"2022-04-18T06:24:49.827668Z","shell.execute_reply":"2022-04-18T06:24:56.181713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}