{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ranking problem\n\nAs LG mentioned in the [discussion](https://www.kaggle.com/competitions/ubiquant-market-prediction/discussion/314237), this task can be simplified as a ranking problem due to the following reasons.\n\n(1) The metric is the average of pearsonrs each time_id.\n\n(2) The distibution of targets each time_id can be fitted to a normal distribution.\n\nThen I tried a LightGBM Ranker to sort investment_ids by targets every time_ids. If you have an idea to improve scores by using a ranking predictor, please leave a comment. Thanks.\n\nI referred the [codes](https://github.com/masahiro-mochizuki/signate-fundamentals-challange-1st-place) of Mochizuki, which was 1st-place resolution to volatility prediction competition in Tokyo market.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#from sklearn.model_selection import GroupKFold\n#from sklearn import linear_model\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm\nfrom sklearn import linear_model\n\nfrom lightgbm import LGBMRanker\n#from lightgbm import LGBMRegressor\n#import sklearn\n#from sklearn.decomposition import PCA\n#import matplotlib.ticker as ticker\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-03-26T12:43:49.532123Z","iopub.execute_input":"2022-03-26T12:43:49.532461Z","iopub.status.idle":"2022-03-26T12:43:52.373142Z","shell.execute_reply.started":"2022-03-26T12:43:49.532424Z","shell.execute_reply":"2022-03-26T12:43:52.372091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"%%time\ntrain = (pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')\n         .sort_values(['time_id', 'investment_id'])\n         .drop(columns=['row_id'])\n         .query('time_id > 599')\n         .reset_index(drop=True));","metadata":{"execution":{"iopub.status.busy":"2022-03-26T12:43:54.441671Z","iopub.execute_input":"2022-03-26T12:43:54.449485Z","iopub.status.idle":"2022-03-26T12:44:38.836281Z","shell.execute_reply.started":"2022-03-26T12:43:54.448751Z","shell.execute_reply":"2022-03-26T12:44:38.835199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T04:47:07.084373Z","iopub.execute_input":"2022-03-26T04:47:07.085323Z","iopub.status.idle":"2022-03-26T04:47:07.102677Z","shell.execute_reply.started":"2022-03-26T04:47:07.085275Z","shell.execute_reply":"2022-03-26T04:47:07.101599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [f'f_{i}' for i in range(300)]\nfor col in features:\n    train[col] = train[col].astype(np.float16)\ntrain.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T12:44:42.770305Z","iopub.execute_input":"2022-03-26T12:44:42.771165Z","iopub.status.idle":"2022-03-26T12:46:41.513403Z","shell.execute_reply.started":"2022-03-26T12:44:42.771094Z","shell.execute_reply":"2022-03-26T12:46:41.512249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby('time_id').target.mean().plot()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T12:07:50.360294Z","iopub.execute_input":"2022-03-26T12:07:50.360857Z","iopub.status.idle":"2022-03-26T12:07:51.024636Z","shell.execute_reply.started":"2022-03-26T12:07:50.36078Z","shell.execute_reply":"2022-03-26T12:07:51.023704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby('time_id').target.std().plot()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T12:07:56.301743Z","iopub.execute_input":"2022-03-26T12:07:56.302082Z","iopub.status.idle":"2022-03-26T12:07:56.590732Z","shell.execute_reply.started":"2022-03-26T12:07:56.302042Z","shell.execute_reply":"2022-03-26T12:07:56.589781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby('time_id').investment_id.nunique().plot()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T02:06:59.765359Z","iopub.execute_input":"2022-03-26T02:06:59.766181Z","iopub.status.idle":"2022-03-26T02:07:00.540309Z","shell.execute_reply.started":"2022-03-26T02:06:59.766135Z","shell.execute_reply":"2022-03-26T02:07:00.539517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in features:\n    train[f'{col}_zscore'] = (train[col] - train.groupby('time_id')[col].transform(np.mean)) / train.groupby('time_id')[col].transform(np.std)\n    train[f'{col}_zscore'] = train[f'{col}_zscore'].astype(np.float16)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T12:47:37.088724Z","iopub.execute_input":"2022-03-26T12:47:37.089752Z","iopub.status.idle":"2022-03-26T12:48:20.686538Z","shell.execute_reply.started":"2022-03-26T12:47:37.089689Z","shell.execute_reply":"2022-03-26T12:48:20.685349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_zscore = [f'f_{i}_zscore' for i in range(300)]\n#train[features_zscore].head()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T12:48:24.511335Z","iopub.execute_input":"2022-03-26T12:48:24.511652Z","iopub.status.idle":"2022-03-26T12:48:35.062449Z","shell.execute_reply.started":"2022-03-26T12:48:24.51162Z","shell.execute_reply":"2022-03-26T12:48:35.061148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LightGBM Ranker","metadata":{}},{"cell_type":"code","source":"target = 'target'\ndef get_model_input(df):\n    query_list = df.loc[:,'time_id'].value_counts()\n    query_list = query_list.sort_index()\n    #df = df.set_index('time_id')\n    #df = df.sort_index(inplace = True)\n    df.loc[:,'target_rank'] = df.groupby('time_id')[target].rank(method='min', ascending = False).astype(np.int)\n    df = df.set_index(['time_id'])\n    return df, query_list","metadata":{"execution":{"iopub.status.busy":"2022-03-23T03:28:02.312825Z","iopub.execute_input":"2022-03-23T03:28:02.313573Z","iopub.status.idle":"2022-03-23T03:28:02.321048Z","shell.execute_reply.started":"2022-03-23T03:28:02.313526Z","shell.execute_reply":"2022-03-23T03:28:02.320075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Tuple\n\n\nclass GroupTimeSeriesSplit:\n    \"\"\"\n    From: https://www.kaggle.com/c/ubiquant-market-prediction/discussion/304036\n    Custom class to create a Group Time Series Split. We ensure\n    that the time id values that are in the testing data are not a part\n    of the training data & the splits are temporal\n    \"\"\"\n    def __init__(self, n_folds: int, holdout_size: int, groups: str) -> None:\n        self.n_folds = n_folds\n        self.holdout_size = holdout_size\n        self.groups = groups\n\n    def split(self, X) -> Tuple[np.array, np.array]:\n        # Take the group column and get the unique values\n        unique_time_ids = np.unique(self.groups.values)\n\n        # Split the time ids into the length of the holdout size\n        # and reverse so we work backwards in time. Also, makes\n        # it easier to get the correct time_id values per\n        # split\n        array_split_time_ids = np.array_split(\n            unique_time_ids, len(unique_time_ids) // self.holdout_size\n        )[::-1]\n\n        # Get the first n_folds values\n        array_split_time_ids = array_split_time_ids[:self.n_folds]\n\n        for time_ids in array_split_time_ids:\n            # Get test index - time id values that are in the time_ids\n            test_condition = X['time_id'].isin(time_ids)\n            test_index = X.loc[test_condition].index\n\n            # Get train index - The train index will be the time\n            # id values right up until the minimum value in the test\n            # data - we can also add a gap to this step by\n            # time id < (min - gap)\n            train_condition = X['time_id'] < (np.min(time_ids))\n            train_index = X.loc[train_condition].index\n\n            yield train_index, test_index","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_labels = train.investment_id.nunique()\nprint(max_labels)\n#del train","metadata":{"execution":{"iopub.status.busy":"2022-03-23T03:16:42.80192Z","iopub.execute_input":"2022-03-23T03:16:42.802309Z","iopub.status.idle":"2022-03-23T03:16:42.825083Z","shell.execute_reply.started":"2022-03-23T03:16:42.80226Z","shell.execute_reply":"2022-03-23T03:16:42.824109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_metric(y, t):\n    from scipy.stats import spearmanr\n    score = spearmanr(t, y, nan_policy=\"propagate\")[0]\n    return 'rho', score, True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n#FEATS = features + ['investment_id', 'time_id']\n\npearsonrs_folds = []\nrankmodels = []\navgmodels = []\nstdmodels = []\n#pearsonrs_bytimeid = {}\n\nFOLDS = 5\ngtss = GroupTimeSeriesSplit(n_folds=FOLDS, holdout_size=60, groups=train['time_id'])\nfor fold, (tr, val) in enumerate(gtss.split(train)):\n    print('FOLD:', fold)\n    \n    # use a fraction to training\n    tr_df = train.loc[tr]\n    tr_df, tr_query_list = get_model_input(tr_df)\n    \n    del tr\n    gc.collect()\n    \n    val_df = train.loc[val]\n    val_df, val_query_list = get_model_input(val_df)\n    del val\n    gc.collect()\n    \n    print('Train time_id range:', tr_df.index.min(), '->',tr_df.index.max())\n    print('Val time_id range:', val_df.index.min(), '->', val_df.index.max())\n    \n    # store time_id to calculate Pearson correlation\n    #time_ids_val = val_df.index.values\n\n    rankmodel = LGBMRanker(\n    #device=\"\",\n    boosting_type=\"gbdt\",\n    objective=\"lambdarank\",\n    metric=\"None\",\n    label_gain=np.arange(1, max_labels+1),\n    lambdarank_truncation_level=max_labels,\n    num_estimators=100,\n    early_stopping_round=10,\n    num_leaves=2**6-1,\n    learning_rate=0.1,\n    max_bin=128,\n    #max_drop=0,\n    #bagging_freq=1,\n    #bagging_fraction=0.8,\n    #feature_fraction=0.5,\n    #lambdarank_norm=False,\n    #seed=123,\n    #min_data_in_leaf=100,\n    #min_sum_hessian_in_leaf=1e-2,\n    n_jobs=-1,\n)\n    rankmodel.fit(\n    tr_df[features_zscore],\n    tr_df['target_rank'],\n    group=tr_query_list,\n    eval_set = [(val_df[features_zscore], val_df['target_rank'])],\n    eval_group=[list(val_query_list)],\n    eval_metric=custom_metric\n)\n    rankmodels.append(rankmodel)\n\n    tr_avg = tr_df.groupby('time_id')[features + [target]].mean()\n    tr_target_std = tr_df.groupby('time_id')[target].std()\n    #val_avg = val_df.groupby('time_id')[features + [target]].mean()\n    #val_target_std = val_df.groupby('time_id')[target].std()\n\n    del tr_df\n    gc.collect()\n    \n    avgmodel = linear_model.Ridge()\n    stdmodel = linear_model.Ridge()\n    \n    avgmodel.fit(tr_avg[features], tr_avg[target])#, eval_set = (val_avg[features], val_avg[target]), early_stopping_rounds = 10)\n    stdmodel.fit(tr_avg[features], tr_target_std)#, eval_set = (val_avg[features], val_target_std), early_stopping_rounds = 10)\n    \n    avgmodels.append(avgmodel)\n    stdmodels.append(stdmodel)\n    \n    del tr_avg, tr_target_std#, val_avg, val_target_std\n    \n    #avg_pred = tr_df.groupby('time_id')[target].mean().median()\n    #std_pred = tr_df.groupby('time_id')[target].std().median()\n    \n    time_ids=[]\n    pearsonrs=[]\n    for t_id in val_df.index.unique():\n    # pred = model.predict(df[df[\"era\"]== era][features])\n        df_tmp = val_df.loc[val_df.index == t_id]\n        df_tmp.loc[:,\"rank_pred\"] = rankmodel.predict(df_tmp[features_zscore])\n        #df_time_id.loc[:,\"rank_pred\"] = df_time_id.loc[:,\"rank_pred\"].rank()\n        #ranks.extend(list(pred))\n        \n        avg_pred = avgmodel.predict(df_tmp[features].mean().values.reshape(1,-1))\n        std_pred = stdmodel.predict(df_tmp[features].mean().values.reshape(1,-1))\n        std_pred = max([std_pred, 0])\n        \n        df_tmp = df_tmp.sort_values('rank_pred')\n        df_tmp.loc[:,'preds'] = np.sort(np.random.normal(avg_pred, std_pred, df_tmp.shape[0]))[::-1]\n        \n        metric = pearsonr(df_tmp['target'], df_tmp['preds'])[0]\n        time_ids.append(t_id)\n        pearsonrs.append(metric)\n    \n    res_df = pd.Series(pearsonrs, index = time_ids)\n    print(f'fold{fold}_Pearsonr:',np.mean(res_df))\n    \n    #pearsonrs_bytimeid[fold] = res_df\n    pearsonrs_folds.append(np.mean(res_df))\n    del res_df, val_df, rankmodel, avgmodel, stdmodel\n    gc.collect()\n    \nprint('-' * 30)\nprint('Mean:', np.mean(pearsonrs_folds))\nprint('Std:', np.std(pearsonrs_folds))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#avg_pred = train.groupby('time_id')[target].mean().median()\n#std_pred = train.groupby('time_id')[target].std().median()\n#print('avg:', avg_pred)\n#print('std:', std_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nimport ubiquant\nenv = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\n\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df['time_id'] = test_df['row_id'].apply(lambda x: int(x.split('_')[0]))\n    test_df['target'] = 0\n    for col in features:\n        test_df[f'{col}_zscore'] = (test_df[col] - test_df.groupby('time_id')[col].transform(np.mean)) / test_df.groupby('time_id')[col].transform(np.std)\n    \n    for rankmodel, avgmodel, stdmodel in zip(rankmodels, avgmodels, stdmodels):\n        test_df['rank_pred'] = rankmodel.predict(test_df[features_zscore])\n    \n        avg_pred = avgmodel.predict(test_df[features].mean().values.reshape(1,-1))\n        std_pred = stdmodel.predict(test_df[features].mean().values.reshape(1,-1))\n        std_pred = max([std_pred, 0])\n    \n        test_df = test_df.sort_values('rank_pred')\n        test_df['target'] += np.sort(np.random.normal(avg_pred, std_pred, test_df.shape[0]))[::-1]\n    # initialize columns\n    #test_df['target']  = 0\n    test_df['target'] /= len(rankmodels)\n    test_df = test_df.sort_values(['time_id', 'investment_id'])\n    \n    env.predict(test_df[['row_id','target']])","metadata":{"execution":{"iopub.status.busy":"2022-03-23T04:29:47.654024Z","iopub.execute_input":"2022-03-23T04:29:47.654426Z","iopub.status.idle":"2022-03-23T04:29:47.694281Z","shell.execute_reply.started":"2022-03-23T04:29:47.654386Z","shell.execute_reply":"2022-03-23T04:29:47.693333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Thanks!","metadata":{}}]}