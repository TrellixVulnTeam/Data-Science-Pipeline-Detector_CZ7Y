{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{"papermill":{"duration":0.016475,"end_time":"2022-01-25T15:39:01.467349","exception":false,"start_time":"2022-01-25T15:39:01.450874","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom scipy import stats\n\ndef add_stock_sum_feature(train):\n    train[\"trading_stock_sum\"] = train[\"time_id\"].map(train.groupby(['time_id'])['f_0'].count().to_dict())\n    return train.rename(columns={'trading_stock_sum': 'f_300'})\n\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\ntrain = add_stock_sum_feature(train)\nn_features = sum([1 for name in train.columns if 'f_' in name])\nfeatures = [f'f_{i}' for i in range(n_features)]\n\n\ninvestment_id = train.pop(\"investment_id\")\ntime_id = train.pop(\"time_id\")\ntrain_mean = train.astype('float64').mean().astype('float16')\ntrain_std = train.astype('float64').std().astype('float16')\n\n\nfrom keras import backend as K\n\nfrom tensorflow.keras.callbacks import Callback\nclass WarmupExponentialDecay(Callback):\n    def __init__(self,lr_base=1e-4,lr_min=0.0,decay=0,warmup_epochs=3):\n        self.num_passed_batchs = 0   #一个计数器\n        self.warmup_epochs=warmup_epochs\n        self.lr=lr_base #learning_rate_base\n        self.lr_min=lr_min #最小的起始学习率,此代码尚未实现\n        self.decay=decay  #指数衰减率\n        self.steps_per_epoch=0 #也是一个计数器\n    def on_batch_begin(self, batch, logs=None):\n        # params是模型自动传递给Callback的一些参数\n        if self.steps_per_epoch==0:\n            #防止跑验证集的时候呗更改了\n            if self.params['steps'] == None:\n                self.steps_per_epoch = np.ceil(1. * self.params['samples'] / self.params['batch_size'])\n            else:\n                self.steps_per_epoch = self.params['steps']\n        if self.num_passed_batchs < self.steps_per_epoch * self.warmup_epochs:\n            K.set_value(self.model.optimizer.lr,\n                        self.lr*(self.num_passed_batchs + 1) / self.steps_per_epoch / self.warmup_epochs)\n        else:\n            K.set_value(self.model.optimizer.lr,\n                        self.lr*((1-self.decay)**(self.num_passed_batchs-self.steps_per_epoch*self.warmup_epochs)))\n        self.num_passed_batchs += 1\n    def on_epoch_begin(self,epoch,logs=None):\n        print(\"learning_rate:\",K.get_value(self.model.optimizer.lr))\n\ndef pearson_r(y_true, y_pred):\n    x = y_true\n    y = y_pred\n    mx = K.mean(x, axis=0)\n    my = K.mean(y, axis=0)\n    xm, ym = x - mx, y - my\n    r_num = K.sum(xm * ym)\n    x_square_sum = K.sum(xm * xm)\n    y_square_sum = K.sum(ym * ym)\n    r_den = K.sqrt(x_square_sum * y_square_sum)\n    r = r_num / r_den\n    return K.mean(r)\n\ndef cccloss(t, o): #  t, 0\n    o_m = K.mean(o,axis=0)\n    o_var = K.std(o, axis=0)\n    t_m = K.mean(t, axis=0)\n    t_var = K.std(t, axis=0)\n    covariance = K.mean((o - o_m) * (t - t_m), axis=0)\n    ccc = 2 * covariance / (o_var + t_var + K.sqrt(o_m - t_m))\n    return 1 - ccc\n\ndef mix_loss(y_true, y_pred):\n    mse_loss = tf.keras.losses.MeanSquaredError()(y_true, y_pred)\n    corr_loss = 1 - pearson_r(y_true, y_pred)\n    ccc_loss = 1 - cccloss(y_true, y_pred)\n    return 2 * mse_loss + corr_loss + ccc_loss\n\ndef preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef inference(models, ds):\n    preds = []\n    for i in range(len(models)):\n        preds.append(models[i].predict(ds).ravel())\n    ans = np.mean(preds, axis=0)\n    ans = ans * train_std[-1] + train_mean[-1]\n    return ans\n\n\ndef load_models(model_path, epoch_version):\n    models = []\n    for i in range(5):\n        models.append(keras.models.load_model(f\"{model_path}/model_{i}/{epoch_version[i]}\", custom_objects={'pearson_r':pearson_r, 'mix_loss':mix_loss}))\n    return models\n\n\n\nmodel_path = \"../input/early-epoch/output/dnn-clip-5-feature-multi-test/100_1300__1\"\nepoch_version = [10 for epoch_version in range(5)]\nmodels = load_models(model_path, epoch_version)\n\nimport ubiquant\n\nenv = ubiquant.make_env()\niter_test = env.iter_test()\nfor (test_df, sample_prediction_df) in iter_test:\n    if 'time_id' not in test_df.columns:\n      test_df['time_id'] = test_df['row_id'].apply(lambda x: int(x.split('_')[0]))\n    \n    test_df = add_stock_sum_feature(test_df)\n    \n    input_feature = ((test_df[features] - train_mean[features]) / train_std[features]).clip(-5, 5)\n    ds = make_test_dataset(input_feature, test_df[\"investment_id\"])\n    sample_prediction_df['target'] = inference(models, ds)\n    env.predict(sample_prediction_df)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T01:49:50.59523Z","iopub.execute_input":"2022-04-10T01:49:50.595605Z","iopub.status.idle":"2022-04-10T01:50:36.644176Z","shell.execute_reply.started":"2022-04-10T01:49:50.595473Z","shell.execute_reply":"2022-04-10T01:50:36.643411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:48:52.917638Z","iopub.execute_input":"2022-03-31T12:48:52.917934Z","iopub.status.idle":"2022-03-31T12:49:00.303046Z","shell.execute_reply.started":"2022-03-31T12:48:52.917902Z","shell.execute_reply":"2022-03-31T12:49:00.302296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:45:30.178636Z","iopub.execute_input":"2022-03-31T12:45:30.179211Z","iopub.status.idle":"2022-03-31T12:45:30.18732Z","shell.execute_reply.started":"2022-03-31T12:45:30.179172Z","shell.execute_reply":"2022-03-31T12:45:30.186466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:49:13.172058Z","iopub.execute_input":"2022-03-31T12:49:13.172738Z","iopub.status.idle":"2022-03-31T12:49:14.60994Z","shell.execute_reply.started":"2022-03-31T12:49:13.172698Z","shell.execute_reply":"2022-03-31T12:49:14.608918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{},"execution_count":null,"outputs":[]}]}