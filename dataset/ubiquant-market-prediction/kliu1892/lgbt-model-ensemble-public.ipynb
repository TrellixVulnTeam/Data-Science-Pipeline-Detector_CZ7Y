{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nfrom matplotlib import pyplot as plt\nfrom tqdm.auto import tqdm\nfrom typing import List\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\nimport sys\n\nimport gc\nprint(sys.version)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ncnt = 0; show = 20\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if cnt >= show:\n            break\n        print(os.path.join(dirname, filename))\n        cnt += 1\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-17T00:27:27.848369Z","iopub.execute_input":"2022-04-17T00:27:27.848703Z","iopub.status.idle":"2022-04-17T00:27:31.381949Z","shell.execute_reply.started":"2022-04-17T00:27:27.848612Z","shell.execute_reply":"2022-04-17T00:27:31.381088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Process **\n1) **Load the entire training data** \n\n**Thank @Rob Mulla for the low memory parquet dataset**  https://www.kaggle.com/code/robikscube/fast-data-loading-and-low-mem-with-parquet-files","metadata":{}},{"cell_type":"code","source":"# train_df = pd.read_pickle(\"/kaggle/input/ubiquantpicklepython37float32/train3-7-float32-reduced.pkl\")\ntrain_df = pd.read_parquet(\"/kaggle/input/ubiquant-parquet/train_low_mem.parquet\")\nfeatures_300 = [f\"f_%d\" % i for i in range(300)]\nprint(train_df.shape)\nsize = sys.getsizeof(train_df)\nprint(f\"The train_df loaded consumes %.2f MB memory.\" % (size/(1024*1024)))\n\ncols = ['target']\nfor i in range(20):\n    cols.append(f\"f_%d\" % i)\nprint(train_df[cols].describe())\ntrain_df.reset_index(drop=True, inplace=True)\ntrain_df.drop(columns=['row_id'], inplace=True)\ndel cols, os, sys","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:27:31.384997Z","iopub.execute_input":"2022-04-17T00:27:31.385273Z","iopub.status.idle":"2022-04-17T00:28:15.102852Z","shell.execute_reply.started":"2022-04-17T00:27:31.385234Z","shell.execute_reply":"2022-04-17T00:28:15.102007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2) **Process the entire training data**\n\n**Thank yoshi_k for sharing some good plotting techques.** https://www.kaggle.com/code/yoshikuwano/eda-and-train-by-rapids-and-tabnet/notebook","metadata":{}},{"cell_type":"code","source":"# Import useful packages and set default plotting params\nfrom matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n                               AutoMinorLocator)\nimport random\n\nplt.style.use('ggplot')\nplt.rcParams.update({'font.size': 12, 'axes.grid': True, \n                    'grid.color': 'gray', 'grid.linestyle': '--'})","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:15.104191Z","iopub.execute_input":"2022-04-17T00:28:15.104618Z","iopub.status.idle":"2022-04-17T00:28:15.110707Z","shell.execute_reply.started":"2022-04-17T00:28:15.104567Z","shell.execute_reply":"2022-04-17T00:28:15.109924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Methods to generate plots\nclass EntireAnalysis:\n    \n    def __init__(self, inputDF: 'pandas.core.frame.DataFrame'):\n        \"\"\" inputDF contains all columns same as the original train.csv file \"\"\"\n        self.df = inputDF\n    \n    def histograms(self, usrColor: str='orange', Range: List[int]=None):\n        \"\"\" Plot target value histogram and time-step histogram of investment ids. \n        Range: [min_val, max_val] for the target bins \"\"\"\n        fig, ax = plt.subplots()\n        ax.hist(self.df['target'], bins=200, color=usrColor, range=Range)\n        ax.set_title(\"Target Histogram\")\n        ax.set_ylabel(\"Frequency\")\n        ax.xaxis.set_major_locator(MultipleLocator(1))\n        ax.xaxis.set_minor_locator(MultipleLocator(0.2))\n        fig.set_figwidth(15)\n        plt.show()\n        \n        timeStepCnt = self.df.groupby(['investment_id'])['investment_id'].count()\n        invIDs = timeStepCnt.index.tolist()\n        timeCnts = timeStepCnt.values.tolist()\n        fig, ax = plt.subplots()\n        ax.plot(invIDs[:500], timeCnts[:500], color='green')\n        # ax.hist(timeStepCnt, bins=200, color=usrColor)\n        ax.set_title(\"Time Steps per Investment Histogram\")\n        ax.set_xlabel(\"Investment ID\")\n        ax.xaxis.set_major_locator(MultipleLocator(50))\n        ax.xaxis.set_major_formatter(FormatStrFormatter('%d'))\n        ax.xaxis.set_minor_locator(MultipleLocator(10))\n        ax.set_xticklabels(labels=ax.get_xticks().astype(int), rotation=60)\n        fig.set_figwidth(15)\n        plt.show()\n        \n    def timeTrend(self, usrColor: str):\n        \"\"\" Display # of investments as a function of time_id, and target mean and std vs. time. \"\"\"\n        invCnts = self.df.groupby(['time_id'])['investment_id'].count()\n        target_mean = self.df.groupby(['time_id'])['target'].mean()\n        target_std = self.df.groupby(['time_id'])['target'].std()\n        time_ids = invCnts.index.tolist()              # 有用\n        invCnts = invCnts.tolist()\n        fig, axes = plt.subplots(2, 1, figsize=[15, 10])\n        axes[0].plot(time_ids, invCnts, color=usrColor)\n        axes[0].set_xlabel(\"Time_ids\")\n        axes[0].set_ylabel(\"Number of investments\")\n        \n        ax1 = axes[1]\n        ax1.plot(time_ids, target_mean, color='b', label='Mean')\n        ax1.set_xlabel(\"Time_ids\")\n        ax1.set_ylabel(\"Target Mean\", color='b')\n        \n        # 有用, Create a twin axis sharing the x axis\n        ax2 = ax1.twinx()\n        ax2.plot(time_ids, target_std, color='gold', label='Std Dev')\n        ax2.set_xlabel(\"Time_ids\")\n        ax2.set_ylabel(\"Target Standard Dev\", color='gold')\n        handle1, label1 = ax1.get_legend_handles_labels()\n        handle2, label2 = ax2.get_legend_handles_labels()\n        ax1.legend([handle2[0],handle1[0]], [label2[0],label1[0]], loc='best')\n        for i in range(2):\n            axes[i].xaxis.set_major_locator(MultipleLocator(50))      # 有用, 定义major tick间隔\n            axes[i].xaxis.set_minor_locator(MultipleLocator(10))      # 有用, 定义minor tick间隔\n            \n        plt.subplots_adjust(hspace=0.25)\n        plt.show()\n        \n    def investTimeTrend(self, usrColors: List[str]=['darkcyan', 'r', 'gold']):\n        \"\"\" Randomly pick 3 investment ids, make target vs. time plots. \"\"\"\n        invIDs = self.df['investment_id'].unique().tolist()\n        random.shuffle(invIDs)\n        invIDs = invIDs[:3]\n        fig, axes = plt.subplots(3, 1, figsize=[15, 12])\n        \n        for i in range(3):\n            idx = (self.df['investment_id'] == invIDs[i])\n            time_ids = self.df.loc[idx, 'time_id'].values.tolist()\n            targets = self.df.loc[idx, 'target'].values.tolist()\n            intvls = partitionTime(time_ids)\n            for start, end in intvls:\n                axes[i].plot(time_ids[start:end+1], targets[start:end+1], color=usrColors[i])\n            axes[i].set_xlabel(\"Time_ids\")\n            axes[i].set_ylabel(\"Target\")\n            axes[i].set_title(f\"Investment %d\" % invIDs[i])\n        \n        plt.subplots_adjust(hspace=0.5)\n        plt.show()\n    \n    def featureMeanStd(self, color1: str='b', color2: str='gold'):\n        \"\"\" Make combined plots for mean and standard deviations for all features \"\"\"\n        features = [f\"f_%d\" % j for j in range(300)]\n        means = self.df[features].mean(axis=0)          # pandas.core.series.Series\n        means = means.tolist()                          # Convert series to list\n        stds = self.df[features].std(axis=0)\n        stds = stds.tolist()\n        print(f\"There are %d means computed; there are %d stds computed.\" \n             % (len(means), len(stds)))\n        \n        fig, axes = plt.subplots(3, 1, figsize=(20, 15))\n        for i in range(3):\n            print(f\"Making plot %d...\" % (i+1))\n            start, end = i*100, (i+1)*100\n            ax1 = axes[i]\n            color1 = 'b'\n            ax1.plot(means[start:end], color=color1, label='Mean')\n            ax1.set_xticks(np.arange(100))\n            ax1.set_xticklabels(labels=features[start:end], rotation=90)\n            ax1.set_xlim(-1, 100)\n            ax1.set_ylim(-0.6, 0.6)\n            ax1.tick_params(axis='y', color=color1, labelcolor=color1)\n            ax1.set_ylabel('Mean', color=color1)\n            # 有用, Create a twin axis sharing the x axis\n            ax2 = ax1.twinx()           \n            color2 = 'orange'\n            ax2.plot(stds[start:end], color=color2, label='Standard Dev')\n            ax2.set_xticks(np.arange(100))\n            ax2.set_xticklabels(labels=features[start:end], rotation=90)\n            ax2.set_xlim(-1, 100)\n            ax2.set_ylim(0, 1.2)\n            ax2.tick_params(axis='y', color=color2, labelcolor=color2)\n            ax2.set_ylabel('Standard Deviation', color=color2)\n            \n            handle1, label1 = ax1.get_legend_handles_labels()\n            handle2, label2 = ax2.get_legend_handles_labels()\n            ax1.legend([handle2[0],handle1[0]], [label2[0],label1[0]], loc='best')\n        \n        plt.subplots_adjust(hspace=0.25)\n        plt.show()\n        \n    def featureDist(self, \n                    feaNames: List[str],       # the list of feature names\n                    usrColor: str,             # the color of the histogram\n                    th: float,                 # corr threshold to delete feature\n                    fea2Corr: dict=None):      # feature->targetCorrelation mapping.\n        \"\"\" Given a list of feature names, plot the logarithmic-scale histograms. \"\"\"\n        n = len(feaNames); i = 0\n        means = self.df[feaNames].mean(axis=0)\n        means = means.to_list()\n        stds = self.df[feaNames].std(axis=0)\n        stds = stds.to_list()\n        del_fs = []\n        \n        while i < n:\n            fig, axes = plt.subplots(1, 3, figsize=[18, 6])\n            for j in range(3):\n                if i + j == n:\n                    break\n                axes[j].hist(self.df[feaNames[i+j]], bins=100, \n                             range=[-10, 10], color=usrColor)\n                axes[j].set_title(feaNames[i+j] + \" Log Histogram\")\n                axes[j].set_ylim(1, 10 ** 6)\n                axes[j].set_yscale('log')\n                axes[j].xaxis.set_major_locator(MultipleLocator(5))\n                axes[j].xaxis.set_minor_locator(MultipleLocator(0.1))\n                axes[j].set_xlabel(f\"Mean = %.3f, Std_dev = %.3f.\" % (means[i+j], stds[i+j]))\n                if fea2Corr:\n                    corr = fea2Corr[feaNames[i+j]]\n                    axes[j].text(0.99, 0.99, f\"TargetCorr: %.3f\" % corr, \n                                 va='top', ha='right', transform=axes[j].transAxes)\n                    if abs(corr) < th:\n                        del_fs.append(feaNames[i+j])\n            \n            plt.subplots_adjust(hspace=0.5)\n            plt.show()\n            i += 3\n            \n        return del_fs\n    \ndef partitionTime(time_ids: List[int]) -> List[List[int]]:\n    \"\"\" Given a list of time_ids, partition into several continuous intervals [[start1, end1], [start2, end2], ...] 将time_id分成连续的区间。 \"\"\"\n    intervals = []\n    start = 0; end = start\n    prevTime = time_ids[0]\n    for idx, tid in enumerate(time_ids[1:], start=1):\n        if tid == prevTime + 1:\n            prevTime += 1\n            end = idx\n        else:\n            intervals.append([start, end])\n            start = idx; end = start\n            prevTime = tid\n    \n    intervals.append([start, end])\n    return intervals","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:15.115751Z","iopub.execute_input":"2022-04-17T00:28:15.116938Z","iopub.status.idle":"2022-04-17T00:28:15.277641Z","shell.execute_reply.started":"2022-04-17T00:28:15.116902Z","shell.execute_reply":"2022-04-17T00:28:15.276764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EA_engine = EntireAnalysis(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:15.281363Z","iopub.execute_input":"2022-04-17T00:28:15.281583Z","iopub.status.idle":"2022-04-17T00:28:15.289995Z","shell.execute_reply.started":"2022-04-17T00:28:15.281558Z","shell.execute_reply":"2022-04-17T00:28:15.289262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot target value histogram and time-step histogram of investment ids. \nEA_engine.histograms(Range=[-5, 5])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:15.291296Z","iopub.execute_input":"2022-04-17T00:28:15.291639Z","iopub.status.idle":"2022-04-17T00:28:16.578893Z","shell.execute_reply.started":"2022-04-17T00:28:15.291583Z","shell.execute_reply":"2022-04-17T00:28:16.57812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display # of investments as a function of time_id, and target mean and std vs. time. \nEA_engine.timeTrend('darkcyan')","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:16.579888Z","iopub.execute_input":"2022-04-17T00:28:16.580111Z","iopub.status.idle":"2022-04-17T00:28:17.959947Z","shell.execute_reply.started":"2022-04-17T00:28:16.580081Z","shell.execute_reply":"2022-04-17T00:28:17.959283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display mean and standard deviations for all 300 features\n# EA_engine.featureMeanStd()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:17.961176Z","iopub.execute_input":"2022-04-17T00:28:17.961522Z","iopub.status.idle":"2022-04-17T00:28:17.967294Z","shell.execute_reply.started":"2022-04-17T00:28:17.961491Z","shell.execute_reply":"2022-04-17T00:28:17.966567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Randomly pick 3 investment ids, make target vs. time plots.\nEA_engine.investTimeTrend()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:17.968805Z","iopub.execute_input":"2022-04-17T00:28:17.969392Z","iopub.status.idle":"2022-04-17T00:28:18.559102Z","shell.execute_reply.started":"2022-04-17T00:28:17.969354Z","shell.execute_reply":"2022-04-17T00:28:18.558433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import argparse\nargs = argparse.Namespace(\n    seed = 2021,\n    n_folds = 4, \n    W = 1,\n    n_threads = 2,\n    n_models = 8,\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:18.56208Z","iopub.execute_input":"2022-04-17T00:28:18.562513Z","iopub.status.idle":"2022-04-17T00:28:18.567155Z","shell.execute_reply.started":"2022-04-17T00:28:18.562479Z","shell.execute_reply":"2022-04-17T00:28:18.566279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del EA_engine, EntireAnalysis\n_ = gc.collect()\ntime.sleep(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:18.568557Z","iopub.execute_input":"2022-04-17T00:28:18.569069Z","iopub.status.idle":"2022-04-17T00:28:23.672873Z","shell.execute_reply.started":"2022-04-17T00:28:18.569032Z","shell.execute_reply":"2022-04-17T00:28:23.672109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3) **Remove some least important features (by training light gradient boosting regressors with all features)**","metadata":{}},{"cell_type":"code","source":"# Remove outliers\ndef getOutliers(input_df: 'pandas.core.frame.DataFrame', k: int):\n    \"\"\" Remove data with any feature that is outside of [mean-std*k, mean+std*k] \"\"\"\n    outliers = []\n    outFeatures = []\n    features = [f\"f_%d\" % i for i in range(300)]\n    means = input_df[features].mean(axis=0).tolist()\n    stds = input_df[features].std(axis=0).tolist()\n    \n    for i, fea in enumerate(features):\n        mu, sigma = means[i], stds[i]\n        currOut = input_df[(input_df[fea] > mu+sigma*k)|(input_df[fea] < mu-sigma*k)]\n        outliers.extend(currOut.index.tolist())\n        outFeatures.extend([fea for _ in range(len(currOut))])\n        \n    outlier_df = pd.DataFrame({'idx': outliers, 'feature': outFeatures})\n    outlier_df.drop_duplicates(subset='idx', inplace=True)\n    \n    return outlier_df\n\ndef prepare_features(input_df: 'pandas.core.frame.DataFrame',\n                     combineF: List[str], \n                     removeF: List[str]) -> List[str]:\n    \"\"\" Given a training dataframe, combine features based on combineF, remove features\n    in removeF. Then return a list of features for training. \"\"\"\n    # Create combined features\n    for combine in combineF:\n        first, second = combine.split('&')\n        input_df[combine] = input_df[first] + input_df[second]\n    # Drop specified features\n    input_df.drop(columns=removeF, inplace=True)\n    \n    use_features = ['investment_id', 'time_id']\n    # use_features = ['time_id']\n    columns = input_df.columns.tolist()\n    for col in columns:\n        if 'f_' in col:\n            use_features.append(col)\n    \n    return use_features","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:23.674018Z","iopub.execute_input":"2022-04-17T00:28:23.674211Z","iopub.status.idle":"2022-04-17T00:28:23.685554Z","shell.execute_reply.started":"2022-04-17T00:28:23.674187Z","shell.execute_reply":"2022-04-17T00:28:23.684923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# out_df = getOutliers(train_df, 30)\n# feature_outs = out_df.groupby(['feature'])['idx'].count()\n# print(feature_outs.sort_values(ascending=False)[:20])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:23.686572Z","iopub.execute_input":"2022-04-17T00:28:23.687208Z","iopub.status.idle":"2022-04-17T00:28:23.700818Z","shell.execute_reply.started":"2022-04-17T00:28:23.687173Z","shell.execute_reply":"2022-04-17T00:28:23.700121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df.shape)\n# Remove outliers\n# train_df.drop(out_df['idx'].tolist(), inplace=True)\n# train_df.reset_index(drop=True, inplace=True)\n\n# Create 8 combined features, accd to https://www.kaggle.com/code/yoshikuwano/eda-and-train-by-rapids-and-tabnet/notebook\n# combine_fs = ['f_231&f_250', 'f_118&f_280', 'f_155&f_297', 'f_25&f_237',\n#               'f_179&f_265', 'f_119&f_270', 'f_71&f_197', 'f_21&f_65']\ncombine_fs = []\n\n# Based on feature rank vs. target mean correlation (average on time_id), pick some least important ones\n# https://www.kaggle.com/code/marketneutral/stacking-feature-importance/notebook\nremove_fs = ['f_97','f_228','f_72','f_49','f_124','f_205','f_148','f_262','f_288','f_258',\n             'f_9','f_144','f_4','f_129','f_266','f_166','f_43','f_245','f_12','f_141',]\n# remove_fs = []\nprint(f\"There are %d features %r will be removed.\" % (len(remove_fs), remove_fs))\n\nfeatures = prepare_features(train_df, combine_fs, remove_fs)\nprint(train_df.shape, features)\n\n# del out_df\n_ = gc.collect()\ntime.sleep(6)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:23.702179Z","iopub.execute_input":"2022-04-17T00:28:23.702469Z","iopub.status.idle":"2022-04-17T00:28:31.807333Z","shell.execute_reply.started":"2022-04-17T00:28:23.702401Z","shell.execute_reply":"2022-04-17T00:28:31.806575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4) **Split training samples into folds**","metadata":{}},{"cell_type":"code","source":"def add_fold_by_timeID(df, continuous=False):\n    \"\"\" Split the dataframe into n_folds by time_id. If continuous is True, then time steps within every fold\n    are continuous. \"\"\"\n    # Initialize fold values to be -1\n    df['fold'] = -1\n    time_ids = df['time_id'].unique().tolist()\n    fold2cnt = dict()\n    for fold in range(args.n_folds):\n        fold2cnt[fold] = 0\n    \n    if continuous:\n        avg_n = df.shape[0] // args.n_folds\n        fold = 0; currCnt = 0\n        for time_id in tqdm(time_ids):\n            time_id_idx = (df['time_id'] == time_id)\n            cnt = sum(time_id_idx)\n            if currCnt + cnt <= avg_n:\n                currCnt += cnt\n            elif fold < args.n_folds-1:              # If not the last fold, increment fold by 1\n                fold += 1; currCnt = cnt\n            else:                                    # Assign all remaining samples to the last fold\n                currCnt += cnt\n            df.loc[time_id_idx, 'fold'] = fold\n            fold2cnt[fold] += cnt\n    else:\n        for time_id in tqdm(time_ids):\n            fold = random.randint(0, args.n_folds-1)\n            time_id_idx = (df['time_id'] == time_id)\n            df.loc[time_id_idx, 'fold'] = fold\n            fold2cnt[fold] += sum(time_id_idx)\n    \n    df['fold'] = df['fold'].astype('int32')\n    print(f\"Number of samples for each fold: %r.\" % fold2cnt)\n    print(f\"Dataframe has %d samples in total, %d have assigned fold.\" % (len(df), sum(fold2cnt.values())))\n    \n\ndef add_fold_by_investID(df):\n    # Initialize fold values to be -1\n    df['fold'] = -1\n    invIDs = df['investment_id'].unique().tolist()\n    fold2cnt = dict()\n    for fold in range(args.n_folds):\n        fold2cnt[fold] = 0\n    \n    for invID in tqdm(invIDs):\n        fold = random.randint(0, args.n_folds-1)\n        invID_idx = (df['investment_id'] == invID)\n        # idx = [i for i, val in enumerate(time_id_bool) if val]\n        df.loc[invID_idx, 'fold'] = fold\n        fold2cnt[fold] += sum(invID_idx)\n        \n    print(f\"Number of samples for each fold: %r.\" % fold2cnt)\n    print(f\"Dataframe has %d samples in total, %d have assigned fold.\" % (len(df), sum(fold2cnt.values())))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:31.808918Z","iopub.execute_input":"2022-04-17T00:28:31.809197Z","iopub.status.idle":"2022-04-17T00:28:31.82389Z","shell.execute_reply.started":"2022-04-17T00:28:31.809163Z","shell.execute_reply":"2022-04-17T00:28:31.823084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"front_bool = (train_df['time_id'] < 80)\nfront_idx = [idx for idx, val in enumerate(front_bool) if val]\ntrain_df.drop(front_idx, inplace=True)\n# add_fold_by_timeID(train_df, True)\ndel front_bool, front_idx\nprint(train_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:31.826269Z","iopub.execute_input":"2022-04-17T00:28:31.826459Z","iopub.status.idle":"2022-04-17T00:28:33.219748Z","shell.execute_reply.started":"2022-04-17T00:28:31.826437Z","shell.execute_reply":"2022-04-17T00:28:33.218993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def updateFeatureImp(model, features, feature2Imp) -> None:\n    \"\"\" Helper function to aggregate feature importance from a model. \"\"\"\n    featureImps = model.feature_importances_\n    for idx, feature in enumerate(features):\n        imp = featureImps[idx]\n        if feature not in feature2Imp:\n            feature2Imp[feature] = [imp]\n        else:\n            feature2Imp[feature].append(imp)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:33.221181Z","iopub.execute_input":"2022-04-17T00:28:33.221433Z","iopub.status.idle":"2022-04-17T00:28:33.227403Z","shell.execute_reply.started":"2022-04-17T00:28:33.221398Z","shell.execute_reply":"2022-04-17T00:28:33.226632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ** Model Training **\n# ** LightGBM Regressior. Original **","metadata":{}},{"cell_type":"code","source":"del AutoMinorLocator, FormatStrFormatter, MultipleLocator, getOutliers\ndel add_fold_by_timeID, add_fold_by_investID, plt, random\nimport lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:33.228295Z","iopub.execute_input":"2022-04-17T00:28:33.228989Z","iopub.status.idle":"2022-04-17T00:28:36.327059Z","shell.execute_reply.started":"2022-04-17T00:28:33.228951Z","shell.execute_reply":"2022-04-17T00:28:36.326293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.remove('investment_id')\nprint(features[:5])\nprint(features[-10:], len(features))\n\n_ = gc.collect()\ntime.sleep(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:36.328494Z","iopub.execute_input":"2022-04-17T00:28:36.328756Z","iopub.status.idle":"2022-04-17T00:28:46.468866Z","shell.execute_reply.started":"2022-04-17T00:28:36.328723Z","shell.execute_reply":"2022-04-17T00:28:46.468112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train method for LGBMRegressor\n# def LGBMR_train(df_train, features, train_params, args, useWts=False):\n#     \"\"\" Perform cross validation on the light GBMRegressor, defined by train_params, \n#         using df_train and given features. \"\"\"\n#     train_corrs = []; val_corrs = []\n#     feature2Imp = dict()\n#     for i in tqdm(range(args.n_folds)):\n#         start = time.time()\n#         train_idx = df_train['fold'] != i\n#         timeIDs = df_train.loc[train_idx, 'time_id'].values.tolist()\n#         train_X = df_train.loc[train_idx, features].values\n#         train_y = df_train.loc[train_idx, 'target'].values\n#         _ = gc.collect()\n#         print(f\"Fold %d, %d training samples, %d validation samples.\" % (i, len(train_y), len(df_train)-len(train_y)))\n#         # Train the model\n#         my_LGBMR = lgb.LGBMRegressor(**train_params)\n#         if useWts:\n#             weights = df_train.loc[train_idx, 'weight'].values.tolist()\n#             print(f\"%d Training weights used.\" % len(weights))\n#         else:   \n#             weights = None\n#         my_LGBMR.fit(train_X, train_y, sample_weight=weights)\n#         end = time.time()\n#         print(f\"It took %.2f minutes to train this light GBMRegressor.\" % ((end-start)/60) )\n#         updateFeatureImp(my_LGBMR, features, feature2Imp)\n#         pred_tr_y = my_LGBMR.predict(train_X)\n#         train_corr = meanCorr(timeIDs, train_y, pred_tr_y)\n#         del train_X, train_y, train_idx\n#         _ = gc.collect()\n#         time.sleep(10)\n#         \n#         # Test the model on validation set\n#         valid_idx = df_train['fold'] == i\n#         timeIDs = df_train.loc[valid_idx, 'time_id'].values.tolist()\n#         valid_X = df_train.loc[valid_idx, features].values\n#         valid_y = df_train.loc[valid_idx, 'target'].values\n#         del valid_idx\n#         _ = gc.collect()\n#         pred_val_y = my_LGBMR.predict(valid_X)\n#         valid_corr = meanCorr(timeIDs, valid_y, pred_val_y)\n#         print(f\"Training correlation %.4f, validation correlation %.4f.\" % (train_corr, valid_corr))\n#         train_corrs.append(train_corr); val_corrs.append(valid_corr)\n#         \n#         del my_LGBMR, valid_X, valid_y\n#         _ = gc.collect()\n#         time.sleep(8)\n#         \n#     Tr_corr = sum(train_corrs) / args.n_folds\n#     Val_corr = sum(val_corrs) / args.n_folds\n#     print(f\"Model hyperparams: %r, average train corr %.4f, average valid corr %.4f.\"\n#          % (train_params, Tr_corr, Val_corr))\n#     feature2Imp = {feature: sum(imp)/len(imp) for feature, imp in sorted(feature2Imp.items(),\n#                                                                   key=lambda x: sum(x[1])/len(x[1]),\n#                                                                   reverse=True)}\n#     print(f\"Most important 30 features: %r.\" % (list(feature2Imp.items())[:30]) )\n#     print(f\"Least important 30 features: %r.\" % (list(feature2Imp.items())[-30:]) )\n    \ndef meanCorr(time_ids: List[int], true_ys: List[float], pred_ys: List[float]) -> float:\n    \"\"\" Compute the mean correlation based on time_id (all indices for one time_id need to be continuous) \"\"\"\n    timeID2Idx = dict()            # Stores time_id -> [start_idx, end_idx] mapping\n    for idx, time_id in enumerate(time_ids):\n        if time_id not in timeID2Idx:\n            timeID2Idx[time_id] = [idx, idx]\n        else:\n            timeID2Idx[time_id][1] = idx\n            \n    corrs = []\n    for start, end in timeID2Idx.values():\n        curr_corr = np.corrcoef(true_ys[start:end+1], pred_ys[start:end+1])[0, 1]\n        if np.isnan(curr_corr):\n            continue\n        corrs.append(curr_corr)\n    \n    return np.mean(corrs)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:46.47006Z","iopub.execute_input":"2022-04-17T00:28:46.470306Z","iopub.status.idle":"2022-04-17T00:28:46.479947Z","shell.execute_reply.started":"2022-04-17T00:28:46.470274Z","shell.execute_reply":"2022-04-17T00:28:46.479223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LGBMR_params = dict(objective='regression',\n#                         n_estimators=250, \n#                         learning_rate=0.06,\n#                         max_depth=8,\n#                         num_leaves=200,\n#                         max_bin=127,\n#                         min_child_samples=500,\n#                         device_type='gpu',\n#                         reg_lambda=80,\n#                         verbosity=-1,\n#                         n_jobs=args.n_threads,\n#                         # feature_fraction=0.7,\n#                         # bagging_fraction=0.9,\n#                         # random_state=seed,\n#                         )\n# LGBMR_train(train_df, features, LGBMR_params, args)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:46.48105Z","iopub.execute_input":"2022-04-17T00:28:46.48182Z","iopub.status.idle":"2022-04-17T00:28:46.491347Z","shell.execute_reply.started":"2022-04-17T00:28:46.48176Z","shell.execute_reply":"2022-04-17T00:28:46.490447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_ids = train_df['time_id'].values\ninvest_ids = train_df['investment_id'].unique().tolist()\ntrain_y = train_df['target'].values\ntrain_df.drop(columns=['investment_id', 'target'], inplace=True)\ngc.collect()\ntime.sleep(8)\n\ntrain_X = train_df[features].values\ndel train_df\nprint(train_X.shape)\ngc.collect()\ntime.sleep(8)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:28:46.493078Z","iopub.execute_input":"2022-04-17T00:28:46.49329Z","iopub.status.idle":"2022-04-17T00:29:03.01955Z","shell.execute_reply.started":"2022-04-17T00:28:46.493259Z","shell.execute_reply":"2022-04-17T00:29:03.018799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use all the samples to train light GBMR, model ensemble\nfeature2Imp = dict() \npred_ys = None\nfor j in range(args.n_models):\n    seed = 2015 + j\n    LGBMR_params = dict(objective='regression',\n                        n_estimators=300, \n                        learning_rate=0.06,\n                        max_depth=8,\n                        num_leaves=200,\n                        max_bin=127,\n                        min_child_samples=500,\n                        device_type='gpu',\n                        reg_lambda=80,\n                        verbosity=-1,\n                        n_jobs=args.n_threads,\n                        feature_fraction=0.8,\n                        # bagging_fraction=0.9,\n                        random_state=seed,\n                        )\n    print(f\"Model %d, %d training samples\" % (j, len(train_y)) )\n    LGBMR = lgb.LGBMRegressor(**LGBMR_params)\n    start = time.time()\n    LGBMR.fit(train_X, train_y, sample_weight=None)\n    print(f\"It took %.2f minutes to train this light GBMRegressor.\" % ((time.time()-start)/60) )\n    updateFeatureImp(LGBMR, features, feature2Imp)\n    \n    start = time.time()\n    pred_y = LGBMR.predict(train_X)\n    LGBMR.booster_.save_model(f\"/kaggle/working/model%d.txt\" % j)\n    if j == 0:\n        pred_ys = np.array([pred_y])\n    else:\n        pred_ys = np.append(pred_ys, [pred_y], axis=0)\n    print(f\"It took %.2f minutes to predict and save the model.\" % ((time.time()-start)/60) )\n    del LGBMR, pred_y\n    gc.collect()\n    time.sleep(10)\n    \nmean_y = np.mean(pred_ys, axis=0)\ntr_corr = meanCorr(time_ids, train_y, mean_y)\nprint(f\"Training correlation %.4f.\" % tr_corr)\ndel pred_ys, mean_y, tr_corr\n_ = gc.collect()\ntime.sleep(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:29:03.020788Z","iopub.execute_input":"2022-04-17T00:29:03.021435Z","iopub.status.idle":"2022-04-17T00:30:29.056724Z","shell.execute_reply.started":"2022-04-17T00:29:03.021397Z","shell.execute_reply":"2022-04-17T00:30:29.055931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature2Imp = {f: sum(imp)/len(imp) for f, imp in sorted(feature2Imp.items(),\n                                                         key=lambda x: sum(x[1])/len(x[1]),\n                                                         reverse=True)}\nprint(list(feature2Imp.items())[:30])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:30:29.058012Z","iopub.execute_input":"2022-04-17T00:30:29.058281Z","iopub.status.idle":"2022-04-17T00:30:29.067056Z","shell.execute_reply.started":"2022-04-17T00:30:29.058245Z","shell.execute_reply":"2022-04-17T00:30:29.066368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(list(feature2Imp.items())[-30:])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:30:29.068414Z","iopub.execute_input":"2022-04-17T00:30:29.068892Z","iopub.status.idle":"2022-04-17T00:30:29.076976Z","shell.execute_reply.started":"2022-04-17T00:30:29.068858Z","shell.execute_reply":"2022-04-17T00:30:29.076118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train_X, train_y, time_ids\n# del train_df\ngc.collect()\ntime.sleep(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:30:29.078255Z","iopub.execute_input":"2022-04-17T00:30:29.078702Z","iopub.status.idle":"2022-04-17T00:30:39.219301Z","shell.execute_reply.started":"2022-04-17T00:30:29.078665Z","shell.execute_reply":"2022-04-17T00:30:39.218562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ** Evaluation **","metadata":{}},{"cell_type":"code","source":"## Make submission (GradientBoostingRegressor)\nimport ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test()\nnewCnt = 0\nmodels = []\nfor j in range(args.n_models):\n    LGBMR = lgb.Booster(model_file=(f'/kaggle/working/model%d.txt' % j))\n    models.append(LGBMR)\n    del LGBMR\n    _ = gc.collect()\n    time.sleep(6)\n\nfor (df_test, df_submission) in iter_test:\n    # Extract 'time_id' from 'row_id'\n    df_test['time_id'] = df_test.row_id.str.extract(r'(\\d+)_.*').astype(np.uint16)\n    # Create features same as df_train\n    cnt = 0\n    test_investIDs = df_test['investment_id'].values.tolist()\n    for test_investID in test_investIDs:\n        if test_investID not in invest_ids:\n            cnt += 1\n    _ = prepare_features(df_test, combine_fs, remove_fs)\n    test_X = df_test[features].values\n    ys = None\n    for j in range(args.n_models):\n        y = models[j].predict(test_X)\n        if j == 0:\n            ys = np.array([y])\n        else:\n            ys = np.append(ys, [y], axis=0)\n    df_submission['target'] = np.mean(ys, axis=0)\n    newCnt += cnt\n    env.predict(df_submission) \n\nprint(f\"There are %d new investments.\" % newCnt)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:30:39.220451Z","iopub.execute_input":"2022-04-17T00:30:39.220874Z","iopub.status.idle":"2022-04-17T00:30:51.609552Z","shell.execute_reply.started":"2022-04-17T00:30:39.220836Z","shell.execute_reply":"2022-04-17T00:30:51.609001Z"},"trusted":true},"execution_count":null,"outputs":[]}]}