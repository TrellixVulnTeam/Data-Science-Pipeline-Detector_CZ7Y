{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Model Ensemble DNN + TabNet\n- TabNet: https://www.kaggle.com/wangqihanginthesky/baseline-tabnet/notebook\n- DNN: https://www.kaggle.com/andrej0marinchenko/ubiquant-market-prediction-dnn","metadata":{}},{"cell_type":"code","source":"!pip -q install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\n!pip -q install ../input/talib-binary/talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:11:07.018823Z","iopub.execute_input":"2022-02-16T01:11:07.01915Z","iopub.status.idle":"2022-02-16T01:12:05.927615Z","shell.execute_reply.started":"2022-02-16T01:11:07.019116Z","shell.execute_reply":"2022-02-16T01:12:05.926539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport joblib\nimport random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom argparse import Namespace\nfrom collections import defaultdict\n\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, train_test_split\n\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 64)\n\ndef seed_everything(seed: int = 42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-16T01:12:05.931735Z","iopub.execute_input":"2022-02-16T01:12:05.932509Z","iopub.status.idle":"2022-02-16T01:12:05.942552Z","shell.execute_reply.started":"2022-02-16T01:12:05.93247Z","shell.execute_reply":"2022-02-16T01:12:05.941735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = Namespace(\n    INFER=True,\n    debug=False,\n    seed=21,\n    folds=5,\n    workers=4,\n    min_time_id=None, \n    holdout=True,\n    num_bins=16,\n    data_path=Path(\"../input/ubiquant-market-prediction-half-precision-pickle\"),\n    dnn_path = '../input/dnnmodel',\n    tabnet_path = '../input/ubiquanttabnetbaseline'\n)\nseed_everything(args.seed)\n\nif args.debug:\n    setattr(args, 'min_time_id', 1100)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:12:05.944148Z","iopub.execute_input":"2022-02-16T01:12:05.944695Z","iopub.status.idle":"2022-02-16T01:12:05.956468Z","shell.execute_reply.started":"2022-02-16T01:12:05.944656Z","shell.execute_reply":"2022-02-16T01:12:05.95558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Dataset","metadata":{}},{"cell_type":"code","source":"%%time\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:12:05.957629Z","iopub.execute_input":"2022-02-16T01:12:05.960464Z","iopub.status.idle":"2022-02-16T01:12:23.6228Z","shell.execute_reply.started":"2022-02-16T01:12:05.960433Z","shell.execute_reply":"2022-02-16T01:12:23.622019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment_id = train.pop(\"investment_id\")\ninvestment_id.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:12:23.625397Z","iopub.execute_input":"2022-02-16T01:12:23.626206Z","iopub.status.idle":"2022-02-16T01:12:23.641519Z","shell.execute_reply.started":"2022-02-16T01:12:23.626163Z","shell.execute_reply":"2022-02-16T01:12:23.640712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = train.pop(\"time_id\")","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:12:23.642658Z","iopub.execute_input":"2022-02-16T01:12:23.642914Z","iopub.status.idle":"2022-02-16T01:12:23.654502Z","shell.execute_reply.started":"2022-02-16T01:12:23.64285Z","shell.execute_reply":"2022-02-16T01:12:23.653563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train.pop(\"target\")\ny.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:12:23.655895Z","iopub.execute_input":"2022-02-16T01:12:23.656191Z","iopub.status.idle":"2022-02-16T01:12:23.672128Z","shell.execute_reply.started":"2022-02-16T01:12:23.656124Z","shell.execute_reply":"2022-02-16T01:12:23.671102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create IntegerLookup Later for investment_id input","metadata":{}},{"cell_type":"code","source":"%%time\ninvestment_ids = list(investment_id.unique())\ninvestment_id_size = len(investment_ids) + 1\ninvestment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\ninvestment_id_lookup_layer.adapt(pd.DataFrame({\"investment_ids\":investment_ids}))","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:12:23.673756Z","iopub.execute_input":"2022-02-16T01:12:23.674336Z","iopub.status.idle":"2022-02-16T01:12:23.902383Z","shell.execute_reply.started":"2022-02-16T01:12:23.674294Z","shell.execute_reply":"2022-02-16T01:12:23.900834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make Tensorflow Dataset","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\ndef preprocess(X, y):\n    return X, y\ndef make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(4096)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:12:23.904831Z","iopub.execute_input":"2022-02-16T01:12:23.905415Z","iopub.status.idle":"2022-02-16T01:12:23.911665Z","shell.execute_reply.started":"2022-02-16T01:12:23.90537Z","shell.execute_reply":"2022-02-16T01:12:23.910833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DNN Model","metadata":{}},{"cell_type":"code","source":"def get_model():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)    \n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n   # investment_id_x = layers.Dropout(0.65)(investment_id_x)\n   \n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.65)(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n   # x = layers.Dropout(0.2)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n  #  x = layers.Dropout(0.4)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.75)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:12:23.913154Z","iopub.execute_input":"2022-02-16T01:12:23.913596Z","iopub.status.idle":"2022-02-16T01:12:23.927728Z","shell.execute_reply.started":"2022-02-16T01:12:23.913555Z","shell.execute_reply":"2022-02-16T01:12:23.926843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model()\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:12:23.929113Z","iopub.execute_input":"2022-02-16T01:12:23.929393Z","iopub.status.idle":"2022-02-16T01:12:24.598686Z","shell.execute_reply.started":"2022-02-16T01:12:23.929357Z","shell.execute_reply":"2022-02-16T01:12:24.597872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(2, shuffle=True, random_state=42)\nmodels = []\nfor index, (train_indices, valid_indices) in enumerate(kfold.split(train, investment_id)):\n    X_train, X_val = train.iloc[train_indices], train.iloc[valid_indices]\n    investment_id_train = investment_id[train_indices]\n    y_train, y_val = y.iloc[train_indices], y.iloc[valid_indices]\n    investment_id_val = investment_id[valid_indices]\n    train_ds = make_dataset(X_train, investment_id_train, y_train)\n    valid_ds = make_dataset(X_val, investment_id_val, y_val, mode=\"valid\")\n    model = get_model()\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{index}\", save_best_only=True)\n    early_stop = keras.callbacks.EarlyStopping(patience=10)\n    history = model.fit(train_ds, epochs=30, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n    models.append(keras.models.load_model(f\"model_{index}\"))\n    \n    pearson_score = stats.pearsonr(model.predict(valid_ds).ravel(), y_val.values)[0]\n    print('Pearson:', pearson_score)\n    pd.DataFrame(history.history, columns=[\"mse\", \"val_mse\"]).plot()\n    plt.title(\"MSE\")\n    plt.show()\n    pd.DataFrame(history.history, columns=[\"mae\", \"val_mae\"]).plot()\n    plt.title(\"MAE\")\n    plt.show()\n    pd.DataFrame(history.history, columns=[\"rmse\", \"val_rmse\"]).plot()\n    plt.title(\"RMSE\")\n    plt.show()\n    del investment_id_train\n    del investment_id_val\n    del X_train\n    del X_val\n    del y_train\n    del y_val\n    del train_ds\n    del valid_ds\n    gc.collect()\n    break","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:12:24.600594Z","iopub.execute_input":"2022-02-16T01:12:24.601199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport joblib\nimport random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom argparse import Namespace\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, train_test_split\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 64)\n\ndef seed_everything(seed: int = 42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom pytorch_tabnet.pretraining import TabNetPretrainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = Namespace(\n    INFER=True,\n    debug=False,\n    seed=21,\n    folds=5,\n    workers=4,\n    min_time_id=None, \n    holdout=True,\n    num_bins=16,\n    data_path=Path(\"../input/ubiquant-parquet/\"),\n)\nseed_everything(args.seed)\n\nif args.debug:\n    setattr(args, 'min_time_id', 1100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain = pd.read_parquet('../input/ubiquanttabnetbaseline/valid.parquet')\nassert train.isnull().any().sum() == 0, \"null exists.\"\nassert train.row_id.str.extract(r\"(?P<time_id>\\d+)_(?P<investment_id>\\d+)\").astype(train.time_id.dtype).equals(train[[\"time_id\", \"investment_id\"]]), \"row_id!=time_id_investment_id\"\n\nif args.min_time_id is not None:\n    train = train.query(\"time_id>=@args.min_time_id\").reset_index(drop=True)\n    gc.collect()\ntrain.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_id_df = (\n    train.filter(regex=r\"^(?!f_).*\")\n    .groupby(\"investment_id\")\n    .agg({\"time_id\": [\"min\", \"max\"]})\n    .reset_index()\n)\ntime_id_df[\"time_span\"] = time_id_df[\"time_id\"].diff(axis=1)[\"max\"]\ntime_id_df.head(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.merge(time_id_df.drop(columns=\"time_id\").droplevel(level=1, axis=1), on=\"investment_id\")\ntrain.time_span.hist(bins=args.num_bins, figsize=(16,8))\ndel time_id_df\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if args.holdout:\n    _target = pd.cut(train.time_span, args.num_bins, labels=False)\n    _train, _valid = train_test_split(_target, stratify=_target)\n    print(f\"train length: {len(_train)}\", f\"holdout length: {len(_valid)}\")\n    valid = train.iloc[_valid.index].sort_values(by=[\"investment_id\", \"time_id\"]).reset_index(drop=True)\n    train = train.iloc[_train.index].sort_values(by=[\"investment_id\", \"time_id\"]).reset_index(drop=True)\n    train.time_span.hist(bins=args.num_bins, figsize=(16,8), alpha=0.8)\n    valid.time_span.hist(bins=args.num_bins, figsize=(16,8), alpha=0.8)\n    valid.drop(columns=\"time_span\").to_parquet(\"valid.parquet\")\n    del valid, _train, _valid, _target\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"fold\"] = -1\n_target = pd.cut(train.time_span, args.num_bins, labels=False)\nskf = StratifiedKFold(n_splits=args.folds)\nfor fold, (train_index, valid_index) in enumerate(skf.split(_target, _target)):\n    train.loc[valid_index, 'fold'] = fold\n    \nfig, axs = plt.subplots(nrows=args.folds, ncols=1, sharex=True, figsize=(16,8), tight_layout=True)\nfor ax, (fold, df) in zip(axs, train[[\"fold\", \"time_span\"]].groupby(\"fold\")):\n    ax.hist(df.time_span, bins=args.num_bins)\n    ax.text(0, 40000, f\"fold: {fold}, count: {len(df)}\", fontsize=16)\nplt.show()\ndel _target, train_index, valid_index\n_=gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_features = [\"investment_id\"]\nnum_features = list(train.filter(like=\"f_\").columns)\nfeatures = num_features + cat_features\n\ntrain = reduce_mem_usage(train.drop(columns=\"time_span\"))\ntrain[[\"investment_id\", \"time_id\"]] = train[[\"investment_id\", \"time_id\"]].astype(np.uint16)\ntrain[\"fold\"] = train[\"fold\"].astype(np.uint8)\ngc.collect()\nfeatures += [\"time_id\"] # https://www.kaggle.com/c/ubiquant-market-prediction/discussion/302429\nlen(features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\nimport torch\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\nfrom sklearn.metrics import mean_squared_error\n\ndef rmse(y_true, y_pred):\n    return mean_squared_error(y_true,y_pred, squared=False)\ndef rmspe(y_true, y_pred):\n    # Function to calculate the root mean squared percentage error\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\nclass RMSPE(Metric):\n    def __init__(self):\n        self._name = \"rmspe\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_score):\n        \n        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n    \n\ndef RMSPELoss(y_pred, y_true):\n    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n\n\n\ncat_idxs = [ i for i, f in enumerate(X.columns.tolist()) if f in cat_features]\n\n\ndef run():    \n    tabnet_params = dict(\n        cat_idxs=cat_idxs,\n        cat_emb_dim=1,\n        n_d = 16,\n        n_a = 16,\n        n_steps = 2,\n        gamma =1.4690246460970766,\n        n_independent = 9,\n        n_shared = 4,\n        lambda_sparse = 0,\n        optimizer_fn = Adam,\n        optimizer_params = dict(lr = (0.0024907164557092944)),\n        mask_type = \"entmax\",\n        scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n        scheduler_fn = CosineAnnealingWarmRestarts,\n        seed = 42,\n        verbose = 10, \n    )    \n    y = train['target']\n    train['preds'] = -1000\n    scores = defaultdict(list)\n    features_importance= pd.DataFrame()\n    \n    for fold in range(args.folds):\n        print(f\"=====================fold: {fold}=====================\")\n        trn_ind, val_ind = train.fold!=fold, train.fold==fold\n        print(f\"train length: {trn_ind.sum()}, valid length: {val_ind.sum()}\")\n        X_train=train.loc[trn_ind, features].values\n        y_train=y.loc[trn_ind].values.reshape(-1,1)\n        X_val=train.loc[val_ind, features].values\n        y_val=y.loc[val_ind].values.reshape(-1,1)\n\n        clf =  TabNetRegressor(**tabnet_params)\n        clf.fit(\n          X_train, y_train,\n          eval_set=[(X_val, y_val)],\n          max_epochs = 355,\n          patience = 50,\n          batch_size = 1024*20, \n          virtual_batch_size = 128*20,\n          num_workers = 4,\n          drop_last = False,\n\n          )\n        \n        clf.save_model(f'TabNet_seed{args.seed}_{fold}')\n\n\n        preds = clf.predict(train.loc[val_ind, features].values)\n        train.loc[val_ind, \"preds\"] = preds\n        \n        scores[\"rmse\"].append(rmse(y.loc[val_ind], preds))\n     \n        del X_train,X_val,y_train,y_val\n        gc.collect()\n        \n        \n    print(f\"TabNet {args.folds} folds mean rmse: {np.mean(scores['rmse'])}\")\n    train.filter(regex=r\"^(?!f_).*\").to_csv(\"preds.csv\", index=False)\n #   return features_importance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TabNet","metadata":{}},{"cell_type":"code","source":"if args.INFER:\n    pass\nelse:\n    run()  \ndel df, train\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Model","metadata":{}},{"cell_type":"code","source":"import os\nimport zipfile\n \ndef zipDir(dirpath, outFullName):\n\n    zip = zipfile.ZipFile(outFullName, \"w\", zipfile.ZIP_DEFLATED)\n    for path, dirnames, filenames in os.walk(dirpath):\n\n        fpath = path.replace(dirpath, '')\n\n        for filename in filenames:\n            zip.write(os.path.join(path, filename), os.path.join(fpath, filename))\n    zip.close()\n    \n\nif args.INFER:\n    for fold in range(5):\n        input_path =f'../input/tabnetv1/TabNet_seed{args.seed}_{fold}'\n        output_path = f\"./fold{fold}.zip\"\n        zipDir(input_path, output_path)\nelse:\n    input_path =f'./TabNet_seed{args.seed}_{fold}'\n    output_path = f\"./fold{fold}.zip\"\n\n    zipDir(input_path, output_path)\ntabnet_params = dict(\n        cat_idxs=cat_idxs,\n        cat_emb_dim=1,\n        n_d = 16,\n        n_a = 16,\n        n_steps = 2,\n        gamma =1.4690246460970766,\n        n_independent = 9,\n        n_shared = 4,\n        lambda_sparse = 0,\n        optimizer_fn = Adam,\n        optimizer_params = dict(lr = (0.0024907164557092944)),\n        mask_type = \"entmax\",\n        scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n        scheduler_fn = CosineAnnealingWarmRestarts,\n        seed = 42,\n        verbose = 10, \n    )    \n\nimport copy\nclf =  TabNetRegressor(**tabnet_params)\nmodels_tabnet = []\nfor fold in range(args.folds):\n    clf.load_model(f\"../input/baseline-tabnet/fold{fold}.zip\")\n    model=copy.deepcopy(clf)\n    models_tabnet.append(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare Test Inference","metadata":{}},{"cell_type":"code","source":"def preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\n\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef make_test_dataset_lgbm(test_df,folds=5):\n    features = [f\"f_{i}\" for i in range(300)]\n    test_df[features] = scaler.fit_transform(test_df[features]) \n    clu = [kmodels[fold].predict(test_df[features]) for fold in range(folds)]\n    test_df_l = [test_df for fold in range(folds)]\n    for f in range(folds):\n        test_df_l[f]['cluster'] = clu[f]\n    return test_df_l\n\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)\n\ndef inference_lgbm(models,ds,folds=5):\n    features = [f\"f_{i}\" for i in range(300)]\n    features_1 = features + ['cluster']\n    final_pred = [models[fold].predict(ds[fold][features_1]) for fold in range(folds)]\n    return np.mean(np.stack(final_pred), axis=0)\n\ndef inference_tabnet(models,test_df,args):\n    num_features = [f\"f_{i}\" for i in range(300)]\n    cat_features = [\"investment_id\"]\n    features = num_features + cat_features\n    features += [\"time_id\"]\n    test_df[\"time_id\"] = test_df.row_id.str.extract(r\"(\\d+)_.*\").astype(np.uint16) # extract time_id form row_id\n    final_pred = [models[fold].predict(test_df[features].values) for fold in range(args.folds)]\n    return np.mean(np.stack(final_pred), axis=0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfor (test_df, sample_prediction_df) in iter_test:\n\n    features_dnn = [f'f_{i}' for i in range(300)]\n    ds = make_test_dataset(test_df[features_dnn], test_df[\"investment_id\"])\n\n    tabnet_output = inference_tabnet(models_tabnet,test_df,args)\n    dnn_output = inference(models,ds)\n    final_output = dnn_output * 0.85 + tabnet_output *0.15\n    #final_output = tabnet_output\n    sample_prediction_df['target'] = final_output\n    env.predict(sample_prediction_df) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}