{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## What's this notebook different from others \n\n\n+ Consider intra-`time_id` context + `BertLayer` + PCC loss function -> hits LB 0.148\n    + > I've just started tuning the model structure and hyperparameters, and I believe there's still a lot of room for improvement!\n+ Provide a flexible template that allows us to validate a wide range of our ideas without changing any code (use command-line arguments if run locally or overwrite the settings in the last cell if run on Kaggle), including:\n    + Basic hyperparameters\n        + optimizer\n        + learning rate\n        + learning rate scheduler\n        + ...\n    + Model structures\n        + number of layers\n        + dimension of each layer\n        + where to add multi-head-attention layers \n        + whether to use dropout, dropout rate\n        + ...\n    + Others\n        + apply MSE or PCC as loss function\n        + number of folds\n        + whether to early stop\n        + dataset split ratio\n        + ...\n\n## More about this notebook\n\nIt is based on PyTorch Lightning and therefore supports a lot of advanced features such as\n\n+ Multi-GPU training, TPU training\n+ 16-bit precision training\n+ TensorBoard visualization\n+ Accumulates grads every k batches\n+ Stochastic weight average\n+ ...\n\nwithout the need to change the code.\n\nIn addition, if you would like to run locally, please directly refer to this [GitHub repo](https://github.com/siahuat0727/ubiquant-market-prediction), install the dependencies follow the README, and it should work! :)\n\n\n## Approach and model structure\n\n> There are many valuable public notebooks I haven't gone through now. (And I am a newbie in market prediction, so the following is just to express my thoughts.) I would appreciate it if you would like to comment and share your experience or opinion!\n\nI'm not going to focus on feature engineering in this notebook, even though it might have some benefits.\nWe know from many public notebooks that the dataset has been preprocessed and is clean enough to be directly fed into a DNN model. Therefore, I am focusing on what context information we let to model to capture and what we want the model to learn.\n\nI believe we can divide the modeling problem into three stages, with each stage gradually considering more context.\n\n**Stage 1 - Consider every target independently**\n\nThis is the structure that most other public notebooks of the DNN approach used. Although this template supports the tuning of Stage 1, I am not going to pay much attention to it.\n\n**Stage 2 - Consider intra-`time_id` context information**\n\nIn this stage, we consider all the data in a `time_id` as a whole and then predict the corresponding targets.\n\nRefer to [What this competition is about](https://www.kaggle.com/c/ubiquant-market-prediction/discussion/303397), we are probably predicting the position of an asset in a day. If that is the case, the target value itself doesn't mean anything if there are no other targets compared to it. Therefore, I would prefer **correlation-based loss** instead of MSE-like loss. ~~Even though many people are sharing that MSE loss can get a better result in this contest.~~\n\n \nTo gain the benefits of intra-`time_id` information, I believe an intuitive way is to treat the investments data of each `time_id` as an unordered sequence and then apply the model in the NLP field. A self-attention (multi-head attention) layer without positional encoding would be a good choice. In particular, I am trying `BertLayer` since the BERT model has been proven effective. Additionally, it contains a skip connection which can (probably) ensure that at least adding a self-attention layer won't be worse.\n\n>  To the extreme, if an identity mapping (_which means don't consider intra-`time_id` context in our case_) were optimal, it would be easier to push the residual (_self-attention layer in our case_) to zero than to fit an identity mapping by a stack of nonlinear layers.  -- _Deep Residual Learning for Image Recognition_\n\n**Stage 3 - Consider inter-`time_id` context information**\n\nIt is not implemented yet. An intuitive way is to have a cross-attention to all the previous inputs, but the time complexity of this structure is unacceptable if the time series is too long. To tackle this challenge, we can define a sufficient number of tokens to join the self-attention and let the result tokens recurrently feed to the next time-step to act as a memory pool.\n\n## Batching strategy and data augmentation\n\nStarting with Stage 2, we expect to sample training data by `time_id`. And we can't stack it directly to create a mini-batch because the number of investments data is different at each `time_id`. An intuitive solution is to set batch size equal to 1, but this is not efficient for training and can lead to training instability. Again, we are probably predicting the location of a set of assets. If that is the case, we can randomly truncate some data to match the number of investments for every `time_id` in the batch. Indeed, I believe this can treat as a **data augmentation** for Stage 2 and 3 since we are implicitly teaching the model that **the absence of some assets data shouldn't affect the relative positions of the remaining assets.** (means nothing for Stage 1 since it doesn't consider other assets data when making a prediction)\n\n## Others\n\n+ unseen investment data augmentation (?)\n\t+ randomly mask the investment embedding when training\n+ cv vs. lb\n\t+ I think the next urgent thing might be learning about cv strategies from the discussion\n\nCheck the following work on [this notebook](https://www.kaggle.com/siahuat/memory-transformer-intra-inter-time-context) if you are interested!\n\n**Please upvote if you like it!**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Talk is cheap. Here is the code.","metadata":{}},{"cell_type":"code","source":"!pip install ../input/202202-libraries/torchmetrics-0.7.2-py3-none-any.whl --user","metadata":{"execution":{"iopub.status.busy":"2022-03-13T15:34:02.237666Z","iopub.execute_input":"2022-03-13T15:34:02.238194Z","iopub.status.idle":"2022-03-13T15:34:31.069433Z","shell.execute_reply.started":"2022-03-13T15:34:02.238065Z","shell.execute_reply":"2022-03-13T15:34:31.068574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchmetrics\nassert torchmetrics.__version__ == '0.7.2', torchmetrics.__version__\n\nimport torch\nprint(torch.cuda.get_device_name(0))","metadata":{"execution":{"iopub.status.busy":"2022-03-13T15:34:31.07321Z","iopub.execute_input":"2022-03-13T15:34:31.073853Z","iopub.status.idle":"2022-03-13T15:34:33.128278Z","shell.execute_reply.started":"2022-03-13T15:34:31.073815Z","shell.execute_reply":"2022-03-13T15:34:33.127502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## constants.py","metadata":{}},{"cell_type":"code","source":"FEATURES = [f'f_{i}' for i in range(300)]","metadata":{"execution":{"iopub.status.busy":"2022-03-13T15:34:33.129531Z","iopub.execute_input":"2022-03-13T15:34:33.129819Z","iopub.status.idle":"2022-03-13T15:34:33.134897Z","shell.execute_reply.started":"2022-03-13T15:34:33.129768Z","shell.execute_reply":"2022-03-13T15:34:33.134054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## model.py","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers import BertConfig\nfrom transformers.models.bert.modeling_bert import BertLayer as _BertLayer\n\n\nclass SafeEmbedding(nn.Embedding):\n    \"Handle unseen id\"\n\n    def forward(self, input):\n        output = torch.empty((*input.size(), self.embedding_dim),\n                             device=input.device,\n                             dtype=self.weight.dtype)\n\n        seen = input < self.num_embeddings\n        unseen = seen.logical_not()\n\n        output[seen] = super().forward(input[seen])\n#         output[unseen] = torch.zeros_like(self.weight[0])\n        output[unseen] = self.weight.mean(dim=0).detach()\n        return output\n\n\nclass FlattenBatchNorm1d(nn.BatchNorm1d):\n    \"BatchNorm1d that treats (N, C, L) as (N*C, L)\"\n\n    def forward(self, input):\n        sz = input.size()\n        return super().forward(input.view(-1, sz[-1])).view(*sz)\n\n\nclass BertLayer(_BertLayer):\n    def forward(self, *args, **kwargs):\n        return super().forward(*args, **kwargs)[0]\n\n\nclass Net(nn.Module):\n    def __init__(self, args, n_feature):\n        super().__init__()\n\n        self.emb = SafeEmbedding(args.n_emb, args.emb_dim)\n\n        in_size = args.emb_dim + n_feature\n        szs = [in_size] + args.szs\n\n        self.layers = nn.Sequential(*self.get_layers(args, szs))\n\n        self.post_init()\n\n    def get_layers(self, args, szs):\n        layers = []\n\n        for layer_i, (in_sz, out_sz) in enumerate(zip(szs[:-1], szs[1:])):\n            layers.append(nn.Linear(in_sz, out_sz))\n            layers.append(FlattenBatchNorm1d(out_sz))\n            layers.append(nn.SiLU(inplace=True))\n\n            if args.dropout > 0.0:\n                layers.append(nn.Dropout(p=args.dropout, inplace=True))\n\n            if layer_i in args.mhas:\n                layers.append(BertLayer(BertConfig(\n                    num_attention_heads=8,\n                    hidden_size=out_sz,\n                    intermediate_size=out_sz)))\n\n        layers.append(nn.Linear(szs[-1], 1))\n        return layers\n\n    def forward(self, x_id, x_feat):\n        x_emb = self.emb(x_id)\n        out = torch.cat((x_emb, x_feat), dim=-1)\n        return self.layers(out).squeeze(-1)\n\n    def post_init(self):\n        for m in self.modules():\n            if isinstance(m, (nn.Linear, SafeEmbedding)):\n                nn.init.kaiming_normal_(\n                    m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, (nn.BatchNorm1d, nn.LayerNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Embedding):\n                m.weight.data.normal_(mean=0.0, std=0.02)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-13T15:34:33.136415Z","iopub.execute_input":"2022-03-13T15:34:33.136964Z","iopub.status.idle":"2022-03-13T15:34:33.186347Z","shell.execute_reply.started":"2022-03-13T15:34:33.136929Z","shell.execute_reply":"2022-03-13T15:34:33.185643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## data_module.py","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nfrom torch.utils.data import DataLoader, random_split\n\n# from constants import FEATURES\n\n\ndef collate_fn(datas):\n    prems = [torch.randperm(data[0].size(0)) for data in datas]\n    length = min(data[0].size(0) for data in datas)\n    return [\n        torch.stack([d[i][perm][:length] for d, perm in zip(datas, prems)])\n        for i in range(3)\n    ]\n\n\nclass MyDataset(torch.utils.data.Dataset):\n    def __init__(self, *tensor_lists) -> None:\n        assert all(len(tensor_lists[0]) == len(\n            t) for t in tensor_lists), \"Size mismatch between tensor_lists\"\n        self.tensor_lists = tensor_lists\n\n    def __getitem__(self, index):\n        return tuple(t[index] for t in self.tensor_lists)\n\n    def __len__(self):\n        return len(self.tensor_lists[0])\n\n\ndef df_to_input_id(df):\n    return torch.tensor(df['investment_id'].to_numpy(dtype=np.int16),\n                        dtype=torch.int)\n\n\ndef df_to_input_feat(df):\n    return torch.tensor(df[FEATURES].to_numpy(),\n                        dtype=torch.float32)\n\n\ndef df_to_target(df):\n    return torch.tensor(df['target'].to_numpy(),\n                        dtype=torch.float32)\n\n\ndef load_data(path):\n    df = pd.read_parquet(path)\n    groups = df.groupby('time_id')\n    return [\n        groups.get_group(v)\n        for v in df.time_id.unique()\n    ]\n\n\ndef split(df_groupby_time, split_ratios):\n    ids = [df_to_input_id(df) for df in df_groupby_time]\n    feats = [df_to_input_feat(df) for df in df_groupby_time]\n    targets = [df_to_target(df) for df in df_groupby_time]\n\n    dataset = MyDataset(ids, feats, targets)\n\n    lengths = []\n    for ratio in split_ratios[:-1]:\n        lengths.append(int(len(dataset)*ratio))\n    lengths.append(len(dataset) - sum(lengths))\n\n    return random_split(dataset, lengths)\n\n\nclass UMPDataModule(pl.LightningDataModule):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n\n        datasets = split(load_data(args.input), args.split_ratios)\n        if len(datasets) == 3:\n            self.tr, self.val, self.test = datasets\n        else:\n            self.tr, self.val = datasets\n            self.test = self.val\n\n    def train_dataloader(self):\n        return DataLoader(self.tr, batch_size=self.args.batch_size,\n                          num_workers=self.args.workers, shuffle=True,\n                          collate_fn=collate_fn, drop_last=True,\n                          pin_memory=True)\n\n    def _val_dataloader(self, dataset):\n        return DataLoader(dataset, batch_size=1,\n                          num_workers=self.args.workers, pin_memory=True)\n\n    def val_dataloader(self):\n        return self._val_dataloader(self.val)\n\n    def test_dataloader(self):\n        return self._val_dataloader(self.test)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T15:34:33.18955Z","iopub.execute_input":"2022-03-13T15:34:33.191096Z","iopub.status.idle":"2022-03-13T15:34:34.638425Z","shell.execute_reply.started":"2022-03-13T15:34:33.191047Z","shell.execute_reply":"2022-03-13T15:34:34.637605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## litmodule.py","metadata":{}},{"cell_type":"code","source":"import torch\nfrom pytorch_lightning import LightningModule\nfrom pytorch_lightning.callbacks import (EarlyStopping, LearningRateMonitor,\n                                         ModelCheckpoint,\n                                         StochasticWeightAveraging)\nfrom torch import nn\nfrom torchmetrics import PearsonCorrCoef\n\n# from constants import FEATURES\n# from model import Net\n\n\ndef get_loss_fn(loss):\n    def mse(preds, y):\n        return nn.MSELoss()(preds, y)\n\n    def pcc(preds, y):\n        assert preds.dim() == 2, preds.size()\n        assert preds.size() == y.size(), (preds.size(), y.size())\n\n        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n        return -cos(preds - preds.mean(dim=1, keepdim=True),\n                    y - y.mean(dim=1, keepdim=True)).mean()\n\n    return {\n        'mse': mse,\n        'pcc': pcc,\n    }[loss]\n\n\nclass UMPLitModule(LightningModule):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        self.model = Net(args, n_feature=len(FEATURES))\n        self.test_pearson = PearsonCorrCoef()\n        self.loss_fn = get_loss_fn(args.loss)\n\n    def forward(self, *args):\n        return self.model(*args)\n\n    def training_step(self, batch, batch_idx):\n        x_id, x_feat, y = batch\n\n        preds = self.forward(x_id, x_feat)\n        loss = self.loss_fn(preds, y)\n        self.log('train_loss', loss, on_epoch=True)\n        return loss\n\n    def _evaluate_step(self, batch, batch_idx, stage):\n        x_id, x_feat, y = batch\n\n        preds = self.forward(x_id, x_feat)\n        self.test_pearson(preds, y)\n        self.log(f'{stage}_pearson', self.test_pearson, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        return self._evaluate_step(batch, batch_idx, 'test')\n\n    def validation_step(self, batch, batch_idx):\n        return self._evaluate_step(batch, batch_idx, 'val')\n\n    def configure_optimizers(self):\n        kwargs = {\n            'lr': self.args.lr,\n            'weight_decay': self.args.weight_decay,\n        }\n\n        optimizer = {\n            'adam': torch.optim.Adam(self.model.parameters(), **kwargs),\n            'adamw': torch.optim.AdamW(self.model.parameters(), **kwargs),\n        }[self.args.optimizer]\n\n        optim_config = {\n            'optimizer': optimizer,\n        }\n        if self.args.lr_scheduler is not None:\n            optim_config['lr_scheduler'] = {\n                'step_lr': torch.optim.lr_scheduler.StepLR(\n                    optimizer, step_size=5, gamma=0.8),\n            }[self.args.lr_scheduler]\n\n        return optim_config\n\n    def configure_callbacks(self):\n        callbacks = [\n            LearningRateMonitor(),\n            ModelCheckpoint(monitor='val_pearson', mode='max', save_last=True,\n                            filename='{epoch}-{val_pearson:.4f}'),\n        ]\n        if self.args.swa:\n            callbacks.append(StochasticWeightAveraging(swa_epoch_start=0.7,\n                                                       device='cpu'))\n        if self.args.early_stop:\n            callbacks.append(EarlyStopping(monitor='val_pearson',\n                                           mode='max', patience=10))\n        return callbacks","metadata":{"execution":{"iopub.status.busy":"2022-03-13T15:34:34.640026Z","iopub.execute_input":"2022-03-13T15:34:34.640293Z","iopub.status.idle":"2022-03-13T15:34:34.664155Z","shell.execute_reply.started":"2022-03-13T15:34:34.640256Z","shell.execute_reply":"2022-03-13T15:34:34.663375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## main.py","metadata":{}},{"cell_type":"code","source":"from argparse import ArgumentParser\n\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.loggers import TensorBoardLogger\n\n# from data_module import (UMPDataModule, df_to_input_feat, df_to_input_id,\n#                          load_data)\n# from litmodule import UMPLitModule\n\n\ndef get_name(args):\n    return '-'.join(filter(None, [  # Remove empty string by filtering\n        'x'.join(str(sz) for sz in args.szs),\n        'x'.join(str(mha) for mha in args.mhas),\n        f'epch{args.max_epochs}',\n        f'btch{args.batch_size}',\n        f'{args.optimizer}',\n        f'drop{args.dropout}',\n        f'schd{args.lr_scheduler}',\n        f'loss{args.loss}',\n        f'lr{args.lr}',\n        f'wd{args.weight_decay}',\n        f'swa{args.swa}',\n        f'emb{args.emb_dim}',\n    ])).replace(' ', '')\n\n\ndef submit(args, ckpts):\n\n    litmodels = [\n        UMPLitModule.load_from_checkpoint(ckpt_path, args=args).eval()\n        for ckpt_path in ckpts\n    ]\n\n    import ubiquant\n    env = ubiquant.make_env()   # initialize the environment\n\n    for test_df, submit_df in env.iter_test():\n        input_ids = df_to_input_id(test_df).unsqueeze(0)\n        input_feats = df_to_input_feat(test_df).unsqueeze(0)\n\n        with torch.no_grad():\n            submit_df['target'] = torch.cat([\n                litmodel.forward(input_ids, input_feats)\n                for litmodel in litmodels\n            ]).mean(dim=0)\n\n        env.predict(submit_df)   # register your predictions\n\n\ndef test(args):\n    seed_everything(args.seed)\n\n    litmodel = UMPLitModule.load_from_checkpoint(args.checkpoint, args=args)\n    dm = UMPDataModule(args)\n\n    Trainer.from_argparse_args(args).test(litmodel, datamodule=dm)\n\n\ndef train_single(args, seed):\n    seed_everything(seed)\n\n    litmodel = UMPLitModule(args)\n    dm = UMPDataModule(args)\n\n    name = get_name(args)\n    logger = TensorBoardLogger(save_dir='logs', name=name)\n\n    trainer = Trainer.from_argparse_args(args,\n                                         logger=logger,\n                                         deterministic=True,\n                                         precision=16)\n    trainer.fit(litmodel, dm)\n\n    best_ckpt = trainer.checkpoint_callback.best_model_path\n    test_result = trainer.test(ckpt_path=best_ckpt,\n                               datamodule=dm)\n\n    return {\n        'ckpt_path': best_ckpt,\n        'test_pearson': test_result[0]['test_pearson']\n    }\n\n\ndef train(args):\n    return [\n        train_single(args, seed)\n        for seed in range(args.seed, args.seed + args.n_fold)\n    ]\n\n\ndef parse_args(is_kaggle=False):\n    parser = ArgumentParser()\n    parser = Trainer.add_argparse_args(parser)\n\n    parser.add_argument('--workers', type=int, default=2)\n    parser.add_argument(\n        '--input', default='../input/ubiquant-parquet/train_low_mem.parquet')\n\n    # Hyperparams\n    parser.add_argument('--batch_size', type=int, default=8)\n    parser.add_argument('--lr', type=float, default=0.001)\n    parser.add_argument('--weight_decay', type=float, default=1e-4)\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--optimizer', default='adam',\n                        choices=['adam', 'adamw'])\n    parser.add_argument('--lr_scheduler', default=None)\n    parser.add_argument('--loss', default='pcc', choices=['mse', 'pcc'])\n    parser.add_argument('--emb_dim', type=int, default=32)\n    parser.add_argument('--n_fold', type=int, default=1)\n    parser.add_argument('--split_ratios', type=float, nargs='+',\n                        default=[0.7, 0.15, 0.15],\n                        help='train, val, and test set (optional) split ratio')\n    parser.add_argument('--early_stop', action='store_true')\n    parser.add_argument('--swa', action='store_true',\n                        help='whether to perform Stochastic Weight Averaging')\n\n    # Model structure\n    parser.add_argument('--n_emb', type=int, default=4000)  # TODO tight\n    parser.add_argument('--szs', type=int, nargs='+',\n                        default=[512, 256, 128, 64])\n    parser.add_argument(\n        '--mhas', type=int, nargs='+', default=[],\n        help=('Insert MHA layer (BertLayer) at the i-th layer (start from 1). '\n              'Every element should be <= len(szs)'))\n    parser.add_argument('--dropout', type=float, default=0.0,\n                        help='Set to 0.0 to disable')\n\n    # Test\n    parser.add_argument('--test', action='store_true')\n\n    # Checkpoint\n    parser.add_argument('--checkpoint', help='path to checkpoints (for test)')\n\n    # Handle kaggle platform\n    args, unknown = parser.parse_known_args()\n\n    if not is_kaggle:\n        assert not unknown, f'unknown args: {unknown}'\n\n    assert all(0 < i <= len(args.szs) for i in args.mhas)\n    return args\n\n\ndef run_local():\n    args = parse_args()\n\n    if args.test:\n        test(args)\n        return\n\n    best_results = train(args)\n    test_pearsons = [res['test_pearson'] for res in best_results]\n    print(f'mean={sum(test_pearsons)/len(test_pearsons)}, {test_pearsons}')\n    print(best_results)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T15:34:34.667477Z","iopub.execute_input":"2022-03-13T15:34:34.667676Z","iopub.status.idle":"2022-03-13T15:34:34.693094Z","shell.execute_reply.started":"2022-03-13T15:34:34.667651Z","shell.execute_reply":"2022-03-13T15:34:34.692388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def kaggle():\n    args = parse_args(True)\n    # On kaggle mode, we are using only the args with default value\n    # To changle arguments, please hard code it below, e.g.:\n    # args.loss = 'mse'\n    # args.szs = [512, 128, 64, 64, 64]\n\n    args.szs = [512, 128, 64, 64, 32]     \n    args.mhas = [3]\n    args.dropout = 0.4\n    args.weight_decay = 0.0002\n    args.split_ratios = [0.8, 0.2]\n    args.gpus = 1\n    args.max_epochs = 1\n    args.early_stop = True\n    args.szs = [512, 256, 64, 64, 32]  \n    args.mhas = [3, 4]     \n    \n    \n    \n    do_submit = True\n    train_on_kaggle = False\n    \n    if train_on_kaggle:\n        best_results = train(args)\n        ckpts = [res['ckpt_path'] for res in best_results]\n\n        test_pearsons = [res['test_pearson'] for res in best_results]\n        print(f'mean={sum(test_pearsons)/len(test_pearsons)}, {test_pearsons}')\n    else:\n        from glob import glob\n        # TODO fill in the ckpt paths\n        ckpts = []\n        ckpts = glob('../input/512x256x64x64x32x3x4x04xplateaux0001x00003/*/version_*/*/e*.ckpt')\n        # ckpts.sort()\n        # ckpts = [ckpts[5]]\n        print('\\n'.join(ckpts))\n\n    assert ckpts\n\n    if do_submit:\n        submit(args, ckpts)\n\n\nif __name__ == '__main__':\n    is_kaggle = True\n    if is_kaggle:\n        kaggle()\n    else:\n        run_local()","metadata":{"execution":{"iopub.status.busy":"2022-03-14T09:53:12.990174Z","iopub.execute_input":"2022-03-14T09:53:12.990577Z","iopub.status.idle":"2022-03-14T09:53:13.097888Z","shell.execute_reply.started":"2022-03-14T09:53:12.990487Z","shell.execute_reply":"2022-03-14T09:53:13.097007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Use tensorboard to visualize the training process**\n\n```\ntensorboard --logdir=logs/\n```","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}