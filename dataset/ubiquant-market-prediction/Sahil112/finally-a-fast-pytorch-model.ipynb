{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome to Pytorch!\nAfter almost everyone using Keras to get good scores in the competition, I took the challenge to use the beloved Pytorch! <br>\nMy Previous tries were good, but they took **4 hours to run**\nNow after I made changes as the great francescopochetti, my excecution time has **come down to 10 minutes** (WOHOOOO!) <br>\nHave a look at the original work of Frances http://francescopochetti.com/pytorch-for-tabular-data-predicting-nyc-taxi-fares/\n","metadata":{}},{"cell_type":"markdown","source":"#### upvote if you find it useful. Sharing is the best way to learn!","metadata":{}},{"cell_type":"markdown","source":"Ideas to imporve:\n* Get a early stopping callback\n* Get learning rate scheduler\n* Make Network Deeper","metadata":{}},{"cell_type":"markdown","source":"# Simple imports","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nimport pathlib\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\npd.set_option('display.max_columns', 500)\nfrom collections import defaultdict\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\n","metadata":{"papermill":{"duration":7.781864,"end_time":"2022-01-25T15:39:09.265726","exception":false,"start_time":"2022-01-25T15:39:01.483862","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-24T13:00:44.613699Z","iopub.execute_input":"2022-02-24T13:00:44.614527Z","iopub.status.idle":"2022-02-24T13:00:45.579863Z","shell.execute_reply.started":"2022-02-24T13:00:44.614468Z","shell.execute_reply":"2022-02-24T13:00:45.57906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pytroch imports","metadata":{}},{"cell_type":"code","source":"from torch.nn import init\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.utils import data\nfrom torch.optim import lr_scheduler\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nfrom tqdm import tqdm # , # tqdm_notebook, # tnrange\nfrom tqdm.notebook import trange as tnrange # will change this to trange later \nfrom tqdm.notebook import tqdm as tqdm_notebook # will change this to tqdm later\ntqdm.pandas(desc='Progress')","metadata":{"papermill":{"duration":7.781864,"end_time":"2022-01-25T15:39:09.265726","exception":false,"start_time":"2022-01-25T15:39:01.483862","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-24T13:00:45.58147Z","iopub.execute_input":"2022-02-24T13:00:45.581765Z","iopub.status.idle":"2022-02-24T13:00:46.877298Z","shell.execute_reply.started":"2022-02-24T13:00:45.581727Z","shell.execute_reply":"2022-02-24T13:00:46.876537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:00:46.878573Z","iopub.execute_input":"2022-02-24T13:00:46.878836Z","iopub.status.idle":"2022-02-24T13:00:47.002464Z","shell.execute_reply.started":"2022-02-24T13:00:46.878801Z","shell.execute_reply":"2022-02-24T13:00:47.00166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:00:47.004639Z","iopub.execute_input":"2022-02-24T13:00:47.005101Z","iopub.status.idle":"2022-02-24T13:00:47.028438Z","shell.execute_reply.started":"2022-02-24T13:00:47.005059Z","shell.execute_reply":"2022-02-24T13:00:47.027435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import dataset","metadata":{"papermill":{"duration":0.015291,"end_time":"2022-01-25T15:39:09.296817","exception":false,"start_time":"2022-01-25T15:39:09.281526","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\ndf.head(2)","metadata":{"papermill":{"duration":16.88418,"end_time":"2022-01-25T15:39:26.19638","exception":false,"start_time":"2022-01-25T15:39:09.3122","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-24T13:00:47.109951Z","iopub.execute_input":"2022-02-24T13:00:47.110251Z","iopub.status.idle":"2022-02-24T13:01:04.273984Z","shell.execute_reply.started":"2022-02-24T13:00:47.110192Z","shell.execute_reply":"2022-02-24T13:01:04.273189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:04.275323Z","iopub.execute_input":"2022-02-24T13:01:04.275904Z","iopub.status.idle":"2022-02-24T13:01:04.281104Z","shell.execute_reply.started":"2022-02-24T13:01:04.275865Z","shell.execute_reply":"2022-02-24T13:01:04.2804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setting investment id as categorical feature (just trying out!)","metadata":{}},{"cell_type":"code","source":"# setting as category feature\ndf['investment_id'] = df['investment_id'].astype('category')","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:04.282319Z","iopub.execute_input":"2022-02-24T13:01:04.283351Z","iopub.status.idle":"2022-02-24T13:01:04.380919Z","shell.execute_reply.started":"2022-02-24T13:01:04.283312Z","shell.execute_reply":"2022-02-24T13:01:04.380118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining some helper functions to make life easy later","metadata":{}},{"cell_type":"code","source":"def split_features(df):\n    catf = ['investment_id']\n    numf = [col for col in df.columns if col not in catf]\n    \n    for c in catf: \n        df[c] = df[c].astype('category').cat.as_ordered()\n        df[c] = df[c].cat.codes + 1\n    \n    return catf, numf","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:04.38216Z","iopub.execute_input":"2022-02-24T13:01:04.382423Z","iopub.status.idle":"2022-02-24T13:01:04.390124Z","shell.execute_reply.started":"2022-02-24T13:01:04.38239Z","shell.execute_reply":"2022-02-24T13:01:04.389425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def emb_init(x):\n    x = x.weight.data\n    sc = 2/(x.size(1)+1)\n    x.uniform_(-sc,sc)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:04.391593Z","iopub.execute_input":"2022-02-24T13:01:04.392134Z","iopub.status.idle":"2022-02-24T13:01:04.399236Z","shell.execute_reply.started":"2022-02-24T13:01:04.392093Z","shell.execute_reply":"2022-02-24T13:01:04.398348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.loc[df['time_id']>400] # filter out old data","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:04.403023Z","iopub.execute_input":"2022-02-24T13:01:04.403297Z","iopub.status.idle":"2022-02-24T13:01:04.409194Z","shell.execute_reply.started":"2022-02-24T13:01:04.403265Z","shell.execute_reply":"2022-02-24T13:01:04.408388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## make use of helper functions!","metadata":{}},{"cell_type":"code","source":"y = df['target']\ndf = df.drop(columns = ['target'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:04.410496Z","iopub.execute_input":"2022-02-24T13:01:04.410947Z","iopub.status.idle":"2022-02-24T13:01:08.785406Z","shell.execute_reply.started":"2022-02-24T13:01:04.41091Z","shell.execute_reply":"2022-02-24T13:01:08.784559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catf, numf = split_features(df)\n\nprint(len(catf))\nprint(catf)\n\nprint(len(numf))\n# numf","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:08.790454Z","iopub.execute_input":"2022-02-24T13:01:08.791049Z","iopub.status.idle":"2022-02-24T13:01:08.813231Z","shell.execute_reply.started":"2022-02-24T13:01:08.79099Z","shell.execute_reply":"2022-02-24T13:01:08.812539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.25, random_state=1)\nprint(X_train.shape, X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:08.817198Z","iopub.execute_input":"2022-02-24T13:01:08.817741Z","iopub.status.idle":"2022-02-24T13:01:16.842006Z","shell.execute_reply.started":"2022-02-24T13:01:08.817703Z","shell.execute_reply":"2022-02-24T13:01:16.841158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_sz = [(c, df[c].max()+1) for c in catf]\nprint(cat_sz)\n\nemb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]\nprint(emb_szs)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:16.843567Z","iopub.execute_input":"2022-02-24T13:01:16.844054Z","iopub.status.idle":"2022-02-24T13:01:16.855884Z","shell.execute_reply.started":"2022-02-24T13:01:16.844011Z","shell.execute_reply":"2022-02-24T13:01:16.854646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define the Dataset by rewriting the data.Dataset module","metadata":{}},{"cell_type":"code","source":"class RegressionColumnarDataset(data.Dataset):\n    def __init__(self, df, cats, y):\n        self.dfcats = df[cats]\n        self.dfconts = df.drop(cats, axis=1)\n        \n        self.cats = np.stack([c.values for n, c in self.dfcats.items()], axis=1).astype(np.int64)\n        self.conts = np.stack([c.values for n, c in self.dfconts.items()], axis=1).astype(np.float32)\n        self.y = y.values.astype(np.float32)\n        \n    def __len__(self): return len(self.y)\n\n    def __getitem__(self, idx):\n        return [self.cats[idx], self.conts[idx], self.y[idx]]","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:16.858892Z","iopub.execute_input":"2022-02-24T13:01:16.859193Z","iopub.status.idle":"2022-02-24T13:01:16.868619Z","shell.execute_reply.started":"2022-02-24T13:01:16.85915Z","shell.execute_reply":"2022-02-24T13:01:16.86736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainds = RegressionColumnarDataset(X_train, catf, y_train)\nvalds = RegressionColumnarDataset(X_test, catf, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:16.870403Z","iopub.execute_input":"2022-02-24T13:01:16.870954Z","iopub.status.idle":"2022-02-24T13:01:49.797364Z","shell.execute_reply.started":"2022-02-24T13:01:16.870915Z","shell.execute_reply":"2022-02-24T13:01:49.796546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X_train, X_test, y_train, y_test","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:49.79886Z","iopub.execute_input":"2022-02-24T13:01:49.799117Z","iopub.status.idle":"2022-02-24T13:01:49.871893Z","shell.execute_reply.started":"2022-02-24T13:01:49.799083Z","shell.execute_reply":"2022-02-24T13:01:49.871148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traindl = data.DataLoader(trainds, batch_size = 1024, shuffle = True, num_workers = 2, pin_memory = True)\nvaldl = data.DataLoader(valds, batch_size = 2048, shuffle = True, num_workers = 2, pin_memory = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:49.873847Z","iopub.execute_input":"2022-02-24T13:01:49.874493Z","iopub.status.idle":"2022-02-24T13:01:49.882148Z","shell.execute_reply.started":"2022-02-24T13:01:49.874439Z","shell.execute_reply":"2022-02-24T13:01:49.881347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_cont = len(df.columns)-len(catf)\nn_cont","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:49.883098Z","iopub.execute_input":"2022-02-24T13:01:49.884689Z","iopub.status.idle":"2022-02-24T13:01:49.89294Z","shell.execute_reply.started":"2022-02-24T13:01:49.884642Z","shell.execute_reply":"2022-02-24T13:01:49.892137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df,trainds, valds","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:49.893878Z","iopub.execute_input":"2022-02-24T13:01:49.894527Z","iopub.status.idle":"2022-02-24T13:01:49.969925Z","shell.execute_reply.started":"2022-02-24T13:01:49.894491Z","shell.execute_reply":"2022-02-24T13:01:49.969175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training!","metadata":{}},{"cell_type":"markdown","source":"## The Neural Network :)","metadata":{}},{"cell_type":"markdown","source":"This may look complex (it does to me!),  but actually this is quite simple. Have a nice read and check it out. <br>\nThe model mainly uses Embedding layers for the categorical variable (investmentid) and simple dense layers otherwise","metadata":{}},{"cell_type":"code","source":"class MixedInputModel(nn.Module):\n    def __init__(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops, use_bn=True):\n        super().__init__()\n        \n        for i,(c,s) in enumerate(emb_szs): \n            assert c > 1, f\"cardinality must be >=2, got emb_szs[{i}]: ({c},{s})\"\n        \n        self.embs = nn.ModuleList([nn.Embedding(c, s) for c,s in emb_szs])\n        \n        for emb in self.embs: emb_init(emb)\n        n_emb = sum(e.embedding_dim for e in self.embs)\n        self.n_emb, self.n_cont = n_emb, n_cont\n        \n        # embeddings are done, now concatatenate \n        szs = [n_emb + n_cont] + szs\n        self.lins = nn.ModuleList([nn.Linear(szs[i], szs[i+1]) for i in range(len(szs)-1)])\n        self.bns = nn.ModuleList([nn.BatchNorm1d(sz) for sz in szs[1:]])\n        \n        # simple lines to make sure the weights are initialised in a kaiming distribution\n        for o in self.lins: nn.init.kaiming_normal_(o.weight.data)\n            \n        self.outp = nn.Linear(szs[-1], out_sz) # define output layer\n        nn.init.kaiming_normal_(self.outp.weight.data)\n\n        # define dropout layers\n        self.emb_drop = nn.Dropout(emb_drop)\n        self.drops = nn.ModuleList([nn.Dropout(drop) for drop in drops])\n        \n        # define batch normalisation layers\n        self.bn = nn.BatchNorm1d(n_cont)\n        self.use_bn = use_bn\n\n    def forward(self, x_cat, x_cont):\n        # print('initial shape HOW TO GET')\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embs)]\n            # print('embs len', len(x), 'elements like', x[:5])\n            x = torch.cat(x, 1)\n            # print('cat', x.shape)\n            x = self.emb_drop(x)\n            # print('emb drop', x.shape)\n            \n        # print('\\n')\n        if self.n_cont != 0:\n            x2 = self.bn(x_cont)\n            # print('bn get x2', x2.shape)\n            x = torch.cat([x, x2], 1) if self.n_emb != 0 else x2\n            # print('cat again', x.shape)\n            \n        # print('\\n')\n        for l,d,b in zip(self.lins, self.drops, self.bns):\n            # changing order to fc - bn - relu - dropouts\n            x = l(x)\n            # print('linear', x.shape)\n            if self.use_bn: x = b(x)\n            # print('bn', x.shape)\n            x = F.silu(x) # silu activation istead of the usual ReLU\n            # print('silu', x.shape)\n            x = d(x)\n            # print('drops', x.shape)\n            # print('\\n')\n            \n        # print('\\n')\n        x = self.outp(x)\n        # print('output', x.shape)\n        \n            \n        return x.squeeze()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:49.973194Z","iopub.execute_input":"2022-02-24T13:01:49.97344Z","iopub.status.idle":"2022-02-24T13:01:49.990112Z","shell.execute_reply.started":"2022-02-24T13:01:49.973411Z","shell.execute_reply":"2022-02-24T13:01:49.989418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = MixedInputModel(emb_szs=emb_szs, \n                    n_cont=n_cont, \n                    emb_drop=0.04, \n                    out_sz=1, \n                    szs=[400, 650, 950, 650, 400, 128, 8], \n                    drops=[0.1, 0.1, 0.1, 0.3, 0.1, 0.01, 0.0001]).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:01:49.991705Z","iopub.execute_input":"2022-02-24T13:01:49.992235Z","iopub.status.idle":"2022-02-24T13:01:52.951282Z","shell.execute_reply.started":"2022-02-24T13:01:49.992198Z","shell.execute_reply":"2022-02-24T13:01:52.950501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### check if the model looks good","metadata":{}},{"cell_type":"code","source":"m","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-24T13:00:24.348859Z","iopub.status.idle":"2022-02-24T13:00:24.349363Z","shell.execute_reply.started":"2022-02-24T13:00:24.349149Z","shell.execute_reply":"2022-02-24T13:00:24.349171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get single elements, and train them on the CPU iteself\nfor cat, cont, y in traindl:\n    print(cat.device, cont.device, y.device)\n    \n    break","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:00:24.35044Z","iopub.status.idle":"2022-02-24T13:00:24.350998Z","shell.execute_reply.started":"2022-02-24T13:00:24.350772Z","shell.execute_reply":"2022-02-24T13:00:24.350797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# better way to see the network\nuncomment the print statements in the network, this will the output <br>\nIt helps to nicely see what is going on. **However, I will try my best to convert this to a nn.Sequential type for east understanding**","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"# m(cat, cont)\n\n# initial shape HOW TO GET?? ( will have to see better)\n# embs len 1 elements like [tensor([[-6.8909e-03,  3.2817e-02, -2.0786e-02,  ...,  3.0483e-02,\n#          -4.5606e-04, -5.0471e-04],\n#         [-5.0236e-03, -2.1645e-02, -4.6295e-05,  ...,  8.9745e-03,\n#           2.3533e-02,  3.0192e-02],\n#         [ 2.5954e-02,  2.4555e-03, -2.6891e-02,  ..., -1.2833e-02,\n#           1.6570e-02, -3.5575e-03],\n#         ...,\n#         [ 1.9291e-02, -2.6346e-02,  4.3786e-03,  ...,  2.5476e-02,\n#          -6.8894e-03,  3.7377e-02],\n#         [-1.0805e-02, -1.9892e-02, -3.4380e-02,  ..., -3.1371e-02,\n#           5.2091e-03,  6.5443e-03],\n#         [ 3.8637e-02,  4.4848e-03, -2.4427e-02,  ..., -1.6600e-02,\n#           9.7741e-03, -2.1790e-05]], grad_fn=<EmbeddingBackward>)]\n# cat torch.Size([1024, 50])\n# emb drop torch.Size([1024, 50])\n\n\n# bn get x2 torch.Size([1024, 301])\n# cat again torch.Size([1024, 351])\n\n\n# linear torch.Size([1024, 400])\n# bn torch.Size([1024, 400])\n# silu torch.Size([1024, 400])\n# drops torch.Size([1024, 400])\n\n\n# linear torch.Size([1024, 500])\n# bn torch.Size([1024, 500])\n# silu torch.Size([1024, 500])\n# drops torch.Size([1024, 500])\n\n\n# linear torch.Size([1024, 750])\n# bn torch.Size([1024, 750])\n# silu torch.Size([1024, 750])\n# drops torch.Size([1024, 750])\n\n\n# linear torch.Size([1024, 500])\n# bn torch.Size([1024, 500])\n# silu torch.Size([1024, 500])\n# drops torch.Size([1024, 500])\n\n\n# linear torch.Size([1024, 400])\n# bn torch.Size([1024, 400])\n# silu torch.Size([1024, 400])\n# drops torch.Size([1024, 400])\n\n\n# linear torch.Size([1024, 128])\n# bn torch.Size([1024, 128])\n# silu torch.Size([1024, 128])\n# drops torch.Size([1024, 128])\n\n\n# linear torch.Size([1024, 8])\n# bn torch.Size([1024, 8])\n# silu torch.Size([1024, 8])\n# drops torch.Size([1024, 8])\n\n\n\n\n# output torch.Size([1024, 1])\n# tensor([ 0.1282,  0.3186,  0.1614,  ..., -0.1414, -0.0461,  0.1904],\n#        grad_fn=<SqueezeBackward0>)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-24T13:00:24.352094Z","iopub.status.idle":"2022-02-24T13:00:24.352659Z","shell.execute_reply.started":"2022-02-24T13:00:24.352399Z","shell.execute_reply":"2022-02-24T13:00:24.352424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# overfit on one batch\nThis is an incredibly important step to make sure that the model 'works'. Highly recommended by a lot of other experts, and I am really happy","metadata":{}},{"cell_type":"code","source":"# compile the neural net\nnetwork = MixedInputModel(emb_szs=emb_szs, \n                    n_cont=n_cont, \n                    emb_drop=0.04, \n                    out_sz=1, \n                    szs=[64, 128, 256, 512, 256, 128, 8], \n                    drops=[0.1, 0.1, 0.1, 0.3, 0.1, 0.01, 0.0001])\n\noptimizer = optim.Adam(network.parameters(), lr=1e-2)\n\ntotal_loss = []\n\nfor i in range(101):\n    # loss\n    loss = F.mse_loss(network(cat, cont), y)\n    total_loss.append(loss)\n    if (i%10 == 0):\n        print(\"Step\", i,\" loss:\", loss.item())\n\n    optimizer.zero_grad()\n    \n    # backprop\n    loss.backward()  # update gradients\n    optimizer.step() # update weights using gradients to minimize loss","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:00:24.353769Z","iopub.status.idle":"2022-02-24T13:00:24.354277Z","shell.execute_reply.started":"2022-02-24T13:00:24.354066Z","shell.execute_reply":"2022-02-24T13:00:24.354088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(total_loss)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:00:24.355311Z","iopub.status.idle":"2022-02-24T13:00:24.355894Z","shell.execute_reply.started":"2022-02-24T13:00:24.35564Z","shell.execute_reply":"2022-02-24T13:00:24.355664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Looks like working well!","metadata":{}},{"cell_type":"markdown","source":"# Fitting loop","metadata":{}},{"cell_type":"code","source":"def fit(model, train_dl, val_dl, loss_fn, opt, epochs = 3):\n    num_batch = len(train_dl)\n    for epoch in tnrange(epochs):   \n        \n        model.train()\n        y_true_train = list()\n        y_pred_train = list()\n        total_loss_train = 0          \n        \n        t = tqdm_notebook(iter(train_dl), leave=False, total = num_batch)\n        \n        for cat, cont, y in t:\n            cat = cat.cuda()\n            cont = cont.cuda()\n            y = y.cuda()\n            \n            t.set_description(f'Epoch {epoch}')\n            \n            opt.zero_grad()\n            pred = model(cat, cont)\n            loss = loss_fn(pred, y)\n            loss.backward()\n            lr[epoch].append(opt.param_groups[0]['lr'])\n            tloss[epoch].append(loss.item())\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), 4.0) # gradient clipping\n            \n            opt.step()\n            \n            \n            t.set_postfix(loss=loss.item())\n            \n            y_true_train += list(y.cpu().data.numpy())\n            y_pred_train += list(pred.cpu().data.numpy())\n            total_loss_train += loss.item()\n            \n        train_loss = total_loss_train/len(train_dl)\n        \n        if val_dl:\n            model.eval()\n            y_true_val = list()\n            y_pred_val = list()\n            total_loss_val = 0\n            \n            for cat, cont, y in tqdm_notebook(val_dl, leave=False):\n                cat = cat.cuda()\n                cont = cont.cuda()\n                y = y.cuda()\n                \n                pred = model(cat, cont)\n                loss = loss_fn(pred, y)\n                \n                y_true_val += list(y.cpu().data.numpy())\n                y_pred_val += list(pred.cpu().data.numpy())\n                total_loss_val += loss.item()\n            \n                vloss[epoch].append(loss.item())\n                \n            valloss = total_loss_val/len(valdl)\n    \n            print(f'Epoch {epoch}: train_loss: {train_loss:.4f}  | val_loss: {valloss:.4f} ')\n        else:\n            print(f'Epoch {epoch}: train_loss: {train_loss:.4f} ')\n    \n    return lr, tloss, vloss","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:03:22.072317Z","iopub.execute_input":"2022-02-24T13:03:22.073192Z","iopub.status.idle":"2022-02-24T13:03:22.091794Z","shell.execute_reply.started":"2022-02-24T13:03:22.073145Z","shell.execute_reply":"2022-02-24T13:03:22.089421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loop through training\n- I would love to implement **Callbacks** in my later models. Feel free to add in the comments any tips you have!","metadata":{}},{"cell_type":"code","source":"opt = optim.Adam(m.parameters(), 1e-6)\nnum_epochs = 8\n\nlr = defaultdict(list)\ntloss = defaultdict(list)\nvloss = defaultdict(list)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:03:29.829953Z","iopub.execute_input":"2022-02-24T13:03:29.830537Z","iopub.status.idle":"2022-02-24T13:03:29.835569Z","shell.execute_reply.started":"2022-02-24T13:03:29.830491Z","shell.execute_reply":"2022-02-24T13:03:29.834784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr, tloss, vloss = fit(model=m, train_dl=traindl, val_dl=valdl, loss_fn=F.mse_loss, opt=opt, epochs=num_epochs)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:03:34.516001Z","iopub.execute_input":"2022-02-24T13:03:34.516581Z","iopub.status.idle":"2022-02-24T13:04:33.352997Z","shell.execute_reply.started":"2022-02-24T13:03:34.51654Z","shell.execute_reply":"2022-02-24T13:04:33.352107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the results","metadata":{}},{"cell_type":"code","source":"t = [np.mean(tloss[el]) for el in tloss]\nv = [np.mean(vloss[el]) for el in vloss]\np = pd.DataFrame({'Train Loss': t, 'Validation Loss': v, 'Epochs': range(num_epochs)})\n\n_ = p.plot(x='Epochs', y=['Train Loss', 'Validation Loss'], \n           title='Train and Validation Loss over Epochs')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T16:55:17.62865Z","iopub.execute_input":"2022-02-22T16:55:17.62893Z","iopub.status.idle":"2022-02-22T16:55:17.890987Z","shell.execute_reply.started":"2022-02-22T16:55:17.628895Z","shell.execute_reply":"2022-02-22T16:55:17.890313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lr","metadata":{"execution":{"iopub.status.busy":"2022-02-22T15:39:16.249466Z","iopub.execute_input":"2022-02-22T15:39:16.249685Z","iopub.status.idle":"2022-02-22T15:39:16.255382Z","shell.execute_reply.started":"2022-02-22T15:39:16.249644Z","shell.execute_reply":"2022-02-22T15:39:16.254588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train some more with lower lr","metadata":{}},{"cell_type":"code","source":"opt = optim.Adam(m.parameters(), 5e-12)\nnum_epochs = 8\n\nlr2 = defaultdict(list)\ntloss2 = defaultdict(list)\nvloss2 = defaultdict(list)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T16:55:32.305613Z","iopub.execute_input":"2022-02-22T16:55:32.305869Z","iopub.status.idle":"2022-02-22T16:55:32.312362Z","shell.execute_reply.started":"2022-02-22T16:55:32.305841Z","shell.execute_reply":"2022-02-22T16:55:32.31041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr2, tloss2, vloss2 = fit(model=m, train_dl=traindl, val_dl=valdl, loss_fn=F.mse_loss, opt=opt, epochs=num_epochs)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T16:55:32.743676Z","iopub.execute_input":"2022-02-22T16:55:32.744234Z","iopub.status.idle":"2022-02-22T16:57:40.364241Z","shell.execute_reply.started":"2022-02-22T16:55:32.744197Z","shell.execute_reply":"2022-02-22T16:57:40.363356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# t2 = [np.mean(tloss2[el]) for el in tloss2]\n# v2 = [np.mean(vloss2[el]) for el in vloss2]\n# p2 = pd.DataFrame({'Train Loss': t2, 'Validation Loss': v2, 'Epochs': range(num_epochs)})\n\n# _ = p2.plot(x='Epochs', y=['Train Loss', 'Validation Loss'], \n#            title='Train and Validation Loss over Epochs')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T16:57:40.366458Z","iopub.execute_input":"2022-02-22T16:57:40.367308Z","iopub.status.idle":"2022-02-22T16:57:40.62302Z","shell.execute_reply.started":"2022-02-22T16:57:40.367247Z","shell.execute_reply":"2022-02-22T16:57:40.622318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# opt = optim.Adam(m.parameters(), 5e-12)\n# num_epochs = 5\n\n# lr3 = defaultdict(list)\n# tloss3 = defaultdict(list)\n# vloss3 = defaultdict(list)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T16:57:54.512683Z","iopub.execute_input":"2022-02-22T16:57:54.51349Z","iopub.status.idle":"2022-02-22T16:57:54.519761Z","shell.execute_reply.started":"2022-02-22T16:57:54.513443Z","shell.execute_reply":"2022-02-22T16:57:54.518861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lr3, tloss3, vloss3 = fit(model=m, train_dl=traindl, val_dl=valdl, loss_fn=F.mse_loss, opt=opt, epochs=num_epochs)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T16:57:54.969885Z","iopub.execute_input":"2022-02-22T16:57:54.970148Z","iopub.status.idle":"2022-02-22T17:00:03.724023Z","shell.execute_reply.started":"2022-02-22T16:57:54.970116Z","shell.execute_reply":"2022-02-22T17:00:03.723121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# t3 = [np.mean(tloss3[el]) for el in tloss3]\n# v3 = [np.mean(vloss3[el]) for el in vloss3]\n# p3 = pd.DataFrame({'Train Loss': t3, 'Validation Loss': v3, 'Epochs': range(num_epochs)})\n\n# _ = p.plot(x='Epochs', y=['Train Loss', 'Validation Loss'], \n#            title='Train and Validation Loss over Epochs')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:00:08.39675Z","iopub.execute_input":"2022-02-22T17:00:08.397377Z","iopub.status.idle":"2022-02-22T17:00:08.648871Z","shell.execute_reply.started":"2022-02-22T17:00:08.397338Z","shell.execute_reply":"2022-02-22T17:00:08.648162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save the model","metadata":{}},{"cell_type":"code","source":"torch.save(m.state_dict(), 'trained_model.pth')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:00:20.772097Z","iopub.execute_input":"2022-02-22T17:00:20.772374Z","iopub.status.idle":"2022-02-22T17:00:20.792193Z","shell.execute_reply.started":"2022-02-22T17:00:20.772344Z","shell.execute_reply":"2022-02-22T17:00:20.791541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make submissions","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache() # just to clear some GPU cache memory\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:00:25.862964Z","iopub.execute_input":"2022-02-22T17:00:25.863773Z","iopub.status.idle":"2022-02-22T17:00:26.018688Z","shell.execute_reply.started":"2022-02-22T17:00:25.86372Z","shell.execute_reply":"2022-02-22T17:00:26.017954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_order = ['investment_id' , 'time_id'] + features","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:02:24.178258Z","iopub.execute_input":"2022-02-24T13:02:24.179086Z","iopub.status.idle":"2022-02-24T13:02:24.183572Z","shell.execute_reply.started":"2022-02-24T13:02:24.179044Z","shell.execute_reply":"2022-02-24T13:02:24.182653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_for_test_data(test_data):\n    catf, numf = split_features(test_df)\n    testds = RegressionColumnarDataset(test_df, catf, pd.Series([1 for i in range(test_df.shape[0])])) # using 1 as y value just for putting something\n    testdl = data.DataLoader(testds, batch_size = 1, shuffle = False, num_workers = 2, pin_memory = True)\n    sub = []\n    \n    for cat_test, cont_test, _ in testdl:\n        cat_test = cat_test.cuda()\n        cont_test = cont_test.cuda()\n        pred = m(cat_test, cont_test)\n        print(pred)\n        sub.append(pred)\n    \n        \n    submission_values = [float(i.detach()) for i in sub]\n    return submission_values","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:02:25.131521Z","iopub.execute_input":"2022-02-24T13:02:25.131795Z","iopub.status.idle":"2022-02-24T13:02:25.142028Z","shell.execute_reply.started":"2022-02-24T13:02:25.131763Z","shell.execute_reply":"2022-02-24T13:02:25.141151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use the submission API to make predictions","metadata":{}},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\n\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    print(\"test_df as loaded by the API\")\n    display(test_df.head(), test_df.shape)\n    \n    # here you need to modify test_df to match the training data\n    test_df['time_id'] = test_df.row_id.str.split(\"_\", expand=True)[0].astype(\"int16\") #re-create time_id\n    test_df = test_df[cols_order]  \n    print(\"test_df after selecting/creating the features the model was trained with\")\n    display(test_df.head(), test_df.shape)\n    \n    # Call our function to make predictions\n    predictions = predict_for_test_data(test_df)\n    sample_prediction_df['target'] = predictions  # make your predictions here\n    env.predict(sample_prediction_df)   # register your predictions\n    print('submission made for this data')\n    display(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T13:02:33.495935Z","iopub.execute_input":"2022-02-24T13:02:33.496728Z","iopub.status.idle":"2022-02-24T13:02:34.281254Z","shell.execute_reply.started":"2022-02-24T13:02:33.496688Z","shell.execute_reply":"2022-02-24T13:02:34.280143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Upvote if useful!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}