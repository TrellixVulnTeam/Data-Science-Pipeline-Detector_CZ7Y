{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import libs\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport gc\nimport optuna\n\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-19T14:13:20.756932Z","iopub.execute_input":"2022-01-19T14:13:20.757798Z","iopub.status.idle":"2022-01-19T14:13:20.762386Z","shell.execute_reply.started":"2022-01-19T14:13:20.757757Z","shell.execute_reply":"2022-01-19T14:13:20.761624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This notebook is a modified version of the following one:\nhttps://www.kaggle.com/yosukeyama/ubiquant-simple-lgbm-train-infer","metadata":{}},{"cell_type":"code","source":"# reduce cols for use to save memory capacity\nbasic_cols = ['row_id', 'time_id', 'investment_id', 'target']\nnum_feat = 5\nfeatures = [f'f_{i}' for i in range(num_feat)]\ncols = basic_cols + features\n\n# load data\ntrain_df = pd.read_csv('../input/ubiquant-market-prediction/train.csv', usecols=cols)\ndisplay(train_df)\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T14:13:20.764154Z","iopub.execute_input":"2022-01-19T14:13:20.764854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I only use 5 features here as an example. If you want to achieve a better score then perhaps you should use more","metadata":{}},{"cell_type":"code","source":"print(train_df.info())\nprint('')\nprint(train_df.describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split train data\ninvestment_ids = train_df['investment_id'].unique()\nnum_ids = len(investment_ids)\ntr_rate = 0.8\n\ntr_ids = investment_ids[:int(num_ids*tr_rate)]\nval_ids = investment_ids[int(num_ids*tr_rate):]\nprint('train: ', len(tr_ids), )\nprint('val: ', len(val_ids),)\n\ntrain = train_df[train_df['investment_id'].isin(tr_ids)]\nvalid = train_df[train_df['investment_id'].isin(val_ids)]\n\ndisplay(train)\ndisplay(valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare for training\ntr_y = train['target'].values\ntr_x = train[features].values\nval_y = valid['target'].values\nval_x = valid[features].values\nlgb_train = lgb.Dataset(tr_x, tr_y)\nlgb_eval = lgb.Dataset(val_x, val_y)\n\ndel train_df, train, valid\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can add more parameters, those are given as an example","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    \n    train_x, test_x, train_y, test_y = tr_x, val_x, tr_y, val_y\n    param = {\n        'metric': 'rmse', \n        'random_state': 2022,\n        'n_estimators': 500,\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 10.0),\n        'max_depth': trial.suggest_int(\"max_depth\", 2, 30),\n        'num_leaves' : trial.suggest_int('num_leaves', 2, 1000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 500),\n    }\n    model = LGBMRegressor(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=10,verbose=-1)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Increase the number of trials (***n_trials***) if you have some time *to waste*","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=3)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training with lgbm\nparams=study.best_params   \nparams['random_state'] = 2022\nparams['n_estimators'] = 1000\nparams['metric'] = 'rmse'\n\nmodel = LGBMRegressor(**params)\n\ntrain_x, test_x, train_y, test_y = tr_x, val_x, tr_y, val_y\n\nmodel.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inference\nimport ubiquant\nenv = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\nfor (test_df, sample_prediction_df) in iter_test:\n    preds = model.predict(test_df[features].values)\n    sample_prediction_df['target'] = preds  # make your predictions here\n    env.predict(sample_prediction_df)   # register your predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}