{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nimport time\n\nimport joblib\n\nfrom sklearn.preprocessing import StandardScaler    # RobustScaler\nfrom sklearn.model_selection import KFold    # GroupKFold\n\n# Warningの無効化\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n# データフレームcolumの全表示\npd.set_option(\"display.max_columns\", None)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-24T00:28:32.48312Z","iopub.execute_input":"2022-01-24T00:28:32.483465Z","iopub.status.idle":"2022-01-24T00:28:35.334796Z","shell.execute_reply.started":"2022-01-24T00:28:32.483392Z","shell.execute_reply":"2022-01-24T00:28:35.334101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            #else:\n            elif str(col_type)[:5] == \"float\":\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype(\"category\")\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_data_strict(file_name=\"/kaggle/input/ump-train-picklefile/train.pkl\"):\n    df = pd.read_pickle(file_name).pipe(reduce_mem_usage)\n    assert df.isnull().any().sum() == 0, \"null exists.\"\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train = pd.read_pickle(\"../input/ump-train-picklefile/train.pkl\")\ndf_train = read_data_strict()\ndf_train","metadata":{"execution":{"iopub.status.busy":"2022-01-21T11:30:25.081673Z","iopub.execute_input":"2022-01-21T11:30:25.08214Z","iopub.status.idle":"2022-01-21T11:31:03.249933Z","shell.execute_reply.started":"2022-01-21T11:30:25.082103Z","shell.execute_reply":"2022-01-21T11:31:03.249172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T11:31:03.250892Z","iopub.execute_input":"2022-01-21T11:31:03.251976Z","iopub.status.idle":"2022-01-21T11:31:03.290988Z","shell.execute_reply.started":"2022-01-21T11:31:03.251937Z","shell.execute_reply":"2022-01-21T11:31:03.290168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T11:31:03.293434Z","iopub.execute_input":"2022-01-21T11:31:03.293763Z","iopub.status.idle":"2022-01-21T11:31:32.980016Z","shell.execute_reply.started":"2022-01-21T11:31:03.293719Z","shell.execute_reply":"2022-01-21T11:31:32.979148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"keys = [\"time_id\", \"investment_id\"]\nfeatures = list(df_train.filter(like=\"f_\").columns)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T11:31:32.990995Z","iopub.execute_input":"2022-01-21T11:31:32.991242Z","iopub.status.idle":"2022-01-21T11:31:32.995984Z","shell.execute_reply.started":"2022-01-21T11:31:32.991201Z","shell.execute_reply":"2022-01-21T11:31:32.995369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# データのスケーリング\nscaler = StandardScaler()    # RobustScaler()\ndf_train[features] = scaler.fit_transform(df_train[features])","metadata":{"execution":{"iopub.status.busy":"2022-01-21T11:31:32.998725Z","iopub.execute_input":"2022-01-21T11:31:32.999268Z","iopub.status.idle":"2022-01-21T11:31:35.765906Z","shell.execute_reply.started":"2022-01-21T11:31:32.99921Z","shell.execute_reply":"2022-01-21T11:31:35.765095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joblib.dump(scaler, \"scaler.joblib\")","metadata":{"execution":{"iopub.status.busy":"2022-01-21T11:31:35.77248Z","iopub.execute_input":"2022-01-21T11:31:35.772737Z","iopub.status.idle":"2022-01-21T11:31:36.023953Z","shell.execute_reply.started":"2022-01-21T11:31:35.772702Z","shell.execute_reply":"2022-01-21T11:31:36.023179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x = df_train[keys + features] #.values\ntrain_y = df_train[\"target\"] #.values","metadata":{"execution":{"iopub.status.busy":"2022-01-21T11:31:36.025443Z","iopub.execute_input":"2022-01-21T11:31:36.025743Z","iopub.status.idle":"2022-01-21T11:31:36.303503Z","shell.execute_reply.started":"2022-01-21T11:31:36.025708Z","shell.execute_reply":"2022-01-21T11:31:36.302526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# time_id列のtime_idを単位として分割することにする\ntime_id = train_x[\"time_id\"]\nunique_time_ids = time_id.unique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_folds = 5","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tensorflowの警告抑制\nimport os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\nimport tensorflow as tf\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T11:31:55.190012Z","iopub.execute_input":"2022-01-21T11:31:55.190339Z","iopub.status.idle":"2022-01-21T11:31:59.051482Z","shell.execute_reply.started":"2022-01-21T11:31:55.190302Z","shell.execute_reply":"2022-01-21T11:31:59.050735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -----------------------------------\n# ニューラルネットの実装\n# -----------------------------------\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.metrics import log_loss","metadata":{"execution":{"iopub.status.busy":"2022-01-21T11:31:59.0531Z","iopub.execute_input":"2022-01-21T11:31:59.053832Z","iopub.status.idle":"2022-01-21T11:31:59.910112Z","shell.execute_reply.started":"2022-01-21T11:31:59.053793Z","shell.execute_reply":"2022-01-21T11:31:59.909334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -----------------------------------\n# アーリーストッピング\n# -----------------------------------\nfrom keras.callbacks import EarlyStopping\n\n# 学習の実行\n# バリデーションデータもモデルに渡し、学習の進行とともにスコアがどう変わるかモニタリングする\nbatch_size = 1024\nepochs = 50","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x[\"preds\"] = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KFoldクラスを用いて、time_id単位で分割する\nkf = KFold(n_splits = n_folds, shuffle = False, random_state = 71)\nfor fold, (tr_group_idx, va_group_idx) in enumerate(kf.split(time_id)):\n    # time_idをtrain/valid（学習に使うデータ、バリデーションデータ）に分割する\n    tr_x, tr_y = train_x.iloc[tr_group_idx], train_y.iloc[tr_group_idx]\n    va_x, va_y = train_x.iloc[va_group_idx], train_y.iloc[va_group_idx]\n    # 各レコードのtime_idがtrain/validのどちらに属しているかによって分割する\n\n    # ニューラルネットモデルの構築\n    model = Sequential()\n    model.add(Dense(256, activation=\"relu\", input_shape=(tr_x.shape[1],)))\n    model.add(Dropout(0.2))\n    model.add(Dense(256, activation=\"relu\"))\n    model.add(Dropout(0.2))\n    model.add(Dense(1, activation=\"sigmoid\"))\n\n    model.compile(loss=\"mean_squared_error\",   # \"mean_squared_logarithmic_error\",\n                  optimizer=\"adam\", metrics=[\"accuracy\"])\n\n    # アーリーストッピングの観察するroundを20とする\n    # restore_best_weightsを設定することで、最適なエポックでのモデルを使用する\n    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True)\n\n    model.fit(tr_x, tr_y,\n            batch_size=batch_size, epochs=epochs,\n            verbose=1, validation_data=(va_x, va_y), callbacks=[early_stopping])\n\n    joblib.dump(model, f\"catb_{fold}.pkl\")\n    \n    # 予測\n    va_pred = model.predict(va_x[features])\n    train_x.loc[va_group_idx, \"preds\"] = va_pred","metadata":{"execution":{"iopub.status.busy":"2022-01-21T11:31:59.914394Z","iopub.execute_input":"2022-01-21T11:31:59.915135Z","iopub.status.idle":"2022-01-21T11:32:00.337171Z","shell.execute_reply.started":"2022-01-21T11:31:59.915093Z","shell.execute_reply":"2022-01-21T11:32:00.335318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict & submit","metadata":{}},{"cell_type":"code","source":"import ubiquant","metadata":{"execution":{"iopub.status.busy":"2022-01-21T11:35:01.936041Z","iopub.execute_input":"2022-01-21T11:35:01.936458Z","iopub.status.idle":"2022-01-21T11:35:01.972544Z","shell.execute_reply.started":"2022-01-21T11:35:01.936417Z","shell.execute_reply":"2022-01-21T11:35:01.971861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = ubiquant.make_env()                   # initialize the environment\niter_test = env.iter_test()                 # an iterator which loops over the test set and sample submission","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = joblib.load(\"scaler.joblib\")\nmodels = [joblib.load(f\"catb_{fold}.pkl\") for fold in range(n_folds)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (test_df, sample_prediction_df) in iter_test:\n    test_df[features] = scaler.fit_transform(test_df[features]) \n    final_pred = [models[fold].predict(test_df[features]) for fold in range(n_folds)]\n    sample_prediction_df[\"target\"] = np.mean(np.stack(final_pred), axis = 0)\n    env.predict(sample_prediction_df) ","metadata":{"execution":{"iopub.status.busy":"2022-01-21T11:35:01.973915Z","iopub.execute_input":"2022-01-21T11:35:01.974177Z","iopub.status.idle":"2022-01-21T11:35:27.540876Z","shell.execute_reply.started":"2022-01-21T11:35:01.97414Z","shell.execute_reply":"2022-01-21T11:35:27.540155Z"},"trusted":true},"execution_count":null,"outputs":[]}]}