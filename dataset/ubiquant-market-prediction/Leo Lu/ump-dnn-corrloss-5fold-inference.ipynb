{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-27T11:32:07.333827Z","iopub.execute_input":"2022-01-27T11:32:07.334317Z","iopub.status.idle":"2022-01-27T11:32:07.342677Z","shell.execute_reply.started":"2022-01-27T11:32:07.334228Z","shell.execute_reply":"2022-01-27T11:32:07.341239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\nfeature_columns = ['investment_id', 'time_id'] + features\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:32:07.345032Z","iopub.execute_input":"2022-01-27T11:32:07.345397Z","iopub.status.idle":"2022-01-27T11:32:15.708313Z","shell.execute_reply.started":"2022-01-27T11:32:07.345352Z","shell.execute_reply":"2022-01-27T11:32:15.707365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment_id = train.pop(\"investment_id\")\ninvestment_id.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:32:15.709772Z","iopub.execute_input":"2022-01-27T11:32:15.710108Z","iopub.status.idle":"2022-01-27T11:32:15.730704Z","shell.execute_reply.started":"2022-01-27T11:32:15.710074Z","shell.execute_reply":"2022-01-27T11:32:15.72985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = train.pop(\"time_id\")","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:32:15.732712Z","iopub.execute_input":"2022-01-27T11:32:15.732967Z","iopub.status.idle":"2022-01-27T11:32:15.748423Z","shell.execute_reply.started":"2022-01-27T11:32:15.732929Z","shell.execute_reply":"2022-01-27T11:32:15.747454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train.pop(\"target\")\ny.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:32:15.749679Z","iopub.execute_input":"2022-01-27T11:32:15.749909Z","iopub.status.idle":"2022-01-27T11:32:15.766Z","shell.execute_reply.started":"2022-01-27T11:32:15.749882Z","shell.execute_reply":"2022-01-27T11:32:15.765086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ninvestment_ids = list(investment_id.unique())\ninvestment_id_size = len(investment_ids) + 1\ninvestment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\nwith tf.device(\"cpu\"):\n    investment_id_lookup_layer.adapt(investment_id)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:32:15.767186Z","iopub.execute_input":"2022-01-27T11:32:15.767477Z","iopub.status.idle":"2022-01-27T11:33:00.687733Z","shell.execute_reply.started":"2022-01-27T11:32:15.767443Z","shell.execute_reply":"2022-01-27T11:33:00.686846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ndef preprocess(X, y):\n    print(X)\n    print(y)\n    return X, y\ndef make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(1024)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:33:00.689047Z","iopub.execute_input":"2022-01-27T11:33:00.689386Z","iopub.status.idle":"2022-01-27T11:33:00.697585Z","shell.execute_reply.started":"2022-01-27T11:33:00.689352Z","shell.execute_reply":"2022-01-27T11:33:00.696933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.python.keras import backend as K\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\n\ndef t(a):\n  \"\"\"For testing: generate a float64 tensor from anything.\"\"\"\n  return tf.constant(a, dtype=tf.float64)\n\ndef tmean(x, axis=-1):\n  \"\"\"Arithmetic mean of a tensor over some axis, default last.\"\"\"\n  x = tf.convert_to_tensor(x)\n  sum = tf.reduce_sum(x, axis=axis)\n  n = tf.cast(tf.shape(x)[axis], x.dtype)\n  return sum / n\n\ndef correlationMetric(x, y, axis=-2):\n  \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n  x = tf.convert_to_tensor(x)\n  y = math_ops.cast(y, x.dtype)\n  n = tf.cast(tf.shape(x)[axis], x.dtype)\n  xsum = tf.reduce_sum(x, axis=axis)\n  ysum = tf.reduce_sum(y, axis=axis)\n  xmean = xsum / n\n  ymean = ysum / n\n  xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n  yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n  cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n  corr = cov / tf.sqrt(xvar * yvar)\n  return tf.constant(1.0, dtype=x.dtype) - corr\n\n\ndef correlationLoss(x,y, axis=-2):\n  \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n  while trying to have the same mean and variance\"\"\"\n  x = tf.convert_to_tensor(x)\n  y = math_ops.cast(y, x.dtype)\n  n = tf.cast(tf.shape(x)[axis], x.dtype)\n  xsum = tf.reduce_sum(x, axis=axis)\n  ysum = tf.reduce_sum(y, axis=axis)\n  xmean = xsum / n\n  ymean = ysum / n\n  xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n  ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n  cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n  corr = cov / tf.sqrt(xsqsum * ysqsum)\n  # absdif = tmean(tf.abs(x - y), axis=axis) / tf.sqrt(yvar)\n  sqdif = tf.reduce_sum(tf.math.squared_difference(x, y), axis=axis) / n / tf.sqrt(ysqsum / n)\n  # meandif = tf.abs(xmean - ymean) / tf.abs(ymean)\n  # vardif = tf.abs(xvar - yvar) / yvar\n  # return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr + (meandif * 0.01) + (vardif * 0.01)) , dtype=tf.float32 )\n  return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr + (0.01 * sqdif)) , dtype=tf.float32 )\n","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:33:00.698968Z","iopub.execute_input":"2022-01-27T11:33:00.69922Z","iopub.status.idle":"2022-01-27T11:33:00.72345Z","shell.execute_reply.started":"2022-01-27T11:33:00.699191Z","shell.execute_reply":"2022-01-27T11:33:00.722618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dnn_model():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(32, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(128, activation='swish')(feature_x)\n    feature_x = layers.Dense(64, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(256, activation='swish', kernel_regularizer=\"l2\")(x)\n    # x = layers.Dense(256, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss=correlationLoss, metrics=['mse', \"mae\", rmse, correlationMetric])\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:33:00.724619Z","iopub.execute_input":"2022-01-27T11:33:00.724816Z","iopub.status.idle":"2022-01-27T11:33:00.741603Z","shell.execute_reply.started":"2022-01-27T11:33:00.72479Z","shell.execute_reply":"2022-01-27T11:33:00.740761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:33:00.744025Z","iopub.execute_input":"2022-01-27T11:33:00.744607Z","iopub.status.idle":"2022-01-27T11:33:00.758227Z","shell.execute_reply.started":"2022-01-27T11:33:00.744559Z","shell.execute_reply":"2022-01-27T11:33:00.757558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lstm_model():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(32, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(128, activation='swish')(feature_x)\n    feature_x = layers.Dense(64, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(256, activation='swish', kernel_regularizer=\"l2\")(x)\n    # x = layers.Dense(256, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    # x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n\n\n    x = layers.BatchNormalization(name='batch_norm1')(x)\n    x = layers.Dense(256, activation='swish', name='dense1')(x)\n    x = layers.Dropout(0.1, name='dropout1')(x)\n    x = layers.Reshape((1, -1), name='reshape1')(x)\n    x = layers.BatchNormalization(name='batch_norm2')(x)\n    x = layers.LSTM(128, return_sequences=True, activation='relu', name='lstm1')(x)\n    x = layers.LSTM(16, return_sequences=False, activation='relu', name='lstm2')(x)\n\n\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss=correlationLoss, metrics=['mse', \"mae\", rmse, correlationMetric])\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:33:00.759336Z","iopub.execute_input":"2022-01-27T11:33:00.76001Z","iopub.status.idle":"2022-01-27T11:33:00.776104Z","shell.execute_reply.started":"2022-01-27T11:33:00.759974Z","shell.execute_reply":"2022-01-27T11:33:00.775451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = []\nfor fold in range(0,5):\n    model = dnn_model()\n    model.summary()\n    model.load_weights(f'../input/ump-dnn-corrloss-5fold-seed-1013/dnn_corrloss_{fold}_1013.tf')\n    models.append(model)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:33:00.77735Z","iopub.execute_input":"2022-01-27T11:33:00.777695Z","iopub.status.idle":"2022-01-27T11:33:02.13626Z","shell.execute_reply.started":"2022-01-27T11:33:00.777665Z","shell.execute_reply":"2022-01-27T11:33:02.135547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(0,5):\n    model = lstm_model()\n    model.summary()\n    model.load_weights(f'../input/ump-lstm-model-fold5-seed-1013/dnn_corrloss_{fold}_1013.tf')\n    models.append(model)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:33:02.137679Z","iopub.execute_input":"2022-01-27T11:33:02.137927Z","iopub.status.idle":"2022-01-27T11:33:04.551685Z","shell.execute_reply.started":"2022-01-27T11:33:02.137896Z","shell.execute_reply":"2022-01-27T11:33:04.550804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(models)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:33:04.552843Z","iopub.execute_input":"2022-01-27T11:33:04.553518Z","iopub.status.idle":"2022-01-27T11:33:04.559892Z","shell.execute_reply.started":"2022-01-27T11:33:04.553483Z","shell.execute_reply":"2022-01-27T11:33:04.559003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfor (test_df, sample_prediction_df) in iter_test:\n    ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n    sample_prediction_df['target'] = inference(models, ds)\n    env.predict(sample_prediction_df) ","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:33:04.56103Z","iopub.execute_input":"2022-01-27T11:33:04.561256Z","iopub.status.idle":"2022-01-27T11:33:04.925502Z","shell.execute_reply.started":"2022-01-27T11:33:04.561218Z","shell.execute_reply":"2022-01-27T11:33:04.924365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_prediction_df","metadata":{"execution":{"iopub.status.busy":"2022-01-27T11:33:04.926483Z","iopub.status.idle":"2022-01-27T11:33:04.926955Z","shell.execute_reply.started":"2022-01-27T11:33:04.926694Z","shell.execute_reply":"2022-01-27T11:33:04.926728Z"},"trusted":true},"execution_count":null,"outputs":[]}]}