{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2022-01-26T09:03:52.031407Z","iopub.execute_input":"2022-01-26T09:03:52.031861Z","iopub.status.idle":"2022-01-26T09:04:20.673997Z","shell.execute_reply.started":"2022-01-26T09:03:52.03175Z","shell.execute_reply":"2022-01-26T09:04:20.672991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy.matlib\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom joblib import Parallel, delayed\n\nimport shutil\nimport glob\nimport gc \nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import KFold\n\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\nimport torch\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\nfrom scipy.stats import pearsonr as p\nimport psutil\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-01-26T09:04:20.676462Z","iopub.execute_input":"2022-01-26T09:04:20.676813Z","iopub.status.idle":"2022-01-26T09:04:23.218597Z","shell.execute_reply.started":"2022-01-26T09:04:20.676771Z","shell.execute_reply":"2022-01-26T09:04:23.217648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain = pd.read_pickle('../input/ump-train-picklefile/train.pkl')\ntrain = train[train.time_id>1000]","metadata":{"execution":{"iopub.status.busy":"2022-01-26T09:04:23.21991Z","iopub.execute_input":"2022-01-26T09:04:23.220137Z","iopub.status.idle":"2022-01-26T09:05:03.032658Z","shell.execute_reply.started":"2022-01-26T09:04:23.220111Z","shell.execute_reply":"2022-01-26T09:05:03.031697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/ubiquant-market-prediction/example_test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-26T09:05:03.033857Z","iopub.execute_input":"2022-01-26T09:05:03.034085Z","iopub.status.idle":"2022-01-26T09:05:03.057099Z","shell.execute_reply.started":"2022-01-26T09:05:03.034059Z","shell.execute_reply":"2022-01-26T09:05:03.05617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\nX_test=test.copy()\nX_test.drop(['time_id','row_id'], axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T09:05:03.059327Z","iopub.execute_input":"2022-01-26T09:05:03.059649Z","iopub.status.idle":"2022-01-26T09:05:03.315622Z","shell.execute_reply.started":"2022-01-26T09:05:03.059606Z","shell.execute_reply":"2022-01-26T09:05:03.314661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nunique = X.nunique()\ntypes = X.dtypes\n\ncategorical_columns = 'investment_id'\ncategorical_dims =  {}\n\n# for col in X.columns:\n#     if  col == 'investment_id':\n#         l_enc = LabelEncoder()\n#         X[col] = l_enc.fit_transform(X[col].values)\n#         X_test[col] = l_enc.transform(X_test[col].values)\n#         categorical_columns.append(col)\n#         categorical_dims[col] = len(l_enc.classes_)\n#     else:\n#         scaler = StandardScaler()\n#         X[col] = scaler.fit_transform(X[col].values.reshape(-1, 1))\n#         X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))\n        \nl_enc = LabelEncoder()\nX[categorical_columns] = l_enc.fit_transform(X[categorical_columns].values)\nX_test[categorical_columns] = l_enc.transform(X_test[categorical_columns].values)\n# categorical_columns.append(categorical_columns)\ncategorical_dims[categorical_columns] = len(l_enc.classes_)\n\ncat_idxs = [ i for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]\n\ncat_dims = [ categorical_dims[f] for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]","metadata":{"execution":{"iopub.status.busy":"2022-01-26T09:05:03.318923Z","iopub.execute_input":"2022-01-26T09:05:03.319153Z","iopub.status.idle":"2022-01-26T09:05:12.7137Z","shell.execute_reply.started":"2022-01-26T09:05:03.319127Z","shell.execute_reply":"2022-01-26T09:05:12.713077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tabnet_params = dict(\n    cat_idxs=cat_idxs,\n    cat_dims=cat_dims,\n    cat_emb_dim=1,\n    n_d = 16,\n    n_a = 16,\n    n_steps = 2,\n    gamma = 2,\n    n_independent = 2,\n    n_shared = 2,\n    lambda_sparse = 0,\n    optimizer_fn = Adam,\n    optimizer_params = dict(lr = (2e-2)),\n    mask_type = \"entmax\",\n    scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n    scheduler_fn = CosineAnnealingWarmRestarts,\n    seed = 42,\n    verbose = 10\n    \n)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T09:05:12.715024Z","iopub.execute_input":"2022-01-26T09:05:12.715261Z","iopub.status.idle":"2022-01-26T09:05:12.720554Z","shell.execute_reply.started":"2022-01-26T09:05:12.715232Z","shell.execute_reply":"2022-01-26T09:05:12.71949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmse(y_true, y_pred):\n    # Function to calculate the root mean squared percentage error\n    return np.sqrt(np.mean(np.square((y_true - y_pred))))\n\nclass RMSE(Metric):\n    def __init__(self):\n        self._name = \"rmse\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_score):\n        \n        return np.sqrt(np.mean(np.square((y_true - y_score))))\n    \n\n\ndef RMSELoss(y_pred, y_true):\n    return torch.sqrt(torch.mean( ((y_true - y_pred)) ** 2 )).clone()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T09:05:12.721652Z","iopub.execute_input":"2022-01-26T09:05:12.721872Z","iopub.status.idle":"2022-01-26T09:05:12.734568Z","shell.execute_reply.started":"2022-01-26T09:05:12.721846Z","shell.execute_reply":"2022-01-26T09:05:12.733685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = KFold(n_splits = 5, random_state = 42, shuffle = True)\n# Create out of folds array\noof_predictions = np.zeros((X.shape[0], 1))\ntest_predictions = np.zeros(X_test.shape[0])\nfeature_importances = pd.DataFrame()\nfeature_importances[\"feature\"] = X.columns.tolist()\nstats = pd.DataFrame()\nexplain_matrices = []\nmasks_ =[]\nmodels = []\nfor fold, (trn_ind, val_ind) in enumerate(kfold.split(X)):\n    print(f'Training fold {fold + 1}')\n    X_train, X_val = X.iloc[trn_ind].values, X.iloc[val_ind].values\n    y_train, y_val = y.iloc[trn_ind].values.reshape(-1,1), y.iloc[val_ind].values.reshape(-1,1)\n\n\n    clf =  TabNetRegressor(**tabnet_params)\n    clf.fit(\n      X_train, y_train,\n      eval_set=[(X_val, y_val)],\n      max_epochs = 100,\n      patience = 10,\n      batch_size = 1024, \n      virtual_batch_size = 128,\n      num_workers = 4,\n      drop_last = False,\n      eval_metric=[RMSE],\n      loss_fn=RMSELoss\n      )\n    \n    saving_path_name = f\"./tabnet_fold{fold}\"\n    saved_filepath = clf.save_model(saving_path_name)\n    \n\n    oof_predictions[val_ind] = clf.predict(X_val)\n    print('>'*20,f'fold_{fold} score: {p(y_val.squeeze(-1),oof_predictions[val_ind].squeeze(-1))[0]}','<'*20)\n    \n    models.append(clf)\n    \n    feature_importances[f\"importance_fold{fold}+1\"] = clf.feature_importances_\n    \n    del clf\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    \nprint(f'OOF score across folds: {p(y, oof_predictions.flatten())[0]}')","metadata":{"execution":{"iopub.status.busy":"2022-01-26T09:05:12.736029Z","iopub.execute_input":"2022-01-26T09:05:12.736303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importances['mean_importance']=feature_importances[['importance_fold0+1','importance_fold1+1']].mean(axis=1)\nfeature_importances.sort_values(by='mean_importance', ascending=False, inplace=True)\nsns.barplot(y=feature_importances['feature'][:25],x=feature_importances['mean_importance'][:25], palette='inferno')\nplt.title('Mean Feature Importance by Folds')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()  \niter_test = env.iter_test()\nfor (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df[\"target\"] = 0.0\n    X_test = test_df.drop([\"row_id\"], axis=1)\n    X_test[categorical_columns] = l_enc.transform(X_test[categorical_columns].values)\n    y_preds = [model.predict(X_test.values).squeeze(-1) for model in models]\n    sample_prediction_df[\"target\"] = sum(y_preds) / len(y_preds)\n    sample_prediction_df[\"target\"] = sample_prediction_df[\"target\"].fillna(0.0)\n    env.predict(sample_prediction_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_prediction_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}