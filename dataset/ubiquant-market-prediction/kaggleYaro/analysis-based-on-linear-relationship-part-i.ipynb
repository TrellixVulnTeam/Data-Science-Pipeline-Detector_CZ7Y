{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n- I am new to Kaggle competition and joined this competition.\n- I have tried my best but could not achieve high score so far.\n- I would like to share what I have done (very basic analysis though...). It would be very helpful if you give me any comments/advices (and upvote if you find this useful)!","metadata":{}},{"cell_type":"markdown","source":"# Contents (List of Quesions I came up)\n\n- Which model should we use? (**Part I**)\n- Should we use all the data for training? If not, how to filter them out? (**Part I**)\n- CV and the baseline score (**Part I**)\n- Should we apply Target preprocessing? If so, how? (**Part II**)\n- Should we apply Feature preprocessing? If so, how? (**Part II**)\n- What are missing?","metadata":{}},{"cell_type":"markdown","source":"# 1. Which model should we use?\n\nFinancial time series data has a couple of characteristics.\n- non-stationarity\n- low signal/noise ratio\n- etc.\n\nWe can easily overfit the training data with common GBDT/NN model (without cares). That is why I started with simple linear regression (with/without regularization) model. We also understand that inter-feature effects can improve the score, so if time allows, I would like to try (Part III?).","metadata":{}},{"cell_type":"markdown","source":"# 2. Should we use all the data for training? If not, how to filter them out?\n\n- Because we use linear model, we would not require massive amount of training samples.\n- At least, we should have enough amount of training samples in each time_id.\n- For each time_id, we could have at most ~3500 entries with different investment_ids.\n- It would be difficult to deal with them asset-by-asset. We ignore info of investment_id in the training.\n- Here we will identify a subset of time_ids with enough number of investment_id belonging.","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport gc\nimport psutil\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Lasso, Ridge, LinearRegression, LassoCV\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nfrom sklearn import metrics\nfrom scipy.stats import pearsonr\nfrom typing import Tuple\n\nmem = psutil.virtual_memory()\nprint(f' Memory Consumption Rate: {mem.percent}')\nprint(f' Memory Consumption: {mem.used/1000/1000/1000}')\nprint(f'Available: {mem.free/1000/1000/1000}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-29T03:55:54.101439Z","iopub.execute_input":"2022-03-29T03:55:54.101907Z","iopub.status.idle":"2022-03-29T03:55:55.46125Z","shell.execute_reply.started":"2022-03-29T03:55:54.101788Z","shell.execute_reply":"2022-03-29T03:55:55.460385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Data\nThanks to https://www.kaggle.com/datasets/robikscube/ubiquant-parquet, we can reduce the memory usage.","metadata":{}},{"cell_type":"code","source":"%%time\ndef reduce_memory_usage(df, features):\n    for feature in features:\n        item = df[feature].astype(np.float16)\n        df[feature] = item\n        del item\n        gc.collect()\n        \ntarget = 'target'\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\nfeature_columns = ['investment_id', 'time_id'] + features\nX = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet', columns=feature_columns + [\"target\"])\nreduce_memory_usage(X, features + [\"target\"])\nprint(X.shape)\nX.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-29T03:55:59.787321Z","iopub.execute_input":"2022-03-29T03:55:59.787747Z","iopub.status.idle":"2022-03-29T04:00:14.03248Z","shell.execute_reply.started":"2022-03-29T03:55:59.787701Z","shell.execute_reply":"2022-03-29T04:00:14.031334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Leave the data with enough number of samples for each time_id and for each investment_id\n- Reorganize the data into fixed size of `[number of time_id, number of investment_id]`\n- Some entries are set to null (if those do not exist in the original data).","metadata":{}},{"cell_type":"code","source":"time_id_list = list(X['time_id'].unique())\ninvetment_id_list = list(X['investment_id'].unique())\ndata_dummy = [np.nan for _ in range(len(time_id_list)*len(invetment_id_list))]\nX_filled = pd.DataFrame(data_dummy, columns=['dummy'])\nX_filled['time_id'] = np.repeat(time_id_list, len(invetment_id_list))\nX_filled['investment_id'] = np.tile(invetment_id_list, len(time_id_list))\nX_filled = X_filled.set_index(['time_id', 'investment_id'])\nX_orig = X.set_index(['time_id','investment_id'])\nX_orig = X_orig[['target']]\nX_filled = X_filled.join(X_orig).drop('dummy', axis=1) # join\nX_filled = X_filled.unstack() # move investment_id to columns\nX_filled = X_filled.T.reset_index(level=0).drop('level_0', axis=1).T # multi columns to single column\nprint(X_filled.shape)\nX_filled.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T04:00:14.034524Z","iopub.execute_input":"2022-03-29T04:00:14.034816Z","iopub.status.idle":"2022-03-29T04:00:31.985628Z","shell.execute_reply.started":"2022-03-29T04:00:14.034781Z","shell.execute_reply":"2022-03-29T04:00:31.984677Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Keep assets (investment_id) with more than 500 samples (time_id)","metadata":{}},{"cell_type":"code","source":"thred = len(X_filled) - 500 # 最低500サンプル（２年分）あれば、通常の学習プロセスで扱う。\n#print(thred)\ndf_null_count = X_filled.isnull().sum(axis=0).sort_values(ascending=False)\ndf_chosen = df_null_count[df_null_count < thred]\ninvestment_id_list = list(df_chosen.index)\nX_filled_chosen = X_filled[investment_id_list]\nprint(\"Number of assets(investment_id) with more than 500 samples(time_id): {}\".format(len(investment_id_list)))","metadata":{"execution":{"iopub.status.busy":"2022-03-29T04:22:10.874361Z","iopub.execute_input":"2022-03-29T04:22:10.87469Z","iopub.status.idle":"2022-03-29T04:22:10.957737Z","shell.execute_reply.started":"2022-03-29T04:22:10.874658Z","shell.execute_reply":"2022-03-29T04:22:10.956628Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove time_ids where:\n- small number of investment_id has the data for and/or\n- number of investment_id reduces drastically around the time_id.","metadata":{}},{"cell_type":"code","source":"df_nnull = X_filled_chosen.isnull().sum(axis=1).to_frame()\ndf_nnull.columns = ['nnull']\ndf_nnull['nnull_diff'] = df_nnull['nnull'].diff(1)\ndf_nnull['too_many_missing'] = (df_nnull['nnull'] > 1000) | (df_nnull['nnull_diff'] > 200)\ndf_nnull.plot(figsize=(20,6), grid=True, title='Number of missing investment_ids w.r.t. time_id')","metadata":{"execution":{"iopub.status.busy":"2022-03-29T04:22:11.36334Z","iopub.execute_input":"2022-03-29T04:22:11.363822Z","iopub.status.idle":"2022-03-29T04:22:11.631925Z","shell.execute_reply.started":"2022-03-29T04:22:11.363771Z","shell.execute_reply":"2022-03-29T04:22:11.631052Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove Missing Values\nprint(df_nnull['too_many_missing'].values.sum())\nX_filled_chosen = X_filled_chosen.iloc[~df_nnull['too_many_missing'].values, :]\nX_filled_chosen.isnull().sum(axis=1).plot(figsize=(20,4), grid=True, title='Number of missing investment_id w.r.t. time_id')","metadata":{"execution":{"iopub.status.busy":"2022-03-29T04:23:32.729335Z","iopub.execute_input":"2022-03-29T04:23:32.729674Z","iopub.status.idle":"2022-03-29T04:23:32.959226Z","shell.execute_reply.started":"2022-03-29T04:23:32.729643Z","shell.execute_reply":"2022-03-29T04:23:32.958251Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ffill the remaining missing values\n- Please note that we could still have missing values for the oldest entries in the historical data.","metadata":{}},{"cell_type":"code","source":"X_filled_chosen = X_filled_chosen.astype(\"object\").fillna(method=\"ffill\").astype(\"float\")\nX_filled_chosen.isnull().sum(axis=1).plot(figsize=(20,4), grid=True, title='Number of missing investment_id w.r.t. time_id')","metadata":{"execution":{"iopub.status.busy":"2022-03-29T04:25:45.710135Z","iopub.execute_input":"2022-03-29T04:25:45.710431Z","iopub.status.idle":"2022-03-29T04:25:47.591342Z","shell.execute_reply.started":"2022-03-29T04:25:45.710402Z","shell.execute_reply":"2022-03-29T04:25:47.590473Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lastly, remove the data with missing values.","metadata":{}},{"cell_type":"code","source":"X_filled_chosen.dropna(inplace=True)\nX_filled_chosen.isnull().sum(axis=1).plot(figsize=(20,4), grid=True, title='Number of missing investment_id w.r.t. time_id')\ntime_id_chosen = list(X_filled_chosen.index)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T04:30:00.082101Z","iopub.execute_input":"2022-03-29T04:30:00.082398Z","iopub.status.idle":"2022-03-29T04:30:00.330276Z","shell.execute_reply.started":"2022-03-29T04:30:00.08237Z","shell.execute_reply":"2022-03-29T04:30:00.329167Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Resulting Tranining Data\n\n- About 500 time_ids are left for training. The nice thing is that the recent data remains in the training data, because we would like to forecast target values in the future.","metadata":{}},{"cell_type":"code","source":"train = X[X['time_id'].isin(time_id_chosen)]\nprint(train.shape)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T04:32:27.239319Z","iopub.execute_input":"2022-03-29T04:32:27.240363Z","iopub.status.idle":"2022-03-29T04:32:34.132458Z","shell.execute_reply.started":"2022-03-29T04:32:27.240296Z","shell.execute_reply":"2022-03-29T04:32:34.131563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a bit of cleaning in memory\ndel X_filled, X_filled_chosen, X_orig, data_dummy\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T06:14:30.585874Z","iopub.execute_input":"2022-03-29T06:14:30.58618Z","iopub.status.idle":"2022-03-29T06:14:30.91271Z","shell.execute_reply.started":"2022-03-29T06:14:30.58615Z","shell.execute_reply":"2022-03-29T06:14:30.911805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. CV and the baseline score\n\nCV is important to know that:\n- mean accuracy of the model over possible different market environment (expected to be the same level as Public/Private LB score. Larger is better).\n- standard dev of the model over possible different market environment (the Public/Private LB score would vary within the width. Smaller is better).\n- When the model works nicely/badly (It can give us idea to improve the model).\n\nThanks to https://www.kaggle.com/c/ubiquant-market-prediction/discussion/304036, we use a simple K-fold CV:\n- Without shuffle.\n- I modified a bit to use all the samples apart from val data for training.\n- Holdout days set to 60 weekdays ( aligned to the possible forecasting period in Private LB (2022/4/18~7/18) )\n- did not do CPCV (from my experience, it would not affect a lot to results. To be honest, I should have checked all the auto-correlation beforehand).\n\nBaseline model is a simple Linear Regression.\n- We do not create custom features (just used 300 features f_0~f_299).\n\nWe can observe the following things from the CV result:\n- We found mean correlation = 11.5% and the stdev = 0.9% averaged over folds.\n- We never know the future, but we can expect Public/Private LB score falls into [11.5-0.9%, 11.5+0.9%] (1-sigma).\n- We can make the model better by digging out the fold showing lowest mean correaltion (for example).","metadata":{}},{"cell_type":"code","source":"class GroupTimeSeriesSplit:\n    \"\"\"\n    From: https://www.kaggle.com/c/ubiquant-market-prediction/discussion/304036\n    Custom class to create a Group Time Series Split. We ensure\n    that the time id values that are in the testing data are not a part\n    of the training data & the splits are temporal\n    \"\"\"\n    def __init__(self, n_folds: int, holdout_size: int, groups: str, cv = False) -> None:\n        self.n_folds = n_folds\n        self.holdout_size = holdout_size\n        self.groups = groups\n        self.cv = cv\n\n    def split(self, X) -> Tuple[np.array, np.array]:\n        # Take the group column and get the unique values\n        unique_time_ids = np.unique(self.groups.values)\n\n        # Split the time ids into the length of the holdout size\n        # and reverse so we work backwards in time. Also, makes\n        # it easier to get the correct time_id values per\n        # split\n        array_split_time_ids = np.array_split(\n            unique_time_ids, len(unique_time_ids) // self.holdout_size\n        )[::-1]\n\n        # Get the first n_folds values\n        array_split_time_ids = array_split_time_ids[:self.n_folds]\n\n        for time_ids in array_split_time_ids:\n            # Get test index - time id values that are in the time_ids\n            test_condition = X['time_id'].isin(time_ids)\n            test_index = X.loc[test_condition].index\n\n            # Get train index - The train index will be the time\n            # id values right up until the minimum value in the test\n            # data - we can also add a gap to this step by\n            # time id < (min - gap)\n            if self.cv:\n                train_condition = ( X['time_id'] < (np.min(time_ids)) ) | ( X['time_id'] > (np.max(time_ids)) )\n            else:\n                train_condition = X['time_id'] < (np.min(time_ids))\n            train_index = X.loc[train_condition].index\n\n            yield train_index, test_index","metadata":{"execution":{"iopub.status.busy":"2022-03-29T05:33:09.535364Z","iopub.execute_input":"2022-03-29T05:33:09.537346Z","iopub.status.idle":"2022-03-29T05:33:09.553031Z","shell.execute_reply.started":"2022-03-29T05:33:09.537269Z","shell.execute_reply":"2022-03-29T05:33:09.551971Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nFEATS = features + ['investment_id', 'time_id']\nmodels = []\npearson_means = []\npearsons_ts = []\nholdout_size = 60 # aligned to forecasting period (4/18~7/18)\nFOLDS = int(len(np.unique(train['time_id'])) / holdout_size)-1 # all the historical data will be used as eval in the training to make the model generalize better.\ngtss = GroupTimeSeriesSplit(n_folds=FOLDS, holdout_size=holdout_size, groups=train['time_id'], cv=True)\nfor fold, (tr, val) in enumerate(gtss.split(train)):\n    print('FOLD:', fold)\n    \n    # Training\n    X_train = train.loc[tr, FEATS]\n    y_train = train.loc[tr, 'target']\n    del tr\n    gc.collect()\n    print('Train time_id range:', X_train['time_id'].min(), '->', X_train['time_id'].max(), 'Nb time_id: ', len(X_train['time_id'].unique()))\n    \n    model = LinearRegression()\n    model.fit(X_train.drop(['investment_id', 'time_id'], axis=1), y_train)\n    models.append(model)    \n    del X_train, y_train\n    gc.collect()\n    \n    # Evaluation\n    X_val = train.loc[val, FEATS]\n    y_val = train.loc[val, 'target']\n    del val\n    gc.collect()\n    print('Val time_id range:', X_val['time_id'].min(), '->', X_val['time_id'].max(), 'Nb time_id: ', len(X_val['time_id'].unique()))\n\n    time_ids_val = X_val['time_id'].values\n    X_val['y_pred'] = model.predict(X_val.drop(['investment_id', 'time_id'], axis=1))\n    X_val['y_true'] = y_val.values\n    X_val['time_id'] = time_ids_val\n    \n    pearson = X_val[['time_id', 'y_true', 'y_pred']].groupby('time_id').apply(lambda x: pearsonr(x['y_true'], x['y_pred'])[0])\n    pearson_mean = pearson.mean()\n    pearson_stdev = pearson.std()\n    print('Pearson Mean: {:.3%}, Pearson Stdev: {:.3%}'.format(pearson_mean, pearson_stdev))\n    print()\n    pearson_means.append(pearson_mean)\n    df_pearson = pd.DataFrame(pearson, index=X_val['time_id'])\n    df_pearson.columns = ['corr']\n    pearsons_ts.append(df_pearson)\n    del X_val, y_val, time_ids_val\n    gc.collect()\n    \nprint(\"Peason Mean over Folds: {:.3%}, Stdev over Folds {:.3%}\".format(np.array(pearson_means).mean(), np.array(pearson_means).std()))\nprint()","metadata":{"execution":{"iopub.status.busy":"2022-03-29T06:18:08.607074Z","iopub.execute_input":"2022-03-29T06:18:08.608253Z","iopub.status.idle":"2022-03-29T06:22:51.738545Z","shell.execute_reply.started":"2022-03-29T06:18:08.608164Z","shell.execute_reply":"2022-03-29T06:22:51.7375Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation in Time for Validation Dataset\n- Correlation oscillates around 10% for all the folds.\n- Volatility depends on fold.\n- Nothing bad, but necessary to lift the average correlation level up!","metadata":{}},{"cell_type":"code","source":"nr = int(len(pearsons_ts)/2)+1\nfig, ax = plt.subplots(nr, 2, figsize=(20, 8+nr))\nfor i in range(len(pearsons_ts)):\n    df_pearson = pearsons_ts[i].groupby('time_id').mean()\n    df_pearson.columns = ['correl']\n    df_pearson.plot(grid=True, ax=ax[int(i/2), i%2])","metadata":{"execution":{"iopub.status.busy":"2022-03-29T05:54:41.621813Z","iopub.execute_input":"2022-03-29T05:54:41.622853Z","iopub.status.idle":"2022-03-29T05:54:43.1079Z","shell.execute_reply.started":"2022-03-29T05:54:41.622674Z","shell.execute_reply":"2022-03-29T05:54:43.107004Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Should we apply Target preprocessing? If so, how?\nSee **Part II**","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"## 4. Should we apply Feature preprocessing? If so, how?\nSee **Part II**","metadata":{}},{"cell_type":"code","source":"print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\nprint(\" ------------------------------------ \")\nfor var_name in dir():\n    if not var_name.startswith(\"_\") and sys.getsizeof(eval(var_name)) > 1000000:\n        print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name))/1000/1000/1000,'|'))","metadata":{"execution":{"iopub.status.busy":"2022-03-29T06:14:37.875706Z","iopub.execute_input":"2022-03-29T06:14:37.876658Z","iopub.status.idle":"2022-03-29T06:14:37.914751Z","shell.execute_reply.started":"2022-03-29T06:14:37.876617Z","shell.execute_reply":"2022-03-29T06:14:37.91362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}