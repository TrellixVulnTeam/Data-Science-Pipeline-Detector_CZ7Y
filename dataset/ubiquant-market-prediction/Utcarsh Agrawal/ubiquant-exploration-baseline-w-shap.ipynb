{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q --disable-pip-version-check install mplcyberpunk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-09T06:32:40.713604Z","iopub.execute_input":"2022-02-09T06:32:40.714503Z","iopub.status.idle":"2022-02-09T06:32:53.496162Z","shell.execute_reply.started":"2022-02-09T06:32:40.714353Z","shell.execute_reply":"2022-02-09T06:32:53.49519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"background-color:#f3ab60;font-family:newtimeroman;color:#662e2e;font-size:130%;text-align:center;border-radius:40px 40px;\">UBIQUANT MARKET PREDICTION</p>","metadata":{}},{"cell_type":"markdown","source":"<h1 align='center'>Introduction üìù</h1>\nThe aim of this competition is to predict obfuscated (unclear) metric which is relevant for making trading decisions from the features derived from real historic data from thousands of investments. This notebook will contain almost all the necessary steps and methods which will be helpful in the competition.","metadata":{}},{"cell_type":"markdown","source":"<h1 align='center'>Dataset Info üìà</h1>\n<b>Columns of the train data-</b> \n\n* ```row_id``` - A unique identifier for the row.\n* ```time_id``` - The ID code for the time the data was gathered. The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set.\n* ```investment_id``` - The ID code for an investment. Not all investment have data in all time IDs.\n* ```target``` - The target.\n* ```[f_0:f_299]``` - Anonymized features generated from market data.","metadata":{}},{"cell_type":"markdown","source":"<h1 align='center'>Evaluation Metric üìê</h1>\nSubmissions are evaluated on the mean of the Pearson correlation coefficient for each time ID.\n\n<img src='https://user-images.githubusercontent.com/55939250/151697692-562f6439-170a-4869-856d-eaa11b2da5f5.jpg' width=500px>\n\nwhere,<br> \n* r = Pearson Correlation Coefficient\n* n = Number of samples\n* x = First variable samples\n* y = Second variable samples","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:#f3ab60;font-family:newtimeroman;color:#662e2e;font-size:130%;text-align:center;border-radius:40px 40px;\">TABLE OF CONTENTS</p>\n<ul style=\"list-style-type:square\">\n    <li><a href=\"#1\">Importing Libraries</a></li>\n    <li><a href=\"#2\">Reading the data</a></li>\n    <li><a href=\"#3\">Explore The Data Analysis</a></li>\n    <ul style=\"list-style-type:disc\">\n        <li><a href=\"#3.1\">Investment_ID Distribution</a></li>\n        <li><a href=\"#3.2\">Time_ID Distribution</a></li>\n        <li><a href=\"#3.3\">Target Distribution</a></li>\n        <li><a href=\"#3.4\">Time_id Categorized</a></li>\n        <li><a href=\"#3.5\">Features Distribution</a></li>\n    </ul>\n    <li><a href=\"#4\">Baseline Model</a></li>\n    <li><a href=\"#5\">Submission</a></li>\n</ul>\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id='1'></a>\n# <p style=\"background-color:#f3ab60;font-family:newtimeroman;color:#662e2e;font-size:130%;text-align:center;border-radius:40px 40px;\">IMPORTING LIBRARIES</p>","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport mplcyberpunk\nplt.style.use('cyberpunk')\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:32:53.498985Z","iopub.execute_input":"2022-02-09T06:32:53.499249Z","iopub.status.idle":"2022-02-09T06:32:55.02823Z","shell.execute_reply.started":"2022-02-09T06:32:53.499213Z","shell.execute_reply":"2022-02-09T06:32:55.027433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2'></a>\n# <p style=\"background-color:#f3ab60;font-family:newtimeroman;color:#662e2e;font-size:130%;text-align:center;border-radius:40px 40px;\">READING THE DATA</p>","metadata":{}},{"cell_type":"code","source":"%%time\ndf = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:32:55.029453Z","iopub.execute_input":"2022-02-09T06:32:55.029672Z","iopub.status.idle":"2022-02-09T06:33:47.040149Z","shell.execute_reply.started":"2022-02-09T06:32:55.029636Z","shell.execute_reply":"2022-02-09T06:33:47.038651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:33:47.043712Z","iopub.execute_input":"2022-02-09T06:33:47.044045Z","iopub.status.idle":"2022-02-09T06:33:47.087391Z","shell.execute_reply.started":"2022-02-09T06:33:47.044013Z","shell.execute_reply":"2022-02-09T06:33:47.086853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:33:47.088419Z","iopub.execute_input":"2022-02-09T06:33:47.089146Z","iopub.status.idle":"2022-02-09T06:33:47.127692Z","shell.execute_reply.started":"2022-02-09T06:33:47.089115Z","shell.execute_reply":"2022-02-09T06:33:47.126554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='3'></a>\n# <p style=\"background-color:#f3ab60;font-family:newtimeroman;color:#662e2e;font-size:130%;text-align:center;border-radius:40px 40px;\">EXPLORE THE DATA</p>","metadata":{}},{"cell_type":"markdown","source":"<a id='3.1'></a>\n# Investment_ID Distribution\n### First of all we will look at the count of samples in each investment id. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18, 7))\n\ndf_temp = df.groupby(\"investment_id\")['row_id'].count().reset_index().rename(columns={'row_id':'Sample_Count'})\nsns.histplot(x=df_temp['investment_id'], bins=50)\nplt.xlabel('Investment_id')\nplt.ylabel('Count')\nplt.title('Sample count of Investment_ID Distribution')\n\nleast_id = int(df_temp[df_temp['Sample_Count'] == df_temp['Sample_Count'].describe()['min']]['investment_id'])\nmax_id = int(df_temp[df_temp['Sample_Count'] == df_temp['Sample_Count'].describe()['max']]['investment_id'])\n\nprint(f\"Number of unique investments - {len(df_temp)}\")\nprint(f\"Investment id with least number of samples - {least_id}, Count - {int(df_temp['Sample_Count'].describe()['min'])}\")\nprint(f\"Investment id with maximum number of samples - {max_id}, Count - {int(df_temp['Sample_Count'].describe()['max'])}\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:33:47.128754Z","iopub.execute_input":"2022-02-09T06:33:47.128959Z","iopub.status.idle":"2022-02-09T06:33:48.045617Z","shell.execute_reply.started":"2022-02-09T06:33:47.12893Z","shell.execute_reply":"2022-02-09T06:33:48.044446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='3.2'></a>\n# Time_ID Distribution\n### Now let us look at the count of samples in each time id. ","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(2,1,figsize=(17,10))\n\ndf_temp = df.iloc[:, 0:3]\ndf_temp2 = df_temp.groupby('time_id')['investment_id'].count().reset_index().rename(columns={'investment_id': 'Sample_Count'})\n\nsns.distplot(x=df_temp2[\"Sample_Count\"], ax=ax[0])\nsns.scatterplot(x=df_temp2[\"time_id\"], y=df_temp2[\"Sample_Count\"], ax=ax[1])\n\nax[0].set_xlabel('Sample_Count')\nax[1].set_xlabel('Time_ID')\n\nleast_id = int(df_temp2[df_temp2['Sample_Count'] == df_temp2['Sample_Count'].describe()['min']]['time_id'])\nmax_id = int(df_temp2[df_temp2['Sample_Count'] == df_temp2['Sample_Count'].describe()['max']]['time_id'])\n\nprint(f\"Number of unique time_ids - {len(df_temp2)}\")\nprint(f\"Time_id with least number of samples - {least_id}, Count - {int(df_temp2['Sample_Count'].describe()['min'])}\")\nprint(f\"Time_id with maximum number of samples - {max_id}, Count - {int(df_temp2['Sample_Count'].describe()['max'])}\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:33:48.047124Z","iopub.execute_input":"2022-02-09T06:33:48.047438Z","iopub.status.idle":"2022-02-09T06:33:48.958171Z","shell.execute_reply.started":"2022-02-09T06:33:48.047394Z","shell.execute_reply":"2022-02-09T06:33:48.956802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='3.3'></a>\n# Target Distribution\n### Then let's analysis the target distribution.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,7))\n\nsns.distplot(df['target'])\nplt.title(\"Target distribution\")\n\nprint(\"Mean of target - \", df['target'].describe()['mean'])\nprint(\"Minimum value of target - \", df['target'].describe()['min'])\nprint(\"Maximum value of target - \", df['target'].describe()['max'])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:33:48.959573Z","iopub.execute_input":"2022-02-09T06:33:48.959831Z","iopub.status.idle":"2022-02-09T06:34:21.415698Z","shell.execute_reply.started":"2022-02-09T06:33:48.959788Z","shell.execute_reply":"2022-02-09T06:34:21.414568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The target is normally distributed with mean of -0.0210915. Let us also look the most skewed target distribution categorized by investment_id and time_id respectively.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(2,2,figsize=(20,10))\n\ndf_temp = df.iloc[:, 0:4]\ndf_temp1 = df_temp.groupby('investment_id').skew()['target'].reset_index()\nid_1 = int(df_temp1[df_temp1['target'] == df_temp1['target'].describe()['max']]['investment_id']) \nid_2 = int(df_temp1[df_temp1['target'] == df_temp1['target'].describe()['min']]['investment_id'])\n\nselect_1 = df_temp[df_temp['investment_id'] == id_1][[\"target\"]]\nselect_2 = df_temp[df_temp['investment_id'] == id_2][[\"target\"]]\nsns.distplot(select_1['target'], ax=ax[0, 0])\nsns.distplot(select_2['target'], ax=ax[0, 1])\nprint(f\"Investment_IDs with most skewed target are - {id_1}, {id_2}\")\ndf_temp2 = df_temp.groupby('time_id').skew()['target'].reset_index()\n\nid_3 = int(df_temp2[df_temp2['target'] == df_temp2['target'].describe()['max']]['time_id']) \nid_4 = int(df_temp2[df_temp2['target'] == df_temp2['target'].describe()['min']]['time_id'])\n\nselect_3 = df_temp[df_temp['time_id'] == id_3][[\"target\"]]\nselect_4 = df_temp[df_temp['time_id'] == id_4][[\"target\"]]\nsns.distplot(select_3['target'], ax=ax[1, 0])\nsns.distplot(select_4['target'], ax=ax[1, 1])\nprint(f\"Time_IDs with most skewed target are - {id_3}, {id_4}\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:34:21.41742Z","iopub.execute_input":"2022-02-09T06:34:21.417671Z","iopub.status.idle":"2022-02-09T06:34:30.615847Z","shell.execute_reply.started":"2022-02-09T06:34:21.417639Z","shell.execute_reply":"2022-02-09T06:34:30.614577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='3.4'></a>\n# Time_id Categorized\n### I decided to group time_id into 5 different categories according to their range, i.e., {'0-250', '251-500', '501-750', '751-1000', '1000+'} to know better about data with time. We will check different things like how many sample counts, missing values per category are there, target distribution across each time id range and later on feature distribution. ","metadata":{}},{"cell_type":"code","source":"def cate_time(time):\n    if time in range(0, 251):\n        x = '0-250'\n    elif time in range(251, 501):\n        x = '251-500'\n    elif time in range(501, 751):\n        x = '501-750'\n    elif time in range(751, 1001):\n        x = '751-1000'\n    else:\n        x = '1000+'\n    return x\n\ndf_temp = df.iloc[:, 0:4]\ndf_temp2 = df_temp.groupby('time_id')['investment_id'].count().reset_index().rename(columns={'investment_id': 'Sample_Count'})\ndf_temp2['time_cat'] = df_temp2['time_id'].apply(lambda x : cate_time(x))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-09T06:34:30.619399Z","iopub.execute_input":"2022-02-09T06:34:30.620202Z","iopub.status.idle":"2022-02-09T06:34:30.745989Z","shell.execute_reply.started":"2022-02-09T06:34:30.620173Z","shell.execute_reply":"2022-02-09T06:34:30.744507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_temp2.head()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-09T06:34:30.747567Z","iopub.execute_input":"2022-02-09T06:34:30.747809Z","iopub.status.idle":"2022-02-09T06:34:30.763296Z","shell.execute_reply.started":"2022-02-09T06:34:30.747777Z","shell.execute_reply":"2022-02-09T06:34:30.761427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(20, 7))\nfig.suptitle('Time Categorized Distribution', size = 20, weight='bold')\n\nsizes = []\nfor x in df_temp2['time_cat'].unique():\n    sizes.append(df_temp2[df_temp2['time_cat']==x]['Sample_Count'].sum())\nlabels = list(df_temp2['time_cat'].unique())\nexplode = (0.05, 0.05, 0.05, 0.05, 0.05)\ncolors = ['#FF2281', '#FF6600', '#13CA91', '#099FFF', '#CC00FF']\nax[0].pie(sizes, colors=colors, explode=explode, startangle=90, labels=labels,\n        autopct='%1.0f%%', pctdistance=0.7,textprops={'fontsize':12}, counterclock=False)\ncentre_circle = plt.Circle((0,0),0.5,fc='#212946')\nax[0].add_artist(centre_circle)\nax[0].axis('equal')\nax[0].set_title(\"Sample Count Distribution\", size = 15)\n\nmissing = {'0-250':0, '251-500':0, '501-750':0, '751-1000':0, '1000+':0}\nprev = 0\n\nfor i in list(df_temp2['time_id'].values):\n    if i-prev == 0:\n        prev = i\n    else:\n        for j in range(int(prev+1), i):\n            if j in range(0, 251):\n                missing['0-250'] += 1\n            elif j in range(251, 501):\n                missing['251-500'] += 1\n            elif j in range(501, 751):\n                missing['501-750'] += 1\n            elif j in range(751, 1001):\n                missing['751-1000'] += 1\n            else:\n                missing['1000+'] += 1\n        prev = i\nsns.barplot(x=list(missing.keys()), y=list(missing.values()), ax=ax[1])\nax[1].set_title(\"Missing value count in each time category\", size = 15)\nplt.show()\n\nplt.figure(figsize=(20, 10))\ndf_temp['time_cat'] = df_temp['time_id'].apply(lambda x : cate_time(x))\nsns.boxplot(y=df_temp['target'], x=df_temp['time_cat'])\nplt.title(\"Target Distribution in each time category\", size = 15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:34:30.765421Z","iopub.execute_input":"2022-02-09T06:34:30.765709Z","iopub.status.idle":"2022-02-09T06:34:40.084154Z","shell.execute_reply.started":"2022-02-09T06:34:30.765679Z","shell.execute_reply":"2022-02-09T06:34:40.082899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='3.5'></a>\n# Features Distribution\n### Now we will focus on targets. There are total 300 anonymized features generated from market data. First we will look at distributions of few features","metadata":{}},{"cell_type":"code","source":"features = ['f_0', 'f_1', 'f_2', 'f_3', 'f_4', 'f_5']\n\nfig, ax = plt.subplots(2,3,figsize=(20,10))\n\nfor i in range(2):\n    for j in range(3):\n        if i == 1:\n            sns.violinplot(y=df[features[i*2+j+1]], ax=ax[i, j])\n        else:\n            sns.violinplot(y=df[features[i*2+j+1]], ax=ax[i, j])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:34:40.085971Z","iopub.execute_input":"2022-02-09T06:34:40.08622Z","iopub.status.idle":"2022-02-09T06:35:39.945527Z","shell.execute_reply.started":"2022-02-09T06:34:40.086187Z","shell.execute_reply":"2022-02-09T06:35:39.944527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ['f_100', 'f_101', 'f_102', 'f_103', 'f_104', 'f_105']\n\nfig, ax = plt.subplots(2,3,figsize=(20,10))\n\nfor i in range(2):\n    for j in range(3):\n        if i == 1:\n            sns.scatterplot(x=df[features[i*2+j+1]], y=df['target'], ax=ax[i, j])\n        else:\n            sns.scatterplot(x=df[features[i+j]], y=df['target'], ax=ax[i, j])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:35:39.946964Z","iopub.execute_input":"2022-02-09T06:35:39.947175Z","iopub.status.idle":"2022-02-09T06:36:22.668068Z","shell.execute_reply.started":"2022-02-09T06:35:39.947146Z","shell.execute_reply":"2022-02-09T06:36:22.667069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Next we will calculate correlation of each feature with the target and will then plot the distribution of most correlated and least correlated features.","metadata":{}},{"cell_type":"code","source":"cor = {}\n\nfor i in range(0, 300):\n    corr_f = df[['target', f'f_{i}']].corr().iloc[0,1]\n    cor[f'f_{i}'] = abs(corr_f)\n    \ncor = {k: v for k, v in sorted(cor.items(), key=lambda item: item[1])}","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:36:22.669273Z","iopub.execute_input":"2022-02-09T06:36:22.670098Z","iopub.status.idle":"2022-02-09T06:36:43.887136Z","shell.execute_reply.started":"2022-02-09T06:36:22.670022Z","shell.execute_reply":"2022-02-09T06:36:43.886526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Highest correlation\nfig, ax = plt.subplots(2,3,figsize=(20,10))\nfor i in range(2):\n    for j in range(3):\n        if i == 1:\n            print(f'target & {list(cor.keys())[-(i*2+j+1+1)]} Correlation is {list(cor.values())[-(i*2+j+1+1)]}')\n            sns.scatterplot(x=df[list(cor.keys())[-(i*2+j+1+1)]], y=df['target'], ax=ax[i, j])\n        else:\n            print(f'target & {list(cor.keys())[-(i+j+1)]} Correlation is {list(cor.values())[-(i+j+1)]}')\n            sns.scatterplot(x=df[list(cor.keys())[-(i+j+1)]], y=df['target'], ax=ax[i, j])\n            \nplt.suptitle(\"Distribution of most correlated features with target\", fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:36:43.888255Z","iopub.execute_input":"2022-02-09T06:36:43.888686Z","iopub.status.idle":"2022-02-09T06:37:27.422249Z","shell.execute_reply.started":"2022-02-09T06:36:43.888645Z","shell.execute_reply":"2022-02-09T06:37:27.421408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lowest correlation\nfig, ax = plt.subplots(2,3,figsize=(20,10))\nfor i in range(2):\n    for j in range(3):\n        if i == 1:\n            print(f'target & {list(cor.keys())[i*2+j+1]} Correlation is {list(cor.values())[i*2+j+1]}')\n            sns.scatterplot(x=df[list(cor.keys())[i*2+j+1]], y=df['target'], ax=ax[i, j])\n        else:\n            print(f'target & {list(cor.keys())[i+j]} Correlation is {list(cor.values())[i+j]}')\n            sns.scatterplot(x=df[list(cor.keys())[i+j]], y=df['target'], ax=ax[i, j])\n            \nplt.suptitle(\"Distribution of least correlated features with target\", fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:37:27.423293Z","iopub.execute_input":"2022-02-09T06:37:27.423883Z","iopub.status.idle":"2022-02-09T06:38:10.908068Z","shell.execute_reply.started":"2022-02-09T06:37:27.423852Z","shell.execute_reply":"2022-02-09T06:38:10.907189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we will see which feature play more importance with respect to different time id range. This could be helpful in analysing more about features and how they are changing with time.","metadata":{}},{"cell_type":"code","source":"df_temp = df.iloc[:, 1:]\ndf_temp['time_cat'] = df_temp['time_id'].apply(lambda x : cate_time(x))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-09T06:38:10.909772Z","iopub.execute_input":"2022-02-09T06:38:10.911168Z","iopub.status.idle":"2022-02-09T06:38:21.637326Z","shell.execute_reply.started":"2022-02-09T06:38:10.911124Z","shell.execute_reply":"2022-02-09T06:38:21.636691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Time_id range - 0-250","metadata":{}},{"cell_type":"code","source":"cor = {}\ndf_temp2 = df_temp[df_temp['time_cat']=='0-250']\nfor i in range(0, 300):\n    corr_f = df_temp2[['target', f'f_{i}']].corr().iloc[0,1]\n    cor[f'f_{i}'] = abs(corr_f)\n    \ncor = {k: v for k, v in sorted(cor.items(), key=lambda item: item[1])}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-09T06:38:21.638548Z","iopub.execute_input":"2022-02-09T06:38:21.639204Z","iopub.status.idle":"2022-02-09T06:38:26.744259Z","shell.execute_reply.started":"2022-02-09T06:38:21.639172Z","shell.execute_reply":"2022-02-09T06:38:26.743251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Highest correlation\nfig, ax = plt.subplots(2,3,figsize=(20,10))\nfor i in range(2):\n    for j in range(3):\n        if i == 1:\n            print(f'target & {list(cor.keys())[-(i*2+j+1+1)]} Correlation is {list(cor.values())[-(i*2+j+1+1)]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[-(i*2+j+1+1)]], y=df_temp2['target'], ax=ax[i, j], color=colors[0])\n        else:\n            print(f'target & {list(cor.keys())[-(i+j+1)]} Correlation is {list(cor.values())[-(i+j+1)]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[-(i+j+1)]], y=df_temp2['target'], ax=ax[i, j], color=colors[0])\n            \nplt.suptitle(\"Distribution of most correlated features with target of time_id between 0-250\", fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:38:26.745588Z","iopub.execute_input":"2022-02-09T06:38:26.745822Z","iopub.status.idle":"2022-02-09T06:38:35.28352Z","shell.execute_reply.started":"2022-02-09T06:38:26.74579Z","shell.execute_reply":"2022-02-09T06:38:35.282232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lowest correlation\nfig, ax = plt.subplots(2,3,figsize=(20,10))\nfor i in range(2):\n    for j in range(3):\n        if i == 1:\n            print(f'target & {list(cor.keys())[i*2+j+1]} Correlation is {list(cor.values())[i*2+j+1]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[i*2+j+1]], y=df_temp2['target'], ax=ax[i, j], color=colors[0])\n        else:\n            print(f'target & {list(cor.keys())[i+j]} Correlation is {list(cor.values())[i+j]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[i+j]], y=df_temp2['target'], ax=ax[i, j], color=colors[0])\n            \nplt.suptitle(\"Distribution of least correlated features with target of time_id between 0-250\", fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:38:35.284757Z","iopub.execute_input":"2022-02-09T06:38:35.285006Z","iopub.status.idle":"2022-02-09T06:38:43.868465Z","shell.execute_reply.started":"2022-02-09T06:38:35.284973Z","shell.execute_reply":"2022-02-09T06:38:43.866993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Time_id range - 251-500","metadata":{}},{"cell_type":"code","source":"cor = {}\ndf_temp2 = df_temp[df_temp['time_cat']=='251-500']\nfor i in range(0, 300):\n    corr_f = df_temp2[['target', f'f_{i}']].corr().iloc[0,1]\n    cor[f'f_{i}'] = abs(corr_f)\n    \ncor = {k: v for k, v in sorted(cor.items(), key=lambda item: item[1])}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-09T06:38:43.870387Z","iopub.execute_input":"2022-02-09T06:38:43.871436Z","iopub.status.idle":"2022-02-09T06:38:48.373494Z","shell.execute_reply.started":"2022-02-09T06:38:43.871391Z","shell.execute_reply":"2022-02-09T06:38:48.371469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Highest correlation\nfig, ax = plt.subplots(2,3,figsize=(20,10))\nfor i in range(2):\n    for j in range(3):\n        if i == 1:\n            print(f'target & {list(cor.keys())[-(i*2+j+1+1)]} Correlation is {list(cor.values())[-(i*2+j+1+1)]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[-(i*2+j+1+1)]], y=df_temp2['target'], ax=ax[i, j], color=colors[1])\n        else:\n            print(f'target & {list(cor.keys())[-(i+j+1)]} Correlation is {list(cor.values())[-(i+j+1)]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[-(i+j+1)]], y=df_temp2['target'], ax=ax[i, j], color=colors[1])\n            \nplt.suptitle(\"Distribution of most correlated features with target of time_id between 251-500\", fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:38:48.375707Z","iopub.execute_input":"2022-02-09T06:38:48.375963Z","iopub.status.idle":"2022-02-09T06:38:56.081776Z","shell.execute_reply.started":"2022-02-09T06:38:48.375937Z","shell.execute_reply":"2022-02-09T06:38:56.080229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lowest correlation\nfig, ax = plt.subplots(2,3,figsize=(20,10))\nfor i in range(2):\n    for j in range(3):\n        if i == 1:\n            print(f'target & {list(cor.keys())[i*2+j+1]} Correlation is {list(cor.values())[i*2+j+1]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[i*2+j+1]], y=df_temp2['target'], ax=ax[i, j], color=colors[1])\n        else:\n            print(f'target & {list(cor.keys())[i+j]} Correlation is {list(cor.values())[i+j]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[i+j]], y=df_temp2['target'], ax=ax[i, j], color=colors[1])\n            \nplt.suptitle(\"Distribution of least correlated features with target of time_id between 251-500\", fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:38:56.083702Z","iopub.execute_input":"2022-02-09T06:38:56.084754Z","iopub.status.idle":"2022-02-09T06:39:03.754632Z","shell.execute_reply.started":"2022-02-09T06:38:56.084668Z","shell.execute_reply":"2022-02-09T06:39:03.752856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Time_id range - 501-750","metadata":{}},{"cell_type":"code","source":"cor = {}\ndf_temp2 = df_temp[df_temp['time_cat']=='501-750']\nfor i in range(0, 300):\n    corr_f = df_temp2[['target', f'f_{i}']].corr().iloc[0,1]\n    cor[f'f_{i}'] = abs(corr_f)\n    \ncor = {k: v for k, v in sorted(cor.items(), key=lambda item: item[1])}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-09T06:39:03.757002Z","iopub.execute_input":"2022-02-09T06:39:03.757755Z","iopub.status.idle":"2022-02-09T06:39:09.177948Z","shell.execute_reply.started":"2022-02-09T06:39:03.757717Z","shell.execute_reply":"2022-02-09T06:39:09.176691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Highest correlation\nfig, ax = plt.subplots(2,3,figsize=(20,10))\nfor i in range(2):\n    for j in range(3):\n        if i == 1:\n            print(f'target & {list(cor.keys())[-(i*2+j+1+1)]} Correlation is {list(cor.values())[-(i*2+j+1+1)]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[-(i*2+j+1+1)]], y=df_temp2['target'], ax=ax[i, j], color=colors[2])\n        else:\n            print(f'target & {list(cor.keys())[-(i+j+1)]} Correlation is {list(cor.values())[-(i+j+1)]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[-(i+j+1)]], y=df_temp2['target'], ax=ax[i, j], color=colors[2])\n            \nplt.suptitle(\"Distribution of most correlated features with target of time_id between 501-750\", fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:39:09.179242Z","iopub.execute_input":"2022-02-09T06:39:09.179467Z","iopub.status.idle":"2022-02-09T06:39:18.867815Z","shell.execute_reply.started":"2022-02-09T06:39:09.179437Z","shell.execute_reply":"2022-02-09T06:39:18.866918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lowest correlation\nfig, ax = plt.subplots(2,3,figsize=(20,10))\nfor i in range(2):\n    for j in range(3):\n        if i == 1:\n            print(f'target & {list(cor.keys())[i*2+j+1]} Correlation is {list(cor.values())[i*2+j+1]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[i*2+j+1]], y=df_temp2['target'], ax=ax[i, j], color=colors[2])\n        else:\n            print(f'target & {list(cor.keys())[i+j]} Correlation is {list(cor.values())[i+j]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[i+j]], y=df_temp2['target'], ax=ax[i, j], color=colors[2])\n            \nplt.suptitle(\"Distribution of least correlated features with target of time_id between 501-750\", fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:39:18.868902Z","iopub.execute_input":"2022-02-09T06:39:18.869165Z","iopub.status.idle":"2022-02-09T06:39:28.464144Z","shell.execute_reply.started":"2022-02-09T06:39:18.869141Z","shell.execute_reply":"2022-02-09T06:39:28.463171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Time_id range - 751-1000","metadata":{}},{"cell_type":"code","source":"cor = {}\ndf_temp2 = df_temp[df_temp['time_cat']=='751-1000']\nfor i in range(0, 300):\n    corr_f = df_temp2[['target', f'f_{i}']].corr().iloc[0,1]\n    cor[f'f_{i}'] = abs(corr_f)\n    \ncor = {k: v for k, v in sorted(cor.items(), key=lambda item: item[1])}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-09T06:39:28.469579Z","iopub.execute_input":"2022-02-09T06:39:28.470498Z","iopub.status.idle":"2022-02-09T06:39:34.311589Z","shell.execute_reply.started":"2022-02-09T06:39:28.470465Z","shell.execute_reply":"2022-02-09T06:39:34.311053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Highest correlation\nfig, ax = plt.subplots(2,3,figsize=(20,10))\nfor i in range(2):\n    for j in range(3):\n        if i == 1:\n            print(f'target & {list(cor.keys())[-(i*2+j+1+1)]} Correlation is {list(cor.values())[-(i*2+j+1+1)]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[-(i*2+j+1+1)]], y=df_temp2['target'], ax=ax[i, j], color=colors[3])\n        else:\n            print(f'target & {list(cor.keys())[-(i+j+1)]} Correlation is {list(cor.values())[-(i+j+1)]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[-(i+j+1)]], y=df_temp2['target'], ax=ax[i, j], color=colors[3])\n            \nplt.suptitle(\"Distribution of most correlated features with target of time_id between 751-1000\", fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:39:34.312563Z","iopub.execute_input":"2022-02-09T06:39:34.312876Z","iopub.status.idle":"2022-02-09T06:39:45.600503Z","shell.execute_reply.started":"2022-02-09T06:39:34.31285Z","shell.execute_reply":"2022-02-09T06:39:45.598862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lowest correlation\nfig, ax = plt.subplots(2,3,figsize=(20,10))\nfor i in range(2):\n    for j in range(3):\n        if i == 1:\n            print(f'target & {list(cor.keys())[i*2+j+1]} Correlation is {list(cor.values())[i*2+j+1]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[i*2+j+1]], y=df_temp2['target'], ax=ax[i, j], color=colors[3])\n        else:\n            print(f'target & {list(cor.keys())[i+j]} Correlation is {list(cor.values())[i+j]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[i+j]], y=df_temp2['target'], ax=ax[i, j], color=colors[3])\n            \nplt.suptitle(\"Distribution of least correlated features with target of time_id between 751-1000\", fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:39:45.602226Z","iopub.execute_input":"2022-02-09T06:39:45.602906Z","iopub.status.idle":"2022-02-09T06:39:56.572025Z","shell.execute_reply.started":"2022-02-09T06:39:45.602868Z","shell.execute_reply":"2022-02-09T06:39:56.571282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Time_id range - 1000+","metadata":{}},{"cell_type":"code","source":"cor = {}\ndf_temp2 = df_temp[df_temp['time_cat']=='1000+']\nfor i in range(0, 300):\n    corr_f = df_temp2[['target', f'f_{i}']].corr().iloc[0,1]\n    cor[f'f_{i}'] = abs(corr_f)\n    \ncor = {k: v for k, v in sorted(cor.items(), key=lambda item: item[1])}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-09T06:39:56.573209Z","iopub.execute_input":"2022-02-09T06:39:56.57405Z","iopub.status.idle":"2022-02-09T06:40:02.418448Z","shell.execute_reply.started":"2022-02-09T06:39:56.574009Z","shell.execute_reply":"2022-02-09T06:40:02.417567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Highest correlation\nfig, ax = plt.subplots(2,3,figsize=(20,10))\nfor i in range(2):\n    for j in range(3):\n        if i == 1:\n            print(f'target & {list(cor.keys())[-(i*2+j+1+1)]} Correlation is {list(cor.values())[-(i*2+j+1+1)]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[-(i*2+j+1+1)]], y=df_temp2['target'], ax=ax[i, j], color=colors[4])\n        else:\n            print(f'target & {list(cor.keys())[-(i+j+1)]} Correlation is {list(cor.values())[-(i+j+1)]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[-(i+j+1)]], y=df_temp2['target'], ax=ax[i, j], color=colors[4])\n            \nplt.suptitle(\"Distribution of most correlated features with target of time_id 1000+\", fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:40:02.419976Z","iopub.execute_input":"2022-02-09T06:40:02.420259Z","iopub.status.idle":"2022-02-09T06:40:13.089676Z","shell.execute_reply.started":"2022-02-09T06:40:02.420204Z","shell.execute_reply":"2022-02-09T06:40:13.08858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lowest correlation\nfig, ax = plt.subplots(2,3,figsize=(20,10))\nfor i in range(2):\n    for j in range(3):\n        if i == 1:\n            print(f'target & {list(cor.keys())[i*2+j+1]} Correlation is {list(cor.values())[i*2+j+1]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[i*2+j+1]], y=df_temp2['target'], ax=ax[i, j], color=colors[4])\n        else:\n            print(f'target & {list(cor.keys())[i+j]} Correlation is {list(cor.values())[i+j]}')\n            sns.scatterplot(x=df_temp2[list(cor.keys())[i+j]], y=df_temp2['target'], ax=ax[i, j], color=colors[4])\n            \nplt.suptitle(\"Distribution of least correlated features with target of time_id 1000+\", fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:40:13.092012Z","iopub.execute_input":"2022-02-09T06:40:13.092395Z","iopub.status.idle":"2022-02-09T06:40:23.758016Z","shell.execute_reply.started":"2022-02-09T06:40:13.092336Z","shell.execute_reply":"2022-02-09T06:40:23.756577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='4'></a>\n# <p style=\"background-color:#f3ab60;font-family:newtimeroman;color:#662e2e;font-size:130%;text-align:center;border-radius:40px 40px;\">BASELINE MODEL</p>","metadata":{}},{"cell_type":"code","source":"# Due to low memory\n%reset -f","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:40:23.759932Z","iopub.execute_input":"2022-02-09T06:40:23.760291Z","iopub.status.idle":"2022-02-09T06:40:24.380041Z","shell.execute_reply.started":"2022-02-09T06:40:23.760252Z","shell.execute_reply":"2022-02-09T06:40:24.379056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport shap\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set(rc={\"axes.facecolor\":\"#212946\",\"figure.facecolor\":\"#ffffff\"})","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:40:24.381517Z","iopub.execute_input":"2022-02-09T06:40:24.38272Z","iopub.status.idle":"2022-02-09T06:40:28.303213Z","shell.execute_reply.started":"2022-02-09T06:40:24.382635Z","shell.execute_reply":"2022-02-09T06:40:28.302285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:40:28.304473Z","iopub.execute_input":"2022-02-09T06:40:28.304715Z","iopub.status.idle":"2022-02-09T06:40:45.601688Z","shell.execute_reply.started":"2022-02-09T06:40:28.304685Z","shell.execute_reply":"2022-02-09T06:40:45.600648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop(['row_id', 'time_id', 'target'], axis=1)\ny = df[\"target\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, shuffle=False, random_state=10)\n\ndel df, X, y\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:40:45.603166Z","iopub.execute_input":"2022-02-09T06:40:45.60393Z","iopub.status.idle":"2022-02-09T06:40:55.922134Z","shell.execute_reply.started":"2022-02-09T06:40:45.603886Z","shell.execute_reply":"2022-02-09T06:40:55.919433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LGBMRegressor(\n        num_leaves=6,\n        learning_rate = 0.05,\n        n_estimators = 1000,\n        min_child_samples = 1000, \n        subsample=0.5, \n        metric=\"rmse\"\n    )","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:40:55.926096Z","iopub.execute_input":"2022-02-09T06:40:55.9265Z","iopub.status.idle":"2022-02-09T06:40:55.937455Z","shell.execute_reply.started":"2022-02-09T06:40:55.926457Z","shell.execute_reply":"2022-02-09T06:40:55.936088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train, early_stopping_rounds=6, eval_set=[(X_val, y_val)], verbose=10)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:40:55.939134Z","iopub.execute_input":"2022-02-09T06:40:55.939421Z","iopub.status.idle":"2022-02-09T06:48:21.486694Z","shell.execute_reply.started":"2022-02-09T06:40:55.939382Z","shell.execute_reply":"2022-02-09T06:48:21.486171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SHAP (SHapley Additive exPlanations)\n### Shap values are floating-point numbers corresponding to data in each row corresponding to each feature. Shap value represents the contribution of that particular data point in predicting the outputs. If the shap value is much closer to zero, we can say that the data point contributes very little to predictions. If the shap value is a strong positive or strong negative value, we can say that the data point greatly contributes to predictions.","metadata":{}},{"cell_type":"code","source":"# Initialize object that can calculate shap values\nexplainer = shap.TreeExplainer(model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(X_val)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:48:21.488003Z","iopub.execute_input":"2022-02-09T06:48:21.48833Z","iopub.status.idle":"2022-02-09T06:48:45.35999Z","shell.execute_reply.started":"2022-02-09T06:48:21.488301Z","shell.execute_reply":"2022-02-09T06:48:45.359305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We will visualize summary plot to get the idea which feature mattered the most to the model.","metadata":{}},{"cell_type":"code","source":"shap.summary_plot(shap_values, X_val, plot_size=(20, 20))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:53:28.606735Z","iopub.execute_input":"2022-02-09T06:53:28.60727Z","iopub.status.idle":"2022-02-09T06:54:21.861284Z","shell.execute_reply.started":"2022-02-09T06:53:28.607231Z","shell.execute_reply":"2022-02-09T06:54:21.859755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Note to study summary plot </b> - Horizontal location shows whether the effect of that value caused a higher or lower prediction and color shows whether that feature was high or low for that row of the dataset","metadata":{}},{"cell_type":"markdown","source":"### We can also plot a simpler summary bar plot by giving argument plot_type='bar'.","metadata":{}},{"cell_type":"code","source":"shap.summary_plot(shap_values, X_val, plot_type='bar', plot_size=(20, 15), color='#ff0090')","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:55:13.506559Z","iopub.execute_input":"2022-02-09T06:55:13.507166Z","iopub.status.idle":"2022-02-09T06:55:14.710456Z","shell.execute_reply.started":"2022-02-09T06:55:13.507132Z","shell.execute_reply":"2022-02-09T06:55:14.709448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lastly, we will look into dependence plot. Dependence plots can be of great use while analyzing feature importance and feature selection. It plots shap values of the desired feature and colorize the dot with respect to another feature.<br> Here I will plot dependence plot of first 9 features.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 3, figsize=(20, 20))\nax = ax.ravel()\nfor i, x in enumerate(X_val.columns):\n    shap.dependence_plot(x, shap_values, X_val, ax=ax[i], show=False)\n    if i == 8:\n        break\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:49:39.352824Z","iopub.execute_input":"2022-02-09T06:49:39.353527Z","iopub.status.idle":"2022-02-09T06:51:09.347286Z","shell.execute_reply.started":"2022-02-09T06:49:39.353486Z","shell.execute_reply":"2022-02-09T06:51:09.346302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5'></a>\n# <p style=\"background-color:#f3ab60;font-family:newtimeroman;color:#662e2e;font-size:130%;text-align:center;border-radius:40px 40px;\">SUBMISSION</p>","metadata":{}},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()  \niter_test = env.iter_test()\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df.drop(['row_id'], axis=1, inplace=True)\n    pred = model.predict(test_df)\n    sample_prediction_df['target'] = pred\n    env.predict(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:51:09.348558Z","iopub.execute_input":"2022-02-09T06:51:09.349127Z","iopub.status.idle":"2022-02-09T06:51:09.61185Z","shell.execute_reply.started":"2022-02-09T06:51:09.349094Z","shell.execute_reply":"2022-02-09T06:51:09.61125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Refernces\n* https://www.kaggle.com/robikscube/fast-data-loading-and-low-mem-with-parquet-files\n* https://www.kaggle.com/miingkang/check-correlation-baseline-lgbm\n* https://www.kaggle.com/edwardcrookenden/eda-and-lgbm-baseline-feature-imp\n* https://shap.readthedocs.io/en/latest/index.html","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n    <h2 align='center'>THANK YOU!!</h2>\n    <h3 align='center'>Please consider upvoting the kernel if you found it useful.</h3>\n</div>","metadata":{}}]}