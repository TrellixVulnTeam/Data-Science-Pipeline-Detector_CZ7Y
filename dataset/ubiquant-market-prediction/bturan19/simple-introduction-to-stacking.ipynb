{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, train_test_split, KFold\nimport matplotlib.pyplot as plt\n\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nfrom category_encoders.target_encoder import TargetEncoder","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-08T19:28:41.492562Z","iopub.execute_input":"2022-02-08T19:28:41.492986Z","iopub.status.idle":"2022-02-08T19:28:44.791978Z","shell.execute_reply.started":"2022-02-08T19:28:41.4929Z","shell.execute_reply":"2022-02-08T19:28:44.791224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#parquet file comes from here: https://www.kaggle.com/avijitduttta/xgboost-basicml-solution\n#Thanks a lot!\ndf_train = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T19:28:54.394642Z","iopub.execute_input":"2022-02-08T19:28:54.394903Z","iopub.status.idle":"2022-02-08T19:29:31.80288Z","shell.execute_reply.started":"2022-02-08T19:28:54.394872Z","shell.execute_reply":"2022-02-08T19:29:31.802098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.shape)\nprint(df_train.investment_id.nunique())\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T19:29:31.805505Z","iopub.execute_input":"2022-02-08T19:29:31.80596Z","iopub.status.idle":"2022-02-08T19:29:31.853199Z","shell.execute_reply.started":"2022-02-08T19:29:31.805917Z","shell.execute_reply":"2022-02-08T19:29:31.852371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"te = TargetEncoder(smoothing=0.2, cols=[\"investment_id\"])\nte.fit_transform(df_train[\"investment_id\"], df_train[\"target\"]).astype(\"float32\")\n\ndf_train[\"investment_te\"] = 0.","metadata":{"execution":{"iopub.status.busy":"2022-02-08T19:29:31.854604Z","iopub.execute_input":"2022-02-08T19:29:31.854853Z","iopub.status.idle":"2022-02-08T19:29:32.429363Z","shell.execute_reply.started":"2022-02-08T19:29:31.85482Z","shell.execute_reply":"2022-02-08T19:29:32.42859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\nfeatures += ['investment_id', 'investment_te']\ninvestment_id = df_train['investment_id']\ntime_id = df_train['time_id']","metadata":{"execution":{"iopub.status.busy":"2022-02-08T19:29:32.430989Z","iopub.execute_input":"2022-02-08T19:29:32.431253Z","iopub.status.idle":"2022-02-08T19:29:32.437266Z","shell.execute_reply.started":"2022-02-08T19:29:32.431216Z","shell.execute_reply":"2022-02-08T19:29:32.436065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ['investment_id','investment_te',\n 'f_196', 'f_0', 'f_284', 'f_98', 'f_99', 'f_151', 'f_208', 'f_7', 'f_142', 'f_227', 'f_123', 'f_83', 'f_74', 'f_103', 'f_91', 'f_35', 'f_81', 'f_70', 'f_257', 'f_277', 'f_224', 'f_230', 'f_77', 'f_218', 'f_182', 'f_187', 'f_11', 'f_167', 'f_102', 'f_13', 'f_177', 'f_232', 'f_248', 'f_63', 'f_296', 'f_71', 'f_198', 'f_265', 'f_145', 'f_21', 'f_170', 'f_290', 'f_200', 'f_174', 'f_237', 'f_149', 'f_138', 'f_37', 'f_112', 'f_68', 'f_61', 'f_22', 'f_75', 'f_8', 'f_23', 'f_287', 'f_266', 'f_252', 'f_189', 'f_239', 'f_231', 'f_121', 'f_105', 'f_283', 'f_181', 'f_220', 'f_215', 'f_286', 'f_275', 'f_30', 'f_27', 'f_18', 'f_108', 'f_202', 'f_236', 'f_132', 'f_201', 'f_15', 'f_110', 'f_114', 'f_118', 'f_53', 'f_93', 'f_183', 'f_185', 'f_276', 'f_73', 'f_85', 'f_186', 'f_199', 'f_109', 'f_48', 'f_289', 'f_84', 'f_19', 'f_229', 'f_16', 'f_166', 'f_6', 'f_78', 'f_44', 'f_64', 'f_191', 'f_154', 'f_115', 'f_225', 'f_251', 'f_260', 'f_42', 'f_57', 'f_26', 'f_46', 'f_127', 'f_263', 'f_126', 'f_129', 'f_10', 'f_141', 'f_80', 'f_246', 'f_24', 'f_219', 'f_40', 'f_128', 'f_38', 'f_100', 'f_55', 'f_124', 'f_5', 'f_165', 'f_66', 'f_122', 'f_33', 'f_222', 'f_135', 'f_282', 'f_153', 'f_62', 'f_176', 'f_34', 'f_299', 'f_82', 'f_293', 'f_92', 'f_281', 'f_210', 'f_111', 'f_253', 'f_244', 'f_238', 'f_241', 'f_45', 'f_162', 'f_32', 'f_20', 'f_9', 'f_12', 'f_258', 'f_120', 'f_175', 'f_56', 'f_146'\n ]\n\ndf_train = df_train.loc[:,['target']+features]","metadata":{"execution":{"iopub.status.busy":"2022-02-08T19:29:32.438727Z","iopub.execute_input":"2022-02-08T19:29:32.438995Z","iopub.status.idle":"2022-02-08T19:29:33.676367Z","shell.execute_reply.started":"2022-02-08T19:29:32.438959Z","shell.execute_reply":"2022-02-08T19:29:33.675625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                #    df[col] = df[col].astype(np.float16)\n                #elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                #    df[col] = df[col].astype(np.float32)\n                #else:\n                df[col] = df[col].astype(np.float16)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\ndf_train = reduce_mem_usage(df_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T19:29:36.404198Z","iopub.execute_input":"2022-02-08T19:29:36.404508Z","iopub.status.idle":"2022-02-08T19:30:29.982033Z","shell.execute_reply.started":"2022-02-08T19:29:36.404476Z","shell.execute_reply":"2022-02-08T19:30:29.981255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class stack_model:\n    def __init__(self,base_models,meta_model,train_X,train_y):\n        self.base_models = base_models\n        self.meta_model  = meta_model\n        self.train_X = train_X\n        self.train_y = train_y\n        #self.time_id = time_id\n\n    def fit(self,cvFolds = 5, random_state= 42, categorical=[], target_encoding=False):\n        meta_model_data = pd.DataFrame()\n        for i,val in enumerate(self.base_models):\n            meta_model_data[f\"class_{i}\"] = None\n        meta_model_data[\"actual\"] = None\n        #meta_model_data[\"time_id\"] = None\n\n        # Create kfolds\n        #kf = KFold(n_splits= cvFolds, shuffle=False)\n        kf = TimeSeriesSplit(n_splits= cvFolds+5)\n        for i, (train_index, test_index) in enumerate(kf.split(self.train_X)):\n            print(\"Fold Number: \", i)\n            if i < 6:\n                continue\n            X_train = self.train_X[train_index]\n            X_test = self.train_X[test_index]\n\n            y_train = self.train_y[train_index]\n            y_test = self.train_y[test_index]\n\n            # Encodings\n            ## target encoding\n            if target_encoding:\n                encoder_df = pd.DataFrame(X_train[:, :2])\n                encoder_df.columns = [\"investment_id\", \"investment_te\"]\n                encoder_df[\"target\"] = y_train\n                te = TargetEncoder(smoothing=0.2, cols=[\"investment_id\"])\n                X_train[:, 1:2] = te.fit_transform(encoder_df[\"investment_id\"], encoder_df[\"target\"]).astype(\"float32\")\n                encoder_test = pd.DataFrame(X_test[:, 0:1])\n                encoder_test.columns = [\"investment_id\"]\n                X_test[:, 1:2] = te.transform(encoder_test[\"investment_id\"]).astype(\"float32\")\n                del encoder_df\n                gc.collect()\n                \n\n            # Get predictions from all the base models\n            this_fold_df = pd.DataFrame()\n            this_fold_df[\"actual\"] = y_test.tolist()\n            for i,val in enumerate(self.base_models):\n                X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, \n                                                                test_size=0.2,\n                                                                shuffle=False)\n                self.base_models[i].fit(X_train,y_train,\n                                        eval_set=[(X_val, y_val)],\n                                        verbose=False,\n                                        early_stopping_rounds=15)\n                this_fold_df[f\"class_{i}\"] = self.base_models[i].predict(X_test)\n                #this_fold_df[\"time_id\"] = self.time_id[test_index]\n                print(f\"Model {i}, MSE SCORE: \", mean_squared_error(y_test, this_fold_df[f\"class_{i}\"]))\n                #print(\"Correlation result from this fold= \", evaluate(this_fold_df, \"actual\", f\"class_{i}\"))\n                print(\"Simple corr: \", pearsonr(this_fold_df[f\"actual\"], this_fold_df[f\"class_{i}\"]))\n            #this_fold_df[\"actual\"] = y_test.tolist()\n            meta_model_data = meta_model_data.append(this_fold_df)\n        #meta_model_data = meta_model_data.dropna()\n        X_meta_train, X_meta_test, y_meta_train, y_meta_test = train_test_split(\n                                                    meta_model_data.drop(\"actual\", axis =1), \n                                                    meta_model_data[\"actual\"],\n                                                    test_size=0.2, \n                                                    shuffle=False)\n        self.meta_model.fit(X_meta_train, y_meta_train,\n                            eval_set=(X_meta_test, y_meta_test),\n                            verbose=50,\n                            early_stopping_rounds=20)\n        #Now release the memory by deleting  train_X since we dont need\n        self.train_X = None\n\n\n    def predict(self,test_X):\n        meta_X = pd.DataFrame()\n        res = pd.DataFrame()\n        for i,val in enumerate(self.base_models):\n            meta_X[f\"class_{i}\"] = self.base_models[i].predict(test_X)\n        res[f\"class_\"] = self.meta_model.predict(meta_X.values)\n        return res[\"class_\"].values","metadata":{"execution":{"iopub.status.busy":"2022-02-08T19:30:29.98468Z","iopub.execute_input":"2022-02-08T19:30:29.98494Z","iopub.status.idle":"2022-02-08T19:30:30.002565Z","shell.execute_reply.started":"2022-02-08T19:30:29.984904Z","shell.execute_reply":"2022-02-08T19:30:30.001667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create a holdout set and also take a partition of train because of memory limitations","metadata":{}},{"cell_type":"code","source":"#HOLD OUT\nholdout = df_train.iloc[int(len(df_train)*0.9):]\ndf_train = df_train.iloc[int(len(df_train)*0.4):] #int(len(df_train)*0.9)\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T19:30:30.00385Z","iopub.execute_input":"2022-02-08T19:30:30.004331Z","iopub.status.idle":"2022-02-08T19:30:30.146641Z","shell.execute_reply.started":"2022-02-08T19:30:30.004294Z","shell.execute_reply":"2022-02-08T19:30:30.145861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will have 3 model and with their out of fold predictions we will build another model","metadata":{}},{"cell_type":"code","source":"meta_models = {}\nscalers = {}\n\n#sc = StandardScaler()\nX = df_train[features].values\ny = df_train['target'].values\n\ncatboost1 = CatBoostRegressor(random_seed=42, task_type='GPU')\nxgboost1 = XGBRegressor(random_state=42, tree_method='gpu_hist',  n_estimators=800,\n    learning_rate=0.05,\n    max_depth=12,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    #colsample_bylevel=0.75,\n    missing=-999,)\nlgbm1 = LGBMRegressor(random_state=42, \n                      n_estimators=800, \n                      device_type='gpu', \n                      extra_trees=True,\n                      lambda_l1=.2,\n                      lambda_l2=.2,\n                      feature_fractio=.8)\n\nbase_models = [\n    catboost1,\n    xgboost1,\n    lgbm1\n]\n\nmeta_model = CatBoostRegressor(iterations=800, random_state=42, task_type='GPU', verbose=0)\nmeta_models[\"meta1\"] = stack_model(base_models,meta_model,X,y)\nmeta_models[\"meta1\"].fit(cvFolds = 5, random_state= 42, categorical=[], target_encoding=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T19:30:30.148463Z","iopub.execute_input":"2022-02-08T19:30:30.148812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets test it with holdout:\nholdout[\"preds\"] = meta_models[\"meta1\"].predict(holdout[features].values)\nprint(f\"Correlation: {pearsonr(holdout['target'].values, holdout['preds'].values)[0]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\nimport ubiquant\nenv = ubiquant.make_env()  \niter_test = env.iter_test()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-07T19:17:37.854884Z","iopub.status.idle":"2022-02-07T19:17:37.85565Z","shell.execute_reply.started":"2022-02-07T19:17:37.85535Z","shell.execute_reply":"2022-02-07T19:17:37.855375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (test_df, sample_prediction_df) in iter_test:\n    \n    test_df[\"investment_te\"] = te.transform(test_df[\"investment_id\"]).astype(\"float32\")\n    x_tt = test_df.loc[:, features].values\n    preds = meta_models[\"meta1\"].predict(x_tt)\n    sample_prediction_df['target'] = preds\n    env.predict(sample_prediction_df) \n    #display(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T19:17:37.858274Z","iopub.status.idle":"2022-02-07T19:17:37.858958Z","shell.execute_reply.started":"2022-02-07T19:17:37.8587Z","shell.execute_reply":"2022-02-07T19:17:37.858743Z"},"trusted":true},"execution_count":null,"outputs":[]}]}