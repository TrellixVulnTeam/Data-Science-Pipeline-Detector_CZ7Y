{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ubiquant Market Prediction\n投資の収益率を予測するモデルを構築<br>\n\n- **row_id**\n- **time_id** - 0~1219\n- **investment_id** - 0~3773\n- **target** - The target.\n- **features** - [f_0:f_299] - 市場データより生成された特徴量\n\nデータの確認 ～ 提出コードの確認まで行っていきます。","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport gc \nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T11:25:38.611478Z","iopub.execute_input":"2022-01-23T11:25:38.612074Z","iopub.status.idle":"2022-01-23T11:25:41.011365Z","shell.execute_reply.started":"2022-01-23T11:25:38.611956Z","shell.execute_reply":"2022-01-23T11:25:41.010608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 学習データ読み込み\n提供されているCSV学習データの容量は18Gほどあります。そのままでは大き過ぎるので、Daskで読み込むかpickleやparquetで扱うことになりそうです。<br>\n今回は、ROB MULLAさんのParquetデータを使用させていただきます。感謝です！<br>\n参考カーネル：[⏫ Fast Data Loading and Low Mem with Parquet Files](https://www.kaggle.com/robikscube/fast-data-loading-and-low-mem-with-parquet-files)","metadata":{}},{"cell_type":"code","source":"%%time\ndf_train = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')\ndf_train","metadata":{"execution":{"iopub.status.busy":"2022-01-23T11:25:41.012674Z","iopub.execute_input":"2022-01-23T11:25:41.012879Z","iopub.status.idle":"2022-01-23T11:26:18.231988Z","shell.execute_reply.started":"2022-01-23T11:25:41.012855Z","shell.execute_reply":"2022-01-23T11:26:18.231095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T11:26:18.233825Z","iopub.execute_input":"2022-01-23T11:26:18.234148Z","iopub.status.idle":"2022-01-23T11:26:18.26349Z","shell.execute_reply.started":"2022-01-23T11:26:18.234106Z","shell.execute_reply":"2022-01-23T11:26:18.262054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.loc[:, 'row_id':'f_3'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T11:26:18.266483Z","iopub.execute_input":"2022-01-23T11:26:18.266821Z","iopub.status.idle":"2022-01-23T11:26:19.113131Z","shell.execute_reply.started":"2022-01-23T11:26:18.266776Z","shell.execute_reply":"2022-01-23T11:26:19.112429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"データ数を確認してみます。欠損ありますね。<br>\ntime_idについては、idが大きいほどデータがそろっている、また300後半でかなりデータの抜けがあります。<br>\ninvestment_idは、bin数を増やすとボコボコしてるのが確認できました。","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(3,1,figsize=(10,10))\nsns.distplot(df_train['target'], ax=ax[0], bins=200, kde=False)\nax[0].set_title('Histgram of target')\n\nsns.distplot(df_train['time_id'], ax=ax[1], bins=200, kde=False)\nax[1].set_title('Histgram of time_id')\n\nsns.distplot(df_train['investment_id'], ax=ax[2], bins=200, kde=False)\nax[2].set_title('Histgram of investment_id')\n\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T11:26:19.114115Z","iopub.execute_input":"2022-01-23T11:26:19.114488Z","iopub.status.idle":"2022-01-23T11:26:21.247195Z","shell.execute_reply.started":"2022-01-23T11:26:19.114451Z","shell.execute_reply":"2022-01-23T11:26:21.246352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 提出確認\nubiquant Python moduleを使用する必要があるとのこと。簡易モデルで提出の確認だけしておきます。<br>\n[Overview/Evaluation](https://www.kaggle.com/c/ubiquant-market-prediction/overview/evaluation)に記載の通り、predictionを行います。","metadata":{}},{"cell_type":"code","source":"df_u300 = df_train[df_train.time_id<=300]\ndf_o1100 = df_train[df_train.time_id>=1100]\ndel df_train\n\nfeatures = ['f_'+str(i) for i in range(300)]\ntarget = 'target'\n\n# 学習に使用するデータを設定\ndata_train = lgb.Dataset(df_u300[features], df_u300[target])\ndata_eval = lgb.Dataset(df_o1100[features], df_o1100[target], reference=data_train) \n\n# LightGBM parameters\nparams = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"num_leaves\": 14,\n        \"max_depth\": 4,\n        \"feature_fraction\": 0.8,\n        \"subsample_freq\": 1,\n        \"bagging_fraction\": 0.7,\n        \"min_data_in_leaf\": 10,\n        \"learning_rate\": 0.2,\n        \"boosting\": \"gbdt\",\n        \"lambda_l1\": 0.4,\n        \"lambda_l2\": 0.4,\n        \"verbosity\": -1,\n        \"random_state\": 42,\n}\n\n# モデルの学習\nmodel = lgb.train(params,\n                  train_set=data_train, # トレーニングデータの指定\n                  valid_sets=data_eval, # 検証データの指定\n                  )\ndf_feature = pd.DataFrame({\n    'features' :features,\n    'importance' : model.feature_importance()})\ndf_feature = df_feature.sort_values(by='importance',ascending = False)\n\nplt.figure(figsize=(16, 6))\nsns.barplot(x='features', y='importance', data=df_feature[0:30])\nplt.title(f'Head LightGBM Feature Importance')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T11:26:21.248914Z","iopub.execute_input":"2022-01-23T11:26:21.250538Z","iopub.status.idle":"2022-01-23T11:27:00.57915Z","shell.execute_reply.started":"2022-01-23T11:26:21.250491Z","shell.execute_reply":"2022-01-23T11:27:00.578348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\n\nenv = ubiquant.make_env()  # initialize the environment\niter_test = env.iter_test()  # an iterator which loops over the test set and sample submission\n\nfor (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df['target'] = model.predict(test_df[features])  # make your predictions here\n    env.predict(sample_prediction_df)   # register your predictions","metadata":{"execution":{"iopub.status.busy":"2022-01-23T11:27:00.580429Z","iopub.execute_input":"2022-01-23T11:27:00.580792Z","iopub.status.idle":"2022-01-23T11:27:00.680032Z","shell.execute_reply.started":"2022-01-23T11:27:00.580761Z","shell.execute_reply":"2022-01-23T11:27:00.679081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}