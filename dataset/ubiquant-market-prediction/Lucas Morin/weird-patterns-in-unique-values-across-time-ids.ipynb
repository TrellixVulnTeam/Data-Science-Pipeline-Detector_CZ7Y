{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Weird pattern in unique values across time ids\n\nThere seems to be some weird pattern in unique values across time ids. This notebook aims to explore that.","metadata":{}},{"cell_type":"markdown","source":"## Other Feature Exploration / Feature engineering for Ubiquant:\n\n- [Complete Feature Exploration](https://www.kaggle.com/lucasmorin/complete-feature-exploration)\n- [Weird pattern in unique values](https://www.kaggle.com/lucasmorin/weird-patterns-in-unique-values-across-time-ids/)\n- [Time x Strategy EDA](https://www.kaggle.com/lucasmorin/time-x-strategy-eda)  \n- [UMAP Data Analysis & Applications](https://www.kaggle.com/lucasmorin/umap-data-analysis-applications)   \n- [LB probing Notebook  ](https://www.kaggle.com/lucasmorin/don-t-mind-me-just-probing-the-lb)\n- On-Line Feature Engineering (in progress)","metadata":{}},{"cell_type":"markdown","source":"## Weird patterns :\n\n- [Counting Values](#Counting_Values)\n- [All Patterns](#All_Patterns)\n- [Unitary Values](#Unitary_Values) (ðŸ”¥ðŸ”¥ðŸ”¥)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nDEBUG = False","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-20T10:25:48.797002Z","iopub.execute_input":"2022-02-20T10:25:48.79755Z","iopub.status.idle":"2022-02-20T10:25:49.774852Z","shell.execute_reply.started":"2022-02-20T10:25:48.797433Z","shell.execute_reply":"2022-02-20T10:25:49.77426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using @slawekbiel Feather dataset: https://www.kaggle.com/slawekbiel/ubiquant-trainfeather-32-bit","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_data = pd.read_feather('../input/ubiquant-trainfeather-32-bit/train32.feather')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T10:25:49.776169Z","iopub.execute_input":"2022-02-20T10:25:49.776612Z","iopub.status.idle":"2022-02-20T10:26:19.91835Z","shell.execute_reply.started":"2022-02-20T10:25:49.776563Z","shell.execute_reply":"2022-02-20T10:26:19.917336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"number of unique values per time ids:","metadata":{}},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T10:26:19.920466Z","iopub.execute_input":"2022-02-20T10:26:19.920693Z","iopub.status.idle":"2022-02-20T10:26:19.953781Z","shell.execute_reply.started":"2022-02-20T10:26:19.920665Z","shell.execute_reply":"2022-02-20T10:26:19.953193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='Counting_Values'></a>\n# Counting values","metadata":{}},{"cell_type":"code","source":"mean_problem = []\nn = 5 if DEBUG else 300\n\nfeatures_of_interest = {}\ncount = train_data[['time_id','investment_id']].groupby(['time_id']).count().values.flatten()\n\nfor i in range(n):\n    f_name = 'f_'+str(i)\n    unique = train_data[['time_id',f_name]].groupby(['time_id']).nunique().values.flatten()\n    id_pattern = (np.log(count)-np.log(unique)>1)\n    mean = id_pattern.mean()\n    mean_problem.append(mean)\n    features_of_interest[f_name] = unique[id_pattern].mean()\n\nprint(f'proportion of features with a problem - above 1%: {np.mean([m>0.01 for m in mean_problem])}')\nprint(f'average proportion of feature values impacted: {np.mean([m for m in mean_problem if m>0.01])}')\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-02-19T16:38:39.873893Z","iopub.execute_input":"2022-02-19T16:38:39.874238Z","iopub.status.idle":"2022-02-19T16:38:44.661943Z","shell.execute_reply.started":"2022-02-19T16:38:39.874208Z","shell.execute_reply":"2022-02-19T16:38:44.661097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mode_max = 1000000\n\ncat_filter = {k: v for k, v in features_of_interest.items() if v<mode_max}\nsmall_cat_dict = sorted(cat_filter.items(), key=lambda x: x[1])\nsmall_cat_dict","metadata":{"execution":{"iopub.status.busy":"2022-02-19T16:43:04.434853Z","iopub.execute_input":"2022-02-19T16:43:04.435129Z","iopub.status.idle":"2022-02-19T16:43:04.442072Z","shell.execute_reply.started":"2022-02-19T16:43:04.4351Z","shell.execute_reply":"2022-02-19T16:43:04.441189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='All_Patterns'></a>\n# All Patterns","metadata":{}},{"cell_type":"code","source":"for i in range(300):\n    feature_name = 'f_'+str(i)\n    print('f_'+str(i))\n    plt.plot(np.log(train_data[['time_id','investment_id']].groupby(['time_id']).count()))\n    plt.plot(np.log(train_data[['time_id',feature_name]].groupby(['time_id']).nunique()))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T10:51:23.257704Z","iopub.execute_input":"2022-01-22T10:51:23.258058Z","iopub.status.idle":"2022-01-22T10:51:34.949644Z","shell.execute_reply.started":"2022-01-22T10:51:23.258023Z","shell.execute_reply":"2022-01-22T10:51:34.948804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='Unitary_Values'></a>\n# Unitary values (Market Features)","metadata":{}},{"cell_type":"code","source":"# small number of values:\n\nsmalls = ['f_170','f_272','f_182','f_124','f_200','f_175']\n\ncount = train_data[['time_id','investment_id']].groupby(['time_id']).count().values.flatten()\n\n\nfor f_name in smalls :\n    #f_name = cat[0]\n    print(f_name)\n    unique = train_data[['time_id',f_name]].groupby(['time_id']).nunique().values.flatten()\n    id_pattern = (np.log(count)-np.log(unique)>1)\n    mean = id_pattern.mean()\n    #plt.plot()\n    fig, axs = plt.subplots(2, 3)\n    fig.set_size_inches(30, 12)\n    \n    f_mean = train_data[['time_id',f_name]].groupby('time_id').agg(np.mean)[f_name]\n    train_data[f_name+'c'] = train_data.time_id.map(round(f_mean))\n    train_data[f_name+'n'] = train_data[f_name] - train_data[f_name+'c']\n    \n    axs[0, 0].plot(np.log(train_data[['time_id',f_name+'n']].groupby(['time_id']).nunique()))\n    axs[0, 0].plot(np.log(train_data[['time_id','investment_id']].groupby(['time_id']).count()))\n    axs[0, 0].set_title('Pattern')\n    \n    axs[1, 0].plot(train_data[['time_id',f_name+'c']].groupby('time_id').agg(np.mean)[f_name+'c'])\n    axs[1, 0].set_title('retrieved Categorical')\n    \n    axs[0, 1].plot(train_data[['time_id',f_name+'n']].groupby('time_id').agg(np.mean)[f_name+'n'])\n    axs[0, 1].set_title('Average Noise')\n    \n    axs[1, 1].plot(train_data[['time_id',f_name+'n']].groupby('time_id').agg(np.std)[f_name+'n'])\n    axs[1, 1].set_title('Noise std')\n    \n    axs[0, 2].plot(train_data[['time_id',f_name+'n']].groupby('time_id').agg(np.min)[f_name+'n'])\n    axs[0, 2].plot(train_data[['time_id',f_name+'n']].groupby('time_id').agg(np.max)[f_name+'n'])\n    axs[0, 2].set_title('Noise min/max')\n    \n    axs[1, 2].plot(train_data[['time_id',f_name]].groupby('time_id').agg(np.mean)[f_name].cumsum())\n    axs[1, 2].plot(train_data[['time_id',f_name+'c']].groupby('time_id').agg(np.mean)[f_name+'c'].cumsum())\n    axs[1, 2].set_title('Cumsum of market data')\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T10:34:43.527446Z","iopub.execute_input":"2022-02-20T10:34:43.52772Z","iopub.status.idle":"2022-02-20T10:35:00.347844Z","shell.execute_reply.started":"2022-02-20T10:34:43.527692Z","shell.execute_reply":"2022-02-20T10:35:00.346939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smalls = ['f_124']\n\nfor f_name in smalls :\n    #f_name = cat[0]\n    print(f_name)\n    unique = train_data[['time_id',f_name]].groupby(['time_id']).nunique().values.flatten()\n    id_pattern = (np.log(count)-np.log(unique)>1)\n    mean = id_pattern.mean()\n    #plt.plot()\n    fig, axs = plt.subplots(2, 3)\n    fig.set_size_inches(30, 12)\n    \n    f_mean = train_data[['time_id',f_name]].groupby('time_id').agg(np.mean)[f_name]\n    train_data[f_name+'c'] = train_data.time_id.map(round(f_mean))\n    train_data[f_name+'n'] = train_data[f_name] - train_data[f_name+'c']\n    \n    axs[0, 0].plot(np.log(train_data[['time_id',f_name+'n']].groupby(['time_id']).nunique()))\n    axs[0, 0].plot(np.log(train_data[['time_id','investment_id']].groupby(['time_id']).count()))\n    axs[1, 0].plot(train_data[['time_id',f_name+'c']].groupby('time_id').agg(np.mean)[f_name+'c'])\n    axs[0, 1].plot(train_data[['time_id',f_name+'n']].groupby('time_id').agg(np.mean)[f_name+'n'])\n    axs[1, 1].plot(train_data[['time_id',f_name+'n']].groupby('time_id').agg(np.std)[f_name+'n'])\n    axs[0, 2].plot(train_data[['time_id',f_name+'n']].groupby('time_id').agg(np.min)[f_name+'n'])\n    axs[1, 2].plot(train_data[['time_id',f_name+'n']].groupby('time_id').agg(np.max)[f_name+'n'])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T10:26:20.684969Z","iopub.status.idle":"2022-02-20T10:26:20.685246Z","shell.execute_reply.started":"2022-02-20T10:26:20.685096Z","shell.execute_reply":"2022-02-20T10:26:20.68511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(np.clip(train_data[['time_id',f_name+'n']].groupby('time_id').agg(np.mean)[f_name+'n'],-0.00001,0.00001))","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:07:25.077067Z","iopub.execute_input":"2022-02-19T17:07:25.077359Z","iopub.status.idle":"2022-02-19T17:07:25.395486Z","shell.execute_reply.started":"2022-02-19T17:07:25.077328Z","shell.execute_reply":"2022-02-19T17:07:25.394555Z"},"trusted":true},"execution_count":null,"outputs":[]}]}