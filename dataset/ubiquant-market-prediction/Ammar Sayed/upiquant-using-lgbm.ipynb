{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Steps of this notebook** \n* Import the required libraries\n* build the functions that we will need to use it for visualization more than one time\n* Load the dataset and show some data analysis (has nulls, std and mean)\n* Remove the outliers from the featuers\n* remove the correlated features that have correlation more than .8 to avoid misleading\n* train the data using LGBM algorithm\n* predict the data\n","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nimport xgboost as xgb\nfrom sklearn.svm import SVR\nimport time\nfrom multiprocessing import Process, Pool\nfrom scipy.stats import pearsonr\n\nseed = 42\nnp.random.seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:33:54.16871Z","iopub.execute_input":"2022-02-16T04:33:54.169638Z","iopub.status.idle":"2022-02-16T04:33:55.370977Z","shell.execute_reply.started":"2022-02-16T04:33:54.169531Z","shell.execute_reply":"2022-02-16T04:33:55.370297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Functions","metadata":{}},{"cell_type":"markdown","source":"## Draw features to detect Normal distribution of features","metadata":{}},{"cell_type":"code","source":"def draw_hist_features(df, fig_size = 10, from_st = 0):\n    plt.figure(figsize = (20, 20))\n    for index in range(from_st, from_st + fig_size):\n        plt.subplot(5, 5, 1 + index)\n        plt.hist(df.loc[:, f'f_{index}'], bins = 100)\n        plt.title(f'f_{index}')","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:33:55.372267Z","iopub.execute_input":"2022-02-16T04:33:55.37261Z","iopub.status.idle":"2022-02-16T04:33:55.378341Z","shell.execute_reply.started":"2022-02-16T04:33:55.372582Z","shell.execute_reply":"2022-02-16T04:33:55.377615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Draw outliers function to visualize the outliers between featuers and target\nwe will need it to draw the outliers compared to train feature","metadata":{}},{"cell_type":"code","source":"def draw_outliers(df, features = [], fig_size = (20, 20)):\n    plt.figure(figsize = fig_size)\n    for index, f in enumerate(features):\n        plt.subplot(5, 5, 1 + index)\n        plt.scatter(df[f], df['target'])\n        plt.xticks([]), plt.yticks([])\n        plt.title(f)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:33:55.39625Z","iopub.execute_input":"2022-02-16T04:33:55.397103Z","iopub.status.idle":"2022-02-16T04:33:55.40515Z","shell.execute_reply.started":"2022-02-16T04:33:55.397065Z","shell.execute_reply":"2022-02-16T04:33:55.404553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Detect outliers used to extract ouliters based on fraction* std that u choose\nthis method used to detect the outliers that out of range std* 70 by default (but in our case it will be std* 35)","metadata":{}},{"cell_type":"code","source":"def detect_outliers(df, featuers, fraction = 70):\n    outliers_list = [] # used to store the index of the outlier record\n    outliers_col = [] # used to store feature that conatins outliers\n    \n    # detect outliers from range +ve and -ve over x-axis\n    for col in features:\n        mean = df[col].mean()\n        std = df[col].std()\n        out = df[(df[col] > mean + std * fraction) | \n                 (df[col] < mean - std * fraction)\n                ]\n        \n        # only remvove the features that hase outliers > 0 and < 10 records\n        if 0 < len(out) < 10:\n            out_list = out.index.to_list()\n            outliers_list.extend(out_list) # save the outlier records\n            outliers_col.append(col) # save this feature as conatins outliers\n            \n            print(f'{col}: {len(out)}')\n        \n    print(f'# of features: {len(outliers_col)}')\n    print(f'ouliers records: {len(outliers_list)}') # print number of outliers\n    \n    return outliers_list, outliers_col","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:33:55.91773Z","iopub.execute_input":"2022-02-16T04:33:55.918582Z","iopub.status.idle":"2022-02-16T04:33:55.926469Z","shell.execute_reply.started":"2022-02-16T04:33:55.918535Z","shell.execute_reply":"2022-02-16T04:33:55.925765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reduce Memory method (not mine) to reduce the train.csv as storage as possible\nThis memory is not mine it's for kaggler who i need to thank him for this powerfull and simple method that reduce the dataset memory for 60% at least in most cases","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df):\n  \n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    \n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n     \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:33:56.518887Z","iopub.execute_input":"2022-02-16T04:33:56.519161Z","iopub.status.idle":"2022-02-16T04:33:56.533066Z","shell.execute_reply.started":"2022-02-16T04:33:56.519133Z","shell.execute_reply":"2022-02-16T04:33:56.532093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the dataset\nfor memory effecieny we will use the compressed dataset of this competition wich reduced the train data from 18GB to 3GB","metadata":{}},{"cell_type":"code","source":"path = '../input/ubiquant-parquet/'\n\nprint('Reading the train data....')\ntrain = pd.read_parquet(path + 'train.parquet')\nprint('Done.')","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:38:16.729343Z","iopub.execute_input":"2022-02-16T04:38:16.730091Z","iopub.status.idle":"2022-02-16T04:39:10.857906Z","shell.execute_reply.started":"2022-02-16T04:38:16.730054Z","shell.execute_reply":"2022-02-16T04:39:10.857018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:05:01.132031Z","iopub.execute_input":"2022-02-16T04:05:01.132968Z","iopub.status.idle":"2022-02-16T04:05:01.167504Z","shell.execute_reply.started":"2022-02-16T04:05:01.132929Z","shell.execute_reply":"2022-02-16T04:05:01.167008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Exploratory","metadata":{}},{"cell_type":"markdown","source":"### show describe method\nin **describe()** method: as we see in the mean and std there are some features that std is away from 1 and mean away from 0","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:05:15.419855Z","iopub.execute_input":"2022-02-16T04:05:15.420147Z","iopub.status.idle":"2022-02-16T04:05:51.058139Z","shell.execute_reply.started":"2022-02-16T04:05:15.420115Z","shell.execute_reply":"2022-02-16T04:05:51.057192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Search for null values","metadata":{}},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:08:04.261178Z","iopub.execute_input":"2022-02-16T04:08:04.261481Z","iopub.status.idle":"2022-02-16T04:08:06.735601Z","shell.execute_reply.started":"2022-02-16T04:08:04.261453Z","shell.execute_reply":"2022-02-16T04:08:06.735013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Draw features to detect Normal distribution of features","metadata":{}},{"cell_type":"code","source":"draw_hist_features(df = train, fig_size = 20)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:08:09.891355Z","iopub.execute_input":"2022-02-16T04:08:09.891932Z","iopub.status.idle":"2022-02-16T04:08:16.473123Z","shell.execute_reply.started":"2022-02-16T04:08:09.891889Z","shell.execute_reply":"2022-02-16T04:08:16.472647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"as we see \"f_1, 2, 5, 6 and 9\" are nearly normal distributed \n\nbut \"f_0, 3, 4, 8, 10, 11, 12\" are away from  normal distributed, so we need to scatter some of these features to make sure they contains outliers\n\n- if the outliers are small records then we will drop them for not affect our model from learning and avoid **misleading**","metadata":{}},{"cell_type":"markdown","source":"### Build Draw outliers function to reduce code duplication","metadata":{}},{"cell_type":"code","source":"draw_outliers(train, features = ['f_3', 'f_4', 'f_8', 'f_6']) # just try some features to detect outliers ","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:08:21.511613Z","iopub.execute_input":"2022-02-16T04:08:21.51191Z","iopub.status.idle":"2022-02-16T04:08:59.213602Z","shell.execute_reply.started":"2022-02-16T04:08:21.511877Z","shell.execute_reply":"2022-02-16T04:08:59.213046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get the feature columns\nGet all features except\n* row_id\n* time_id\n* target","metadata":{}},{"cell_type":"code","source":"%%time \nfeatures = train.columns.to_list()\nfeatures = [f for f in features if f not in ['row_id', 'time_id', 'target']]  # remove features \"time_id, row_id, target\"","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:47:09.848345Z","iopub.execute_input":"2022-02-16T04:47:09.849378Z","iopub.status.idle":"2022-02-16T04:47:09.860036Z","shell.execute_reply.started":"2022-02-16T04:47:09.849289Z","shell.execute_reply":"2022-02-16T04:47:09.859223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove outliers from std * 35 and len < 10","metadata":{}},{"cell_type":"code","source":"%%time\n# loop over all features f_0 till f_299 to find outliers\noutliers_list, outliers_col = detect_outliers(train, features, fraction = 35)  # outliers with fraction 35%","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:47:11.982094Z","iopub.execute_input":"2022-02-16T04:47:11.982791Z","iopub.status.idle":"2022-02-16T04:47:20.578938Z","shell.execute_reply.started":"2022-02-16T04:47:11.982756Z","shell.execute_reply":"2022-02-16T04:47:20.578169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"as we see there are **36** features contains outliers that are out of range stdx35 which give **102** records of outlier vlaues\n\nWe need to remove these outliers\n\n* before dropping the outliers let's see how they affect on the distribution of the features","metadata":{}},{"cell_type":"code","source":"%%time\ndraw_outliers(train, features=['f_12', 'f_214', 'f_295', 'f_165'])\n","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:09:12.529325Z","iopub.execute_input":"2022-02-16T04:09:12.529503Z","iopub.status.idle":"2022-02-16T04:09:50.925452Z","shell.execute_reply.started":"2022-02-16T04:09:12.529482Z","shell.execute_reply":"2022-02-16T04:09:50.92461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's drop these outliers to see how they affect the distribution of features","metadata":{}},{"cell_type":"code","source":"%%time\nprint(f'Before: {len(train)}')\ntrain.drop(train.index[outliers_list], inplace = True)\nprint(f'After: {len(train)}')","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:47:25.330633Z","iopub.execute_input":"2022-02-16T04:47:25.331242Z","iopub.status.idle":"2022-02-16T04:47:32.544631Z","shell.execute_reply.started":"2022-02-16T04:47:25.331205Z","shell.execute_reply":"2022-02-16T04:47:32.543694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time\ndraw_outliers(train, features=['f_12', 'f_214', 'f_295', 'f_165'])\n","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:10:24.529927Z","iopub.execute_input":"2022-02-16T04:10:24.530229Z","iopub.status.idle":"2022-02-16T04:11:03.239627Z","shell.execute_reply.started":"2022-02-16T04:10:24.5302Z","shell.execute_reply":"2022-02-16T04:11:03.238825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see the distribution of the features became much better after removing the outliers \n\n- u r free to choose the fraction u need to remove the outliers i tried std*35 but maybe chaning the fraction to other value would make detect much outliers ","metadata":{}},{"cell_type":"markdown","source":"## Reduce train memory\nThis method is referenced to [GUILLAUME MARTIN ](https://www.kaggle.com/gemartin/load-data-reduce-memory-usage) its very useful to reduce memory usage \n\nby the way he inspired this method from [this notebook](https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65)\n\nThanks [GUILLAUME](https://www.kaggle.com/gemartin) ","metadata":{}},{"cell_type":"code","source":"%%time\n\ntrain = reduce_mem_usage(train) ","metadata":{"execution":{"iopub.status.busy":"2022-02-15T21:44:59.506488Z","iopub.execute_input":"2022-02-15T21:44:59.50709Z","iopub.status.idle":"2022-02-15T21:47:24.467268Z","shell.execute_reply.started":"2022-02-15T21:44:59.507055Z","shell.execute_reply":"2022-02-15T21:47:24.466237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**WOW** how wonderful this method reduce the memory in huage scale, take it a try,  it's realy useful","metadata":{}},{"cell_type":"markdown","source":"# Save the dataset as Pickle files\ni used this step as when i train the model the memory is overfitted so i need to restart over again, so to avoid these steps i saved the dataset as Pickle file\n\nif the memory overfits the next time just import the updated data and start from this cell and ignore the above steps\n\n**Why i didn't save it as parquet format rather than Pickle?!!**\n* Hmmm, nice question, because the parquet doesn't support float16 data and when i used reduce_mem_usage() method i transformed the float32 and float64 to **float16** also the same with **int16**, so i have to save it as Pickle or back the data to float32 again then save it as parquet format as u like","metadata":{}},{"cell_type":"code","source":"# save train set as parquest format\ntrain.to_pickle('final_train.pickle')","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:36:03.420367Z","iopub.execute_input":"2022-02-14T15:36:03.420709Z","iopub.status.idle":"2022-02-14T15:36:10.281468Z","shell.execute_reply.started":"2022-02-14T15:36:03.420669Z","shell.execute_reply":"2022-02-14T15:36:10.280556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Memory crached?!* start from here next time\n**Get the train set again by starting from this cell if the memory overfits**\n\nIf the memory overfits again for the 1000 time with you \"as happened to me\", I'll begin from here","metadata":{}},{"cell_type":"code","source":"train = pd.read_pickle('final_train.pickle')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get target from train","metadata":{}},{"cell_type":"code","source":"target = train['target']\ntrain.drop(['target'], axis = 1, inplace = True)\n\ntarget","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:47:49.950193Z","iopub.execute_input":"2022-02-16T04:47:49.950461Z","iopub.status.idle":"2022-02-16T04:47:54.216341Z","shell.execute_reply.started":"2022-02-16T04:47:49.950435Z","shell.execute_reply":"2022-02-16T04:47:54.215422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop time_id, row_id from train","metadata":{}},{"cell_type":"code","source":"train.drop(['row_id', 'time_id'], inplace = True, axis = 1)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove Correlated Features more than 0.80\nTo avoid misleading we need to find the feaures that has more than 80% correlation and remove one of them\n\nalso this step will avoid more computation","metadata":{}},{"cell_type":"code","source":"%%time \ncorr_f = train.corr()\ncorr_f.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T21:28:56.507216Z","iopub.execute_input":"2022-02-15T21:28:56.507509Z","iopub.status.idle":"2022-02-15T21:41:49.007799Z","shell.execute_reply.started":"2022-02-15T21:28:56.507465Z","shell.execute_reply":"2022-02-15T21:41:49.007023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(corr_f)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:16:51.234426Z","iopub.execute_input":"2022-02-15T20:16:51.235293Z","iopub.status.idle":"2022-02-15T20:16:52.14428Z","shell.execute_reply.started":"2022-02-15T20:16:51.235247Z","shell.execute_reply":"2022-02-15T20:16:52.143544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Find the features that has features that has corr more than 0.80","metadata":{}},{"cell_type":"code","source":"%%time\nf_len = len(features)\nexclude = []\n\nfor i in range(f_len):\n    for j in range(f_len):\n        if corr_f.iloc[i, j] > .80 and i !=  j: # remove any feature of them\n            exclude.append(features[j])\n\nexclude = set(exclude) # remove duplicated feature name","metadata":{"execution":{"iopub.status.busy":"2022-02-15T21:44:36.034511Z","iopub.execute_input":"2022-02-15T21:44:36.034803Z","iopub.status.idle":"2022-02-15T21:44:38.29057Z","shell.execute_reply.started":"2022-02-15T21:44:36.034774Z","shell.execute_reply":"2022-02-15T21:44:38.289689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(set(exclude))","metadata":{"execution":{"iopub.status.busy":"2022-02-15T21:44:38.29213Z","iopub.execute_input":"2022-02-15T21:44:38.292946Z","iopub.status.idle":"2022-02-15T21:44:38.299332Z","shell.execute_reply.started":"2022-02-15T21:44:38.292888Z","shell.execute_reply":"2022-02-15T21:44:38.298354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove the features that has corr more than .80","metadata":{}},{"cell_type":"code","source":"print(f'train len, Before: {len(train.columns)}')\ntrain.drop(exclude, inplace=True, axis = 1)\nprint(f'train len, After: {len(train.columns)}')","metadata":{"execution":{"iopub.status.busy":"2022-02-15T21:44:38.425951Z","iopub.execute_input":"2022-02-15T21:44:38.426872Z","iopub.status.idle":"2022-02-15T21:44:39.536034Z","shell.execute_reply.started":"2022-02-15T21:44:38.426832Z","shell.execute_reply":"2022-02-15T21:44:39.535177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"as we see the train features droped from **301** to **270** which are **31** features are greater than 80% corr with other exsiting features","metadata":{}},{"cell_type":"markdown","source":"# Split valid and train data","metadata":{}},{"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(train, target, test_size = 5000)\n\nx_train.shape, x_valid.shape, y_train.shape, y_valid.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:19:07.349653Z","iopub.execute_input":"2022-02-15T20:19:07.350407Z","iopub.status.idle":"2022-02-15T20:19:20.015736Z","shell.execute_reply.started":"2022-02-15T20:19:07.35036Z","shell.execute_reply":"2022-02-15T20:19:20.014812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove some variables don't need to reduce memory usage\nDon't be optimistic it will not be reduced that much :)","metadata":{}},{"cell_type":"code","source":"%%time\noutliers_col = None\noutliers_list = None\nfeatures = None\ntarget = [[]]\ntrain = [[]]","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:19:31.342637Z","iopub.execute_input":"2022-02-15T20:19:31.342984Z","iopub.status.idle":"2022-02-15T20:19:31.363618Z","shell.execute_reply.started":"2022-02-15T20:19:31.342951Z","shell.execute_reply":"2022-02-15T20:19:31.362616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build the Model","metadata":{}},{"cell_type":"markdown","source":"## 1- LGBM","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport lightgbm as lgbm\n\nlgbm_reg = lgbm.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        n_estimators=100)\n\nlgbm_reg.fit(train, target) # (x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T21:58:30.657816Z","iopub.execute_input":"2022-02-15T21:58:30.659415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_reg.score(x_valid, y_valid)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T21:23:36.395214Z","iopub.status.idle":"2022-02-14T21:23:36.395939Z","shell.execute_reply.started":"2022-02-14T21:23:36.395732Z","shell.execute_reply":"2022-02-14T21:23:36.395755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also we can use Pool which uses custom number of CPUs to work on\n\nKaggle Notebooks contains 4 CPUs so, you can use until 400% from the process\n\n* this step for more processing and reduced memory","metadata":{}},{"cell_type":"code","source":"# %%time\n# with Pool(2) as pool:  # use 2 CPUs \n#     result = pool.map(lgbm_reg.fit, (x_train, y_train))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T05:39:49.422626Z","iopub.execute_input":"2022-02-14T05:39:49.424379Z","iopub.status.idle":"2022-02-14T05:39:59.784074Z","shell.execute_reply.started":"2022-02-14T05:39:49.424317Z","shell.execute_reply":"2022-02-14T05:39:59.7829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2 - MLPRegressor","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.neural_network import MLPRegressor\n\nmlp_reg = MLPRegressor(solver='adam',\n                        hidden_layer_sizes=(128, 128),\n                        activation='relu',\n                        verbose=True,\n                        warm_start=True)\n\nmlp_reg.partial_fit(train, target)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:58:13.368724Z","iopub.execute_input":"2022-02-14T15:58:13.369604Z","iopub.status.idle":"2022-02-14T16:00:41.961419Z","shell.execute_reply.started":"2022-02-14T15:58:13.369519Z","shell.execute_reply":"2022-02-14T16:00:41.960408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlp_reg.score(x_valid, y_valid)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T16:21:13.902498Z","iopub.execute_input":"2022-02-14T16:21:13.902833Z","iopub.status.idle":"2022-02-14T16:21:13.960688Z","shell.execute_reply.started":"2022-02-14T16:21:13.902796Z","shell.execute_reply":"2022-02-14T16:21:13.959617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ny_pred = mlp_reg.predict(x_valid)\n\npearsonr(y_pred, y_valid)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T16:26:59.68609Z","iopub.execute_input":"2022-02-14T16:26:59.686479Z","iopub.status.idle":"2022-02-14T16:26:59.753543Z","shell.execute_reply.started":"2022-02-14T16:26:59.686434Z","shell.execute_reply":"2022-02-14T16:26:59.752427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction and Submission","metadata":{}},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T21:23:36.396806Z","iopub.status.idle":"2022-02-14T21:23:36.397632Z","shell.execute_reply.started":"2022-02-14T21:23:36.397416Z","shell.execute_reply":"2022-02-14T21:23:36.397448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (test_df, sample_prediction_df) in iter_test:\n    \n    test_df = test_df[features]\n    y_pred  = lgbm_reg.predict(test_df)\n    sample_prediction_df[\"target\"] = y_pred\n    \n    display(test_df)\n    display(sample_prediction_df)\n    \n    env.predict(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T21:23:36.398816Z","iopub.status.idle":"2022-02-14T21:23:36.399111Z","shell.execute_reply.started":"2022-02-14T21:23:36.398958Z","shell.execute_reply":"2022-02-14T21:23:36.398973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**If it helps u hope to upvote thanks ❤️**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}