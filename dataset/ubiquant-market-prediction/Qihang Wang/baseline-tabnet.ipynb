{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TabNet Baseline\n\n- TabNet is widely used in table-based competitions. This notebook is just the most basic attempt based on LightGMB's baseline. I hope you will like it and give me some more suggestions.\n- 大家好，这是我第一次上传我的notebook，TabNet是表格类数据比赛中常用的手法之一。这个笔记本是根据别人开源的LightGBM稍微更改了一点的笔记本。希望大家多给一些意见。\n- 今回は初めて自分のノートブックをアップロードしました。TabNetは表データのコンペでよく使われている手法です。このノートブックはLightGBMのベースラインからTabNetが使えるように少し変更したものです。みんなさんからアドバイスを頂けたら嬉しいです。\n\n- [Paper](https://arxiv.org/abs/1908.07442v5)\n\n# What I want to try next.\n- Version 1 :Baseline\n- Version 2 :Feature filtering (based on importance, etc.), change the validation method\n- Version 3 :Optuna tuning parameters\n- Version 4 :Ensable with LightGBM","metadata":{}},{"cell_type":"markdown","source":"## INSTALL","metadata":{}},{"cell_type":"code","source":"!pip -q install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\n!pip -q install ../input/talib-binary/talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2022-02-22T05:46:13.329792Z","iopub.execute_input":"2022-02-22T05:46:13.330059Z","iopub.status.idle":"2022-02-22T05:47:09.239645Z","shell.execute_reply.started":"2022-02-22T05:46:13.330023Z","shell.execute_reply":"2022-02-22T05:47:09.23847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport joblib\nimport random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom argparse import Namespace\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, train_test_split\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 64)\n\ndef seed_everything(seed: int = 42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-22T05:47:09.242003Z","iopub.execute_input":"2022-02-22T05:47:09.242456Z","iopub.status.idle":"2022-02-22T05:47:10.375654Z","shell.execute_reply.started":"2022-02-22T05:47:09.242414Z","shell.execute_reply":"2022-02-22T05:47:10.374927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LightGBM","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2022-02-22T05:47:10.376959Z","iopub.execute_input":"2022-02-22T05:47:10.377213Z","iopub.status.idle":"2022-02-22T05:47:12.404584Z","shell.execute_reply.started":"2022-02-22T05:47:10.37718Z","shell.execute_reply":"2022-02-22T05:47:12.403817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TabNet","metadata":{}},{"cell_type":"code","source":"from pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom pytorch_tabnet.pretraining import TabNetPretrainer","metadata":{"execution":{"iopub.status.busy":"2022-02-22T05:47:12.40663Z","iopub.execute_input":"2022-02-22T05:47:12.406872Z","iopub.status.idle":"2022-02-22T05:47:13.509042Z","shell.execute_reply.started":"2022-02-22T05:47:12.406841Z","shell.execute_reply":"2022-02-22T05:47:13.508329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## config","metadata":{}},{"cell_type":"code","source":"args = Namespace(\n    INFER=True,\n    debug=False,\n    seed=21,\n    folds=5,\n    workers=4,\n    min_time_id=None, \n    holdout=True,\n    num_bins=16,\n    data_path=Path(\"../input/ubiquant-parquet/\"),\n)\nseed_everything(args.seed)\n\nif args.debug:\n    setattr(args, 'min_time_id', 1100)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T05:47:13.510234Z","iopub.execute_input":"2022-02-22T05:47:13.511191Z","iopub.status.idle":"2022-02-22T05:47:13.517561Z","shell.execute_reply.started":"2022-02-22T05:47:13.511159Z","shell.execute_reply":"2022-02-22T05:47:13.516071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain = pd.read_parquet(args.data_path.joinpath(\"train_low_mem.parquet\"))\nassert train.isnull().any().sum() == 0, \"null exists.\"\nassert train.row_id.str.extract(r\"(?P<time_id>\\d+)_(?P<investment_id>\\d+)\").astype(train.time_id.dtype).equals(train[[\"time_id\", \"investment_id\"]]), \"row_id!=time_id_investment_id\"\n\nif args.min_time_id is not None:\n    train = train.query(\"time_id>=@args.min_time_id\").reset_index(drop=True)\n    gc.collect()\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-22T05:47:13.518745Z","iopub.execute_input":"2022-02-22T05:47:13.519224Z","iopub.status.idle":"2022-02-22T05:48:06.758614Z","shell.execute_reply.started":"2022-02-22T05:47:13.519189Z","shell.execute_reply":"2022-02-22T05:48:06.757925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# StratifiedKFold by time_span","metadata":{}},{"cell_type":"code","source":"time_id_df = (\n    train.filter(regex=r\"^(?!f_).*\")\n    .groupby(\"investment_id\")\n    .agg({\"time_id\": [\"min\", \"max\"]})\n    .reset_index()\n)\ntime_id_df[\"time_span\"] = time_id_df[\"time_id\"].diff(axis=1)[\"max\"]\ntime_id_df.head(6)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T05:48:18.736967Z","iopub.execute_input":"2022-02-22T05:48:18.737641Z","iopub.status.idle":"2022-02-22T05:48:18.946495Z","shell.execute_reply.started":"2022-02-22T05:48:18.737606Z","shell.execute_reply":"2022-02-22T05:48:18.94563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.merge(time_id_df.drop(columns=\"time_id\").droplevel(level=1, axis=1), on=\"investment_id\")\ntrain.time_span.hist(bins=args.num_bins, figsize=(16,8))\ndel time_id_df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T05:48:30.009719Z","iopub.execute_input":"2022-02-22T05:48:30.010072Z","iopub.status.idle":"2022-02-22T05:48:38.984754Z","shell.execute_reply.started":"2022-02-22T05:48:30.010039Z","shell.execute_reply":"2022-02-22T05:48:38.984006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if args.holdout:\n    _target = pd.cut(train.time_span, args.num_bins, labels=False)\n    _train, _valid = train_test_split(_target, stratify=_target)\n    print(f\"train length: {len(_train)}\", f\"holdout length: {len(_valid)}\")\n    valid = train.iloc[_valid.index].sort_values(by=[\"investment_id\", \"time_id\"]).reset_index(drop=True)\n    train = train.iloc[_train.index].sort_values(by=[\"investment_id\", \"time_id\"]).reset_index(drop=True)\n    train.time_span.hist(bins=args.num_bins, figsize=(16,8), alpha=0.8)\n    valid.time_span.hist(bins=args.num_bins, figsize=(16,8), alpha=0.8)\n    valid.drop(columns=\"time_span\").to_parquet(\"valid.parquet\")\n    del valid, _train, _valid, _target\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T05:48:43.986278Z","iopub.execute_input":"2022-02-22T05:48:43.98696Z","iopub.status.idle":"2022-02-22T05:49:15.773359Z","shell.execute_reply.started":"2022-02-22T05:48:43.986922Z","shell.execute_reply":"2022-02-22T05:49:15.772555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"fold\"] = -1\n_target = pd.cut(train.time_span, args.num_bins, labels=False)\nskf = StratifiedKFold(n_splits=args.folds)\nfor fold, (train_index, valid_index) in enumerate(skf.split(_target, _target)):\n    train.loc[valid_index, 'fold'] = fold\n    \nfig, axs = plt.subplots(nrows=args.folds, ncols=1, sharex=True, figsize=(16,8), tight_layout=True)\nfor ax, (fold, df) in zip(axs, train[[\"fold\", \"time_span\"]].groupby(\"fold\")):\n    ax.hist(df.time_span, bins=args.num_bins)\n    ax.text(0, 40000, f\"fold: {fold}, count: {len(df)}\", fontsize=16)\nplt.show()\ndel _target, train_index, valid_index\n_=gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T05:49:21.6592Z","iopub.execute_input":"2022-02-22T05:49:21.659516Z","iopub.status.idle":"2022-02-22T05:49:23.145748Z","shell.execute_reply.started":"2022-02-22T05:49:21.65948Z","shell.execute_reply":"2022-02-22T05:49:23.145026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_features = [\"investment_id\"]\nnum_features = list(train.filter(like=\"f_\").columns)\nfeatures = num_features + cat_features\n\ntrain = reduce_mem_usage(train.drop(columns=\"time_span\"))\ntrain[[\"investment_id\", \"time_id\"]] = train[[\"investment_id\", \"time_id\"]].astype(np.uint16)\ntrain[\"fold\"] = train[\"fold\"].astype(np.uint8)\ngc.collect()\nfeatures += [\"time_id\"] # https://www.kaggle.com/c/ubiquant-market-prediction/discussion/302429\nlen(features)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T05:49:27.545571Z","iopub.execute_input":"2022-02-22T05:49:27.546054Z","iopub.status.idle":"2022-02-22T05:51:50.12969Z","shell.execute_reply.started":"2022-02-22T05:49:27.54601Z","shell.execute_reply":"2022-02-22T05:51:50.128974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TabNet Train Script\n\n# !TabNet parameters are tuned in an other competition, no tuning has been done with optuna for this competition.","metadata":{}},{"cell_type":"code","source":"X = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']","metadata":{"execution":{"iopub.status.busy":"2022-02-22T05:51:54.879244Z","iopub.execute_input":"2022-02-22T05:51:54.879506Z","iopub.status.idle":"2022-02-22T05:51:57.21292Z","shell.execute_reply.started":"2022-02-22T05:51:54.879461Z","shell.execute_reply":"2022-02-22T05:51:57.21214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\nimport torch\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\nfrom sklearn.metrics import mean_squared_error\n\ndef rmse(y_true, y_pred):\n    return mean_squared_error(y_true,y_pred, squared=False)\ndef rmspe(y_true, y_pred):\n    # Function to calculate the root mean squared percentage error\n    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n\nclass RMSPE(Metric):\n    def __init__(self):\n        self._name = \"rmspe\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_score):\n        \n        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n    \n\ndef RMSPELoss(y_pred, y_true):\n    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n\n\n\ncat_idxs = [ i for i, f in enumerate(X.columns.tolist()) if f in cat_features]\n\n\ndef run():    \n    tabnet_params = dict(\n        cat_idxs=cat_idxs,\n        cat_emb_dim=1,\n        n_d = 16,\n        n_a = 16,\n        n_steps = 2,\n        gamma =1.4690246460970766,\n        n_independent = 9,\n        n_shared = 4,\n        lambda_sparse = 0,\n        optimizer_fn = Adam,\n        optimizer_params = dict(lr = (0.024907164557092944)),\n        mask_type = \"entmax\",\n        scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n        scheduler_fn = CosineAnnealingWarmRestarts,\n        seed = 42,\n        verbose = 10, \n    )    \n    y = train['target']\n    train['preds'] = -1000\n    scores = defaultdict(list)\n    features_importance= pd.DataFrame()\n    \n    for fold in range(args.folds):\n        print(f\"=====================fold: {fold}=====================\")\n        trn_ind, val_ind = train.fold!=fold, train.fold==fold\n        print(f\"train length: {trn_ind.sum()}, valid length: {val_ind.sum()}\")\n        X_train=train.loc[trn_ind, features].values\n        y_train=y.loc[trn_ind].values.reshape(-1,1)\n        X_val=train.loc[val_ind, features].values\n        y_val=y.loc[val_ind].values.reshape(-1,1)\n\n        clf =  TabNetRegressor(**tabnet_params)\n        clf.fit(\n          X_train, y_train,\n          eval_set=[(X_val, y_val)],\n          max_epochs = 355,\n          patience = 50,\n          batch_size = 1024*20, \n          virtual_batch_size = 128*20,\n          num_workers = 4,\n          drop_last = False,\n\n          )\n        \n        clf.save_model(f'TabNet_seed{args.seed}_{fold}')\n\n\n        preds = clf.predict(train.loc[val_ind, features].values)\n        train.loc[val_ind, \"preds\"] = preds\n        \n        scores[\"rmse\"].append(rmse(y.loc[val_ind], preds))\n     \n        del X_train,X_val,y_train,y_val\n        gc.collect()\n        \n        \n    print(f\"TabNet {args.folds} folds mean rmse: {np.mean(scores['rmse'])}\")\n    train.filter(regex=r\"^(?!f_).*\").to_csv(\"preds.csv\", index=False)\n #   return features_importance","metadata":{"execution":{"iopub.status.busy":"2022-02-22T05:52:03.941049Z","iopub.execute_input":"2022-02-22T05:52:03.941473Z","iopub.status.idle":"2022-02-22T05:52:03.960827Z","shell.execute_reply.started":"2022-02-22T05:52:03.941439Z","shell.execute_reply":"2022-02-22T05:52:03.959623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if args.INFER:\n    pass\nelse:\n    run()  \ndel df, train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T05:52:04.27671Z","iopub.execute_input":"2022-02-22T05:52:04.277044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare models\n- I have uploaded my trained model.","metadata":{}},{"cell_type":"code","source":"import os\nimport zipfile\n \ndef zipDir(dirpath, outFullName):\n\n    zip = zipfile.ZipFile(outFullName, \"w\", zipfile.ZIP_DEFLATED)\n    for path, dirnames, filenames in os.walk(dirpath):\n\n        fpath = path.replace(dirpath, '')\n\n        for filename in filenames:\n            zip.write(os.path.join(path, filename), os.path.join(fpath, filename))\n    zip.close()\n    \n\nif args.INFER:\n    for fold in range(1):\n        input_path =f'../input/tabnetv2/TabNet_seed{args.seed}_{fold}'\n        output_path = f\"./fold{fold}.zip\"\n        zipDir(input_path, output_path)\nelse:\n    input_path =f'./TabNet_seed{args.seed}_{fold}'\n    output_path = f\"./fold{fold}.zip\"\n\n    zipDir(input_path, output_path)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:37:28.025591Z","iopub.execute_input":"2022-01-27T10:37:28.026154Z","iopub.status.idle":"2022-01-27T10:37:28.159524Z","shell.execute_reply.started":"2022-01-27T10:37:28.026118Z","shell.execute_reply":"2022-01-27T10:37:28.158833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Infer","metadata":{}},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()  \niter_test = env.iter_test()\ntabnet_params = dict(\n        cat_idxs=cat_idxs,\n        cat_emb_dim=1,\n        n_d = 16,\n        n_a = 16,\n        n_steps = 2,\n        gamma =1.4690246460970766,\n        n_independent = 9,\n        n_shared = 4,\n        lambda_sparse = 0,\n        optimizer_fn = Adam,\n        optimizer_params = dict(lr = (0.024907164557092944)),\n        mask_type = \"entmax\",\n        scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n        scheduler_fn = CosineAnnealingWarmRestarts,\n        seed = 42,\n        verbose = 10, \n    )    \n\n\n\nimport copy\nclf =  TabNetRegressor(**tabnet_params)\nmodels = []\nfor fold in range(1):\n    clf.load_model(f\"./fold{fold}.zip\")\n    model=copy.deepcopy(clf)\n    models.append(model)\n    \n\n\n# if args.holdout:\n#     valid = pd.read_parquet(\"valid.parquet\")\n#     valid_pred = np.mean(np.stack([models[fold].predict(valid[features].values) for fold in range(args.folds)]), axis=0)\n#     print(f\"tabnet {args.folds} folds holdout rmse: {rmse(valid.target, valid_pred)}\")\n#     del valid, valid_pred\n#     gc.collect()\n\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df[\"time_id\"] = test_df.row_id.str.extract(r\"(\\d+)_.*\").astype(np.uint16) # extract time_id form row_id\n    final_pred = [models[fold].predict(test_df[features].values) for fold in range(1)]\n    sample_prediction_df['target'] = np.mean(np.stack(final_pred), axis=0)\n    env.predict(sample_prediction_df) \n    display(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T10:37:34.759708Z","iopub.execute_input":"2022-01-27T10:37:34.760182Z","iopub.status.idle":"2022-01-27T10:39:48.498441Z","shell.execute_reply.started":"2022-01-27T10:37:34.760145Z","shell.execute_reply":"2022-01-27T10:39:48.497791Z"},"trusted":true},"execution_count":null,"outputs":[]}]}