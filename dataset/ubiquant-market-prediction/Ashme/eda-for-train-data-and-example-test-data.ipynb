{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ubiquant Market Prediction EDA  \nFirst of all, please note that English is not an my official language, so it will be a poor English.  \nI read 500,000 data in this notebook, but I read all data in local environment so my comments are based on local results.  \n- Competition URL: https://www.kaggle.com/c/ubiquant-market-prediction/  \n\n## Competition Overview  \n- input data: sequential data  \n- output data: scaler(investment's return rate)  \n\n### Evaluation function  \n**Pearson corelation coefficient**  \nThe mean of the Pearson correlation coefficient for each time ID.\n\n$$ \\rho = \\frac{\\sum^{n}_{i=1}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum^{n}_{i=1}(x_i - \\bar{x})^2}\\sqrt{\\sum^{n}_{i=1}(y_i - \\bar{y})^2}} $$  \n\nMaybe, X and Y are target data and prediction data for each time ID data. 'i' is investment id(I can also think investment ID and time ID are the opposite).  \n\nFor example;\n- $X_1$: Target data with time id 1.  \n- $Y_1$: Prediction data with time id 1.  \n- $x_1$: A value with investment id 1 in target data with time id 1.  \n- $y_1$: A value with investment id 1 in prediction data with time id 1.  ","metadata":{}},{"cell_type":"markdown","source":"## Import packages  ","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport ubiquant  # Unique library of this competition\nsns.set()\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:52:31.969739Z","iopub.execute_input":"2022-01-31T20:52:31.970207Z","iopub.status.idle":"2022-01-31T20:52:33.024104Z","shell.execute_reply.started":"2022-01-31T20:52:31.970095Z","shell.execute_reply":"2022-01-31T20:52:33.023232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Dataset  ","metadata":{}},{"cell_type":"code","source":"DIR = \"../input/ubiquant-market-prediction\"\ntrain_df = pd.read_csv(os.path.join(DIR, \"train.csv\"), nrows=500000)  # read 500,000 rows(all data size is about 3,000,000)\n\nprint(f\"train data shape: {train_df.shape}\")\ndisplay(train_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:52:33.026222Z","iopub.execute_input":"2022-01-31T20:52:33.026841Z","iopub.status.idle":"2022-01-31T20:53:45.248124Z","shell.execute_reply.started":"2022-01-31T20:52:33.026777Z","shell.execute_reply":"2022-01-31T20:53:45.247283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['time_id'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:53:45.249515Z","iopub.execute_input":"2022-01-31T20:53:45.249775Z","iopub.status.idle":"2022-01-31T20:53:45.274526Z","shell.execute_reply.started":"2022-01-31T20:53:45.249743Z","shell.execute_reply":"2022-01-31T20:53:45.273652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['investment_id'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:53:45.276659Z","iopub.execute_input":"2022-01-31T20:53:45.276882Z","iopub.status.idle":"2022-01-31T20:53:45.296099Z","shell.execute_reply.started":"2022-01-31T20:53:45.276855Z","shell.execute_reply":"2022-01-31T20:53:45.295261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Memory lack\ntotal_null = 0\nfor column in train_df.columns:\n    total_null += train_df[column].isnull().sum()\n    gc.collect()\n\nprint(f\"Number of null: {total_null}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:53:45.297167Z","iopub.execute_input":"2022-01-31T20:53:45.297401Z","iopub.status.idle":"2022-01-31T20:54:21.618391Z","shell.execute_reply.started":"2022-01-31T20:53:45.297374Z","shell.execute_reply":"2022-01-31T20:54:21.617453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Infomation of train data set  \n- **non null**  \n- Number of train samples: 3,141,410  \n- Number of original features: 300(No names)  \n- Number of target: 1  \n- time_id: 0 to 1219  \n- investment_id: 0 to 3773  \n- row_id: {time_id}\\_{investment_id}  ","metadata":{}},{"cell_type":"markdown","source":"## Data types  \n- target: float  \n- features: float(all features)  \n\nIn short, this data don't include a categorical feature.  ","metadata":{}},{"cell_type":"code","source":"# Get dtypes target and features\nset(train_df.iloc[:, 3:].dtypes.values.tolist())","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:54:21.619674Z","iopub.execute_input":"2022-01-31T20:54:21.620012Z","iopub.status.idle":"2022-01-31T20:54:22.012115Z","shell.execute_reply.started":"2022-01-31T20:54:21.619978Z","shell.execute_reply":"2022-01-31T20:54:22.011188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target data distribution  \n- Target distributin is close gaussian? -> easy handle target without transform.  \n- But, there is a concentrated part in distribution.  ","metadata":{}},{"cell_type":"markdown","source":"### Plot with seaborn  ","metadata":{}},{"cell_type":"code","source":"f = plt.figure(figsize=(16, 10))\nsns.histplot(train_df['target'].values, kde=True)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:54:22.013393Z","iopub.execute_input":"2022-01-31T20:54:22.013632Z","iopub.status.idle":"2022-01-31T20:54:26.132465Z","shell.execute_reply.started":"2022-01-31T20:54:22.013602Z","shell.execute_reply":"2022-01-31T20:54:26.131488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target summary statistic  ","metadata":{}},{"cell_type":"code","source":"train_df['target'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:54:26.13364Z","iopub.execute_input":"2022-01-31T20:54:26.13387Z","iopub.status.idle":"2022-01-31T20:54:26.163008Z","shell.execute_reply.started":"2022-01-31T20:54:26.133842Z","shell.execute_reply":"2022-01-31T20:54:26.161935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features distribution  \n- Plot some feature distributions because number of feature samples about 3,000,000  \n- Feature distributions are different each other. I should transform features when training model.  ","metadata":{}},{"cell_type":"code","source":"f = plt.figure(figsize=(16, 10))\nsns.histplot(train_df['f_0'].values, kde=True)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:54:26.164488Z","iopub.execute_input":"2022-01-31T20:54:26.164713Z","iopub.status.idle":"2022-01-31T20:54:29.953727Z","shell.execute_reply.started":"2022-01-31T20:54:26.164685Z","shell.execute_reply":"2022-01-31T20:54:29.952793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = plt.figure(figsize=(16, 10))\nsns.histplot(train_df['f_58'].values, kde=True)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:54:29.956288Z","iopub.execute_input":"2022-01-31T20:54:29.95654Z","iopub.status.idle":"2022-01-31T20:54:33.447988Z","shell.execute_reply.started":"2022-01-31T20:54:29.956508Z","shell.execute_reply":"2022-01-31T20:54:33.447026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = plt.figure(figsize=(16, 10))\nsns.histplot(train_df['f_249'].values, kde=True)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:54:33.449096Z","iopub.execute_input":"2022-01-31T20:54:33.449347Z","iopub.status.idle":"2022-01-31T20:54:51.907158Z","shell.execute_reply.started":"2022-01-31T20:54:33.449318Z","shell.execute_reply":"2022-01-31T20:54:51.906196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot sequential data  \n- Sampling some investment id and plot target in chronological order.  \n- Don't know how long the time interval is, but each graph seems to be periodicity.  ","metadata":{}},{"cell_type":"code","source":"ids = [3, 28, 169]\nfor investment_id in ids:\n    tmp_df = train_df.query(\"investment_id == @investment_id\").sort_values(\"time_id\")\n\n    plt.figure(figsize=(16, 8))\n    plt.plot(tmp_df['time_id'].values, tmp_df['target'].values)\n    plt.title(f\"target data(investment_id={investment_id})\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:54:51.908411Z","iopub.execute_input":"2022-01-31T20:54:51.908643Z","iopub.status.idle":"2022-01-31T20:54:52.742888Z","shell.execute_reply.started":"2022-01-31T20:54:51.908614Z","shell.execute_reply":"2022-01-31T20:54:52.741858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analysis example test data  ","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(os.path.join(DIR, \"example_test.csv\"))\n\nprint(f\"example test data shape: {test_df.shape}\")\ndisplay(test_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:54:52.744637Z","iopub.execute_input":"2022-01-31T20:54:52.745372Z","iopub.status.idle":"2022-01-31T20:54:52.798849Z","shell.execute_reply.started":"2022-01-31T20:54:52.745316Z","shell.execute_reply":"2022-01-31T20:54:52.797986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['time_id'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:54:52.800181Z","iopub.execute_input":"2022-01-31T20:54:52.800903Z","iopub.status.idle":"2022-01-31T20:54:52.81299Z","shell.execute_reply.started":"2022-01-31T20:54:52.800849Z","shell.execute_reply":"2022-01-31T20:54:52.812081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['investment_id'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:54:52.814474Z","iopub.execute_input":"2022-01-31T20:54:52.815079Z","iopub.status.idle":"2022-01-31T20:54:52.831048Z","shell.execute_reply.started":"2022-01-31T20:54:52.815034Z","shell.execute_reply":"2022-01-31T20:54:52.830065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Information of example test data set  \n- **non null**(maybe in all test data)  \n- time_id: 1220-  \n- investment_id: Maybe, equal train data  \n\nI could't observe all test data.  ","metadata":{}},{"cell_type":"code","source":"total_null = 0\nfor column in test_df.columns:\n    total_null += test_df[column].isnull().sum()\n    gc.collect()\n\nprint(f\"Number of null: {total_null}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:54:52.832756Z","iopub.execute_input":"2022-01-31T20:54:52.83346Z","iopub.status.idle":"2022-01-31T20:57:08.825463Z","shell.execute_reply.started":"2022-01-31T20:54:52.833418Z","shell.execute_reply":"2022-01-31T20:57:08.824398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Caluculate correlation coefficient  \n- Train data is very large so narrow down the train data to caluculate correlation coefficient.  \n- Nothing has a large correlation between target and features.  \n- There seems to be large correlations between part of a features(For example, a correlation between f_4 and f228 is 9.29).  ","metadata":{}},{"cell_type":"code","source":"# It takes so many time for executing\ntrain_df = train_df.iloc[:, 3:]  # except id\ncorr = train_df.corr()\n\nf = plt.figure(figsize=(50, 50))\nsns.heatmap(corr, square=True, cmap=sns.color_palette(\"coolwarm\", 200))\n\n# plt.savefig(\"correlation_coefficient.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:57:08.826686Z","iopub.execute_input":"2022-01-31T20:57:08.826919Z","iopub.status.idle":"2022-01-31T20:59:18.122171Z","shell.execute_reply.started":"2022-01-31T20:57:08.826891Z","shell.execute_reply":"2022-01-31T20:59:18.121219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in corr.columns:\n    corr.loc[column, column] = 0  # set the diagonal values to 0\ncorr.max().describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:59:18.123786Z","iopub.execute_input":"2022-01-31T20:59:18.124202Z","iopub.status.idle":"2022-01-31T20:59:18.162242Z","shell.execute_reply.started":"2022-01-31T20:59:18.124165Z","shell.execute_reply":"2022-01-31T20:59:18.16101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr.max()[corr.max() > 0.9]","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:59:18.165133Z","iopub.execute_input":"2022-01-31T20:59:18.16573Z","iopub.status.idle":"2022-01-31T20:59:18.178593Z","shell.execute_reply.started":"2022-01-31T20:59:18.165682Z","shell.execute_reply":"2022-01-31T20:59:18.177426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## I thought about this evaluation function  \nI think the evaluation function in competitions to solve regression task is almost MAE or MSE etc. What can be understand by using the Pearson correlation coefficient for evaluation function.  \nAt first, the Pearson correlation coefficient is shown this.  \n\n$$ \\rho = \\frac{\\sum^{n}_{i=1}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum^{n}_{i=1}(x_i - \\bar{x})^2}\\sqrt{\\sum^{n}_{i=1}(y_i - \\bar{y})^2}} $$  \n\nAs I mentioned at the beginning, I think X and Y are target data and prediction data for each time ID data. 'i' is investment id.  ","metadata":{}},{"cell_type":"code","source":"# Implement honestly the Pearson correlation coefficient\ndef eval_func(x: np.ndarray, y: np.ndarray):\n    x_mean = x.mean()\n    y_mean = y.mean()\n\n    cov = ((x - x_mean) * (y - y_mean)).sum()\n    x_std = np.sqrt(((x - x_mean) ** 2).sum())\n    y_std = np.sqrt(((y - y_mean) ** 2).sum())\n\n    return cov / (x_std * y_std)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:59:18.180282Z","iopub.execute_input":"2022-01-31T20:59:18.180599Z","iopub.status.idle":"2022-01-31T20:59:18.18855Z","shell.execute_reply.started":"2022-01-31T20:59:18.180557Z","shell.execute_reply":"2022-01-31T20:59:18.187418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.arange(10)  # Temporary target variable\nY = X.copy()  # Suppose prediction and target is equal\n\nprint(f\"Pearson correlation coefficient(X = Y): {eval_func(X, Y)}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:59:18.190404Z","iopub.execute_input":"2022-01-31T20:59:18.190731Z","iopub.status.idle":"2022-01-31T20:59:18.20166Z","shell.execute_reply.started":"2022-01-31T20:59:18.190687Z","shell.execute_reply":"2022-01-31T20:59:18.200976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this way, the Pearson correlation coefficient is 1.0 if can predict perfectly.  \nNext, change values of prediction data(Y) by 1.  ","metadata":{}},{"cell_type":"code","source":"for i in range(Y.shape[0]):\n    Y[i] += 1\n    print(f\"X variance: {X.std()}, Y variance: {Y.std()}\")\n    print(f\"Pearson correlation coefficient(change value {i+1}): {eval_func(X, Y)}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:59:18.202756Z","iopub.execute_input":"2022-01-31T20:59:18.203421Z","iopub.status.idle":"2022-01-31T20:59:18.222728Z","shell.execute_reply.started":"2022-01-31T20:59:18.203382Z","shell.execute_reply":"2022-01-31T20:59:18.222054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, change values of prediction data(Y) by 2.  ","metadata":{}},{"cell_type":"code","source":"for i in range(Y.shape[0]):\n    Y[i] += 2\n    print(f\"X variance: {X.std()}, Y variance: {Y.std()}\")\n    print(f\"Pearson correlation coefficient(change value {i+1}): {eval_func(X, Y)}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-01-31T20:59:18.224017Z","iopub.execute_input":"2022-01-31T20:59:18.224368Z","iopub.status.idle":"2022-01-31T20:59:18.238941Z","shell.execute_reply.started":"2022-01-31T20:59:18.224325Z","shell.execute_reply":"2022-01-31T20:59:18.238094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Pearson correlation coefficient is 1 if prediction and target isn't equal but increase amount is equal at each point in time. In the competition description it is written that we will build a model that in this competition forecasts an investment's return rate, but I think we should focus increase amount in this competition since the pearson correlation coefficient is 1 even if each mean isn't equal, if each increase amount is equal at each point in time.  \n\nPlease see for reference only and give me various opinion and your knowledge because I'm just a student, neither a statistician nor a machine learning expert.  \nIn addition, I would be grateful if you could tell me other things that should be added to EDA. Thank you for watching!  ","metadata":{}}]}