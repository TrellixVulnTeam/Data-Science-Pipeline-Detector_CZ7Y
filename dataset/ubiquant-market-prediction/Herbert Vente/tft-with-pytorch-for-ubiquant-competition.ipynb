{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMPORTING DATA ","metadata":{}},{"cell_type":"code","source":"#!pip install --upgrade pip\n!pip install pytorch_forecasting\n#0.9.2\n\nimport os\nimport time\nimport torch\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom pickle import dump\nfrom pickle import load\nimport dask.dataframe as dd\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_forecasting.data import GroupNormalizer\nfrom pytorch_forecasting.metrics import QuantileLoss\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom sklearn.feature_selection import mutual_info_regression\nfrom pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\nfrom pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters    ","metadata":{"execution":{"iopub.status.busy":"2022-04-20T19:42:28.084267Z","iopub.execute_input":"2022-04-20T19:42:28.085089Z","iopub.status.idle":"2022-04-20T19:42:37.7213Z","shell.execute_reply.started":"2022-04-20T19:42:28.08505Z","shell.execute_reply":"2022-04-20T19:42:37.720392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# make_mi_scores \njust calculate the mutual information from all the variables in the X dataset with our target y.","metadata":{}},{"cell_type":"code","source":"def make_mi_scores(X, y):\n    mi_scores = mutual_info_regression(X, y)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n","metadata":{"execution":{"iopub.status.busy":"2022-04-20T05:11:38.026939Z","iopub.execute_input":"2022-04-20T05:11:38.027231Z","iopub.status.idle":"2022-04-20T05:11:38.035873Z","shell.execute_reply.started":"2022-04-20T05:11:38.027193Z","shell.execute_reply":"2022-04-20T05:11:38.034685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# You could use this function...\nto see the principal components importance visually","metadata":{}},{"cell_type":"code","source":"def plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs","metadata":{"execution":{"iopub.status.busy":"2022-04-20T05:11:38.039435Z","iopub.execute_input":"2022-04-20T05:11:38.039745Z","iopub.status.idle":"2022-04-20T05:11:38.052399Z","shell.execute_reply.started":"2022-04-20T05:11:38.039711Z","shell.execute_reply":"2022-04-20T05:11:38.051721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading the data from parquet files and first processing\nadding the number of time_id values per investment_id\nto use as weight on the model or enconder variable, to keep processing","metadata":{}},{"cell_type":"code","source":"def process_data_1(case):\n\n    files = [os.path.join(dirname,file) for dirname,_,files in os.walk(\"/kaggle/input\") for file in files]\n\n    names = [file for file in files if \"investment_ids\" in file][:20]\n\n    #num_files = [int(file.split(\"/\")[-1].split(\".\")[0]) for file in names]\n\n    #dic = {k:v for k,v in zip(num_files,names)}\n\n    #files = [dic[k] for k in range(50) if k in dic.keys()]\n    \n    if case==\"a\":\n        \n        df = pd.concat([pd.read_parquet(file) for file in names \n\n                            if pd.read_parquet(file).shape[0] >= 500])\n    \n    else:\n        \n        df = pd.read_parquet([file for file in files if \"train_low_mem\" in file][0])\n    \n    print(df.shape)\n    \n    #value_counts = df.investment_id.value_counts()\n    \n    #ids = value_counts[value_counts>500]\n    \n    #df = df[df.investment_id.isin(ids.index)]\n    \n    x = pd.DataFrame()\n    \n    x[\"time_id_counts_by_investment\"] = df.groupby(\"investment_id\").time_id.transform(\"count\")\n    \n    df = df.join(x)\n    \n    return df\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-20T19:42:40.406522Z","iopub.execute_input":"2022-04-20T19:42:40.406861Z","iopub.status.idle":"2022-04-20T19:42:40.41445Z","shell.execute_reply.started":"2022-04-20T19:42:40.406812Z","shell.execute_reply":"2022-04-20T19:42:40.413837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef process_data_2():\n    \n    train = process_data_1()\n    \n    print(train)\n    \n    extra = []\n\n    for id in train.investment_id.unique():\n\n        fourier = CalendarFourier(freq=\"11m\", order=5)\n\n        df_1 = train.loc[train.investment_id==id]\n\n        dp = DeterministicProcess(\n        index=df_1.index,additional_terms=[fourier]\n        )\n\n        df_det = dp.in_sample()\n        \n        #pca_2 = PCA(n_components=10)\n        \n        #X = pca_2.fit_transform(df_det)\n        \n        #X = pd.DataFrame(columns=[f\"sinuoidal_col_{num}\" for num in range(1,11)],data=X,index=df_det.index)\n\n        #plot_periodogram(df_1.target)\n\n        #plt.show()\n\n        #df_det.mean(axis=1).plot()\n\n        #df_1.target.plot()\n\n        #plt.show()\n\n        df_1 = df_1.merge(df_det,left_index=True,right_index=True)\n\n        extra.append(df_1)\n        \n    train = pd.concat(extra)\n        \n    train[\"investment_id\"] = train[\"investment_id\"].apply(lambda x: str(x))\n    \n    train[\"row_id\"] = train[\"row_id\"].apply(lambda x: str(x))\n    \n    train.index = np.arange(1,len(train)+1)\n    \n    return train","metadata":{"execution":{"iopub.status.busy":"2022-04-20T05:11:38.064523Z","iopub.execute_input":"2022-04-20T05:11:38.06498Z","iopub.status.idle":"2022-04-20T05:11:38.07456Z","shell.execute_reply.started":"2022-04-20T05:11:38.064944Z","shell.execute_reply":"2022-04-20T05:11:38.073976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_3():\n    \n    case = \"b\"\n    \n    df = process_data_1(case)\n    \n    if case==\"a\":\n        \n        q = df\n        \n    else:\n        \n        q = df.iloc[int(2e+5):int(4e+5)]\n    \n    mi_scores = make_mi_scores(q[[col for col in q.columns if not col in [\"row_id\",\"target\",\"investment_id\",\"time_id\",\"time_id_counts_by_investment\"]]],q.target)\n    \n    c_extra = mi_scores.head(50).index.to_list()\n    \n    c_extra_2 = [\"target\",\"investment_id\",\"time_id\",\"time_id_counts_by_investment\"]\n    \n    \n    columns = c_extra_2+c_extra\n    \n    scaler = MinMaxScaler((0,1))\n    \n    X = scaler.fit_transform(df[c_extra])\n    \n    pca = PCA(n_components=10)\n    \n    X = pca.fit_transform(X)\n    \n    print(pca.explained_variance_ratio_)\n    \n    plot_variance(pca)\n    \n    pca_c = [f\"col_{a+1}\" for a in range(X.shape[1])]\n    \n    train = pd.DataFrame(columns=pca_c,data=X,index=df.index)\n    \n    kmean = KMeans(n_clusters=15,n_init=10,algorithm=\"elkan\")\n    \n    df = df[c_extra_2].join(train)\n    \n    df[\"target\"] = df.target\n    \n    df[\"Cluster\"] = kmean.fit_predict(train)\n    \n    X_cd = kmean.transform(train)\n    \n    centroid_c = [f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n    \n    X_cd = pd.DataFrame(X_cd,\n                        columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])],\n                        index=df.index)\n    \n    df = df.join(X_cd)\n    \n    df[\"Cluster\"] = df.Cluster.astype(\"object\")\n    \n    df[\"investment_id_str\"] = df.investment_id.apply(str)\n    \n    df[\"time_id_counts_by_investment_str\"] = df[\"time_id_counts_by_investment\"].apply(str)\n    \n    df[\"time_id\"]=df.time_id.apply(int)\n    \n    return df, mi_scores, pca, scaler, kmean","metadata":{"execution":{"iopub.status.busy":"2022-04-20T05:11:38.075805Z","iopub.execute_input":"2022-04-20T05:11:38.076277Z","iopub.status.idle":"2022-04-20T05:11:38.09049Z","shell.execute_reply.started":"2022-04-20T05:11:38.076241Z","shell.execute_reply":"2022-04-20T05:11:38.089703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1m\")\n    freqencies, spectrum = periodogram(\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104,8760,525600/5])\n    ax.set_xticklabels(\n        [\n            \"Annual (1)\",\n            \"Semiannual (2)\",\n            \"Quarterly (4)\",\n            \"Bimonthly (6)\",\n            \"Monthly (12)\",\n            \"Biweekly (26)\",\n            \"Weekly (52)\",\n            \"Semiweekly (104)\",\n            \"hourly (8760)\",\n            \"every five minutes (105120)\"\n        ],\n        rotation=30,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax\n","metadata":{"execution":{"iopub.status.busy":"2022-04-20T05:11:38.091842Z","iopub.execute_input":"2022-04-20T05:11:38.092401Z","iopub.status.idle":"2022-04-20T05:11:38.102563Z","shell.execute_reply.started":"2022-04-20T05:11:38.092353Z","shell.execute_reply":"2022-04-20T05:11:38.101636Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def substitute_process(df=pd.DataFrame()):\n    \n    if df.shape[0] == 0:\n        \n        train = process_data_1(\"b\")\n        \n    else:\n        \n        train = df\n        \n        dicc_time_id = load(open(\"/kaggle/input/utilities-model/dicc_time_id.pkl\",\"rb\"))\n        \n        train[\"time_id_counts_by_investment\"] = train.investment_id.apply(lambda x: dicc_time_id[x])\n    \n    if \"target\" in train:\n        \n        c_extra_2 = [\"target\",\"investment_id\",\"time_id\",\"time_id_counts_by_investment\"]\n        \n    else:\n        c_extra_2 = [\"investment_id\",\"time_id\",\"time_id_counts_by_investment\"]\n\n    mi_scores = load(open(\"/kaggle/input/utilities-model/mi_scores.pkl\",\"rb\"))\n\n    pca = load(open(\"/kaggle/input/utilities-model/pca.pkl\",\"rb\"))\n\n    scaler = load(open(\"/kaggle/input/utilities-model/MinMaxScaler.pkl\",\"rb\"))\n    \n    kmean = load(open(\"/kaggle/input/utilities-model/kmean_2.pkl\",\"rb\"))\n    \n    mee = load(open(\"/kaggle/input/utilities-model/meestimateencoder.pkl\",\"rb\"))\n        \n    scaler_d = scaler.transform(train[mi_scores.head(50).index])\n\n    pca_d = pca.transform(scaler_d)\n\n    pca_c = [f\"col_{a+1}\" for a in range(pca_d.shape[1])]\n\n    pca_d = pd.DataFrame(data=pca_d,columns=pca_c,index=train.index)\n    \n    pca_d = train[c_extra_2].join(pca_d)\n    \n    pca_d[\"Cluster\"] = kmean.predict(pca_d[pca_c].astype(np.float32))\n    \n    X_cd = kmean.transform(pca_d[pca_c])\n\n    centroid_c = [f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n\n    X_cd = pd.DataFrame(data=X_cd,columns=centroid_c,index=train.index)\n\n    pca_d = pca_d.join(X_cd)\n\n    pca_d[\"investment_id_str\"] = pca_d.investment_id.apply(str)\n\n    pca_d[\"time_id_counts_by_investment_str\"] = pca_d[\"time_id_counts_by_investment\"].apply(str)        \n    \n    pca_d[\"weight_investment\"] = pca_d.investment_id.apply(lambda x: mee[x]) \n    \n    return pca_d","metadata":{"execution":{"iopub.status.busy":"2022-04-20T21:37:19.968446Z","iopub.execute_input":"2022-04-20T21:37:19.968721Z","iopub.status.idle":"2022-04-20T21:37:19.981266Z","shell.execute_reply.started":"2022-04-20T21:37:19.968693Z","shell.execute_reply":"2022-04-20T21:37:19.980199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = substitute_process()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T19:42:46.733971Z","iopub.execute_input":"2022-04-20T19:42:46.734266Z","iopub.status.idle":"2022-04-20T19:43:39.411993Z","shell.execute_reply.started":"2022-04-20T19:42:46.734239Z","shell.execute_reply":"2022-04-20T19:43:39.410562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"train, mi_scores, pca, scaler, kmean = process_3()\n\ndump(mi_scores,open(\"mi_scores.pkl\",\"wb\"))\n\ndump(pca,open(\"pca.pkl\",\"wb\"))\n\ndump(train,open(\"data_processed.pkl\",\"wb\"))\n\ndump(scaler,open(\"MinMaxScaler.pkl\",\"wb\"))\n\ndump(kmean,open(\"kmean.pkl\",\"wb\"))\n\ndicc_time_id = {k:v for k,v in train[[\"investment_id\",\"time_id_counts_by_investment\"]].value_counts().index}\n\ndump(dicc_time_id,open(\"dicc_time_id.pkl\",\"wb\"))\n\nencoder = MEstimateEncoder(cols=[\"investment_id_str\"],m=5)\n\nencoder.fit(test[[col for col in test.columns if col not in [\"target\",\"weight_investment\"]]],test[\"target\"])\n\ntrain[\"weight_investment\"] = encoder.transform(train[[col for col in test.columns if col not in [\"target\",\"weight_investment\"]]], train[\"target\"]).investment_id_str\n\nq = train[[\"weight_investment\",\"investment_id\"]].value_counts().index\n\nq = {k:v for k,v in q}\n\ndump(q,open(\"meestimateencoder.pkl\",\"wb\"))\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-04-20T05:12:28.71874Z","iopub.execute_input":"2022-04-20T05:12:28.718999Z","iopub.status.idle":"2022-04-20T05:12:28.726652Z","shell.execute_reply.started":"2022-04-20T05:12:28.718959Z","shell.execute_reply":"2022-04-20T05:12:28.725922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train,test = train_test_split(train,test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T19:44:03.637621Z","iopub.execute_input":"2022-04-20T19:44:03.638159Z","iopub.status.idle":"2022-04-20T19:44:07.404224Z","shell.execute_reply.started":"2022-04-20T19:44:03.638114Z","shell.execute_reply":"2022-04-20T19:44:07.403389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = [col for col in train.columns if \"f_\" in col or \"Cen\" in col]\n\ncosines = [col for col in train.columns if \"cos\" in col or \"sin\" in col]","metadata":{"execution":{"iopub.status.busy":"2022-04-20T19:44:07.405991Z","iopub.execute_input":"2022-04-20T19:44:07.406199Z","iopub.status.idle":"2022-04-20T19:44:12.88994Z","shell.execute_reply.started":"2022-04-20T19:44:07.406174Z","shell.execute_reply":"2022-04-20T19:44:12.889092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"time_id\"] = train.time_id.astype(\"int\")\n\ntrain[\"Cluster\"] = train.Cluster.astype(\"str\")","metadata":{"execution":{"iopub.status.busy":"2022-04-20T19:44:12.891558Z","iopub.execute_input":"2022-04-20T19:44:12.891768Z","iopub.status.idle":"2022-04-20T19:44:14.175381Z","shell.execute_reply.started":"2022-04-20T19:44:12.891742Z","shell.execute_reply":"2022-04-20T19:44:14.174418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training = TimeSeriesDataSet(\n    train,\n    time_idx = \"time_id\",\n    target=\"target\",\n    group_ids = [\"investment_id_str\",\"Cluster\"],\n    min_encoder_length = 0,\n    max_encoder_length = 20,\n    min_prediction_length= 1,\n    #lags= {\"target\":[a for a in range(1,5)]},\n    max_prediction_length = 50,\n    static_categoricals = [\"investment_id_str\",\"Cluster\"],\n    static_reals = [\"weight_investment\"],\n    weight=\"time_id_counts_by_investment\",\n    time_varying_known_reals = [\"time_id\"]+columns,\n    time_varying_unknown_reals = [\"target\"],\n    #target_normalizer= GroupNormalizer(groups=[\"Cluster\",\"investment_id_str\"],transformation=\"softplus\"),\n    add_relative_time_idx=True,\n    add_target_scales=True,\n    add_encoder_length=True,\n    allow_missing_timesteps=True\n    )","metadata":{"execution":{"iopub.status.busy":"2022-04-20T19:44:14.17708Z","iopub.execute_input":"2022-04-20T19:44:14.177308Z","iopub.status.idle":"2022-04-20T19:44:57.171022Z","shell.execute_reply.started":"2022-04-20T19:44:14.177277Z","shell.execute_reply":"2022-04-20T19:44:57.170098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation = TimeSeriesDataSet.from_dataset(training,train,\n                                            predict=True,\n                                            stop_randomization=True\n                                           )\n\nbatch_size = 1500  # set this between 32 to 128\ntrain_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\nval_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T19:44:57.173098Z","iopub.execute_input":"2022-04-20T19:44:57.173412Z","iopub.status.idle":"2022-04-20T19:45:30.424188Z","shell.execute_reply.started":"2022-04-20T19:44:57.173371Z","shell.execute_reply":"2022-04-20T19:45:30.423228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pl.seed_everything(42)\ntrainer = pl.Trainer(\n    #gpus = 1,\n    #tpu_cores=8,\n    max_epochs=1,\n    sync_batchnorm=True,\n    #auto_scale_batch_size=\"binsearch\",\n    #limit_train_batches=500,\n    # clipping gradients is a hyperparameter and important to prevent divergance\n    # of the gradient for recurrent neural networks\n    gradient_clip_val=0.1,\n)\n\n\ntft = TemporalFusionTransformer.from_dataset(\n    training,\n    # not meaningful for finding the learning rate but otherwise very important\n    #n_trials=100,\n    learning_rate=0.000213756,\n    #batch_size = 256,\n    lstm_layers=1,\n    hidden_size=32,  # most important hyperparameter apart from learning rate\n    # number of attention heads. Set to up to 4 for large datasets\n    attention_head_size=5,\n    dropout=0.1,  # between 0.1 and 0.3 are good values\n    hidden_continuous_size=16,  # set to <= hidden_size\n    output_size=9,  # 7 quantiles by default\n    loss=QuantileLoss([0.001,0.05,0.1,0.25,0.5,0.75,0.9,0.95,0.99]),\n    # reduce learning rate if no improvement in validation loss after x epochs\n    #reduce_on_plateau_patience=4,\n)\nprint(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-20T19:45:30.754915Z","iopub.execute_input":"2022-04-20T19:45:30.755146Z","iopub.status.idle":"2022-04-20T19:45:31.056336Z","shell.execute_reply.started":"2022-04-20T19:45:30.755118Z","shell.execute_reply":"2022-04-20T19:45:31.055372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tft.summarize()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T05:21:10.370075Z","iopub.execute_input":"2022-04-20T05:21:10.37063Z","iopub.status.idle":"2022-04-20T05:21:10.404723Z","shell.execute_reply.started":"2022-04-20T05:21:10.37059Z","shell.execute_reply":"2022-04-20T05:21:10.403898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tft.load_state_dict(torch.load(\"/kaggle/input/utilities-model/preliminar_model.pth\",map_location=torch.device(\"cpu\")))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T19:47:42.571285Z","iopub.execute_input":"2022-04-20T19:47:42.571578Z","iopub.status.idle":"2022-04-20T19:47:42.698206Z","shell.execute_reply.started":"2022-04-20T19:47:42.571546Z","shell.execute_reply":"2022-04-20T19:47:42.697394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"res = trainer.tuner.lr_find(\n    tft,\n    train_dataloaders=train_dataloader,\n    val_dataloaders=val_dataloader,\n    max_lr=1,\n    min_lr=0.001,\n)\n\nprint(f\"suggested learning rate: {res.suggestion()}\")\nfig = res.plot(show=True, suggest=True)\nfig.show()\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-04-20T05:13:58.032888Z","iopub.execute_input":"2022-04-20T05:13:58.033154Z","iopub.status.idle":"2022-04-20T05:13:58.038429Z","shell.execute_reply.started":"2022-04-20T05:13:58.033119Z","shell.execute_reply":"2022-04-20T05:13:58.037768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"import gc\ngc.collect(generation=0)\ntorch.cuda.empty_cache()\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-04-20T05:13:58.039825Z","iopub.execute_input":"2022-04-20T05:13:58.040296Z","iopub.status.idle":"2022-04-20T05:13:58.049682Z","shell.execute_reply.started":"2022-04-20T05:13:58.04026Z","shell.execute_reply":"2022-04-20T05:13:58.049003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"import tensorflow as tf\nimport tensorboard as tb\n\ntf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n\ntrainer.fit(\n        tft,\n        train_dataloaders=train_dataloader,\n        val_dataloaders=val_dataloader,\n    )\n\ntorch.save(tft.state_dict(),\"preliminar_model.pth\")\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-04-20T05:23:39.189611Z","iopub.execute_input":"2022-04-20T05:23:39.189875Z","iopub.status.idle":"2022-04-20T08:03:50.71413Z","shell.execute_reply.started":"2022-04-20T05:23:39.189837Z","shell.execute_reply":"2022-04-20T08:03:50.713373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ntesting = TimeSeriesDataSet(\n    test,\n    time_idx = \"time_id\",\n    target=\"target\",\n    group_ids = [\"investment_id_str\",\"Cluster\"],\n    min_encoder_length = 0,\n    max_encoder_length = 20,\n    min_prediction_length= 1,\n    #lags= {\"target\":[a for a in range(1,5)]},\n    max_prediction_length = 50,\n    static_categoricals = [\"investment_id_str\",\"Cluster\"],\n    static_reals = [\"weight_investment\"],\n    weight=\"time_id_counts_by_investment\",\n    time_varying_known_reals = [\"time_id\"]+columns,\n    time_varying_unknown_reals = [\"target\"],\n    #target_normalizer= GroupNormalizer(groups=[\"Cluster\",\"investment_id_str\"],transformation=\"softplus\"),\n    add_relative_time_idx=True,\n    add_target_scales=True,\n    add_encoder_length=True,\n    allow_missing_timesteps=True\n    )\ntesting = TimeSeriesDataSet.from_dataset(testing,test,\n                                            predict=True,\n                                            stop_randomization=True\n                                           )\n\ntest_dataloader = testing.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-04-20T19:55:40.646054Z","iopub.execute_input":"2022-04-20T19:55:40.646979Z","iopub.status.idle":"2022-04-20T19:55:41.815312Z","shell.execute_reply.started":"2022-04-20T19:55:40.646938Z","shell.execute_reply":"2022-04-20T19:55:41.814367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"params = {k:v for k,v in testing.get_parameters().items() if not \"target\" in k}\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-04-20T20:09:29.188956Z","iopub.execute_input":"2022-04-20T20:09:29.189356Z","iopub.status.idle":"2022-04-20T20:09:29.193663Z","shell.execute_reply.started":"2022-04-20T20:09:29.189327Z","shell.execute_reply":"2022-04-20T20:09:29.192758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"!dir /kaggle/input\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-04-20T20:17:47.205563Z","iopub.execute_input":"2022-04-20T20:17:47.205898Z","iopub.status.idle":"2022-04-20T20:17:48.108253Z","shell.execute_reply.started":"2022-04-20T20:17:47.205862Z","shell.execute_reply":"2022-04-20T20:17:48.10703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"outside_data = pd.read_parquet(\"/kaggle/input/ubiquant-parquet/example_test.parquet\")\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-04-20T21:09:17.291613Z","iopub.execute_input":"2022-04-20T21:09:17.29192Z","iopub.status.idle":"2022-04-20T21:09:17.327177Z","shell.execute_reply.started":"2022-04-20T21:09:17.291884Z","shell.execute_reply":"2022-04-20T21:09:17.326305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"outside_data = substitute_process(df=outside_data)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-04-20T21:38:37.799659Z","iopub.execute_input":"2022-04-20T21:38:37.800383Z","iopub.status.idle":"2022-04-20T21:38:37.8599Z","shell.execute_reply.started":"2022-04-20T21:38:37.80034Z","shell.execute_reply":"2022-04-20T21:38:37.858893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"outside_data[\"target\"] = 0\noutside_data[\"Cluster\"] = outside_data.Cluster.apply(str)\noutside_data[\"time_id\"] = outside_data.time_id.apply(int)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-04-20T21:43:17.820782Z","iopub.execute_input":"2022-04-20T21:43:17.821103Z","iopub.status.idle":"2022-04-20T21:43:17.830485Z","shell.execute_reply.started":"2022-04-20T21:43:17.821067Z","shell.execute_reply":"2022-04-20T21:43:17.829435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"testing_2 = TimeSeriesDataSet(\n    outside_data,\n    time_idx = \"time_id\",\n    target=\"target\",\n    group_ids = [\"investment_id_str\",\"Cluster\"],\n    min_encoder_length = 0,\n    max_encoder_length = 20,\n    min_prediction_length= 1,\n    #lags= {\"target\":[a for a in range(1,5)]},\n    max_prediction_length = 3,\n    static_categoricals = [\"investment_id_str\",\"Cluster\"],\n    static_reals = [\"weight_investment\"],\n    weight=\"time_id_counts_by_investment\",\n    time_varying_known_reals = [\"time_id\"]+columns,\n    time_varying_unknown_reals = [\"target\"],\n    #target_normalizer= GroupNormalizer(groups=[\"Cluster\",\"investment_id_str\"],transformation=\"softplus\"),\n    add_relative_time_idx=True,\n    add_target_scales=True,\n    add_encoder_length=True,\n    allow_missing_timesteps=True\n    )\ntesting_2 = TimeSeriesDataSet.from_dataset(testing_2,outside_data,\n                                            predict=True,\n                                            stop_randomization=True\n                                           )\n\ntest_2_dataloader = testing.to_dataloader(train=True, batch_size=1, num_workers=0)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:35:21.633781Z","iopub.execute_input":"2022-04-20T22:35:21.634147Z","iopub.status.idle":"2022-04-20T22:35:21.81726Z","shell.execute_reply.started":"2022-04-20T22:35:21.634112Z","shell.execute_reply":"2022-04-20T22:35:21.816105Z"},"trusted":true},"execution_count":null,"outputs":[]}]}