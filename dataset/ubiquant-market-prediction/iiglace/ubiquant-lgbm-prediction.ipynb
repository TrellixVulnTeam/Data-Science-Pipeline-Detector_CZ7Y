{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport joblib\nimport random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom argparse import Namespace\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, train_test_split\n\n\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 64)\n\ndef seed_everything(seed: int = 42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)#生成指定随机数\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)#使实验可复现\n    \n# 从int8-int64占用内存逐渐增大，float16-float64同理。\n#考察数据集中每一列的最大值与最小值，为每一列分配合适的数据类型，减少内存占用\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-13T04:37:43.283813Z","iopub.execute_input":"2022-04-13T04:37:43.28416Z","iopub.status.idle":"2022-04-13T04:37:43.314587Z","shell.execute_reply.started":"2022-04-13T04:37:43.28408Z","shell.execute_reply":"2022-04-13T04:37:43.313741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"args = Namespace(\n    debug=False,\n    seed=21,\n    folds=5,\n    workers=4,\n    min_time_id=None, \n    holdout=True,\n    num_bins=16,\n    data_path=Path(\"../input/ubiquant-parquet/\"),\n)\nseed_everything(args.seed)\n\nif args.debug:\n    setattr(args, 'min_time_id', 1100)#args.min_time_id=1100","metadata":{"execution":{"iopub.status.busy":"2022-01-24T03:18:07.260262Z","iopub.execute_input":"2022-01-24T03:18:07.26085Z","iopub.status.idle":"2022-01-24T03:18:07.26764Z","shell.execute_reply.started":"2022-01-24T03:18:07.260803Z","shell.execute_reply":"2022-01-24T03:18:07.26677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cell代码运行一次所花费的时间\n%%time\ntrain = pd.read_parquet(args.data_path.joinpath(\"train_low_mem.parquet\"))\nassert train.isnull().any().sum() == 0, \"null exists.\"\nassert train.row_id.str.extract(r\"(?P<time_id>\\d+)_(?P<investment_id>\\d+)\").astype(train.time_id.dtype).equals(train[[\"time_id\", \"investment_id\"]]), \"row_id!=time_id_investment_id\"\n\nif args.min_time_id is not None:\n    train = train.query(\"time_id>=@args.min_time_id\").reset_index(drop=True)\n    gc.collect()\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-24T03:18:07.268845Z","iopub.execute_input":"2022-01-24T03:18:07.269546Z","iopub.status.idle":"2022-01-24T03:18:27.003875Z","shell.execute_reply.started":"2022-01-24T03:18:07.269502Z","shell.execute_reply":"2022-01-24T03:18:27.002882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# StratifiedKFold by time_span: [discussion](https://www.kaggle.com/c/ubiquant-market-prediction/discussion/302429)","metadata":{}},{"cell_type":"code","source":"time_id_df = (\n    train.filter(regex=r\"^(?!f_).*\")#过滤掉f_开头\n    .groupby(\"investment_id\")#基于行操作，按investment_id进行分类\n    .agg({\"time_id\": [\"min\", \"max\"]})#基于列操作，找到最大最小值\n    .reset_index()#重置索引\n)\ntime_id_df[\"time_span\"] = time_id_df[\"time_id\"].diff(axis=1)[\"max\"]#time_span=max-min\ntime_id_df.head(6)#显示前六行","metadata":{"execution":{"iopub.status.busy":"2022-01-24T03:18:27.937332Z","iopub.execute_input":"2022-01-24T03:18:27.937676Z","iopub.status.idle":"2022-01-24T03:18:27.984503Z","shell.execute_reply.started":"2022-01-24T03:18:27.937633Z","shell.execute_reply":"2022-01-24T03:18:27.983552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.merge(time_id_df.drop(columns=\"time_id\").droplevel(level=1, axis=1), on=\"investment_id\")#按investment_id合并time_id_df到训练集，在它删除time_id中max这一列后\ntrain.time_span.hist(bins=args.num_bins, figsize=(16,8))#显示训练集时间span直方图，16个竖条，宽16，高8\ndel time_id_df#删除变量\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T03:18:27.985825Z","iopub.execute_input":"2022-01-24T03:18:27.986084Z","iopub.status.idle":"2022-01-24T03:18:29.210899Z","shell.execute_reply.started":"2022-01-24T03:18:27.98605Z","shell.execute_reply":"2022-01-24T03:18:29.20994Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if args.holdout:\n    _target = pd.cut(train.time_span, args.num_bins, labels=False)#数据离散化，只返回time_span中的数据在哪个bin\n    _train, _valid = train_test_split(_target, stratify=_target)#分离训练集和验证集，stratify保持类的分布\n    print(f\"train length: {len(_train)}\", f\"holdout length: {len(_valid)}\")\n    valid = train.iloc[_valid.index].sort_values(by=[\"investment_id\", \"time_id\"]).reset_index(drop=True)#通过行号取行数据并排序\n    train = train.iloc[_train.index].sort_values(by=[\"investment_id\", \"time_id\"]).reset_index(drop=True)\n    train.time_span.hist(bins=args.num_bins, figsize=(16,8), alpha=0.8)#alpha透明度，训练集的直方图\n    valid.time_span.hist(bins=args.num_bins, figsize=(16,8), alpha=0.8)#测试集的直方图\n    valid.drop(columns=\"time_span\").to_parquet(\"valid.parquet\")\n    del valid, _train, _valid, _target\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T03:18:29.212581Z","iopub.execute_input":"2022-01-24T03:18:29.212806Z","iopub.status.idle":"2022-01-24T03:18:33.775788Z","shell.execute_reply.started":"2022-01-24T03:18:29.21278Z","shell.execute_reply":"2022-01-24T03:18:33.774939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"fold\"] = -1\n_target = pd.cut(train.time_span, args.num_bins, labels=False)#数据离散化，只返回time_span中的数据在哪个bin\nskf = StratifiedKFold(n_splits=args.folds)#5折交叉验证，对划分的训练集和测试集保留每个类别的样本百分比\nfor fold, (train_index, valid_index) in enumerate(skf.split(_target, _target)):\n    train.loc[valid_index, 'fold'] = fold#按索引取数据\n    \nfig, axs = plt.subplots(nrows=args.folds, ncols=1, sharex=True, figsize=(16,8), tight_layout=True)#绘制子图，5行1列\nfor ax, (fold, df) in zip(axs, train[[\"fold\", \"time_span\"]].groupby(\"fold\")):\n    ax.hist(df.time_span, bins=args.num_bins)#直方图\n    ax.text(0, 40000, f\"fold: {fold}, count: {len(df)}\", fontsize=16)\nplt.show()\ndel _target, train_index, valid_index\n_=gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T03:18:33.777561Z","iopub.execute_input":"2022-01-24T03:18:33.7786Z","iopub.status.idle":"2022-01-24T03:18:34.920124Z","shell.execute_reply.started":"2022-01-24T03:18:33.778549Z","shell.execute_reply":"2022-01-24T03:18:34.919083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_features = [\"investment_id\"]\nnum_features = list(train.filter(like=\"f_\").columns)#数字特征，如f_\nfeatures = num_features + cat_features\n\ntrain = reduce_mem_usage(train.drop(columns=\"time_span\"))#减少内存使用\ntrain[[\"investment_id\", \"time_id\"]] = train[[\"investment_id\", \"time_id\"]].astype(np.uint16)#将数据类型转化为16位无符号整型\ntrain[\"fold\"] = train[\"fold\"].astype(np.uint8)#将数据类型转化为8位无符号整型\ngc.collect()\nfeatures += [\"time_id\"] \nlen(features)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T03:18:34.921673Z","iopub.execute_input":"2022-01-24T03:18:34.921979Z","iopub.status.idle":"2022-01-24T03:18:54.684002Z","shell.execute_reply.started":"2022-01-24T03:18:34.921937Z","shell.execute_reply":"2022-01-24T03:18:54.683098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# corr_matrix = train.filter(like=\"f_\").corr().abs()\n# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# # Find features with correlation greater than 0.97\n# to_drop = [column for column in upper.columns if any(upper[column] >= 0.97)]\n# sorted(to_drop)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T03:18:54.685533Z","iopub.execute_input":"2022-01-24T03:18:54.685815Z","iopub.status.idle":"2022-01-24T03:18:54.690083Z","shell.execute_reply.started":"2022-01-24T03:18:54.685783Z","shell.execute_reply":"2022-01-24T03:18:54.689259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#均根方误差\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\n# 自定义损失函数\ndef feval_rmse(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'rmse', rmse(y_true, y_pred), False\n\n#皮尔逊系数\ndef feval_pearsonr(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'pearsonr', pearsonr(y_true, y_pred)[0], True\n\ndef run():    \n    params = {\n        'learning_rate':0.05,\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        'boosting_type': \"gbdt\",\n        'verbosity': -1,\n        'n_jobs': -1, \n        'seed': args.seed,\n        'lambda_l1': 2.7223413643193285e-08, \n        'lambda_l2': 0.009462714717237544, \n        'num_leaves': 108, \n        'feature_fraction': 0.5298125662824026, \n        'bagging_fraction': 0.7279540797730281, \n        'bagging_freq': 6, \n        'max_depth': 10, \n        'max_bin': 487, \n        'min_data_in_leaf': 158,\n        'n_estimators': 1000, \n    }\n    \n    y = train['target']\n    train['preds'] = -1000\n    scores = defaultdict(list)#字典，通过键值对来存取，可为不存在的键值返回一个默认值\n    features_importance= pd.DataFrame()#创建重要特征数据集\n    \n    #每一次迭代完都调用feval_rmse，传入验证数据集的此轮迭代的预测值与验证数据集\n    for fold in range(args.folds):\n        print(f\"=====================fold: {fold}=====================\")\n        trn_ind, val_ind = train.fold!=fold, train.fold==fold\n        print(f\"train length: {trn_ind.sum()}, valid length: {val_ind.sum()}\")\n        train_dataset = lgb.Dataset(train.loc[trn_ind, features], y.loc[trn_ind], categorical_feature=cat_features)#按标签取数据\n        valid_dataset = lgb.Dataset(train.loc[val_ind, features], y.loc[val_ind], categorical_feature=cat_features)\n\n        model = lgb.train(\n            params,\n            train_set = train_dataset, \n            valid_sets = [train_dataset, valid_dataset], \n            verbose_eval=100,\n            early_stopping_rounds=50,\n            feval = feval_pearsonr\n        )\n        joblib.dump(model, f'lgbm_seed{args.seed}_{fold}.pkl')#序列化对象\n\n        preds = model.predict(train.loc[val_ind, features])\n        train.loc[val_ind, \"preds\"] = preds\n        \n        scores[\"rmse\"].append(rmse(y.loc[val_ind], preds))\n        scores[\"pearsonr\"].append(pearsonr(y.loc[val_ind], preds)[0])\n        \n        fold_importance_df= pd.DataFrame({'feature': features, 'importance': model.feature_importance(), 'fold': fold})#字典创建，列分别为feature、importance、fold\n        features_importance = pd.concat([features_importance, fold_importance_df], axis=0)#沿着纵坐标将多个对象堆叠在一起\n        \n        del train_dataset, valid_dataset, model\n        gc.collect()\n    print(f\"lgbm {args.folds} folds mean rmse: {np.mean(scores['rmse'])}, mean pearsonr: {np.mean(scores['pearsonr'])}\")\n    train.filter(regex=r\"^(?!f_).*\").to_csv(\"preds.csv\", index=False)\n    return features_importance","metadata":{"execution":{"iopub.status.busy":"2022-01-24T03:18:54.691383Z","iopub.execute_input":"2022-01-24T03:18:54.69217Z","iopub.status.idle":"2022-01-24T03:18:54.712054Z","shell.execute_reply.started":"2022-01-24T03:18:54.692133Z","shell.execute_reply":"2022-01-24T03:18:54.711049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_importance = run()\ndf = train[[\"target\", \"preds\"]].query(\"preds!=-1000\")#对数据框进行挑选行的操作\nprint(f\"lgbm {args.folds} folds mean rmse: {rmse(df.target, df.preds)}, mean pearsonr: {pearsonr(df.target, df.preds)[0]}\")\ndel df, train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T03:18:54.713503Z","iopub.execute_input":"2022-01-24T03:18:54.713971Z","iopub.status.idle":"2022-01-24T03:33:26.236322Z","shell.execute_reply.started":"2022-01-24T03:18:54.713931Z","shell.execute_reply":"2022-01-24T03:33:26.235368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfolds_mean_importance = (\n    features_importance.groupby(\"feature\")\n    .importance.mean()\n    .reset_index()\n    .sort_values(by=\"importance\", ascending=False)\n)\nfeatures_importance.to_csv(\"features_importance.csv\", index=False)\nfolds_mean_importance.to_csv(\"folds_mean_feature_importance.csv\", index=False)\n\n#绘制头部重要特征、尾部重要特征条形图\nplt.figure(figsize=(16, 10))\nplt.subplot(1,2,1)#1代表行，2代表列，1代表此时绘制第1个图\nsns.barplot(x=\"importance\", y=\"feature\", data=folds_mean_importance.head(50))#条形图，横纵坐标\nplt.title(f'Head LightGBM Features (avg over {args.folds} folds)')\nplt.subplot(1,2,2)#1代表行，2代表列，2代表此时绘制第2个图\nsns.barplot(x=\"importance\", y=\"feature\", data=folds_mean_importance.tail(50))\nplt.title(f'Tail LightGBM Features (avg over {args.folds} folds)')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T03:33:26.238747Z","iopub.execute_input":"2022-01-24T03:33:26.238969Z","iopub.status.idle":"2022-01-24T03:33:28.151875Z","shell.execute_reply.started":"2022-01-24T03:33:26.238942Z","shell.execute_reply":"2022-01-24T03:33:28.150854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()  \niter_test = env.iter_test()\n\nmodels = [joblib.load(f'lgbm_seed{args.seed}_{fold}.pkl') for fold in range(args.folds)]\nif args.holdout:\n    valid = pd.read_parquet(\"valid.parquet\")#读文件\n    valid_pred = np.mean(np.stack([models[fold].predict(valid[features]) for fold in range(args.folds)]), axis=0)#对各列求均值\n    print(f\"lgbm {args.folds} folds holdout rmse: {rmse(valid.target, valid_pred)}, holdout pearsonr: {pearsonr(valid.target, valid_pred)[0]}\")\n    del valid, valid_pred\n    gc.collect()\n\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df[\"time_id\"] = test_df.row_id.str.extract(r\"(\\d+)_.*\").astype(np.uint16) # extract time_id form row_id\n    final_pred = [models[fold].predict(test_df[features]) for fold in range(args.folds)]\n    sample_prediction_df['target'] = np.mean(np.stack(final_pred), axis=0)#对各列求均值\n    env.predict(sample_prediction_df) \n    display(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T03:33:28.153301Z","iopub.execute_input":"2022-01-24T03:33:28.153532Z","iopub.status.idle":"2022-01-24T03:33:53.180073Z","shell.execute_reply.started":"2022-01-24T03:33:28.153504Z","shell.execute_reply":"2022-01-24T03:33:53.179261Z"},"trusted":true},"execution_count":null,"outputs":[]}]}