{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport random\nimport lightgbm\nfrom scipy.stats import probplot, pearsonr\nfrom sklearn.model_selection import StratifiedKFold, TimeSeriesSplit\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import LinearRegression\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\nimport ubiquant\nfrom scipy.stats import pearsonr\nimport tensorflow.keras.layers as L\nfrom tensorflow.python.ops import math_ops\nimport tensorflow.keras.models as M\nimport tensorflow.keras.backend as K","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-19T12:52:37.239843Z","iopub.execute_input":"2022-04-19T12:52:37.240189Z","iopub.status.idle":"2022-04-19T12:52:44.987221Z","shell.execute_reply.started":"2022-04-19T12:52:37.240103Z","shell.execute_reply":"2022-04-19T12:52:44.986496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_low = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet', engine='pyarrow') \ntrain_low.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T12:52:44.988778Z","iopub.execute_input":"2022-04-19T12:52:44.989023Z","iopub.status.idle":"2022-04-19T12:53:54.519156Z","shell.execute_reply.started":"2022-04-19T12:52:44.988988Z","shell.execute_reply":"2022-04-19T12:53:54.518383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_col = train_low.drop(['row_id','time_id','investment_id','target'],axis=1).columns\nf_col","metadata":{"execution":{"iopub.status.busy":"2022-04-19T12:53:54.520554Z","iopub.execute_input":"2022-04-19T12:53:54.520998Z","iopub.status.idle":"2022-04-19T12:53:55.743878Z","shell.execute_reply.started":"2022-04-19T12:53:54.520958Z","shell.execute_reply":"2022-04-19T12:53:55.743134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def anomaly_detect(df_train):\n    outlier_list = []\n    outlier_col = []\n\n    for col in f_col :\n\n        temp_df = df_train[(df_train[col] > df_train[col].mean() + df_train[col].std() * 70) |\n                           (df_train[col] < df_train[col].mean() - df_train[col].std() * 70) ]\n        temp2_df = df_train[(df_train[col] > df_train[col].mean() + df_train[col].std() * 35) |\n                            (df_train[col] < df_train[col].mean() - df_train[col].std() * 35) ]\n        if len(temp_df) >0 : \n            outliers = temp_df.index.to_list()\n            outlier_list.extend(outliers)\n            outlier_col.append(col)\n            print(col, len(temp_df))\n        elif len(temp2_df)>0 and len(temp2_df) <6 :\n            outliers = temp2_df.index.to_list()\n            outlier_list.extend(outliers)\n            outlier_col.append(col)\n            print(col, len(temp2_df))\n\n    outlier_list = list(set(outlier_list))\n    print(len(outlier_col), len(outlier_list))\n    return outlier_col, outlier_list\n\noutlier_col, outlier_list = anomaly_detect(train_low)\ntrain_low.drop(train_low.index[outlier_list], inplace = True)\ntrain_low","metadata":{"execution":{"iopub.status.busy":"2022-04-19T12:53:55.74613Z","iopub.execute_input":"2022-04-19T12:53:55.746617Z","iopub.status.idle":"2022-04-19T12:54:16.439947Z","shell.execute_reply.started":"2022-04-19T12:53:55.746578Z","shell.execute_reply":"2022-04-19T12:54:16.439275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(pd.DataFrame(train_low['investment_id']))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-19T12:54:16.443812Z","iopub.execute_input":"2022-04-19T12:54:16.445786Z","iopub.status.idle":"2022-04-19T12:54:16.488384Z","shell.execute_reply.started":"2022-04-19T12:54:16.445749Z","shell.execute_reply":"2022-04-19T12:54:16.487764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_dataset(df):\n    inv_df = df['investment_id']\n    f_df = df[f_col]\n    scaled_investment_id = scaler.transform(pd.DataFrame(inv_df))\n    df['investment_id'] = scaled_investment_id\n    data_x = pd.concat([df['investment_id'], f_df], axis=1)\n    return data_x\n\ndef make_dataset_train(df):\n    f_df = df[f_col]\n    data_x = pd.concat([df['investment_id'], f_df], axis=1)\n    return data_x","metadata":{"execution":{"iopub.status.busy":"2022-04-19T12:54:16.492159Z","iopub.execute_input":"2022-04-19T12:54:16.494075Z","iopub.status.idle":"2022-04-19T12:54:16.503013Z","shell.execute_reply.started":"2022-04-19T12:54:16.494038Z","shell.execute_reply":"2022-04-19T12:54:16.501185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = train_low.astype('float16')\ndf_x = make_dataset(df)\ndf_x","metadata":{"execution":{"iopub.status.busy":"2022-04-19T12:54:16.504247Z","iopub.execute_input":"2022-04-19T12:54:16.504689Z","iopub.status.idle":"2022-04-19T12:54:29.94551Z","shell.execute_reply.started":"2022-04-19T12:54:16.504654Z","shell.execute_reply":"2022-04-19T12:54:29.944714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_y = pd.DataFrame(df['target'])\ndf_y","metadata":{"execution":{"iopub.status.busy":"2022-04-19T12:54:29.947014Z","iopub.execute_input":"2022-04-19T12:54:29.947776Z","iopub.status.idle":"2022-04-19T12:54:29.960547Z","shell.execute_reply.started":"2022-04-19T12:54:29.947736Z","shell.execute_reply":"2022-04-19T12:54:29.959765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df","metadata":{"execution":{"iopub.status.busy":"2022-04-19T12:54:29.962075Z","iopub.execute_input":"2022-04-19T12:54:29.962324Z","iopub.status.idle":"2022-04-19T12:54:30.017957Z","shell.execute_reply.started":"2022-04-19T12:54:29.962292Z","shell.execute_reply":"2022-04-19T12:54:30.017238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_rnn_v2():\n    f300_in = L.Input(shape = [301], name='301 feature input')\n    x = L.BatchNormalization(name='batch_norm1')(f300_in)\n    x = L.Dense(256, activation='swish', name='dense1')(x)\n    x = L.Dropout(0.1, name='dropout1')(x)\n    x = L.Reshape((1, -1), name='reshape1')(x)\n    x = L.BatchNormalization(name='batch_norm2')(x)\n    x = L.LSTM(128, dropout=0.3, recurrent_dropout=0.3, return_sequences=True, activation='relu', name='lstm1')(x)\n    x = L.LSTM(16, dropout=0.1, return_sequences=False, activation='relu', name='lstm2')(x)\n    output_layer = L.Dense(1, name='output')(x)\n\n    model = M.Model([f300_in], \n                    [output_layer])\n    \n    learning_sch = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate = 0.003,\n        decay_steps = 9700,\n        decay_rate = 0.98\n    )\n    adam = tf.keras.optimizers.Adam(learning_rate = learning_sch)\n    rmse = tf.keras.metrics.RootMeanSquaredError()\n    model.compile(optimizer=adam, loss='mse', metrics=[rmse])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-19T12:54:30.020745Z","iopub.execute_input":"2022-04-19T12:54:30.021147Z","iopub.status.idle":"2022-04-19T12:54:30.032267Z","shell.execute_reply.started":"2022-04-19T12:54:30.021107Z","shell.execute_reply":"2022-04-19T12:54:30.031495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_gru():\n    inp = tf.keras.layers.Input(shape = [df_x.shape[1]], name = \"input_layer\")\n    x = tf.keras.layers.Dense(256, activation = \"swish\")(inp)\n    x = tf.keras.layers.Dense(256, activation = \"swish\")(x)\n    x = tf.keras.layers.Dense(256, activation = \"swish\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Reshape((1, -1))(x)\n    x = tf.keras.layers.GRU(128, recurrent_dropout = 0.2, dropout = 0.2, return_sequences = True)(x)\n    x = tf.keras.layers.GRU(128, recurrent_dropout = 0.1, dropout = 0.1, return_sequences = True)(x)\n    x = tf.keras.layers.GRU(128, recurrent_dropout = 0.1, dropout = 0.1, return_sequences = False)(x)\n    x = tf.keras.layers.Dense(16, activation = \"swish\")(x)\n    x = tf.keras.layers.Dense(16, activation = \"swish\")(x)\n    x = tf.keras.layers.Dense(16, activation = \"swish\")(x)\n    out = tf.keras.layers.Dense(1, name = \"output_layer\")(x)\n    \n    model = tf.keras.Model(inp, out)\n    \n    learning_sch = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate = 0.003,\n        decay_steps = 9700,\n        decay_rate = 0.98\n    )\n    adam = tf.keras.optimizers.Adam(learning_rate = learning_sch)\n    rmse = tf.keras.metrics.RootMeanSquaredError()\n    model.compile(optimizer=adam, loss='mse', metrics=[rmse])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-19T12:54:30.033606Z","iopub.execute_input":"2022-04-19T12:54:30.033938Z","iopub.status.idle":"2022-04-19T12:54:30.046822Z","shell.execute_reply.started":"2022-04-19T12:54:30.033902Z","shell.execute_reply":"2022-04-19T12:54:30.046008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pythonash_model():\n    inputs_ = tf.keras.Input(shape = [301])\n    x = tf.keras.layers.Dense(64, kernel_initializer = 'he_normal')(inputs_)\n    batch = tf.keras.layers.BatchNormalization()(x)\n    leaky = tf.keras.layers.LeakyReLU(0.1)(batch)\n    \n    x = tf.keras.layers.Dense(128, kernel_initializer = 'he_normal')(leaky)\n    batch = tf.keras.layers.BatchNormalization()(x)\n    leaky = tf.keras.layers.LeakyReLU(0.1)(batch)\n    \n    x = tf.keras.layers.Dense(256, kernel_initializer = 'he_normal')(leaky)\n    batch = tf.keras.layers.BatchNormalization()(x)\n    leaky = tf.keras.layers.LeakyReLU(0.1)(batch)\n    \n    x = tf.keras.layers.Dense(512, kernel_initializer = 'he_normal')(leaky)\n    batch = tf.keras.layers.BatchNormalization()(x)\n    leaky = tf.keras.layers.LeakyReLU(0.1)(batch)\n    \n    x = tf.keras.layers.Dense(256, kernel_initializer = 'he_normal')(leaky)\n    batch = tf.keras.layers.BatchNormalization()(x)\n    leaky = tf.keras.layers.LeakyReLU(0.1)(batch)\n    drop = tf.keras.layers.Dropout(0.4)(leaky)\n    \n    x = tf.keras.layers.Dense(128, kernel_initializer = 'he_normal')(drop)\n    batch = tf.keras.layers.BatchNormalization()(x)\n    leaky = tf.keras.layers.LeakyReLU(0.1)(batch)\n    \n    x = tf.keras.layers.Dense(8, kernel_initializer = 'he_normal')(leaky)\n    batch = tf.keras.layers.BatchNormalization()(x)\n    leaky = tf.keras.layers.LeakyReLU(0.1)(batch)\n    drop = tf.keras.layers.Dropout(0.4)(leaky)\n    \n    outputs_ = tf.keras.layers.Dense(1)(drop)\n    \n    model = tf.keras.Model(inputs = inputs_, outputs = outputs_)\n    \n    rmse = tf.keras.metrics.RootMeanSquaredError()\n\n    learning_sch = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate = 0.003,\n    decay_steps = 9700,\n    decay_rate = 0.98)\n    adam = tf.keras.optimizers.Adam(learning_rate = learning_sch)\n    \n    model.compile(loss = 'mse', metrics = rmse, optimizer = adam)\n    return model\n\npythonash_model().summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T12:54:30.048373Z","iopub.execute_input":"2022-04-19T12:54:30.048617Z","iopub.status.idle":"2022-04-19T12:54:32.496167Z","shell.execute_reply.started":"2022-04-19T12:54:30.048586Z","shell.execute_reply":"2022-04-19T12:54:32.494836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dnn_inv():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-19T12:54:32.498322Z","iopub.execute_input":"2022-04-19T12:54:32.498567Z","iopub.status.idle":"2022-04-19T12:54:32.509365Z","shell.execute_reply.started":"2022-04-19T12:54:32.498533Z","shell.execute_reply":"2022-04-19T12:54:32.508685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tf.keras.utils.plot_model(pythonash_model(),show_shapes=True,expand_nested=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T12:54:32.510765Z","iopub.execute_input":"2022-04-19T12:54:32.511018Z","iopub.status.idle":"2022-04-19T12:54:32.52071Z","shell.execute_reply.started":"2022-04-19T12:54:32.510983Z","shell.execute_reply":"2022-04-19T12:54:32.520015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment_id_lookup_layer = L.IntegerLookup(max_tokens=train_low.investment_id.nunique())\ninvestment_id_lookup_layer.adapt(pd.DataFrame({\"investment_ids\":train_low.investment_id}))","metadata":{"execution":{"iopub.status.busy":"2022-04-19T12:54:32.522149Z","iopub.execute_input":"2022-04-19T12:54:32.522389Z","iopub.status.idle":"2022-04-19T12:56:54.702609Z","shell.execute_reply.started":"2022-04-19T12:54:32.522356Z","shell.execute_reply":"2022-04-19T12:56:54.70171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def correlationMetric(x, y, axis=-2):\n    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xvar * yvar)\n    return tf.constant(1.0, dtype=x.dtype) - corr\n\n\ndef correlationLoss(x,y, axis=-2):\n    \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n    while trying to have the same mean and variance\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xsqsum * ysqsum)\n    return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr ) , dtype=tf.float32 )\n\ndef correlationMetric_01mse(x, y, axis=-2):\n    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xvar * yvar)\n    return tf.constant(1.0, dtype=x.dtype) - corr\n\ngc.collect()\n\n# list(GroupKFold(5).split(train , groups = train.index))[0]\ndef pearson_coef(data):\n    return data.corr()['target']['preds']\n\ndef evaluate_metric(valid_df):\n    return np.mean(valid_df[['time_id_', 'target', 'preds']].groupby('time_id').apply(pearson_coef))\n\n\ndef get_model_corr(ft_units, x_units, x_dropout):\n    \n    # investment_id\n    inputs = tf.keras.Input((301, ), dtype=tf.float16)\n    investment_id_x = investment_id_lookup_layer(inputs[:,0:1])\n    investment_id_x = L.Embedding(train_low.investment_id.nunique(), 32, input_length=1)(investment_id_x)\n    investment_id_x = L.Reshape((-1, ))(investment_id_x)\n    investment_id_x = L.Dense(128, activation='swish')(investment_id_x)\n    investment_id_x = L.Dense(128, activation='swish')(investment_id_x) \n    investment_id_x = L.Dense(128, activation='swish')(investment_id_x)\n    \n    bn = tf.keras.layers.BatchNormalization()(inputs[:,1:])\n    gn = tf.keras.layers.GaussianNoise(0.035)(bn)\n    feature_x = L.Dense(300, activation='swish')(gn)\n    feature_x = tf.keras.layers.Dropout(0.5)(feature_x)\n    \n    for hu in ft_units:\n        feature_x = L.Dense(hu, activation='swish')(feature_x)\n        feature_x = tf.keras.layers.Dropout(0.35)(feature_x)\n    \n    x = L.Concatenate(axis=1)([investment_id_x, feature_x])\n    \n    for i in range(len(x_units)):\n        x = tf.keras.layers.Dense(x_units[i], kernel_regularizer=\"l2\")(x) \n        x = tf.keras.layers.Activation('swish')(x)\n        x = tf.keras.layers.Dropout(x_dropout[i])(x)\n        \n    output = L.Dense(1)(x)\n    model = tf.keras.Model(inputs=[inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.0001), loss=correlationLoss, \n                  metrics=['mse', \"mae\", correlationMetric])\n    return model\n\n\nparams = {\n    'ft_units': [150, 75, 150 ,200],\n    'x_units': [512, 256, 128, 32],\n    'x_dropout': [0.44, 0.4, 0.33, 0.2] #4, 3, 2, 1\n         }","metadata":{"execution":{"iopub.status.busy":"2022-04-19T12:56:54.704001Z","iopub.execute_input":"2022-04-19T12:56:54.70425Z","iopub.status.idle":"2022-04-19T12:56:55.026444Z","shell.execute_reply.started":"2022-04-19T12:56:54.704218Z","shell.execute_reply":"2022-04-19T12:56:55.025706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import clear_output\n\n# def fit_1fold(get_model_func, train_index, val_index, fold_i, model_name, params=None):\n#     print(f\"{get_model_func} fold = {fold_i}\")\n#     epochs = 20\n#     train_x, train_y = df_x.iloc[train_index], df_y.iloc[train_index]\n#     tf_train = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n    \n#     if params:\n#         model = get_model_func(**params)\n#     else:\n#         model = get_model_func()\n\n    \n#     if val_index is not None:\n#         val_x, val_y = df_x.iloc[val_index], df_y.iloc[val_index]\n#         tf_val = tf.data.Dataset.from_tensor_slices((val_x, val_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n#         epochs = 50\n        \n#         callbacks = [\n#             tf.keras.callbacks.ModelCheckpoint(f'{model_name}_{fold_i}.h5', save_best_only = True),\n#             tf.keras.callbacks.EarlyStopping(min_delta=0.0001, patience=7, verbose=1)\n#         ]\n#         model.fit(tf_train, callbacks=callbacks, epochs=epochs, validation_data=(tf_val), shuffle=True)\n#     else:\n#         callbacks = [\n#             tf.keras.callbacks.ModelCheckpoint(f'{model_name}_{fold_i}.h5', save_best_only=True, monitor='loss'),\n#             tf.keras.callbacks.EarlyStopping(min_delta=0.0001, patience=7, verbose=1, monitor='loss')\n#         ]\n#         model.fit(tf_train, callbacks=callbacks, epochs=epochs, shuffle=True)\n\n#     clear_output()\n    \n#     del tf_train\n    \n#     if val_index is not None:\n#         del tf_val\n#     del model\n#     gc.collect()\n    \n\n# kfold_generator = TimeSeriesSplit(max_train_size=int(df_x.shape[0] * 0.6))\n\n# i = 0\n# train_val_ids = list(kfold_generator.split(df_x, df_y))\n# train_val_ids.append((np.arange(int(df_x.shape[0] * 0.41), df_x.shape[0]), None))\n\n# # for train_index, val_index in train_val_ids:\n# #     fit_1fold(get_model_corr, train_index, val_index, i, 'dnn_corr', params)\n# #     i += 1\n\n\n# gc.collect()\n\n# for train_index, val_index in train_val_ids:\n#     fit_1fold(get_gru, train_index, val_index, i, 'gru')\n#     i += 1\n\n\n# gc.collect()\n\n# for train_index, val_index in train_val_ids:\n#     fit_1fold(pythonash_model, train_index, val_index, i, 'pythonash_model')\n#     i += 1\n\n\n# gc.collect()\n\n# i = 0\n# for train_index, val_index in train_val_ids:\n#     fit_1fold(get_rnn_v2, train_index, val_index, i, 'rnn')\n#     i += 1","metadata":{"execution":{"iopub.status.busy":"2022-04-19T12:56:55.027792Z","iopub.execute_input":"2022-04-19T12:56:55.028233Z","iopub.status.idle":"2022-04-19T12:56:55.035302Z","shell.execute_reply.started":"2022-04-19T12:56:55.028198Z","shell.execute_reply":"2022-04-19T12:56:55.034686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_val_ids\n# ","metadata":{"execution":{"iopub.status.busy":"2022-04-19T12:56:55.036324Z","iopub.execute_input":"2022-04-19T12:56:55.036766Z","iopub.status.idle":"2022-04-19T12:56:55.046175Z","shell.execute_reply.started":"2022-04-19T12:56:55.036729Z","shell.execute_reply":"2022-04-19T12:56:55.045531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = ubiquant.make_env()   \niter_test = env.iter_test()   \ncoefs = [0, 0, 0, 0.1, 0.15, 0.75]\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df_corr = make_dataset_train(test_df)\n    test_df = make_dataset(test_df)\n    pred_dnn = None\n    pred_rnn = None\n    pred_gru = None\n    pred_dnn_corr = None\n    for i in range(3, 6):\n        model = tf.keras.models.load_model(f'../input/ubmodels/pythonash_model_{i+6}.h5')\n        if pred_dnn is None:\n            pred_dnn = coefs[i] * model.predict(test_df)\n        else: \n            pred_dnn += coefs[i] * model.predict(test_df)\n            \n        model = tf.keras.models.load_model(f'../input/ubmodels/rnn_{i}.h5')\n        if pred_rnn is None:\n            pred_rnn = coefs[i] * model.predict(test_df)\n        else: \n            pred_rnn += coefs[i] * model.predict(test_df)\n            \n#         model = tf.keras.models.load_model(f'../input/ubmodels/gru_{i}.h5')    \n#         if pred_gru is None:\n#             pred_gru = coefs[i] * model.predict(test_df)\n#         else: \n#             pred_gru += coefs[i] * model.predict(test_df)\n        \n\n        model = tf.keras.models.load_model(\n            f'../input/ubmodels/dnn_corr_{i}.tf',\n            custom_objects={\n                'correlationMetric':correlationMetric,\n                'correlationLoss': correlationLoss\n            })        \n        if pred_dnn_corr is None:\n            pred_dnn_corr = coefs[i] * model.predict(test_df_corr)\n        else: \n            pred_dnn_corr += coefs[i] * model.predict(test_df_corr)\n            \n    sample_prediction_df['target'] = pred_dnn_corr*0.5 + pred_dnn*0.3 + pred_rnn*0.2\n    env.predict(sample_prediction_df)\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T12:56:55.047359Z","iopub.execute_input":"2022-04-19T12:56:55.048824Z","iopub.status.idle":"2022-04-19T12:56:56.09197Z","shell.execute_reply.started":"2022-04-19T12:56:55.048787Z","shell.execute_reply":"2022-04-19T12:56:56.090502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}