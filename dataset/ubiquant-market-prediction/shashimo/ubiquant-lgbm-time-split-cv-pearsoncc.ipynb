{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0.Import libraries and pickle file","metadata":{}},{"cell_type":"markdown","source":"- The original data is too large to load, so we will load the Pickle file created in [this note](https://www.kaggle.com/shashimo/ubiquant-how-to-make-pickle-file).","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport pickle\nimport lightgbm as lgb\nimport numpy as np\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\nimport gc\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-03-02T14:04:06.317881Z","iopub.execute_input":"2022-03-02T14:04:06.318426Z","iopub.status.idle":"2022-03-02T14:04:08.124739Z","shell.execute_reply.started":"2022-03-02T14:04:06.318312Z","shell.execute_reply":"2022-03-02T14:04:08.123921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/ubiquanttrainpicklefile/train.pickle', 'rb') as f:\n    train = pickle.load(f)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T14:04:13.556995Z","iopub.execute_input":"2022-03-02T14:04:13.557601Z","iopub.status.idle":"2022-03-02T14:04:30.0719Z","shell.execute_reply.started":"2022-03-02T14:04:13.557556Z","shell.execute_reply":"2022-03-02T14:04:30.071028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['investment_id'] = train['investment_id'].astype('category')\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T14:04:32.990198Z","iopub.execute_input":"2022-03-02T14:04:32.990804Z","iopub.status.idle":"2022-03-02T14:04:33.103032Z","shell.execute_reply.started":"2022-03-02T14:04:32.990752Z","shell.execute_reply":"2022-03-02T14:04:33.102251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.Check dataset","metadata":{}},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-02T13:02:58.577342Z","iopub.execute_input":"2022-03-02T13:02:58.57766Z","iopub.status.idle":"2022-03-02T13:02:58.585775Z","shell.execute_reply.started":"2022-03-02T13:02:58.57763Z","shell.execute_reply":"2022-03-02T13:02:58.58481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T09:55:42.591896Z","iopub.execute_input":"2022-03-02T09:55:42.592264Z","iopub.status.idle":"2022-03-02T09:55:42.636538Z","shell.execute_reply.started":"2022-03-02T09:55:42.592223Z","shell.execute_reply":"2022-03-02T09:55:42.635309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train['time_id'].unique())","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:47:32.128769Z","iopub.execute_input":"2022-03-02T12:47:32.129132Z","iopub.status.idle":"2022-03-02T12:47:32.159956Z","shell.execute_reply.started":"2022-03-02T12:47:32.129093Z","shell.execute_reply":"2022-03-02T12:47:32.159228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.Feature engineering","metadata":{}},{"cell_type":"code","source":"def feature_eng(df,features):\n    df['mean'] = df[features].mean(axis=1)\n    #df['median'] = df[features].median(axis=1)\n    #df['q01'] = df[features].quantile(q=0.01, axis=1)\n    #df['q05'] = df[features].quantile(q=0.05, axis=1)\n    #df['q10'] = df[features].quantile(q=0.10, axis=1)\n    #df['q25'] = df[features].quantile(q=0.25, axis=1)\n    #df['q75'] = df[features].quantile(q=0.75, axis=1)\n    #df['q90'] = df[features].quantile(q=0.90, axis=1)\n    #df['q95'] = df[features].quantile(q=0.95, axis=1)\n    #df['q99'] = df[features].quantile(q=0.99, axis=1)\n    #df['max'] = df[features].max(axis=1)\n    #df['min'] = df[features].min(axis=1)\n    gc.collect()\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.Pearson correlation coefficient","metadata":{}},{"cell_type":"markdown","source":"- Evaluation metrics of this competition is the mean of the Pearson correlation coefficient(PCC) for each time ID.\n- Prepare here to calculate score of PCC.","metadata":{}},{"cell_type":"code","source":"def pcc_score(t,x,y):\n    _t_df = pd.DataFrame(t)\n    _t_df.reset_index(drop=True,inplace=True)\n    _x_df = pd.DataFrame(x)\n    _x_df.reset_index(drop=True,inplace=True)\n    _y_df = pd.DataFrame(y)\n    _y_df.reset_index(drop=True,inplace=True)\n    _calc_df=pd.concat([_t_df,_x_df],axis=1)\n    _calc_df['pred'] = _y_df\n    _pccs=[]\n    test=[]\n    for i,j in enumerate(_calc_df['time_id'].unique()):\n        _tmp=_calc_df.loc[_calc_df['time_id']==j,['target']]\n        _tmp2=_calc_df.loc[_calc_df['time_id']==j,['pred']]\n        _pcc=pcc(_tmp.target,_tmp2.pred)\n        test.append(j)\n        _pccs.append(_pcc)\n    return np.mean(_pccs)\n\ndef pcc(x,y):\n    x_diff = x - np.mean(x)\n    y_diff = y - np.mean(y)\n    return np.dot(x_diff, y_diff) / (np.sqrt(sum(x_diff ** 2)) * np.sqrt(sum(y_diff ** 2)))","metadata":{"execution":{"iopub.status.busy":"2022-03-02T14:09:15.55489Z","iopub.execute_input":"2022-03-02T14:09:15.555146Z","iopub.status.idle":"2022-03-02T14:09:15.564117Z","shell.execute_reply.started":"2022-03-02T14:09:15.55512Z","shell.execute_reply":"2022-03-02T14:09:15.563307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.LGBM with TimeSeriesSplit Cross-Validation","metadata":{}},{"cell_type":"markdown","source":"- TimeSeriesSplit can't exactly split 'time_id' without leaks in multiple time series (like this competition), but I guess it is one of the best options that are easily available.","metadata":{}},{"cell_type":"code","source":"train.drop(['row_id'], axis=1, inplace=True)\ny=train.target\nx=train.drop('target',axis=1)\ndel train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T14:04:41.28367Z","iopub.execute_input":"2022-03-02T14:04:41.284415Z","iopub.status.idle":"2022-03-02T14:04:49.227245Z","shell.execute_reply.started":"2022-03-02T14:04:41.284378Z","shell.execute_reply":"2022-03-02T14:04:49.226336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"splits=5\ncv = TimeSeriesSplit(n_splits=splits)\nscores_rmse=[]\nscores_pcc=[]\nmodels = []\ncategorical_features = ['investment_id']\nfeats = [f for f in x.columns if f not in ['time_id','investment_id']]\ngc.collect()\n\nx=feature_eng(x,feats)\ngc.collect()\n\nfor fold, (train_idx, val_idx) in enumerate(cv.split(x)):\n    train_x,val_x = x.iloc[train_idx], x.iloc[val_idx]\n    train_y,val_y = y.iloc[train_idx], y.iloc[val_idx]\n    print(f'Start Fold:{fold}')\n    #print(f'time_id of train_x is{train_x.time_id.unique()}') #check time_id\n    #print(f'time_id of val_x is{val_x.time_id.unique()}')\n    train_x.drop(['time_id'], axis=1, inplace=True)\n    val_time=val_x.time_id\n    val_x.drop(['time_id'], axis=1, inplace=True)\n    print(f'size of train_x is{train_x.shape}',f'size of val_x is{val_x.shape}')\n    \n    model=lgb.LGBMRegressor(random_state=0,learning_rate=0.05,n_estimators=10000)\n    \n    model.fit(train_x,train_y,eval_set=[(val_x,val_y),(train_x,train_y)],\n              categorical_feature='auto',verbose=20,eval_metric='rmse',early_stopping_rounds=10)\n\n    pred=model.predict(val_x)\n    score_rmse=np.sqrt(mean_squared_error(val_y,pred))\n    score_pcc=pcc_score(val_time,val_y,pred)\n    scores_rmse.append(score_rmse)\n    scores_pcc.append(score_pcc)\n    models.append(model)\n    print(f'Score of Fold{fold} is RMSE:{score_rmse}, PCC:{score_pcc}')\n    print('*'*80)\n    gc.collect()\n\nprint(f'Result RMSE:{np.mean(scores_rmse)}, PCC:{np.mean(scores_pcc)}')","metadata":{"execution":{"iopub.status.busy":"2022-03-02T14:09:19.157237Z","iopub.execute_input":"2022-03-02T14:09:19.157824Z","iopub.status.idle":"2022-03-02T14:11:02.070222Z","shell.execute_reply.started":"2022-03-02T14:09:19.157762Z","shell.execute_reply":"2022-03-02T14:11:02.068203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.Submit","metadata":{}},{"cell_type":"code","source":"def inference(models,df):\n    y_preds = []\n    df['investment_id'] = df['investment_id'].astype('category')\n    df=feature_eng(df,feats)\n    for model in models:\n        y_pred = model.predict(df)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_cols = ['investment_id']\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\nfeatures = test_cols + features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test()\nfor (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df['target'] = inference(models,test_df[features])\n    env.predict(sample_prediction_df) ","metadata":{"execution":{"iopub.status.busy":"2022-03-01T15:37:55.15958Z","iopub.execute_input":"2022-03-01T15:37:55.159896Z","iopub.status.idle":"2022-03-01T15:37:55.16622Z","shell.execute_reply.started":"2022-03-01T15:37:55.159848Z","shell.execute_reply":"2022-03-01T15:37:55.165476Z"},"trusted":true},"execution_count":null,"outputs":[]}]}