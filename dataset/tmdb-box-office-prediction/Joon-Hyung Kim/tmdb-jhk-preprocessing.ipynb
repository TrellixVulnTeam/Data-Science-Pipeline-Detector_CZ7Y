{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TMDB prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport ast\nimport json\n\nfrom collections import Counter\n\nimport itertools\nfrom itertools import zip_longest\n\nimport re\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')\nimport string\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\n\nimport eli5\n\n%matplotlib inline\n%precision 3\npd.set_option('precision', 3)\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#データを読み取る\n#\ntrain = pd.read_csv('../input/tmdb-box-office-prediction/train.csv')\n#\ntest = pd.read_csv('../input/tmdb-box-office-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape,test.shape)\ntrain.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 調べた欠測データ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train['id'] == 391,'runtime'] = 96 #The Worst Christmas of My Lifeの上映時間を調べて入力\ntrain.loc[train['id'] == 592,'runtime'] = 90 #А поутру они проснулисьの上映時間を調べて入力\ntrain.loc[train['id'] == 925,'runtime'] = 86 #¿Quién mató a Bambi?の上映時間を調べて入力\ntrain.loc[train['id'] == 978,'runtime'] = 93 #La peggior settimana della mia vitaの上映時間を調べて入力\ntrain.loc[train['id'] == 1256,'runtime'] = 92 #Cry, Onion!の上映時間を調べて入力\ntrain.loc[train['id'] == 1542,'runtime'] = 93 #All at Onceの上映時間を調べて入力\ntrain.loc[train['id'] == 1875,'runtime'] = 93 #Vermistの上映時間を調べて入力\ntrain.loc[train['id'] == 2151,'runtime'] = 108 #Mechenosetsの上映時間を調べて入力\ntrain.loc[train['id'] == 2499,'runtime'] = 86 #Na Igre 2. Novyy Urovenの上映時間を調べて入力\ntrain.loc[train['id'] == 2646,'runtime'] = 98 #My Old Classmateの上映時間を調べて入力\ntrain.loc[train['id'] == 2786,'runtime'] = 111 #Revelationの上映時間を調べて入力\ntrain.loc[train['id'] == 2866,'runtime'] = 96 #Tutto tutto niente nienteの上映時間を調べて入力","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.loc[test['id'] == 3244,'runtime'] = 93 #La caliente niña Julietta\tの上映時間を調べて入力\ntest.loc[test['id'] == 4490,'runtime'] = 90 #Pancho, el perro millonarioの上映時間を調べて入力\ntest.loc[test['id'] == 4633,'runtime'] = 108 #Nunca en horas de claseの上映時間を調べて入力\ntest.loc[test['id'] == 6818,'runtime'] = 90 #Miesten välisiä keskustelujaの上映時間を調べて入力\n\ntest.loc[test['id'] == 4074,'runtime'] = 103 #Shikshanachya Aaicha Ghoの上映時間を調べて入力\ntest.loc[test['id'] == 4222,'runtime'] = 91 #Street Knightの上映時間を調べて入力\ntest.loc[test['id'] == 4431,'runtime'] = 96 #Plus oneの上映時間を調べて入力\ntest.loc[test['id'] == 5520,'runtime'] = 86 #Glukhar v kinoの上映時間を調べて入力\ntest.loc[test['id'] == 5845,'runtime'] = 83 #Frau Müller muss weg!の上映時間を調べて入力\ntest.loc[test['id'] == 5849,'runtime'] = 140 #Shabdの上映時間を調べて入力\ntest.loc[test['id'] == 6210,'runtime'] = 104 #The Last Breathの上映時間を調べて入力\ntest.loc[test['id'] == 6804,'runtime'] = 140 #Chaahat Ek Nasha...の上映時間を調べて入力\ntest.loc[test['id'] == 7321,'runtime'] = 87 #El truco del mancoの上映時間を調べて入力","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## df作成","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train, test]).set_index(\"id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#columnsを確認し、除外する変数をdrop\nprint(df.columns)\n# 使わない列を消す\ndf = df.drop([\"poster_path\", \"status\", \"original_title\"], axis=1) # \"overview\",  \"imdb_id\", ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# logを取っておく\ndf[\"log_revenue\"] = np.log10(df[\"revenue\"])\n# homepage: 有無に\ndf[\"homepage\"] = ~df[\"homepage\"].isnull()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 各列の処理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfdic_feature = {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# JSON text を辞書型のリストに変換\nimport ast\ndict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\nfor col in dict_columns:\n       df[col]=df[col].apply(lambda x: [] if pd.isna(x) else ast.literal_eval(x) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 各ワードの有無を表す 01 のデータフレームを作成\ndef count_word_list(series):\n    len_max = series.apply(len).max() # ジャンル数の最大値\n    tmp = series.map(lambda x: x+[\"nashi\"]*(len_max-len(x))) # listの長さをそろえる\n    \n    word_set = set(sum(list(series.values), [])) # 全ジャンル名のset\n    for n in range(len_max):\n        word_dfn = pd.get_dummies(tmp.apply(lambda x: x[n]))\n        word_dfn = word_dfn.reindex(word_set, axis=1).fillna(0).astype(int)\n        if n==0:\n            word_df = word_dfn\n        else:\n            word_df = word_df + word_dfn\n    \n    return word_df#.drop(\"nashi\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# budget","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#budgetが0の物を予測（テスト）、0でない物をtrainingデータとする\nbudget0 = df[df[\"budget\"] == 0]\nbudget = df[df[\"budget\"] != 0]\ntrain_X = budget[[\"popularity\",\"runtime\"]]\ntrain_y = budget[\"budget\"]\ntest_X = budget0[[\"popularity\",\"runtime\"]]\ntest_y = budget0[\"budget\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.fillna(0, inplace = True)\ntest_X.fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#budgetが0の物を線形回帰で予測\nfrom sklearn.linear_model import RidgeCV\nrcv= RidgeCV(cv=3, alphas = 10**np.arange(-2, 2, 0.1))\nrcv.fit(train_X, train_y)\ny_pred = rcv.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"budget0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"budget0.index = range(0,2023)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"budget_pred = pd.DataFrame(y_pred,columns=[\"pred\"])\nbudget_pred = pd.concat([budget.index,budget_pred],axis = 1)\nbudget_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#予算が0を下回っているものはおかしいので0に戻す。\nbudget_pred.loc[budget_pred[\"pred\"] < 0, \"pred\"] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.merge(df, budget_pred, on=\"id\", how=\"left\") \ndf.loc[budget_pred[\"id\"]-1, \"budget\"] = df.loc[budget_pred[\"id\"]-1, \"pred\"]\ndf = df.drop(\"pred\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## genres","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"genre_names\"] = df[\"genres\"].apply(lambda x : [ i[\"name\"] for i in x])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfdic_feature[\"genre\"] = count_word_list(df[\"genre_names\"])\n# TV movie は1件しかないので削除\ndfdic_feature[\"genre\"] = dfdic_feature[\"genre\"].drop(\"TV Movie\", axis=1)\ndfdic_feature[\"genre\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## original language","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# train内の作品数が10件未満の言語は \"small\" に集約\nn_language = df.loc[:train.index[-1], \"original_language\"].value_counts()\nlarge_language = n_language[n_language>=10].index\ndf.loc[~df[\"original_language\"].isin(large_language), \"original_language\"] = \"small\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"original_language\"] = df[\"original_language\"].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one_hot_encoding\ndfdic_feature[\"original_language\"] = pd.get_dummies(df[\"original_language\"])\n#dfdic_feature[\"original_language\"] = dfdic_feature[\"original_language\"].loc[:, dfdic_feature[\"original_language\"].sum()>0]\ndfdic_feature[\"original_language\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## production company","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"production_names\"] = df[\"production_companies\"].apply(lambda x : [ i[\"name\"] for i in x])\n#.fillna(\"[{'name': 'nashi'}]\").map(to_name_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time tmp = count_word_list(df[\"production_names\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train内の件数が多い物のみ選ぶ\ndef select_top_n(df, topn=9999, nmin=2):  # topn:上位topn件, nmin:作品数nmin以上\n#    if \"small\" in df.columns:\n#        df = df.drop(\"small\", axis=1)\n    n_word = (df.loc[train[\"id\"]]>0).sum().sort_values(ascending=False)\n    # 作品数がnmin件未満\n    smallmin = n_word[n_word<nmin].index\n    # 上位topn件に入っていない\n    smalln = n_word.iloc[topn+1:].index\n    small = set(smallmin) | set(smalln)\n    # 件数の少ないタグのみの作品\n    df[\"small\"] = df[small].sum(axis=1) #>0\n    \n    return df.drop(small, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# trainに50本以上作品のある会社\ndfdic_feature[\"production_companies\"] = select_top_n(tmp, nmin=50)\ndfdic_feature[\"production_companies\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## production contries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 国名のリストに\ndf[\"country_names\"] = df[\"production_countries\"].apply(lambda x : [ i[\"name\"] for i in x])\ndf_country = count_word_list(df[\"country_names\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2か国だったら、0.5ずつに\ndf_country = (df_country.T/df_country.sum(axis=1)).T.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 30作品以上の国のみ\ndfdic_feature[\"production_countries\"] = select_top_n(df_country, nmin=30)\ndfdic_feature[\"production_countries\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Keyword","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"keyword_list\"] = df[\"Keywords\"].apply(lambda x : [ i[\"name\"] for i in x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_topn_onehot(series, topn):\n    # 多いワード順に\n    word_count = pd.Series(collections.Counter(sum(list(series.values), [])))\n    word_count = word_count.sort_values(ascending=False)\n    \n    df_topn = df[[]].copy()  # index のみのDF\n    # 上位topn件のキーワードのみ\n    for word in word_count.iloc[:topn].index:  # .drop(\"nashi\")\n        df_topn[word] = series.apply(lambda x: word in x)*1\n    \n    return df_topn\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfdic_feature[\"Keywords\"] = encode_topn_onehot(df[\"keyword_list\"], 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"num_Keywords\"] = df[\"keyword_list\"].apply(len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## spoken laguages","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"language_names\"] = df[\"spoken_languages\"].apply(lambda x : [ i[\"name\"] for i in x])\ndf[\"n_language\"] = df[\"language_names\"].apply(len)\n# 欠損値は１にする(データを見ると無声映画ではない)\ndf.loc[df[\"n_language\"]==0, \"n_language\"] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 英語が含まれるか否か\ndf[\"speak_English\"] = df[\"language_names\"].apply(lambda x : \"English\" in x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## release_date","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 公開日の欠損1件 id=3829\n# May,2000 (https://www.imdb.com/title/tt0210130/) \n# 日は不明。1日を入れておく\ndf.loc[3829, \"release_date\"] = \"5/1/00\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"release_year\"] = pd.to_datetime(df[\"release_date\"]).dt.year.astype(int)\n# 年の20以降を、2020年より後の未来と判定してしまうので、補正。\ndf.loc[df[\"release_year\"]>2020, \"release_year\"] = df.loc[df[\"release_year\"]>2020, \"release_year\"]-100\n\ndf[\"release_month\"] = pd.to_datetime(df[\"release_date\"]).dt.month.astype(int)\ndf[\"release_day\"] = pd.to_datetime(df[\"release_date\"]).dt.day.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# datetime型に\ndf[\"release_date\"] = df.apply(lambda s: datetime.datetime(\n    year=s[\"release_year\"],month=s[\"release_month\"],day=s[\"release_day\"]), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"release_dayofyear\"] = df[\"release_date\"].dt.dayofyear\ndf[\"release_dayofweek\"] = df[\"release_date\"].dt.dayofweek","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 月、曜日は カテゴリ型に\ndf[\"release_month\"] = df[\"release_month\"].astype('category')\ndf[\"release_dayofweek\"] = df[\"release_dayofweek\"].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## belongs to collection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# collection 名を抽出\ndf[\"collection_name\"] = df[\"belongs_to_collection\"].apply(lambda x : x[0][\"name\"] if len(x)>0 else \"nashi\")\n# 無い場合、\"nashi\"に","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# シリーズの作品数\n#df = pd.merge( df, df.groupby(\"collection_name\").count()[[\"budget\"]].rename(columns={\"budget\":\"count_collection\"}), \n#         on=\"collection_name\", how=\"left\")\n# indexがずれるので、戻す\n#df.index = df.index+1\n\ndf[\"count_collection\"] = df[\"collection_name\"].apply(lambda x : (df[\"collection_name\"]==x).sum())\n# シリーズ以外の場合0\ndf.loc[df[\"collection_name\"]==\"nashi\", \"count_collection\"] = 0\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# シリーズ何作目か\ndf[\"number_in_collection\"] = df.sort_values(\"release_date\").groupby(\"collection_name\").cumcount()+1\n# シリーズ以外の場合0\ndf.loc[df[\"collection_name\"]==\"nashi\", \"number_in_collection\"] = 0\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# 同シリーズの自分より前の作品の平均log(revenue)\ndf[\"collection_av_logrevenue\"] = [ df.loc[(df[\"collection_name\"]==row[\"collection_name\"]) & \n                                          (df[\"number_in_collection\"]<row[\"number_in_collection\"]),\n                                          \"log_revenue\"].mean() \n     for key,row in df.iterrows() ]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 欠損(nashi) の場合、nashi での平均\ndf.loc[df[\"collection_name\"]==\"nashi\", \"collection_av_logrevenue\"] = df.loc[df[\"collection_name\"]==\"nashi\", \"log_revenue\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train に無くtestだけにあるシリーズの場合、シリーズもの全部の平均\ncollection_mean = df.loc[df[\"collection_name\"]!=\"nashi\", \"log_revenue\"].mean()  # シリーズもの全部の平均\ndf[\"collection_av_logrevenue\"] = df[\"collection_av_logrevenue\"].fillna(collection_mean)  \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 連結","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features = pd.concat(dfdic_feature, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## runtime　欠測処理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 欠測と0は、0ではないものの平均で埋める\ndf[\"runtime\"] = df[\"runtime\"].fillna(df.loc[df[\"runtime\"]>0, \"runtime\"].mean())\ndf.loc[df[\"runtime\"]==0, \"runtime\"] = df.loc[df[\"runtime\"]>0, \"runtime\"].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## budget","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.scatter(df[\"budget\"]+1, df[\"log_revenue\"], s=1)\n#plt.xscale(\"log\")\n#plt.xrange([])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 整形","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[[\"original_language\", \"collection_name\"]] = df[[\"original_language\", \"collection_name\"]].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use = df[['budget', 'homepage', 'popularity','runtime','n_language', \n             \"num_Keywords\", \"speak_English\",\n             'release_year', 'release_month','release_dayofweek', \n             'collection_av_logrevenue' ,\"count_collection\",\"number_in_collection\"\n            ]]\ndf_use.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use = pd.get_dummies(df_use)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Additional data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_add = pd.read_csv('../input/tmdb-competition-additional-features/TrainAdditionalFeatures.csv')\ntest_add = pd.read_csv('../input/tmdb-competition-additional-features/TestAdditionalFeatures.csv')\ntrain_add.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.merge(df, pd.concat([train_add, test_add]), on=\"imdb_id\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"add_cols = [\"popularity2\", \"rating\", \"totalVotes\"]\ndf[add_cols] = df[add_cols].fillna(df[add_cols].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2 = pd.read_csv('../input/tmdb-box-office-prediction-more-training-data/additionalTrainData.csv')\ntrain3 = pd.read_csv('../input/tmdb-box-office-prediction-more-training-data/trainV3.csv')\ntrain3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 言語処理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#全て小文字に変換\ndef lower_text(text):\n    return text.lower()\n\n#記号の排除\ndef remove_punct(text):\n    text = text.replace('-', ' ')  # - は単語の区切りとみなす\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ndef remove_stopwords(words, stopwords):#不要な単語を削除\n    words = [word for word in words if word not in stopwords]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 英語以外","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 英語でよく使う単語が入っていない文章を確認\ndf.loc[df[\"overview\"].apply(lambda x : str(x)).apply(lambda x : lower_text(x)\n                                ).str.contains(\"nan|the|where|with|from|and|for|his|her|over\")==False, \"overview\"]\n#train3.loc[train3[\"overview\"].apply(lambda x : str(x)).apply(lambda x : lower_text(x)).str.contains(\"nan|the|where|with|from|and|for|his|her|over\")==False, \"overview\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_english_overview_id = [157, 2863, 4616]   # 上のデータを目で確認\nno_english_tagline_id = [3255, 3777, 4937]   # Tfidf で非英語の単語があったもの","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### word2vec","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import word2vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_text = [\"overview\", \"tagline\"] # \"title\", \nall_text = pd.concat([df[col_text], train2[col_text], train3[col_text]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 英語以外と\"nan\"は除外\nall_text.loc[no_english_overview_id, \"overview\"] = np.nan\nall_text.loc[no_english_tagline_id, \"tagline\"] = np.nan\nall_text.loc[all_text[\"tagline\"]==\"nan\", \"tagline\"] = np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_texts = all_text.stack()\nall_texts=all_texts.apply(lambda x : str(x))\nall_texts=all_texts.apply(lambda x : lower_text(x))\nall_texts=all_texts.apply(lambda x : remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_texts.to_csv(\"./alltexts_for_w2v.txt\", index=False, header=False)\ndocs = word2vec.LineSentence(\"alltexts_for_w2v.txt\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmodel = word2vec.Word2Vec(docs, sg=1, size=100, min_count=5, window=5, iter=100)\nmodel.save(\"./alltexts_w2v1_sg.model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = word2vec.Word2Vec.load(\"./alltexts_w2v1_cbow.model\")\n# model = word2vec.Word2Vec.load(\"./alltexts_w2v1_sg.model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.most_similar(positive=['father'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.most_similar(positive=['human'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 単語ベクトルの mean, max を文章ベクトルにする\ndef get_doc_vector(doc, method=\"mean\", weight=None):\n    split_doc = doc.split(\" \")\n    if weight==None:\n        weight = dict(zip(model.wv.vocab.keys(), np.ones(len(model.wv.vocab))))\n        \n    word_vecs = [ model[word]*weight[word] for word in split_doc if word in model.wv.vocab.keys() ]\n    \n    if len(word_vecs)==0:\n        doc_vec = []\n    elif method==\"mean\":\n        doc_vec =  np.mean(word_vecs, axis=0)\n    elif method==\"max\":\n        doc_vec =  np.max(word_vecs, axis=0)\n    elif method==\"meanmax\":\n        doc_vec =  np.mean(word_vecs, axis=0)+np.max(word_vecs, axis=0)\n    return doc_vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## overview","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#単語数\ndf['overview_word_count'] = df['overview'].apply(lambda x: len(str(x).split()))\n#文字数\ndf['overview_char_count'] = df['overview'].apply(lambda x: len(str(x)))\n# 記号の個数\ndf['overview_punctuation_count'] = df['overview'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 前処理\ndf['_overview']=df['overview'].apply(lambda x : str(x)\n                            ).apply(lambda x : lower_text(x)).apply(lambda x : remove_punct(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#短縮形を元に戻す\nshortened = {\n    '\\'m': ' am',\n    '\\'re': ' are',\n    'don\\'t': 'do not',\n    'doesn\\'t': 'does not',\n    'didn\\'t': 'did not',\n    'won\\'t': 'will not',\n    'wanna': 'want to',\n    'gonna': 'going to',\n    'gotta': 'got to',\n    'hafta': 'have to',\n    'needa': 'need to',\n    'outta': 'out of',\n    'kinda': 'kind of',\n    'sorta': 'sort of',\n    'lotta': 'lot of',\n    'lemme': 'let me',\n    'gimme': 'give me',\n    'getcha': 'get you',\n    'gotcha': 'got you',\n    'letcha': 'let you',\n    'betcha': 'bet you',\n    'shoulda': 'should have',\n    'coulda': 'could have',\n    'woulda': 'would have',\n    'musta': 'must have',\n    'mighta': 'might have',\n    'dunno': 'do not know',\n}\ndf[\"overview\"] = df[\"overview\"].replace(shortened)\ntrain[\"overview\"] = train[\"overview\"].replace(shortened)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"overview\"]=df[\"overview\"].apply(lambda x : remove_punct(x))\ntrain[\"overview\"]=train[\"overview\"].apply(lambda x : remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 連続した数字を0で置換\ndef normalize_number(text):\n    replaced_text = re.sub(r'\\d+', '0', text)\n    return replaced_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"overview\"]=df[\"overview\"].apply(lambda x : normalize_number(x))\ntrain[\"overview\"]=train[\"overview\"].apply(lambda x : normalize_number(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#レンマ化\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nwnl = WordNetLemmatizer()\ndf[\"overview\"]=df[\"overview\"].apply(wnl.lemmatize)\ntrain[\"overview\"]=train[\"overview\"].apply(wnl.lemmatize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#空白ごとの文章の分割\ndf[\"overview\"]=df[\"overview\"].apply(lambda x : str(x).split())\ntrain[\"overview\"]=train[\"overview\"].apply(lambda x : str(x).split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_overview = df[\"overview\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def most_common(docs, n=100):#(文章、上位n個の単語)#上位n個の単語を抽出\n    fdist = Counter()\n    for doc in docs:\n        for word in doc:\n            fdist[word] += 1\n    common_words = {word for word, freq in fdist.most_common(n)}\n    print('{}/{}'.format(n, len(fdist)))\n    return common_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_common(df_overview,100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_stop_words(docs, n=100, min_freq=1):#上位n個の単語、頻度がmin_freq以下の単語を列挙（あまり特徴のない単語等）\n    fdist = Counter()\n    for doc in docs:\n        for word in doc:\n            fdist[word] += 1\n    common_words = {word for word, freq in fdist.most_common(n)}\n    rare_words = {word for word, freq in fdist.items() if freq <= min_freq}\n    stopwords = common_words.union(rare_words)\n    print('{}/{}'.format(len(stopwords), len(fdist)))\n    return stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = get_stop_words(df_overview)\nstopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(words, stopwords):#不要な単語を削除\n    words = [word for word in words if word not in stopwords]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"overview\"]=df[\"overview\"].apply(lambda x : remove_stopwords(x,stopwords))\ntrain[\"overview\"]=train[\"overview\"].apply(lambda x : remove_stopwords(x,stopwords))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"overview\"]=[\" \".join(review) for review in df[\"overview\"].values]\ntrain[\"overview\"]=[\" \".join(review) for review in train[\"overview\"].values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer#ベクトル化\nvec_tfidf = TfidfVectorizer()\nX = vec_tfidf.fit_transform(df[\"overview\"])\nTfid_overview = pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())\n\nX2 = vec_tfidf.fit_transform(df[\"overview\"])\nTfid_train_overview = pd.DataFrame(X2.toarray(), columns=vec_tfidf.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['_tagline']=df['tagline'].apply(lambda x : str(x)\n                                 ).apply(lambda x : lower_text(x)).apply(lambda x : remove_punct(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ベクトル化\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# vec_tfidf = TfidfVectorizer()\n# X = vec_tfidf.fit_transform(df['tagline'])\n# Tfidf_tagline = pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())\n# X = vec_tfidf.fit_transform(df['overview'].dropna())\n# Tfidf_overview = pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time df_tagline =  df[\"_tagline\"].apply(get_doc_vector, method=\"meanmax\").apply(pd.Series)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tagline = df_tagline.fillna(0).add_prefix(\"tagline_\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## titleの前処理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#単語数\ndf['title_word_count'] = df['title'].apply(lambda x: len(str(x).split()))\n#文字数\ndf['title_char_count'] = df['title'].apply(lambda x: len(str(x)))\n# 記号の個数\ndf['title_punctuation_count'] = df['title'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tagline']=df['tagline'].apply(lambda x : str(x))\ndf[\"tagline\"] = df[\"tagline\"].replace(shortened)\ndf['tagline']=df['tagline'].apply(lambda x : lower_text(x))\ndf['tagline']=df['tagline'].apply(lambda x : remove_punct(x))\ndf[\"tagline\"]=df[\"tagline\"].apply(lambda x : normalize_number(x))\ndf['tagline']=df['tagline'].apply(lambda x : str(x).split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tagline = df[\"tagline\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_common(tagline)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = get_stop_words(tagline)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tagline']=df['tagline'].apply(lambda x : remove_stopwords(x,stopwords))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nan = {\"nan\"}\ndef remove_nan(words):\n    words = [word for word in words if word not  in nan]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tagline']=df['tagline'].apply(lambda x : remove_nan(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tagline']=[\" \".join(review) for review in df['tagline'].values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ベクトル化\nX = vec_tfidf.fit_transform(df['tagline'])\nTfid_tagline = pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use2 = df[[\"runtime\",'budget','tagline_char_count']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use2 = pd.concat([df_use2,Tfid_overview],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#使用する変数\ndf_use2 = df_use2.loc[:,~df_use.columns.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## keywordもword2vecベクトル化すると？","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keywords を全部並べたものを、文とみなしてベクトル化\n%time df_keyword_w2v = df[\"keyword_list\"].apply(\" \".join).apply(get_doc_vector, method=\"mean\").apply(pd.Series)\ndf_keyword_w2v = df_keyword_w2v.fillna(0).add_prefix(\"keyword_\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## cast","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#castの中にある俳優の名前をリスト化させる\nlist_of_cast_names = list(df['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ndf['num_cast'] = df['cast'].apply(lambda x: len(x) if x != {} else 0)\ndf['all_cast'] = df['cast'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n\n\ntop_cast_names = [m[0] for m in Counter([i for j in list_of_cast_names for i in j]).most_common(30)]\nfor g in top_cast_names:\n    df['cast_name_' + g] = df['all_cast'].apply(lambda x: 1 if g in x else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_cast_genders = list(df['cast'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\n\ndf['genders_0_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ndf['genders_1_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ndf['genders_2_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))    \n\n#df = df.drop(['cast'], axis=1)\n\ndf['cast_gen0_ratio'] = df['genders_0_cast'].sum()/df['num_cast'].sum()\ndf['cast_gen1_ratio'] = df['genders_1_cast'].sum()/df['num_cast'].sum()\ndf['cast_gen2_ratio'] = df['genders_2_cast'].sum()/df['num_cast'].sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Crew","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#crewのname\nlist_of_crew_names = list(df['crew'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ndf['num_crew'] = df['crew'].apply(lambda x: len(x) if x != {} else 0)\ndf['all_crew'] = df['crew'].apply(lambda x: ','.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_crew_names = [m[0] for m in Counter([i for j in list_of_crew_names for i in j]).most_common(15)]\nfor g in top_crew_names:\n    df['crew_name_' + g] = df['all_crew'].apply(lambda x: 1 if g in x else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_crew_department = list(df['crew'].apply(lambda x: [i['department'] for i in x] if x != {} else []).values)\ndf['all_department'] = df['crew'].apply(lambda x: '　'.join(sorted([i['department']for i in x])) if x != {} else '')\ntop_crew_department = [m[0] for m in Counter(i for j in list_of_crew_department for i in j).most_common(12)]\nfor g in top_crew_department:\n    df['crew_department_' + g] = df['crew'].apply(lambda x: sum([1 for i in x if i['department'] == g]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_crew_job = list(df['crew'].apply(lambda x: [i['job'] for i in x] if x != {} else []).values)\ntop_crew_job = [m[0] for m in Counter(i for j in list_of_crew_job for i in j).most_common(10)]\nfor g in top_crew_job:\n    df['crew_job_' + g] = df['crew'].apply(lambda x: sum([1 for i in x if i['job'] == g]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['genders_0_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ndf['genders_1_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ndf['genders_2_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\ndf['crew_gen0_ratio'] = df['genders_0_crew'].sum()/df['num_crew'].sum()\ndf['crew_gen1_ratio'] = df['genders_1_crew'].sum()/df['num_crew'].sum()\ndf['crew_gen2_ratio'] = df['genders_2_crew'].sum()/df['num_crew'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# crew n department","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_crew_job = [m[0] for m in Counter([i for j in list_of_crew_job for i in j]).most_common()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_crew_department = [m[0] for m in Counter([i for j in list_of_crew_department for i in j]).most_common()]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def select_department(list_dict, department):\n    return [ dic['name'] for dic in list_dict if dic['department']==department]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for z in all_crew_department:\n    df['{}_list'.format(z)] = df[\"crew\"].apply(select_department, department=z)\n    globals()[z] = [m[0] for m in Counter([i for j in df['{}_list'.format(z)] for i in j]).most_common(15)]\n    for i in globals()[z]:\n        df['crew_{}_{}'.format(z,i)] = df['{}_list'.format(z)].apply(lambda x: sum([1 for i in x]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def select_job(list_dict, job):\n    return [ dic[\"name\"] for dic in list_dict if dic[\"job\"]==job]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for z in top_crew_job:\n    df['{}_list'.format(z)] = df[\"crew\"].apply(select_job, job=z)\n    globals()[z] = [m[0] for m in Counter([i for j in df['{}_list'.format(z)] for i in j]).most_common(15)]\n    for i in globals()[z]:\n        df['crew_{}_{}'.format(z,i)] = df['{}_list'.format(z)].apply(lambda x: sum([1 for i in x]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 整理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use3=df[['num_cast','all_cast','cast_name_Samuel L. Jackson','cast_name_Robert De Niro','cast_name_Bruce Willis',\n'cast_name_Morgan Freeman','cast_name_Liam Neeson','cast_name_Willem Dafoe','cast_name_Steve Buscemi',\n'cast_name_Sylvester Stallone','cast_name_Nicolas Cage','cast_name_Matt Damon','cast_name_J.K. Simmons',\n'cast_name_John Goodman','cast_name_Julianne Moore','cast_name_Christopher Walken','cast_name_Robin Williams',\n'cast_name_Johnny Depp','cast_name_Stanley Tucci','cast_name_Harrison Ford','cast_name_Richard Jenkins',\n'cast_name_Ben Stiller','cast_name_Susan Sarandon','cast_name_Brad Pitt','cast_name_Tom Hanks',\n'cast_name_Keith David','cast_name_John Leguizamo','cast_name_Woody Harrelson','cast_name_Bill Murray','cast_name_Dennis Quaid','cast_name_James Franco','cast_name_Dustin Hoffman','genders_0_cast','genders_1_cast',\n'genders_2_cast','cast_gen0_ratio','cast_gen1_ratio','cast_gen2_ratio','num_crew','all_crew','crew_name_Avy Kaufman','crew_name_Steven Spielberg',\n'crew_name_Robert Rodriguez','crew_name_Mary Vernieu','crew_name_Deborah Aquila','crew_name_Bob Weinstein','crew_name_Harvey Weinstein','crew_name_Hans Zimmer','crew_name_Tricia Wood','crew_name_James Newton Howard',\n'crew_name_James Horner','crew_name_Luc Besson','crew_name_Francine Maisler','crew_name_Kerry Barden','crew_name_Jerry Goldsmith','all_department','crew_department_Production','crew_department_Sound',\n'crew_department_Art','crew_department_Crew','crew_department_Writing','crew_department_Costume & Make-Up','crew_department_Camera','crew_department_Directing','crew_department_Editing','crew_department_Visual Effects','crew_department_Lighting','crew_department_Actors','crew_job_Producer','crew_job_Executive Producer','crew_job_Director','crew_job_Screenplay','crew_job_Editor','crew_job_Casting','crew_job_Director of Photography','crew_job_Original Music Composer','crew_job_Art Direction','crew_job_Production Design',\n'genders_0_crew','genders_1_crew','genders_2_crew','crew_gen0_ratio','crew_gen1_ratio','crew_gen2_ratio',\n'crew_Production_Avy Kaufman','crew_Production_Mary Vernieu','crew_Production_Deborah Aquila','crew_Production_Bob Weinstein','crew_Production_Harvey Weinstein','crew_Production_Tricia Wood','crew_Production_Francine Maisler','crew_Production_Kerry Barden','crew_Production_Billy Hopkins','crew_Production_Steven Spielberg','crew_Production_Suzanne Smith',\n'crew_Production_Arnon Milchan','crew_Production_Scott Rudin','crew_Production_John Papsidera','crew_Production_Tim Bevan','crew_Sound_James Newton Howard','crew_Sound_Hans Zimmer','crew_Sound_James Horner','crew_Sound_Jerry Goldsmith','crew_Sound_John Williams',\n'crew_Sound_Alan Silvestri','crew_Sound_Danny Elfman',\"crew_Sound_Dan O'Connell\",'crew_Sound_Mark Isham','crew_Sound_John Debney','crew_Sound_Marco Beltrami',\n'crew_Sound_Kevin Kaska','crew_Sound_Christophe Beck','crew_Sound_Graeme Revell','crew_Sound_Carter Burwell','crew_Art_Helen Jarvis','crew_Art_Ray Fisher','crew_Art_Rosemary Brandenburg',\n'crew_Art_Cedric Gibbons','crew_Art_Walter M. Scott','crew_Art_Nancy Haigh','crew_Art_Robert Gould','crew_Art_J. Michael Riva','crew_Art_Maher Ahmad','crew_Art_Henry Bumstead','crew_Art_Leslie A. Pope',\n'crew_Art_Gene Serdena','crew_Art_Jann Engel','crew_Art_David F. Klassen','crew_Art_Cindy Carr','crew_Crew_J.J. Makaro','crew_Crew_Brian N. Bentley',\n'crew_Crew_Brian Avery','crew_Crew_James Bamford','crew_Crew_Mark Edward Wright','crew_Crew_Karin Silvestri','crew_Crew_Gregory Nicotero','crew_Crew_G.A. Aguilar',\n'crew_Crew_Doug Coleman','crew_Crew_Sean Button',\"crew_Crew_Chris O'Connell\",'crew_Crew_Tim Monich','crew_Crew_Denny Caira',\n'crew_Crew_Susan Hegarty','crew_Crew_Michael Queen','crew_Writing_Luc Besson','crew_Writing_Stephen King','crew_Writing_Woodyallen','crew_Writing_John Hughes',\n'crew_Writing_Ian Fleming','crew_Writing_Robert Mark Kamen','crew_Writing_Sylvester Stallone','crew_Writing_David Koepp','crew_Writing_Terry Rossio',\n'crew_Writing_George Lucas','crew_Writing_Stan Lee','crew_Writing_Akiva Goldsman','crew_Writing_Brian Helgeland','crew_Writing_Ted Elliott','crew_Writing_William Goldman','crew_Costume & Make-Up_Ve Neill',\n'crew_Costume & Make-Up_Bill Corso','crew_Costume & Make-Up_Colleen Atwood','crew_Costume & Make-Up_Camille Friend','crew_Costume & Make-Up_Edith Head','crew_Costume & Make-Up_Louise Frogley','crew_Costume & Make-Up_Ellen Mirojnick',\n'crew_Costume & Make-Up_Mary Zophres','crew_Costume & Make-Up_Edouard F. Henriques','crew_Costume & Make-Up_Jean Ann Black','crew_Costume & Make-Up_Marlene Stewart','crew_Costume & Make-Up_Ann Roth','crew_Costume & Make-Up_Deborah La Mia Denaver',\n'crew_Costume & Make-Up_Alex Rouse','crew_Costume & Make-Up_Shay Cunliffe','crew_Camera_Hans Bjerno','crew_Camera_Roger Deakins','crew_Camera_Dean Semler',\n'crew_Camera_David B. Nowell','crew_Camera_Mark Irwin','crew_Camera_John Marzano','crew_Camera_Matthew F. Leonetti','crew_Camera_Dean Cundey',\n'crew_Camera_Frank Masi','crew_Camera_Oliver Wood','crew_Camera_Robert Elswit','crew_Camera_Pete Romano','crew_Camera_Merrick Morton',\n'crew_Camera_Robert Richardson','crew_Camera_Philippe Rousselot','crew_Directing_Steven Spielberg','crew_Directing_Clint Eastwood','crew_Directing_Woodyallen',\n'crew_Directing_Ridley Scott','crew_Directing_Karen Golden','crew_Directing_Alfred Hitchcock','crew_Directing_Kerry Lyn McKissick','crew_Directing_Ron Howard','crew_Directing_Dianne Dreyer','crew_Directing_Wilma Garscadden-Gahret',\n'crew_Directing_Martin Scorsese','crew_Directing_Brian De Palma','crew_Directing_Ana Maria Quintana','crew_Directing_Dug Rotstein',\n'crew_Directing_Tim Burton','crew_Editing_Michael Kahn','crew_Editing_Chris Lebenzon','crew_Editing_Jim Passon',\n'crew_Editing_Gary Burritt','crew_Editing_Dale E. Grahn','crew_Editing_Joel Cox','crew_Editing_Mark Goldblatt',\n'crew_Editing_Conrad Buff IV','crew_Editing_John C. Stuver','crew_Editing_Pietro Scalia','crew_Editing_Paul Hirsch',\n'crew_Editing_Don Zimmerman','crew_Editing_Robert Troy','crew_Editing_Steven Rosenblum','crew_Editing_Dennis McNeill',\n'crew_Visual Effects_Dottie Starling','crew_Visual Effects_Phil Tippett','crew_Visual Effects_James Baker','crew_Visual Effects_Hugo Dominguez',\n'crew_Visual Effects_Larry White','crew_Visual Effects_Ray McIntyre Jr.','crew_Visual Effects_James Baxter','crew_Visual Effects_Aaron Williams',\"crew_Visual Effects_Julie D'Antoni\",'crew_Visual Effects_Frank Thomas','crew_Visual Effects_Milt Kahl','crew_Visual Effects_Peter Chiang','crew_Visual Effects_Chuck Duke','crew_Visual Effects_Dave Kupczyk','crew_Visual Effects_Craig Barron','crew_Lighting_Justin Hammond','crew_Lighting_Howard R. Campbell',\n'crew_Lighting_Arun Ram-Mohan','crew_Lighting_Chuck Finch','crew_Lighting_Russell Engels','crew_Lighting_Frank Dorowsky',\n'crew_Lighting_Bob E. Krattiger','crew_Lighting_Ian Kincaid','crew_Lighting_Thomas Neivelt','crew_Lighting_Dietmar Haupt','crew_Lighting_James J. Gilson',\n'crew_Lighting_Dan Cornwall','crew_Lighting_Andy Ryan','crew_Lighting_Lee Walters','crew_Lighting_Jay Kemp','crew_Actors_Francois Grobbelaar',\n\"crew_Actors_Mick 'Stuntie' Milligan\",'crew_Actors_Sol Gorss','crew_Actors_Mark De Alessandro','crew_Actors_Leigh Walsh',\n'crew_Producer_Joel Silver','crew_Producer_Brian Grazer','crew_Producer_Scott Rudin','crew_Producer_Neal H. Moritz',\n'crew_Producer_Tim Bevan','crew_Producer_Eric Fellner','crew_Producer_Jerry Bruckheimer','crew_Producer_Arnon Milchan',\n'crew_Producer_Gary Lucchesi','crew_Producer_John Davis','crew_Producer_Jason Blum','crew_Producer_Tom Rosenberg','crew_Producer_Kathleen Kennedy',\n'crew_Producer_Luc Besson','crew_Producer_Steven Spielberg','crew_Executive Producer_Bob Weinstein','crew_Executive Producer_Harvey Weinstein','crew_Executive Producer_Bruce Berman',\n'crew_Executive Producer_Steven Spielberg','crew_Executive Producer_Toby Emmerich','crew_Executive Producer_Stan Lee','crew_Executive Producer_Ryan Kavanaugh','crew_Executive Producer_Ben Waisbren','crew_Executive Producer_Michael Paseornek','crew_Executive Producer_Thomas Tull','crew_Executive Producer_Arnon Milchan','crew_Executive Producer_Nathan Kahane','crew_Executive Producer_John Lasseter','crew_Executive Producer_Tessa Ross',\n'crew_Executive Producer_Gary Barber','crew_Director_Steven Spielberg','crew_Director_Clint Eastwood','crew_Director_Woodyallen','crew_Director_Ridley Scott',\n'crew_Director_Alfred Hitchcock','crew_Director_Ron Howard','crew_Director_Brian De Palma','crew_Director_Martin Scorsese','crew_Director_Tim Burton',\n'crew_Director_Blake Edwards','crew_Director_Joel Schumacher','crew_Director_Oliver Stone','crew_Director_Robert Zemeckis','crew_Director_Steven Soderbergh',\n'crew_Director_Wes Craven','crew_Screenplay_Sylvester Stallone','crew_Screenplay_Luc Besson','crew_Screenplay_John Hughes','crew_Screenplay_Akiva Goldsman','crew_Screenplay_David Koepp','crew_Screenplay_William Goldman','crew_Screenplay_Robert Mark Kamen','crew_Screenplay_Oliver Stone',\n'crew_Screenplay_Woodyallen','crew_Screenplay_Richard Maibaum','crew_Screenplay_John Logan','crew_Screenplay_Terry Rossio','crew_Screenplay_Harold Ramis',\n'crew_Screenplay_Brian Helgeland','crew_Screenplay_Ted Elliott','crew_Editor_Michael Kahn','crew_Editor_Chris Lebenzon','crew_Editor_Joel Cox',\n'crew_Editor_Mark Goldblatt','crew_Editor_Conrad Buff IV','crew_Editor_Pietro Scalia','crew_Editor_Paul Hirsch','crew_Editor_Don Zimmerman',\n'crew_Editor_Christian Wagner','crew_Editor_Anne V. Coates','crew_Editor_William Goldenberg','crew_Editor_Michael Tronick','crew_Editor_Daniel P. Hanley',\n'crew_Editor_Paul Rubell','crew_Editor_Stephen Mirrione','crew_Casting_Avy Kaufman','crew_Casting_Mary Vernieu','crew_Casting_Deborah Aquila',\n'crew_Casting_Tricia Wood','crew_Casting_Kerry Barden','crew_Casting_Francine Maisler','crew_Casting_Billy Hopkins','crew_Casting_Suzanne Smith',\n'crew_Casting_John Papsidera','crew_Casting_Denise Chamian','crew_Casting_Jane Jenkins','crew_Casting_Janet Hirshenson','crew_Casting_Mike Fenton',\n'crew_Casting_Mindy Marin','crew_Casting_Sarah Finn','crew_Director of Photography_Dean Semler','crew_Director of Photography_Roger Deakins','crew_Director of Photography_Mark Irwin',\n'crew_Director of Photography_Matthew F. Leonetti','crew_Director of Photography_Dean Cundey','crew_Director of Photography_Oliver Wood','crew_Director of Photography_Robert Elswit','crew_Director of Photography_Robert Richardson',\n'crew_Director of Photography_Philippe Rousselot','crew_Director of Photography_Dante Spinotti','crew_Director of Photography_Julio Macat','crew_Director of Photography_Dariusz Wolski','crew_Director of Photography_Don Burgess',\n'crew_Director of Photography_Janusz Kami≈Ñski','crew_Director of Photography_Peter Deming','crew_Original Music Composer_James Newton Howard','crew_Original Music Composer_James Horner','crew_Original Music Composer_Hans Zimmer',\n'crew_Original Music Composer_Jerry Goldsmith','crew_Original Music Composer_John Williams','crew_Original Music Composer_Danny Elfman','crew_Original Music Composer_Christophe Beck','crew_Original Music Composer_Alan Silvestri',\n'crew_Original Music Composer_John Powell','crew_Original Music Composer_Marco Beltrami','crew_Original Music Composer_Howard Shore','crew_Original Music Composer_Graeme Revell','crew_Original Music Composer_John Debney',\n'crew_Original Music Composer_Carter Burwell','crew_Original Music Composer_Mark Isham','crew_Art Direction_Cedric Gibbons','crew_Art Direction_Hal Pereira','crew_Art Direction_Helen Jarvis',\n'crew_Art Direction_Lyle R. Wheeler','crew_Art Direction_David Lazan','crew_Art Direction_Andrew Max Cahn','crew_Art Direction_Jack Martin Smith','crew_Art Direction_Robert Cowper',\n'crew_Art Direction_Stuart Rose','crew_Art Direction_David F. Klassen','crew_Art Direction_Dan Webster','crew_Art Direction_Steven Lawrence','crew_Art Direction_Jesse Rosenthal',\n'crew_Art Direction_Richard L. Johnson','crew_Art Direction_Kevin Constant','crew_Production Design_J. Michael Riva','crew_Production Design_Jon Hutman','crew_Production Design_Carol Spier',\n'crew_Production Design_Ida Random','crew_Production Design_Dennis Gassner','crew_Production Design_Perry Andelin Blake','crew_Production Design_David Gropman',\n'crew_Production Design_Mark Friedberg','crew_Production Design_Rick Carter','crew_Production Design_Stuart Craig','crew_Production Design_Jim Clay',\n'crew_Production Design_Kristi Zea','crew_Production Design_David Wasco','crew_Production Design_Wynn Thomas','crew_Production Design_Dante Ferretti]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.index = df.index\n\ndf_use.index = df.index\ndf_use2.index = df.index\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use4 = df[add_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_input = pd.concat([df_use, df_use2, df_use3, df_use4, df_features], axis=1) # .drop(\"belongs_to_collection\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tfid_tagline.index = df_use.index\n#df_use_Tfid = Tfid_tagline.loc[:, Tfid_tagline[:3000].nunique()>1]\n#df_use_Tfid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 全て繋げた特徴量\ndf_input = pd.concat([df_input, df_tagline, df_overview, df_keyword_w2v, df_castname, df_crewname], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 欠測ナシを確認\ndf_input.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cols = df_input.loc[:, df_input.isnull().sum()>0].columns\n#df_input.loc[:, cols] = df_input[cols].fillna(df_input[cols].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 保存\nimport pickle\nwith open('df_input.pkl', 'wb') as f:\n      pickle.dump(df_input , f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"ln_revenue\"] = np.log(df[\"revenue\"]+1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 学習用データ作成","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 数値化できい列を確認\nno_numeric = df_input.apply(lambda s:pd.to_numeric(s, errors='coerce')).isnull().all()\nno_numeric[no_numeric]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_all = df_input  # .drop([\"collection_av_logrevenue\"], axis=1)\ny_all = df[\"ln_revenue\"]\ny_all.index = X_all.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[ c for c in X_all.columns if \"revenue\" in str(c)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 標準化\n# X_train_all_mean = X_all[:3000].mean()\n# X_train_all_std  = X_all[:3000].std()\n# X_all = (X_all-X_train_all_mean)/X_train_all_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, val_X, train_y, val_y = train_test_split(X_all[:train.index[-1]], \n                                                  y_all[:train.index[-1]], \n                                                  test_size=0.25, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# randomforest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf2 = RandomForestRegressor(n_jobs=3, random_state=1)  # max_depth=, min_samples_split=, \nclf2.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_pred = clf2.predict(val_X)\nprint(\"RMSLE score for validation data\")\nnp.sqrt(mean_squared_error(val_pred, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(np.exp(val_pred)+1, np.exp(val_y)+1, s=3)\nplt.xlabel(\"prediction\")\nplt.ylabel(\"true revenue\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# submit","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf2 = RandomForestRegressor(n_jobs=3, random_state=1, n_estimators=500)  # \nclf2.fit(X_all[:train.index[-1]], y_all[:train.index[-1]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_importance = pd.DataFrame([clf2.feature_importances_], columns=train_X.columns, index=[\"importance\"]).T\ndf_importance.sort_values(\"importance\", ascending=False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = clf2.predict(X_all[3000:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_revenue = np.exp(test_pred)-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/tmdb-box-office-prediction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_RF = sample_submission.copy()\nsubmission_RF[\"revenue\"] = test_revenue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_RF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_RF.to_csv('submission_RF.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}