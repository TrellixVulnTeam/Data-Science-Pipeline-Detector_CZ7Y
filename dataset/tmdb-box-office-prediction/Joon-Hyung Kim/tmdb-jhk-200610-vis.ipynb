{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport ast\nimport json\n\nfrom collections import Counter\n\nimport itertools\nfrom itertools import zip_longest\n\nimport re\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')\nimport string\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\n\nimport eli5\n\n%matplotlib inline\n%precision 3\npd.set_option('precision', 3)\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#データを読み取る\ntrain = pd.read_csv('../input/tmdb-box-office-prediction/train.csv')\ntest = pd.read_csv('../input/tmdb-box-office-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#train = pd.concat([train, test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#columnsを確認し、除外する変数をdrop\ntrain.drop(columns=['status','imdb_id','poster_path','original_title'], inplace = True)\nprint(train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['log_revenue'] = np.log10(train['revenue'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overview","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train['overview'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['overview'].fillna('none', inplace = True)\ntrain['overview'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['overview'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stop_words = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def lower_text(text):\n    return text.lower()\n\n#記号の排除\ndef remove_punct(text):\n    text = text.replace('-', ' ')  # - は単語の区切りとみなす\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ndef remove_stopwords(words, stopwords):#不要な単語を削除\n    words = [word for word in words if word not in stopwords]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(string.punctuation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['overview'] = train['overview'].apply(lambda x: lower_text(x))\ntrain['overview'] = train['overview'].apply(lambda x: remove_punct(x))\n#train['overview'] = train['overview'].apply(lambda x: remove_stopwords(x,stop_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['overview']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['overview'] = train['overview'].apply(lambda x: ''.join(map(str,x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['overview']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (12, 12))\ntext = ''.join(train['overview'].values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top words in overview')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"vec_tfidf = TfidfVectorizer()\n\nX = vec_tfidf.fit_transform(train['overview'])\n\nprint('Vocabulary size: {}'.format(len(vec_tfidf.vocabulary_)))\nprint('Vocabulary content: {}'.format(vec_tfidf.vocabulary_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"vectorizer = TfidfVectorizer(\n            sublinear_tf=True,\n            analyzer='word',\n            token_pattern=r'\\w{1,}',\n            ngram_range=(1, 2),\n            min_df=5)\n\noverview_text = vectorizer.fit_transform(train['overview'].fillna(''))\nlinreg = LinearRegression()\nlinreg.fit(overview_text, train['log_revenue'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"eli5.show_weights(linreg, vec = vectorizer, top=20, feature_filter=lambda x: x != '<BIAS>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['tagline'].fillna('none', inplace = True)\ntrain['tagline'] = train['tagline'].apply(lambda x: lower_text(x))\ntrain['tagline'] = train['tagline'].apply(lambda x: remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['tagline'] = train['tagline'].apply(lambda x: ''.join(map(str,x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (12,12))\ntext = ''.join(train['tagline'].values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top words in overview')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X = vec_tfidf.fit_transform(train['tagline'])\n\nvectorizer = TfidfVectorizer(\n            sublinear_tf=True,\n            analyzer='word',\n            token_pattern=r'\\w{1,}',\n            ngram_range=(1, 2),\n            min_df=5)\n\noverview_text = vectorizer.fit_transform(train['tagline'].fillna(''))\nlinreg = LinearRegression()\nlinreg.fit(overview_text, train['log_revenue'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"eli5.show_weights(linreg, vec = vectorizer, top=20, feature_filter=lambda x: x != '<BIAS>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def select_lgbm(x):\n    params = {'learning_rate': [0.01,0.05,0.1 \n              'max_depth': [8,16,32]\n              'boosting': 'gbdt', \n              'objective': 'regression', \n              'metric': 'mse', \n              'is_training_metric': True, \n              'num_leaves': 144, \n              'feature_fraction': 0.9, \n              'bagging_fraction': 0.7, \n              'bagging_freq': 5, \n              }\n\nlgb = LGBMClassifier()\ngrid = GridSearchCV(lgb, param_grid=params)\n\nprint('best_parameter: ', grid.best_params_)\nprint('rmse:', )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def gridcv_lgb(cv, data, train, test):\n\n    best_models, best_params = [], []\n    for folds_num, (train_idx, val_idx) in enumerate(cv.split(data)):\n        print(f\"\\n------- Fold: ({folds_num + 1} / {cv.get_n_splits()}) ------\\n\")\n        train, valid = data.iloc[train_idx], data.iloc[val_idx]\n        X_train, X_valid = train.iloc[:train_days], train.iloc[test_days:]\n        y_train, y_valid = train.iloc[train_days:,0], valid.iloc[:,0]\n\n        train_set = lgb.Dataset(X_train, label=y_train)\n        valid_set = lgb.Dataset(X_valid, label=y_valid)\n        \n        best_rmse = float('inf')\n        for subsam in [0.3, 0.5, 0.7]:\n            for lr in [0.1, 0.05, 0.01]:\n                for num_iter in [2**9, 2**10, 2**11]:\n                    for num_leaves in [2**5, 2**6, 2**7, 2**8]:\n                        for m_depth in [2**2, 2**3, 2**4]:\n\n                            set_param = {'boosting_type': 'gbdt',\n                                         'objective': 'regression',\n                                         'metric': 'rmse',\n                                         'subsample': subsam,\n                                         'learning_rate': lr,\n                                         'max_bin': 50,\n                                         'num_iterations': num_iter,\n                                         'num_leaves': num_leaves,\n                                         'max_depth': m_depth,\n                                         'verbosity': -1}\n\n                            model = lgb.train(params=set_param,\n                                              train_set=train_set,\n                                              valid_sets=[valid_set],\n                                              early_stopping_rounds=10,\n                                              verbose_eval=False)\n\n                            pred = model.predict(X_valid)\n                            rmse = np.sqrt(mse(y_valid.iloc[:30], pred[:30]))\n\n                            if rmse < best_rmse:\n                                best_rmse = rmse\n                                best_model = model\n                                best_param = set_param\n                                print('RMSE: {:.5f}'.format(rmse))\n\n        best_models.append(best_model)\n        best_params.append(best_param)\n    \n    return best_models, best_params\n\n\ndef make_pred(model, data, train_days, pred_days, test):\n    pred = model.predict(data.iloc[len(data)-train_days:len(data)])\n    pred = pd.DataFrame(pred, columns=['prediction'])\n    \n    rmse = np.sqrt(mse(test.iloc[:pred_days], pred.iloc[:pred_days]))\n    print('RMSE: {:.5f}'.format(rmse))\n    \n    # 評価用データと予測データを図で比較\n    pred.index = test.index\n    for_plot = pd.concat([test.iloc[:pred_days],\n                          pred.iloc[:pred_days]],axis=1)\n\n    for_plot.plot(figsize=(12,6));\n    \n    return pred, rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}