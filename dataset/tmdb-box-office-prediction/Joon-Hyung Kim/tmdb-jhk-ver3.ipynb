{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TMDB prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport json\n\nfrom collections import Counter\n\nimport itertools\n\nimport re\nimport string\nimport collections\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\n\n%matplotlib inline\n%precision 3\npd.set_option('precision', 3)\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#データを読み取る\n#\ntrain = pd.read_csv('../input/tmdb-box-office-prediction/train.csv')\n#\ntest = pd.read_csv('../input/tmdb-box-office-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape,test.shape)\ntrain.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 調べた欠測データ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train['id'] == 391,'runtime'] = 96 #The Worst Christmas of My Lifeの上映時間を調べて入力\ntrain.loc[train['id'] == 592,'runtime'] = 90 #А поутру они проснулисьの上映時間を調べて入力\ntrain.loc[train['id'] == 925,'runtime'] = 86 #¿Quién mató a Bambi?の上映時間を調べて入力\ntrain.loc[train['id'] == 978,'runtime'] = 93 #La peggior settimana della mia vitaの上映時間を調べて入力\ntrain.loc[train['id'] == 1256,'runtime'] = 92 #Cry, Onion!の上映時間を調べて入力\ntrain.loc[train['id'] == 1542,'runtime'] = 93 #All at Onceの上映時間を調べて入力\ntrain.loc[train['id'] == 1875,'runtime'] = 93 #Vermistの上映時間を調べて入力\ntrain.loc[train['id'] == 2151,'runtime'] = 108 #Mechenosetsの上映時間を調べて入力\ntrain.loc[train['id'] == 2499,'runtime'] = 86 #Na Igre 2. Novyy Urovenの上映時間を調べて入力\ntrain.loc[train['id'] == 2646,'runtime'] = 98 #My Old Classmateの上映時間を調べて入力\ntrain.loc[train['id'] == 2786,'runtime'] = 111 #Revelationの上映時間を調べて入力\ntrain.loc[train['id'] == 2866,'runtime'] = 96 #Tutto tutto niente nienteの上映時間を調べて入力","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.loc[test['id'] == 3244,'runtime'] = 93 #La caliente niña Julietta\tの上映時間を調べて入力\ntest.loc[test['id'] == 4490,'runtime'] = 90 #Pancho, el perro millonarioの上映時間を調べて入力\ntest.loc[test['id'] == 4633,'runtime'] = 108 #Nunca en horas de claseの上映時間を調べて入力\ntest.loc[test['id'] == 6818,'runtime'] = 90 #Miesten välisiä keskustelujaの上映時間を調べて入力\n\ntest.loc[test['id'] == 4074,'runtime'] = 103 #Shikshanachya Aaicha Ghoの上映時間を調べて入力\ntest.loc[test['id'] == 4222,'runtime'] = 91 #Street Knightの上映時間を調べて入力\ntest.loc[test['id'] == 4431,'runtime'] = 96 #Plus oneの上映時間を調べて入力\ntest.loc[test['id'] == 5520,'runtime'] = 86 #Glukhar v kinoの上映時間を調べて入力\ntest.loc[test['id'] == 5845,'runtime'] = 83 #Frau Müller muss weg!の上映時間を調べて入力\ntest.loc[test['id'] == 5849,'runtime'] = 140 #Shabdの上映時間を調べて入力\ntest.loc[test['id'] == 6210,'runtime'] = 104 #The Last Breathの上映時間を調べて入力\ntest.loc[test['id'] == 6804,'runtime'] = 140 #Chaahat Ek Nasha...の上映時間を調べて入力\ntest.loc[test['id'] == 7321,'runtime'] = 87 #El truco del mancoの上映時間を調べて入力","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## df作成","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train, test]).set_index(\"id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df.index == 90,'budget'] = 30000000\ndf.loc[df.index == 118,'budget'] = 60000000\ndf.loc[df.index == 149,'budget'] = 18000000\ndf.loc[df.index == 464,'budget'] = 20000000\ndf.loc[df.index == 819,'budget'] = 90000000\ndf.loc[df.index == 1112,'budget'] = 6000000\ndf.loc[df.index == 1131,'budget'] = 4300000\ndf.loc[df.index == 1359,'budget'] = 10000000\ndf.loc[df.index == 1570,'budget'] = 15800000\ndf.loc[df.index == 1714,'budget'] = 46000000\ndf.loc[df.index == 1865,'budget'] = 80000000\ndf.loc[df.index == 2602,'budget'] = 31000000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#columnsを確認し、除外する変数をdrop\nprint(df.columns)\n# 使わない列を消す\ndf = df.drop([\"poster_path\", \"status\", \"original_title\"], axis=1) # \"overview\",  \"imdb_id\", ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# logを取っておく\ndf[\"log_revenue\"] = np.log10(df[\"revenue\"])\n# homepage: 有無に\n#df[\"homepage\"] =  ~df['homepage'].isnull()\ndf['has_homepage'] = 1\ndf.loc[pd.isnull(df['homepage']) ,\"has_homepage\"] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 各列の処理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfdic_feature = {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# JSON text を辞書型のリストに変換\nimport ast\ndict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\nfor col in dict_columns:\n       df[col]=df[col].apply(lambda x: [] if pd.isna(x) else ast.literal_eval(x) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 各ワードの有無を表す 01 のデータフレームを作成\ndef count_word_list(series):\n    len_max = series.apply(len).max() # ジャンル数の最大値\n    tmp = series.map(lambda x: x+[\"nashi\"]*(len_max-len(x))) # listの長さをそろえる\n    \n    word_set = set(sum(list(series.values), [])) # 全ジャンル名のset\n    for n in range(len_max):\n        word_dfn = pd.get_dummies(tmp.apply(lambda x: x[n]))\n        word_dfn = word_dfn.reindex(word_set, axis=1).fillna(0).astype(int)\n        if n==0:\n            word_df = word_dfn\n        else:\n            word_df = word_df + word_dfn\n    \n    return word_df#.drop(\"nashi\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## genres","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"genre_names\"] = df[\"genres\"].apply(lambda x : [ i[\"name\"] for i in x])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"genre_names\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfdic_feature[\"genre\"] = count_word_list(df[\"genre_names\"])\n# TV movie は1件しかないので削除\ndfdic_feature[\"genre\"] = dfdic_feature[\"genre\"].drop(\"TV Movie\", axis=1)\ndfdic_feature[\"genre\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## original language","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# train内の作品数が10件未満の言語は \"small\" に集約\nn_language = df.loc[:train.index[-1], \"original_language\"].value_counts()\nlarge_language = n_language[n_language>=10].index\ndf.loc[~df[\"original_language\"].isin(large_language), \"original_language\"] = \"small\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"original_language\"] = df[\"original_language\"].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one_hot_encoding\n#dfdic_feature[\"original_language\"] = pd.get_dummies(df[\"original_language\"])\n#dfdic_feature[\"original_language\"] = dfdic_feature[\"original_language\"].loc[:, dfdic_feature[\"original_language\"].sum()>0]\n#dfdic_feature[\"original_language\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## production company","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"production_names\"] = df[\"production_companies\"].apply(lambda x : [ i[\"name\"] for i in x])\n#.fillna(\"[{'name': 'nashi'}]\").map(to_name_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = count_word_list(df[\"production_names\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train内の件数が多い物のみ選ぶ\ndef select_top_n(df, topn=9999, nmin=2):  # topn:上位topn件, nmin:作品数nmin以上\n#    if \"small\" in df.columns:\n#        df = df.drop(\"small\", axis=1)\n    n_word = (df.loc[train[\"id\"]]>0).sum().sort_values(ascending=False)\n    # 作品数がnmin件未満\n    smallmin = n_word[n_word<nmin].index\n    # 上位topn件に入っていない\n    smalln = n_word.iloc[topn+1:].index\n    small = set(smallmin) | set(smalln)\n    # 件数の少ないタグのみの作品\n    df[\"small\"] = df[small].sum(axis=1) #>0\n    \n    return df.drop(small, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# trainに50本以上作品のある会社\n#dfdic_feature[\"production_companies\"] = select_top_n(tmp, nmin=50)\n#dfdic_feature[\"production_companies\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## production contries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 国名のリストに\ndf[\"country_names\"] = df[\"production_countries\"].apply(lambda x : [ i[\"name\"] for i in x])\ndf_country = count_word_list(df[\"country_names\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2か国だったら、0.5ずつに\ndf_country = (df_country.T/df_country.sum(axis=1)).T.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 30作品以上の国のみ\n#dfdic_feature[\"production_countries\"] = select_top_n(df_country, nmin=30)\n#dfdic_feature[\"production_countries\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Keyword","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"keyword_list\"] = df[\"Keywords\"].apply(lambda x : [ i[\"name\"] for i in x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"num_Keywords\"] = df[\"keyword_list\"].apply(len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## spoken laguages","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#crewのname\ndf_lang = pd.DataFrame(df['spoken_languages'])\nlist_lang_names = list(df_lang['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ndf_lang['num_spoken_languages'] = df_lang['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\ndf_lang['all_spoken_languages'] = df_lang['spoken_languages'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_lang_names = [m[0] for m in Counter([i for j in list_lang_names for i in j]).most_common(15)]\nfor g in top_lang_names:\n    df_lang[g] = df_lang['all_spoken_languages'].apply(lambda x: 1 if g in x else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lang.rename(columns ={'English':'spoken_en',\n 'Français':'spoken_fr',\n 'Español':'spoken_es',\n 'Deutsch':'spoken_gr',\n 'Pусский':'spoken_sv',\n 'Italiano':'spoken_it',\n '日本語':'spoken_ja',\n '普通话':'spoken_ch1',\n 'हिन्दी':'spoken_in',\n '':'spoken_unknown',\n 'العربية':'spoken_arb',\n 'Português':'spoken_por',\n '广州话 / 廣州話':'spoken_ch2',\n '한국어/조선말':'spoken_kr',\n 'Polski':'spoken_pol'}, inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lang.drop(columns=['spoken_languages', 'all_spoken_languages'], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"language_names\"] = df[\"spoken_languages\"].apply(lambda x : [ i[\"name\"] for i in x])\ndf[\"n_language\"] = df[\"language_names\"].apply(len)\n# 欠損値は１にする(データを見ると無声映画ではない)\ndf.loc[df[\"n_language\"]==0, \"n_language\"] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 英語が含まれるか否か\ndf[\"speak_English\"] = df[\"language_names\"].apply(lambda x : \"English\" in x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['speak_English'] = pd.get_dummies(df['speak_English'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['speak_English']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## release_date","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since only last two digits of year are provided, this is the correct way of getting the year.\ndf[['release_month','release_day','release_year']]=df['release_date'].str.split('/',expand=True).replace(np.nan, -1).astype(int)\n# Some rows have 4 digits of year instead of 2, that's why I am applying (df['release_year'] < 100) this condition\ndf.loc[ (df['release_year'] <= 19) & (df['release_year'] < 100), \"release_year\"] += 2000\ndf.loc[ (df['release_year'] > 19)  & (df['release_year'] < 100), \"release_year\"] += 1900\n\nreleaseDate = pd.to_datetime(df['release_date']) \ndf['release_dayofweek'] = releaseDate.dt.dayofweek\ndf['release_quarter'] = releaseDate.dt.quarter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['mean_revenue_year'] = df.groupby('release_year')['revenue'].transform('mean')\ndf['mean_revenue_year'].plot(figsize=(15,5))\nplt.xticks(np.arange(1920,2018,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['mean_revenue_year']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['mean_revenue_month'] = df.groupby('release_month')['revenue'].transform('mean')\n\ndf['mean_revenue_month'].plot(figsize=(15,5))\nplt.xticks(np.arange(1,13))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['mean_revenue_day'] = df.groupby('release_day')['revenue'].transform('mean')\n\ndf['mean_revenue_day'].plot(figsize=(15,5))\nplt.xticks(np.arange(1,32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['mean_dayofweek'] = df.groupby('release_dayofweek')['revenue'].transform('mean')\n\ndf['mean_dayofweek'].plot(figsize=(15,5))\nplt.xticks(np.arange(0,7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['mean_quarter'] = df.groupby('release_quarter')['revenue'].transform('mean')\n\ndf['mean_quarter'].plot(figsize=(15,5))\nplt.xticks(np.arange(1,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##import datetime\n# 公開日の欠損1件 id=3829\n# May,2000 (https://www.imdb.com/title/tt0210130/) \n# 日は不明。1日を入れておく\n##df.loc[3829, \"release_date\"] = \"5/1/00\"\n\n##df[\"release_year\"] = pd.to_datetime(df[\"release_date\"]).dt.year.astype(int)\n# 年の20以降を、2020年より後の未来と判定してしまうので、補正。\n##df.loc[df[\"release_year\"]>2020, \"release_year\"] = df.loc[df[\"release_year\"]>2020, \"release_year\"]-100\n\n##df[\"release_month\"] = pd.to_datetime(df[\"release_date\"]).dt.month.astype(int)\n##df[\"release_day\"] = pd.to_datetime(df[\"release_date\"]).dt.day.astype(int)\n\n# datetime型に\n##df[\"release_date\"] = df.apply(lambda s: datetime.datetime(\n##    year=s[\"release_year\"],month=s[\"release_month\"],day=s[\"release_day\"]), axis=1)\n\n##df[\"release_dayofyear\"] = df[\"release_date\"].dt.dayofyear\n##df[\"release_dayofweek\"] = df[\"release_date\"].dt.dayofweek\n\n# 月、曜日は カテゴリ型に\n##df[\"release_month\"] = df[\"release_month\"].astype('category')\n##df[\"release_dayofweek\"] = df[\"release_dayofweek\"].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## belongs to collection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# collection 名を抽出\ndf[\"collection_name\"] = df[\"belongs_to_collection\"].apply(lambda x : x[0][\"name\"] if len(x)>0 else \"nashi\")\n# 無い場合、\"nashi\"に","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# シリーズの作品数\n#df = pd.merge( df, df.groupby(\"collection_name\").count()[[\"budget\"]].rename(columns={\"budget\":\"count_collection\"}), \n#         on=\"collection_name\", how=\"left\")\n# indexがずれるので、戻す\n#df.index = df.index+1\n\ndf[\"count_collection\"] = df[\"collection_name\"].apply(lambda x : (df[\"collection_name\"]==x).sum())\n# シリーズ以外の場合0\ndf.loc[df[\"collection_name\"]==\"nashi\", \"count_collection\"] = 0\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# シリーズ何作目か\ndf[\"number_in_collection\"] = df.sort_values(\"release_date\").groupby(\"collection_name\").cumcount()+1\n# シリーズ以外の場合0\ndf.loc[df[\"collection_name\"]==\"nashi\", \"number_in_collection\"] = 0\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 同シリーズの自分より前の作品の平均log(revenue)\ndf[\"collection_av_logrevenue\"] = [ df.loc[(df[\"collection_name\"]==row[\"collection_name\"]) & \n                                          (df[\"number_in_collection\"]<row[\"number_in_collection\"]),\n                                          \"log_revenue\"].mean() \n     for key,row in df.iterrows() ]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 欠損(nashi) の場合、nashi での平均\ndf.loc[df[\"collection_name\"]==\"nashi\", \"collection_av_logrevenue\"] = df.loc[df[\"collection_name\"]==\"nashi\", \"log_revenue\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train に無くtestだけにあるシリーズの場合、シリーズもの全部の平均\ncollection_mean = df.loc[df[\"collection_name\"]!=\"nashi\", \"log_revenue\"].mean()  # シリーズもの全部の平均\ndf[\"collection_av_logrevenue\"] = df[\"collection_av_logrevenue\"].fillna(collection_mean)  \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 連結","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features = pd.concat(dfdic_feature, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 整形","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[[\"original_language\", \"collection_name\"]] = df[[\"original_language\", \"collection_name\"]].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use = df[['budget', 'has_homepage', 'popularity','runtime','n_language', \n             \"num_Keywords\", \"speak_English\",\n             'release_year', 'release_month','release_day','release_dayofweek', \n             'mean_revenue_year','mean_revenue_day','collection_av_logrevenue' ,\"count_collection\",\"number_in_collection\"\n            ]]\ndf_use.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use = pd.get_dummies(df_use)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Additional data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_add = pd.read_csv('../input/tmdb-competition-additional-features/TrainAdditionalFeatures.csv')\ntest_add = pd.read_csv('../input/tmdb-competition-additional-features/TestAdditionalFeatures.csv')\ntrain_add.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.merge(df, pd.concat([train_add, test_add]), on=\"imdb_id\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"add_cols = [\"popularity2\", \"rating\", \"totalVotes\"]\ndf[add_cols] = df[add_cols].fillna(df[add_cols].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2 = pd.read_csv('../input/tmdb-box-office-prediction-more-training-data/additionalTrainData.csv')\ntrain3 = pd.read_csv('../input/tmdb-box-office-prediction-more-training-data/trainV3.csv')\ntrain3.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 言語処理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#全て小文字に変換\ndef lower_text(text):\n    return text.lower()\n\n#記号の排除\ndef remove_punct(text):\n    text = text.replace('-', ' ')  # - は単語の区切りとみなす\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ndef remove_stopwords(words, stopwords):#不要な単語を削除\n    words = [word for word in words if word not in stopwords]\n    return words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 英語以外","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 英語でよく使う単語が入っていない文章を確認\n#df.loc[df[\"overview\"].apply(lambda x : str(x)).apply(lambda x : lower_text(x)\n#                                ).str.contains(\"nan|the|where|with|from|and|for|his|her|over\")==False, \"overview\"]\n#train3.loc[train3[\"overview\"].apply(lambda x : str(x)).apply(lambda x : lower_text(x)).str.contains(\"nan|the|where|with|from|and|for|his|her|over\")==False, \"overview\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_english_overview_id = [157, 2863, 4616]   # 上のデータを目で確認\nno_english_tagline_id = [3255, 3777, 4937]   # Tfidf で非英語の単語があったもの","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## overview","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#単語数\ndf['overview_word_count'] = df['overview'].apply(lambda x: len(str(x).split()))\n#文字数\n#df['overview_char_count'] = df['overview'].apply(lambda x: len(str(x)))\n# 記号の個数\n#df['overview_punctuation_count'] = df['overview'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 前処理\ndf['_overview']=df['overview'].apply(lambda x : str(x)\n                            ).apply(lambda x : lower_text(x)).apply(lambda x : remove_punct(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#単語数\ndf['tagline_word_count'] = df['tagline'].apply(lambda x: len(str(x).split()))\n#文字数\n#df['tagline_char_count'] = df['tagline'].apply(lambda x: len(str(x)))\n# 記号の個数\n#df['tagline_punctuation_count'] = df['tagline'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['_tagline']=df['tagline'].apply(lambda x : str(x)\n                                 ).apply(lambda x : lower_text(x)).apply(lambda x : remove_punct(x))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## titleの前処理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#単語数\ndf['title_word_count'] = df['title'].apply(lambda x: len(str(x).split()))\n#文字数\n#df['title_char_count'] = df['title'].apply(lambda x: len(str(x)))\n# 記号の個数\n#df['title_punctuation_count'] = df['title'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use2 = df[[\"has_homepage\",\"runtime\",'budget']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## cast","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#castの中にある俳優の名前をリスト化させる\nlist_of_cast_names = list(df['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ndf['num_cast'] = df['cast'].apply(lambda x: len(x) if x != {} else 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_cast_genders = list(df['cast'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\n\ndf['genders_0_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ndf['genders_1_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ndf['genders_2_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Crew","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#crewのname\nlist_of_crew_names = list(df['crew'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ndf['num_crew'] = df['crew'].apply(lambda x: len(x) if x != {} else 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"department_count = pd.Series(Counter([job for lst in df[\"crew\"].apply(lambda x : [ i[\"department\"] for i in x]).values for job in lst]))\ndepartment_count.sort_values(ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"job_count = pd.Series(Counter([job for lst in df[\"crew\"].apply(lambda x : [ i[\"job\"] for i in x]).values for job in lst]))\njob_count.sort_values(ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_crew = { idx : pd.DataFrame([ [crew[\"department\"], crew[\"job\"], crew[\"name\"]] \n                        for crew in x], columns=[\"department\", \"job\", \"name\"]) \n    for idx, x in df[\"crew\"].iteritems() }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_crew = pd.concat(df_crew)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def select_job(list_dict, key, value):\n    return [ dic[\"name\"] for dic in list_dict if dic[key]==value]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for department in department_count.index:\n    df['dep_{}_num'.format(department)] = df[\"crew\"].apply(select_job, key=\"department\", value=department).apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_crewname = pd.DataFrame([], index=df.index)\nfor job in [\"Producer\", \"Director\", \"Screenplay\", \"Casting\", \"Original Music Composer\"]:\n    col = 'job_{}_list'.format(job)\n    df[col] = df[\"crew\"].apply(select_job, key=\"job\", value=job)\n\n    top_list = [m[0] for m in Counter([i for j in df[col] for i in j]).most_common(15)]\n    for i in top_list:\n        df_crewname['{}_{}'.format(job,i)] = df[col].apply(lambda x: i in x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for job in [\"Sound\", \"Art\", \"Costume & Make-Up\", \"Camera\", \"Visual Effects\"]:\n    col = 'department_{}_list'.format(job)\n    df[col] = df[\"crew\"].apply(select_job, key=\"department\", value=job)\n\n    top_list = [m[0] for m in Counter([i for j in df[col] for i in j]).most_common(15)]\n    for i in top_list:\n        df_crewname['{}_{}'.format(job,i)] = df[col].apply(lambda x: i in x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"raw","source":"df['genders_0_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ndf['genders_1_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ndf['genders_2_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 整理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nwith open(\"/kaggle/input/private-jhk/df_use_nagano.pkl\",\"rb\") as fr:\n    df_use_nagano = pickle.load(fr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use_nagano","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use_nagano = df_use_nagano[['production_countries_count', 'production_companies_count']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use4 = df[add_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_input = pd.concat([df_use, df_use4, df_features,df_use_nagano], axis=1) # .drop(\"belongs_to_collection\", axis=1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 欠測ナシを確認\ndf_input.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"ln_revenue\"] = np.log(df[\"revenue\"]+1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# budget","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_input['log_budget'] = np.log10(df_input['budget'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_input['budget/popularity1'] = df_input['budget']/df_input['popularity']\ndf_input['budget/popularity2'] = df_input['budget']/df_input['popularity2']\ndf_input['budget/runtime'] = df_input['budget']/df_input['runtime']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_input['budget/popularity1'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_input['budget/popularity2'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_input['budget/runtime'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Popularity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_input['_popularity_mean_year']=df['popularity']/df.groupby(\"release_year\")[\"popularity\"].transform('mean')\ndf_input['_budget_runtime_ratio']=df['budget']/df['runtime']\ndf_input['_budget_popularity_ratio']=df['budget']/df['popularity']\ndf_input['_budget_year_ratio']=df['budget']/(df['release_year']*df['release_year'])\ndf_input['_releaseYear_popularity_ratio']=df['release_year']/df['popularity']\ndf_input['_releaseYear_popularity_ratio2']=df['popularity']/df['release_year']\ndf_input['_popularity_totalVotes_ratio']=df['totalVotes']/df['popularity']\ndf_input['_rating_popularity_ratio']=df['rating']/df['popularity']\ndf_input['_rating_totalVotes_ratio']=df['totalVotes']/df['rating']\ndf_input['_totalVotes_releaseYear_ratio']=df['totalVotes']/df['release_year']\ndf_input['_budget_rating_ratio']=df['budget']/df['rating']\ndf_input['_runtime_rating_ratio']=df['runtime']/df['rating']\ndf_input['_budget_totalVotes_ratio']=df['budget']/df['totalVotes']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 学習用データ作成","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = df_input.loc[:, df_input.isnull().sum()>0].columns\ndf_input.loc[:, cols] = df_input[cols].fillna(df_input[cols].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 保存\nimport pickle\nwith open('df_input.pkl', 'wb') as f:\n      pickle.dump(df_input , f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 数値化できい列を確認\nno_numeric = df_input.apply(lambda s:pd.to_numeric(s, errors='coerce')).isnull().all()\nno_numeric[no_numeric]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_input.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df_input.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[df_input.isnull().sum()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_all = df_input  # .drop([\"collection_av_logrevenue\"], axis=1)\nX_all.drop([0],inplace = True)\ny_all = df[\"ln_revenue\"]\ny_all.index = X_all.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_all.drop(columns = ['budget'],inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nX_all = X_all.drop(columns = ['__genre____Fantasy__',\n '__original_language____cn__',\n '__original_language____it__',\n '__production_companies____Columbia_Pictures_Corporation__',\n '__original_language____ko__',\n '__production_companies____Walt_Disney_Pictures__',\n '__production_companies____Twentieth_Century_Fox_Film_Corporation__',\n '__original_language____ta__',\n '__genre____History__',\n '__production_companies____TriStar_Pictures__',\n '__production_companies____Metro_Goldwyn_Mayer__MGM___',\n '__production_countries____Russia__',\n '__original_language____small__',\n '__production_companies____Warner_Bros___',\n '__genre____War__',\n '__original_language____ru__',\n '__production_countries____Hong_Kong__',\n '__genre____Animation__',\n '__production_companies____Columbia_Pictures__',\n '__original_language____ja__',\n '__production_companies____New_Line_Cinema__',\n '__original_language____de__',\n '__genre____Science_Fiction__',\n '__production_countries____Spain__',\n '__genre____Adventure__',\n '__genre____Mystery__',\n '__original_language____es__',\n '__genre____Music__',\n '__genre____Horror__',\n '__original_language____hi__',\n '__original_language____en__',])\n '''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[ c for c in X_all.columns if \"revenue\" in str(c)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#標準化\n#X_train_all_mean = X_all[:3000].mean()\n#X_train_all_std  = X_all[:3000].std()\n#X_all = (X_all-X_train_all_mean)/X_train_all_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X = X_all.iloc[3000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, val_X, train_y, val_y = train_test_split(X_all.iloc[:3000], \n                                                  y_all.iloc[:3000], \n                                                  test_size=0.25, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nrandom_seed = 2019\nk = 10\nfold = list(KFold(k, shuffle = True, random_state = random_seed).split(train))\nnp.random.seed(random_seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost.sklearn import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nxgb = XGBRegressor()\n\n'''\nparams = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['reg:linear'],\n              'learning_rate': [0.03], #so called `eta` value\n              'max_depth': [4],\n              'min_child_weight': [4],\n              'silent': [1],\n              'subsample': [0.7],\n              'colsample_bytree': [0.3],\n              'n_estimators': [500]}\n\nxgb_grid = GridSearchCV(xgb,\n                        params,\n                        cv = 4,\n                        n_jobs = 5,\n                        verbose=True)\n\nxgb_grid.fit(train_X, train_y)\nprint(xgb_grid.best_score_)\nprint(xgb_grid.best_params_)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = XGBRegressor(max_depth=4, \n                            min_child_weight=4,\n                            learning_rate=0.03, \n                            n_estimators=500, \n                            objective='reg:linear',\n                            nthread = 4,\n                            gamma=1.3,  \n                            silent=1,\n                            subsample=0.7, \n                            colsample_bytree=0.3, \n                            colsample_bylevel=0.5)\nxgb_model.fit(train_X,train_y)\nxgb_prediction = xgb_model.predict(val_X)\nxgb_rmse = mean_squared_error(val_y, xgb_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nimportances = pd.Series(xgb_model.feature_importances_, index = X_all.columns)\nimportances = importances.sort_values()\nimportances.plot(kind = \"barh\")\nplt.title(\"imporance in the xgboost Model\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\nmath.sqrt(xgb_rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_pred = xgb_model.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_xgb = pd.DataFrame(np.exp(xgb_pred)-1,columns=[\"revenue\"])\npred_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id = test[\"id\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub=pd.concat([test_id, pred_xgb],axis=1)\nsub.to_csv('TMDB_xgb.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor\n\nlgbm = LGBMRegressor()\n'''\nparams = {'n_estimators': [500],\n          'objection' :['regressor'],\n          'metric':['rmse'],\n          'max_depth ': [2],\n          'num_leaves':[10],\n          'min_child_samples':[500],\n          'learning_rate':[0.01],\n          'boosting ': ['gbdt'],\n          'num_iterations' : [1500],\n          'min_data_in_leaf': [10],\n          'bagging_freq ': [1],\n          'bagging_fraction ': [0.9],\n          'feature_fraction' : [0.7],\n          'importance_type':['gain'],\n          'use_best_model':[True]}\n\nlgbm_grid = GridSearchCV(estimator=lgbm, param_grid=params,cv=4, n_jobs=5, verbose=True)\n\nlgbm_grid.fit(train_X, train_y)\nprint(lgbm_grid.best_score_)\nprint(lgbm_grid.best_params_)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_model = LGBMRegressor(n_estimators= 500,\n          objection ='regressor',\n          metric='rmse',\n          max_depth = 2,\n          num_leaves=10,\n          min_child_samples=500,\n          learning_rate=0.01,\n          boosting = 'gbdt',\n          num_iterations = 1500,\n          min_data_in_leaf= 10,\n          bagging_freq = 1,\n          bagging_fraction = 0.9,\n          feature_fraction = 0.7,\n          importance_type='gain',\n          use_best_model=True)\nlgbm_model.fit(train_X, train_y)\nlgbm_prediction = lgbm_model.predict(val_X)\nlgbm_rmse = mean_squared_error(val_y, lgbm_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nimportances = pd.Series(lgbm_model.feature_importances_, index = X_all.columns)\nimportances = importances.sort_values()\nimportances.plot(kind = \"barh\")\nplt.title(\"imporance in the LightGBM Model\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"math.sqrt(lgbm_rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_pred = lgbm_model.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_lgbm = pd.DataFrame(np.exp(lgbm_pred)-1,columns=[\"revenue\"])\npred_lgbm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub=pd.concat([test_id, pred_lgbm],axis=1)\nsub.to_csv('TMDB_lgbm.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor\n'''\ncat = CatBoostRegressor()\n\nparams = {'iterations' : [2000], \n                                 'learning_rate' : [0.01], \n                                 'depth' : [6], \n                                 'eval_metric' : ['RMSE'],\n                                 'colsample_bylevel' : [0.6],\n                                 'bagging_temperature' : [0.1],\n                                 'early_stopping_rounds' : [200]}\n\ncat_grid = GridSearchCV(estimator=cat, param_grid=params,cv=4, n_jobs=5, verbose=True)\n\ncat_grid.fit(train_X, train_y)\nprint(cat_grid.best_score_)\nprint(cat_grid.best_params_)\n'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_model = CatBoostRegressor(iterations=2000, \n                                 learning_rate=0.01, \n                                 depth=6, \n                                 eval_metric='RMSE',\n                                 colsample_bylevel=0.6,\n                                 bagging_temperature = 0.1,\n                                 metric_period = None,\n                                 early_stopping_rounds=200)\ncat_model.fit(train_X, train_y)\ncat_prediction = cat_model.predict(val_X)\ncat_rmse = mean_squared_error(val_y, cat_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"math.sqrt(cat_rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nimportances = pd.Series(cat_model.feature_importances_, index = X_all.columns)\nimportances = importances.sort_values()\nimportances.plot(kind = \"barh\")\nplt.title(\"imporance in the CatBoost Model\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_pred = cat_model.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_cat = pd.DataFrame(np.exp(cat_pred)-1,columns=[\"revenue\"])\npred_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub=pd.concat([test_id, pred_cat],axis=1)\nsub.to_csv('TMDB_cat.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ansamble = 0.4 * pred_lgbm[\"revenue\"] + 0.2 * pred_xgb[\"revenue\"] + 0.4 * pred_cat[\"revenue\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub3=pd.concat([test_id, ansamble],axis=1)\nsub3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub3.to_csv('TMDB_ansamble.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ansamble2 = 0.35 * pred_lgbm[\"revenue\"] + 0.3 * pred_xgb[\"revenue\"] + 0.35 * pred_cat[\"revenue\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub4=pd.concat([test_id, ansamble2],axis=1)\nsub4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub4.to_csv('TMDB_ansamble2.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ansamble = 0.2 * pred_lgbm[\"revenue\"] + 0.2 * pred_xgb[\"revenue\"] + 0.6 * pred_cat[\"revenue\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub5=pd.concat([test_id, ansamble],axis=1)\nsub5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub3.to_csv('TMDB_ansamble3.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}