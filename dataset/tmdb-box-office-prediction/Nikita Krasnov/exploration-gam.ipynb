{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Overview"},{"metadata":{},"cell_type":"markdown","source":"It's worth to mention several moments before looking at the data more carefully:\n\nBusiness case:\n1. Some variables in dataset contain information that we get only after film was actually released( per example popularity )\n2. Each country usually create own poster for the audience. So using poster analysis for global revenue is not fully correct\n3. It is better to predict box office revenue for the fix amount of days for country with first release date. More accurately to mitigate affect of rising prices for the cinema from year to year and rising cinemas themselves it is better to use some sort of ratio spectators/cinemas.\n\nMath methods:\n1. To get information from texts variables, where there are no big connection between items( like list of actors, crew, genres and production companies ) I'm using PMI(pairwise mutual index ) to create embeddings and kmeans for final clustering.\n2. For bigger and more connected words ( like overview ot taglines ) I'm using averaging of wordvec representation.\n3. For images I'm using pretrained ResNet50. It is hard to understand the differrence in final labels, but these clustering gives additional information\n"},{"metadata":{"id":"Ah6y2lUgs5rC","colab_type":"text"},"cell_type":"markdown","source":"## Additional modules"},{"metadata":{"id":"_SODqrQys3pt","colab_type":"code","outputId":"53c3087a-cb9d-4344-e223-faae0b8fd096","colab":{"base_uri":"https://localhost:8080/","height":658},"trusted":true},"cell_type":"code","source":"!pip -q install pyLDAvis\n!python3 -q -m spacy download en\n!pip -q install pygam","execution_count":null,"outputs":[]},{"metadata":{"id":"EUmCwA-ps8aj","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing import image\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.preprocessing import image\nfrom keras.applications.resnet50 import preprocess_input as resnet50_preprocess, decode_predictions\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom tqdm import tqdm\n\nimport gensim.corpora as corpora\nimport gensim\nfrom gensim.utils import lemmatize, simple_preprocess\nimport time\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import TruncatedSVD\nimport re\nfrom pprint import pprint\nimport spacy\nfrom sklearn.metrics import silhouette_score, mean_squared_error\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\nfrom collections import Counter\nimport seaborn as sns\n\n####Ignoring warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom pprint import pprint\nfrom scipy.sparse import csc_matrix\nfrom scipy.sparse.linalg import svds, eigs\n\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"id":"q4VaX1mm3rUZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d752e101-883a-4462-861f-610e65a6b1bc","trusted":true},"cell_type":"code","source":"pd.read_csv('../input/tmdb-box-office-prediction/test.csv').shape","execution_count":null,"outputs":[]},{"metadata":{"id":"WFka4l5QtJmr","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"input_data = pd.read_csv('../input/tmdb-box-office-prediction/train.csv')\ntest = pd.read_csv('../input/tmdb-box-office-prediction/test.csv')\nfull_data = pd.concat([input_data, test])","execution_count":null,"outputs":[]},{"metadata":{"id":"_Xgt3Dzc6dr3","colab_type":"text"},"cell_type":"markdown","source":"* <b> text fields </b> <br>\n\nKeywords - several tags, phrases which is described current film\n  1.  label_keyword = PMI cluster\n  2.  amount_keyword\n  \ncast - json of actors<br>\n  1. label_cast = PMI cluster\n\ncrew - json of film stuff ( directoring, camera, costumes , light ,music and etc. ) <br>\n 1. label_crew = PMI cluster\n 2. Previous info on director ( years to lst film, film amount)\n \ngenres - json of genres, characterizing film  <br>\n  1. label_genre_list = PMI cluster\n  \noverview - small text about film <br>\n  1. label_overview = Wordvec average cluster\n  \ntagline - key lines from film or characterizing film ( per example \"I'll be back\" from terminator )\n  1. label_tagline = Wordvec average cluster <br>\n  \nproduction_companies - companies like Paramount, 20th Century Fox <br>\n  1. label_prod_list = PMI cluster\nproduction_countries  : PMI cluster DONE <br> \n\nbelongs_to_collection <br> \n  1.  Previous info on collection ( years to lst film, film amount): \n\ntitle <br>\n\n\n* <b> images </b> <br>\nposter_image  <br>\n 1. Resnet50 cluster\n\n* <b> regular variables </b><br>\npopularity <br>\nruntime <br>\nrelease_date <br>\n 1. year, month, day <br>\n \nbudget <br>\n* <b> TARGET </b><br>\nlog_revenue\n"},{"metadata":{"id":"z_FjRaxkHLU5","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold\nfrom sklearn.cluster import KMeans\nkfold = KFold(n_splits = 5, random_state = 32)\ntrain_id = []\nval_id = []\nfor (x1,x2) in kfold.split(np.array(np.arange(input_data.shape[0]), ndmin = 2 ).T):\n    train_id.append(x1)\n    val_id.append(x2)","execution_count":null,"outputs":[]},{"metadata":{"id":"3HpHmQiiKYKJ","colab_type":"code","outputId":"da533988-ae25-413f-ca30-d04571d022c8","colab":{"base_uri":"https://localhost:8080/","height":268},"trusted":true},"cell_type":"code","source":"input_data['log_revenue'] = np.log(input_data['revenue'])\nlog_rev_distribution = sns.boxplot(input_data['log_revenue'].values)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZBhm4neIMEcD","colab_type":"code","outputId":"242ac0de-1c28-44e2-816d-438691498efd","colab":{"base_uri":"https://localhost:8080/","height":70},"trusted":true},"cell_type":"code","source":"Q1 = input_data['log_revenue'].quantile(0.25)\nQ3 = input_data['log_revenue'].quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)\nprint('amount of outliers: {}'.format(\ninput_data[(input_data['log_revenue'] < (Q1 - 1.5 * IQR)) | (input_data['log_revenue'] > (Q3 + 1.5 * IQR) ) ].shape[0]\n) )","execution_count":null,"outputs":[]},{"metadata":{"id":"9buBG8gA5_HV","colab_type":"text"},"cell_type":"markdown","source":"## Additional variables"},{"metadata":{"id":"htyhIbTGi85d","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def var_creation(df, flag_train = True):\n  \n    df['release_year'] = df['release_date'].fillna('01/01/00').apply( lambda x: int('19' + x.split('/')[-1]) if int(x.split('/')[-1]) > 20 else int('20' + x.split('/')[-1]) ) \n    df['years_to_release'] = 2020 - df['release_year']\n    df['release_month'] = df['release_date'].fillna('01/01/00').apply( lambda x: int(x.split('/')[0]) )\n    df['release_day'] = df['release_date'].fillna('01/01/00').apply( lambda x: int(x.split('/')[1]) )\n    df['flag_release_season'] =  df['release_month'].apply(lambda x: x//3  )\n\n    df['runtime'].fillna(np.median(df['runtime'].dropna()), inplace = True)\n\n\n    df['budget'].fillna(np.median(df['budget'].dropna()), inplace = True)\n    #replace seros with mean \n    df['budget'].replace(0,np.median(df['budget'].dropna()), inplace = True)\n\n    df['log_budget'] = np.log(df['budget'])\n    df['flag_en'] = df['original_language'].apply(lambda x: 1 if x == 'en' else 0 )\n    df['release_dt'] = df.apply(lambda x: datetime(x['release_year'], x['release_month'], x['release_day']), axis = 1)\n\n    df['crew_director'] = df['crew'].fillna('').apply(lambda x : ''.join([item['name'] for item in eval(x) if  item['job'] == 'Director'  ][:1]) if  x != '' else '' )\n    df['release_dt'] = df.apply(lambda x: datetime(x['release_year'], x['release_month'], x['release_day']), axis = 1)\n\n\n    df['genre_list'] = df['genres'].fillna('').apply(lambda x: [w['name'] for w in eval(x)  ] if  x != '' else [])\n    df['country_list'] = df['production_countries'].fillna('').apply(lambda x : [item['name'] for item in eval(x) ] if  x != '' else [] )\n    df['keywords_list'] =  df['Keywords'].fillna('').apply(lambda x : [item['name'] for item in eval(x)] if  x != '' else [] )\n    df['crew_list'] =  df['crew'].fillna('').apply(lambda x : [item['name'] for item in eval(x)] if  x != '' else [] )\n    df['cast_list'] =  df['cast'].fillna('').apply(lambda x : [item['name'] for item in eval(x)] if  x != '' else [] )\n    df['prod_list'] =  df['production_companies'].fillna('').apply(lambda x : [item['name'] for item in eval(x)] if  x != '' else [] )\n    df['overview_fst'] = df['overview'].fillna('').apply(lambda x: ' '.join(x.split(' ')[:50]) )\n\n    if flag_train == True:\n        df['log_revenue'] = np.log(df['revenue'] )\n\n    return df\n\n\ninput_data = var_creation(input_data,flag_train = True)\ntest = var_creation(test,flag_train = False)\nfull_data = var_creation(full_data,flag_train = True)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"cGVNlamYH1Ek","colab_type":"text"},"cell_type":"markdown","source":"## PMI text encoding"},{"metadata":{"id":"ucAPe6jd2yII","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def get_emb_by_pmi(text, column, top_w, flag_svd = False ):\n  \n \n  text_item_dict = sorted(dict(Counter(text.sum())).items(), key = lambda x: x[1], reverse = True)\n\n  df = pd.DataFrame(text_item_dict)\n  df['rate'] = df[1].cumsum()/df[1].sum()\n\n  item_to_idx = dict([(item, idx) for (idx, item) in enumerate(df[df['rate'] <= top_w][0].values)])\n  idx_to_item = dict([(idx, item) for (idx, item) in enumerate(df[df['rate'] <= top_w][0].values)])\n\n  item_df = pd.DataFrame( data = np.stack(text.apply( lambda x: [1 if item in x  else 0 for (item, c) in item_to_idx.items()] ).values),\n               columns =  [item for (item, _) in item_to_idx.items()] )\n  imem_df = item_df[item_df.sum(axis = 1) >0 ]\n\n  \n  N = item_df.shape[0]\n  V = len(item_to_idx)\n  add = 1\n  # V = 0\n  pmi_matrix = []\n  for (item, _) in item_to_idx.items():\n    pmi_matrix.append( item_df[item_df[item] == 1].sum().values )\n\n  pmi_matrix = (( np.stack(pmi_matrix) + add ) /( N+V) )/ np.dot( np.array( ( item_df.sum(axis = 0 ).values + add ) / (N+V) , ndmin = 2 ).T,\n         np.array(( item_df.sum(axis = 0 ).values + add ) / (N+V), ndmin = 2 )\n          )\n\n  for i in range(pmi_matrix.shape[0]):\n    for j in range(pmi_matrix.shape[0]):\n      pmi_matrix[i][j] = 0 if pmi_matrix[i][j] < 1 else np.log(pmi_matrix[i][j])\n\n  #using pmi_matrix as emb vectors 0 for unknown genres\n  emb_genre_to_idx = dict([(item,item_to_idx[item]+1) for item, _ in item_to_idx.items() ])\n  emb_matrix = np.vstack( [np.mean(pmi_matrix,axis =0),\n      pmi_matrix])\n  \n  if flag_svd:\n#     svd = TruncatedSVD(n_components = 100, random_state = 42)\n    m = csc_matrix(emb_matrix, dtype=float)\n    u, s, vt = svds(m, k=100)\n    emb_matrix = u*s\n#     print('SVD explained ratio', sum(svd.explained_variance_ratio_) )\n    \n  return item_to_idx, emb_matrix\n\n# column = 'genres'\n# top_w = 1\n# i = 1 \n# train = pd.merge( pd.DataFrame( data = train_id[1].reshape(-1), columns = ['id']), input_data )\n# get_emb_by_pmi(train, column, top_w )\n\ndef fit_pmi_cluster(inp, column, top_w, nums_cluster = [2,4,8,16,32,64],flag_svd = False):\n  \n  base_rmse = {}\n  train_rmse = {}\n  val_rmse = {}\n  sil_score = {}\n    \n  for k in nums_cluster :\n    \n    base_rmse[k] = []\n    train_rmse[k] = []\n    val_rmse[k] = []\n    sil_score[k] = []\n      \n    st = time.time()\n    for i in range(5):\n\n      sc = StandardScaler()\n      train = pd.merge( pd.DataFrame( data = train_id[i].reshape(-1), columns = ['id']), inp )\n      \n      text = train[train[column].apply(lambda x: len(x) >= 1)][column] \n      item_to_idx, emb_matrix = get_emb_by_pmi(text = text, column = column, top_w = top_w, flag_svd = flag_svd)\n      train['encode'] = train[column].apply(lambda x: [0] if len([w for w in x if item_to_idx.get(w) is not None]) == 0 else [item_to_idx.get(w) for w in x if item_to_idx.get(w) is not None] ) \n      train['emb'] = train['encode'].apply(lambda x: np.mean(np.vstack([emb_matrix[i] for i in x]), axis = 0) )\n      x_train = np.stack(train['emb'].values)\n      x_train_sc = sc.fit_transform(x_train)\n      \n      val = pd.merge( pd.DataFrame( data = val_id[i].reshape(-1), columns = ['id']), inp )\n      \n      \n      val['encode'] = val[column].apply(lambda x: [0] if len([w for w in x if item_to_idx.get(w) is not None]) == 0 else [item_to_idx.get(w) for w in x if item_to_idx.get(w) is not None])\n      val['emb'] = val['encode'].apply(lambda x: np.mean(np.vstack([emb_matrix[i] for i in x]), axis = 0) )\n      x_val = np.stack(val['emb'].values)\n      x_val_sc = sc.fit_transform(x_val)\n\n\n      kmeans = KMeans(n_clusters = k, random_state = 42)\n      kmeans.fit(x_train_sc)\n\n      train['label'] = kmeans.predict(x_train_sc)\n      train_group = train.groupby('label').aggregate({'log_revenue': ['mean','count']}).reset_index()\n      train_group.columns = train_group.columns.map('_'.join).str.strip('_')\n\n      train = pd.merge(train , train_group , on = 'label', how ='left')\n      val['label'] = kmeans.predict(x_val_sc)\n      val = pd.merge(val , train_group , on = 'label', how = 'left')\n      val['log_revenue_mean'] = val['log_revenue_mean'].fillna(train['log_revenue'].mean())   \n\n      train_rmse_base = np.sqrt(mean_squared_error(train['log_revenue'], np.repeat(train['log_revenue'].mean() , train.shape[0]))) \n\n      base_rmse[k].append(train_rmse_base)\n      sil_score[k].append(silhouette_score(x_train_sc, kmeans.labels_))\n      train_rmse[k].append(np.sqrt(mean_squared_error(train['log_revenue'], train['log_revenue_mean']) ))\n      val_rmse[k].append(np.sqrt(mean_squared_error(val['log_revenue'], val['log_revenue_mean']) ))\n    print(k, 'clusters Done', (time.time() - st)/60, 'min' )\n    \n  return  base_rmse, train_rmse,val_rmse, sil_score\n\n\ndef get_pmi_cluster(inp, column, top_w, nums_cluster, item_to_idx, emb_matrix, sc, kmeans,  flag_svd = False, flag_train = True):\n  \n     \n  text = inp[inp[column].apply(lambda x: len(x) >= 1)][column] \n\n  if flag_train:\n    sc = StandardScaler()\n\n    item_to_idx, emb_matrix = get_emb_by_pmi(text = text, column = column, top_w = top_w, flag_svd = flag_svd)\n\n    inp['encode'] = inp[column].apply(lambda x: [0] if len([w for w in x if item_to_idx.get(w) is not None]) == 0 else [item_to_idx.get(w) for w in x if item_to_idx.get(w) is not None] ) \n    inp['emb'] = inp['encode'].apply(lambda x: np.mean(np.vstack([emb_matrix[i] for i in x]), axis = 0) )\n    x = np.stack(inp['emb'].values)\n    sc.fit(x)\n    x_sc = sc.transform(x)\n    kmeans = KMeans(n_clusters = nums_cluster, random_state = 42)\n    kmeans.fit(x_sc)\n\n  else:\n\n    inp['encode'] = inp[column].apply(lambda x: [0] if len([w for w in x if item_to_idx.get(w) is not None]) == 0 else [item_to_idx.get(w) for w in x if item_to_idx.get(w) is not None] ) \n    inp['emb'] = inp['encode'].apply(lambda x: np.mean(np.vstack([emb_matrix[i] for i in x]), axis = 0) )\n    x = np.stack(inp['emb'].values)\n    x_sc = sc.transform(x)\n\n\n  inp['label_' + column] = kmeans.predict(x_sc)\n\n  return  inp, item_to_idx, emb_matrix, sc, kmeans\n","execution_count":null,"outputs":[]},{"metadata":{"id":"mP91dZusTGGT","colab_type":"text"},"cell_type":"markdown","source":"### Country production  ( Best number for cluster = 4 )"},{"metadata":{"id":"xpklT3bpTFrQ","colab_type":"code","outputId":"03b7ac2c-e240-4d26-bc73-085c53ae4b25","colab":{"base_uri":"https://localhost:8080/","height":405},"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(sorted(dict(Counter(input_data['country_list'].sum())).items(), key = lambda x: x[1], reverse = True))\ndf['rate'] = df[1].cumsum()/df[1].sum()\nfig, ax = plt.subplots(1,2, figsize = (12,4))\nax[0].plot(df['rate'])\nax[0].set_title('Cumulative plot per keywords')\nax[1].barh( input_data['country_list'].apply(lambda x: len(x)).value_counts().index, \n         input_data['country_list'].apply(lambda x: len(x)).value_counts() )\nax[1].set_title('Amout of films VS number of production countries')\n\nprint(df.head(5))\nprint('25% of actors cover 50-60%  of all cast')","execution_count":null,"outputs":[]},{"metadata":{"id":"Y-tTGtZEQ3Pu","colab_type":"code","outputId":"1c7637b5-f06d-442a-fdfc-6eb103f4660e","colab":{"base_uri":"https://localhost:8080/","height":87},"trusted":true},"cell_type":"code","source":"base_rmse, train_rmse,val_rmse, sil_score = fit_pmi_cluster(inp = input_data[['id','log_revenue','country_list']], column = 'country_list', \n                                                            top_w = 1 , nums_cluster = [2,4,8,16] )","execution_count":null,"outputs":[]},{"metadata":{"id":"xREjU3bkTR4Q","colab_type":"code","outputId":"f70145da-8ace-4220-d54a-14068cf04a33","colab":{"base_uri":"https://localhost:8080/","height":461},"trusted":true},"cell_type":"code","source":"nums_cluster = [2,4,8,16]\nfig, ax = plt.subplots(1,1, figsize = (8,6))\nax.plot(nums_cluster,[np.mean(base_rmse[i]) for i in nums_cluster], ls = '--', color = 'black' )\nax.plot(nums_cluster,[np.mean(train_rmse[i]) for i in nums_cluster], color = 'red' )\nax.plot(nums_cluster,[np.mean(val_rmse[i]) for i in nums_cluster] , color = 'blue')\nax.set_title('RMSE VS Number of clusters')\nax.legend(['base rmse','mean train rmse','mean val rmse'])\npprint([(k,np.mean(x)) for k,x in val_rmse.items()])","execution_count":null,"outputs":[]},{"metadata":{"id":"KfWqXwSChm1a","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"input_data, country_item_to_idx, country_emb_matrix, country_sc, country_kmeans = get_pmi_cluster(input_data, column = 'country_list', top_w = 1,\n                                                           nums_cluster = 4, \n                                                           item_to_idx = None,\n                                                           emb_matrix = None,\n                                                           sc = None,\n                                                           kmeans = None,\n                                                           flag_svd = False, \n                                                           flag_train = True)\n\ntest, _, _, _, _ = get_pmi_cluster(test, column = 'country_list', top_w = 1,\n                                                           nums_cluster = 4, \n                                                           item_to_idx = country_item_to_idx,\n                                                           emb_matrix = country_emb_matrix,\n                                                           sc = country_sc,\n                                                           kmeans = country_kmeans,\n                                                           flag_svd = False, \n                                                           flag_train = False)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ZRwU3amgTrNm","colab_type":"text"},"cell_type":"markdown","source":"### Genres  ( Best number for cluster = 8 )"},{"metadata":{"id":"NaqTWtovUuf8","colab_type":"code","outputId":"9b58d826-6d50-45e9-d486-16853e610631","colab":{"base_uri":"https://localhost:8080/","height":105},"trusted":true},"cell_type":"code","source":"base_rmse, train_rmse,val_rmse, sil_score = fit_pmi_cluster(inp = input_data[['id','log_revenue','genre_list']], column = 'genre_list', top_w = 1 , nums_cluster = [2,4,8,16,32] )","execution_count":null,"outputs":[]},{"metadata":{"id":"IUa5a7dUV0xp","colab_type":"code","outputId":"6415a7a1-21e6-48d6-cd3a-ee91a873135f","colab":{"base_uri":"https://localhost:8080/","height":407},"trusted":true},"cell_type":"code","source":"nums_cluster = [2,4,8,16,32]\nfig, ax = plt.subplots(1,2, figsize = (15,6))\nax[0].plot(nums_cluster,[np.mean(base_rmse[i]) for i in nums_cluster], ls = '--', color = 'black' )\nax[0].plot(nums_cluster,[np.mean(train_rmse[i]) for i in nums_cluster], color = 'red' )\nax[0].plot(nums_cluster,[np.mean(val_rmse[i]) for i in nums_cluster] , color = 'blue')\nax[0].set_title('RMSE VS Number of clusters')\nax[0].legend(['base rmse','mean train rmse','mean val rmse'])\nax[1].plot(nums_cluster,[np.mean(sil_score[i]) for i in nums_cluster] )\n\nax[1].set_title('Silhouette score VS Number of clusters')","execution_count":null,"outputs":[]},{"metadata":{"id":"gDdqSjJPWFEW","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"input_data, genre_item_to_idx, genre_emb_matrix, genre_sc, genre_kmeans = get_pmi_cluster(input_data, column = 'genre_list', top_w = 1,\n                                                           nums_cluster = 8, \n                                                           item_to_idx = None,\n                                                           emb_matrix = None,\n                                                           sc = None,\n                                                           kmeans = None,\n                                                           flag_svd = False, \n                                                           flag_train = True)\n\ntest, _, _, _, _ = get_pmi_cluster(test, column = 'genre_list', top_w = 1,\n                                                           nums_cluster = 8, \n                                                           item_to_idx = genre_item_to_idx,\n                                                           emb_matrix = genre_emb_matrix,\n                                                           sc = genre_sc,\n                                                           kmeans = genre_kmeans,\n                                                           flag_svd = False, \n                                                           flag_train = False)","execution_count":null,"outputs":[]},{"metadata":{"id":"7_XkRWs775Cr","colab_type":"text"},"cell_type":"markdown","source":"### Keywords ( Best number for cluster = 8 )"},{"metadata":{"id":"3KHxf8oD738t","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":405},"outputId":"42408196-724a-4a8f-bd60-fb0fa4e4ad8e","trusted":true},"cell_type":"code","source":"# df = pd.DataFrame(sorted(dict(Counter(keywords.sum())).items(), key = lambda x: x[1], reverse = True))\ndf = pd.DataFrame(sorted(dict(Counter(input_data['keywords_list'].sum())).items(), key = lambda x: x[1], reverse = True))\n\ndf['rate'] = df[1].cumsum()/df[1].sum()\nplt.plot(df['rate'])\nplt.title('Cumulative plot per keywords')\nprint(df.head(5))\nprint('Top base keywords & cummulative plot of all keywords we can get 80% of all by using ~ 3000 ')","execution_count":null,"outputs":[]},{"metadata":{"id":"-OpGNxcl8nkB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"dec0dd90-52e4-4908-c06c-401616baa54e","trusted":true},"cell_type":"code","source":"base_rmse, train_rmse,val_rmse, sil_score = fit_pmi_cluster(inp = input_data[['id','log_revenue','keywords_list']], column = 'keywords_list', top_w = 0.8 , \n                                                        nums_cluster = [2,4,8,16] )","execution_count":null,"outputs":[]},{"metadata":{"id":"bwnEydAl8s8O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":407},"outputId":"dae8b6c0-d0f4-47d8-cca7-25dc934e5b78","trusted":true},"cell_type":"code","source":"nums_cluster = [2,4,8,16]\nfig, ax = plt.subplots(1,2, figsize = (15,6))\nax[0].plot(nums_cluster,[np.mean(base_rmse[i]) for i in nums_cluster], ls = '--', color = 'black' )\nax[0].plot(nums_cluster,[np.mean(train_rmse[i]) for i in nums_cluster], color = 'red' )\nax[0].plot(nums_cluster,[np.mean(val_rmse[i]) for i in nums_cluster] , color = 'blue')\nax[0].set_title('RMSE VS Number of clusters')\nax[0].legend(['base rmse','mean train rmse','mean val rmse'])\nax[1].plot(nums_cluster,[np.mean(sil_score[i]) for i in nums_cluster] )\n\nax[1].set_title('Silhouette score VS Number of clusters')","execution_count":null,"outputs":[]},{"metadata":{"id":"UOXqp9YsBkyI","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"input_data, keyword_item_to_idx, keyword_emb_matrix, keyword_sc, keyword_kmeans = get_pmi_cluster(input_data, column = 'keywords_list', top_w = 1,\n                                                           nums_cluster = 8, \n                                                           item_to_idx = None,\n                                                           emb_matrix = None,\n                                                           sc = None,\n                                                           kmeans = None,\n                                                           flag_svd = False, \n                                                           flag_train = True)\n\ntest, _, _, _, _ = get_pmi_cluster(test, column = 'keywords_list', top_w = 1,\n                                                           nums_cluster = 8, \n                                                           item_to_idx = keyword_item_to_idx,\n                                                           emb_matrix = keyword_emb_matrix,\n                                                           sc = keyword_sc,\n                                                           kmeans = keyword_kmeans,\n                                                           flag_svd = False, \n                                                           flag_train = False)","execution_count":null,"outputs":[]},{"metadata":{"id":"FJnxrej19uDD","colab_type":"text"},"cell_type":"markdown","source":"### Crew ( Best number for cluster = 8 )"},{"metadata":{"id":"lBPUqr7VC72V","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":584},"outputId":"b52c802d-2910-4429-c3ce-ec743481fdc2","trusted":true},"cell_type":"code","source":"inp =  input_data[['id','crew']].dropna().set_index('id')\n###\n\ncrew_departments  = inp['crew'].apply(lambda x: [item['department'] for item in eval(x) ] )\ncrew_gender = pd.DataFrame( data = inp['crew'].apply(lambda x: [item['gender'] for item in eval(x) ] ).values,\n                           index = inp.index )\ncrew_dep_dict = dict(Counter(crew_departments.sum()))\n###\n\ncrew_df = pd.DataFrame( data = np.stack(crew_departments.apply(lambda x: [dict(Counter(x)).get(d,0) for d,_ in crew_dep_dict.items()] ).values),\n             columns = ['crew.' + w for w in list(crew_dep_dict.keys())], \n                      index = inp.index )\n\nsum_ = crew_df.sum(axis = 1)\nmax_ = crew_df.max(axis = 1)\ncrew_df['sum_crew_dep'] = sum_\ncrew_df['max_crew_dep'] = max_\n\ncrew_gender['male_crew'] = crew_gender[0].apply(lambda x: sum([w == 2 for w in x]))\ncrew_gender['female_crew'] = crew_gender[0].apply(lambda x: sum([w == 1 for w in x]))\ncrew_gender['def_gender_crew'] = crew_gender['male_crew'] + crew_gender['female_crew']\n\n###\n\ninp = pd.concat([inp,crew_df, crew_gender[['male_crew','female_crew','def_gender_crew']]], axis = 1)\ninp['argmax_crew_dep'] = inp[['crew.' + v for v in list(crew_dep_dict.keys())]].apply(lambda x: list(crew_dep_dict.keys())[np.argmax(np.array(x))], axis = 1)\ninp1 = pd.merge(input_data[['id','revenue','release_year']], inp.reset_index().rename(columns = {'index': 'id'}) , how = 'left', on = 'id')\n\ndf_part = (inp1['sum_crew_dep'].value_counts().sort_index().cumsum()/inp1['sum_crew_dep'].shape[0]).reset_index()\n\n########\nfig, ax = plt.subplots(2,3, figsize = (16,8))\naxi = ax.flatten()\naxi[0].plot(df_part['index'], df_part['sum_crew_dep'])\naxi[0].set_title('Percent of films VS amount of crew on film')\naxi[1].bar(inp1['sum_crew_dep'].value_counts().index, inp1['sum_crew_dep'].value_counts().values )\naxi[1].set_title('Histogram per amount of crew')\naxi[2].bar(list(inp1.groupby('release_year').aggregate({'sum_crew_dep':'mean'}).index), inp1.groupby('release_year').aggregate({'sum_crew_dep':'mean'}).values.reshape(-1) )\naxi[2].set_title('Mean amount of crew per release year')\n\n\naxi[3].bar(list(inp1.groupby('release_year').count().index), inp1.groupby('release_year').aggregate({'def_gender_crew':'sum'}).values.reshape(-1)/inp1.groupby('release_year').aggregate({'sum_crew_dep':'sum'}).values.reshape(-1) )\naxi[3].set_ylim([0,1])\naxi[3].set_title('Amount of crew with defined gender per release year')\n\naxi[4].bar(list(inp1.groupby('release_year').count().index), inp1.groupby('release_year').aggregate({'male_crew':'sum'}).values.reshape(-1)/inp1.groupby('release_year').aggregate({'sum_crew_dep':'sum'}).values.reshape(-1) )\naxi[4].set_ylim([0,1])\naxi[4].set_title('Amount of crew with male gender per release year')\n\naxi[5].bar(list(inp1.groupby('release_year').count().index), inp1.groupby('release_year').aggregate({'female_crew':'sum'}).values.reshape(-1)/inp1.groupby('release_year').aggregate({'sum_crew_dep':'sum'}).values.reshape(-1) )\naxi[5].set_ylim([0,1])\naxi[5].set_title('Amount of crew with female gender per release year')\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"OuAdkWJb92C3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":405},"outputId":"d144c57f-0f9e-42b9-c5db-9704c08c59ab","trusted":true},"cell_type":"code","source":"df = pd.DataFrame(sorted(dict(Counter(input_data['crew_list'].sum())).items(), key = lambda x: x[1], reverse = True))\ndf['rate'] = df[1].cumsum()/df[1].sum()\nplt.plot(df['rate'])\nplt.title('Cumulative plot per crew')\nprint(df.head(5))\nprint('25% of crew cover 50-60%  of all crew')","execution_count":null,"outputs":[]},{"metadata":{"id":"9pxke4ZF-QLO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"c1114d6a-0e68-4ef1-b22f-57ae107dab6d","trusted":true},"cell_type":"code","source":"base_rmse, train_rmse,val_rmse, sil_score = fit_pmi_cluster(inp = input_data[['id','log_revenue','crew_list']], column = 'crew_list', top_w = 0.3, \n                                                        nums_cluster = [2,4,8,16], flag_svd = False )","execution_count":null,"outputs":[]},{"metadata":{"id":"WPl0N0zzxY8j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":407},"outputId":"37bf3d6b-b955-476a-a61a-a5f67c90a271","trusted":true},"cell_type":"code","source":"nums_cluster = [2,4,8,16]\nfig, ax = plt.subplots(1,2, figsize = (15,6))\nax[0].plot(nums_cluster,[np.mean(base_rmse[i]) for i in nums_cluster], ls = '--', color = 'black' )\nax[0].plot(nums_cluster,[np.mean(train_rmse[i]) for i in nums_cluster], color = 'red' )\nax[0].plot(nums_cluster,[np.mean(val_rmse[i]) for i in nums_cluster] , color = 'blue')\nax[0].set_title('RMSE VS Number of clusters')\nax[0].legend(['base rmse','mean train rmse','mean val rmse'])\nax[1].plot(nums_cluster,[np.mean(sil_score[i]) for i in nums_cluster] )\n\nax[1].set_title('Silhouette score VS Number of clusters')","execution_count":null,"outputs":[]},{"metadata":{"id":"qnjLM9crG1_k","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"input_data, crew_item_to_idx, crew_emb_matrix, crew_sc, crew_kmeans = get_pmi_cluster(input_data, column = 'crew_list', top_w = 0.3,\n                                                           nums_cluster = 8, \n                                                           item_to_idx = None,\n                                                           emb_matrix = None,\n                                                           sc = None,\n                                                           kmeans = None,\n                                                           flag_svd = False, \n                                                           flag_train = True)\n\n\ntest, _, _, _, _ = get_pmi_cluster(test, column = 'crew_list', top_w = 0.3,\n                                                           nums_cluster = 8, \n                                                           item_to_idx = crew_item_to_idx,\n                                                           emb_matrix = crew_emb_matrix,\n                                                           sc = crew_sc,\n                                                           kmeans = crew_kmeans,\n                                                           flag_svd = False, \n                                                           flag_train = False)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"-fjv9veRx-qm","colab_type":"text"},"cell_type":"markdown","source":"### Cast ( Best number of cluster = 8)"},{"metadata":{"id":"4cWpGznLyDKK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":584},"outputId":"d8130d03-695d-4799-b3a1-25c2364289ea","trusted":true},"cell_type":"code","source":"inp =  input_data[['id','cast']].dropna().set_index('id')\n###\n\ncast_gender = pd.DataFrame( data = inp['cast'].apply(lambda x: [item['gender'] for item in eval(x) ] ).values,\n                           index = inp.index )\n\n\ncast_gender['male_cast'] = cast_gender[0].apply(lambda x: sum([w == 2 for w in x]))\ncast_gender['female_cast'] = cast_gender[0].apply(lambda x: sum([w == 1 for w in x]))\ncast_gender['sum_cast'] = cast_gender[0].apply(lambda x: len(x))\ncast_gender['def_gender_cast'] = cast_gender['male_cast'] + cast_gender['female_cast']\n\n###\n\ninp = pd.concat([inp, cast_gender[['male_cast','female_cast','def_gender_cast', 'sum_cast']]], axis = 1)\ninp1 = pd.merge(input_data[['id','revenue','release_year']], inp.reset_index().rename(columns = {'index': 'id'}) , how = 'left', on = 'id')\n\ndf_part = (inp1['sum_cast'].value_counts().sort_index().cumsum()/inp1['sum_cast'].shape[0]).reset_index()\n\n########\nfig, ax = plt.subplots(2,3, figsize = (16,8))\naxi = ax.flatten()\naxi[0].plot(df_part['index'], df_part['sum_cast'])\naxi[0].set_title('Percent of films VS amount of cast on film')\naxi[1].bar(inp1['sum_cast'].value_counts().index, inp1['sum_cast'].value_counts().values )\naxi[1].set_title('Histogram per amount of cast')\naxi[2].bar(list(inp1.groupby('release_year').aggregate({'sum_cast':'mean'}).index), inp1.groupby('release_year').aggregate({'sum_cast':'mean'}).values.reshape(-1) )\naxi[2].set_title('Mean amount of cast per release year')\n\n\naxi[3].bar(list(inp1.groupby('release_year').count().index), inp1.groupby('release_year').aggregate({'def_gender_cast':'sum'}).values.reshape(-1)/inp1.groupby('release_year').aggregate({'sum_cast':'sum'}).values.reshape(-1) )\naxi[3].set_ylim([0,1])\naxi[3].set_title('Amount of cast with defined gender per release year')\n\naxi[4].bar(list(inp1.groupby('release_year').count().index), inp1.groupby('release_year').aggregate({'male_cast':'sum'}).values.reshape(-1)/inp1.groupby('release_year').aggregate({'sum_cast':'sum'}).values.reshape(-1) )\naxi[4].set_ylim([0,1])\naxi[4].set_title('Amount of cast with male gender per release year')\n\naxi[5].bar(list(inp1.groupby('release_year').count().index), inp1.groupby('release_year').aggregate({'female_cast':'sum'}).values.reshape(-1)/inp1.groupby('release_year').aggregate({'sum_cast':'sum'}).values.reshape(-1) )\naxi[5].set_ylim([0,1])\naxi[5].set_title('Amount of cast with female gender per release year')\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"UFEfjcIFPGzh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":405},"outputId":"02347ff6-01c9-4c4c-e060-7f6487021b6b","trusted":true},"cell_type":"code","source":"df = pd.DataFrame(sorted(dict(Counter(input_data['cast_list'].sum())).items(), key = lambda x: x[1], reverse = True))\ndf['rate'] = df[1].cumsum()/df[1].sum()\nplt.plot(df['rate'])\nplt.title('Cumulative plot per cast')\nprint(df.head(5))\nprint('25% of cast cover 50-60%  of all cast')","execution_count":null,"outputs":[]},{"metadata":{"id":"tQkWqvCQPQE-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"26b8d193-04d6-4c50-b265-ac3407e58c39","trusted":true},"cell_type":"code","source":"base_rmse, train_rmse,val_rmse, sil_score = fit_pmi_cluster(inp = input_data[['id','log_revenue','cast_list']], column = 'cast_list', top_w = 0.3, \n                                                        nums_cluster = [2,4,8,16], flag_svd = False )","execution_count":null,"outputs":[]},{"metadata":{"id":"1yqkLqRAPse-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":407},"outputId":"71638d31-f4de-4c4c-b3c8-ff998242da76","trusted":true},"cell_type":"code","source":"nums_cluster = [2,4,8,16]\nfig, ax = plt.subplots(1,2, figsize = (15,6))\nax[0].plot(nums_cluster,[np.mean(base_rmse[i]) for i in nums_cluster], ls = '--', color = 'black' )\nax[0].plot(nums_cluster,[np.mean(train_rmse[i]) for i in nums_cluster], color = 'red' )\nax[0].plot(nums_cluster,[np.mean(val_rmse[i]) for i in nums_cluster] , color = 'blue')\nax[0].set_title('RMSE VS Number of clusters')\nax[0].legend(['base rmse','mean train rmse','mean val rmse'])\nax[1].plot(nums_cluster,[np.mean(sil_score[i]) for i in nums_cluster] )\n\nax[1].set_title('Silhouette score VS Number of clusters')","execution_count":null,"outputs":[]},{"metadata":{"id":"9N_hdPUMPxsk","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"input_data, cast_item_to_idx, cast_emb_matrix, cast_sc, cast_kmeans = get_pmi_cluster(input_data, column = 'cast_list', top_w = 0.3,\n                                                           nums_cluster = 8, \n                                                           item_to_idx = None,\n                                                           emb_matrix = None,\n                                                           sc = None,\n                                                           kmeans = None,\n                                                           flag_svd = False, \n                                                           flag_train = True)\n\n\ntest, _, _, _, _ = get_pmi_cluster(test, column = 'cast_list', top_w = 0.3,\n                                                           nums_cluster = 8, \n                                                           item_to_idx = cast_item_to_idx,\n                                                           emb_matrix = cast_emb_matrix,\n                                                           sc = cast_sc,\n                                                           kmeans = cast_kmeans,\n                                                           flag_svd = False, \n                                                           flag_train = False)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"aFwh3itVUaUQ","colab_type":"text"},"cell_type":"markdown","source":"### Company production ( Best number of cluster = 16)"},{"metadata":{"id":"HuJeRbS-UkLe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":423},"outputId":"417e4cd3-a7af-4457-caab-046e5fce4646","trusted":true},"cell_type":"code","source":"df = pd.DataFrame(sorted(dict(Counter(input_data['prod_list'].sum())).items(), key = lambda x: x[1], reverse = True))\ndf['rate'] = df[1].cumsum()/df[1].sum()\nplt.plot(df['rate'])\nplt.title('Cumulative plot per prod companies')\nprint(df.head(5))\nprint('Top prod companies & cummulative plot of all prod companies we can get 80% of all by using ~ 3000 ')\nprint(df[0].nunique())","execution_count":null,"outputs":[]},{"metadata":{"id":"AisRTgefUo2Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":123},"outputId":"9cabe87f-597b-426b-92dd-0d8139915551","trusted":true},"cell_type":"code","source":"base_rmse, train_rmse,val_rmse, sil_score = fit_pmi_cluster(inp = input_data[['id','log_revenue','prod_list']], column = 'prod_list', top_w = 0.4, \n                                                        nums_cluster = [2,4,8,16,32,64] )","execution_count":null,"outputs":[]},{"metadata":{"id":"7ISyPS94U3RA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":407},"outputId":"caec35e5-04e9-42be-961e-5d26628e9244","trusted":true},"cell_type":"code","source":"nums_cluster = [2,4,8,16,32,64]\nfig, ax = plt.subplots(1,2, figsize = (15,6))\nax[0].plot(nums_cluster,[np.mean(base_rmse[i]) for i in nums_cluster], ls = '--', color = 'black' )\nax[0].plot(nums_cluster,[np.mean(train_rmse[i]) for i in nums_cluster], color = 'red' )\nax[0].plot(nums_cluster,[np.mean(val_rmse[i]) for i in nums_cluster] , color = 'blue')\nax[0].set_title('RMSE VS Number of clusters')\nax[0].legend(['base rmse','mean train rmse','mean val rmse'])\nax[1].plot(nums_cluster,[np.mean(sil_score[i]) for i in nums_cluster] )\n\nax[1].set_title('Silhouette score VS Number of clusters')","execution_count":null,"outputs":[]},{"metadata":{"id":"2an2aqVhU97P","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"input_data, prod_item_to_idx, prod_emb_matrix, prod_sc, prod_kmeans = get_pmi_cluster(input_data, column = 'prod_list', top_w = 0.4,\n                                                           nums_cluster = 16, \n                                                           item_to_idx = None,\n                                                           emb_matrix = None,\n                                                           sc = None,\n                                                           kmeans = None,\n                                                           flag_svd = False, \n                                                           flag_train = True)\n\ntest, _, _, _, _ = get_pmi_cluster(test, column = 'prod_list', top_w = 0.4,\n                                                           nums_cluster = 16, \n                                                           item_to_idx = prod_item_to_idx,\n                                                           emb_matrix = prod_emb_matrix,\n                                                           sc = prod_sc,\n                                                           kmeans = prod_kmeans,\n                                                           flag_svd = False, \n                                                           flag_train = False)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"b_SwHjvfW2s9","colab_type":"text"},"cell_type":"markdown","source":"## Wordvec average encoding"},{"metadata":{"id":"W1-JsJpRY7AX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"24b263f1-1b6c-4268-9538-e20db81d9d0a","trusted":true},"cell_type":"code","source":"import gensim\n# Load Google's pre-trained Word2Vec model.\nwordvec = gensim.models.KeyedVectors.load_word2vec_format('../input/wordvec/googlenews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin', binary=True) \n\n\nimport nltk\nnltk.download('stopwords')\n# NLTK Stop words\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\n\ndef preprocessing(inp):\n  data = inp.values.tolist()\n\n  # Remove single quotes\n  data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n\n#   pprint(data[:1]))\n  \n  return data\n\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\n  \n# !python3 -m spacy download en  # run in terminal once\ndef process_words(inp_words, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n    \n    bigram = gensim.models.Phrases(inp_words, min_count=5, threshold=10) # higher threshold fewer phrases.\n    bigram_mod = gensim.models.phrases.Phraser(bigram)\n\n    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in inp_words]\n    texts = [bigram_mod[doc] for doc in texts]\n#     texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n    texts_out = []\n    nlp = spacy.load('en', disable=['parser', 'ner'])\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    # remove stopwords once more after lemmatization\n    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n    return texts_out\n\n  \n  \ndef format_topics_sentences(ldamodel, corpus, texts):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list            \n        # print(row)\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)","execution_count":null,"outputs":[]},{"metadata":{"id":"BGKqQgyRXBOf","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def average_vec_sentences(df, w2id, emb_matrix, column, wv_model, flag_train = True):\n  \n  data = preprocessing(df[column].dropna())\n  data_words = list(sent_to_words(data))\n  data_ready = process_words(inp_words = data_words)\n  \n  if flag_train:\n    id2word = corpora.Dictionary(data_ready)\n    w2id = dict([(w,i+1) for i,w in enumerate([w for w in list(id2word.token2id.keys()) if wv_model.vocab.get(w) is not None ] ) ])\n    emb_matrix = np.vstack( [ np.mean(np.stack([wv_model.wv[w] for w,_ in w2id.items()]), axis = 0, keepdims = True),\n    np.stack([wv_model.wv[w] for w,_ in w2id.items()])\n       ]\n                          )\n    \n  data_encode = pd.Series(data_ready).apply(lambda x: [w2id.get(w) for w in x if w2id.get(w) is not None] if len([w for w in x if w2id.get(w) is not None]) > 0 else [0])\n  X = np.stack(data_encode.apply(lambda x: np.mean([emb_matrix[i,:] for i in x], axis = 0)).values)\n  return X, w2id, emb_matrix\n\n\ndef fit_av_cluster(inp, column, wv_model, nums_cluster = [2,4,8,16,32,64]):\n  \n  base_rmse = {}\n  train_rmse = {}\n  val_rmse = {}\n  sil_score = {}\n    \n  for k in nums_cluster :\n    \n    base_rmse[k] = []\n    train_rmse[k] = []\n    val_rmse[k] = []\n    sil_score[k] = []\n      \n    st = time.time()\n    \n    for i in range(5):\n\n      train = pd.merge( pd.DataFrame( data = train_id[i].reshape(-1), columns = ['id']), inp )\n\n      x_train, w2id, emb_matrix = average_vec_sentences(train[['id',column]].dropna(), w2id = None, emb_matrix=None,column = column, wv_model = wv_model, flag_train = True)\n      kmeans = KMeans(n_clusters = k, random_state = 42)\n      kmeans.fit(x_train)\n\n      train = pd.merge( train[['id',column,'log_revenue']], \n      pd.DataFrame( {'id' : train[~train[column].isna()]['id'], 'label' : kmeans.predict(x_train)} ).reset_index().rename(columns= {'index': 'Document_No'} ) , how = 'left')\n      \n      \n      train['label'].fillna(-1, inplace = True)\n      \n      train_group = train.groupby('label').aggregate({'log_revenue': ['mean','count']}).reset_index()\n      train_group.columns = train_group.columns.map('_'.join).str.strip('_')\n\n      train = pd.merge(train , train_group , on = 'label', how ='left')\n      \n      \n      \n      val = pd.merge( pd.DataFrame( data = val_id[i].reshape(-1), columns = ['id']), inp )\n\n      x_val, _, _ = average_vec_sentences(val[['id',column]].dropna(), w2id = w2id, emb_matrix=emb_matrix,column = column, wv_model=wv_model, flag_train = False)\n\n      val = pd.merge( val[['id',column,'log_revenue']], \n      pd.DataFrame( {'id' : val[~val[column].isna()]['id'], 'label' : kmeans.predict(x_val)} ).reset_index().rename(columns= {'index': 'Document_No'} ) , how = 'left')\n\n      \n      val['label'].fillna(-1, inplace = True)\n      val = pd.merge(val , train_group , on = 'label', how = 'left')\n      \n      \n      val['log_revenue_mean'] = val['log_revenue_mean'].fillna(train['log_revenue'].mean())   \n      train_rmse_base = np.sqrt(mean_squared_error(train['log_revenue'], np.repeat(train['log_revenue'].mean() , train.shape[0]))) \n      \n      base_rmse[k].append(train_rmse_base) \n      sil_score[k].append(silhouette_score(x_train, kmeans.labels_))\n\n      train_rmse[k].append(np.sqrt(mean_squared_error(train['log_revenue'], train['log_revenue_mean']) ))\n      val_rmse[k].append(np.sqrt(mean_squared_error(val['log_revenue'], val['log_revenue_mean']) ))\n    print(k, 'clusters Done', (time.time() - st)/60, 'min' )\n    \n  return  base_rmse, train_rmse,val_rmse, sil_score\n\n\n\ndef get_av_cluster(inp, column, wv_model,  nums_cluster, w2id, emb_matrix, kmeans, flag_train = True):\n  \n\n  if flag_train:\n\n    x, w2id, emb_matrix = average_vec_sentences(inp[['id',column]].dropna(), w2id = None, emb_matrix=None,column = column, wv_model = wv_model, flag_train = True)\n    kmeans = KMeans(n_clusters = nums_cluster, random_state = 42)\n    kmeans.fit(x)\n      \n\n  else:\n    x, _, _ = average_vec_sentences(inp[['id',column]].dropna(), w2id = w2id, emb_matrix=emb_matrix,column = column, wv_model = wv_model, flag_train = False)\n\n\n  inp = pd.merge( inp, \n  pd.DataFrame( {'id' : inp[~inp[column].isna()]['id'], 'label_' + column : kmeans.predict(x)} ).reset_index().rename(columns= {'index': 'Document_No'} ) , how = 'left')\n\n\n  inp['label_' + column].fillna(-1, inplace = True)\n\n  return  inp, w2id, emb_matrix, kmeans\n","execution_count":null,"outputs":[]},{"metadata":{"id":"I42feKS4Ynaa","colab_type":"text"},"cell_type":"markdown","source":"### Tagline( best number of clusters = 2)"},{"metadata":{"id":"t4PKuuobYqxU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"eb0e7be6-1de8-4331-ee5a-f7d0fd191e94","trusted":true},"cell_type":"code","source":"base_rmse, train_rmse,val_rmse, sil_score = fit_av_cluster(input_data[['id','log_revenue','tagline']], column = 'tagline', wv_model = wordvec, nums_cluster = [2,4,8])","execution_count":null,"outputs":[]},{"metadata":{"id":"Md2gGJGocqKE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":407},"outputId":"e7e1e783-e9b9-41cb-e810-1ffaa537a8d7","trusted":true},"cell_type":"code","source":"nums_cluster = [2,4,8]\nfig, ax = plt.subplots(1,1, figsize = (8,6))\nax.plot(nums_cluster,[np.mean(base_rmse[i]) for i in nums_cluster], ls = '--', color = 'black' )\nax.plot(nums_cluster,[np.mean(train_rmse[i]) for i in nums_cluster], color = 'red' )\nax.plot(nums_cluster,[np.mean(val_rmse[i]) for i in nums_cluster] , color = 'blue')\nax.set_title('RMSE VS Number of clusters')\nax.legend(['base rmse','mean train rmse','mean val rmse'])\npprint([(k,np.mean(x)) for k,x in val_rmse.items()])","execution_count":null,"outputs":[]},{"metadata":{"id":"Ys01tAGidtu-","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"input_data, tag_w2id, tag_emb_matrix, tag_kmeans = get_av_cluster(input_data, \n                                                                           column = 'tagline',\n                                                                                  wv_model = wordvec, \n                                                           nums_cluster = 2, \n                                                           w2id = None,\n                                                           emb_matrix = None,\n                                                          \n                                                           kmeans = None,\n                                                     \n                                                           flag_train = True)\n\n\n\ntest, _, _, _  = get_av_cluster(test, column = 'tagline',wv_model = wordvec,\n                                                           nums_cluster = 2, \n                                                           w2id = tag_w2id,\n                                                           emb_matrix = tag_emb_matrix,\n                                                         \n                                                           kmeans = tag_kmeans,\n                                                        \n                                                           flag_train = False)","execution_count":null,"outputs":[]},{"metadata":{"id":"Zh98d477mTXi","colab_type":"text"},"cell_type":"markdown","source":"### Overview_fst ( best number of clusters = 4 )"},{"metadata":{"id":"RIMvYKYImWH7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":281},"outputId":"a41f1cca-2f34-4ab4-a0c8-b0d2738721af","trusted":true},"cell_type":"code","source":"plt.hist(input_data['overview'].fillna('').apply(lambda x: len(x.split(' '))))\nplt.title('word length of overview')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"EtouhR1zmhn_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"396e53be-aec0-4e3a-821d-c0e78274452f","trusted":true},"cell_type":"code","source":"base_rmse, train_rmse,val_rmse, sil_score = fit_av_cluster(input_data[['id','log_revenue','overview_fst']], column = 'overview_fst', wv_model = wordvec, nums_cluster = [2,4,8,16])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"RP1xlSP1nBbz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":461},"outputId":"a333085c-b5ab-4bb8-f88a-beadfa6211fa","trusted":true},"cell_type":"code","source":"nums_cluster = [2,4,8,16]\nfig, ax = plt.subplots(1,1, figsize = (8,6))\nax.plot(nums_cluster,[np.mean(base_rmse[i]) for i in nums_cluster], ls = '--', color = 'black' )\nax.plot(nums_cluster,[np.mean(train_rmse[i]) for i in nums_cluster], color = 'red' )\nax.plot(nums_cluster,[np.mean(val_rmse[i]) for i in nums_cluster] , color = 'blue')\nax.set_title('RMSE VS Number of clusters')\nax.legend(['base rmse','mean train rmse','mean val rmse'])\npprint([(k,np.mean(x)) for k,x in val_rmse.items()])","execution_count":null,"outputs":[]},{"metadata":{"id":"17BLJJrNpsEo","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"input_data, overview_w2id, overview_emb_matrix, overview_kmeans = get_av_cluster(input_data, \n                                                                           column = 'overview_fst',\n                                                                                  wv_model = wordvec, \n                                                           nums_cluster = 4, \n                                                           w2id = None,\n                                                           emb_matrix = None,\n                                                          \n                                                           kmeans = None,\n                                                     \n                                                           flag_train = True)\n\ntest, _, _, _  = get_av_cluster(test, column = 'overview_fst',wv_model = wordvec,\n                                                           nums_cluster = 4, \n                                                           w2id = tag_w2id,\n                                                           emb_matrix = tag_emb_matrix,\n                                                         \n                                                           kmeans = tag_kmeans,\n                                                        \n                                                           flag_train = False)","execution_count":null,"outputs":[]},{"metadata":{"id":"R0qsIn64xa5K","colab_type":"text"},"cell_type":"markdown","source":"## Posters( best number cluster = 4 )"},{"metadata":{"id":"ewpV7wft0jif","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def fit_img_cluster(inp, input_data_img, nums_cluster = [2,4,8,16,32,64,128]):\n  \n  base_rmse = {}\n  train_rmse = {}\n  val_rmse = {}\n  sil_score = {}\n    \n  for k in nums_cluster :\n    \n    base_rmse[k] = []\n    train_rmse[k] = []\n    val_rmse[k] = []\n    sil_score[k] = []\n      \n    st = time.time()\n    for i in range(5):\n\n      sc = StandardScaler()\n      \n      \n      train = pd.merge( pd.DataFrame( data = train_id[i].reshape(-1), columns = ['id']), input_data_img )\n      x_train = train.drop(columns = ['id']).values\n      x_train_sc = sc.fit_transform(x_train)\n\n      val = pd.merge( pd.DataFrame( data = val_id[i].reshape(-1), columns = ['id']), input_data_img )\n      x_val = val.drop(columns = ['id']).values\n      x_val_sc = sc.transform(x_val)\n\n      kmeans = KMeans(n_clusters = k, random_state = 42)\n      kmeans.fit(x_train_sc)\n\n      train['label'] = kmeans.predict(x_train_sc)\n      train = pd.merge(  pd.DataFrame( data = train_id[i].reshape(-1), columns = ['id']) , train, how = 'left' )\n      train = pd.merge( train, input_data[['id','revenue']] )\n      train['label'].fillna(0, inplace = True)\n      \n      \n      train_group = train.groupby('label').aggregate({'revenue': ['mean','count']}).reset_index()\n      train_group.columns = train_group.columns.map('_'.join).str.strip('_')\n     \n      train = pd.merge(train , train_group , on = 'label', how ='left')\n      \n      val['label'] = kmeans.predict(x_val_sc)\n      val = pd.merge(  pd.DataFrame( data = val_id[i].reshape(-1), columns = ['id']) , val, how = 'left' )\n      val = pd.merge( val, input_data[['id','revenue']])\n      \n      val['label'].fillna(0, inplace = True)\n      \n      val = pd.merge(val , train_group , on = 'label', how ='left')\n      val['revenue_mean'] = val['revenue_mean'].fillna(train['revenue'].mean())   \n      \n      train_rmse_base = np.sqrt(mean_squared_error(train['revenue'], np.repeat(train['revenue'].mean() , train.shape[0]))) \n\n      base_rmse[k].append(train_rmse_base)\n      sil_score[k].append(silhouette_score(x_train_sc, kmeans.labels_))\n      train_rmse[k].append(np.sqrt(mean_squared_error(train['revenue'], train['revenue_mean']) ))\n      val_rmse[k].append(np.sqrt(mean_squared_error(val['revenue'], val['revenue_mean']) ))\n    print(k, 'clusters Done', (time.time() - st)/60, 'min' )\n    \n  return  base_rmse, train_rmse,val_rmse, sil_score\n\n\ndef get_img_cluster(inp, input_data_img,  nums_cluster, sc, kmeans, flag_train = True):\n  \n\n  if flag_train:\n      \n      sc = StandardScaler()\n        \n      data = pd.merge( inp[['id']], input_data_img )\n      x = data.drop(columns = ['id']).values\n      x_sc = sc.fit_transform(x)\n\n      kmeans = KMeans(n_clusters = nums_cluster, random_state = 42)\n      kmeans.fit(x_sc)\n\n      data['img_label'] = kmeans.predict(x_sc)\n      inp = pd.merge( inp , data[['id','img_label']], how = 'left' )\n      inp['img_label'].fillna(0, inplace = True)\n      \n\n  else:\n    data = pd.merge( inp[['id']], input_data_img )\n    x = data.drop(columns = ['id']).values\n    x_sc = sc.transform(x)\n\n    data['img_label' ] = kmeans.predict(x_sc)\n    inp = pd.merge( inp , data[['id','img_label']], how = 'left' )\n    inp['img_label'].fillna(0, inplace = True)\n    \n\n  return  inp, sc, kmeans","execution_count":null,"outputs":[]},{"metadata":{"id":"5YEK6-8nSVug","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"67a94943-8101-4d85-99fc-e3379c089829","trusted":true},"cell_type":"code","source":"model_resnet50 = ResNet50(weights='imagenet', include_top=False, pooling = 'avg')\n# model_resnet50.summary()\n\nimg_path = '../input/film_posters/posters-20190619t213659z-001/posters/train/1.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nimg_data = image.img_to_array(img)\nimg_data = np.expand_dims(img_data, axis=0)\nimg_data = resnet50_preprocess(img_data)\n\nresnet50_feature = model_resnet50.predict(img_data)\n\nprint (resnet50_feature.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_path = '../input/film_posters/posters-20190619t213659z-001/posters/train/1.jpg'\nimage.load_img(img_path, target_size=(224, 224))","execution_count":null,"outputs":[]},{"metadata":{"id":"dnJ-xra8Si77","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"465ba4ed-b584-49be-817a-d52176cf41a1","trusted":true},"cell_type":"code","source":"resnet50_feature_list = []\nimg_id = []\n\nfor idx, f in tqdm(enumerate(os.listdir('../input/film_posters/posters-20190619t213659z-001/posters/train/'))):\n#     print(f)\n    f = f.split('.')[0]\n    try:\n      img_path = '../input/film_posters/posters-20190619t213659z-001/posters/train/' + f + '.jpg'\n      img = image.load_img(img_path, target_size=(224, 224))\n      img_data = image.img_to_array(img)\n      img_data = np.expand_dims(img_data, axis=0)\n      img_data = resnet50_preprocess(img_data)\n      \n      resnet50_feature = model_resnet50.predict(img_data)\n      resnet50_feature_np = np.array(resnet50_feature)\n      resnet50_feature_list.append(resnet50_feature_np.flatten())\n      img_id.append(f)\n    except:\n      continue \n        \nresnet50_feature_list_np = np.array(resnet50_feature_list)\n\nimg_df = pd.concat( [ pd.DataFrame({'id': img_id}), pd.DataFrame(resnet50_feature_list_np, columns = ['f_' + str(i) for i in range(resnet50_feature_list_np.shape[1])]) ] , axis = 1 )\nimg_df['id'] = img_df['id'].astype('int32')\ninput_data_img = pd.merge(input_data[['id']], img_df , how = 'inner')","execution_count":null,"outputs":[]},{"metadata":{"id":"6vy20fNqSspl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"53e8cf6a-3165-42d7-f587-c282ff75c982","trusted":true},"cell_type":"code","source":"nums_cluster = [2,4,8]\nbase_rmse, train_rmse,val_rmse, sil_score = fit_img_cluster(input_data, input_data_img, nums_cluster = [2,4,8])","execution_count":null,"outputs":[]},{"metadata":{"id":"Pjq86TlnSw4a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":407},"outputId":"5fa3f9cd-891e-423a-a0ff-add37ef6bbc9","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize = (15,6))\nax[0].plot(nums_cluster,[np.mean(base_rmse[i]) for i in nums_cluster], ls = '--', color = 'black' )\nax[0].plot(nums_cluster,[np.mean(train_rmse[i]) for i in nums_cluster], color = 'red' )\nax[0].plot(nums_cluster,[np.mean(val_rmse[i]) for i in nums_cluster] , color = 'blue')\nax[0].set_title('RMSE VS Number of clusters')\nax[0].legend(['base rmse','mean train rmse','mean val rmse'])\nax[1].plot(nums_cluster,[np.mean(sil_score[i]) for i in nums_cluster] )\n\nax[1].set_title('Silhouette score VS Number of clusters')","execution_count":null,"outputs":[]},{"metadata":{"id":"PCoZu59zXVrt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":642},"outputId":"1195b284-d26b-45e7-cf99-aa474ea031f5","trusted":true},"cell_type":"code","source":"sc = StandardScaler()\ntrain = pd.merge( pd.DataFrame( data = train_id[0].reshape(-1), columns = ['id']), input_data_img )\nx_train = train.drop(columns = ['id']).values\nx_train_sc = sc.fit_transform(x_train)\n\nkmeans = KMeans(n_clusters =4, random_state = 42)\nkmeans.fit(x_train_sc)\ntrain['label'] = kmeans.predict(x_train_sc)\nprint(train.groupby('label').aggregate({'id':'count'}).rename(columns = {'id':'count'}).T)\nfig, ax = plt.subplots(3,4, figsize = (15,10))\n\nfor i in range(3):\n  for j in range(4):\n    img_path = '../input/film_posters/posters-20190619t213659z-001/posters/train/' + str(train[train['label'] == j]['id'].sample(1).iloc[0]) + '.jpg'       \n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    ax[i][j].imshow(img)\n    ax[i][j].set_title('label ' + str(j))\n    ax[i][j].axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"NtcVqwlxcbJ2","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"input_data, img_sc, img_kmeans = get_img_cluster(input_data, \n                                                 input_data_img,  \n                                                 nums_cluster = 4 , \n                                                 sc = None, \n                                                 kmeans = None,\n                                                 flag_train = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"jUxI49A9N4Yg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a91fb3f7-5f7a-4276-a874-fb5c4b1b2e0a","trusted":true},"cell_type":"code","source":"resnet50_feature_list = []\nimg_id = []\n\nfor idx, f in tqdm(enumerate(os.listdir('../input/film_posters/posters-20190619t213659z-001/posters/test/'))):\n#     print(f)\n    f = f.split('.')[0]\n    try:\n      img_path = '../input/film_posters/posters-20190619t213659z-001/posters/test/' + f + '.jpg'\n      img = image.load_img(img_path, target_size=(224, 224))\n      img_data = image.img_to_array(img)\n      img_data = np.expand_dims(img_data, axis=0)\n      img_data = resnet50_preprocess(img_data)\n      \n      resnet50_feature = model_resnet50.predict(img_data)\n      resnet50_feature_np = np.array(resnet50_feature)\n      resnet50_feature_list.append(resnet50_feature_np.flatten())\n      img_id.append(f)\n    except:\n      continue \n        \nresnet50_feature_list_np = np.array(resnet50_feature_list)\n\nimg_df = pd.concat( [ pd.DataFrame({'id': img_id}), pd.DataFrame(resnet50_feature_list_np, columns = ['f_' + str(i) for i in range(resnet50_feature_list_np.shape[1])]) ] , axis = 1 )\nimg_df['id'] = img_df['id'].astype('int32')\ntest_img = pd.merge(test[['id']], img_df , how = 'inner')","execution_count":null,"outputs":[]},{"metadata":{"id":"lmTVclT_OHvy","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"test,_, _ = get_img_cluster(test, \n                                                 test_img,  \n                                                 nums_cluster = 4 , \n                                                 sc = img_sc, \n                                                 kmeans = img_kmeans,\n                                                 flag_train = False)","execution_count":null,"outputs":[]},{"metadata":{"id":"S_TKEoynim49","colab_type":"text"},"cell_type":"markdown","source":"## Previous data on film( director, collection)"},{"metadata":{"id":"6Z1YVbFJir2q","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"input_data['crew_director'] = input_data['crew'].fillna('').apply(lambda x : [item['name'] for item in eval(x) if  item['job'] == 'Director'  ][0] if  x != '' and len([item['name'] for item in eval(x) if  item['job'] == 'Director'  ]) > 0 else '' )\ntest['crew_director'] = test['crew'].fillna('').apply(lambda x : [item['name'] for item in eval(x) if  item['job'] == 'Director'  ][0] if  x != '' and len([item['name'] for item in eval(x) if  item['job'] == 'Director'  ]) > 0  else '' )\n","execution_count":null,"outputs":[]},{"metadata":{"id":"tUyhZ1UcjYPW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":230},"outputId":"9fd689d0-d03b-40ca-aed9-f2f26c5c11a1","trusted":true},"cell_type":"code","source":"print('Top crew directors by amount of films')\nfull_data['crew_director'].value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"92dyhvOPjzTk","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"prev_input = pd.merge(input_data[['id','crew_director','release_dt']], full_data[['crew_director','release_dt','log_revenue','id']].rename(columns = {'log_revenue': 'log_revenue_dir',\n                                                                                          'release_dt': 'prev_release_dt',\n                                                                                                            'id': 'prev_id'}) , on = 'crew_director' )\n\nprev_input = prev_input[(prev_input['prev_release_dt'] < prev_input['release_dt'])& (prev_input['crew_director'] != '')]\n\nprev_input['year_to_lst_film_dir'] = prev_input.apply(lambda x: (x['release_dt']- x['prev_release_dt']).days/365, axis = 1)\nprev_group = prev_input.groupby('id').aggregate({'prev_id':'count','year_to_lst_film_dir': 'min', 'log_revenue_dir' : 'mean'}).reset_index().rename(columns = {'prev_id': 'film_count_dir'})\n\nprev_test = pd.merge(test[['id','crew_director','release_dt']] , full_data[['crew_director','release_dt','id', 'log_revenue']].rename(columns = {  'log_revenue': 'log_revenue_dir',\n                                                                                          'release_dt': 'prev_release_dt',\n                                                                                                            'id': 'prev_id'}) , on = 'crew_director' )\n\nprev_test = prev_test[(prev_test['prev_release_dt'] < prev_test['release_dt'])& (prev_test['crew_director'] != '')]\n\nprev_test['year_to_lst_film_dir'] = prev_test.apply(lambda x: (x['release_dt']- x['prev_release_dt']).days/365, axis = 1)\nprev_test_group = prev_test.groupby('id').aggregate({'prev_id':'count','year_to_lst_film_dir': 'min', 'log_revenue_dir' : 'mean'}).reset_index().rename(columns = {'prev_id': 'film_count_dir'})\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"40tOavL1Oen8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"5e20a0b1-6e48-4810-830f-62213b7e7dba","trusted":true},"cell_type":"code","source":"try:\n  test.drop(columns = ['film_count_dir'\t, 'year_to_lst_film_dir', 'log_revenue_dir'], inplace = True)\nexcept:\n  print('clear')\n  \ntry:\n  input_data.drop(columns = ['film_count_dir'\t, 'year_to_lst_film_dir', 'log_revenue_dir'], inplace = True)\nexcept:\n  print('clear')\n\ninput_data = pd.merge(input_data, prev_group, on ='id', how = 'left' )\ninput_data['year_to_lst_film_dir'].fillna(-100, inplace = True) \ninput_data['film_count_dir'].fillna(0, inplace = True)\ninput_data['log_revenue_dir'].fillna(0, inplace = True)\n\n\ntest = pd.merge(test, prev_test_group, on ='id', how = 'left' )\ntest['year_to_lst_film_dir'].fillna(-100, inplace = True) \ntest['film_count_dir'].fillna(0, inplace = True)\ntest['log_revenue_dir'].fillna(0, inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"LXqd4gpglIJ2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"2976cc86-4f85-4dd6-aaeb-4e311ef2761f","trusted":true},"cell_type":"code","source":"print( 'train rate without additional info', input_data[input_data['film_count_dir'] == 0].shape[0]/input_data.shape[0] )\nprint( 'test rate without additional info', test[test['film_count_dir'] == 0].shape[0]/test.shape[0] )","execution_count":null,"outputs":[]},{"metadata":{"id":"Nank9sOioY6J","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"input_data['collection'] = input_data['belongs_to_collection'].fillna('').apply(lambda x : [item['name'] for item in eval(x)][0] if  x != '' else '' )\ntest['collection'] = test['belongs_to_collection'].fillna('').apply(lambda x : [item['name'] for item in eval(x)][0] if  x != '' else '' )\nfull_data['collection'] = full_data['belongs_to_collection'].fillna('').apply(lambda x : [item['name'] for item in eval(x)][0] if  x != '' else '' )\n\nprev_collection =  pd.merge(input_data[['id','collection','release_dt']], full_data[['collection','release_dt','id', 'log_revenue']].rename(columns = { 'log_revenue': 'log_revenue_col',\n                                                                                          'release_dt': 'prev_col_release_dt',\n                                                                                                            'id': 'prev_id'}) , on = 'collection' )\n\nprev_collection = prev_collection[(prev_collection['prev_col_release_dt'] < prev_collection['release_dt'])& (prev_collection['collection'] != '')]\n\nprev_collection['year_to_lst_film_col'] = prev_collection.apply(lambda x: (x['release_dt']- x['prev_col_release_dt']).days/365, axis = 1)\nprev_collection = prev_collection.groupby('id').aggregate({'prev_id':'count','year_to_lst_film_col': 'min', 'log_revenue_col': 'mean' }).reset_index().rename(columns = {'prev_id': 'film_count_col'})\n\n\nprev_test_collection =  pd.merge(test[['id','collection','release_dt']], full_data[['collection','release_dt','id','log_revenue']].rename(columns = { 'log_revenue': 'log_revenue_col',\n                                                                                          'release_dt': 'prev_col_release_dt',\n                                                                                                            'id': 'prev_id'}) , on = 'collection' )\n\nprev_test_collection = prev_test_collection[(prev_test_collection['prev_col_release_dt'] < prev_test_collection['release_dt'])& (prev_test_collection['collection'] != '')]\n\nprev_test_collection['year_to_lst_film_col'] = prev_test_collection.apply(lambda x: (x['release_dt']- x['prev_col_release_dt']).days/365, axis = 1)\nprev_test_collection = prev_test_collection.groupby('id').aggregate({'prev_id':'count','year_to_lst_film_col': 'min',  'log_revenue_col': 'mean'}).reset_index().rename(columns = {'prev_id': 'film_count_col'})\n","execution_count":null,"outputs":[]},{"metadata":{"id":"mqkEcGpK9N92","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"fd8fb3d4-b51b-430d-fce3-763ae66b0b9d","trusted":true},"cell_type":"code","source":"try:\n  test.drop(columns = ['film_count_col'\t, 'year_to_lst_film_col', ' log_revenue_col'], inplace = True)\nexcept:\n  print('clear')\n  \ntry:\n  input_data.drop(columns = ['film_count_col'\t, 'year_to_lst_film_col',  ' log_revenue_col'], inplace = True)\nexcept:\n  print('clear')\n\ninput_data = pd.merge(input_data, prev_collection, on ='id', how = 'left' )\ninput_data['year_to_lst_film_col'].fillna(-100, inplace = True) \ninput_data['film_count_col'].fillna(0, inplace = True)\ninput_data['log_revenue_col'].fillna(0, inplace = True)\n \ntest = pd.merge(test, prev_test_collection, on ='id', how = 'left' )\ntest['year_to_lst_film_col'].fillna(-100, inplace = True) \ntest['film_count_col'].fillna(0, inplace = True)\ntest['log_revenue_col'].fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"3OOQ1z5RohrQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"977cfa1e-323d-495f-e6e8-a5c60322b28d","trusted":true},"cell_type":"code","source":"print( 'rate without additional info', input_data[input_data['film_count_col'] == 0].shape[0]/input_data.shape[0] )","execution_count":null,"outputs":[]},{"metadata":{"id":"rY-2FakWsLVF","colab_type":"text"},"cell_type":"markdown","source":"## Regular variables"},{"metadata":{"id":"bz4i_mwCsibn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":404},"outputId":"5d520380-9c0a-488c-9a75-6fc0b8c647f2","trusted":true},"cell_type":"code","source":"import seaborn as sns\nfig, ax = plt.subplots(1,2, figsize = (25,6)) \nax[0].bar(input_data['release_year'].value_counts().index, input_data['release_year'].value_counts().values)\nax[0].set_title('Amount of films by release year')\n\n# sns.heatmap(pd.crosstab(input_data['release_month'], input_data['release_day']) , ax=ax[1])\nsns.heatmap(pd.crosstab(index = input_data['release_month'], columns = input_data['release_day'],values = input_data['revenue'], aggfunc = 'count') , ax=ax[1])\n\n\nax[1].set_title('heatmap : amount of released films per month vs days')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"YLtQx_xJWmEx","colab_type":"text"},"cell_type":"markdown","source":"## Model"},{"metadata":{"id":"QiI0cVjrF0YT","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"input_data['log_budget'] = input_data['budget'].fillna(0).apply(lambda x: np.log(x+1))\ntest['log_budget'] = test['budget'].fillna(0).apply(lambda x: np.log(x+1))","execution_count":null,"outputs":[]},{"metadata":{"id":"YRGL1dqSPUDb","colab_type":"code","outputId":"7b35c9fb-5ce4-40a6-92a9-7296d664a655","colab":{"base_uri":"https://localhost:8080/","height":914},"trusted":true},"cell_type":"code","source":"#Delete outliers \n# ind = (input_data['log_revenue'] >= (Q1 - 1.5 * IQR)) & (input_data['log_revenue'] <= (Q3 + 1.5 * IQR) ) \nind = np.repeat(True, input_data.shape[0])\nfrom pygam import LinearGAM, s, l, f, te\ngam = LinearGAM(f(0) + f(1) + f(2) + f(3) + f(4) + f(5) + f(6) + f(7) + f(8) +\n               s(9) + s(10)  + s(11) + \n               s(12) + s(13) + s(14) +\n               s(15) + s(16) +s(17) +s(18) + s(19) +s(20))\ngam.gridsearch(input_data[ind][['label_country_list', 'label_genre_list','label_keywords_list', \n                           'label_crew_list', 'label_cast_list', 'label_prod_list', \n                          'label_tagline', 'label_overview_fst', 'img_label',\n                            'year_to_lst_film_col','film_count_col', 'log_revenue_col',\n          'year_to_lst_film_dir','film_count_dir', 'log_revenue_dir',\n                           \n                           'popularity','runtime', 'release_year', 'release_month', 'log_budget', 'release_day'\n                 \n                          ]].values, input_data[ind]['log_revenue'].values)\n\n\n## plotting\nplt.figure();\nfig, axs = plt.subplots(4,5 , figsize = (20,15))\n\naxi = axs.flatten()\ntitles = ['label_country_list', 'label_genre_list', 'label_keywords_list', \n          'label_crew_list', 'label_cast_list', 'label_prod_list',\n         'label_tagline', 'label_overview_fst', 'img_label',\n          'year_to_lst_film_col','film_count_col', 'log_revenue_col',\n          'year_to_lst_film_dir','film_count_dir','log_revenue_dir',\n          \n          'popularity','runtime', 'release_year', 'release_month', 'log_budget', 'release_day'\n         \n         \n         ]\nfor i, ax in enumerate(axi):\n#     if i > 17:\n#       break\n    XX = gam.generate_X_grid(term=i)\n    ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))\n    ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX, width=.95)[1], c='r', ls='--')\n    \n    \n    ax.set_title(titles[i])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"cjHkl3c9EQU0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":107},"outputId":"7f4fdc62-8dd4-469f-f9ac-e525ba477cf4","trusted":true},"cell_type":"code","source":"best_lam = gam.lam\nbest_gam = LinearGAM(f(0) + f(1) + f(2) + f(3) + f(4) + f(5) + f(6) + f(7) + f(8) +\n               s(9) + s(10)  + s(11) + \n               s(12) + s(13) + s(14) +\n               s(15) + s(16) + s(17) + s(18) + s(19) + s(20), lam = best_lam  )\nbest_gam.fit(input_data[ind][['label_country_list', 'label_genre_list','label_keywords_list', \n                           'label_crew_list', 'label_cast_list', 'label_prod_list', \n                          'label_tagline', 'label_overview_fst', 'img_label',\n                            'year_to_lst_film_col','film_count_col', 'log_revenue_col',\n          'year_to_lst_film_dir','film_count_dir', 'log_revenue_dir',\n                           \n                           'popularity','runtime', 'release_year', 'release_month', 'log_budget', 'release_day'\n                 \n                          ]].values, input_data[ind]['log_revenue'].values)","execution_count":null,"outputs":[]},{"metadata":{"id":"uYjNNJ0-W0-Y","colab_type":"code","outputId":"e97c6875-6e79-4e3d-f381-d66182bf04ca","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"np.sqrt(mean_squared_error(best_gam.predict(input_data[['label_country_list', 'label_genre_list','label_keywords_list', \n                           'label_crew_list', 'label_cast_list', 'label_prod_list', \n                          'label_tagline', 'label_overview_fst', 'img_label',\n                            'year_to_lst_film_col','film_count_col', 'log_revenue_col',\n          'year_to_lst_film_dir','film_count_dir', 'log_revenue_dir',\n                           \n                           'popularity','runtime', 'release_year', 'release_month',  'log_budget', 'release_day'\n                 \n                          ]].values), \n                           input_data['log_revenue'].values) )","execution_count":null,"outputs":[]},{"metadata":{"id":"NDX2L40iIOFo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"218868b9-abce-4e22-d928-bdcf504460d8","trusted":true},"cell_type":"code","source":"input_data[(input_data['log_revenue'] >= (Q1 - 1.5 * IQR)) & (input_data['log_revenue'] <= (Q3 + 1.5 * IQR) ) ].shape[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"aP0McmylQt-o","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"test_pred = best_gam.predict(test[['label_country_list', 'label_genre_list','label_keywords_list', \n                           'label_crew_list', 'label_cast_list', 'label_prod_list', \n                          'label_tagline', 'label_overview_fst', 'img_label',\n                            'year_to_lst_film_col','film_count_col', 'log_revenue_col',\n          'year_to_lst_film_dir','film_count_dir', 'log_revenue_dir',\n                           \n                           'popularity','runtime', 'release_year', 'release_month', 'log_budget' , 'release_day'\n                                                  ]].values)","execution_count":null,"outputs":[]},{"metadata":{"id":"jguZk7Sh-4v-","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"pd.DataFrame( data = {'id' : test['id'], 'revenue': np.exp(test_pred) }).to_csv('gam_submit_model.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"final_solution.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["cGVNlamYH1Ek","mP91dZusTGGT","ZRwU3amgTrNm","7_XkRWs775Cr","FJnxrej19uDD","-fjv9veRx-qm","aFwh3itVUaUQ","I42feKS4Ynaa"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":1}