{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\ndef get_dictionary(s):\n    try:\n        i = eval(s)\n    except:\n        i = {}\n    return i\n\ndef prepare(df):\n    global json_cols\n    global train_dict\n    \n    df[['release_month', 'release_day', 'release_year']] = df['release_date'].str.split('/', expand=True).replace(np.nan, 0).astype(int)\n    \n    #df['release_year'] = df['release_year']\n    #df.loc[(df['release_year'] <= 19) & (df['release_year'] < 100), 'release_year'] += 2000\n    #df.loc[(df['release_year'] < 19) & (df['release_year'] < 100), 'release_year'] += 1900\n    df['release_year'].map(lambda x : x if x > 100 else x + (2000 if (x <= 18) else 1900))\n    \n    releaseDate = pd.to_datetime(df['release_date'])\n    \n    df['release_dayofweek'] = releaseDate.dt.dayofweek\n    df['release_quarter'] = releaseDate.dt.quarter\n    \n    #df['rating'] = df.rating.fillna(1.5)\n    #df['totalVotes'] = df.totalVotes.fillna(6)\n    \n    df['rating'] = df.rating.fillna(df['rating'].median())\n    df['totalVotes'] = df.totalVotes.fillna(df['rating'].median())\n    \n    # Weighte Rating log\n    df['weighteRating'] = np.log(df['rating'] * df['totalVotes'])\n    \n    df['originalBudget'] = df['budget']\n    \n    # Inflation simple formula\n    df['inflationBudget'] = df['budget'] + df['budget'] * 1.8 / 100 * (2018 - df['release_year'])\n    # Budget log\n    df['budget'] = np.log1p(df['budget'])\n    \n    # Gender number of people\n    df['genders_0_crew'] = df['crew'].map(lambda x : sum([1 for i in get_dictionary(x) if i['gender'] == 0]))\n    df['genders_1_crew'] = df['crew'].map(lambda x : sum([1 for i in get_dictionary(x) if i['gender'] == 1]))\n    df['genders_2_crew'] = df['crew'].map(lambda x : sum([1 for i in get_dictionary(x) if i['gender'] == 2]))\n    \n    # Has collection name\n    df['_collection_name'] = df['belongs_to_collection'].map(lambda x : [i['name'] for i in get_dictionary(x)])\n    df['_collection_name'] = le.fit_transform(list(df['_collection_name'].fillna('').astype(str)))\n    \n    # Key word number of Movie\n    df['_num_Keywords'] = df['Keywords'].map(lambda x : len(eval(x)) if get_dictionary(x) != {} else 0)\n    # Cast number of peple\n    df['_num_cast'] = df['cast'].map(lambda x : len(eval(x)) if get_dictionary(x) != {} else 0)\n    \n    # Popularity by Year\n    df['_popularity_mean_year'] = df['popularity'] / df.groupby('release_year')['popularity'].transform('mean')\n    \n    # Each ratio\n    df['_budget_runtime_ratio'] = df.budget / df.runtime\n    df['_release_year_popularity_ratio'] = df.release_year / df.popularity\n    df['_release_year_polularity_ratio2'] = df.popularity / df.release_year\n    df['_popularity_totalVote_ratio'] = df.totalVotes / df.popularity\n    df['_rating_popularity_ratio'] = df.rating / df.popularity\n    df['_rating_totalVotes_ratio'] = df.totalVotes / df.rating\n    df['_totalVotes_releaseYear_ratio'] = df.totalVotes / df.release_year\n    \n    # Has Homepage one hot encoding\n    # I have 1 and I Don't have 0\n    df['has_homepage'] = 1\n    df.loc[df['homepage'].isnull(), 'has_homepage'] = 0\n    \n    # is belongs to collection one hot encoding\n    # is belongs_to_collectionNa = 0\n    # Does not belongs to collection = 1\n    df['isbelongs_to_collectionNA'] = 0\n    df.loc[df['belongs_to_collection'].isnull(), 'isbelongs_to_collectionNA'] = 1\n    \n    # Tagline one hot encoding\n    # isTagllineNa = 0\n    # Does not Tagline = 1\n    df['isTaglineNA'] = 0\n    df.loc[df['tagline'].isnull(), 'isTaglineNA'] = 1\n    \n    # original_language one hot encoding\n    # en = 1 outher = 0\n    df['isOriginalLanguage'] = 0\n    df.loc[df['original_language'] == 'en', 'isOriginalLanguage'] = 1\n    \n    # original_title one hot encoding\n    # isTitleDifferent = 1\n    # The Title and the original_title are the same\n    df['isTitleDifferent'] = 1\n    df.loc[df['original_title'] == df['title'], 'isTitleDifferent'] = 0\n    \n    # Movie Released and Rumored one hot encoding\n    # Movie Released = 0 Rumored = 1\n    df['isMovieReleased'] = 1\n    df.loc[df['status'] == 'Released', 'isMovieReleased'] = 0\n    \n    # get collection id\n    df['collection_id'] = df['belongs_to_collection'].map(lambda x : np.nan if len(get_dictionary(x)) == 0 else get_dictionary(x)[0]['id'])   \n    # Original Title Letter Count\n    df['original_title_letter_count'] = df['original_title'].str.len()\n    # Original Title Word Count\n    df['original_title_word_count'] = df['original_title'].str.split().str.len()  \n    # Title Word Count\n    df['title_word_count'] = df['title'].str.split().str.len()    \n    # Tagline Word Count\n    df['tagline_word_count'] = df['tagline'].str.split().str.len()  \n    # Production Countries Count\n    df['production_countries_count'] = df['production_countries'].map(lambda x : len(get_dictionary(x))) \n    # Cast Count\n    df['cast_count'] = df['cast'].map(lambda x : len(get_dictionary(x)))\n    # Crew Count\n    df['crew_count'] = df['crew'].map(lambda x : len(get_dictionary(x)))\n    # Mean Runtime By Release Year\n    df['meanruntimeByYear'] = df.groupby('release_year')['runtime'].transform('mean').round()\n    # Mean Popularity By Release Year\n    df['meanPopularityByYear'] = df.groupby('release_year')['popularity'].transform('mean').round()\n    # Mean Budget By Release Year\n    df['meanBudgetByYear'] = df.groupby('release_year')['budget'].transform('mean')\n    # Mean Total Votes By Release Year\n    df['meanTotalVotesByYear'] = df.groupby('release_year')['totalVotes'].transform('mean').round()\n    # Mean Totalvotes By Rating\n    df['meanTotalVotesByRating'] = df.groupby('rating')['totalVotes'].transform('mean').round()\n    # Median Budget By Release Year\n    df['medianBudgetByYear'] = df.groupby('release_year')['budget'].transform('median')\n    # get_dummies\n    for col in ['genres', 'production_countries', 'spoken_languages', 'production_companies']:\n        df[col] = df[col].map(lambda x : sorted(list(set([n if n in train_dict['genres'] else 'genres' + '_etc' for n in [i['name'] for i in get_dictionary(x)]])))).map(lambda x : ','.join(map(str, x)))\n        temp = df[col].str.get_dummies(sep=',')\n        df = pd.concat([df, temp], axis=1, sort=False)        \n    df.drop(['genres_etc'], axis=1, inplace=True)\n    df = df.drop(['id', 'revenue', 'belongs_to_collection',\n                         'genres', 'homepage', 'imdb_id', 'overview',\n                         'runtime', 'poster_path', 'production_companies',\n                         'production_countries', 'release_date', 'spoken_languages',\n                         'status', 'title', 'Keywords', 'cast', 'crew',\n                         'original_language', 'original_title', 'tagline',\n                         'collection_id'], axis=1)\n    df.fillna(value=0.0, inplace=True)\n    return df\n\ntrain = pd.read_csv('../input/tmdb-box-office-prediction/train.csv')\n\ntest = pd.read_csv('../input/tmdb-box-office-prediction/test.csv')\n\ntest['revenue'] = np.nan\n\nTrainAdditionalFeatures = pd.read_csv('../input/tmdb-competition-additional-features/TrainAdditionalFeatures.csv')\nTestAdditionalFeatures = pd.read_csv('../input/tmdb-competition-additional-features/TestAdditionalFeatures.csv')\n\ntrain = pd.merge(train, TrainAdditionalFeatures, how='left', on=['imdb_id'])\ntest = pd.merge(test, TestAdditionalFeatures, how='left', on=['imdb_id'])\n\nadditionalTrainData = pd.read_csv('../input/tmdb-box-office-prediction-more-training-data/additionalTrainData.csv')\nadditionalTrainData['release_date'] = additionalTrainData['release_date'].astype('str')\nadditionalTrainData['release_date'] = additionalTrainData['release_date'].str.replace('-', '/')\n\ntrain = pd.concat([train, additionalTrainData])\n\ntrain['revenue'] = np.log1p(train['revenue'])\ny = train['revenue'].values    \n     \njson_cols = ['genres', 'production_companies', 'production_countries',\n             'spoken_languages', 'Keywords', 'cast', 'crew']\n\ndef get_json_dict(df):\n    global json_cols\n    result = {}\n    for e_col in json_cols:\n        d = {}\n        rows = df[e_col].values\n        for row in rows:\n            if row is None: continue\n            if type(row) is str:\n                for i in get_dictionary(row):\n                    if i['name'] not in d:\n                        d[i['name']] = 0\n                    d[i['name']] += 1\n        result[e_col] = d\n    return result\n\ntrain_dict = get_json_dict(train)\ntest_dict = get_json_dict(test)\n\nfor col in json_cols:\n    remove = []\n    train_id = set(list(train_dict[col].keys()))\n    test_id = set(list(test_dict[col].keys()))   \n\n    remove += list(train_id - test_id) + list(test_id - train_id)\n    for i in train_id.union(test_id) - set(remove) :\n        if train_dict[col][i] < 10 or i == '' :\n            remove += [i]\n    for i in remove :\n        if i in train_dict[col] :\n            del train_dict[col][i]\n        if i in test_dict[col] :\n            del test_dict[col][i]\n\nall_data = prepare(pd.concat([train, test]).reset_index(drop = True))\ntrain = all_data.loc[:train.shape[0] - 1,:]\ntest = all_data.loc[train.shape[0]:,:] \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\n\nprint(y.shape)\n\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_seed = 2019\nk = 10\nfold = list(KFold(k, shuffle=True, random_state=random_seed).split(train))\nnp.random.seed(random_seed)\n\n# k = k回 listを作る\n# KFold（K-分割交差検証）\n# データをk個に分け，n個を訓練用，k-n個をテスト用として使う．\n# 分けられたn個のデータがテスト用として必ず1回使われるようにn回検定する．\n\n# n_split：データの分割数．つまりk．検定はここで指定した数値の回数おこなわれる．\n# shuffle：Trueなら連続する数字でグループ分けせず，ランダムにデータを選択する．\n# random_state：乱数のシードを指定できる．\n\n# numpy.random.seed(seed=シードに用いる値) をシード (種) を指定することで、\n# 発生する乱数をあらかじめ固定することが可能です。\n# 乱数を用いる分析や処理で、再現性が必要な場合などに用いられます","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_dict = {}\nval_pred = np.zeros(train.shape[0])\ntest_pred = np.zeros(test.shape[0])\nfinal_err = 0\nverbose = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def xgb_model(trn_x, trn_y, val_x, val_y, test, verbose):\n    \n    params = {'objective':'reg:linear',\n             'eta': 0.01,\n             'max_depth':6,\n             'subsample':0.6,\n             'colsample_bytree':0.7,\n             'eval_metric':'rmse',\n             'seed':random_seed,\n             'silent':True,\n             }\n    \n    \n    record = dict()\n    model = xgb.train(params,\n                     xgb.DMatrix(trn_x, trn_y),\n                     100000,\n                     [(xgb.DMatrix(trn_x, trn_y), 'train'), \n                     (xgb.DMatrix(val_x, val_y), 'valid')],\n                     verbose_eval = verbose,\n                     early_stopping_rounds = 500,\n                     callbacks = [xgb.callback.record_evaluation(record)])\n    best_idx = np.argmin(np.array(record['valid']['rmse']))\n    \n    val_pred = model.predict(xgb.DMatrix(val_x), ntree_limit=model.best_ntree_limit)\n    test_pred = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit)\n    \n    return {'val' : val_pred, \n            'test' : test_pred, \n            'error': record['valid']['rmse'][best_idx],\n           'importance': [i for k, i in model.get_score().items()]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\n\nfor e, (trn, val) in enumerate(fold):\n    print(e + 1, '  fold.    RMSE')\n    #print('trn', trn)\n    #print('val', val)\n    \n    trn_x = train.loc[trn, :]\n    #print(trn_x.shape)\n    trn_y = y[trn]\n    #print(trn_y.shape)\n    val_x = train.loc[val, :]\n    #print(val_x.shape)\n    val_y = y[val]\n    #print(val_y.shape)\n    \n    fold_val_pred = []\n    fold_test_pred = []\n    fold_err = []\n    \n    # xgboost\n    start = datetime.datetime.now()\n    result = xgb_model(trn_x, trn_y, val_x, val_y, test, verbose)\n    fold_val_pred.append(result['val'])\n    fold_test_pred.append(result['test'])\n    fold_err.append(result['error'])\n    print('xgb model' , '{0:.5f}'.format(result['error']), '(' + str(int((datetime.datetime.now() - start).seconds/60)) + 'm)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/tmdb-box-office-prediction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reve = np.expm1(fold_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reve[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub = pd.DataFrame()\ndf_sub['id'] = sub['id']\ndf_sub['revenue'] = reve[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.to_csv('submission_1.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}