{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TMDB Box Office Prediction EDA + ML\n\n![](https://cdn-images-1.medium.com/max/1200/1*vIR7iO-1GnY2xYxL6NiYkw.png)\n[image-source](https://cdn-images-1.medium.com/max/1200/1*vIR7iO-1GnY2xYxL6NiYkw.png)\n\nIn a world... where movies made an estimated $41.7 billion in 2018, the film industry is more popular than ever. But what movies make the most money at the box office? How much does a director matter? Or the budget? For some movies, it's \"You had me at 'Hello.'\" For others, the trailer falls short of expectations and you think \"What we have here is a failure to communicate.\"\n\nIn this competition, you're presented with metadata on over 7,000 past films from The Movie Database to try and predict their overall worldwide box office revenue. Data points provided include cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries. You can collect other publicly available data to use in your model predictions, but in the spirit of this competition, use only data that would have been available before a movie's release.\n\n## *Kernel in progress, is continuously being updated and extended*"},{"metadata":{},"cell_type":"markdown","source":"## Preparations - Prerequisities"},{"metadata":{},"cell_type":"markdown","source":"![](https://images-na.ssl-images-amazon.com/images/I/91HTK796%2BML._SX425_.jpg)\n[image-source](https://images-na.ssl-images-amazon.com/images/I/91HTK796%2BML._SX425_.jpg)"},{"metadata":{},"cell_type":"markdown","source":"### Loading Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"../input\"))\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nimport gc\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading the data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\nsub_df = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inspecting the train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a variaty of data, numerical, categorical and even lists of json formats."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for NA values in trainset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"![](https://prod-discovery.edx-cdn.org/media/course/image/2102f79d-9a44-41e9-9d92-884bec46dc65-ff40350cad17.small.jpg)\n[image-source](https://prod-discovery.edx-cdn.org/media/course/image/2102f79d-9a44-41e9-9d92-884bec46dc65-ff40350cad17.small.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So many columns and features to investigate, lets start by inspecting one by one each feature."},{"metadata":{},"cell_type":"markdown","source":"### Univariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Revenue\nOur target variable that must be predicted"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(3, figsize=(12,7))\n\nsns.set(rc={'figure.figsize':(12,8)})\nsns.boxplot(x=train_df.revenue, ax = ax[0])\nax[0].set_title(\"revenue Boxplot\")\nsns.distplot(a=train_df.revenue, kde = False, ax = ax[1])\nax[1].set_title(\"revenue Histogram\")\nsns.distplot(a=np.log1p(train_df.revenue), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed revenue Histogram\")\nf.tight_layout()\n\ntrain_df[\"log_revenue\"] = np.log1p(train_df.revenue)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Title\nLets generate a wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud().generate(train_df.title.to_string())\n\nsns.set(rc={'figure.figsize':(12,8)})\n\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see the length of each movie"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"title\"] = train_df[\"title\"].fillna(\"\")\ntest_df[\"title\"] = test_df[\"title\"].fillna(\"\")\n\ntrain_df[\"title_len\"] = train_df[\"title\"].apply(len)\ntest_df[\"title_len\"] = test_df[\"title\"].apply(len)\n\nf, ax = plt.subplots(3, figsize=(12,7))\nsns.set(rc={'figure.figsize':(12,8)})\nsns.boxplot(x=train_df.title_len, ax = ax[0])\nax[0].set_title(\"titles' length Boxplot\")\nsns.distplot(a=train_df.title_len, kde = False, ax = ax[1])\nax[1].set_title(\"titles' length Histogram\")\nsns.distplot(a=np.log1p(train_df.title_len), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed titles' length Histogram\")\nf.tight_layout()\n\ntrain_df[\"log_title_len\"] = np.log1p(train_df.title_len)\ntest_df[\"log_title_len\"] = np.log1p(test_df.title_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Overview\nLets visualize movies' overview wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud().generate(train_df.overview.to_string())\n\nsns.set(rc={'figure.figsize':(12,8)})\n\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inspecting movies' overview length"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"overview\"] = train_df[\"overview\"].fillna(\"\")\ntest_df[\"overview\"] = test_df[\"overview\"].fillna(\"\")\n\ntrain_df[\"overview_len\"] = train_df[\"overview\"].apply(len)\ntest_df[\"overview_len\"] = test_df[\"overview\"].apply(len)\n\nf, ax = plt.subplots(3, figsize=(12,7))\nsns.set(rc={'figure.figsize':(12,8)})\nsns.boxplot(x=train_df.overview_len, ax = ax[0])\nax[0].set_title(\"overview' length Boxplot\")\nsns.distplot(a=train_df.overview_len, kde = False, ax = ax[1])\nax[1].set_title(\"overview' length Histogram\")\nsns.distplot(a=np.log1p(train_df.overview_len), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed overview' length Histogram\")\nf.tight_layout()\n\ntrain_df[\"log_overview_len\"] = np.log1p(train_df.overview_len)\ntest_df[\"log_overview_len\"] = np.log1p(test_df.overview_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tagline"},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud().generate(train_df.tagline.to_string())\n\nsns.set(rc={'figure.figsize':(12,8)})\n\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"tagline\"] = train_df[\"tagline\"].fillna(\"\")\ntest_df[\"tagline\"] = test_df[\"tagline\"].fillna(\"\")\n\ntrain_df[\"tagline_len\"] = train_df[\"tagline\"].apply(len)\ntest_df[\"tagline_len\"] = test_df[\"tagline\"].apply(len)\n\nf, ax = plt.subplots(3, figsize=(12,7))\nsns.set(rc={'figure.figsize':(12,8)})\nsns.boxplot(x=train_df.tagline_len, ax = ax[0])\nax[0].set_title(\"tagline_len' length Boxplot\")\nsns.distplot(a=train_df.tagline_len, kde = False, ax = ax[1])\nax[1].set_title(\"tagline_len' length Histogram\")\nsns.distplot(a=np.log1p(train_df.tagline_len), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed tagline_len' length Histogram\")\nf.tight_layout()\n\ntrain_df[\"log_tagline_len\"] = np.log1p(train_df.tagline_len)\ntest_df[\"log_tagline_len\"] = np.log1p(test_df.tagline_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Budget"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(3, figsize=(12,7))\n\nsns.set(rc={'figure.figsize':(12,8)})\nsns.boxplot(x=train_df.budget, ax = ax[0])\nax[0].set_title(\"budget Boxplot\")\nsns.distplot(a=train_df.budget, kde = False, ax = ax[1])\nax[1].set_title(\"budget Histogram\")\nsns.distplot(a=np.log1p(train_df.budget), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed budget Histogram\")\nf.tight_layout()\n\ntrain_df[\"log_budget\"] = np.log1p(train_df.budget)\ntest_df[\"log_budget\"] = np.log1p(test_df.budget)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Genres"},{"metadata":{"trusted":true},"cell_type":"code","source":"def genres_preprocessing(elem):\n    string = str(elem)\n    str1 = string.replace(']','').replace('[','').replace('{','').replace('}','').replace('\\'','').replace(' ','').replace(\"name\", \"\").replace(\"id\", \"\").replace(\":\", \"\")\n    ll = str1.split(\",\")[1::2]\n    return ll\n\ntrain_df[\"genres_processed\"] = train_df.genres.apply(lambda elem: genres_preprocessing(elem))\ntest_df[\"genres_processed\"] = test_df.genres.apply(lambda elem: genres_preprocessing(elem))\n\ngenres_dict = dict()\n\nfor genre in train_df[\"genres_processed\"]:\n    for elem in genre:\n        if elem not in genres_dict:\n            genres_dict[elem] = 1\n        else:\n            genres_dict[elem] += 1\n\n\nsns.set(rc={'figure.figsize':(12,8)})\ngenres_df = pd.DataFrame.from_dict(genres_dict, orient='index')\ngenres_df.columns = [\"number_of_movies\"]\ngenres_df = genres_df.sort_values(by=\"number_of_movies\", ascending=False)\ngenres_df.plot.bar()\nplt.title(\"Number of films per genre\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Genres"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(9,8)})\ntrain_df['num_genres'] = train_df['genres_processed'].apply(lambda x: len(x) if x != {} else 0)\ntest_df['num_genres'] = test_df['genres_processed'].apply(lambda x: len(x) if x != {} else 0)\n\ntrain_df['num_genres'].value_counts().plot.bar()\nplt.title(\"Number of films with more than 1 genre\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### isGenre, feature engineering, creating new feature\nisDrama, isComedy etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"genres_df.index.values\nfor g in genres_df.index.values:\n    train_df['isGenre_' + g] = train_df['genres_processed'].apply(lambda x: 1 if g in x else 0)\n    test_df['isGenre_' + g] = test_df['genres_processed'].apply(lambda x: 1 if g in x else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Original Language"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.original_language.value_counts()[:10].plot.bar()\nplt.title(\"Number of films per language\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### English and Non-English movies"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_df[\"is_english_language\"] = train_df.original_language.apply(lambda x: 1 if x == \"en\" else 0)\ntest_df[\"is_english_language\"] = test_df.original_language.apply(lambda x: 1 if x == \"en\" else 0)\n\ntrain_df.is_english_language = train_df.is_english_language.fillna(1)\ntest_df.is_english_language = test_df.is_english_language.fillna(1)\n\nsns.set(rc={'figure.figsize':(12,8)})\nax = sns.countplot(x=\"is_english_language\", data=train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Production Companies"},{"metadata":{"trusted":true},"cell_type":"code","source":"def production_companies_preprocessing(elem):\n    string = str(elem)\n    str1 = string.replace(']','').replace('[','').replace('{','').replace('}','').replace(' ','').replace(\"name\", \"\").replace(\"id\", \"\").replace(\":\", \"\").replace(\"\\'\", \"\")\n    ll = str1.split(\",\")[0::2]\n    return ll\n\ntrain_df[\"production_companies\"] = train_df.production_companies.fillna('NoProductionCompany')\ntrain_df[\"production_companies\"] = test_df.production_companies.fillna('NoProductionCompany')\n\ntrain_df[\"production_companies_processed\"] = train_df.production_companies.apply(lambda elem: production_companies_preprocessing(elem))\ntest_df[\"production_companies_processed\"] = test_df.production_companies.apply(lambda elem: production_companies_preprocessing(elem))\n\n\n\nproduction_companies_dict = dict()\n\nfor production_company in train_df[\"production_companies_processed\"]:\n    for elem in production_company:\n        if elem not in production_companies_dict:\n            production_companies_dict[elem] = 1\n        else:\n            production_companies_dict[elem] += 1\n\n\nsns.set(rc={'figure.figsize':(12,8)})\nproduction_companies_df = pd.DataFrame.from_dict(production_companies_dict, orient='index')\nproduction_companies_df.columns = [\"number_of_movies\"]\nproduction_companies_df = production_companies_df.sort_values(by=\"number_of_movies\", ascending=False)\nproduction_companies_df.head(10).plot.bar()\nplt.title(\"Number of films per production company\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Production Companies"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_production_companies\"] = train_df.production_companies_processed.apply(len)\ntest_df[\"num_of_production_companies\"] = test_df.production_companies_processed.apply(len)\n\ntrain_df[\"num_of_production_companies\"].value_counts().plot.bar()\nplt.title(\"Number of multiple production companies per movie\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### isProductionCompany feature engineering\nFeature Engineering with all the production companies"},{"metadata":{"trusted":true},"cell_type":"code","source":"for g in production_companies_df.index.values:\n    train_df['isProductionCompany_' + g] = train_df['production_companies_processed'].apply(lambda x: 1 if g in x else 0)\n    test_df['isProductionCompany_' + g] = test_df['production_companies_processed'].apply(lambda x: 1 if g in x else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### production_countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"def production_countries_preprocessing(elem):\n    string = str(elem)\n    str1 = string.replace(']','').replace('[','').replace('{','').replace('}','').replace(' ','').replace(\"name\", \"\").replace(\"iso_3166_1\", \"\").replace(\":\", \"\").replace(\"\\'\", \"\")\n    ll = str1.split(\",\")[0::2]\n    return ll\n\ntrain_df[\"production_countries_processed\"] = train_df.production_countries.fillna(\"NaN\").apply(lambda elem: production_countries_preprocessing(elem))\ntest_df[\"production_countries_processed\"] = test_df.production_countries.fillna(\"NaN\").apply(lambda elem: production_countries_preprocessing(elem))\n\n\nproduction_countries_dict = dict()\n\nfor production_country in train_df[\"production_countries_processed\"]:\n    for elem in production_country:\n        if elem not in production_countries_dict:\n            production_countries_dict[elem] = 1\n        else:\n            production_countries_dict[elem] += 1\n\n\n\nproduction_countries_df = pd.DataFrame.from_dict(production_countries_dict, orient='index')\nproduction_countries_df.columns = [\"number_of_movies\"]\nproduction_countries_df = production_countries_df.sort_values(by=\"number_of_movies\", ascending=False)\nproduction_countries_df.head(10).plot.bar()\nplt.title(\"Number of films per production country\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### isProduction_country feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in production_countries_df.index.values:\n    train_df['isProductionCountry_' + c] = train_df['production_countries_processed'].apply(lambda x: 1 if c in x else 0)\n    test_df['isProductionCountry_' + c] = test_df['production_countries_processed'].apply(lambda x: 1 if c in x else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### popularity"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(3, figsize=(12,7))\nsns.boxplot(x=train_df.popularity, ax = ax[0])\nax[0].set_title(\"Popularity Boxplot\")\nsns.distplot(a=train_df.popularity, kde = False, ax = ax[1])\nax[1].set_title(\"Popularity Histogram\")\nsns.distplot(a=np.log1p(train_df.popularity), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed Popularity Histogram\")\nf.tight_layout()\n\ntrain_df[\"log_popularity\"] = np.log1p(train_df.popularity)\ntest_df[\"log_popularity\"] = np.log1p(test_df.popularity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Runtime"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"runtime\"] = train_df[\"runtime\"].fillna(train_df[\"runtime\"].mode()[0])\ntest_df[\"runtime\"] = test_df[\"runtime\"].fillna(test_df[\"runtime\"].mode()[0])\n\nf, ax = plt.subplots(4, figsize=(12,7))\n\ntrain_df.runtime = train_df.runtime.fillna(train_df.runtime.mode())\n\nsns.boxplot(x=train_df.runtime, ax = ax[0])\nax[0].set_title(\"Runtime Boxplot\")\nsns.distplot(a=train_df.runtime, kde = False, ax = ax[1])\nax[1].set_title(\"Runtime Histogram\")\nsns.distplot(a=train_df.runtime/360, kde = False, ax = ax[2])\nax[2].set_title(\"Runtime in Hours Histogram\")\nsns.distplot(a=np.log1p(train_df.runtime), kde = False, ax = ax[3])\nax[3].set_title(\"Log1p transformed Runtime Histogram\")\nf.tight_layout()\n\ntrain_df[\"runtime_in_hours\"] = train_df.runtime/360\ntest_df[\"runtime_in_hours\"] = test_df.runtime/360\n\ntrain_df[\"log_runtime\"] = np.log1p(train_df.runtime)\ntest_df[\"log_runtime\"] = np.log1p(test_df.runtime)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Release Date preprocessing before EDA and ML"},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\n\n# fill possible NA values with the statistical mode\ntrain_df[\"release_date\"] = train_df[\"release_date\"].fillna(train_df[\"release_date\"].mode()[0])\ntest_df[\"release_date\"] = test_df[\"release_date\"].fillna(test_df[\"release_date\"].mode()[0])\n\n\ntrain_df['temp'] = train_df.release_date.apply(lambda x: datetime.strptime(x, '%m/%d/%y'))\n\ntrain_df[\"month\"] = train_df.temp.apply(lambda x: x.month)\ntrain_df[\"season\"] = train_df[\"month\"]%4\ntrain_df[\"year\"] = train_df.temp.apply(lambda x: x.year)\ntrain_df[\"day_of_week\"] = train_df.temp.apply(lambda x: x.weekday()+1)\ntrain_df[\"week_of_year\"] = train_df.temp.apply(lambda x: x.isocalendar()[1])\n\ntrain_df = train_df.drop(['temp'], axis=1)\n\n\ntest_df['temp'] = test_df.release_date.apply(lambda x: datetime.strptime(x, '%m/%d/%y'))\n\ntest_df[\"month\"] = test_df.temp.apply(lambda x: x.month)\ntest_df[\"season\"] = test_df[\"month\"]%4\ntest_df[\"year\"] = test_df.temp.apply(lambda x: x.year)\ntest_df[\"day_of_week\"] = test_df.temp.apply(lambda x: x.weekday()+1)\ntest_df[\"week_of_year\"] = test_df.temp.apply(lambda x: x.isocalendar()[1])\n\ntest_df = test_df.drop(['temp'], axis=1)\n\n\n\ntrain_df[\"day_of_week\"] = train_df[\"day_of_week\"].fillna(train_df[\"day_of_week\"].mode()[0])\ntest_df[\"day_of_week\"] = test_df[\"day_of_week\"].fillna(test_df[\"day_of_week\"].mode()[0])\n\ntrain_df[\"year\"] = train_df[\"year\"].fillna(train_df[\"year\"].mode()[0])\ntest_df[\"year\"] = test_df[\"year\"].fillna(test_df[\"year\"].mode()[0])\n\ntrain_df[\"month\"] = train_df[\"month\"].fillna(train_df[\"month\"].mode()[0])\ntest_df[\"month\"] = test_df[\"month\"].fillna(test_df[\"month\"].mode()[0])\n\ntrain_df[\"week_of_year\"] = train_df[\"week_of_year\"].fillna(train_df[\"week_of_year\"].mode()[0])\ntest_df[\"week_of_year\"] = test_df[\"week_of_year\"].fillna(test_df[\"week_of_year\"].mode()[0])\n\ntrain_df[\"season\"] = train_df[\"season\"].fillna(train_df[\"season\"].mode()[0])\ntest_df[\"season\"] = test_df[\"season\"].fillna(test_df[\"season\"].mode()[0])\n\ntrain_df[[\"release_date\", \"month\", \"year\", \"day_of_week\", \"week_of_year\", \"season\"]].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Month of Release, which month has most of the releases"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12,8)})\ntrain_df.month.value_counts().plot.bar()\nplt.title('Number of films per month')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Week of year, which week of the year has most of the releases"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(16,8)})\ntrain_df.week_of_year.value_counts().plot.bar()\nplt.title('Number of films per week_of_year')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Season of Release, which season has most of the releases"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(16,8)})\ntrain_df.season.value_counts().plot.bar()\nplt.title('Number of films per season')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Day of Release, which day of the week has most of the releases"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12,8)})\ntrain_df.day_of_week.value_counts().plot.bar()\nplt.title('Number of films per day_of_week')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Year of Release, which year has most of the releases"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(20,8)})\ntrain_df.year.value_counts().plot.bar()\nplt.title('Number of films per year')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Identifying top actors in movies based on mean movies' revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nactors_dict = {}\nsize_of_actors = len(train_df) - train_df.cast.isna().sum()\n\nfor element in train_df[[\"revenue\", \"cast\"]].values:\n    if type(element[1]) == type(str()):\n        \n        result = re.findall('name\\': \\'\\w+\\s*\\w*', element[1])\n        result = [x.replace(\"name\\': \\'\", \"\") for x in result]\n\n        for actor in result:\n            if actor not in actors_dict:\n                actors_dict[actor] = element[0]\n            else:\n                actors_dict[actor] += element[0]\n                \nfor actor in actors_dict:\n    actors_dict[actor] = actors_dict[actor]/size_of_actors\n    \n\n\nactors_df = pd.DataFrame.from_dict(actors_dict, orient='index', columns=[\"mean_movies_revenue\"])\nactors_df.sort_values(by=\"mean_movies_revenue\", ascending=False).head(20).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### feature engineering, creating the has_top_actor columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_top_actor_from_cast(top_actor, element):\n    \n    result = []\n    if type(element) == type(str()):\n\n        result = re.findall('name\\': \\'\\w+\\s*\\w*', element)\n        result = [x.replace(\"name\\': \\'\", \"\") for x in result]\n        \n    if top_actor in result:\n        return 1\n    else:\n        return 0\n\nfor top_actor in actors_df.sort_values(by=\"mean_movies_revenue\", ascending=False).head(10).index.values:\n    train_df[\"has_top_actor_\"+ top_actor] = train_df.cast.apply(lambda element: find_top_actor_from_cast(top_actor, element))\n    test_df[\"has_top_actor_\"+ top_actor] = test_df.cast.apply(lambda element: find_top_actor_from_cast(top_actor, element))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Identifying the top keywords based on mean movie revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nkeywords_dict = {}\nsize_of_keywords = len(train_df) - train_df.Keywords.isna().sum()\n\nfor element in train_df[[\"revenue\", \"Keywords\"]].values:\n    if type(element[1]) == type(str()):\n        \n        result = re.findall('name\\': \\'\\w+\\s*\\w*', element[1])\n        result = [x.replace(\"name\\': \\'\", \"\") for x in result]\n\n        for key in result:\n            if key not in keywords_dict:\n                keywords_dict[key] = element[0]\n            else:\n                keywords_dict[key] += element[0]\n                \nfor key in keywords_dict:\n    keywords_dict[key] = keywords_dict[key]/size_of_keywords\n    \nkeywords_df = pd.DataFrame.from_dict(keywords_dict, orient='index', columns=[\"mean_movies_revenue\"])\nkeywords_df.sort_values(by=\"mean_movies_revenue\", ascending=False).head(10).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### feature engineering has_top_keyword based on mean movies' revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_top_keywords_from_cast(top_keyword, element):\n    \n    result = []\n    if type(element) == type(str()):\n\n        result = re.findall('name\\': \\'\\w+\\s*\\w*', element)\n        result = [x.replace(\"name\\': \\'\", \"\") for x in result]\n        \n    if top_keyword in result:\n        return 1\n    else:\n        return 0\n\nfor top_keyword in keywords_df.sort_values(by=\"mean_movies_revenue\", ascending=False).head(10).index.values:\n    train_df[\"has_top_keyword_\"+ top_keyword] = train_df.Keywords.apply(lambda element: find_top_keywords_from_cast(top_keyword, element))\n    test_df[\"has_top_keyword_\"+ top_keyword] = test_df.Keywords.apply(lambda element: find_top_keywords_from_cast(top_keyword, element))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Cast\nNumber of cast"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_cast\"] = train_df[\"cast\"].str.count(\"name\")\ntest_df[\"num_of_cast\"] = test_df[\"cast\"].str.count(\"name\")\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_cast = train_df.num_of_cast.fillna(0)\ntest_df.num_of_cast = test_df.num_of_cast.fillna(0)\n\nsns.boxplot(x=train_df.num_of_cast, ax = ax[0])\nax[0].set_title(\"num_of_cast Boxplot\")\nsns.distplot(a=train_df.num_of_cast, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_cast Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_cast), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_cast Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_cast\"] = np.log1p(train_df.num_of_cast)\ntest_df[\"log_num_of_cast\"] = np.log1p(test_df.num_of_cast)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Male cast\nnumber of male cast"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_male_cast\"] = train_df[\"cast\"].str.count(\"'gender': 2\")\ntest_df[\"num_of_male_cast\"] = test_df[\"cast\"].str.count(\"'gender': 2\")\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_male_cast = train_df.num_of_male_cast.fillna(0)\ntest_df.num_of_male_cast = test_df.num_of_male_cast.fillna(0)\n\nsns.boxplot(x=train_df.num_of_male_cast, ax = ax[0])\nax[0].set_title(\"num_of_male_cast Boxplot\")\nsns.distplot(a=train_df.num_of_male_cast, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_male_cast Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_male_cast), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_male_cast Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_male_cast\"] = np.log1p(train_df.num_of_male_cast)\ntest_df[\"log_num_of_male_cast\"] = np.log1p(test_df.num_of_male_cast)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Female cast\nnumber of female cast"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_female_cast\"] = train_df[\"cast\"].str.count(\"'gender': 1\")\ntest_df[\"num_of_female_cast\"] = test_df[\"cast\"].str.count(\"'gender': 1\")\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_female_cast = train_df.num_of_female_cast.fillna(0)\ntest_df.num_of_female_cast = test_df.num_of_female_cast.fillna(0)\n\nsns.boxplot(x=train_df.num_of_female_cast, ax = ax[0])\nax[0].set_title(\"num_of_female_cast Boxplot\")\nsns.distplot(a=train_df.num_of_female_cast, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_female_cast Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_female_cast), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_female_cast Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_female_cast\"] = np.log1p(train_df.num_of_female_cast)\ntest_df[\"log_num_of_female_cast\"] = np.log1p(test_df.num_of_female_cast)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Crew\nCounting the number of crew"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_crew\"] = train_df[\"crew\"].str.count(\"'job\")\ntest_df[\"num_of_crew\"] = test_df[\"crew\"].str.count(\"'job\")\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_crew = train_df.num_of_crew.fillna(0)\ntest_df.num_of_crew = test_df.num_of_crew.fillna(0)\n\nsns.boxplot(x=train_df.num_of_crew, ax = ax[0])\nax[0].set_title(\"num_of_crew Boxplot\")\nsns.distplot(a=train_df.num_of_crew, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_crew Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_crew), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_crew Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_crew\"] = np.log1p(train_df.num_of_crew)\ntest_df[\"log_num_of_crew\"] = np.log1p(test_df.num_of_crew)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Male Crew\nCounting the number of male crew"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_male_crew\"] = train_df[\"crew\"].str.count(\"'gender': 2\")\ntest_df[\"num_of_male_crew\"] = test_df[\"crew\"].str.count(\"'gender': 2\")\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_male_crew = train_df.num_of_male_crew.fillna(0)\ntest_df.num_of_male_crew = test_df.num_of_crew.fillna(0)\n\nsns.boxplot(x=train_df.num_of_male_crew, ax = ax[0])\nax[0].set_title(\"num_of_male_crew Boxplot\")\nsns.distplot(a=train_df.num_of_male_crew, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_male_crew Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_male_crew), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_male_crew Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_male_crew\"] = np.log1p(train_df.num_of_male_crew)\ntest_df[\"log_num_of_male_crew\"] = np.log1p(test_df.num_of_male_crew)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Female Crew\nCounting the number of female crew"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_female_crew\"] = train_df[\"crew\"].str.count(\"'gender': 1\")\ntest_df[\"num_of_female_crew\"] = test_df[\"crew\"].str.count(\"'gender': 1\")\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_female_crew = train_df.num_of_female_crew.fillna(0)\ntest_df.num_of_female_crew = test_df.num_of_female_crew.fillna(0)\n\nsns.boxplot(x=train_df.num_of_female_crew, ax = ax[0])\nax[0].set_title(\"num_of_female_crew Boxplot\")\nsns.distplot(a=train_df.num_of_female_crew, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_female_crew Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_female_crew), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_female_crew Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_female_crew\"] = np.log1p(train_df.num_of_female_crew)\ntest_df[\"log_num_of_female_crew\"] = np.log1p(test_df.num_of_female_crew)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Identifying top directors based on average movie revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndirectors_dict = {}\nsize_of_crew = len(train_df) - train_df.crew.isna().sum()\n\nfor element in train_df[[\"revenue\", \"crew\"]].values:\n    if type(element[1]) == type(str()):\n        \n        result = re.findall('Director\\', \\'name\\': \\'\\w+\\s*\\w*', element[1])\n        result = [x.replace(\"Director\\', \\'name\\': \\'\", \"\") for x in result]\n\n        for key in result:\n            if key not in directors_dict:\n                directors_dict[key] = element[0]\n            else:\n                directors_dict[key] += element[0]\n                \nfor key in directors_dict:\n    directors_dict[key] = directors_dict[key]/size_of_crew\n    \ndirectors_df = pd.DataFrame.from_dict(directors_dict, orient='index', columns=[\"mean_movies_revenue\"])\ndirectors_df.sort_values(by=\"mean_movies_revenue\", ascending=False).head(10).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### feature engineering, finding has_top_director in movies"},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_top_directors_from_crew(top_director, element):\n    \n    result = []\n    if type(element) == type(str()):\n\n        result = re.findall('Director\\', \\'name\\': \\'\\w+\\s*\\w*', element)\n        result = [x.replace(\"Director\\', \\'name\\': \\'\", \"\") for x in result]\n        \n    if top_director in result:\n        return 1\n    else:\n        return 0\n\nfor top_director in directors_df.sort_values(by=\"mean_movies_revenue\", ascending=False).head(10).index.values:\n    train_df[\"has_top_director_\"+ top_director] = train_df.crew.apply(lambda element: find_top_directors_from_crew(top_director, element))\n    test_df[\"has_top_director_\"+ top_director] = test_df.crew.apply(lambda element: find_top_directors_from_crew(top_director, element))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Identifying top Producers based on average movie salary"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nproducers_dict = {}\nsize_of_crew = len(train_df) - train_df.crew.isna().sum()\n\nfor element in train_df[[\"revenue\", \"crew\"]].values:\n    if type(element[1]) == type(str()):\n        \n        result = re.findall('Producer\\', \\'name\\': \\'\\w+\\s*\\w*', element[1])\n        result = [x.replace(\"Producer\\', \\'name\\': \\'\", \"\") for x in result]\n\n        for key in result:\n            if key not in producers_dict:\n                producers_dict[key] = element[0]\n            else:\n                producers_dict[key] += element[0]\n                \nfor key in producers_dict:\n    producers_dict[key] = producers_dict[key]/size_of_crew\n    \nproducers_df = pd.DataFrame.from_dict(producers_dict, orient='index', columns=[\"mean_movies_revenue\"])\nproducers_df.sort_values(by=\"mean_movies_revenue\", ascending=False).head(10).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### feature engineering, finding has_top_producers in movies"},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_top_producers_from_crew(top_producer, element):\n    \n    result = []\n    if type(element) == type(str()):\n\n        result = re.findall('Director\\', \\'name\\': \\'\\w+\\s*\\w*', element)\n        result = [x.replace(\"Director\\', \\'name\\': \\'\", \"\") for x in result]\n        \n    if top_producer in result:\n        return 1\n    else:\n        return 0\n\nfor top_producer in producers_df.sort_values(by=\"mean_movies_revenue\", ascending=False).head(10).index.values:\n    train_df[\"has_top_producer_\"+ top_producer] = train_df.crew.apply(lambda element: find_top_producers_from_crew(top_producer, element))\n    test_df[\"has_top_producer_\"+ top_producer] = test_df.crew.apply(lambda element: find_top_producers_from_crew(top_producer, element))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Directors in a movie"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_directors\"] = train_df[\"crew\"].str.count(\"Directing\")\ntest_df[\"num_of_directors\"] = test_df[\"crew\"].str.count(\"Directing\")\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_directors = train_df.num_of_directors.fillna(0)\ntest_df.num_of_directors = test_df.num_of_directors.fillna(0)\n\nsns.boxplot(x=train_df.num_of_directors, ax = ax[0])\nax[0].set_title(\"num_of_directors Boxplot\")\nsns.distplot(a=train_df.num_of_directors, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_directors Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_directors), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_directors Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_directors\"] = np.log1p(train_df.num_of_directors)\ntest_df[\"log_num_of_directors\"] = np.log1p(test_df.num_of_directors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Producers"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_producers\"] = train_df[\"crew\"].str.count(\"Production\")\ntest_df[\"num_of_producers\"] = test_df[\"crew\"].str.count(\"Production\")\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_producers = train_df.num_of_producers.fillna(0)\ntest_df.num_of_producers = test_df.num_of_producers.fillna(0)\n\nsns.boxplot(x=train_df.num_of_producers, ax = ax[0])\nax[0].set_title(\"num_of_producers Boxplot\")\nsns.distplot(a=train_df.num_of_producers, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_producers Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_producers), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_producers Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_producers\"] = np.log1p(train_df.num_of_producers)\ntest_df[\"log_num_of_producers\"] = np.log1p(test_df.num_of_producers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Writers"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_writers\"] = train_df[\"crew\"].str.count(\"Writing\")\ntest_df[\"num_of_writers\"] = test_df[\"crew\"].str.count(\"Writing\")\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_writers = train_df.num_of_writers.fillna(0)\ntest_df.num_of_writers = test_df.num_of_writers.fillna(0)\n\nsns.boxplot(x=train_df.num_of_writers, ax = ax[0])\nax[0].set_title(\"num_of_writers Boxplot\")\nsns.distplot(a=train_df.num_of_writers, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_writers Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_writers), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_writers Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_writers\"] = np.log1p(train_df.num_of_writers)\ntest_df[\"log_num_of_writers\"] = np.log1p(test_df.num_of_writers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Editors"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_editors\"] = train_df[\"crew\"].str.count(\"Editing\")\ntest_df[\"num_of_editors\"] = test_df[\"crew\"].str.count(\"Editing\")\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_editors = train_df.num_of_editors.fillna(0)\ntest_df.num_of_editors = test_df.num_of_editors.fillna(0)\n\nsns.boxplot(x=train_df.num_of_editors, ax = ax[0])\nax[0].set_title(\"num_of_editors Boxplot\")\nsns.distplot(a=train_df.num_of_editors, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_editors Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_editors), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_editors Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_editors\"] = np.log1p(train_df.num_of_editors)\ntest_df[\"log_num_of_editors\"] = np.log1p(test_df.num_of_editors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Art crew"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_art_crew\"] = train_df[\"crew\"].str.count(\"Art\")\ntest_df[\"num_of_art_crew\"] = test_df[\"crew\"].str.count(\"Art\")\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_art_crew = train_df.num_of_art_crew.fillna(0)\ntest_df.num_of_art_crew = test_df.num_of_art_crew.fillna(0)\n\nsns.boxplot(x=train_df.num_of_art_crew, ax = ax[0])\nax[0].set_title(\"num_of_art_crew Boxplot\")\nsns.distplot(a=train_df.num_of_art_crew, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_art_crew Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_art_crew), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_art_crew Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_art_crew\"] = np.log1p(train_df.num_of_art_crew)\ntest_df[\"log_num_of_art_crew\"] = np.log1p(test_df.num_of_art_crew)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Sound crew"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_sound_crew\"] = train_df[\"crew\"].str.count(\"Sound\")\ntest_df[\"num_of_sound_crew\"] = test_df[\"crew\"].str.count(\"Sound\")\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_sound_crew = train_df.num_of_sound_crew.fillna(0)\ntest_df.num_of_sound_crew = test_df.num_of_sound_crew.fillna(0)\n\nsns.boxplot(x=train_df.num_of_sound_crew, ax = ax[0])\nax[0].set_title(\"num_of_sound_crew Boxplot\")\nsns.distplot(a=train_df.num_of_sound_crew, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_sound_crew Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_sound_crew), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_sound_crew Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_sound_crew\"] = np.log1p(train_df.num_of_sound_crew)\ntest_df[\"log_num_of_sound_crew\"] = np.log1p(test_df.num_of_sound_crew)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Costume and Make-Up crew"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_costume_crew\"] = train_df[\"crew\"].str.count(\"Costume & Make-Up\")\ntest_df[\"num_of_costume_crew\"] = test_df[\"crew\"].str.count(\"Costume & Make-Up\")\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_costume_crew = train_df.num_of_costume_crew.fillna(0)\ntest_df.num_of_costume_crew = test_df.num_of_costume_crew.fillna(0)\n\nsns.boxplot(x=train_df.num_of_costume_crew, ax = ax[0])\nax[0].set_title(\"num_of_costume_crew Boxplot\")\nsns.distplot(a=train_df.num_of_costume_crew, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_costume_crew Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_costume_crew), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_costume_crew Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_costume_crew\"] = np.log1p(train_df.num_of_costume_crew)\ntest_df[\"log_num_of_costume_crew\"] = np.log1p(test_df.num_of_costume_crew)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Camera crew"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_camera_crew\"] = train_df[\"crew\"].str.count(\"\\'department\\': \\'Camera\\'\")\ntest_df[\"num_of_camera_crew\"] = test_df[\"crew\"].str.count(\"\\'department\\': \\'Camera\\'\")\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_camera_crew = train_df.num_of_camera_crew.fillna(0)\ntest_df.num_of_camera_crew = test_df.num_of_camera_crew.fillna(0)\n\nsns.boxplot(x=train_df.num_of_camera_crew, ax = ax[0])\nax[0].set_title(\"num_of_camera_crew Boxplot\")\nsns.distplot(a=train_df.num_of_camera_crew, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_camera_crew Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_camera_crew), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_camera_crew Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_camera_crew\"] = np.log1p(train_df.num_of_camera_crew)\ntest_df[\"log_num_of_camera_crew\"] = np.log1p(test_df.num_of_camera_crew)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Visual Effects Crew"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_visual_effects_crew\"] = train_df[\"crew\"].str.count(\"\\'department\\': \\'Visual Effects\\'\")\ntest_df[\"num_of_visual_effects_crew\"] = test_df[\"crew\"].str.count(\"\\'department\\': \\'Visual Effects\\'\")\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_visual_effects_crew = train_df.num_of_visual_effects_crew.fillna(0)\ntest_df.num_of_visual_effects_crew = test_df.num_of_visual_effects_crew.fillna(0)\n\nsns.boxplot(x=train_df.num_of_visual_effects_crew, ax = ax[0])\nax[0].set_title(\"num_of_visual_effects_crew Boxplot\")\nsns.distplot(a=train_df.num_of_visual_effects_crew, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_visual_effects_crew Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_visual_effects_crew), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_visual_effects_crew Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_visual_effects_crew\"] = np.log1p(train_df.num_of_visual_effects_crew)\ntest_df[\"log_num_of_visual_effects_crew\"] = np.log1p(test_df.num_of_visual_effects_crew)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Lighting crew"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_lighting_crew\"] = train_df[\"crew\"].str.count(\"\\'department\\': \\'Lighting\\'\")\ntest_df[\"num_of_lighting_crew\"] = test_df[\"crew\"].str.count(\"\\'department\\': \\'Lighting\\'\")\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_lighting_crew = train_df.num_of_lighting_crew.fillna(0)\ntest_df.num_of_lighting_crew = test_df.num_of_lighting_crew.fillna(0)\n\nsns.boxplot(x=train_df.num_of_lighting_crew, ax = ax[0])\nax[0].set_title(\"num_of_lighting_crew Boxplot\")\nsns.distplot(a=train_df.num_of_lighting_crew, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_lighting_crew Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_lighting_crew), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_lighting_crew Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_lighting_crew\"] = np.log1p(train_df.num_of_lighting_crew)\ntest_df[\"log_num_of_lighting_crew\"] = np.log1p(test_df.num_of_lighting_crew)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Other crew"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_other_crew\"] = train_df[\"crew\"].str.count(\"\\'department\\': \\'Crew\\'\")\ntest_df[\"num_of_other_crew\"] = test_df[\"crew\"].str.count(\"\\'department\\': \\'Crew\\'\")\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_other_crew = train_df.num_of_other_crew.fillna(0)\ntest_df.num_of_other_crew = test_df.num_of_other_crew.fillna(0)\n\nsns.boxplot(x=train_df.num_of_other_crew, ax = ax[0])\nax[0].set_title(\"num_of_other_crew Boxplot\")\nsns.distplot(a=train_df.num_of_other_crew, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_other_crew Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_other_crew), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_other_crew Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_other_crew\"] = np.log1p(train_df.num_of_other_crew)\ntest_df[\"log_num_of_other_crew\"] = np.log1p(test_df.num_of_other_crew)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Production Countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_production_countries\"] = train_df.production_countries_processed.apply(len)\ntest_df[\"num_of_production_countries\"] = test_df.production_countries_processed.apply(len)\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_production_countries = train_df.num_of_production_countries.fillna(0)\ntest_df.num_of_production_countries = test_df.num_of_production_countries.fillna(0)\n\nsns.boxplot(x=train_df.num_of_production_countries, ax = ax[0])\nax[0].set_title(\"num_of_production_countries Boxplot\")\nsns.distplot(a=train_df.num_of_production_countries, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_production_countries Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_production_countries), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_production_countries Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_production_countries\"] = np.log1p(train_df.num_of_production_countries)\ntest_df[\"log_num_of_production_countries\"] = np.log1p(test_df.num_of_production_countries)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Genres in a movie"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_genres\"] = train_df.genres_processed.apply(len)\ntest_df[\"num_of_genres\"] = test_df.genres_processed.apply(len)\n\nf, ax = plt.subplots(3, figsize=(12,7))\n\ntrain_df.num_of_genres = train_df.num_of_genres.fillna(0)\ntest_df.num_of_genres = test_df.num_of_genres.fillna(0)\n\nsns.boxplot(x=train_df.num_of_genres, ax = ax[0])\nax[0].set_title(\"num_of_genres Boxplot\")\nsns.distplot(a=train_df.num_of_genres, kde = False, ax = ax[1])\nax[1].set_title(\"num_of_genres Histogram\")\nsns.distplot(a=np.log1p(train_df.num_of_genres), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed num_of_genres Histogram\")\nf.tight_layout()\n\n\ntrain_df[\"log_num_of_genres\"] = np.log1p(train_df.num_of_genres)\ntest_df[\"log_num_of_genres\"] = np.log1p(test_df.num_of_genres)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA - Bivariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Bivariate Analysis for numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(20,27)})\n\n# Compute the correlation matrix\ncorr = train_df[[\"revenue\", \"budget\", \"popularity\", \"runtime\", \"num_of_cast\", \"num_of_male_cast\",\n                 \"num_of_female_cast\",\n                 \"num_genres\", \"num_of_production_countries\", \"day_of_week\", \"month\", \"year\", \"week_of_year\", \"season\",\n                 \"title_len\", \"overview_len\", \"tagline_len\",\n                 \"num_of_directors\", \"num_of_producers\", \"num_of_editors\", \"num_of_art_crew\", \"num_of_sound_crew\",\n                 \"num_of_costume_crew\", \"num_of_camera_crew\", \"num_of_visual_effects_crew\", \"num_of_lighting_crew\",\n                 \"num_of_other_crew\"]].corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, mask=mask, \n            annot=True, \n            fmt=\".2f\", \n            cmap='coolwarm')\n\nplt.title(\"Correlation between numerical features\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bivariate Analysis for log-transformed numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(18,20)})\n\n# Compute the correlation matrix\ncorr = train_df[[\"log_revenue\", \"log_budget\", \"log_popularity\", \"log_runtime\",\n                 \"log_num_of_cast\", \"log_num_of_male_cast\",\n                 \"log_num_of_female_cast\", \"num_genres\", \"num_of_production_countries\",\n                \"day_of_week\", \"month\", \"year\", \"week_of_year\", \"season\",\n                \"log_title_len\", \"log_overview_len\", \"log_tagline_len\",\n                \"log_num_of_directors\", \"log_num_of_producers\", \"log_num_of_editors\", \"log_num_of_art_crew\", \"log_num_of_sound_crew\",\n                       \"log_num_of_costume_crew\", \"log_num_of_camera_crew\", \"log_num_of_visual_effects_crew\", \"log_num_of_lighting_crew\",\n                        \"log_num_of_other_crew\"]].corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, mask=mask, \n            annot=True, \n            fmt=\".2f\", \n            cmap='coolwarm')\n\nplt.title(\"Correlation between log1p transformed numerical features\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bivariate Analysis and Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"#### has_collection and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['has_collection'] = [0 if pd.isnull(x) else 1 for x in train_df['belongs_to_collection']]\ntest_df['has_collection'] = [0 if pd.isnull(x) else 1 for x in test_df['belongs_to_collection']]\nprint(train_df['has_collection'].value_counts())\n\nsns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='has_collection', y='revenue', data=train_df)\nplt.title('Revenue for film with and without being in a collection')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### homepage and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['has_homepage'] = [0 if pd.isnull(x) else 1 for x in train_df['homepage']]\ntest_df['has_homepage'] = [0 if pd.isnull(x) else 1 for x in test_df['homepage']]\nprint(train_df['has_homepage'].value_counts())\n\nsns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='has_homepage', y='revenue', data=train_df)\nplt.title('Revenue for film with and without homepage')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### tagline and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['has_tag'] = [0 if len(x) == 0 else 1 for x in train_df['tagline']]\ntest_df['has_tag'] = [0 if len(x) == 0 else 1 for x in test_df['tagline']]\nprint(train_df['has_tag'].value_counts())\n\nsns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='has_tag', y='revenue', data=train_df)\nplt.title('Revenue for film with and without tagline')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Genres per movie and revenues"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='num_of_genres', y='revenue', data=train_df)\nplt.title('Revenues for films with multiple genres')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bivariate Analysis between each genre and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, g in enumerate(genres_df.index.values):\n    genres_df.loc[g, \"median_salary\"] = train_df[train_df['isGenre_' + g]==1].revenue.median()\n\ngenres_df.sort_values(by=[\"number_of_movies\", \"median_salary\"], ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genres_df.sort_values(by=[\"median_salary\"], ascending=False).median_salary.plot.bar()\nplt.title(\"Sorted movie genres by median revenue\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, p in enumerate(genres_df.sort_values(by=[\"number_of_movies\", \"median_salary\"], ascending=False).head(10).index.values):\n    train_df['isTopGenre_' + p] = train_df['genres_processed'].apply(lambda x: 1 if p in x else 0)\n    train_df['isTopGenre_Other'] = train_df['genres_processed'].apply(lambda x: 1 if p not in x else 0)\n    test_df['isTopGenre_' + p] = test_df['genres_processed'].apply(lambda x: 1 if p in x else 0)\n    test_df['isTopGenre_Other'] = test_df['genres_processed'].apply(lambda x: 1 if p not in x else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(len(genres_df.index.values), 1, figsize=(15,160))\n\nfor i, g in enumerate(genres_df.index.values):\n    sns.boxplot(x=train_df['isGenre_' + g], y='revenue', ax=ax[i], data=train_df)\n    ax[i].set_title('isGenre_' + g +\" and revenue boxplot\")\nf.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### original_language and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='original_language', y='revenue', data=train_df)\nplt.title('Revenue for a movie and its and original_language')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### English and non english movies vs revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='is_english_language', y='revenue', data=train_df)\nplt.title('Revenue for a movie in contrast with english and non-english language')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### production country and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='num_of_production_countries', y='revenue', data=train_df)\nplt.title('number of production countries for a movie and revenue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(len(production_countries_df.index.values), 1, figsize=(15,350))\n\nfor i, c in enumerate(production_countries_df.index.values):\n    sns.boxplot(x=train_df['isProductionCountry_' + c], y='revenue', ax=ax[i], data=train_df)\n    ax[i].set_title('isProductionCountry_' + c +\" and revenue boxplot\")\nf.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, c in enumerate(production_countries_df.index.values):\n    production_countries_df.loc[c, \"median_salary\"] = train_df[train_df['isProductionCountry_' + c]==1].revenue.median()\n\nproduction_countries_df.sort_values(by=[\"number_of_movies\", \"median_salary\"], ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, p in enumerate(production_countries_df.sort_values(by=[\"number_of_movies\", \"median_salary\"], ascending=False).head(10).index.values):\n    train_df['isTopProductionCountry_' + p] = train_df['production_countries_processed'].apply(lambda x: 1 if p in x else 0)\n    test_df['isTopProductionCountry_' + p] = test_df['production_countries_processed'].apply(lambda x: 1 if p in x else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Day of the week when the movie released and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='day_of_week', y='revenue', data=train_df)\nplt.title('day_of_week when the movie release and revenue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Week of year when the movie released and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='week_of_year', y='revenue', data=train_df)\nplt.title('day_of_week when the movie release and revenue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Month when the movie released and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='month', y='revenue', data=train_df)\nplt.title('month when the movie release and revenue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Season when the movie released and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='season', y='revenue', data=train_df)\nplt.title('season when the movie release and revenue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Year when the movie released and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(20, 8)})\ng = sns.boxplot(x='year', y='revenue', data=train_df)\nplt.xticks(rotation=90)\nplt.title('Year when the movie release and revenue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Production Companies and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='num_of_production_companies', y='revenue', data=train_df)\nplt.title('number of production companies for a movie and revenue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(len(production_companies_df.head(5).index.values), 1, figsize=(15,20))\n\nfor i, p in enumerate(production_companies_df.head(5).index.values):\n    sns.boxplot(x=train_df['isProductionCompany_' + p], y='revenue', ax=ax[i], data=train_df)\n    ax[i].set_title('isProductionCompany_' + p +\" and revenue boxplot\")\nf.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, p in enumerate(production_companies_df.index.values):\n    production_companies_df.loc[p, \"median_salary\"] = train_df[train_df['isProductionCompany_' + p]==1].revenue.median()\n\nproduction_companies_df.sort_values(by=[\"number_of_movies\", \"median_salary\"], ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, p in enumerate(production_companies_df.sort_values(by=[\"number_of_movies\", \"median_salary\"], ascending=False).head(10).index.values):\n    train_df['isTopProductionCompany_' + p] = train_df['production_companies_processed'].apply(lambda x: 1 if p in x else 0)\n    #train_df['isTopProductionCompany_Other'] = train_df['production_companies_processed'].apply(lambda x: 1 if p not in x else 0)\n    test_df['isTopProductionCompany_' + p] = test_df['production_companies_processed'].apply(lambda x: 1 if p in x else 0)\n    #test_df['isTopProductionCompany_Other'] = test_df['production_companies_processed'].apply(lambda x: 1 if p not in x else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"![](https://cmci.colorado.edu/classes/INFO-4604/fa17/wordcloud.png)\n[image-source](https://cmci.colorado.edu/classes/INFO-4604/fa17/wordcloud.png)"},{"metadata":{},"cell_type":"markdown","source":"#### Preparations before ML modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_for_training = [\"log_budget\", \"log_popularity\", \"log_runtime\", \"day_of_week\", \"year\", \"month\", \"week_of_year\", \"season\",\n                        \"num_genres\", \"num_of_production_countries\", \"log_num_of_cast\", \"log_num_of_male_cast\", \"log_num_of_female_cast\", \"has_collection\", \n                        \"has_homepage\", \"has_tag\", \"is_english_language\",\n                       \"log_num_of_crew\", \"log_num_of_male_crew\", \"log_num_of_female_crew\",\n                       \"log_title_len\", \"log_overview_len\", \"log_tagline_len\",\n                       \"log_num_of_directors\", \"log_num_of_producers\", \"log_num_of_editors\", \"log_num_of_art_crew\", \"log_num_of_sound_crew\",\n                       \"log_num_of_costume_crew\", \"log_num_of_camera_crew\", \"log_num_of_visual_effects_crew\", \"log_num_of_lighting_crew\",\n                        \"log_num_of_other_crew\"]\n\n\n# adding isTopGenre_ columns for features before ML modeling\ncolumns_for_training.extend(train_df.select(lambda col: col.startswith('isTopGenre_'), axis=1).columns.values)\n\n# adding isTopProductionCompany_ columns for features before ML modeling\ncolumns_for_training.extend(train_df.select(lambda col: col.startswith('isTopProductionCompany_'), axis=1).columns.values)\n\n# adding isTopProductionCountry_ columns for features before ML modeling\ncolumns_for_training.extend(train_df.select(lambda col: col.startswith('isTopProductionCountry_'), axis=1).columns.values)\n\n# adding has_top_actor_ columns for features before ML modeling\ncolumns_for_training.extend(train_df.select(lambda col: col.startswith('has_top_actor_'), axis=1).columns.values)\n\n# adding has_top_keyword_ columns for features before ML modeling\ncolumns_for_training.extend(train_df.select(lambda col: col.startswith('has_top_keyword_'), axis=1).columns.values)\n\n# adding has_top_director_ columns for features before ML modeling\ncolumns_for_training.extend(train_df.select(lambda col: col.startswith('has_top_director_'), axis=1).columns.values)\n\n# adding has_top_producer_ columns for features before ML modeling\ncolumns_for_training.extend(train_df.select(lambda col: col.startswith('has_top_producer_'), axis=1).columns.values)\n\ncolumns_for_training","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[columns_for_training].head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking for NA values in feature before training"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[columns_for_training].isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(columns_for_training)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### a small snippet code for t-test significance between 2 groups, I may use it in the future:\n\n\n#from scipy import stats\n\n#columns_to_test = train_df.select(lambda col: col.startswith('isProductionCompany_'), axis=1).columns.values\n\n#def check_catagorical_to_revenue_statistical_difference(train_df):\n    \n#    for col in columns_to_test:\n\n#        a = train_df[train_df[col]==0].revenue\n#        b = train_df[train_df[col]==1].revenue\n#        t2, p2 = stats.ttest_ind(a,b)\n#        if p2<0.05:\n#            print(col , \" is important for prediction with p-value:\", p2)\n        \n#check_catagorical_to_revenue_statistical_difference(train_df)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df['log_revenue']\nX = train_df[columns_for_training]\nkfold_splits = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Baseline XGBoost modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\nimport scikitplot as skplt\nimport time\nimport random\n\nimport xgboost as xgb\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\n\npredictions_test_xgb = np.zeros(len(test_df))\nnum_fold = 0\nnum_of_splits = kfold_splits\noof_rmse = 0\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    print()\n\n    clf_stra_xgb = xgb.XGBRegressor(n_estimators=10000, seed=42, nthread=-1)\n\n    clf_stra_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=1000, eval_metric='rmse', verbose=100)\n\n    predictions_valid = clf_stra_xgb.predict(xvalid)\n    rmse_valid = np.sqrt(mean_squared_error(yvalid, predictions_valid))\n    \n    print(\"Fold\",num_fold,\"xvalid rmse:\",rmse_valid)\n    num_fold = num_fold + 1\n    \n    oof_rmse += rmse_valid\n\n    predictions_test_xgb += clf_stra_xgb.predict(test_df[xtrain.columns])/num_of_splits\n\n\npredictions_test_xgb = np.expm1(predictions_test_xgb)\nprint()\nprint(predictions_test_xgb)\nprint(\"OOF Out-of-fold rmse:\", oof_rmse/num_of_splits)\n\nf, ax = plt.subplots(2, figsize=(12,7))\n\n\nsns.set(rc={'figure.figsize':(9,86)})\nsns.distplot(train_df.revenue, ax=ax[0])\nax[0].set_title(\"Train Set Revenue Histogram\")\nsns.distplot(predictions_test_xgb, ax=ax[1])\nax[1].set_title(\"Test Set Revenue Prediction Histogram\")\nf.tight_layout()\n\nxgb.plot_importance(clf_stra_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bayesian Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bayesian_tuning(xtrain, ytrain):\n    \n    from skopt import BayesSearchCV\n    import xgboost as xgb\n    \n    \n    # Classifier\n    bayes_cv_tuner = BayesSearchCV(\n        estimator = xgb.XGBRegressor(\n            nthread = -1,\n            objective = 'reg:linear',\n            verbosity=1,\n            random_state=42\n        ),\n        search_spaces = {\n            'learning_rate': (0.01, 1.0, 'log-uniform'),\n            'min_child_weight': (0, 10),\n            'n_estimators': (50, 300),\n            'max_depth': (2, 12),\n            'gamma': (1e-3, 1, 'log-uniform'),\n            'subsample': (0.01, 1.0, 'uniform'),\n            'colsample_bytree': (0.01, 1.0, 'uniform'),\n            'colsample_bylevel': (0.01, 1.0, 'uniform'),\n            'reg_lambda': (1e-1, 10, 'log-uniform'),\n            'reg_alpha': (1e-2, 1.0, 'log-uniform')\n        },\n        cv = KFold(\n            n_splits=kfold_splits,\n            shuffle=True,\n            random_state=42\n        ),\n        scoring = 'neg_mean_squared_error',\n        n_jobs = 2,\n        n_iter = 12,\n        verbose=0,\n        refit = True,\n        random_state = 42\n    )\n\n    def status_print(optim_result):\n        \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n\n        # Get all the models tested so far in DataFrame format\n        all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n\n        # Get current parameters and the best parameters    \n        best_params = pd.Series(bayes_cv_tuner.best_params_)\n        print('Model #{}\\nBest score: {}\\nBest params: {}\\n'.format(\n            len(all_models),\n            np.round(bayes_cv_tuner.best_score_, 4),\n            bayes_cv_tuner.best_params_\n        ))\n        \n    result = bayes_cv_tuner.fit(xtrain, ytrain, callback = status_print)\n    return result\n    \n# Fit the model\n#xtrain, ytrain = prepare_for_tuning(X, y, type_of_training=type_of_training)\nresult = bayesian_tuning(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost Training after tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport xgboost as xgb\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\npredictions_test_xgb_tuned = np.zeros(len(test_df))\nnum_fold = 0\noof_rmse = 0\nnum_of_splits = kfold_splits\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    print()\n    \n    clf_stra_tuned_xgb = xgb.XGBRegressor(colsample_bytree = result.best_params_[\"colsample_bytree\"],\n                                          colsample_bylevel = result.best_params_[\"colsample_bylevel\"],\n                                    gamma=result.best_params_[\"gamma\"],                 \n                                    learning_rate=result.best_params_[\"learning_rate\"],\n                                    max_depth=result.best_params_[\"max_depth\"],\n                                    min_child_weight=result.best_params_[\"min_child_weight\"],\n                                    n_estimators=10000,\n                                    reg_alpha=result.best_params_[\"reg_alpha\"],\n                                    reg_lambda=result.best_params_[\"reg_lambda\"],\n                                    subsample=result.best_params_[\"subsample\"],\n                                    seed=42,\n                                    nthread = -1)\n\n    clf_stra_tuned_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=1000, eval_metric='rmse', verbose=100)\n\n    predictions_valid = clf_stra_tuned_xgb.predict(xvalid)\n    rmse_valid = np.sqrt(mean_squared_error(yvalid, predictions_valid))\n    print(\"Fold\",num_fold,\"xvalid rmse:\",rmse_valid)\n    num_fold = num_fold + 1\n    \n    oof_rmse += rmse_valid\n\n    predictions_test_xgb_tuned += clf_stra_tuned_xgb.predict(test_df[xtrain.columns])/num_of_splits\n    \nprint()\npredictions_test_xgb_tuned = np.expm1(predictions_test_xgb_tuned)\nprint(predictions_test_xgb_tuned)\nprint(\"OOF Out-of-fold rmse:\", oof_rmse/num_of_splits)\n\nf, ax = plt.subplots(2, figsize=(12,7))\n\n\nsns.set(rc={'figure.figsize':(9,86)})\nsns.distplot(train_df.revenue, ax=ax[0])\nax[0].set_title(\"Train Set Revenue Histogram\")\nsns.distplot(predictions_test_xgb_tuned, ax=ax[1])\nax[1].set_title(\"Test Set Revenue Prediction Histogram\")\nf.tight_layout()\n\nxgb.plot_importance(clf_stra_tuned_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extra Trees Baseline Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\nimport scikitplot as skplt\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\npredictions_extra_trees_test = np.zeros(len(test_df))\nnum_fold = 0\nnum_of_splits = kfold_splits\noof_rmse = 0\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    print()\n\n    clf_extra_trees = ExtraTreesRegressor(n_estimators=100, random_state=42)\n\n    clf_extra_trees.fit(xtrain_stra, ytrain_stra)\n\n    predictions_valid = clf_extra_trees.predict(xvalid)\n    rmse_valid = np.sqrt(mean_squared_error(yvalid, predictions_valid))\n    print(\"Fold\" ,num_fold, \"xvalid rmse:\", rmse_valid)\n    num_fold = num_fold + 1\n    oof_rmse += rmse_valid\n\n    predictions_extra_trees_test += clf_extra_trees.predict(test_df[xtrain.columns])/num_of_splits\n\n\npredictions_extra_trees_test = np.expm1(predictions_extra_trees_test)\nprint()\nprint(predictions_extra_trees_test)\nprint()\nprint(\"OOF Out-of-fold rmse:\", oof_rmse/num_of_splits)\n\nf, ax = plt.subplots(2, figsize=(12,7))\n\n\nsns.set(rc={'figure.figsize':(9,14)})\nsns.distplot(train_df.revenue, ax=ax[0])\nax[0].set_title(\"Train Set Revenue Histogram\")\nsns.distplot(predictions_extra_trees_test, ax=ax[1])\nax[1].set_title(\"Test Set Revenue Prediction Histogram\")\nf.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extra Trees Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bayesian_tuning_extra_trees(xtrain, ytrain):\n    \n    from skopt import BayesSearchCV\n    from sklearn.ensemble import ExtraTreesRegressor\n    \n    \n    # Classifier\n    bayes_cv_tuner = BayesSearchCV(\n        estimator = ExtraTreesRegressor(\n            random_state=42\n        ),\n        search_spaces = {\n            'n_estimators': (10, 500),\n            'max_depth': (1, 12),\n            'min_samples_split': (2, 20),\n            'min_samples_leaf': (1, 20)\n        },\n        cv = KFold(\n            n_splits=kfold_splits,\n            shuffle=True,\n            random_state=42\n        ),\n        scoring = 'neg_mean_squared_error',\n        n_jobs = 2,\n        n_iter = 12,   \n        verbose = 0,\n        refit = True,\n        random_state = 42\n    )\n\n    def status_print(optim_result):\n        \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n\n        # Get all the models tested so far in DataFrame format\n        all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n\n        # Get current parameters and the best parameters    \n        best_params = pd.Series(bayes_cv_tuner.best_params_)\n        print('Model #{}\\nBest score: {}\\nBest params: {}\\n'.format(\n            len(all_models),\n            np.round(bayes_cv_tuner.best_score_, 4),\n            bayes_cv_tuner.best_params_\n        ))\n        \n    result_extra_trees = bayes_cv_tuner.fit(xtrain, ytrain, callback = status_print)\n    return result_extra_trees\n    \n# Fit the model\n#xtrain, ytrain = prepare_for_tuning(X, y, type_of_training=type_of_training)\nresult_extra_trees = bayesian_tuning_extra_trees(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_extra_trees.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extra Trees Training after tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\nimport scikitplot as skplt\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\npredictions_extra_trees_tuned_test = np.zeros(len(test_df))\nnum_fold = 0\nnum_of_splits = kfold_splits\noof_rmse = 0\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    print()\n\n    clf_extra_trees_tuned = ExtraTreesRegressor(random_state=42, \n                                                max_depth = result_extra_trees.best_params_['max_depth'], \n                                                min_samples_leaf = result_extra_trees.best_params_['min_samples_leaf'], \n                                                min_samples_split = result_extra_trees.best_params_['min_samples_split'], \n                                                n_estimators = result_extra_trees.best_params_['n_estimators'])\n\n    clf_extra_trees_tuned.fit(xtrain_stra, ytrain_stra)\n\n    predictions_valid = clf_extra_trees_tuned.predict(xvalid)\n    rmse_valid = np.sqrt(mean_squared_error(yvalid, predictions_valid))\n    print(\"Fold\" ,num_fold, \"xvalid rmse:\", rmse_valid)\n    num_fold = num_fold + 1\n    oof_rmse += rmse_valid\n\n    predictions_extra_trees_tuned_test += clf_extra_trees_tuned.predict(test_df[xtrain.columns])/num_of_splits\n\n\npredictions_extra_trees_tuned_test = np.expm1(predictions_extra_trees_tuned_test)\nprint()\nprint(predictions_extra_trees_tuned_test)\nprint()\nprint(\"OOF Out-of-fold rmse:\", oof_rmse/num_of_splits)\n\nf, ax = plt.subplots(2, figsize=(12,7))\n\n\nsns.set(rc={'figure.figsize':(9,14)})\nsns.distplot(train_df.revenue, ax=ax[0])\nax[0].set_title(\"Train Set Revenue Histogram\")\nsns.distplot(predictions_extra_trees_tuned_test, ax=ax[1])\nax[1].set_title(\"Test Set Revenue Prediction Histogram\")\nf.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\nimport scikitplot as skplt\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\npredictions_random_forest_test = np.zeros(len(test_df))\nnum_fold = 0\nnum_of_splits = kfold_splits\noof_rmse = 0\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    print()\n    \n    clf_random_forest = RandomForestRegressor(random_state=42, n_estimators = 100)\n\n    clf_random_forest.fit(xtrain_stra, ytrain_stra)\n\n    predictions_valid = clf_random_forest.predict(xvalid)\n    rmse_valid = np.sqrt(mean_squared_error(yvalid, predictions_valid))\n    print(\"Fold\" ,num_fold, \"xvalid rmse:\", rmse_valid)\n    num_fold = num_fold + 1\n    oof_rmse += rmse_valid\n\n    predictions_random_forest_test += clf_random_forest.predict(test_df[xtrain.columns])/num_of_splits\n\n\npredictions_random_forest_test = np.expm1(predictions_random_forest_test)\nprint()\nprint(predictions_random_forest_test)\nprint()\nprint(\"OOF Out-of-fold rmse:\", oof_rmse/num_of_splits)\n\nf, ax = plt.subplots(2, figsize=(12,7))\n\n\nsns.set(rc={'figure.figsize':(9,14)})\nsns.distplot(train_df.revenue, ax=ax[0])\nax[0].set_title(\"Train Set Revenue Histogram\")\nsns.distplot(predictions_random_forest_test, ax=ax[1])\nax[1].set_title(\"Test Set Revenue Prediction Histogram\")\nf.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bayesian_tuning_random_forest(xtrain, ytrain):\n    \n    from skopt import BayesSearchCV\n    from sklearn.ensemble import RandomForestRegressor\n    \n    \n    # Classifier\n    bayes_cv_tuner = BayesSearchCV(\n        estimator = RandomForestRegressor(\n            random_state=42\n        ),\n        search_spaces = {\n            'n_estimators': (10, 500),\n            'max_depth': (1, 10),\n            'min_samples_split': (2, 20),\n            'min_samples_leaf': (1, 20)\n        },\n        cv = KFold(\n            n_splits=kfold_splits,\n            shuffle=True,\n            random_state=42\n        ),\n        scoring = 'neg_mean_squared_error',\n        n_jobs = 2,\n        n_iter = 12,   \n        verbose = 0,\n        refit = True,\n        random_state = 42\n    )\n\n    def status_print(optim_result):\n        \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n\n        # Get all the models tested so far in DataFrame format\n        all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n\n        # Get current parameters and the best parameters    \n        best_params = pd.Series(bayes_cv_tuner.best_params_)\n        print('Model #{}\\nBest score: {}\\nBest params: {}\\n'.format(\n            len(all_models),\n            np.round(bayes_cv_tuner.best_score_, 4),\n            bayes_cv_tuner.best_params_\n        ))\n        \n    result_random_forest = bayes_cv_tuner.fit(xtrain, ytrain, callback = status_print)\n    return result_random_forest\n    \n# Fit the model\n#xtrain, ytrain = prepare_for_tuning(X, y, type_of_training=type_of_training)\nresult_random_forest = bayesian_tuning_random_forest(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_random_forest.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest After tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\nimport scikitplot as skplt\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\npredictions_random_forest_tuned_test = np.zeros(len(test_df))\nnum_fold = 0\nnum_of_splits = kfold_splits\noof_rmse = 0\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    print()\n    \n    clf_random_forest_tuned = RandomForestRegressor(random_state=42, \n                                              n_estimators = result_random_forest.best_params_['n_estimators'],\n                                              min_samples_leaf = result_random_forest.best_params_['min_samples_leaf'],\n                                              min_samples_split = result_random_forest.best_params_['min_samples_split'])\n\n    clf_random_forest_tuned.fit(xtrain_stra, ytrain_stra)\n\n    predictions_valid = clf_random_forest_tuned.predict(xvalid)\n    rmse_valid = np.sqrt(mean_squared_error(yvalid, predictions_valid))\n    print(\"Fold xvalid rmse:\", rmse_valid)\n    oof_rmse += rmse_valid\n\n    predictions_random_forest_tuned_test += clf_random_forest_tuned.predict(test_df[xtrain.columns])/num_of_splits\n\n\npredictions_random_forest_tuned_test = np.expm1(predictions_random_forest_tuned_test)\nprint()\nprint(predictions_random_forest_test)\nprint()\nprint(\"OOF Out-of-fold rmse:\", oof_rmse/num_of_splits)\n\nf, ax = plt.subplots(2, figsize=(12,7))\n\n\nsns.set(rc={'figure.figsize':(9,14)})\nsns.distplot(train_df.revenue, ax=ax[0])\nax[0].set_title(\"Train Set Revenue Histogram\")\nsns.distplot(predictions_random_forest_tuned_test, ax=ax[1])\nax[1].set_title(\"Test Set Revenue Prediction Histogram\")\nf.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LightGBM Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport lightgbm as lgb\n\nparams = {\n    \"metric\": 'rmse',\n    \"verbosity\": -1\n}\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\npredictions_test_lgb = np.zeros(len(test_df))\nnum_fold = 0\noof_rmse = 0\nnum_of_splits = kfold_splits\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    print()\n    \n    model_lgb = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n    model_lgb.fit(xtrain_stra, ytrain_stra, \n        eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], eval_metric='rmse',\n        verbose=100, early_stopping_rounds=1000)\n\n    predictions_valid = model_lgb.predict(xvalid)\n    rmse_valid = np.sqrt(mean_squared_error(yvalid, predictions_valid))\n    print(\"Fold\" ,num_fold, \"xvalid rmse:\", rmse_valid)\n    num_fold = num_fold + 1\n    \n    oof_rmse += rmse_valid\n\n    predictions_test_lgb += model_lgb.predict(test_df[xtrain.columns])/num_of_splits\n    \n\npredictions_test_lgb = np.expm1(predictions_test_lgb)\nprint()\nprint(predictions_test_lgb)\nprint(\"OOF Out-of-fold rmse:\", oof_rmse/num_of_splits)\n\nf, ax = plt.subplots(2, figsize=(12,7))\n\nsns.set(rc={'figure.figsize':(9,14)})\nsns.distplot(train_df.revenue, ax=ax[0])\nax[0].set_title(\"Train Set Revenue Histogram\")\nsns.distplot(predictions_test_lgb, ax=ax[1])\nax[1].set_title(\"Test Set Revenue Prediction Histogram\")\nf.tight_layout()\n\n#xgb.plot_importance(clf_stra_fs_tuned_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tuning the LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bayesian_tuning_lgb(xtrain, ytrain):\n    \n    from skopt import BayesSearchCV    \n    \n    # Classifier\n    bayes_cv_tuner = BayesSearchCV(\n        estimator = lgb.LGBMRegressor(\n            boosting_type='gbdt', n_jobs=2, nthread = 4, verbose=-1\n        ),\n        search_spaces = {\n            'num_leaves': (10, 100),\n            'min_data_in_leaf': (10, 100),\n            'n_estimators': (50, 100),\n            'max_depth': (3, 12),\n            'learning_rate': (0.01, 0.2, 'log-uniform'),\n            \"feature_fraction\": (0.1, 1, 'uniform'),\n            \"bagging_fraction\": (0.1, 1, 'uniform'),\n            'lambda_l1': (0.1, 1, 'log-uniform'),\n            'lambda_l2': (0.1, 1, 'log-uniform')\n        },\n        cv = KFold(\n            n_splits=kfold_splits,\n            shuffle=True,\n            random_state=42\n        ),\n        scoring = 'neg_mean_squared_error',\n        n_jobs = 1,\n        n_iter = 12,   \n        verbose = 0,\n        refit = True,\n        random_state = 42\n    )\n\n    def status_print(optim_result):\n        \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n\n        # Get all the models tested so far in DataFrame format\n        all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n\n        # Get current parameters and the best parameters    \n        best_params = pd.Series(bayes_cv_tuner.best_params_)\n        print('Model #{}\\nBest score: {}\\nBest params: {}\\n'.format(\n            len(all_models),\n            np.round(bayes_cv_tuner.best_score_, 4),\n            bayes_cv_tuner.best_params_\n        ))\n        \n    result_lgbm = bayes_cv_tuner.fit(xtrain, ytrain, callback = status_print)\n    return result_lgbm\n    \n# Fit the model\n#xtrain, ytrain = prepare_for_tuning(X, y, type_of_training=type_of_training)\nresult_lgbm = bayesian_tuning_lgb(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_lgbm.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LGBM training after tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'num_leaves': result_lgbm.best_params_[\"num_leaves\"],\n    'min_data_in_leaf': result_lgbm.best_params_[\"min_data_in_leaf\"],\n    'max_depth': result_lgbm.best_params_[\"max_depth\"],\n    'learning_rate': result_lgbm.best_params_[\"learning_rate\"],\n    \"boosting\": \"gbdt\",\n    \"feature_fraction\": result_lgbm.best_params_[\"feature_fraction\"],\n    \"bagging_freq\": 1,\n    \"bagging_fraction\": result_lgbm.best_params_[\"bagging_fraction\"],\n    \"bagging_seed\": 11,\n    \"metric\": 'rmse',\n    \"lambda_l1\": result_lgbm.best_params_[\"lambda_l1\"],\n    \"lambda_l2\": result_lgbm.best_params_[\"lambda_l2\"],\n    \"verbosity\": -1\n}\n\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport lightgbm as lgb\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\npredictions_test_lgb_tuned = np.zeros(len(test_df))\nnum_fold = 0\noof_rmse = 0\nnum_of_splits = kfold_splits\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    print()\n    \n    model_lgb_tuned = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n    model_lgb_tuned.fit(xtrain_stra, ytrain_stra, \n        eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], eval_metric='rmse',\n        verbose=100, early_stopping_rounds=1000)\n\n    predictions_valid = model_lgb_tuned.predict(xvalid)\n    rmse_valid = np.sqrt(mean_squared_error(yvalid, predictions_valid))\n    print(\"Fold\" ,num_fold, \"xvalid rmse:\", rmse_valid)\n    num_fold = num_fold + 1\n    oof_rmse += rmse_valid\n\n    predictions_test_lgb_tuned += model_lgb_tuned.predict(test_df[xtrain.columns])/num_of_splits\n    \n\npredictions_test_lgb_tuned = np.expm1(predictions_test_lgb_tuned)\nprint()\nprint(predictions_test_lgb_tuned)\nprint(\"OOF Out-of-fold rmse:\", oof_rmse/num_of_splits)\n\nf, ax = plt.subplots(2, figsize=(12,7))\n\nsns.set(rc={'figure.figsize':(9,14)})\nsns.distplot(train_df.revenue, ax=ax[0])\nax[0].set_title(\"Train Set Revenue Histogram\")\nsns.distplot(predictions_test_lgb_tuned, ax=ax[1])\nax[1].set_title(\"Test Set Revenue Prediction Histogram\")\nf.tight_layout()\n\n#xgb.plot_importance(clf_stra_fs_tuned_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection for xgboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\n\nmax_selected_features = 200\nsel = SelectFromModel(clf_stra_xgb, max_features = max_selected_features, threshold=0.005, prefit=True)\n\nfeature_idx = sel.get_support()\nselected_features_xgb = X.columns[feature_idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features_xgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Baseline XGBoost with Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nimport time\nimport random\nfrom sklearn.metrics import mean_squared_error\n\nimport xgboost as xgb\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X[selected_features_xgb], y, random_state=42, test_size=0.3)\n\nimport xgboost as xgb\n\nstart_time = time.time()\n\npredictions_test_xgb_fs = np.zeros(len(test_df))\nnum_fold = 0\noof_rmse = 0\nnum_of_splits = kfold_splits\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    print()\n    \n    \n    clf_stra_fs_xgb = xgb.XGBRegressor(n_estimators=10000, seed=42, nthread = -1)\n\n    clf_stra_fs_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=1000, eval_metric='rmse', verbose=100)\n\n    predictions_valid = clf_stra_fs_xgb.predict(xvalid)\n    rmse_valid = np.sqrt(mean_squared_error(yvalid, predictions_valid))\n    print(\"Fold\" ,num_fold, \"xvalid rmse:\", rmse_valid)\n    num_fold = num_fold + 1\n    \n    oof_rmse += rmse_valid\n\n    predictions_test_xgb_fs += clf_stra_fs_xgb.predict(test_df[xtrain.columns])/num_of_splits\n    \n\npredictions_test_xgb_fs = np.expm1(predictions_test_xgb_fs)\nprint(predictions_test_xgb_fs)\nprint(\"OOF Out-of-fold rmse:\", oof_rmse/num_of_splits)\n\nf, ax = plt.subplots(2, figsize=(12,7))\n\nsns.set(rc={'figure.figsize':(9,14)})\nsns.distplot(train_df.revenue, ax=ax[0])\nax[0].set_title(\"Train Set Revenue Histogram\")\nsns.distplot(predictions_test_xgb_fs, ax=ax[1])\nax[1].set_title(\"Test Set Revenue Prediction Histogram\")\nf.tight_layout()\n\nxgb.plot_importance(clf_stra_fs_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tuning with feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"result = bayesian_tuning(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost training with Feature Selection and tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold\nimport time\nimport random\nfrom sklearn.metrics import mean_squared_error\n\nimport xgboost as xgb\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X[selected_features_xgb], y, random_state=42, test_size=0.3)\n\npredictions_test_xgb_fs_tuned = np.zeros(len(test_df))\nnum_fold = 0\noof_rmse = 0\nnum_of_splits = kfold_splits\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    print()\n    \n    \n    clf_stra_fs_tuned_xgb = xgb.XGBRegressor(colsample_bytree = result.best_params_[\"colsample_bytree\"],\n                                    gamma=result.best_params_[\"gamma\"],                 \n                                    learning_rate=result.best_params_[\"learning_rate\"],\n                                    max_depth=result.best_params_[\"max_depth\"],\n                                    min_child_weight=result.best_params_[\"min_child_weight\"],\n                                    n_estimators=10000,\n                                    reg_alpha=result.best_params_[\"reg_alpha\"],\n                                    reg_lambda=result.best_params_[\"reg_lambda\"],\n                                    subsample=result.best_params_[\"subsample\"],\n                                    seed=42,\n                                    nthread = -1)\n\n    clf_stra_fs_tuned_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=1000, eval_metric='rmse', verbose=100)\n\n    predictions_valid = clf_stra_fs_tuned_xgb.predict(xvalid)\n    rmse_valid = np.sqrt(mean_squared_error(yvalid, predictions_valid))\n    print(\"Fold\" ,num_fold, \"xvalid rmse:\", rmse_valid)\n    num_fold = num_fold + 1\n    \n    oof_rmse += rmse_valid\n\n    predictions_test_xgb_fs_tuned += clf_stra_fs_tuned_xgb.predict(test_df[xtrain.columns])/num_of_splits\n    \n\npredictions_test_xgb_fs_tuned = np.expm1(predictions_test_xgb_fs_tuned)\nprint(predictions_test_xgb_fs_tuned)\nprint(\"OOF Out-of-fold rmse:\", oof_rmse/num_of_splits)\n\nf, ax = plt.subplots(2, figsize=(12,7))\n\nsns.set(rc={'figure.figsize':(9,14)})\nsns.distplot(train_df.revenue, ax=ax[0])\nax[0].set_title(\"Train Set Revenue Histogram\")\nsns.distplot(predictions_test_xgb_fs_tuned, ax=ax[1])\nax[1].set_title(\"Test Set Revenue Prediction Histogram\")\nf.tight_layout()\n\nxgb.plot_importance(clf_stra_fs_tuned_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection for LGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\n\nmax_selected_features = 200\nsel = SelectFromModel(model_lgb, max_features = max_selected_features, threshold=0.005, prefit=True)\n\nfeature_idx = sel.get_support()\nselected_features_lgb = X.columns[feature_idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features_lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Baseline LGB Training with Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport lightgbm as lgb\n\nparams = {\n    \"metric\": 'rmse',\n    \"verbosity\": -1\n}\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X[selected_features_lgb], y, random_state=42, test_size=0.3)\n\npredictions_test_lgb_fs = np.zeros(len(test_df))\nnum_fold = 0\noof_rmse = 0\nnum_of_splits = kfold_splits\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    print()\n    \n    model_lgb_fs = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n    model_lgb_fs.fit(xtrain_stra, ytrain_stra, \n        eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], eval_metric='rmse',\n        verbose=100, early_stopping_rounds=1000)\n\n    predictions_valid = model_lgb_fs.predict(xvalid)\n    rmse_valid = np.sqrt(mean_squared_error(yvalid, predictions_valid))\n    print(\"Fold\" ,num_fold, \"xvalid rmse:\", rmse_valid)\n    num_fold = num_fold + 1\n    \n    oof_rmse += rmse_valid\n\n    predictions_test_lgb_fs += model_lgb_fs.predict(test_df[xtrain.columns])/num_of_splits\n    \n\npredictions_test_lgb_fs = np.expm1(predictions_test_lgb_fs)\nprint()\nprint(predictions_test_lgb_fs)\nprint(\"OOF Out-of-fold rmse:\", oof_rmse/num_of_splits)\n\nf, ax = plt.subplots(2, figsize=(12,7))\n\nsns.set(rc={'figure.figsize':(9,14)})\nsns.distplot(train_df.revenue, ax=ax[0])\nax[0].set_title(\"Train Set Revenue Histogram\")\nsns.distplot(predictions_test_lgb_fs, ax=ax[1])\nax[1].set_title(\"Test Set Revenue Prediction Histogram\")\nf.tight_layout()\n\n#xgb.plot_importance(clf_stra_fs_tuned_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LGB Tuning with Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"result_lgb = bayesian_tuning_lgb(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LGB Training after Feature Selection and Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'num_leaves': result_lgbm.best_params_[\"num_leaves\"],\n    'min_data_in_leaf': result_lgbm.best_params_[\"min_data_in_leaf\"],\n    'max_depth': result_lgbm.best_params_[\"max_depth\"],\n    'learning_rate': result_lgbm.best_params_[\"learning_rate\"],\n    \"boosting\": \"gbdt\",\n    \"feature_fraction\": result_lgbm.best_params_[\"feature_fraction\"],\n    \"bagging_freq\": 1,\n    \"bagging_fraction\": result_lgbm.best_params_[\"bagging_fraction\"],\n    \"bagging_seed\": 11,\n    \"metric\": 'rmse',\n    \"lambda_l1\": result_lgbm.best_params_[\"lambda_l1\"],\n    \"lambda_l2\": result_lgbm.best_params_[\"lambda_l2\"],\n    \"verbosity\": -1\n}\n\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport lightgbm as lgb\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X[selected_features_lgb], y, random_state=42, test_size=0.3)\n\npredictions_test_lgb_fs_tuned = np.zeros(len(test_df))\nnum_fold = 0\noof_rmse = 0\nnum_of_splits = kfold_splits\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    print()\n    \n    model_lgb_fs_tuned = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n    model_lgb_fs_tuned.fit(xtrain_stra, ytrain_stra, \n        eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], eval_metric='rmse',\n        verbose=100, early_stopping_rounds=1000)\n\n    predictions_valid = model_lgb_fs_tuned.predict(xvalid)\n    rmse_valid = np.sqrt(mean_squared_error(yvalid, predictions_valid))\n    print(\"Fold\" ,num_fold, \"xvalid rmse:\", rmse_valid)\n    num_fold = num_fold + 1\n    oof_rmse += rmse_valid\n\n    predictions_test_lgb_fs_tuned += model_lgb_fs_tuned.predict(test_df[xtrain.columns])/num_of_splits\n    \n\npredictions_test_lgb_fs_tuned = np.expm1(predictions_test_lgb_fs_tuned)\nprint()\nprint(predictions_test_lgb_fs_tuned)\nprint(\"OOF Out-of-fold rmse:\", oof_rmse/num_of_splits)\n\nf, ax = plt.subplots(2, figsize=(12,7))\n\nsns.set(rc={'figure.figsize':(9,14)})\nsns.distplot(train_df.revenue, ax=ax[0])\nax[0].set_title(\"Train Set Revenue Histogram\")\nsns.distplot(predictions_test_lgb_fs_tuned, ax=ax[1])\nax[1].set_title(\"Test Set Revenue Prediction Histogram\")\nf.tight_layout()\n\n#xgb.plot_importance(clf_stra_fs_tuned_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ML Blends"},{"metadata":{"trusted":true},"cell_type":"code","source":"####### xgb with the other models\n\npredictions_test_xgb_extra_trees = (0.5 * predictions_test_xgb) + (0.5 * predictions_extra_trees_test)\npredictions_test_xgb_random_forest = (0.5 * predictions_test_xgb) + (0.5 * predictions_random_forest_test)\npredictions_test_extra_trees_random_forest = (0.5 * predictions_extra_trees_test) + (0.5 * predictions_random_forest_test)\n\npredictions_test_tuned_xgb_extra_trees = (0.5 * predictions_test_xgb_tuned) + (0.5 * predictions_extra_trees_tuned_test)\npredictions_test_tuned_xgb_random_forest = (0.5 * predictions_test_xgb_tuned) + (0.5 * predictions_random_forest_tuned_test)\npredictions_test_tuned_extra_trees_random_forest = (0.5 * predictions_extra_trees_tuned_test) + (0.5 * predictions_random_forest_tuned_test)\n\npredictions_test_xgb_fs_extra_trees =  (0.5 * predictions_test_xgb_fs) + (0.5 * predictions_extra_trees_test)\npredictions_test_xgb_fs_tuned_extra_trees = (0.5 * predictions_test_xgb_fs_tuned) + (0.5 * predictions_extra_trees_test)\n\npredictions_test_baseline_xgb_tuned_extra_trees = (0.5 * predictions_test_xgb) + (0.5 * predictions_extra_trees_tuned_test)\n\n####### lgb with the other models\n\npredictions_test_lgb_xgb = (0.5 * predictions_test_lgb) + (0.5 * predictions_test_xgb)\npredictions_test_tuned_lgb_xgb = (0.5 * predictions_test_lgb_tuned) + (0.5 * predictions_test_xgb_tuned)\n\npredictions_test_lgb_extra_trees = (0.5 * predictions_test_lgb) + (0.5 * predictions_extra_trees_test)\npredictions_test_lgb_random_forest = (0.5 * predictions_test_lgb) + (0.5 * predictions_random_forest_test)\n\npredictions_test_tuned_lgb_extra_trees = (0.5 * predictions_test_lgb_tuned) + (0.5 * predictions_extra_trees_tuned_test)\npredictions_test_tuned_lgb_random_forest = (0.5 * predictions_test_lgb_tuned) + (0.5 * predictions_random_forest_tuned_test)\n\npredictions_test_xgb_fs_lgb =  (0.5 * predictions_test_xgb_fs) + (0.5 * predictions_test_lgb_fs)\npredictions_test_xgb_fs_tuned_lgb = (0.5 * predictions_test_xgb_fs_tuned) + (0.5 * predictions_test_lgb_fs_tuned)\n\n\npredictions_test_baseline_lgb_tuned_extra_trees = (0.5 * predictions_test_lgb) + (0.5 * predictions_extra_trees_tuned_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing for submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb baseline\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_xgb\nsubmission.to_csv('clf_xgb_baseline.csv', index=False)\n\n# xgb tuning\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_xgb_tuned\nsubmission.to_csv('clf_xgb_tuned.csv', index=False)\n\n# lgb baseline\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_lgb\nsubmission.to_csv('clf_lgb_baseline.csv', index=False)\n\n# lgb tuning\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_lgb_tuned\nsubmission.to_csv('clf_lgb_tuned.csv', index=False)\n\n# extra trees baseline\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_extra_trees_test\nsubmission.to_csv('clf_extra_trees_baseline.csv', index=False)\n\n# extra trees tuning\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_extra_trees_tuned_test\nsubmission.to_csv('clf_extra_trees_tuned.csv', index=False)\n\n# xgb baseline with feature selection\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_xgb_fs\nsubmission.to_csv('clf_xgb_fs_baseline.csv', index=False)\n\n# xgb tuning with feature selection\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_xgb_fs_tuned\nsubmission.to_csv('clf_xgb_fs_tuned.csv', index=False)\n\n# lgb baseline with feature selection\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_lgb_fs\nsubmission.to_csv('clf_lgb_fs_baseline.csv', index=False)\n\n# lgb tuning with feature selection\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_lgb_fs_tuned\nsubmission.to_csv('clf_lgb_fs_tuned.csv', index=False)\n\n# Blend 1\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_xgb_extra_trees\nsubmission.to_csv('blend_xgb_extra_trees_baselines.csv', index=False)\n\n# Blend 2\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_xgb_random_forest\nsubmission.to_csv('blend_xgb_random_forest_baselines.csv', index=False)\n\n# Blend 3\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_extra_trees_random_forest\nsubmission.to_csv('blend_extra_trees_random_forest_baselines.csv', index=False)\n\n# Blend 4\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_tuned_xgb_extra_trees\nsubmission.to_csv('blend_xgb_extra_trees_tuned.csv', index=False)\n\n# Blend 5\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_tuned_xgb_random_forest\nsubmission.to_csv('blend_xgb_random_forest_tuned.csv', index=False)\n\n# Blend 6\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_tuned_extra_trees_random_forest\nsubmission.to_csv('blend_extra_trees_random_forest_tuned.csv', index=False)\n\n# Blend 7\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_baseline_xgb_tuned_extra_trees\nsubmission.to_csv('blend_baseline_xgb_tuned_extra_trees.csv', index=False)\n\n# Blend 8\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_xgb_fs_extra_trees\nsubmission.to_csv('blend_xgb_fs_extra_trees.csv', index=False)\n\n# Blend 9\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_xgb_fs_tuned_extra_trees\nsubmission.to_csv('blend_xgb_fs_tuned_extra_trees.csv', index=False)\n\n# Blend 10\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_lgb_xgb\nsubmission.to_csv('blend_lgb_xgb.csv', index=False)\n\n# Blend 11\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_tuned_lgb_xgb\nsubmission.to_csv('blend_tuned_lgb_xgb.csv', index=False)\n\n# Blend 12\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_lgb_extra_trees\nsubmission.to_csv('blend_tuned_lgb_extra_trees.csv', index=False)\n\n# Blend 13\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_lgb_extra_trees\nsubmission.to_csv('blend_tuned_lgb_extra_trees.csv', index=False)\n\n# Blend 14\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_lgb_random_forest\nsubmission.to_csv('blend_tuned_lgb_random_forest.csv', index=False)\n\n# Blend 15\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_tuned_lgb_extra_trees\nsubmission.to_csv('blend_tuned_lgb_extra_trees.csv', index=False)\n\n# Blend 16\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_tuned_lgb_random_forest\nsubmission.to_csv('blend_tuned_lgb_random_forest.csv', index=False)\n\n# Blend 17\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_baseline_lgb_tuned_extra_trees\nsubmission.to_csv('blend_baseline_lgb_tuned_extra_trees.csv', index=False)\n\n# Blend 18\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_xgb_fs_lgb\nsubmission.to_csv('blend_xgb_fs_lgb.csv', index=False)\n\n# Blend 19\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_xgb_fs_tuned_lgb\nsubmission.to_csv('blend_xgb_fs_tuned_lgb.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### *Thank for your time! Any suggestions are welcomed on how to improve my models performance*"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}