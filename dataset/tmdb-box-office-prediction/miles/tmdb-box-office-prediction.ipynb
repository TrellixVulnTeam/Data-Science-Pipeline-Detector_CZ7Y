{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport seaborn as sns\nfrom sklearn import *\nimport xgboost as xg\n# import GPy\n# import GPyOpt\n\n# from GPyOpt.methods import BayesianOptimization\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"# Libraries\n\nimport numpy as np\nimport pandas as pd\nfrom functools import reduce\npd.set_option('max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('ggplot')\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, KFold\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import StandardScaler\nstop = set(stopwords.words('english'))\nimport os\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport json\nimport ast\nimport math\nimport json\nimport ast\nimport eli5\nimport shap\nfrom catboost import CatBoostRegressor\nfrom urllib.request import urlopen\nfrom PIL import Image\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_log_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Load"},{"metadata":{"trusted":true},"cell_type":"code","source":"or_train = pd.read_csv('../input/tmdb-box-office-prediction/train.csv')\nor_test = pd.read_csv('../input/tmdb-box-office-prediction/test.csv')\n\n# from this kernel: https://www.kaggle.com/gravix/gradient-in-a-box\ndict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\ndef text_to_dict(df):\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n    return df\n        \nor_train = text_to_dict(or_train)\nor_test = text_to_dict(or_test)\n\ntrain = or_train.copy()\ntest = or_test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['belongs_to_collection'].head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"chs = ['belongs_to_collection']\nfor table in [train, test]:\n    for col in chs:\n        trs = table[col].apply(lambda x:sorted(list(map(lambda y:y['name'], x))))\n        text = trs.apply(lambda x:\" \".join(x))\n        table[col] = text","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"## Text Pretrained"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"text_train = or_train.copy()\ntext_test = or_test.copy()\n\ntext_train=text_train.fillna('')\ntext_test=text_test.fillna('')\n\ntext_cols = ['belongs_to_collection','genres','production_companies','production_countries','Keywords','cast','crew']\nfor col in text_cols:\n    trs=text_train[col].apply(lambda x:sorted(list(map(lambda y:y['name'], x))))\n    text = trs.apply(lambda x:\" \".join(x)+\" \" if len(x)>0 else \"\")\n    text_train[col] = text.apply(lambda x:x.strip())\n    \n    trs=text_test[col].apply(lambda x:sorted(list(map(lambda y:y['name'], x))))\n    text = trs.apply(lambda x:\" \".join(x)+\" \" if len(x)>0 else \"\")\n    text_test[col] = text.apply(lambda x:x.strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"choose_text = ['belongs_to_collection','genres','production_companies','production_countries','overview']\ntext_train['text'] = text_train[choose_text].apply(lambda x: ''.join(x), axis=1)\ntext_train = text_train['text']\ntext_train = text_train.apply(lambda x:x.split(' '))\n\ntext_test['text'] = text_test[choose_text].apply(lambda x: ''.join(x), axis=1)\ntext_test = text_test['text']\ntext_test = text_test.apply(lambda x:x.split(' '))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {}\nwith open(os.path.join('../input/glove6b50dtxt/glove.6B.50d.txt')) as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_DIM = max(max(text_train.apply(lambda x:len(x))), max(text_test.apply(lambda x:len(x))))*50\nnum_words = len(text_train)\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\ntest_matrix = np.zeros((len(text_test), EMBEDDING_DIM))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for row in range(len(text_train)):\n    line = text_train[row]\n    for i in range(len(line)):\n        k = line[i].lower()\n        if k in embeddings_index:\n            vec = embeddings_index[k]\n            embedding_matrix[row][i*50:(i+1)*50] = vec\nfor row in range(len(text_test)):\n    line = text_test[row]\n    for i in range(len(line)):\n        k = line[i].lower()\n        if k in embeddings_index:\n            vec = embeddings_index[k]\n            test_matrix[row][i*50:(i+1)*50] = vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_indices = np.random.permutation(3000)\ntrain_indices = rand_indices[0:2500]\nvalid_indices = rand_indices[2500:]\n\nx_val = embedding_matrix[valid_indices]\n# y_val = ys[valid_indices]\n\nx_tr = embedding_matrix[train_indices]\n# y_tr = ys[train_indices]\n\nprint('Shape of x_tr: ' + str(x_tr.shape))\n# print('Shape of y_tr: ' + str(y_tr.shape))\nprint('Shape of x_val: ' + str(x_val.shape))\n# print('Shape of y_val: ' + str(y_val.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Input\nfrom keras import models\nfrom keras import regularizers\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n\ninput_img = Input(shape=(EMBEDDING_DIM,), name='input_img')\n\nencode1 = Dense(EMBEDDING_DIM//2, activation='relu', name='encode1')(input_img)\nhidden = Dropout(0.5)(encode1)\nencode2 = Dense(128, activation='relu', name='encode2')(hidden)\nencode3 = Dense(32, activation='relu', name='encode3')(encode2)\nbottleneck = Dense(8, activation='relu', name='bottleneck')(encode3)\ndecode1 = Dense(32, activation='relu', name='decode1')(bottleneck)\ndecode2 = Dense(128, activation='relu', name='decode2')(decode1)\ndecode3 = Dense(EMBEDDING_DIM//2, activation='relu', name='decode3')(decode2)\nhidden = Dropout(0.5)(decode3)\ndecode4 = Dense(EMBEDDING_DIM, activation='relu', name='decode4')(decode3)\n\n# hidden1 = Dense(128, activation='relu')(bottleneck)\n# hidden2 = Dense(265, activation='relu', kernel_regularizer=regularizers.l2(0.01))(hidden1)\n# hidden3 = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01))(hidden2)\n# hidden4 = Dropout(0.5)(hidden3)\n# hidden5 = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(hidden4)\n# classifier = Dense(1, activation='linear', name='reg', activity_regularizer=regularizers.l1(0.01))(hidden5)\n\nae = models.Model(input_img, decode4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import optimizers\n\nlearning_rate = 1E-3 # to be tuned!\n\nae.compile(loss='mean_squared_error', optimizer=optimizers.RMSprop(lr=learning_rate))\nhistory = ae.fit(x_tr, x_tr, \n                 batch_size=128, \n                 epochs=20, \n                 validation_data=(x_val, x_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(loss))\n\nplt.plot(epochs, loss, 'bo', label='Training Loss')\nplt.plot(epochs, val_loss, 'r', label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"ae_encoder = models.Model(input_img, bottleneck)\nencoded_test = ae_encoder.predict(test_matrix)\nencoded_test\nencoded_train = ae_encoder.predict(embedding_matrix)\nencoded_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Analytics"},{"metadata":{},"cell_type":"markdown","source":"1. 投资与收益的关系，明显，投资与收益呈正相关"},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_train = xg.DMatrix(data=train.loc[:,['budget','popularity','runtime']], label=np.array(train['revenue']))\nparams = {'eval_metric' : 'rmse', 'silent' : 1}\nxg_model = xg.train(params, xg_train)\nxg_test = xg.DMatrix(data=train.loc[:,['budget', 'popularity', 'runtime']])\nxg_pred = xg_model.predict(xg_test)\n\neli5.show_weights(xg_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\nplt.scatter(train['budget'], train['revenue'])\n\nlinereg = linear_model.LinearRegression()\nlinereg.fit(np.array(train['budget']).reshape(-1, 1),train['revenue'])\nli_pre = linereg.predict(np.array(train['budget']).reshape(-1, 1))\nplt.plot(train['budget'], li_pre, color='orange', label='linear')\n\nplt.scatter(train['budget'], xg_pred, label = 'xgboost')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.sqrt(metrics.mean_squared_log_error(train['revenue'], li_pre)))\nprint(np.sqrt(metrics.mean_squared_log_error(train['revenue'], xg_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['log_budget'] = np.log1p(train['budget'])\ntrain['log_revenue'] = np.log1p(train['revenue'])\ntest['log_budget'] = np.log1p(test['budget'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"sns.boxenplot(x='original_language', y='log_revenue', data=train.loc[train['original_language'].isin(train['original_language'].value_counts().head(10).index)])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"for table in [train, test]:\n    table['has_homepage'] = 0\n    table.loc[table['homepage'].isnull() == True, 'has_homepage'] = 0\n    table.loc[table['homepage'].isnull() == False, 'has_homepage'] = 1","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"sns.catplot(x='has_homepage', y='revenue', data=train)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nsns.boxenplot(x='original_language', y='revenue', data=train.loc[train['original_language'].isin(train['original_language'].value_counts().head(10).index)])\nplt.subplot(1,2,2)\nsns.boxenplot(x='original_language', y='log_revenue', data=train.loc[train['original_language'].isin(train['original_language'].value_counts().head(10).index)])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# plt.figure(figsize=(12,12))\n# text = \" \".join(train.loc[train['revenue'].sort_values(ascending=False).head(200).index, 'crew'])\n# wc = WordCloud(background_color='white', width=1200, height=1000).generate(text)\n# plt.axis(\"off\")\n# plt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# vec = TfidfVectorizer(sublinear_tf=True, analyzer='word', ngram_range=(1,2), min_df=5)\n# overview_text = vec.fit_transform(train['overview'].fillna(''))\n# linreg = LinearRegression()\n# linreg.fit(overview_text, train['log_revenue'])\n# eli5.show_weights(linreg, vec=vec, top=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Runtime"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,6))\nplt.subplot(1,2,1)\nplt.hist(train['runtime'].fillna(0), bins=40)\nplt.subplot(1,2,2)\nplt.scatter(train['runtime'].fillna(0), train['revenue'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Date"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fix_date(x):\n    year = x.split('/')[2]\n    if int(year) <= 19:\n        return x[:-2]+\"20\"+year\n    else:\n        return x[:-2]+\"19\"+year\n\ntest.loc[test['release_date'].isnull(), 'release_date'] = '5/1/00'\nfor table in [train, test]:\n    table['release_date'] = table['release_date'].apply(lambda x:fix_date(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trans(table):\n    df = pd.DataFrame()\n    t = pd.to_datetime(table['release_date'])\n    date_parts = [\"year\", \"weekday\", \"month\", 'weekofyear', 'day', 'quarter']\n    for part in date_parts:\n        part_col = 'release_date' + \"_\" + part\n        df[part_col] = getattr(t.dt, part).astype(int)\n    table = pd.concat([table, df], axis=1)\n    return table\ntrain = trans(train)\ntest = trans(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d1 = train['release_date_year'].value_counts().sort_index()\nd2 = train.groupby(['release_date_year'])['revenue'].sum()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='film count'), go.Scatter(x=d2.index, y=d2.values, name='total revenue', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Number of films and total revenue per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  yaxis2=dict(title='Total revenue', overlaying='y', side='right')\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='release_date_year', y='revenue', data=train);\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train['num_countries'] = train['production_countries'].apply(lambda x: len(x) if x != {} else 0)\ntest['num_countries'] = test['production_countries'].apply(lambda x: len(x) if x != {} else 0)\nsns.catplot(x='num_countries', y='revenue', data=train);\nplt.title('Revenue for different number of countries producing the film');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Process"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['homepage', 'imdb_id', 'poster_path', 'release_date', 'status','log_revenue'], axis=1)\ntest = test.drop(['homepage', 'imdb_id', 'poster_path', 'release_date','status'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.rename(columns={'belongs_to_collection':\"collection_name\"}, inplace=True)\ntest.rename(columns={'belongs_to_collection':\"collection_name\"}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['log_budget'] = np.log1p(train['budget'])\ntest['log_budget'] = np.log1p(test['budget'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_genres = list(train['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ntrain['num_genres'] = train['genres'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_genres'] = train['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_genres = [m[0] for m in Counter([i for j in list_of_genres for i in j]).most_common(15)]\nfor g in top_genres:\n    train['genre_' + g] = train['all_genres'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_genres'] = test['genres'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_genres'] = test['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_genres:\n    test['genre_' + g] = test['all_genres'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['genres'], axis=1)\ntest = test.drop(['genres'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_companies = list(train['production_companies'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ntrain['num_companies'] = train['production_companies'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_production_companies'] = train['production_companies'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_companies = [m[0] for m in Counter([i for j in list_of_companies for i in j]).most_common(30)]\nfor g in top_companies:\n    train['production_company_' + g] = train['all_production_companies'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_companies'] = test['production_companies'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_production_companies'] = test['production_companies'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_companies:\n    test['production_company_' + g] = test['all_production_companies'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['production_companies', 'all_production_companies'], axis=1)\ntest = test.drop(['production_companies', 'all_production_companies'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_countries = list(train['production_countries'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ntrain['num_countries'] = train['production_countries'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_countries'] = train['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_countries = [m[0] for m in Counter([i for j in list_of_countries for i in j]).most_common(25)]\nfor g in top_countries:\n    train['production_country_' + g] = train['all_countries'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_countries'] = test['production_countries'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_countries'] = test['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_countries:\n    test['production_country_' + g] = test['all_countries'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['production_countries', 'all_countries'], axis=1)\ntest = test.drop(['production_countries', 'all_countries'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_languages = list(train['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ntrain['num_languages'] = train['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_languages'] = train['spoken_languages'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_languages = [m[0] for m in Counter([i for j in list_of_languages for i in j]).most_common(30)]\nfor g in top_languages:\n    train['language_' + g] = train['all_languages'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_languages'] = test['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_languages'] = test['spoken_languages'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_languages:\n    test['language_' + g] = test['all_languages'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['spoken_languages', 'all_languages'], axis=1)\ntest = test.drop(['spoken_languages', 'all_languages'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_keywords = list(train['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ntrain['num_Keywords'] = train['Keywords'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_Keywords'] = train['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_keywords = [m[0] for m in Counter([i for j in list_of_keywords for i in j]).most_common(30)]\nfor g in top_keywords:\n    train['keyword_' + g] = train['all_Keywords'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_Keywords'] = test['Keywords'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_Keywords'] = test['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_keywords:\n    test['keyword_' + g] = test['all_Keywords'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['Keywords', 'all_Keywords'], axis=1)\ntest = test.drop(['Keywords', 'all_Keywords'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_cast_names = list(train['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nlist_of_cast_genders = list(train['cast'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\nlist_of_cast_characters = list(train['cast'].apply(lambda x: [i['character'] for i in x] if x != {} else []).values)\ntrain['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\ntop_cast_names = [m[0] for m in Counter([i for j in list_of_cast_names for i in j]).most_common(15)]\nfor g in top_cast_names:\n    train['cast_name_' + g] = train['cast'].apply(lambda x: 1 if g in str(x) else 0)\ntrain['genders_0_cast'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntrain['genders_1_cast'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntrain['genders_2_cast'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\ntop_cast_characters = [m[0] for m in Counter([i for j in list_of_cast_characters for i in j]).most_common(15)]\nfor g in top_cast_characters:\n    train['cast_character_' + g] = train['cast'].apply(lambda x: 1 if g in str(x) else 0)\n    \ntest['num_cast'] = test['cast'].apply(lambda x: len(x) if x != {} else 0)\nfor g in top_cast_names:\n    test['cast_name_' + g] = test['cast'].apply(lambda x: 1 if g in str(x) else 0)\ntest['genders_0_cast'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntest['genders_1_cast'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntest['genders_2_cast'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\nfor g in top_cast_characters:\n    test['cast_character_' + g] = test['cast'].apply(lambda x: 1 if g in str(x) else 0)\n\ntrain = train.drop(['cast'], axis=1)\ntest = test.drop(['cast'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_crew_names = list(train['crew'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nlist_of_crew_jobs = list(train['crew'].apply(lambda x: [i['job'] for i in x] if x != {} else []).values)\nlist_of_crew_genders = list(train['crew'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\nlist_of_crew_departments = list(train['crew'].apply(lambda x: [i['department'] for i in x] if x != {} else []).values)\ntrain['num_crew'] = train['crew'].apply(lambda x: len(x) if x != {} else 0)\ntop_crew_names = [m[0] for m in Counter([i for j in list_of_crew_names for i in j]).most_common(15)]\nfor g in top_crew_names:\n    train['crew_name_' + g] = train['crew'].apply(lambda x: 1 if g in str(x) else 0)\ntrain['genders_0_crew'] = train['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntrain['genders_1_crew'] = train['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntrain['genders_2_crew'] = train['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\ntop_cast_characters = [m[0] for m in Counter([i for j in list_of_cast_characters for i in j]).most_common(15)]\nfor g in top_cast_characters:\n    train['crew_character_' + g] = train['crew'].apply(lambda x: 1 if g in str(x) else 0)\ntop_crew_jobs = [m[0] for m in Counter([i for j in list_of_crew_jobs for i in j]).most_common(15)]\nfor j in top_crew_jobs:\n    train['jobs_' + j] = train['crew'].apply(lambda x: sum([1 for i in x if i['job'] == j]))\ntop_crew_departments = [m[0] for m in Counter([i for j in list_of_crew_departments for i in j]).most_common(15)]\nfor j in top_crew_departments:\n    train['departments_' + j] = train['crew'].apply(lambda x: sum([1 for i in x if i['department'] == j])) \n    \ntest['num_crew'] = test['crew'].apply(lambda x: len(x) if x != {} else 0)\nfor g in top_crew_names:\n    test['crew_name_' + g] = test['crew'].apply(lambda x: 1 if g in str(x) else 0)\ntest['genders_0_crew'] = test['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntest['genders_1_crew'] = test['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntest['genders_2_crew'] = test['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\nfor g in top_cast_characters:\n    test['crew_character_' + g] = test['crew'].apply(lambda x: 1 if g in str(x) else 0)\nfor j in top_crew_jobs:\n    test['jobs_' + j] = test['crew'].apply(lambda x: sum([1 for i in x if i['job'] == j]))\nfor j in top_crew_departments:\n    test['departments_' + j] = test['crew'].apply(lambda x: sum([1 for i in x if i['department'] == j])) \n\ntrain = train.drop(['crew'], axis=1)\ntest = test.drop(['crew'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for table in [test, train]:\n    table['has_collection'] = table['collection_name'].apply(lambda x: 1 if len(x)!=0 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['original_language', 'collection_name', 'all_genres']:\n    le = LabelEncoder()\n    le.fit(list(train[col].fillna('')) + list(test[col].fillna('')))\n    train[col] = le.transform(train[col].fillna('').astype(str))\n    test[col] = le.transform(test[col].fillna('').astype(str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_texts = train[['title', 'tagline', 'overview', 'original_title']]\ntest_texts = test[['title', 'tagline', 'overview', 'original_title']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['title', 'tagline', 'overview', 'original_title']:\n    train['len_' + col] = train[col].fillna('').apply(lambda x: len(str(x)))\n    train['words_' + col] = train[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n    train = train.drop(col, axis=1)\n    test['len_' + col] = test[col].fillna('').apply(lambda x: len(str(x)))\n    test['words_' + col] = test[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n    test = test.drop(col, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['id', 'revenue'], axis=1)\ny = np.log1p(train['revenue'])\nX_test = test.drop(['id'], axis=1)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_valid.shape)\nprint(y_train.shape)\nprint(y_valid.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\nmodel1 = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\nmodel1.fit(X_train, y_train, \n        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n        verbose=1000, early_stopping_rounds=200)\neli5.show_weights(model1, feature_filter=lambda x: x != '<BIAS>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n\n    oof = np.zeros(X.shape[0])\n    prediction = np.zeros(X_test.shape[0])\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        if model_type == 'sklearn':\n            X_train, X_valid = X[train_index], X[valid_index]\n        else:\n            X_train, X_valid = X.values[train_index], X.values[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n                    verbose=1000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test.values), ntree_limit=model.best_ntree_limit)\n\n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = mean_squared_error(y_valid, y_pred_valid)\n            \n            y_pred = model.predict(X_test)\n            \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric='RMSE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n        \n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] /= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction\n    \n    else:\n        return oof, prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 30,\n         'min_data_in_leaf': 10,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\noof_lgb, prediction_lgb, _ = train_model(X, X_test, y, params=params, model_type='lgb', plot_feature_importance=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xg_train = xg.DMatrix(data=X, label=np.array(y))\n# params = {'eval_metric' : 'rmse', 'silent' : 1}\n# xg_model = xg.train(params, xg_train)\n# xg_test = xg.DMatrix(data=X_test)\n# xg_pred = xg_model.predict(xg_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.DataFrame(encoded_train)\nX=pd.concat([X, df2], axis=1)\n\ndf2 = pd.DataFrame(encoded_test)\nX_test = pd.concat([X_test, df2], axis=1)\n\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBRegressor\nxg_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n               colsample_bytree=1, gamma=4.541003990662603, importance_type='gain',\n               learning_rate=0.08209238500752991, max_delta_step=0, max_depth=4,\n               min_child_weight=8, missing=None, n_estimators=137, n_jobs=1,\n               nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n               reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n               subsample=1)\n\nxg_train = xg.DMatrix(data=X, label=np.array(y))\nxg_model.fit(X, y)\nxg_test = xg.DMatrix(data=X_test)\nxg_pred = xg_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred = xg_model.predict(X)\ntrain_pred = np.expm1(train_pred)\nnp.sqrt(mean_squared_log_error(train['revenue'], train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/tmdb-box-office-prediction/sample_submission.csv')\nsub['revenue'] = np.expm1(xg_pred)\nsub.to_csv(\"xgb.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBRegressor\n\n# xgb1 = XGBRegressor()\n# parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n#               'objective':['reg:linear'],\n#               'learning_rate': [.03, 0.05, .07], #so called `eta` value\n#               'max_depth': [5, 6, 7],\n#               'min_child_weight': [4],\n#               'silent': [1],\n#               'subsample': [0.7],\n#               'colsample_bytree': [0.7],\n#               'n_estimators': [500]}\n\n\n# folds = 3\n# param_comb = 5\n\n# kf = KFold(n_splits=10)\n\n# random_search = RandomizedSearchCV(xgb, param_distributions=parameters, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=kf.split(X,y), verbose=3, random_state=1001 )\n# random_search.fit(X, y)\n\n# print(xgb_grid.best_score_)\n# print(xgb_grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import datasets\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score\n\nfrom scipy.stats import uniform\nfrom xgboost import XGBRegressor\n\nxgb = XGBRegressor()\n\nbaseline = cross_val_score(xgb, X, y, scoring='neg_mean_squared_error').mean()\nbaseline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_dist = {\"learning_rate\": uniform(0, 1),\n              \"gamma\": uniform(0, 5),\n              \"max_depth\": range(1,50),\n              \"n_estimators\": range(1,300),\n              \"min_child_weight\": range(1,10)}\n\nrs = RandomizedSearchCV(xgb, param_distributions=param_dist, \n                        scoring='neg_mean_squared_error', n_iter=25)\n\n# Run random search for 25 iterations\nrs.fit(X, y);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# random_search = rs\n# print('\\n All results:')\n# print(random_search.cv_results_)\n# print('\\n Best estimator:')\n# print(random_search.best_estimator_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import GPy\n# import GPyOpt\n# from GPyOpt.methods import BayesianOptimization\n\n# bds = [{'name': 'learning_rate', 'type': 'continuous', 'domain': (0, 1)},\n#         {'name': 'gamma', 'type': 'continuous', 'domain': (0, 5)},\n#         {'name': 'max_depth', 'type': 'discrete', 'domain': (1, 50)},\n#         {'name': 'n_estimators', 'type': 'discrete', 'domain': (1, 300)},\n#         {'name': 'min_child_weight', 'type': 'discrete', 'domain': (1, 10)}]\n\n# # Optimization objective \n# def cv_score(parameters):\n#     parameters = parameters[0]\n#     score = cross_val_score(\n#                 XGBRegressor(learning_rate=parameters[0],\n#                               gamma=int(parameters[1]),\n#                               max_depth=int(parameters[2]),\n#                               n_estimators=int(parameters[3]),\n#                               min_child_weight = parameters[4]), \n#                 X, y, scoring='neg_mean_squared_error').mean()\n#     score = np.array(score)\n#     return score\n\n# optimizer = BayesianOptimization(f=cv_score, \n#                                  domain=bds,\n#                                  model_type='GP',\n#                                  acquisition_type ='EI',\n#                                  acquisition_jitter = 0.05,\n#                                  exact_feval=True, \n#                                  maximize=True)\n\n# # Only 20 iterations because we have 5 initial random points\n# optimizer.run_optimization(max_iter=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimizer.Y_best","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_rs = np.maximum.accumulate(rs.cv_results_['mean_test_score'])\n# y_bo = np.maximum.accumulate(-optimizer.Y).ravel()\n\nprint(f'Baseline neg. MSE = {baseline:.2f}')\nprint(f'Random search neg. MSE = {y_rs[-1]:.2f}')\n# print(f'Bayesian optimization neg. MSE = {y_bo[-1]:.2f}')\n\nplt.plot(y_rs, 'ro-', label='Random search')\n# plt.plot(y_bo, 'bo-', label='Bayesian optimization')\nplt.xlabel('Iteration')\nplt.ylabel('Neg. MSE')\nplt.title('Value of the best sampled CV score');\nplt.legend();","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}