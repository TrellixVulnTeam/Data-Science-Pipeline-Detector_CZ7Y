{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport json\nimport ast\nfrom collections import Counter, OrderedDict\nimport time\nimport datetime\nimport random\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nimport eli5\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":350,"outputs":[{"output_type":"stream","text":"['moviestmdb-datapreparation', 'tmdb-box-office-prediction']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## Import data, append train and test sets for feature engineering and transform some strings into dictionaries"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/tmdb-box-office-prediction/train.csv')\ndf_test = pd.read_csv('../input/tmdb-box-office-prediction/test.csv')","execution_count":351,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=df_train.drop(['revenue'],axis=1).append(df_test).reset_index()","execution_count":352,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from this kernel: https://www.kaggle.com/gravix/gradient-in-a-box\ndict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']","execution_count":353,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_to_dict(df, columns_to_parse):\n    for column in columns_to_parse:\n        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n    return df","execution_count":354,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_clean = text_to_dict(features,dict_columns)","execution_count":355,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - we will also add date variables, since they will be useful right next after this"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fix_date(x):\n    \"\"\"\n    Fixes dates which are in 20xx\n    \"\"\"\n    year = x.split('/')[2]\n    if int(year) <= 19:\n        return x[:-2] + '20' + year\n    else:\n        return x[:-2] + '19' + year","execution_count":356,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_clean.loc[f_clean['release_date'].isnull() == True, 'release_date'] = '01/01/98' ","execution_count":357,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_clean['release_date'] = f_clean['release_date'].apply(lambda x: fix_date(x))\nf_clean['release_date'] = pd.to_datetime(f_clean['release_date'])","execution_count":358,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_clean['year']=pd.DatetimeIndex(f_clean['release_date']).year\nf_clean['month']=pd.DatetimeIndex(f_clean['release_date']).month\nf_clean['yr_mth']=f_clean['year']*100+f_clean['month']","execution_count":359,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_date_months = f_clean[\"year\"].min()*12 + f_clean[\"month\"].min()\n\ndef change_time_to_num(year_month, min_date):\n    date_to_months = year_month.apply(lambda x: int(str(x)[:4]) * 12 + int(str(x)[-2:]))\n    return date_to_months.apply(lambda x: x - min_date)\n\nf_clean['timediff'] = change_time_to_num(f_clean['yr_mth'], min_date_months)","execution_count":360,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We will try to learn embeddings from movie-keyword pairs to generate extra features"},{"metadata":{},"cell_type":"markdown","source":"(closely following this method: https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb)****"},{"metadata":{},"cell_type":"markdown","source":"### Add movie and keyword index dictionaries"},{"metadata":{},"cell_type":"markdown","source":" - before creating movie indices, we should transform the `original_title`, since there are some duplicates that may mix up our results"},{"metadata":{"trusted":true},"cell_type":"code","source":"f_clean['original_title'][f_clean.duplicated('original_title')].shape","execution_count":361,"outputs":[{"output_type":"execute_result","execution_count":361,"data":{"text/plain":"(133,)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_clean['edited_title'] = f_clean['original_title'].copy()\nf_clean['edited_title'][f_clean.duplicated('original_title')] = f_clean['edited_title'].map(str) + ' (' + f_clean['year'].map(str) + ')'","execution_count":362,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_clean['edited_title'][f_clean.duplicated('edited_title')].shape","execution_count":363,"outputs":[{"output_type":"execute_result","execution_count":363,"data":{"text/plain":"(0,)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":" - now that these 133 duplicates were taken care of, we can use `edited_title` to produce the movie indices"},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_index={v: k for k, v in f_clean['edited_title'].to_dict().items()}\nindex_movie=f_clean['edited_title'].to_dict()","execution_count":364,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - let us now produce the indices for keywords"},{"metadata":{"trusted":true},"cell_type":"code","source":"f_clean['list_keywords']=f_clean['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values","execution_count":365,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set([i for j in f_clean['list_keywords'] for i in j])) # check number of unique keywords","execution_count":366,"outputs":[{"output_type":"execute_result","execution_count":366,"data":{"text/plain":"11930"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_keywords=Counter([i for j in f_clean['list_keywords'] for i in j]).most_common()","execution_count":367,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_keywords[0:10] # check most common keywords","execution_count":368,"outputs":[{"output_type":"execute_result","execution_count":368,"data":{"text/plain":"[('woman director', 457),\n ('independent film', 384),\n ('duringcreditsstinger', 350),\n ('based on novel', 312),\n ('murder', 305),\n ('violence', 245),\n ('love', 190),\n ('revenge', 188),\n ('sex', 186),\n ('aftercreditsstinger', 183)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1,5):\n    print(f'There are {len([t[0] for t in count_keywords if t[1] == i])} keywords that appear in {i} movies')\n\nprint(f'There are {len([t[0] for t in count_keywords if t[1] > 4])} keywords that appear in 5 or more movies')\nprint(f'In total, {len([t[0] for t in count_keywords if t[1] > 1])} keywords appear more than once')","execution_count":369,"outputs":[{"output_type":"stream","text":"There are 5963 keywords that appear in 1 movies\nThere are 1978 keywords that appear in 2 movies\nThere are 985 keywords that appear in 3 movies\nThere are 623 keywords that appear in 4 movies\nThere are 2381 keywords that appear in 5 or more movies\nIn total, 5967 keywords appear more than once\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":" - we will disregard keywords that appear only once"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords = [t[0] for t in count_keywords if t[1] > 1]","execution_count":370,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - we will investigate which movies have greater and lower keyword count"},{"metadata":{"trusted":true},"cell_type":"code","source":"kcount=pd.concat([f_clean['edited_title'],\n                  f_clean['list_keywords'].apply(lambda x: [i for i in x if i in keywords]),\n                  f_clean['list_keywords'].apply(lambda x: len([i for i in x if i in keywords]))],\n                 axis=1)\nkcount.columns=['movie','keywords','kcount']","execution_count":371,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kcount.sort_values(by='kcount',ascending=False)[0:10]","execution_count":372,"outputs":[{"output_type":"execute_result","execution_count":372,"data":{"text/plain":"                                           movie  ...   kcount\n1793                      Werckmeister harmóniák  ...      126\n2955                           Brooklyn's Finest  ...       78\n6804                             Southland Tales  ...       51\n2448                                  15 Minutes  ...       42\n3832                            Fair Game (1995)  ...       41\n1463                                  Hard Candy  ...       38\n5139                      Straight Outta Compton  ...       38\n5344  Ein Lied von Liebe und Tod – Gloomy Sunday  ...       37\n4075                                Lost Highway  ...       37\n153                            The Boy Next Door  ...       36\n\n[10 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movie</th>\n      <th>keywords</th>\n      <th>kcount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1793</th>\n      <td>Werckmeister harmóniák</td>\n      <td>[dancing, male nudity, circus, moon, bathroom,...</td>\n      <td>126</td>\n    </tr>\n    <tr>\n      <th>2955</th>\n      <td>Brooklyn's Finest</td>\n      <td>[male nudity, female nudity, tattoo, gambling,...</td>\n      <td>78</td>\n    </tr>\n    <tr>\n      <th>6804</th>\n      <td>Southland Tales</td>\n      <td>[suicide, brother brother relationship, spy, p...</td>\n      <td>51</td>\n    </tr>\n    <tr>\n      <th>2448</th>\n      <td>15 Minutes</td>\n      <td>[new york, female nudity, prison, prostitute, ...</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>3832</th>\n      <td>Fair Game (1995)</td>\n      <td>[bomb, miami, sex, detective, handcuffs, based...</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>1463</th>\n      <td>Hard Candy</td>\n      <td>[suicide, rape, age difference, photographer, ...</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>5139</th>\n      <td>Straight Outta Compton</td>\n      <td>[brother brother relationship, aids, police br...</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>5344</th>\n      <td>Ein Lied von Liebe und Tod – Gloomy Sunday</td>\n      <td>[suicide, male nudity, female nudity, poison, ...</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>4075</th>\n      <td>Lost Highway</td>\n      <td>[schizophrenia, prison, pornography, sadistic,...</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>153</th>\n      <td>The Boy Next Door</td>\n      <td>[male nudity, female nudity, sex, adultery, in...</td>\n      <td>36</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"kcount['movie'].loc[kcount['kcount']==0].count() # check how many movies have zero keywords","execution_count":373,"outputs":[{"output_type":"execute_result","execution_count":373,"data":{"text/plain":"724"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"- let us create keyword indices"},{"metadata":{"trusted":true},"cell_type":"code","source":"kword_index = {kword: idx for idx, kword in enumerate(keywords)}\nindex_kword = {idx: kword for kword, idx in kword_index.items()}","execution_count":374,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let us now create the embedding learning task"},{"metadata":{"trusted":true},"cell_type":"code","source":"pairs = []\n\nfor movie in movie_index.values():\n    pairs.extend((movie,kword_index[kword]) for kword in kcount['keywords'][kcount.index==movie].iloc[0]) ","execution_count":375,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - Setting up a Random training example generator (as in the wikipedia book example)\n - Since the neural network will be trained one batch at a time, the Random training example generator is made in a way that it yields batches of samples each time it is called (which will happen during training of the network)"},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(100)\n\npairs_set = set(pairs)\n\ndef generate_batch(pairs, n_positive = 50, negative_ratio = 1.0, classification = False):\n    \"\"\"Generate batches of samples for training\"\"\"\n    batch_size = n_positive * (1 + negative_ratio)\n    batch = np.zeros((batch_size, 3))\n    \n    # Adjust label based on task\n    if classification:\n        neg_label = 0\n    else:\n        neg_label = -1\n    \n    # This creates a generator\n    while True:\n        # randomly choose positive examples\n        for idx, (movie_id, kword_id) in enumerate(random.sample(pairs, n_positive)):\n            batch[idx, :] = (movie_id, kword_id, 1)\n\n        # Increment idx by 1\n        idx += 1\n        \n        # Add negative examples until reach batch size\n        while idx < batch_size:\n            \n            # random selection\n            random_movie = random.randrange(len(index_movie))\n            random_kword = random.randrange(len(index_kword))\n            \n            # Check to make sure this is not a positive example\n            if (random_movie, random_kword) not in pairs_set:\n                \n                # Add to batch and increment index\n                batch[idx, :] = (random_movie, random_kword, neg_label)\n                idx += 1\n                \n        # Make sure to shuffle order\n        np.random.shuffle(batch)\n        yield {'movie': batch[:, 0], 'kword': batch[:, 1]}, batch[:, 2]","execution_count":376,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Neural network embedding model"},{"metadata":{},"cell_type":"markdown","source":"The neural network will have the following layers:\n1. Input layer (movie and keyword inputs)\n2. Embedding layer (embeddings for movies and keywords. These will be trained to map our inputs into a 50-dimensional vector)\n3. Dot product layer\n4. Reshape layer (to correct the shape)\n5. Dense (in classification): fully connected layer with sigmoid activation to generate output for classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Input, Embedding, Dot, Reshape, Dense\nfrom keras.models import Model","execution_count":377,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef embedding_model(embedding_size = 50, classification = False):\n      \n    # Layer 1: 1-dimensional inputs\n    movie = Input(name = 'movie', shape = [1])\n    kword = Input(name = 'kword', shape = [1])\n    \n    # Layer 2: Embedding the movie (shape will be (None, 1, 50))\n    movie_embedding = Embedding(name = 'movie_embedding',\n                               input_dim = len(movie_index),\n                               output_dim = embedding_size)(movie)\n    \n    # Layer 2: Embedding the keyword (shape will be (None, 1, 50))\n    kword_embedding = Embedding(name = 'kword_embedding',\n                               input_dim = len(kword_index),\n                               output_dim = embedding_size)(kword)\n    \n    # Layer 3: Merge the layers with a dot product along the second axis (shape will be (None, 1, 1))\n    merged = Dot(name = 'dot_product', normalize = True, axes = 2)([movie_embedding, kword_embedding])\n    \n    # Layer 4: Reshape to be a single number (shape will be (None, 1))\n    merged = Reshape(target_shape = [1])(merged)\n    \n    # If classifcation, add extra layer and loss function is binary cross entropy\n    if classification:\n        merged = Dense(1, activation = 'sigmoid')(merged) # layer 5: for classification\n        model = Model(inputs = [movie, kword], outputs = merged)\n        model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    \n    # Otherwise loss function is mean squared error\n    else:\n        model = Model(inputs = [movie, kword], outputs = merged)\n        model.compile(optimizer = 'Adam', loss = 'mse')\n    \n    return model","execution_count":378,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = embedding_model()\nmodel.summary()","execution_count":379,"outputs":[{"output_type":"stream","text":"__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nmovie (InputLayer)              (None, 1)            0                                            \n__________________________________________________________________________________________________\nkword (InputLayer)              (None, 1)            0                                            \n__________________________________________________________________________________________________\nmovie_embedding (Embedding)     (None, 1, 50)        369900      movie[0][0]                      \n__________________________________________________________________________________________________\nkword_embedding (Embedding)     (None, 1, 50)        298350      kword[0][0]                      \n__________________________________________________________________________________________________\ndot_product (Dot)               (None, 1, 1)         0           movie_embedding[0][0]            \n                                                                 kword_embedding[0][0]            \n__________________________________________________________________________________________________\nreshape_4 (Reshape)             (None, 1)            0           dot_product[0][0]                \n==================================================================================================\nTotal params: 668,250\nTrainable params: 668,250\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_positive = 1024\n\ngen = generate_batch(pairs, n_positive, negative_ratio = 2)\n\n# Train\nh = model.fit_generator(gen, epochs = 50, \n                        steps_per_epoch = len(pairs) // n_positive,\n                        verbose = 2)","execution_count":380,"outputs":[{"output_type":"stream","text":"Epoch 1/50\n - 1s - loss: 0.9820\nEpoch 2/50\n - 1s - loss: 0.9328\nEpoch 3/50\n - 1s - loss: 0.8902\nEpoch 4/50\n - 1s - loss: 0.8478\nEpoch 5/50\n - 1s - loss: 0.8174\nEpoch 6/50\n - 1s - loss: 0.7934\nEpoch 7/50\n - 1s - loss: 0.7713\nEpoch 8/50\n - 1s - loss: 0.7404\nEpoch 9/50\n - 1s - loss: 0.6948\nEpoch 10/50\n - 1s - loss: 0.6498\nEpoch 11/50\n - 1s - loss: 0.6023\nEpoch 12/50\n - 1s - loss: 0.5565\nEpoch 13/50\n - 1s - loss: 0.5256\nEpoch 14/50\n - 1s - loss: 0.5152\nEpoch 15/50\n - 1s - loss: 0.5054\nEpoch 16/50\n - 1s - loss: 0.5048\nEpoch 17/50\n - 1s - loss: 0.4920\nEpoch 18/50\n - 1s - loss: 0.4930\nEpoch 19/50\n - 1s - loss: 0.4798\nEpoch 20/50\n - 1s - loss: 0.4831\nEpoch 21/50\n - 1s - loss: 0.4750\nEpoch 22/50\n - 1s - loss: 0.4717\nEpoch 23/50\n - 1s - loss: 0.4664\nEpoch 24/50\n - 1s - loss: 0.4732\nEpoch 25/50\n - 1s - loss: 0.4628\nEpoch 26/50\n - 1s - loss: 0.4682\nEpoch 27/50\n - 1s - loss: 0.4656\nEpoch 28/50\n - 1s - loss: 0.4678\nEpoch 29/50\n - 1s - loss: 0.4594\nEpoch 30/50\n - 1s - loss: 0.4556\nEpoch 31/50\n - 1s - loss: 0.4555\nEpoch 32/50\n - 1s - loss: 0.4549\nEpoch 33/50\n - 1s - loss: 0.4606\nEpoch 34/50\n - 1s - loss: 0.4483\nEpoch 35/50\n - 1s - loss: 0.4560\nEpoch 36/50\n - 1s - loss: 0.4460\nEpoch 37/50\n - 1s - loss: 0.4426\nEpoch 38/50\n - 1s - loss: 0.4543\nEpoch 39/50\n - 1s - loss: 0.4440\nEpoch 40/50\n - 1s - loss: 0.4452\nEpoch 41/50\n - 1s - loss: 0.4487\nEpoch 42/50\n - 1s - loss: 0.4480\nEpoch 43/50\n - 1s - loss: 0.4559\nEpoch 44/50\n - 1s - loss: 0.4453\nEpoch 45/50\n - 1s - loss: 0.4408\nEpoch 46/50\n - 1s - loss: 0.4433\nEpoch 47/50\n - 1s - loss: 0.4508\nEpoch 48/50\n - 1s - loss: 0.4454\nEpoch 49/50\n - 1s - loss: 0.4438\nEpoch 50/50\n - 1s - loss: 0.4528\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Extract the embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_layer = model.get_layer('movie_embedding')\nmovie_weights = movie_layer.get_weights()[0]\nmovie_weights.shape","execution_count":381,"outputs":[{"output_type":"execute_result","execution_count":381,"data":{"text/plain":"(7398, 50)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":" - each movie can now be represented on the 50-dimensional vector obtained based on keywords\n - to be able to calculate similarities based on cosine similarity, we should first normalize embeddings so that they have the dot product of two movie embeddings is the cosine similarity\n - this normalization is achieved by dividing each vector by the square root of the sum of squared components"},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_weights = movie_weights / np.linalg.norm(movie_weights, axis = 1).reshape((-1, 1))","execution_count":382,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def similar_movies(name, n=10):\n    \n    dists = np.dot(movie_weights, movie_weights[movie_index[name]])\n    sorted_dists = np.argsort(dists)\n    closest = sorted_dists[-n:]\n    max_width = max([len(index_movie[c]) for c in closest])\n    \n    for c in reversed(closest):\n        print(f'Movie: {index_movie[c]:{max_width + 2}} Similarity: {dists[c]:.{2}}')","execution_count":383,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"similar_movies('Avatar')","execution_count":384,"outputs":[{"output_type":"stream","text":"Movie: Avatar                    Similarity: 1.0\nMovie: Aliens                    Similarity: 0.75\nMovie: Treasure Planet           Similarity: 0.74\nMovie: Apollo 13                 Similarity: 0.72\nMovie: The Martian               Similarity: 0.72\nMovie: Starship Troopers         Similarity: 0.71\nMovie: Interstellar              Similarity: 0.7\nMovie: Alien: Covenant           Similarity: 0.7\nMovie: Sunshine                  Similarity: 0.69\nMovie: Star Trek Into Darkness   Similarity: 0.69\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = ['membed_'+str(i) for i in range(1,51)]\n\nmovie_embeds = pd.DataFrame(movie_weights,columns=col_names)","execution_count":385,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_clean = pd.concat([f_clean,movie_embeds],axis=1)","execution_count":386,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FE: Now we will again separate the feature data set into train and test sets and work on feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([f_clean.iloc[0:df_train.shape[0]],df_train['revenue']],axis=1)\ntest = f_clean.iloc[df_train.shape[0]:]","execution_count":387,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - start by examining the target variable:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['revenue'].describe().T","execution_count":388,"outputs":[{"output_type":"execute_result","execution_count":388,"data":{"text/plain":"count    3.000000e+03\nmean     6.672585e+07\nstd      1.375323e+08\nmin      1.000000e+00\n25%      2.379808e+06\n50%      1.680707e+07\n75%      6.891920e+07\nmax      1.519558e+09\nName: revenue, dtype: float64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":" - some movies with revenue of 1\\$, which seems very unusual. Let's see how many movies are under 300\\$ revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['original_title','revenue']][train['revenue']<300].sort_values(by=['revenue'])","execution_count":389,"outputs":[{"output_type":"execute_result","execution_count":389,"data":{"text/plain":"                                     original_title  revenue\n1917                                The Merry Widow        1\n1754                                   Mute Witness        1\n1874                                        Vermist        1\n347                         The Wind in the Willows        1\n695                                       Tere Naam        2\n2383                                      Borsalino        3\n1541                                    Все и сразу        3\n334                                           Saamy        3\n2117                                 American Adobo        4\n1346                                   East of Eden        5\n2490                        Never Talk to Strangers        6\n1190     He-Man and She-Ra: The Secret of the Sword        7\n2582                                          Kopps        8\n639                                         Pollock        8\n2251                                      Bodyguard        8\n665                                    Elektra Luxx       10\n280                                            Bats       10\n1141                                           북촌방향       11\n312                                     The Cookout       12\n450                                 Chasing Liberty       12\n2759                  Dou Sing 2 - Gai Tau Dou Sing       13\n2399                                   カイジ2 人生奪回ゲーム       15\n150                                      Windwalker       18\n2090                                       Deadfall       18\n269     Glass: A Portrait of Philip in Twelve Parts       20\n1884                                     In the Cut       23\n2032                             A Farewell to Arms       25\n2874                                Nice Guy Johnny       25\n498                                    Lake of Fire       25\n1138                                    The Getaway       30\n1006                                    Zyzzyx Road       30\n2433                                    Black Sheep       32\n1281                             Death at a Funeral       46\n1007                        Billy Gardell: Halftime       60\n664                                 My Summer Story       70\n2474                                            小时代       79\n1198                      Every Which Way But Loose       85\n579                               Blood on the Moon       88\n2577  The Life and Death of 9413, a Hollywood Extra       97\n2864           Die Angst des Tormanns beim Elfmeter      100\n2255                                   Lost & Found      100\n1161                  Sunrise: A Song of Two Humans      121\n1479                            A Band Called Death      125\n1240                                    Khiladi 786      126\n1800                             The Cherry Orchard      135\n15                                          Šišanje      198\n1948                                  मालामाल वीकली      204\n152                                      Paperhouse      241","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original_title</th>\n      <th>revenue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1917</th>\n      <td>The Merry Widow</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1754</th>\n      <td>Mute Witness</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1874</th>\n      <td>Vermist</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>347</th>\n      <td>The Wind in the Willows</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>695</th>\n      <td>Tere Naam</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2383</th>\n      <td>Borsalino</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1541</th>\n      <td>Все и сразу</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>334</th>\n      <td>Saamy</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2117</th>\n      <td>American Adobo</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1346</th>\n      <td>East of Eden</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2490</th>\n      <td>Never Talk to Strangers</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1190</th>\n      <td>He-Man and She-Ra: The Secret of the Sword</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2582</th>\n      <td>Kopps</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>639</th>\n      <td>Pollock</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2251</th>\n      <td>Bodyguard</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>665</th>\n      <td>Elektra Luxx</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>280</th>\n      <td>Bats</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>1141</th>\n      <td>북촌방향</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>312</th>\n      <td>The Cookout</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>450</th>\n      <td>Chasing Liberty</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>2759</th>\n      <td>Dou Sing 2 - Gai Tau Dou Sing</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>2399</th>\n      <td>カイジ2 人生奪回ゲーム</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>150</th>\n      <td>Windwalker</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>2090</th>\n      <td>Deadfall</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>269</th>\n      <td>Glass: A Portrait of Philip in Twelve Parts</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>1884</th>\n      <td>In the Cut</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>2032</th>\n      <td>A Farewell to Arms</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>2874</th>\n      <td>Nice Guy Johnny</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>498</th>\n      <td>Lake of Fire</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1138</th>\n      <td>The Getaway</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>1006</th>\n      <td>Zyzzyx Road</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>2433</th>\n      <td>Black Sheep</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>1281</th>\n      <td>Death at a Funeral</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>1007</th>\n      <td>Billy Gardell: Halftime</td>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>664</th>\n      <td>My Summer Story</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>2474</th>\n      <td>小时代</td>\n      <td>79</td>\n    </tr>\n    <tr>\n      <th>1198</th>\n      <td>Every Which Way But Loose</td>\n      <td>85</td>\n    </tr>\n    <tr>\n      <th>579</th>\n      <td>Blood on the Moon</td>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th>2577</th>\n      <td>The Life and Death of 9413, a Hollywood Extra</td>\n      <td>97</td>\n    </tr>\n    <tr>\n      <th>2864</th>\n      <td>Die Angst des Tormanns beim Elfmeter</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>2255</th>\n      <td>Lost &amp; Found</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>1161</th>\n      <td>Sunrise: A Song of Two Humans</td>\n      <td>121</td>\n    </tr>\n    <tr>\n      <th>1479</th>\n      <td>A Band Called Death</td>\n      <td>125</td>\n    </tr>\n    <tr>\n      <th>1240</th>\n      <td>Khiladi 786</td>\n      <td>126</td>\n    </tr>\n    <tr>\n      <th>1800</th>\n      <td>The Cherry Orchard</td>\n      <td>135</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Šišanje</td>\n      <td>198</td>\n    </tr>\n    <tr>\n      <th>1948</th>\n      <td>मालामाल वीकली</td>\n      <td>204</td>\n    </tr>\n    <tr>\n      <th>152</th>\n      <td>Paperhouse</td>\n      <td>241</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":" - some of these values may be correct. However, let us assume that a minimum 300\\$ is more acceptable"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['revenue'][train['revenue']<300] = 300","execution_count":390,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \"\"\"Entry point for launching an IPython kernel.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":" - `director` might be important?"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['director'] = train['crew'].apply(lambda x: [i['name'] for i in x if i['job'] == 'Director']).apply(pd.Series).iloc[:,0]","execution_count":391,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['revenue'].groupby(train['director']).count().describe().T","execution_count":392,"outputs":[{"output_type":"execute_result","execution_count":392,"data":{"text/plain":"count    1857.000000\nmean        1.606893\nstd         1.221448\nmin         1.000000\n25%         1.000000\n50%         1.000000\n75%         2.000000\nmax        11.000000\nName: revenue, dtype: float64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":" - there are 1,857 directors in the training set, which means most directors only show up once. These will not be very informative to discriminate revenue\n  - how many directors show up twice or more?"},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_rev= pd.concat([train['revenue'].groupby(train['director']).count(),\n           train['revenue'].groupby(train['director']).sum(),\n           train['revenue'].groupby(train['director']).mean(),\n           train['revenue'].groupby(train['director']).max(),\n           train['revenue'].groupby(train['director']).min()], axis=1).reset_index()\n\ndir_rev.columns = ['director','N_movies','Total_rev','Average_rev','Highest_rev','Lowest_rev']","execution_count":393,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_rev[dir_rev['N_movies']>1].sort_values(by=['Average_rev'],ascending=False)[0:10]","execution_count":394,"outputs":[{"output_type":"execute_result","execution_count":394,"data":{"text/plain":"              director  N_movies     ...      Highest_rev  Lowest_rev\n935        Joss Whedon         2     ...       1519557910  1405403694\n718         James Gunn         2     ...        863416141   773328629\n253    Carlos Saldanha         3     ...        886686817   500188435\n1337     Peter Jackson         7     ...       1021103568    29359216\n89      Andrew Stanton         3     ...       1028570889   284139100\n1441      Rob Marshall         2     ...       1045713802   162242962\n205          Brad Bird         3     ...        694713380   209154322\n586       George Lucas         3     ...        850000000     2437000\n517   Francis Lawrence         3     ...        653428261   230884728\n1160       Michael Bay         8     ...       1123746996    69411370\n\n[10 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>director</th>\n      <th>N_movies</th>\n      <th>Total_rev</th>\n      <th>Average_rev</th>\n      <th>Highest_rev</th>\n      <th>Lowest_rev</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>935</th>\n      <td>Joss Whedon</td>\n      <td>2</td>\n      <td>2924961604</td>\n      <td>1.462481e+09</td>\n      <td>1519557910</td>\n      <td>1405403694</td>\n    </tr>\n    <tr>\n      <th>718</th>\n      <td>James Gunn</td>\n      <td>2</td>\n      <td>1636744770</td>\n      <td>8.183724e+08</td>\n      <td>863416141</td>\n      <td>773328629</td>\n    </tr>\n    <tr>\n      <th>253</th>\n      <td>Carlos Saldanha</td>\n      <td>3</td>\n      <td>2047816032</td>\n      <td>6.826053e+08</td>\n      <td>886686817</td>\n      <td>500188435</td>\n    </tr>\n    <tr>\n      <th>1337</th>\n      <td>Peter Jackson</td>\n      <td>7</td>\n      <td>4450044134</td>\n      <td>6.357206e+08</td>\n      <td>1021103568</td>\n      <td>29359216</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>Andrew Stanton</td>\n      <td>3</td>\n      <td>1834021849</td>\n      <td>6.113406e+08</td>\n      <td>1028570889</td>\n      <td>284139100</td>\n    </tr>\n    <tr>\n      <th>1441</th>\n      <td>Rob Marshall</td>\n      <td>2</td>\n      <td>1207956764</td>\n      <td>6.039784e+08</td>\n      <td>1045713802</td>\n      <td>162242962</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>Brad Bird</td>\n      <td>3</td>\n      <td>1535309794</td>\n      <td>5.117699e+08</td>\n      <td>694713380</td>\n      <td>209154322</td>\n    </tr>\n    <tr>\n      <th>586</th>\n      <td>George Lucas</td>\n      <td>3</td>\n      <td>1501835328</td>\n      <td>5.006118e+08</td>\n      <td>850000000</td>\n      <td>2437000</td>\n    </tr>\n    <tr>\n      <th>517</th>\n      <td>Francis Lawrence</td>\n      <td>3</td>\n      <td>1469661999</td>\n      <td>4.898873e+08</td>\n      <td>653428261</td>\n      <td>230884728</td>\n    </tr>\n    <tr>\n      <th>1160</th>\n      <td>Michael Bay</td>\n      <td>8</td>\n      <td>3906752453</td>\n      <td>4.883441e+08</td>\n      <td>1123746996</td>\n      <td>69411370</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":" - perhaps having a high-profile director helps a movie becoming more successful. We could try to extract some features from this\n - let's add summary variables such as average revenue of the director and (High-Low)/Average to give a sense of revenue dispersion\n - we will calculate these measures based on cast members in the training set, but we will then trickle that information down to the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_rev['Hi_lo_rev'] = (dir_rev['Highest_rev'] - dir_rev['Lowest_rev']) / dir_rev['Average_rev']","execution_count":395,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_clean['director'] = f_clean['crew'].apply(lambda x: [i['name'] for i in x if i['job'] == 'Director']).apply(pd.Series).iloc[:,0]","execution_count":396,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_clean = f_clean.merge(dir_rev[['director','Average_rev','Hi_lo_rev']],how='left',on='director')\nf_clean = f_clean.rename(columns={'Average_rev': 'Dir_avg_rev', 'Hi_lo_rev': 'Dir_HL_rev'})","execution_count":397,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - what about the relation between `cast` and revenue?"},{"metadata":{"trusted":true},"cell_type":"code","source":"cast_list = train['cast'].apply(lambda x: [i['name'] for i in x])","execution_count":398,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set([i for j in cast_list for i in j])) # over 38,000 unique cast members in the training set","execution_count":399,"outputs":[{"output_type":"execute_result","execution_count":399,"data":{"text/plain":"38588"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cast_revenue = []\n\nfor i,r in enumerate(train['revenue'].values):\n    cast_revenue.extend((act,r) for act in cast_list[cast_list.index==i].iloc[0])\n    \ncast_revenue = pd.DataFrame(list(cast_revenue), columns=['Name','Revenue'])\n\ncast_rev_sum = pd.concat([cast_revenue.groupby(['Name']).count(),\n                          cast_revenue.groupby(['Name']).sum(),\n                          cast_revenue.groupby(['Name']).mean(),\n                          cast_revenue.groupby(['Name']).max(),\n                          cast_revenue.groupby(['Name']).min()], axis=1)\n\ncast_rev_sum.columns = ['N_movies','Total_rev','Average_rev','Highest_rev','Lowest_rev']","execution_count":400,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cast_rev_sum.sort_values(by=['Highest_rev'],ascending=False)[0:10]","execution_count":401,"outputs":[{"output_type":"execute_result","execution_count":401,"data":{"text/plain":"                        N_movies     ...      Lowest_rev\nName                                 ...                \nBrent Reichert                 1     ...      1519557910\nRobert B. Schneider IV         1     ...      1519557910\nJeff Seich                     1     ...      1519557910\nMaria Perossa                  1     ...      1519557910\nJeff Wolfe                     3     ...        78054825\nChristina Shaffer              1     ...      1519557910\nKelly Ruble                    1     ...      1519557910\nJeffrey Feingold               2     ...       154026136\nWalter Perez                   1     ...      1519557910\nKelley Robins                  1     ...      1519557910\n\n[10 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>N_movies</th>\n      <th>Total_rev</th>\n      <th>Average_rev</th>\n      <th>Highest_rev</th>\n      <th>Lowest_rev</th>\n    </tr>\n    <tr>\n      <th>Name</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Brent Reichert</th>\n      <td>1</td>\n      <td>1519557910</td>\n      <td>1.519558e+09</td>\n      <td>1519557910</td>\n      <td>1519557910</td>\n    </tr>\n    <tr>\n      <th>Robert B. Schneider IV</th>\n      <td>1</td>\n      <td>1519557910</td>\n      <td>1.519558e+09</td>\n      <td>1519557910</td>\n      <td>1519557910</td>\n    </tr>\n    <tr>\n      <th>Jeff Seich</th>\n      <td>1</td>\n      <td>1519557910</td>\n      <td>1.519558e+09</td>\n      <td>1519557910</td>\n      <td>1519557910</td>\n    </tr>\n    <tr>\n      <th>Maria Perossa</th>\n      <td>1</td>\n      <td>1519557910</td>\n      <td>1.519558e+09</td>\n      <td>1519557910</td>\n      <td>1519557910</td>\n    </tr>\n    <tr>\n      <th>Jeff Wolfe</th>\n      <td>3</td>\n      <td>1702813638</td>\n      <td>5.676045e+08</td>\n      <td>1519557910</td>\n      <td>78054825</td>\n    </tr>\n    <tr>\n      <th>Christina Shaffer</th>\n      <td>1</td>\n      <td>1519557910</td>\n      <td>1.519558e+09</td>\n      <td>1519557910</td>\n      <td>1519557910</td>\n    </tr>\n    <tr>\n      <th>Kelly Ruble</th>\n      <td>1</td>\n      <td>1519557910</td>\n      <td>1.519558e+09</td>\n      <td>1519557910</td>\n      <td>1519557910</td>\n    </tr>\n    <tr>\n      <th>Jeffrey Feingold</th>\n      <td>2</td>\n      <td>1673584046</td>\n      <td>8.367920e+08</td>\n      <td>1519557910</td>\n      <td>154026136</td>\n    </tr>\n    <tr>\n      <th>Walter Perez</th>\n      <td>1</td>\n      <td>1519557910</td>\n      <td>1.519558e+09</td>\n      <td>1519557910</td>\n      <td>1519557910</td>\n    </tr>\n    <tr>\n      <th>Kelley Robins</th>\n      <td>1</td>\n      <td>1519557910</td>\n      <td>1.519558e+09</td>\n      <td>1519557910</td>\n      <td>1519557910</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":" - the relation between cast members and revenue could be slightly trickier than that of directors and revenue? Should try to summarize this somehow\n - we will try to use summary measures such as number of actors which have participated in movies with extremly high or relatively low revenue, or the minimum highest revenue any cast member has achieved in the past\n - we will calculate these measures based on cast members in the training set, but we will then trickle that information down to the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"cast_rev_sum['rev99p'] = (cast_rev_sum['Highest_rev'] >cast_revenue['Revenue'].quantile(0.99))*1\ncast_rev_sum['rev20p'] = (cast_rev_sum['Highest_rev'] <cast_revenue['Revenue'].quantile(0.20))*1","execution_count":402,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_cast_list = f_clean['cast'].apply(lambda x: [i['name'] for i in x])","execution_count":403,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_cast = []\n\nfor i,r in enumerate(f_clean['id'].values):\n    id_cast.extend((r,act) for act in full_cast_list[full_cast_list.index==i].iloc[0])\n    \nid_cast = pd.DataFrame(list(id_cast), columns=['id','Name'])","execution_count":404,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cast_rev_movie = id_cast.merge(cast_rev_sum,how='left',on='Name')","execution_count":405,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cast_rev_summary = pd.concat([cast_rev_movie.groupby(['id']).sum()['rev99p'],\n                             cast_rev_movie.groupby(['id']).sum()['rev20p'],\n                             cast_rev_movie.groupby(['id']).min()['Highest_rev']],\n                             axis=1).reset_index()","execution_count":406,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_clean = f_clean.merge(cast_rev_summary,how='left',on='id')\nf_clean = f_clean.rename(columns={'rev99p': 'N_cast_99p', 'rev20p': 'N_cast_20p','Highest_rev':'Cast_low_bound'})","execution_count":407,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - examining `production_companies` and `revenue`\n - we will treat this variable similarly to the `cast` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"companies_list = train['production_companies'].apply(lambda x: [i['name'] for i in x])","execution_count":408,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_revenue = []\n\nfor i,r in enumerate(train['revenue'].values):\n    comp_revenue.extend((comp,r) for comp in companies_list[companies_list.index==i].iloc[0])\n    \ncomp_revenue = pd.DataFrame(list(comp_revenue), columns=['Company','Revenue'])\n\ncomp_rev_sum = pd.concat([comp_revenue.groupby(['Company']).count(),\n                          comp_revenue.groupby(['Company']).sum(),\n                          comp_revenue.groupby(['Company']).mean(),\n                          comp_revenue.groupby(['Company']).max(),\n                          comp_revenue.groupby(['Company']).min()], axis=1)\n\ncomp_rev_sum.columns = ['N_movies','Total_rev','Average_rev','Highest_rev','Lowest_rev']","execution_count":409,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_rev_sum['rev75p'] = (comp_rev_sum['Highest_rev'] >comp_revenue['Revenue'].quantile(0.75))*1\ncomp_rev_sum['rev25p'] = (comp_rev_sum['Highest_rev'] <comp_revenue['Revenue'].quantile(0.25))*1","execution_count":410,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_comp_list = f_clean['production_companies'].apply(lambda x: [i['name'] for i in x])","execution_count":411,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_comp = []\n\nfor i,r in enumerate(f_clean['id'].values):\n    id_comp.extend((r,comp) for comp in full_comp_list[full_comp_list.index==i].iloc[0])\n    \nid_comp = pd.DataFrame(list(id_comp), columns=['id','Company'])","execution_count":412,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_rev_movie = id_comp.merge(comp_rev_sum,how='left',on='Company')","execution_count":413,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_rev_summary = pd.concat([comp_rev_movie.groupby(['id']).sum()['rev75p'],\n                             comp_rev_movie.groupby(['id']).sum()['rev25p'],\n                             comp_rev_movie.groupby(['id']).min()['Highest_rev']],\n                             axis=1).reset_index()","execution_count":414,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_clean = f_clean.merge(comp_rev_summary,how='left',on='id')\nf_clean = f_clean.rename(columns={'rev75p': 'N_comp_75p', 'rev25p': 'N_comp_25p','Highest_rev':'Comp_low_bound'})","execution_count":415,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - there seem to be some zero values for `budget`, which is odd. We will simply replace them by the training set median, due to time constraints"},{"metadata":{"trusted":true},"cell_type":"code","source":"med_budget = train['budget'].median()","execution_count":416,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_clean['budget'] = f_clean['budget'].replace(0, med_budget)","execution_count":417,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - there are still some `NaN` values in some of the created columns and in `runtime`. We will replace them by the mode or median"},{"metadata":{"trusted":true},"cell_type":"code","source":"f_clean['runtime'] = f_clean['runtime'].fillna(train['runtime'].median())\nf_clean['Dir_avg_rev'] = f_clean['Dir_avg_rev'].fillna(f_clean['Dir_avg_rev'].median())\nf_clean['Dir_HL_rev'] = f_clean['Dir_HL_rev'].fillna(f_clean['Dir_HL_rev'].median())\nf_clean['N_cast_99p'] = f_clean['N_cast_99p'].fillna(0)\nf_clean['N_cast_20p'] = f_clean['N_cast_20p'].fillna(0)\nf_clean['Cast_low_bound'] = f_clean['Cast_low_bound'].fillna(f_clean['Cast_low_bound'].median())\nf_clean['N_comp_75p'] = f_clean['N_comp_75p'].fillna(1)\nf_clean['N_comp_25p'] = f_clean['N_comp_25p'].fillna(0)\nf_clean['Comp_low_bound'] = f_clean['Comp_low_bound'].fillna(f_clean['Comp_low_bound'].median())\n","execution_count":418,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - we will extract some features from this notebook https://www.kaggle.com/joanalpinto/moviestmdb-datapreparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"extra_train = pd.read_csv('../input/moviestmdb-datapreparation/train_prep.csv')\nextra_test = pd.read_csv('../input/moviestmdb-datapreparation/test_prep.csv')","execution_count":419,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_add = ['has_collection','num_cast','num_crew','genres_name_Drama','genres_name_Comedy','genres_name_Thriller',\n            'genres_name_Action','genres_name_Romance','genres_name_Crime','genres_name_Adventure',\n            'genres_name_Horror','genres_name_Science Fiction','genres_name_Family',\n            'production_countries_name_United States of America',\n            'spoken_languages_name_English','spoken_languages_name_Français','spoken_languages_name_Español']","execution_count":420,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([f_clean.iloc[0:df_train.shape[0]],extra_train[cols_add],df_train['revenue']],axis=1)\ntest = pd.concat([f_clean.iloc[df_train.shape[0]:].reset_index(),extra_test[cols_add]],axis=1)","execution_count":460,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = ['index','id','belongs_to_collection','genres','homepage','imdb_id','original_language',\n                'original_title','overview','poster_path','production_companies','production_countries',\n               'release_date','spoken_languages','status','tagline','title','Keywords','cast','crew',\n               'edited_title','list_keywords','director']","execution_count":461,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['revenue'],axis=1).drop(cols_to_drop,axis=1)\ny = np.log1p(train['revenue'])\nX_test = test.drop(['level_0'],axis=1).drop(cols_to_drop,axis=1)","execution_count":471,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)","execution_count":472,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\n\nlgbm = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\nlgbm.fit(X_train, y_train, \n        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n        verbose=1000, early_stopping_rounds=200)","execution_count":473,"outputs":[{"output_type":"stream","text":"Training until validation scores don't improve for 200 rounds.\n[1000]\ttraining's rmse: 0.313195\tvalid_1's rmse: 0.386213\n[2000]\ttraining's rmse: 0.212507\tvalid_1's rmse: 0.372151\nEarly stopping, best iteration is:\n[2203]\ttraining's rmse: 0.198274\tvalid_1's rmse: 0.370867\n","name":"stdout"},{"output_type":"execute_result","execution_count":473,"data":{"text/plain":"LGBMRegressor(bagging_fraction=0.9, bagging_freq=1, bagging_seed=11,\n       boosting='gbdt', boosting_type='gbdt', class_weight=None,\n       colsample_bytree=1.0, feature_fraction=0.9, importance_type='split',\n       lambda_l1=0.2, learning_rate=0.01, max_depth=5, metric='rmse',\n       min_child_samples=20, min_child_weight=0.001, min_data_in_leaf=20,\n       min_split_gain=0.0, n_estimators=20000, n_jobs=-1, nthread=4,\n       num_leaves=30, objective='regression', random_state=None,\n       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n       subsample_for_bin=200000, subsample_freq=0, verbosity=-1)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(lgbm, feature_filter=lambda x: x != '<BIAS>')","execution_count":474,"outputs":[{"output_type":"execute_result","execution_count":474,"data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <style>\n    table.eli5-weights tr:hover {\n        filter: brightness(85%);\n    }\n</style>\n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n    <thead>\n    <tr style=\"border: none;\">\n        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n    </tr>\n    </thead>\n    <tbody>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.8462\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Cast_low_bound\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 96.34%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0747\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Dir_avg_rev\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 97.70%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0386\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                N_cast_20p\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 98.67%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0175\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Comp_low_bound\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.21%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0083\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                num_cast\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.61%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0031\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                budget\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.65%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0026\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                popularity\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.83%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0009\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                num_crew\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.85%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0008\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                Dir_HL_rev\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.90%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0005\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                membed_22\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.90%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0004\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                month\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.91%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0004\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                runtime\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.92%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0003\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                membed_45\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.93%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0002\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                membed_32\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.93%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0002\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                membed_3\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.93%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0002\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                membed_27\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.94%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0002\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                membed_34\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.94%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0002\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                membed_42\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.94%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0002\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                membed_18\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.94%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0002\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                membed_10\n            </td>\n        </tr>\n    \n    \n        \n            <tr style=\"background-color: hsl(120, 100.00%, 99.94%); border: none;\">\n                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n                    <i>&hellip; 62 more &hellip;</i>\n                </td>\n            </tr>\n        \n    \n    </tbody>\n</table>\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = linear_model.Lasso(alpha=0.1)\nprint(np.sqrt(-cross_val_score(lasso, X, y, cv=10, scoring='neg_mean_squared_error')))","execution_count":475,"outputs":[{"output_type":"stream","text":"[2.10195367 2.28832129 2.0321788  2.39715568 2.07826831 1.87932137\n 2.37074068 2.2762209  1.99636467 1.72257812]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso.fit(X,y)","execution_count":476,"outputs":[{"output_type":"execute_result","execution_count":476,"data":{"text/plain":"Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n   normalize=False, positive=False, precompute=False, random_state=None,\n   selection='cyclic', tol=0.0001, warm_start=False)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_lasso = lasso.predict(X_test)","execution_count":479,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/tmdb-box-office-prediction/sample_submission.csv')\nsub['revenue'] = np.expm1(preds_lasso)\nsub.to_csv(\"lasso_sub.csv\", index=False)","execution_count":480,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}