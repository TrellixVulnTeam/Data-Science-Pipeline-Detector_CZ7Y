{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV,StratifiedShuffleSplit,StratifiedKFold,RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.svm import SVC,SVR\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import auc,roc_auc_score, accuracy_score, confusion_matrix, f1_score, precision_score, \\\nrecall_score, matthews_corrcoef, precision_recall_curve\nimport xgboost as xgb\nfrom xgboost import XGBClassifier,XGBRegressor\nfrom sklearn.preprocessing import PolynomialFeatures,LabelEncoder\nfrom sklearn.feature_selection import VarianceThreshold\npd.set_option('display.max_columns', 100)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.metrics import make_scorer, r2_score, mean_squared_error\nimport lightgbm as lgb\nimport eli5\nimport shap\n\n#Credits:\n#a lot of the tasks done here is taken from below.\n#https://www.kaggle.com/artgor/eda-feature-engineering-and-model-interpretation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nimport ast\nfrom collections import Counter\nimport plotly.graph_objs as go\nimport plotly.offline as py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_to_dic(df,columns):\n    for column in columns:\n        df[column]=df[column].apply(lambda x:{} if pd.isnull(x) else ast.literal_eval(x))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tmdb-box-office-prediction/train.csv')\ntest = pd.read_csv('../input/tmdb-box-office-prediction/test.csv')\ntrain_original=train.copy()\ntest_original=test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\ntrain.head(1) # there are a lot of columns that have srings ( and dictionaries within them). Wil have to think how to fix them","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['belongs_to_collection'][0] # this is a string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_to_dic(df,columns):\n    for column in columns:\n        df[column]=df[column].apply(lambda x:{} if pd.isnull(x) else ast.literal_eval(x))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=text_to_dic(train,dict_columns)\ntest=text_to_dic(test,dict_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, value in enumerate(train[dict_columns[0]][0:4]):\n    print(i,value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['collection_name']=train['belongs_to_collection'].apply(lambda x:x[0]['name'] if x!={} else 0)\ntrain['has_collection_name']=train['belongs_to_collection'].apply(lambda x:0 if x=={} else 1)\n#delete the original column\ntrain.drop(['belongs_to_collection','collection_name'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['collection_name']=test['belongs_to_collection'].apply(lambda x:x[0]['name'] if x!={} else 0)\ntest['has_collection_name']=test['belongs_to_collection'].apply(lambda x:0 if x=={} else 1)\n#delete the original column\ntest.drop(['belongs_to_collection','collection_name'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['genres']=train['genres'].apply(lambda x: [i['name'] for i in x] if x!={} else [])\ntrain['genre_count']=train['genres'].apply(lambda x: len(x) if x!={} else 0)\n#test\ntest['genres']=test['genres'].apply(lambda x: [i['name'] for i in x] if x!={} else [])\ntest['genre_count']=test['genres'].apply(lambda x: len(x) if x!={} else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_genres=[i for i in train['genres']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\ntext = ' '.join([i for j in list_of_genres for i in j])\nwordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top genres')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_genres_test=[i for i in test['genres']]\nplt.figure(figsize = (10, 6))\ntext = ' '.join([i for j in list_of_genres_test for i in j])\nwordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top genres')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_genres=set()\nfor i in list_of_genres:\n    all_genres=all_genres.union(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_genres_test=set()\nfor i in list_of_genres_test:\n    all_genres_test=all_genres_test.union(i)\nprint(all_genres_test),print(len(all_genres_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_common_genre=Counter([i for j in list_of_genres for i in j]).most_common(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for genre in most_common_genre:\n    train['genre_'+genre[0]]=train['genres'].apply(lambda x: 1 if genre[0] in x else 0)\n    test['genre_'+genre[0]]=test['genres'].apply(lambda x: 1 if genre[0] in x else 0)\n    \n#drop genre colum\n\ntrain.drop('genres',axis=1,inplace=True)\ntest.drop('genres',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in train['production_companies'][0:4]:\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['production_cos']=train['production_companies'].apply(lambda x :[i['name']  for i in x] if x!={} else [] )\ntest['production_cos']=test['production_companies'].apply(lambda x :[i['name']  for i in x] if x!={} else [] )\n#take the count\ntrain['Count_production_cos']=train['production_cos'].apply( lambda x: 0 if x==[] else len(x) )\ntest['Count_production_cos']=test['production_cos'].apply( lambda x: 0 if x==[] else len(x) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets create binary columns for th top 10 most occuring production firms\nmost_common_production_cos=Counter([i for j in train['production_cos'] for i in j]).most_common(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in most_common_production_cos:\n    train['Prod_cos_'+i[0]]=train['production_cos'].apply(lambda x: 1 if i[0] in x else 0)\n    test['Prod_cos_'+i[0]]=test['production_cos'].apply(lambda x: 1 if i[0] in x else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['production_companies','production_cos'],axis=1,inplace=True)\ntest.drop(['production_companies','production_cos'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in train['production_countries'][18:20]:\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['prod_countries']=train['production_countries'].apply(lambda x:[i['name'] for i in x] if x!={} else [])\ntest['prod_countries']=test['production_countries'].apply(lambda x:[i['name'] for i in x] if x!={} else [])\ntrain['prod_country_count']=train['production_countries'].apply(lambda x:0 if x=={} else len(x))\ntest['prod_country_count']=test['production_countries'].apply(lambda x:0 if x=={} else len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_common_production_countries=Counter([i for j in train['prod_countries'] for i in j]).most_common(10) #take top 10 countries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in most_common_production_countries:\n    train['prod_country_'+i[0]]=train['prod_countries'].apply(lambda x:1 if i[0] in x else 0)\n    test['prod_country_'+i[0]]=test['prod_countries'].apply(lambda x:1 if i[0] in x else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['production_countries','prod_countries'],axis=1,inplace=True)\ntest.drop(['production_countries','prod_countries'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Language']=train['spoken_languages'].apply(lambda x:[i['name'] for i in x] if x!={} else [])\ntest['Language']=test['spoken_languages'].apply(lambda x:[i['name'] for i in x] if x!={} else [])\n#count\ntrain['Count_Language']=train['Language'].apply(lambda x:len(x) if x!={} else 0)\ntest['Count_Language']=test['spoken_languages'].apply(lambda x:len(x) if x!={} else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_common_languages=Counter([i for j in train['Language'] for i in j]).most_common(9) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in most_common_languages:\n    train['Language_'+i[0]]=train['Language'].apply(lambda x: 1 if i[0] in x else 0)\n    test['Language_'+i[0]]=test['Language'].apply(lambda x: 1 if i[0] in x else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['spoken_languages','Language'],axis=1,inplace=True)\ntest.drop(['spoken_languages','Language'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Keywords']=train['Keywords'].apply(lambda x:[i['name'] for i in x] if x!={} else [])\ntest['Keywords']=test['Keywords'].apply(lambda x:[i['name'] for i in x] if x!={} else [])\n#count\ntrain['Count_Keywords']=train['Keywords'].apply(lambda x:len(x) if x!={} else 0)\ntest['Count_Keywords']=test['Keywords'].apply(lambda x:len(x) if x!={} else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_Keywords_train=[i for i in train['Keywords']]\nplt.figure(figsize = (10, 6))\ntext = ' '.join(['_'.join(i.split(' ')) for j in list_of_Keywords_train for i in j]) \n#Since keywords for the same records may have spaces, added a _ to avoid mixing them with words from other records\nwordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top Keywords')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_common_Keywords=Counter([i for j in train['Keywords'] for i in j]).most_common(10) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in most_common_Keywords:\n    train['Keywords_'+i[0]]=train['Keywords'].apply(lambda x:1 if i[0] in x else 0)\n    test['Keywords_'+i[0]]=test['Keywords'].apply(lambda x:1 if i[0] in x else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop('Keywords',axis=1,inplace=True)\ntest.drop('Keywords',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Cast_name']=train['cast'].apply(lambda x:[i['name'] for i in x] if x!={} else [])\ntest['Cast_name']=test['cast'].apply(lambda x:[i['name'] for i in x] if x!={} else [])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#count cast\ntrain['Cast_count']=train['Cast_name'].apply(lambda x:0 if x==[] else len(x))\ntest['Cast_count']=test['Cast_name'].apply(lambda x:0 if x==[] else len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_common_Cast_names=Counter([i for j in train['Cast_name'] for i in j]).most_common(15)\nfor i in most_common_Cast_names:\n    train['Cast_names_'+i[0]]=train['Cast_name'].apply(lambda x: 1 if i[0] in x else 0)\n    test['Cast_names_'+i[0]]=test['Cast_name'].apply(lambda x: 1 if i[0] in x else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gender in cast\n# gender of all the cast is not important. I wanted to check if the actor in lead role is male or female But there is no\n#way to get this info. We can however have a count of #of male, #of Female and #of Unasigned gender as 3 column in the data\ntrain['Cast_gender']=train['cast'].apply(lambda x:[i['gender'] for i in x]if x !=[] else [0]) #male 2,Female 1 & 0 is Unassigned\ntest['Cast_gender']=test['cast'].apply(lambda x:[i['gender'] for i in x] if x !=[] else [0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Cast_male_2']=train['Cast_gender'].apply(lambda x:sum([i==2 for i in x]) if x!=[] else 0)\ntrain['Cast_female_1']=train['Cast_gender'].apply(lambda x:sum([i==1 for i in x]) if x!=[] else 0)\ntrain['Cast_gender_UnAss0']=train['Cast_gender'].apply(lambda x:sum([i==0 for i in x]) if x!=[] else 0)\n\n#for test\n\ntest['Cast_male_2']=test['Cast_gender'].apply(lambda x:sum([i==2 for i in x]) if x!=[] else 0)\ntest['Cast_female_1']=test['Cast_gender'].apply(lambda x:sum([i==1 for i in x]) if x!=[] else 0)\ntest['Cast_gender_UnAss0']=test['Cast_gender'].apply(lambda x:sum([i==0 for i in x]) if x!=[] else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['cast','Cast_name','Cast_gender'],axis=1,inplace=True)\ntest.drop(['cast','Cast_name','Cast_gender'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['crew_gender']=train['crew'].apply(lambda x:[i['gender'] for i in x] if x!={} else [])\ntest['crew_gender']=test['crew'].apply(lambda x:[i['gender'] for i in x] if x!={} else [])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['crew_gender_0']=train['crew_gender'].apply(lambda x:sum([i==0 for i in x]) if x!=[] else 0)\ntrain['crew_gender_1']=train['crew_gender'].apply(lambda x:sum([i==1 for i in x]) if x!=[] else 0)\ntrain['crew_gender_2']=train['crew_gender'].apply(lambda x:sum([i==2 for i in x]) if x!=[] else 0)\ntest['crew_gender_0']=test['crew_gender'].apply(lambda x:sum([i==0 for i in x]) if x!=[] else 0)\ntest['crew_gender_1']=test['crew_gender'].apply(lambda x:sum([i==1 for i in x]) if x!=[] else 0)\ntest['crew_gender_2']=test['crew_gender'].apply(lambda x:sum([i==2 for i in x]) if x!=[] else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['crew','crew_gender'],axis=1,inplace=True)\ntest.drop(['crew','crew_gender'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['log_budget'] = np.log1p(train['budget'])\ntest['log_budget'] = np.log1p(test['budget'])\ntrain['runtime'].fillna(value=train['runtime'].mean(),inplace=True)\ntest['runtime'].fillna(value=train['runtime'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#since there are nulls, lets create a binary to mark 1 /0 corrosponding to if Homepage is present or not\ntrain['has_homepage'] = 1\ntrain.loc[train['homepage'].isnull(),'has_homepage']=0\ntest['has_homepage'] = 1\ntest.loc[test['homepage'].isnull(),'has_homepage']=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Original title ahs the title of the movie. This may not make sense for non english movies and i doubt if this will be \n#useful for the Model. However lets just take a look at the word cloud\nplt.figure(figsize = (10, 6))\ntext = ' '.join(train['original_title'].values)\nwordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=1200, height=1000).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top genres')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.loc[test['release_date'].isnull(),'release_date']='01/01/98'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fix_date(x):\n    \"\"\"\n    fix dates\n    \"\"\"\n    year=x.split('/')[2]\n    if int(year)<20:\n        return x[:-2]+'20'+year\n    else:\n        return x[:-2]+'19'+year\n    \ntrain['release_date']=train['release_date'].apply(lambda x:fix_date(x))  \ntest['release_date']=test['release_date'].apply(lambda x:fix_date(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['release_date'] = pd.to_datetime(train['release_date'])\ntest['release_date'] = pd.to_datetime(test['release_date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_date(df):\n    date_parts = [\"year\", \"weekday\", \"month\", 'weekofyear', 'day', 'quarter']\n    for part in date_parts:\n        part_col = 'release_date' + \"_\" + part\n        df[part_col] = getattr(df['release_date'].dt, part).astype(int)\n    \n    return df\n\ntrain = process_date(train)\ntest = process_date(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def new_features(df):\n    df['budget_to_popularity'] = df['budget'] / df['popularity']\n    df['budget_to_runtime'] = df['budget'] / df['runtime']     #runtime has some 0's so wil result in INF\n    \n    # some features from https://www.kaggle.com/somang1418/happy-valentines-day-and-keep-kaggling-3\n    df['_budget_year_ratio'] = df['budget'] / (df['release_date_year'] * df['release_date_year'])\n    df['_releaseYear_popularity_ratio'] = df['release_date_year'] / df['popularity']\n    df['_releaseYear_popularity_ratio2'] = df['popularity'] / df['release_date_year']\n    #df.groupby(\"release_date_year\")[\"runtime\"].transform('mean') \n    #this gives the value of the avg corresponding to the release_date_year\n    df['runtime_to_mean_year'] = df['runtime'] / df.groupby(\"release_date_year\")[\"runtime\"].transform('mean') \n    #runtime has some 0's so wil result in INF\n    df['popularity_to_mean_year'] = df['popularity'] / df.groupby(\"release_date_year\")[\"popularity\"].transform('mean')\n    df['budget_to_mean_year'] = df['budget'] / df.groupby(\"release_date_year\")[\"budget\"].transform('mean')\n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['budget']==0].shape[0]/train.shape[0] #27% in train has Budget=0\ntest[test['budget']==0].shape[0]/test.shape[0]    #27% in test has Budget=0\n#before running the above feature engineering steps, we have to fix budget=0 as this is definitely not correct.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['popularity'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(train['popularity']<1)/train.shape[0]\nsum(test['popularity']<1)/test.shape[0] #7-8% in train/test has popularity<1,max value is 250\n#so fix popularity before creating the feature enginnered variables.\nsns.distplot(train['popularity']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets replace the popularity by its Z values\nfrom sklearn import preprocessing\nscaler=preprocessing.StandardScaler()\ntrain['popularity']=5+scaler.fit_transform(np.array(train['popularity']).reshape(-1,1))#added 5 to avoid the values being 0 (z can have 0 as a value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replace runtime ==0 with avg of runtime in the release year\ntrain.loc[(train['runtime']==0),'runtime']=train.groupby(['release_date_year'])['runtime'].transform('mean')[train[(train['runtime']==0)].index]\ntest.loc[(test['runtime']==0),'runtime']=test.groupby(['release_date_year'])['runtime'].transform('mean')[test[(test['runtime']==0)].index]\n\n#replace budget==0 with avg budget of the year and the original_language.\ntrain.loc[(train['budget']==0),'budget']=\\\n            train.groupby(['release_date_year','original_language'])['budget'].transform('mean')[train[(train['budget']==0)].index]\ntest.loc[(test['budget']==0),'budget']=\\\n            test.groupby(['release_date_year','original_language'])['budget'].transform('mean')[test[(test['budget']==0)].index]\n#if there are still budget==0, fill them with avg of just the language\ntrain.loc[(train['budget']==0),'budget']=\\\n            train.groupby(['original_language'])['budget'].transform('mean')[train[(train['budget']==0)].index]\ntest.loc[(test['budget']==0),'budget']=\\\n            test.groupby(['original_language'])['budget'].transform('mean')[test[(test['budget']==0)].index]\n\n#and if still use just the release year t0 fill\ntrain.loc[(train['budget']==0),'budget']=\\\n            train.groupby(['release_date_year'])['budget'].transform('mean')[train[(train['budget']==0)].index]\ntest.loc[(test['budget']==0),'budget']=\\\n            test.groupby(['release_date_year'])['budget'].transform('mean')[test[(test['budget']==0)].index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=new_features(train)\ntest=new_features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Surprisingly films releases on Wednesdays and on Thursdays tend to have a higher revenue.\ntrain.drop('release_date',axis=1,inplace=True)\ntest.drop('release_date',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Text Columns. From columns that are pure texts lets take the len of each col as a feature\nfor col in ['title', 'tagline', 'overview', 'original_title']:\n    train['len_' + col] = train[col].fillna('').apply(lambda x: len(str(x)))\n    train['words_' + col] = train[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n    #train = train.drop(col, axis=1)\n    test['len_' + col] = test[col].fillna('').apply(lambda x: len(str(x)))\n    test['words_' + col] = test[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n    #test = test.drop(col, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['homepage', 'imdb_id','original_title','overview','poster_path',\\\n                    'status','tagline', 'title'], axis=1)\ntest = test.drop(['homepage', 'imdb_id','original_title','overview','poster_path',\\\n                    'status','tagline', 'title'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check is all values are same in any column\nfor col in train.columns:\n    if train[col].nunique() == 1:\n        print(col)\n        train = train.drop([col], axis=1)\n        test = test.drop([col], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(train.dtypes[train.dtypes.values=='object'].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in list(train.dtypes[train.dtypes.values=='object'].index):\n    le=LabelEncoder()\n    le.fit(list(train[i].fillna(''))+list(test[i].fillna('')))\n    train[i] = le.transform(train[i].fillna('').astype(str))\n    test[i]  = le.transform(test[i].fillna('').astype(str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop(['id', 'revenue'], axis=1)\ny_train = train['revenue']\ny_train_log=np.log1p(y_train)\nX_test = test.drop(['id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape,X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n#since columns names have speacial character this is needed\n#M a big fan of HINDI Movies-->Language_हिन्दी'\nX_train.rename({'Language_日本語':'Language_Japan','Language_普通话':'Language_China','Language_हिन्दी':'Language_India'},axis=1,inplace=True)\nX_train.columns=[\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\nX_train = X_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\nX_test.rename({'Language_日本語':'Language_Japan','Language_普通话':'Language_China','Language_हिन्दी':'Language_India'},axis=1,inplace=True)\nX_test.columns=[\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test.columns]\nX_test = X_test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(y_true, y_pred):\n    diff = mean_squared_error(y_true, y_pred)\n    return diff**0.5\nmy_scorer = make_scorer(rmse,greater_is_better=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Provide a K-fold function that generate out-of-fold predictions for train data.\nclass Modelling():\n    def __init__(self,X,y,test_X,folds,N):\n        self.X=X\n        self.y=y\n        self.test_X=test_X\n        self.folds=folds\n        self.N=N\n     \n    def Single_Model(self,Regressor): #for all other Models like LInear,NB ,KNN etc\n        stacker_train = np.zeros((self.X.shape[0], 1))\n        stacker_test  = np.zeros(self.test_X.shape[0])        \n        for index, (trn_idx,val_idx) in enumerate(self.folds.split(self.X)):\n            trn_x, val_x = self.X[trn_idx], self.X[val_idx]\n            trn_y, val_y = self.y[trn_idx], self.y[val_idx]\n            print('Train model in fold {}'.format(index+1))           \n            Regressor.fit(trn_x,np.log1p(trn_y))\n            val_pred = np.expm1(Regressor.predict(val_x))\n            stacker_train[val_idx,0]=val_pred\n            val_rmse=rmse(val_y, val_pred)            \n            print('fold {} RMSE score on VAL is {:.6f}'.format(index+1, val_rmse))\n            #for test\n            pred_test= np.expm1(Regressor.predict(self.test_X))\n            stacker_test+=(pred_test/self.N)\n            \n        #evaluate for entire train data (oof)\n        train_rmse=rmse(self.y,stacker_train)\n        print(\"CV score on TRAIN (OOF) is RMSE: {}\".format(train_rmse))   \n        return stacker_test,stacker_train        \n        \n        \n        \n    def SingleRF_oof(self,params):\n        clf_rf=RandomForestRegressor(**rf_params)\n        stacker_train = np.zeros((self.X.shape[0], 1))\n        stacker_test  = np.zeros(self.test_X.shape[0])\n        for index, (trn_idx,val_idx) in enumerate(self.folds.split(self.X,self.y)):\n            trn_x, val_x = self.X[trn_idx], self.X[val_idx]\n            trn_y, val_y = self.y[trn_idx], self.y[val_idx]\n            print('Train model in fold {}'.format(index+1))         \n            clf_rf.fit(trn_x,trn_y)\n            val_pred = clf_rf.predict(val_x)\n            stacker_train[val_idx,0]=val_pred\n            val_rmse=rmse(val_y, val_pred)    \n                        \n            print('fold {} RMSE score on VAL is {:.6f}'.format(index+1,val_rmse))\n            #for test\n            pred_test= clf_rf.predict(self.test_X)\n            stacker_test+=(pred_test/self.N)\n        #evaluate for entire train data (oof)\n        train_rmse=rmse(self.y,stacker_train)\n        print(\"CV score on TRAIN (OOF) is RMSE: {}\".format(train_rmse))   \n        return stacker_test,stacker_train    \n\n    \n    def SingleXGB_oof(self,params,num_boost_round):\n        stacker_train = np.zeros((self.X.shape[0], 1))\n        stacker_test=np.zeros(self.test_X.shape[0])\n        dtest=xgb.DMatrix(self.test_X)\n        for index, (trn_idx,val_idx) in enumerate(self.folds.split(self.X)):\n            trn_x, val_x = self.X[trn_idx], self.X[val_idx]\n            trn_y, val_y = self.y[trn_idx], self.y[val_idx]\n            dtrn = xgb.DMatrix(data=trn_x, label=np.log1p(trn_y))\n            dval = xgb.DMatrix(data=val_x, label=np.log1p(val_y))\n            print('Train model in fold {}'.format(index+1)) \n            cv_model = xgb.train(params=params,dtrain=dtrn,num_boost_round=num_boost_round\\\n                                 ,evals=[(dtrn, 'train'), (dval, 'val')],verbose_eval=10,early_stopping_rounds=200)\n                        \n            pred_test = np.expm1(cv_model.predict(dtest, ntree_limit=cv_model.best_ntree_limit))\n            stacker_test+=(pred_test/self.N)\n            val_pred=np.expm1(cv_model.predict(dval, ntree_limit=cv_model.best_ntree_limit))\n            stacker_train[val_idx,0]=val_pred\n            val_rmse=rmse(val_y, val_pred)\n            \n            print('fold {} RMSE score on VAL is {:.6f}'.format(index+1, val_rmse))\n            \n        #evaluate for entire train data (oof)\n        train_rmse=rmse(self.y,stacker_train)\n        print(\"CV score on TRAIN (OOF) is RMSE: {}\".format(train_rmse))   \n        return stacker_test,stacker_train\n    \n    \n    def SingleLGBM_oof(self,params,num_boost_round,colnames,importance_plot=False): #passing the col names to print the Feature imp\n        stacker_train = np.zeros((self.X.shape[0], 1))\n        stacker_test=np.zeros(self.test_X.shape[0])\n        feature_importance =pd.DataFrame()\n        for index, (trn_idx,val_idx) in enumerate(self.folds.split(self.X,self.y)):\n            trn_x, val_x = self.X[trn_idx], self.X[val_idx]\n            trn_y, val_y = self.y[trn_idx], self.y[val_idx]\n\n            print('Train model in fold {}'.format(index+1)) \n            lgb_train = lgb.Dataset(trn_x,np.log1p(trn_y))\n            lgb_val = lgb.Dataset(val_x, np.log1p(val_y), reference=lgb_train)\n            \n            lgb_model = lgb.train(params,\n                        lgb_train,\n                        num_boost_round=num_boost_round,\n                        valid_sets=lgb_val,\n                        early_stopping_rounds=200,\n                        verbose_eval=10)\n            \n            val_pred=np.expm1(lgb_model.predict(val_x))\n            val_rmse=rmse(val_y, val_pred)\n            print('fold {} RMSE score on VAL is {:.6f}'.format(index+1, val_rmse))\n            stacker_train[val_idx,0]=val_pred\n\n            pred_test = np.expm1(lgb_model.predict(self.test_X))\n            stacker_test+=(pred_test/self.N)\n            #feature importance\n            fold_importance = pd.DataFrame()\n            \n            fold_importance[\"feature\"] = colnames\n            fold_importance[\"importance\"] = lgb_model.feature_importance()\n            fold_importance[\"fold\"] = index+1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n        \n        if importance_plot:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:30].index\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n            plt.figure(figsize=(12, 9));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGBM Features (avg over folds,Top Few)');\n                \n        \n        #evaluate for entire train data (oof)\n        train_rmse=rmse(self.y,stacker_train)\n        print(\"CV score on TRAIN (OOF) is RMSE: {}\".format(train_rmse))   \n        return stacker_test,stacker_train\n    \n    \n    def SingleCatBoost_oof(self,params): #simple catboost without the cat columns\n        stacker_train = np.zeros((self.X.shape[0], 1))\n        stacker_test=np.zeros(self.test_X.shape[0])\n        \n        for index, (trn_idx,val_idx) in enumerate(self.folds.split(self.X)):\n            trn_x, val_x = self.X[trn_idx], self.X[val_idx]\n            trn_y, val_y = self.y[trn_idx], self.y[val_idx]\n            print('Train model in fold {}'.format(index+1))              \n                \n            cat_model = CatBoostRegressor(**params)\n            cat_model.fit(trn_x,np.log1p(trn_y),eval_set=(val_x,np.log1p(val_y)),use_best_model=True,verbose=False)\n            val_pred = np.expm1(cat_model.predict(val_x))\n            stacker_train[val_idx,0]=val_pred\n            val_rmse=rmse(val_y, val_pred)            \n            print('fold {} RMSE score on VAL is {:.6f}'.format(index+1, val_rmse))\n            #for test\n            pred_test= np.expm1(cat_model.predict(self.test_X))\n            stacker_test+=(pred_test/self.N)\n            \n        #evaluate for entire train data (oof)\n        train_rmse=rmse(self.y,stacker_train)\n        print(\"CV score on TRAIN (OOF) is RMSE: {}\".format(train_rmse))   \n        return stacker_test,stacker_train   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#call the models\nfrom sklearn.model_selection import KFold\nNumber_of_folds = 5\n#We have to make sure same K fold splits are used for all Models. This avoids Overfitting and Leakage\nfolds = KFold(n_splits=Number_of_folds, shuffle=True, random_state=2017)\nmodelling_object = Modelling(X=X_train.values, y=y_train.values, test_X=X_test.values, folds=folds, N=Number_of_folds)\n#NOTE above that we are calling train, test all as ..values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Call LightGBM\ncat_params= {\n    'iterations':10000,\n    'learning_rate':0.004,\n   'depth':5,\n    'eval_metric':'RMSE',\n    'colsample_bylevel':0.8,\n    'random_seed' : 2017,\n    'bagging_temperature' : 0.2,\n    'early_stopping_rounds':200\n} ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_stacked_cat,stacker_train_cat=\\\nmodelling_object.SingleCatBoost_oof(params=cat_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Call LightGBM\nlgbm_params= {#\"max_depth\": 5,          #max depth for tree model\n              \"learning_rate\" : 0.02,\n              #\"num_leaves\": 25,        #max number of leaves in one tree\n              # 'feature_fraction':0.6,  #LightGBM will randomly select part of features on each tree node\n               'bagging_fraction':0.6,    #randomly select part of data without resampling\n              # 'max_drop': 5,         #used only in dart,max number of dropped trees during one boosting iteration\n               'lambda_l1': 1,\n               'lambda_l2': 0.01,\n              'min_child_samples':400,  #minimal number of data in one leaf\n                'max_bin':20, #max number of bins that feature values will be bucketed in. Higher value--> Overfitting\n                'subsample':0.6,  #randomly select part of data without resampling\n                'colsample_bytree':0.8, #same as feature_fraction\n               'boosting_type': 'dart',   #options are gbdt(gradientboosting decision trees), rf,dart,goss\n               'task': 'train'}  #weight of labels with positive class\n\ntest_pred_stacked_lgbm,stacker_train_lgbm=\\\nmodelling_object.SingleLGBM_oof(params=lgbm_params,num_boost_round=10000,colnames=X_train.columns,importance_plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Call XGB\nparams_for_xgb = {\n    'objective': 'reg:squarederror',  #the learning task and the corresponding learning objective\n    'eval_metric': 'rmse',            #Evaluation metrics for validation data\n    'eta': 0.04,          #learning_rate          \n    'max_depth': 3,       #Maximum depth of a tree. High will make the model more complex and more likely to overfit.\n    'min_child_weight': 5, #[0,inf] Higher the value,lesser the number of splits\n    'gamma': 1.5,       #Minimum loss reduction required to make a further partition on a leaf node of the tree    'subsample': 0.8,    #Subsample ratio of the training instances\n    'colsample_bytree': 0.6,  #subsample ratio of columns when constructing each tree\n    'alpha': 5,  #L1 regularization term on weights\n    'lambda': 5,\n    'subsample':0.6,\n    'seed': 2017}\n\ntest_pred_stacked_xgb,stacker_train_xgb=modelling_object.SingleXGB_oof(params=params_for_xgb,num_boost_round=10000) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_params = {'n_estimators': 2000,\n              'max_features': 'auto', #, 'sqrt','auto'\n              #'criterion':  'gini', #'entropy',\n              'max_depth': 30,\n              'min_samples_leaf': 15,\n            # 'min_samples_split':5,\n            # 'class_weight':'balanced',\n             'random_state':0\n            }\n\ntest_pred_stacked_rf,stacker_train_rf=modelling_object.SingleRF_oof(params=rf_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stacking\ncolumns=['catboost','xgb','lgbm','rf']\ntrain_pred_df_list=[stacker_train_cat,stacker_train_xgb, stacker_train_lgbm, stacker_train_rf]\ntest_pred_df_list=[test_pred_stacked_cat,test_pred_stacked_xgb,test_pred_stacked_lgbm,test_pred_stacked_rf]\nlv1_train_df=pd.DataFrame(columns=columns)\nlv1_test_df=pd.DataFrame(columns=columns)\nfor i in range(len(columns)):\n    lv1_train_df[columns[i]]=train_pred_df_list[i][:,0]\n    lv1_test_df[columns[i]]=test_pred_df_list[i]\n    \nlv1_train_df['Y']=y_train #add the dependendt variable to training","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l2_modelling_object = Modelling(X=lv1_train_df.drop('Y',axis=1).values, y=lv1_train_df['Y'].values, \\\n                                test_X=lv1_test_df.values, folds=folds, N=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_stacked_lgbm_L2,stacker_train_lgbm_L2=\\\nl2_modelling_object.SingleLGBM_oof(params=lgbm_params,num_boost_round=10000,colnames=columns,importance_plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGB Model scores 1.98 on LB.( Best out of all models) If I edit the data \n#like a lot of kernels have done, i.e. correct the revenue/budget in train dataset for many movies, i probably will get better at LB\n#Stacking doesnot help and in fact scores poorly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=pd.DataFrame({'id':test['id'],'revenue':test_pred_stacked_lgbm_L2})\nresults.to_csv('All_models_stacked_lgbm_L2.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLinks\nFileLinks('.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=pd.DataFrame({'id':test['id'],'revenue':test_pred_stacked_lm_L2})\nresults.to_csv('Linear_Model_L2.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLinks\nFileLinks('.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}