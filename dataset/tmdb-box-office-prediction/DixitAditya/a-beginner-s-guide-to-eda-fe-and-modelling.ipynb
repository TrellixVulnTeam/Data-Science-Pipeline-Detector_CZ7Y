{"cells":[{"metadata":{},"cell_type":"markdown","source":"****“Give them pleasure. The same pleasure they have when they wake up from a nightmare.” ****\n                                                                    \n                                                                    Alfred Hitchcock"},{"metadata":{},"cell_type":"markdown","source":"**Import the Libraries and load the datasets**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport ast\nsns.set(rc={'figure.figsize':(15,5)})","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have the first look at the datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)\nprint(train.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.head(1).T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's first combine the train and test datasets so that the EDA and feature engineering is done only once."},{"metadata":{"trusted":true},"cell_type":"code","source":"combined = pd.concat([train,test],axis=0,sort=False)\nprint(combined.shape)\ncombined.index = range(len(combined))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok. So there are many columns with dictionary datatype represented as strings. Let's fix that. \nThanks to [Andrew's Kernel](https://www.kaggle.com/artgor/eda-feature-engineering-and-model-interpretation), from where I picked up the below function."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\ndef text_to_dict(df):\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n    return df\n        \ncombined = text_to_dict(combined)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we extract the weekday, month and year features from the release date. "},{"metadata":{"trusted":true},"cell_type":"code","source":"t = pd.DatetimeIndex(combined['release_date'])\ncombined['release_date'] = t\ncombined['release_year'] = t.year\ncombined['release_month'] = t.month\ncombined['release_day_of_week'] = t.weekday","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now look at the count of movies released in every decade."},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.groupby([combined.release_year // 10 * 10,'status'])['id'].count().unstack().fillna(0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is something fishy here. There are movies showing up for 2060s, albeit the counts are very low. Since most of the movies in each decade are already released, we are misrepresenting the dates. Python converted 1960 to 2060 and so on. Let's look at the count of movies released in 2010-2020 in each month."},{"metadata":{"trusted":true},"cell_type":"code","source":"combined[(combined.release_year > 2010) & (combined.release_year < 2020)].groupby(['release_year','release_month'])['id'].count().unstack().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the data seems to be updated upto August 2017 and for the movies released from then to 2060s ,we can subtract 100 years. We will then fix the variables created earlier - Month, Year and Weekday."},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.loc[combined.release_date > '2017-08-31','release_date'] = combined.loc[combined.release_date > '2017-08-31','release_date'].apply(lambda x: x - pd.DateOffset(years=100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = pd.DatetimeIndex(combined['release_date'])\ncombined['release_date'] = t\ncombined['release_year'] = t.year\ncombined['release_month'] = t.month\ncombined['release_day_of_week'] = t.weekday\ncombined['profit'] = combined.revenue - combined.budget","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.groupby([combined.release_year // 10 * 10,'status'])['id'].count().unstack().fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot decade wise revenue, budget and profits!"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncombined.groupby(combined.release_year // 10 * 10)[['revenue','budget','profit']].mean().plot(kind='bar',title='Revenue,Budget and Profit - Decade Wise')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The trend looks surprizing. After 1970s, the revenue and profit from movies went down in 1980s. We will revisit this later. The budget seems to be in line with expectations.\n\nLet's now look at dictionary columns and create meaningful features out of them, starting with genres column.\nSome movies have multiple genres(as seen in combined.head() above) so let's create a flag column for each genre."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get list of unique genres and create columns for each, i.e. one hot encoding\ndef dict_to_cols(colname):\n     for i in range(len(combined)):\n        #running for each row\n         for j in range(len(combined[colname][i])):\n            #creating and filling values for each genre column based on current value\n             combined.loc[i,str(colname) + \"_\" + str(combined.loc[i,colname][j]['name'])] = 1\n    \n    #fill 0 value where a genre doesn't exist for a movie\n     t_col = combined.columns.str.startswith(colname)\n     combined.iloc[:,t_col] = combined.iloc[:,t_col].fillna(0)\n\ndict_to_cols('genres')\ncombined.drop('genres',axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get list of unique languages and create columns for each, i.e. one hot encoding\ndef dict_to_cols(colname,value):\n     for i in range(len(combined)):\n        #running for each row\n         for j in range(len(combined[colname][i])):\n            #creating and filling values for each genre column based on current value\n             combined.loc[i,str(colname) + \"_\" + str(combined.loc[i,colname][j][value])] = 1\n    \n    #fill 0 value where a genre doesn't exist for a movie\n     t_col = combined.columns.str.startswith(colname)\n     combined.iloc[:,t_col] = combined.iloc[:,t_col].fillna(0)\n\ndict_to_cols('spoken_languages','iso_639_1')\ncombined.drop('spoken_languages',axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.head(10).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For production countries, we just take the count of production countries for each movie. "},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['prod_countries_count'] = combined.production_countries.apply(lambda x:len(x))\ncombined.drop('production_countries',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Belongs to collection doesn't look very useful, but has null values for a few movies. Let's create a flag variable to denote whether it's populated or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['belongs_to_collection_flag'] = combined['belongs_to_collection'].apply(lambda x: 1 if len(x) > 0 else 0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's explore the homepage column. Since the internet became popular in mid 90's, I wouldn't expect the movies released before then to have it populated. Let's create a flag columns to check whether it exists or not. And look at whether it has any impact on revenue."},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['homepage_exists'] = combined['homepage'].notnull().astype(int)\ncombined.drop('homepage',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.groupby([combined.release_year // 10 * 10,'homepage_exists'])['revenue'].mean().unstack().fillna(0).plot(kind='bar',title='Decade wise counts')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This makes sense now! The movies which were released before the advent of internet, and still have a homepage are the ones which are still in popular culture. The homepages were probably created by the fanbase. But let's leave it at that. The combination of release year and homepage_exists will probably take care of revenue projections.\n\nLet's now look at the overview_length. It's a summary of the movie plot and I don't think it would be very helpful in revenue prediction per say. But let's also not ignore it completely. Movies with longer description might indicate a more complex plot, or just better attention to detail. Let's create a feature of the length of this field."},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['overview_length'] = combined.overview.apply(lambda x:len(str(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's drop the unwanted columns\ncombined.drop(['id','belongs_to_collection','release_date','overview','original_title','poster_path','tagline','title'],axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['production_companies'].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have the following variables left to explore. Let's count the number of entries in dictionary datatypes for them.\n* production_companies\n* Keywords\n* cast\n* crew\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.Keywords.apply(lambda x: [x[i]['name'] for i in range(len(x))])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will revist the below code later"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# def get_names_from_dict(column):\n#     df = pd.DataFrame(columns=['id','year','name','revenue','budget'])\n#     for i in range(len(combined)):\n#     #running for each row\n#         for j in range(len(combined[column][i])):\n#         #creating and filling values for the column based on current value\n#             df = df.append({'name':combined.loc[i,column][j]['name'],\n#                             'revenue':combined.loc[i,'revenue'],\n#                             'budget':combined.loc[i,'budget'],\n#                            'year':combined.loc[i,'release_year'],\n#                            'id':combined.loc[i,'imdb_id']},\n#                            ignore_index = True)\n#     return(df)\n# df = get_names_from_dict('production_companies')\n\n# try:\n#     df['production_roi'] = df.revenue / df.budget\n# except:\n#     df['production_roi'] = 0.00\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df[['budget','revenue']].head().T\n\n# df.groupby(['name',df.year // 10 * 10])['revenue'].mean().fillna(0).sort_values(ascending=False)\n# We now have the information that the movies from a given production house in a given decade earned what revenues on average\n# For movies with multiple production houses, we can take the average of revenues in that decade and create a proxy for it\n# This gives us an idea of whether the production house is big or small\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.drop(['imdb_id','original_language','production_companies','status','Keywords','cast','crew','profit'] \\\n              ,axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined1 = combined.copy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined1.loc[combined1.runtime.isna(),'runtime'] = combined1.runtime.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(combined1.loc[combined.release_year.isna(),:])\nprint(combined1.loc[combined.release_year.isna(),:])\nprint(combined1.loc[combined.release_year.isna(),:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined1.loc[combined1.release_year.isna(),'release_year'] = 2000\ncombined1.loc[-combined1.release_year.isna(),'release_month'] = 5\ncombined1.loc[combined1.release_day_of_week.isna(),'release_day_of_week'] = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final = combined1.loc[-combined1.revenue.isna()].copy()\ntest_final = combined1.loc[combined1.revenue.isna()].copy()\nprint(train_final.shape)\nprint(test_final.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_final.drop('revenue',axis=1,inplace=True)\ny = train_final['revenue']\nX = train_final.drop('revenue',axis=1,inplace=True)\n\nprint(test_final.shape)\nprint(X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_holdout, y_train, y_holdout = train_test_split(train_final, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_holdout.shape)\nprint(y_train.head())\nprint(y_holdout.head())\nprint(train_final.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=17)\n\nforest_params = {\n#     'max_depth': range(10, 21,5)\n'max_features': range(20, 120,10)\n                }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_grid = GridSearchCV(forest, forest_params,\n                           cv=4, n_jobs=-1, verbose=True,scoring='neg_mean_squared_error')\nforest_grid.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_grid.best_params_, forest_grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"holdout_pred = forest_grid.predict(X_holdout)\n# print(holdout_pred)\nfrom sklearn.metrics import mean_squared_error\n100*mean_squared_error(y_holdout, holdout_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = forest_grid.predict(test_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = pd.read_csv('../input/sample_submission.csv')\npred['revenue'] = test_pred\npred.to_csv(\"RFR.csv\", index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor\n# from sklearn import cross_validation, metrics   \nfrom sklearn.model_selection import GridSearchCV\n","execution_count":150,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nxgb_params = {\n    'learning_rate': [.01,.02,.03,.04,.05,.1,.15,.22,.3]}\n\nxgb1 = XGBRegressor(n_estimators=100, gamma=0, subsample=0.75,\n                           colsample_bytree=1,eval_metric='rmse',objective= 'reg:linear',\n                      seed=27)\n\nxgb_grid = GridSearchCV(xgb1, xgb_params,\n                           cv=5, n_jobs=-1, verbose=True,scoring='neg_mean_squared_error')\nxgb_grid.fit(X_train, y_train)","execution_count":156,"outputs":[{"output_type":"stream","text":"Fitting 5 folds for each of 9 candidates, totalling 45 fits\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:   18.5s finished\n/opt/conda/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n  if getattr(data, 'base', None) is not None and \\\n","name":"stderr"},{"output_type":"execute_result","execution_count":156,"data":{"text/plain":"GridSearchCV(cv=5, error_score='raise-deprecating',\n       estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, eval_metric='rmse', gamma=0,\n       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27, silent=True,\n       subsample=0.75),\n       fit_params=None, iid='warn', n_jobs=-1,\n       param_grid={'learning_rate': [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.15, 0.22, 0.3]},\n       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n       scoring='neg_mean_squared_error', verbose=True)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(xgb_grid.best_params_, xgb_grid.best_score_)\nholdout_pred = xgb_grid.predict(X_holdout)\nmean_squared_error(y_holdout, holdout_pred)\n\n","execution_count":157,"outputs":[{"output_type":"stream","text":"{'learning_rate': 0.05} -6723073473742575.0\n","name":"stdout"},{"output_type":"execute_result","execution_count":157,"data":{"text/plain":"4676300390803395.0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nmean_squared_error(y_holdout, holdout_pred)\ntest_pred = xgb_grid.predict(test_final)\npred_XG = pd.read_csv('../input/sample_submission.csv')\npred['revenue'] = test_pred\npred.to_csv(\"XGBR.csv\", index=False)\n","execution_count":158,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}