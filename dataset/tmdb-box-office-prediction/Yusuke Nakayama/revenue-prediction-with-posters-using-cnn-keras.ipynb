{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm, skew \nfrom scipy import stats\nimport datetime\nimport ast\nimport json\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\n#Standardization\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\".\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/tmdb-box-office-prediction/train.csv\")\ndf_test = pd.read_csv(\"../input/tmdb-box-office-prediction/test.csv\")\n\nprint(df_train.index)\nprint(df_test.index)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_all = pd.concat([df_train, df_test], sort=False).reset_index()\n\n\nprint(df_all.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"preparing for loading posters"},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\n\ndef loadPosterImages(df, _path_to_base):\n    \n    images = []\n    \n    for i in df.id:\n        path_to_img = _path_to_base + str(i) + \".jpeg\"\n        if os.path.exists(path_to_img) == False:\n            print(path_to_img + \" is null\")\n            image = np.zeros([64,64,3],dtype=np.uint8)\n        else:\n            image = cv2.imread(path_to_img)\n            image = cv2.resize(image, (64, 64))\n        \n        images.append(image)\n    \n    \n    return np.array(images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"use_npy=0\n\nif use_npy != 1:\n    poster_train_img = loadPosterImages(df_train, \"../input/tmdb-box-office-prediction-posters/tmdb_box_office_prediction_posters/tmdb_box_office_prediction_posters/train/\")\n    poster_test_img = loadPosterImages(df_test, \"../input/tmdb-box-office-prediction-posters/tmdb_box_office_prediction_posters/tmdb_box_office_prediction_posters/test/\")\n\n    np.save('poster_train_img.npy', poster_train_img)\n    np.save('poster_test_img.npy', poster_test_img)\nelse:\n    #Loading images needs long time, so I save them as numpy binary and load from the npy files instead.\n    poster_train_img = np.load('poster_train_img.npy')\n    poster_test_img = np.load('poster_test_img.npy')\n\n\nposter_train_img = poster_train_img / 255.0\nposter_test_img = poster_test_img / 255.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are missing posters in the dataset. <br>\nTrain : 2303 <br>\nTest : 3829, 4925 <br>\nTheir titles are...\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.loc[df_train[\"id\"] == 2303, \"title\"])\nprint(df_test.loc[df_test[\"id\"] == 3829, \"title\"])\nprint(df_test.loc[df_test[\"id\"] == 4925, \"title\"])\n\ndf_train.drop(df_train.loc[df_train[\"id\"] == 2303].index, inplace=True)\nprint(df_train.loc[df_train[\"id\"] == 2303, \"title\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_col_name = \"revenue\"\nsc = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprocessing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_distribution(y):\n    \n    plt.figure(figsize=(15, 8))\n    sns.distplot(y,fit=norm)\n    mu,sigma=norm.fit(y)\n    plt.legend([\"Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f})\".format(mu,sigma)])\n    plt.title(\"Distribution\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n    \n    \ndef visualize_probplot(y):\n    plt.figure(figsize=(15, 8))\n    stats.probplot(y,plot=plt)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def showJointPlot(df, col_name):\n    \n\n    if not col_name in df.columns:\n        print(col_name + \" is not inside columns\")\n        return\n\n    print(\"***[\" + col_name + \"]***\")\n    print(\"describe : \")\n    print(df[col_name].describe())\n    print(\"skew : \")\n    print(df[col_name].skew())\n\n    #correlation\n    corrmat = df.corr()\n    num_of_col = len(corrmat.columns)\n    cols = corrmat.nlargest(num_of_col, col_name)[col_name]\n    print(\"*****[ corr : \" + col_name + \" ]*****\")\n    print(cols)\n    print(\"*****[\" + col_name + \"]*****\")\n    print(\"\\n\")\n\n\n    visualize_distribution(df[col_name].dropna())\n    visualize_probplot(df[col_name].dropna())\n\n\n    if col_name != target_col_name and target_col_name in df.columns:\n        plt.figure(figsize=(15, 8))\n        sns.jointplot(col_name, target_col_name, df)\n\n    print(\"******\\n\")\n    return ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def showValueCount(df, col_name):\n\n    if not col_name in df.columns:\n        print(col_name, \" is not inside columns\")\n        return\n\n    print(\"***[\" + col_name + \"]***\")\n    print(\"describe :\")\n    print(df[col_name].describe())\n\n    df_value = df[col_name].value_counts(dropna=False)\n    print(\"value_counts :\")\n    print(df_value)\n\n    plt.figure(figsize=(15,8))\n    sns.barplot(df_value.index, df_value.values, alpha=0.8)\n    plt.ylabel('Number of each element', fontsize=12)\n    plt.xlabel(col_name, fontsize=12)\n    plt.xticks(rotation=90, size='small')\n    plt.show()\n\n\n    if col_name != target_col_name and target_col_name in df.columns:\n        plt.figure(figsize=(15, 8))\n        plt.xticks(rotation=90, size='small')\n        sns.boxplot(x=df[col_name], y =df[target_col_name])\n        plt.show()\n\n    print(\"******\\n\")\n    return ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def countAndExpandFromDictToColumns(df, expanded_col_name, _val_name):\n    \n    df[expanded_col_name] = df[expanded_col_name].dropna().map(lambda x : ast.literal_eval(x))\n    \n    def connectToString(x, prefix, val_name):\n\n        str_names = []\n        for val in x:\n\n            str_names.append(prefix + \"_\" + val[val_name])\n\n        return \",\".join(str_names)\n\n\n    df[expanded_col_name] = df[expanded_col_name].dropna().map(lambda x : connectToString(x, prefix=expanded_col_name, val_name=_val_name))\n    #print(df[expanded_col_name].head())\n    df_tmp = df[expanded_col_name].dropna().str.get_dummies(sep=',')\n    #print(df_tmp.info())\n    df = pd.concat([df, df_tmp], axis=1, sort=False)\n\n    df.drop(expanded_col_name, axis=1, inplace=True)\n\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def countFromDict(df, count_col_name, _val_name=None):\n    \n    count_dic = {}\n\n    def countFromDf(x):\n        for val in ast.literal_eval(x):\n\n            if _val_name != None:\n                element_val = val[_val_name]\n            else:\n                element_val = json.dumps(val)\n            if count_dic.get(element_val, 0) == 0:\n                count_dic[element_val] = 1\n            else:\n                count_dic[element_val] = count_dic[element_val] + 1\n        \n        return x\n\n    _ = df[count_col_name].dropna().map(lambda x : countFromDf(x))\n    \n    df_count = pd.DataFrame(list(count_dic.items()),columns=['key_name','num'])\n    df_count.sort_values(\"num\", ascending=False, inplace=True)\n\n    return df_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**revenue**"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = \"revenue\"\n\nshowJointPlot(df_all, col_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"revenue's minimum is 1 dollar...?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_million_index = df_all.loc[df_all[col_name] < 100].index\ndf_thousand_index = df_all.loc[df_all[col_name] < 1000].index\n\ndf_all.loc[df_million_index, col_name] = df_all.loc[df_million_index, col_name].map(lambda x: x * 1000000)\ndf_all.loc[df_thousand_index, col_name] = df_all.loc[df_thousand_index, col_name].map(lambda x: x * 1000)\n\ndf_all[col_name] = np.log(df_all[col_name])\nshowJointPlot(df_all, col_name)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Budget**"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = \"budget\"\n\nshowJointPlot(df_all, col_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"budget contains many 0..."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_million_budget_index = df_all.loc[df_all[col_name] < 100].index\ndf_thousand_budget_index = df_all.loc[df_all[col_name] < 1000].index\n\ndf_all.loc[df_million_budget_index, col_name] = df_all.loc[df_million_budget_index, col_name].map(lambda x: x * 1000000)\ndf_all.loc[df_thousand_budget_index, col_name] = df_all.loc[df_thousand_budget_index, col_name].map(lambda x: x * 1000)\n\n\n\n\n#df_all[col_name] = pd.Series(sc.fit_transform(df_all[col_name].values.reshape(-1, 1)).flatten())\ndf_all[col_name] = np.log1p(df_all[col_name])\nshowJointPlot(df_all, col_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**release_date**"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = \"release_date\"\n\n            \nprint(\"\\n**fillNanValues : \" + col_name +  \"***\")\nprint(df_all.loc[df_all[\"release_date\"].isnull() == True, [\"title\", \"imdb_id\", col_name]])\n\n#fill from IMDB\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0210130\", col_name] = \"3/20/01\"\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[col_name + \"_year\"] = df_all[col_name].map(lambda x: int(x.split(\"/\")[2]))\ndf_all[col_name + \"_month\"] = df_all[col_name].map(lambda x: int(x.split(\"/\")[0]))\ndf_all[col_name + \"_day\"] = df_all[col_name].map(lambda x: int(x.split(\"/\")[1]))\n\n\n#we assume that release years are between 1900 and 2017\ndf_all.loc[(df_all[col_name + \"_year\"] < 18), col_name + \"_year\"] += 2000\ndf_all.loc[(df_all[col_name + \"_year\"] >= 18) & (df_all[col_name + \"_year\"] < 100), col_name + \"_year\"] += 1900\n\ndf_all[col_name] = df_all.apply(lambda x: datetime.datetime(x[col_name + \"_year\"], x[col_name + \"_month\"], x[col_name + \"_day\"]), axis=1)\n\ndf_all[col_name + \"_month\"] = df_all[col_name + \"_month\"].replace({1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\", 7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"})\n\ndow = [\"Mon\",\"Tue\",\"Wed\",\"Thr\",\"Fri\",\"Sat\",\"Sun\"]\ndf_all[col_name + \"_dayofweek\"] = df_all[col_name].map(lambda x: dow[x.weekday()])\n\ndf_all[col_name + \"_week\"]= pd.Series(len(df_all[col_name]), index=df_all.index)\ndf_all.loc[df_all[col_name + \"_day\"] <= 7, col_name + \"_week\"] = \"w1\"\ndf_all.loc[(df_all[col_name + \"_day\"] > 7) & (df_all[col_name + \"_day\"] <= 14), col_name + \"_week\"] = \"w2\"\ndf_all.loc[(df_all[col_name + \"_day\"] > 14) & (df_all[col_name + \"_day\"] <= 21), col_name + \"_week\"] = \"w3\"\ndf_all.loc[(df_all[col_name + \"_day\"] > 21) & (df_all[col_name + \"_day\"] <= 28), col_name + \"_week\"] = \"w4\"\ndf_all.loc[(df_all[col_name + \"_day\"] > 28), col_name + \"_week\"] = \"w5\"\n\ndf_all[col_name + \"_MonthWeek\"] = df_all[col_name + \"_month\"] + df_all[col_name + \"_week\"]\n\n\ndf_all.drop(col_name + \"_month\", inplace=True, axis=1)\ndf_all.drop(col_name + \"_week\", inplace=True, axis=1)\ndf_all.drop(col_name + \"_day\", inplace=True, axis=1)\ndf_all.drop(col_name, inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"showValueCount(df_all, col_name + \"_year\")\nshowValueCount(df_all, col_name + \"_dayofweek\")\nshowValueCount(df_all, col_name + \"_MonthWeek\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**belongs_to_collection**"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = \"belongs_to_collection\"\n\ndf_all[\"inCollection\"] = 0\ndf_all.loc[df_all[col_name].isnull() == False, \"inCollection\"] = 1\n\nshowValueCount(df_all, \"inCollection\")\ndf_all.drop(col_name, inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I add the feature \"inCollection\" which means each movie belongs its collection or not."},{"metadata":{},"cell_type":"markdown","source":"**genres**"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = \"genres\"\n\nprint(df_all.loc[df_all[col_name].isnull(), [\"imdb_id\", \"title\", col_name]])\nprint(\"\\n**fillNanValues : \" + col_name +  \"***\")\n\n#fill from IMDB\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0349159\", col_name] = \"[{'id': 12, 'name': 'Adventure'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0261755\", col_name] = \"[{'id': 18, 'name': 'Drama'}, {'id': 35, 'name': 'Comedy'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0110289\", col_name] = \"[{'id': 35, 'name': 'Comedy'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0352622\", col_name] = \"[{'id': 10749, 'name': 'Romance'}, {'id': 18, 'name': 'Drama'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0984177\", col_name] = \"[{'id': 28, 'name': 'Action'}, {'id': 18, 'name': 'Drama'}, {'id': 10749, 'name': 'Romance'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0833448\", col_name] = \"[{'id': 53, 'name': 'Thriller'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt1766044\", col_name] = \"[{'id': 18, 'name': 'Drama'}, {'id': 14, 'name': 'Fantasy'}, {'id': 9648, 'name': 'Mystery'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0090904\", col_name] = \"[{'id': 28, 'name': 'Action'}, {'id': 80, 'name': 'Crime'}, {'id': 53, 'name': 'Thriller'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0086405\", col_name] = \"[{'id': 18, 'name': 'Drama'}, {'id': 10749, 'name': 'Romance'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0044177\", col_name] = \"[{'id': 12, 'name': 'Adventure'}, {'id': 99999999, 'name': 'Biography'}, {'id': 18, 'name': 'Drama'}, {'id': 10749, 'name': 'Romance'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0108234\", col_name] = \"[{'id': 28, 'name': 'Action'}, {'id': 80, 'name': 'Crime'}, {'id': 18, 'name': 'Drama'}, {'id': 53, 'name': 'Thriller'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt1572916\", col_name] = \"[{'id': 18, 'name': 'Drama'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt1569465\", col_name] = \"[{'id': 35, 'name': 'Comedy'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0405699\", col_name] = \"[{'id': 28, 'name': 'Action'}, {'id': 80, 'name': 'Crime'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0461892\", col_name] = \"[{'id': 28, 'name': 'Action'}, {'id': 18, 'name': 'Drama'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt3121604\", col_name] = \"[{'id': 18, 'name': 'Drama'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt1164092\", col_name] = \"[{'id': 99, 'name': 'Documentary'}, {'id': 99999999, 'name': 'Biography'}, {'id': 10751, 'name': 'Family'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0250282\", col_name] = \"[{'id': 35, 'name': 'Comedy'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt1620464\", col_name] = \"[{'id': 35, 'name': 'Comedy'}, {'id': 80, 'name': 'Crime'}, {'id': 9648, 'name': 'Mystery'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0073317\", col_name] = \"[{'id': 35, 'name': 'Comedy'}, {'id': 80, 'name': 'Crime'}, {'id': 18, 'name': 'Drama'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0361498\", col_name] = \"[{'id': 35, 'name': 'Comedy'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0361596\", col_name] = \"[{'id': 99, 'name': 'Documentary'}, {'id': 18, 'name': 'Drama'}, {'id': 10752, 'name': 'War'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt2192844\", col_name] = \"[{'id': 18, 'name': 'Drama'}]\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[col_name + \"_len\"] = df_all[col_name].map(lambda x: len(ast.literal_eval(x)) if pd.isnull(x) == False else 0)\ndf_all.loc[df_all[col_name + \"_len\"] > 5, col_name + \"_len\"] = 5\nshowValueCount(df_all, col_name + \"_len\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = countAndExpandFromDictToColumns(df_all, col_name, \"name\")\nprint(df_all.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**runtime**"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = \"runtime\"\n\nprint(\"\\n**fillNanValues : \" + col_name +  \"***\")\nprint(df_all.loc[df_all[\"runtime\"].isnull() == True, [\"title\", \"imdb_id\", \"release_date\", col_name]])\n\n\nshowJointPlot(df_all, col_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the longest and shortest movies are ..."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.sort_values(col_name, ascending=False)[[\"title\", col_name]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.loc[df_all[col_name] == 0, col_name] = np.nan\ndf_all[col_name].fillna(df_all[col_name].median(), inplace=True)\n\n#df_all[col_name] = pd.Series(sc.fit_transform(df_all[col_name].values.reshape(-1, 1)).flatten())\ndf_all[col_name] = np.log(df_all[col_name])\n#df_all.drop(index=df_all.loc[df_all[col_name] < 2.4, [\"title\", col_name]].index, inplace=True)\n#print(df_all.loc[df_all[col_name] < 2.4, [\"title\", col_name]])\nshowJointPlot(df_all, col_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**spoken_languages**"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = \"spoken_languages\"\n\nprint(\"\\n**fillNanValues : \" + col_name +  \"***\")\n\ndf_count_sLangugaes = countFromDict(df_all, col_name)\ndf_count_sLangugaes[\"iso\"] = df_count_sLangugaes[\"key_name\"].map(lambda x: ast.literal_eval(x)[\"iso_639_1\"])\n#print(df_count_sLangugaes.head(len(df_count_sLangugaes)))\n\n\ndef returnStr(x):\n    \n    lis = df_count_sLangugaes.loc[df_count_sLangugaes[\"iso\"] == x.original_language, \"key_name\"].values\n    if len(lis) == 0:\n        #print(x, lis)\n        x.spoken_languages = np.nan\n    else:\n        str_dic = lis[0]\n        x.spoken_languages =  str(\"[\" + str_dic + \"]\")\n    \n    return x\n\ndf_all.loc[(df_all[col_name].isnull() == True), [col_name, \"original_language\"]] = df_all.loc[(df_all[col_name].isnull() == True), [col_name, \"original_language\"]].apply(lambda x: returnStr(x), axis=1)\ndf_all[\"lang_len\"] = df_all[col_name].map(lambda x: len(ast.literal_eval(x)))\n\ndf_all = countAndExpandFromDictToColumns(df_all, col_name, \"iso_639_1\")\ndf_all.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**original_language**"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = \"original_language\"\n\n#print(df_all[col_name].isnull().sum())\n\n#df_all[\"isEnglish\"] = 0\n#df_all.loc[(df_all[col_name] == \"en\") | (df_all[\"spoken_languages_en\"] == 1), \"isEnglish\"] = 1\n\nshowValueCount(df_all, col_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I add new feature \"isEnglish\""},{"metadata":{},"cell_type":"markdown","source":"**overview**"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = \"overview\"\ndf_all[col_name].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"this feature is used in \"Keywords\""},{"metadata":{},"cell_type":"markdown","source":"**popularity**"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = \"popularity\"\n\nshowJointPlot(df_all, col_name)\ndf_all[col_name].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_all.shape)\ndf_all[col_name + \"_RANK\"] = 2\ndf_all.loc[df_all[col_name] > 50, col_name + \"_RANK\"]= 3\ndf_all.loc[df_all[col_name] <= 3, col_name + \"_RANK\"] = 1\n\n#df_all.drop(df_all.loc[(df_all[\"revenue\"] < 10) & (df_all[col_name] > 25)].index, inplace=True)\ndf_all.loc[(df_all[col_name] < 0.0003), col_name] = 0.0003\n\n#df_all[col_name] = pd.Series(sc.fit_transform(df_all[col_name].values.reshape(-1, 1)).flatten())\ndf_all[col_name] = np.log(df_all[col_name])\n\nprint(df_all.shape)\n#showJointPlot(df_all, col_name)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**crew**"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = \"crew\"\n\ndf_all[col_name + \"_num\"] = df_all[col_name].map(lambda x: len(ast.literal_eval(x)) if pd.isnull(x) == False else np.nan)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I add \"crew_num\""},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill na and 0 \ndf_all[col_name + \"_num\"].fillna(df_all[col_name + \"_num\"].median(), inplace=True)\n\n#df_all[col_name + \"_num\"] = pd.Series(sc.fit_transform(df_all[col_name + \"_num\"].values.reshape(-1, 1)).flatten())\ndf_all[col_name + \"_num\"] = np.log(df_all[col_name + \"_num\"])\n\nshowJointPlot(df_all, col_name + \"_num\")\n\n\ndf_all.drop(col_name, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**cast**"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = \"cast\"\n        \ndf_all[col_name + \"_num\"] = df_all[col_name].map(lambda x: len(ast.literal_eval(x)) if pd.isnull(x) == False else np.nan)\ndf_all.loc[df_all[col_name + \"_num\"] == 0, col_name + \"_num\"] = np.nan\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncast_dict = {}\n\ndef countName(x):\n    cast_list = ast.literal_eval(x.cast)\n    \n    for each_cast in cast_list:\n        cast_name = each_cast[\"name\"]\n        \n        \n        if cast_name not in cast_dict.keys():\n            cast_dict[cast_name] = {\"train\":0, \"test\":0}\n        \n        which_data = \"train\"\n        if x.id > 3000:\n            which_data = \"test\"\n        cast_dict[cast_name][which_data] += 1\n    \n    return x\n\ndf_all = df_all.apply(lambda x: countName(x) if pd.isnull(x.cast) == False else x, axis=1)\n\ndf_cast_count = pd.DataFrame(cast_dict).T\ndf_cast_count.reset_index(inplace=True)\ndf_cast_count.rename(columns={'index': 'name'}, inplace=True)\ndf_cast_count[\"total\"] = df_cast_count[\"train\"] + df_cast_count[\"test\"]\ndf_cast_count.sort_values(\"total\", inplace=True)\nprint(df_cast_count.shape)\nprint(df_cast_count.columns)\nprint(df_cast_count.index)\ndf_cast_count\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fill na and 0 \ndf_all[col_name + \"_num\"].fillna( df_all[col_name + \"_num\"].median(), inplace=True)\n\n\n#df_all[col_name + \"_num\"] = pd.Series(sc.fit_transform(df_all[col_name + \"_num\"].values.reshape(-1, 1)).flatten())\ndf_all[col_name + \"_num\"] = np.log(df_all[col_name + \"_num\"])\n\nshowJointPlot(df_all, col_name + \"_num\")\n\ndf_all.drop(col_name, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Keywords**"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = \"Keywords\"\n\n#df_all[col_name + \"_len\"] = 0\n#df_count_keywords = countFromDict(df_all, col_name, \"name\")\n\n            \ndef serachOverview(x):\n\n    val = 0\n    freq_keywords = []\n    keywords_len = 0\n    if pd.isna(x[col_name]) == False:\n        overview_text = x[\"overview\"]\n        keyword_list = ast.literal_eval(x[col_name])\n        keywords_len = len(keyword_list)\n\n\n        for keyword in keyword_list:\n            word = str(keyword[\"name\"])\n            #print(word)\n            freq_num = df_count_keywords.loc[df_count_keywords[\"key_name\"] == word, \"num\"].values[0]\n            #print(freq_num)\n            if freq_num > 100:\n                freq_keywords.append(col_name + \"_\" + word)\n\n            if overview_text != 0 & str(overview_text).find(word) != -1:\n                val = val + 1\n\n    x[\"overview\"] = val\n    x[col_name] = \",\".join(freq_keywords)\n    x[col_name + \"_len\"] = keywords_len\n    return x\n\n\n\n#df_all = df_all.apply(lambda x: serachOverview(x), axis=1)\n\n#df_tmp = df_all[col_name].str.get_dummies(sep=',')\n#print(df_tmp.info())\n#df_all = pd.concat([df_all, df_tmp], axis=1, sort=False)\ndf_all.drop([\"Keywords\", \"overview\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**production_countries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = \"production_countries\"\n        \nprint(\"\\n**fillNanValues : \" + col_name +  \"***\")\n\n#df = fillNAN_productionC(df)\ndf_all[\"production_countries\"].fillna(\"[{'iso_3166_1': 'US', 'name': 'United States of America'}]\", inplace=True)\ndf_all = countAndExpandFromDictToColumns(df_all, col_name, \"name\")\ndf_all.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**homepage**"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_name = \"homepage\"\n\ndf_all[\"hasHomepage\"] = 1\ndf_all.loc[df_all[col_name].isnull() == True, \"hasHomepage\"] = 0\n\ndf_all.drop(col_name, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**others**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[\"budget_year_ratio\"] = df_all[\"budget\"] / df_all[\"release_date_year\"]\n\ndrop_cols = [\"imdb_id\", \"poster_path\", \"original_title\", \"tagline\", \"title\", \"status\", \"production_companies\"]\ndf_all.drop(drop_cols, axis=1, inplace=True)\n\ndf_all = pd.get_dummies(df_all, drop_first=True, columns=['release_date_dayofweek'])\n\ndf_all = pd.get_dummies(df_all)\ndf_all.shape\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import fbeta_score, make_scorer\nimport keras.backend as K\nfrom keras.layers import Input, Dense, Activation, BatchNormalization, MaxPooling2D\nfrom keras.models import Model\nfrom keras.layers import concatenate\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.callbacks import EarlyStopping \nfrom keras.callbacks import LearningRateScheduler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_idx = df_all[~df_all[target_col_name].isnull()].index.values\ndf_train = df_all.loc[train_idx].set_index('id')\ndf_train.drop([\"index\"], axis=1, inplace=True)\n\nmax_price = df_train[\"revenue\"].max()\ndf_train[\"revenue\"] = df_train[\"revenue\"] / max_price\n            \ntest_idx = df_all[df_all[target_col_name].isnull()].index.values\ndf_test = df_all.loc[test_idx].set_index('id')\ndf_test.drop([target_col_name, \"index\"], axis=1, inplace=True)\n\ndf_train_X = df_train.drop(target_col_name, axis=1)\ndf_train_y = df_train[target_col_name]\nprint(df_train.shape)\nprint(df_test.shape)\nprint(df_train.columns)\nprint(df_test.columns)\n\n\nscorer = make_scorer(mean_squared_error, greater_is_better = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nindices = np.array(range(df_train.shape[0]))\nvalid_train_X, valid_test_X, valid_train_y, valid_test_y, indices_valid_train, indices_valid_test  = train_test_split(df_train_X, df_train_y, indices, test_size=0.2, shuffle=True, random_state=64)\n\nprint(\"valid_train_X\", valid_train_X.shape)\nprint(\"valid_test_X\", valid_test_X.shape)\nprint(\"valid_train_y\", valid_train_y.shape)\nprint(\"valid_test_y\", valid_test_y.shape)\n\n \nvalid_train_img_X = poster_train_img[indices_valid_train]\nvalid_test_img_X = poster_train_img[indices_valid_test]\ntest_img_X = poster_test_img\n\nprint(\"valid_train_img_X\", valid_train_img_X.shape)\nprint(\"valid_test_img_X\", valid_test_img_X.shape)\nprint(\"test_img_X\", test_img_X.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Neural Network model creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def createNNmodel(_input_dim, regress=False):\n    \n    model = Sequential()\n    model.add(Dense(5, input_dim=_input_dim, activation=\"relu\"))\n    model.add(Dense(4, activation=\"relu\"))\n    model.add(Dense(3, activation=\"relu\"))\n    model.add(Dense(2, activation=\"relu\"))\n    \n    if regress:\n        model.add(Dense(1, activation=\"linear\"))\n    \n    return model\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def createCNNmodel(width, height, depth, filters, regress=False):\n\n    inputShape = (height, width, depth)\n    chanDim = -1\n\n    inputs = Input(shape=inputShape)\n    \n    for (i, f) in enumerate(filters):\n\n        if i == 0:\n            x = inputs\n \n        # CONV => RELU => BN => POOL\n        x = Conv2D(f, (3, 3), padding=\"same\")(x)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization(axis=chanDim)(x)\n        x = MaxPooling2D(data_format='channels_last', pool_size=(2, 2))(x)\n        x = Dropout(0.25)(x)\n    \n    # flatten the volume, then FC => RELU => BN => DROPOUT\n    x = Flatten()(x)\n    x = Dense(16)(x)\n    x = Activation(\"relu\")(x)\n    x = BatchNormalization(axis=chanDim)(x)\n    x = Dropout(0.5)(x)\n\n    x = Dense(4)(x)\n    x = Activation(\"relu\")(x)\n\n    if regress:\n        x = Dense(1, activation=\"linear\")(x)\n        \n\n    model = Model(inputs, x)\n    \n    return model  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def createBoxOfficeNetModel(_input_dim_NN, width_img, height_img, depth_img, filters_CNN):\n    \n    model_NN = createNNmodel(_input_dim_NN)\n    model_CNN = createCNNmodel(width_img, height_img, depth_img, filters=filters_CNN)\n\n    combinedInput = concatenate([model_NN.output, model_CNN.output])\n\n    x = Dense(4, activation=\"relu\")(combinedInput)\n    x = Dense(1, activation=\"linear\")(x)\n    \n    \n    model = Model(inputs=[model_NN.input, model_CNN.input], outputs=x)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# my loss function for RMSE\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def drawResultCurves(_history):\n    # Plot the loss and accuracy curves for training and validation \n\n    fig, ax = plt.subplots(2,1) \n    ax[0].plot(_history.history['loss'], color='b', label=\"Training loss\")\n    ax[0].plot(_history.history['val_loss'], color='r', label=\"Validation loss\",axes =ax[0])\n    legend = ax[0].legend(loc='best', shadow=True)\n\n    ax[1].plot(_history.history['mean_squared_logarithmic_error'], color='b', label=\"Training accuracy\")\n    ax[1].plot(_history.history['val_mean_squared_logarithmic_error'], color='r',label=\"Validation accuracy\")\n    legend = ax[1].legend(loc='best', shadow=True)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_filters_CNN=[8]\nmodel = createBoxOfficeNetModel(valid_train_X.shape[1], 64, 64, 3, filters_CNN=my_filters_CNN)\n\n\nopt = Adam(lr=1e-5, decay=1e-4)\nmodel.compile(loss=root_mean_squared_error, optimizer=opt, metrics=['msle'])\n\n#model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_epochs=100\n\ndef step_decay(epoch):\n    x = 1e-3\n    if epoch >= 30: x = 1e-4\n    return x\nlr_decay = LearningRateScheduler(step_decay)\n\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=20, verbose=0, mode='auto')\n# train the model\nhistory = model.fit([valid_train_X, valid_train_img_X], valid_train_y, \n                    validation_data=([valid_test_X, valid_test_img_X], valid_test_y),\n                    #callbacks=[early_stopping],\n                    callbacks=[early_stopping, lr_decay],\n                    epochs=_epochs, batch_size=8, verbose = 2)\n\n              \ndrawResultCurves(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict results with trained parameters\n         \npreds = model.predict([df_test.values, test_img_X])\n#pred_test_all_y = np.exp(preds)\npred_test_all_y = np.exp(preds * max_price)\n\ndf_test.loc[:, \"revenue\"] = pred_test_all_y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Submission**"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_Price = pd.DataFrame({\n            \"id\": df_test.index,\n            \"revenue\": np.nan\n        }, index=df_test.index)\n        \n               \nsubmission_Price.loc[df_test.index, \"revenue\"] = df_test[\"revenue\"]\n\nsubmission_Price.to_csv('submission.csv', index=False)\n\n\nsubmission_Price\n#for i in range(len(submission_Price)):\n    #print(submission_Price.iloc[i])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}