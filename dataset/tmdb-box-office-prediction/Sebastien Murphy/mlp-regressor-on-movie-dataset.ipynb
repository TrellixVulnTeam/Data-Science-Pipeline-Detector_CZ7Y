{"cells":[{"metadata":{"colab_type":"text","id":"UUsrOtToyNIO"},"cell_type":"markdown","source":"# Imports"},{"metadata":{"colab_type":"code","id":"CVUmf1UayVPU","outputId":"0572be33-fd6c-4935-bcca-ba70b17c319e","colab":{"base_uri":"https://localhost:8080/","height":54},"trusted":false},"cell_type":"code","source":"#the usual\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\n#colored printing output\nfrom termcolor import colored\n\n#I/O\nimport io\nimport os\nimport requests\n\n#pickle\nimport pickle\n\n#math\nimport math\n\n#scipy\nfrom scipy import stats\n\n#sk learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import linear_model\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import preprocessing\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import learning_curve\nfrom itertools import combinations\nfrom mlxtend.feature_selection import ColumnSelector\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\n\n\n#sns style\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.despine()\nsns.set_context(\"talk\") #larger display of plots axis labels etc..\n\n\n","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"Sr0KosnvMP_v","outputId":"56701a0c-92ce-4593-b31c-d68a293c6826","colab":{"base_uri":"https://localhost:8080/","height":54},"trusted":false},"cell_type":"code","source":"#use TPUs for faster processing (not sure if enabled by default)\nif 'COLAB_TPU_ADDR' not in os.environ:\n  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\nelse:\n  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n  print ('TPU address is', tpu_address)\n","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"hxtbSZd1ytIK"},"cell_type":"markdown","source":"# Functions"},{"metadata":{"colab_type":"code","id":"aiG8tIokywSj","colab":{},"trusted":false},"cell_type":"code","source":"#scatter plots for all numerical values of a df\ndef ScatterPlots(df, n_plots=3):\n  fig, axes = plt.subplots(1, n_plots)\n  i=0\n  for key, value in df.iloc[:, :-1].iteritems(): \n    print(key) \n    df.plot(kind=\"scatter\",x=key, y=\"revenue\",color=\"green\",ax=axes[i],figsize=(20,10))\n    i+=1","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"i-26pVMZzEZU","colab":{},"trusted":false},"cell_type":"code","source":"#check for missing value, zeros etc..\n#prints with colour output depending on set limit value\ndef print_color(text, values, limit=math.inf):\n  to_compare=False\n  if isinstance(values, float):\n    to_compare=(values>limit)\n  else:\n    to_compare=(values>limit).sum()    \n  if to_compare==True:\n    print(colored(text,color=\"magenta\",attrs=['reverse', 'blink'])+colored(values,color=\"magenta\"))\n  else:\n    print(colored(text,color=\"green\")+colored(values,color=\"green\"))\n        \ndef CheckValues(X, Y, detail=False):\n  print(\"---- in X-----\")\n  print (\"shape:\", train_x.shape)\n  if detail==True:  \n    print_color(\"percentage of Nan:\\n\", values=X.isna().mean().round(4)*100,limit=5.)\n    print_color(\"percentage of zeros:\\n\", values=X.eq(0).mean().round(4)*100,limit=5.)\n    print_color(\"percentage of negative:\\n\", values=X.lt(0).mean().round(4)*100,limit=5.)\n  \n  print_color(\"percentage of Nan for entire df:\",values=round(X.isna().mean().mean()*100,2),limit=5.)\n  print_color(\"percentage of zeros for entire df:\",values=round(X.eq(0).mean().mean()*100,2),limit=5.)\n  print_color(\"percentage of negative values for entire df:\",values=round(X.lt(0).mean().mean()*100,2),limit=5.)\n  \n  print(\"---- in Y-----\")\n  print(\"shape:\", Y.shape)\n  if detail==True:\n    print_color(\"percentage of Nan:\\n\", values=Y.isna().mean().round(4)*100,limit=5.)\n    print_color(\"percentage of zeros:\\n\", values=Y.eq(0).mean().round(4)*100,limit=5.)\n    print_color(\"percentage of negative:\\n\", values=Y.lt(0).mean().round(4)*100,limit=5.)\n  \n  print_color(\"percentage of Nan for entire df:\",values=round(Y.isna().mean().mean()*100,2),limit=5.)\n  print_color(\"percentage of zeros for entire df:\",values=round(Y.eq(0).mean().mean()*100,2),limit=5.)\n  print_color(\"percentage of negative values for entire df:\",values=round(Y.lt(0).mean().mean()*100,2),limit=5.)\n  \n\n\n#remove missing values replace by mean\ndef RemoveMissVal(X,Y, verbose=False):\n  if(verbose):\n    print(\"percentage of Nan in X before removal:\\n\",X.isna().mean().round(4)*100)\n    print(\"percentage of Nan in Y before removal:\\n\",Y.isna().mean().round(4)*100)\n  X_clean=X.fillna(X.mean())\n  Y_clean=Y.fillna(Y.mean())\n  \n  if(verbose):\n    print(\"-----DONE------------\")\n    print(\"percentage of Nan in X after removal:\\n\",X_clean.isna().mean().round(4)*100)\n    print(\"percentage of Nan in Y after removal:\\n\",Y_clean.isna().mean().round(4)*100)\n  \n  return X_clean, Y_clean\n\n#remove outliers (values larger than std_dev will be removed)\ndef RemoveSigma(X,Y,std_dev,verbose=False):\n    if(verbose):\n      print(\"---- before----\")\n      print(X.shape)\n      print(Y.shape)\n    #print(np.abs(stats.zscore(X)))\n    X_cut = X[(np.abs(stats.zscore(X)) < float(std_dev)).all(axis=1)]\n    Y_cut = Y[(np.abs(stats.zscore(X)) < float(std_dev)).all(axis=1)]\n    if(verbose):\n      print(\"---- after----\")\n      print(X_cut.shape)\n      print(Y_cut.shape)\n    return X_cut, Y_cut","execution_count":0,"outputs":[]},{"metadata":{"id":"SOeNRiYWe5JR","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"","execution_count":0,"outputs":[]},{"metadata":{"id":"7ubVtQ5qX7PC","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"356jA2nk0m2F","colab":{},"trusted":false},"cell_type":"code","source":"#plot histograms from two df on same plot\ndef DoubleDfHist(df,df2,n_hist):\n  fig, ax = plt.subplots(1, 3, sharex='col', sharey='row', figsize=(20, 6))\n  for j in range(n_hist):\n    df.hist(column=df.columns[j], bins=100, ax=ax[j], alpha=1, color='red',log=True)\n    df2.hist(column=df2.columns[j], bins=100, ax=ax[j], alpha=0.5, color='blue',log=True)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"ODZJDc4LCZeE","colab":{},"trusted":false},"cell_type":"code","source":"def rmsle(y_pred, y_test) : \n    #clip zero values\n    assert len(y_test) == len(y_pred)\n    return np.sqrt(np.mean((np.log(list(np.asarray(y_pred).clip(min=0) + 1)) - np.log(list(np.asarray(y_test).clip(min=0) + 1)))**2))","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"zu02q62GCrjn","colab":{},"trusted":false},"cell_type":"code","source":"def cross_val_predict(train_X,train_y, model, k_fold=5, use_scaling=True, Verbose=False, score_rmsle=True):\n    cv = KFold(n_splits = k_fold)\n    test_y_overall = []\n    predict_y_overall = []\n    train_X=train_X.values\n    train_y=train_y.values    \n    for train_index, test_index in cv.split(train_X):\n      train_X_fi, train_y_fi = train_X[train_index], train_y[train_index]\n      test_X_fi, test_y_fi = train_X[test_index], train_y[test_index]\n      \n      #if train_X, Y are not np arrays use thise:\n      #train_X_fi, train_y_fi = train_X.iloc[train_index], train_y.iloc[train_index]\n      #test_X_fi, test_y_fi = train_X.iloc[test_index], train_y.iloc[test_index]\n\n      #scale, train the model and evaluate it\n      scaler = StandardScaler()\n      train_scaled = scaler.fit_transform(train_X_fi)\n      test_scaled  = scaler.fit_transform(test_X_fi)\n      \n      if use_scaling:\n        model.fit(train_scaled, train_y_fi)\n        prediction = model.predict(test_scaled)\n      else:\n        model.fit(train_X_fi, train_y_fi)\n        prediction = model.predict(test_X_fi)\n      \n            \n      #store the target var and the prediction for later analysis\n      test_y_overall.extend(test_y_fi)\n      predict_y_overall.extend(prediction)     \n    \n      cross_val_error_rmsle = rmsle(predict_y_overall, test_y_overall)\n      cross_val_error_r2 = r2_score(predict_y_overall, test_y_overall)    \n      \n     #calculate and pring both rmsle and r2 scores, return only one of them \n    if(Verbose==True):\n      print(\"cross_val_error_rmsle is:\",cross_val_error_rmsle)\n      print(\"cross_val_error_r2 is:\",cross_val_error_r2)\n      \n    if score_rmsle:\n      cross_val_error=cross_val_error_rmsle\n    else:\n      cross_val_error=cross_val_error_r2\n      \n    return cross_val_error","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"VzO8t1BPEYKN","colab":{},"trusted":false},"cell_type":"code","source":"#plot validation curve\ndef PlotValidationCurve(train_scores,valid_scores,param_range,param_name,logx=False,verbose=False):\n  train_scores_mean = np.mean(train_scores, axis=1)\n  train_scores_std = np.std(train_scores, axis=1)\n  valid_scores_mean = np.mean(valid_scores, axis=1)\n  valid_scores_std = np.std(valid_scores, axis=1)\n  if verbose==True:\n    print(\"train_scores_mean:\",train_scores_mean)\n    print(\"valid_scores_mean:\",valid_scores_mean)\n  plt.figure(figsize=(10, 5), dpi=80)\n  plt.title(\"Validation curve\")\n  plt.plot(param_range, train_scores_mean, label=\"Training score\",\n             color=\"orange\", lw=2,marker=\".\")  \n  plt.fill_between(param_range, train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, alpha=0.2,\n                 color=\"orange\", lw=0)\n    \n  plt.plot(param_range, valid_scores_mean, label=\"Cross-validation score\",\n             color=\"black\", lw=2)\n  plt.fill_between(param_range, valid_scores_mean - valid_scores_std,\n                 valid_scores_mean + valid_scores_std, alpha=0.2,\n                 color=\"black\", lw=0)\n  if(logx==True):\n    plt.xscale('log')\n  plt.ylim(-.2, 1.1)\n  plt.xlabel(str(param_name))\n  plt.ylabel(\"score\")\n  plt.ylabel(\"Score\")\n  \n  plt.legend(loc=0)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"JqjhZ3vK9OfV","colab":{},"trusted":false},"cell_type":"code","source":"def PlotLearningCurve(train_sizes,train_scores,valid_scores,param_range,logx=False,verbose=False,ymin=0,ymax=1.):\n#plot validation curve\n  train_scores_mean = np.mean(train_scores, axis=1)\n  train_scores_std = np.std(train_scores, axis=1)\n  valid_scores_mean = np.mean(valid_scores, axis=1)\n  valid_scores_std = np.std(valid_scores, axis=1)\n  if verbose==True:\n    print(\"train_scores_mean:\",train_scores_mean)\n    print(\"valid_scores_mean:\",valid_scores_mean)\n  plt.figure(figsize=(10, 5), dpi=80)\n  plt.title(\"Learning curve\")\n  plt.grid()\n  plt.plot(train_sizes, train_scores_mean, label=\"Training score\",\n             color=\"red\", lw=2,marker=\".\")  \n  plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, alpha=0.2,\n                 color=\"red\", lw=0)\n    \n  plt.plot(train_sizes, valid_scores_mean, label=\"Cross-validation score\",\n             color=\"navy\", lw=2)\n  plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std,\n                 valid_scores_mean + valid_scores_std, alpha=0.2,\n                 color=\"navy\", lw=0)\n  if(logx==True):\n    plt.xscale('log')\n  plt.ylim(ymin, ymax)\n  plt.xlabel(\"training set size\")\n  plt.ylabel(\"score\")\n  plt.ylabel(\"Score\")\n  \n  plt.legend(loc=0)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"M4jTLgfuomfr","colab":{},"trusted":false},"cell_type":"code","source":"#Calculates the best model with all combinations fo features, of up to max_size features of X.\n#TODO: need to check implementation with rmsle\ndef best_subset(estimator, X, y, max_size=8, cv=5, use_rmsle=False, verbose=False):\n  n_features = X.shape[1]\n  subsets = (combinations(range(n_features), k + 1) \n               for k in range(min(n_features, max_size)))\n  best_size_subset = []\n  for subsets_k in subsets:  # for each list of subsets of the same size      \n      best_score = -np.inf\n      best_subset = None\n      for subset in subsets_k: # for each subset\n          estimator.fit(X.iloc[:, list(subset)], y)\n           # get the subset with the best score among subsets of the same size\n          score = estimator.score(X.iloc[:, list(subset)], y)         \n          #score=rmsle(X.iloc[:, list(subset)].values, y.values) #TODO: this needs to be fixed\n          if score > best_score:\n                best_score, best_subset = score, subset      \n        # first store the best subset of each size\n      best_size_subset.append(best_subset)\n\n    # compare best subsets of each size\n  best_score = -np.inf\n  best_subset = None\n  list_scores = []\n  for subset in best_size_subset:\n      if(use_rmsle):\n        score=cross_val_predict(X.iloc[:, list(subset)].astype(float), y, estimator, Verbose=verbose, score_rmsle=True) #home made scorer with rmsle\n      else:\n        score = cross_val_score(estimator, X.iloc[:, list(subset)], y, cv=cv).mean()\n      list_scores.append(score)\n      if score > best_score:\n        best_score, best_subset = score, subset\n  return best_subset, best_score, best_size_subset, list_scores","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"J-inzgYgygff"},"cell_type":"markdown","source":"# Load dataframe"},{"metadata":{"colab_type":"code","id":"Y1C6wdl-yDwU","outputId":"b95dce70-18b2-4ccf-caf8-e71a1e9aed39","colab":{"base_uri":"https://localhost:8080/","height":80},"trusted":false},"cell_type":"code","source":"import os\nfrom google.colab import drive\ndrive.mount('/gdrive')\n!pwd","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"Dk7lPzMO-oLg","colab":{},"trusted":true},"cell_type":"code","source":"#load test and train data\ntrain_data = pd.read_csv(\"../input/tmdb-box-office-prediction/train.csv\")\ntest_data = pd.read_csv(\"../input/tmdb-box-office-prediction/test.csv\")\n","execution_count":5,"outputs":[]},{"metadata":{"colab_type":"code","id":"ffEcdSCs9AJ1","outputId":"344f0447-32dc-4f79-bdd0-f1c97096bccc","colab":{"base_uri":"https://localhost:8080/","height":652},"trusted":false},"cell_type":"code","source":"pd.set_option('display.max_columns', 500)\ntrain_data.head()","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"NesGRmDT7IXA","outputId":"b0f9922b-bb2a-4e22-c5c1-855da3cfc8e0","colab":{"base_uri":"https://localhost:8080/","height":297},"trusted":false},"cell_type":"code","source":"train_data.describe()","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"PjNHc8FFrdAO","outputId":"5fe89d66-6981-4afa-fef7-17967e4680a3","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":false},"cell_type":"code","source":"train_x = train_data.select_dtypes(include=['float',\"int\"])\ntrain_x = train_x.drop(columns=[\"id\"])#irelevant feature\ntrain_x = train_x.drop(columns=[\"revenue\"])#drop the label\ntrain_y=train_data[\"revenue\"]\ntrain_x.head()","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"qA7_FENFo51-","outputId":"1eb2b03f-9b4a-457f-98bb-cf86bf53525b","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":false},"cell_type":"code","source":"test_x = test_data.select_dtypes(include=['float',\"int\"])\ntest_x = test_x.drop(columns=[\"id\"])\ntest_x.head()","execution_count":0,"outputs":[]},{"metadata":{"id":"b8FrZ9F4R1RQ","colab_type":"code","outputId":"5a6e3772-164a-4b83-8e6f-aee761a81f2d","colab":{"base_uri":"https://localhost:8080/","height":68},"trusted":false},"cell_type":"code","source":"#print shapes\nprint(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"g6FWjR32aAnt","outputId":"4b2061d3-b375-47fc-ee7e-918190cbb536","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":false},"cell_type":"code","source":"#make a df with only the selected x and y columns\ndf_train=pd.concat([train_x, train_y], axis=1)\ndf_train.head()","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"U__Mch69x6GR"},"cell_type":"markdown","source":"# EDA on dataframe"},{"metadata":{"colab_type":"code","id":"Td_HEu_CrhpX","outputId":"f24e8d47-2842-409b-8f42-ed1318640274","colab":{"base_uri":"https://localhost:8080/","height":1020},"trusted":false},"cell_type":"code","source":"#plt.subplot()\ndf_train.hist(figsize=(20,15),bins=100,color=\"red\",log=True,layout=(4,1))","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"Wo5ghVukrjnz","outputId":"7702e8fe-9f71-4620-92ad-afd4a919946d","colab":{"base_uri":"https://localhost:8080/","height":735},"trusted":false},"cell_type":"code","source":"ScatterPlots(df_train)","execution_count":0,"outputs":[]},{"metadata":{"id":"Dfi2-P92VtY6","colab_type":"text"},"cell_type":"markdown","source":"not many Nans only very small percentage in runtime feature.  Many zeros in budget"},{"metadata":{"colab_type":"code","id":"otOnxHy2f1N0","outputId":"3535db33-5edd-4219-f528-f1b175602d25","colab":{"base_uri":"https://localhost:8080/","height":544},"trusted":false},"cell_type":"code","source":"CheckValues(train_x,train_y,detail=True)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"let-zP5JrpmT","colab":{},"trusted":false},"cell_type":"code","source":"train_x_clean,train_y_clean= RemoveMissVal(train_x,train_y)\ntrain_x_cut,train_y_cut= RemoveSigma(train_x_clean,train_y_clean,3.)\ntest_x_clean,test_x_clean = RemoveMissVal(test_x,test_x)","execution_count":0,"outputs":[]},{"metadata":{"id":"JVTBn-6grcX5","colab_type":"code","outputId":"01928365-8986-4cd0-9c6d-dcdf22dd66e9","colab":{"base_uri":"https://localhost:8080/","height":544},"trusted":false},"cell_type":"code","source":"CheckValues(train_x_clean,train_y_clean,detail=True)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"szBn7EL6mOr-","outputId":"03968958-83ce-4a4e-a1ab-95e8aea119e7","colab":{"base_uri":"https://localhost:8080/","height":735},"trusted":false},"cell_type":"code","source":"df_train_cut=pd.concat([train_x_cut, train_y_cut], axis=1)\nScatterPlots(df_train_cut)","execution_count":0,"outputs":[]},{"metadata":{"id":"3xEXh9NfTrdB","colab_type":"text"},"cell_type":"markdown","source":"maybe films with english original language have more revenue? (on the other hand the pie chart shows that most films are in english). Let's try anyhow to add this categorical variable to a new df as a one hot encoded value (en=1, all others=0)"},{"metadata":{"id":"rqtH-IUfTqF7","colab_type":"code","outputId":"d60d20d1-e4aa-4f08-f398-447d0d9a39a7","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":false},"cell_type":"code","source":"one_hot = pd.get_dummies(train_data['original_language'])\ntrain_hot_lang = train_data.drop('original_language',axis = 1)\none_hot[\"en\"]\ntrain_x_lang=train_x.join(one_hot[\"en\"])\ntrain_x_lang.head()","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"AiEYT5ZL_ezf","outputId":"4cf995a4-9b81-4146-ad08-8d81d37a4158","colab":{"base_uri":"https://localhost:8080/","height":592},"trusted":false},"cell_type":"code","source":"train_data[\"original_language\"].value_counts().plot(kind='pie',figsize= (10,10));","execution_count":0,"outputs":[]},{"metadata":{"id":"f9piMb-8me3P","colab_type":"code","outputId":"f087e8f3-1dc2-4fd6-df5a-7dba36356618","colab":{"base_uri":"https://localhost:8080/","height":752},"trusted":false},"cell_type":"code","source":"df_train_lang=pd.concat([train_x_lang, train_y], axis=1)\nScatterPlots(df_train_lang,n_plots=4)","execution_count":0,"outputs":[]},{"metadata":{"id":"NDfpQgJ9UjIh","colab_type":"text"},"cell_type":"markdown","source":" from the scatter plots budget seems to be the most correlated to revenue, create a df with only budget as feature"},{"metadata":{"id":"seo8qmLlUgoA","colab_type":"code","outputId":"9e1a3393-c3bb-4c42-9482-6add6b9e2c23","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":false},"cell_type":"code","source":"train_x_only_budget = train_x.drop(columns=[\"popularity\",\"runtime\"])\ntrain_x_only_budget.head()","execution_count":0,"outputs":[]},{"metadata":{"id":"Yb5k88CZmVP5","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"train_x_budget_clean,train_y_budget_clean= RemoveMissVal(train_x_only_budget,train_y)\ntrain_x_lang_clean,train_y_lang_clean= RemoveMissVal(train_x_lang,train_y)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"OWulnUp3BRma"},"cell_type":"markdown","source":"# Linear regression"},{"metadata":{"colab_type":"code","id":"XSm84dQDmyfC","outputId":"4b213d04-59c5-4d6a-a2dd-f46419afba6d","colab":{"base_uri":"https://localhost:8080/","height":457},"trusted":false},"cell_type":"code","source":"\nlin_reg = linear_model.LinearRegression(normalize=True)\nlin_reg.fit(train_x_clean, train_y_clean)\n\nprint(\"---coeffs-----\")\nprint(\"coef:\",lin_reg.coef_)\nprint(\"intercept:\",lin_reg.intercept_)\n\ny_pred = lin_reg.predict(train_x_clean)\ny_pred_test= lin_reg.predict(test_x_clean)\n\nprint(\"---scoring----\")\nprint(\"r2 score on y_train is:\", r2_score(train_y_clean, y_pred))\nprint(\"lin reg score:\",lin_reg.score(train_x_clean, train_y_clean))\n\nplt.scatter(train_y,y_pred,color=\"teal\")\nplt.xlabel(\"y train\")\nplt.ylabel(\"y pred\")","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"W8tJfQ0kphVd","outputId":"ae2a9725-6c8a-4b33-d7fa-a65805b2e96b","colab":{"base_uri":"https://localhost:8080/","height":162},"trusted":false},"cell_type":"code","source":"coeff_df = pd.DataFrame(lin_reg.coef_, train_x_clean.columns, columns=['Coefficient'])  \ncoeff_df","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"q154jNnZqXm9","outputId":"e0293765-9688-44f9-bdfd-c4fb7a51a60b","colab":{"base_uri":"https://localhost:8080/","height":379},"trusted":false},"cell_type":"code","source":"df_pred = pd.DataFrame({'Actual': train_y, 'Predicted': y_pred, \"difference\":train_y-y_pred, \"ratio\":train_y/y_pred})\ndf_pred.head(10)","execution_count":0,"outputs":[]},{"metadata":{"id":"Menb2wLknCJc","colab_type":"text"},"cell_type":"markdown","source":"show the true and predicet on a hist for first 100 rows"},{"metadata":{"colab_type":"code","id":"bvSzQdQyrfjd","outputId":"265975ae-383a-45ab-e225-5b576291d85e","colab":{"base_uri":"https://localhost:8080/","height":536},"trusted":false},"cell_type":"code","source":"df_pred = pd.DataFrame({'Actual': train_y[:100], 'Predicted': y_pred[:100]})\ndf_pred.plot(kind='bar',figsize=(50,8))\nplt.show()","execution_count":0,"outputs":[]},{"metadata":{"id":"R7LpvltSnM99","colab_type":"text"},"cell_type":"markdown","source":"cross validation on the training set, compare output of rmsle, r2 with homemade function and r2 with built in sk learn (not sure why we get a difference between the homemade r1 and sk learn r2 here..)"},{"metadata":{"colab_type":"code","id":"Nj3_34WRw8qS","outputId":"152f1b0c-dfdb-4bde-ee41-7685a183b73a","colab":{"base_uri":"https://localhost:8080/","height":88},"trusted":false},"cell_type":"code","source":"cross_val_predict(train_x_clean,train_y_clean, lin_reg, Verbose=True)\nprint(\"cross_val_r2 with standard sk learn lib is: \",(cross_val_score(lin_reg, train_x_clean, train_y_clean, cv=5,scoring=\"r2\")).mean())","execution_count":0,"outputs":[]},{"metadata":{"id":"WYf44tKAnlDf","colab_type":"text"},"cell_type":"markdown","source":"get the regressor parameters that give the best fit with GridSearchCV. Results show the defaults were indeed the best with the addition that normalize must be  set to True (which I was doing)"},{"metadata":{"colab_type":"code","id":"jL7KaNnNXLkd","outputId":"940fa1b6-2a5d-4051-ae0a-1383dad1aad3","colab":{"base_uri":"https://localhost:8080/","height":54},"trusted":false},"cell_type":"code","source":"lin_reg.get_params().keys()","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"OjC1mN57V_d0","outputId":"2c30a3d3-040a-4bf9-8994-9e38966dd6c7","colab":{"base_uri":"https://localhost:8080/","height":71},"trusted":false},"cell_type":"code","source":"#try something new here with gridSearchCV\nparams = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\ngrid = GridSearchCV(estimator=lin_reg,param_grid=params, cv=5, n_jobs=-1,scoring=\"r2\")\n\ngrid.fit(train_x_clean, train_y_clean)\nprint('Best parameters:', grid.best_params_)\nprint('Best performance:', grid.best_score_)\n\n","execution_count":0,"outputs":[]},{"metadata":{"id":"E_hygXwQoW9g","colab_type":"text"},"cell_type":"markdown","source":"look for the best subset with the built in function. Based on the r2 score the subset (0,1,2) (i.e containing columns budget, popularity and runtime) is the best. RMSLE suggests to take only the oth colum (i.e only budget)"},{"metadata":{"colab_type":"code","id":"kDsqcNADb2Mu","outputId":"50368b57-b477-4200-dd34-7ed70f32830a","colab":{"base_uri":"https://localhost:8080/","height":105},"trusted":false},"cell_type":"code","source":"best_subset(lin_reg, train_x_clean, train_y_clean, max_size=8, cv=5)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"-nj3tPSAsz_Z","outputId":"4e9c5428-a140-4d7b-c353-6f0b9c057045","colab":{"base_uri":"https://localhost:8080/","height":105},"trusted":false},"cell_type":"code","source":"best_subset(lin_reg, train_x_clean, train_y_clean, max_size=8, cv=5, use_rmsle=True,verbose=False)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"kzsZNTDSuNbA","outputId":"ad94ee2e-5e97-43a1-c38d-5bdd70dc27b6","colab":{"base_uri":"https://localhost:8080/","height":105},"trusted":false},"cell_type":"code","source":"best_subset(lin_reg, train_x_cut, train_y_cut, max_size=8, cv=5, use_rmsle=True,verbose=False)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"Z_UwS0nWDnn7"},"cell_type":"markdown","source":"# MLP regressor"},{"metadata":{"colab_type":"code","id":"ZzAWvcGW57--","outputId":"c97e118e-b659-49a7-cbe8-38dfd019bd76","colab":{"base_uri":"https://localhost:8080/","height":697},"trusted":false},"cell_type":"code","source":"mlp=MLPRegressor(hidden_layer_sizes=(100,),alpha=1e-4,learning_rate_init=0.001,verbose=10,tol=0.00001,solver='adam',activation=\"relu\")\nmlp.fit(train_x_clean, train_y_clean)\nprint(\"score:\",mlp.score(train_x_clean,train_y_clean))\nprint('current loss computed with the loss function: ',mlp.loss_)\n#print('coefs: ', mlp.coefs_)\n#print('intercepts: ',mlp.intercepts_)\nprint(' number of iterations for the solver: ', mlp.n_iter_)\nprint('num of layers: ', mlp.n_layers_)\n#print('Num of o/p: ', mlp.n_outputs_)\n","execution_count":0,"outputs":[]},{"metadata":{"id":"3dfw1BOIsduB","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"","execution_count":0,"outputs":[]},{"metadata":{"id":"rWIP8T7ir3va","colab_type":"text"},"cell_type":"markdown","source":"Score is comparable to the one from linear regression. Don't understand why the loss is soo high though, tried tweeking various input parameters in the MLP but doesn't change.."},{"metadata":{"colab_type":"code","id":"L5dupBfteaBj","outputId":"2e4ad7cd-a663-42d9-9104-8793e2a69cc9","colab":{"base_uri":"https://localhost:8080/","height":465},"trusted":false},"cell_type":"code","source":"#plot the output and compare with lin reg\ny_pred_mlp=mlp.predict(train_x_clean)\ny_pred_test_mlp=mlp.predict(test_x_clean)\nplt.figure(figsize=(6, 4), dpi=100)\nplt.scatter(train_y_clean,y_pred_mlp, c=\"orange\",label=\"MLP-regressor\",marker=\".\")\nplt.scatter(train_y_clean,y_pred, color=\"teal\",alpha=0.5,label=\"lin reg.\",marker=\".\")\nplt.xlabel(\"y train\")\nplt.ylabel(\"y pred\")\nplt.legend(loc=0)\n\nprint(mlp.coefs_[0].shape)\nprint(mlp.coefs_[1].shape)\n\n","execution_count":0,"outputs":[]},{"metadata":{"id":"X4g_NCgmtgnF","colab_type":"code","outputId":"b4d14909-2805-42a4-9207-aa8f8c39297a","colab":{"base_uri":"https://localhost:8080/","height":130},"trusted":false},"cell_type":"code","source":"plots to compare linear and MLP","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"hgSEtEwIcsEP","colab":{},"trusted":false},"cell_type":"code","source":"print(\"---coeffs-----\")\nprint(\"coef:\",lin_reg.coef_)\nprint(\"intercept:\",lin_reg.intercept_)\n\nprint(train_x_clean[\"budget\"].values)\nf = plt.figure(figsize=(20,5))\nf.add_subplot(131)\nplt.gca().set_title('budget')\nplt.xlabel(\"budget\")\nplt.ylabel(\"revenue\")\nplt.scatter(train_x_clean[\"budget\"],train_y_clean,label=\"train\")\nplt.scatter(train_x_clean[\"budget\"],y_pred,label=\"predicted lin. reg\")\nplt.scatter(train_x_clean[\"budget\"],y_pred_mlp,label=\"predicted MLP\")\nx=np.linspace(0,4e8,1000)\nplt.plot(x,lin_reg.coef_[0]*x+lin_reg.intercept_,color=\"r\")\nplt.legend(loc=0)\n\nf.add_subplot(132)\nplt.gca().set_title('popularity')\nplt.xlabel(\"popularity\")\nplt.ylabel(\"revenue\")\nplt.xlim(-10,100)\nplt.scatter(train_x_clean[\"popularity\"],train_y_clean,label=\"train\")\nplt.scatter(train_x_clean[\"popularity\"],y_pred,label=\"predicted lin. reg\")\nplt.scatter(train_x_clean[\"popularity\"],y_pred_mlp,label=\"predicted MLP\")\nx=np.linspace(0,1000,100)\nplt.plot(x,lin_reg.coef_[1]*x+lin_reg.intercept_,color=\"r\")\n#plt.scatter(test_x_clean[\"popularity\"],y_pred_test_mlp,label=\"predicted MLP test\")\nplt.legend(loc=0)\n\nf.add_subplot(133)\nplt.gca().set_title('runtime')\nplt.xlabel(\"runtime\")\nplt.ylabel(\"revenue\")\n#plt.xlim(0,100)\nplt.scatter(train_x_clean[\"runtime\"],train_y_clean,label=\"train\")\nplt.scatter(train_x_clean[\"runtime\"],y_pred,label=\"predicted lin. reg\")\nplt.scatter(train_x_clean[\"runtime\"],y_pred_mlp,label=\"predicted MLP\")\nx=np.linspace(0,1000,100)\nplt.plot(x,lin_reg.coef_[2]*x+lin_reg.intercept_,color=\"r\")\nplt.legend(loc=0)\n\n","execution_count":0,"outputs":[]},{"metadata":{"id":"GURNrTFLtz2T","colab_type":"text"},"cell_type":"markdown","source":"find the best subset, based on r2 score the best subset is 0,1 with score of 0.549"},{"metadata":{"colab_type":"code","id":"NOgRHdQZvMMG","colab":{},"trusted":false},"cell_type":"code","source":"mlp=MLPRegressor(hidden_layer_sizes=(100,),alpha=1e-4,learning_rate_init=0.001,tol=0.00001,solver='adam',activation=\"relu\")\nbest_subset(mlp, train_x_clean, train_y_clean, max_size=8, cv=5, use_rmsle=False,verbose=False)","execution_count":0,"outputs":[]},{"metadata":{"id":"pngdoAHkwkRl","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"","execution_count":0,"outputs":[]},{"metadata":{"id":"9QMtrvXLvh-F","colab_type":"text"},"cell_type":"markdown","source":"perform grid search to find best set of  parameters (only some scans shown there, to be tunes in params)"},{"metadata":{"id":"rJV2P1b8ugV5","colab_type":"code","colab":{},"trusted":false},"cell_type":"code","source":"params = {'hidden_layer_sizes': [i for i in range(95,105)],\n              'activation': ['relu'],\n              'solver': ['adam'],\n              'learning_rate': ['constant'],\n              'learning_rate_init': [0.001],\n              'power_t': [0.5],\n              'alpha': [0.0001],\n              'max_iter': [1000],\n              'early_stopping': [False,True],\n              'warm_start': [False]}\n\n#grid = GridSearchCV(estimator=lin_reg,param_grid=params, cv=5, n_jobs=-1,scoring=\"r2\")\n\ngrid = GridSearchCV(mlp, param_grid=params, scoring=\"r2\",\n                   cv=5, pre_dispatch='2*n_jobs')\ngrid.fit(train_x_clean, train_y_clean)\nprint('Best parameters:', grid.best_params_)\nprint('Best performance:', grid.best_score_)","execution_count":0,"outputs":[]},{"metadata":{"id":"W7WwCvXsw3yd","colab_type":"text"},"cell_type":"markdown","source":"validation and learning curves. The cross validation score follows closely the training score, something strange there. To be checked"},{"metadata":{"colab_type":"code","id":"zvGwravZpx34","colab":{},"trusted":false},"cell_type":"code","source":"#hidden_layer_sizes\nscan_range=np.arange(1, 100, 20)\nprint(scan_range)\ntrain_scores, valid_scores = validation_curve(MLPRegressor(tol=1e-4), train_x_clean, train_y_clean, \"hidden_layer_sizes\",scan_range,cv=5)\nPlotValidationCurve(train_scores,valid_scores,scan_range,\"hidden_layer_size\",verbose=True)\n","execution_count":0,"outputs":[]},{"metadata":{"id":"Dnh50pP47zU7","colab_type":"code","outputId":"4776d517-2515-40d2-f8cf-219374179903","colab":{"base_uri":"https://localhost:8080/","height":136},"trusted":false},"cell_type":"code","source":"model = MLPRegressor(activation=\"relu\",random_state=9,hidden_layer_sizes=(150,100,20,),learning_rate_init=0.01)\nmodel.fit(train_x_clean, train_y_clean)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"bQaMlLvrXBmx","outputId":"0bd94af5-e943-48ae-b86d-af419f450068","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":false},"cell_type":"code","source":"# when does the parameter alpha kick in ?\nsumit = 0\nalpha = 1e+20\nloss_all = 4101055500334117.5 # I took the overall loss previously computed\nfor i in range(len(model.coefs_[0][0])): # let's take the last layer of MLP\n  #print(sumit,model.coefs_[0][0][i])\n  sumit+= alpha*model.coefs_[0][0][i]**2\nprint(sumit/loss_all) # the effect","execution_count":0,"outputs":[]},{"metadata":{"id":"9kswZK56837Y","colab_type":"code","outputId":"955a848b-0f8b-4b6e-8567-dc2c5241d864","colab":{"base_uri":"https://localhost:8080/","height":544},"trusted":false},"cell_type":"code","source":"model.loss_curve_","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"dg-6h64CLNtZ","outputId":"38ae92c7-f5af-4126-9335-6c15a4c445a1","colab":{"base_uri":"https://localhost:8080/","height":414},"trusted":false},"cell_type":"code","source":"scan_range=np.logspace(15, 19, 5)\ntrain_scores, valid_scores = validation_curve(MLPRegressor(activation=\"relu\",random_state=9,hidden_layer_sizes=(150,100,20,),learning_rate_init=0.01), train_x_clean, train_y_clean, \"alpha\",scan_range,cv=2)\nPlotValidationCurve(train_scores,valid_scores,scan_range,\"alpha\",logx=True)","execution_count":0,"outputs":[]},{"metadata":{"id":"0e5AqOb7xJHk","colab_type":"text"},"cell_type":"markdown","source":"No improvement with the regularisation term. Even with alpha 1e6 seems we don't overfit.. This is to be checked"},{"metadata":{"colab_type":"code","id":"xeYE4utP5LWm","colab":{},"trusted":false},"cell_type":"code","source":"","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"XwNOG5C03Poi","outputId":"f1384138-fe5e-43ae-b2fc-2abac385f928","colab":{"base_uri":"https://localhost:8080/","height":412},"trusted":false},"cell_type":"code","source":"train_sizes, train_scores, valid_scores = learning_curve(MLPRegressor(activation=\"relu\",random_state=9), train_x_clean, train_y_clean, train_sizes=[0.1, 0.15,0.5, 1], cv=5)\nPlotLearningCurve(train_sizes,train_scores,valid_scores,scan_range,\"train size\",ymin=0.2,ymax=0.8)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"Uj3zwjG5BEdR","outputId":"0c85d7a4-f581-4e2e-a837-d0078fbf5c2d","colab":{"base_uri":"https://localhost:8080/","height":412},"trusted":false},"cell_type":"code","source":"train_sizes, train_scores, valid_scores = learning_curve(linear_model.LinearRegression(), train_x_clean, train_y_clean, train_sizes=[0.1, 0.15,0.5, 1], cv=5)\nPlotLearningCurve(train_sizes,train_scores,valid_scores,scan_range,\"train size\",ymin=0.2,ymax=0.8)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"c9zAcqnJBWgF","outputId":"523f5dc3-ba43-4ce6-cfdc-a71aec5e0618","colab":{"base_uri":"https://localhost:8080/","height":38},"trusted":false},"cell_type":"code","source":"%%html\n<marquee style='width: 30%; color: blue;'><b>Try to see if we get improvements with the only buget and with added language feature</b></marquee>","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"4CSG26v5Ed1O","outputId":"65eaa7cb-4f64-4d02-f6d6-df1e924a1b76","colab":{"base_uri":"https://localhost:8080/","height":17},"trusted":false},"cell_type":"code","source":"%%html\n<!--<marquee style='width: 30%; color: blue;'><b>Well  not quite, try with df adding language and also df with only budget </b></marquee>-->","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"L2auhVj6vw0t","outputId":"ce96c83d-b5e0-43ea-8a05-e9878b17e9b2","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":false},"cell_type":"code","source":"train_x_lang_clean.head()","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"WpcV1Lfs06nd","outputId":"a1fcc172-3cf7-433c-ca44-439978a47113","colab":{"base_uri":"https://localhost:8080/","height":712},"trusted":false},"cell_type":"code","source":"df_train_lang=pd.concat([train_x_lang_clean, train_y_clean], axis=1)\nScatterPlots(df_train_lang,n_plots=4)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"AB8GZJ4_1bEl","outputId":"2f8e6cd3-499e-4735-97ea-2b4d35a303e7","colab":{"base_uri":"https://localhost:8080/","height":102},"trusted":false},"cell_type":"code","source":"mlp=MLPRegressor(hidden_layer_sizes=(20,20,20,),alpha=0.0001,random_state=9,max_iter=200)\nmlp.fit(train_x_lang_clean, train_y_lang_clean)\nprint(\"score:\",mlp.score(train_x_lang_clean,train_y_lang_clean))\nprint('current loss computed with the loss function: ',mlp.loss_)\n#print('coefs: ', mlp.coefs_)\n#print('intercepts: ',mlp.intercepts_)\nprint(' number of iterations for the solver: ', mlp.n_iter_)\nprint('num of layers: ', mlp.n_layers_)\nprint('Num of o/p: ', mlp.n_outputs_)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"f6_rWpln18E6","outputId":"632759ae-39c9-4b10-ef2b-a3b2a099fdf9","colab":{"base_uri":"https://localhost:8080/","height":383},"trusted":false},"cell_type":"code","source":"best_subset(mlp, train_x_lang_clean, train_y_lang_clean, max_size=8, cv=5, use_rmsle=True,verbose=False)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"npw0TyOG4mhU","outputId":"a69b470c-b58e-4f67-b5e9-172e89da8a7e","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":false},"cell_type":"code","source":"best_subset(mlp, train_x_budget_clean, train_y_budget_clean, max_size=8, cv=5, use_rmsle=False,verbose=False)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"q-JgwZXN4LY9","outputId":"90fecb6b-397f-427e-d772-2c0657fd6201","colab":{"base_uri":"https://localhost:8080/","height":264},"trusted":false},"cell_type":"code","source":"grid.fit(train_x_lang_clean, train_y_lang_clean)\nprint('Best parameters:', grid.best_params_)\nprint('Best performance:', grid.best_score_)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"zVggUEEF4YMM","outputId":"3e6b2951-76b3-4ee4-e19b-0a1b797405b4","colab":{"base_uri":"https://localhost:8080/","height":244},"trusted":false},"cell_type":"code","source":"grid.fit(train_x_budget_clean, train_y_budget_clean)\nprint('Best parameters:', grid.best_params_)\nprint('Best performance:', grid.best_score_)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"uLLgJaIY_5rO","outputId":"3b8ea683-8d11-4586-94d0-3177ee2bf4bd","colab":{"base_uri":"https://localhost:8080/","height":673},"trusted":false},"cell_type":"code","source":"scan_range=np.arange(1, 100, 20)\nprint(scan_range)\ntrain_scores, valid_scores = validation_curve(MLPRegressor(tol=1e-4), train_x_budget_clean, train_y_budget_clean, \"hidden_layer_sizes\",scan_range,cv=5)\nPlotValidationCurve(train_scores,valid_scores,scan_range,\"hidden_layer_size\",verbose=True)","execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"9EpstCiqHO35"},"cell_type":"markdown","source":"* validation curve and gridsearchCV: Tried many parameters tuning on the MLP, the most sensitive seems to be the hidden_layer_size (should be above 20 no point in going above), and the activation function (relu is the best)\n* learning curve: points to a dataset of more than 1000 from the training score (although the cv score is flat independently of the data set size, so something strange there)\n* seems I cannot overfit even with a very large reg. term\n* best_subset: adding the language feature doesn't improve the score. The best score of 0.55  is acheived from fitting only the budget with the MLP.\n"},{"metadata":{"colab_type":"code","id":"p7pSkAKCJIPl","outputId":"3be429b6-0ee1-4b13-b2d0-68d987fcdc97","colab":{"base_uri":"https://localhost:8080/","height":38},"trusted":false},"cell_type":"code","source":"%%html\n<marquee style='width: 30%; color: blue;'><b>Whee finished!</b></marquee>","execution_count":0,"outputs":[]}],"metadata":{"colab":{"name":"MLP_movie_save.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["OWulnUp3BRma"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":1}