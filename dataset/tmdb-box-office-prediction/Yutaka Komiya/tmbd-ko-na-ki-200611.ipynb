{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TMDB prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport ast\nimport json\nimport collections\nfrom collections import Counter\n\nimport string\n#from janome.tokenizer import Tokenizer\nimport re\nfrom nltk.corpus import stopwords\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error \n\n%precision 3\npd.set_option('precision', 3)\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#データを読み取る\n#\ntrain = pd.read_csv('../input/tmdb-box-office-prediction/train.csv')\n#\ntest = pd.read_csv('../input/tmdb-box-office-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape,test.shape)\ntrain.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 調べた欠測データ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train['id'] == 391,'runtime'] = 96 #The Worst Christmas of My Lifeの上映時間を調べて入力\ntrain.loc[train['id'] == 592,'runtime'] = 90 #А поутру они проснулисьの上映時間を調べて入力\ntrain.loc[train['id'] == 925,'runtime'] = 86 #¿Quién mató a Bambi?の上映時間を調べて入力\ntrain.loc[train['id'] == 978,'runtime'] = 93 #La peggior settimana della mia vitaの上映時間を調べて入力\ntrain.loc[train['id'] == 1256,'runtime'] = 92 #Cry, Onion!の上映時間を調べて入力\ntrain.loc[train['id'] == 1542,'runtime'] = 93 #All at Onceの上映時間を調べて入力\ntrain.loc[train['id'] == 1875,'runtime'] = 93 #Vermistの上映時間を調べて入力\ntrain.loc[train['id'] == 2151,'runtime'] = 108 #Mechenosetsの上映時間を調べて入力\ntrain.loc[train['id'] == 2499,'runtime'] = 86 #Na Igre 2. Novyy Urovenの上映時間を調べて入力\ntrain.loc[train['id'] == 2646,'runtime'] = 98 #My Old Classmateの上映時間を調べて入力\ntrain.loc[train['id'] == 2786,'runtime'] = 111 #Revelationの上映時間を調べて入力\ntrain.loc[train['id'] == 2866,'runtime'] = 96 #Tutto tutto niente nienteの上映時間を調べて入力","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.loc[test['id'] == 3244,'runtime'] = 93 #La caliente niña Julietta\tの上映時間を調べて入力\ntest.loc[test['id'] == 4490,'runtime'] = 90 #Pancho, el perro millonarioの上映時間を調べて入力\ntest.loc[test['id'] == 4633,'runtime'] = 108 #Nunca en horas de claseの上映時間を調べて入力\ntest.loc[test['id'] == 6818,'runtime'] = 90 #Miesten välisiä keskustelujaの上映時間を調べて入力\n\ntest.loc[test['id'] == 4074,'runtime'] = 103 #Shikshanachya Aaicha Ghoの上映時間を調べて入力\ntest.loc[test['id'] == 4222,'runtime'] = 91 #Street Knightの上映時間を調べて入力\ntest.loc[test['id'] == 4431,'runtime'] = 96 #Plus oneの上映時間を調べて入力\ntest.loc[test['id'] == 5520,'runtime'] = 86 #Glukhar v kinoの上映時間を調べて入力\ntest.loc[test['id'] == 5845,'runtime'] = 83 #Frau Müller muss weg!の上映時間を調べて入力\ntest.loc[test['id'] == 5849,'runtime'] = 140 #Shabdの上映時間を調べて入力\ntest.loc[test['id'] == 6210,'runtime'] = 104 #The Last Breathの上映時間を調べて入力\ntest.loc[test['id'] == 6804,'runtime'] = 140 #Chaahat Ek Nasha...の上映時間を調べて入力\ntest.loc[test['id'] == 7321,'runtime'] = 87 #El truco del mancoの上映時間を調べて入力","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## df作成","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train, test]).set_index(\"id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#columnsを確認し、除外する変数をdrop\nprint(df.columns)\n# 使わない列を消す\ndf = df.drop([\"poster_path\", \"status\", \"original_title\"], axis=1) # \"overview\",  \"imdb_id\", ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# logを取っておく\ndf[\"log_revenue\"] = np.log10(df[\"revenue\"])\n# homepage: 有無に\ndf[\"homepage\"] = ~df[\"homepage\"].isnull()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 各列の処理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfdic_feature = {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# JSON text を辞書型のリストに変換\nimport ast\ndict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\nfor col in dict_columns:\n       df[col]=df[col].apply(lambda x: [] if pd.isna(x) else ast.literal_eval(x) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 各ワードの有無を表す 01 のデータフレームを作成\ndef count_word_list(series):\n    len_max = series.apply(len).max() # ジャンル数の最大値\n    tmp = series.map(lambda x: x+[\"nashi\"]*(len_max-len(x))) # listの長さをそろえる\n    \n    word_set = set(sum(list(series.values), [])) # 全ジャンル名のset\n    for n in range(len_max):\n        word_dfn = pd.get_dummies(tmp.apply(lambda x: x[n]))\n        word_dfn = word_dfn.reindex(word_set, axis=1).fillna(0).astype(int)\n        if n==0:\n            word_df = word_dfn\n        else:\n            word_df = word_df + word_dfn\n    \n    return word_df#.drop(\"nashi\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## genres","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"genre_names\"] = df[\"genres\"].apply(lambda x : [ i[\"name\"] for i in x])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfdic_feature[\"genre\"] = count_word_list(df[\"genre_names\"])\n# TV movie は1件しかないので削除\ndfdic_feature[\"genre\"] = dfdic_feature[\"genre\"].drop(\"TV Movie\", axis=1)\ndfdic_feature[\"genre\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## original language","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# train内の作品数が10件未満の言語は \"small\" に集約\nn_language = df.loc[:train.index[-1], \"original_language\"].value_counts()\nlarge_language = n_language[n_language>=10].index\ndf.loc[~df[\"original_language\"].isin(large_language), \"original_language\"] = \"small\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"original_language\"] = df[\"original_language\"].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one_hot_encoding\ndfdic_feature[\"original_language\"] = pd.get_dummies(df[\"original_language\"])\n#dfdic_feature[\"original_language\"] = dfdic_feature[\"original_language\"].loc[:, dfdic_feature[\"original_language\"].sum()>0]\ndfdic_feature[\"original_language\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## production company","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"production_names\"] = df[\"production_companies\"].apply(lambda x : [ i[\"name\"] for i in x])\n#.fillna(\"[{'name': 'nashi'}]\").map(to_name_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time tmp = count_word_list(df[\"production_names\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train内の件数が多い物のみ選ぶ\ndef select_top_n(df, topn=9999, nmin=2):  # topn:上位topn件, nmin:作品数nmin以上\n#    if \"small\" in df.columns:\n#        df = df.drop(\"small\", axis=1)\n    n_word = (df.loc[train[\"id\"]]>0).sum().sort_values(ascending=False)\n    # 作品数がnmin件未満\n    smallmin = n_word[n_word<nmin].index\n    # 上位topn件に入っていない\n    smalln = n_word.iloc[topn+1:].index\n    small = set(smallmin) | set(smalln)\n    # 件数の少ないタグのみの作品\n    df[\"small\"] = df[small].sum(axis=1) #>0\n    \n    return df.drop(small, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# trainに50本以上作品のある会社\ndfdic_feature[\"production_companies\"] = select_top_n(tmp, nmin=50)\ndfdic_feature[\"production_companies\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## production contries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 国名のリストに\ndf[\"country_names\"] = df[\"production_countries\"].apply(lambda x : [ i[\"name\"] for i in x])\ndf_country = count_word_list(df[\"country_names\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2か国だったら、0.5ずつに\ndf_country = (df_country.T/df_country.sum(axis=1)).T.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 30作品以上の国のみ\ndfdic_feature[\"production_countries\"] = select_top_n(df_country, nmin=30)\ndfdic_feature[\"production_countries\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Keyword","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"keyword_list\"] = df[\"Keywords\"].apply(lambda x : [ i[\"name\"] for i in x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_topn_onehot(series, topn):\n    # 多いワード順に\n    word_count = pd.Series(collections.Counter(sum(list(series.values), [])))\n    word_count = word_count.sort_values(ascending=False)\n    \n    df_topn = df[[]].copy()  # index のみのDF\n    # 上位topn件のキーワードのみ\n    for word in word_count.iloc[:topn].index:  # .drop(\"nashi\")\n        df_topn[word] = series.apply(lambda x: word in x)*1\n    \n    return df_topn\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfdic_feature[\"Keywords\"] = encode_topn_onehot(df[\"keyword_list\"], 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"num_Keywords\"] = df[\"keyword_list\"].apply(len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## spoken laguages","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"language_names\"] = df[\"spoken_languages\"].apply(lambda x : [ i[\"name\"] for i in x])\ndf[\"n_language\"] = df[\"language_names\"].apply(len)\n# 欠損値は１にする(データを見ると無声映画ではない)\ndf.loc[df[\"n_language\"]==0, \"n_language\"] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 英語が含まれるか否か\ndf[\"speak_English\"] = df[\"language_names\"].apply(lambda x : \"English\" in x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## release_date","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 公開日の欠損1件 id=3829\n# May,2000 (https://www.imdb.com/title/tt0210130/) \n# 日は不明。1日を入れておく\ndf.loc[3829, \"release_date\"] = \"5/1/00\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"release_year\"] = pd.to_datetime(df[\"release_date\"]).dt.year.astype(int)\n# 年の20以降を、2020年より後の未来と判定してしまうので、補正。\ndf.loc[df[\"release_year\"]>2020, \"release_year\"] = df.loc[df[\"release_year\"]>2020, \"release_year\"]-100\n\ndf[\"release_month\"] = pd.to_datetime(df[\"release_date\"]).dt.month.astype(int)\ndf[\"release_day\"] = pd.to_datetime(df[\"release_date\"]).dt.day.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# datetime型に\ndf[\"release_date\"] = df.apply(lambda s: datetime.datetime(\n    year=s[\"release_year\"],month=s[\"release_month\"],day=s[\"release_day\"]), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"release_dayofyear\"] = df[\"release_date\"].dt.dayofyear\ndf[\"release_dayofweek\"] = df[\"release_date\"].dt.dayofweek","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 月、曜日は カテゴリ型に\ndf[\"release_month\"] = df[\"release_month\"].astype('category')\ndf[\"release_dayofweek\"] = df[\"release_dayofweek\"].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## belongs to collection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# collection 名を抽出\ndf[\"collection_name\"] = df[\"belongs_to_collection\"].apply(lambda x : x[0][\"name\"] if len(x)>0 else \"nashi\")\n# 無い場合、\"nashi\"に","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# シリーズの作品数\n#df = pd.merge( df, df.groupby(\"collection_name\").count()[[\"budget\"]].rename(columns={\"budget\":\"count_collection\"}), \n#         on=\"collection_name\", how=\"left\")\n# indexがずれるので、戻す\n#df.index = df.index+1\n\ndf[\"count_collection\"] = df[\"collection_name\"].apply(lambda x : (df[\"collection_name\"]==x).sum())\n# シリーズ以外の場合0\ndf.loc[df[\"collection_name\"]==\"nashi\", \"count_collection\"] = 0\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# シリーズ何作目か\ndf[\"number_in_collection\"] = df.sort_values(\"release_date\").groupby(\"collection_name\").cumcount()+1\n# シリーズ以外の場合0\ndf.loc[df[\"collection_name\"]==\"nashi\", \"number_in_collection\"] = 0\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# 同シリーズの自分より前の作品の平均log(revenue)\ndf[\"collection_av_logrevenue\"] = [ df.loc[(df[\"collection_name\"]==row[\"collection_name\"]) & \n                                          (df[\"number_in_collection\"]<row[\"number_in_collection\"]),\n                                          \"log_revenue\"].mean() \n     for key,row in df.iterrows() ]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 欠損(nashi) の場合、nashi での平均\ndf.loc[df[\"collection_name\"]==\"nashi\", \"collection_av_logrevenue\"] = df.loc[df[\"collection_name\"]==\"nashi\", \"log_revenue\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train に無くtestだけにあるシリーズの場合、シリーズもの全部の平均\ncollection_mean = df.loc[df[\"collection_name\"]!=\"nashi\", \"log_revenue\"].mean()  # シリーズもの全部の平均\ndf[\"collection_av_logrevenue\"] = df[\"collection_av_logrevenue\"].fillna(collection_mean)  \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 連結","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features = pd.concat(dfdic_feature, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## runtime　欠測処理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 欠測と0は、0ではないものの平均で埋める\ndf[\"runtime\"] = df[\"runtime\"].fillna(df.loc[df[\"runtime\"]>0, \"runtime\"].mean())\ndf.loc[df[\"runtime\"]==0, \"runtime\"] = df.loc[df[\"runtime\"]>0, \"runtime\"].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## budget","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.scatter(df[\"budget\"]+1, df[\"log_revenue\"], s=1)\n#plt.xscale(\"log\")\n#plt.xrange([])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 整形","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[[\"original_language\", \"collection_name\"]] = df[[\"original_language\", \"collection_name\"]].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use = df[['budget', 'homepage', 'popularity','runtime','n_language', \n             \"num_Keywords\", \"speak_English\",\n             'release_year', 'release_month','release_dayofweek', \n             'collection_av_logrevenue' ,\"count_collection\",\"number_in_collection\"\n            ]]\ndf_use.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use = pd.get_dummies(df_use)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Additional data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_add = pd.read_csv('../input/tmdb-competition-additional-features/TrainAdditionalFeatures.csv')\ntest_add = pd.read_csv('../input/tmdb-competition-additional-features/TestAdditionalFeatures.csv')\ntrain_add.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.merge(df, pd.concat([train_add, test_add]), on=\"imdb_id\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"add_cols = [\"popularity2\", \"rating\", \"totalVotes\"]\ndf[add_cols] = df[add_cols].fillna(df[add_cols].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2 = pd.read_csv('../input/tmdb-box-office-prediction-more-training-data/additionalTrainData.csv')\ntrain3 = pd.read_csv('../input/tmdb-box-office-prediction-more-training-data/trainV3.csv')\ntrain3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 言語処理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#全て小文字に変換\ndef lower_text(text):\n    return text.lower()\n\n#記号の排除\ndef remove_punct(text):\n    text = text.replace('-', ' ')  # - は単語の区切りとみなす\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ndef remove_stopwords(words, stopwords):#不要な単語を削除\n    words = [word for word in words if word not in stopwords]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstopwords = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# レンマ化\nfrom nltk.stem.wordnet import WordNetLemmatizer\nwnl = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer#ベクトル化\nvec_tfidf = TfidfVectorizer()\n\ndef vectrize_tfidf(series):\n    X = vec_tfidf.fit_transform(series)\n    return pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#短縮形を元に戻す\nshortened = {\n    '\\'m': ' am',\n    '\\'re': ' are',\n    'don\\'t': 'do not',\n    'doesn\\'t': 'does not',\n    'didn\\'t': 'did not',\n    'won\\'t': 'will not',\n    'wanna': 'want to',\n    'gonna': 'going to',\n    'gotta': 'got to',\n    'hafta': 'have to',\n    'needa': 'need to',\n    'outta': 'out of',\n    'kinda': 'kind of',\n    'sorta': 'sort of',\n    'lotta': 'lot of',\n    'lemme': 'let me',\n    'gimme': 'give me',\n    'getcha': 'get you',\n    'gotcha': 'got you',\n    'letcha': 'let you',\n    'betcha': 'bet you',\n    'shoulda': 'should have',\n    'coulda': 'could have',\n    'woulda': 'would have',\n    'musta': 'must have',\n    'mighta': 'might have',\n    'dunno': 'do not know',\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 英語以外","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 英語でよく使う単語が入っていない文章を確認\ndf.loc[df[\"overview\"].apply(lambda x : str(x)).apply(lambda x : lower_text(x)\n                                ).str.contains(\"nan|the|where|with|from|and|for|his|her|over\")==False, \"overview\"]\n#train3.loc[train3[\"overview\"].apply(lambda x : str(x)).apply(lambda x : lower_text(x)).str.contains(\"nan|the|where|with|from|and|for|his|her|over\")==False, \"overview\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_english_overview_id = [157, 2863, 4616]   # 上のデータを目で確認\nno_english_tagline_id = [3255, 3777, 4937]   # Tfidf で非英語の単語があったもの","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### word2vec","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import word2vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_text = [\"overview\", \"tagline\"] # \"title\", \nall_text = pd.concat([df[col_text], train2[col_text], train3[col_text]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 英語以外と\"nan\"は除外\nall_text.loc[no_english_overview_id, \"overview\"] = np.nan\nall_text.loc[no_english_tagline_id, \"tagline\"] = np.nan\nall_text.loc[all_text[\"tagline\"]==\"nan\", \"tagline\"] = np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_texts = all_text.stack()\n\n# 前処理\nall_texts = all_texts.apply(lambda x : str(x))             # \nall_texts = all_texts.apply(lambda x : lower_text(x))      # 小文字に\nall_texts = all_texts.apply(lambda x : remove_punct(x))    # 記号除去\n\nall_texts = all_texts.apply(wnl.lemmatize)                 # レンマ化（複数形などを戻す）\nall_texts = all_texts.apply(lambda x : remove_stopwords(x,stopwords))  # stopward 除去\nfor before, after in shortened.items():\n    all_texts = all_texts.str.replace(before, after)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_texts.to_csv(\"./alltexts_for_w2v.txt\", index=False, header=False)\ndocs = word2vec.LineSentence(\"alltexts_for_w2v.txt\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmodel = word2vec.Word2Vec(docs, sg=0, size=10, min_count=5, window=5, iter=100)\nmodel.save(\"./alltexts_w2v1_cbow10.model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = word2vec.Word2Vec.load(\"./alltexts_w2v1_cbow.model\")\n# model = word2vec.Word2Vec.load(\"./alltexts_w2v1_sg2.model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.most_similar(positive=['father'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.most_similar(positive=['human'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 単語ベクトルの mean, max を文章ベクトルにする\ndef get_doc_vector(doc, method=\"mean\", weight=None):\n    split_doc = doc.split(\" \")\n    if weight==None:\n        weight = dict(zip(model.wv.vocab.keys(), np.ones(len(model.wv.vocab))))\n        \n    word_vecs = [ model[word]*weight[word] for word in split_doc if word in model.wv.vocab.keys() ]\n    \n    if len(word_vecs)==0:\n        doc_vec = []\n    elif method==\"mean\":\n        doc_vec =  np.mean(word_vecs, axis=0)\n    elif method==\"max\":\n        doc_vec =  np.max(word_vecs, axis=0)\n    elif method==\"meanmax\":\n        doc_vec =  np.mean(word_vecs, axis=0)+np.max(word_vecs, axis=0)\n    return doc_vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## overview","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#単語数\ndf['overview_word_count'] = df['overview'].apply(lambda x: len(str(x).split()))\n#文字数\ndf['overview_char_count'] = df['overview'].apply(lambda x: len(str(x)))\n# 記号の個数\ndf['overview_punctuation_count'] = df['overview'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 前処理\ndf['_overview']=df['overview'].apply(lambda x : str(x)\n                            ).apply(lambda x : lower_text(x)).apply(lambda x : remove_punct(x))\n\ndf[\"_overview\"]=df[\"_overview\"].apply(wnl.lemmatize)\ndf[\"_overview\"]=df[\"_overview\"].apply(lambda x : remove_stopwords(x,stopwords))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time df_overview =  df[\"_overview\"].apply(get_doc_vector, method=\"meanmax\").apply(pd.Series)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_overview = df_overview.fillna(0).add_prefix(\"overview_\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## taglineの前処理と特徴量の追加","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#単語数\ndf['tagline_word_count'] = df['tagline'].apply(lambda x: len(str(x).split()))\n#文字数\ndf['tagline_char_count'] = df['tagline'].apply(lambda x: len(str(x)))\n# 記号の個数\ndf['tagline_punctuation_count'] = df['tagline'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['_tagline']=df['tagline'].apply(lambda x : str(x)\n                                 ).apply(lambda x : lower_text(x)).apply(lambda x : remove_punct(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ベクトル化\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# vec_tfidf = TfidfVectorizer()\n# X = vec_tfidf.fit_transform(df['tagline'])\n# Tfidf_tagline = pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())\n# X = vec_tfidf.fit_transform(df['overview'].dropna())\n# Tfidf_overview = pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time df_tagline =  df[\"_tagline\"].apply(get_doc_vector, method=\"meanmax\").apply(pd.Series)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tagline = df_tagline.fillna(0).add_prefix(\"tagline_\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## titleの前処理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#単語数\ndf['title_word_count'] = df['title'].apply(lambda x: len(str(x).split()))\n#文字数\ndf['title_char_count'] = df['title'].apply(lambda x: len(str(x)))\n# 記号の個数\ndf['title_punctuation_count'] = df['title'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use2 = df[[\"tagline_char_count\",\"tagline_word_count\",\"tagline_punctuation_count\",\n              \"overview_char_count\",\"overview_word_count\",\"overview_punctuation_count\",\n              \"title_char_count\",\"title_word_count\",\"title_punctuation_count\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## keywordもword2vecベクトル化すると？","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keywords を全部並べたものを、文とみなしてベクトル化\n%time df_keyword_w2v = df[\"keyword_list\"].apply(\" \".join).apply(get_doc_vector, method=\"mean\").apply(pd.Series)\ndf_keyword_w2v = df_keyword_w2v.fillna(0).add_prefix(\"keyword_\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## cast","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#映画の中にどれだけの人がキャストされたか表示\nprint('Number of casted persons in films')\ndf['cast'].apply(len).value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['num_cast'] = df['cast'].apply(len)  # 人数\n# df['all_cast'] = df['cast'].apply(lambda x: [i['name'] for i in x])  # \n\n# 出演作品数上位50人について、one-hot-encoding\ndf_castname = pd.DataFrame([], index=df.index)\nlist_of_cast_names = list(df['cast'].apply(lambda x: [i['name'] for i in x]).values)  # 俳優名のリストのリスト\ntop_cast_names = [m[0] for m in Counter([i for j in list_of_cast_names for i in j]).most_common(50)]\nfor g in top_cast_names:\n    df_castname[g] = df['cast'].apply(lambda x: g in [i['name'] for i in x])\n\n    \n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"\"\"\"\n#実際の比較\n\ncast_name_Samuel_L_Jackson=df.loc[df['cast_name_Samuel L. Jackson']==1,]\ncast_name_Robert_De_Niro=df.loc[df['cast_name_Robert De Niro']==1,]\ncast_name_Morgan_Freeman=df.loc[df['cast_name_Morgan Freeman']==1,]\ncast_name_J_K_Simmons=df.loc[df['cast_name_J.K. Simmons']==1,]\ncast_name_Bruce_Willis=df.loc[df['cast_name_Bruce Willis']==1,]\ncast_name_Liam_Neeson=df.loc[df['cast_name_Liam Neeson']==1,]\ncast_name_Susan_Sarandon=df.loc[df['cast_name_Susan Sarandon']==1,]\ncast_name_Bruce_McGill=df.loc[df['cast_name_Bruce McGill']==1,]\ncast_name_John_Turturro=df.loc[df['cast_name_John Turturro']==1,]\ncast_name_Forest_Whitaker=df.loc[df['cast_name_Forest Whitaker']==1,]\n\n\ncast_name_Samuel_L_Jackson_revenue=cast_name_Samuel_L_Jackson.mean()['revenue']\ncast_name_Robert_De_Niro_revenue=cast_name_Robert_De_Niro.mean()['revenue']\ncast_name_Morgan_Freeman_revenue=cast_name_Morgan_Freeman.mean()['revenue']\ncast_name_J_K_Simmons_revenue=cast_name_J_K_Simmons.mean()['revenue']\ncast_name_Bruce_Willis_revenue=cast_name_Bruce_Willis.mean()['revenue']\ncast_name_Liam_Neeson_revenue=cast_name_Liam_Neeson.mean()['revenue']\ncast_name_Susan_Sarandon_revenue=cast_name_Susan_Sarandon.mean()['revenue']\ncast_name_Bruce_McGill_revenue=cast_name_Bruce_McGill.mean()['revenue']\ncast_name_John_Turturro_revenue=cast_name_John_Turturro.mean()['revenue']\ncast_name_Forest_Whitaker_revenue=cast_name_Forest_Whitaker.mean()['revenue']\n\n\ncast_revenue_concat = pd.Series([cast_name_Samuel_L_Jackson_revenue,cast_name_Robert_De_Niro_revenue,cast_name_Morgan_Freeman_revenue,cast_name_J_K_Simmons_revenue,\n                                cast_name_Bruce_Willis_revenue,cast_name_Liam_Neeson_revenue,cast_name_Susan_Sarandon_revenue,cast_name_Bruce_McGill_revenue,\n                                cast_name_John_Turturro_revenue,cast_name_Forest_Whitaker_revenue])\ncast_revenue_concat.index=['Samuel L. Jackson','Robert De Niro','Morgan Freeman','J.K. Simmons','Bruce Willis','Liam Neeson','Susan Sarandon','Bruce McGill',\n                            'John Turturro','Forest Whitaker']\n\nfig = plt.figure(figsize=(13, 7))\ncast_revenue_concat.sort_values(ascending=True).plot(kind='barh',title='mean Revenue (100 million dollars) by Top 10 Most Common Cast')\nplt.xlabel('Revenue (100 million dollars)')\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 性別比\nlist_of_cast_genders = list(df['cast'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\n\ndf['genders_0_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))/df[\"num_cast\"]\ndf['genders_1_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))/df[\"num_cast\"]\n\n# 欠損は平均で埋める\ndf[['genders_0_cast', 'genders_1_cast']] = df[['genders_0_cast', 'genders_1_cast']].fillna(df[['genders_0_cast', 'genders_1_cast']].mean())\n\n\n\n# df = df.drop(['cast'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Crew","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"crew\"][1][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 部署別　のべ人数\ndepartment_count = pd.Series(Counter([job for lst in df[\"crew\"].apply(lambda x : [ i[\"department\"] for i in x]).values for job in lst]))\ndepartment_count.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# job別　のべ人数(top30)\njob_count = pd.Series(Counter([job for lst in df[\"crew\"].apply(lambda x : [ i[\"job\"] for i in x]).values for job in lst]))\njob_count.sort_values(ascending=False).head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"job_count[\"Visual Effects\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf_crew = { idx : pd.DataFrame([ [crew[\"department\"], crew[\"job\"], crew[\"name\"]] \n                        for crew in x], columns=[\"department\", \"job\", \"name\"]) \n    for idx, x in df[\"crew\"].iteritems() }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_crew = pd.concat(df_crew)\ndf_crew.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#crewのname\ndf['num_crew'] = df['crew'].apply(len)\n\n# crew gender\ndf['genders_0_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))/df[\"num_crew\"]\ndf['genders_1_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))/df[\"num_crew\"]\n\n# 欠損は平均で埋める\ndf[['genders_0_crew', 'genders_1_crew']] = df[['genders_0_crew', 'genders_1_crew']].fillna(df[['genders_0_crew', 'genders_1_crew']].mean())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def select_job(list_dict, key, value):\n    return [ dic[\"name\"] for dic in list_dict if dic[key]==value]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 各部署の人数\nfor department in department_count.index:\n    df['dep_{}_num'.format(department)] = df[\"crew\"].apply(select_job, key=\"department\", value=department).apply(len)  \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## job","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Animationの人数（アニメ映画で重要そうなので入れてみる）\ndf['job_Animation_num'] = df[\"crew\"].apply(select_job, key=\"job\", value=\"Animation\").apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# 重要と思われるjobについて、参加作品数上位15人で one-hot-encoding\n# 製作、監督、脚本、キャスティング、作曲\ndf_crewname = pd.DataFrame([], index=df.index)\nfor job in [\"Producer\", \"Director\", \"Screenplay\", \"Casting\", \"Original Music Composer\"]:\n    col = 'job_{}_list'.format(job)\n    df[col] = df[\"crew\"].apply(select_job, key=\"job\", value=job)\n\n    top_list = [m[0] for m in Counter([i for j in df[col] for i in j]).most_common(15)]\n    for i in top_list:\n        df_crewname['{}_{}'.format(job,i)] = df[col].apply(lambda x: i in x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 技術部門はdepartment毎に、参加作品数上位15人で one-hot-encoding\nfor job in [\"Sound\", \"Art\", \"Costume & Make-Up\", \"Camera\", \"Visual Effects\"]:\n    col = 'department_{}_list'.format(job)\n    df[col] = df[\"crew\"].apply(select_job, key=\"department\", value=job)\n\n    top_list = [m[0] for m in Counter([i for j in df[col] for i in j]).most_common(15)]\n    for i in top_list:\n        df_crewname['{}_{}'.format(job,i)] = df[col].apply(lambda x: i in x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 監督が複数の作品数\n(df[\"job_Director_list\"].apply(len)>1).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 整理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use3=df[['num_cast', 'genders_0_cast',\n       'genders_1_cast', 'num_crew', 'genders_0_crew', 'genders_1_crew',\n       'dep_Directing_num', 'dep_Writing_num', 'dep_Production_num',\n       'dep_Sound_num', 'dep_Camera_num', 'dep_Editing_num', 'dep_Art_num',\n       'dep_Costume & Make-Up_num', 'dep_Crew_num', 'dep_Lighting_num',\n       'dep_Visual Effects_num', 'dep_Actors_num', 'job_Animation_num']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.index = df.index\n\ndf_use.index = df.index\n# df_use2.index = df.index\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use4 = df[add_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_input = pd.concat([df_use, df_use2, df_use3, df_use4], axis=1) # , df_features .drop(\"belongs_to_collection\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tfid_tagline.index = df_use.index\n#df_use_Tfid = Tfid_tagline.loc[:, Tfid_tagline[:3000].nunique()>1]\n#df_use_Tfid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 全て繋げた特徴量\ndf_input = pd.concat([df_input, df_tagline, df_overview], axis=1)\n# df_tagline, df_overview, df_keyword_w2v, , df_keyword_w2v, df_castname, df_crewname","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 欠測ナシを確認\ndf_input.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cols = df_input.loc[:, df_input.isnull().sum()>0].columns\n#df_input.loc[:, cols] = df_input[cols].fillna(df_input[cols].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 保存\nimport pickle\nwith open('df_input.pkl', 'wb') as f:\n      pickle.dump(df_input , f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"ln_revenue\"] = np.log(df[\"revenue\"]+1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 学習用データ作成","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 数値化できい列を確認\nno_numeric = df_input.apply(lambda s:pd.to_numeric(s, errors='coerce')).isnull().all()\nno_numeric[no_numeric]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_all = df_input[['budget', 'totalVotes', 'popularity', 'release_year', 'popularity2',\n       'runtime', 'rating', 'num_cast']]  # .drop([\"collection_av_logrevenue\"], axis=1)\ny_all = df[\"ln_revenue\"]\ny_all.index = X_all.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[ c for c in X_all.columns if \"revenue\" in str(c)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 標準化\n# X_train_all_mean = X_all[:3000].mean()\n# X_train_all_std  = X_all[:3000].std()\n# X_all = (X_all-X_train_all_mean)/X_train_all_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, val_X, train_y, val_y = train_test_split(X_all[:train.index[-1]], \n                                                  y_all[:train.index[-1]], \n                                                  test_size=0.25, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# randomforest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf2 = RandomForestRegressor(n_jobs=3, random_state=1)  # max_depth=, min_samples_split=, \nclf2.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_pred = clf2.predict(val_X)\nprint(\"RMSLE score for validation data\")\nnp.sqrt(mean_squared_error(val_pred, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(np.exp(val_pred)+1, np.exp(val_y)+1, s=3)\nplt.xlabel(\"prediction\")\nplt.ylabel(\"true revenue\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_importance.sort_values(\"importance\", ascending=False).head(20).index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_importance = pd.DataFrame([clf2.feature_importances_], columns=train_X.columns, index=[\"importance\"]).T\ndf_importance.sort_values(\"importance\", ascending=False).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# submit","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf2 = RandomForestRegressor(n_jobs=3, random_state=1, n_estimators=500)  # \nclf2.fit(X_all[:train.index[-1]], y_all[:train.index[-1]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_importance = pd.DataFrame([clf2.feature_importances_], columns=train_X.columns, index=[\"importance\"]).T\ndf_importance.sort_values(\"importance\", ascending=False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = clf2.predict(X_all[3000:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_revenue = np.exp(test_pred)-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/tmdb-box-office-prediction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_RF = sample_submission.copy()\nsubmission_RF[\"revenue\"] = test_revenue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_RF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_RF.to_csv('submission_RF.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}