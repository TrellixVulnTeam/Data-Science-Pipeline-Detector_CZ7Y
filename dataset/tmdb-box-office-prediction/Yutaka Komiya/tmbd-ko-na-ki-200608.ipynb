{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TMDB prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport ast\nimport json\nimport collections\nfrom collections import Counter\n\nimport string\n#from janome.tokenizer import Tokenizer\nimport re\nfrom nltk.corpus import stopwords\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error \n\n%precision 3\npd.set_option('precision', 3)\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#データを読み取る\n#\ntrain = pd.read_csv('../input/tmdb-box-office-prediction/train.csv')\n#\ntest = pd.read_csv('../input/tmdb-box-office-prediction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape,test.shape)\ntrain.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 調べた欠測データ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train['id'] == 391,'runtime'] = 96 #The Worst Christmas of My Lifeの上映時間を調べて入力\ntrain.loc[train['id'] == 592,'runtime'] = 90 #А поутру они проснулисьの上映時間を調べて入力\ntrain.loc[train['id'] == 925,'runtime'] = 86 #¿Quién mató a Bambi?の上映時間を調べて入力\ntrain.loc[train['id'] == 978,'runtime'] = 93 #La peggior settimana della mia vitaの上映時間を調べて入力\ntrain.loc[train['id'] == 1256,'runtime'] = 92 #Cry, Onion!の上映時間を調べて入力\ntrain.loc[train['id'] == 1542,'runtime'] = 93 #All at Onceの上映時間を調べて入力\ntrain.loc[train['id'] == 1875,'runtime'] = 93 #Vermistの上映時間を調べて入力\ntrain.loc[train['id'] == 2151,'runtime'] = 108 #Mechenosetsの上映時間を調べて入力\ntrain.loc[train['id'] == 2499,'runtime'] = 86 #Na Igre 2. Novyy Urovenの上映時間を調べて入力\ntrain.loc[train['id'] == 2646,'runtime'] = 98 #My Old Classmateの上映時間を調べて入力\ntrain.loc[train['id'] == 2786,'runtime'] = 111 #Revelationの上映時間を調べて入力\ntrain.loc[train['id'] == 2866,'runtime'] = 96 #Tutto tutto niente nienteの上映時間を調べて入力","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.loc[test['id'] == 3244,'runtime'] = 93 #La caliente niña Julietta\tの上映時間を調べて入力\ntest.loc[test['id'] == 4490,'runtime'] = 90 #Pancho, el perro millonarioの上映時間を調べて入力\ntest.loc[test['id'] == 4633,'runtime'] = 108 #Nunca en horas de claseの上映時間を調べて入力\ntest.loc[test['id'] == 6818,'runtime'] = 90 #Miesten välisiä keskustelujaの上映時間を調べて入力\n\ntest.loc[test['id'] == 4074,'runtime'] = 103 #Shikshanachya Aaicha Ghoの上映時間を調べて入力\ntest.loc[test['id'] == 4222,'runtime'] = 91 #Street Knightの上映時間を調べて入力\ntest.loc[test['id'] == 4431,'runtime'] = 96 #Plus oneの上映時間を調べて入力\ntest.loc[test['id'] == 5520,'runtime'] = 86 #Glukhar v kinoの上映時間を調べて入力\ntest.loc[test['id'] == 5845,'runtime'] = 83 #Frau Müller muss weg!の上映時間を調べて入力\ntest.loc[test['id'] == 5849,'runtime'] = 140 #Shabdの上映時間を調べて入力\ntest.loc[test['id'] == 6210,'runtime'] = 104 #The Last Breathの上映時間を調べて入力\ntest.loc[test['id'] == 6804,'runtime'] = 140 #Chaahat Ek Nasha...の上映時間を調べて入力\ntest.loc[test['id'] == 7321,'runtime'] = 87 #El truco del mancoの上映時間を調べて入力","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## df作成","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train, test]).set_index(\"id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"log_revenue\"] = np.log10(df[\"revenue\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#columnsを確認し、除外する変数をdrop\nprint(df.columns)\n# 使わない列を消す\ndf = df.drop([\"poster_path\", \"status\", \"original_title\"], axis=1) # \"overview\",  \"imdb_id\", ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"homepage\"] = ~df[\"homepage\"].isnull()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 各列の処理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfdic_feature = {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#辞書型に変換\nimport ast\ndict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\nfor col in dict_columns:\n       df[col]=df[col].apply(lambda x: [] if pd.isna(x) else ast.literal_eval(x) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## genres","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 文字列から、\"name\" 情報を抽出しリストに\ndef to_name_list(text):\n    txt_list = re.sub(\"[\\[\\]]\", \"\", text).replace(\"}, {\", \"}|{\").split(\"|\")\n    return [ ast.literal_eval(txt)[\"name\"] for txt in txt_list ]\n\ndef to_id_list(text):\n    txt_list = re.sub(\"[\\[\\]]\", \"\", text).replace(\"}, {\", \"}|{\").split(\"|\")\n    return [ ast.literal_eval(txt)[\"id\"] for txt in txt_list ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"genre_names\"] = df[\"genres\"].fillna(\"[{'name': 'nashi'}]\").map(to_name_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 各ワードの有無を表す 01 のデータフレームを作成\ndef count_word_list(series):\n    len_max = series.apply(len).max() # ジャンル数の最大値\n    tmp = series.map(lambda x: x+[\"nashi\"]*(len_max-len(x))) # listの長さをそろえる\n    \n    word_set = set(sum(list(series.values), [])) # 全ジャンル名のset\n    for n in range(len_max):\n        word_dfn = pd.get_dummies(tmp.apply(lambda x: x[n]))\n        word_dfn = word_dfn.reindex(word_set, axis=1).fillna(0).astype(int)\n        if n==0:\n            word_df = word_dfn\n        else:\n            word_df = word_df + word_dfn\n    \n    return word_df.drop(\"nashi\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfdic_feature[\"genre\"] = count_word_list(df[\"genre_names\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TV movie は1件しかないので削除\ndfdic_feature[\"genre\"] = dfdic_feature[\"genre\"].drop(\"TV Movie\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## original language","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# train内の作品数が10件未満の言語は \"small\" に集約\nn_language = df.loc[:train.index[-1], \"original_language\"].value_counts()\nlarge_language = n_language[n_language>=10].index\ndf.loc[~df[\"original_language\"].isin(large_language), \"original_language\"] = \"small\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one_hot_encoding\ndfdic_feature[\"original_language\"] = pd.get_dummies(df[\"original_language\"])\ndfdic_feature[\"original_language\"] = dfdic_feature[\"original_language\"].loc[:, dfdic_feature[\"original_language\"].sum()>0]\ndfdic_feature[\"original_language\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## production company","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"production_names\"] = df[\"production_companies\"].fillna(\"[{'name': 'nashi'}]\").map(to_name_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = count_word_list(df[\"production_names\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train内の件数が多い物のみ選ぶ\ndef select_top_n(df, topn=9999, nmin=2):  # topn:上位topn件, nmin:作品数nmin以上\n    if \"small\" in df.columns:\n        df = df.drop(\"small\", axis=1)\n    n_word = (df.loc[train.index]>0).sum().sort_values(ascending=False)\n    # 作品数がnmin件未満\n    smallmin = n_word[n_word<nmin].index\n    # 上位topn件に入っていない\n    smalln = n_word.iloc[topn+1:].index\n    small = set(smallmin) | set(smalln)\n    # 件数の少ないタグのみの作品\n    df[\"small\"] = (df[small].sum(axis=1)>0)*1\n    \n    return df.drop(small, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# trainに50本以上作品のある会社\ndfdic_feature[\"production_companies\"] = select_top_n(tmp, nmin=50)\ndfdic_feature[\"production_companies\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## production contries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 国名のリストに\ndf[\"country_names\"] = df[\"production_countries\"].str.replace(\"United States of America\", \"USA\"\n                                                            ).fillna(\"[{'name': 'nashi'}]\").map(to_name_list)\ndf_country = count_word_list(df[\"country_names\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2か国だったら、0.5ずつに\ndf_country = (df_country.T/df_country.sum(axis=1)).T.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 30作品以上の国のみ\ndfdic_feature[\"production_countries\"] = select_top_n(df_country, nmin=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Keyword","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"keyword_list\"] = df[\"Keywords\"].fillna(\"[{'name': 'nashi'}]\").map(to_name_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 全キーワードの種類\nkeyword_set = set(sum(list(df[\"keyword_list\"].values), []))\nlen(keyword_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 多いキーワードtop20\nkeyword_count = pd.Series(collections.Counter(sum(list(df[\"keyword_list\"].values), [])))\nkeyword_count = keyword_count.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_keyword = df[[]].copy()\n# 上位１００件のキーワードのみ\nfor word in keyword_count.drop(\"nashi\").iloc[:100].index:\n    df_keyword[word] = df[\"keyword_list\"].apply(lambda x: word in x)*1\n\ndfdic_feature[\"Keywords\"] = df_keyword","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## spoken laguages","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_tag_list(text, tag):\n    txt_list = re.sub(\"[\\[\\]]\", \"\", text).replace(\"}, {\", \"}|{\").split(\"|\")\n    return [ ast.literal_eval(txt)[tag] for txt in txt_list ]\n\n# df[\"language_names\"] = df[\"spoken_languages\"].fillna(\"[{'iso_639_1': 'nashi'}]\").apply(to_tag_list, tag = 'iso_639_1')\n# df_spklanguage = count_word_list(df[\"language_names\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"language_names\"] = df[\"spoken_languages\"].fillna(\"[{'iso_639_1': 'nashi'}]\").apply(to_tag_list, tag = 'iso_639_1')\n# 欠損値は１になる\ndf[\"n_language\"] = df[\"language_names\"].apply(len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## release_date","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 公開日の欠損1件\n# May,2000 (https://www.imdb.com/title/tt0210130/) \n# 日は不明。1日を入れておく\ndf.loc[3829, \"release_date\"] = \"5/1/00\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"release_year\"] = pd.to_datetime(df[\"release_date\"]).dt.year.astype(int)\n# 年の30を、2030年と判定してしまうので、補正。\ndf.loc[df[\"release_year\"]>2020, \"release_year\"] = df.loc[df[\"release_year\"]>2020, \"release_year\"]-100\n\ndf[\"release_month\"] = pd.to_datetime(df[\"release_date\"]).dt.month.astype(int)\ndf[\"release_day\"] = pd.to_datetime(df[\"release_date\"]).dt.day.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"release_date\"] = df.apply(lambda s: datetime.datetime(\n    year=s[\"release_year\"],month=s[\"release_month\"],day=s[\"release_day\"]), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"release_dayofyear\"] = df[\"release_date\"].dt.dayofyear\ndf[\"release_dayofweek\"] = df[\"release_date\"].dt.dayofweek","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"release_month\"] = df[\"release_month\"].astype('category')\ndf[\"release_dayofweek\"] = df[\"release_dayofweek\"].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## belongs to collection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# collection 名を抽出\n# 欠損は nashi\ndf[\"collection_name\"] = df[\"belongs_to_collection\"].fillna(\"[{'name': 'nashi'}]\").map(to_name_list).map(lambda x: x[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# 同シリーズの自分以外の作品の平均log(revenue)\ndf[\"collection_av_logrevenue\"] = [ df.drop(n).loc[df[\"collection_name\"]==cname].loc[:3000,\"log_revenue\"].mean() \n     for n,cname in df[\"collection_name\"].iteritems() ]\n# 欠損(nashi) の場合、nashi での平均","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train に無くtestだけにあるシリーズの場合、シリーズもの全部の平均\ncollection_mean = df.loc[df[\"collection_name\"]!=\"nashi\", \"log_revenue\"].mean()  # シリーズもの全部の平均\ndf[\"collection_av_logrevenue\"] = df[\"collection_av_logrevenue\"].fillna(collection_mean)  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# シリーズの作品数\ndf = pd.merge( df, df.groupby(\"collection_name\").count()[[\"budget\"]].rename(columns={\"budget\":\"count_collection\"}), \n         on=\"collection_name\", how=\"left\")\n# シリーズ以外の場合0\ndf.loc[df[\"collection_name\"]==\"nashi\", \"count_collection\"] = 0\n\n# indexがずれるので、戻す\ndf.index = df.index+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# シリーズ何作目か\ndf[\"number_in_collection\"] = df.sort_values(\"release_date\").groupby(\"belongs_to_collection\").cumcount()+1\n# シリーズ以外の場合0\ndf.loc[df[\"belongs_to_collection\"].isnull(), \"number_in_collection\"] = 0\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 連結","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features = pd.concat(dfdic_feature, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## runtime　欠測処理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 欠測と0は、0ではないものの平均で埋める\ndf[\"runtime\"] = df[\"runtime\"].fillna(df.loc[df[\"runtime\"]>0, \"runtime\"].mean())\ndf.loc[df[\"runtime\"]==0, \"runtime\"] = df.loc[df[\"runtime\"]>0, \"runtime\"].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## budget","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(df[\"budget\"]+1, df[\"log_revenue\"], s=1)\nplt.xscale(\"log\")\n#plt.xrange([])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 整形","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[[\"original_language\", \"collection_name\"]] = df[[\"original_language\", \"collection_name\"]].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use = df[['budget', 'homepage', 'popularity','runtime','n_language', \n             'collection_av_logrevenue', \"number_in_collection\", \"count_collection\", \n             'release_year', 'release_month','release_dayofweek']]\ndf_use.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use = pd.get_dummies(df_use)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Additional data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_add = pd.read_csv('../input/tmdb-competition-additional-features/TrainAdditionalFeatures.csv')\ntest_add = pd.read_csv('../input/tmdb-competition-additional-features/TestAdditionalFeatures.csv')\ntrain_add.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2 = pd.read_csv('../input/tmdb-box-office-prediction-more-training-data/additionalTrainData.csv')\ntrain3 = pd.read_csv('../input/tmdb-box-office-prediction-more-training-data/trainV3.csv')\ntrain3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 言語処理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#全て小文字に変換\ndef lower_text(text):\n    return text.lower()\n\n#記号の排除\ndef remove_punct(text):\n    text = text.replace('-', ' ')  # - は単語の区切りとみなす\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ndef remove_stopwords(words, stopwords):#不要な単語を削除\n    words = [word for word in words if word not in stopwords]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 英語以外","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 英語でよく使う単語が入っていない文章を確認\ndf.loc[df[\"overview\"].apply(lambda x : str(x)).apply(lambda x : lower_text(x)\n                                ).str.contains(\"nan|the|where|with|from|and|for|his|her|over\")==False, \"overview\"]\n#train3.loc[train3[\"overview\"].apply(lambda x : str(x)).apply(lambda x : lower_text(x)).str.contains(\"nan|the|where|with|from|and|for|his|her|over\")==False, \"overview\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_english_overview_id = [157, 2863, 4616]   # 上のデータを目で確認\nno_english_tagline_id = [3255, 3777, 4937]   # Tfidf で非英語の単語があったもの","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### word2vec","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import word2vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_text = [\"overview\", \"tagline\"] # \"title\", \nall_text = pd.concat([df[col_text], train2[col_text], train3[col_text]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 英語以外と\"nan\"は除外\nall_text.loc[no_english_overview_id, \"overview\"] = np.nan\nall_text.loc[no_english_tagline_id, \"tagline\"] = np.nan\nall_text.loc[all_text[\"tagline\"]==\"nan\", \"tagline\"] = np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_texts = all_text.stack()\nall_texts=all_texts.apply(lambda x : str(x))\nall_texts=all_texts.apply(lambda x : lower_text(x))\nall_texts=all_texts.apply(lambda x : remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_texts.to_csv(\"./alltexts_for_w2v.txt\", index=False, header=False)\ndocs = word2vec.LineSentence(\"alltexts_for_w2v.txt\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmodel = word2vec.Word2Vec(docs, sg=1, size=100, min_count=5, window=5, iter=100)\nmodel.save(\"./alltexts_w2v1_sg.model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = word2vec.Word2Vec.load(\"./alltexts_w2v1_cbow.model\")\nmodel = word2vec.Word2Vec.load(\"./alltexts_w2v1_sg.model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.most_similar(positive=['father'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.most_similar(positive=['human'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 単語ベクトルの mean, max を文章ベクトルにする\ndef get_doc_vector(doc, method=\"mean\", weight=None):\n    split_doc = doc.split(\" \")\n    if weight==None:\n        weight = dict(zip(model.wv.vocab.keys(), np.ones(len(model.wv.vocab))))\n        \n    word_vecs = [ model[word]*weight[word] for word in split_doc if word in model.wv.vocab.keys() ]\n    \n    if len(word_vecs)==0:\n        doc_vec = []\n    elif method==\"mean\":\n        doc_vec =  np.mean(word_vecs, axis=0)\n    elif method==\"max\":\n        doc_vec =  np.max(word_vecs, axis=0)\n    elif method==\"meanmax\":\n        doc_vec =  np.mean(word_vecs, axis=0)+np.max(word_vecs, axis=0)\n    return doc_vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## overview","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#単語数\ndf['overview_word_count'] = df['overview'].apply(lambda x: len(str(x).split()))\n#文字数\ndf['overview_char_count'] = df['overview'].apply(lambda x: len(str(x)))\n# 記号の個数\ndf['overview_punctuation_count'] = df['overview'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 前処理\ndf['_overview']=df['overview'].apply(lambda x : str(x)\n                            ).apply(lambda x : lower_text(x)).apply(lambda x : remove_punct(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time df_overview =  df[\"_overview\"].apply(get_doc_vector, method=\"meanmax\").apply(pd.Series)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_overview = df_overview.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## taglineの前処理と特徴量の追加","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#単語数\ndf['tagline_word_count'] = df['tagline'].apply(lambda x: len(str(x).split()))\n#文字数\ndf['tagline_char_count'] = df['tagline'].apply(lambda x: len(str(x)))\n# 記号の個数\ndf['tagline_punctuation_count'] = df['tagline'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['_tagline']=df['tagline'].apply(lambda x : str(x)\n                                 ).apply(lambda x : lower_text(x)).apply(lambda x : remove_punct(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ベクトル化\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# vec_tfidf = TfidfVectorizer()\n# X = vec_tfidf.fit_transform(df['tagline'])\n# Tfidf_tagline = pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())\n# X = vec_tfidf.fit_transform(df['overview'].dropna())\n# Tfidf_overview = pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time df_tagline =  df[\"_tagline\"].apply(get_doc_vector, method=\"meanmax\").apply(pd.Series)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tagline = df_tagline.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## titleの前処理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#単語数\ndf['title_word_count'] = df['title'].apply(lambda x: len(str(x).split()))\n#文字数\ndf['title_char_count'] = df['title'].apply(lambda x: len(str(x)))\n# 記号の個数\ndf['title_punctuation_count'] = df['title'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use2 = df[[\"tagline_char_count\",\"tagline_word_count\",\"tagline_punctuation_count\",\n              \"overview_char_count\",\"overview_word_count\",\"overview_punctuation_count\",\n              \"title_char_count\",\"title_word_count\",\"title_punctuation_count\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## keywordもword2vecベクトル化すると？","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_keyword_w2v = df[\"keyword_list\"].apply(\" \".join).apply(get_doc_vector, method=\"mean\").apply(pd.Series).fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## cast","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"cast\"] = df[\"cast\"].apply(lambda x: [] if pd.isna(x) else ast.literal_eval(x) )","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#映画の中にどれだけの人がキャストされたか表示\nprint('Number of casted persons in films')\ndf['cast'].apply(len).value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['num_cast'] = df['cast'].apply(len)  # 人数\n# df['all_cast'] = df['cast'].apply(lambda x: [i['name'] for i in x])  # \n\ndf_castname = pd.DataFrame([], index=df.index)\nlist_of_cast_names = list(df['cast'].apply(lambda x: [i['name'] for i in x]).values)  # 俳優名のリストのリスト\ntop_cast_names = [m[0] for m in Counter([i for j in list_of_cast_names for i in j]).most_common(50)]\nfor g in top_cast_names:\n    df_castname[g] = df['cast'].apply(lambda x: g in [i['name'] for i in x])\n\n    \n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"\"\"\"\n#実際の比較\n\ncast_name_Samuel_L_Jackson=df.loc[df['cast_name_Samuel L. Jackson']==1,]\ncast_name_Robert_De_Niro=df.loc[df['cast_name_Robert De Niro']==1,]\ncast_name_Morgan_Freeman=df.loc[df['cast_name_Morgan Freeman']==1,]\ncast_name_J_K_Simmons=df.loc[df['cast_name_J.K. Simmons']==1,]\ncast_name_Bruce_Willis=df.loc[df['cast_name_Bruce Willis']==1,]\ncast_name_Liam_Neeson=df.loc[df['cast_name_Liam Neeson']==1,]\ncast_name_Susan_Sarandon=df.loc[df['cast_name_Susan Sarandon']==1,]\ncast_name_Bruce_McGill=df.loc[df['cast_name_Bruce McGill']==1,]\ncast_name_John_Turturro=df.loc[df['cast_name_John Turturro']==1,]\ncast_name_Forest_Whitaker=df.loc[df['cast_name_Forest Whitaker']==1,]\n\n\ncast_name_Samuel_L_Jackson_revenue=cast_name_Samuel_L_Jackson.mean()['revenue']\ncast_name_Robert_De_Niro_revenue=cast_name_Robert_De_Niro.mean()['revenue']\ncast_name_Morgan_Freeman_revenue=cast_name_Morgan_Freeman.mean()['revenue']\ncast_name_J_K_Simmons_revenue=cast_name_J_K_Simmons.mean()['revenue']\ncast_name_Bruce_Willis_revenue=cast_name_Bruce_Willis.mean()['revenue']\ncast_name_Liam_Neeson_revenue=cast_name_Liam_Neeson.mean()['revenue']\ncast_name_Susan_Sarandon_revenue=cast_name_Susan_Sarandon.mean()['revenue']\ncast_name_Bruce_McGill_revenue=cast_name_Bruce_McGill.mean()['revenue']\ncast_name_John_Turturro_revenue=cast_name_John_Turturro.mean()['revenue']\ncast_name_Forest_Whitaker_revenue=cast_name_Forest_Whitaker.mean()['revenue']\n\n\ncast_revenue_concat = pd.Series([cast_name_Samuel_L_Jackson_revenue,cast_name_Robert_De_Niro_revenue,cast_name_Morgan_Freeman_revenue,cast_name_J_K_Simmons_revenue,\n                                cast_name_Bruce_Willis_revenue,cast_name_Liam_Neeson_revenue,cast_name_Susan_Sarandon_revenue,cast_name_Bruce_McGill_revenue,\n                                cast_name_John_Turturro_revenue,cast_name_Forest_Whitaker_revenue])\ncast_revenue_concat.index=['Samuel L. Jackson','Robert De Niro','Morgan Freeman','J.K. Simmons','Bruce Willis','Liam Neeson','Susan Sarandon','Bruce McGill',\n                            'John Turturro','Forest Whitaker']\n\nfig = plt.figure(figsize=(13, 7))\ncast_revenue_concat.sort_values(ascending=True).plot(kind='barh',title='mean Revenue (100 million dollars) by Top 10 Most Common Cast')\nplt.xlabel('Revenue (100 million dollars)')\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_cast_genders = list(df['cast'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\n\ndf['genders_0_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))/df[\"num_cast\"]\ndf['genders_1_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))/df[\"num_cast\"]\n\n\n\n\n\n# df = df.drop(['cast'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Crew","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"crew\"] = df[\"crew\"].apply(lambda x: [] if pd.isna(x) else ast.literal_eval(x) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"crew\"][1][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"department_count = pd.Series(Counter([job for lst in df[\"crew\"].apply(lambda x : [ i[\"department\"] for i in x]).values for job in lst]))\ndepartment_count.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"job_count = pd.Series(Counter([job for lst in df[\"crew\"].apply(lambda x : [ i[\"job\"] for i in x]).values for job in lst]))\njob_count.sort_values(ascending=False).head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf_crew = { idx : pd.DataFrame([ [crew[\"department\"], crew[\"job\"], crew[\"name\"]] \n                        for crew in x], columns=[\"department\", \"job\", \"name\"]) \n    for idx, x in df[\"crew\"].iteritems() }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_crew = pd.concat(df_crew)\ndf_crew.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#crewのname\ndf['num_crew'] = df['crew'].apply(len)\n\n# crew gender\ndf['genders_0_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))/df[\"num_crew\"]\ndf['genders_1_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))/df[\"num_crew\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def select_job(list_dict, key, value):\n    return [ dic[\"name\"] for dic in list_dict if dic[key]==value]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 各部署の人数\nfor department in department_count.index:\n    df['dep_{}_num'.format(department)] = df[\"crew\"].apply(select_job, key=\"department\", value=department).apply(len)  \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## job","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Animationの人数\ndf['job_Animation_num'] = df[\"crew\"].apply(select_job, key=\"job\", value=\"Animation\").apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_crewname = pd.DataFrame([], index=df.index)\nfor job in [\"Producer\", \"Director\", \"Casting\", \"Writer\", \"Original Music Composer\"]:\n    col = 'job_{}_list'.format(job)\n    df[col] = df[\"crew\"].apply(select_job, key=\"job\", value=job)\n\n    top_list = [m[0] for m in Counter([i for j in df[col] for i in j]).most_common(15)]\n    for i in top_list:\n        df_crewname['{}_{}'.format(job,i)] = df[col].apply(lambda x: i in x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 監督が複数の作品数\n(df[\"job_Director_list\"].apply(len)>1).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 整理","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use3=df[['num_cast', 'genders_0_cast',\n       'genders_1_cast', 'num_crew', 'genders_0_crew', 'genders_1_crew',\n       'dep_Directing_num', 'dep_Writing_num', 'dep_Production_num',\n       'dep_Sound_num', 'dep_Camera_num', 'dep_Editing_num', 'dep_Art_num',\n       'dep_Costume & Make-Up_num', 'dep_Crew_num', 'dep_Lighting_num',\n       'dep_Visual Effects_num', 'dep_Actors_num', 'job_Animation_num']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_use.index = df_features.index\ndf_use2.index = df_use.index\ndf_use3.index = df_use2.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_input = pd.concat([df_use, df_features], axis=1) # .drop(\"belongs_to_collection\", axis=1)\ndf_input = pd.concat([df_input, df_use2], axis=1)\ndf_input = pd.concat([df_input, df_use3], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tfid_tagline.index = df_use.index\n#df_use_Tfid = Tfid_tagline.loc[:, Tfid_tagline[:3000].nunique()>1]\n#df_use_Tfid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_input = pd.concat([df_input, df_tagline, df_overview, df_castname, df_crewname], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"ln_revenue\"] = np.log(df[\"revenue\"]+1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 学習用データ作成","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"no_numeric = df_input.apply(lambda s:pd.to_numeric(s, errors='coerce')).isnull().all()\nno_numeric[no_numeric]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_all = df_input #.drop([\"collection_av_logrevenue\", \"all_cast\", \"all_crew\"], axis=1)\ny_all = df[\"ln_revenue\"]\ny_all.index = X_all.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 標準化\n# X_train_all_mean = X_all[:3000].mean()\n# X_train_all_std  = X_all[:3000].std()\n# X_all = (X_all-X_train_all_mean)/X_train_all_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 欠損確認\nX_all.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, val_X, train_y, val_y = train_test_split(X_all[:3000], \n                                                  y_all[:3000], \n                                                  test_size=0.25, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lasso","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso, Ridge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = Lasso(alpha=0.1, max_iter=3000, random_state=1)  # default alpha=1, max_iter=1000\nclf.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_pred = clf.predict(val_X)\nprint(\"RMSLE score for validation data\")\nnp.sqrt(mean_squared_error(val_pred, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(np.exp(val_pred)+1, np.exp(val_y)+1, s=3)\nplt.xlabel(\"prediction\")\nplt.ylabel(\"true revenue\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coef = pd.Series(clf.coef_, index=train_X.columns)\ndf_coef = pd.DataFrame(coef[coef!=0], columns=[\"coef\"])\ndf_coef[abs(df_coef[\"coef\"])>0.1].sort_values(\"coef\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# submit","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = Lasso(alpha=0.1, max_iter=3000, random_state=1)  # default alpha=1, max_iter=1000\nclf.fit(X_all[:3000], y_all[:3000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coef = pd.Series(clf.coef_, index=train_X.columns)\ndf_coef = pd.DataFrame(coef[coef!=0], columns=[\"coef\"])\ndf_coef[abs(df_coef[\"coef\"])>0.1].sort_values(\"coef\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = clf.predict(X_all[3000:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_revenue = np.exp(test_pred)-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/tmdb-box-office-prediction/sample_submission.csv')\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission[\"revenue\"] = test_revenue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# randomforest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf2 = RandomForestRegressor(max_depth=15, min_samples_split=5, n_jobs=3, random_state=1)  # default alpha=1, max_iter=1000\nclf2.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_pred = clf2.predict(val_X)\nprint(\"RMSLE score for validation data\")\nnp.sqrt(mean_squared_error(val_pred, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(np.exp(val_pred)+1, np.exp(val_y)+1, s=3)\nplt.xlabel(\"prediction\")\nplt.ylabel(\"true revenue\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# submit","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf2 = RandomForestRegressor(max_depth=15, min_samples_split=5, n_jobs=3, random_state=1)  # default alpha=1, max_iter=1000\nclf2.fit(X_all[:3000], y_all[:3000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_importance = pd.DataFrame([clf2.feature_importances_], columns=train_X.columns, index=[\"importance\"]).T\ndf_importance.sort_values(\"importance\", ascending=False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = clf2.predict(X_all[3000:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_revenue = np.exp(test_pred)-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_RF = sample_submission.copy()\nsubmission_RF[\"revenue\"] = test_revenue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_RF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_RF.to_csv('submission_RF.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}