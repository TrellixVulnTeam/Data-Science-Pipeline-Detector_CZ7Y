{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Python ≥3.5 is required\nimport sys\n#assert sys.version_info >= (3, 5)\n\n# Scikit-Learn ≥0.20 is required\nimport sklearn\n#assert sklearn.__version__ >= \"0.20\"\n\n# Common imports\nimport numpy as np\nimport os # operator system\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n# ======= To plot pretty figures =======\n\n# Make your plot outputs appear and be stored within the notebook.\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n# Number format\nimport pandas as pd\npd.options.display.float_format = '{:,.4f}'.format\n\n# For Black background only\nCOLOR = 'black'\n\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\nmpl.rcParams['text.color'] = COLOR\nmpl.rcParams['axes.labelcolor'] = COLOR\nmpl.rcParams['xtick.color'] = COLOR\nmpl.rcParams['ytick.color'] = COLOR\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SPLITTER = \" \"\n#COLUMN_EXCLUDE_PATTERN = \"id|revenue|homepage|title|overview|poster_path|tagline\"\nCOLUMN_EXCLUDE_PATTERN = \"id|revenue\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT_FIELDS =[(\"belongs_to_collection\", \"id\"),\n                (\"genres\", \"name\"), \n                (\"production_companies\", \"id\"),                \n                (\"production_countries\", \"iso_3166_1\"),\n                (\"spoken_languages\", \"name\"),\n                (\"Keywords\", \"name\"),\n                (\"cast\", \"name\"),                         \n                (\"crew\", \"name\"),                \n             ]\nTEXT_FIELDS2 =[ (\"production_companies\", \"name\"),\n                (\"belongs_to_collection\", \"name\"),\n                (\"cast\", \"character\"),  \n                (\"cast\", \"job\"),  \n                (\"cast\", \"profile_path\"),\n                (\"crew\", \"job\"),\n                (\"crew\", \"department\"),\n             ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv(\"/kaggle/input/tmdb-box-office-prediction/train.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\n# plot histogram for all columns\ndata.hist(bins=50, figsize=(12,9)) # bins is number of groups of values\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create a Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#train_set, test_set = train_test_split(data, test_size=0.2, random_state=42) # use 20% data for testing\ntrain_set = data.copy()\ntest_set = pd.read_csv(\"/kaggle/input/tmdb-box-office-prediction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_column = \"revenue\"\nX_train = train_set.copy().drop(train_set.filter(regex=COLUMN_EXCLUDE_PATTERN), axis=1)\ny_train = train_set[label_column].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data tuning 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass LimitedColumnsFilter(BaseEstimator, TransformerMixin):\n    def __init__(self, filters): # no *args or **kargs\n        self.filters = filters   \n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X, y=None):        \n        return X.copy().filter(items=self.filters)\n\nfilters = ('budget', 'original_title', 'popularity', 'original_language') #test\nresult = LimitedColumnsFilter(filters).transform(X_train)\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\n#Convert string \"2/20/15\" to datetime64 \"2015-02-20\"\nclass DateTimeImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, replace=True): # no *args or **kargs\n        self.replace = replace\n        pass\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X, y=None):\n        clone_X = X.copy()                            \n        for feature in X.select_dtypes(include=[np.object]).columns:\n            try:\n                clone_X[feature] = pd.to_datetime(X[feature], infer_datetime_format=True)\n            except:\n                pass\n        return clone_X\n\nresult = DateTimeImputer().transform(X_train)\nfilters = list(X_train.filter(like=\"date\").columns)\nresult[filters].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\n#Split date \"2015-02-20\" to 3 more features _Y=205, _M=02, _D=20\nclass DateDissolver(BaseEstimator, TransformerMixin):\n    def __init__(self, replace=False): # no *args or **kargs\n        self.replace = replace\n        pass\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X, y=None):\n        clone_X = X.copy()\n        for feature in X.select_dtypes(include=[np.datetime64]).columns:\n            if self.replace:\n                clone_X = clone_X.drop([feature], axis=1)                  \n            try:        \n                clone_X['{0}_Y'.format(feature)] = X[feature].dt.year\n                clone_X['{0}_M'.format(feature)] = X[feature].dt.month\n                clone_X['{0}_D'.format(feature)] = X[feature].dt.day\n            except:\n                pass\n\n        return clone_X\n\nresult = DateTimeImputer().transform(X_train)\nresult = DateDissolver(replace=True).transform(result)\nfilters = list(result.filter(like=\"date\").columns)\nresult[filters].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass NumberFilter(BaseEstimator, TransformerMixin):\n    def __init__(self): # no *args or **kargs\n        pass\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X, y=None):      \n        return X.copy().select_dtypes(include=[np.int64, np.float64])        \n\nresult = NumberFilter().transform(X_train)\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass CategoryFilter(BaseEstimator, TransformerMixin):\n    def __init__(self): # no *args or **kargs\n        pass\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X, y=None):        \n        return X.copy().select_dtypes(include=[np.object])        \n\nresult = CategoryFilter().transform(X_train)\nresult.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Tunning 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"import ast\n\nclass InfoExtractor(BaseEstimator, TransformerMixin):\n    def __init__(self, field, replace=False):\n        self.field = field\n        self.replace = replace\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        clone_X = X.copy()   \n        for feature, field_name in self.field:\n            if self.replace:\n                clone_X[feature] = X[feature].apply(lambda x: self.extract_field(x, field_name))\n            else:\n                clone_X[\"{0}_{1}\".format(feature, field_name)] = X[feature].apply(lambda x: self.extract_field(x, field_name))\n        return clone_X\n        \n    def extract_field(self, data, field_name):\n        if(data is not np.nan):\n            info = ast.literal_eval(data)            \n            result = SPLITTER.join(\"{}\".format(x[field_name]).replace(SPLITTER, \"_\") for x in info)\n            return result\n        return np.nan\n    \ninfoExtractor = InfoExtractor(field=TEXT_FIELDS, replace=True)\nresult = infoExtractor.transform(X_train)\n#filters = list(result.filter(regex=\"collection|genres|crew\").columns)\nfilters = list(result.filter(regex=\"collection|cast|production|genres|languages|Keywords|crew\").columns)\nresult[filters].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\nclass TextEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, field=None, replace=False):\n        self.field = field\n        self.replace = replace\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        clone_X = X.copy()\n        if self.field is None:\n            self.field = X.copy().select_dtypes(include=[np.object], exclude=[np.datetime64]).columns               \n        for feature in self.field:               \n            if self.replace:               \n                clone_X[feature] = pd.Series(data=self.encode_textBySum(X[feature]), index=clone_X.index)\n               #clone_X[\"{0}_{1}\".format(feature, 'encode')] = pd.Series(data=self.encode_textForOneHot(X[feature]), index=clone_X.index)\n            else:                \n                clone_X[\"{0}_{1}\".format(feature, 'count')] = pd.Series(data=self.encode_textBySum(X[feature]), index=clone_X.index)\n               #clone_X[\"{0}_{1}\".format(feature, 'encode')] = pd.Series(data=self.encode_textForOneHot(X[feature]), index=clone_X.index) \n        return clone_X\n        \n    def encode_textBySum(self, df_feature):\n        tokenizer = Tokenizer()\n        clone_feature = df_feature.copy().fillna('')        \n        tokenizer.fit_on_texts(clone_feature)\n       #encoded_docs = tokenizer.texts_to_matrix(clone_feature, mode='count')\n        encoded_docs = tokenizer.texts_to_matrix(clone_feature, mode='tfidf')\n        encoded_nums = np.sum(encoded_docs,axis=1) #sum encoded matrix/we may use other strategy later...              \n        return encoded_nums\n    \n    def encode_textForOneHot(self, df_feature):\n        tokenizer = Tokenizer()\n        clone_feature = df_feature.copy().fillna('')        \n        tokenizer.fit_on_texts(clone_feature)\n        encoded_docs = tokenizer.texts_to_matrix(clone_feature, mode='binary')        \n        encoded_onehot = pd.DataFrame(data=encoded_docs).applymap(\"{:1.0f}\".format).apply(\"\".join, axis=1)                      \n        return encoded_onehot\n\n# infoExtractor = InfoExtractor(field=TEXT_FIELDS, replace=True)\n# textEncoder = TextEncoder(replace=True)\ninfoExtractor = InfoExtractor(field=[(\"cast\", \"name\")], replace=True)                \ntextEncoder = TextEncoder(field=[\"homepage\", \"cast\"], replace=True)\nresult = infoExtractor.transform(X_train)\nresult = textEncoder.transform(result)\nfilters = list(result.filter(regex=\"date|cast|homepage\").columns)\n#filters = list(result_text.filter(regex=\"collection|cast|production|genres|languages|Keywords|crew\").columns)\nresult[filters].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transformation Pipelines"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\ntext_pipeline = Pipeline([        \n       #('cat_filter'  , CategoryFilter()),                \n       #('inf_extor2'  , InfoExtractor(field=TEXT_FIELDS2, replace=False)),\n        ('inf_extor'   , InfoExtractor(field=TEXT_FIELDS, replace=True)),\n        ('txt_encoder' , TextEncoder(replace=True)),\n    ])\n\nresult = DateTimeImputer().transform(X_train)\nresult = DateDissolver(replace=True).transform(result)\nresult = text_pipeline.fit_transform(result)\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\nnum_pipeline = Pipeline([        \n        ('num_filter', NumberFilter()),\n        ('imputer'   , SimpleImputer(strategy=\"median\")),     # fill nan/empty cells        \n        ('mm_scaler' , MinMaxScaler(feature_range=(-1, 1))),  # feature scaling\n       #('std_scaler', StandardScaler()),  # feature scaling\n    ])\n\nresult = num_pipeline.fit_transform(X_train)\npd.DataFrame(data=result).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\ncat_pipeline = Pipeline([\n        ('cat_filter', CategoryFilter()),\n        ('imputer'   , SimpleImputer(strategy='constant', fill_value='Missing')),  # fill nan/empty cells\n       #('cat'       , OneHotEncoder(handle_unknown='ignore', sparse=False)),       \n    ])\n\nfilters = ('budget', 'original_language') #test\nresult = LimitedColumnsFilter(filters).transform(X_train)\nresult = cat_pipeline.fit_transform(result)\nresult[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.compose import make_column_selector\n\n\nfull_pipeline = make_column_transformer(              \n    (num_pipeline , make_column_selector(dtype_include=[np.int64, np.float64])),            \n   #(cat_pipeline , make_column_selector(dtype_include=np.object)),\n)\n\n# full_pipeline = ColumnTransformer([\n#     (\"num\", num_pipeline, list(getNumberColumns(X_train))),\n#     (\"cat\", cat_pipeline, list(getCategoryColumns(X_train))),\n# ])\n\nfilters = list(X_train.filter(regex=\"date|budget|original_language\").columns) #test\nresult = LimitedColumnsFilter(filters).transform(X_train)\nresult = full_pipeline.fit_transform(result)\nresult[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Models & Scoring"},{"metadata":{"trusted":true},"cell_type":"code","source":"#filters = list(X_train.filter(regex=\"date|budget|original_language\").columns) #test\n#X_train_pp_df = LimitedColumnsFilter(filters).transform(X_train)\n\nX_train_pp_df = DateTimeImputer().transform(X_train)\nX_train_pp_df = DateDissolver(replace=True).transform(X_train_pp_df)\nX_train_pp_df = text_pipeline.fit_transform(X_train_pp_df)\n#cat_pipeline.fit_transform(X_train_pp_df)\n\nX_train_pp = full_pipeline.fit_transform(X_train_pp_df)\nX_train_pp[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\n\nsvm_reg = SVR(kernel=\"linear\")\nsvm_reg.fit(X_train_pp, y_train)\npredictions = svm_reg.predict(X_train_pp)\n\nmse = mean_squared_error(y_train, predictions)\nprint(\"rmse ->\", np.sqrt(mse))\nmsle = mean_squared_log_error(y_train, predictions)\nprint(\"rmsle ->\", np.sqrt(msle))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicts -> \", list(predictions[0:5]))\nprint(\"Labels   -> \", list(y_train[0:5]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fine Tune Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_pp.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n\nparam_grid = [\n   #{'n_estimators': [10, 50, 100], 'max_features': [5, 10, 20, X_train_pp.shape[1]]},\n    {'n_estimators': [10], 'max_features': [X_train_pp.shape[1]]},\n   #{'n_estimators': [100], 'max_features': [5]},\n  ]\n\ngrid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid\n                         , cv=5, scoring='neg_mean_squared_log_error', return_train_score=True)\ngrid_search.fit(X_train_pp, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nnum_columns = NumberFilter().transform(X_train_pp_df).columns\ncat_columns = CategoryFilter().transform(X_train_pp_df).columns\n#cat_columns = cat_pipeline.named_steps['cat'].get_feature_names(input_features=cat_columns)\ncolumns = list(num_columns) + list(cat_columns)\nfeature_importance = pd.Series(data= grid_search.best_estimator_.feature_importances_, index = np.array(columns))\n#features_top = feature_importance.sort_values(ascending=False)[0:20].index\n#features_top = list(map(lambda x: re.sub(\"_.+$\", \"\", x), features_top)) #Remove _XX from OneHotEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance.sort_values(ascending=False)[0:5].plot(figsize=(20, 10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Try Neuron Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_pp.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\nann_model = keras.models.Sequential()\nann_model.add(keras.layers.Dense(5000 , activation=\"relu\", input_shape=X_train_pp.shape[1:]))\nann_model.add(keras.layers.Dense(1000, activation=\"relu\"))\nann_model.add(keras.layers.Dense(2000, activation=\"relu\"))\nann_model.add(keras.layers.Dense(100, activation=\"relu\"))\nann_model.add(keras.layers.Dense(500, activation=\"relu\"))\nann_model.add(keras.layers.Dense(X_train_pp.shape[1], activation=\"relu\"))\nann_model.add(keras.layers.Dense(1))\n\n#model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\nann_model.compile(loss=\"mean_squared_logarithmic_error\", optimizer=keras.optimizers.SGD(lr=1e-1))\nhistory = ann_model.fit(X_train_pp, y_train, epochs=100, batch_size=32, validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.summary()\nkeras.utils.plot_model(ann_model, show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict the TEST"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test_set.copy().drop(test_set.filter(regex=COLUMN_EXCLUDE_PATTERN), axis=1)\n\nX_test_pp_df = DateTimeImputer().transform(X_test)\nX_test_pp_df = DateDissolver(replace=True).transform(X_test_pp_df)\nX_test_pp_df = text_pipeline.fit_transform(X_test_pp_df)\n\nX_test_pp = full_pipeline.transform(X_test_pp_df)\nX_test_pp[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error\n\n#final_model = grid_search.best_estimator_\nfinal_model = ann_model\nfinal_predictions = final_model.predict(X_test_pp)\n#final_msle = mean_squared_log_error(y_test, final_predictions)\n#print(\"rmsle -> \", np.sqrt(final_mse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predicts -> \", list(final_predictions[0:5]))\n#print(\"Labels   -> \", list(y_test[0:5]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set['revenue'] = final_predictions\ntest_set[['id', 'revenue']].to_csv('./submission_hvdang.csv', header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set[['id', 'revenue']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}