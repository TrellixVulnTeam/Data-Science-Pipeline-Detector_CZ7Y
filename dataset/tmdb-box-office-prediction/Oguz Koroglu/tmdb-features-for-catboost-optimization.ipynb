{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>TMDB Features for Catboost and Catboost Optimization</h1>\n<h2>What is Catboost</h2>\n\n<p>A fast, scalable, high performance Gradient Boosting on Decision Trees library, used for ranking, classification, regression and other machine learning tasks for Python, R, Java, C++. Supports computation on CPU and GPU.</p>\n\n<p>Provided by Yandex and basicly it is the russian tensorflow and focused on Gradient Boosting insteed of neural network. Also has a lot of GPU features.</p>\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"./\"))\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['train.csv', 'sample_submission.csv', 'test.csv']\n['.ipynb_checkpoints', '__notebook_source__.ipynb']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h3>Imports and Setup</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import preprocessing, model_selection, neighbors, svm\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, MultiLabelBinarizer\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MaxAbsScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.impute import SimpleImputer\n\nfrom catboost import CatBoostRegressor, Pool\n\nfrom tqdm import tqdm\nimport json\nimport ast\n\nfrom datetime import datetime","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Paths and Definitions</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_DATA_PATH = \"../input/train.csv\"\nTEST_DATA_PATH = \"../input/test.csv\"\nSUBMISSON_PATH = \"../input/sample_submission.csv\"\nLABEL_COL_NAME = \"revenue\"","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Functions and Feature Generation</h3>\n<p>Here we are pasing json data and distribute over pandas dataframe. Like Crew, Cast, Genre, Production Company etc... . We are defining every unique value as column in our dataframe. If the movie has that value row value of that column will be 1.</p> \n<p>Also by the way we try to find use full features too. They could be important somehow.</p>\n<ul>\n    <li>Is the title is different of the original title.</li>\n    <li>Count of casts</li>\n    <li>Count of crews</li>\n    <li>Count of casts gender</li>\n    <li>Count of crews gender</li>\n    <li>Has a home page or not</li>\n    <li>Is released or not</li>\n    <li>Count of keywords</li>\n    <li>Count of production companies and countries</li>\n    <li>Release Day, Month and Year as seperate features.</li>\n    <li>title and original title length</li>\n</ul>\n\n<p>Budget and Revenues are so big skewed values. Not good for machine learning. We are using log of them.</p>\n<p>More over we are imputing the budget with median strategy. It may increase the score.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def date(x):\n    x=str(x)\n    year=x.split('/')[2]\n    if int(year)<19:\n        return x[:-2]+'20'+year\n    else:\n        return x[:-2]+'19'+year\n\ndef isNaN(x):\n    return str(x) == str(1e400 * 0)\n\ndef getIsoListFormJson(data, isoKey='id', forceInt=False):\n    datas = data.values.flatten()\n    ids = []\n    for c in (datas):    \n        ccc = []\n        if isNaN(c) == False:\n            c = json.dumps(ast.literal_eval(c))        \n            c = json.loads(c)            \n            for cc in c:\n                if forceInt:\n                    ccStr = int(cc[isoKey])\n                else:\n                    ccStr = str(cc[isoKey])\n                ccc.append(ccStr)\n        else:\n            if forceInt:\n                ccc.append(0)\n            else:\n                ccc.append('0')\n        ids.append(ccc)    \n    return np.array(ids)\n\ndef distributeIdsOverData(data, colName, isoKey='id', forceInt=True):\n    arr = getIsoListFormJson(data[colName], isoKey, forceInt)    \n\n    gsi = -1\n    for gs in tqdm(arr):\n        gsi += 1\n        gs.sort()\n        for g in gs:\n            gi = gs.index(g)\n            try:\n                data.loc[gsi, f\"{colName}_{gi}\"] = float(g)                \n            except :\n                data.loc[gsi, f\"{colName}_{gi}\"] = g                \n            \n    data.drop(colName, axis=1, inplace=True)\n    print(f\"{colName} distributed over data, cols: {len(data.columns)}\")\n\ndef imput_title(df):\n    for index, row in df.iterrows():\n        if row['title'] == \"none\":\n            df.at[index,'title'] = df.loc[index]['original_title']\n    return df    \n    \ndef prepareData(data):    \n    data = imput_title(data)\n\n    data[\"different_title\"] = data[\"original_title\"] != data[\"title\"]\n\n    data.drop(\"overview\", axis=1, inplace=True)\n    data.drop(\"poster_path\", axis=1, inplace=True)\n    data.drop('imdb_id', axis=1, inplace=True)    \n\n    data[\"belongs_to_collection\"] = getIsoListFormJson(data[\"belongs_to_collection\"])\n\n    cast = data['cast'].fillna('none')\n    cast = cast.apply(lambda x: {} if x == 'none' else ast.literal_eval(x))\n    data['num_cast'] = cast.apply(lambda x: len(x) if x != {} else 0)\n    # Get the sum of each of the cast genders in a film: 0 `unknown`, 1 `female`, 2 `male`\n    data['genders_0_cast'] = cast.apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n    data['genders_1_cast'] = cast.apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n    data['genders_2_cast'] = cast.apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n    distributeIdsOverData(data,'cast','cast_id')\n\n    crew = data['crew'].fillna('none')\n    crew = crew.apply(lambda x: {} if x == 'none' else ast.literal_eval(x))    \n    data['num_crew'] = crew.apply(lambda x: len(x) if x != {} else 0)    \n    # Get the sum of each of the cast genders in a film: 0 `unknown`, 1 `female`, 2 `male`\n    data['genders_0_crew'] = crew.apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n    data['genders_1_crew'] = crew.apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n    data['genders_2_crew'] = crew.apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n    distributeIdsOverData(data,'crew','name',False) \n\n    distributeIdsOverData(data,'genres')\n    \n    keywords = data['Keywords'].fillna('none')\n    keywords = keywords.apply(lambda x: {} if x == 'none' else ast.literal_eval(x))\n    data['num_keywords'] = keywords.apply(lambda x: len(x) if x != {} else 0)\n    distributeIdsOverData(data,'Keywords')\n\n    data[\"Has_HomePage\"] = list(map(lambda c: float(c is not np.nan), data[\"homepage\"]))\n    data.drop('homepage', axis=1, inplace=True)\n\n    data[\"IsReleased\"] = list(map(lambda c: float(c == \"Released\"), data[\"status\"]))\n    data.drop(\"status\", axis=1, inplace=True)\n  \n    data[\"original_title_len\"] = list(map(lambda c: float(len(str(c))), data[\"original_title\"]))\n    data.drop(\"original_title\", axis=1, inplace=True)\n    \n    production_companies = data['production_companies'].fillna('none')\n    production_companies = production_companies.apply(lambda x: {} if x == 'none' else ast.literal_eval(x))\n    data['num_production_companies'] = production_companies.apply(lambda x: len(x) if x != {} else 0)\n    distributeIdsOverData(data,'production_companies')    \n\n    production_countries = data['production_countries'].fillna('none')\n    production_countries = production_countries.apply(lambda x: {} if x == 'none' else ast.literal_eval(x))\n    data['num_production_countries'] = production_countries.apply(lambda x: len(x) if x != {} else 0)    \n    distributeIdsOverData(data,'production_countries','iso_3166_1',False)\n\n    data['release_date']=data['release_date'].fillna('1/1/90').apply(lambda x: date(x))\n    data['release_date']=data['release_date'].apply(lambda x: datetime.strptime(x,'%m/%d/%Y'))\n    data['release_day']=data['release_date'].apply(lambda x:x.weekday())\n    data['release_month']=data['release_date'].apply(lambda x:x.month)\n    data['release_year']=data['release_date'].apply(lambda x:x.year)\n    data.drop('release_date', axis=1, inplace=True)\n    \n    spoken_languages = data['spoken_languages'].fillna('none')\n    spoken_languages = spoken_languages.apply(lambda x: {} if x == 'none' else ast.literal_eval(x))\n    data['num_spoken_languages'] = spoken_languages.apply(lambda x: len(x) if x != {} else 0)\n    distributeIdsOverData(data,'spoken_languages','iso_639_1',False)\n\n    data[\"tagline_len\"] = list(map(lambda c: float(len(str(c))), data[\"tagline\"]))\n    data.drop(\"tagline\", axis=1, inplace=True)\n\n    data[\"title_len\"] = list(map(lambda c: float(len(str(c))), data[\"title\"]))\n    data.drop(\"title\", axis=1, inplace=True)    \n\n    data.fillna(0, inplace=True)\n    data[\"budget\"] = np.log1p(SimpleImputer(missing_values=0, strategy=\"median\", verbose=1).fit_transform(data[\"budget\"].values.reshape(-1,1)))\n    #data[\"budget\"] = np.log1p(data[\"budget\"])\n\n    data[LABEL_COL_NAME] = np.log1p(data[LABEL_COL_NAME])","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Loading the test and train data</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(TRAIN_DATA_PATH, index_col='id')\nprint(\"Train Data Loaded\")\ntest = pd.read_csv(TEST_DATA_PATH, index_col = 'id')\nprint(\"Test Data Loaded\")","execution_count":5,"outputs":[{"output_type":"stream","text":"Train Data Loaded\nTest Data Loaded\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"There are some missing values in the test and train data. If we know them from imdb then we could fill them manually."},{"metadata":{"trusted":true},"cell_type":"code","source":"if not os.path.exists(\"all_data.pickle\"):   \n    ##FILLING MISSIN BUDGET DATA\n    train.loc[16,'revenue'] = 192864          # Skinning\n    train.loc[90,'budget'] = 30000000         # Sommersby          \n    train.loc[118,'budget'] = 60000000        # Wild Hogs\n    train.loc[149,'budget'] = 18000000        # Beethoven\n    train.loc[313,'revenue'] = 12000000       # The Cookout \n    train.loc[451,'revenue'] = 12000000       # Chasing Liberty\n    train.loc[464,'budget'] = 20000000        # Parenthood\n    train.loc[470,'budget'] = 13000000        # The Karate Kid, Part II\n    train.loc[513,'budget'] = 930000          # From Prada to Nada\n    train.loc[797,'budget'] = 8000000         # Welcome to Dongmakgol\n    train.loc[819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip\n    train.loc[850,'budget'] = 90000000        # Modern Times\n    train.loc[1112,'budget'] = 7500000        # An Officer and a Gentleman\n    train.loc[1131,'budget'] = 4300000        # Smokey and the Bandit   \n    train.loc[1359,'budget'] = 10000000       # Stir Crazy \n    train.loc[1542,'budget'] = 1              # All at Once\n    train.loc[1542,'budget'] = 15800000       # Crocodile Dundee II\n    train.loc[1571,'budget'] = 4000000        # Lady and the Tramp\n    train.loc[1714,'budget'] = 46000000       # The Recruit\n    train.loc[1721,'budget'] = 17500000       # Cocoon\n    train.loc[1865,'revenue'] = 25000000      # Scooby-Doo 2: Monsters Unleashed\n    train.loc[2268,'budget'] = 17500000       # Madea Goes to Jail budget\n    train.loc[2491,'revenue'] = 6800000       # Never Talk to Strangers\n    train.loc[2602,'budget'] = 31000000       # Mr. Holland's Opus\n    train.loc[2612,'budget'] = 15000000       # Field of Dreams\n    train.loc[2696,'budget'] = 10000000       # Nurse 3-D\n    train.loc[2801,'budget'] = 10000000       # Fracture\n\n    test.loc[3889,'budget'] = 15000000       # Colossal\n    test.loc[6733,'budget'] = 5000000        # The Big Sick\n    test.loc[3197,'budget'] = 8000000        # High-Rise\n    test.loc[6683,'budget'] = 50000000       # The Pink Panther 2\n    test.loc[5704,'budget'] = 4300000        # French Connection II\n    test.loc[6109,'budget'] = 281756         # Dogtooth\n    test.loc[7242,'budget'] = 10000000       # Addams Family Values\n    test.loc[7021,'budget'] = 17540562       #  Two Is a Family\n    test.loc[5591,'budget'] = 4000000        # The Orphanage\n    test.loc[4282,'budget'] = 20000000       # Big Top Pee-wee\n\n    train.loc[391,'runtime'] = 86 #Il peor natagle de la meva vida\n    train.loc[592,'runtime'] = 90 #А поутру они проснулись\n    train.loc[925,'runtime'] = 95 #¿Quién mató a Bambi?\n    train.loc[978,'runtime'] = 93 #La peggior settimana della mia vita\n    train.loc[1256,'runtime'] = 92 #Cipolla Colt\n    train.loc[1542,'runtime'] = 93 #Все и сразу\n    train.loc[1875,'runtime'] = 86 #Vermist\n    train.loc[2151,'runtime'] = 108 #Mechenosets\n    train.loc[2499,'runtime'] = 108 #Na Igre 2. Novyy Uroven\n    train.loc[2646,'runtime'] = 98 #同桌的妳\n    train.loc[2786,'runtime'] = 111 #Revelation\n    train.loc[2866,'runtime'] = 96 #Tutto tutto niente niente\n    \n    test.loc[4074,'runtime'] = 103 #Shikshanachya Aaicha Gho\n    test.loc[4222,'runtime'] = 93 #Street Knight\n    test.loc[4431,'runtime'] = 100 #Плюс один\n    test.loc[5520,'runtime'] = 86 #Glukhar v kino\n    test.loc[5845,'runtime'] = 83 #Frau Müller muss weg!\n    test.loc[5849,'runtime'] = 140 #Shabd\n    test.loc[6210,'runtime'] = 104 #Le dernier souffle\n    test.loc[6804,'runtime'] = 145 #Chaahat Ek Nasha..\n    test.loc[7321,'runtime'] = 87 #El truco del manco\n\n    all_data = train.append(test)\n    print(\"Preparing All Data\")\n    prepareData(all_data)    \n    all_data.to_pickle(\"all_data.pickle\")\n    print(\"saved all data\")\nelse: \n    all_data = pd.read_pickle(\"all_data.pickle\")\n    print(\"saved all data\")","execution_count":6,"outputs":[{"output_type":"stream","text":"Preparing All Data\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\nof pandas will change to not sort by default.\n\nTo accept the future behavior, pass 'sort=False'.\n\nTo retain the current behavior and silence the warning, pass 'sort=True'.\n\n  sort=sort)\n100%|██████████| 7398/7398 [01:26<00:00, 85.72it/s] \n","name":"stderr"},{"output_type":"stream","text":"cast distributed over data, cols: 188\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 7399/7399 [03:22<00:00, 36.47it/s]\n","name":"stderr"},{"output_type":"stream","text":"crew distributed over data, cols: 385\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 7399/7399 [00:10<00:00, 714.90it/s]\n","name":"stderr"},{"output_type":"stream","text":"genres distributed over data, cols: 392\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 7399/7399 [00:32<00:00, 226.34it/s]\n","name":"stderr"},{"output_type":"stream","text":"Keywords distributed over data, cols: 541\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 7399/7399 [00:12<00:00, 616.04it/s]\n","name":"stderr"},{"output_type":"stream","text":"production_companies distributed over data, cols: 567\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 7399/7399 [00:11<00:00, 634.78it/s]\n","name":"stderr"},{"output_type":"stream","text":"production_countries distributed over data, cols: 579\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 7399/7399 [00:12<00:00, 596.65it/s]\n","name":"stderr"},{"output_type":"stream","text":"spoken_languages distributed over data, cols: 590\nsaved all data\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = all_data[:len(train)]\ntest = all_data[len(train):]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"   belongs_to_collection     budget    ...     tagline_len  title_len\nid                                     ...                           \n1                 313576  16.454568    ...            52.0       22.0\n2                 107674  17.504390    ...            60.0       40.0\n3                      0  15.009433    ...            47.0        8.0\n4                      0  13.997833    ...             3.0        7.0\n5                      0  16.648724    ...             3.0       10.0\n\n[5 rows x 590 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>belongs_to_collection</th>\n      <th>budget</th>\n      <th>original_language</th>\n      <th>popularity</th>\n      <th>revenue</th>\n      <th>runtime</th>\n      <th>different_title</th>\n      <th>num_cast</th>\n      <th>genders_0_cast</th>\n      <th>genders_1_cast</th>\n      <th>genders_2_cast</th>\n      <th>cast_0</th>\n      <th>cast_1</th>\n      <th>cast_2</th>\n      <th>cast_3</th>\n      <th>cast_4</th>\n      <th>cast_5</th>\n      <th>cast_6</th>\n      <th>cast_7</th>\n      <th>cast_8</th>\n      <th>cast_9</th>\n      <th>cast_10</th>\n      <th>cast_11</th>\n      <th>cast_12</th>\n      <th>cast_13</th>\n      <th>cast_14</th>\n      <th>cast_15</th>\n      <th>cast_16</th>\n      <th>cast_17</th>\n      <th>cast_18</th>\n      <th>cast_19</th>\n      <th>cast_20</th>\n      <th>cast_21</th>\n      <th>cast_22</th>\n      <th>cast_23</th>\n      <th>cast_24</th>\n      <th>cast_25</th>\n      <th>cast_26</th>\n      <th>cast_27</th>\n      <th>cast_28</th>\n      <th>...</th>\n      <th>production_companies_14</th>\n      <th>production_companies_15</th>\n      <th>production_companies_16</th>\n      <th>production_companies_17</th>\n      <th>production_companies_18</th>\n      <th>production_companies_19</th>\n      <th>production_companies_20</th>\n      <th>production_companies_21</th>\n      <th>production_companies_22</th>\n      <th>production_companies_23</th>\n      <th>production_companies_24</th>\n      <th>production_companies_25</th>\n      <th>num_production_countries</th>\n      <th>production_countries_0</th>\n      <th>production_countries_1</th>\n      <th>production_countries_2</th>\n      <th>production_countries_3</th>\n      <th>production_countries_4</th>\n      <th>production_countries_5</th>\n      <th>production_countries_6</th>\n      <th>production_countries_7</th>\n      <th>production_countries_8</th>\n      <th>production_countries_9</th>\n      <th>production_countries_10</th>\n      <th>production_countries_11</th>\n      <th>release_day</th>\n      <th>release_month</th>\n      <th>release_year</th>\n      <th>num_spoken_languages</th>\n      <th>spoken_languages_0</th>\n      <th>spoken_languages_1</th>\n      <th>spoken_languages_2</th>\n      <th>spoken_languages_3</th>\n      <th>spoken_languages_4</th>\n      <th>spoken_languages_5</th>\n      <th>spoken_languages_6</th>\n      <th>spoken_languages_7</th>\n      <th>spoken_languages_8</th>\n      <th>tagline_len</th>\n      <th>title_len</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>313576</td>\n      <td>16.454568</td>\n      <td>en</td>\n      <td>6.575393</td>\n      <td>16.326300</td>\n      <td>93.0</td>\n      <td>False</td>\n      <td>24.0</td>\n      <td>6.0</td>\n      <td>8.0</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>7.0</td>\n      <td>17.0</td>\n      <td>18.0</td>\n      <td>19.0</td>\n      <td>20.0</td>\n      <td>21.0</td>\n      <td>22.0</td>\n      <td>23.0</td>\n      <td>24.0</td>\n      <td>25.0</td>\n      <td>26.0</td>\n      <td>27.0</td>\n      <td>28.0</td>\n      <td>29.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>US</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2</td>\n      <td>2015</td>\n      <td>1</td>\n      <td>en</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>52.0</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>107674</td>\n      <td>17.504390</td>\n      <td>en</td>\n      <td>8.248895</td>\n      <td>18.370959</td>\n      <td>113.0</td>\n      <td>False</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>10.0</td>\n      <td>10.0</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>11.0</td>\n      <td>12.0</td>\n      <td>13.0</td>\n      <td>14.0</td>\n      <td>15.0</td>\n      <td>22.0</td>\n      <td>23.0</td>\n      <td>24.0</td>\n      <td>25.0</td>\n      <td>26.0</td>\n      <td>27.0</td>\n      <td>29.0</td>\n      <td>31.0</td>\n      <td>32.0</td>\n      <td>33.0</td>\n      <td>34.0</td>\n      <td>35.0</td>\n      <td>36.0</td>\n      <td>37.0</td>\n      <td>38.0</td>\n      <td>39.0</td>\n      <td>40.0</td>\n      <td>41.0</td>\n      <td>42.0</td>\n      <td>43.0</td>\n      <td>44.0</td>\n      <td>45.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>US</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>8</td>\n      <td>2004</td>\n      <td>1</td>\n      <td>en</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>60.0</td>\n      <td>40.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>15.009433</td>\n      <td>en</td>\n      <td>64.299990</td>\n      <td>16.387512</td>\n      <td>105.0</td>\n      <td>False</td>\n      <td>51.0</td>\n      <td>31.0</td>\n      <td>7.0</td>\n      <td>13.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>7.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>IN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>10</td>\n      <td>2014</td>\n      <td>1</td>\n      <td>en</td>\n      <td>hi</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>47.0</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>13.997833</td>\n      <td>hi</td>\n      <td>3.174936</td>\n      <td>16.588099</td>\n      <td>122.0</td>\n      <td>False</td>\n      <td>7.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>KR</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2012</td>\n      <td>2</td>\n      <td>ko</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.0</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>16.648724</td>\n      <td>ko</td>\n      <td>1.148070</td>\n      <td>15.182615</td>\n      <td>118.0</td>\n      <td>True</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>6.0</td>\n      <td>7.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2009</td>\n      <td>1</td>\n      <td>en</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.0</td>\n      <td>10.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"            budget   popularity     ...       tagline_len    title_len\ncount  3000.000000  3000.000000     ...       3000.000000  3000.000000\nmean     16.408080     8.463274     ...         36.358000    15.159000\nstd       1.649808    12.104000     ...         28.321474     8.329196\nmin       0.693147     0.000001     ...          3.000000     1.000000\n25%      16.012735     4.018053     ...         17.000000    10.000000\n50%      16.648724     7.374861     ...         32.000000    13.000000\n75%      17.216708    10.890983     ...         50.000000    19.000000\nmax      19.755682   294.337037     ...        232.000000    62.000000\n\n[8 rows x 372 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>budget</th>\n      <th>popularity</th>\n      <th>revenue</th>\n      <th>runtime</th>\n      <th>num_cast</th>\n      <th>genders_0_cast</th>\n      <th>genders_1_cast</th>\n      <th>genders_2_cast</th>\n      <th>cast_0</th>\n      <th>cast_1</th>\n      <th>cast_2</th>\n      <th>cast_3</th>\n      <th>cast_4</th>\n      <th>cast_5</th>\n      <th>cast_6</th>\n      <th>cast_7</th>\n      <th>cast_8</th>\n      <th>cast_9</th>\n      <th>cast_10</th>\n      <th>cast_11</th>\n      <th>cast_12</th>\n      <th>cast_13</th>\n      <th>cast_14</th>\n      <th>cast_15</th>\n      <th>cast_16</th>\n      <th>cast_17</th>\n      <th>cast_18</th>\n      <th>cast_19</th>\n      <th>cast_20</th>\n      <th>cast_21</th>\n      <th>cast_22</th>\n      <th>cast_23</th>\n      <th>cast_24</th>\n      <th>cast_25</th>\n      <th>cast_26</th>\n      <th>cast_27</th>\n      <th>cast_28</th>\n      <th>cast_29</th>\n      <th>cast_30</th>\n      <th>cast_31</th>\n      <th>...</th>\n      <th>Keywords_146</th>\n      <th>Keywords_147</th>\n      <th>Keywords_148</th>\n      <th>Has_HomePage</th>\n      <th>IsReleased</th>\n      <th>original_title_len</th>\n      <th>num_production_companies</th>\n      <th>production_companies_0</th>\n      <th>production_companies_1</th>\n      <th>production_companies_2</th>\n      <th>production_companies_3</th>\n      <th>production_companies_4</th>\n      <th>production_companies_5</th>\n      <th>production_companies_6</th>\n      <th>production_companies_7</th>\n      <th>production_companies_8</th>\n      <th>production_companies_9</th>\n      <th>production_companies_10</th>\n      <th>production_companies_11</th>\n      <th>production_companies_12</th>\n      <th>production_companies_13</th>\n      <th>production_companies_14</th>\n      <th>production_companies_15</th>\n      <th>production_companies_16</th>\n      <th>production_companies_17</th>\n      <th>production_companies_18</th>\n      <th>production_companies_19</th>\n      <th>production_companies_20</th>\n      <th>production_companies_21</th>\n      <th>production_companies_22</th>\n      <th>production_companies_23</th>\n      <th>production_companies_24</th>\n      <th>production_companies_25</th>\n      <th>num_production_countries</th>\n      <th>release_day</th>\n      <th>release_month</th>\n      <th>release_year</th>\n      <th>num_spoken_languages</th>\n      <th>tagline_len</th>\n      <th>title_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.00000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.00000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>...</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.00000</td>\n      <td>3000.00000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.0</td>\n      <td>3000.0</td>\n      <td>3000.0</td>\n      <td>3000.0</td>\n      <td>3000.0</td>\n      <td>3000.0</td>\n      <td>3000.0</td>\n      <td>3000.0</td>\n      <td>3000.0</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n      <td>3000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>16.408080</td>\n      <td>8.463274</td>\n      <td>15.977370</td>\n      <td>108.170000</td>\n      <td>20.603667</td>\n      <td>6.776333</td>\n      <td>4.511000</td>\n      <td>9.316333</td>\n      <td>18.262667</td>\n      <td>18.846000</td>\n      <td>19.797000</td>\n      <td>20.856333</td>\n      <td>21.951000</td>\n      <td>22.441333</td>\n      <td>22.921000</td>\n      <td>23.796333</td>\n      <td>23.844667</td>\n      <td>23.992333</td>\n      <td>24.056667</td>\n      <td>23.247000</td>\n      <td>22.761667</td>\n      <td>22.753000</td>\n      <td>22.639667</td>\n      <td>21.173333</td>\n      <td>19.801000</td>\n      <td>18.873000</td>\n      <td>17.957667</td>\n      <td>17.054000</td>\n      <td>15.960333</td>\n      <td>15.623333</td>\n      <td>14.595667</td>\n      <td>13.836667</td>\n      <td>13.437333</td>\n      <td>13.12600</td>\n      <td>12.752333</td>\n      <td>12.355000</td>\n      <td>11.986333</td>\n      <td>11.46900</td>\n      <td>10.802333</td>\n      <td>10.293333</td>\n      <td>...</td>\n      <td>71.692000</td>\n      <td>71.730333</td>\n      <td>73.724000</td>\n      <td>0.315333</td>\n      <td>0.998667</td>\n      <td>14.802000</td>\n      <td>2.698333</td>\n      <td>4094.46900</td>\n      <td>6856.01100</td>\n      <td>7345.976333</td>\n      <td>4844.554000</td>\n      <td>3584.525667</td>\n      <td>2771.314333</td>\n      <td>1748.981333</td>\n      <td>1089.431667</td>\n      <td>714.049667</td>\n      <td>285.320333</td>\n      <td>277.132667</td>\n      <td>168.727667</td>\n      <td>124.855667</td>\n      <td>98.038333</td>\n      <td>87.162000</td>\n      <td>57.416667</td>\n      <td>24.199667</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.326333</td>\n      <td>3.269333</td>\n      <td>6.775333</td>\n      <td>1999.713000</td>\n      <td>1.452333</td>\n      <td>36.358000</td>\n      <td>15.159000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.649808</td>\n      <td>12.104000</td>\n      <td>3.024962</td>\n      <td>21.198842</td>\n      <td>16.629635</td>\n      <td>9.508441</td>\n      <td>4.712248</td>\n      <td>7.374804</td>\n      <td>117.499051</td>\n      <td>114.711478</td>\n      <td>113.324575</td>\n      <td>111.949950</td>\n      <td>109.160197</td>\n      <td>104.567216</td>\n      <td>99.832391</td>\n      <td>98.415650</td>\n      <td>93.585226</td>\n      <td>90.232471</td>\n      <td>88.725202</td>\n      <td>81.291698</td>\n      <td>77.198532</td>\n      <td>75.722084</td>\n      <td>73.927841</td>\n      <td>65.044940</td>\n      <td>60.072165</td>\n      <td>57.558398</td>\n      <td>54.961084</td>\n      <td>52.046614</td>\n      <td>48.761891</td>\n      <td>49.054034</td>\n      <td>45.406388</td>\n      <td>45.284466</td>\n      <td>45.514788</td>\n      <td>45.90299</td>\n      <td>46.007592</td>\n      <td>46.178463</td>\n      <td>46.048499</td>\n      <td>45.99751</td>\n      <td>41.634910</td>\n      <td>41.536141</td>\n      <td>...</td>\n      <td>3926.732559</td>\n      <td>3928.832162</td>\n      <td>4038.029783</td>\n      <td>0.464726</td>\n      <td>0.036497</td>\n      <td>8.310264</td>\n      <td>2.014121</td>\n      <td>10387.60635</td>\n      <td>13728.56336</td>\n      <td>15828.172697</td>\n      <td>13364.062956</td>\n      <td>12606.312034</td>\n      <td>11834.314487</td>\n      <td>9712.495928</td>\n      <td>7610.761385</td>\n      <td>6591.862661</td>\n      <td>4072.133572</td>\n      <td>4350.607382</td>\n      <td>3469.924494</td>\n      <td>3068.378447</td>\n      <td>2605.128146</td>\n      <td>2536.487924</td>\n      <td>2091.424359</td>\n      <td>1325.470332</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.752349</td>\n      <td>1.306150</td>\n      <td>3.409115</td>\n      <td>15.423313</td>\n      <td>0.887688</td>\n      <td>28.321474</td>\n      <td>8.329196</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.693147</td>\n      <td>0.000001</td>\n      <td>0.693147</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1921.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>16.012735</td>\n      <td>4.018053</td>\n      <td>14.691963</td>\n      <td>94.000000</td>\n      <td>11.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>5.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>4.000000</td>\n      <td>6.000000</td>\n      <td>7.000000</td>\n      <td>8.000000</td>\n      <td>9.000000</td>\n      <td>10.000000</td>\n      <td>11.000000</td>\n      <td>11.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>9.000000</td>\n      <td>1.000000</td>\n      <td>33.00000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>4.000000</td>\n      <td>1993.000000</td>\n      <td>1.000000</td>\n      <td>17.000000</td>\n      <td>10.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>16.648724</td>\n      <td>7.374861</td>\n      <td>16.637310</td>\n      <td>104.000000</td>\n      <td>16.000000</td>\n      <td>4.000000</td>\n      <td>3.000000</td>\n      <td>8.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>5.000000</td>\n      <td>6.000000</td>\n      <td>8.000000</td>\n      <td>10.000000</td>\n      <td>11.000000</td>\n      <td>13.000000</td>\n      <td>14.000000</td>\n      <td>15.000000</td>\n      <td>16.000000</td>\n      <td>17.000000</td>\n      <td>17.000000</td>\n      <td>18.000000</td>\n      <td>19.000000</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>13.000000</td>\n      <td>2.000000</td>\n      <td>491.00000</td>\n      <td>1173.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>4.000000</td>\n      <td>7.000000</td>\n      <td>2004.000000</td>\n      <td>1.000000</td>\n      <td>32.000000</td>\n      <td>13.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>17.216708</td>\n      <td>10.890983</td>\n      <td>18.046365</td>\n      <td>118.000000</td>\n      <td>24.000000</td>\n      <td>8.000000</td>\n      <td>6.000000</td>\n      <td>12.000000</td>\n      <td>5.000000</td>\n      <td>7.000000</td>\n      <td>8.000000</td>\n      <td>11.000000</td>\n      <td>13.000000</td>\n      <td>15.000000</td>\n      <td>17.000000</td>\n      <td>19.000000</td>\n      <td>20.000000</td>\n      <td>21.250000</td>\n      <td>23.000000</td>\n      <td>24.000000</td>\n      <td>25.000000</td>\n      <td>26.000000</td>\n      <td>27.000000</td>\n      <td>29.000000</td>\n      <td>30.000000</td>\n      <td>30.000000</td>\n      <td>30.000000</td>\n      <td>30.000000</td>\n      <td>29.000000</td>\n      <td>29.000000</td>\n      <td>28.000000</td>\n      <td>26.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>18.000000</td>\n      <td>4.000000</td>\n      <td>3644.25000</td>\n      <td>7495.50000</td>\n      <td>7503.000000</td>\n      <td>192.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>4.000000</td>\n      <td>10.000000</td>\n      <td>2011.000000</td>\n      <td>2.000000</td>\n      <td>50.000000</td>\n      <td>19.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>19.755682</td>\n      <td>294.337037</td>\n      <td>21.141685</td>\n      <td>338.000000</td>\n      <td>156.000000</td>\n      <td>114.000000</td>\n      <td>87.000000</td>\n      <td>84.000000</td>\n      <td>1021.000000</td>\n      <td>1022.000000</td>\n      <td>1023.000000</td>\n      <td>1024.000000</td>\n      <td>1028.000000</td>\n      <td>1029.000000</td>\n      <td>1030.000000</td>\n      <td>1037.000000</td>\n      <td>1038.000000</td>\n      <td>1035.000000</td>\n      <td>1044.000000</td>\n      <td>1071.000000</td>\n      <td>1038.000000</td>\n      <td>1068.000000</td>\n      <td>1069.000000</td>\n      <td>1070.000000</td>\n      <td>1071.000000</td>\n      <td>1072.000000</td>\n      <td>1073.000000</td>\n      <td>1074.000000</td>\n      <td>1075.000000</td>\n      <td>1076.000000</td>\n      <td>1077.000000</td>\n      <td>1078.000000</td>\n      <td>1079.000000</td>\n      <td>1080.00000</td>\n      <td>1081.000000</td>\n      <td>1082.000000</td>\n      <td>1083.000000</td>\n      <td>1084.00000</td>\n      <td>1049.000000</td>\n      <td>1050.000000</td>\n      <td>...</td>\n      <td>215076.000000</td>\n      <td>215191.000000</td>\n      <td>221172.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>62.000000</td>\n      <td>17.000000</td>\n      <td>94930.00000</td>\n      <td>94301.00000</td>\n      <td>94198.000000</td>\n      <td>95595.000000</td>\n      <td>95018.000000</td>\n      <td>95406.000000</td>\n      <td>95342.000000</td>\n      <td>95295.000000</td>\n      <td>95296.000000</td>\n      <td>92199.000000</td>\n      <td>92231.000000</td>\n      <td>92232.000000</td>\n      <td>92233.000000</td>\n      <td>87850.000000</td>\n      <td>87851.000000</td>\n      <td>87852.000000</td>\n      <td>72599.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>8.000000</td>\n      <td>6.000000</td>\n      <td>12.000000</td>\n      <td>2017.000000</td>\n      <td>9.000000</td>\n      <td>232.000000</td>\n      <td>62.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"     belongs_to_collection     budget    ...     tagline_len  title_len\nid                                       ...                           \n3001                 34055  16.648724    ...            51.0       28.0\n3002                     0  11.385103    ...            96.0       27.0\n3003                     0  16.648724    ...            41.0       16.0\n3004                     0  15.732433    ...            55.0        9.0\n3005                     0  14.508658    ...           221.0       18.0\n\n[5 rows x 590 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>belongs_to_collection</th>\n      <th>budget</th>\n      <th>original_language</th>\n      <th>popularity</th>\n      <th>revenue</th>\n      <th>runtime</th>\n      <th>different_title</th>\n      <th>num_cast</th>\n      <th>genders_0_cast</th>\n      <th>genders_1_cast</th>\n      <th>genders_2_cast</th>\n      <th>cast_0</th>\n      <th>cast_1</th>\n      <th>cast_2</th>\n      <th>cast_3</th>\n      <th>cast_4</th>\n      <th>cast_5</th>\n      <th>cast_6</th>\n      <th>cast_7</th>\n      <th>cast_8</th>\n      <th>cast_9</th>\n      <th>cast_10</th>\n      <th>cast_11</th>\n      <th>cast_12</th>\n      <th>cast_13</th>\n      <th>cast_14</th>\n      <th>cast_15</th>\n      <th>cast_16</th>\n      <th>cast_17</th>\n      <th>cast_18</th>\n      <th>cast_19</th>\n      <th>cast_20</th>\n      <th>cast_21</th>\n      <th>cast_22</th>\n      <th>cast_23</th>\n      <th>cast_24</th>\n      <th>cast_25</th>\n      <th>cast_26</th>\n      <th>cast_27</th>\n      <th>cast_28</th>\n      <th>...</th>\n      <th>production_companies_14</th>\n      <th>production_companies_15</th>\n      <th>production_companies_16</th>\n      <th>production_companies_17</th>\n      <th>production_companies_18</th>\n      <th>production_companies_19</th>\n      <th>production_companies_20</th>\n      <th>production_companies_21</th>\n      <th>production_companies_22</th>\n      <th>production_companies_23</th>\n      <th>production_companies_24</th>\n      <th>production_companies_25</th>\n      <th>num_production_countries</th>\n      <th>production_countries_0</th>\n      <th>production_countries_1</th>\n      <th>production_countries_2</th>\n      <th>production_countries_3</th>\n      <th>production_countries_4</th>\n      <th>production_countries_5</th>\n      <th>production_countries_6</th>\n      <th>production_countries_7</th>\n      <th>production_countries_8</th>\n      <th>production_countries_9</th>\n      <th>production_countries_10</th>\n      <th>production_countries_11</th>\n      <th>release_day</th>\n      <th>release_month</th>\n      <th>release_year</th>\n      <th>num_spoken_languages</th>\n      <th>spoken_languages_0</th>\n      <th>spoken_languages_1</th>\n      <th>spoken_languages_2</th>\n      <th>spoken_languages_3</th>\n      <th>spoken_languages_4</th>\n      <th>spoken_languages_5</th>\n      <th>spoken_languages_6</th>\n      <th>spoken_languages_7</th>\n      <th>spoken_languages_8</th>\n      <th>tagline_len</th>\n      <th>title_len</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3001</th>\n      <td>34055</td>\n      <td>16.648724</td>\n      <td>ja</td>\n      <td>3.851534</td>\n      <td>0.0</td>\n      <td>90.0</td>\n      <td>True</td>\n      <td>7.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n      <td>10.0</td>\n      <td>11.0</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>US</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>7</td>\n      <td>2007</td>\n      <td>2</td>\n      <td>en</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>51.0</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>3002</th>\n      <td>0</td>\n      <td>11.385103</td>\n      <td>en</td>\n      <td>3.559789</td>\n      <td>0.0</td>\n      <td>65.0</td>\n      <td>False</td>\n      <td>10.0</td>\n      <td>6.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>11.0</td>\n      <td>12.0</td>\n      <td>13.0</td>\n      <td>14.0</td>\n      <td>15.0</td>\n      <td>16.0</td>\n      <td>17.0</td>\n      <td>18.0</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>US</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>1958</td>\n      <td>1</td>\n      <td>en</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>96.0</td>\n      <td>27.0</td>\n    </tr>\n    <tr>\n      <th>3003</th>\n      <td>0</td>\n      <td>16.648724</td>\n      <td>en</td>\n      <td>8.085194</td>\n      <td>0.0</td>\n      <td>100.0</td>\n      <td>False</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>7.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n      <td>13.0</td>\n      <td>14.0</td>\n      <td>17.0</td>\n      <td>18.0</td>\n      <td>19.0</td>\n      <td>20.0</td>\n      <td>21.0</td>\n      <td>22.0</td>\n      <td>23.0</td>\n      <td>24.0</td>\n      <td>25.0</td>\n      <td>26.0</td>\n      <td>88.0</td>\n      <td>89.0</td>\n      <td>90.0</td>\n      <td>91.0</td>\n      <td>92.0</td>\n      <td>93.0</td>\n      <td>94.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>CA</td>\n      <td>FR</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1997</td>\n      <td>1</td>\n      <td>ar</td>\n      <td>en</td>\n      <td>fr</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>41.0</td>\n      <td>16.0</td>\n    </tr>\n    <tr>\n      <th>3004</th>\n      <td>0</td>\n      <td>15.732433</td>\n      <td>fr</td>\n      <td>8.596012</td>\n      <td>0.0</td>\n      <td>130.0</td>\n      <td>False</td>\n      <td>23.0</td>\n      <td>15.0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>US</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>9</td>\n      <td>2010</td>\n      <td>3</td>\n      <td>en</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>55.0</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>3005</th>\n      <td>0</td>\n      <td>14.508658</td>\n      <td>en</td>\n      <td>3.217680</td>\n      <td>0.0</td>\n      <td>92.0</td>\n      <td>False</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>7.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n      <td>10.0</td>\n      <td>12.0</td>\n      <td>13.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>US</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2</td>\n      <td>2005</td>\n      <td>1</td>\n      <td>en</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>221.0</td>\n      <td>18.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"            budget   popularity     ...       tagline_len    title_len\ncount  4399.000000  4399.000000     ...       4399.000000  4399.000000\nmean     16.363691     8.548286     ...         36.651512    15.116845\nstd       1.803768    12.208307     ...         28.882250     8.394436\nmin       0.693147     0.000000     ...          3.000000     1.000000\n25%      16.012735     3.888453     ...         18.000000     9.000000\n50%      16.648724     7.481524     ...         33.000000    13.000000\n75%      17.147715    10.936597     ...         50.000000    19.000000\nmax      19.376192   547.488298     ...        252.000000   104.000000\n\n[8 rows x 372 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>budget</th>\n      <th>popularity</th>\n      <th>revenue</th>\n      <th>runtime</th>\n      <th>num_cast</th>\n      <th>genders_0_cast</th>\n      <th>genders_1_cast</th>\n      <th>genders_2_cast</th>\n      <th>cast_0</th>\n      <th>cast_1</th>\n      <th>cast_2</th>\n      <th>cast_3</th>\n      <th>cast_4</th>\n      <th>cast_5</th>\n      <th>cast_6</th>\n      <th>cast_7</th>\n      <th>cast_8</th>\n      <th>cast_9</th>\n      <th>cast_10</th>\n      <th>cast_11</th>\n      <th>cast_12</th>\n      <th>cast_13</th>\n      <th>cast_14</th>\n      <th>cast_15</th>\n      <th>cast_16</th>\n      <th>cast_17</th>\n      <th>cast_18</th>\n      <th>cast_19</th>\n      <th>cast_20</th>\n      <th>cast_21</th>\n      <th>cast_22</th>\n      <th>cast_23</th>\n      <th>cast_24</th>\n      <th>cast_25</th>\n      <th>cast_26</th>\n      <th>cast_27</th>\n      <th>cast_28</th>\n      <th>cast_29</th>\n      <th>cast_30</th>\n      <th>cast_31</th>\n      <th>...</th>\n      <th>Keywords_146</th>\n      <th>Keywords_147</th>\n      <th>Keywords_148</th>\n      <th>Has_HomePage</th>\n      <th>IsReleased</th>\n      <th>original_title_len</th>\n      <th>num_production_companies</th>\n      <th>production_companies_0</th>\n      <th>production_companies_1</th>\n      <th>production_companies_2</th>\n      <th>production_companies_3</th>\n      <th>production_companies_4</th>\n      <th>production_companies_5</th>\n      <th>production_companies_6</th>\n      <th>production_companies_7</th>\n      <th>production_companies_8</th>\n      <th>production_companies_9</th>\n      <th>production_companies_10</th>\n      <th>production_companies_11</th>\n      <th>production_companies_12</th>\n      <th>production_companies_13</th>\n      <th>production_companies_14</th>\n      <th>production_companies_15</th>\n      <th>production_companies_16</th>\n      <th>production_companies_17</th>\n      <th>production_companies_18</th>\n      <th>production_companies_19</th>\n      <th>production_companies_20</th>\n      <th>production_companies_21</th>\n      <th>production_companies_22</th>\n      <th>production_companies_23</th>\n      <th>production_companies_24</th>\n      <th>production_companies_25</th>\n      <th>num_production_countries</th>\n      <th>release_day</th>\n      <th>release_month</th>\n      <th>release_year</th>\n      <th>num_spoken_languages</th>\n      <th>tagline_len</th>\n      <th>title_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.0</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>...</td>\n      <td>4399.0</td>\n      <td>4399.0</td>\n      <td>4399.0</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n      <td>4399.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>16.363691</td>\n      <td>8.548286</td>\n      <td>0.0</td>\n      <td>107.713799</td>\n      <td>21.191862</td>\n      <td>7.204137</td>\n      <td>4.650602</td>\n      <td>9.337122</td>\n      <td>25.846101</td>\n      <td>26.218459</td>\n      <td>27.319618</td>\n      <td>28.012730</td>\n      <td>28.726756</td>\n      <td>29.004774</td>\n      <td>29.940441</td>\n      <td>29.956581</td>\n      <td>28.968857</td>\n      <td>28.933621</td>\n      <td>27.755399</td>\n      <td>26.565583</td>\n      <td>25.675835</td>\n      <td>25.305297</td>\n      <td>24.613549</td>\n      <td>23.084792</td>\n      <td>22.189589</td>\n      <td>21.112071</td>\n      <td>20.117072</td>\n      <td>19.141850</td>\n      <td>17.951353</td>\n      <td>17.542851</td>\n      <td>16.697204</td>\n      <td>15.875199</td>\n      <td>14.450102</td>\n      <td>13.522846</td>\n      <td>13.276199</td>\n      <td>12.929984</td>\n      <td>12.331212</td>\n      <td>11.929757</td>\n      <td>11.546261</td>\n      <td>10.960900</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.322801</td>\n      <td>0.997727</td>\n      <td>14.821778</td>\n      <td>2.775404</td>\n      <td>3959.911343</td>\n      <td>7029.519436</td>\n      <td>7224.090020</td>\n      <td>5573.641055</td>\n      <td>3839.711525</td>\n      <td>2659.363719</td>\n      <td>1623.482155</td>\n      <td>1101.272107</td>\n      <td>867.653785</td>\n      <td>459.159127</td>\n      <td>406.545579</td>\n      <td>225.395090</td>\n      <td>189.354853</td>\n      <td>156.375313</td>\n      <td>140.325756</td>\n      <td>148.367811</td>\n      <td>113.940441</td>\n      <td>116.981132</td>\n      <td>98.165038</td>\n      <td>66.877472</td>\n      <td>55.300296</td>\n      <td>32.700159</td>\n      <td>14.754944</td>\n      <td>14.755399</td>\n      <td>12.200500</td>\n      <td>12.200727</td>\n      <td>1.336895</td>\n      <td>3.226642</td>\n      <td>6.886338</td>\n      <td>1999.670834</td>\n      <td>1.441464</td>\n      <td>36.651512</td>\n      <td>15.116845</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.803768</td>\n      <td>12.208307</td>\n      <td>0.0</td>\n      <td>20.817570</td>\n      <td>17.981498</td>\n      <td>10.682749</td>\n      <td>4.616389</td>\n      <td>7.390047</td>\n      <td>145.386772</td>\n      <td>142.379990</td>\n      <td>141.624291</td>\n      <td>139.352724</td>\n      <td>135.403558</td>\n      <td>130.495478</td>\n      <td>128.857468</td>\n      <td>124.648013</td>\n      <td>117.536894</td>\n      <td>113.892978</td>\n      <td>105.181922</td>\n      <td>97.965496</td>\n      <td>92.489098</td>\n      <td>90.445506</td>\n      <td>84.365382</td>\n      <td>79.219822</td>\n      <td>76.735035</td>\n      <td>74.021072</td>\n      <td>69.640770</td>\n      <td>66.517682</td>\n      <td>64.851212</td>\n      <td>65.077059</td>\n      <td>63.419169</td>\n      <td>61.523973</td>\n      <td>52.943320</td>\n      <td>46.049604</td>\n      <td>46.350886</td>\n      <td>46.446462</td>\n      <td>43.780106</td>\n      <td>43.786304</td>\n      <td>41.159558</td>\n      <td>38.181007</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.467600</td>\n      <td>0.047630</td>\n      <td>8.337420</td>\n      <td>2.296967</td>\n      <td>9868.395794</td>\n      <td>14674.804112</td>\n      <td>16028.797132</td>\n      <td>15051.383047</td>\n      <td>13100.112056</td>\n      <td>11251.828769</td>\n      <td>8904.257387</td>\n      <td>7517.042627</td>\n      <td>6926.744211</td>\n      <td>4885.615114</td>\n      <td>4890.954131</td>\n      <td>3314.296663</td>\n      <td>3001.888252</td>\n      <td>2584.117096</td>\n      <td>2510.281626</td>\n      <td>2728.659758</td>\n      <td>2570.825674</td>\n      <td>2688.117559</td>\n      <td>2489.663162</td>\n      <td>2061.304500</td>\n      <td>1839.797677</td>\n      <td>1448.975499</td>\n      <td>826.682644</td>\n      <td>826.700489</td>\n      <td>809.197653</td>\n      <td>809.212730</td>\n      <td>0.815483</td>\n      <td>1.343558</td>\n      <td>3.371805</td>\n      <td>15.286348</td>\n      <td>0.899026</td>\n      <td>28.882250</td>\n      <td>8.394436</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.693147</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1922.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>16.012735</td>\n      <td>3.888453</td>\n      <td>0.0</td>\n      <td>94.000000</td>\n      <td>11.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>5.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>4.000000</td>\n      <td>6.000000</td>\n      <td>7.000000</td>\n      <td>8.000000</td>\n      <td>9.000000</td>\n      <td>10.000000</td>\n      <td>11.000000</td>\n      <td>11.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>9.000000</td>\n      <td>1.000000</td>\n      <td>33.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>4.000000</td>\n      <td>1992.000000</td>\n      <td>1.000000</td>\n      <td>18.000000</td>\n      <td>9.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>16.648724</td>\n      <td>7.481524</td>\n      <td>0.0</td>\n      <td>104.000000</td>\n      <td>16.000000</td>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>8.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>5.000000</td>\n      <td>6.000000</td>\n      <td>8.000000</td>\n      <td>10.000000</td>\n      <td>11.000000</td>\n      <td>13.000000</td>\n      <td>14.000000</td>\n      <td>15.000000</td>\n      <td>16.000000</td>\n      <td>17.000000</td>\n      <td>17.000000</td>\n      <td>18.000000</td>\n      <td>18.000000</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>13.000000</td>\n      <td>2.000000</td>\n      <td>441.000000</td>\n      <td>1088.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>4.000000</td>\n      <td>7.000000</td>\n      <td>2004.000000</td>\n      <td>1.000000</td>\n      <td>33.000000</td>\n      <td>13.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>17.147715</td>\n      <td>10.936597</td>\n      <td>0.0</td>\n      <td>118.000000</td>\n      <td>24.000000</td>\n      <td>8.000000</td>\n      <td>6.000000</td>\n      <td>12.000000</td>\n      <td>5.000000</td>\n      <td>7.000000</td>\n      <td>9.000000</td>\n      <td>10.000000</td>\n      <td>13.000000</td>\n      <td>15.000000</td>\n      <td>17.000000</td>\n      <td>19.000000</td>\n      <td>20.000000</td>\n      <td>22.000000</td>\n      <td>23.000000</td>\n      <td>25.000000</td>\n      <td>26.000000</td>\n      <td>27.000000</td>\n      <td>28.000000</td>\n      <td>29.000000</td>\n      <td>30.000000</td>\n      <td>30.000000</td>\n      <td>30.000000</td>\n      <td>30.000000</td>\n      <td>30.000000</td>\n      <td>29.000000</td>\n      <td>28.000000</td>\n      <td>27.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>18.000000</td>\n      <td>4.000000</td>\n      <td>3688.500000</td>\n      <td>7364.000000</td>\n      <td>7295.000000</td>\n      <td>932.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>4.000000</td>\n      <td>10.000000</td>\n      <td>2011.000000</td>\n      <td>2.000000</td>\n      <td>50.000000</td>\n      <td>19.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>19.376192</td>\n      <td>547.488298</td>\n      <td>0.0</td>\n      <td>320.000000</td>\n      <td>165.000000</td>\n      <td>143.000000</td>\n      <td>60.000000</td>\n      <td>85.000000</td>\n      <td>1055.000000</td>\n      <td>1056.000000</td>\n      <td>1057.000000</td>\n      <td>1059.000000</td>\n      <td>1060.000000</td>\n      <td>1061.000000</td>\n      <td>1062.000000</td>\n      <td>1063.000000</td>\n      <td>1064.000000</td>\n      <td>1065.000000</td>\n      <td>1066.000000</td>\n      <td>1092.000000</td>\n      <td>1093.000000</td>\n      <td>1094.000000</td>\n      <td>1095.000000</td>\n      <td>1096.000000</td>\n      <td>1101.000000</td>\n      <td>1102.000000</td>\n      <td>1103.000000</td>\n      <td>1104.000000</td>\n      <td>1105.000000</td>\n      <td>1106.000000</td>\n      <td>1107.000000</td>\n      <td>1108.000000</td>\n      <td>1060.000000</td>\n      <td>1061.000000</td>\n      <td>1062.000000</td>\n      <td>1063.000000</td>\n      <td>1064.000000</td>\n      <td>1065.000000</td>\n      <td>1066.000000</td>\n      <td>1067.000000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>104.000000</td>\n      <td>26.000000</td>\n      <td>95102.000000</td>\n      <td>96035.000000</td>\n      <td>96043.000000</td>\n      <td>95674.000000</td>\n      <td>95983.000000</td>\n      <td>95345.000000</td>\n      <td>93119.000000</td>\n      <td>89163.000000</td>\n      <td>95289.000000</td>\n      <td>95290.000000</td>\n      <td>93466.000000</td>\n      <td>92187.000000</td>\n      <td>79077.000000</td>\n      <td>76994.000000</td>\n      <td>77813.000000</td>\n      <td>88099.000000</td>\n      <td>88100.000000</td>\n      <td>88101.000000</td>\n      <td>88102.000000</td>\n      <td>79438.000000</td>\n      <td>79439.000000</td>\n      <td>78943.000000</td>\n      <td>53668.000000</td>\n      <td>53669.000000</td>\n      <td>53670.000000</td>\n      <td>53671.000000</td>\n      <td>12.000000</td>\n      <td>6.000000</td>\n      <td>12.000000</td>\n      <td>2018.000000</td>\n      <td>9.000000</td>\n      <td>252.000000</td>\n      <td>104.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Now we have our train and test data, we should define the categorical data for catboost. Because catboost could also handle string categorical data if defined. No need to make some label encoding for catboost."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tr = train.drop(LABEL_COL_NAME, axis = 1)\ny_tr = train[LABEL_COL_NAME]\nnumerical_features = ['budget',\n                      'popularity', \n                      'runtime', \n                      'title_len', \n                      'original_title_len', \n                      'tagline_len',\n                      'num_crew',\n                      'num_cast',\n                      'num_keywords',\n                      'num_production_companies',\n                      'num_production_countries',\n                      'num_spoken_languages']\ncat_features = set(X_tr.columns) - set(numerical_features)\ncat_features = [list(X_tr.columns).index(c) for c in cat_features]","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Cross Validator Functions and Hyper Parameter Optimizations.</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import required packages\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nimport gc\nfrom hyperopt import hp, tpe, Trials, STATUS_OK\nfrom hyperopt.fmin import fmin\nfrom hyperopt.pyll.stochastic import sample\n#optional but advised\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#GLOBAL HYPEROPT PARAMETERS\nNUM_EVALS = 100 #number of hyperopt evaluation rounds\nN_FOLDS = 3 #number of cross-validation folds on data in each evaluation round\n\n#LIGHTGBM PARAMETERS\nLGBM_MAX_LEAVES = 2**11 #maximum number of leaves per tree for LightGBM\nLGBM_MAX_DEPTH = 25 #maximum tree depth for LightGBM\nEVAL_METRIC_LGBM_REG = 'mae' #LightGBM regression metric. Note that 'rmse' is more commonly used \nEVAL_METRIC_LGBM_CLASS = 'auc'#LightGBM classification metric\n\n#XGBOOST PARAMETERS\nXGB_MAX_LEAVES = 2**12 #maximum number of leaves when using histogram splitting\nXGB_MAX_DEPTH = 25 #maximum tree depth for XGBoost\nEVAL_METRIC_XGB_REG = 'mae' #XGBoost regression metric\nEVAL_METRIC_XGB_CLASS = 'auc' #XGBoost classification metric\n\n#CATBOOST PARAMETERS\nCB_MAX_DEPTH = 6 #maximum tree depth in CatBoost\nOBJECTIVE_CB_REG = 'RMSE' #CatBoost regression metric\nOBJECTIVE_CB_CLASS = 'Logloss' #CatBoost classification metric\n\ndef quick_hyperopt(data, labels, package='lgbm', num_evals=NUM_EVALS, diagnostic=False, cat_features=[]):\n    \n    #==========\n    #LightGBM\n    #==========\n    \n    if package=='lgbm':\n        \n        print('Running {} rounds of LightGBM parameter optimisation:'.format(num_evals))\n        #clear space\n        gc.collect()\n        \n        integer_params = ['max_depth',\n                         'num_leaves',\n                          'max_bin',\n                         'min_data_in_leaf',\n                         'min_data_in_bin']\n        \n        def objective(space_params):\n            \n            #cast integer params from float to int\n            for param in integer_params:\n                space_params[param] = int(space_params[param])\n            \n            #extract nested conditional parameters\n            if space_params['boosting']['boosting'] == 'goss':\n                top_rate = space_params['boosting'].get('top_rate')\n                other_rate = space_params['boosting'].get('other_rate')\n                #0 <= top_rate + other_rate <= 1\n                top_rate = max(top_rate, 0)\n                top_rate = min(top_rate, 0.5)\n                other_rate = max(other_rate, 0)\n                other_rate = min(other_rate, 0.5)\n                space_params['top_rate'] = top_rate\n                space_params['other_rate'] = other_rate\n            \n            subsample = space_params['boosting'].get('subsample', 1.0)\n            space_params['boosting'] = space_params['boosting']['boosting']\n            space_params['subsample'] = subsample\n            \n            #for classification, set stratified=True and metrics=EVAL_METRIC_LGBM_CLASS\n            cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=False,\n                                early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_REG, seed=42)\n            \n            best_loss = cv_results['l1-mean'][-1] #'l2-mean' for rmse\n            #for classification, comment out the line above and uncomment the line below:\n            #best_loss = 1 - cv_results['auc-mean'][-1]\n            #if necessary, replace 'auc-mean' with '[your-preferred-metric]-mean'\n            return{'loss':best_loss, 'status': STATUS_OK }\n        \n        train = lgb.Dataset(data, labels)\n                \n        #integer and string parameters, used with hp.choice()\n        boosting_list = [{'boosting': 'gbdt',\n                          'subsample': hp.uniform('subsample', 0.5, 1)},\n                         {'boosting': 'goss',\n                          'subsample': 1.0,\n                         'top_rate': hp.uniform('top_rate', 0, 0.5),\n                         'other_rate': hp.uniform('other_rate', 0, 0.5)}] #if including 'dart', make sure to set 'n_estimators'\n        metric_list = ['MAE', 'RMSE'] \n        #for classification comment out the line above and uncomment the line below\n        #modify as required for other classification metrics classification\n        #metric_list = ['auc']\n        objective_list_reg = ['huber', 'gamma', 'fair', 'tweedie']\n        objective_list_class = ['logloss', 'cross_entropy']\n        #for classification set objective_list = objective_list_class\n        objective_list = objective_list_reg\n\n        space ={'boosting' : hp.choice('boosting', boosting_list),\n                'num_leaves' : hp.quniform('num_leaves', 2, LGBM_MAX_LEAVES, 1),\n                'max_depth': hp.quniform('max_depth', 2, LGBM_MAX_DEPTH, 1),\n                'max_bin': hp.quniform('max_bin', 32, 255, 1),\n                'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 256, 1),\n                'min_data_in_bin': hp.quniform('min_data_in_bin', 1, 256, 1),\n                'lambda_l1' : hp.uniform('lambda_l1', 0, 5),\n                'lambda_l2' : hp.uniform('lambda_l2', 0, 5),\n                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n                'metric' : hp.choice('metric', metric_list),\n                'objective' : hp.choice('objective', objective_list),\n                'feature_fraction' : hp.quniform('feature_fraction', 0.5, 1, 0.01),\n                'bagging_fraction' : hp.quniform('bagging_fraction', 0.5, 1, 0.01)\n            }\n        \n        #optional: activate GPU for LightGBM\n        #follow compilation steps here:\n        #https://www.kaggle.com/vinhnguyen/gpu-acceleration-for-lightgbm/\n        #then uncomment lines below:\n        #space['device'] = 'gpu'\n        #space['gpu_platform_id'] = 0,\n        #space['gpu_device_id'] =  0\n\n        trials = Trials()\n        best = fmin(fn=objective,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=num_evals, \n                    trials=trials)\n                \n        #fmin() will return the index of values chosen from the lists/arrays in 'space'\n        #to obtain actual values, index values are used to subset the original lists/arrays\n        best['boosting'] = boosting_list[best['boosting']]['boosting']#nested dict, index twice\n        best['metric'] = metric_list[best['metric']]\n        best['objective'] = objective_list[best['objective']]\n        \n        #cast floats of integer params to int\n        for param in integer_params:\n            best[param] = int(best[param])\n            \n        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n        if diagnostic:\n            return(best, trials)\n        else:\n            return(best)\n    \n    #==========\n    #XGBoost\n    #==========\n    \n    if package=='xgb':\n        \n        print('Running {} rounds of XGBoost parameter optimisation:'.format(num_evals))\n        #clear space\n        gc.collect()\n        \n        integer_params = ['max_depth']\n        \n        def objective(space_params):\n            \n            for param in integer_params:\n                space_params[param] = int(space_params[param])\n                \n            #extract multiple nested tree_method conditional parameters\n            #libera te tutemet ex inferis\n            if space_params['tree_method']['tree_method'] == 'hist':\n                max_bin = space_params['tree_method'].get('max_bin')\n                space_params['max_bin'] = int(max_bin)\n                if space_params['tree_method']['grow_policy']['grow_policy']['grow_policy'] == 'depthwise':\n                    grow_policy = space_params['tree_method'].get('grow_policy').get('grow_policy').get('grow_policy')\n                    space_params['grow_policy'] = grow_policy\n                    space_params['tree_method'] = 'hist'\n                else:\n                    max_leaves = space_params['tree_method']['grow_policy']['grow_policy'].get('max_leaves')\n                    space_params['grow_policy'] = 'lossguide'\n                    space_params['max_leaves'] = int(max_leaves)\n                    space_params['tree_method'] = 'hist'\n            else:\n                space_params['tree_method'] = space_params['tree_method'].get('tree_method')\n                \n            #for classification replace EVAL_METRIC_XGB_REG with EVAL_METRIC_XGB_CLASS\n            cv_results = xgb.cv(space_params, train, nfold=N_FOLDS, metrics=[EVAL_METRIC_XGB_REG],\n                             early_stopping_rounds=100, stratified=False, seed=42)\n            \n            best_loss = cv_results['test-mae-mean'].iloc[-1] #or 'test-rmse-mean' if using RMSE\n            #for classification, comment out the line above and uncomment the line below:\n            #best_loss = 1 - cv_results['test-auc-mean'].iloc[-1]\n            #if necessary, replace 'test-auc-mean' with 'test-[your-preferred-metric]-mean'\n            return{'loss':best_loss, 'status': STATUS_OK }\n        \n        train = xgb.DMatrix(data, labels)\n        \n        #integer and string parameters, used with hp.choice()\n        boosting_list = ['gbtree', 'gblinear'] #if including 'dart', make sure to set 'n_estimators'\n        metric_list = ['MAE', 'RMSE'] \n        #for classification comment out the line above and uncomment the line below\n        #metric_list = ['auc']\n        #modify as required for other classification metrics classification\n        \n        tree_method = [{'tree_method' : 'exact'},\n               {'tree_method' : 'approx'},\n               {'tree_method' : 'hist',\n                'max_bin': hp.quniform('max_bin', 2**3, 2**7, 1),\n                'grow_policy' : {'grow_policy': {'grow_policy':'depthwise'},\n                                'grow_policy' : {'grow_policy':'lossguide',\n                                                  'max_leaves': hp.quniform('max_leaves', 32, XGB_MAX_LEAVES, 1)}}}]\n        \n        #if using GPU, replace 'exact' with 'gpu_exact' and 'hist' with\n        #'gpu_hist' in the nested dictionary above\n        \n        objective_list_reg = ['reg:linear', 'reg:gamma', 'reg:tweedie']\n        objective_list_class = ['reg:logistic', 'binary:logistic']\n        #for classification change line below to 'objective_list = objective_list_class'\n        objective_list = objective_list_reg\n        \n        space ={'boosting' : hp.choice('boosting', boosting_list),\n                'tree_method' : hp.choice('tree_method', tree_method),\n                'max_depth': hp.quniform('max_depth', 2, XGB_MAX_DEPTH, 1),\n                'reg_alpha' : hp.uniform('reg_alpha', 0, 5),\n                'reg_lambda' : hp.uniform('reg_lambda', 0, 5),\n                'min_child_weight' : hp.uniform('min_child_weight', 0, 5),\n                'gamma' : hp.uniform('gamma', 0, 5),\n                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n                'eval_metric' : hp.choice('eval_metric', metric_list),\n                'objective' : hp.choice('objective', objective_list),\n                'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1, 0.01),\n                'colsample_bynode' : hp.quniform('colsample_bynode', 0.1, 1, 0.01),\n                'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),\n                'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n                'nthread' : -1\n            }\n        \n        #optional: activate GPU for XGBoost\n        #uncomment line below\n        #space['tree_method'] = 'gpu_hist'\n        \n        trials = Trials()\n        best = fmin(fn=objective,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=num_evals, \n                    trials=trials)\n        \n        best['tree_method'] = tree_method[best['tree_method']]['tree_method']\n        best['boosting'] = boosting_list[best['boosting']]\n        best['eval_metric'] = metric_list[best['eval_metric']]\n        best['objective'] = objective_list[best['objective']]\n        \n        #cast floats of integer params to int\n        for param in integer_params:\n            best[param] = int(best[param])\n        if 'max_leaves' in best:\n            best['max_leaves'] = int(best['max_leaves'])\n        if 'max_bin' in best:\n            best['max_bin'] = int(best['max_bin'])\n        \n        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n        \n        if diagnostic:\n            return(best, trials)\n        else:\n            return(best)\n    \n    #==========\n    #CatBoost\n    #==========\n    \n    if package=='cb':\n        \n        print('Running {} rounds of CatBoost parameter optimisation:'.format(num_evals))\n        \n        #clear memory \n        gc.collect()\n            \n        integer_params = ['depth',\n                          'one_hot_max_size', #for categorical data\n                          'min_data_in_leaf',\n                          'max_bin']\n        \n        def objective(space_params):\n                        \n            #cast integer params from float to int\n            for param in integer_params:\n                space_params[param] = int(space_params[param])\n                \n            #extract nested conditional parameters\n            if space_params['bootstrap_type']['bootstrap_type'] == 'Bayesian':\n                bagging_temp = space_params['bootstrap_type'].get('bagging_temperature')\n                space_params['bagging_temperature'] = bagging_temp\n                \n            if space_params['grow_policy']['grow_policy'] == 'LossGuide':\n                max_leaves = space_params['grow_policy'].get('max_leaves')\n                space_params['max_leaves'] = int(max_leaves)\n                \n            space_params['bootstrap_type'] = space_params['bootstrap_type']['bootstrap_type']\n            space_params['grow_policy'] = space_params['grow_policy']['grow_policy']\n                           \n            #random_strength cannot be < 0\n            space_params['random_strength'] = max(space_params['random_strength'], 0)\n            #fold_len_multiplier cannot be < 1\n            space_params['fold_len_multiplier'] = max(space_params['fold_len_multiplier'], 1)\n                       \n            #for classification set stratified=True\n            cv_results = cb.cv(train, space_params, fold_count=N_FOLDS, \n                             early_stopping_rounds=25, stratified=False, partition_random_seed=42)\n           \n            #best_loss = cv_results['test-MAE-mean'].iloc[-1] \n            best_loss = cv_results['test-RMSE-mean'].iloc[-1] \n            \n            #for classification, comment out the line above and uncomment the line below:\n            #best_loss = cv_results['test-Logloss-mean'].iloc[-1]\n            #if necessary, replace 'test-Logloss-mean' with 'test-[your-preferred-metric]-mean'\n            \n            return{'loss':best_loss, 'status': STATUS_OK}\n        \n        train = cb.Pool(data, labels.astype('float32'), cat_features=cat_features)\n        \n        #integer and string parameters, used with hp.choice()\n        bootstrap_type = [\n                          {'bootstrap_type':'Poisson'}, \n                          {'bootstrap_type':'Bayesian', 'bagging_temperature' : hp.loguniform('bagging_temperature', np.log(1), np.log(50))},\n                          {'bootstrap_type':'Bernoulli'}] \n        LEB = ['No', 'AnyImprovement', 'Armijo'] #remove 'Armijo' if not using GPU\n        #score_function = ['Correlation', 'L2', 'NewtonCorrelation', 'NewtonL2']\n        grow_policy = [{'grow_policy':'SymmetricTree'},\n                       {'grow_policy':'Depthwise'},\n                       {'grow_policy':'Lossguide',\n                        'max_leaves': hp.quniform('max_leaves', 2, 32, 1)}]\n        eval_metric_list_reg = ['MAE', 'RMSE', 'Poisson']\n        eval_metric_list_class = ['Logloss', 'AUC', 'F1']\n        #for classification change line below to 'eval_metric_list = eval_metric_list_class'\n        eval_metric_list = eval_metric_list_reg\n                \n        space ={'depth': hp.quniform('depth', 2, CB_MAX_DEPTH, 1),\n                'max_bin' : hp.quniform('max_bin', 1, 32, 1), #if using CPU just set this to 254\n                #'max_bin': 254,\n                'l2_leaf_reg' : hp.uniform('l2_leaf_reg', 0, 5),\n                'min_data_in_leaf' : hp.quniform('min_data_in_leaf', 1, 50, 1),\n                'random_strength' : hp.loguniform('random_strength', np.log(0.005), np.log(5)),\n                'one_hot_max_size' : hp.quniform('one_hot_max_size', 2, 16, 1), #uncomment if using categorical features\n                'bootstrap_type' : hp.choice('bootstrap_type', bootstrap_type),\n                'learning_rate' : hp.uniform('learning_rate', 0.05, 0.25),\n                'eval_metric' : hp.choice('eval_metric', eval_metric_list),\n                'objective' : OBJECTIVE_CB_REG,\n                #'score_function' : hp.choice('score_function', score_function), #crashes kernel - reason unknown\n                'leaf_estimation_backtracking' : hp.choice('leaf_estimation_backtracking', LEB),\n                'grow_policy': hp.choice('grow_policy', grow_policy),\n                #'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),# CPU only\n                'fold_len_multiplier' : hp.loguniform('fold_len_multiplier', np.log(1.01), np.log(2.5)),\n                'od_type' : 'Iter',\n                'od_wait' : 25,\n                'task_type' : 'GPU',\n                'verbose' : 0,\n                'cat_features': cat_features\n            }\n        \n        #optional: run CatBoost without GPU\n        #uncomment line below\n        #space['task_type'] = 'CPU'\n            \n        trials = Trials()\n        best = fmin(fn=objective,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=num_evals, \n                    trials=trials)\n        \n        #unpack nested dicts first\n        best['bootstrap_type'] = bootstrap_type[best['bootstrap_type']]['bootstrap_type']\n        best['grow_policy'] = grow_policy[best['grow_policy']]['grow_policy']\n        best['eval_metric'] = eval_metric_list[best['eval_metric']]\n        \n        #best['score_function'] = score_function[best['score_function']] \n        #best['leaf_estimation_method'] = LEM[best['leaf_estimation_method']] #CPU only\n        best['leaf_estimation_backtracking'] = LEB[best['leaf_estimation_backtracking']]        \n        \n        #cast floats of integer params to int\n        for param in integer_params:\n            best[param] = int(best[param])\n        if 'max_leaves' in best:\n            best['max_leaves'] = int(best['max_leaves'])\n        \n        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n        \n        if diagnostic:\n            return(best, trials)\n        else:\n            return(best)\n    \n    else:\n        print('Package not recognised. Please use \"lgbm\" for LightGBM, \"xgb\" for XGBoost or \"cb\" for CatBoost.')     ","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cb_params = quick_hyperopt(X_tr, y_tr, 'cb', 15, cat_features=cat_features)\nnp.save('cb_params.npy', cb_params)\nprint(cb_params)","execution_count":14,"outputs":[{"output_type":"stream","text":"Running 15 rounds of CatBoost parameter optimisation:\n100%|██████████| 15/15 [27:13<00:00, 91.19s/it, best loss: 2.1659739060428613] \n{bootstrap_type: Bernoulli\ndepth: 6\neval_metric: MAE\nfold_len_multiplier: 1.8090889285313607\ngrow_policy: Depthwise\nl2_leaf_reg: 2.8807709614379338\nleaf_estimation_backtracking: Armijo\nlearning_rate: 0.06499349671407013\nmax_bin: 32\nmin_data_in_leaf: 28\none_hot_max_size: 5\nrandom_strength: 0.28920711927167503}\n{'bootstrap_type': 'Bernoulli', 'depth': 6, 'eval_metric': 'MAE', 'fold_len_multiplier': 1.8090889285313607, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.8807709614379338, 'leaf_estimation_backtracking': 'Armijo', 'learning_rate': 0.06499349671407013, 'max_bin': 32, 'min_data_in_leaf': 28, 'one_hot_max_size': 5, 'random_strength': 0.28920711927167503}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    model = CatBoostRegressor(**cb_params, task_type='GPU')\n    model.fit(X_tr, y_tr, cat_features=cat_features)    \nexcept:\n    print(\"GPU grow_policy error, just remove it\")\n    cb_params.pop('grow_policy')\n    model = CatBoostRegressor(**cb_params, task_type='GPU')\n    model.fit(X_tr, y_tr, cat_features=cat_features)","execution_count":15,"outputs":[{"output_type":"stream","text":"0:\tlearn: 14.9390690\ttotal: 20.4ms\tremaining: 20.4s\n1:\tlearn: 13.9697474\ttotal: 39.9ms\tremaining: 19.9s\n2:\tlearn: 13.0677435\ttotal: 57ms\tremaining: 18.9s\n3:\tlearn: 12.2277031\ttotal: 74.4ms\tremaining: 18.5s\n4:\tlearn: 11.4445313\ttotal: 90.9ms\tremaining: 18.1s\n5:\tlearn: 10.7143965\ttotal: 107ms\tremaining: 17.8s\n6:\tlearn: 10.0334518\ttotal: 123ms\tremaining: 17.5s\n7:\tlearn: 9.3984492\ttotal: 139ms\tremaining: 17.3s\n8:\tlearn: 8.8058600\ttotal: 153ms\tremaining: 16.9s\n9:\tlearn: 8.2522637\ttotal: 167ms\tremaining: 16.5s\n10:\tlearn: 7.7355313\ttotal: 180ms\tremaining: 16.1s\n11:\tlearn: 7.2533906\ttotal: 192ms\tremaining: 15.8s\n12:\tlearn: 6.8029635\ttotal: 205ms\tremaining: 15.6s\n13:\tlearn: 6.3818034\ttotal: 217ms\tremaining: 15.3s\n14:\tlearn: 5.9910872\ttotal: 230ms\tremaining: 15.1s\n15:\tlearn: 5.6273665\ttotal: 242ms\tremaining: 14.9s\n16:\tlearn: 5.2897643\ttotal: 254ms\tremaining: 14.7s\n17:\tlearn: 4.9758577\ttotal: 267ms\tremaining: 14.5s\n18:\tlearn: 4.6850430\ttotal: 278ms\tremaining: 14.4s\n19:\tlearn: 4.4162474\ttotal: 290ms\tremaining: 14.2s\n20:\tlearn: 4.1663053\ttotal: 302ms\tremaining: 14.1s\n21:\tlearn: 3.9346608\ttotal: 314ms\tremaining: 14s\n22:\tlearn: 3.7202653\ttotal: 333ms\tremaining: 14.1s\n23:\tlearn: 3.5231331\ttotal: 344ms\tremaining: 14s\n24:\tlearn: 3.3407272\ttotal: 356ms\tremaining: 13.9s\n25:\tlearn: 3.1738457\ttotal: 368ms\tremaining: 13.8s\n26:\tlearn: 3.0159180\ttotal: 379ms\tremaining: 13.7s\n27:\tlearn: 2.8727878\ttotal: 391ms\tremaining: 13.6s\n28:\tlearn: 2.7404134\ttotal: 402ms\tremaining: 13.5s\n29:\tlearn: 2.6180566\ttotal: 413ms\tremaining: 13.4s\n30:\tlearn: 2.5025498\ttotal: 424ms\tremaining: 13.3s\n31:\tlearn: 2.3995273\ttotal: 436ms\tremaining: 13.2s\n32:\tlearn: 2.3044279\ttotal: 447ms\tremaining: 13.1s\n33:\tlearn: 2.2154229\ttotal: 459ms\tremaining: 13s\n34:\tlearn: 2.1318118\ttotal: 470ms\tremaining: 13s\n35:\tlearn: 2.0585793\ttotal: 482ms\tremaining: 12.9s\n36:\tlearn: 1.9861364\ttotal: 495ms\tremaining: 12.9s\n37:\tlearn: 1.9238317\ttotal: 507ms\tremaining: 12.8s\n38:\tlearn: 1.8663158\ttotal: 519ms\tremaining: 12.8s\n39:\tlearn: 1.8128752\ttotal: 531ms\tremaining: 12.7s\n40:\tlearn: 1.7631007\ttotal: 543ms\tremaining: 12.7s\n41:\tlearn: 1.7167490\ttotal: 555ms\tremaining: 12.7s\n42:\tlearn: 1.6741240\ttotal: 567ms\tremaining: 12.6s\n43:\tlearn: 1.6379014\ttotal: 578ms\tremaining: 12.6s\n44:\tlearn: 1.6028203\ttotal: 589ms\tremaining: 12.5s\n45:\tlearn: 1.5699609\ttotal: 600ms\tremaining: 12.4s\n46:\tlearn: 1.5404845\ttotal: 613ms\tremaining: 12.4s\n47:\tlearn: 1.5142145\ttotal: 624ms\tremaining: 12.4s\n48:\tlearn: 1.4913434\ttotal: 635ms\tremaining: 12.3s\n49:\tlearn: 1.4700693\ttotal: 646ms\tremaining: 12.3s\n50:\tlearn: 1.4486574\ttotal: 657ms\tremaining: 12.2s\n51:\tlearn: 1.4285133\ttotal: 668ms\tremaining: 12.2s\n52:\tlearn: 1.4100410\ttotal: 680ms\tremaining: 12.1s\n53:\tlearn: 1.3952327\ttotal: 691ms\tremaining: 12.1s\n54:\tlearn: 1.3823822\ttotal: 702ms\tremaining: 12.1s\n55:\tlearn: 1.3686693\ttotal: 713ms\tremaining: 12s\n56:\tlearn: 1.3560258\ttotal: 724ms\tremaining: 12s\n57:\tlearn: 1.3437567\ttotal: 736ms\tremaining: 12s\n58:\tlearn: 1.3325765\ttotal: 748ms\tremaining: 11.9s\n59:\tlearn: 1.3231086\ttotal: 759ms\tremaining: 11.9s\n60:\tlearn: 1.3121091\ttotal: 770ms\tremaining: 11.8s\n61:\tlearn: 1.3035410\ttotal: 782ms\tremaining: 11.8s\n62:\tlearn: 1.2966865\ttotal: 793ms\tremaining: 11.8s\n63:\tlearn: 1.2908799\ttotal: 804ms\tremaining: 11.8s\n64:\tlearn: 1.2859169\ttotal: 818ms\tremaining: 11.8s\n65:\tlearn: 1.2781444\ttotal: 830ms\tremaining: 11.7s\n66:\tlearn: 1.2744497\ttotal: 841ms\tremaining: 11.7s\n67:\tlearn: 1.2686555\ttotal: 853ms\tremaining: 11.7s\n68:\tlearn: 1.2624289\ttotal: 864ms\tremaining: 11.7s\n69:\tlearn: 1.2585417\ttotal: 876ms\tremaining: 11.6s\n70:\tlearn: 1.2554521\ttotal: 887ms\tremaining: 11.6s\n71:\tlearn: 1.2523136\ttotal: 898ms\tremaining: 11.6s\n72:\tlearn: 1.2462618\ttotal: 910ms\tremaining: 11.6s\n73:\tlearn: 1.2430934\ttotal: 921ms\tremaining: 11.5s\n74:\tlearn: 1.2410286\ttotal: 936ms\tremaining: 11.5s\n75:\tlearn: 1.2383561\ttotal: 946ms\tremaining: 11.5s\n76:\tlearn: 1.2358546\ttotal: 958ms\tremaining: 11.5s\n77:\tlearn: 1.2334686\ttotal: 969ms\tremaining: 11.4s\n78:\tlearn: 1.2299735\ttotal: 980ms\tremaining: 11.4s\n79:\tlearn: 1.2280417\ttotal: 991ms\tremaining: 11.4s\n80:\tlearn: 1.2247734\ttotal: 1s\tremaining: 11.4s\n81:\tlearn: 1.2206328\ttotal: 1.02s\tremaining: 11.4s\n82:\tlearn: 1.2176193\ttotal: 1.03s\tremaining: 11.4s\n83:\tlearn: 1.2142018\ttotal: 1.04s\tremaining: 11.4s\n84:\tlearn: 1.2117678\ttotal: 1.05s\tremaining: 11.3s\n85:\tlearn: 1.2099486\ttotal: 1.06s\tremaining: 11.3s\n86:\tlearn: 1.2082473\ttotal: 1.07s\tremaining: 11.3s\n87:\tlearn: 1.2056410\ttotal: 1.09s\tremaining: 11.3s\n88:\tlearn: 1.2045319\ttotal: 1.1s\tremaining: 11.2s\n89:\tlearn: 1.2030874\ttotal: 1.11s\tremaining: 11.2s\n90:\tlearn: 1.2011277\ttotal: 1.12s\tremaining: 11.2s\n91:\tlearn: 1.1988005\ttotal: 1.13s\tremaining: 11.2s\n92:\tlearn: 1.1978519\ttotal: 1.14s\tremaining: 11.1s\n93:\tlearn: 1.1969718\ttotal: 1.15s\tremaining: 11.1s\n94:\tlearn: 1.1946210\ttotal: 1.17s\tremaining: 11.1s\n95:\tlearn: 1.1923549\ttotal: 1.18s\tremaining: 11.1s\n96:\tlearn: 1.1913282\ttotal: 1.19s\tremaining: 11.1s\n97:\tlearn: 1.1891233\ttotal: 1.2s\tremaining: 11.1s\n98:\tlearn: 1.1884199\ttotal: 1.22s\tremaining: 11.1s\n99:\tlearn: 1.1866545\ttotal: 1.23s\tremaining: 11.1s\n100:\tlearn: 1.1860107\ttotal: 1.24s\tremaining: 11s\n101:\tlearn: 1.1858050\ttotal: 1.25s\tremaining: 11s\n102:\tlearn: 1.1847982\ttotal: 1.26s\tremaining: 11s\n103:\tlearn: 1.1836871\ttotal: 1.27s\tremaining: 11s\n104:\tlearn: 1.1827293\ttotal: 1.28s\tremaining: 10.9s\n105:\tlearn: 1.1816187\ttotal: 1.29s\tremaining: 10.9s\n106:\tlearn: 1.1808337\ttotal: 1.3s\tremaining: 10.9s\n107:\tlearn: 1.1781473\ttotal: 1.31s\tremaining: 10.9s\n108:\tlearn: 1.1764229\ttotal: 1.32s\tremaining: 10.8s\n109:\tlearn: 1.1758946\ttotal: 1.33s\tremaining: 10.8s\n110:\tlearn: 1.1746969\ttotal: 1.35s\tremaining: 10.8s\n111:\tlearn: 1.1736120\ttotal: 1.36s\tremaining: 10.8s\n112:\tlearn: 1.1731056\ttotal: 1.37s\tremaining: 10.7s\n113:\tlearn: 1.1704616\ttotal: 1.38s\tremaining: 10.7s\n114:\tlearn: 1.1687598\ttotal: 1.39s\tremaining: 10.7s\n115:\tlearn: 1.1677977\ttotal: 1.4s\tremaining: 10.7s\n116:\tlearn: 1.1670576\ttotal: 1.41s\tremaining: 10.7s\n117:\tlearn: 1.1665572\ttotal: 1.42s\tremaining: 10.6s\n118:\tlearn: 1.1653041\ttotal: 1.44s\tremaining: 10.6s\n119:\tlearn: 1.1649982\ttotal: 1.45s\tremaining: 10.6s\n120:\tlearn: 1.1634437\ttotal: 1.46s\tremaining: 10.6s\n121:\tlearn: 1.1623582\ttotal: 1.47s\tremaining: 10.6s\n122:\tlearn: 1.1612513\ttotal: 1.48s\tremaining: 10.6s\n123:\tlearn: 1.1603726\ttotal: 1.49s\tremaining: 10.5s\n124:\tlearn: 1.1586698\ttotal: 1.5s\tremaining: 10.5s\n125:\tlearn: 1.1578730\ttotal: 1.51s\tremaining: 10.5s\n126:\tlearn: 1.1570946\ttotal: 1.53s\tremaining: 10.5s\n127:\tlearn: 1.1565949\ttotal: 1.54s\tremaining: 10.5s\n128:\tlearn: 1.1553950\ttotal: 1.55s\tremaining: 10.5s\n129:\tlearn: 1.1536431\ttotal: 1.56s\tremaining: 10.4s\n130:\tlearn: 1.1529384\ttotal: 1.57s\tremaining: 10.4s\n131:\tlearn: 1.1518843\ttotal: 1.58s\tremaining: 10.4s\n132:\tlearn: 1.1511416\ttotal: 1.59s\tremaining: 10.4s\n133:\tlearn: 1.1497556\ttotal: 1.6s\tremaining: 10.4s\n134:\tlearn: 1.1479811\ttotal: 1.62s\tremaining: 10.4s\n135:\tlearn: 1.1473336\ttotal: 1.63s\tremaining: 10.3s\n136:\tlearn: 1.1466108\ttotal: 1.64s\tremaining: 10.3s\n137:\tlearn: 1.1449661\ttotal: 1.65s\tremaining: 10.3s\n138:\tlearn: 1.1443770\ttotal: 1.66s\tremaining: 10.3s\n139:\tlearn: 1.1440939\ttotal: 1.67s\tremaining: 10.3s\n140:\tlearn: 1.1423171\ttotal: 1.68s\tremaining: 10.2s\n141:\tlearn: 1.1413506\ttotal: 1.69s\tremaining: 10.2s\n142:\tlearn: 1.1404292\ttotal: 1.7s\tremaining: 10.2s\n143:\tlearn: 1.1397842\ttotal: 1.71s\tremaining: 10.2s\n144:\tlearn: 1.1389657\ttotal: 1.72s\tremaining: 10.2s\n145:\tlearn: 1.1380827\ttotal: 1.73s\tremaining: 10.1s\n146:\tlearn: 1.1369529\ttotal: 1.75s\tremaining: 10.1s\n147:\tlearn: 1.1357678\ttotal: 1.75s\tremaining: 10.1s\n148:\tlearn: 1.1345032\ttotal: 1.77s\tremaining: 10.1s\n149:\tlearn: 1.1338519\ttotal: 1.78s\tremaining: 10.1s\n150:\tlearn: 1.1324246\ttotal: 1.79s\tremaining: 10.1s\n151:\tlearn: 1.1321444\ttotal: 1.8s\tremaining: 10s\n152:\tlearn: 1.1296297\ttotal: 1.81s\tremaining: 10s\n153:\tlearn: 1.1284433\ttotal: 1.82s\tremaining: 10s\n154:\tlearn: 1.1280734\ttotal: 1.83s\tremaining: 10s\n155:\tlearn: 1.1272821\ttotal: 1.85s\tremaining: 9.99s\n156:\tlearn: 1.1264237\ttotal: 1.86s\tremaining: 9.98s\n157:\tlearn: 1.1247733\ttotal: 1.87s\tremaining: 9.97s\n158:\tlearn: 1.1243783\ttotal: 1.88s\tremaining: 9.95s\n159:\tlearn: 1.1234608\ttotal: 1.89s\tremaining: 9.93s\n160:\tlearn: 1.1208047\ttotal: 1.9s\tremaining: 9.92s\n161:\tlearn: 1.1197254\ttotal: 1.92s\tremaining: 9.93s\n162:\tlearn: 1.1189163\ttotal: 1.93s\tremaining: 9.92s\n163:\tlearn: 1.1180708\ttotal: 1.94s\tremaining: 9.9s\n164:\tlearn: 1.1175785\ttotal: 1.96s\tremaining: 9.91s\n165:\tlearn: 1.1161012\ttotal: 1.97s\tremaining: 9.89s\n166:\tlearn: 1.1149861\ttotal: 1.98s\tremaining: 9.88s\n167:\tlearn: 1.1136603\ttotal: 1.99s\tremaining: 9.87s\n168:\tlearn: 1.1127995\ttotal: 2s\tremaining: 9.86s\n169:\tlearn: 1.1116973\ttotal: 2.02s\tremaining: 9.84s\n","name":"stdout"},{"output_type":"stream","text":"170:\tlearn: 1.1107217\ttotal: 2.03s\tremaining: 9.85s\n171:\tlearn: 1.1082000\ttotal: 2.04s\tremaining: 9.84s\n172:\tlearn: 1.1075855\ttotal: 2.06s\tremaining: 9.82s\n173:\tlearn: 1.1063364\ttotal: 2.07s\tremaining: 9.81s\n174:\tlearn: 1.1062466\ttotal: 2.08s\tremaining: 9.79s\n175:\tlearn: 1.1053781\ttotal: 2.09s\tremaining: 9.78s\n176:\tlearn: 1.1040479\ttotal: 2.1s\tremaining: 9.77s\n177:\tlearn: 1.1031024\ttotal: 2.11s\tremaining: 9.75s\n178:\tlearn: 1.1027451\ttotal: 2.12s\tremaining: 9.73s\n179:\tlearn: 1.1019338\ttotal: 2.13s\tremaining: 9.72s\n180:\tlearn: 1.1009847\ttotal: 2.15s\tremaining: 9.71s\n181:\tlearn: 1.1000177\ttotal: 2.15s\tremaining: 9.69s\n182:\tlearn: 1.0994211\ttotal: 2.17s\tremaining: 9.67s\n183:\tlearn: 1.0966113\ttotal: 2.18s\tremaining: 9.66s\n184:\tlearn: 1.0962386\ttotal: 2.19s\tremaining: 9.64s\n185:\tlearn: 1.0958319\ttotal: 2.2s\tremaining: 9.62s\n186:\tlearn: 1.0956440\ttotal: 2.21s\tremaining: 9.6s\n187:\tlearn: 1.0952132\ttotal: 2.22s\tremaining: 9.58s\n188:\tlearn: 1.0950298\ttotal: 2.23s\tremaining: 9.57s\n189:\tlearn: 1.0943931\ttotal: 2.24s\tremaining: 9.56s\n190:\tlearn: 1.0938962\ttotal: 2.25s\tremaining: 9.54s\n191:\tlearn: 1.0932181\ttotal: 2.27s\tremaining: 9.53s\n192:\tlearn: 1.0923642\ttotal: 2.28s\tremaining: 9.52s\n193:\tlearn: 1.0906348\ttotal: 2.29s\tremaining: 9.51s\n194:\tlearn: 1.0896060\ttotal: 2.3s\tremaining: 9.49s\n195:\tlearn: 1.0884444\ttotal: 2.31s\tremaining: 9.48s\n196:\tlearn: 1.0882564\ttotal: 2.32s\tremaining: 9.46s\n197:\tlearn: 1.0875215\ttotal: 2.33s\tremaining: 9.45s\n198:\tlearn: 1.0871949\ttotal: 2.34s\tremaining: 9.44s\n199:\tlearn: 1.0849998\ttotal: 2.35s\tremaining: 9.42s\n200:\tlearn: 1.0836356\ttotal: 2.37s\tremaining: 9.41s\n201:\tlearn: 1.0832872\ttotal: 2.38s\tremaining: 9.39s\n202:\tlearn: 1.0823693\ttotal: 2.39s\tremaining: 9.38s\n203:\tlearn: 1.0811753\ttotal: 2.4s\tremaining: 9.37s\n204:\tlearn: 1.0807504\ttotal: 2.41s\tremaining: 9.35s\n205:\tlearn: 1.0802720\ttotal: 2.42s\tremaining: 9.34s\n206:\tlearn: 1.0800745\ttotal: 2.44s\tremaining: 9.34s\n207:\tlearn: 1.0798863\ttotal: 2.45s\tremaining: 9.32s\n208:\tlearn: 1.0796546\ttotal: 2.46s\tremaining: 9.3s\n209:\tlearn: 1.0787767\ttotal: 2.47s\tremaining: 9.29s\n210:\tlearn: 1.0776763\ttotal: 2.48s\tremaining: 9.28s\n211:\tlearn: 1.0771123\ttotal: 2.49s\tremaining: 9.26s\n212:\tlearn: 1.0765595\ttotal: 2.5s\tremaining: 9.24s\n213:\tlearn: 1.0756004\ttotal: 2.51s\tremaining: 9.23s\n214:\tlearn: 1.0744525\ttotal: 2.52s\tremaining: 9.21s\n215:\tlearn: 1.0739055\ttotal: 2.53s\tremaining: 9.2s\n216:\tlearn: 1.0723581\ttotal: 2.55s\tremaining: 9.19s\n217:\tlearn: 1.0705798\ttotal: 2.56s\tremaining: 9.18s\n218:\tlearn: 1.0703065\ttotal: 2.57s\tremaining: 9.16s\n219:\tlearn: 1.0698700\ttotal: 2.58s\tremaining: 9.14s\n220:\tlearn: 1.0692114\ttotal: 2.59s\tremaining: 9.13s\n221:\tlearn: 1.0684515\ttotal: 2.6s\tremaining: 9.11s\n222:\tlearn: 1.0681664\ttotal: 2.61s\tremaining: 9.1s\n223:\tlearn: 1.0672688\ttotal: 2.62s\tremaining: 9.08s\n224:\tlearn: 1.0655127\ttotal: 2.63s\tremaining: 9.08s\n225:\tlearn: 1.0648698\ttotal: 2.65s\tremaining: 9.06s\n226:\tlearn: 1.0644529\ttotal: 2.66s\tremaining: 9.05s\n227:\tlearn: 1.0637845\ttotal: 2.67s\tremaining: 9.04s\n228:\tlearn: 1.0624456\ttotal: 2.68s\tremaining: 9.02s\n229:\tlearn: 1.0617762\ttotal: 2.69s\tremaining: 9.01s\n230:\tlearn: 1.0613287\ttotal: 2.7s\tremaining: 9s\n231:\tlearn: 1.0597929\ttotal: 2.71s\tremaining: 8.98s\n232:\tlearn: 1.0583966\ttotal: 2.73s\tremaining: 8.97s\n233:\tlearn: 1.0567244\ttotal: 2.74s\tremaining: 8.96s\n234:\tlearn: 1.0561257\ttotal: 2.75s\tremaining: 8.95s\n235:\tlearn: 1.0551377\ttotal: 2.76s\tremaining: 8.94s\n236:\tlearn: 1.0539986\ttotal: 2.77s\tremaining: 8.93s\n237:\tlearn: 1.0533721\ttotal: 2.78s\tremaining: 8.91s\n238:\tlearn: 1.0525861\ttotal: 2.79s\tremaining: 8.9s\n239:\tlearn: 1.0519478\ttotal: 2.81s\tremaining: 8.89s\n240:\tlearn: 1.0512650\ttotal: 2.82s\tremaining: 8.87s\n241:\tlearn: 1.0507969\ttotal: 2.83s\tremaining: 8.85s\n242:\tlearn: 1.0504679\ttotal: 2.84s\tremaining: 8.85s\n243:\tlearn: 1.0486201\ttotal: 2.85s\tremaining: 8.84s\n244:\tlearn: 1.0484421\ttotal: 2.87s\tremaining: 8.84s\n245:\tlearn: 1.0482082\ttotal: 2.88s\tremaining: 8.84s\n246:\tlearn: 1.0480497\ttotal: 2.89s\tremaining: 8.82s\n247:\tlearn: 1.0471929\ttotal: 2.9s\tremaining: 8.81s\n248:\tlearn: 1.0453932\ttotal: 2.92s\tremaining: 8.8s\n249:\tlearn: 1.0450059\ttotal: 2.93s\tremaining: 8.79s\n250:\tlearn: 1.0439041\ttotal: 2.94s\tremaining: 8.78s\n251:\tlearn: 1.0430776\ttotal: 2.95s\tremaining: 8.77s\n252:\tlearn: 1.0420750\ttotal: 2.96s\tremaining: 8.76s\n253:\tlearn: 1.0418324\ttotal: 2.98s\tremaining: 8.74s\n254:\tlearn: 1.0409308\ttotal: 2.99s\tremaining: 8.73s\n255:\tlearn: 1.0404948\ttotal: 3s\tremaining: 8.72s\n256:\tlearn: 1.0399404\ttotal: 3.01s\tremaining: 8.71s\n257:\tlearn: 1.0391921\ttotal: 3.02s\tremaining: 8.69s\n258:\tlearn: 1.0376162\ttotal: 3.03s\tremaining: 8.68s\n259:\tlearn: 1.0370815\ttotal: 3.05s\tremaining: 8.67s\n260:\tlearn: 1.0358601\ttotal: 3.06s\tremaining: 8.66s\n261:\tlearn: 1.0355304\ttotal: 3.07s\tremaining: 8.65s\n262:\tlearn: 1.0344242\ttotal: 3.08s\tremaining: 8.64s\n263:\tlearn: 1.0339766\ttotal: 3.09s\tremaining: 8.62s\n264:\tlearn: 1.0328848\ttotal: 3.1s\tremaining: 8.61s\n265:\tlearn: 1.0311323\ttotal: 3.12s\tremaining: 8.6s\n266:\tlearn: 1.0303145\ttotal: 3.13s\tremaining: 8.59s\n267:\tlearn: 1.0293499\ttotal: 3.14s\tremaining: 8.57s\n268:\tlearn: 1.0285859\ttotal: 3.15s\tremaining: 8.56s\n269:\tlearn: 1.0282023\ttotal: 3.16s\tremaining: 8.55s\n270:\tlearn: 1.0270103\ttotal: 3.17s\tremaining: 8.54s\n271:\tlearn: 1.0251942\ttotal: 3.19s\tremaining: 8.53s\n272:\tlearn: 1.0247516\ttotal: 3.2s\tremaining: 8.52s\n273:\tlearn: 1.0245885\ttotal: 3.21s\tremaining: 8.5s\n274:\tlearn: 1.0239300\ttotal: 3.22s\tremaining: 8.49s\n275:\tlearn: 1.0225776\ttotal: 3.23s\tremaining: 8.48s\n276:\tlearn: 1.0212318\ttotal: 3.25s\tremaining: 8.47s\n277:\tlearn: 1.0208044\ttotal: 3.26s\tremaining: 8.46s\n278:\tlearn: 1.0206468\ttotal: 3.27s\tremaining: 8.44s\n279:\tlearn: 1.0196878\ttotal: 3.28s\tremaining: 8.43s\n280:\tlearn: 1.0189307\ttotal: 3.29s\tremaining: 8.42s\n281:\tlearn: 1.0181517\ttotal: 3.3s\tremaining: 8.4s\n282:\tlearn: 1.0175179\ttotal: 3.31s\tremaining: 8.39s\n283:\tlearn: 1.0168695\ttotal: 3.32s\tremaining: 8.38s\n284:\tlearn: 1.0162033\ttotal: 3.33s\tremaining: 8.36s\n285:\tlearn: 1.0154469\ttotal: 3.34s\tremaining: 8.35s\n286:\tlearn: 1.0138193\ttotal: 3.35s\tremaining: 8.34s\n287:\tlearn: 1.0127651\ttotal: 3.37s\tremaining: 8.32s\n288:\tlearn: 1.0115783\ttotal: 3.38s\tremaining: 8.31s\n289:\tlearn: 1.0111776\ttotal: 3.39s\tremaining: 8.3s\n290:\tlearn: 1.0098390\ttotal: 3.4s\tremaining: 8.29s\n291:\tlearn: 1.0085433\ttotal: 3.41s\tremaining: 8.27s\n292:\tlearn: 1.0075411\ttotal: 3.42s\tremaining: 8.26s\n293:\tlearn: 1.0062776\ttotal: 3.44s\tremaining: 8.25s\n294:\tlearn: 1.0062708\ttotal: 3.45s\tremaining: 8.24s\n295:\tlearn: 1.0058825\ttotal: 3.46s\tremaining: 8.23s\n296:\tlearn: 1.0054872\ttotal: 3.47s\tremaining: 8.22s\n297:\tlearn: 1.0049487\ttotal: 3.48s\tremaining: 8.21s\n298:\tlearn: 1.0045046\ttotal: 3.49s\tremaining: 8.19s\n299:\tlearn: 1.0040192\ttotal: 3.5s\tremaining: 8.18s\n300:\tlearn: 1.0025883\ttotal: 3.52s\tremaining: 8.16s\n301:\tlearn: 1.0022493\ttotal: 3.53s\tremaining: 8.15s\n302:\tlearn: 1.0017734\ttotal: 3.54s\tremaining: 8.14s\n303:\tlearn: 1.0012528\ttotal: 3.55s\tremaining: 8.12s\n304:\tlearn: 0.9998944\ttotal: 3.56s\tremaining: 8.11s\n305:\tlearn: 0.9987370\ttotal: 3.57s\tremaining: 8.1s\n306:\tlearn: 0.9982709\ttotal: 3.58s\tremaining: 8.08s\n307:\tlearn: 0.9980820\ttotal: 3.59s\tremaining: 8.07s\n308:\tlearn: 0.9978238\ttotal: 3.6s\tremaining: 8.06s\n309:\tlearn: 0.9971905\ttotal: 3.61s\tremaining: 8.04s\n310:\tlearn: 0.9969259\ttotal: 3.62s\tremaining: 8.03s\n311:\tlearn: 0.9963703\ttotal: 3.63s\tremaining: 8.02s\n312:\tlearn: 0.9960371\ttotal: 3.65s\tremaining: 8.01s\n313:\tlearn: 0.9954107\ttotal: 3.66s\tremaining: 8s\n314:\tlearn: 0.9946010\ttotal: 3.67s\tremaining: 7.99s\n315:\tlearn: 0.9932410\ttotal: 3.69s\tremaining: 7.98s\n316:\tlearn: 0.9929927\ttotal: 3.7s\tremaining: 7.96s\n317:\tlearn: 0.9919411\ttotal: 3.71s\tremaining: 7.95s\n318:\tlearn: 0.9905234\ttotal: 3.72s\tremaining: 7.94s\n319:\tlearn: 0.9902741\ttotal: 3.73s\tremaining: 7.92s\n320:\tlearn: 0.9898599\ttotal: 3.74s\tremaining: 7.91s\n321:\tlearn: 0.9896760\ttotal: 3.75s\tremaining: 7.89s\n322:\tlearn: 0.9895540\ttotal: 3.76s\tremaining: 7.88s\n323:\tlearn: 0.9886779\ttotal: 3.77s\tremaining: 7.87s\n324:\tlearn: 0.9885518\ttotal: 3.78s\tremaining: 7.85s\n325:\tlearn: 0.9884910\ttotal: 3.79s\tremaining: 7.84s\n326:\tlearn: 0.9877186\ttotal: 3.8s\tremaining: 7.83s\n327:\tlearn: 0.9871546\ttotal: 3.81s\tremaining: 7.82s\n328:\tlearn: 0.9863486\ttotal: 3.83s\tremaining: 7.8s\n329:\tlearn: 0.9854194\ttotal: 3.84s\tremaining: 7.79s\n","name":"stdout"},{"output_type":"stream","text":"330:\tlearn: 0.9838386\ttotal: 3.85s\tremaining: 7.79s\n331:\tlearn: 0.9836050\ttotal: 3.87s\tremaining: 7.78s\n332:\tlearn: 0.9822715\ttotal: 3.88s\tremaining: 7.76s\n333:\tlearn: 0.9821781\ttotal: 3.89s\tremaining: 7.75s\n334:\tlearn: 0.9811847\ttotal: 3.9s\tremaining: 7.74s\n335:\tlearn: 0.9803892\ttotal: 3.91s\tremaining: 7.73s\n336:\tlearn: 0.9793239\ttotal: 3.92s\tremaining: 7.71s\n337:\tlearn: 0.9790115\ttotal: 3.93s\tremaining: 7.7s\n338:\tlearn: 0.9779425\ttotal: 3.94s\tremaining: 7.69s\n339:\tlearn: 0.9772667\ttotal: 3.96s\tremaining: 7.68s\n340:\tlearn: 0.9768381\ttotal: 3.97s\tremaining: 7.67s\n341:\tlearn: 0.9760866\ttotal: 3.98s\tremaining: 7.65s\n342:\tlearn: 0.9753236\ttotal: 3.99s\tremaining: 7.64s\n343:\tlearn: 0.9742718\ttotal: 4s\tremaining: 7.63s\n344:\tlearn: 0.9740565\ttotal: 4.01s\tremaining: 7.62s\n345:\tlearn: 0.9734099\ttotal: 4.02s\tremaining: 7.6s\n346:\tlearn: 0.9721651\ttotal: 4.03s\tremaining: 7.59s\n347:\tlearn: 0.9715697\ttotal: 4.05s\tremaining: 7.58s\n348:\tlearn: 0.9710143\ttotal: 4.06s\tremaining: 7.57s\n349:\tlearn: 0.9700210\ttotal: 4.07s\tremaining: 7.56s\n350:\tlearn: 0.9691501\ttotal: 4.08s\tremaining: 7.55s\n351:\tlearn: 0.9685369\ttotal: 4.09s\tremaining: 7.54s\n352:\tlearn: 0.9674836\ttotal: 4.11s\tremaining: 7.52s\n353:\tlearn: 0.9671011\ttotal: 4.12s\tremaining: 7.51s\n354:\tlearn: 0.9664951\ttotal: 4.13s\tremaining: 7.5s\n355:\tlearn: 0.9659650\ttotal: 4.14s\tremaining: 7.49s\n356:\tlearn: 0.9654189\ttotal: 4.15s\tremaining: 7.47s\n357:\tlearn: 0.9649105\ttotal: 4.16s\tremaining: 7.46s\n358:\tlearn: 0.9644293\ttotal: 4.17s\tremaining: 7.45s\n359:\tlearn: 0.9634185\ttotal: 4.18s\tremaining: 7.44s\n360:\tlearn: 0.9625450\ttotal: 4.19s\tremaining: 7.42s\n361:\tlearn: 0.9618149\ttotal: 4.21s\tremaining: 7.41s\n362:\tlearn: 0.9610288\ttotal: 4.22s\tremaining: 7.4s\n363:\tlearn: 0.9589717\ttotal: 4.23s\tremaining: 7.39s\n364:\tlearn: 0.9581939\ttotal: 4.25s\tremaining: 7.38s\n365:\tlearn: 0.9573646\ttotal: 4.26s\tremaining: 7.37s\n366:\tlearn: 0.9561825\ttotal: 4.27s\tremaining: 7.36s\n367:\tlearn: 0.9554185\ttotal: 4.28s\tremaining: 7.35s\n368:\tlearn: 0.9547960\ttotal: 4.29s\tremaining: 7.33s\n369:\tlearn: 0.9543354\ttotal: 4.3s\tremaining: 7.32s\n370:\tlearn: 0.9538830\ttotal: 4.31s\tremaining: 7.31s\n371:\tlearn: 0.9534744\ttotal: 4.32s\tremaining: 7.3s\n372:\tlearn: 0.9516556\ttotal: 4.33s\tremaining: 7.29s\n373:\tlearn: 0.9510005\ttotal: 4.35s\tremaining: 7.27s\n374:\tlearn: 0.9507208\ttotal: 4.36s\tremaining: 7.26s\n375:\tlearn: 0.9491354\ttotal: 4.37s\tremaining: 7.25s\n376:\tlearn: 0.9485693\ttotal: 4.38s\tremaining: 7.24s\n377:\tlearn: 0.9481079\ttotal: 4.39s\tremaining: 7.23s\n378:\tlearn: 0.9466234\ttotal: 4.4s\tremaining: 7.21s\n379:\tlearn: 0.9457018\ttotal: 4.42s\tremaining: 7.2s\n380:\tlearn: 0.9451166\ttotal: 4.43s\tremaining: 7.19s\n381:\tlearn: 0.9445242\ttotal: 4.44s\tremaining: 7.18s\n382:\tlearn: 0.9440744\ttotal: 4.45s\tremaining: 7.17s\n383:\tlearn: 0.9434677\ttotal: 4.46s\tremaining: 7.16s\n384:\tlearn: 0.9432049\ttotal: 4.47s\tremaining: 7.15s\n385:\tlearn: 0.9426823\ttotal: 4.49s\tremaining: 7.13s\n386:\tlearn: 0.9425481\ttotal: 4.5s\tremaining: 7.12s\n387:\tlearn: 0.9422660\ttotal: 4.51s\tremaining: 7.11s\n388:\tlearn: 0.9417744\ttotal: 4.52s\tremaining: 7.1s\n389:\tlearn: 0.9408770\ttotal: 4.53s\tremaining: 7.08s\n390:\tlearn: 0.9404682\ttotal: 4.54s\tremaining: 7.07s\n391:\tlearn: 0.9397098\ttotal: 4.55s\tremaining: 7.06s\n392:\tlearn: 0.9392485\ttotal: 4.57s\tremaining: 7.05s\n393:\tlearn: 0.9390581\ttotal: 4.58s\tremaining: 7.04s\n394:\tlearn: 0.9385758\ttotal: 4.59s\tremaining: 7.03s\n395:\tlearn: 0.9379786\ttotal: 4.6s\tremaining: 7.01s\n396:\tlearn: 0.9376435\ttotal: 4.61s\tremaining: 7s\n397:\tlearn: 0.9373774\ttotal: 4.62s\tremaining: 6.99s\n398:\tlearn: 0.9365815\ttotal: 4.63s\tremaining: 6.97s\n399:\tlearn: 0.9360420\ttotal: 4.64s\tremaining: 6.96s\n400:\tlearn: 0.9354377\ttotal: 4.66s\tremaining: 6.96s\n401:\tlearn: 0.9347899\ttotal: 4.67s\tremaining: 6.94s\n402:\tlearn: 0.9341440\ttotal: 4.68s\tremaining: 6.93s\n403:\tlearn: 0.9336100\ttotal: 4.69s\tremaining: 6.92s\n404:\tlearn: 0.9331156\ttotal: 4.7s\tremaining: 6.91s\n405:\tlearn: 0.9328979\ttotal: 4.71s\tremaining: 6.9s\n406:\tlearn: 0.9317052\ttotal: 4.73s\tremaining: 6.89s\n407:\tlearn: 0.9312808\ttotal: 4.74s\tremaining: 6.88s\n408:\tlearn: 0.9307296\ttotal: 4.75s\tremaining: 6.86s\n409:\tlearn: 0.9300243\ttotal: 4.76s\tremaining: 6.85s\n410:\tlearn: 0.9295051\ttotal: 4.77s\tremaining: 6.84s\n411:\tlearn: 0.9292883\ttotal: 4.78s\tremaining: 6.83s\n412:\tlearn: 0.9286779\ttotal: 4.79s\tremaining: 6.81s\n413:\tlearn: 0.9283086\ttotal: 4.8s\tremaining: 6.8s\n414:\tlearn: 0.9281738\ttotal: 4.81s\tremaining: 6.79s\n415:\tlearn: 0.9276333\ttotal: 4.82s\tremaining: 6.77s\n416:\tlearn: 0.9271778\ttotal: 4.84s\tremaining: 6.76s\n417:\tlearn: 0.9268314\ttotal: 4.85s\tremaining: 6.75s\n418:\tlearn: 0.9261134\ttotal: 4.86s\tremaining: 6.75s\n419:\tlearn: 0.9257043\ttotal: 4.88s\tremaining: 6.73s\n420:\tlearn: 0.9255682\ttotal: 4.88s\tremaining: 6.72s\n421:\tlearn: 0.9244377\ttotal: 4.9s\tremaining: 6.71s\n422:\tlearn: 0.9243915\ttotal: 4.91s\tremaining: 6.69s\n423:\tlearn: 0.9243085\ttotal: 4.92s\tremaining: 6.68s\n424:\tlearn: 0.9234957\ttotal: 4.93s\tremaining: 6.67s\n425:\tlearn: 0.9228722\ttotal: 4.94s\tremaining: 6.66s\n426:\tlearn: 0.9222633\ttotal: 4.95s\tremaining: 6.64s\n427:\tlearn: 0.9218128\ttotal: 4.96s\tremaining: 6.63s\n428:\tlearn: 0.9212786\ttotal: 4.97s\tremaining: 6.62s\n429:\tlearn: 0.9202235\ttotal: 4.99s\tremaining: 6.61s\n430:\tlearn: 0.9197569\ttotal: 5s\tremaining: 6.6s\n431:\tlearn: 0.9185710\ttotal: 5.01s\tremaining: 6.59s\n432:\tlearn: 0.9173433\ttotal: 5.02s\tremaining: 6.58s\n433:\tlearn: 0.9163625\ttotal: 5.03s\tremaining: 6.56s\n434:\tlearn: 0.9157758\ttotal: 5.04s\tremaining: 6.55s\n435:\tlearn: 0.9154510\ttotal: 5.06s\tremaining: 6.54s\n436:\tlearn: 0.9147253\ttotal: 5.07s\tremaining: 6.53s\n437:\tlearn: 0.9138538\ttotal: 5.08s\tremaining: 6.52s\n438:\tlearn: 0.9129457\ttotal: 5.09s\tremaining: 6.51s\n439:\tlearn: 0.9127229\ttotal: 5.11s\tremaining: 6.5s\n440:\tlearn: 0.9119041\ttotal: 5.12s\tremaining: 6.49s\n441:\tlearn: 0.9103671\ttotal: 5.13s\tremaining: 6.47s\n442:\tlearn: 0.9098934\ttotal: 5.14s\tremaining: 6.46s\n443:\tlearn: 0.9097073\ttotal: 5.15s\tremaining: 6.45s\n444:\tlearn: 0.9085568\ttotal: 5.16s\tremaining: 6.44s\n445:\tlearn: 0.9080848\ttotal: 5.17s\tremaining: 6.42s\n446:\tlearn: 0.9076752\ttotal: 5.18s\tremaining: 6.41s\n447:\tlearn: 0.9073887\ttotal: 5.19s\tremaining: 6.4s\n448:\tlearn: 0.9061904\ttotal: 5.21s\tremaining: 6.39s\n449:\tlearn: 0.9038230\ttotal: 5.22s\tremaining: 6.38s\n450:\tlearn: 0.9035479\ttotal: 5.23s\tremaining: 6.37s\n451:\tlearn: 0.9024067\ttotal: 5.24s\tremaining: 6.36s\n452:\tlearn: 0.9022370\ttotal: 5.25s\tremaining: 6.34s\n453:\tlearn: 0.9014948\ttotal: 5.26s\tremaining: 6.33s\n454:\tlearn: 0.9008367\ttotal: 5.28s\tremaining: 6.32s\n455:\tlearn: 0.8996431\ttotal: 5.29s\tremaining: 6.31s\n456:\tlearn: 0.8991660\ttotal: 5.3s\tremaining: 6.3s\n457:\tlearn: 0.8989034\ttotal: 5.31s\tremaining: 6.28s\n458:\tlearn: 0.8984695\ttotal: 5.32s\tremaining: 6.27s\n459:\tlearn: 0.8975623\ttotal: 5.33s\tremaining: 6.26s\n460:\tlearn: 0.8971615\ttotal: 5.34s\tremaining: 6.25s\n461:\tlearn: 0.8968083\ttotal: 5.35s\tremaining: 6.24s\n462:\tlearn: 0.8965121\ttotal: 5.37s\tremaining: 6.22s\n463:\tlearn: 0.8960211\ttotal: 5.38s\tremaining: 6.21s\n464:\tlearn: 0.8953446\ttotal: 5.39s\tremaining: 6.2s\n465:\tlearn: 0.8948042\ttotal: 5.4s\tremaining: 6.19s\n466:\tlearn: 0.8930905\ttotal: 5.41s\tremaining: 6.18s\n467:\tlearn: 0.8917795\ttotal: 5.43s\tremaining: 6.17s\n468:\tlearn: 0.8916213\ttotal: 5.44s\tremaining: 6.16s\n469:\tlearn: 0.8914363\ttotal: 5.45s\tremaining: 6.14s\n470:\tlearn: 0.8897822\ttotal: 5.46s\tremaining: 6.13s\n471:\tlearn: 0.8893405\ttotal: 5.47s\tremaining: 6.12s\n472:\tlearn: 0.8887694\ttotal: 5.48s\tremaining: 6.11s\n473:\tlearn: 0.8883763\ttotal: 5.5s\tremaining: 6.1s\n474:\tlearn: 0.8879427\ttotal: 5.51s\tremaining: 6.09s\n475:\tlearn: 0.8876619\ttotal: 5.52s\tremaining: 6.07s\n476:\tlearn: 0.8868638\ttotal: 5.53s\tremaining: 6.06s\n477:\tlearn: 0.8866771\ttotal: 5.54s\tremaining: 6.05s\n478:\tlearn: 0.8864041\ttotal: 5.55s\tremaining: 6.03s\n479:\tlearn: 0.8857936\ttotal: 5.56s\tremaining: 6.02s\n480:\tlearn: 0.8852606\ttotal: 5.57s\tremaining: 6.01s\n481:\tlearn: 0.8849728\ttotal: 5.58s\tremaining: 6s\n482:\tlearn: 0.8833955\ttotal: 5.59s\tremaining: 5.99s\n483:\tlearn: 0.8827043\ttotal: 5.6s\tremaining: 5.97s\n484:\tlearn: 0.8817537\ttotal: 5.62s\tremaining: 5.96s\n485:\tlearn: 0.8816654\ttotal: 5.63s\tremaining: 5.95s\n486:\tlearn: 0.8808282\ttotal: 5.64s\tremaining: 5.94s\n487:\tlearn: 0.8796575\ttotal: 5.65s\tremaining: 5.93s\n488:\tlearn: 0.8786333\ttotal: 5.66s\tremaining: 5.91s\n489:\tlearn: 0.8782008\ttotal: 5.67s\tremaining: 5.9s\n","name":"stdout"},{"output_type":"stream","text":"490:\tlearn: 0.8772027\ttotal: 5.68s\tremaining: 5.89s\n491:\tlearn: 0.8763219\ttotal: 5.7s\tremaining: 5.88s\n492:\tlearn: 0.8757806\ttotal: 5.71s\tremaining: 5.87s\n493:\tlearn: 0.8755321\ttotal: 5.72s\tremaining: 5.86s\n494:\tlearn: 0.8754364\ttotal: 5.73s\tremaining: 5.84s\n495:\tlearn: 0.8746247\ttotal: 5.74s\tremaining: 5.83s\n496:\tlearn: 0.8736840\ttotal: 5.75s\tremaining: 5.82s\n497:\tlearn: 0.8729630\ttotal: 5.76s\tremaining: 5.81s\n498:\tlearn: 0.8711711\ttotal: 5.78s\tremaining: 5.8s\n499:\tlearn: 0.8708353\ttotal: 5.79s\tremaining: 5.79s\n500:\tlearn: 0.8701146\ttotal: 5.8s\tremaining: 5.78s\n501:\tlearn: 0.8695938\ttotal: 5.82s\tremaining: 5.77s\n502:\tlearn: 0.8687146\ttotal: 5.83s\tremaining: 5.76s\n503:\tlearn: 0.8684126\ttotal: 5.84s\tremaining: 5.75s\n504:\tlearn: 0.8665877\ttotal: 5.85s\tremaining: 5.74s\n505:\tlearn: 0.8665072\ttotal: 5.86s\tremaining: 5.72s\n506:\tlearn: 0.8661143\ttotal: 5.87s\tremaining: 5.71s\n507:\tlearn: 0.8652985\ttotal: 5.89s\tremaining: 5.7s\n508:\tlearn: 0.8645328\ttotal: 5.9s\tremaining: 5.69s\n509:\tlearn: 0.8643176\ttotal: 5.91s\tremaining: 5.68s\n510:\tlearn: 0.8640129\ttotal: 5.92s\tremaining: 5.67s\n511:\tlearn: 0.8631391\ttotal: 5.93s\tremaining: 5.66s\n512:\tlearn: 0.8627552\ttotal: 5.95s\tremaining: 5.64s\n513:\tlearn: 0.8625195\ttotal: 5.96s\tremaining: 5.63s\n514:\tlearn: 0.8619933\ttotal: 5.97s\tremaining: 5.62s\n515:\tlearn: 0.8614441\ttotal: 5.98s\tremaining: 5.61s\n516:\tlearn: 0.8606156\ttotal: 5.99s\tremaining: 5.6s\n517:\tlearn: 0.8599365\ttotal: 6s\tremaining: 5.59s\n518:\tlearn: 0.8597030\ttotal: 6.01s\tremaining: 5.57s\n519:\tlearn: 0.8594621\ttotal: 6.02s\tremaining: 5.56s\n520:\tlearn: 0.8587699\ttotal: 6.04s\tremaining: 5.55s\n521:\tlearn: 0.8579666\ttotal: 6.05s\tremaining: 5.54s\n522:\tlearn: 0.8566556\ttotal: 6.06s\tremaining: 5.53s\n523:\tlearn: 0.8565142\ttotal: 6.07s\tremaining: 5.51s\n524:\tlearn: 0.8562276\ttotal: 6.08s\tremaining: 5.5s\n525:\tlearn: 0.8557000\ttotal: 6.09s\tremaining: 5.49s\n526:\tlearn: 0.8555391\ttotal: 6.1s\tremaining: 5.48s\n527:\tlearn: 0.8549173\ttotal: 6.11s\tremaining: 5.47s\n528:\tlearn: 0.8547954\ttotal: 6.13s\tremaining: 5.45s\n529:\tlearn: 0.8536214\ttotal: 6.14s\tremaining: 5.44s\n530:\tlearn: 0.8531904\ttotal: 6.15s\tremaining: 5.43s\n531:\tlearn: 0.8518438\ttotal: 6.16s\tremaining: 5.42s\n532:\tlearn: 0.8514679\ttotal: 6.17s\tremaining: 5.41s\n533:\tlearn: 0.8510707\ttotal: 6.18s\tremaining: 5.39s\n534:\tlearn: 0.8505668\ttotal: 6.19s\tremaining: 5.38s\n535:\tlearn: 0.8503458\ttotal: 6.2s\tremaining: 5.37s\n536:\tlearn: 0.8497415\ttotal: 6.21s\tremaining: 5.36s\n537:\tlearn: 0.8492373\ttotal: 6.22s\tremaining: 5.35s\n538:\tlearn: 0.8485850\ttotal: 6.24s\tremaining: 5.33s\n539:\tlearn: 0.8483632\ttotal: 6.25s\tremaining: 5.32s\n540:\tlearn: 0.8476216\ttotal: 6.26s\tremaining: 5.31s\n541:\tlearn: 0.8469937\ttotal: 6.27s\tremaining: 5.3s\n542:\tlearn: 0.8464548\ttotal: 6.28s\tremaining: 5.29s\n543:\tlearn: 0.8458228\ttotal: 6.3s\tremaining: 5.28s\n544:\tlearn: 0.8452833\ttotal: 6.31s\tremaining: 5.27s\n545:\tlearn: 0.8442467\ttotal: 6.32s\tremaining: 5.25s\n546:\tlearn: 0.8439818\ttotal: 6.34s\tremaining: 5.25s\n547:\tlearn: 0.8429185\ttotal: 6.35s\tremaining: 5.24s\n548:\tlearn: 0.8415524\ttotal: 6.36s\tremaining: 5.23s\n549:\tlearn: 0.8409521\ttotal: 6.38s\tremaining: 5.22s\n550:\tlearn: 0.8402775\ttotal: 6.39s\tremaining: 5.21s\n551:\tlearn: 0.8401903\ttotal: 6.4s\tremaining: 5.19s\n552:\tlearn: 0.8393166\ttotal: 6.41s\tremaining: 5.18s\n553:\tlearn: 0.8388664\ttotal: 6.42s\tremaining: 5.17s\n554:\tlearn: 0.8376584\ttotal: 6.43s\tremaining: 5.16s\n555:\tlearn: 0.8371378\ttotal: 6.44s\tremaining: 5.14s\n556:\tlearn: 0.8368099\ttotal: 6.45s\tremaining: 5.13s\n557:\tlearn: 0.8362438\ttotal: 6.47s\tremaining: 5.12s\n558:\tlearn: 0.8357563\ttotal: 6.48s\tremaining: 5.11s\n559:\tlearn: 0.8342861\ttotal: 6.49s\tremaining: 5.1s\n560:\tlearn: 0.8335843\ttotal: 6.5s\tremaining: 5.09s\n561:\tlearn: 0.8326274\ttotal: 6.52s\tremaining: 5.08s\n562:\tlearn: 0.8317859\ttotal: 6.53s\tremaining: 5.07s\n563:\tlearn: 0.8315677\ttotal: 6.54s\tremaining: 5.05s\n564:\tlearn: 0.8302408\ttotal: 6.55s\tremaining: 5.04s\n565:\tlearn: 0.8295683\ttotal: 6.56s\tremaining: 5.03s\n566:\tlearn: 0.8286860\ttotal: 6.57s\tremaining: 5.02s\n567:\tlearn: 0.8280435\ttotal: 6.58s\tremaining: 5.01s\n568:\tlearn: 0.8279661\ttotal: 6.59s\tremaining: 5s\n569:\tlearn: 0.8276587\ttotal: 6.61s\tremaining: 4.98s\n570:\tlearn: 0.8270080\ttotal: 6.62s\tremaining: 4.97s\n571:\tlearn: 0.8264031\ttotal: 6.63s\tremaining: 4.96s\n572:\tlearn: 0.8256766\ttotal: 6.64s\tremaining: 4.95s\n573:\tlearn: 0.8252239\ttotal: 6.65s\tremaining: 4.94s\n574:\tlearn: 0.8249289\ttotal: 6.66s\tremaining: 4.93s\n575:\tlearn: 0.8242992\ttotal: 6.67s\tremaining: 4.91s\n576:\tlearn: 0.8242175\ttotal: 6.68s\tremaining: 4.9s\n577:\tlearn: 0.8227352\ttotal: 6.7s\tremaining: 4.89s\n578:\tlearn: 0.8221025\ttotal: 6.71s\tremaining: 4.88s\n579:\tlearn: 0.8217877\ttotal: 6.72s\tremaining: 4.87s\n580:\tlearn: 0.8216224\ttotal: 6.73s\tremaining: 4.86s\n581:\tlearn: 0.8209814\ttotal: 6.75s\tremaining: 4.84s\n582:\tlearn: 0.8201706\ttotal: 6.76s\tremaining: 4.83s\n583:\tlearn: 0.8195163\ttotal: 6.77s\tremaining: 4.82s\n584:\tlearn: 0.8191619\ttotal: 6.79s\tremaining: 4.82s\n585:\tlearn: 0.8184865\ttotal: 6.8s\tremaining: 4.8s\n586:\tlearn: 0.8175983\ttotal: 6.81s\tremaining: 4.79s\n587:\tlearn: 0.8168493\ttotal: 6.83s\tremaining: 4.78s\n588:\tlearn: 0.8165708\ttotal: 6.84s\tremaining: 4.77s\n589:\tlearn: 0.8162477\ttotal: 6.85s\tremaining: 4.76s\n590:\tlearn: 0.8158381\ttotal: 6.86s\tremaining: 4.75s\n591:\tlearn: 0.8155101\ttotal: 6.87s\tremaining: 4.74s\n592:\tlearn: 0.8151411\ttotal: 6.88s\tremaining: 4.72s\n593:\tlearn: 0.8147450\ttotal: 6.89s\tremaining: 4.71s\n594:\tlearn: 0.8146045\ttotal: 6.91s\tremaining: 4.7s\n595:\tlearn: 0.8137287\ttotal: 6.92s\tremaining: 4.69s\n596:\tlearn: 0.8135265\ttotal: 6.93s\tremaining: 4.68s\n597:\tlearn: 0.8131324\ttotal: 6.94s\tremaining: 4.67s\n598:\tlearn: 0.8115928\ttotal: 6.96s\tremaining: 4.66s\n599:\tlearn: 0.8111364\ttotal: 6.96s\tremaining: 4.64s\n600:\tlearn: 0.8106134\ttotal: 6.98s\tremaining: 4.63s\n601:\tlearn: 0.8100955\ttotal: 6.99s\tremaining: 4.62s\n602:\tlearn: 0.8098241\ttotal: 7s\tremaining: 4.61s\n603:\tlearn: 0.8093451\ttotal: 7.01s\tremaining: 4.6s\n604:\tlearn: 0.8083224\ttotal: 7.02s\tremaining: 4.58s\n605:\tlearn: 0.8081335\ttotal: 7.03s\tremaining: 4.57s\n606:\tlearn: 0.8074390\ttotal: 7.05s\tremaining: 4.56s\n607:\tlearn: 0.8069734\ttotal: 7.06s\tremaining: 4.55s\n608:\tlearn: 0.8068099\ttotal: 7.07s\tremaining: 4.54s\n609:\tlearn: 0.8055431\ttotal: 7.08s\tremaining: 4.53s\n610:\tlearn: 0.8042113\ttotal: 7.09s\tremaining: 4.51s\n611:\tlearn: 0.8039061\ttotal: 7.1s\tremaining: 4.5s\n612:\tlearn: 0.8035171\ttotal: 7.11s\tremaining: 4.49s\n613:\tlearn: 0.8030396\ttotal: 7.13s\tremaining: 4.48s\n614:\tlearn: 0.8021222\ttotal: 7.14s\tremaining: 4.47s\n615:\tlearn: 0.8017562\ttotal: 7.15s\tremaining: 4.46s\n616:\tlearn: 0.8010430\ttotal: 7.16s\tremaining: 4.45s\n617:\tlearn: 0.8003503\ttotal: 7.17s\tremaining: 4.43s\n618:\tlearn: 0.8000176\ttotal: 7.19s\tremaining: 4.42s\n619:\tlearn: 0.7998247\ttotal: 7.2s\tremaining: 4.41s\n620:\tlearn: 0.7996214\ttotal: 7.21s\tremaining: 4.4s\n621:\tlearn: 0.7994916\ttotal: 7.22s\tremaining: 4.39s\n622:\tlearn: 0.7992433\ttotal: 7.23s\tremaining: 4.38s\n623:\tlearn: 0.7988255\ttotal: 7.24s\tremaining: 4.37s\n624:\tlearn: 0.7982397\ttotal: 7.26s\tremaining: 4.35s\n625:\tlearn: 0.7972467\ttotal: 7.27s\tremaining: 4.34s\n626:\tlearn: 0.7964634\ttotal: 7.28s\tremaining: 4.33s\n627:\tlearn: 0.7960464\ttotal: 7.29s\tremaining: 4.32s\n628:\tlearn: 0.7956566\ttotal: 7.3s\tremaining: 4.31s\n629:\tlearn: 0.7954676\ttotal: 7.32s\tremaining: 4.3s\n630:\tlearn: 0.7947432\ttotal: 7.33s\tremaining: 4.29s\n631:\tlearn: 0.7944320\ttotal: 7.34s\tremaining: 4.28s\n632:\tlearn: 0.7941743\ttotal: 7.36s\tremaining: 4.26s\n633:\tlearn: 0.7931471\ttotal: 7.37s\tremaining: 4.25s\n634:\tlearn: 0.7918504\ttotal: 7.38s\tremaining: 4.24s\n635:\tlearn: 0.7911829\ttotal: 7.39s\tremaining: 4.23s\n636:\tlearn: 0.7901849\ttotal: 7.4s\tremaining: 4.22s\n637:\tlearn: 0.7895133\ttotal: 7.41s\tremaining: 4.21s\n638:\tlearn: 0.7893293\ttotal: 7.42s\tremaining: 4.19s\n639:\tlearn: 0.7893640\ttotal: 7.43s\tremaining: 4.18s\n640:\tlearn: 0.7887389\ttotal: 7.45s\tremaining: 4.17s\n641:\tlearn: 0.7886785\ttotal: 7.46s\tremaining: 4.16s\n642:\tlearn: 0.7881830\ttotal: 7.47s\tremaining: 4.15s\n643:\tlearn: 0.7870217\ttotal: 7.48s\tremaining: 4.13s\n644:\tlearn: 0.7867211\ttotal: 7.49s\tremaining: 4.12s\n645:\tlearn: 0.7857410\ttotal: 7.5s\tremaining: 4.11s\n646:\tlearn: 0.7856098\ttotal: 7.51s\tremaining: 4.1s\n647:\tlearn: 0.7849521\ttotal: 7.52s\tremaining: 4.09s\n648:\tlearn: 0.7846777\ttotal: 7.54s\tremaining: 4.08s\n649:\tlearn: 0.7844009\ttotal: 7.55s\tremaining: 4.06s\n650:\tlearn: 0.7838960\ttotal: 7.56s\tremaining: 4.05s\n651:\tlearn: 0.7833873\ttotal: 7.57s\tremaining: 4.04s\n652:\tlearn: 0.7829914\ttotal: 7.58s\tremaining: 4.03s\n653:\tlearn: 0.7824273\ttotal: 7.59s\tremaining: 4.02s\n654:\tlearn: 0.7822699\ttotal: 7.6s\tremaining: 4s\n655:\tlearn: 0.7821455\ttotal: 7.61s\tremaining: 3.99s\n656:\tlearn: 0.7816902\ttotal: 7.63s\tremaining: 3.98s\n657:\tlearn: 0.7811453\ttotal: 7.64s\tremaining: 3.97s\n658:\tlearn: 0.7805435\ttotal: 7.65s\tremaining: 3.96s\n659:\tlearn: 0.7799727\ttotal: 7.66s\tremaining: 3.95s\n660:\tlearn: 0.7797396\ttotal: 7.67s\tremaining: 3.93s\n661:\tlearn: 0.7793641\ttotal: 7.68s\tremaining: 3.92s\n662:\tlearn: 0.7789702\ttotal: 7.7s\tremaining: 3.91s\n663:\tlearn: 0.7785828\ttotal: 7.71s\tremaining: 3.9s\n664:\tlearn: 0.7776385\ttotal: 7.72s\tremaining: 3.89s\n665:\tlearn: 0.7764884\ttotal: 7.73s\tremaining: 3.88s\n","name":"stdout"},{"output_type":"stream","text":"666:\tlearn: 0.7762463\ttotal: 7.75s\tremaining: 3.87s\n667:\tlearn: 0.7758167\ttotal: 7.76s\tremaining: 3.86s\n668:\tlearn: 0.7756099\ttotal: 7.77s\tremaining: 3.85s\n669:\tlearn: 0.7747450\ttotal: 7.78s\tremaining: 3.83s\n670:\tlearn: 0.7739166\ttotal: 7.8s\tremaining: 3.82s\n671:\tlearn: 0.7735563\ttotal: 7.81s\tremaining: 3.81s\n672:\tlearn: 0.7734185\ttotal: 7.82s\tremaining: 3.8s\n673:\tlearn: 0.7732432\ttotal: 7.83s\tremaining: 3.79s\n674:\tlearn: 0.7723215\ttotal: 7.85s\tremaining: 3.78s\n675:\tlearn: 0.7720518\ttotal: 7.86s\tremaining: 3.77s\n676:\tlearn: 0.7717111\ttotal: 7.87s\tremaining: 3.75s\n677:\tlearn: 0.7712239\ttotal: 7.88s\tremaining: 3.74s\n678:\tlearn: 0.7709723\ttotal: 7.89s\tremaining: 3.73s\n679:\tlearn: 0.7707190\ttotal: 7.9s\tremaining: 3.72s\n680:\tlearn: 0.7703640\ttotal: 7.92s\tremaining: 3.71s\n681:\tlearn: 0.7700815\ttotal: 7.92s\tremaining: 3.69s\n682:\tlearn: 0.7695950\ttotal: 7.94s\tremaining: 3.68s\n683:\tlearn: 0.7684272\ttotal: 7.95s\tremaining: 3.67s\n684:\tlearn: 0.7677002\ttotal: 7.96s\tremaining: 3.66s\n685:\tlearn: 0.7670934\ttotal: 7.98s\tremaining: 3.65s\n686:\tlearn: 0.7667021\ttotal: 7.99s\tremaining: 3.64s\n687:\tlearn: 0.7657076\ttotal: 8s\tremaining: 3.63s\n688:\tlearn: 0.7652854\ttotal: 8.01s\tremaining: 3.62s\n689:\tlearn: 0.7646226\ttotal: 8.02s\tremaining: 3.6s\n690:\tlearn: 0.7640204\ttotal: 8.03s\tremaining: 3.59s\n691:\tlearn: 0.7638496\ttotal: 8.04s\tremaining: 3.58s\n692:\tlearn: 0.7628698\ttotal: 8.05s\tremaining: 3.57s\n693:\tlearn: 0.7624312\ttotal: 8.06s\tremaining: 3.56s\n694:\tlearn: 0.7618837\ttotal: 8.08s\tremaining: 3.54s\n695:\tlearn: 0.7615889\ttotal: 8.09s\tremaining: 3.53s\n696:\tlearn: 0.7614615\ttotal: 8.1s\tremaining: 3.52s\n697:\tlearn: 0.7606433\ttotal: 8.11s\tremaining: 3.51s\n698:\tlearn: 0.7603335\ttotal: 8.12s\tremaining: 3.5s\n699:\tlearn: 0.7595708\ttotal: 8.13s\tremaining: 3.48s\n700:\tlearn: 0.7584006\ttotal: 8.14s\tremaining: 3.47s\n701:\tlearn: 0.7580522\ttotal: 8.16s\tremaining: 3.46s\n702:\tlearn: 0.7575257\ttotal: 8.17s\tremaining: 3.45s\n703:\tlearn: 0.7570397\ttotal: 8.18s\tremaining: 3.44s\n704:\tlearn: 0.7565164\ttotal: 8.19s\tremaining: 3.43s\n705:\tlearn: 0.7563203\ttotal: 8.2s\tremaining: 3.42s\n706:\tlearn: 0.7552474\ttotal: 8.22s\tremaining: 3.41s\n707:\tlearn: 0.7545003\ttotal: 8.23s\tremaining: 3.4s\n708:\tlearn: 0.7539401\ttotal: 8.25s\tremaining: 3.38s\n709:\tlearn: 0.7524181\ttotal: 8.26s\tremaining: 3.37s\n710:\tlearn: 0.7519568\ttotal: 8.28s\tremaining: 3.36s\n711:\tlearn: 0.7517469\ttotal: 8.29s\tremaining: 3.35s\n712:\tlearn: 0.7497826\ttotal: 8.3s\tremaining: 3.34s\n713:\tlearn: 0.7496354\ttotal: 8.32s\tremaining: 3.33s\n714:\tlearn: 0.7493088\ttotal: 8.33s\tremaining: 3.32s\n715:\tlearn: 0.7488687\ttotal: 8.34s\tremaining: 3.31s\n716:\tlearn: 0.7481690\ttotal: 8.36s\tremaining: 3.3s\n717:\tlearn: 0.7473223\ttotal: 8.37s\tremaining: 3.29s\n718:\tlearn: 0.7470282\ttotal: 8.39s\tremaining: 3.28s\n719:\tlearn: 0.7462920\ttotal: 8.4s\tremaining: 3.27s\n720:\tlearn: 0.7449894\ttotal: 8.42s\tremaining: 3.26s\n721:\tlearn: 0.7447171\ttotal: 8.43s\tremaining: 3.25s\n722:\tlearn: 0.7442435\ttotal: 8.44s\tremaining: 3.23s\n723:\tlearn: 0.7433600\ttotal: 8.46s\tremaining: 3.23s\n724:\tlearn: 0.7430319\ttotal: 8.48s\tremaining: 3.21s\n725:\tlearn: 0.7423906\ttotal: 8.49s\tremaining: 3.21s\n726:\tlearn: 0.7418200\ttotal: 8.51s\tremaining: 3.19s\n727:\tlearn: 0.7412044\ttotal: 8.52s\tremaining: 3.18s\n728:\tlearn: 0.7412344\ttotal: 8.53s\tremaining: 3.17s\n729:\tlearn: 0.7409714\ttotal: 8.55s\tremaining: 3.16s\n730:\tlearn: 0.7408867\ttotal: 8.56s\tremaining: 3.15s\n731:\tlearn: 0.7402212\ttotal: 8.58s\tremaining: 3.14s\n732:\tlearn: 0.7399979\ttotal: 8.59s\tremaining: 3.13s\n733:\tlearn: 0.7394480\ttotal: 8.61s\tremaining: 3.12s\n734:\tlearn: 0.7393761\ttotal: 8.62s\tremaining: 3.11s\n735:\tlearn: 0.7386075\ttotal: 8.63s\tremaining: 3.1s\n736:\tlearn: 0.7379500\ttotal: 8.64s\tremaining: 3.08s\n737:\tlearn: 0.7377087\ttotal: 8.66s\tremaining: 3.07s\n738:\tlearn: 0.7368652\ttotal: 8.67s\tremaining: 3.06s\n739:\tlearn: 0.7362466\ttotal: 8.69s\tremaining: 3.05s\n740:\tlearn: 0.7359626\ttotal: 8.7s\tremaining: 3.04s\n741:\tlearn: 0.7356699\ttotal: 8.71s\tremaining: 3.03s\n742:\tlearn: 0.7353737\ttotal: 8.72s\tremaining: 3.02s\n743:\tlearn: 0.7351729\ttotal: 8.74s\tremaining: 3.01s\n744:\tlearn: 0.7349566\ttotal: 8.76s\tremaining: 3s\n745:\tlearn: 0.7337723\ttotal: 8.77s\tremaining: 2.99s\n746:\tlearn: 0.7333923\ttotal: 8.79s\tremaining: 2.98s\n747:\tlearn: 0.7328004\ttotal: 8.8s\tremaining: 2.96s\n748:\tlearn: 0.7317210\ttotal: 8.82s\tremaining: 2.95s\n749:\tlearn: 0.7313538\ttotal: 8.83s\tremaining: 2.94s\n750:\tlearn: 0.7310617\ttotal: 8.84s\tremaining: 2.93s\n751:\tlearn: 0.7305352\ttotal: 8.86s\tremaining: 2.92s\n752:\tlearn: 0.7303813\ttotal: 8.87s\tremaining: 2.91s\n753:\tlearn: 0.7302860\ttotal: 8.88s\tremaining: 2.9s\n754:\tlearn: 0.7295238\ttotal: 8.9s\tremaining: 2.89s\n755:\tlearn: 0.7290649\ttotal: 8.91s\tremaining: 2.88s\n756:\tlearn: 0.7278331\ttotal: 8.93s\tremaining: 2.87s\n757:\tlearn: 0.7270714\ttotal: 8.94s\tremaining: 2.85s\n758:\tlearn: 0.7259753\ttotal: 8.96s\tremaining: 2.84s\n759:\tlearn: 0.7257046\ttotal: 8.97s\tremaining: 2.83s\n760:\tlearn: 0.7249652\ttotal: 8.99s\tremaining: 2.82s\n761:\tlearn: 0.7243653\ttotal: 9s\tremaining: 2.81s\n762:\tlearn: 0.7234445\ttotal: 9.02s\tremaining: 2.8s\n763:\tlearn: 0.7229177\ttotal: 9.03s\tremaining: 2.79s\n764:\tlearn: 0.7223223\ttotal: 9.04s\tremaining: 2.78s\n765:\tlearn: 0.7221180\ttotal: 9.05s\tremaining: 2.77s\n766:\tlearn: 0.7217196\ttotal: 9.07s\tremaining: 2.75s\n767:\tlearn: 0.7203016\ttotal: 9.08s\tremaining: 2.74s\n768:\tlearn: 0.7201291\ttotal: 9.1s\tremaining: 2.73s\n769:\tlearn: 0.7200551\ttotal: 9.11s\tremaining: 2.72s\n770:\tlearn: 0.7199993\ttotal: 9.12s\tremaining: 2.71s\n771:\tlearn: 0.7189801\ttotal: 9.14s\tremaining: 2.7s\n772:\tlearn: 0.7183231\ttotal: 9.15s\tremaining: 2.69s\n773:\tlearn: 0.7176597\ttotal: 9.16s\tremaining: 2.67s\n774:\tlearn: 0.7170735\ttotal: 9.18s\tremaining: 2.67s\n775:\tlearn: 0.7165509\ttotal: 9.19s\tremaining: 2.65s\n776:\tlearn: 0.7159607\ttotal: 9.21s\tremaining: 2.64s\n777:\tlearn: 0.7157030\ttotal: 9.22s\tremaining: 2.63s\n778:\tlearn: 0.7151270\ttotal: 9.24s\tremaining: 2.62s\n779:\tlearn: 0.7150783\ttotal: 9.25s\tremaining: 2.61s\n780:\tlearn: 0.7146807\ttotal: 9.27s\tremaining: 2.6s\n781:\tlearn: 0.7136872\ttotal: 9.28s\tremaining: 2.59s\n782:\tlearn: 0.7134207\ttotal: 9.3s\tremaining: 2.58s\n783:\tlearn: 0.7132016\ttotal: 9.31s\tremaining: 2.56s\n784:\tlearn: 0.7120524\ttotal: 9.33s\tremaining: 2.56s\n785:\tlearn: 0.7116635\ttotal: 9.35s\tremaining: 2.54s\n786:\tlearn: 0.7112284\ttotal: 9.36s\tremaining: 2.53s\n787:\tlearn: 0.7104307\ttotal: 9.37s\tremaining: 2.52s\n788:\tlearn: 0.7098573\ttotal: 9.39s\tremaining: 2.51s\n789:\tlearn: 0.7091970\ttotal: 9.4s\tremaining: 2.5s\n790:\tlearn: 0.7089177\ttotal: 9.42s\tremaining: 2.49s\n791:\tlearn: 0.7085802\ttotal: 9.43s\tremaining: 2.48s\n792:\tlearn: 0.7083968\ttotal: 9.44s\tremaining: 2.46s\n793:\tlearn: 0.7078385\ttotal: 9.45s\tremaining: 2.45s\n794:\tlearn: 0.7071999\ttotal: 9.46s\tremaining: 2.44s\n795:\tlearn: 0.7063870\ttotal: 9.47s\tremaining: 2.43s\n796:\tlearn: 0.7061868\ttotal: 9.48s\tremaining: 2.42s\n797:\tlearn: 0.7060956\ttotal: 9.49s\tremaining: 2.4s\n798:\tlearn: 0.7056837\ttotal: 9.51s\tremaining: 2.39s\n799:\tlearn: 0.7051576\ttotal: 9.52s\tremaining: 2.38s\n800:\tlearn: 0.7047121\ttotal: 9.53s\tremaining: 2.37s\n801:\tlearn: 0.7042417\ttotal: 9.54s\tremaining: 2.35s\n802:\tlearn: 0.7036606\ttotal: 9.55s\tremaining: 2.34s\n803:\tlearn: 0.7030909\ttotal: 9.56s\tremaining: 2.33s\n804:\tlearn: 0.7024028\ttotal: 9.58s\tremaining: 2.32s\n805:\tlearn: 0.7017843\ttotal: 9.59s\tremaining: 2.31s\n806:\tlearn: 0.7016751\ttotal: 9.6s\tremaining: 2.3s\n807:\tlearn: 0.7010461\ttotal: 9.61s\tremaining: 2.28s\n808:\tlearn: 0.7006172\ttotal: 9.62s\tremaining: 2.27s\n809:\tlearn: 0.6996060\ttotal: 9.64s\tremaining: 2.26s\n810:\tlearn: 0.6992588\ttotal: 9.65s\tremaining: 2.25s\n811:\tlearn: 0.6984474\ttotal: 9.66s\tremaining: 2.24s\n812:\tlearn: 0.6980942\ttotal: 9.67s\tremaining: 2.22s\n813:\tlearn: 0.6974769\ttotal: 9.68s\tremaining: 2.21s\n814:\tlearn: 0.6970500\ttotal: 9.7s\tremaining: 2.2s\n815:\tlearn: 0.6963276\ttotal: 9.72s\tremaining: 2.19s\n816:\tlearn: 0.6961296\ttotal: 9.74s\tremaining: 2.18s\n817:\tlearn: 0.6955010\ttotal: 9.75s\tremaining: 2.17s\n818:\tlearn: 0.6951473\ttotal: 9.77s\tremaining: 2.16s\n819:\tlearn: 0.6944212\ttotal: 9.78s\tremaining: 2.15s\n820:\tlearn: 0.6936154\ttotal: 9.8s\tremaining: 2.14s\n821:\tlearn: 0.6933629\ttotal: 9.82s\tremaining: 2.13s\n822:\tlearn: 0.6932126\ttotal: 9.84s\tremaining: 2.12s\n823:\tlearn: 0.6931458\ttotal: 9.85s\tremaining: 2.1s\n824:\tlearn: 0.6929333\ttotal: 9.87s\tremaining: 2.09s\n825:\tlearn: 0.6923811\ttotal: 9.88s\tremaining: 2.08s\n826:\tlearn: 0.6919414\ttotal: 9.9s\tremaining: 2.07s\n827:\tlearn: 0.6911240\ttotal: 9.91s\tremaining: 2.06s\n828:\tlearn: 0.6899783\ttotal: 9.93s\tremaining: 2.05s\n829:\tlearn: 0.6898299\ttotal: 9.94s\tremaining: 2.04s\n830:\tlearn: 0.6897428\ttotal: 9.96s\tremaining: 2.02s\n831:\tlearn: 0.6892548\ttotal: 9.97s\tremaining: 2.01s\n832:\tlearn: 0.6881345\ttotal: 9.99s\tremaining: 2s\n","name":"stdout"},{"output_type":"stream","text":"833:\tlearn: 0.6877500\ttotal: 10s\tremaining: 1.99s\n834:\tlearn: 0.6874496\ttotal: 10s\tremaining: 1.98s\n835:\tlearn: 0.6872695\ttotal: 10s\tremaining: 1.97s\n836:\tlearn: 0.6867301\ttotal: 10s\tremaining: 1.96s\n837:\tlearn: 0.6854186\ttotal: 10.1s\tremaining: 1.95s\n838:\tlearn: 0.6843673\ttotal: 10.1s\tremaining: 1.93s\n839:\tlearn: 0.6839549\ttotal: 10.1s\tremaining: 1.92s\n840:\tlearn: 0.6832650\ttotal: 10.1s\tremaining: 1.91s\n841:\tlearn: 0.6826971\ttotal: 10.1s\tremaining: 1.9s\n842:\tlearn: 0.6822694\ttotal: 10.1s\tremaining: 1.89s\n843:\tlearn: 0.6814019\ttotal: 10.1s\tremaining: 1.88s\n844:\tlearn: 0.6802561\ttotal: 10.2s\tremaining: 1.86s\n845:\tlearn: 0.6801510\ttotal: 10.2s\tremaining: 1.85s\n846:\tlearn: 0.6796740\ttotal: 10.2s\tremaining: 1.84s\n847:\tlearn: 0.6792148\ttotal: 10.2s\tremaining: 1.83s\n848:\tlearn: 0.6789285\ttotal: 10.2s\tremaining: 1.82s\n849:\tlearn: 0.6786059\ttotal: 10.2s\tremaining: 1.8s\n850:\tlearn: 0.6782533\ttotal: 10.2s\tremaining: 1.79s\n851:\tlearn: 0.6767786\ttotal: 10.2s\tremaining: 1.78s\n852:\tlearn: 0.6767048\ttotal: 10.3s\tremaining: 1.77s\n853:\tlearn: 0.6764043\ttotal: 10.3s\tremaining: 1.75s\n854:\tlearn: 0.6757319\ttotal: 10.3s\tremaining: 1.74s\n855:\tlearn: 0.6747865\ttotal: 10.3s\tremaining: 1.73s\n856:\tlearn: 0.6745471\ttotal: 10.3s\tremaining: 1.72s\n857:\tlearn: 0.6745222\ttotal: 10.3s\tremaining: 1.71s\n858:\tlearn: 0.6735476\ttotal: 10.3s\tremaining: 1.69s\n859:\tlearn: 0.6731830\ttotal: 10.3s\tremaining: 1.68s\n860:\tlearn: 0.6728604\ttotal: 10.3s\tremaining: 1.67s\n861:\tlearn: 0.6726364\ttotal: 10.4s\tremaining: 1.66s\n862:\tlearn: 0.6721508\ttotal: 10.4s\tremaining: 1.65s\n863:\tlearn: 0.6716289\ttotal: 10.4s\tremaining: 1.63s\n864:\tlearn: 0.6713639\ttotal: 10.4s\tremaining: 1.62s\n865:\tlearn: 0.6711565\ttotal: 10.4s\tremaining: 1.61s\n866:\tlearn: 0.6709493\ttotal: 10.4s\tremaining: 1.6s\n867:\tlearn: 0.6708639\ttotal: 10.4s\tremaining: 1.58s\n868:\tlearn: 0.6704695\ttotal: 10.4s\tremaining: 1.57s\n869:\tlearn: 0.6699237\ttotal: 10.4s\tremaining: 1.56s\n870:\tlearn: 0.6694211\ttotal: 10.5s\tremaining: 1.55s\n871:\tlearn: 0.6687379\ttotal: 10.5s\tremaining: 1.54s\n872:\tlearn: 0.6681833\ttotal: 10.5s\tremaining: 1.52s\n873:\tlearn: 0.6679041\ttotal: 10.5s\tremaining: 1.51s\n874:\tlearn: 0.6674571\ttotal: 10.5s\tremaining: 1.5s\n875:\tlearn: 0.6670822\ttotal: 10.5s\tremaining: 1.49s\n876:\tlearn: 0.6668399\ttotal: 10.5s\tremaining: 1.48s\n877:\tlearn: 0.6662930\ttotal: 10.5s\tremaining: 1.47s\n878:\tlearn: 0.6653481\ttotal: 10.6s\tremaining: 1.45s\n879:\tlearn: 0.6646681\ttotal: 10.6s\tremaining: 1.44s\n880:\tlearn: 0.6643921\ttotal: 10.6s\tremaining: 1.43s\n881:\tlearn: 0.6641432\ttotal: 10.6s\tremaining: 1.42s\n882:\tlearn: 0.6639861\ttotal: 10.6s\tremaining: 1.4s\n883:\tlearn: 0.6631335\ttotal: 10.6s\tremaining: 1.39s\n884:\tlearn: 0.6624435\ttotal: 10.6s\tremaining: 1.38s\n885:\tlearn: 0.6621612\ttotal: 10.6s\tremaining: 1.37s\n886:\tlearn: 0.6604616\ttotal: 10.7s\tremaining: 1.36s\n887:\tlearn: 0.6599887\ttotal: 10.7s\tremaining: 1.34s\n888:\tlearn: 0.6590203\ttotal: 10.7s\tremaining: 1.33s\n889:\tlearn: 0.6585675\ttotal: 10.7s\tremaining: 1.32s\n890:\tlearn: 0.6581156\ttotal: 10.7s\tremaining: 1.31s\n891:\tlearn: 0.6574977\ttotal: 10.7s\tremaining: 1.3s\n892:\tlearn: 0.6572389\ttotal: 10.7s\tremaining: 1.28s\n893:\tlearn: 0.6568673\ttotal: 10.7s\tremaining: 1.27s\n894:\tlearn: 0.6566706\ttotal: 10.7s\tremaining: 1.26s\n895:\tlearn: 0.6562180\ttotal: 10.8s\tremaining: 1.25s\n896:\tlearn: 0.6560383\ttotal: 10.8s\tremaining: 1.24s\n897:\tlearn: 0.6559692\ttotal: 10.8s\tremaining: 1.22s\n898:\tlearn: 0.6557262\ttotal: 10.8s\tremaining: 1.21s\n899:\tlearn: 0.6550384\ttotal: 10.8s\tremaining: 1.2s\n900:\tlearn: 0.6546397\ttotal: 10.8s\tremaining: 1.19s\n901:\tlearn: 0.6543400\ttotal: 10.8s\tremaining: 1.18s\n902:\tlearn: 0.6534116\ttotal: 10.8s\tremaining: 1.16s\n903:\tlearn: 0.6529806\ttotal: 10.9s\tremaining: 1.15s\n904:\tlearn: 0.6524625\ttotal: 10.9s\tremaining: 1.14s\n905:\tlearn: 0.6522540\ttotal: 10.9s\tremaining: 1.13s\n906:\tlearn: 0.6518812\ttotal: 10.9s\tremaining: 1.12s\n907:\tlearn: 0.6514498\ttotal: 10.9s\tremaining: 1.1s\n908:\tlearn: 0.6514219\ttotal: 10.9s\tremaining: 1.09s\n909:\tlearn: 0.6509412\ttotal: 10.9s\tremaining: 1.08s\n910:\tlearn: 0.6508723\ttotal: 10.9s\tremaining: 1.07s\n911:\tlearn: 0.6502365\ttotal: 10.9s\tremaining: 1.06s\n912:\tlearn: 0.6495250\ttotal: 11s\tremaining: 1.04s\n913:\tlearn: 0.6490005\ttotal: 11s\tremaining: 1.03s\n914:\tlearn: 0.6487934\ttotal: 11s\tremaining: 1.02s\n915:\tlearn: 0.6483669\ttotal: 11s\tremaining: 1.01s\n916:\tlearn: 0.6477225\ttotal: 11s\tremaining: 996ms\n917:\tlearn: 0.6470246\ttotal: 11s\tremaining: 984ms\n918:\tlearn: 0.6466182\ttotal: 11s\tremaining: 972ms\n919:\tlearn: 0.6464485\ttotal: 11s\tremaining: 960ms\n920:\tlearn: 0.6458129\ttotal: 11.1s\tremaining: 948ms\n921:\tlearn: 0.6455075\ttotal: 11.1s\tremaining: 936ms\n922:\tlearn: 0.6451653\ttotal: 11.1s\tremaining: 924ms\n923:\tlearn: 0.6449597\ttotal: 11.1s\tremaining: 912ms\n924:\tlearn: 0.6445826\ttotal: 11.1s\tremaining: 900ms\n925:\tlearn: 0.6442107\ttotal: 11.1s\tremaining: 888ms\n926:\tlearn: 0.6441617\ttotal: 11.1s\tremaining: 876ms\n927:\tlearn: 0.6439003\ttotal: 11.1s\tremaining: 864ms\n928:\tlearn: 0.6437642\ttotal: 11.1s\tremaining: 852ms\n929:\tlearn: 0.6437399\ttotal: 11.2s\tremaining: 840ms\n930:\tlearn: 0.6426772\ttotal: 11.2s\tremaining: 828ms\n931:\tlearn: 0.6425634\ttotal: 11.2s\tremaining: 816ms\n932:\tlearn: 0.6416508\ttotal: 11.2s\tremaining: 804ms\n933:\tlearn: 0.6409229\ttotal: 11.2s\tremaining: 792ms\n934:\tlearn: 0.6405045\ttotal: 11.2s\tremaining: 780ms\n935:\tlearn: 0.6402355\ttotal: 11.2s\tremaining: 768ms\n936:\tlearn: 0.6398046\ttotal: 11.2s\tremaining: 756ms\n937:\tlearn: 0.6395703\ttotal: 11.3s\tremaining: 744ms\n938:\tlearn: 0.6393948\ttotal: 11.3s\tremaining: 732ms\n939:\tlearn: 0.6391059\ttotal: 11.3s\tremaining: 720ms\n940:\tlearn: 0.6388925\ttotal: 11.3s\tremaining: 708ms\n941:\tlearn: 0.6385627\ttotal: 11.3s\tremaining: 696ms\n942:\tlearn: 0.6383979\ttotal: 11.3s\tremaining: 684ms\n943:\tlearn: 0.6377996\ttotal: 11.3s\tremaining: 672ms\n944:\tlearn: 0.6366004\ttotal: 11.3s\tremaining: 660ms\n945:\tlearn: 0.6359718\ttotal: 11.3s\tremaining: 648ms\n946:\tlearn: 0.6353062\ttotal: 11.4s\tremaining: 636ms\n947:\tlearn: 0.6349300\ttotal: 11.4s\tremaining: 624ms\n948:\tlearn: 0.6345467\ttotal: 11.4s\tremaining: 612ms\n949:\tlearn: 0.6337596\ttotal: 11.4s\tremaining: 600ms\n950:\tlearn: 0.6332474\ttotal: 11.4s\tremaining: 587ms\n951:\tlearn: 0.6329694\ttotal: 11.4s\tremaining: 575ms\n952:\tlearn: 0.6321729\ttotal: 11.4s\tremaining: 564ms\n953:\tlearn: 0.6318239\ttotal: 11.4s\tremaining: 552ms\n954:\tlearn: 0.6316772\ttotal: 11.4s\tremaining: 540ms\n955:\tlearn: 0.6307787\ttotal: 11.5s\tremaining: 528ms\n956:\tlearn: 0.6306930\ttotal: 11.5s\tremaining: 515ms\n957:\tlearn: 0.6304583\ttotal: 11.5s\tremaining: 503ms\n958:\tlearn: 0.6300766\ttotal: 11.5s\tremaining: 491ms\n959:\tlearn: 0.6295225\ttotal: 11.5s\tremaining: 479ms\n960:\tlearn: 0.6293806\ttotal: 11.5s\tremaining: 467ms\n961:\tlearn: 0.6291983\ttotal: 11.5s\tremaining: 455ms\n962:\tlearn: 0.6285629\ttotal: 11.5s\tremaining: 443ms\n963:\tlearn: 0.6275431\ttotal: 11.6s\tremaining: 431ms\n964:\tlearn: 0.6272764\ttotal: 11.6s\tremaining: 419ms\n965:\tlearn: 0.6267611\ttotal: 11.6s\tremaining: 407ms\n966:\tlearn: 0.6266888\ttotal: 11.6s\tremaining: 395ms\n967:\tlearn: 0.6263310\ttotal: 11.6s\tremaining: 383ms\n968:\tlearn: 0.6255504\ttotal: 11.6s\tremaining: 371ms\n969:\tlearn: 0.6252138\ttotal: 11.6s\tremaining: 359ms\n970:\tlearn: 0.6246776\ttotal: 11.6s\tremaining: 347ms\n971:\tlearn: 0.6243467\ttotal: 11.6s\tremaining: 335ms\n972:\tlearn: 0.6243239\ttotal: 11.7s\tremaining: 323ms\n973:\tlearn: 0.6233110\ttotal: 11.7s\tremaining: 311ms\n974:\tlearn: 0.6229497\ttotal: 11.7s\tremaining: 300ms\n975:\tlearn: 0.6224433\ttotal: 11.7s\tremaining: 288ms\n976:\tlearn: 0.6222835\ttotal: 11.7s\tremaining: 276ms\n977:\tlearn: 0.6220023\ttotal: 11.7s\tremaining: 264ms\n978:\tlearn: 0.6217417\ttotal: 11.7s\tremaining: 252ms\n979:\tlearn: 0.6215440\ttotal: 11.7s\tremaining: 240ms\n980:\tlearn: 0.6209382\ttotal: 11.8s\tremaining: 228ms\n981:\tlearn: 0.6201927\ttotal: 11.8s\tremaining: 216ms\n982:\tlearn: 0.6200016\ttotal: 11.8s\tremaining: 204ms\n983:\tlearn: 0.6187016\ttotal: 11.8s\tremaining: 192ms\n984:\tlearn: 0.6183940\ttotal: 11.8s\tremaining: 180ms\n985:\tlearn: 0.6179751\ttotal: 11.8s\tremaining: 168ms\n986:\tlearn: 0.6175670\ttotal: 11.8s\tremaining: 156ms\n987:\tlearn: 0.6175389\ttotal: 11.8s\tremaining: 144ms\n988:\tlearn: 0.6169610\ttotal: 11.8s\tremaining: 132ms\n989:\tlearn: 0.6163229\ttotal: 11.9s\tremaining: 120ms\n990:\tlearn: 0.6159871\ttotal: 11.9s\tremaining: 108ms\n991:\tlearn: 0.6157393\ttotal: 11.9s\tremaining: 95.8ms\n992:\tlearn: 0.6147594\ttotal: 11.9s\tremaining: 83.8ms\n993:\tlearn: 0.6142522\ttotal: 11.9s\tremaining: 71.9ms\n994:\tlearn: 0.6138865\ttotal: 11.9s\tremaining: 59.9ms\n995:\tlearn: 0.6133904\ttotal: 11.9s\tremaining: 47.9ms\n996:\tlearn: 0.6128821\ttotal: 11.9s\tremaining: 35.9ms\n997:\tlearn: 0.6125813\ttotal: 12s\tremaining: 24ms\n998:\tlearn: 0.6123700\ttotal: 12s\tremaining: 12ms\n999:\tlearn: 0.6120662\ttotal: 12s\tremaining: 0us\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.drop(LABEL_COL_NAME, axis = 1)\ny_test = np.expm1(model.predict(test))","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(SUBMISSON_PATH, index_col='id')\nsubmission[LABEL_COL_NAME] = y_test[:-1]\nsubmission.to_csv(f'submission.csv')\nprint(submission)","execution_count":17,"outputs":[{"output_type":"stream","text":"           revenue\nid                \n3001  6.325281e+05\n3002  2.362490e+05\n3003  6.607285e+06\n3004  1.597385e+07\n3005  1.886249e+06\n3006  4.377182e+06\n3007  2.625564e+06\n3008  3.261047e+07\n3009  2.550607e+07\n3010  4.095561e+08\n3011  1.053723e+06\n3012  3.220981e+05\n3013  2.413199e+07\n3014  1.177211e+06\n3015  2.020941e+07\n3016  5.809602e+05\n3017  4.610647e+07\n3018  1.195674e+08\n3019  1.314391e+07\n3020  2.242504e+08\n3021  5.084288e+07\n3022  3.264951e+07\n3023  4.653937e+05\n3024  1.573245e+07\n3025  1.047255e+06\n3026  1.327498e+08\n3027  2.252228e+06\n3028  8.503923e+07\n3029  4.909475e+05\n3030  8.915429e+07\n...            ...\n7369  1.044012e+07\n7370  8.661743e+07\n7371  6.015235e+05\n7372  6.897314e+07\n7373  2.506530e+08\n7374  1.942182e+07\n7375  2.313075e+07\n7376  6.284405e+06\n7377  2.359627e+07\n7378  1.522728e+07\n7379  2.716889e+07\n7380  9.132610e+05\n7381  7.569085e+05\n7382  3.681252e+06\n7383  1.317653e+05\n7384  1.542667e+07\n7385  4.446349e+07\n7386  2.947730e+07\n7387  8.466639e+06\n7388  1.777729e+06\n7389  4.249713e+05\n7390  7.243007e+04\n7391  8.330549e+06\n7392  1.330901e+07\n7393  5.585713e+08\n7394  1.591115e+08\n7395  3.904441e+07\n7396  8.091513e+07\n7397  5.470975e+06\n7398  1.163145e+05\n\n[4398 rows x 1 columns]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}