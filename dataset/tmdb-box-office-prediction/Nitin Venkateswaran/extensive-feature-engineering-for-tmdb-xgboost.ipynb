{"cells":[{"metadata":{},"cell_type":"markdown","source":"This Kernel does extensive Feature Engineering on the TMDB DataSet. This results in a whole lot of features (3500+) which can be put through a Random Forest or a Gradient Boosted Regression Tree. \n\nThe number of features for some variables can be controlled by setting the hyper-parameter. This will be detailed in the relevant section.\n\n[1. Import all the needed libraries](#one) <br />\n[2. Import the data and merge train and test datasets](#two) <br />\n[3. Create One-Hot Encodings for the following variables as well as their counts](#three) <br />\n[4. Extract cast and crew information](#four) <br />\n[5. Create new features from cast flat file](#five) <br />\n[6. Create new features from crew flat file](#six) <br />\n[7. Get characters and their actors as one-hot encoded features, that appeared at least 'x' number of times in the entire dataset](#seven) <br />\n[8. Get crew as one-hot encoded features, that appeared at least 'x' number of times in the entire dataset](#eight)<br />\n[9. Length and number of words in the original title, title, overview, and tagline](#nine) <br />\n[10. Sentiment and Polarity of the overview and tagline of the movie](#ten) <br />\n[11. Date-based feature extraction (year, weekday, month, weekofyear, day, and quarter)](#eleven) <br />\n[12. Movie rank by release year](#twelve) <br />\n[13. Label Encode the collection and original language features](#thirteen)<br />\n[14. Generate some Bag-Of-Words features](#fourteen) <br />\n[15. Align the train and test datasets and dump the new files to output](#fifteen) <br />\n[16. Run the XGBoost](#sixteen) <br />\n\nThe feature engineered files are saved to output for your use (train_data_features.csv and test_data_features.csv)\n\nEDIT: The Kaggle data crunching feature crashes the kernel. Until this is fixed, it would be best to run this kernel locally on your machine to generate the two csv feature files above. To get a smaller file, the hyper-parameter in sections #7 and #8 can be changed to a larger number to get a smaller featureset, which would hopefully not crash the page!"},{"metadata":{},"cell_type":"markdown","source":"<a id='one'></a>\n> **1. Import all the needed libraries** \n\nIn addition to some standard libraries, we will use **textblob** here to get the sentiment and polarity of the tagline and overview texts. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nimport math\nimport uuid\npd.set_option('display.max_columns', None)\nimport json\nimport os\nfrom collections import Counter\nfrom pandas import DataFrame\n#!pip install wordcloud\n#!pip install textblob\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline  \nimport nltk\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer, PorterStemmer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom textblob import TextBlob    \nstop = stopwords.words('english')\n\nimport pandas as pd\nimport numpy as np\nfrom numpy import inf\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport xgboost as xgb\n\nscaler = MinMaxScaler(feature_range=(0,1))\n\n#!pip install eli5\nimport eli5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='two'></a>\n> **2. Import the data and merge train and test datasets**\n\nThe merging is done to combine all the categorical features of categorical variables, from which one-hot encoding can be done smoothly"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/tmdb-clean-dataset/train.csv')\ntest_df = pd.read_csv('../input/tmdb-clean-dataset/test.csv')\n\n# Merging datasets to ensure things like encoding are created on a joint dataset\nmerged_df = train_df.append(test_df,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='three'></a>\n\n> **3. Create One-Hot Encodings for the following variables as well as their counts:**\n\na. Genres <br />\nb. Production Countries <br />\nc. Spoken Languages <br />\nd. Keywords <br />\ne. Belongs to Collection <br />\nf. Production Companies <br />\n\nThis process explodes the json in each variable, and creates a one-hot encoding for each value in the json across the train and test datasets. This results in a lot of features.\n\nThe process also gets the count of each variable for each movie, into a separate variable (6 new variables for the counts in the json element of each variable)."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nHelper functions to explode json data into columns for the train and test datasets\n\"\"\"\n\nimport uuid\n\ndef get_count_houses(data_column,col_name):\n    \n    ls = []\n    data = [extract_meta_2(str(row),col_name) for row in data_column]\n    data_2 = [house for production_houses in data for house in production_houses]\n    house_count = Counter(data_2)\n    \n    return house_count\n\n\ndef extract_single_meta(json_data,col_name):\n    if (json_data == 'nan'):\n        return 'x'\n    \n    # Some text cleaning\n    json_data = json_data.replace(\"'\",'\"')\n    json_data = json_data.replace('None','\"None\"')\n    json_data = json_data.replace('s n\"','s n ')\n    json_data = json_data.replace('ed\"','ed ')\n    json_data = json_data.replace('ld\"','ld ')\n    json_data = json_data.replace('n\"s','ns' )\n    json_data = json_data.replace('o\"s','os' )\n    json_data = json_data.replace('\"s','s' )\n\n    data = json.loads(json_data)\n    meta = [d[col_name] for d in data]\n    return meta[0]\n\n\ndef extract_meta(json_data,col_name):\n    if (json_data == 'nan'):\n        return 'x' \n    \n    json_data = json_data.replace(\"'\",'\"')\n    str_json_data = str(json_data)\n    str_json_data = str_json_data.replace(\"l\\\"A\",\"l\")\n    data = None\n    data= json.loads(str_json_data)\n    \n    meta = [d[col_name] for d in data]\n    \n    return meta\n\ndef extract_meta_2(json_data,col_name):\n    if (json_data == 'nan'):\n        return 'x' \n    \n    json_data = json_data.replace(u'\\xa0',u' ')\n    json_data = json_data.replace(\"{'\",'{\"')\n    json_data = json_data.replace(\"'}\",'\"}')\n    json_data = json_data.replace(\"':\",'\":')\n    json_data = json_data.replace(\": '\",': \"')\n    json_data = json_data.replace(\"',\",'\",')\n    json_data = json_data.replace(\", '\",', \"')\n    json_data = json_data.replace(\"'\",'')\n    \n    str_json_data = str(json_data)\n    data= None\n    data= json.loads(str_json_data)\n        \n    meta = [d[col_name] for d in data]\n    \n    return meta\n    \n\n\ndef extract_metadata(data_column, col_name, col_type = \"blank\"):\n   \n    # Extract all meta-genres\n    all_meta = set()\n    counter = Counter()\n        \n    if (col_type != 'production_companies' and col_type != 'keyword'):\n        distinct_meta = [extract_meta(str(row),col_name) for row in data_column]\n    else:\n        counter = get_count_houses(data_column,col_name)\n        counter = {k:counter[k] for k in counter if counter[k] > 5}\n    \n        distinct_meta = [extract_meta_2(str(row),col_name) for row in data_column]\n        flattened_meta = [data for row in distinct_meta for data in row]\n        \n        distinct_meta = []\n        for k in counter:\n            if k in flattened_meta:\n                distinct_meta.append(k)\n    \n    for data in distinct_meta:\n        if (col_type != 'production_companies' and col_type != 'keyword'):\n            all_meta.update(set(data))\n        else:\n            all_meta.update(set([data]))\n        \n    all_meta_dict = dict()\n    i = 0\n    for item in all_meta:\n        all_meta_dict[item] = i\n        i += 1\n    all_meta_list = list(all_meta)\n    \n    return all_meta, all_meta_dict, all_meta_list\n\n\ndef add_meta_to_dataframe(all_meta_dict,all_meta_list, _df, data_column, col_name, col_type='blank'):\n\n    if (col_type != 'production_companies' and col_type != 'keyword'):\n        distinct_meta = [extract_meta(str(row),col_name) for row in data_column]\n    else:\n        distinct_meta = [extract_meta_2(str(row),col_name) for row in data_column]\n        \n    one_hot_np = np.empty([len(_df),len(all_meta_dict)])\n    for i in range(0, len(_df)):\n        one_hot_meta = np.zeros(len(all_meta_dict))\n        for j in range(0,len(distinct_meta[i])):\n            if (distinct_meta[i][j] in all_meta_dict):\n                one_hot_meta[all_meta_dict[distinct_meta[i][j]]] = 1\n        one_hot_np[i] = one_hot_meta\n        \n    one_hot_df = pd.DataFrame(one_hot_np)\n    one_hot_df.columns = all_meta_list\n    \n    if (col_name == 'iso_639_1'):\n        #one_hot_df.drop('id',axis=1,inplace=True)\n        one_hot_df = one_hot_df['en'] \n    else:\n        one_hot_df['sum' + col_name] = one_hot_df.sum(axis=1,skipna=False)\n    \n    # Final train df\n    _df_new = pd.concat([_df.reset_index(drop=True),one_hot_df],axis=1)\n    \n    return _df_new\n\n\n# Genres        \ngenres = merged_df['genres']\ngenres.fillna('nan',inplace=True)\nall_meta, all_meta_dict, all_meta_list = extract_metadata(genres,\"name\")\ngenres = train_df['genres']\ngenres.fillna('nan',inplace=True)\ntrain_df_new = add_meta_to_dataframe(all_meta_dict, all_meta_list, train_df, genres,\"name\")\ngenres_test = test_df['genres']\ngenres_test.fillna('nan',inplace=True)\ntest_df_new = add_meta_to_dataframe(all_meta_dict,all_meta_list,test_df,genres_test,\"name\")\n\n# Spoken languages\nspoken_languages = merged_df['spoken_languages']\nspoken_languages.fillna('nan',inplace=True)\nall_meta, all_meta_dict, all_meta_list = extract_metadata(spoken_languages,\"iso_639_1\")\nspoken_languages = train_df['spoken_languages']\nspoken_languages.fillna('nan',inplace=True)\n\ntrain_df_new = add_meta_to_dataframe(all_meta_dict, all_meta_list, train_df_new, spoken_languages,\"iso_639_1\")\n\n\nspoken_languages_test = test_df['spoken_languages']\nspoken_languages_test.fillna('nan',inplace=True)\ntest_df_new = add_meta_to_dataframe(all_meta_dict,all_meta_list,test_df_new,spoken_languages_test,\"iso_639_1\")\n\n\n# production countries\nproduction_countries = merged_df['production_countries']\nproduction_countries.fillna('nan',inplace=True)\nall_meta, all_meta_dict, all_meta_list = extract_metadata(production_countries,\"iso_3166_1\")\nproduction_countries = train_df['production_countries']\nproduction_countries.fillna('nan',inplace=True)\ntrain_df_new = add_meta_to_dataframe(all_meta_dict, all_meta_list, train_df_new, production_countries,\"iso_3166_1\")\n\n\nproduction_countries_test = test_df['production_countries']\nproduction_countries_test.fillna('nan',inplace=True)\ntest_df_new = add_meta_to_dataframe(all_meta_dict,all_meta_list,test_df_new,production_countries_test,\"iso_3166_1\")\n\n\"\"\"\n# Keywords\nkeywords = merged_df['Keywords']\nkeywords.fillna('nan',inplace=True)\nall_meta, all_meta_dict, all_meta_list = extract_metadata(keywords,\"name\",\"keyword\")\nkeywords = train_df['Keywords']\nkeywords.fillna('nan',inplace=True)\ntrain_df_new = add_meta_to_dataframe(all_meta_dict, all_meta_list, train_df_new, keywords,\"name\",\"keyword\")\n\n\nkeywords_test = test_df['Keywords']\nkeywords_test.fillna('nan',inplace=True)\ntest_df_new = add_meta_to_dataframe(all_meta_dict,all_meta_list,test_df_new,keywords_test,\"name\",\"keyword\")\n\"\"\"\n\n\n# Belongs to collection\nbelongs_to_collection = train_df['belongs_to_collection']\nbelongs_to_collection.fillna('nan',inplace=True)\nresult = [extract_single_meta(str(row),'name') for row in belongs_to_collection]\nresult = pd.DataFrame(result)\nresult.columns = ['collection']\ntrain_df_new = pd.concat([train_df_new.reset_index(drop=True),result],axis=1)\n\n\nbelongs_to_collection_test = test_df['belongs_to_collection']\nbelongs_to_collection_test.fillna('nan',inplace=True)\nresult = [extract_single_meta(str(row),'name') for row in belongs_to_collection_test]\nresult = pd.DataFrame(result)\nresult.columns = ['collection']\ntest_df_new = pd.concat([test_df_new.reset_index(drop=True),result],axis=1)\n\n# production companies\nproduction_companies = merged_df['production_companies']\nproduction_companies.fillna('nan',inplace=True)\nall_meta, all_meta_dict, all_meta_list = extract_metadata(production_companies,\"name\",\"production_companies\")\nproduction_companies = train_df['production_companies']\nproduction_companies.fillna('nan',inplace=True)\ntrain_df_new = add_meta_to_dataframe(all_meta_dict, all_meta_list, train_df_new, production_companies,\"name\",\"production_companies\")\n\n\nproduction_companies_test = test_df['production_companies']\nproduction_companies_test.fillna('nan',inplace=True)\ntest_df_new = add_meta_to_dataframe(all_meta_dict,all_meta_list,test_df_new,production_companies_test,\"name\",\"production_companies\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='four'></a>\n> **4. Extract cast and crew information**\n\nThis extracts the cast and crew json into csv format, and adds a foreign key - the movie id (imdb_id) - to the flattened files. The foreign key facilitates joins to the main dataset, which we will see become useful later on"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nExtracts cast and crew information into train and test flat files csv \n\n\n\"\"\"\ndef extract_single_meta(json_data,col_name):\n    if (json_data == 'nan'):\n        return 'x'\n    \n    json_data = json_data.replace(\"'\",'\"')\n    json_data = json_data.replace('None','\"None\"')\n    json_data = json_data.replace('s n\"','s n ')\n    json_data = json_data.replace('ed\"','ed ')\n    json_data = json_data.replace('ld\"','ld ')\n    json_data = json_data.replace('n\"s','ns' )\n    json_data = json_data.replace('o\"s','os' )\n    json_data = json_data.replace('\"s','s' )\n\n    data = json.loads(json_data)\n    meta = [d for d in data]\n    return meta[0]\n\n\n\"\"\"\nTrain dataset\n\"\"\"\n\n# Cast\nimdb_id = train_df['imdb_id']\ncast = train_df['cast']\nmain_ls = []\nx = 0\ni = 0\nfor item in cast:\n    i+= 1\n    ls = []\n    \n    if str(item) == '[]':\n        main_ls.append(['-1','nan','nan','-1','-1','nan','10000','nan'])\n        x += 1\n        continue\n    \n    item = str(item)\n    item = item.replace(\"', '\",\"'; '\")\n    item = item.replace(\", '\",\"; '\")\n    item= item.replace(\"{\",\"\")\n    item= item.replace(\"},\",\";\")\n    item= item.replace(\"[\",\"\")\n    item= item.replace(\"]\",\"\")\n    item= item.replace(\"'\",\"\")\n    item = item.replace('Jimmy;,','Jimmy')\n    item = item.replace('Elektra Quartet;','Elektra Quartet')\n    \n    item_ls = item.split(';')\n    i = 0\n    ls = []\n    for it in item_ls:\n        i += 1\n        \n        it = it.replace('[','')\n        it = it.replace(']','')\n        it_ls = it.split(':')\n        ls.append(str(it_ls[1]).strip())\n    \n        if i % 8 == 0:\n            #print (imdb_id[x])\n            ls.append(imdb_id[x])\n            main_ls.append(ls)\n            ls = []\n    x += 1\n            \ncast_df = DataFrame.from_records(main_ls)\ncast_df.columns = ['cast_id','character','credit_id','gender','id','name','order','profile_path','imdb_id']\n\n        \n\"\"\"\nTest dataset\n\"\"\"\n# Cast\nimdb_id = test_df['imdb_id']\ncast = test_df['cast']\nmain_ls = []\nx = 0\ni = 0\nfor item in cast:\n    i+= 1\n    ls = []\n    \n    if str(item) == '[]':\n        main_ls.append(['-1','nan','nan','-1','-1','nan','10000','nan'])\n        x += 1\n        continue\n    \n    item = str(item)\n    item = item.replace(\"', '\",\"'; '\")\n    item = item.replace(\", '\",\"; '\")\n    item= item.replace(\"{\",\"\")\n    item= item.replace(\"},\",\";\")\n    item= item.replace(\"[\",\"\")\n    item= item.replace(\"]\",\"\")\n    item= item.replace(\"'\",\"\")\n    \n    item = item.replace('Jimmy;,','Jimmy')\n    item = item.replace('Elektra Quartet;','Elektra Quartet')\n    \n    item_ls = item.split(';')\n    i = 0\n    ls = []\n    for it in item_ls:\n        i += 1\n        \n        it = it.replace('[','')\n        it = it.replace(']','')\n        it_ls = it.split(':')\n        ls.append(str(it_ls[1]).strip())\n        \n        if i % 8 == 0:\n            #print (imdb_id[x])\n            ls.append(imdb_id[x])\n            main_ls.append(ls)\n            ls = []\n    x += 1\n\ncast_df_test = DataFrame.from_records(main_ls)\ncast_df_test.columns = ['cast_id','character','credit_id','gender','id','name','order','profile_path','imdb_id']\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nExtract (explode) crew information for train and test datasets\n\"\"\"\n\n\n\"\"\"\nTrain data\n\"\"\"\nimdb_id = train_df['imdb_id']\ncast = train_df['crew']\nmain_ls = []\nx = 0\ni = 0\nfor item in cast:\n    i+= 1\n    ls = []\n    #print (item)\n    \n    if str(item) == '[]':\n        main_ls.append(['nan','nan','nan','nan','nan','nan','nan'])\n        x += 1\n        continue\n    \n    item = str(item)\n    item = item.replace(\"', '\",\"'; '\")\n    item = item.replace(\", '\",\"; '\")\n    item= item.replace(\"{\",\"\")\n    item= item.replace(\"},\",\";\")\n    item= item.replace(\"[\",\"\")\n    item= item.replace(\"]\",\"\")\n    item= item.replace(\"'\",\"\")\n    \n    #item = item.replace(\",\",\"\")\n    item = item.replace('Jimmy;,','Jimmy')\n    item = item.replace('Elektra Quartet;','Elektra Quartet')\n    \n    item_ls = item.split(';')\n    i = 0\n    ls = []\n    for it in item_ls:\n        i += 1\n        \n        it = it.replace('[','')\n        it = it.replace(']','')\n        it_ls = it.split(':')\n        ls.append(str(it_ls[1]).strip())\n    \n        if i % 7 == 0:\n            #print (imdb_id[x])\n            ls.append(imdb_id[x])\n            main_ls.append(ls)\n            ls = []\n    x += 1\n\ncrew_df = DataFrame.from_records(main_ls)\ncrew_df.columns = ['credit_id','department','gender','id','job','name','profile_path','imdb_id']\n\n\n\"\"\"\nTest data\n\"\"\"\nimdb_id = test_df['imdb_id']\ncast = test_df['crew']\n\nmain_ls = []\nx = 0\ni = 0\nfor item in cast:\n    i+= 1\n    ls = []\n    #print (item)\n    \n    if str(item) == '[]':\n        main_ls.append(['nan','nan','nan','nan','nan','nan','nan'])\n        x += 1\n        continue\n    \n    item = str(item)\n    item = item.replace(\"', '\",\"'; '\")\n    item = item.replace(\", '\",\"; '\")\n    item= item.replace(\"{\",\"\")\n    item= item.replace(\"},\",\";\")\n    item= item.replace(\"[\",\"\")\n    item= item.replace(\"]\",\"\")\n    item= item.replace(\"'\",\"\")\n    item = item.replace('Jimmy;,','Jimmy')\n    item = item.replace('Elektra Quartet;','Elektra Quartet')\n    \n    item_ls = item.split(';')\n    i = 0\n    ls = []\n    for it in item_ls:\n        i += 1\n        \n        it = it.replace('[','')\n        it = it.replace(']','')\n        \n        it_ls = it.split(':')\n        ls.append(str(it_ls[1]).strip())\n        \n        if i % 7 == 0:\n            #print (imdb_id[x])\n            ls.append(imdb_id[x])\n            main_ls.append(ls)\n            ls = []\n    x += 1\n\ncrew_df_test = DataFrame.from_records(main_ls)\ncrew_df_test.columns = ['credit_id','department','gender','id','job','name','profile_path','imdb_id']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='five'></a>\n> **5. Create new features from cast flat file**\n\na. Count of cast by movie <br />\nb. Count of cast by gender and movie <br />\nc. Gender of the top 3 actors by movie (to check the hypothesis of the effect of male lead roles versus women lead roles). This can be tuned by hyper-parameter to get more top actor features <br/>\nd. Count of top actors in a movie as defined by IMDB (see the external dataset attached to this kernel) <br />\n\n**Pivot Tables** in pandas were used throughout the process to get the counts by the grouping key, with the movie id remaining in the row key for joins purposes"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nMerge cast features:\n1) Count by movie\n2) Count of gender by movie\n3) Count of gender by cast/crew attributes & movie\n4) count of top actors in a movie as defined by iMDB\n5) gender of top 3 actors in the movie defined by order \n\"\"\"\n\n\"\"\"\nTrain\n\"\"\"\n\n# Cast counts by movie\ncast_counts = cast_df[['cast_id','imdb_id']].groupby('imdb_id').agg('count')\ncast_counts.columns = ['cast_counts']\ncast_counts.reset_index(inplace=True)\n\n\ndf_train= train_df_new.merge(cast_counts, how='left',left_on='imdb_id',right_on='imdb_id')\n\n\n\n# Cast gender counts by movie\ncast_gender_counts = cast_df[['cast_id', 'gender','imdb_id']].groupby(['imdb_id','gender'],as_index=False).agg('count')\ncast_gender_counts = cast_gender_counts.pivot(index='imdb_id',columns='gender',values='cast_id')\ncast_gender_counts.reset_index(inplace=True)\ncast_gender_counts.fillna(0,inplace=True) # reasonable assumption\ncast_gender_counts.columns=['imdb_id','gender_cast_count_0','gender_cast_count_1','gender_cast_count_2']\ndf_train = df_train.merge(cast_gender_counts,how='left',left_on='imdb_id',right_on='imdb_id')\n\n\n# Get gender of top 3 actors\ncast_gender_top_3 = cast_df[['order','gender','imdb_id']]\ncast_gender_top_3['int_order'] = cast_gender_top_3.order.astype(int)\ncast_gender_top_3 = cast_gender_top_3.loc[cast_gender_top_3['int_order'] <= 2,]\ncast_gender_top_3.drop('order',inplace=True,axis=1)\ncast_gender_top_3 = cast_gender_top_3.pivot_table(index='imdb_id',columns='int_order',values='gender',aggfunc='first')\ncast_gender_top_3.reset_index(inplace=True)\ncast_gender_top_3.columns=['imdb_id','cast_1_gender','cast_2_gender','cast_3_gender']\ndf_train = df_train.merge(cast_gender_top_3,how='left',left_on='imdb_id',right_on='imdb_id')\n\n\n# Count of top actors per movie as defined by IMDB\ntop_actors = pd.read_csv('../input/top-1000-tmdb-actors/Top 1000 Actors and Actresses.csv',encoding='iso-8859-1')\ntop_actors_name = top_actors[['Name']]\ncast_df = cast_df.merge(top_actors_name,how='left',left_on='name',right_on='Name')\n\nprint (cast_df.head(10))\n\ncast_top_actors = cast_df[['imdb_id','Name']]\ncast_top_actors = cast_top_actors.dropna()\n\ncast_top_actors_2= cast_top_actors.groupby(\"imdb_id\").count()\ncast_top_actors_2.reset_index(inplace=True)\ncast_top_actors_2.columns=['imdb_id','count_top_actors']\ndf_train = df_train.merge(cast_top_actors_2,how='left',left_on='imdb_id',right_on='imdb_id')\n\n\"\"\"\nTest\n\"\"\"\n\n# Cast counts by movie\ncast_counts = cast_df_test[['cast_id','imdb_id']].groupby('imdb_id').agg('count')\ncast_counts.columns = ['cast_counts']\ncast_counts.reset_index(inplace=True)\ndf_test= test_df_new.merge(cast_counts, how='left',left_on='imdb_id',right_on='imdb_id')\n\n# Cast gender counts by movie\ncast_gender_counts = cast_df_test[['cast_id', 'gender','imdb_id']].groupby(['imdb_id','gender'],as_index=False).agg('count')\ncast_gender_counts = cast_gender_counts.pivot(index='imdb_id',columns='gender',values='cast_id')\ncast_gender_counts.reset_index(inplace=True)\ncast_gender_counts.fillna(0,inplace=True) # reasonable assumption\ncast_gender_counts.columns=['imdb_id','gender_cast_count_0','gender_cast_count_1','gender_cast_count_2']\ndf_test = df_test.merge(cast_gender_counts,how='left',left_on='imdb_id',right_on='imdb_id')\n\n\n# Get gender of top 3 actors\ncast_gender_top_3 = cast_df_test[['order','gender','imdb_id']]\ncast_gender_top_3['int_order'] = cast_gender_top_3.order.astype(int)\ncast_gender_top_3 = cast_gender_top_3.loc[cast_gender_top_3['int_order'] <= 2,]\ncast_gender_top_3.drop('order',inplace=True,axis=1)\ncast_gender_top_3 = cast_gender_top_3.pivot_table(index='imdb_id',columns='int_order',values='gender',aggfunc='first')\ncast_gender_top_3.reset_index(inplace=True)\ncast_gender_top_3.columns=['imdb_id','cast_1_gender','cast_2_gender','cast_3_gender']\ndf_test = df_test.merge(cast_gender_top_3,how='left',left_on='imdb_id',right_on='imdb_id')\n\n\n# Count of top actors per movie as defined by IMDB\ntop_actors = pd.read_csv('../input/top-1000-tmdb-actors/Top 1000 Actors and Actresses.csv',encoding='iso-8859-1')\ntop_actors_name = top_actors[['Name']]\ncast_df_test = cast_df_test.merge(top_actors_name,how='left',left_on='name',right_on='Name')\n\ncast_top_actors = cast_df_test[['imdb_id','Name']]\ncast_top_actors = cast_top_actors.dropna()\n\ncast_top_actors_2= cast_top_actors.groupby(\"imdb_id\").count()\ncast_top_actors_2.reset_index(inplace=True)\ncast_top_actors_2.columns=['imdb_id','count_top_actors']\ndf_test = df_test.merge(cast_top_actors_2,how='left',left_on='imdb_id',right_on='imdb_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='six'></a>\n\n> **6. Create new features from crew flat file**\n\na. Count of crew by movie <br />\nb. Count of crew by gender and movie <br />\nc. Count of crew by gender, movie, and department <br />\nd. Count of crew by job and movie <br />\ne. Count of crew by department and movie <br/>\n\nThe count of crew by job, gender, and movie was not done due to the explosion of features this would result in.\n\n**Pivot Tables** in pandas were used throughout the process to get the counts by the grouping key, with the movie id remaining in the row key for joins purposes"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nMerge crew features:\n1) Count by movie\n2) Count of gender by movie\n3) Count of gender by cast/crew attributes & movie\n4) count of top actors in a movie as defined by iMDB\n5) gender of top 3 actors in the movie defined by order \n\"\"\"\n\n\"\"\"\nTrain\n\"\"\"\n\n# Crew counts by movie\ncast_counts = crew_df[['credit_id','imdb_id']].groupby('imdb_id').agg('count')\ncast_counts.columns = ['crew_counts']\ncast_counts.reset_index(inplace=True)\ndf_train= df_train.merge(cast_counts, how='left',left_on='imdb_id',right_on='imdb_id')\n#df_train.drop('credit_id',axis=1,inplace=True)\n\n# Crew gender counts by movie\ncast_gender_counts = crew_df[['credit_id', 'gender','imdb_id']].groupby(['imdb_id','gender'],as_index=False).agg('count')\ncast_gender_counts = cast_gender_counts.pivot(index='imdb_id',columns='gender',values='credit_id')\ncast_gender_counts.reset_index(inplace=True)\ncast_gender_counts.fillna(0,inplace=True) # reasonable assumption\ncast_gender_counts.columns=['imdb_id','gender_crew_count_0','gender_crew_count_1','gender_crew_count_2']\ndf_train = df_train.merge(cast_gender_counts,how='left',left_on='imdb_id',right_on='imdb_id')\n\n# Crew gender counts by movie and dept\ncast_gender_counts = crew_df[['credit_id', 'gender','imdb_id','department']].groupby(['imdb_id','gender','department'],as_index=False).agg('count')\ncast_gender_counts = cast_gender_counts.pivot_table(index='imdb_id',columns=['gender','department'],values='credit_id')\ncast_gender_counts.reset_index(inplace=True)\ncast_gender_counts.fillna(0,inplace=True) # reasonable assumption\ndf_train = df_train.merge(cast_gender_counts,how='left',left_on='imdb_id',right_on='imdb_id')\n\n# Crew job counts by movie\ncrew_job_counts = crew_df[['credit_id', 'job','imdb_id']].groupby(['imdb_id','job'],as_index=False).agg('count')\ncrew_job_counts = crew_job_counts.pivot(index='imdb_id',columns='job',values='credit_id')\ncrew_job_counts.reset_index(inplace=True)\ncrew_job_counts.fillna(0,inplace=True) # reasonable assumption\ndf_train = df_train.merge(crew_job_counts,how='left',left_on='imdb_id',right_on='imdb_id')\n\n# Crew department counts by movie\ncrew_dept_counts = crew_df[['credit_id', 'department','imdb_id']].groupby(['imdb_id','department'],as_index=False).agg('count')\ncrew_dept_counts = crew_dept_counts.pivot(index='imdb_id',columns='department',values='credit_id')\ncrew_dept_counts.reset_index(inplace=True)\ncrew_dept_counts.fillna(0,inplace=True) # reasonable assumption\ndf_train = df_train.merge(crew_dept_counts,how='left',left_on='imdb_id',right_on='imdb_id')\n\n\"\"\"\nTest\n\"\"\"\n\n\n# Crew counts by movie\ncast_counts = crew_df_test[['credit_id','imdb_id']].groupby('imdb_id').agg('count')\ncast_counts.columns = ['crew_counts']\ncast_counts.reset_index(inplace=True)\ndf_test= df_test.merge(cast_counts, how='left',left_on='imdb_id',right_on='imdb_id')\n#df_test.drop('credit_id',axis=1,inplace=True)\n\n# Crew gender counts by movie\ncast_gender_counts = crew_df_test[['credit_id', 'gender','imdb_id']].groupby(['imdb_id','gender'],as_index=False).agg('count')\ncast_gender_counts = cast_gender_counts.pivot(index='imdb_id',columns='gender',values='credit_id')\ncast_gender_counts.reset_index(inplace=True)\ncast_gender_counts.fillna(0,inplace=True) # reasonable assumption\ncast_gender_counts.columns=['imdb_id','gender_crew_count_0','gender_crew_count_1','gender_crew_count_2']\ndf_test = df_test.merge(cast_gender_counts,how='left',left_on='imdb_id',right_on='imdb_id')\n\n# Crew gender counts by movie and dept\ncast_gender_counts = crew_df_test[['credit_id', 'gender','imdb_id','department']].groupby(['imdb_id','gender','department'],as_index=False).agg('count')\ncast_gender_counts = cast_gender_counts.pivot_table(index='imdb_id',columns=['gender','department'],values='credit_id')\ncast_gender_counts.reset_index(inplace=True)\ncast_gender_counts.fillna(0,inplace=True) # reasonable assumption\ndf_test = df_test.merge(cast_gender_counts,how='left',left_on='imdb_id',right_on='imdb_id')\n\n# Crew job counts by movie\ncrew_job_counts = crew_df_test[['credit_id', 'job','imdb_id']].groupby(['imdb_id','job'],as_index=False).agg('count')\ncrew_job_counts = crew_job_counts.pivot(index='imdb_id',columns='job',values='credit_id')\ncrew_job_counts.reset_index(inplace=True)\ncrew_job_counts.fillna(0,inplace=True) # reasonable assumption\ndf_test = df_test.merge(crew_job_counts,how='left',left_on='imdb_id',right_on='imdb_id')\n\n# Crew department counts by movie\ncrew_dept_counts = crew_df_test[['credit_id', 'department','imdb_id']].groupby(['imdb_id','department'],as_index=False).agg('count')\ncrew_dept_counts = crew_dept_counts.pivot(index='imdb_id',columns='department',values='credit_id')\ncrew_dept_counts.reset_index(inplace=True)\ncrew_dept_counts.fillna(0,inplace=True) # reasonable assumption\ndf_test = df_test.merge(crew_dept_counts,how='left',left_on='imdb_id',right_on='imdb_id')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='seven'></a>\n\n> **7. Get characters and their actors as one-hot encoded features, that appeared at least 'x' number of times in the entire dataset**\n\nThe 'x' can be controlled by tuning. Here, x is 40."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nGet top characters and their actors that appeared at least 5 times in the dataset\n\"\"\"\n\ncharacter = cast_df['character']\ncast = cast_df['name']\n\ncharacter.fillna('x',inplace=True)\ncast.fillna('x',inplace=True)\ncharacter_counts = Counter(character)\ncast_counts = Counter(cast)\n\ncounter_char = {x: character_counts[x] for x in character_counts if character_counts[x] >= 40}\ncounter_cast = {x: cast_counts[x] for x in cast_counts if cast_counts[x] >= 40}\n\ncharacters = [x for x in counter_char]\ncasts = [x for x in counter_cast]\n\ncast_top = cast_df[cast_df['name'].isin(casts)]\ncast_top_counts = cast_top[['imdb_id','name','cast_id']].groupby(['imdb_id','name']).agg('count')\ncast_top_counts.reset_index(inplace=True)\n\n\nchar_top = cast_df[cast_df['character'].isin(characters)]\nchar_top_counts = char_top[['imdb_id','character','cast_id']].groupby(['imdb_id','character']).agg('count')\nchar_top_counts.reset_index(inplace=True)\n\n\ncast_top_test = cast_df_test[cast_df_test['name'].isin(casts)]\ncast_top_test_counts = cast_top_test[['imdb_id','name','cast_id']].groupby(['imdb_id','name']).agg('count')\ncast_top_test_counts.reset_index(inplace=True)\n\n\nchar_top_test = cast_df_test[cast_df_test['character'].isin(characters)]\nchar_top_test_counts = char_top_test[['imdb_id','character','cast_id']].groupby(['imdb_id','character']).agg('count')\nchar_top_test_counts.reset_index(inplace=True)\n\n\ncast_top_counts = cast_top_counts.pivot(index='imdb_id',columns='name',values='cast_id')\ncast_top_counts.fillna(0,inplace=True)\n\ncast_top_counts.reset_index(inplace=True)\ncast_top_test_counts = cast_top_test_counts.pivot(index='imdb_id',columns='name',values='cast_id')\ncast_top_test_counts.fillna(0,inplace=True)\n\ncast_top_test_counts.reset_index(inplace=True)\n\nchar_top_counts = char_top_counts.pivot(index='imdb_id',columns='character',values='cast_id')\nchar_top_counts.fillna(0,inplace=True)\n\nchar_top_counts.reset_index(inplace=True)\n\nchar_top_test_counts = char_top_test_counts.pivot(index='imdb_id',columns='character',values='cast_id')\nchar_top_test_counts.fillna(0,inplace=True)\n\nchar_top_test_counts.reset_index(inplace=True)\n\ndf_train = df_train.merge(char_top_counts,how='left',left_on='imdb_id',right_on='imdb_id')\ndf_train = df_train.merge(cast_top_counts,how='left',left_on='imdb_id',right_on='imdb_id')\ndf_train.fillna(0,inplace=True)\n\n\ndf_test = df_test.merge(char_top_test_counts,how='left',left_on='imdb_id',right_on='imdb_id')\ndf_test = df_test.merge(cast_top_test_counts,how='left',left_on='imdb_id',right_on='imdb_id')\ndf_test.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='eight'></a>\n\n> **8. Get crew as one-hot encoded features, that appeared at least 'x' number of times in the entire dataset**\n\nThe 'x' can be controlled by tuning. Here, x is 40. "},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nGet crew that appeared at least 5 times in the dataset\n\"\"\"\n\n#character = crew_df['job']\ncast = crew_df['name']\n\n#character.fillna('x',inplace=True)\ncast.fillna('x',inplace=True)\n#character_counts = Counter(character)\ncast_counts = Counter(cast)\n\n#counter_char = {x: character_counts[x] for x in character_counts if character_counts[x] >= 15}\ncounter_cast = {x: cast_counts[x] for x in cast_counts if cast_counts[x] >= 40}\n\n#characters = [x for x in counter_char]\ncasts = [x for x in counter_cast]\n\ncast_top = crew_df[crew_df['name'].isin(casts)]\ncast_top_counts = cast_top[['imdb_id','name','credit_id']].groupby(['imdb_id','name']).agg('count')\ncast_top_counts.reset_index(inplace=True)\n\n#char_top = crew_df[crew_df['job'].isin(characters)]\n#char_top_counts = char_top[['imdb_id','job','credit_id']].groupby(['imdb_id','job']).agg('count')\n#char_top_counts.reset_index(inplace=True)\n\ncast_top_test = crew_df_test[crew_df_test['name'].isin(casts)]\ncast_top_test_counts = cast_top_test[['imdb_id','name','credit_id']].groupby(['imdb_id','name']).agg('count')\ncast_top_test_counts.reset_index(inplace=True)\n\n#char_top_test = crew_df_test[crew_df_test['job'].isin(characters)]\n#char_top_test_counts = char_top_test[['imdb_id','job','credit_id']].groupby(['imdb_id','job']).agg('count')\n#char_top_test_counts.reset_index(inplace=True)\n\ncast_top_counts = cast_top_counts.pivot(index='imdb_id',columns='name',values='credit_id')\ncast_top_counts.fillna(0,inplace=True)\ncast_top_counts.reset_index(inplace=True)\ncast_top_test_counts = cast_top_test_counts.pivot(index='imdb_id',columns='name',values='credit_id')\ncast_top_test_counts.fillna(0,inplace=True)\ncast_top_test_counts.reset_index(inplace=True)\n\n\n#char_top_counts = char_top_counts.pivot(index='imdb_id',columns='job',values='credit_id')\n#char_top_counts.fillna(0,inplace=True)\n#char_top_counts.reset_index(inplace=True)\n#char_top_test_counts = char_top_test_counts.pivot(index='imdb_id',columns='job',values='credit_id')\n#char_top_test_counts.fillna(0,inplace=True)\n#char_top_test_counts.reset_index(inplace=True)\n\n#df_train = df_train.merge(char_top_counts,how='left',left_on='imdb_id',right_on='imdb_id')\ndf_train = df_train.merge(cast_top_counts,how='left',left_on='imdb_id',right_on='imdb_id')\ndf_train.fillna(0,inplace=True)\n\n\n#df_test = df_test.merge(char_top_test_counts,how='left',left_on='imdb_id',right_on='imdb_id')\ndf_test = df_test.merge(cast_top_test_counts,how='left',left_on='imdb_id',right_on='imdb_id')\ndf_test.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='nine'></a>\n\n> **9. Length and number of words in the original title, title, overview, and tagline**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['overview'].fillna('',inplace=True)\ndf_test['overview'].fillna('',inplace=True)\n\ndf_train['len_original_title'] = df_train.original_title.apply(str).apply(len)\ndf_test['len_original_title'] = df_test.original_title.apply(len)\n\ndf_train['len_overview'] = df_train.overview.apply(str).apply(len)\ndf_test['len_overview'] = df_test.overview.apply(str).apply(len)\n\ndf_train['len_tagline'] = df_train.tagline.apply(str).apply(len)\ndf_test['len_tagline'] = df_test.tagline.apply(str).apply(len)\n\ndf_train['len_title'] = df_train.title.apply(str).apply(len)\ndf_test['len_title'] = df_test.title.apply(str).apply(len)\n\ndf_train['num_words_title'] = df_train.title.apply(lambda x: len(str(x).split(' ')))\ndf_test['num_words_title'] = df_test.title.apply(lambda x: len(str(x).split(' ')))\n\ndf_train['num_words_original_title'] = df_train.original_title.apply(lambda x: len(str(x).split(' ')))\ndf_test['num_words_original_title'] = df_test.original_title.apply(lambda x: len(str(x).split(' ')))\n\ndf_train['num_words_tagline'] = df_train.tagline.apply(lambda x: len(str(x).split(' ')))\ndf_test['num_words_tagline'] = df_test.tagline.apply(lambda x: len(str(x).split(' ')))\n\ndf_train['num_words_overview'] = df_train.overview.apply(lambda x: len(str(x).split(' ')))\ndf_test['num_words_overview'] = df_test.overview.apply(lambda x: len(str(x).split(' ')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='ten'></a>\n\n> **10. Sentiment and Polarity of the overview and tagline of the movie**\n\nUnder the hypothesis that the more polar or sentimental the tagline / overview, the more / less interest in the movie"},{"metadata":{"trusted":true},"cell_type":"code","source":"def senti(x):\n    return TextBlob(x).sentiment\n\noverview_train = df_train['overview']\noverview_test = df_test['overview']\ntagline_train = df_train['tagline']\ntagline_test = df_test['tagline']\n\noverview_train = overview_train.apply(lambda x: \" \".join(str(x).lower() for x in str(x).split()))\noverview_train = overview_train.str.replace('[^\\w\\s]','')\noverview_test = overview_test.apply(lambda x: \" \".join(str(x).lower() for x in str(x).split()))\noverview_test = overview_test.str.replace('[^\\w\\s]','')\n\ntagline_train = tagline_train.apply(lambda x: \" \".join(str(x).lower() for x in str(x).split()))\ntagline_train = tagline_train.str.replace('[^\\w\\s]','')\ntagline_test = tagline_test.apply(lambda x: \" \".join(str(x).lower() for x in str(x).split()))\ntagline_test = tagline_test.str.replace('[^\\w\\s]','')\n\n\noverview_train = overview_train.apply(lambda x: \" \".join(x for x in str(x).split() if x not in stop))\noverview_test = overview_test.apply(lambda x: \" \".join(x for x in str(x).split() if x not in stop))\ntagline_train = tagline_train.apply(lambda x: \" \".join(x for x in str(x).split() if x not in stop))\ntagline_test = tagline_test.apply(lambda x: \" \".join(x for x in str(x).split() if x not in stop))\n\n\nst = PorterStemmer()\noverview_train = overview_train.apply(lambda x: \" \".join([st.stem(word) for word in str(x).split()]))\noverview_test = overview_test.apply(lambda x: \" \".join([st.stem(word) for word in str(x).split()]))\ntagline_train = tagline_train.apply(lambda x: \" \".join([st.stem(word) for word in str(x).split()]))\ntagline_test = tagline_test.apply(lambda x: \" \".join([st.stem(word) for word in str(x).split()]))\n\n\noverview_train_senti = overview_train.apply(senti)\noverview_test_senti = overview_test.apply(senti)\n\ntagline_train_senti = tagline_train.apply(senti)\ntagline_test_senti = tagline_test.apply(senti)\n\ndf_train['overview_senti_1'] = overview_train_senti.apply(lambda x: x[0])\ndf_train['overview_senti_2'] = overview_train_senti.apply(lambda x: x[1])\ndf_train['tagline_senti_1'] = tagline_train_senti.apply(lambda x: x[0])\ndf_train['tagline_senti_2'] = tagline_train_senti.apply(lambda x: x[1])\n\ndf_test['overview_senti_1'] = overview_test_senti.apply(lambda x: x[0])\ndf_test['overview_senti_2'] = overview_test_senti.apply(lambda x: x[1])\ndf_test['tagline_senti_1'] = tagline_test_senti.apply(lambda x: x[0])\ndf_test['tagline_senti_2'] = tagline_test_senti.apply(lambda x: x[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='eleven'></a>\n\n> **11. Date-based feature extraction (year, weekday, month, weekofyear, day, and quarter).**\n\nCode graciously borrowed from other kernels "},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating features based on dates\ndef process_date(df):\n    date_parts = [\"year\", \"weekday\", \"month\", 'weekofyear', 'day', 'quarter']\n    for part in date_parts:\n        part_col = 'release_date' + \"_\" + part\n        df[part_col] = getattr(df['release_date_2'].dt, part).astype(int)\n    \n    return df\n\n\ndef fix_date(x):\n    \"\"\"\n    Fixes dates which are in 20xx\n    \"\"\"\n    \n    \n    year = x.split('/')[2]\n    \n    if len(year) == 2:\n        if int(year) <= 19:\n            return x[:-2] + '20' + year\n        else:\n            return x[:-2] + '19' + year\n    else:\n        return x\n    \n    \ndf_train['release_date_2'] = df_train['release_date'].apply(lambda x: fix_date(x))\ndf_test['release_date_2'] = df_test['release_date'].apply(lambda x: fix_date(x) if x != 0 else np.NaN)\ndf_train['release_date_2'] = pd.to_datetime(df_train['release_date'])\ndf_test['release_date_2'] = pd.to_datetime(df_test['release_date'])\n\n\ndf_train = process_date(df_train)\ndf_test = process_date(df_test)\n\ndf_train.drop('release_date_2',axis=1,inplace=True)\ndf_test.drop('release_date_2',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> <a id='twelve'></a>\n> **12. Movie rank by release year**\n\nTesting the hypothesis that movie sequels exist only because the original movie was popular, and that the sequels may be eagerly anticipated."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sub = df_train[['id','collection','release_date_year']]\ntrain_sub = train_sub[train_sub['collection'] != 'x']\n\n\n\ntrain_sub['movie_rank'] = train_sub.groupby('collection')['release_date_year'].rank(ascending=True)\n\ntrain_sub.drop('collection',inplace=True,axis=1)\ntrain_sub.drop('release_date_year',inplace=True,axis=1)\n\n\ndf_train = df_train.merge(train_sub,how='left',left_on='id',right_on='id')\n\n\ntest_sub = df_test[['id','collection','release_date_year']]\ntest_sub = test_sub[test_sub['collection'] != 'x']\n\ntest_sub['movie_rank'] = test_sub.groupby('collection')['release_date_year'].rank(ascending=True)\ntest_sub.drop('collection',inplace=True,axis=1)\ntest_sub.drop('release_date_year',inplace=True,axis=1)\ndf_test = df_test.merge(test_sub,how='left',left_on='id',right_on='id')\n\ndf_train.loc[df_train.collection == 'x','movie_rank'] = np.NaN \ndf_test.loc[df_test.collection == 'x','movie_rank'] = np.NaN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='thirteen'></a>\n> **13. Label Encode the collection and original language features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle2 = LabelEncoder()\nle3 = LabelEncoder()\n\nmerged_collection = pd.concat([df_train.collection,df_test.collection])\nmerged_language = merged_df['original_language']\n\nle.fit(merged_collection)\nle2.fit(merged_language)\n\ntrain_collection = df_train['collection']\ntest_collection = df_test['collection']\n\ntrain_language = df_train['original_language']\ntest_language = df_test['original_language']\n\ntrain_collection_labels = le.transform(train_collection)\ntest_collection_labels = le.transform(test_collection)\n\ntrain_language_labels = le2.transform(train_language)\ntest_language_labels = le2.transform(test_language)\n\ndf_train['collection_encoded'] = train_collection_labels\ndf_test['collection_encoded'] = test_collection_labels\ndf_train['original_language_encoded'] = train_language_labels\ndf_test['original_language_encoded'] = test_language_labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='fourteen'></a>\n> **14. Add some bag-of-words features for the overview and title **\n\nBag-of-words features are enabled for words that occur at least 30 times in the training corpus only"},{"metadata":{"trusted":true},"cell_type":"code","source":"import collections, re\nfrom nltk.corpus import stopwords\n\nstop_english = stopwords.words('english')\nls_all_words = []\nls_all_words_tagline = []\ncount_words_param = 30\n\n\n\"\"\"\nTrain\n\"\"\"\n\n#dict_corpus_overview = {}\ncorpus_overview = df_train['overview']\ncorpus_overview_list = corpus_overview.tolist()\n\ncorpus_overview = [str(re.sub(' +',' ',str(txt))) for txt in corpus_overview_list]\ncorpus_overview = [str(re.sub('[^A-Za-z0-9\\' ]+','',str(txt))) for txt in corpus_overview]\ncorpus_overview = [str(txt).lower() for txt in corpus_overview]\nbagofwords = [collections.Counter(re.findall(r'\\w+',txt)) for txt in corpus_overview]\nsumbags = sum(bagofwords,collections.Counter())\nsumbags_filter =  {x : sumbags[x] for x in sumbags if sumbags[x] >= count_words_param} # words that appeared at least x times in corpus\n\n\nfor ele in sumbags_filter:\n    if (ele not in stop_english and ele not in ls_all_words):\n        ls_all_words.append(ele)\n\nlist_index = list(range(len(df_train)))\ndf = pd.DataFrame(0, index=list_index, columns=ls_all_words)\n\ni = 0\nfor item in bagofwords:\n    for ele in item:\n        if (ele in ls_all_words):\n            df.set_value(i,ele,item[ele])\n    i += 1\n\n    \ndf_train = pd.concat([df_train.reset_index(drop=True),df],axis=1)\n\n\ncorpus_overview = df_train['tagline']\ncorpus_overview_list = corpus_overview.tolist()\n\ncorpus_overview = [str(re.sub(' +',' ',str(txt))) for txt in corpus_overview_list]\ncorpus_overview = [str(re.sub('[^A-Za-z0-9\\' ]+','',str(txt))) for txt in corpus_overview]\ncorpus_overview = [str(txt).lower() for txt in corpus_overview]\nbagofwords = [collections.Counter(re.findall(r'\\w+',txt)) for txt in corpus_overview]\nsumbags = sum(bagofwords,collections.Counter())\nsumbags_filter =  {x : sumbags[x] for x in sumbags if sumbags[x] >= count_words_param} # words that appeared at least x times in corpus\n\nfor ele in sumbags_filter:\n    if (ele not in stop_english and ele not in ls_all_words_tagline):\n        ls_all_words_tagline.append(ele)\n\nlist_index = list(range(len(df_train)))\ndf = pd.DataFrame(0, index=list_index, columns=ls_all_words_tagline)\n\ni = 0\nfor item in bagofwords:\n    for ele in item:\n        if (ele in ls_all_words_tagline):\n            df.set_value(i,ele,item[ele])\n    i += 1\n\n    \ndf_train = pd.concat([df_train.reset_index(drop=True),df],axis=1)\n\n\n\"\"\"\nTest\n\"\"\" \n\ncorpus_overview_test = df_test['overview']\ncorpus_overview_test_list = corpus_overview_test.tolist()\n\ncorpus_overview_test = [str(re.sub(' +',' ',str(txt))) for txt in corpus_overview_test_list]\ncorpus_overview_test = [str(re.sub('[^A-Za-z0-9\\' ]+','',str(txt))) for txt in corpus_overview_test]\ncorpus_overview_test = [str(txt).lower() for txt in corpus_overview_test]\nbagofwords = [collections.Counter(re.findall(r'\\w+',txt)) for txt in corpus_overview_test]\nsumbags = sum(bagofwords,collections.Counter())\nsumbags_filter =  {x : sumbags[x] for x in sumbags if sumbags[x] >= count_words_param} # words that appeared at least x times in corpus\n\n#for ele in sumbags_filter:\n#    if (ele not in stop_english and ele not in ls_all_words):\n#        ls_all_words.append(ele)\n\nlist_index = list(range(len(df_test)))\ndf = pd.DataFrame(0, index=list_index, columns=ls_all_words) # reuse training words only\n\ni = 0\nfor item in bagofwords:\n    for ele in item:\n        if (ele in ls_all_words):\n            df.set_value(i,ele,item[ele])\n    i += 1\n\n\ndf_test = pd.concat([df_test.reset_index(drop=True),df],axis=1)\n\ncorpus_overview_test = df_test['tagline']\ncorpus_overview_test_list = corpus_overview_test.tolist()\n\ncorpus_overview_test = [str(re.sub(' +',' ',str(txt))) for txt in corpus_overview_test_list]\ncorpus_overview_test = [str(re.sub('[^A-Za-z0-9\\' ]+','',str(txt))) for txt in corpus_overview_test]\ncorpus_overview_test = [str(txt).lower() for txt in corpus_overview_test]\nbagofwords = [collections.Counter(re.findall(r'\\w+',txt)) for txt in corpus_overview_test]\nsumbags = sum(bagofwords,collections.Counter())\nsumbags_filter =  {x : sumbags[x] for x in sumbags if sumbags[x] >= count_words_param} # words that appeared at least x times in corpus\n\n#for ele in sumbags_filter:\n#    if (ele not in stop_english and ele not in ls_all_words):\n#        ls_all_words.append(ele)\n\nlist_index = list(range(len(df_test)))\ndf = pd.DataFrame(0, index=list_index, columns=ls_all_words_tagline) # reuse training words only\n\ni = 0\nfor item in bagofwords:\n    for ele in item:\n        if (ele in ls_all_words_tagline):\n            df.set_value(i,ele,item[ele])\n    i += 1\n\n\ndf_test = pd.concat([df_test.reset_index(drop=True),df],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.loc[df_test['id'] == 3033,'budget'] = 250 \ndf_test.loc[df_test['id'] == 3051,'budget'] = 50\ndf_test.loc[df_test['id'] == 3084,'budget'] = 337\ndf_test.loc[df_test['id'] == 3224,'budget'] = 4  \ndf_test.loc[df_test['id'] == 3594,'budget'] = 25  \ndf_test.loc[df_test['id'] == 3619,'budget'] = 500  \ndf_test.loc[df_test['id'] == 3831,'budget'] = 3  \ndf_test.loc[df_test['id'] == 3935,'budget'] = 500  \ndf_test.loc[df_test['id'] == 4049,'budget'] = 995946 \ndf_test.loc[df_test['id'] == 4424,'budget'] = 3  \ndf_test.loc[df_test['id'] == 4460,'budget'] = 8  \ndf_test.loc[df_test['id'] == 4555,'budget'] = 1200000 \ndf_test.loc[df_test['id'] == 4624,'budget'] = 30 \ndf_test.loc[df_test['id'] == 4645,'budget'] = 500 \ndf_test.loc[df_test['id'] == 4709,'budget'] = 450 \ndf_test.loc[df_test['id'] == 4839,'budget'] = 7\ndf_test.loc[df_test['id'] == 3125,'budget'] = 25 \ndf_test.loc[df_test['id'] == 3142,'budget'] = 1\ndf_test.loc[df_test['id'] == 3201,'budget'] = 450\ndf_test.loc[df_test['id'] == 3222,'budget'] = 6\ndf_test.loc[df_test['id'] == 3545,'budget'] = 38\ndf_test.loc[df_test['id'] == 3670,'budget'] = 18\ndf_test.loc[df_test['id'] == 3792,'budget'] = 19\ndf_test.loc[df_test['id'] == 3881,'budget'] = 7\ndf_test.loc[df_test['id'] == 3969,'budget'] = 400\ndf_test.loc[df_test['id'] == 4196,'budget'] = 6\ndf_test.loc[df_test['id'] == 4221,'budget'] = 11\ndf_test.loc[df_test['id'] == 4222,'budget'] = 500\ndf_test.loc[df_test['id'] == 4285,'budget'] = 11\ndf_test.loc[df_test['id'] == 4319,'budget'] = 1\ndf_test.loc[df_test['id'] == 4639,'budget'] = 10\ndf_test.loc[df_test['id'] == 4719,'budget'] = 45\ndf_test.loc[df_test['id'] == 4822,'budget'] = 22\ndf_test.loc[df_test['id'] == 4829,'budget'] = 20\ndf_test.loc[df_test['id'] == 4969,'budget'] = 20\ndf_test.loc[df_test['id'] == 5021,'budget'] = 40 \ndf_test.loc[df_test['id'] == 5035,'budget'] = 1 \ndf_test.loc[df_test['id'] == 5063,'budget'] = 14 \ndf_test.loc[df_test['id'] == 5119,'budget'] = 2 \ndf_test.loc[df_test['id'] == 5214,'budget'] = 30 \ndf_test.loc[df_test['id'] == 5221,'budget'] = 50 \ndf_test.loc[df_test['id'] == 4903,'budget'] = 15\ndf_test.loc[df_test['id'] == 4983,'budget'] = 3\ndf_test.loc[df_test['id'] == 5102,'budget'] = 28\ndf_test.loc[df_test['id'] == 5217,'budget'] = 75\ndf_test.loc[df_test['id'] == 5224,'budget'] = 3 \ndf_test.loc[df_test['id'] == 5469,'budget'] = 20 \ndf_test.loc[df_test['id'] == 5840,'budget'] = 1 \ndf_test.loc[df_test['id'] == 5960,'budget'] = 30\ndf_test.loc[df_test['id'] == 6506,'budget'] = 11 \ndf_test.loc[df_test['id'] == 6553,'budget'] = 280\ndf_test.loc[df_test['id'] == 6561,'budget'] = 7\ndf_test.loc[df_test['id'] == 6582,'budget'] = 218\ndf_test.loc[df_test['id'] == 6638,'budget'] = 5\ndf_test.loc[df_test['id'] == 6749,'budget'] = 8 \ndf_test.loc[df_test['id'] == 6759,'budget'] = 50 \ndf_test.loc[df_test['id'] == 6856,'budget'] = 10\ndf_test.loc[df_test['id'] == 6858,'budget'] =  100\ndf_test.loc[df_test['id'] == 6876,'budget'] =  250\ndf_test.loc[df_test['id'] == 6972,'budget'] = 1\ndf_test.loc[df_test['id'] == 7079,'budget'] = 8000000\ndf_test.loc[df_test['id'] == 7150,'budget'] = 118\ndf_test.loc[df_test['id'] == 6506,'budget'] = 118\ndf_test.loc[df_test['id'] == 7225,'budget'] = 6\ndf_test.loc[df_test['id'] == 7231,'budget'] = 85\ndf_test.loc[df_test['id'] == 5222,'budget'] = 5\ndf_test.loc[df_test['id'] == 5322,'budget'] = 90\ndf_test.loc[df_test['id'] == 5350,'budget'] = 70\ndf_test.loc[df_test['id'] == 5378,'budget'] = 10\ndf_test.loc[df_test['id'] == 5545,'budget'] = 80\ndf_test.loc[df_test['id'] == 5810,'budget'] = 8\ndf_test.loc[df_test['id'] == 5926,'budget'] = 300\ndf_test.loc[df_test['id'] == 5927,'budget'] = 4\ndf_test.loc[df_test['id'] == 5986,'budget'] = 1\ndf_test.loc[df_test['id'] == 6053,'budget'] = 20\ndf_test.loc[df_test['id'] == 6104,'budget'] = 1\ndf_test.loc[df_test['id'] == 6130,'budget'] = 30\ndf_test.loc[df_test['id'] == 6301,'budget'] = 150\ndf_test.loc[df_test['id'] == 6276,'budget'] = 100\ndf_test.loc[df_test['id'] == 6473,'budget'] = 100\ndf_test.loc[df_test['id'] == 6842,'budget'] = 30\n\ndf_train.loc[df_train['id'] == 90,'budget'] = 30000000                  \ndf_train.loc[df_train['id'] == 118,'budget'] = 60000000       \ndf_train.loc[df_train['id'] == 149,'budget'] = 18000000       \ndf_train.loc[df_train['id'] == 464,'budget'] = 20000000       \ndf_train.loc[df_train['id'] == 470,'budget'] = 13000000       \ndf_train.loc[df_train['id'] == 513,'budget'] = 930000         \ndf_train.loc[df_train['id'] == 797,'budget'] = 8000000        \ndf_train.loc[df_train['id'] == 819,'budget'] = 90000000       \ndf_train.loc[df_train['id'] == 850,'budget'] = 90000000       \ndf_train.loc[df_train['id'] == 1007,'budget'] = 2              \ndf_train.loc[df_train['id'] == 1112,'budget'] = 7500000       \ndf_train.loc[df_train['id'] == 1131,'budget'] = 4300000        \ndf_train.loc[df_train['id'] == 1359,'budget'] = 10000000       \ndf_train.loc[df_train['id'] == 1542,'budget'] = 1             \ndf_train.loc[df_train['id'] == 1570,'budget'] = 15800000       \ndf_train.loc[df_train['id'] == 1571,'budget'] = 4000000        \ndf_train.loc[df_train['id'] == 1714,'budget'] = 46000000       \ndf_train.loc[df_train['id'] == 1721,'budget'] = 17500000       \n   \ndf_train.loc[df_train['id'] == 1885,'budget'] = 12             \ndf_train.loc[df_train['id'] == 2091,'budget'] = 10             \ndf_train.loc[df_train['id'] == 2268,'budget'] = 17500000       \ndf_train.loc[df_train['id'] == 2491,'budget'] = 6              \ndf_train.loc[df_train['id'] == 2602,'budget'] = 31000000       \ndf_train.loc[df_train['id'] == 2612,'budget'] = 15000000       \ndf_train.loc[df_train['id'] == 2696,'budget'] = 10000000      \ndf_train.loc[df_train['id'] == 2801,'budget'] = 10000000       \ndf_train.loc[df_train['id'] == 335,'budget'] = 2 \ndf_train.loc[df_train['id'] == 348,'budget'] = 12\ndf_train.loc[df_train['id'] == 470,'budget'] = 13000000 \ndf_train.loc[df_train['id'] == 513,'budget'] = 1100000\ndf_train.loc[df_train['id'] == 640,'budget'] = 6 \ndf_train.loc[df_train['id'] == 696,'budget'] = 1\ndf_train.loc[df_train['id'] == 797,'budget'] = 8000000 \ndf_train.loc[df_train['id'] == 850,'budget'] = 1500000\ndf_train.loc[df_train['id'] == 1199,'budget'] = 5 \ndf_train.loc[df_train['id'] == 1282,'budget'] = 9              \ndf_train.loc[df_train['id'] == 1347,'budget'] = 1\ndf_train.loc[df_train['id'] == 1755,'budget'] = 2\ndf_train.loc[df_train['id'] == 1801,'budget'] = 5\ndf_train.loc[df_train['id'] == 1918,'budget'] = 592 \ndf_train.loc[df_train['id'] == 2033,'budget'] = 4\ndf_train.loc[df_train['id'] == 2118,'budget'] = 344 \ndf_train.loc[df_train['id'] == 2252,'budget'] = 130\ndf_train.loc[df_train['id'] == 2256,'budget'] = 1 \ndf_train.loc[df_train['id'] == 2696,'budget'] = 10000000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='fifteen'></a>\n> **15. Align the train and test datasets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nThis is slightly cheating - features are known after the movie is released\n\"\"\"\n\ntrain_add_features = pd.read_csv('../input/tmdb-additional-features/TrainAdditionalFeatures.csv')\ntest_add_features = pd.read_csv('../input/tmdb-additional-features/TestAdditionalFeatures.csv')\n\ndf_train = df_train.merge(train_add_features, how='left',left_on='imdb_id',right_on='imdb_id')\ndf_test = df_test.merge(test_add_features, how='left',left_on='imdb_id',right_on='imdb_id')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_cols = df_train.columns\ntest_data_cols = df_test.columns\n\n\nfor col_test in test_data_cols:\n    if str(col_test) not in train_data_cols and str(col_test) != 'year_bucket' and not str(col_test).__contains__('0') and not str(col_test).__contains__('1') and not str(col_test).__contains__('2'):\n        print ('dropping ' + str(col_test))\n        df_test.drop(str(col_test),axis=1,inplace=True)\n\n\nfor col_test in train_data_cols:\n    if str(col_test) not in test_data_cols and str(col_test) != 'year_bucket' and not str(col_test).__contains__('0') and not str(col_test).__contains__('1') and not str(col_test).__contains__('2'):\n        if (str(col_test) == 'revenue'):\n            continue\n        print ('dropping ' + str(col_test))\n        df_train.drop(str(col_test),axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('The final shapes of train and test data')\nprint (df_train.shape)\nprint (df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_collection(row):\n    if (row == 0):\n        return 0\n    else:\n        return 1\n    \ndef is_homepage(row):\n    if (row == 0):\n        return 0\n    else:\n        return 1\n\ndf_train['is_collection'] = df_train.belongs_to_collection.apply(is_collection)\ndf_train['is_homepage'] = df_train.homepage.apply(is_homepage)\ndf_test['is_collection'] = df_test.belongs_to_collection.apply(is_collection)\ndf_test['is_homepage'] = df_test.homepage.apply(is_homepage)\n\ndf_train['has_production_comp'] = df_train.production_companies.apply(is_collection)\ndf_test['has_production_comp'] = df_test.production_companies.apply(is_collection)\n\ndf_train['has_production_count'] = df_train.production_countries.apply(is_collection)\ndf_test['has_production_count'] = df_test.production_countries.apply(is_collection)\n\ndf_train['has_keywords'] = df_train.Keywords.apply(is_collection)\ndf_test['has_keywords'] = df_test.Keywords.apply(is_collection)\n\ndf_train['has_tagline'] = df_train.tagline.apply(is_collection)\ndf_test['has_tagline'] = df_test.tagline.apply(is_collection)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop('belongs_to_collection',axis=1,inplace=True)\ndf_train.drop('collection',axis=1,inplace=True)\ndf_train.drop('genres',axis=1,inplace=True)\ndf_train.drop('homepage',axis=1,inplace=True)\ndf_train.drop('imdb_id',axis=1,inplace=True)\ndf_train.drop('original_language',axis=1,inplace=True)\ndf_train.drop('original_title',axis=1,inplace=True)\ndf_train.drop('overview',axis=1,inplace=True)\n\ndf_train.drop('poster_path',axis=1,inplace=True)\ndf_train.drop('production_companies',axis=1,inplace=True)\ndf_train.drop('production_countries',axis=1,inplace=True)\ndf_train.drop('release_date',axis=1,inplace=True)\n#df_train.drop('release_date_2',axis=1,inplace=True)\ndf_train.drop('spoken_languages',axis=1,inplace=True)\ndf_train.drop('status',axis=1,inplace=True)\ndf_train.drop('tagline',axis=1,inplace=True)\ndf_train.drop('title',axis=1,inplace=True)\ndf_train.drop('Keywords',axis=1,inplace=True)\ndf_train.drop('cast',axis=1,inplace=True)\ndf_train.drop('crew',axis=1,inplace=True)\n\ndf_test.drop('belongs_to_collection',axis=1,inplace=True)\ndf_test.drop('collection',axis=1,inplace=True)\ndf_test.drop('genres',axis=1,inplace=True)\ndf_test.drop('homepage',axis=1,inplace=True)\ndf_test.drop('imdb_id',axis=1,inplace=True)\ndf_test.drop('original_language',axis=1,inplace=True)\ndf_test.drop('original_title',axis=1,inplace=True)\ndf_test.drop('overview',axis=1,inplace=True)\n\ndf_test.drop('poster_path',axis=1,inplace=True)\ndf_test.drop('production_companies',axis=1,inplace=True)\ndf_test.drop('production_countries',axis=1,inplace=True)\ndf_test.drop('release_date',axis=1,inplace=True)\n#df_test.drop('release_date_2',axis=1,inplace=True)\ndf_test.drop('spoken_languages',axis=1,inplace=True)\ndf_test.drop('status',axis=1,inplace=True)\ndf_test.drop('tagline',axis=1,inplace=True)\ndf_test.drop('title',axis=1,inplace=True)\ndf_test.drop('Keywords',axis=1,inplace=True)\ndf_test.drop('cast',axis=1,inplace=True)\ndf_test.drop('crew',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['revenue'].fillna(0,inplace=True)\ndf_train['log_revenue'] = np.log(df_train['revenue'])\ndf_train['budget'].fillna(0,inplace=True)\ndf_train['log_budget'] = np.log(df_train['budget'])\n\ndf_train.drop('revenue',axis=1,inplace=True)\ndf_train.drop('budget',axis=1,inplace=True)\n\ndf_test['budget'].fillna(0,inplace=True)\ndf_test['log_budget'] = np.log(df_test['budget'])\ndf_test.drop('budget',axis=1,inplace=True)\n\ndf_train['budget_year'] = df_train['log_budget'] / df_train['release_date_year']\ndf_test['budget_year'] = df_test['log_budget'] / df_test['release_date_year']\n\ndf_train['popularity_year'] = df_train['popularity'] / df_train['release_date_year']\ndf_test['popularity_year'] = df_test['popularity'] / df_test['release_date_year']\n\ndf_train.runtime.fillna(0,inplace=True)\ndf_test.runtime.fillna(0,inplace=True)\nbins = [0, 40, 50, 60, 70, 80,90, 100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,350]\nlabels = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32]\ndf_train['binned_runtime'] = pd.cut(df_train['runtime'], bins=bins, labels=labels)\ndf_test['binned_runtime'] = pd.cut(df_test['runtime'], bins=bins, labels=labels)\n\ndf_train['binned_runtime'] = pd.to_numeric(df_train['binned_runtime'])\ndf_test['binned_runtime'] = pd.to_numeric(df_test['binned_runtime'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='sixteen'></a>\n> **16. Run the XGBoost **"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBRegressor\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\n\nnum_rounds = 10000\n\ny_label = df_train['log_revenue']\nX = df_train.drop('log_revenue',axis=1)\nX.drop('id',axis=1,inplace=True)\nX['cast_1_gender'] = X['cast_1_gender'].apply(int)\nX['cast_2_gender'] = X['cast_2_gender'].apply(int)\nX['cast_3_gender'] = X['cast_3_gender'].apply(int)\nX = X.replace([np.inf, -np.inf], 0).fillna(0)\nX.fillna(0,inplace=True)\n\n\n\nX_test = df_test.copy()\nX_test.fillna(0,inplace=True)\nX_test_id = df_test['id']\nX_test.drop('id',axis=1,inplace=True)\nX_test['cast_1_gender'] = X_test['cast_1_gender'].apply(int)\nX_test['cast_2_gender'] = X_test['cast_2_gender'].apply(int)\nX_test['cast_3_gender'] = X_test['cast_3_gender'].apply(int)\nX_test = X_test.replace([np.inf, -np.inf], 0).fillna(0)\nX_test.fillna(0,inplace=True)\n\nX = X.loc[:,~X.columns.duplicated()]\nX_test = X_test.loc[:,~X_test.columns.duplicated()]\n\ndtrain = xgb.DMatrix(data=X,label=y_label)\ndtest = xgb.DMatrix(data=X_test)\nwatchlist = [(dtest, 'eval'), (dtrain, 'train')]\n\nparam = {'eta':0.01, 'max_depth': 2, 'booster': 'gbtree', 'colsample_bytree' : 0.3,'subsample' : 0.9}\n\nres = xgb.cv(param,dtrain,num_rounds,10,metrics={'rmse'})\nprint ('cross validation')\nprint (res)\n\n\nbst = xgb.train(param,dtrain,num_rounds)\ny_pred = bst.predict(dtrain)\nprint ('Train rmsle')\nprint (mean_squared_error(y_label, y_pred) ** 0.5)\n\n\ny_pred_reg = bst.predict(dtest)\ny_pred_reg_exp = np.exp(y_pred_reg)\ny_pred_reg_exp = pd.DataFrame(y_pred_reg_exp)\ntest_preds = pd.concat([X_test_id,y_pred_reg_exp],axis=1)\ntest_preds.columns = ['id','revenue']\n#test_preds.head(10)\ntest_preds.to_csv('submission_xgb.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}