{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tmdb-box-office-prediction/train.csv')\ntest = pd.read_csv('../input/tmdb-box-office-prediction/test.csv')\ndataset = train.copy()\ndataset1 = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.loc[dataset['id'] == 16, 'revenue'] = 192864\ndataset.loc[dataset['id'] == 313, 'revenue'] = 12000000       \ndataset.loc[dataset['id'] == 451, 'revenue'] = 12000000\ndataset.loc[dataset['id'] == 1865, 'revenue'] = 25000000 \ndataset.loc[dataset['id'] == 2491, 'revenue'] = 6800000\ndataset.loc[dataset['id'] == 90, 'budget'] = 30000000                \ndataset.loc[dataset['id'] == 118, 'budget'] = 60000000  \ndataset.loc[dataset['id'] == 149, 'budget'] = 18000000  \ndataset.loc[dataset['id'] == 464, 'budget'] = 20000000       \ndataset.loc[dataset['id'] == 470, 'budget'] = 13000000      \ndataset.loc[dataset['id'] == 513, 'budget'] = 930000          \ndataset.loc[dataset['id'] == 797, 'budget'] = 8000000       \ndataset.loc[dataset['id'] == 819, 'budget'] = 90000000       \ndataset.loc[dataset['id'] == 850, 'budget'] = 90000000  \ndataset.loc[dataset['id'] == 1112, 'budget'] = 7500000  \ndataset.loc[dataset['id'] == 1131, 'budget'] = 4300000      \ndataset.loc[dataset['id'] == 1359, 'budget'] = 10000000      \ndataset.loc[dataset['id'] == 1542, 'budget'] = 1500000          \ndataset.loc[dataset['id'] == 1542, 'budget'] = 15800000      \ndataset.loc[dataset['id'] == 1571, 'budget'] = 4000000        \ndataset.loc[dataset['id'] == 1714, 'budget'] = 46000000       \ndataset.loc[dataset['id'] == 1721, 'budget'] = 17500000            \ndataset.loc[dataset['id'] == 2268, 'budget'] = 17500000      \ndataset.loc[dataset['id'] == 2602, 'budget'] = 31000000\ndataset.loc[dataset['id'] == 2612, 'budget'] = 15000000\ndataset.loc[dataset['id'] == 2696, 'budget'] = 10000000\ndataset.loc[dataset['id'] == 2801, 'budget'] = 10000000\ndataset.loc[dataset['id'] == 3889, 'budget'] = 15000000       \ndataset.loc[dataset['id'] == 6733, 'budget'] = 5000000     \ndataset.loc[dataset['id'] == 3197, 'budget'] = 8000000     \ndataset.loc[dataset['id'] == 6683, 'budget'] = 50000000     \ndataset.loc[dataset['id'] == 5704, 'budget'] = 4300000     \ndataset.loc[dataset['id'] == 6109, 'budget'] = 281756      \ndataset.loc[dataset['id'] == 7242, 'budget'] = 10000000     \ndataset.loc[dataset['id'] == 7021, 'budget'] = 17540562\ndataset.loc[dataset['id'] == 5591, 'budget'] = 4000000      \ndataset.loc[dataset['id'] == 4282, 'budget'] = 20000000\ndataset.loc[dataset['id'] == 391, 'runtime'] = 86 \ndataset.loc[dataset['id'] == 592, 'runtime'] = 90 \ndataset.loc[dataset['id'] == 925, 'runtime'] = 95 \ndataset.loc[dataset['id'] == 978, 'runtime'] = 93 \ndataset.loc[dataset['id'] == 1256, 'runtime'] = 92 \ndataset.loc[dataset['id'] == 1542, 'runtime'] = 93\ndataset.loc[dataset['id'] == 1875, 'runtime'] = 86 \ndataset.loc[dataset['id'] == 2151, 'runtime'] = 108\ndataset.loc[dataset['id'] == 2499, 'runtime'] = 108 \ndataset.loc[dataset['id'] == 2646, 'runtime'] = 98\ndataset.loc[dataset['id'] == 2786, 'runtime'] = 111\ndataset.loc[dataset['id'] == 2866, 'runtime'] = 96\ndataset.loc[dataset['id'] == 4074, 'runtime'] = 103 \ndataset.loc[dataset['id'] == 4222, 'runtime'] = 93\ndataset.loc[dataset['id'] == 4431, 'runtime'] = 100 \ndataset.loc[dataset['id'] == 5520, 'runtime'] = 86 \ndataset.loc[dataset['id'] == 5845, 'runtime'] = 83 \ndataset.loc[dataset['id'] == 5849, 'runtime'] = 140\ndataset.loc[dataset['id'] == 6210, 'runtime'] = 104\ndataset.loc[dataset['id'] == 6804, 'runtime'] = 145 \ndataset.loc[dataset['id'] == 7321, 'runtime'] = 87\ndataset.dropna()\ndataset.loc[dataset.release_date.isnull(), 'release_date'] = '05/01/2000'\n\ndataset['release_year'] = dataset.release_date.str.extract('\\S+/\\S+/(\\S+)', expand=False).astype(np.int16)\ndataset['release_month'] = dataset.release_date.str.extract('(\\S+)/\\S+/\\S+', expand=False).astype(np.int16)\ndataset['release_day'] = dataset.release_date.str.extract('\\S+/(\\S+)/\\S+', expand=False).astype(np.int16)\n\ndataset.loc[(21 <= dataset.release_year) & (dataset.release_year <= 99), 'release_year'] += 1900\ndataset.loc[dataset.release_year < 21, 'release_year'] += 2000\n\ndataset['release_date'] = pd.to_datetime(dataset.release_day.astype(str) + '-' + \n                                         dataset.release_month.astype(str) + '-' + \n                                         dataset.release_year.astype(str))\n\ndataset['release_weekday'] = dataset.release_date.dt.weekday + 1\ndataset['release_quarter'] = dataset.release_date.dt.quarter\nol_count = dataset['original_language'].value_counts()\nfor lang, count in ol_count.loc[ol_count > 80].iteritems():\n    feature = 'ol_' + lang\n    dataset[feature] = 0\n    dataset.loc[dataset.original_language == lang, feature] = 1\nthreshold = 80\nfor feature in ['genres', 'production_companies', 'production_countries', 'spoken_languages']:\n    dataset.loc[dataset[feature].isnull(), feature] = '{}'\n    dataset[feature] = dataset[feature].apply(lambda x: sorted([d['name'] for d in eval(x)]))\n    dataset['num_of_' + feature] = dataset[feature].apply(lambda x: len(x))\n    dataset[feature] = dataset[feature].apply(lambda x: ','.join(map(str, x)))\n    \n    tmp = dataset[feature].str.get_dummies(sep=',')\n    tmp = tmp.loc[:, tmp.sum() > threshold]\n    dataset = pd.concat([dataset, tmp], axis=1)\n    \ndataset['has_budget'] = 1\ndataset.loc[dataset.budget == 0, 'has_budget'] = 0\ndataset['has_collection'] = 1\ndataset.loc[dataset.belongs_to_collection.isnull(), 'has_collection'] = 0\ndataset['has_homepage'] = 1\ndataset.loc[dataset.homepage.isnull(), 'has_homepage'] = 0\ndataset['has_tagline'] = 1\ndataset.loc[dataset.tagline.isnull(), 'has_tagline'] = 0\n\nfor feature in ['Keywords', 'cast', 'crew']:\n    dataset.loc[dataset[feature].isnull(), feature] = '{}'\n    dataset['num_of_' + feature] = dataset[feature].apply(lambda x: len([d['name'] for d in eval(x)]))\n    \nscaler = MinMaxScaler()\nnumeric_features = ['runtime', 'budget', 'popularity', 'release_year', \n                    'release_month', 'release_day', 'release_quarter', \n                    'num_of_Keywords', 'num_of_cast', 'num_of_crew']\nfor feature in numeric_features:\n    if feature == 'budget':\n        dataset.loc[dataset[feature] == 0, feature] = np.nanmedian(dataset[feature].loc[dataset[feature] != 0])\n        dataset[feature] = np.log2(dataset[feature] + 1)\n    dataset.loc[dataset[feature].isnull(), feature] = np.nanmedian(dataset[feature])\n    dataset[feature] = scaler.fit_transform(dataset[feature].values.reshape(-1, 1))\n    \ndataset = dataset.drop(['id', 'belongs_to_collection', 'genres', 'homepage', 'imdb_id', \n                        'original_language', 'original_title', 'overview', 'poster_path', \n                        'production_companies', 'production_countries', 'release_date', \n                        'spoken_languages', 'status', 'tagline', 'title', 'Keywords', 'cast', 'crew'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset1.loc[dataset1['id'] == 16, 'revenue'] = 192864\ndataset1.loc[dataset1['id'] == 313, 'revenue'] = 12000000       \ndataset1.loc[dataset1['id'] == 451, 'revenue'] = 12000000\ndataset1.loc[dataset1['id'] == 1865, 'revenue'] = 25000000 \ndataset1.loc[dataset1['id'] == 2491, 'revenue'] = 6800000\ndataset1.loc[dataset1['id'] == 90, 'budget'] = 30000000                \ndataset1.loc[dataset1['id'] == 118, 'budget'] = 60000000  \ndataset1.loc[dataset1['id'] == 149, 'budget'] = 18000000  \ndataset1.loc[dataset1['id'] == 464, 'budget'] = 20000000       \ndataset1.loc[dataset1['id'] == 470, 'budget'] = 13000000      \ndataset1.loc[dataset1['id'] == 513, 'budget'] = 930000          \ndataset1.loc[dataset1['id'] == 797, 'budget'] = 8000000       \ndataset1.loc[dataset1['id'] == 819, 'budget'] = 90000000       \ndataset1.loc[dataset1['id'] == 850, 'budget'] = 90000000  \ndataset1.loc[dataset1['id'] == 1112, 'budget'] = 7500000  \ndataset1.loc[dataset1['id'] == 1131, 'budget'] = 4300000      \ndataset1.loc[dataset1['id'] == 1359, 'budget'] = 10000000      \ndataset1.loc[dataset1['id'] == 1542, 'budget'] = 1500000          \ndataset1.loc[dataset1['id'] == 1542, 'budget'] = 15800000      \ndataset1.loc[dataset1['id'] == 1571, 'budget'] = 4000000        \ndataset1.loc[dataset1['id'] == 1714, 'budget'] = 46000000       \ndataset1.loc[dataset1['id'] == 1721, 'budget'] = 17500000            \ndataset1.loc[dataset1['id'] == 2268, 'budget'] = 17500000      \ndataset1.loc[dataset1['id'] == 2602, 'budget'] = 31000000\ndataset1.loc[dataset1['id'] == 2612, 'budget'] = 15000000\ndataset1.loc[dataset1['id'] == 2696, 'budget'] = 10000000\ndataset1.loc[dataset1['id'] == 2801, 'budget'] = 10000000\ndataset1.loc[dataset1['id'] == 3889, 'budget'] = 15000000       \ndataset1.loc[dataset1['id'] == 6733, 'budget'] = 5000000     \ndataset1.loc[dataset1['id'] == 3197, 'budget'] = 8000000     \ndataset1.loc[dataset1['id'] == 6683, 'budget'] = 50000000     \ndataset1.loc[dataset1['id'] == 5704, 'budget'] = 4300000     \ndataset1.loc[dataset1['id'] == 6109, 'budget'] = 281756      \ndataset1.loc[dataset1['id'] == 7242, 'budget'] = 10000000     \ndataset1.loc[dataset1['id'] == 7021, 'budget'] = 17540562\ndataset1.loc[dataset1['id'] == 5591, 'budget'] = 4000000      \ndataset1.loc[dataset1['id'] == 4282, 'budget'] = 20000000\ndataset1.loc[dataset1['id'] == 391, 'runtime'] = 86 \ndataset1.loc[dataset1['id'] == 592, 'runtime'] = 90 \ndataset1.loc[dataset1['id'] == 925, 'runtime'] = 95 \ndataset1.loc[dataset1['id'] == 978, 'runtime'] = 93 \ndataset1.loc[dataset1['id'] == 1256, 'runtime'] = 92 \ndataset1.loc[dataset1['id'] == 1542, 'runtime'] = 93\ndataset1.loc[dataset1['id'] == 1875, 'runtime'] = 86 \ndataset1.loc[dataset1['id'] == 2151, 'runtime'] = 108\ndataset1.loc[dataset1['id'] == 2499, 'runtime'] = 108 \ndataset1.loc[dataset1['id'] == 2646, 'runtime'] = 98\ndataset1.loc[dataset1['id'] == 2786, 'runtime'] = 111\ndataset1.loc[dataset1['id'] == 2866, 'runtime'] = 96\ndataset1.loc[dataset1['id'] == 4074, 'runtime'] = 103 \ndataset1.loc[dataset1['id'] == 4222, 'runtime'] = 93\ndataset1.loc[dataset1['id'] == 4431, 'runtime'] = 100 \ndataset1.loc[dataset1['id'] == 5520, 'runtime'] = 86 \ndataset1.loc[dataset1['id'] == 5845, 'runtime'] = 83 \ndataset1.loc[dataset1['id'] == 5849, 'runtime'] = 140\ndataset1.loc[dataset1['id'] == 6210, 'runtime'] = 104\ndataset1.loc[dataset1['id'] == 6804, 'runtime'] = 145 \ndataset1.loc[dataset1['id'] == 7321, 'runtime'] = 87\ndataset1.dropna()\ndataset1.loc[dataset1.release_date.isnull(), 'release_date'] = '05/01/2000'\n\ndataset1['release_year'] = dataset1.release_date.str.extract('\\S+/\\S+/(\\S+)', expand=False).astype(np.int16)\ndataset1['release_month'] = dataset1.release_date.str.extract('(\\S+)/\\S+/\\S+', expand=False).astype(np.int16)\ndataset1['release_day'] = dataset1.release_date.str.extract('\\S+/(\\S+)/\\S+', expand=False).astype(np.int16)\n\ndataset1.loc[(21 <= dataset1.release_year) & (dataset1.release_year <= 99), 'release_year'] += 1900\ndataset1.loc[dataset1.release_year < 21, 'release_year'] += 2000\n\ndataset1['release_date'] = pd.to_datetime(dataset1.release_day.astype(str) + '-' + \n                                         dataset1.release_month.astype(str) + '-' + \n                                         dataset1.release_year.astype(str))\n\ndataset1['release_weekday'] = dataset1.release_date.dt.weekday + 1\ndataset1['release_quarter'] = dataset1.release_date.dt.quarter\nol_count = dataset1['original_language'].value_counts()\nfor lang, count in ol_count.loc[ol_count > 80].iteritems():\n    feature = 'ol_' + lang\n    dataset1[feature] = 0\n    dataset1.loc[dataset1.original_language == lang, feature] = 1\nthreshold = 80\nfor feature in ['genres', 'production_companies', 'production_countries', 'spoken_languages']:\n    dataset1.loc[dataset1[feature].isnull(), feature] = '{}'\n    dataset1[feature] = dataset1[feature].apply(lambda x: sorted([d['name'] for d in eval(x)]))\n    dataset1['num_of_' + feature] = dataset1[feature].apply(lambda x: len(x))\n    dataset1[feature] = dataset1[feature].apply(lambda x: ','.join(map(str, x)))\n    \n    tmp = dataset1[feature].str.get_dummies(sep=',')\n    tmp = tmp.loc[:, tmp.sum() > threshold]\n    dataset1 = pd.concat([dataset1, tmp], axis=1)\n    \ndataset1['has_budget'] = 1\ndataset1.loc[dataset1.budget == 0, 'has_budget'] = 0\ndataset1['has_collection'] = 1\ndataset1.loc[dataset1.belongs_to_collection.isnull(), 'has_collection'] = 0\ndataset1['has_homepage'] = 1\ndataset1.loc[dataset1.homepage.isnull(), 'has_homepage'] = 0\ndataset1['has_tagline'] = 1\ndataset1.loc[dataset1.tagline.isnull(), 'has_tagline'] = 0\n\nfor feature in ['Keywords', 'cast', 'crew']:\n    dataset1.loc[dataset1[feature].isnull(), feature] = '{}'\n    dataset1['num_of_' + feature] = dataset1[feature].apply(lambda x: len([d['name'] for d in eval(x)]))\n    \nscaler = MinMaxScaler()\nnumeric_features = ['runtime', 'budget', 'popularity', 'release_year', \n                    'release_month', 'release_day', 'release_quarter', \n                    'num_of_Keywords', 'num_of_cast', 'num_of_crew']\nfor feature in numeric_features:\n    if feature == 'budget':\n        dataset1.loc[dataset1[feature] == 0, feature] = np.nanmedian(dataset1[feature].loc[dataset1[feature] != 0])\n        dataset1[feature] = np.log2(dataset1[feature] + 1)\n    dataset1.loc[dataset1[feature].isnull(), feature] = np.nanmedian(dataset1[feature])\n    dataset1[feature] = scaler.fit_transform(dataset1[feature].values.reshape(-1, 1))\n    \ndataset1 = dataset1.drop(['id', 'belongs_to_collection', 'genres', 'homepage', 'imdb_id', \n                        'original_language', 'original_title', 'overview', 'poster_path', \n                        'production_companies', 'production_countries', 'release_date', \n                        'spoken_languages', 'status', 'tagline', 'title', 'Keywords', 'cast', 'crew'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First we separate into input and output components\nX_dataset = dataset.drop('revenue', axis=1)\nY_dataset = dataset['revenue']\nX = X_dataset.values\ny = Y_dataset.values\nnp.set_printoptions(suppress=True)\npd.DataFrame(X).head()\npd.DataFrame(y).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = dataset1[X_dataset.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_dataset.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN Regression \nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.neighbors import KNeighborsRegressor\n\nkfold=KFold(n_splits=10, random_state=7)\n\nmodel=KNeighborsRegressor()\nscoring = \"neg_mean_squared_error\"\n\nresults=cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n\nprint(f'KNN Regression - MSE {results.mean():.3f} std {results.std():.3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X,y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'id': test.id, 'revenue': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}