{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Box office prediction"},{"metadata":{},"cell_type":"markdown","source":"This kernel provides a simple solution to the TMDB Box Office Prediction competition. A few characteristics of this kernel:\n* no external data are used; data have not been modified in any way\n* the number of features used in the model has been minimized to simplify interpretation of the model\n* categorical features are weighted using a revenue-based weighting scheme\n* a simple Random Forest model is used\n* best-fit model parameters are found using Bayesian Optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom bayes_opt import BayesianOptimization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ntrain.set_index('id', inplace=True)\ntest.set_index('id', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We turn the `revenue` feature into its logarithm to reduce the weight of the highest earning movies in the modeling (and because it's required by the competition)."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.revenue = np.log1p(train.revenue)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Inspection"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inventory\n\n* `train`: 22 columns, 3000 rows of data\n* `test`: 21 columns, 4398 rows of data\n\n## Completeness\n\nThe following bar plots show the completeness levels of the individual features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_completeness = pd.DataFrame({'filled': [train.loc[:, col].dropna().count() for col in train.columns],\n                                   'total': [len(train)]*len(train.columns)},\n                                  index=train.columns)\ntrain_completeness.drop('revenue')\ntest_completeness = pd.DataFrame({'filled': [test.loc[:, col].dropna().count() for col in test.columns],\n                                   'total': [len(test)]*len(test.columns)},\n                                 index=test.columns)\n\nf, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(15,5))\ntrain_completeness.filled.plot.bar(title='train sample', ax=ax1)\ntest_completeness.filled.plot.bar(title='test sample', color='orange', ax=ax2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering, EDA, and Completion\n\nWe discuss individual features, create new features, complete feature data, and define `featurelist`, a preliminary list of features that will be used in the modeling."},{"metadata":{"trusted":true},"cell_type":"code","source":"featurelist = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### belongs_to_collection\n\nFirst, let's check if being part of a collection correlates with higher revenues."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['in_collection'] = train.belongs_to_collection.agg(lambda x: 1)\ntrain.loc[train.belongs_to_collection.isna(), 'in_collection'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby('in_collection').revenue.plot.hist(alpha=0.3, bins=np.linspace(0, 10, 20), legend=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Yes, movies in collections are more likely to generate higher revenue. This is useful information and we add this feature to the list."},{"metadata":{"trusted":true},"cell_type":"code","source":"test['in_collection'] = test.belongs_to_collection.agg(lambda x: 1)\ntest.loc[test.belongs_to_collection.isna(), 'in_collection'] = 0\n\nfeaturelist += ['in_collection']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Next question: does the number of movies per collection matter?"},{"metadata":{"trusted":false},"cell_type":"code","source":"train['nmovies_in_collection'] = train.belongs_to_collection.apply(\n    lambda x: train.loc[train.belongs_to_collection == x].belongs_to_collection.count())\ntrain.loc[train.belongs_to_collection.isna(), 'nmovies_in_collection'] = 1\ntrain.boxplot(by='nmovies_in_collection', column='revenue', figsize=(8,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some systematics, but the effect seems rather small. Let's ignore this feature for now."},{"metadata":{},"cell_type":"markdown","source":"Are revenues within movie collections consistent?"},{"metadata":{"trusted":false},"cell_type":"code","source":"train.groupby('belongs_to_collection').revenue.std().plot.hist(alpha=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes, the majority has relatively small standard deviations in revenue. This implies that other movies from the same collection are likely to have large revenues, too.\n\nAre there movies in the `test` sample that belong to the same collections as those in the `train` sample?"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_collections = set(train.belongs_to_collection.unique())\ntest_collections = set(test.belongs_to_collection.unique())\nprint('number of unique collections:  train: {}, test: {}'.format(len(train_collections), len(test_collections)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('number of collections present in both samples: {}'.format(len(train_collections.intersection(test_collections))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes, there is an overlap of 229 collections. We take advantage of this situation and create a new feature `median_revenue_collection` that assigns the median revenue of the corresponding collection to its member movies - movies that are not part of any collection get assigned the median revenue across all movies that are not part of a collection."},{"metadata":{"trusted":false},"cell_type":"code","source":"train['median_revenue_collection'] = train.belongs_to_collection.apply(\n    lambda x: train.loc[train.belongs_to_collection == x].revenue.median())\ntrain.median_revenue_collection.fillna(train.loc[train.in_collection == 0].revenue.median(), inplace=True)\n\ntest['median_revenue_collection'] = test.belongs_to_collection.apply(\n    lambda x: train.loc[train.belongs_to_collection == x].revenue.median())\ntest.median_revenue_collection.fillna(train.loc[train.in_collection == 0].revenue.median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.plot.scatter('median_revenue_collection', 'revenue', alpha=0.2)\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(train.median_revenue_collection, train.revenue)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"featurelist += ['median_revenue_collection']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### budget\n\nSince we applied a logarithmic transformation to `revenue`, we have to apply the same transformation to `budget` to keep both quantities aligned. At the same time, we fill missing data points with the median `budget` value across both samples. \n\nIs there a significant correlation between `budget` and `revenue`? We plot the data and use a Pearson $r$ correlation test to check for significance."},{"metadata":{"trusted":false},"cell_type":"code","source":"train.budget = np.log1p(train.budget.replace(0, pd.concat([train, test], sort=False).budget.median()))\ntest.budget = np.log1p(test.budget.replace(0, pd.concat([train, test], sort=False).budget.median()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.plot.scatter(x='budget', y='revenue', alpha=0.1)\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(train.budget, train.revenue)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes, there is a significant correlation between `budget` and `revenue`. We add `budget` as a model feature."},{"metadata":{"trusted":false},"cell_type":"code","source":"featurelist += ['budget']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### genres"},{"metadata":{},"cell_type":"markdown","source":"Movies can be assigned to multiple genres. How many unique genre categories are there?"},{"metadata":{"trusted":false},"cell_type":"code","source":"genres = set([genre['name'] for genre in eval(\"+\".join(\n    [g for g in pd.concat([train, test], sort=False).genres.fillna(\n    \"[{'id':999, 'name':'NoGenre'}]\")]))])\nprint(len(genres), 'different genres')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although this is a rather limited number of genres, we introduce a weighting schema that we will use for most categorical features throughout this data set. \n\nIn this schema, we add up the total revenue (on a linear - not logarithmic - scale multiplied with some constant factor) of each of the categories."},{"metadata":{"trusted":false},"cell_type":"code","source":"# extract only those genres members that occur in train\ngenre_train = set([genre['name'] for genre in eval(\"+\".join(\n    [c for c in train.genres.fillna(\n    \"[{'id':999, 'name':'NoGenre'}]\")]))])\n\n# calculate cumulative revenue per genre\ncum_rev_genre = [train.loc[train.genres.fillna(\n    \"[{'id':999, 'name':'NoGenre'}]\").str.contains(\"'name': '{}'\".format(c)), 'revenue'].agg('exp').sum()\n                  for c in genre_train]\n\n# sort genres based on revenue\ncum_rev_genre = pd.Series(cum_rev_genre, index=genre_train).sort_values(ascending=False)\n\n# scale the cumulative revenue\ncumulative_revenue = cum_rev_genre.cumsum()/cum_rev_genre.cumsum()[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12,5))\n\ncumulative_revenue.plot.line(rot=90, ax=ax)\n\nax.set_xticks(range(len(cumulative_revenue)))\nax.set_xticklabels(list(cumulative_revenue.index))\nax.set_ylabel('cumulative revenue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot shows that there is a clear preference for some genres to have higher revenues than others. For instance, the subset of [action, adventure, drama, comedy] movies make up 50% of the total revenue of all movies as a function of genre.\n\nWe take advantage of this fact and divide the field of genres into tiers, each of which covers a 10% quantile of the total cumulative revenue (technically, the total cumulative is a multiple of the real total revenue since we are assigning the same revenue to multiple companies). The idea behing this approach is that the most successful genres that earned 10% of the total cumulative revenue are assigned to tier 10, the next most successful 10% into tier 9, etc. The tier number can then subsequently be used as a weight to express the company's success."},{"metadata":{"trusted":false},"cell_type":"code","source":"genre_weights = []\n\n# define tiers and weighting scheme; high-revenue = high weight\nweight = 10\nthreshold = 0.1\nfor i in range(len(cumulative_revenue)): \n    if cumulative_revenue[i] > threshold:\n        weight -= 1\n        threshold += 0.1\n    genre_weights.append(weight)\n\ngenre_weights = pd.Series(genre_weights + [0], \n                          index=list(cum_rev_genre.index) + [\"NoGenre\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, the genre weights are applied to the data samples by forming the geometric average weight of all genres that are assigned to an individual movie. The geometric average is chosen to favor genres in the top-tier and to put less weight on genres in the lower tiers."},{"metadata":{"trusted":false},"cell_type":"code","source":"train['genre_weight'] = train.genres.fillna(\n    \"[{'id':999, 'name':'NoGenre'}]\").apply(\n    lambda x: np.sqrt(np.sum([genre_weights[c['name']]**2 for c in eval(x)])))\n\n# genre members that occur in test but not in train get zero weights assigned (no revenue information available)\ntest['genre_weight'] = test.genres.fillna(\n    \"[{'id':999, 'name':'NoGenre'}]\").apply(\n    lambda x: np.sqrt(np.sum([genre_weights[c['name']]**2 \n                              if c['name'] in genre_train else 0 \n                              for c in eval(x)])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A quick check:"},{"metadata":{"trusted":false},"cell_type":"code","source":"train.loc[[15, 24], ['original_title', 'genre_weight', 'revenue']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at the correlation between `genre_weight` and revenue across all movies in the training sample:"},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots()\n\nax.scatter(train.genre_weight, train.revenue, alpha=0.1)\nax.set_xlabel('genre_weight')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(train.genre_weight, train.revenue)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a weak but statistically significant correlation. We keep this feature and apply the same weighting schema to other categorical features."},{"metadata":{"trusted":false},"cell_type":"code","source":"featurelist += ['genre_weight']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### homepage\n\nThe existence of a homepage might have an effect on `revenue`. Let's check:"},{"metadata":{"trusted":false},"cell_type":"code","source":"train.homepage.fillna(0, inplace=True)\ntrain.loc[train.homepage != 0, 'homepage'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.groupby('homepage').revenue.plot.hist(alpha=0.3, bins=np.linspace(0, 10, 20), legend=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, there might be a small effect in the sense that having a homepage increases the likelihood of high revenues, but is it significant?"},{"metadata":{"trusted":false},"cell_type":"code","source":"train.loc[train.homepage == 1, 'revenue'].agg(['mean', 'std'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.loc[train.homepage == 0, 'revenue'].agg(['mean', 'std'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No, it's not siginificant. We will not use `homepage` in the model learning."},{"metadata":{},"cell_type":"markdown","source":"### imdb_id\n\nThis feature represents a unique identifier for each movie and will hence be ignored."},{"metadata":{},"cell_type":"markdown","source":"### original_language\n\nThe movie's language is most likely to have a large impact on the revenue. How many languages are present in both data sets?"},{"metadata":{"trusted":false},"cell_type":"code","source":"languages = pd.concat([train, test], sort=False).original_language.unique()\nprint(len(languages), 'different languages')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Many of these languages should be extremely rare. Let's clip all languages that are used in less than 10 movies each across both samples and then apply our weighting schema."},{"metadata":{"trusted":false},"cell_type":"code","source":"language_counts = pd.concat([train, test], sort=False).original_language.value_counts()\n\n# rename languages with less than 10 occurences to 'ot' for 'other'\nfor lang in language_counts[language_counts < 10].index:\n    train.loc[train.original_language == lang, 'original_language'] = 'ot'\n    test.loc[test.original_language == lang, 'original_language'] = 'ot'\n    \nlanguages = pd.concat([train, test], sort=False).original_language.unique()\nprint('languages used in more than 10 movies:', languages)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# extract only those languages that occur in train\nolang_train = train.original_language.unique()\n\n# calculate cumulative revenue per olang member\ncum_rev_olang = [train.loc[train.original_language == l, 'revenue'].agg('exp').sum()\n                  for l in olang_train]\ncum_rev_olang = pd.Series(cum_rev_olang, index=olang_train).sort_values(ascending=False)\n\ncumulative_revenue = cum_rev_olang.cumsum()/cum_rev_olang.cumsum()[-1]\n\nolang_weights = []\n\n# define tiers and weighting scheme; high-revenue = high weight\nweight = 10\nthreshold = 0.1\nfor i in range(len(cumulative_revenue)): \n    if cumulative_revenue[i] > threshold:\n        weight -= 1\n        threshold += 0.1\n    olang_weights.append(weight)\n\nolang_weights = pd.Series(olang_weights + [0], \n                          index=list(cum_rev_olang.index) + [\"xx\"])\n\ntrain['olang_weight'] = train.original_language.map(olang_weights)\n\n# olang members that occur in test but not in train get zero weights assigned (no revenue information available)\ntest['olang_weight'] = test.original_language.map(olang_weights).fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Is there a correlation with `revenue`?"},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots()\n\nax.scatter(train.olang_weight, train.revenue, alpha=0.1)\nax.set_xlabel('olang_weight')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(train.olang_weight, train.revenue)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, there's a weak correlation. Let's keep this feature for now."},{"metadata":{"trusted":false},"cell_type":"code","source":"featurelist += ['olang_weight']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### original_title\n\nNLP would be useful to extract keywords from movie titles which can then be used as features. Information on the language could be involved as well.\n\nHere, we simply check if the title length somehow correlates with the movie's revenue."},{"metadata":{"trusted":false},"cell_type":"code","source":"titlelen = train.original_title.apply(lambda x: len(x))\n\nf, ax = plt.subplots()\nax.scatter(titlelen, train.revenue, alpha=0.1)\nax.set_xlabel('title length in characters')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(titlelen, train.revenue)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no significant correlation between the movie's title length and its revenue. We ignore this feature. "},{"metadata":{},"cell_type":"markdown","source":"### overview\n\nAgain, NLP would be definitely useful here, but for the sake of simplicity, let's repeat the length-analysis previously applied to the movie titles:"},{"metadata":{"trusted":false},"cell_type":"code","source":"overviewlen = train.overview.fillna('notext').apply(lambda x: len(x))\n\nf, ax = plt.subplots()\nax.scatter(overviewlen, train.revenue, alpha=0.1)\nax.set_xlabel('overview length in characters')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(overviewlen, train.revenue)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is an extremely weak trend at the 2.6% level between the length of `overview` and `revenue` - not significant enough. We drop this feature."},{"metadata":{},"cell_type":"markdown","source":"### popularity\n\nWe rescale the `popularity` on a logarithmic scale and find a clear correlation with `revenue`."},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots()\n\nax.scatter(np.log(train.popularity), train.revenue, alpha=0.1)\nax.set_xlabel('log(popularity)')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(np.log(train.popularity), train.revenue)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a clear correlation. We apply the rescaling to both data samples."},{"metadata":{"trusted":false},"cell_type":"code","source":"train.popularity = np.log1p(train.popularity)\ntest.popularity = np.log1p(test.popularity)\nfeaturelist += ['popularity']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### poster_path\n\nThis feature provides a filename and is hence not diagnostic of `revenue`."},{"metadata":{},"cell_type":"markdown","source":"### production_companies\n\nWe apply our weighting schema to this feature."},{"metadata":{"trusted":false},"cell_type":"code","source":"companies = set([company['name'] for company in eval(\"+\".join(\n    [c for c in pd.concat([train, test], sort=False).production_companies.dropna()]))])\nprint(len(companies), 'different production companies')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# calculate cumulative revenue per company\ncum_rev_companies = [train.loc[train.production_companies.fillna('').str.contains(c), 'revenue'].agg('exp').sum()\n                     for c in companies]\ncum_rev_companies = pd.Series(cum_rev_companies, index=companies).sort_values(ascending=False)\n\ncumulative_revenue = cum_rev_companies.cumsum()/cum_rev_companies.cumsum()[-1]\n\ncompany_weights = []\n\n# define tiers and weighting scheme; high-revenue = high weight\nweight = 10\nthreshold = 0.1\nfor i in range(len(cumulative_revenue)): \n    if cumulative_revenue[i] > threshold:\n        weight -= 1\n        threshold += 0.1\n    company_weights.append(weight)\n\ncompany_weights = pd.Series(company_weights + [0], index=list(cum_rev_companies.index) + [\"nocompany\"])\n\ntrain['production_companies_weight'] = train.production_companies.fillna(\"[{'name': 'nocompany'}]\").apply(\n    lambda x: np.sqrt(np.sum([company_weights[company['name']]**2 for company in eval(x)])))\n\ntest['production_companies_weight'] = test.production_companies.fillna(\"[{'name': 'nocompany'}]\").apply(\n    lambda x: np.sqrt(np.sum([company_weights[company['name']]**2 for company in eval(x)])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Is there a correlation between `production_companies_weights` and `revenue`?"},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots()\n\nax.scatter(train.production_companies_weight, train.revenue, alpha=0.1)\nax.set_xlabel('production_companies_weight')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(train.production_companies_weight, train.revenue)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"featurelist += ['production_companies_weight']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### production_countries\n\nWe take advantage of our weighting schema again."},{"metadata":{"trusted":false},"cell_type":"code","source":"countries = set([company['name'] for company in eval(\"+\".join(\n    [c for c in pd.concat([train, test], sort=False).production_countries.dropna()]))])\nprint(len(countries), 'different production countries')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# calculate cumulative revenue per country\ncum_rev_countries = [train.loc[train.production_countries.fillna('').str.contains(c), 'revenue'].agg('exp').sum()\n                     for c in countries]\ncum_rev_countries = pd.Series(cum_rev_countries, index=countries).sort_values(ascending=False)\n\ncumulative_revenue = cum_rev_countries.cumsum()/cum_rev_countries.cumsum()[-1]\n\ncountry_weights = []\n\n# define tiers and weighting scheme; high-revenue = high weight\nweight = 10\nthreshold = 0.1\nfor i in range(len(cumulative_revenue)): \n    if cumulative_revenue[i] > threshold:\n        weight -= 1\n        threshold += 0.1\n    country_weights.append(weight)\n\ncountry_weights = pd.Series(country_weights + [0], index=list(cum_rev_countries.index) + [\"nocountry\"])\n\ntrain['production_countries_weight'] = train.production_countries.fillna(\"[{'name': 'nocountry'}]\").apply(\n    lambda x: np.sqrt(np.sum([country_weights[country['name']]**2 for country in eval(x)])))\n\ntest['production_countries_weight'] = test.production_countries.fillna(\"[{'name': 'nocountry'}]\").apply(\n    lambda x: np.sqrt(np.sum([country_weights[country['name']]**2 for country in eval(x)])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots()\n\nax.scatter(train.production_countries_weight, train.revenue, alpha=0.1)\nax.set_xlabel('production_countries_weight')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(train.production_countries_weight, train.revenue)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a significant correlation."},{"metadata":{"trusted":false},"cell_type":"code","source":"featurelist += ['production_countries_weight']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### release_date\n\nWe turn `release_date` into a datetime object. Before we can do that, we have to split the two-digit years into year belonging into the 20th century and years belonging into the 21st century."},{"metadata":{"trusted":false},"cell_type":"code","source":"train.release_date = train.release_date.apply(\n        lambda x: \"{}/{}/{}\".format(\n            x.split('/')[0],\n            x.split('/')[1],\n            (str(20)+x.split('/')[2] if (float(x.split('/')[2]) < 20) \n                                     else str(19)+x.split('/')[2])))\n\ntest.release_date = test.release_date.fillna(test.release_date.iloc[1000]).apply(\n        lambda x: \"{}/{}/{}\".format(\n            x.split('/')[0],\n            x.split('/')[1],\n            (str(20)+x.split('/')[2] if (float(x.split('/')[2]) < 20) \n                                     else str(19)+x.split('/')[2])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.release_date = pd.to_datetime(train.release_date)\ntest.release_date = pd.to_datetime(test.release_date)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see if the year a movie was released affects its revenue."},{"metadata":{"trusted":false},"cell_type":"code","source":"release_year = train.release_date.apply(lambda x: x.year)\nf, ax = plt.subplots()\n\nax.scatter(release_year, train.revenue, alpha=0.1)\nax.set_xlabel('release_year')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(release_year, train.revenue)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a rather weak (~5% significance) correlation between the release year and revenue. Let's keep this feature for future use, but we will not use it in the immediate modeling."},{"metadata":{"trusted":false},"cell_type":"code","source":"train['release_year'] = pd.cut(train.release_date.apply(lambda x: x.year), \n                               bins=np.arange(1920, 2030, 10), right=True, labels=False)\ntest['release_year'] = pd.cut(test.release_date.apply(lambda x: x.year), \n                               bins=np.arange(1920, 2030, 10), right=True, labels=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What about the release month? Is there a correlation with `revenue`?"},{"metadata":{"trusted":false},"cell_type":"code","source":"release_month = train.release_date.apply(lambda x: x.month)\nf, ax = plt.subplots()\n\nax.scatter(release_month, train.revenue, alpha=0.1)\nax.set_xlabel('release_month')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(release_month, train.revenue)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The month barely plays a role, the correlation is not significant enough to be considered. What about the season? We can use a $\\sin$ transformation to map the month on the season (0 for winter, 1 for summer; for the northern hemisphere)."},{"metadata":{"trusted":false},"cell_type":"code","source":"release_season = train.release_date.apply(lambda x: np.sin(x.month/12*np.pi))\n# winter is 0, summer is 1 (northern hemisphere)\n\nf, ax = plt.subplots()\n\nax.scatter(release_season, train.revenue, alpha=0.1)\nax.set_xlabel('release_month')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(release_season, train.revenue)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No, this is not significant. What about the day of release in the respective month?"},{"metadata":{"trusted":false},"cell_type":"code","source":"release_day = train.release_date.apply(lambda x: x.day)\n\nf, ax = plt.subplots()\n\nax.scatter(release_day, train.revenue, alpha=0.1)\nax.set_xlabel('release_day')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(release_day, train.revenue)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, there is a weak but significant correlation: revenues seem to be higher when a movie is released closer to the end of the month. Maybe people are more likely to spend money on movie tickets at the end of the month? \n\nAlthough interesting, this feature is not strong enough to make it into our model."},{"metadata":{},"cell_type":"markdown","source":"### runtime\n\nThe runtime of a movie should affect its success. Let's look at the distribution of runtimes in both samples."},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.concat([train, test], sort=False).runtime.replace(0, np.nan).fillna(train.runtime.median()).plot.hist(\n    alpha=0.3, bins=np.arange(0, 400, 30), logy=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The vast majority of movies runs between 1 and 2 hours, but there are some outlines. Let's check whether the runtime affects the revenue."},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots()\n\nax.scatter(train.runtime.replace(0, np.nan).fillna(train.runtime.median()), train.revenue, alpha=0.1)\nax.set_xlabel('runtime (min)')\nax.set_ylabel('log(revenue)')\nax.set_xticks([0, 60, 90, 120, 180, 360])\nax.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are different regimes: movies with runtimes between 2 and 3 hours seem to have consistenly higher revenues than shorter movies. We transform `runtime` into discrete buckets based on the grid structure chosen above, which is supposed to capture significant changes in the revenue as a function of movie runtime."},{"metadata":{"trusted":false},"cell_type":"code","source":"train['runtime_bin'] = pd.cut(train.runtime.replace(0, np.nan).fillna(train.runtime.median()), \n                              bins=[0, 60, 90, 120, 180, 360], right=True, include_lowest=True, labels=False)\ntest['runtime_bin'] = pd.cut(test.runtime.replace(0, np.nan).fillna(train.runtime.median()), \n                             bins=[0, 60, 90, 120, 180, 360], right=True, include_lowest=True, labels=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"featurelist += ['runtime_bin']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### spoken_languages\n\nAgain, we take advantage of our weighting schema."},{"metadata":{"trusted":false},"cell_type":"code","source":"languages = set([lang['name'] for lang in eval(\"+\".join(\n    [l for l in pd.concat([train, test], sort=False).spoken_languages.dropna()]))])\nprint(len(languages), 'different spoken languages')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# extract only those languages that occur in train\nlanguages_train = set([lang['name'] for lang in eval(\"+\".join(\n    [l for l in train.spoken_languages.dropna()]))])\n\n# calculate cumulative revenue per language\ncum_rev_languages = [train.loc[train.spoken_languages.fillna('').str.contains(l), 'revenue'].agg('exp').sum()\n                     for l in languages_train]\ncum_rev_languages = pd.Series(cum_rev_languages, index=languages_train).sort_values(ascending=False)\n\ncumulative_revenue = cum_rev_languages.cumsum()/cum_rev_languages.cumsum()[-1]\n\nlanguage_weights = []\n\n# define tiers and weighting scheme; high-revenue = high weight\nweight = 10\nthreshold = 0.1\nfor i in range(len(cumulative_revenue)): \n    if cumulative_revenue[i] > threshold:\n        weight -= 1\n        threshold += 0.1\n    language_weights.append(weight)\n\nlanguage_weights = pd.Series(language_weights + [0], \n                             index=list(cum_rev_languages.index) + [\"nolang\"])\n\ntrain['spoken_languages_weight'] = train.spoken_languages.fillna(\"[{'name': 'nolang'}]\").apply(\n    lambda x: np.sqrt(np.sum([language_weights[lang['name']]**2 for lang in eval(x)])))\n\n# languages that occur in test but not in train get zero weights assigned (no revenue information available)\ntest['spoken_languages_weight'] = test.spoken_languages.fillna(\"[{'name': 'nolang'}]\").apply(\n    lambda x: np.sqrt(np.sum([language_weights[lang['name']]**2 \n                              if lang['name'] in languages_train else 0 \n                              for lang in eval(x)])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots()\n\nax.scatter(train.spoken_languages_weight, train.revenue, alpha=0.1)\nax.set_xlabel('spoken_languages_weight')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(train.spoken_languages_weight, train.revenue)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a significant correlation of `spoken_languages_weight` with `revenue`."},{"metadata":{"trusted":false},"cell_type":"code","source":"featurelist += ['spoken_languages_weight']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### status\n\nAll movies in the data set should be released. Let's check."},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.concat([train, test], sort=False).status.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some movies are actually not yet released. This is no big deal for the `test` sample, but all data points in `train` should have a reliable measurement of `revenue` and this should not be available if the movie is not yet released. \n\nWe drop all data points from the `train` data sample with a `status` flag that differs from `Released`."},{"metadata":{"trusted":false},"cell_type":"code","source":"train = train.loc[train.status == 'Released']\nlen(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This measure drops four data points from the `train` data sample."},{"metadata":{},"cell_type":"markdown","source":"### tagline\n\nAnother opportunity for NLP, but again, we simply check if the existence of a tagline has any impact on `revenue`."},{"metadata":{"trusted":false},"cell_type":"code","source":"train.tagline.fillna(0, inplace=True)\ntrain.loc[train.tagline != 0, 'tagline'] = 1\n\ntrain.groupby('tagline').revenue.plot.hist(alpha=0.3, bins=np.linspace(0, 10, 20), legend=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two distributions look very similar. We ignore this feature."},{"metadata":{},"cell_type":"markdown","source":"### title\n\nWe skip NLP again and simply check for a correlation between the length of a movie title and `revenue`."},{"metadata":{"trusted":false},"cell_type":"code","source":"titlelen = train.title.apply(lambda x: len(x))\n\nf, ax = plt.subplots()\nax.scatter(titlelen, train.revenue, alpha=0.1)\nax.set_xlabel('title length in characters')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(titlelen, train.revenue)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no significant correlation between the length of a movie's title and `revenue`. We ignore this feature."},{"metadata":{},"cell_type":"markdown","source":"### Keywords"},{"metadata":{"trusted":false},"cell_type":"code","source":"keywords_train = set([keyword['name'] for keyword in eval(\"+\".join([k for k in train.Keywords.dropna()]))])\nprint('{} different keywords like {}...'.format(len(keywords_train), \", \".join(list(keywords_train)[:10])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's a large number of keywords. We repeat part of the `production_companies` analysis to see if there are keywords that have an unusually high impact on `revenue`."},{"metadata":{"trusted":false},"cell_type":"code","source":"cum_rev_keywords = [train.loc[train.Keywords.fillna('').str.contains(l), 'revenue'].agg('exp').sum()\n                    for l in keywords_train]\ncum_rev_keywords = pd.Series(cum_rev_keywords, index=keywords_train).sort_values(ascending=False)\ncumulative_revenue = cum_rev_keywords.cumsum()/cum_rev_keywords.cumsum()[-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If all keywords were equally important for `revenue`, their cumulative distribution would follow a straight line. "},{"metadata":{"trusted":false},"cell_type":"code","source":"cumulative_revenue.plot.line()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The actual distribution rises steeply for a small fraction of keywords and then flattens out. Only a small fraction of all keywords make up 50% of the total revenue."},{"metadata":{"trusted":false},"cell_type":"code","source":"len(cumulative_revenue[cumulative_revenue < 0.5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will label these keywords as *hot* keywords that potentially lead to higher revenues. We introduce a feature `hot_keyword` that flags  whether either of the keywords for one movie is among these hot keywords."},{"metadata":{"trusted":false},"cell_type":"code","source":"train['hot_keyword'] = [1 if any([keyword['name'] in cumulative_revenue[cumulative_revenue < 0.5] \n                                  for keyword in eval(x)]) else 0\n                        for x in train.Keywords.fillna(\"''\")]\n\ntest['hot_keyword'] = [1 if any([keyword['name'] in cumulative_revenue[cumulative_revenue < 0.5] \n                                  for keyword in eval(x)]) else 0\n                        for x in test.Keywords.fillna(\"''\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.groupby('hot_keyword').revenue.plot.hist(alpha=0.3, bins=np.linspace(0, 10, 20), legend=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, both distributions look rather similar. We ignore this feature."},{"metadata":{},"cell_type":"markdown","source":"### cast\n\nWe apply our weighting schema once again."},{"metadata":{"trusted":false},"cell_type":"code","source":"actors = set([actor['name'] for actor in eval(\"+\".join(\n    [a for a in pd.concat([train, test], sort=False).cast.dropna()]))])\nprint(len(actors), 'different actors')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# extract only those actors that occur in train\nactors_train = set([actor['name'] for actor in eval(\"+\".join(\n    [a for a in train.cast.dropna()]))])\n\n# calculate cumulative revenue per actor\ncum_rev_actors = [train.loc[train.cast.fillna('').str.contains(\"'name': '{}'\".format(a)), 'revenue'].agg('exp').sum()\n                  for a in actors_train]\ncum_rev_actors = pd.Series(cum_rev_actors, index=actors_train).sort_values(ascending=False)\n\ncumulative_revenue = cum_rev_actors.cumsum()/cum_rev_actors.cumsum()[-1]\n\nactor_weights = []\n\n# define tiers and weighting scheme; high-revenue = high weight\nweight = 10\nthreshold = 0.1\nfor i in range(len(cumulative_revenue)): \n    if cumulative_revenue[i] > threshold:\n        weight -= 1\n        threshold += 0.1\n    actor_weights.append(weight)\n\nactor_weights = pd.Series(actor_weights + [0], \n                          index=list(cum_rev_actors.index) + [\"noactor\"])\n\ntrain['cast_weight'] = train.cast.fillna(\"[{'name': 'noactor'}]\").apply(\n    lambda x: np.sqrt(np.sum([actor_weights[a['name']]**2 for a in eval(x)])))\n\n# actors that occur in test but not in train get zero weights assigned (no revenue information available)\ntest['cast_weight'] = test.cast.fillna(\"[{'name': 'noactor'}]\").apply(\n    lambda x: np.sqrt(np.sum([actor_weights[a['name']]**2 \n                              if a['name'] in actors_train else 0 \n                              for a in eval(x)])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at those 10 actors that produce the highest revenue:"},{"metadata":{"trusted":false},"cell_type":"code","source":"cum_rev_actors.iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots()\n\nax.scatter(np.log1p(train.cast_weight), train.revenue, alpha=0.1)\nax.set_xlabel('cast_weight')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(train.cast_weight, train.revenue)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a strong correlation between `cast_weight` and `revenue`."},{"metadata":{"trusted":false},"cell_type":"code","source":"featurelist += ['cast_weight']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### crew"},{"metadata":{"trusted":false},"cell_type":"code","source":"crew = set([crew['name'] for crew in eval(\"+\".join(\n    [c for c in pd.concat([train, test], sort=False).crew.dropna()]))])\nprint(len(crew), 'different crew members')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# extract only those crew members that occur in train\ncrew_train = set([crew['name'] for crew in eval(\"+\".join(\n    [c for c in train.crew.dropna()]))])\n\n# calculate cumulative revenue per crew member\ncum_rev_crew = [train.loc[train.crew.fillna('').str.contains(\"'name': '{}'\".format(c)), 'revenue'].agg('exp').sum()\n                  for c in crew_train]\ncum_rev_crew = pd.Series(cum_rev_crew, index=crew_train).sort_values(ascending=False)\n\ncumulative_revenue = cum_rev_crew.cumsum()/cum_rev_crew.cumsum()[-1]\n\ncrew_weights = []\n\n# define tiers and weighting scheme; high-revenue = high weight\nweight = 10\nthreshold = 0.1\nfor i in range(len(cumulative_revenue)): \n    if cumulative_revenue[i] > threshold:\n        weight -= 1\n        threshold += 0.1\n    crew_weights.append(weight)\n\ncrew_weights = pd.Series(crew_weights + [0], \n                          index=list(cum_rev_crew.index) + [\"nocrew\"])\n\ntrain['crew_weight'] = train.crew.fillna(\"[{'name': 'nocrew'}]\").apply(\n    lambda x: np.sqrt(np.sum([crew_weights[c['name']]**2 for c in eval(x)])))\n\n# crew members that occur in test but not in train get zero weights assigned (no revenue information available)\ntest['crew_weight'] = test.crew.fillna(\"[{'name': 'noactor'}]\").apply(\n    lambda x: np.sqrt(np.sum([crew_weights[c['name']]**2 \n                              if c['name'] in crew_train else 0 \n                              for c in eval(x)])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots()\n\nax.scatter(np.log1p(train.crew_weight), train.revenue, alpha=0.1)\nax.set_xlabel('crew_weight')\nax.set_ylabel('log(revenue)')\n\nprint('Pearson correlation test: r={} p={}'.format(*pearsonr(train.crew_weight, train.revenue)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`crew_weight` correlates with `revenue`."},{"metadata":{"trusted":false},"cell_type":"code","source":"featurelist += ['crew_weight']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{},"cell_type":"markdown","source":"We use a simple Random Forest Regressor to predict the revenues of the test sample. \n\nBest-fit parameters are found using a Bayesian Optimizer. "},{"metadata":{"trusted":false},"cell_type":"code","source":"random_state = 1\nn_folds = 5\n\ndef prepare_params(n_estimators, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features,\n                  max_leaf_nodes, min_impurity_decrease):\n    params = {'n_jobs': -1, 'random_state': 42}\n    params['n_estimators'] = max(min(int(n_estimators), 10000), 0)\n    params['max_depth'] = max(min(int(max_depth), 100), 1)\n    params['min_samples_split'] = max(min(int(min_samples_split), 10), 2)\n    params['min_samples_leaf'] = max(min(int(min_samples_leaf), 20), 1)\n    params['min_weight_fraction_leaf'] = max(min(min_weight_fraction_leaf, 1), 0)\n    params['max_features'] = max(min(int(max_features), len(featurelist)), 1)\n    params['max_leaf_nodes'] = max(min(max_leaf_nodes, 1), 10000)\n    params['min_impurity_decrease'] = max(min(min_impurity_decrease, 1), 0)\n    return params\n    \ndef eval_model(n_estimators, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf,\n               max_features, max_leaf_nodes, min_impurity_decrease):\n    params = prepare_params(n_estimators, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf,\n               max_features, max_leaf_nodes, min_impurity_decrease)\n\n    model = RandomForestRegressor(**params)\n    \n    cv_result = np.mean(cross_val_score(model, train.loc[:, featurelist], train.revenue, cv=n_folds, \n                                    scoring='neg_mean_squared_error'))\n    \n    return -np.sqrt(-cv_result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_optimizer = BayesianOptimization(f=eval_model, \n                                    pbounds={'n_estimators': (800, 1200), \n                                             'max_depth': (7, 10), \n                                             'min_samples_split': (2, 5),\n                                             'min_samples_leaf': (1, 2), \n                                             'min_weight_fraction_leaf': (0.01, 0.03),\n                                             'max_features': (3, 6),\n                                             'max_leaf_nodes': (8, 12),\n                                             'min_impurity_decrease': (0.003, 0.008)}, #len(featurelist))},\n                                    random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# rf_optimizer.maximize(init_points=100, n_iter=20, alpha=1e-4)\n# rf_optimizer.max","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use the best-fit set of parameters and predict revenues for the test set movies."},{"metadata":{"trusted":false},"cell_type":"code","source":"bestfit = {'params': {'max_depth': 9.873668590451505,\n  'max_features': 4.599495854919051,\n  'max_leaf_nodes': 10.767508455801893,\n  'min_impurity_decrease': 0.004577578155030315,\n  'min_samples_leaf': 1.6865009276815837,\n  'min_samples_split': 4.503877015692119,\n  'min_weight_fraction_leaf': 0.010365765546883836,\n  'n_estimators': 950.0288629889935},\n 'target': -2.1252538162396446}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"params = bestfit['params']\nparams = prepare_params(params['n_estimators'], params['max_depth'], params['min_samples_split'], \n                        params['min_samples_leaf'], params['min_weight_fraction_leaf'], \n                        params['max_features'], params['max_leaf_nodes'], params['min_impurity_decrease'])\n\n# train a model using the best-fit parameters\nmodel = RandomForestRegressor(**params).fit(train.loc[:, featurelist], train.revenue)\n\n# make the prediction\npred = model.predict(test.loc[:, featurelist])\npred = pd.DataFrame({'revenue': np.expm1(pred).astype(np.int64)}, index=test.index)\npred.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Out of curiosity: what is the distribution of feature importances?"},{"metadata":{"trusted":false},"cell_type":"code","source":"fimp = pd.Series(model.feature_importances_, index=featurelist)\nfimp.plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seem to be only 5 major features that drive a movies revenue. Interesting..."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}