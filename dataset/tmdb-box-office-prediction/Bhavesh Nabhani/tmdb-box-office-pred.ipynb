{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tmdb-box-office-prediction/train.csv')\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/tmdb-box-office-prediction/test.csv')\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import Libraries\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nimport plotly as py\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().values.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get Name From Data\ndef get_name(data):\n    try:\n        x = eval(data) \n        return x[0]['name']\n    except:\n        return ''\n\n# Get length of data\ndef get_len(data):\n    try:\n        x = eval(data)\n        return len(x)\n    except:\n        return 0\n\n# Get all data by column\ndef all_data(data, key = 'name'):\n    try:\n        x = eval(data)\n        return ' '.join(sorted([i[key] for i in x]))\n    except:\n        return \n\n# all data of column convert into list \ndef make_list(data, key = 'name'):\n    try:\n        data = eval(data)\n        return [i[key] for i in data]\n    except: \n        return []\n\n# applying one hot encoding for multiple data\ndef apply_encode(data, name):\n    try:\n        if name in data:\n            return 1\n        else:\n            return 0\n    except:\n        return\n\n# Get Gender Type [0,1,2]  \ndef get_gender(data,index):\n    try:\n        data = eval(str(data))\n        count = 0\n        for i in data:\n            if i['gender'] == index:\n                count += 1\n        return count\n    except:\n        return 0\n\n# Change feature name to singular verb\ndef singular_verb(feature_name):\n    if feature_name == 'production_countries':\n        return 'production_country'\n    elif feature_name == 'production_companies':\n        return 'production_company'\n    elif feature_name == 'spoken_languages':\n        return 'spoken_language'\n    elif feature_name == 'Keywords':\n        return 'Keyword'\n    else:\n        return feature_name\n\n# Fixes dates which are in 20xx\ndef fix_date(x):\n    year = x.split('/')[2]\n    if int(year) <= 19:\n        return x[:-2] + '20' + year\n    else:\n        return x[:-2] + '19' + year\n\n# creating features based on dates\ndef process_date(df):\n    date_parts = [\"year\", \"weekday\", \"month\", 'weekofyear', 'day', 'quarter']\n    for part in date_parts:\n        part_col = 'release_date' + \"_\" + part\n        df[part_col] = getattr(df['release_date'].dt, part).astype(int)\n    return df\n\n#...............................\n#\n# Remove same data\n# def unique_list(data):\n#    if data:\n#        k=set()\n#        for i in data:\n#            for m in i:\n#                k.add(m)\n#        return list(k)\n#\n# old count word function\n# count = 0\n# def count_word(data, name):\n#    try:\n#        if name in data:\n#            global count\n#            count += 1\n#            return name\n#    except:\n#        return\n#\n# old comman word finder function\n# def comman_word(feature, all_word, fetch = 25):\n#    make_count = []\n#    for name in all_word:\n#        global count\n#        count = 0\n#       not_to_print = feature.apply(lambda x:count_word(x, name))\n#        make_count.append([name, count])\n#        print([name, count])\n#    k = sorted(make_count, key = lambda x: x[1])[::-1]\n#   return np.array(k)[:fetch,0]\n#\n#.................................","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train missing values\ntrain['runtime'].fillna(0, inplace=True)\ntrain['status'].fillna('Released', inplace=True)\ntrain['release_date'].fillna(train['release_date'].mode()[0], inplace=True)\n\n# test missing values\ntest['runtime'].fillna(0, inplace=True)\ntest['status'].fillna('Released', inplace=True)\ntest['release_date'].fillna(test['release_date'].mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train data cleaning\ntrain['collection_name'] = train['belongs_to_collection'].apply(get_name)\ntrain['num_of_collection'] = train['belongs_to_collection'].apply(get_len)\ntrain['num_of_genres'] = train['genres'].apply(get_len)\ntrain['num_of_countries'] = train['production_countries'].apply(get_len)\ntrain['num_of_companies'] = train['production_companies'].apply(get_len)\ntrain['num_of_spoken_languages'] = train['spoken_languages'].apply(get_len)\ntrain['num_of_cast'] = train['cast'].apply(get_len)\ntrain['num_of_crew'] = train['crew'].apply(get_len)\ntrain['num_of_keywords'] = train['Keywords'].apply(get_len)\ntrain['has_homepage'] = 0\ntrain.loc[train['homepage'].isnull() == False, 'has_homepage'] = 1\n\n# test data cleaning\ntest['collection_name'] = test['belongs_to_collection'].apply(get_name)\ntest['num_of_collection'] = test['belongs_to_collection'].apply(get_len)\ntest['num_of_genres'] = test['genres'].apply(get_len)\ntest['num_of_countries'] = test['production_countries'].apply(get_len)\ntest['num_of_companies'] = test['production_companies'].apply(get_len)\ntest['num_of_spoken_languages'] = test['spoken_languages'].apply(get_len)\ntest['num_of_cast'] = test['cast'].apply(get_len)\ntest['num_of_crew'] = test['crew'].apply(get_len)\ntest['num_of_keywords'] = test['Keywords'].apply(get_len)\ntest['has_homepage'] = 0\ntest.loc[test['homepage'].isnull() == False, 'has_homepage'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature name\nfeature_list = ['genres', 'production_countries', 'production_companies',\n                'spoken_languages', 'Keywords']\n# train feature encoding\nfor feature_name in feature_list: \n    field = train[feature_name].apply(all_data)\n    list_feature = train[feature_name].apply(make_list)\n    list_of_feature = list(list_feature.values)\n    top_feature_data = [c[0] for c in Counter([i for j in list_of_feature for i in j]).most_common(12)]\n    feature_name_sv = singular_verb(feature_name)\n    for name in top_feature_data:\n        train[feature_name_sv + '_' + name] = field.apply(lambda x:apply_encode(x,name))\n        train[feature_name_sv + '_' + name] = train[feature_name_sv + '_' + name].fillna(0).astype('int32')\n    train = train.drop([feature_name], axis=1)\n\n# test feature encoding\nfor feature_name in feature_list: \n    field = test[feature_name].apply(all_data)\n    list_feature = test[feature_name].apply(make_list)\n    list_of_feature = list(list_feature.values)\n    top_feature_data = [c[0] for c in Counter([i for j in list_of_feature for i in j]).most_common(12)]\n    feature_name_sv = singular_verb(feature_name)\n    for name in top_feature_data:\n        test[feature_name_sv + '_' + name] = field.apply(lambda x:apply_encode(x,name))\n        test[feature_name_sv + '_' + name] = test[feature_name_sv + '_' + name].fillna(0).astype('int32')\n    test = test.drop([feature_name], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cast and crew encoding train\nfeature_list = ['cast','crew']\nfor feature_name in feature_list:\n    if feature_name == 'cast':\n        # cast_keys\n        list_keys = ['name','character']\n        for i_key in list_keys:\n            feature = train[feature_name].apply(lambda x: all_data(x,i_key))\n            list_feature = train[feature_name].apply(lambda x: make_list(x,i_key))\n            list_of_feature = list(list_feature.values)\n            top_feature_data = [c[0] for c in Counter([i for j in list_of_feature for i in j]).most_common(10)]\n            feature_name_sv = singular_verb(feature_name)\n            for name in top_feature_data:\n                train[feature_name_sv  + '_'+ i_key + '_' + name] = feature.apply(lambda x:apply_encode(x,name))\n                train[feature_name_sv  + '_'+ i_key + '_' + name] = train[feature_name_sv  + '_'+ i_key + '_' + name].fillna(0).astype('int32')\n            # feature and list_feature are dataframe so it can't be reassign.\n            del feature\n            del list_feature\n    if feature_name == 'crew':\n        # crew_keys\n        list_keys = ['name','job','department']\n        for i_key in list_keys:\n            feature = train[feature_name].apply(lambda x: all_data(x,i_key))\n            list_feature = train[feature_name].apply(lambda x: make_list(x,i_key))\n            list_of_feature = list(list_feature.values)\n            top_feature_data = [c[0] for c in Counter([i for j in list_of_feature for i in j]).most_common(10)]\n            feature_name_sv = singular_verb(feature_name)\n            for name in top_feature_data:\n                train[feature_name_sv + '_'+ i_key + '_' + name] = feature.apply(lambda x:apply_encode(x,name))\n                train[feature_name_sv + '_'+ i_key + '_' + name] = train[feature_name_sv + '_'+ i_key + '_' + name].fillna(0).astype('int32')\n            # feature and list_feature are dataframe so it can't be reassign. \n            del feature\n            del list_feature\n    # cast and crew gender encoding\n    train[feature_name +'_genders_0'] = train[feature_name].apply(lambda x: get_gender(x,0))\n    train[feature_name +'_genders_1'] = train[feature_name].apply(lambda x: get_gender(x,1))\n    train[feature_name +'_genders_2'] = train[feature_name].apply(lambda x: get_gender(x,2))\n\n# cast and crew encoding test\nfeature_list = ['cast','crew']\nfor feature_name in feature_list:\n    if feature_name == 'cast':\n        # cast_keys\n        list_keys = ['name','character']\n        for i_key in list_keys:\n            feature = test[feature_name].apply(lambda x: all_data(x,i_key))\n            list_feature = test[feature_name].apply(lambda x: make_list(x,i_key))\n            list_of_feature = list(list_feature.values)\n            top_feature_data = [c[0] for c in Counter([i for j in list_of_feature for i in j]).most_common(10)]\n            feature_name_sv = singular_verb(feature_name)\n            for name in top_feature_data:\n                test[feature_name_sv  + '_'+ i_key + '_' + name] = feature.apply(lambda x:apply_encode(x,name))\n                test[feature_name_sv  + '_'+ i_key + '_' + name] = test[feature_name_sv  + '_'+ i_key + '_' + name].fillna(0).astype('int32')\n            # feature and list_feature are dataframe so it can't be reassign.\n            del feature\n            del list_feature\n    if feature_name == 'crew':\n        # crew_keys\n        list_keys = ['name','job','department']\n        for i_key in list_keys:\n            feature = test[feature_name].apply(lambda x: all_data(x,i_key))\n            list_feature = test[feature_name].apply(lambda x: make_list(x,i_key))\n            list_of_feature = list(list_feature.values)\n            top_feature_data = [c[0] for c in Counter([i for j in list_of_feature for i in j]).most_common(10)]\n            feature_name_sv = singular_verb(feature_name)\n            for name in top_feature_data:\n                test[feature_name_sv + '_'+ i_key + '_' + name] = feature.apply(lambda x:apply_encode(x,name))\n                test[feature_name_sv + '_'+ i_key + '_' + name] = test[feature_name_sv + '_'+ i_key + '_' + name].fillna(0).astype('int32')\n            # feature and list_feature are dataframe so it can't be reassign. \n            del feature\n            del list_feature\n    # cast and crew gender encoding\n    test[feature_name +'_genders_0'] = test[feature_name].apply(lambda x: get_gender(x,0))\n    test[feature_name +'_genders_1'] = test[feature_name].apply(lambda x: get_gender(x,1))\n    test[feature_name +'_genders_2'] = test[feature_name].apply(lambda x: get_gender(x,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train release_date feature process\ntrain['release_date'] = train['release_date'].apply(lambda x: fix_date(x))\ntrain['release_date'] = pd.to_datetime(train['release_date'])\ntrain = process_date(train)\n\n# test release_date feature process\ntest['release_date'] = test['release_date'].apply(lambda x: fix_date(x))\ntest['release_date'] = pd.to_datetime(test['release_date'])\ntest = process_date(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['original_language', 'collection_name']:\n    le = LabelEncoder()\n    le.fit(list(train[col].fillna('')) + list(test[col].fillna('')))\n    train[col] = le.transform(train[col].fillna('').astype(str))\n    test[col] = le.transform(test[col].fillna('').astype(str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop columns\ntrain = train.drop(['belongs_to_collection','cast','crew','homepage', 'imdb_id', 'poster_path', 'status'], axis=1)\ntest = test.drop(['belongs_to_collection','cast','crew','homepage', 'imdb_id', 'poster_path', 'status'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d1 = train['release_date_year'].value_counts().sort_index()\nd2 = train.groupby(['release_date_year'])['revenue'].sum()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='film count'), \n        go.Scatter(x=d2.index, y=d2.values, name='total revenue', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Number of films and total revenue per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  yaxis2=dict(title='Total revenue', overlaying='y', side='right')\n                  ),legend=dict(\n                orientation=\"v\"))\npy.offline.iplot(dict(data=data, layout=layout))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d1 = train['release_date_year'].value_counts().sort_index()\nd2 = train.groupby(['release_date_year'])['revenue'].mean()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='film count'), go.Scatter(x=d2.index, y=d2.values, name='mean revenue', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Number of films and average revenue per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  yaxis2=dict(title='Average revenue', overlaying='y', side='right')\n                  ),legend=dict(\n                orientation=\"v\"))\npy.offline.iplot(dict(data=data, layout=layout))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='release_date_weekday', y='revenue', data=train);\nplt.title('Revenue on different days of week of release');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 6))\nplt.subplot(1, 3, 1)\nplt.hist(train['runtime']/ 60, bins=40);\nplt.title('Distribution of length of film in hours');\nplt.subplot(1, 3, 2)\nplt.scatter(train['runtime'], train['revenue'])\nplt.title('runtime vs revenue');\nplt.subplot(1, 3, 3)\nplt.scatter(train['runtime'], train['popularity'])\nplt.title('runtime vs popularity');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='num_of_genres', y='revenue', data=train);\nplt.title('Revenue for different number of genres in the film');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='num_of_countries', y='revenue', data=train);\nplt.title('Revenue for different number of countries producing the film');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop column with only 1 value\nfor col in train.columns:\n    if train[col].nunique() == 1:\n        print(col)\n        train = train.drop([col], axis=1)\n        test = test.drop([col], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.drop(['original_title', 'overview','release_date','tagline','title'], axis=1)\ntest = test.drop(['original_title', 'overview','release_date','tagline','title'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check Null\ntrain.isnull().any().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train.drop(['id', 'revenue'], axis=1).values\ny_train = np.log1p(train['revenue']).values\ntest_id = test['id']\ntest = test.drop(['id'], axis=1).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_reg = xgb.XGBRegressor(colsample_bytree = 0.3, learning_rate = 0.001,\n                max_depth = 3, alpha = 10, n_estimators = 15000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_reg.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = xg_reg.predict(test)\ntest_pred = np.expm1(test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dictionary of lists  \ndict = {'id': test_id, 'revenue': test_pred}  \n     \ndf = pd.DataFrame(dict) \n  \n# saving the dataframe \ndf.to_csv('my_submission_file.csv',index=False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}