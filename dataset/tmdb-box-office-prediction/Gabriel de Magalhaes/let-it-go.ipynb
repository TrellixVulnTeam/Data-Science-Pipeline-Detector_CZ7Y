{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nTHIS NOTEBOOK IS A WORK IN PROGRESS! In this notebook I plan to do some exploratory data analysis and create some simple linear models. I might add tree-based models at some point."},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\nimport warnings\n\n# Do not limit number of columns in output.\npd.set_option('display.max_columns', None)\n\nwarnings.filterwarnings('ignore')","execution_count":9,"outputs":[{"output_type":"stream","text":"['sample_submission.csv', 'test.csv', 'train.csv']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/train.csv')","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a peek on the data to see how it looks like:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"   id                              belongs_to_collection    budget  \\\n0   1  [{'id': 313576, 'name': 'Hot Tub Time Machine ...  14000000   \n1   2  [{'id': 107674, 'name': 'The Princess Diaries ...  40000000   \n2   3                                                NaN   3300000   \n3   4                                                NaN   1200000   \n4   5                                                NaN         0   \n\n                                              genres  \\\n0                     [{'id': 35, 'name': 'Comedy'}]   \n1  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...   \n2                      [{'id': 18, 'name': 'Drama'}]   \n3  [{'id': 53, 'name': 'Thriller'}, {'id': 18, 'n...   \n4  [{'id': 28, 'name': 'Action'}, {'id': 53, 'nam...   \n\n                            homepage    imdb_id original_language  \\\n0                                NaN  tt2637294                en   \n1                                NaN  tt0368933                en   \n2  http://sonyclassics.com/whiplash/  tt2582802                en   \n3         http://kahaanithefilm.com/  tt1821480                hi   \n4                                NaN  tt1380152                ko   \n\n                             original_title  \\\n0                    Hot Tub Time Machine 2   \n1  The Princess Diaries 2: Royal Engagement   \n2                                  Whiplash   \n3                                   Kahaani   \n4                                      마린보이   \n\n                                            overview  popularity  \\\n0  When Lou, who has become the \"father of the In...    6.575393   \n1  Mia Thermopolis is now a college graduate and ...    8.248895   \n2  Under the direction of a ruthless instructor, ...   64.299990   \n3  Vidya Bagchi (Vidya Balan) arrives in Kolkata ...    3.174936   \n4  Marine Boy is the story of a former national s...    1.148070   \n\n                        poster_path  \\\n0  /tQtWuwvMf0hCc2QR2tkolwl7c3c.jpg   \n1  /w9Z7A0GHEhIp7etpj0vyKOeU1Wx.jpg   \n2  /lIv1QinFqz4dlp5U4lQ6HaiskOZ.jpg   \n3  /aTXRaPrWSinhcmCrcfJK17urp3F.jpg   \n4  /m22s7zvkVFDU9ir56PiiqIEWFdT.jpg   \n\n                                production_companies  \\\n0  [{'name': 'Paramount Pictures', 'id': 4}, {'na...   \n1        [{'name': 'Walt Disney Pictures', 'id': 2}]   \n2  [{'name': 'Bold Films', 'id': 2266}, {'name': ...   \n3                                                NaN   \n4                                                NaN   \n\n                                production_countries release_date  runtime  \\\n0  [{'iso_3166_1': 'US', 'name': 'United States o...      2/20/15     93.0   \n1  [{'iso_3166_1': 'US', 'name': 'United States o...       8/6/04    113.0   \n2  [{'iso_3166_1': 'US', 'name': 'United States o...     10/10/14    105.0   \n3            [{'iso_3166_1': 'IN', 'name': 'India'}]       3/9/12    122.0   \n4      [{'iso_3166_1': 'KR', 'name': 'South Korea'}]       2/5/09    118.0   \n\n                                    spoken_languages    status  \\\n0           [{'iso_639_1': 'en', 'name': 'English'}]  Released   \n1           [{'iso_639_1': 'en', 'name': 'English'}]  Released   \n2           [{'iso_639_1': 'en', 'name': 'English'}]  Released   \n3  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...  Released   \n4           [{'iso_639_1': 'ko', 'name': '한국어/조선말'}]  Released   \n\n                                             tagline  \\\n0  The Laws of Space and Time are About to be Vio...   \n1  It can take a lifetime to find true love; she'...   \n2    The road to greatness can take you to the edge.   \n3                                                NaN   \n4                                                NaN   \n\n                                      title  \\\n0                    Hot Tub Time Machine 2   \n1  The Princess Diaries 2: Royal Engagement   \n2                                  Whiplash   \n3                                   Kahaani   \n4                                Marine Boy   \n\n                                            Keywords  \\\n0  [{'id': 4379, 'name': 'time travel'}, {'id': 9...   \n1  [{'id': 2505, 'name': 'coronation'}, {'id': 42...   \n2  [{'id': 1416, 'name': 'jazz'}, {'id': 1523, 'n...   \n3  [{'id': 10092, 'name': 'mystery'}, {'id': 1054...   \n4                                                NaN   \n\n                                                cast  \\\n0  [{'cast_id': 4, 'character': 'Lou', 'credit_id...   \n1  [{'cast_id': 1, 'character': 'Mia Thermopolis'...   \n2  [{'cast_id': 5, 'character': 'Andrew Neimann',...   \n3  [{'cast_id': 1, 'character': 'Vidya Bagchi', '...   \n4  [{'cast_id': 3, 'character': 'Chun-soo', 'cred...   \n\n                                                crew   revenue  \n0  [{'credit_id': '59ac067c92514107af02c8c8', 'de...  12314651  \n1  [{'credit_id': '52fe43fe9251416c7502563d', 'de...  95149435  \n2  [{'credit_id': '54d5356ec3a3683ba0000039', 'de...  13092000  \n3  [{'credit_id': '52fe48779251416c9108d6eb', 'de...  16000000  \n4  [{'credit_id': '52fe464b9251416c75073b43', 'de...   3923970  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>belongs_to_collection</th>\n      <th>budget</th>\n      <th>genres</th>\n      <th>homepage</th>\n      <th>imdb_id</th>\n      <th>original_language</th>\n      <th>original_title</th>\n      <th>overview</th>\n      <th>popularity</th>\n      <th>poster_path</th>\n      <th>production_companies</th>\n      <th>production_countries</th>\n      <th>release_date</th>\n      <th>runtime</th>\n      <th>spoken_languages</th>\n      <th>status</th>\n      <th>tagline</th>\n      <th>title</th>\n      <th>Keywords</th>\n      <th>cast</th>\n      <th>crew</th>\n      <th>revenue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[{'id': 313576, 'name': 'Hot Tub Time Machine ...</td>\n      <td>14000000</td>\n      <td>[{'id': 35, 'name': 'Comedy'}]</td>\n      <td>NaN</td>\n      <td>tt2637294</td>\n      <td>en</td>\n      <td>Hot Tub Time Machine 2</td>\n      <td>When Lou, who has become the \"father of the In...</td>\n      <td>6.575393</td>\n      <td>/tQtWuwvMf0hCc2QR2tkolwl7c3c.jpg</td>\n      <td>[{'name': 'Paramount Pictures', 'id': 4}, {'na...</td>\n      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n      <td>2/20/15</td>\n      <td>93.0</td>\n      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n      <td>Released</td>\n      <td>The Laws of Space and Time are About to be Vio...</td>\n      <td>Hot Tub Time Machine 2</td>\n      <td>[{'id': 4379, 'name': 'time travel'}, {'id': 9...</td>\n      <td>[{'cast_id': 4, 'character': 'Lou', 'credit_id...</td>\n      <td>[{'credit_id': '59ac067c92514107af02c8c8', 'de...</td>\n      <td>12314651</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[{'id': 107674, 'name': 'The Princess Diaries ...</td>\n      <td>40000000</td>\n      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...</td>\n      <td>NaN</td>\n      <td>tt0368933</td>\n      <td>en</td>\n      <td>The Princess Diaries 2: Royal Engagement</td>\n      <td>Mia Thermopolis is now a college graduate and ...</td>\n      <td>8.248895</td>\n      <td>/w9Z7A0GHEhIp7etpj0vyKOeU1Wx.jpg</td>\n      <td>[{'name': 'Walt Disney Pictures', 'id': 2}]</td>\n      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n      <td>8/6/04</td>\n      <td>113.0</td>\n      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n      <td>Released</td>\n      <td>It can take a lifetime to find true love; she'...</td>\n      <td>The Princess Diaries 2: Royal Engagement</td>\n      <td>[{'id': 2505, 'name': 'coronation'}, {'id': 42...</td>\n      <td>[{'cast_id': 1, 'character': 'Mia Thermopolis'...</td>\n      <td>[{'credit_id': '52fe43fe9251416c7502563d', 'de...</td>\n      <td>95149435</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>3300000</td>\n      <td>[{'id': 18, 'name': 'Drama'}]</td>\n      <td>http://sonyclassics.com/whiplash/</td>\n      <td>tt2582802</td>\n      <td>en</td>\n      <td>Whiplash</td>\n      <td>Under the direction of a ruthless instructor, ...</td>\n      <td>64.299990</td>\n      <td>/lIv1QinFqz4dlp5U4lQ6HaiskOZ.jpg</td>\n      <td>[{'name': 'Bold Films', 'id': 2266}, {'name': ...</td>\n      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n      <td>10/10/14</td>\n      <td>105.0</td>\n      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n      <td>Released</td>\n      <td>The road to greatness can take you to the edge.</td>\n      <td>Whiplash</td>\n      <td>[{'id': 1416, 'name': 'jazz'}, {'id': 1523, 'n...</td>\n      <td>[{'cast_id': 5, 'character': 'Andrew Neimann',...</td>\n      <td>[{'credit_id': '54d5356ec3a3683ba0000039', 'de...</td>\n      <td>13092000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>1200000</td>\n      <td>[{'id': 53, 'name': 'Thriller'}, {'id': 18, 'n...</td>\n      <td>http://kahaanithefilm.com/</td>\n      <td>tt1821480</td>\n      <td>hi</td>\n      <td>Kahaani</td>\n      <td>Vidya Bagchi (Vidya Balan) arrives in Kolkata ...</td>\n      <td>3.174936</td>\n      <td>/aTXRaPrWSinhcmCrcfJK17urp3F.jpg</td>\n      <td>NaN</td>\n      <td>[{'iso_3166_1': 'IN', 'name': 'India'}]</td>\n      <td>3/9/12</td>\n      <td>122.0</td>\n      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n      <td>Released</td>\n      <td>NaN</td>\n      <td>Kahaani</td>\n      <td>[{'id': 10092, 'name': 'mystery'}, {'id': 1054...</td>\n      <td>[{'cast_id': 1, 'character': 'Vidya Bagchi', '...</td>\n      <td>[{'credit_id': '52fe48779251416c9108d6eb', 'de...</td>\n      <td>16000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>[{'id': 28, 'name': 'Action'}, {'id': 53, 'nam...</td>\n      <td>NaN</td>\n      <td>tt1380152</td>\n      <td>ko</td>\n      <td>마린보이</td>\n      <td>Marine Boy is the story of a former national s...</td>\n      <td>1.148070</td>\n      <td>/m22s7zvkVFDU9ir56PiiqIEWFdT.jpg</td>\n      <td>NaN</td>\n      <td>[{'iso_3166_1': 'KR', 'name': 'South Korea'}]</td>\n      <td>2/5/09</td>\n      <td>118.0</td>\n      <td>[{'iso_639_1': 'ko', 'name': '한국어/조선말'}]</td>\n      <td>Released</td>\n      <td>NaN</td>\n      <td>Marine Boy</td>\n      <td>NaN</td>\n      <td>[{'cast_id': 3, 'character': 'Chun-soo', 'cred...</td>\n      <td>[{'credit_id': '52fe464b9251416c75073b43', 'de...</td>\n      <td>3923970</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Some columns have data in the format of list of objects. These cannot be directly used as input for learning algorithm so we convert them to CSV (comma-separated values) by taking the attribute that gives us more information and discard the rest. For example, for `genres` column we project the `name` attribute of the object and discard `id`s."},{"metadata":{"trusted":false},"cell_type":"code","source":"import ast\n\ndef flatten_field(field, attribute):\n    \"\"\"\n    Convert a field from Python AST representation to a plain CSV string. To do that, project\n    only `attribute` and ignore remaining field of the object.\n    \n    For example, the original field\n        {'id': 35, 'name': 'Comedy'}, {'id': 18, 'name': 'Action'}\n    is converted to\n        \"comedy,action\"\n        \n    Note that this function converts all values to lower case.\n    \"\"\"\n    if pd.isna(field):\n        return ''\n    else:\n        obj_list = ast.literal_eval(field)\n        result = []\n        for obj in obj_list:\n            result.append(obj[attribute].lower())\n            \n    return ','.join(result)\n\n\ndef flatten_data(data):\n    col_attribute_mapping = {'belongs_to_collection': 'name', \n                             'genres': 'name', \n                             'production_countries': 'iso_3166_1',\n                             'production_companies': 'name',\n                             # For spoken language we actually want the ISO code instead of name to \n                             # avoid non-ASCII characters.\n                             'spoken_languages': 'iso_639_1',\n                             'Keywords': 'name', \n                             'cast': 'name', \n                             'crew': 'name'}\n    \n    for col in col_attribute_mapping.keys():\n        data[col] = data.apply(lambda row: flatten_field(row[col], col_attribute_mapping[col]), \n                                           axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now actually calling the function to flatten data."},{"metadata":{"trusted":false},"cell_type":"code","source":"flatten_data(train_data)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the code cell below to generate a CSV file with the flatten data."},{"metadata":{"trusted":false},"cell_type":"code","source":"train_data.to_csv('train_flatten.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"We can start thinking about variable selection, that is, what columns do we think that can provide us information predicting the revenue. Some variables are obvious candidates: barring some big flops, films with bigger budgets are better produced and consequently interest more people. `popularity` is another good one, as it indirectly tells how many people have watched, or are at least familiar with the film. There are other less obvious columns that can still provide good information, like genres column, as some genres are more popular than others. This is also the case for `spoken_languages` column, as it tells us what is the main market for the movie.\nLet's now investigate the distribution of `revenue` variable."},{"metadata":{"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,5))\nfig.suptitle('Revenue Distribution', fontsize=15)\nsns.distplot(train_data['revenue'], bins=50, kde=False)\nax.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the distribution of revenue is right skewed, and this is a problem for linear regression. Later on we will see how to transform the target variable for a better fit."},{"metadata":{},"cell_type":"markdown","source":"Some plots to give us an idea of how some of the variables relate to the target variable, `revenue`."},{"metadata":{},"cell_type":"markdown","source":"_Budget vs Revenue_"},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.jointplot(x=\"budget\", y=\"revenue\", data=train_data, height=11, ratio=4, color=\"g\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_Popularity vs Revenue_"},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.jointplot(x=\"popularity\", y=\"revenue\", data=train_data, height=11, ratio=4, color=\"g\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_Year vs Average Revenue_"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Copied from https://www.kaggle.com/kamalchhirang/eda-feature-engineering-lgb-xgb-cat.\n#Since only last two digits of year are provided, this is the correct way of getting the year.\ntrain_data[['release_month','release_day','release_year']] = train_data['release_date'].str.split('/',expand=True).replace(np.nan, -1).astype(int)\n# Some rows have 4 digits of year instead of 2, that's why I am applying (train['release_year'] < 100) this condition\ntrain_data.loc[ (train_data['release_year'] <= 19) & (train_data['release_year'] < 100), \"release_year\"] += 2000\ntrain_data.loc[ (train_data['release_year'] > 19)  & (train_data['release_year'] < 100), \"release_year\"] += 1900","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"avg_revenue_by_year = train_data.groupby(['release_year'], as_index=False)['revenue'].mean()\n\nsns.jointplot(x='release_year', y='revenue', data=avg_revenue_by_year, height=11, ratio=4, color=\"g\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_Original Language vs Average Revenue_"},{"metadata":{"trusted":false},"cell_type":"code","source":"avg_revenue_by_language = train_data.groupby(['original_language'], as_index=False)['revenue'].mean()\nplt.figure(figsize=(16, 6))\nsns.barplot(x='original_language', y='revenue', data=avg_revenue_by_language)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A Simple Linear Regression Model"},{"metadata":{},"cell_type":"markdown","source":"Our first attempt is to create a simple linear model including only numerical variables:"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Note we are not including 'id' column.\ntrain_data_subset = train_data[['budget', 'popularity', 'revenue']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before creating a simple model using Linear Regression we need to make sure there are no NA values in the dataset:"},{"metadata":{"trusted":false},"cell_type":"code","source":"np.sum(pd.isna(train_data_subset))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see there are two rows with NA in the runtime column so we removem them before fitting the linear regression model:"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_data_subset = train_data_subset.fillna(0)\n\nnp.sum(pd.isna(train_data_subset))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now procceed to fit and evaluate a simple linear regression model. The library `statsmodels` offers many statistical methods, including linear regression. As these offer out-of-the-box model summaries we use them. Later on we will use the version present in `scikit-learn` library."},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy import stats\n\nrevenue = stats.boxcox(train_data_subset['revenue'])[0]\n\nrevenue.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import statsmodels\nimport statsmodels.formula.api as smf\n\n\nlr_model = smf.ols('revenue ~ budget + popularity', data=train_data_subset).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"lr_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some observations about the model summary above:\n* F-statistic is 1593, way higher than 1. This offers compelling evidence that the null hypothesis - that all coefficients are zero - is not true.\n* After some assurance from F-statistic, we look into `t` values for each coefficient. Again, we have high values and they are all significant $P>|t| < 0.05$.\n* The $R^2$ of around 0.61 shows the model fits well the data.\n\nTaking `budget` coefficient as an example, we see that a 100,000 dollars increase in the budget is associated with an additional 248,230 dollars in revenue.\n"},{"metadata":{},"cell_type":"markdown","source":"### Assessing the Accuracy of the Model\n\n#### Residual Standard Error\n\nThe residual standard error is defined as\n\n$$RSE=\\sqrt{\\frac{1}{n-p-1}RSS}$$\n\nwhere\n\n$$RSS=\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2$$"},{"metadata":{},"cell_type":"markdown","source":"$n$ is the number of observations and $p$ is the number of predictors."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Number of observations, number of columns\nn, n_cols = train_data_subset.shape\n# Number of predictors\np = n_cols - 1\nrss = np.sum(np.power(lr_model.resid, 2))\nrse = np.sqrt((1/(n-p-1)) * rss)\nrse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"$RSE$ is an estimate of the standard deviation of the error. Roughly speaking,  it is the average amount that the response will deviate from the true regression line. In our example, it means that the predicted value will deviate from true value by approximately $85424537, on average. To have a better idea of the impact of this absolute number we compare it to the average revenue:"},{"metadata":{"trusted":false},"cell_type":"code","source":"rse / np.mean(train_data_subset['revenue'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's conflicting information: the high $R^2$ value indicates there's a good fit but the high $RSE$ shows the opposite, that the predicted values deviate from the true model. In the next session we will see how we can transform our data to get better results."},{"metadata":{},"cell_type":"markdown","source":"### Checking Linear Regression Assumptions"},{"metadata":{},"cell_type":"markdown","source":"When we fit a linear regression to data we assume the following holds:\n1. The relationship between the response variable $y$ and the predictors is approximately linear.\n2. The error term $\\epsilon$ has zero mean.\n3. The error term $\\epsilon$ has constant variance $\\sigma^2$.\n4. The errors are uncorrelated.\n5. The errors are normally distributed.\n\nTaken together, assumptions 4 and 5 imply that the errors are independent random variables."},{"metadata":{},"cell_type":"markdown","source":"#### Linearity of Predictors-Response Relationship\n\nWe can use residual plots to identify non-linearity."},{"metadata":{"trusted":false},"cell_type":"code","source":"#fig, ax = plt.subplots(figsize=(6,2.5))\n#ax.scatter(lr_model.fittedvalues, lr_model.resid)\nimport seaborn as sns\n\nfit_data = pd.DataFrame({'Fitted': lr_model.fittedvalues, 'Residuals': lr_model.resid})\n\nsns.regplot(x='Fitted', y='Residuals', data=fit_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's no much pattern in the residuals but there's a strong concentration of points in the lower left corner. This could indicate a non-linearity in the data."},{"metadata":{},"cell_type":"markdown","source":"#### Normality"},{"metadata":{"trusted":false},"cell_type":"code","source":"import scipy as sp\nfig, ax = plt.subplots(figsize=(6,2.5))\n_, (__, ___, r) = sp.stats.probplot(lr_model.resid, plot=ax, fit=True)\nr**2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictive Performance\n\nNow we test the predictive performance of the linear model on a hold-out test set. To do that we take the original training set and split that into a new training set and a test set. We then train the model on the new training set, and predict values for the test set. As we have the labels for the test set we created we are able to measure the performance."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\ny = train_data['revenue']\nX = train_data[['budget', 'popularity']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nlm = LinearRegression().fit(X_train, y_train)\ny_predict = lm.predict(X_test)\ny_predict[y_predict < 0] = 0\nprint(\"Mean squared error (linear model): {:.2f}\".format(mean_squared_error(y_test, y_predict)))\nprint(\"Mean squared log error (linear model): {:.2f}\".format(mean_squared_log_error(y_test, y_predict)))\nprint(\"r2_score (linear model): {:.2f}\".format(r2_score(y_test, y_predict)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scaling data:"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\ny = train_data['revenue']\nX = train_data[['budget', 'popularity']]\n\nX.head()\n\nX.loc[:,['budget', 'popularity']] = StandardScaler().fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nlm = LinearRegression().fit(X_train, y_train)\ny_predict = lm.predict(X_test)\ny_predict[y_predict < 0] = 0\nprint(\"Mean squared error (linear model): {:.2f}\".format(mean_squared_error(y_test, y_predict)))\nprint(\"Mean squared log error (linear model): {:.2f}\".format(mean_squared_log_error(y_test, y_predict)))\nprint(\"r2_score (linear model): {:.2f}\".format(r2_score(y_test, y_predict)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y = np.log(train_data['revenue'] + 1)\nX = train_data[['budget', 'popularity']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nlm_log = LinearRegression().fit(X_train, y_train)\ny_predict = lm_log.predict(X_test)\ny_predict[y_predict < 0] = 0\nprint(\"Mean squared error (linear model, log(revenue)): {:.2f}\".format(mean_squared_error(y_test, y_predict)))\nprint(\"Mean squared log error (linear model, log(revenue)): {:.2f}\".format(mean_squared_log_error(y_test, y_predict)))\nprint(\"r2_score (linear model(log(revenue))): {:.2f}\".format(r2_score(y_test, y_predict)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Preparing submission from linear model and linear model on log-transformed data.\n\ntest_data = pd.read_csv('../input/test.csv')\nids_test = test_data['id']\ntest_data_subset = test_data[['budget', 'popularity']]\n\ny = train_data['revenue']\nX = train_data[['budget', 'popularity']]\n\nmean_y = np.mean(y)\n\nlm = LinearRegression().fit(X, y)\ny_predict = lm.predict(test_data_subset)\n\npred_df = pd.DataFrame({'id': ids_test, 'revenue': y_predict})\npred_df.loc[pred_df.revenue < 0, ['revenue']] = mean_y\npred_df.to_csv('lm_predictions.csv', index=False)\n\ny = np.log(train_data['revenue'] + 1)\nX = train_data[['budget', 'popularity']]\n\nlm_log = LinearRegression().fit(X, y)\ny_predict = lm_log.predict(test_data_subset)\n\npred_df = pd.DataFrame({'id': ids_test, 'revenue': np.exp(y_predict) - 1})\npred_df.loc[pred_df.revenue < 0, ['revenue']] = mean_y\npred_df.to_csv('lm_predictions_log.csv', index=False)\npred_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"mean_y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding Qualitative Predictors\n\nSo far we only included numerical predictors into our model. We can also include qualitative predictors into our model provided we convert them to a suitable representation.\n\nIt makes sense to assume that the language of the movie impacts revenue, as it is a proxy to the country it was filmed and consequently size of the market."},{"metadata":{},"cell_type":"markdown","source":"## Tree-based Methods"},{"metadata":{},"cell_type":"markdown","source":"We procceed to fit XGBoost to the data to check how it performs. First, let's load again the data and preprocess it."},{"metadata":{"trusted":false},"cell_type":"code","source":"train_data = pd.read_csv('../input/train.csv')\nflatten_data(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create the dataset that will be using for training our model. We will take some columns as is, dummy encode others and discard the rest.\n\nSome columns have a high number of unique values and adding dummies for all values may not help the model. One way of going about it is to select the top $N$ most frequent values for the field and encode based on those. Instead, we dummify the field and remove resulting columns that have low number of 1 values, which has the same effect."},{"metadata":{"trusted":false},"cell_type":"code","source":"def count_unique(data, field_name):\n    s = set()\n    \n    def count(field):\n        for genre in field.split(','):\n            s.add(genre)\n        \n    data.apply(lambda x: count(x[field_name]), axis=1)\n    return s\n\nprint(\"Distinct values for 'genres':\", len(count_unique(train_data, 'genres')))\nprint(\"Distinct values for 'original_language':\", len(count_unique(train_data, 'original_language')))\nprint(\"Distinct values for 'production_companies':\", len(count_unique(train_data, 'production_companies')))\nprint(\"Distinct values for 'production_countries':\", len(count_unique(train_data, 'production_countries')))\nprint(\"Distinct values for 'spoken_languages':\", len(count_unique(train_data, 'spoken_languages')))\nprint(\"Distinct values for 'Keywords':\", len(count_unique(train_data, 'Keywords')))\nprint(\"Distinct values for 'cast':\", len(count_unique(train_data, 'cast')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def is_high_variance(data):\n    return (np.sum(data) / data.shape[0]) > 0.15\n\n\ndef dummify_columns(data, add_label=True):\n    # production_companies, Keywords and cast columns will be dealt with later.\n    columns_to_dummify = ['genres', 'original_language', 'production_countries', 'spoken_languages']\n    processed_data = data\n\n    for column in columns_to_dummify:\n        # Split column in values separated by comma and create dummies based on that instead\n        # of considering the column value as a whole.\n        dummies = processed_data[column].str.get_dummies(sep=\",\")\n        processed_data.drop([column], axis=1, inplace=True)\n        # Prefix dummy columns with the name of the original column so we don't end up with duplicated\n        # column names when columns share the same level (what breaks XGBoost).\n        dummies.columns = [column + \"_\" + dummy_column for dummy_column in dummies.columns ]\n        before_len = dummies.shape[1]\n        # Get rid of columns that do not provide much information.\n        dummies = dummies.loc[:, is_high_variance(dummies)]\n        after_len = dummies.shape[1]\n        print(column, before_len, ' -> ', after_len)\n        processed_data = pd.concat([processed_data, dummies], axis=1)\n        \n    return processed_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"processed_train_data = dummify_columns(train_data)\nprocessed_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split `release_date` into separate fields and use only month and year."},{"metadata":{"trusted":false},"cell_type":"code","source":"from datetime import datetime\n\ndef add_delta_col(row):\n    format = \"%m/%d/%Y\"\n    first_day_str = '1/1/' + str(row['release_year'])\n    first_date = datetime.strptime(first_day_str, format)\n    release_date_str = str(row['release_month']) + '/' + str(row['release_day']) + '/' + str(row['release_year'])\n    release_date = datetime.strptime(release_date_str, format)\n    \n    return (release_date - first_date).days\n    \n\ndef add_date_features(data):\n    date_format = \"%m/%d/%Y\"\n    \n    # Split 'release_date' into separate fields for month and year, ignoring day.\n    date_features = data['release_date'].str.split('/', expand=True).replace(np.nan, 1).astype(int)\n    data['release_month'] = date_features[0]\n    data['release_day'] = date_features[1]\n    data['release_year'] = date_features[2]\n    data.loc[(data['release_year'] <= 19) & (data['release_year'] < 100), \"release_year\"] += 2000\n    data.loc[(data['release_year'] > 19)  & (data['release_year'] < 100), \"release_year\"] += 1900\n    data['release_date_delta'] = data.apply(add_delta_col, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"add_date_features(processed_train_data)\nprocessed_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Testing the performance of the system on a hold-out testset."},{"metadata":{"trusted":false},"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n\ndef report_scores(y, y_hat):\n    print(\"Mean squared error (XGBoost): {:.2f}\".format(mean_squared_error(y_test, y_predict)))\n    print(\"Mean squared log error (XGBoost): {:.2f}\".format(mean_squared_log_error(y_test, y_predict)))\n    print(\"r2_score (XGBoost): {:.2f}\".format(r2_score(y_test, y_predict)))    \n\n    \ny = processed_train_data['revenue']\nX = processed_train_data.drop(['revenue'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nxgb_model = XGBRegressor(objective=\"reg:linear\", random_state=0).fit(X_train, y_train)\ny_predict = xgb_model.predict(X_test)\ny_predict[y_predict < 0] = 0\nreport_scores(y_test, y_predict)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparing data to find best parameters using grid search."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\n\ntrain_data = pd.read_csv('../input/train.csv')\nflatten_data(train_data)\nadd_date_features(train_data)\ncolumns_to_drop = ['id', 'belongs_to_collection', 'homepage', 'imdb_id', 'original_title', 'overview',\n                   'poster_path', 'status', 'tagline', 'title', 'Keywords', 'cast', 'crew',\n                  'production_companies', 'release_date', 'release_day']\n\ntrain_data = train_data.drop(columns_to_drop, axis=1)\ntrain_data = dummify_columns(train_data)\n\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"#scorer = make_scorer(mean_squared_error, greater_is_better=False)\nfrom scipy import stats, special\n\ngrid_values = {\n    'colsample_bytree': [0.5],\n    'subsample': [0.9],\n    'max_depth': [3, 5, 7],\n    'n_estimators': [300, 500, 600, 700, 800],\n    'gamma': [0],\n    'min_child_weight': [2, 3, 4, 5],\n    'learning_rate': [0.01, 0.02, 0.03, 0.04, 0.05]\n}\n\ny = np.log(train_data['revenue'] + 1)\nX = train_data.drop(['revenue'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n# UNCOMMENT NEXT LINE TO PERFORM GRID SEARCH.\n#xgb_model = XGBRegressor(objective=\"reg:linear\", n_jobs=4, random_state=0)\n# Best so far.\n# xgb_model = XGBRegressor(objective=\"reg:linear\", n_jobs=4, learning_rate=0.05, n_estimators=300, max_depth=3, min_child_weight=5, random_state=0)\n# Experimenting with tuning colsample_bytree and subsample first.\n#grid_clf_acc = GridSearchCV(xgb_model, grid_values, fit_params={'eval_metric': 'rmse'}, scoring=scorer)\ngrid_clf_acc = GridSearchCV(xgb_model, grid_values, n_jobs=4, cv=5, fit_params={'eval_metric': 'rmse'}, scoring='neg_mean_squared_error')\ngrid_clf_acc.fit(X_train, y_train)\n#y_decision_fn_scores_acc = grid_clf_acc.decision_function(X_test) \n\nprint('Grid best parameter (max. MSE): ', grid_clf_acc.best_params_)\nprint('Grid best score (MSE): ', grid_clf_acc.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing model feature importances:"},{"metadata":{"trusted":false},"cell_type":"code","source":"xgb.plot_importance(grid_clf_acc.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a submission for XGBoost."},{"metadata":{"trusted":false},"cell_type":"code","source":"train_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')\nlen_train_data = train_data.shape[0]\ny = np.log(train_data['revenue'] + 1)\ntrain_data = train_data.drop(['revenue'], axis=1)\nids_test = test_data['id']\n\n# Create a single dataset to do preprocessing and split for training and prediction.\ndata = pd.concat([train_data, test_data], axis=0)\nflatten_data(data)\nadd_date_features(data)\ncolumns_to_drop = ['id', 'belongs_to_collection', 'homepage', 'imdb_id', 'original_title', 'overview',\n                   'poster_path', 'status', 'tagline', 'title', 'Keywords', 'cast', 'crew',\n                  'production_companies', 'release_date', 'release_day']\ndata = data.drop(columns_to_drop, axis=1)\ndata = dummify_columns(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import xgboost as xgb\n\nX_train = data[0:len_train_data]\nX_test = data[len_train_data:data.shape[0]]\nxgb_model = XGBRegressor(objective=\"reg:linear\", n_jobs=4, learning_rate=0.03, n_estimators=500, max_depth=3, min_child_weight=5, colsample_bytree=0.5, subsample=0.9, random_state=0)\nxgb_model.fit(X_train, y, eval_metric='rmse')\n\ny_predict = np.exp(xgb_model.predict(X_test)) - 1\n\npred_df = pd.DataFrame({'id': ids_test, 'revenue': y_predict})\npred_df.loc[pred_df.revenue < 0, ['revenue']] = 0\npred_df.to_csv('xgb_predictions.csv', index=False)\npred_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PCA"},{"metadata":{},"cell_type":"markdown","source":"Principal Component Analysis (PCA) transforms data by finding linear combination of predictors. The first PC captures most of the variance of the data and subsequent PCs capture increasing portions of the variance of the data. Can be used on a group of highly correlated features as an alternative of manually creating a linear combination of the features or choosing arbitrarily a subset of features. PCA creates uncorrelated components, what helps some predictive methods.\n\nPCA seeks linear combinations of features that maximize variability and thus if the measurement scales of the original predictors differ in orders of magnitude, the first PC will focus on summarizing higher magnitude predictors (e.g. income in dollars vs height in centimeters). Important to transform skewed predictors and then center and scale the data before applying PCA.\n\n(Kjell and Kuhn, Applied Predictive Modeling)"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nX_normalized = StandardScaler().fit(train_data).transform(train_data)\n\npca = PCA(n_components=2).fit(X_normalized)\n\nX_pca = pca.transform(X_normalized)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"col = X_train['release_month'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Ideas to Explore\n\n- number of days since the beginning of the year to capture seasonal trends\n- adjust budget by inflation\n- SVD on Keywords\n- check for outliers\n- transformations on numerical predictors?\n- TF-IDF and Non-negative Matrix Factorization for keywords"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}