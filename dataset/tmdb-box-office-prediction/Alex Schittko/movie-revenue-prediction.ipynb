{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Get Started\n\nAlex Schittko\n\nRobert Waguespack\n\nRun this block to set up the notebook\n\n**You must enable Internet in Kaggle before running this notebook!**\n\n**This requires you to verify your Kaggle account!**\n\n* Download CSV Files test/training sets\n* Import python dependencies\n\nGoal is to [predict box office revenues](https://www.kaggle.com/c/tmdb-box-office-prediction/submit)","metadata":{"id":"1iNojPmgLLs9"}},{"cell_type":"code","source":"output_path = \"./\" # Where are outputs stored in your notebook? (With trailing slash, eg /output/)\ninput_path = \"../input/tmdb-box-office-prediction/\" # Where can inputs be stored? (With trailing slash, eg /input/)\n\n# Uncomment these if the test.csv and train.csv don't exist in INPUT_PATH\n#!wget -q --show-progress --no-check-certificate 'https://docs.google.com/uc?export=download&id=13f3n4H67RjbEHPl_A4i9R6oY9jUa2eOm' -O {input_path}test.csv\n#!wget -q --show-progress --no-check-certificate 'https://docs.google.com/uc?export=download&id=1JxEPMg415Y6NIslXcL9mWGr8RMx86B6Y' -O {input_path}train.csv\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport missingno as mno\nimport math\nimport json\nfrom multiprocessing import Pool\nimport multiprocessing\nfrom tqdm import tqdm,trange,tqdm_notebook\nfrom time import sleep\nfrom sklearn.model_selection import train_test_split\nprint(\"Ready\")","metadata":{"id":"tx3yxnlCHAqu","outputId":"335bc65b-c603-4a53-d5ab-dc45b05be5e1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(input_path + 'train.csv', parse_dates=[\"release_date\"])\ndf_test = pd.read_csv(input_path + 'test.csv', parse_dates=[\"release_date\"])\ndf_test.info()\ndf_all = pd.concat([df_train, df_test])\ndf_all.reset_index(inplace=True)\n\n# Initialize your new features here\ndf_all['cast_json'] = \"\"\ndf_all['crew_json'] = \"\"\ndf_all.insert(0, 'popularity_cast', np.float64(0))\ndf_all.insert(0, 'popularity_crew', np.float64(0))\ndf_all.insert(0, 'has_homepage', 0)","metadata":{"id":"n6_FFjK9HBP_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The JSON payloads are not valid JSON!!\n# These functions help us parse the invalid JSON to native python objects\n# Using regular expressions\n\nimport re\ndef repl_quotes(m):\n  preq = m.group(1)\n  qbody = m.group(2)\n  qbody = re.sub(r'\"', r\"'\", qbody)\n  return preq + '\"' + qbody + '\"'\n\n\n# Thanks user1384220 of StackOverflow\n# https://stackoverflow.com/questions/62012736/regex-replace-double-quotes-in-json\n# Takes an unsafe JSON s, and returns the native py object and safe json string\ndef to_json(s):\n  safe = s.replace(\"'\", '\"')\n  safe = re.sub(r'(\"[\\s\\w]*)\"([\\s\\w]*\")',r\"\\1'\\2\", safe)  # O'Brien\n  safe = re.sub( r'([:\\[,{]\\s*)\"(.*?)\"(?=\\s*[:,\\]}])', repl_quotes, safe ) # Alex \"Nickname\" Schittko\n  safe = safe.replace(\"None\", 'null')\n  safe = safe.replace(\"\\\\'\", \"'\")\n  safe = safe.replace(\"\\\\x92\", \"'\")\n  safe = safe.replace(\"\\\\xa0\", \"-\")\n  safe = safe.replace(\"\\\\xad\", \"-\")\n\n  #print(safe)\n  try:\n    cast_json = json.loads(safe)\n  except:\n    print(\"to_json() failed for string\")\n    print(safe)\n\n  return cast_json, safe","metadata":{"id":"qJUrl1aSY51U","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This code splits df_all back to df_test/df_train\n\ndef split_from_all(df):\n    df_train = df.iloc[:3000,:]\n    df_test = df.iloc[3000:,:]\n    return df_train,df_test\n\n# We can iterate df's faster as dicts.\ndef quickIt(df):\n    data = df.to_dict('index')\n    idxs = df.index.values\n    return data, idxs\n","metadata":{"id":"zUUaviKmovNM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"1STSsKjQY47f"}},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","metadata":{"id":"lUpRJYIwHJqS"}},{"cell_type":"markdown","source":"## Training Set","metadata":{"id":"Fq2CVmjd69QC"}},{"cell_type":"code","source":"df_train.head()","metadata":{"id":"oMqxwdu57Gwf","outputId":"72733ee0-6ae8-4b06-d3d8-684a5039874a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"id":"60s188so7HzB","outputId":"f99de6e2-9be4-4df2-b94d-31ff3ae8cd99","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see between from the info output that we have some incomplete features, as well as about 3000 entries to train with.\n\nThis should be enough for some classification models, however I don't think it will be enough to construct a Neural Network.","metadata":{"id":"c4jBjHVl7KpS"}},{"cell_type":"code","source":"mno.matrix(df_train, (20,6))","metadata":{"id":"o8IOBvNDHOOL","outputId":"325efd29-1412-4777-90a5-ca746b3e31b9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By analyzing the mno \"missing number\" matrix, we can see we need to do something about these incomplete features.\n\n* belongs_to_collection\n* homepage\n* overview\n* genres\n* poster_path\n* production_companies\n* production_countries\n* runtime\n* spoken_languages\n* tagline\n* Keywords\n* cast\n* crew\n\nWe have a few options:\n\n1. Discard the feature. We should only do this if we believe the data isn't correlated.\n2. Impute on the dataset.  We could use IterativeImputer or SimpleImputer to fill in the blanks.\n3. Feature engineering.  We can extract boolean facts, eg \"has_homepage\" and replace this new feature with the current \"homepage\" feature. This only makes sense for certain features we can turn into classifications.  Eg, the presence of a homepage or tagline may have some influence on the target. ","metadata":{"id":"3aukHKuJ7deT"}},{"cell_type":"code","source":"# How else do we tell if the data is correlated","metadata":{"id":"RBDwv62XIhRJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical Features","metadata":{"id":"FX4EebJo3avx"}},{"cell_type":"markdown","source":"#### Overview","metadata":{"id":"86dRNqWM4Biu"}},{"cell_type":"markdown","source":"Here we look at categorical columns in a pie chart to understand the spread of the dataset (original code from [this notebook](https://www.kaggle.com/sisharaneranjana/titanic-survival-prediction-complete-analysis))","metadata":{"id":"0rrEfmBhzu6x"}},{"cell_type":"code","source":"categorical_cols_train= df_train.select_dtypes(include=['object'])\ncategorical_cols_test= df_test.select_dtypes(include=['object'])\nprint(f'The train dataset contains {len(categorical_cols_train.columns.tolist())} categorical columns')\nfor cols in categorical_cols_train.columns:\n    print(cols,':', len(categorical_cols_train[cols].unique()),'labels')\n\nprint(f'The test dataset contains {len(categorical_cols_test.columns.tolist())} categorical columns')\nfor cols in categorical_cols_test.columns:\n    print(cols,':', len(categorical_cols_test[cols].unique()),'labels')\n\ncategorical_cols_train.describe()","metadata":{"id":"dfNCYEBg4NdL","outputId":"36684d1f-a2b6-47ba-a798-0ee93f4839e4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The features with > 50 labels are very unique, perhaps we can use specifics about them later in our analysis for correlation.  Perhaps a movie with Danny DeVito has more revenue than one with unheard of actors?\n\nIt looks like original_language and status are small enough that we could try to see if their values correlated with revenues.\n\nWe'll need an approach to bring some order to the categorical values before trying to model the problem.","metadata":{"id":"QPRYke_oz_6s"}},{"cell_type":"code","source":"print(\"Scalar features\")\nfor col in df_all.columns:\n    if col not in categorical_cols_train:\n        print(col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Status Feature","metadata":{"id":"juD0xuk74Cu8"}},{"cell_type":"code","source":"# Checking status' values\ndf_all['status'].unique()\n\n","metadata":{"id":"CqCujU2v0UB2","outputId":"03c208fa-f338-4a54-d970-5a39572bb315","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\n\nnight_colors = ['#D3DBDD',  'navy',  '#57A7F3']\nlabels = [x for x in df_train.status.value_counts().index]\nvalues = df_train.status.value_counts()\n\n# Use `hole` to create a donut-like pie chart\nfig=go.Figure(data=[go.Pie(labels=labels,values=values,hole=.3,pull=[0,0,0.06,0])])\n\nfig.update_layout(\n    title_text=\"Training Set - Movie Status\")\nfig.update_traces(marker=dict(colors=night_colors))\nfig.show()","metadata":{"id":"2zCAo2Gy23E9","outputId":"e744b045-eb04-473f-982e-1c9f001f9182","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nnight_colors = ['#D3DBDD',  'navy',  '#57A7F3']\nlabels = [x for x in df_test.status.value_counts().index]\nvalues = df_test.status.value_counts()\n\n# Use `hole` to create a donut-like pie chart\nfig=go.Figure(data=[go.Pie(labels=labels,values=values,hole=.3,pull=[0,0,0.06,0])])\n\nfig.update_layout(\n    title_text=\"Test Set - Movie Status\")\nfig.update_traces(marker=dict(colors=night_colors))\nfig.show()","metadata":{"id":"PxTKkDHb3Q5O","outputId":"6469b0c4-25ab-4a3d-93d3-ba11561e6540","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here's a consideration - is Released & Post Production related?","metadata":{"id":"vB8VcJMu3Vxd"}},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{"id":"3-oUC_ll4I3O"}},{"cell_type":"markdown","source":"### Collection Sequence Feature\n\nWe're going to engineer a feature named \"collection_iteration_seq\" that represents which position in a series a movie is. Eg, the 3rd movie in the Dark Knight series will have a value \"3\", where the first movie will have a value \"1\"\n\nWe'll engineer another feature called \"single\" that will be boolean 0/1, if the movie is a singleton or not.\n\nWe'll use `df_all` to make sure this feature is complete.\n\nAfter we create the feature on `df_all`, `df_train` and `df_test` will be RECREATED with the new feature.  They'll be split based on having or not having the `revenue` feature.","metadata":{"id":"VBOuI4pM8cmz"}},{"cell_type":"code","source":"# Collection Iteration Feature\n\ncreated_collection_iteration = False\n\n# First we can iterate over the rows and determine who doesn't belong to a collection.\n# They'll be the first movies in their Series, for now\n\ntotalSingle = 0\nfor k, v in df_all.iterrows():\n    collect = v['belongs_to_collection']\n    if pd.isna(collect):\n      #print(\"pd.isna(collect): \" + str(collect))\n      if not created_collection_iteration:\n        try:\n          df_all.insert(k, \"collection_iteration_seq\", 0)\n          df_all.insert(k, \"single\", 0)\n          created_collection_iteration = True\n          df_all.at[k, 'collection_iteration_seq'] = 1\n          df_all.at[k, 'single'] = 1\n        except:\n          df_all.at[k, 'collection_iteration_seq'] = 1\n          df_all.at[k, 'single'] = 1\n      else:\n        df_all.at[k, 'collection_iteration_seq'] = 1\n        df_all.at[k, 'single'] = 1\n\n      totalSingle+=1\n\nprint(\"Set \" + str(totalSingle) + \" singles\")\ndf_all.info()\nmno.matrix(df_all, (10,5))","metadata":{"id":"4AHqlTFo8bAl","outputId":"d168431a-89db-4b01-af94-b0cb4bf193f2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all['collection_iteration_seq'].unique()","metadata":{"id":"KYEY2CcsLhEz","outputId":"d4a14eb0-9b5b-4b61-a68b-9a1762562c5d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identifying \"Single movies\"\n\ncollection_ids = []\ncreated_collection_id = False\n\nmoviesInSeries = 0\n# Iterate all the rows again, and safely read the JSON string in belongs_to_collection\nall = df_all.to_dict('index')","metadata":{"id":"_RuYuUsEy4Nm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"for k in all:\n  v = all[k]\n  safe = str(v['belongs_to_collection'])\n  safe = safe.replace(\"n' \", 'n')\n  safe = safe.replace(\"'\", '\"')\n  safe = safe.replace(\"\\\"s \", \"'s\")\n  safe = safe.replace(\"None\", 'null')\n  safe = safe.replace(\"N\\\"E\", \"N'E\")\n  safe = safe.replace(\"We\\\"re\", \"We're\")\n  safe = safe.replace(\"L\\\"a\", \"L'a\")\n  if safe != \"nan\": # Only get entries with a belongs_to_collection\n    parsed = json.loads(safe)\n    collection_id = parsed[0]['id']\n    collection_ids.append(collection_id)\n    # We show here there are only 0 or 1 collection entries on a movie object.\n    if (len(parsed) > 1):\n      print(parsed)\n\n    if not created_collection_id:\n      try:\n        df_all.insert(k, \"collection_id\", 0)\n        df_all.at[k, 'collection_id'] = collection_id\n        df_all.at[k, 'single'] = 0\n        created_collection_id = True\n      except:\n        df_all.at[k, 'collection_id'] = collection_id\n        df_all.at[k, 'single'] = 0\n    else:\n      df_all.at[k, 'collection_id'] = collection_id\n      df_all.at[k, 'single'] = 0\n","metadata":{"id":"JIk4-KYp_hQz"}},{"cell_type":"code","source":"df_all['collection_iteration_seq'].unique()","metadata":{"id":"88xs8cCFMEEL","outputId":"e1246149-8c36-4f44-d831-6cde2bde3870","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identifying series position for movies\n\n# Silence Pandas SettingWithCopyWarning\npd.options.mode.chained_assignment = None\nmoviesInSeries = 0\nfor cid in collection_ids:\n    movies = df_all.loc[df_all['collection_id'] == cid]\n    # SettingWithCopyWarning thrown here but it's ok, we know we're doing this on a copy\n    # We don't need the copy after we set the counter in the next loop.\n    movies.sort_values(by='release_date', inplace=True)\n    \n    counter = 1\n    # Apply the value to collection_iteration_seq\n    for k, v in movies.iterrows():\n      df_all.at[k, 'collection_iteration_seq'] = counter\n      counter += 1    \n      moviesInSeries += 1\n\nprint(\"Marked collection_iteration_seq on \" + str(moviesInSeries))\n\n# Put back Pandas SettingWithCopyWarning\npd.options.mode.chained_assignment = \"warn\"\nmno.matrix(df_all, (10,5))","metadata":{"id":"ONBuCeehLn-J","outputId":"5cc8e603-9534-4ad6-a973-bab3c03b9fe1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all['collection_iteration_seq'].unique()","metadata":{"id":"30NcsvE_MCMZ","outputId":"29e09f05-0dac-474b-cf58-76fcab2827d3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now drop collection_id and belongs_to_collection\n\ndf_all.drop(labels=['belongs_to_collection'], axis=1, inplace=True)","metadata":{"id":"3KE4RvlUHPKu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now create df_train and df_test again\ndf_train,df_test = split_from_all(df_all)","metadata":{"id":"BHxQyRWaIHlr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mno.matrix(df_train, (10,5))","metadata":{"id":"b6D3ep-tIz3j","outputId":"6b883daf-5f4c-4781-dc1d-ff7649c8877e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['single'].unique()","metadata":{"id":"UOPQbwp0IytS","outputId":"54b88e40-a30c-4730-9775-9a3871ec1333","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['collection_iteration_seq'].unique()","metadata":{"id":"gplFZ9mDIxUS","outputId":"3e619bf6-2180-4278-c472-a909d8c8a8aa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.describe()","metadata":{"id":"2cWVofTseeGn","outputId":"5aba5f8e-aead-4c63-f075-b814f6817bb4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reduce fragmenting of DataFrame\ndf_all = df_all.copy()","metadata":{"id":"PFu3R6S2ondU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"GLho4MBr41e8"}},{"cell_type":"markdown","source":"### Cast Popularity Feature\n\nWe're going to build a dataset called `df_cast` that has two features\n`name` as a key\n`rating` as a popularity rating\n\nOnce we have this dataset, we can add a feature called `popularity_cast`, which is a weighted sum/average of the cast's popularity ratings, on the `df_all` set.\n\nAfterwards, we should be able to drop the cast feature.\n","metadata":{"id":"eqr4VX_Z45_D"}},{"cell_type":"code","source":"# Here we build a list of all actors\n\nactors=[]\nall = df_all.to_dict('index')\nfor k in all:\n  v = all[k]\n  cast_str = v['cast']\n  if str(cast_str) == \"nan\":\n    continue\n  cast_json, safe = to_json(cast_str)\n  df_all.at[k,'cast_json'] = safe\n  for actor in cast_json:\n    #print(actor['name'])\n    actors.append(actor['name'])\ndf_all.drop(['cast'], axis=1)\ndf_all = df_all.copy()\n\n# Now we remove duplicates and create a dataframe to contain our actors\n\nactors=list(set(actors))\nactors_dict=[]\nfor actor in actors:\n    actors_dict.append({'name':actor,'rating':0,'movies':0})\n\ndf_cast = pd.DataFrame(actors_dict)\ndf_cast.drop_duplicates(subset=['name'], keep='first')\n# This speeds us up from 5 frames per second to thousands of frames per second, CPU Only.\ndf_cast.set_index(['name'],inplace=True)\ndf_cast.info()\ndf_cast.index.name","metadata":{"id":"70YlQYTkHy16","outputId":"8b52538d-7ea1-437d-c500-c570fcf69e2d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow!  We have 76k unique actors!  This should give us some good insight!\n\nWe're going to make a \"rating\" for each actor, then use these \"rating\"s to extract a \"cast_rating\" feature for the films.\n\nWe'll sum the popularity each film has, on each actor's record.  We'll also keep track of how many films an actor has been in.\n\nThis lets us average the score of an actor based on the movies they've participated in.","metadata":{"id":"fDsLnl9VH9xt"}},{"cell_type":"code","source":"%%time\n# This part computes the sum of movies & ratings in the df_cast dataframe\n#dict_cast = df_cast.to_dict('records')\nall = df_all.to_dict('index')\n\nidxs = df_all.index.values\nfor k in tqdm(idxs,desc=\"Computing sums\",unit=\"Film\"):\n    #print(\"k: \" + str(k))\n    v = all[k]\n    cast_json = v['cast_json']\n    popularity = v['popularity']\n    #print(cast_json)\n    #print(popularity)\n    if str(cast_json) == \"nan\" or str(cast_json) == \"\":\n      #print(\"bail\")\n      continue\n    actors = json.loads(cast_json)\n    #print(type(actors))\n    for actor in actors:\n      #print(actor)\n      idx = actor['name']\n      df_cast.at[idx, 'rating'] += popularity\n      df_cast.at[idx, 'movies'] += 1\n\n\n#print(df_cast[0\ndf_cast.info()\ndf_cast.describe()\n","metadata":{"id":"iocjb1uLJ_E-","outputId":"aaa255cd-cd91-474c-a9a7-dcee468d5b50","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we compute the average ratings per actor\ncast_idxs = df_cast.index.values\ncast = df_cast.to_dict('index')\nfor k in tqdm(cast_idxs,desc=\"Computing averages\",unit=\"Actor\"):\n  v = cast[k]\n  sum_movies = v['movies']\n  sum_rating = v['rating']\n  try:\n    new_rating = sum_rating / sum_movies \n  except:\n    new_rating = 0\n  \n  df_cast.at[k,'rating'] = new_rating","metadata":{"id":"91VFfDkMy4Nr","outputId":"d6e3734d-741f-4aac-cd2a-5de6e5664dca","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cast.describe()","metadata":{"id":"CTLRJKemy4Nr","outputId":"6d0b3ba0-bfb3-4678-f004-9e0e2f8c05f2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see what the new feature looks like\nax = plt.gca()\n\ndf_cast.plot(kind='scatter',x='movies',y='rating',color='blue',ax=ax)\n\nplt.show()","metadata":{"id":"S_c9Hp36hgah","outputId":"3c21d1a3-96bd-443e-f060-aff1efab4f46","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So this plot shows us that actors with more movies typically have a lower rating.\n\nNow we're going to iterate all the films once more, and engineer this `popularity_cast` rating ","metadata":{"id":"SbAEhF68y4Ns"}},{"cell_type":"code","source":"for idx in tqdm(idxs,desc=\"Comptuing film popularity_cast\",unit=\"Film\"):\n    film = all[idx]\n    popularity_cast = 0\n    count = 0\n    cast_json = df_all.at[idx,'cast_json']\n    if cast_json == \"\":\n        continue\n    try:\n        actors = json.loads(cast_json)\n        if len(actors) > 0:\n            for actor in actors:\n                popularity_cast += df_cast.at[actor['name'],'rating']\n                count+=1\n    except Exception as e:\n        print(\"Failed for film\")\n        print(film)\n        print(e)\n        \n    try:\n        rating = popularity_cast / count\n    except:\n        rating = 0\n    \n    if (rating > 100):\n        print(film['original_title'] + \" \" + str(rating))\n    \n    df_all.at[idx,'popularity_cast'] = rating\n\ndf_all[df_all['original_title'] == 'Minions']['popularity_cast']","metadata":{"id":"QY2Uwyory4Ns","outputId":"a7bce31a-ba58-4879-936c-d14766db2b2b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all.describe()","metadata":{"id":"Cmlxzw3Vy4Ns","outputId":"0a464d0e-6457-4790-ad6e-cecd6e816e41","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see what it looks like\nax = plt.gca()\n\ndf_all.plot(kind='scatter',x='revenue',y='popularity_cast',color='blue',ax=ax, figsize=(12,8))\n\nplt.show()\n\nax = plt.gca()\n\ndf_all.plot(kind='scatter',x='revenue',y='popularity',color='red',ax=ax, figsize=(12,8))\n\nplt.show()","metadata":{"id":"kZrUPFUCy4Nt","outputId":"00d776b6-512b-4f95-ccd4-6d09d774b676","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice, this looks like a natural feature now - it's distribution looks similar to that of popularity.","metadata":{"id":"dbUgvXKUy4Nu"}},{"cell_type":"markdown","source":"### Crew Popularity Feature\n\nWe're going to build a dataset called `df_crew` that has two features\n`name` as a key\n`rating` as a popularity rating\n\nOnce we have this dataset, we can add a feature called \"popularity_crew\" to the movies data sets which is a weighted sum/average of the cast's popularity ratings.\n\nAfterwards, we should be able to drop the crew feature.\n\nThis should pretty much mirror what happened in the cast popularity feature","metadata":{"id":"sDqeQ7Zo5Pz4"}},{"cell_type":"code","source":"# Here we build a list of all cast members\n\ncrews=[]\nall = df_all.to_dict('index')\nfor k in all:\n  v = all[k]\n  crew_str = v['crew']\n  if str(crew_str) == \"nan\":\n    continue\n  crew_json, safe = to_json(crew_str)\n  df_all.at[k,'crew_json'] = safe\n  for crew in crew_json:\n    crews.append(crew['name'])\ndf_all.drop(['crew'], axis=1)\ndf_all = df_all.copy()\n\n# Now we remove duplicates and create a dataframe to contain our actors\n\ncrews=list(set(crews))\ncrews_dict=[]\nfor crew in crews:\n    crews_dict.append({'name':crew,'rating':0,'movies':0})\n\ndf_crew = pd.DataFrame(crews_dict)\ndf_crew.drop_duplicates(subset=['name'], keep='first')\n# This speeds us up from 5 frames per second to thousands of frames per second, CPU Only.\ndf_crew.set_index(['name'],inplace=True)\ndf_crew.info()\ndf_crew.index.name","metadata":{"id":"WWuUhHQQy4Nu","outputId":"ca5108a1-b0f0-4ca2-e9d0-5723cc9e57ed","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# This part computes the sum of movies & ratings in the df_crew dataframe\nall = df_all.to_dict('index')\n\nidxs = df_all.index.values\nfor k in tqdm(idxs,desc=\"Computing sums\",unit=\"Film\"):\n    v = all[k]\n    crew_json = v['crew_json']\n    popularity = v['popularity']\n    if str(crew_json) == \"nan\" or str(crew_json) == \"\":\n      continue\n    crews = json.loads(crew_json)\n    for crew in crews:\n      idx = crew['name']\n      df_crew.at[idx, 'rating'] += popularity\n      df_crew.at[idx, 'movies'] += 1\n\ndf_crew.info()\ndf_crew.describe()\n","metadata":{"id":"kV-XMQBDy4Nv","outputId":"197c9082-cba8-4e35-a586-bf0cc730fee5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we compute the average ratings per crew member\ncrew_idxs = df_crew.index.values\ncrew = df_crew.to_dict('index')\nfor k in tqdm(crew_idxs,desc=\"Computing averages\",unit=\"Actor\"):\n  v = crew[k]\n  sum_movies = v['movies']\n  sum_rating = v['rating']\n  try:\n    new_rating = sum_rating / sum_movies \n  except:\n    new_rating = 0\n  \n  df_crew.at[k,'rating'] = new_rating\n    \ndf_crew.describe()\n","metadata":{"id":"Zr3-ByWFy4Nv","outputId":"22832788-2d52-4202-8f8b-3c69160cad2a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = plt.gca()\n\ndf_crew.plot(kind='scatter',x='movies',y='rating',color='blue',ax=ax)\n\nplt.show()","metadata":{"id":"2fBPlyHwy4Nw","outputId":"d1bbaf92-93ac-47ea-bcb8-e361cdf29549","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx in tqdm(idxs,desc=\"Comptuing film popularity_crew\",unit=\"Film\"):\n    film = all[idx]\n    popularity_crew = 0\n    count = 0\n    crew_json = df_all.at[idx,'crew_json']\n    if crew_json == \"\":\n        continue\n    try:\n        crews = json.loads(crew_json)\n        if len(crews) > 0:\n            for crew in crews:\n                popularity_crew += df_crew.at[crew['name'],'rating']\n                count+=1\n    except Exception as e:\n        print(\"Failed for film\")\n        print(film)\n        print(e)\n        \n    try:\n        rating = popularity_crew / count\n    except:\n        rating = 0\n    \n    if (rating > 100):\n        print(film['original_title'] + \" \" + str(rating))\n    \n    df_all.at[idx,'popularity_crew'] = rating\n\ndf_all[df_all['original_title'] == 'Minions']['popularity_crew']","metadata":{"id":"LQUxuXIAy4Nw","outputId":"429acb8c-3d85-4996-ceca-37a1a4b6f858","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see what it looks like\nax = plt.gca()\n\ndf_all.plot(kind='scatter',x='revenue',y='popularity_crew',color='blue',ax=ax, figsize=(20,8))\ndf_all.plot(kind='scatter',x='revenue',y='popularity_cast',color='red',ax=ax, figsize=(20,8))\ndf_all.plot(kind='scatter',x='revenue',y='popularity',color='green',ax=ax, figsize=(20,8))\n\nplt.show()","metadata":{"id":"G57_2uo_1ChO","outputId":"943d037d-8af2-416f-9211-5cef57ee93f5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Keywords Feature\n\nWe should be able to extract keyword_rating feature like we do for Cast & Crew","metadata":{"id":"ziVVQF8G5dSv"}},{"cell_type":"markdown","source":"","metadata":{"id":"GtrLIZ955jD8"}},{"cell_type":"code","source":"### ","metadata":{"id":"bLGby4lX45J5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Homepage Feature\n\nWe can easily set a boolean for \"has_homepage\" and replace \"homepage\" feature with this","metadata":{"id":"ftZJMeXd5rQb"}},{"cell_type":"code","source":"data, idxs = quickIt(df_all)\n#print(data)\nfor k in idxs:\n    v = data[k]\n    if v['homepage'] == \"\" or str(v['homepage']) == \"nan\":\n      df_all.at[k,'has_homepage'] = 0\n    else:\n      df_all.at[k,'has_homepage'] = 1  ","metadata":{"id":"YGXyKDr-y4Nx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all['has_homepage'].unique()\ndf_test.info()\ndf_train, df_test = split_from_all(df_all)\ndf_test.info()\n","metadata":{"id":"GuvyZCvQ4MP6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = plt.gca()\n\ndf_all.plot(kind='scatter',x='revenue',y='has_homepage',color='blue',ax=ax, figsize=(20,8))\n\nplt.show()","metadata":{"id":"RSXpIoGB4UJt","outputId":"6b0f4e73-b727-4caf-99bf-6ce8218040bb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all.drop(labels=['homepage'], axis=1, inplace=True)\n","metadata":{"id":"14FVQ3Tg4LmQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all.describe()","metadata":{"id":"EYOU8au45WUJ","outputId":"7e33fd2a-fb2a-496a-b1c2-7e3e4b9f0317","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all.info()","metadata":{"id":"QwdaQ25a5d3m","outputId":"8e188a06-7765-4699-9e5e-a80daed6f07b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### original_language\n\nWe can replace `original_language` with a dummy because it's unique count is *low*","metadata":{"id":"kmbFuswG5xIE"}},{"cell_type":"code","source":"len(df_all['original_language'].unique())","metadata":{"id":"AYxIkvbC5r48","outputId":"25fced68-d701-46aa-bb7e-0da3dc876261","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### spoken_languages\n\nnot sure what to do with this one","metadata":{"id":"_GOJxkWV6U4H"}},{"cell_type":"markdown","source":"### Status\n\nI think any movie in post-production should go to 'released' status to be included in the larger dataset.  Do you?\n","metadata":{"id":"zFjQ3v_d6XsT"}},{"cell_type":"code","source":"","metadata":{"id":"wjwbf78T6eKy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drops\n\nLeave this last. We drop everything else we don't need for the model here.","metadata":{"id":"z5v1_i1G568z"}},{"cell_type":"code","source":"# df_all.drop([\"original_title\",\"cast\",\"crew\",\"cast_json\",\"crew_json\",\"title\",\"imdb_id\"],axis=1,inplace=True)","metadata":{"id":"a0pJjuCx6AZx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Understanding Test Dataset","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions","metadata":{"id":"k_xQvl_PQ53i"}},{"cell_type":"code","source":"df_train,df_test = split_from_all(df_all)\ndf_train.info()","metadata":{"id":"88HSWt0TREQq","outputId":"89a4dc0a-b3ca-41ff-8a98-6a8cb381ce03","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"id":"OIpWljY3RHKz","outputId":"d3a76ec8-fb94-4ceb-c0aa-3d019cb64e80","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mno.matrix(df_test,(20,6))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LinearRegressor","metadata":{}},{"cell_type":"code","source":"# Training the model\nX = df_train.drop(['single','has_homepage','cast_json','crew_json','crew','cast','Keywords','title','tagline','status','spoken_languages','release_date','production_companies','production_countries','poster_path','overview','original_title','original_language','imdb_id','genres','id','index'], axis=1)\nX.dropna(inplace=True)\nX.info()\ny = X['revenue'].to_numpy()\nX.drop(['revenue'],axis=1,inplace=True)\nX = X.to_numpy()\n\n# This makes it so the model test later sees data it's seen before\n# This concept stinks, but we want to maximize the amount of data we use for training too.\n\n_, X_test, _, y_test = train_test_split(\n     X, y, test_size=0.33)\n\nX_train = X\ny_train = y\n\nimport sklearn.ensemble as ske\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\n\ntrain_scores = []\nlr_models = []\nmax_depth_k = 2 # After about depth 14 the score stays stagnant\nfor k in tqdm(range(1,max_depth_k),desc=\"Training models\",unit=\"LinearRegression\"):\n  #regr = DecisionTreeRegressor(max_depth=k)\n  #rfeRegr = ske.RandomForestRegressor(max_depth=k)\n  lr = LinearRegression()\n  lr.fit(X_train, y_train)\n  train_scores.append(lr.score(X_train, y_train))\n  lr_models.append(lr)\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(1, max_depth_k, step=1)\ny = train_scores\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\n\nplt.xlabel('max_depth')\nplt.xticks(np.arange(0, max_depth_k, step=1)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Accuracy Score (%)')\nplt.title('n_estimators for AdaBoostRegressor + base LinearRegressor')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\nplt.axhline(y=0.80, color='r', linestyle='-')\nplt.text(0.5, 0.70, '80% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()\n","metadata":{"id":"pQoo6IWDRLiZ","outputId":"9ce8a25d-bc52-460e-81c3-f11466918824","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pydotplus\n\nfrom sklearn.tree import export_graphviz\nimport pydotplus\nfrom IPython.display import Image\n\n# This can show a decision tree for RandomForestRegressor\n#gvz = export_graphviz(selected_model.estimators_[0]) \n#graph = pydotplus.graph_from_dot_data(gvz) \n#Image(graph.create_png())","metadata":{"id":"uC1ZkI4MY-7C","outputId":"120bbd82-d391-4952-93af-c6f50fd2b9f1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RandomForestRegressor","metadata":{}},{"cell_type":"code","source":"# Training the model\nX = df_train.drop(['single','has_homepage','cast_json','crew_json','crew','cast','Keywords','title','tagline','status','spoken_languages','release_date','production_companies','production_countries','poster_path','overview','original_title','original_language','imdb_id','genres','id','index'], axis=1)\nX.dropna(inplace=True)\nX.info()\ny = X['revenue'].to_numpy()\nX.drop(['revenue'],axis=1,inplace=True)\nX = X.to_numpy()\n\n# This makes it so the model test later sees data it's seen before\n# This concept stinks, but we want to maximize the amount of data we use for training too.\n\n_, X_test, _, y_test = train_test_split(\n     X, y, test_size=0.33)\n\nX_train = X\ny_train = y\n\nimport sklearn.ensemble as ske\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import make_regression\n\ntrain_scores = []\nrandom_forest_models = []\nmax_depth_k = 21 # After about depth 14 the score stays stagnant\nfor k in tqdm(range(1,max_depth_k),desc=\"Training models\",unit=\"RandomForestRegressor\"):\n  #regr = DecisionTreeRegressor(max_depth=k)\n  regr = ske.RandomForestRegressor(max_depth=k)\n  #regr = ske.AdaBoostRegressor(base_estimator=rfeRegr,n_estimators=100)\n  regr.fit(X_train, y_train)\n  train_scores.append(regr.score(X_train, y_train))\n  random_forest_models.append(regr)\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(1, max_depth_k, step=1)\ny = train_scores\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\n\nplt.xlabel('max_depth')\nplt.xticks(np.arange(0, max_depth_k, step=1)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Accuracy Score (%)')\nplt.title('RandomForestRegressor')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\nplt.axhline(y=0.80, color='r', linestyle='-')\nplt.text(0.5, 0.70, '80% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DecisionTreeRegressor","metadata":{}},{"cell_type":"code","source":"# Training the model\nX = df_train.drop(['single','has_homepage','cast_json','crew_json','crew','cast','Keywords','title','tagline','status','spoken_languages','release_date','production_companies','production_countries','poster_path','overview','original_title','original_language','imdb_id','genres','id','index'], axis=1)\nX.dropna(inplace=True)\nX.info()\ny = X['revenue'].to_numpy()\nX.drop(['revenue'],axis=1,inplace=True)\nX = X.to_numpy()\n\n# This makes it so the model test later sees data it's seen before\n# This concept stinks, but we want to maximize the amount of data we use for training too.\n\n_, X_test, _, y_test = train_test_split(\n     X, y, test_size=0.33)\n\nX_train = X\ny_train = y\n\nimport sklearn.ensemble as ske\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import make_regression\n\ntrain_scores = []\ndecision_tree_models = []\nmax_depth_k = 21 # After about depth 14 the score stays stagnant\nfor k in tqdm(range(1,max_depth_k),desc=\"Training models\",unit=\"DecisionTreeRegressor\"):\n  regr = DecisionTreeRegressor(max_depth=k)\n  #regr = ske.RandomForestRegressor(max_depth=k)\n  #regr = ske.AdaBoostRegressor(base_estimator=rfeRegr,n_estimators=100)\n  regr.fit(X_train, y_train)\n  train_scores.append(regr.score(X_train, y_train))\n  decision_tree_models.append(regr)\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(1, max_depth_k, step=1)\ny = train_scores\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\n\nplt.xlabel('max_depth')\nplt.xticks(np.arange(0, max_depth_k, step=1)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Accuracy Score (%)')\nplt.title('DecisionTreeRegressor')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\nplt.axhline(y=0.80, color='r', linestyle='-')\nplt.text(0.5, 0.70, '80% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AdaBoost + LinearRegression","metadata":{}},{"cell_type":"code","source":"# Training the model\nX = df_train.drop(['single','has_homepage','cast_json','crew_json','crew','cast','Keywords','title','tagline','status','spoken_languages','release_date','production_companies','production_countries','poster_path','overview','original_title','original_language','imdb_id','genres','id','index'], axis=1)\nX.dropna(inplace=True)\nX.info()\ny = X['revenue'].to_numpy()\nX.drop(['revenue'],axis=1,inplace=True)\nX = X.to_numpy()\n\n# This makes it so the model test later sees data it's seen before\n# This concept stinks, but we want to maximize the amount of data we use for training too.\n\n_, X_test, _, y_test = train_test_split(\n     X, y, test_size=0.33)\n\nX_train = X\ny_train = y\n\nimport sklearn.ensemble as ske\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\n\ntrain_scores = []\nada_lr_models = []\nmax_depth_k = 2 # After about depth 14 the score stays stagnant\nfor k in tqdm(range(1,max_depth_k),desc=\"Training models\",unit=\"LinearRegression\"):\n  #regr = DecisionTreeRegressor(max_depth=k)\n  #rfeRegr = ske.RandomForestRegressor(max_depth=k)\n  lr = LinearRegression()\n  regr = ske.AdaBoostRegressor(base_estimator=lr)\n  regr.fit(X_train, y_train)\n  train_scores.append(regr.score(X_train, y_train))\n  ada_lr_models.append(regr)\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(1, max_depth_k, step=1)\ny = train_scores\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\n\nplt.xlabel('max_depth')\nplt.xticks(np.arange(0, max_depth_k, step=1)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Accuracy Score (%)')\nplt.title('AdaBoost + Linear Regression')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\nplt.axhline(y=0.80, color='r', linestyle='-')\nplt.text(0.5, 0.70, '80% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AdaBoost + RandomForest","metadata":{}},{"cell_type":"code","source":"# SLOW! ~10-15 minutes\n# Training the model\nX = df_train.drop(['single','has_homepage','cast_json','crew_json','crew','cast','Keywords','title','tagline','status','spoken_languages','release_date','production_companies','production_countries','poster_path','overview','original_title','original_language','imdb_id','genres','id','index'], axis=1)\nX.dropna(inplace=True)\nX.info()\ny = X['revenue'].to_numpy()\nX.drop(['revenue'],axis=1,inplace=True)\nX = X.to_numpy()\n\n# This makes it so the model test later sees data it's seen before\n# This concept stinks, but we want to maximize the amount of data we use for training too.\n\n_, X_test, _, y_test = train_test_split(\n     X, y, test_size=0.33)\n\nX_train = X\ny_train = y\n\nimport sklearn.ensemble as ske\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import make_regression\n\ntrain_scores = []\nada_rf_models = []\nmax_depth_k = 14 # After about depth 15 it's useless to keep trying\nfor k in tqdm(range(0,max_depth_k),desc=\"Training models\",unit=\"AdaBoostRegressor\"):\n  #regr = DecisionTreeRegressor(max_depth=k)\n  #rfeRegr = ske.RandomForestRegressor(max_depth=k)\n  regr = ske.AdaBoostRegressor(base_estimator=random_forest_models[k])\n  regr.fit(X_train, y_train)\n  train_scores.append(regr.score(X_train, y_train))\n  ada_rf_models.append(regr)\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(0, max_depth_k, step=1)\ny = train_scores\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\n\nplt.xlabel('Random Forest #')\nplt.xticks(np.arange(0, max_depth_k, step=1)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Accuracy Score (%)')\nplt.title('AdaBoost + Random Forest')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\nplt.axhline(y=0.80, color='r', linestyle='-')\nplt.text(0.5, 0.70, '80% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AdaBoost + DecisionTree","metadata":{}},{"cell_type":"code","source":"# Training the model\nX = df_train.drop(['single','has_homepage','cast_json','crew_json','crew','cast','Keywords','title','tagline','status','spoken_languages','release_date','production_companies','production_countries','poster_path','overview','original_title','original_language','imdb_id','genres','id','index'], axis=1)\nX.dropna(inplace=True)\nX.info()\ny = X['revenue'].to_numpy()\nX.drop(['revenue'],axis=1,inplace=True)\nX = X.to_numpy()\n\n# This makes it so the model test later sees data it's seen before\n# This concept stinks, but we want to maximize the amount of data we use for training too.\n\n_, X_test, _, y_test = train_test_split(\n     X, y, test_size=0.33)\n\nX_train = X\ny_train = y\n\nimport sklearn.ensemble as ske\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import make_regression\n\ntrain_scores = []\nada_dt_models = []\nmax_depth_k = 20 # After about depth 14 the score stays stagnant\nfor k in tqdm(range(0,max_depth_k),desc=\"Training models\",unit=\"AdaBoostRegressor\"):\n  #regr = DecisionTreeRegressor(max_depth=k)\n  #rfeRegr = ske.RandomForestRegressor(max_depth=k)\n  regr = ske.AdaBoostRegressor(base_estimator=decision_tree_models[k])\n  regr.fit(X_train, y_train)\n  train_scores.append(regr.score(X_train, y_train))\n  ada_dt_models.append(regr)\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(0, max_depth_k, step=1)\ny = train_scores\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\n\nplt.xlabel('Decision Tree #')\nplt.xticks(np.arange(0, max_depth_k, step=1)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Accuracy Score (%)')\nplt.title('AdaBoost + Decision Tree')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\nplt.axhline(y=0.80, color='r', linestyle='-')\nplt.text(0.5, 0.70, '80% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GradientBoostRegressor + RandomForest","metadata":{}},{"cell_type":"code","source":"# Training the model\nX = df_train.drop(['single','has_homepage','cast_json','crew_json','crew','cast','Keywords','title','tagline','status','spoken_languages','release_date','production_companies','production_countries','poster_path','overview','original_title','original_language','imdb_id','genres','id','index'], axis=1)\nX.dropna(inplace=True)\nX.info()\ny = X['revenue'].to_numpy()\nX.drop(['revenue'],axis=1,inplace=True)\nX = X.to_numpy()\n\n# This makes it so the model test later sees data it's seen before\n# This concept stinks, but we want to maximize the amount of data we use for training too.\n\n_, X_test, _, y_test = train_test_split(\n     X, y, test_size=0.33)\n\nX_train = X\ny_train = y\n\nimport sklearn.ensemble as ske\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import make_regression\n\ntrain_scores = []\ngb_rf_models = []\nmax_depth_k = 20 # After about depth 14 the score stays stagnant\nfor k in tqdm(range(0,max_depth_k),desc=\"Training models\",unit=\"GradientBoostingRegressor\"):\n  #regr = DecisionTreeRegressor(max_depth=k)\n  #rfeRegr = ske.RandomForestRegressor(max_depth=k)\n  regr = ske.GradientBoostingRegressor(init=random_forest_models[k])\n  regr.fit(X_train, y_train)\n  train_scores.append(regr.score(X_train, y_train))\n  gb_rf_models.append(regr)\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(0, max_depth_k, step=1)\ny = train_scores\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\n\nplt.xlabel('Random Forest #')\nplt.xticks(np.arange(0, max_depth_k, step=1)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Accuracy Score (%)')\nplt.title('GradientBoostingRegressor + Random Forest')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\nplt.axhline(y=0.80, color='r', linestyle='-')\nplt.text(0.5, 0.70, '80% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GradientBoostRegressor + DecisionTree","metadata":{}},{"cell_type":"code","source":"# Training the model\nX = df_train.drop(['single','has_homepage','cast_json','crew_json','crew','cast','Keywords','title','tagline','status','spoken_languages','release_date','production_companies','production_countries','poster_path','overview','original_title','original_language','imdb_id','genres','id','index'], axis=1)\nX.dropna(inplace=True)\nX.info()\ny = X['revenue'].to_numpy()\nX.drop(['revenue'],axis=1,inplace=True)\nX = X.to_numpy()\n\n# This makes it so the model test later sees data it's seen before\n# This concept stinks, but we want to maximize the amount of data we use for training too.\n\n_, X_test, _, y_test = train_test_split(\n     X, y, test_size=0.33)\n\nX_train = X\ny_train = y\n\nimport sklearn.ensemble as ske\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import make_regression\n\ntrain_scores = []\ngb_dt_models = []\nmax_depth_k = 20 # After about depth 14 the score stays stagnant\nfor k in tqdm(range(0,max_depth_k),desc=\"Training models\",unit=\"GradientBoostingRegressor\"):\n  #regr = DecisionTreeRegressor(max_depth=k)\n  #rfeRegr = ske.RandomForestRegressor(max_depth=k)\n  regr = ske.GradientBoostingRegressor(init=decision_tree_models[k])\n  regr.fit(X_train, y_train)\n  train_scores.append(regr.score(X_train, y_train))\n  gb_dt_models.append(regr)\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(0, max_depth_k, step=1)\ny = train_scores\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\n\nplt.xlabel('Decision Tree #')\nplt.xticks(np.arange(0, max_depth_k, step=1)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Accuracy Score (%)')\nplt.title('GradientBoostingRegressor + Decision Tree')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\nplt.axhline(y=0.80, color='r', linestyle='-')\nplt.text(0.5, 0.70, '80% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Output","metadata":{}},{"cell_type":"code","source":"import datetime\n\ndef generate_submission(X, model, name):\n    y_submit = model.predict(X)\n    submit_list = []\n    idx = 3001\n    for y in y_submit:\n        submit_list.append({'id': idx, 'revenue': y})\n        idx+=1\n\n    submission = pd.DataFrame(submit_list)\n    timestamp = datetime.datetime.now().isoformat()\n    sanitized_name = \"\".join([c for c in name if re.match(r'\\w', c)])\n    submission.to_csv(output_path + sanitized_name + \"-\" + timestamp + \".csv\", index=False)\n    print(name + \" output available!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_models = [\n    {\n        'name': 'RandomForest - max depth 5',\n        'model': random_forest_models[4]\n    },\n    {\n        'name': 'RandomForest - max depth 12',\n        'model': random_forest_models[11]\n    },\n    {\n        'name': 'DecisionTree - max depth 6',\n        'model': random_forest_models[5]\n    },\n    {\n        'name': 'DecisionTree - max depth 11',\n        'model': decision_tree_models[10]\n    },\n    {\n        'name': 'AdaBoost + RandomForest 8',\n        'model': ada_rf_models[7]\n    },\n    {\n        'name': 'AdaBoost + RandomForest 11',\n        'model': ada_rf_models[10]\n    },\n    {\n        'name': 'AdaBoost + DecisionTree 6',\n        'model': ada_dt_models[5]\n    },\n    {\n        'name': 'AdaBoost + DecisionTree 9',\n        'model': ada_dt_models[8]\n    },\n    {\n        'name': 'GradientBoostRegressor + RandomForest 5',\n        'model': gb_rf_models[4]\n    },\n    {\n        'name': 'GradientBoostRegressor + RandomForest 11',\n        'model': gb_rf_models[10]\n    },\n    {\n        'name': 'GradientBoostRegressor + DecisionTree 5',\n        'model': gb_rf_models[4]\n    },\n    {\n        'name': 'GradientBoostRegressor + DecisionTree 10',\n        'model': gb_rf_models[9]\n    }\n]\n\nprint(\"Loaded models for submission\")\nfor m in selected_models:\n    print(m['model'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\ndf_test.info()\ndropped_features = ['single','has_homepage','cast_json','crew_json','crew','cast','Keywords','title','tagline','status','spoken_languages','release_date','production_companies','production_countries','poster_path','overview','original_title','original_language','imdb_id','genres','id','index']\ndf_submit = df_test.drop(dropped_features, axis=1)\ndf_runtime_null = df_submit[df_submit['runtime'].isna()]\ndf_runtime_complete = df_submit[df_submit['runtime'].notna()]\n\nruntime_imputer = IterativeImputer(random_state=42)\n\nruntime_imputer.fit(df_runtime_complete)\nruntimes = runtime_imputer.transform(df_runtime_null)\n#print(runtimes)\nruntime_null = df_runtime_null.to_dict('index')\nit = 0\nfor k in runtime_null:\n    v = runtime_null[k]\n    df_submit.at[k,'runtime'] = runtimes[it][5]\n    it+=1\n    \ndf_submit.info()\n\nX_submit = df_submit.drop(['revenue'],axis=1).to_numpy()\nfor m in selected_models:\n    generate_submission(X_submit, m['model'], m['name'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"timestamp = datetime.datetime.now().isoformat()\n!zip -r9 outputs-{timestamp}.zip *.csv","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}