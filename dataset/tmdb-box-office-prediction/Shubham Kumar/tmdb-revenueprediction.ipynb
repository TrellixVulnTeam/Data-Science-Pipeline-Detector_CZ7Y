{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport seaborn as sns \nfrom scipy import stats\nfrom scipy.stats import norm,skew\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nimport xgboost as xgb\nimport catboost as catb\nimport operator\nimport time\nimport ast\nfrom collections import Counter\nimport itertools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The shape of train data is:', train.shape)\nprint('The shape of test data is:', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The columns present in train data are:', train.columns)\nprint('The columns present in test data are:', test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)\nmiss = print(missing_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n- belongs_to_collection is having 79.86% of missing value which is the maximum\n- homepage is having 68.47% of missing value."},{"metadata":{},"cell_type":"markdown","source":"### Graphical representation for the NA's"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,8))\nplt.title(\"Distriution of missing values\")\ntrain.isna().sum().sort_values(ascending=True).plot(kind='bar', colors='Red', fontsize=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Overview and Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option(\"display.max_columns\", 100)\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dropna().shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for Outliers!"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['revenue', 'budget', 'runtime']].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning Outliers"},{"metadata":{},"cell_type":"markdown","source":"**Train Revenue and Budget**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train['id'] == 16,'revenue'] = 192864          # Skinning\ntrain.loc[train['id'] == 90,'budget'] = 30000000         # Sommersby          \ntrain.loc[train['id'] == 118,'budget'] = 60000000        # Wild Hogs\ntrain.loc[train['id'] == 149,'budget'] = 18000000        # Beethoven\ntrain.loc[train['id'] == 313,'revenue'] = 12000000       # The Cookout \ntrain.loc[train['id'] == 451,'revenue'] = 12000000       # Chasing Liberty\ntrain.loc[train['id'] == 464,'budget'] = 20000000        # Parenthood\ntrain.loc[train['id'] == 470,'budget'] = 13000000        # The Karate Kid, Part II\ntrain.loc[train['id'] == 513,'budget'] = 930000          # From Prada to Nada\ntrain.loc[train['id'] == 797,'budget'] = 8000000         # Welcome to Dongmakgol\ntrain.loc[train['id'] == 819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip\ntrain.loc[train['id'] == 850,'budget'] = 90000000        # Modern Times\ntrain.loc[train['id'] == 1112,'budget'] = 7500000        # An Officer and a Gentleman\ntrain.loc[train['id'] == 1131,'budget'] = 4300000        # Smokey and the Bandit   \ntrain.loc[train['id'] == 1359,'budget'] = 10000000       # Stir Crazy \ntrain.loc[train['id'] == 1542,'budget'] = 1              # All at Once\ntrain.loc[train['id'] == 1542,'budget'] = 15800000       # Crocodile Dundee II\ntrain.loc[train['id'] == 1571,'budget'] = 4000000        # Lady and the Tramp\ntrain.loc[train['id'] == 1714,'budget'] = 46000000       # The Recruit\ntrain.loc[train['id'] == 1721,'budget'] = 17500000       # Cocoon\ntrain.loc[train['id'] == 1865,'revenue'] = 25000000      # Scooby-Doo 2: Monsters Unleashed\ntrain.loc[train['id'] == 2268,'budget'] = 17500000       # Madea Goes to Jail budget\ntrain.loc[train['id'] == 2491,'revenue'] = 6800000       # Never Talk to Strangers\ntrain.loc[train['id'] == 2602,'budget'] = 31000000       # Mr. Holland's Opus\ntrain.loc[train['id'] == 2612,'budget'] = 15000000       # Field of Dreams\ntrain.loc[train['id'] == 2696,'budget'] = 10000000       # Nurse 3-D\ntrain.loc[train['id'] == 2801,'budget'] = 10000000       # Fracture","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Test Budget**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.loc[test['id'] == 3889,'budget'] = 15000000       # Colossal\ntest.loc[test['id'] == 6733,'budget'] = 5000000        # The Big Sick\ntest.loc[test['id'] == 3197,'budget'] = 8000000        # High-Rise\ntest.loc[test['id'] == 6683,'budget'] = 50000000       # The Pink Panther 2\ntest.loc[test['id'] == 5704,'budget'] = 4300000        # French Connection II\ntest.loc[test['id'] == 6109,'budget'] = 281756         # Dogtooth\ntest.loc[test['id'] == 7242,'budget'] = 10000000       # Addams Family Values\ntest.loc[test['id'] == 7021,'budget'] = 17540562       #  Two Is a Family\ntest.loc[test['id'] == 5591,'budget'] = 4000000        # The Orphanage\ntest.loc[test['id'] == 4282,'budget'] = 20000000       # Big Top Pee-wee","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train Runtime**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.runtime[train.id == 391] = 86                     #Il peor natagle de la meva vida\ntrain.runtime[train.id == 592] = 90                     #А поутру они проснулись\ntrain.runtime[train.id == 925] = 95                     #¿Quién mató a Bambi?\ntrain.runtime[train.id == 978] = 93                     #La peggior settimana della mia vita\ntrain.runtime[train.id == 1256] = 92                    #Cipolla Colt\ntrain.runtime[train.id == 1542] = 93                    #Все и сразу\ntrain.runtime[train.id == 1875] = 86                    #Vermist\ntrain.runtime[train.id == 2151] = 108                   #Mechenosets\ntrain.runtime[train.id == 2499] = 108                   #Na Igre 2. Novyy Uroven\ntrain.runtime[train.id == 2646] = 98                    #同桌的妳\ntrain.runtime[train.id == 2786] = 111                   #Revelation\ntrain.runtime[train.id == 2866] = 96                    #Tutto tutto niente niente\n\n# TEST\ntest.runtime[test.id == 4074] = 103                     #Shikshanachya Aaicha Gho\ntest.runtime[test.id == 4222] = 93                      #Street Knight\ntest.runtime[test.id == 4431] = 100                     #Плюс один\ntest.runtime[test.id == 5520] = 86                      #Glukhar v kino\ntest.runtime[test.id == 5845] = 83                      #Frau Müller muss weg!\ntest.runtime[test.id == 5849] = 140                     #Shabd\ntest.runtime[test.id == 6210] = 104                     #Le dernier souffle\ntest.runtime[test.id == 6804] = 145                     #Chaahat Ek Nasha..\ntest.runtime[test.id == 7321] = 87                      #El truco del manco","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fixing revenue and budget for those movies which are significantly low**"},{"metadata":{"trusted":true},"cell_type":"code","source":"power_six = train.id[train.budget > 1000][train.revenue < 100]\n\nfor k in power_six :\n    train.loc[train['id'] == k,'revenue'] =  train.loc[train['id'] == k,'revenue'] * 1000000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for normal distribution"},{"metadata":{},"cell_type":"markdown","source":"**User defined function to visualize plots**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nfrom scipy.stats import norm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_distribution(y):\n    sns.distplot(y,fit=norm)\n    mu,sigma=norm.fit(y)\n    plt.legend([\"Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f})\".format(mu,sigma)])\n    plt.title(\"Distribution of revenue\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n    \n    \ndef visualize_probplot(y):\n    stats.probplot(y,plot=plt)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_distribution(test.budget)\nvisualize_probplot(test.budget)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot, it sems the log transformation is the one which makes the probability plot close to linear, hence we will apply to both train and test. <br> We will kee the original revenue until model deployment for associaltion check with different columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['budget'] = np.log1p(train['budget'])\ntest['budget'] = np.log1p(test['budget'])\n\ntrain['popularity'] = np.log1p(train['popularity'])\ntest['popularity'] = np.log1p(test['popularity'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_distribution(train.budget)\nvisualize_probplot(train.budget)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_distribution(train.revenue)\nvisualize_probplot(train.revenue)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping useless features"},{"metadata":{},"cell_type":"markdown","source":"We will drop some of the features which are not useful in the model deployment and is visible at the first glance:\n- **imdb_id**\n- **poster_path**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['imdb_id', 'poster_path'], axis = 1)\ntest = test.drop(['imdb_id', 'poster_path'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cast and Crew"},{"metadata":{"trusted":true},"cell_type":"code","source":"import ast","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train[\"cast\"].notnull(),\"cast\"]=train.loc[train[\"cast\"].notnull(),\"cast\"].apply(lambda x : ast.literal_eval(x))\ntrain.loc[train[\"crew\"].notnull(),\"crew\"]=train.loc[train[\"crew\"].notnull(),\"crew\"].apply(lambda x : ast.literal_eval(x))\n\ntest.loc[test[\"cast\"].notnull(),\"cast\"]=test.loc[test[\"cast\"].notnull(),\"cast\"].apply(lambda x : ast.literal_eval(x))\ntest.loc[test[\"crew\"].notnull(),\"crew\"]=test.loc[test[\"crew\"].notnull(),\"crew\"].apply(lambda x : ast.literal_eval(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train[\"cast\"].notnull(),\"cast\"]=train.loc[train[\"cast\"].notnull(),\"cast\"]\\\n.apply(lambda x : [y[\"name\"] for y in x if y[\"order\"]<6]) \n\ntest.loc[test[\"cast\"].notnull(),\"cast\"]=test.loc[test[\"cast\"].notnull(),\"cast\"]\\\n.apply(lambda x : [y[\"name\"] for y in x if y[\"order\"]<6]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Director, Producer, Executive Producer**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_DirProdExP(df):\n    df[\"Director\"]=[[] for i in range(df.shape[0])]\n    df[\"Producer\"]=[[] for i in range(df.shape[0])]\n    df[\"Executive Producer\"]=[[] for i in range(df.shape[0])]\n\n    df[\"Director\"]=df.loc[df[\"crew\"].notnull(),\"crew\"]\\\n    .apply(lambda x : [y[\"name\"] for y in x if y[\"job\"]==\"Director\"])\n\n    df[\"Producer\"]=df.loc[df[\"crew\"].notnull(),\"crew\"]\\\n    .apply(lambda x : [y[\"name\"] for y in x if y[\"job\"]==\"Producer\"])\n\n    df[\"Executive Producer\"]=df.loc[df[\"crew\"].notnull(),\"crew\"]\\\n    .apply(lambda x : [y[\"name\"] for y in x if y[\"job\"]==\"Executive Producer\"])\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = get_DirProdExP(train)\ntest = get_DirProdExP(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Quantitative features\n- Budget\n- Popularity\n- Runtime\n- Target variable: Revenue"},{"metadata":{},"cell_type":"markdown","source":"**Missing value in Quantitative date**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('budget: ' + str(sum(train['budget'].isna())) + ', popularity: ' + str(sum(train['popularity'].isna())) + \n      ', runtime: ' + str(sum(train['runtime'].isna())) + ', revenue: ' + str(sum(train['revenue'].isna())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pair = ['budget', 'popularity', 'runtime', 'revenue']\nsns.pairplot(train[pair].dropna())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation**<br>\nBudget and revenue seems correlated"},{"metadata":{},"cell_type":"markdown","source":"### Language"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"raw format:\", train['spoken_languages'].iloc[0])\n\ntrain['spoken_languages'] = train['spoken_languages'].apply(lambda x: list(map(lambda d: list(d.values())[0], ast.literal_eval(x)) if isinstance(x, str) else []))\ntest['spoken_languages'] = test['spoken_languages'].apply(lambda x: list(map(lambda d: list(d.values())[0], ast.literal_eval(x)) if isinstance(x, str) else []))\n\ntrain.head().spoken_languages","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating two features:\n    - number of spoken languages, and\n    - Whether it is english or not"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['nb_spoken_languages'] = train.spoken_languages.apply(len)\ntest['nb_spoken_languages'] = test.spoken_languages.apply(len)\n\ntrain['english_spoken'] = train.spoken_languages.apply(lambda x: 'en' in x)\ntest['english_spoken'] = test.spoken_languages.apply(lambda x: 'en' in x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['nb_spoken_languages'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation**<br>\nThere is one movie in which 9 languages are spoken, or may be it was translated into 9 different languages."},{"metadata":{},"cell_type":"markdown","source":"### Original Language"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_languages = pd.concat([train.original_language, test.original_language], axis=0).value_counts()\nall_languages[all_languages > 10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Main Languages**"},{"metadata":{"trusted":true},"cell_type":"code","source":"main_languages = list(all_languages[all_languages>20].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Categorizing them and add them as Other**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_language = dict(zip(main_languages, range(1, len(main_languages)+1)))\ndict_language['other'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Keeping only the main languages**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.original_language = train.original_language.apply(lambda x: x if x in main_languages else 'other')\ntest.original_language = test.original_language.apply(lambda x: x if x in main_languages else 'other')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Arranging language to indexes of the disctionary**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['language'] = train.original_language.apply(lambda x: dict_language[x])\ntest['language'] = test.original_language.apply(lambda x: dict_language[x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Movie Genre"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.genres = train.genres.apply(lambda x: list(map(lambda d: list(d.values())[1], ast.literal_eval(x)) if isinstance(x, str) else []))\ntest.genres = test.genres.apply(lambda x: list(map(lambda d: list(d.values())[1], ast.literal_eval(x)) if isinstance(x, str) else []))\n\ntrain.genres.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nThe distribution of the number of genres per movie. There are 3 movies with 7 genres."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(train.genres.apply(len).value_counts().sort_index().keys(), train.genres.apply(len).value_counts().sort_index())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for v in train[train.genres.apply(len)==7][['title', 'genres']].values:\n    print('film:', v[0], '\\ngenres:', *v[1], '\\n')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Frequency of All genres**"},{"metadata":{"trusted":true},"cell_type":"code","source":"genres = Counter(itertools.chain.from_iterable(pd.concat((train.genres, test.genres), axis=0).values))\ngenres","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\nHere instead of creating 20 categorical features, one for each genre, we will reduce it using __SVD__"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntemp_train = train[['id', 'genres']]\ntemp_test = test[['id', 'genres']]\n\nfor g in genres:\n    temp_train[g] = temp_train.genres.apply(lambda x: 1 if g in x else 0)\n    temp_test[g] = temp_test.genres.apply(lambda x: 1 if g in x else 0)\n    \nX_train = temp_train.drop(['genres', 'id'], axis=1).values\nX_test = temp_test.drop(['genres', 'id'], axis=1).values\n\n# Number of features we want for genres\nn_comp_genres = 3\n\n# Build the SVD pipeline\nsvd = make_pipeline(\n    TruncatedSVD(n_components=n_comp_genres),\n    Normalizer(norm='l2', copy=False)\n)\n\n# Here are our new features\nf_train = svd.fit_transform(X_train)\nf_test = svd.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_genres = [g for g in genres if g!= 'TV Movie']\nmy_genres","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([train, temp_train.iloc[:,1:]], axis=1) \ntrain.drop(train.columns[-1],axis=1, inplace = True)\n\ntest = pd.concat([test, temp_test.iloc[:,1:]], axis=1) \ntest.drop(test.columns[-1], axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Keywords"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Keywords = train.Keywords.apply(lambda x: list(map(lambda d: list(d.values())[1], ast.literal_eval(x)) if isinstance(x, str) else []))\ntest.Keywords = test.Keywords.apply(lambda x: list(map(lambda d: list(d.values())[1], ast.literal_eval(x)) if isinstance(x, str) else []))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['nb_keywords'] = train.Keywords.apply(len)\ntest['nb_keywords'] = test.Keywords.apply(len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Production_Companies"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.production_companies = train.production_companies.apply(lambda x: list(map(lambda d: list(d.values())[0], ast.literal_eval(x)) if isinstance(x, str) else []))\ntest.production_companies = test.production_companies.apply(lambda x: list(map(lambda d: list(d.values())[0], ast.literal_eval(x)) if isinstance(x, str) else []))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"production_companies = Counter(itertools.chain.from_iterable(pd.concat((train.production_companies, test.production_companies), axis=0).values))\nprint(\"Number of different production companies:\", len(production_companies))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['nb_production_companies'] = train.production_companies.apply(len)\ntest['nb_production_companies'] = test.production_companies.apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('Applying SVD on production companies to create reduced features')\n\n# Factorizing all the little production companies into an 'other' variable\nbig_companies = [p for p in production_companies if production_companies[p] > 30]\ntrain.production_companies = train.production_companies.apply(lambda l: list(map(lambda x: x if x in big_companies else 'other', l)))\n\ntemp_train = train[['id', 'production_companies']]\ntemp_test = test[['id', 'production_companies']]\n\nfor p in big_companies + ['other']:\n    temp_train[p] = temp_train.production_companies.apply(lambda x: 1 if p in x else 0)\n    temp_test[p] = temp_test.production_companies.apply(lambda x: 1 if p in x else 0)\n    \nX_train = temp_train.drop(['production_companies', 'id'], axis=1).values\nX_test = temp_test.drop(['production_companies', 'id'], axis=1).values\n\n# Number of features we want for genres\nn_comp_production_companies = 3\n\n# Build the SVD pipeline\nsvd = make_pipeline(\n    TruncatedSVD(n_components=n_comp_production_companies),\n    Normalizer(norm='l2', copy=False)\n)\n\n# Here are our new features\nf_train = svd.fit_transform(X_train)\nf_test = svd.transform(X_test)\n\nfor i in range(n_comp_production_companies):\n    train['production_companies_reduced_{}'.format(i)] = f_train[:, i]\n    test['production_companies_reduced_{}'.format(i)] = f_test[:, i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['production_companies_reduced_0', 'production_companies_reduced_1', 'production_companies_reduced_2']].head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Production_Countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.production_countries = train.production_countries.apply(lambda x: list(map(lambda d: list(d.values())[0], ast.literal_eval(x)) if isinstance(x, str) else []))\ntest.production_countries = test.production_countries.apply(lambda x: list(map(lambda d: list(d.values())[0], ast.literal_eval(x)) if isinstance(x, str) else []))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"production_countries = Counter(itertools.chain.from_iterable(pd.concat((train.production_countries, test.production_countries), axis=0).values))\nprint(\"Number of different production companies:\", len(production_countries))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('Applying SVD on production countries to create reduced features')\n\n# Factorizing all the little production companies into an 'other' variable\nbig_countries = [p for p in production_countries if production_countries[p] > 30]\ntrain.production_countries = train.production_countries.apply(lambda l: list(map(lambda x: x if x in big_countries else 'other', l)))\n\ntemp_train = train[['id', 'production_countries']]\ntemp_test = test[['id', 'production_countries']]\n\nfor p in big_countries + ['other']:\n    temp_train[p] = temp_train.production_countries.apply(lambda x: 1 if p in x else 0)\n    temp_test[p] = temp_test.production_countries.apply(lambda x: 1 if p in x else 0)\n    \nX_train = temp_train.drop(['production_countries', 'id'], axis=1).values\nX_test = temp_test.drop(['production_countries', 'id'], axis=1).values\n\n# Number of features we want for genres\nn_comp_production_countries = 3\n\n# Build the SVD pipeline\nsvd = make_pipeline(\n    TruncatedSVD(n_components=n_comp_production_countries),\n    Normalizer(norm='l2', copy=False)\n)\n\n# Here are our new features\nf_train = svd.fit_transform(X_train)\nf_test = svd.transform(X_test)\n\nfor i in range(n_comp_production_countries):\n    train['production_countries_reduced_{}'.format(i)] = f_train[:, i]\n    test['production_countries_reduced_{}'.format(i)] = f_test[:, i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['production_countries_reduced_0', 'production_countries_reduced_1', 'production_countries_reduced_2']].head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Releasing Date"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.loc[test.release_date.isna(), 'release_date'] = '05/01/00'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train\ntrain['release_date'] = pd.to_datetime(train['release_date'], format='%m/%d/%y')\ntrain['Year'] = train.release_date.dt.year\ntrain['Month'] = train.release_date.dt.month\ntrain['Day'] = train.release_date.dt.day\ntrain['dayofweek'] = train.release_date.dt.dayofweek \ntrain['quarter'] = train.release_date.dt.quarter   \n#Test\ntest['release_date'] = pd.to_datetime(test['release_date'], format='%m/%d/%y')\ntest['Year'] = test.release_date.dt.year\ntest['Month'] = test.release_date.dt.month\ntest['Day'] = test.release_date.dt.day\ntest['dayofweek'] = test.release_date.dt.dayofweek \ntest['quarter'] = test.release_date.dt.quarter  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dummy Month"},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies = pd.get_dummies(train['Month'] ,drop_first=True).rename(columns=lambda x: 'Month' + str(x))\ndummies2 = pd.get_dummies(test['Month'] ,drop_first=True).rename(columns=lambda x: 'Month' + str(int(x)))\ntrain = pd.concat([train, dummies], axis=1)\ntest = pd.concat([test, dummies2], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dummy DayofWeek"},{"metadata":{"trusted":true},"cell_type":"code","source":"ddow = pd.get_dummies(train['dayofweek'] ,drop_first=True).rename(columns=lambda x: 'dayofweek' + str(x))\nddow2 = pd.get_dummies(test['dayofweek'] ,drop_first=True).rename(columns=lambda x: 'dayofweek' + str(int(x)))\ntrain = pd.concat([train, ddow], axis=1)\ntest = pd.concat([test, ddow2], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fix Year"},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Train: ' + str(max(train.Year)) + ' Test: ' + str(max(test.Year)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation:\nThe max year can't be 2068!!\n- After cross verifying we have found that the date > 2068 start with 19XX."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train\ntrain.loc[train['Year'] > 2018, 'Year'] = train.loc[train['Year'] > 2018, 'Year'].apply(lambda x: x - 100)\n#Test\ntest.loc[test['Year'] > 2018, 'Year'] = test.loc[test['Year'] > 2018, 'Year'].apply(lambda x: x - 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.Year.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Year Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_plot = train[['revenue', 'Year']]\nmoney_Y = data_plot.groupby('Year')['revenue'].sum()\n\nmoney_Y.plot(figsize=(15,8))\nplt.xlabel(\"Year of release\")\nplt.ylabel(\"revenue\")\nplt.xticks(np.arange(1960,2015,5))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Month Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(18, 10))\nplt.bar(train.Month, train.revenue, color = 'Red')\nplt.xlabel(\"Month of release\")\nplt.ylabel(\"revenue\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Day of Week Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(15, 10))\nplt.bar(train.dayofweek, train.revenue, color = 'Red')\nplt.xlabel(\"Dayofweek of release\")\nplt.ylabel(\"revenue\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fuzzy features"},{"metadata":{},"cell_type":"markdown","source":"In the data, there are movies having budget =0, so we are using median to impute the values."},{"metadata":{"trusted":true},"cell_type":"code","source":"def fuzzy_feat(df):\n    \n    df['Ratiobudgetbypopularity'] = df['budget']/df['popularity']\n    df['RatiopopularitybyYear'] = df['popularity']/df['Year']\n    df['RatoioruntimebyYear'] = df['runtime']/df['Year']\n    \n    \n    df['budget_runtime_ratio'] = df['budget']/df['runtime'] \n    df['budget_Year_ratio'] = df['budget']/df['Year']\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = fuzzy_feat(train)\ntest = fuzzy_feat(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features from NAs has homepage"},{"metadata":{"trusted":true},"cell_type":"code","source":"# NAs\n\ntrain['has_homepage'] = np.where(train['homepage'].isna(), 0, 1)\ntrain ['has_collection'] = np.where(train['belongs_to_collection'].isna(), 0, 1)\n\ntest['has_homepage'] = np.where(test['homepage'].isna(), 0, 1)\ntest ['has_collection'] = np.where(test['belongs_to_collection'].isna(), 0, 1)\n\ntrain['has_tagline'] = np.where (train['tagline'].isna(), 0, 1)\ntest['has_tagline'] = np.where (test['tagline'].isna(), 0, 1)\n\n#Fix Strange occurences\n\ntrain['title_different'] = np.where(train['original_title'] == train['title'], 0, 1)\ntest['title_different'] = np.where(test['original_title'] == test['title'], 0, 1)\n\ntrain['isReleased'] = np.where(train['status'] != 'Released', 0, 1)\ntest['isReleased'] = np.where(test['status'] != 'Released', 0, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['budget', \n            'popularity', \n            'runtime', \n            'nb_spoken_languages', \n            'nb_production_companies',\n            'english_spoken', \n            'language',\n            'has_homepage', 'has_collection', 'isReleased', 'has_tagline', 'title_different',\n            'Day',\n            'quarter', 'Year',\n            'nb_keywords', \n            'Month2', 'Month3',  'Month4', 'Month5',  'Month6', 'Month7',\n            'Ratiobudgetbypopularity', 'RatiopopularitybyYear',\n            'RatoioruntimebyYear', 'budget_runtime_ratio', 'budget_Year_ratio',\n            'Month8', 'Month9',  'Month10', 'Month11', 'Month12']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features += [col for col in train.columns if 'dayofweek' in col and col != \"dayofweek\"]\nfeatures += my_genres\nfeatures += ['production_companies_reduced_{}'.format(i) for i in range(n_comp_production_companies)]\nfeatures += ['production_countries_reduced_{}'.format(i) for i in range(n_comp_production_countries)]\nX = train[features]\nX['revenue'] = train.revenue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation Heatmap "},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_features = X[['revenue', 'budget',  'popularity', 'runtime', 'nb_spoken_languages', 'nb_production_companies',\n            'Day', 'quarter', 'Year','nb_keywords' ]]\nf,ax = plt.subplots(figsize=(20, 12))\nsns.heatmap(cor_features.corr(), annot=True, linewidths=.7, fmt= '.2f',ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop(['revenue'], axis = 1)\ny = train.revenue.apply(np.log1p)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoosst Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'objective': 'reg:linear', \n          'eta': 0.01, \n          'max_depth': 6, \n          'min_child_weight': 3,\n          'subsample': 0.8,\n          'colsample_bytree': 0.8,\n          'colsample_bylevel': 0.50, \n          'gamma': 1.45, \n          'eval_metric': 'rmse', \n          'seed': 12, \n          'silent': True    \n}\n\n# create dataset for xgboost\nxgb_data = [(xgb.DMatrix(X_train, y_train), 'train'), (xgb.DMatrix(X_test, y_test), 'valid')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Starting training...')\nxgb_model = xgb.train(params, \n                  xgb.DMatrix(X_train, y_train),\n                  5000,  \n                  xgb_data, \n                  verbose_eval=200,\n                  early_stopping_rounds=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model_full = xgb.XGBRegressor(objective  = 'reg:linear', \n          eta = 0.01, \n          max_depth = 6,\n          min_child_weight = 3,\n          subsample = 0.8, \n          colsample_bytree = 0.8,\n          colsample_bylevel = 0.50, \n          gamma = 1.45, \n          eval_metric = 'rmse',\n          seed = 12, n_estimators = 2000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model_full.fit (X.values, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"catmodel = catb.CatBoostRegressor(iterations=10000, \n                                 learning_rate=0.01, \n                                 depth=5, \n                                 eval_metric='RMSE',\n                                 colsample_bylevel=0.7,\n                                 bagging_temperature = 0.2,\n                                 metric_period = None,\n                                 early_stopping_rounds=200,\n                                 random_seed=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ti=time.time()\ncatmodel.fit(X, y, \n             eval_set=(X_train, y_train), \n             verbose=500, \n             use_best_model=True)\n\nprint(\"Number of minutes of training of model_cal = {:.2f}\".format((time.time()-ti)/60))\n\ncat_pred_train=catmodel.predict(X)\ncat_pred_train[cat_pred_train<0]=0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"fea_imp = pd.DataFrame({'imp': catmodel.feature_importances_, 'col': X.columns})\nfea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\nfea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 12))\nplt.savefig('catboost_feature_importance.png') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,12))\nxgb.plot_importance(xgb_model, max_num_features=30, height = 0.8, ax = ax)\nplt.title('XGBOOST Features (avg over folds)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred = xgb_model.predict(xgb.DMatrix(X), ntree_limit=xgb_model.best_ntree_limit)\nplt.figure(figsize=(32,15))\nplt.plot(y[:500],label=\"Real\")\nplt.plot(train_pred[:500],label=\"train_pred\")\nplt.legend(fontsize=15)\nplt.title(\"Real and predicted revenue of first 500 entries of train set\",fontsize=24)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(32,15))\nplt.plot(y[:500],label=\"Real\")\nplt.plot(cat_pred_train[:500],label=\"train_pred\")\nplt.legend(fontsize=15)\nplt.title(\"Real and predicted revenue of first 500 entries of train set\",fontsize=24)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(35,18))\nplt.plot(y[:600],label=\"Real\", color = \"red\")\nplt.plot(xgb_model.predict(xgb.DMatrix(X), ntree_limit=xgb_model.best_ntree_limit)[:600],label=\"xgb\", color = \"blue\")\nplt.plot(cat_pred_train[:600],label=\"catb\", color = \"green\")\nplt.legend(fontsize=15)\nplt.title(\"Real and predicted revenue of first 500 entries of train set\",fontsize=24)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Export\n#### XGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test[features]\nxgb_pred = np.expm1(xgb_model.predict(xgb.DMatrix(X_test), ntree_limit=xgb_model.best_ntree_limit))\npd.DataFrame({'id': test.id, 'revenue': xgb_pred}).to_csv('xgbsubmission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_pred[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGB FULL"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_pred_f = np.expm1(xgb_model_full.predict(X_test.values))\npd.DataFrame({'id': test.id, 'revenue': xgb_pred_f}).to_csv('xgbfullsubmission.csv', index=False)\nxgb_pred_f[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CATB"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test[features]\ncatb_pred = np.expm1(catmodel.predict(X_test.values))\npd.DataFrame({'id': test.id, 'revenue': catb_pred}).to_csv('catbsubmission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"catb_pred[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"ens_pred = 0.3*xgb_pred_f + 0.7*catb_pred\npd.DataFrame({'id': test.id, 'revenue': ens_pred}).to_csv('enssubmission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ens_pred[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'id': test.id, 'revenue': ens_pred}).head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}