{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting Movie Revenues\n0. Explore data\n1. Feature Pre-processing: check for missing values. \n2. Feature Engineering: skewness, handle categorical data, scale features, selecting meaningful features, assess importance with random forests.\n3. Further Data Exploration.\n4. Choose evaluation metrics.\n5. Create a benchmark to improve from it.\n6. Split Data into K-folds.\n7. Choose models, implement them.\n8. Create an ensemble model.\n9. Create submission files."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom IPython.display import display # Allows the use of display() for DataFrames\nfrom time import time\nimport matplotlib.pyplot as plt\nimport seaborn as sns # Plotting library\nfrom scipy import stats\nfrom collections import Counter\nimport json\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport lightgbm as lgb\n\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Always show all columns\npd.set_option('display.max_columns', 999)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Open the train and test data into a dataframe.\ndata_train_raw = pd.read_csv(\"../input/train.csv\")\ndata_test_raw = pd.read_csv(\"../input/test.csv\")\n\n# Display the data to see what it looks like\ndisplay(data_train_raw.head(n=3))\n\ndisplay(data_test_raw.head(n=3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 0. Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data_train_raw.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Revenues are skewed\nfig, ax = plt.subplots(figsize=(10,5))\nfig.suptitle('Revenue Distribution', fontsize=15)\nsns.distplot(data_train_raw['revenue'], bins=50, kde=False)\nax.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the distribution of some of the continous features\nfig, ax = plt.subplots(ncols=3, figsize=(20,5))\nfig.suptitle('Distribution of continuous features', fontsize=15)\n\nfeatures_to_explore = ['budget', 'popularity', 'runtime']\n\nfor col, feature in enumerate(features_to_explore):\n    myplot = sns.distplot(data_train_raw[feature], bins=50, kde=False, ax=ax[col])\n    myplot.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check correlation between revenue and some of the continous features\nfig, ax = plt.subplots(ncols=3, figsize=(20,5))\nfig.suptitle('Correlation between some continuous features\\n and target feature `revenue`', fontsize=15)\n\nfor col, feature in enumerate(features_to_explore):\n    myplot = sns.regplot(x='revenue', y=feature, data=data_train_raw, ax=ax[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Data Pre-Processing\n\nReference for feature pre-processing: [EDA + FE + LGBM (no external data)](https://www.kaggle.com/dude431/eda-fe-lgbm-no-external-data)\n\nThings to do:\n* Check for missing values (NaN)\n* Check for 0 values"},{"metadata":{},"cell_type":"markdown","source":"### Runtime\n\nThe feature ```runtime``` has some values with the value 0. This doesn't make much sense as any film must run for at least some time, therefore we will swap all the 0 values with the mean."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Train data\ndata_train_preprocessed = data_train_raw.copy()\n## Test data\ndata_test_preprocessed = data_test_raw.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Swap NaN values with the mean\ndef runtime_pre_process(df):\n    df['runtime'] = df['runtime'].fillna(0)\n    \n    # Swap runtime with 0 values for the average\n    runtime_mean = df['runtime'].mean()\n    \n    df['runtime'] = df['runtime'].replace(0, runtime_mean)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_preprocessed = runtime_pre_process(data_train_preprocessed)\ndata_test_preprocessed = runtime_pre_process(data_test_preprocessed)\n\n# Before\nprint(data_train_raw['runtime'].loc[data_train_raw['runtime'] == 0].count())\n# After\nprint(data_train_preprocessed['runtime'].loc[data_train_preprocessed['runtime'] == 0].count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Release time\n\nWe will separate release date into ```release year```, ```release month``` and ```release day```"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_test_preprocessed['release_date'].value_counts()[:1])\ndisplay(data_test_preprocessed['release_date'].isna().value_counts())\n# Fill any NaN values with  the most common Value\ndata_test_preprocessed.loc[data_test_preprocessed['release_date'].isnull() == True, 'release_date'] = '01/01/98'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def release_year_pre_process (df):\n    # Fill any NaN values\n    df.loc[df['release_date'].isnull() == True, 'release_date'] = '01/01/98'\n    \n    df['release_date'] = pd.to_datetime(df['release_date'])\n    df['release_month'] = df['release_date'].apply(lambda d: d.month)\n    df['release_day'] = df['release_date'].apply(lambda d: d.day)\n    df['release_weekday'] = df['release_date'].apply(lambda d: d.weekday())\n\n    # For some reason some dates were put into the future\n    df['release_year'] = df['release_date'].apply(lambda d: d.year if d.year < 2018 else d.year -100)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_preprocessed = release_year_pre_process(data_train_preprocessed)\ndata_test_preprocessed = release_year_pre_process(data_test_preprocessed)\n\ndisplay(data_train_preprocessed.head(n=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Homepage\n\nIf it has a homepage I will substitute it by a 1. Otherwise a 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"def homepage_pre_process(df):\n    # Replace NaN values with 0\n    df['homepage'] = df['homepage'].fillna(0)\n    \n    # Replace rows with websites with 1\n    df.loc[df['homepage'] != 0, 'homepage'] = 1\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_preprocessed = homepage_pre_process(data_train_preprocessed)\ndata_test_preprocessed = homepage_pre_process(data_test_preprocessed)\n\ndisplay(data_train_preprocessed['homepage'].head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Poster\n\nIf it has a poster I will substitute it by a 1. Otherwise a 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"def poster_pre_process(df):\n    # Replace NaN values with 0\n    df['poster_path'] = df['poster_path'].fillna(0)\n    \n    # Replace rows with websites with 1\n    df.loc[df['poster_path'] != 0, 'poster_path'] = 1\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_preprocessed = poster_pre_process(data_train_preprocessed)\ndata_test_preprocessed = poster_pre_process(data_test_preprocessed)\n\ndisplay(data_train_preprocessed['poster_path'].head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Budget\n\nThe other feature that includes some 0 values is ```budget```. This in the otherhand could happen. However it doesn't make much sense after exploring that some production companies like 'Walt Disney' or '20th Century Fox' would produce a film with 0 budget."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count how many films have budget 0\nprint(data_train_preprocessed['budget'].loc[data_train_preprocessed['budget'] == 0].count())\n\nprint(data_test_preprocessed['budget'].loc[data_test_preprocessed['budget'] == 0].count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We check the correlation of the different features with budget to see which ones are affect most the budget feature.\n\nThere seems to be some correlation between ```budget``` and ```popularity``` and ```budget``` and ```release year```."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(8,8))\nfig.suptitle('Correlation Matrix', fontsize=16)\nsns.heatmap(data_train_preprocessed.corr(), annot=True, ax=axs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for correlations between budget and other features\nnumerical_features = ['budget', 'popularity', 'poster_path', 'homepage', 'release_year', 'release_month', 'release_day', 'runtime']\n\ncorrelation_budget = []\nfor column in data_train_preprocessed[numerical_features].loc[data_train_preprocessed['budget'] != 0]:\n    pearson_corr, _ = stats.pearsonr(data_train_preprocessed['budget'], data_train_preprocessed[column])\n    correlation_budget.append(('budget', column, pearson_corr))\n    \ndisplay(pd.DataFrame(data=correlation_budget, columns=['Start Feature', 'Target', 'Correlation value']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will input the budget with ceros with the year's average\n# Also we will add a column that contains that year's budget average for all the movies release in that year\ndef imputing_budget(df):\n    df['budget'] = df['budget'].fillna(0)\n    year_mean = df.groupby(['release_year']).mean()['budget']\n    df['year_mean_budget'] = 0\n    \n    for index, row in df.iterrows():\n        year_of_release = row['release_year']\n        \n        if row['budget'] == 0:\n            df.at[index,'budget'] = year_mean[year_of_release]\n        \n        # Average budget that year\n        df.at[index,'year_mean_budget'] = year_mean[year_of_release]\n            \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Update our data\ndata_train_preprocessed = imputing_budget(data_train_preprocessed)\ndata_test_preprocessed = imputing_budget(data_test_preprocessed)\n\n# Check how many values have 0 now.\nprint(data_train_preprocessed['budget'].loc[data_train_preprocessed['budget'] == 0].count())\nprint(data_test_preprocessed['budget'].loc[data_test_preprocessed['budget'] == 0].count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Belongs to Collection\n\nFill NaN values with ´None´"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count how many ´belongs to collection´ have NaN values\nprint(data_train_preprocessed['belongs_to_collection'].head())\nprint('\\n')\nprint(data_train_preprocessed['belongs_to_collection'].isna().value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def collection_pre_process(df):\n    # Change NaN values with ´0´.\n    df['belongs_to_collection'] = df['belongs_to_collection'].fillna('0')\n    \n    # Replace rows that belong to collection with 1\n    df.loc[df['belongs_to_collection'] != 0, 'belongs_to_collection'] = 1\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_preprocessed = collection_pre_process(data_train_preprocessed)\ndata_test_preprocessed = collection_pre_process(data_test_preprocessed)\n\n# Check for NaN values\nprint(data_train_preprocessed['belongs_to_collection'].isna().value_counts())\ndisplay(data_train_preprocessed.head(n=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Genres\n\nFor the genres I will input any NaN values with the most common. I am not sure this is the best way to go about it, but I'll do this for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change NaN values with the most common value\nprint(data_train_preprocessed['genres'].isna().value_counts())\nprint('\\n')\nprint(data_train_preprocessed['genres'].value_counts()[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def genres_pre_processing(df):\n    # Most common value is Drama, so I'll fill empty values with [{'id': 18, 'name': 'Drama'}]\n    df['genres'] = df['genres'].fillna(\"[{'id': 18, 'name': 'Drama'}]\")\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_preprocessed = genres_pre_processing(data_train_preprocessed)\ndata_test_preprocessed = genres_pre_processing(data_test_preprocessed)\n\n# Check for NaN values\nprint(data_train_preprocessed['genres'].isna().value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Original Title"},{"metadata":{"trusted":true},"cell_type":"code","source":"# No missing values in train\nprint(data_train_preprocessed['original_title'].isna().value_counts())\n\n# No missing values in test\nprint(data_test_preprocessed['original_title'].isna().value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Title\n\nFor any title missing values we will fill it with the same name as ```original_title```"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill ´none´ titles with the same value as ´original_title´\ndef imput_title(df):\n    df['title'] = df['title'].fillna(\"none\")\n\n    for index, row in df.iterrows():\n        if row['title'] == \"none\":\n            df.at[index,'title'] = df.loc[index]['original_title']\n    return df        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_preprocessed = imput_title(data_train_preprocessed)\ndata_test_preprocessed = imput_title(data_test_preprocessed)\n\n# Check Nan for train\nprint(data_train_preprocessed['original_title'].isna().value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Original language"},{"metadata":{"trusted":true},"cell_type":"code","source":"# No missing values in train\nprint(data_train_preprocessed['original_language'].isna().value_counts())\n\n# No missing values in test\nprint(data_test_preprocessed['original_language'].isna().value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Production Companies\n\nReplace NaN values with 'none'"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prod_comp_pre_processing(df):\n    # Replace NaN values with 'none'\n    df['production_companies'] = df['production_companies'].fillna(\"none\")\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_preprocessed = prod_comp_pre_processing(data_train_preprocessed)\ndata_test_preprocessed = prod_comp_pre_processing(data_test_preprocessed)\n\n# Check for NaNs\nprint(data_train_preprocessed['production_companies'].isna().value_counts())\nprint(data_test_preprocessed['production_companies'].isna().value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Spoken languages\n\nReplace NaN values with the most common"},{"metadata":{"trusted":true},"cell_type":"code","source":"def spoken_lang_pre_processing(df):\n    # Replace NaN values with the most common [{'iso_639_1': 'en', 'name': 'English'}]\n    df['spoken_languages'] = df['spoken_languages'].fillna(\"[{'iso_639_1': 'en', 'name': 'English'}]\")\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_preprocessed = spoken_lang_pre_processing(data_train_preprocessed)\ndata_test_preprocessed = spoken_lang_pre_processing(data_test_preprocessed)\n\n# Check for NaN values\nprint(data_train_preprocessed['spoken_languages'].isna().value_counts())\nprint(data_test_preprocessed['spoken_languages'].isna().value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Keywords\n\nReplace NaN values with 'none'"},{"metadata":{"trusted":true},"cell_type":"code","source":"def keywords_pre_processing(df):\n    # Replace NaN values with 'none'\n    df['Keywords'] = df['Keywords'].fillna(\"none\")\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_preprocessed = keywords_pre_processing(data_train_preprocessed)\ndata_test_preprocessed = keywords_pre_processing(data_test_preprocessed)\n\n# Check for NaN values\nprint(data_train_preprocessed['Keywords'].isna().value_counts())\nprint(data_test_preprocessed['Keywords'].isna().value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cast\n\nReplace NaN values with 'none'"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cast_pre_processing(df):\n    # Replace NaN values with 'none'\n    df['cast'] = df['cast'].fillna(\"none\")\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_preprocessed = cast_pre_processing(data_train_preprocessed)\ndata_test_preprocessed = cast_pre_processing(data_test_preprocessed)\n\n# Check for NaN values\nprint(data_train_preprocessed['cast'].isna().value_counts())\nprint(data_test_preprocessed['cast'].isna().value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Crew\n\nReplace NaN values with 'none'"},{"metadata":{"trusted":true},"cell_type":"code","source":"def crew_pre_processing(df):\n    # Replace NaN values with 'none'\n    df['crew'] = df['crew'].fillna(\"none\")\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_preprocessed = crew_pre_processing(data_train_preprocessed)\ndata_test_preprocessed = crew_pre_processing(data_test_preprocessed)\n\n# Check for NaN values\nprint(data_train_preprocessed['crew'].isna().value_counts())\nprint(data_test_preprocessed['crew'].isna().value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Overview\n\nReplace NaN values with \" \""},{"metadata":{"trusted":true},"cell_type":"code","source":"def overview_pre_process(df):\n    # Replace NaN values with \"\"\n    df['overview'] = df['overview'].fillna(\"\")\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_preprocessed = overview_pre_process(data_train_preprocessed)\ndata_test_preprocessed = overview_pre_process(data_test_preprocessed)\n\n# Check for NaN values\nprint(data_train_preprocessed['overview'].isna().value_counts())\nprint(data_test_preprocessed['overview'].isna().value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tagline"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tagline_pre_process(df):\n    # Replace NaN values with ''\n    df['tagline'] = df['tagline'].fillna(\"\")\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_preprocessed = tagline_pre_process(data_train_preprocessed)\ndata_test_preprocessed = tagline_pre_process(data_test_preprocessed)\n\n# Check for NaN values\nprint(data_train_preprocessed['tagline'].isna().value_counts())\nprint(data_test_preprocessed['tagline'].isna().value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Production Countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_preprocessed['production_countries'].value_counts()[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prod_countries_pre_process(df):\n    # Replace NaN values with most common\n    df['production_countries'] = df['production_countries'].fillna(\"[{'iso_3166_1': 'US', 'name': 'United States of America'}]\")\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_preprocessed = prod_countries_pre_process(data_train_preprocessed)\ndata_test_preprocessed = prod_countries_pre_process(data_test_preprocessed)\n\n# Check for NaN values\nprint(data_train_preprocessed['production_countries'].isna().value_counts())\nprint(data_test_preprocessed['production_countries'].isna().value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Status\n\nFill any `NaN` values with the most common"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"NaN values in train['status'] : {}\".format(data_train_preprocessed['status'].isna().any()))\nprint(\"NaN values in test['status'] : {}\".format(data_test_preprocessed['status'].isna().any()))\nprint('\\nValue Counts train:')\nprint(data_train_preprocessed['status'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def status_pre_process(df):\n    df['status'] = df['status'].fillna(\"Released\")\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_preprocessed = status_pre_process(data_train_preprocessed)\ndata_test_preprocessed = status_pre_process(data_test_preprocessed)\n\ndisplay(data_test_preprocessed['status'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert String List to Actual Lists"},{"metadata":{"trusted":true},"cell_type":"code","source":"import ast\n\n# Convert panda list string to actual list\nstring_lists = ['genres', 'production_companies', 'production_countries', 'spoken_languages', 'cast', 'crew', 'Keywords']\n\ndef string2_list(df):\n    for string_list in string_lists:\n        df[string_list] = df[string_list].apply(lambda x: {} if x == 'none' else ast.literal_eval(x))\n        \n    return df\n        \ndata_train_preprocessed = string2_list(data_train_preprocessed)\ndata_test_preprocessed = string2_list(data_test_preprocessed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FInal check for NaN values in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data_train_preprocessed.isna().any())\ndisplay(data_test_preprocessed.isna().any())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Feature Engineering\n\nReference: [EDA, Feature Engineering and model interpretation](https://www.kaggle.com/artgor/eda-feature-engineering-and-model-interpretation)\n\n* Combine existing features\n* Handle data with multiple values\n* Deal with skeweness\n* Deal with Text data\n* Handle Categorical data\n* Scale features\n* Select Meaningful features\n* Assess importance with random features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop some columns that we won't use\ndata_train_cleaned = data_train_preprocessed.copy()\ndata_test_cleaned = data_test_preprocessed.copy()\n\ndata_train_cleaned = data_train_cleaned.drop(['imdb_id'], axis=1)\ndata_test_cleaned = data_test_cleaned.drop(['imdb_id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handling data with multiple values\n\nIn this section I'll take care of cells that contain an array or dict of values. I'll separate them into different columns for each unique value (one hot encoding). In some cases there are too many unique values, in those situtations I will have to stablish a limit, selecting only the top-k most popular features.\n\n* genres\n* production_companies \n* production_countries\n* spoken_languages\n* Keywords\n* cast\n* crew"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Flattens a a list of lists of dicts into a simple list of dicts\ndef flatten_data_column(mylist):\n    flattened_list = []\n    for elements in mylist:\n        for element in elements:\n            flattened_list.append(element)\n    return flattened_list\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Groups by repeated value and counts the ocurrences\ndef create_counter(mylist, key='name'):\n    return Counter([i[key] for i in mylist]).most_common()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### **Genres**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets get the list of genres\ngenres_list = flatten_data_column(data_train_cleaned['genres'])\n\ngenres_list_test = flatten_data_column(data_test_cleaned['genres'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a counter of the most popular genres\ngenres_list_counter = create_counter(genres_list)\ngenres_list_counter_test = create_counter(genres_list_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets separate the columns into 20 different genres"},{"metadata":{"trusted":true},"cell_type":"code","source":"def genres_FE(df, genres_counter):\n    df['num_genres'] = df['genres'].apply(lambda x: len(x) if x != {} else 0)\n    df['all_genres'] = df['genres'].apply(lambda x: [i['name'] for i in x])\n    \n    for genre, count in genres_counter:\n        df['genre_' + genre] = df['all_genres'].apply(lambda g_list: 1 if genre in g_list else 0)\n    \n    df['all_genres'] = df['all_genres'].apply(lambda x: \" \".join(x))\n    df = df.drop(['genres'], axis=1)\n    \n    return df\n        \ndata_train_cleaned = genres_FE(data_train_cleaned, genres_list_counter)\ndata_test_cleaned = genres_FE(data_test_cleaned, genres_list_counter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data_train_cleaned.head(n=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Production Companies\n\nHere we will do something similar to genres. Find out how many unique values there are, and create a column for each with a 1 if the film contains that production company, and a 0 if not."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets get the list of production companies\nprod_companies_list = flatten_data_column(data_train_cleaned['production_companies'])\nprod_companies_list_test = flatten_data_column(data_test_cleaned['production_companies'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a counter of the most popular\nprod_companies_list_counter = create_counter(prod_companies_list)\nprod_companies_list_counter_test = create_counter(prod_companies_list_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's select the top 30 companies\nprint(prod_companies_list_counter[:30])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prod_companies_FE(df, prod_companies_counter, limit=100):\n    df['num_production_companies'] = df['production_companies'].apply(lambda x: len(x) if x != {} else 0)\n    df['all_production_companies'] = df['production_companies'].apply(lambda x: [i['name'] for i in x])\n    \n    for prod_company, count in prod_companies_counter[:limit]:\n        df['production_company_' + \"_\".join(prod_company.split(\" \"))] = df['all_production_companies'].apply(lambda pcomp_list: 1 if prod_company in pcomp_list else 0)\n    \n    df['all_production_companies'] = df['all_production_companies'].apply(lambda x: \" \".join(x))\n    df = df.drop(['production_companies'], axis=1)\n        \n    return df\n\ndata_train_cleaned = prod_companies_FE(data_train_cleaned, prod_companies_list_counter, 30)\ndata_test_cleaned = prod_companies_FE(data_test_cleaned, prod_companies_list_counter_test, 30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Spoken Languages"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets get the list of spoken languages\nspoken_languages_list = flatten_data_column(data_train_cleaned['spoken_languages'])\nspoken_languages_list_test = flatten_data_column(data_test_cleaned['spoken_languages'])\n\n# Create a counter of the most popular\nspoken_languages_list_counter = create_counter(spoken_languages_list)\nspoken_languages_list_counter_test = create_counter(spoken_languages_list_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(spoken_languages_list_counter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def spoken_languages_FE(df, spoken_languages_counter, limit=56):\n    df['num_spoken_languages'] = df['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\n    df['all_spoken_languages'] = df['spoken_languages'].apply(lambda x: [i['name'] for i in x])\n    \n    for spoken_language, count in spoken_languages_counter[:limit]:\n        df['language_' + spoken_language] = df['all_spoken_languages'].apply(lambda language_list: 1 if spoken_language in language_list else 0)\n    \n    df['all_spoken_languages'] = df['all_spoken_languages'].apply(lambda x: \" \".join(x))\n    df = df.drop(['spoken_languages'], axis=1)\n        \n    return df\n\nspoken_languages_limit = 25\n\ndata_train_cleaned = spoken_languages_FE(data_train_cleaned, spoken_languages_list_counter, spoken_languages_limit)\ndata_test_cleaned = spoken_languages_FE(data_test_cleaned, spoken_languages_list_counter_test, spoken_languages_limit)\n\ndisplay(data_train_cleaned.head(n=2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Original Language"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creates a List of orignal languages\ndef list_original_languages(mylist):\n    flattened_list = []\n    for element in mylist:\n            flattened_list.append(element)\n    return flattened_list\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets get the list of spoken languages\noriginal_language_list = list_original_languages(data_train_cleaned['original_language'])\noriginal_language_list_test = list_original_languages(data_test_cleaned['original_language'])\n\n# Create a counter of the most popular\noriginal_language_list_counter = Counter(original_language_list).most_common()\noriginal_language_list_counter_test = Counter(original_language_list_test).most_common()\n\nprint(original_language_list_counter)\nprint(len(original_language_list_counter))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def original_language_FE(df, original_languages_counter, limit=56):\n    for original_language, count in original_languages_counter:\n        df['original_language_' + original_language] = df['original_language'].apply(lambda lang: 1 if lang == original_language else 0)\n\n    df = df.drop(['original_language'], axis=1)\n        \n    return df\n\ndata_train_cleaned = original_language_FE(data_train_cleaned, original_language_list_counter)\ndata_test_cleaned = original_language_FE(data_test_cleaned, original_language_list_counter_test)\n\ndisplay(data_train_cleaned.head(n=2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Keywords"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets get the list of kewords\nkeywords_list = flatten_data_column(data_train_cleaned['Keywords'])\nkeywords_list_test = flatten_data_column(data_test_cleaned['Keywords'])\n\n# Create a counter of the most popular keywords\nkeywords_list_counter = create_counter(keywords_list)\nkeywords_list_counter_test = create_counter(keywords_list_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(keywords_list_counter[:30])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def keywords_FE(df, keywords_counter, limit=100):\n    df['num_keywords'] = df['Keywords'].apply(lambda x: len(x) if x != {} else 0)\n    df['all_keywords'] = df['Keywords'].apply(lambda x: [i['name'] for i in x])\n    \n    for keyword, count in keywords_counter[:limit]:\n        df['keyword_' + keyword] = df['all_keywords'].apply(lambda keyword_list: 1 if keyword in keyword_list else 0)\n    \n    df['all_keywords'] = df['all_keywords'].apply(lambda x: \" \".join(x))\n    df = df.drop(['Keywords'], axis=1)\n        \n    return df\n\nkeywords_limit = 30\n\ndata_train_cleaned = keywords_FE(data_train_cleaned, keywords_list_counter, keywords_limit)\ndata_test_cleaned = keywords_FE(data_test_cleaned, keywords_list_counter_test, keywords_limit)\n\ndisplay(data_train_cleaned.head(n=2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Status"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets get the list of spoken languages\nstatus_list = list_original_languages(data_train_cleaned['status'])\nstatus_list_test = list_original_languages(data_test_cleaned['status'])\n\n# Create a counter of the most popular\nstatus_list_counter = Counter(status_list).most_common()\nstatus_list_counter_test = Counter(status_list_test).most_common()\n\nprint(status_list_counter)\nprint(status_list_counter_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is a bit weird that there are `Rumored` films in train data for which we already have revenue data. I think I'll just make two features out of this: `released` and `not-released`"},{"metadata":{"trusted":true},"cell_type":"code","source":"def status_FE(df):\n    # Status: 1 released, 0 not released.\n    df['status_released'] = df['status'].apply(lambda x: 1 if x == 'Released' else 0)\n    df = df.drop(['status'], axis=1)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ndata_train_cleaned = status_FE(data_train_cleaned)\ndata_test_cleaned = status_FE(data_test_cleaned)\n\ndisplay(data_train_cleaned.head(n=2))\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Production Countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets get the list of production countries\nprod_countries_list = flatten_data_column(data_train_cleaned['production_countries'])\nprod_countries_list_test = flatten_data_column(data_test_cleaned['production_countries'])\n\n# Create a counter of the most popular production countries\nprod_countries_list_counter = create_counter(prod_countries_list)\nprod_countries_list_counter_test = create_counter(prod_countries_list_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prod_countries_FE(df, prod_countries_counter, limit=100):\n    df['num_production_countries'] = df['production_countries'].apply(lambda x: len(x) if x != {} else 0)\n    df['all_production_countries'] = df['production_countries'].apply(lambda x: [i['name'] for i in x])\n    \n    for prod_country, count in prod_countries_counter[:limit]:\n        df['production_country_' + \"_\".join(prod_country.split(\" \"))] = df['all_production_countries'].apply(lambda pcountry_list: 1 if prod_country in pcountry_list else 0)\n    \n    df['all_production_countries'] = df['all_production_countries'].apply(lambda x: \" \".join(x))    \n    df = df.drop(['production_countries'], axis=1)\n        \n    return df\n\nprod_countries_limit = 30\n\ndata_train_cleaned = prod_countries_FE(data_train_cleaned, prod_countries_list_counter, prod_countries_limit)\ndata_test_cleaned = prod_countries_FE(data_test_cleaned, prod_countries_list_counter_test, prod_countries_limit)\n\ndisplay(data_train_cleaned.head(n=2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Cast\n\nFor Cast we don't only have the name, but also the gender. 0 is unespecified, 1 is female, and 2 is male. For the cast we can find the` most common names`, as well as the` most common characters` for each film. We can also find the` number of male/female/unkown` characters in each cast."},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data_train_cleaned.iloc[0]['cast'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets get the list of different cast\ncast_list = flatten_data_column(data_train_cleaned['cast'])\ncast_list_test = flatten_data_column(data_test_cleaned['cast'])\n\ncast_name_list_counter = create_counter(cast_list, 'name')\ncast_name_list_counter_test = create_counter(cast_list_test, 'name')\n\ncast_character_list_counter = create_counter(cast_list, 'character')\ncast_character_list_counter_test = create_counter(cast_list_test, 'character')\n\n\nprint(cast_name_list_counter[:10])\nprint(cast_character_list_counter[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cast_FE(df, cast_counter, character_counter, limit=100):\n    df['num_cast'] = df['cast'].apply(lambda x: len(x) if x != {} else 0)\n    df['all_cast'] = df['cast'].apply(lambda x: [i['name'] for i in x])\n    df['all_characters'] = df['cast'].apply(lambda x: [i['character'] for i in x])\n    \n    # Get the sum of each of the cast genders in a film: 0 `unknown`, 1 `female`, 2 `male`\n    df['genders_0_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n    df['genders_1_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n    df['genders_2_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n    \n    # Create new columns for actors\n    for cast, count in cast_counter[:limit]:\n        df['cast_name_' + \"_\".join(cast.split(\" \"))] = df['all_cast'].apply(lambda cast_list: 1 if cast in cast_list else 0)\n        \n    # Create new columns for characters\n    for character, count in character_counter[:limit]:\n        df['cast_char_' + \"_\".join(character.split(\" \"))] = df['all_characters'].apply(lambda char_list: 1 if character in char_list else 0)\n    \n    df['all_cast'] = df['all_cast'].apply(lambda x: \" \".join(x))\n    df['all_characters'] = df['all_characters'].apply(lambda x: \" \".join(x))  \n    df = df.drop(['cast'], axis=1)\n        \n    return df\n\ncast_limit = 30\n\ndata_train_cleaned = cast_FE(data_train_cleaned, cast_name_list_counter, cast_character_list_counter, cast_limit)\ndata_test_cleaned = cast_FE(data_test_cleaned, cast_name_list_counter_test, cast_character_list_counter_test, cast_limit)\n\n\ndisplay(data_train_cleaned.head(n=2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll create 3 different types of columns: top n characters, top n actors and count of female/male/unkown in crew."},{"metadata":{},"cell_type":"markdown","source":"#### Crew\n\nThe crew has `department`, `gender`, `job` and `name`. All of this information can be useful for our predictions as the crew plays an important part on the quality of the film. For the crew we can find the most common people for each department, the amount of males, females and 'unespecified' in the crew."},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data_train_cleaned.iloc[0]['crew'][:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets get the list of different cast\ncrew_list = flatten_data_column(data_train_cleaned['crew'])\ncrew_list_test = flatten_data_column(data_test_cleaned['crew'])\n\ncrew_name_list_counter = create_counter(crew_list, 'name')\ncrew_name_list_counter_test = create_counter(crew_list_test, 'name')\n\ncrew_job_list_counter = create_counter(crew_list, 'job')\ncrew_job_list_counter_test = create_counter(crew_list_test, 'job')\n\ncrew_dep_list_counter = create_counter(crew_list, 'department')\ncrew_dep_list_counter_test = create_counter(crew_list_test, 'department')\n\nprint(crew_name_list_counter[:10])\nprint(crew_job_list_counter[:10])\nprint(crew_dep_list_counter[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crew_FE(df, crew_counter, job_counter, dep_counter, limit=100):\n    df['num_crew'] = df['crew'].apply(lambda x: len(x) if x != {} else 0)\n    df['all_crew'] = df['crew'].apply(lambda x: [i['name'] for i in x])\n\n    \n    # Get the sum of each of the cast genders in a film: 0 `unknown`, 1 `female`, 2 `male`\n    df['genders_0_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n    df['genders_1_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n    df['genders_2_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n    \n    # Create new columns for crew members\n    for crew_name, count in crew_counter[:limit]:\n        df['crew_name_' + \"_\".join(crew_name.split(\" \"))] = df['all_crew'].apply(lambda crew_list: 1 if crew_name in crew_list else 0)\n        \n    # Create new columns for crew jobs\n    for crew_job, count in job_counter[:limit]:\n        df['crew_job_' + \"_\".join(crew_job.split(\" \"))] = df['crew'].apply(lambda job_list: sum([1 for i in job_list if i['job'] == crew_job]))\n    \n    # Create new columns for crew deparments\n    for crew_dep, count in job_counter[:limit]:\n        df['crew_department_' + \"_\".join(crew_dep.split(\" \"))] = df['crew'].apply(lambda dep_list: sum([1 for i in dep_list if i['job'] == crew_dep]))\n        \n    \n    df['all_crew'] = df['all_crew'].apply(lambda x: \" \".join(x))  \n    df = df.drop(['crew'], axis=1)\n        \n    return df\n\ncrew_limit = 30\n\ndata_train_cleaned = crew_FE(data_train_cleaned, crew_name_list_counter, crew_job_list_counter, crew_dep_list_counter, crew_limit)\ndata_test_cleaned = crew_FE(data_test_cleaned, crew_name_list_counter_test, crew_job_list_counter_test, crew_dep_list_counter_test, crew_limit)\n\ndisplay(data_train_cleaned.head(n=2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dealing with Skewness\n\nLets explore some of the numerical data and check for skewness.\n\n* Budget\n* Popularity\n* Revenue (target)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Formula to apply logarithmic transformation for skewed data\ndef log_transform(df, feature):\n    df['log_' + feature] = df[feature].apply(lambda x: np.log(x + 1))\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Budget skewness**\n\nIt seems that the budget has a positive skew. We can apply logarithmic transformation  to make the data more even."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(20,10))\nfig.suptitle('Budget Distribution', fontsize=15)\nfig.subplots_adjust(hspace=0.5)\n\nax[0,0].set_title(\"Train data\")\nplt1 = sns.distplot(data_train_cleaned['budget'], ax=ax[0,0])\nplt1.grid()\n\nax[0,1].set_title(\"Test data\")\nplt2 = sns.distplot(data_test_cleaned['budget'], ax=ax[0,1])\nplt2.grid()\n\nax[1,0].set_title(\"Train data log\")\nplt1 = sns.distplot(log_transform(data_train_cleaned, 'budget')['log_budget'], ax=ax[1,0])\nplt1.grid()\n\nax[1,1].set_title(\"Test data log\")\nplt2 = sns.distplot(log_transform(data_test_cleaned, 'budget')['log_budget'], ax=ax[1,1])\nplt2.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate skweness\nprint(\"Skweness budget for train data: {}\".format(stats.skew(data_train_cleaned['budget'])))\nprint(\"Skweness budget for test data: {}\\n\".format(stats.skew(data_test_cleaned['budget'])))\n\n# Skeweness of log data\nprint(\"Skweness budget for log train data: {}\".format(stats.skew(log_transform(data_train_cleaned, 'budget')['log_budget'])))\nprint(\"Skweness budget for log test data: {}\".format(stats.skew(log_transform(data_test_cleaned, 'budget')['log_budget'])))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After applying logarithmic transform to `budget` we can see that it only skews the data to the left. However it does even the data a little bit."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_cleaned = data_train_cleaned.drop(['budget'], axis=1)\ndata_test_cleaned = data_test_cleaned.drop(['budget'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Popularity skewness**\n\nAs we can see below popularity data is very skewed. We'll need to apply logarithmic transformations."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(20,10))\nfig.suptitle('Popularity Distribution', fontsize=15)\nfig.subplots_adjust(hspace=0.5)\n\n\nax[0,0].set_title(\"Train data\")\nplt1 = sns.distplot(data_train_cleaned['popularity'], bins=50, ax=ax[0,0])\nplt1.grid()\n\nax[0,1].set_title(\"Test data\")\nplt2 = sns.distplot(data_test_cleaned['popularity'],  bins=50, ax=ax[0,1])\nplt2.grid()\n\n\nax[1,0].set_title(\"Train data log\")\nplt1 = sns.distplot(log_transform(data_train_cleaned, 'popularity')['log_popularity'], ax=ax[1,0])\nplt1.grid()\n\nax[1,1].set_title(\"Test data log\")\nplt2 = sns.distplot(log_transform(data_test_cleaned, 'popularity')['log_popularity'], ax=ax[1,1])\nplt2.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skweness popularity for train data: {}\".format(stats.skew(data_train_cleaned['popularity'])))\nprint(\"Skweness popularity for test data: {}\\n\".format(stats.skew(data_test_cleaned['popularity'])))\n\nprint(\"Skweness popularity for log train data: {}\".format(stats.skew(data_train_cleaned['log_popularity'])))\nprint(\"Skweness popularity for log test data: {}\".format(stats.skew(data_test_cleaned['log_popularity'])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying a logarithm transformation definitely reduces skeweness in popularity. We'll use the log values instead of the original data."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_cleaned = data_train_cleaned.drop(['popularity'], axis=1)\ndata_test_cleaned = data_test_cleaned.drop(['popularity'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Revenue Skewness**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, figsize=(20,5))\nfig.suptitle('Revenue Distribution', fontsize=15)\nfig.subplots_adjust(hspace=0.5)\n\n\nax[0].set_title(\"Train data\")\nplt1 = sns.distplot(data_train_cleaned['revenue'], bins=50, ax=ax[0])\nplt1.grid()\n\nax[1].set_title(\"Train data log\")\nplt1 = sns.distplot(log_transform(data_train_cleaned, 'revenue')['log_revenue'], ax=ax[1])\nplt1.grid()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skweness popularity for train data: {}\".format(stats.skew(data_train_cleaned['revenue'])))\nprint(\"Skweness popularity for log train data: {}\".format(stats.skew(data_train_cleaned['log_revenue'])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the dependent variable (revenue) is skewed, thus we apply logarithmic transformation. However I need to remember to apply the inverse (exp) to the predicted values at the end."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the original values of revenue\ndata_train_cleaned = data_train_cleaned.drop(['revenue'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dealing with text data\n\n**To-do**: Find out how to turn text data into some sort of score later on?\n\n* Overview\n* Tagline"},{"metadata":{},"cell_type":"markdown","source":"Calculating Word Frequencies with TF-IDF vectorizer. This stands for *Term Frequency - Inverse document frequency* which are the scores resulting from applying it:\n* Term Frequency: How often a word appears within a document\n* Inverse Document Frequency: Downscales words that appear a lot accross documents.\n\n[How to Prepare Text Data for Machine Learning with scikit-learn](https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/)"},{"metadata":{},"cell_type":"markdown","source":"**Overview**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LinearRegression\nimport eli5\n\nvectorizer_overview = TfidfVectorizer(\n            sublinear_tf=True,\n            analyzer='word',\n            token_pattern=r'\\w{1,}',\n            ngram_range=(1, 2),\n            min_df=5)\n\nvectorizer_overview = vectorizer_overview.fit(data_train_cleaned['overview'].fillna(''))\noverview_text = vectorizer_overview.transform(data_train_cleaned['overview'].fillna(''))\nlinreg_overview = LinearRegression()\nlinreg_overview.fit(overview_text, data_train_cleaned['log_revenue'])\neli5.show_weights(linreg_overview, vec=vectorizer_overview, top=20, feature_filter=lambda x: x != '<BIAS>')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that I have vectorized the words into a matrix and trained a basic model I can add revenue predictions to each row based on the overview."},{"metadata":{"trusted":true},"cell_type":"code","source":"def overview_FE(df, predictor, vect):\n    df['word_revenue'] = df['overview'].apply(lambda x: predictor.predict(vect.transform([x]))[0]) \n    \n    return df\n\n'''\ndata_train_cleaned =  overview_FE(data_train_cleaned, linreg, vectorizer_overview)\n\ndisplay(data_train_cleaned['word_revenue'].head(n=5))\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tagline**\n\nLets do the same we have done for overview, for tagline."},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer_tagline = TfidfVectorizer(\n            sublinear_tf=True,\n            analyzer='word',\n            token_pattern=r'\\w{1,}',\n            ngram_range=(1, 2),\n            min_df=5)\n\nvectorizer_tagline = vectorizer_tagline.fit(data_train_cleaned['tagline'].fillna(''))\noverview_text = vectorizer_tagline.transform(data_train_cleaned['tagline'].fillna(''))\nlinreg_tagline = LinearRegression()\nlinreg_tagline.fit(overview_text, data_train_cleaned['log_revenue'])\neli5.show_weights(linreg_tagline, vec=vectorizer_tagline, top=20, feature_filter=lambda x: x != '<BIAS>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ndef tagline_FE(df, predictor, vect):\n    df['tagline_revenue'] = df['tagline'].apply(lambda x: predictor.predict(vect.transform([x]))[0]) \n    \n    return df\n\ndata_train_cleaned =  tagline_FE(data_train_cleaned, linreg_tagline, vectorizer_tagline)\n\ndisplay(data_train_cleaned['tagline_revenue'].head(n=5))\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data_train_cleaned.head(n=2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove unwanted features\n\nFinally we remove some of the features that we won't need anymore (at least for now). I will call them Final, however I still might need to apply some scaling if we I am goingn to use something like a Neural Network."},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_drop = ['original_title', 'overview', 'release_date', 'tagline', 'title',\n                   'all_genres', 'all_production_companies', 'all_spoken_languages',\n                   'all_keywords', 'all_production_countries', 'all_cast', 'all_cast',\n                   'all_crew', 'all_characters']\n\ndef drop_features(df, features_list):\n    df = df.drop(features_list, axis=1)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_cleaned = drop_features(data_train_cleaned, features_to_drop)\ndata_test_cleaned = drop_features(data_test_cleaned, features_to_drop)\n\ndisplay(data_train_cleaned.head())\ndisplay(data_test_cleaned.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding up all the Data Pre-Processing and Feature Engineering\n\nI'll create a function to add up all the data pre-processing and Feauture Engineering into a single function that we can apply individually to each model when performing Cross Validation. This will help avoid Overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering (df):\n    \"\"\"\n    Data Pre-Processing\n    \"\"\"\n    df = runtime_pre_process(df)\n    \n    df = release_year_pre_process(df)\n    \n    df = homepage_pre_process(df)\n    \n    df = poster_pre_process(df)\n    \n    df = imputing_budget(df)\n    \n    df = collection_pre_process(df)\n    \n    df = genres_pre_processing(df)\n    \n    df = imput_title(df)\n    \n    df = prod_comp_pre_processing(df)\n    \n    df = spoken_lang_pre_processing(df)\n    \n    df = keywords_pre_processing(df)\n    \n    df = cast_pre_processing(df)\n    \n    df = crew_pre_processing(df)\n    \n    df = overview_pre_process(df)\n    \n    df = tagline_pre_process(df)\n    \n    df = prod_countries_pre_process(df)\n    \n    df = status_pre_process(df)\n    \n    \"\"\"\n    Feature Engineering\n    \"\"\"\n    # Convert panda string to list\n    df = string2_list(df)\n    \n    # Genres\n    genres_list = flatten_data_column(data_train_preprocessed['genres'])\n    genres_list_counter = create_counter(genres_list)\n    df = genres_FE(df, genres_list_counter)\n    \n    # Production Companies\n    prod_companies_list = flatten_data_column(data_train_preprocessed['production_companies'])\n    prod_companies_list_counter = create_counter(prod_companies_list)\n    df = prod_companies_FE(df, prod_companies_list_counter, 30)\n    \n    # Spoken Languages\n    spoken_languages_list = flatten_data_column(data_train_preprocessed['spoken_languages'])\n    spoken_languages_list_counter = create_counter(spoken_languages_list)\n    df = spoken_languages_FE(df, spoken_languages_list_counter, 25)\n    \n    # Original Language\n    original_language_list = list_original_languages(data_train_preprocessed['original_language'])\n    original_language_list_counter = Counter(original_language_list).most_common()\n    df = original_language_FE(df, original_language_list_counter)\n    \n    # Keywords\n    keywords_list = flatten_data_column(data_train_preprocessed['Keywords'])\n    keywords_list_counter = create_counter(keywords_list)\n    df = keywords_FE(df, keywords_list_counter, 30)\n    \n    # Status\n    status_list = list_original_languages(data_train_preprocessed['status'])\n    status_list_counter = Counter(status_list).most_common()\n    df = status_FE(df)\n    \n    # Production Countries\n    prod_countries_list = flatten_data_column(data_train_preprocessed['production_countries'])\n    prod_countries_list_counter = create_counter(prod_countries_list)\n    df = prod_countries_FE(df, prod_countries_list_counter, 30)\n    \n    # Cast\n    cast_list = flatten_data_column(data_train_preprocessed['cast'])\n    cast_name_list_counter = create_counter(cast_list, 'name')\n    cast_character_list_counter = create_counter(cast_list, 'character')\n    df = cast_FE(df, cast_name_list_counter, cast_character_list_counter, 30)\n    \n    # Crew\n    crew_list = flatten_data_column(data_train_preprocessed['crew'])\n    crew_name_list_counter = create_counter(crew_list, 'name')\n    crew_job_list_counter = create_counter(crew_list, 'job')\n    crew_dep_list_counter = create_counter(crew_list, 'department')\n    df = crew_FE(df, crew_name_list_counter, crew_job_list_counter, crew_dep_list_counter, 30)\n    \n    # Budget fix skewness\n    df = log_transform(df, 'budget')\n    df = df.drop(['budget'], axis=1)\n    \n    # Popularity fix skewness\n    df = log_transform(df, 'popularity')\n    df = df.drop(['popularity'], axis=1)\n\n    \n    # Revenue fix skewness\n    #df = log_transform(df, 'revenue')\n    #df = df.drop(['revenue'], axis=1)\n    \n    # Overview ---> Overfits the data\n    # df =  overview_FE(df, linreg_overview, vectorizer_overview)\n    \n    # Tagline ----> Overfits the data\n    # df =  tagline_FE(df, linreg_tagline, vectorizer_tagline)\n\n    \n    # Drop unwanted Features\n    features_to_drop = ['original_title', 'overview', 'release_date', 'tagline', 'title',\n                   'all_genres', 'all_production_companies', 'all_spoken_languages',\n                   'all_keywords', 'all_production_countries', 'all_cast', 'all_cast',\n                   'all_crew', 'all_characters', 'imdb_id']\n    \n    df = drop_features(df, features_to_drop)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3 Further data exploration\n\nNow that the data looks nice and clean I can do more data exploration:\n* `Revenue` vs `Popularity`\n* `Number of films` released vs `release_year`\n* `Average Revenue` vs `year`\n* `Number of films` released vs `month`\n* `Number of films` vs day of the `month`\n* `Belongs to collection` vs `not belongs to collection`\n* `Revenue` vs `genre`\n* `Revenue` vs `num_genres`\n* `Revenue` vs `production_company`\n* `Revenue` vs `num production_company`\n* `Revenue` vs `production_country`\n* `Revenue` vs `num_production_countries`\n* `Revenue` vs `cast_member`\n* `Revenue` vs `num_cast_member`\n* `Revenue` vs `crew_member`\n* `Revenue` vs `num_crew_member`"},{"metadata":{},"cell_type":"markdown","source":"## 4. Choose evaluation metrics\n\nThe evaulation metric used by the competition is the [Root-mean-square deviation](https://en.wikipedia.org/wiki/Root-mean-square_deviation), [Khan-academy video](https://www.youtube.com/watch?v=zMFdb__sUpw)\n\nThe root-mean-square deviation is (as the name says) the square root of the standard deviation of the squared residuals (distance from the point to the line on the y axis)"},{"metadata":{},"cell_type":"markdown","source":"## 5. Create a benchmark an improve from it\n\nLets create a simple model with no optimization see how well it does. Lets try to improve from there."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nseed = np.random.seed(34)\n\n# Train data\ndata_train_bench = data_train_raw.copy().drop(['id', 'revenue'], axis=1)\ndata_train_bench = feature_engineering(data_train_bench)\n# Test data\ndata_test_bench = data_test_raw.copy().drop(['id'], axis=1)\ndata_test_bench = feature_engineering(data_test_bench)\n\n# Target data\ntarget_train_bench = data_train_cleaned['log_revenue']\n\ntrain_X_bench,test_X_bench, train_y_bench, test_y_bench = train_test_split(data_train_bench,\n                                                                           target_train_bench,\n                                                                           test_size=0.25,\n                                                                           random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_bench = lgb.Dataset(train_X_bench, label=train_y_bench)\nval_data_bench = lgb.Dataset(test_X_bench, label=test_y_bench)\n\nnum_round = 1000\n\nparam = {'num_leaves':31,\n         'metric': 'rmse',\n         'objective':'regression'}\n\n# Train the model\nclf = lgb.train(param,\n                train_data_bench,\n                num_round,\n                valid_sets=[train_data_bench, val_data_bench],\n                verbose_eval=5000,\n                early_stopping_rounds=100)\n\nypred = clf.predict(test_X_bench, num_iteration=clf.best_iteration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the rms of the model.\nrms_benchmark = sqrt(mean_squared_error(test_y_bench, ypred))\nprint(\"The rms score is: {}\".format(rms_benchmark))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The score to beat is **2.079** in the valid dataset."},{"metadata":{},"cell_type":"markdown","source":"## 6. Split the data into k-folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate the target variable \ntarget = data_train_cleaned['log_revenue']\n# Drop target data from train\ndata_train = data_train_raw.copy().drop(['id', 'revenue'], axis=1)\n\n# Test data\ndata_test = data_test_raw.copy().drop(['id'], axis=1)\ndata_test = feature_engineering(data_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\n# Split k-fold validation\nn_splits = 7\nrandom_state = np.random.seed(654658)\n\nkf = KFold(n_splits=n_splits, random_state=random_state, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Choose Models and implement them\n\nProblably LightGBM + Feed Forward Network and maybe something else."},{"metadata":{},"cell_type":"markdown","source":"**LightGBM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 3,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**FIX**: When applying FE to X_train and X_val separately I get a different amount of columns (features). This can be because the valid fold, being smaller, generates less features when one hot encoding some of them, such as language, country etc.\n\nPossible fixes:\n* Apply a minimum of features that need to be present. If the features available is smaller than the minimum, fill with 0s??\n* Only use the top 5-10 of every feature rather than top 30\n\nFor now I will apply FE to all the data and then split it, just to check if the model works."},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof -> Out of fold. One single vector with all the validation predictions to\n# then calculate the error upon this predictions.\noof = np.zeros(len(data_train))\npredictions_test = np.zeros(len(data_test_cleaned))\n\n# K-fold CV\nfor epoch, (train_index, val_index) in enumerate(kf.split(data_train.values)):\n    # Only temporary\n    training_data = data_train.copy()\n    \n    X_train, X_val = training_data.loc[train_index], training_data.loc[val_index]\n    y_train, y_val = target.loc[train_index], target.loc[val_index]\n    \n    # Need to fix this\n    X_train = feature_engineering(X_train)\n    X_val = feature_engineering(X_val)\n    \n    print(\"Fold index: {}\".format(epoch + 1))\n    \n    train_data = lgb.Dataset(X_train, label=y_train)\n    val_data = lgb.Dataset(X_val, label=y_val)\n    \n    num_round = 1000000\n    \n    # Train the model\n    clf = lgb.train(params,\n                    train_data,\n                    num_round,\n                    valid_sets=[train_data, val_data],\n                    verbose_eval=5000,\n                    early_stopping_rounds=1000) \n    \n    # Out of fold vector\n    oof[val_index] = clf.predict(X_val.astype('float32'), num_iteration=clf.best_iteration)\n    \n    # Calculate the average predictions for all folds for the test submsission data\n    predictions_test += (clf.predict(data_test, num_iteration=clf.best_iteration) / kf.n_splits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the rms of the model.\nrms = sqrt(mean_squared_error(target, oof))\nprint(\"The rms score is: {}\".format(rms))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CatBoost**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import Pool, CatBoostRegressor\n\nclf_cat = CatBoostRegressor(iterations=10000,\n                            learning_rate=0.01,\n                            depth=5, \n                            eval_metric='RMSE',\n                            random_seed=23,\n                            early_stopping_rounds=200,\n                            logging_level='Verbose')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof -> Out of fold. One single vector with all the validation predictions to\n# then calculate the error upon this predictions.\noof_cat = np.zeros(len(data_train))\npredictions_test_cat = np.zeros(len(data_test_cleaned))\n\n# K-fold CV\nfor epoch, (train_index, val_index) in enumerate(kf.split(data_train.values)):\n    training_data = data_train.copy()\n    \n    X_train, X_val = training_data.loc[train_index], training_data.loc[val_index]\n    y_train, y_val = target.loc[train_index], target.loc[val_index]\n    \n    X_train = feature_engineering(X_train)\n    X_val = feature_engineering(X_val)\n    \n    print(\"Fold index: {}\".format(epoch + 1))\n    \n    # train the model\n    clf_cat.fit(X_train, y_train,\n                eval_set=(X_val,y_val),\n                use_best_model=True,\n                verbose=False)\n    \n    # Out of fold vector\n    oof_cat[val_index] = clf_cat.predict(X_val.astype('float32'))\n    \n    # Calculate the average predictions for all folds for the test submsission data\n    predictions_test_cat += (clf_cat.predict(data_test) / kf.n_splits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the rms of the model.\nrms = sqrt(mean_squared_error(target, oof_cat))\nprint(\"The rms score is: {}\".format(rms))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feed Forward Neural Network**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imports\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch import optim\nimport torch.utils.data as Data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MinMax Scaler function"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n\n\ndef scaled_data(df):\n    scaler = MinMaxScaler()\n    features = [feature for feature in df.columns]\n    \n    df[features] = scaler.fit_transform(df[features])\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checks if GPU is available.\nisGPUAvailable = torch.cuda.is_available()\ndevice = \"cpu\"\n\nif isGPUAvailable:\n    device = \"cuda\"\n    print(\"Training on GPU\")\nelse:\n    device = \"cpu\"\n    print(\"Training on CPU\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model\nclass Classifier(nn.Module):\n    def __init__(self, in_classes, dropout=0.5):\n        super().__init__()\n        self.input_dim = in_classes\n        self.hidden_1 = int(self.input_dim)\n        self.fc1 = nn.Linear(self.input_dim, int(self.hidden_1))\n        self.fc2 = nn.Linear(int(self.hidden_1), int(self.hidden_1/2))\n        self.fc3 = nn.Linear(int(self.hidden_1/2), 1)\n        #self.fc4 = nn.Linear(int(self.hidden_1/2), int(self.hidden_1/4))\n        #self.fc5 = nn.Linear(int(self.hidden_1/4), 1)\n\n        # Dropout\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x):\n        # make sure input tensor is flattened\n        x = x.view(x.shape[0], -1)\n\n        # Now with dropout\n        x = self.dropout(F.relu(self.fc1(x)))\n        x = self.dropout(F.relu(self.fc2(x)))\n        #x = self.dropout(F.relu(self.fc3(x)))\n        #x = self.dropout(F.relu(self.fc4(x)))\n\n\n        # output so no dropout here\n        out = self.fc3(x)\n\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Create the model instance\nin_classes = data_test.shape[1]\ndropout = 0.4\n\n\n# Create the feed forward deep learning classifier\nff_classifier = Classifier(in_classes=in_classes, dropout=dropout)\n\n# Move classifier to GPU\nff_classifier.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and validation\ndef train (clf, train_loader, valid_loader, epochs=40, min_valid_loss=np.Inf, lr=0.01):\n    best_clf = clf\n    optimizer = torch.optim.SGD(clf.parameters(), lr = lr, momentum=0.5, nesterov=True)\n    #optimizer = torch.optim.Adam(clf.parameters(), lr = lr, weight_decay=0.0001)\n    #optimizer = torch.optim.RMSprop(clf.parameters(), lr = lr, weight_decay=0.0001, momentum=0.1)\n    \n    criterion = nn.MSELoss()\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, verbose=True,\n                                                     patience=20, min_lr=0.00001)\n    \n    for epoch in range(epochs):\n        train_loss = 0.0\n        validation_loss = 0.0\n        \n        \n        # train\n        clf.train()\n        for inputs, labels in train_loader:\n            # Reset optimizer for every iteration.\n            optimizer.zero_grad()\n            \n            # Move tensors to GPU\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            # Forward pass\n            output = clf(inputs)\n            \n            # Loss\n            loss = criterion(output.squeeze(), labels)\n            \n            # Backward pass (Backpropagation)\n            loss.backward()\n            \n            # Update weights\n            optimizer.step()\n            \n            # Update the loss.\n            train_loss += loss.item()\n            \n        # Validation\n        clf.eval()\n        for inputs, labels in valid_loader:\n            # Move tensors to GPU\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            # Forward pass\n            output = clf(inputs)\n            \n            # Loss\n            loss = criterion(output.squeeze(), labels)\n            \n            # Update the validation loss.\n            validation_loss += loss.item()\n            \n        \n        # Calculate the losses.\n        train_loss = train_loss/len(train_loader)\n        validation_loss = validation_loss/len(valid_loader)\n        \n        #Update lr\n        scheduler.step(validation_loss)\n        \n        # Print the losses\n        print(\"Epoch {0}\".format(epoch + 1))\n        #print('LR:', scheduler.get_lr())\n        print(\"Train loss = {0}\".format(train_loss))\n        print(\"Validation loss = {0}\".format(validation_loss))\n        \n        # Check if validation loss has reduced, and therefore the model predicts better\n        if validation_loss < min_valid_loss:\n            min_valid_loss = validation_loss\n            print(\"Validation loss has decreased. Saving the model...\")\n            best_clf = clf\n        print(\"------------------------------------\")\n    return best_clf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-fold cross validation\nlr = 0.001\nepochs = 40\nbatch_size = 60\n\n# Convert test data to tensor\nx_test = np.array(scaled_data(data_test))\nx_test_tensor = torch.tensor(x_test, dtype=torch.float).to(device)\n\ntest_preds_nn = np.zeros((len(data_test)))\noof_nn = np.zeros(len(data_train))\n\n\nfor fold_i, (train_index, val_index) in enumerate(kf.split(data_train.values)):\n    \n    print(\"\\n\")\n    print(\"Fold {0}\".format(fold_i + 1))\n    \n    training_data = data_train.copy()\n    \n    x_train_raw, x_val_raw = training_data.loc[train_index], training_data.loc[val_index]\n    y_train_raw, y_val_raw = target.loc[train_index].values, target.loc[val_index].values\n    \n    x_train_raw = np.array(scaled_data(feature_engineering(x_train_raw)))\n    x_val_raw = np.array(scaled_data(feature_engineering(x_val_raw)))\n    \n    \n    x_train_fold = torch.tensor(x_train_raw, dtype=torch.float)\n    y_train_fold = torch.tensor(y_train_raw, dtype=torch.float32)\n    \n    x_val_fold = torch.tensor(x_val_raw, dtype=torch.float)\n    y_val_fold = torch.tensor(y_val_raw, dtype=torch.float32)\n    \n    train_dataset = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n    valid_dataset = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n    \n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n    \n    \n    model = train(ff_classifier, train_loader, valid_loader, epochs=epochs, lr = lr)\n    \n    # Out of fold data to check accuracy at the end\n    oof_nn[val_index] = model(x_val_fold.to(device)).squeeze().to('cpu').detach().numpy()   \n    \n    test_preds_nn += model(x_test_tensor).squeeze().to('cpu').detach().numpy() / kf.n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the rms of the model.\nrms = sqrt(mean_squared_error(target, oof_nn))\nprint(\"The rms score is: {}\".format(rms))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Create an ensemble model\n\nIn this section I'll create an ensemble model. Then I'll submit the ensemble that has the best accuracy in the oof.\n\n**Note**: I don't need to reverse the logarithms of revenue for the train predictions (oof) because this data is not going to use to make the submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_oof = 0.3*oof + 0.4*oof_nn + 0.3*oof_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the rms of the model.\nrms_lgbm = sqrt(mean_squared_error(target, oof))\nprint(\"The rms score for LigthGBM is: {}\".format(rms_lgbm))\n\n# Calculate the rms of the model.\nrms_lgbm = sqrt(mean_squared_error(target, oof_nn))\nprint(\"The rms score for Neural Netowork is: {}\".format(rms_lgbm))\n\n# Calculate the rms of the model.\nrms_lgbm = sqrt(mean_squared_error(target, oof_cat))\nprint(\"The rms score for CatBoost is: {}\".format(rms_lgbm))\n\n# Calculate the rms of the model.\nrms = sqrt(mean_squared_error(target, ensemble_oof))\nprint(\"The rms score for ensemble is: {}\".format(rms))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_oof_2 = 0.4*oof + 0.3*oof_nn + 0.3*oof_cat\nensemble_oof_3 = 0.3*oof + 0.3*oof_nn + 0.4*oof_cat\nensemble_trees = 0.6*oof + 0.4*oof_cat\n\n\n# Calculate the rms of the model.\nrms = sqrt(mean_squared_error(target, ensemble_oof_2))\nprint(\"The rms score for ensemble 2 is: {}\".format(rms))\n# Calculate the rms of the model.\nrms = sqrt(mean_squared_error(target, ensemble_oof_3))\nprint(\"The rms score for ensemble 3 is: {}\".format(rms))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### FINAL PREDICTIONS FOR SUMBISSION\n\nI applied the logarithm for the target `revenue` before to deal with skewness. Now I have to undo this before I make the final submission."},{"metadata":{},"cell_type":"markdown","source":"**Predictions for the test data with LGBM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Log PREDICTION results from the lgbm\nlog_predictions_lgbm = pd.DataFrame(data=predictions_test, columns=['revenue'])\n# Undo logs for the final submission\npredictions_lgbm = log_predictions_lgbm['revenue'].apply(lambda x: np.exp(x) - 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Predictions for the test data with Neural Network**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Log PREDICTION results from the Neural Network\nlog_predictions_nn = pd.DataFrame(data=test_preds_nn, columns=['revenue'])\n# Undo logs for the final submission\npredictions_nn = log_predictions_nn['revenue'].apply(lambda x: np.exp(x) - 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Predictions for the test data with Neural Network**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Log PREDICTION results from the Neural Network\nlog_predictions_cat = pd.DataFrame(data=predictions_test_cat, columns=['revenue'])\n# Undo logs for the final submission\npredictions_cat = log_predictions_nn['revenue'].apply(lambda x: np.exp(x) - 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ensemble of Test Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_submission = 0.3*predictions_lgbm + 0.4*predictions_nn + 0.3*predictions_cat\nensemble_submission_2 = 0.4*predictions_lgbm + 0.3*predictions_nn + 0.3*predictions_cat\nensemble_submission_3 = 0.3*predictions_lgbm + 0.3*predictions_nn + 0.4*predictions_cat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Create submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(ensemble_submission.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission 1\nsubmission_ensemble = pd.DataFrame({\"id\": data_test_raw[\"id\"].values})\nsubmission_ensemble[\"revenue\"] = ensemble_submission\nsubmission_ensemble.to_csv(\"submission.csv\", index=False)\n\n# Submission 2\nsubmission_ensemble_2 = pd.DataFrame({\"id\": data_test_raw[\"id\"].values})\nsubmission_ensemble_2[\"revenue\"] = ensemble_submission_2\nsubmission_ensemble_2.to_csv(\"submission2.csv\", index=False)\n\n# Submission 3\nsubmission_ensemble_3 = pd.DataFrame({\"id\": data_test_raw[\"id\"].values})\nsubmission_ensemble_3[\"revenue\"] = ensemble_submission_3\nsubmission_ensemble_3.to_csv(\"submission3.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(len(submission_ensemble))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}