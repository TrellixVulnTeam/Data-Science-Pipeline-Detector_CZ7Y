{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('ggplot')\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, KFold\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import StandardScaler\nstop = set(stopwords.words('english'))\nimport os\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nimport json\nimport ast\nimport eli5\nimport shap\nfrom catboost import CatBoostRegressor\nfrom urllib.request import urlopen\nfrom PIL import Image\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import linear_model\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\n\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9784909832dfe96a30d577ac04d47a1826aa9147"},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv')\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"643dc1dffda7179589e9dc46573e4e5677dcf6be"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7dc5c2222190b7a05a32f05f813082fd69508c6c"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c62dce9f22244cfaa56f07f5236757044d6c7ce2"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4933d2d81f80b38cd8045d2750217d208d2a89b"},"cell_type":"code","source":"# load data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bab482b0d474df601ce430ffcbc62b0b1570b11e"},"cell_type":"code","source":"dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\ndef text_to_dict(df):\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n    return df\n\ntrain = text_to_dict(train)\nprint(train.head())\ntest = text_to_dict(test)\n# print(test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b3b44c4c651e1ea59f67d1e329e060e04e67de5"},"cell_type":"code","source":"# Train数据中只有3000个样本！希望这足够训练模特。\n\n# 我们可以看到一些列包含有字典的列表。有些列表包含一个字典，有些则包含几个然后我们从这些列中提取数据！","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f0f892f1001127a6451af42173ce0af5d88e443"},"cell_type":"code","source":"for i, e in enumerate(train['belongs_to_collection'][:5]):\n    print(i, e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2a59b2309857fc0660de8260b8daa0d71114133"},"cell_type":"code","source":"train['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c95a3d56fa94aa5bae38454c27939e6af4484f3","_kg_hide-input":false},"cell_type":"code","source":"\ntrain['collection_name'] = train['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0)\ntrain['has_collection'] = train['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\n\ntest['collection_name'] = test['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0)\ntest['has_collection'] = test['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\n\ntrain = train.drop(['belongs_to_collection'], axis=1)\ntest = test.drop(['belongs_to_collection'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(train['collection_name'])\n# print(train['has_collection'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6080fb74727794714a140f4dc9335b76f0d217cf"},"cell_type":"code","source":"for i, e in enumerate(train['genres'][:5]):\n    print(i, e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b52467d2581835848c2dbf18148040b652d016d"},"cell_type":"code","source":"print('Number of genres in films')\ngenres_count=train['genres'].apply(lambda x: len(x) if x != {} else 0).value_counts()\ngenres_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eabac54b04f8d87e92e9d41d6b942e61ec672a12"},"cell_type":"code","source":"# 类型栏包含电影所属类型的名称和ID。大多数电影有2-3个体裁，5-6个体裁是可能的。\n# 我想0和7是异常值。让我们提取体裁！我将创建一个包含电影中所有类型的专栏，并为每个类型分开专栏。\n#首先让我们来看看流派本身。","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0454f1cb2c8fce2a669391c449ed33e3914a1adc"},"cell_type":"code","source":"list_of_genres = list(train['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nlist_of_genres","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab4c1e152c194def7ed65a225cd671fb3abb60d7"},"cell_type":"code","source":"Counter([i for j in list_of_genres for i in j]).most_common()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc98ffc920dfbf6f4930afb8a3cf9a30aa740e89"},"cell_type":"code","source":"train['num_genres'] = train['genres'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_genres'] = train['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n\ntop_genres = [m[0] for m in Counter([i for j in list_of_genres for i in j]).most_common(15)]\nfor g in top_genres:\n    train['genre_' + g] = train['all_genres'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_genres'] = test['genres'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_genres'] = test['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_genres:\n    test['genre_' + g] = test['all_genres'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['genres'], axis=1)\ntest = test.drop(['genres'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02cb51ec41050e5993663eef3383e75d4b0fe556"},"cell_type":"code","source":"for i, e in enumerate(train['production_companies'][:5]):\n    print(i, e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d4a01c51538504c6eeeb6e34acce9e712390f52"},"cell_type":"code","source":"print('Number of production companies in films')\ntrain['production_companies'].apply(lambda x: len(x) if x != {} else 0).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d74dbb73c1cd5877648f8b6f2fa636919256aa5"},"cell_type":"code","source":"train[train['production_companies'].apply(lambda x: len(x) if x != {} else 0) > 11]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c724da4eeb953a22a2c9b9846a63411853ee2a0a"},"cell_type":"code","source":"# 现在我不知道如何处理这些数据。我只需要为前30部电影创建二进制专栏","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc1b38b244541e17544e60ae154523046a377c17"},"cell_type":"code","source":"list_of_companies = list(train['production_companies'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_companies for i in j]).most_common(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87dada6b4dccd2879a7720310728a9b514223c2d"},"cell_type":"code","source":"for i, e in enumerate(train['production_countries'][:5]):\n    print(i, e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08038ca0f0a532f94149c2292e63b4dddb45b3be"},"cell_type":"code","source":"print('Number of production countries in films')\ntrain['production_countries'].apply(lambda x: len(x) if x != {} else 0).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c386628b4d81af2f6294737283b659609b60c1e1"},"cell_type":"code","source":"list_of_countries = list(train['production_countries'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_countries for i in j]).most_common(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca3851c69ec9a07186681eb09657ac1a869fd52c"},"cell_type":"code","source":"train['num_countries'] = train['production_countries'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_countries'] = train['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_countries = [m[0] for m in Counter([i for j in list_of_countries for i in j]).most_common(25)]\nfor g in top_countries:\n    train['production_country_' + g] = train['all_countries'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_countries'] = test['production_countries'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_countries'] = test['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_countries:\n    test['production_country_' + g] = test['all_countries'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['production_countries', 'all_countries'], axis=1)\ntest = test.drop(['production_countries', 'all_countries'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"047a7032ed73e984f3acc25c870e0b81e232ce1c"},"cell_type":"code","source":"for i, e in enumerate(train['spoken_languages'][:5]):\n    print(i, e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb00bfa48814d75ab5355309a338c6df71b0aad1"},"cell_type":"code","source":"print('Number of spoken languages in films')\ntrain['spoken_languages'].apply(lambda x: len(x) if x != {} else 0).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ab99eedd2b2fe139eac52bc81020b3728bc1f01"},"cell_type":"code","source":"list_of_languages = list(train['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_languages for i in j]).most_common(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc81f52f8215ff28906b9ce077d80414c999a591"},"cell_type":"code","source":"train['num_languages'] = train['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_languages'] = train['spoken_languages'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_languages = [m[0] for m in Counter([i for j in list_of_languages for i in j]).most_common(30)]\nfor g in top_languages:\n    train['language_' + g] = train['all_languages'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_languages'] = test['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_languages'] = test['spoken_languages'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_languages:\n    test['language_' + g] = test['all_languages'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['spoken_languages', 'all_languages'], axis=1)\ntest = test.drop(['spoken_languages', 'all_languages'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff71a96d11659acea2faf7c8edf79bb0c1ab878a"},"cell_type":"code","source":"for i, e in enumerate(train['Keywords'][:5]):\n    print(i, e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7513a9f59582ca98a1a84fd0fe0fc48722a88e76"},"cell_type":"code","source":"print('Number of Keywords in films')\ntrain['Keywords'].apply(lambda x: len(x) if x != {} else 0).value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f35298adae31c0e50af23e50b16ee3e577a27518"},"cell_type":"code","source":"list_of_keywords = list(train['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ntrain['num_Keywords'] = train['Keywords'].apply(lambda x: len(x) if x != {} else 0)\ntrain['all_Keywords'] = train['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_keywords = [m[0] for m in Counter([i for j in list_of_keywords for i in j]).most_common(30)]\nfor g in top_keywords:\n    train['keyword_' + g] = train['all_Keywords'].apply(lambda x: 1 if g in x else 0)\n    \ntest['num_Keywords'] = test['Keywords'].apply(lambda x: len(x) if x != {} else 0)\ntest['all_Keywords'] = test['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nfor g in top_keywords:\n    test['keyword_' + g] = test['all_Keywords'].apply(lambda x: 1 if g in x else 0)\n\ntrain = train.drop(['Keywords', 'all_Keywords'], axis=1)\ntest = test.drop(['Keywords', 'all_Keywords'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f677bd93ab4e8ddf44be50a9a107aa0cb05c873e"},"cell_type":"code","source":"for i, e in enumerate(train['cast'][:1]):\n    print(i, e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"315289f005e94bd7642fc441f49c153d8bdcc9fc"},"cell_type":"code","source":"print('Number of casted persons in films')\ntrain['cast'].apply(lambda x: len(x) if x != {} else 0).value_counts().head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac22fdc1b9ceace56f55526b83f020be970b3cf9"},"cell_type":"code","source":"# 那些被抛弃的人对电影的质量有很大的影响。我们不仅有演员的名字，还有性别和角色的名字/类型。","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"639e1196fc3fc7c8cd3256ab969b44401be2e8c0"},"cell_type":"code","source":"list_of_cast_names = list(train['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_cast_names for i in j]).most_common(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"331bdfca78e50676c7461bdc4d3859034afb3d41"},"cell_type":"code","source":"list_of_cast_genders = list(train['cast'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_cast_genders for i in j]).most_common()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9fe9d9452f98c4f9cf2124cbd96c60b3f19cb3c"},"cell_type":"code","source":"list_of_cast_characters = list(train['cast'].apply(lambda x: [i['character'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_cast_characters for i in j]).most_common(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e87108bc754e3ea4aca3072bb66bf87dfa81b345"},"cell_type":"code","source":"train['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\ntop_cast_names = [m[0] for m in Counter([i for j in list_of_cast_names for i in j]).most_common(15)]\nfor g in top_cast_names:\n    train['cast_name_' + g] = train['cast'].apply(lambda x: 1 if g in str(x) else 0)\ntrain['genders_0_cast'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntrain['genders_1_cast'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntrain['genders_2_cast'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\ntop_cast_characters = [m[0] for m in Counter([i for j in list_of_cast_characters for i in j]).most_common(15)]\nfor g in top_cast_characters:\n    train['cast_character_' + g] = train['cast'].apply(lambda x: 1 if g in str(x) else 0)\n    \ntest['num_cast'] = test['cast'].apply(lambda x: len(x) if x != {} else 0)\nfor g in top_cast_names:\n    test['cast_name_' + g] = test['cast'].apply(lambda x: 1 if g in str(x) else 0)\ntest['genders_0_cast'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntest['genders_1_cast'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntest['genders_2_cast'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\nfor g in top_cast_characters:\n    test['cast_character_' + g] = test['cast'].apply(lambda x: 1 if g in str(x) else 0)\n\ntrain = train.drop(['cast'], axis=1)\ntest = test.drop(['cast'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac117c3c55ce3eddcd3f73b560f5935d0a663e4f"},"cell_type":"code","source":"for i, e in enumerate(train['crew'][:1]):\n    print(i, e[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e11b99df02c8b5c1717b90faa036a81299bb520"},"cell_type":"code","source":"print('Number of casted persons in films')\ntrain['crew'].apply(lambda x: len(x) if x != {} else 0).value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f492378d89b057334eaf2ba91eeacd977ad869b"},"cell_type":"code","source":"list_of_crew_names = list(train['crew'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_crew_names for i in j]).most_common(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b59e99014691ed69669a87e1351c1fd6a7eb6c4"},"cell_type":"code","source":"list_of_crew_jobs = list(train['crew'].apply(lambda x: [i['job'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_crew_jobs for i in j]).most_common(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e93b57d6e3d29ab8369a58f32b43fa28a7cc1425"},"cell_type":"code","source":"list_of_crew_genders = list(train['crew'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_crew_genders for i in j]).most_common(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1eb3c7a6f68f9b3df2564069f3e2b3be7e1593b"},"cell_type":"code","source":"list_of_crew_departments = list(train['crew'].apply(lambda x: [i['department'] for i in x] if x != {} else []).values)\nCounter([i for j in list_of_crew_departments for i in j]).most_common(14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e71e0afafcff1fce69d7d6b3bd1ba4c26c4c7da1"},"cell_type":"code","source":"train['num_crew'] = train['crew'].apply(lambda x: len(x) if x != {} else 0)\ntop_crew_names = [m[0] for m in Counter([i for j in list_of_crew_names for i in j]).most_common(15)]\nfor g in top_crew_names:\n    train['crew_name_' + g] = train['crew'].apply(lambda x: 1 if g in str(x) else 0)\ntrain['genders_0_crew'] = train['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntrain['genders_1_crew'] = train['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntrain['genders_2_crew'] = train['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\ntop_cast_characters = [m[0] for m in Counter([i for j in list_of_cast_characters for i in j]).most_common(15)]\nfor g in top_cast_characters:\n    train['crew_character_' + g] = train['crew'].apply(lambda x: 1 if g in str(x) else 0)\ntop_crew_jobs = [m[0] for m in Counter([i for j in list_of_crew_jobs for i in j]).most_common(15)]\nfor j in top_crew_jobs:\n    train['jobs_' + j] = train['crew'].apply(lambda x: sum([1 for i in x if i['job'] == j]))\ntop_crew_departments = [m[0] for m in Counter([i for j in list_of_crew_departments for i in j]).most_common(15)]\nfor j in top_crew_departments:\n    train['departments_' + j] = train['crew'].apply(lambda x: sum([1 for i in x if i['department'] == j])) \n    \ntest['num_crew'] = test['crew'].apply(lambda x: len(x) if x != {} else 0)\nfor g in top_crew_names:\n    test['crew_name_' + g] = test['crew'].apply(lambda x: 1 if g in str(x) else 0)\ntest['genders_0_crew'] = test['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntest['genders_1_crew'] = test['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntest['genders_2_crew'] = test['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\nfor g in top_cast_characters:\n    test['crew_character_' + g] = test['crew'].apply(lambda x: 1 if g in str(x) else 0)\nfor j in top_crew_jobs:\n    test['jobs_' + j] = test['crew'].apply(lambda x: sum([1 for i in x if i['job'] == j]))\nfor j in top_crew_departments:\n    test['departments_' + j] = test['crew'].apply(lambda x: sum([1 for i in x if i['department'] == j])) \n\ntrain = train.drop(['crew'], axis=1)\ntest = test.drop(['crew'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ceaed67c0f33b6008c382de5827269a822916e8"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af7a28bd793f84bee9764991bae2da0b2c87b740"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9a9ebea382739936e20d82f825c5bc9464e41a3"},"cell_type":"code","source":"#收入分配\nfig, ax = plt.subplots(figsize = (16, 6))\nplt.subplot(1, 2, 1)\nplt.hist(train['revenue']);\nplt.title('Distribution of revenue');\nplt.subplot(1, 2, 2)\nplt.hist(np.log1p(train['revenue']));\nplt.title('Distribution of log of revenue');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87d9981e3118a964ee13a4bbec37f2a2f3bc11ae"},"cell_type":"code","source":"train['log_revenue'] = np.log1p(train['revenue'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f38c2b7497d92120b9b63f3b84241534a46f14df"},"cell_type":"code","source":"#收入和预测\nplt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(train['budget'], train['revenue'])\nplt.title('Revenue vs budget');\nplt.subplot(1, 2, 2)\nplt.scatter(np.log1p(train['budget']), train['log_revenue'])\nplt.title('Log Revenue vs log budget');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2c4ae93c2c4d3ca473c004983a8b3923bf314f8"},"cell_type":"code","source":"#归一化\ntrain['log_budget'] = np.log1p(train['budget'])\ntest['log_budget'] = np.log1p(test['budget'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80cbb40a089d0841be40d38da386b0805ce497cc"},"cell_type":"code","source":"train['homepage'].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12414f3f63957048584edd18fb83ab46f49a0369"},"cell_type":"code","source":"#电影是否有无主页与收入的关系\ntrain['has_homepage'] = 0\ntrain.loc[train['homepage'].isnull() == False, 'has_homepage'] = 1\ntest['has_homepage'] = 0\ntest.loc[test['homepage'].isnull() == False, 'has_homepage'] = 1\n\nsns.catplot(x='has_homepage', y='revenue', data=train);\nplt.title('Revenue for film with and without homepage');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0783b68201877d3c89457ae0ae39d6e6e8beaa5"},"cell_type":"code","source":"#电影语言与收入的关系\nplt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nsns.boxplot(x='original_language', y='revenue', data=train.loc[train['original_language'].isin(train['original_language'].value_counts().head(10).index)]);\nplt.title('Mean revenue per language');\nplt.subplot(1, 2, 2)\nsns.boxplot(x='original_language', y='log_revenue', data=train.loc[train['original_language'].isin(train['original_language'].value_counts().head(10).index)]);\nplt.title('Mean log revenue per language');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9665887516987691c73873152d762e8bdb6f3092"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LinearRegression\nimport eli5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49b7b0b9f9a0243300598948fe19051b120ee980"},"cell_type":"code","source":"vectorizer = TfidfVectorizer(\n            sublinear_tf=True,\n            analyzer='word',\n            token_pattern=r'\\w{1,}',\n            ngram_range=(1, 2),\n            min_df=5)\n\noverview_text = vectorizer.fit_transform(train['overview'].fillna(''))\nlinreg = LinearRegression()\nlinreg.fit(overview_text, train['log_revenue'])\neli5.show_weights(linreg, vec=vectorizer, top=20, feature_filter=lambda x: x != '<BIAS>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2cb44c1a84c2901bbf123ebf11fe2dca8ead2e7"},"cell_type":"code","source":"print('Target value:', train['log_revenue'][1000])\neli5.show_prediction(linreg, doc=train['overview'].values[1000], vec=vectorizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3e96f46e7a9ab83575e62b9c542ea86274de4fd"},"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(train['popularity'], train['revenue'])\nplt.title('Revenue vs popularity');\nplt.subplot(1, 2, 2)\nplt.scatter(train['popularity'], train['log_revenue'])\nplt.title('Log Revenue vs popularity');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24fd72850216c256bc1458ea705d4c2ef3beda55"},"cell_type":"code","source":"test.loc[test['release_date'].isnull() == True, 'release_date'] = '01/01/98'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74800699f9960f495c516da79ad7bde3dfe34017"},"cell_type":"code","source":"def fix_date(x):\n    \"\"\"\n    Fixes dates which are in 20xx\n    \"\"\"\n    year = x.split('/')[2]\n    if int(year) <= 19:\n        return x[:-2] + '20' + year\n    else:\n        return x[:-2] + '19' + year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff42b5127bef4655a340e60d916c2adba2d2b32c"},"cell_type":"code","source":"train['release_date'] = train['release_date'].apply(lambda x: fix_date(x))\ntest['release_date'] = test['release_date'].apply(lambda x: fix_date(x))\ntrain['release_date'] = pd.to_datetime(train['release_date'])\ntest['release_date'] = pd.to_datetime(test['release_date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"537ed14a8ed291ef0cda51f6e5491b87768ee84e"},"cell_type":"code","source":"# creating features based on dates\ndef process_date(df):\n    date_parts = [\"year\", \"weekday\", \"month\", 'weekofyear', 'day', 'quarter']\n    for part in date_parts:\n        part_col = 'release_date' + \"_\" + part\n        df[part_col] = getattr(df['release_date'].dt, part).astype(int)\n    \n    return df\n\ntrain = process_date(train)\ntest = process_date(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91fd9afd26ca7368461bd42971b2c01a558472f6"},"cell_type":"code","source":"import plotly.graph_objs as go\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.tools as tls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3399946bca762dd292d1cddd971a63cd6b78d6d3"},"cell_type":"code","source":"#每年的电影数量\nd1 = train['release_date_year'].value_counts().sort_index()\nd2 = test['release_date_year'].value_counts().sort_index()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='train'), go.Scatter(x=d2.index, y=d2.values, name='test')]\nlayout = go.Layout(dict(title = \"Number of films per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf0e72419f35493e902c206771723e12737b77fd"},"cell_type":"code","source":"#每年的电影数量和总收入\nd1 = train['release_date_year'].value_counts().sort_index()\nd2 = train.groupby(['release_date_year'])['revenue'].sum()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='film count'), go.Scatter(x=d2.index, y=d2.values, name='total revenue', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Number of films and total revenue per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  yaxis2=dict(title='Total revenue', overlaying='y', side='right')\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d3de2944f723f6b076ee2b9016566780589f14a"},"cell_type":"code","source":"#每年电影数量和平均收入\nd1 = train['release_date_year'].value_counts().sort_index()\nd2 = train.groupby(['release_date_year'])['revenue'].mean()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='film count'), go.Scatter(x=d2.index, y=d2.values, name='mean revenue', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Number of films and average revenue per year\",\n                  xaxis = dict(title = 'Year'),\n                  yaxis = dict(title = 'Count'),\n                  yaxis2=dict(title='Average revenue', overlaying='y', side='right')\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae25fe2b7f4f34ed4152ba24321d178cd3ab4912"},"cell_type":"code","source":"# 我们可以看到电影的数量和总收入都在增长，\n# 这是可以预料的。但在过去的几年里，成功的电影数量很多，带来了高额的收入。","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c2d4b5659a3903b26ea3b45ad90a5725672c6a7"},"cell_type":"code","source":"sns.catplot(x='release_date_weekday', y='revenue', data=train);\nplt.title('Revenue on different days of week of release');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0080f2e5b7dfc732aa6992f75f75e2f9e32f40e"},"cell_type":"code","source":"plt.figure(figsize=(20, 6))\nplt.subplot(1, 3, 1)\nplt.hist(train['runtime'].fillna(0) / 60, bins=40);\nplt.title('Distribution of length of film in hours');\nplt.subplot(1, 3, 2)\nplt.scatter(train['runtime'].fillna(0), train['revenue'])\nplt.title('runtime vs revenue');\nplt.subplot(1, 3, 3)\nplt.scatter(train['runtime'].fillna(0), train['popularity'])\nplt.title('runtime vs popularity');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5f979f3596db197696b8a67477e1510c0ba296d"},"cell_type":"code","source":"# 看来大多数电影的片长都是1.5-2小时，收入最高的电影也在这个范围内","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fc06963075967242fd81298d49469d3a9ba6e2a"},"cell_type":"code","source":"train['status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6cc5869c8f94e82bd50ad06cdcc60858ce700c8"},"cell_type":"code","source":"test['status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0efeba98df354760cc09948d142bede41df713e8"},"cell_type":"code","source":"sns.catplot(x='num_genres', y='revenue', data=train);\nplt.title('Revenue for different number of genres in the film');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0019b5c591dd0cdba3d0988d49fbaf45e6e01513"},"cell_type":"code","source":"sns.violinplot(x='genre_Drama', y='revenue', data=train[:100]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a9dd39c3c616cd66557029acdc265970534025b"},"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.scatter(train['num_crew'], train['revenue'])\nplt.title('Number of crew members vs revenue');\nplt.subplot(1, 2, 2)\nplt.scatter(train['num_crew'], train['log_revenue'])\nplt.title('Log Revenue vs number of crew members');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb96f79e6c743519fc79d7c5f5b5465551c8c240"},"cell_type":"code","source":"f, axes = plt.subplots(3, 5, figsize=(24, 18))\nplt.suptitle('Violinplot of revenue vs jobs')\nfor i, e in enumerate([col for col in train.columns if 'jobs_' in col]):\n    sns.violinplot(x=e, y='revenue', data=train, ax=axes[i // 5][i % 5]);","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"train = train.drop(['homepage', 'imdb_id', 'poster_path', 'release_date', 'status', 'log_revenue'], axis=1)\ntest = test.drop(['homepage', 'imdb_id', 'poster_path', 'release_date', 'status'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for col in train.columns:\n#     if train[col].nunique() == 1:\n#         print(col)\n#         train = train.drop([col], axis=1)\n#         test = test.drop([col], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['original_language', 'collection_name', 'all_genres']:\n    le = LabelEncoder()\n    le.fit(list(train[col].fillna('')) + list(test[col].fillna('')))\n    train[col] = le.transform(train[col].fillna('').astype(str))\n    test[col] = le.transform(test[col].fillna('').astype(str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_texts = train[['title', 'tagline', 'overview', 'original_title']]\ntest_texts = test[['title', 'tagline', 'overview', 'original_title']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['title', 'tagline', 'overview', 'original_title']:\n    train['len_' + col] = train[col].fillna('').apply(lambda x: len(str(x)))\n    train['words_' + col] = train[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n    train = train.drop(col, axis=1)\n    test['len_' + col] = test[col].fillna('').apply(lambda x: len(str(x)))\n    test['words_' + col] = test[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n    test = test.drop(col, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train['id'] == 16,'revenue'] = 192864          # Skinning\ntrain.loc[train['id'] == 90,'budget'] = 30000000         # Sommersby          \ntrain.loc[train['id'] == 118,'budget'] = 60000000        # Wild Hogs\ntrain.loc[train['id'] == 149,'budget'] = 18000000        # Beethoven\ntrain.loc[train['id'] == 313,'revenue'] = 12000000       # The Cookout \ntrain.loc[train['id'] == 451,'revenue'] = 12000000       # Chasing Liberty\ntrain.loc[train['id'] == 464,'budget'] = 20000000        # Parenthood\ntrain.loc[train['id'] == 470,'budget'] = 13000000        # The Karate Kid, Part II\ntrain.loc[train['id'] == 513,'budget'] = 930000          # From Prada to Nada\ntrain.loc[train['id'] == 797,'budget'] = 8000000         # Welcome to Dongmakgol\ntrain.loc[train['id'] == 819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip\ntrain.loc[train['id'] == 850,'budget'] = 90000000        # Modern Times\ntrain.loc[train['id'] == 1112,'budget'] = 7500000        # An Officer and a Gentleman\ntrain.loc[train['id'] == 1131,'budget'] = 4300000        # Smokey and the Bandit   \ntrain.loc[train['id'] == 1359,'budget'] = 10000000       # Stir Crazy \ntrain.loc[train['id'] == 1542,'budget'] = 1              # All at Once\ntrain.loc[train['id'] == 1570,'budget'] = 15800000       # Crocodile Dundee II\ntrain.loc[train['id'] == 1571,'budget'] = 4000000        # Lady and the Tramp\ntrain.loc[train['id'] == 1714,'budget'] = 46000000       # The Recruit\ntrain.loc[train['id'] == 1721,'budget'] = 17500000       # Cocoon\ntrain.loc[train['id'] == 1865,'revenue'] = 25000000      # Scooby-Doo 2: Monsters Unleashed\ntrain.loc[train['id'] == 2268,'budget'] = 17500000       # Madea Goes to Jail budget\ntrain.loc[train['id'] == 2491,'revenue'] = 6800000       # Never Talk to Strangers\ntrain.loc[train['id'] == 2602,'budget'] = 31000000       # Mr. Holland's Opus\ntrain.loc[train['id'] == 2612,'budget'] = 15000000       # Field of Dreams\ntrain.loc[train['id'] == 2696,'budget'] = 10000000       # Nurse 3-D\ntrain.loc[train['id'] == 2801,'budget'] = 10000000       # Fracture\ntest.loc[test['id'] == 3889,'budget'] = 15000000       # Colossal\ntest.loc[test['id'] == 6733,'budget'] = 5000000        # The Big Sick\ntest.loc[test['id'] == 3197,'budget'] = 8000000        # High-Rise\ntest.loc[test['id'] == 6683,'budget'] = 50000000       # The Pink Panther 2\ntest.loc[test['id'] == 5704,'budget'] = 4300000        # French Connection II\ntest.loc[test['id'] == 6109,'budget'] = 281756         # Dogtooth\ntest.loc[test['id'] == 7242,'budget'] = 10000000       # Addams Family Values\ntest.loc[test['id'] == 7021,'budget'] = 17540562       #  Two Is a Family\ntest.loc[test['id'] == 5591,'budget'] = 4000000        # The Orphanage\ntest.loc[test['id'] == 4282,'budget'] = 20000000       # Big Top Pee-wee\n\npower_six = train.id[train.budget > 1000][train.revenue < 100]\n\nfor k in power_six :\n    train.loc[train['id'] == k,'revenue'] =  train.loc[train['id'] == k,'revenue'] * 1000000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['id', 'revenue','production_companies'], axis=1)\ny = np.log1p(train['revenue'])\nX_test = test.drop(['id','production_companies'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\n# print(X_train.dtypes.value_counts())\n# print(X_valid)\n# print(y_train)\n# print(y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n# specify your configurations as a dict\nparams = {'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\n# train\nprint('Start training...')\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=10000,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=100)\n\nprint('Start predicting...')\n\npreds = gbm.predict(X_valid, num_iteration=gbm.best_iteration)  # 输出的是概率结果\n\n# 导出结果\nfor pred in preds:\n    result = prediction = int(np.argmax(pred))\n\n# 导出特征重要性\n# importance = gbm.feature_importance()\n# names = gbm.feature_name()\n# with open('./feature_importance.txt', 'w+') as file:\n#     for index, im in enumerate(importance):\n#         string = names[index] + ', ' + str(im) + '\\n'\n#         print(string)\n#         file.write(string)\nmodel1 = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\nmodel1.fit(X_train, y_train, \n        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n        verbose=1000, early_stopping_rounds=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(model1, feature_filter=lambda x: x != '<BIAS>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n\n    oof = np.zeros(X.shape[0])\n    prediction = np.zeros(X_test.shape[0])\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        if model_type == 'sklearn':\n            X_train, X_valid = X[train_index], X[valid_index]\n        else:\n            X_train, X_valid = X.values[train_index], X.values[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n                    verbose=1000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test.values), ntree_limit=model.best_ntree_limit)\n\n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = mean_squared_error(y_valid, y_pred_valid)\n            \n            y_pred = model.predict(X_test)\n            \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric='RMSE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n        \n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] /= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction\n    \n    else:\n        return oof, prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 30,\n         'min_data_in_leaf': 10,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\noof_lgb, prediction_lgb, _ = train_model(X, X_test, y, params=params, model_type='lgb', plot_feature_importance=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train_texts.columns:\n    vectorizer = TfidfVectorizer(\n                sublinear_tf=True,\n                analyzer='word',\n                token_pattern=r'\\w{1,}',\n                ngram_range=(1, 2),\n                min_df=10\n    )\n    vectorizer.fit(list(train_texts[col].fillna('')) + list(test_texts[col].fillna('')))\n    train_col_text = vectorizer.transform(train_texts[col].fillna(''))\n    test_col_text = vectorizer.transform(test_texts[col].fillna(''))\n    model = linear_model.RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0), scoring='neg_mean_squared_error', cv=folds)\n    oof_text, prediction_text = train_model(train_col_text, test_col_text, y, params=None, model_type='sklearn', model=model)\n    \n    X[col + '_oof'] = oof_text\n    X_test[col + '_oof'] = prediction_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def new_features(df):\n    df['budget_to_popularity'] = df['budget'] / df['popularity']\n    df['budget_to_runtime'] = df['budget'] / df['runtime']\n    \n    # some features from https://www.kaggle.com/somang1418/happy-valentines-day-and-keep-kaggling-3\n    df['_budget_year_ratio'] = df['budget'] / (df['release_date_year'] * df['release_date_year'])\n    df['_releaseYear_popularity_ratio'] = df['release_date_year'] / df['popularity']\n    df['_releaseYear_popularity_ratio2'] = df['popularity'] / df['release_date_year']\n    \n    df['runtime_to_mean_year'] = df['runtime'] / df.groupby(\"release_date_year\")[\"runtime\"].transform('mean')\n    df['popularity_to_mean_year'] = df['popularity'] / df.groupby(\"release_date_year\")[\"popularity\"].transform('mean')\n    df['budget_to_mean_year'] = df['budget'] / df.groupby(\"release_date_year\")[\"budget\"].transform('mean')\n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = new_features(X)\nX_test = new_features(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_lgb, prediction_lgb, _ = train_model(X, X_test, y, params=params, model_type='lgb', plot_feature_importance=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_params = {'eta': 0.01,\n              'objective': 'reg:linear',\n              'max_depth': 7,\n              'subsample': 0.8,\n              'colsample_bytree': 0.8,\n              'eval_metric': 'rmse',\n              'seed': 11,\n              'silent': True}\noof_xgb, prediction_xgb= train_model(X, X_test, y, params=xgb_params, model_type='xgb', plot_feature_importance=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_params = {'learning_rate': 0.002,\n              'depth': 5,\n              'l2_leaf_reg': 10,\n              # 'bootstrap_type': 'Bernoulli',\n              'colsample_bylevel': 0.8,\n              'bagging_temperature': 0.2,\n              #'metric_period': 500,\n              'od_type': 'Iter',\n              'od_wait': 100,\n              'random_seed': 11,\n              'allow_writing_files': False}\noof_cat, prediction_cat = train_model(X, X_test, y, params=cat_params, model_type='cat')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_stack = np.vstack([oof_lgb, oof_xgb, oof_cat]).transpose()\ntrain_stack = pd.DataFrame(train_stack, columns=['lgb', 'xgb', 'cat'])\ntest_stack = np.vstack([prediction_lgb, prediction_xgb, prediction_cat]).transpose()\ntest_stack = pd.DataFrame(test_stack, columns=['lgb', 'xgb', 'cat'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arams = {'num_leaves': 8,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 2,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\noof_lgb_stack, prediction_lgb_stack, _ = train_model(train_stack, test_stack, y, params=params, model_type='lgb', plot_feature_importance=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = linear_model.RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0), scoring='neg_mean_squared_error', cv=folds)\noof_rcv_stack, prediction_rcv_stack = train_model(train_stack.values, test_stack.values, y, params=None, model_type='sklearn', model=model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['revenue'] = np.expm1(prediction_lgb)\nsub.to_csv(\"lgb.csv\", index=False)\nsub['revenue'] = np.expm1((prediction_lgb + prediction_xgb) / 2)\nsub.to_csv(\"blend.csv\", index=False)\nsub['revenue'] = np.expm1((prediction_lgb + prediction_xgb + prediction_cat) / 3)\nsub.to_csv(\"blend1.csv\", index=False)\nsub['revenue'] = prediction_lgb_stack\nsub.to_csv(\"stack_lgb.csv\", index=False)\nsub['revenue'] = prediction_rcv_stack\nsub.to_csv(\"stack_rcv.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}