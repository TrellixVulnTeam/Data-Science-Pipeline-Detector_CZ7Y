{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b253d3ce-da4e-ee50-39ba-702180856bf7"},"source":"Welcome everyone to my post that will describe my experiments to get good scores for this problem. My aim will be to transfer my knowledge and make it easy for others to follow along. \n\n**Talking about easy, we will in fact be building and training our neural networks without doing programming. Instead we will use drag and drop GUI based platform (Deep Learning Studio) to build and train neural network.** We will try different experiments as we move forward with this competition.\n\nI will try to documents as much details as I can on this notebook. Please feel free to send your suggestions and comments. \n\nToday we will try 3D Convolutional Neural Network for this problem.\n\nFull discloure: I am one of the cofounder of the company who developed Deep Learning Studio software. Deep Learning Studio has a free monthly plan and it offers 2 hours of complementary training time on best GPU available in the Cloud (Nvidia K80 with 12GB RAM)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b04419ac-79f4-af60-fd5a-4bc5d1cdafcc"},"source":"1. Pre-processing\n==============\nWe will do following preprocessing on given CT Scans to make our life easier.\n\nCode for these steps is mostly borrowed from excellent notebook of Guido Zuidhof. Please refer to that Guido Zuidhof notebook to understand these steps in detail. Here I will just list major high level preprocessing that we will do on the dataset.\n\n1. **Load and Convert DICOM file to NUMPY array.**\n2. **Do Lung Segmentation on these scans.**\n3. **Pad or Trim slices at the end such that every scan has exactly 256 slices.**\n4. **Threshold values to below -1100 to -1100 and values above 700 to 700**\n5. **Divide all values with 1100 to bring the range between -1 and 1**\n\nYou can find full source code for pre-processing in section 3. Following experiment uses this preprocessed data as input."},{"cell_type":"markdown","metadata":{"_cell_guid":"1b3ba0b6-1fdb-7c9a-d59b-0298c885c579"},"source":"2. First Experiment: 3D Convolutional Neural Networks\n=========================================\nConvolutional neural networks have been very successful in image classification and other types of imaging tasks. Traditionally convolution neural network operate on a 2D image possibly comprising of 1 or 3 color channels. Convolutional networks learns to extract low level features of image automatically. This ability comes in handy when tackling with complex real world images.\n\nYou can watch following video to get gentle introduction to convolutional neural network. https://www.youtube.com/watch?v=JiN9p5vWHDY&ab_channel=DeepLearning.TV\n\nOur CT scan dataset is actually comprise of set of slices (each slice is 512x512 pixel image). We have information if the CT scan contain the cancer or not as a whole. Which means that we must process all slices together and then let network correct itself in the end.\n\n3D convolutional neural network fit the bill but they tend to consume a lots of GPU memory and are difficult to converge. But let's make a network and give it a shot.\n\n\n\nStep-1: Get Access\n---------------------\nSign up and get access to Deep Learning Studio at \n\n[http://deepcognition.ai/][1] \n\n\nStep-2: Enable Cached Dataset\n----------------------------------------\nEnable cached dataset in your account by uploading two small files that you must download from your Kaggle account. These files must be uploaded for to verify that user is infact has access to Kaggle dataset (Follow markers 1 to 4)\n\n ![Enable Access to Cached Dataset][2]\n\n\nStep-3: Create and Open a New Project\n---------------------------------------------------\nLet's build a new project by going to project menu on left and clicking on + button.\n\n![enter image description here][3]\n\nGive a name and description to your project.\nNow open the project by clicking on box+arrow icon on project bar.\n\n![Open Project][4]\n\n\n\nStep-4: Select Dataset and do training/validation set division\n---------------------------------------------------------------------------\nWe will do training with 1200 samples and we will use 197 samples for validation for this example.\n\n![Training and Validation Split][5]\n\n\n\nStep-5: Build model\n-------------------------------\nOnce dataset is selected click on \"Model\" Tab and start building model as shown below by dragging layers from left menu bar to the canvas and connecting these layer blocks. \n\n![Architecture][6]\n\nYou will also need to set the parameters of the layers. Below is the actual generated source code (using view code <> button in Model tab)  for the model that I built and you can reference it to get parameter values.\n\n  \n\n\n    def get_model():\n    \tInput_1 = Input(shape=(256, 512, 512, 1))\n    \tMaxPooling3D_27 = MaxPooling3D(pool_size= (1,3,3))(Input_1)\n    \tConvolution3D_1 = Convolution3D(kernel_dim1= 4,nb_filter= 10,activation= 'relu' ,kernel_dim3= 4,kernel_dim2= 4)(MaxPooling3D_27)\n    \tConvolution3D_7 = Convolution3D(kernel_dim1= 4,nb_filter= 10,activation= 'relu' ,kernel_dim3= 4,kernel_dim2= 4)(Convolution3D_1)\n    \tBatchNormalization_28 = BatchNormalization()(Convolution3D_7)\n    \tMaxPooling3D_12 = MaxPooling3D(pool_size= (2,2,2))(BatchNormalization_28)\n    \tSpatialDropout3D_1 = SpatialDropout3D(p= 0.5)(MaxPooling3D_12)\n    \tConvolution3D_9 = Convolution3D(kernel_dim1= 2,nb_filter= 20,activation= 'relu' ,kernel_dim3= 2,kernel_dim2= 2)(SpatialDropout3D_1)\n    \tConvolution3D_11 = Convolution3D(kernel_dim1= 2,nb_filter= 20,activation= 'relu' ,kernel_dim3= 2,kernel_dim2= 2)(Convolution3D_9)\n    \tBatchNormalization_9 = BatchNormalization()(Convolution3D_11)\n    \tMaxPooling3D_14 = MaxPooling3D(pool_size= (2,2,2))(BatchNormalization_9)\n    \tSpatialDropout3D_4 = SpatialDropout3D(p= 0.5)(MaxPooling3D_14)\n    \tConvolution3D_12 = Convolution3D(kernel_dim1= 2,nb_filter= 40,activation= 'relu' ,kernel_dim3= 2,kernel_dim2= 2)(SpatialDropout3D_4)\n    \tConvolution3D_13 = Convolution3D(kernel_dim1= 2,nb_filter= 40,activation= 'relu' ,kernel_dim3= 2,kernel_dim2= 2)(Convolution3D_12)\n    \tMaxPooling3D_23 = MaxPooling3D(pool_size= (2,2,2))(Convolution3D_13)\n    \tBatchNormalization_23 = BatchNormalization()(MaxPooling3D_23)\n    \tSpatialDropout3D_5 = SpatialDropout3D(p= 0.5)(BatchNormalization_23)\n    \tGlobalMaxPooling3D_1 = GlobalMaxPooling3D()(SpatialDropout3D_5)\n    \tDense_1 = Dense(activation= 'relu' ,output_dim= 10)(GlobalMaxPooling3D_1)\n    \tDropout_14 = Dropout(p= 0.3)(Dense_1)\n    \tDense_6 = Dense(activation= 'relu' ,output_dim= 10)(Dropout_14)\n    \tDense_2 = Dense(activation= 'softmax' ,output_dim= 2)(Dense_6)\n    \n    \treturn Model([Input_1],[Dense_2])\n\n**Rationale for this architecture**\n\nFirst MaxPooling3D layer is done to reduce size of the scan (kind of downscaling) because even the GPUs like K80 with 12GB RAM are not able to fit this scan with reasonable model in memory.\n\nOur architecture is based on stacking multiple blocks of following:\nConv3D->Conv3D->BatchNorm-> MaxPooling3D-> SpatialDropout3D\n\nPurpose of first two Conv3D layers is to extract features from input. BatchNormalization layer is added to accelerate the training. (see https://arxiv.org/abs/1502.03167 ). MaxPool is added to reduce spacial dimensions for future blocks. SpacialDropout3D is added added to make system more robust and less prone to over-fitting.\n\nAt the end of convolutional network we do Global max pooling to pool the features which then go into three dense layers to bring the final dimension to 2 which is the size of our output/label (cancer or no cancer).\n\nNote that by no mean this is the best architecture but I wanted to share my experiment with you guys in the hope it can help you build even better network. Your suggestions are welcome.\n\n\n\nStep-6: Training and Results\n-------------------------------------------\nNow you can go to \"Hyperparameters\" tab and make sure batch size is set to 1. This is important because anything bigger will not fit GPUs memory and training will fail.\n\n![Hyperparameters][7]\n\nFinally you can move to \"Training\" tab. Select GPU-K80 as instance and click on \"Start Instance\". Once Instance has been started. Click on \"Start Training\". Note that training is going to be very slow because of sheer size of dataset and computations needed. \n\nAfter trying out 2 epochs I was able to get loss of about 0.58 on validation set.\n\n![Training Dashboard][8]\n\n\n  [1]: http://deepcognition.ai/\n  [2]: https://s3-us-west-2.amazonaws.com/deepcognition/3dconvnet_cached_dataset.jpg\n  [3]: https://s3-us-west-2.amazonaws.com/deepcognition/3dconvnet_new_project.jpg\n  [4]: https://s3-us-west-2.amazonaws.com/deepcognition/3dconvnet_open_project.jpg\n  [5]: https://s3-us-west-2.amazonaws.com/deepcognition/3dconvnet_dataset_selection.jpg\n  [6]: https://s3-us-west-2.amazonaws.com/deepcognition/3dconvnet_model.jpg\n  [7]: https://s3-us-west-2.amazonaws.com/deepcognition/3dconvnet_hyperparameters.jpg\n  [8]: https://s3-us-west-2.amazonaws.com/deepcognition/3dconvnet_training_dashboard.jpg"},{"cell_type":"markdown","metadata":{"_cell_guid":"d750c71b-c22a-e396-4ead-68c9959d785d"},"source":"3. Pre-processing Code\n==================\n\nThis notebook converts DICOM scans to Numpy array along with doing segmentation, normalization etc.\n\n1. It allows to use multi-CPU to do segmentation.\n2. slow_slice() function is designed to show scan at full resolution. Basic imshow only shows scaled version of scan.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"75a2e650-5606-a160-bf29-20178f425942"},"outputs":[],"source":"# Please check excellent notebook of Guido Zuidhof for full explanation of this code\n%matplotlib inline\nimport sys\nimport numpy as np\nfrom numpy import *\nfrom scipy import stats\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nfrom sklearn.metrics import *\nimport glob\nfrom sklearn.model_selection import train_test_split\nimport datetime\nimport math\nimport os.path\nfrom importlib import reload\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\nfrom multiprocessing import Pool\nimport time\nfrom skimage import measure, morphology, segmentation\nimport scipy.ndimage as ndimage\nimport dicom\n\nimport keras\nfrom keras.layers.core import *\nfrom keras.layers.normalization import *\nfrom keras.layers.pooling import *\nfrom keras.layers import Input\nfrom keras.layers.convolutional import *\nfrom keras.regularizers import *\nfrom keras.optimizers import *\nfrom keras.callbacks import *\nfrom keras.models import Model, Sequential\nfrom keras.models import load_model\nimport tensorflow as tf\n\n#INPUT_SCAN_FOLDER = '/data/kaggle_cancer_2017/stage1/'\n#OUTPUT_FOLDER = '/data/kaggle_preprocessed_output/'\n\n# For Kaggle I have added  sample_image directory only\nINPUT_SCAN_FOLDER = '../input/sample_images/'\nOUTPUT_FOLDER = None\n\nTHRESHOLD_HIGH = 700\nTHRESHOLD_LOW = -1100\n\n# fix random seed for reproducibility\nnp.random.seed(17)\n\n\n# Simple Function to show the slice at full resolustion normal imshow would downscale this image.\n# It can accept either (image_width, image_height) array or (image_width, image_height, 1) numpy as input.\n# Optional Value range is a tuple of fixed max value and min value. This is useful if you do not want color \n#  to change between different scan slices.\n\ndef show_slice(arr, value_range = None):\n    if len (list(arr.shape)) > 2:\n        arr2 = arr.copy()\n        arr2 = np.reshape (arr, (arr.shape[0],arr.shape[1]))\n    else:\n        arr2 = arr\n\n    dpi = 80\n    margin = 0.05 # (5% of the width/height of the figure...)\n    xpixels, ypixels = arr2.shape[0], arr2.shape[1]\n\n    # Make a figure big enough to accomodate an axis of xpixels by ypixels\n    # as well as the ticklabels, etc...\n    figsize = (1 + margin) * ypixels / dpi, (1 + margin) * xpixels / dpi\n\n    fig = plt.figure(figsize=figsize, dpi=dpi)\n    # Make the axis the right size...\n    ax = fig.add_axes([margin, margin, 1 - 2*margin, 1 - 2*margin])\n\n    if value_range is None:\n        plt.imshow(arr2, cmap=plt.cm.gray)\n    else:        \n        ax.imshow(arr2, vmin=value_range[0], vmax=1, cmap=plt.cm.gray, interpolation='none')\n    plt.show()\n\ndef preprocess_all_scans_mp (in_folder, out_folder, demo=False):\n\n    dicom_folder_list = [ name for name in os.listdir(in_folder) if os.path.isdir(os.path.join(in_folder, name)) ]\n   \n    # For Testing feed just load one scan\n    segment_pad_and_save_ct_scan_as_npz  (dicom_folder_list[0], demo=True)\n    \n    if not demo:\n        # Multi-threaded processes to utilize all available CPUs for this task. Note that many threads will block on IO\n        # so creating more than number of CPUs.    \n        thread_pool = Pool(32)\n        thread_pool.map (segment_pad_and_save_ct_scan_as_npz, dicom_folder_list)\n        \n        # Cleanup\n        thread_pool.close()\n        thread_pool.join_thread()\n        \ndef segment_pad_and_save_ct_scan_as_npz (scanid, demo=False):\n    \n    scan_dir = INPUT_SCAN_FOLDER + str(scanid)\n    \n    scan = load_scan_as_HU_nparray(scan_dir)\n    \n    # For demo reduce number of slices to 5 to save time\n    if demo:\n        scan = scan[78:82]\n    \n    if demo:\n        print (\"----Loaded Scan and Converted to HU units----\")\n        print (\"Shape: \", scan.shape)\n        show_slice (scan[3])\n    \n    scan = seperate_lungs_and_pad (scan)\n    \n    if demo:\n        print (\"----Segmented Lung and Padded/Trimmed to have 256 slices----\")\n        print (\"Shape: \", scan.shape)\n        show_slice (scan[3])\n        \n    scan = threshold_and_normalize_scan (scan)\n    \n    if demo:\n        print (\"----Thresholded and Normalized----\")\n        print (\"Shape: \", scan.shape)\n        show_slice (scan[3]) \n    \n    # For Convnet we will need one extra dimension representing color channel\n    scan = scan.reshape((256,512,512,1))\n    \n    if demo:\n        print (\"----Expanded dimensions for color channel representation ----\")\n        print (\"Shape: \", scan.shape)\n        show_slice (scan[3], value_range=(-1,1))         \n    \n    # Save output file to compressed npz file for easy reading.\n    if not demo:\n        out_file = OUTPUT_FOLDER + 'stage1/' + scanid + '.npz'    \n        np.savez_compressed (out_file, scan)\n    \n# Load the scans in given folder path\ndef load_scan_as_HU_nparray(path):\n    slices = [dicom.read_file(path + '/' + s) for s in os.listdir(path)]\n    slices.sort(key = lambda x: int(x.ImagePositionPatient[2]))\n    try:\n        slice_thickness = np.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2])\n    except:\n        slice_thickness = np.abs(slices[0].SliceLocation - slices[1].SliceLocation)\n        \n    for s in slices:\n        s.SliceThickness = slice_thickness\n    \n    image = np.stack([s.pixel_array for s in slices])\n    \n    # Convert to int16 (from sometimes int16), \n    # should be possible as values should always be low enough (<32k)\n    image = image.astype(np.int16)\n\n    # Set outside-of-scan pixels to 0\n    # The intercept is usually -1024, so air is approximately 0\n    image[image == -2000] = 0\n    \n    # Convert to Hounsfield units (HU)\n    for slice_number in range(len(slices)):\n        \n        intercept = slices[slice_number].RescaleIntercept\n        slope = slices[slice_number].RescaleSlope\n        \n        if slope != 1:\n            image[slice_number] = slope * image[slice_number].astype(np.float64)\n            image[slice_number] = image[slice_number].astype(np.int16)\n            \n        image[slice_number] += np.int16(intercept)\n    \n    return np.array(image, dtype=np.int16)        \n\n\ndef seperate_lungs_and_pad(scan):\n    \n    # make total 256 slices fill in -1100 as exterme value \n    segmented_scan = np.full ((256, 512, 512), THRESHOLD_LOW)\n    \n    for i, image in enumerate (scan):\n        \n        # Ignore all slices later than 255 if required.\n        if (i == 256):\n            break\n        \n        # Creation of the internal Marker\n        marker_internal = image < -400\n        marker_internal = segmentation.clear_border(marker_internal)\n        marker_internal_labels = measure.label(marker_internal)\n        areas = [r.area for r in measure.regionprops(marker_internal_labels)]\n        areas.sort()\n        if len(areas) > 2:\n            for region in measure.regionprops(marker_internal_labels):\n                if region.area < areas[-2]:\n                    for coordinates in region.coords:                \n                           marker_internal_labels[coordinates[0], coordinates[1]] = 0\n        marker_internal = marker_internal_labels > 0\n        #Creation of the external Marker\n        external_a = ndimage.binary_dilation(marker_internal, iterations=10)\n        external_b = ndimage.binary_dilation(marker_internal, iterations=55)\n        marker_external = external_b ^ external_a\n        #Creation of the Watershed Marker matrix\n        marker_watershed = np.zeros((512, 512), dtype=np.int)\n        marker_watershed += marker_internal * 255\n        marker_watershed += marker_external * 128\n\n        #Creation of the Sobel-Gradient\n        sobel_filtered_dx = ndimage.sobel(image, 1)\n        sobel_filtered_dy = ndimage.sobel(image, 0)\n        sobel_gradient = np.hypot(sobel_filtered_dx, sobel_filtered_dy)\n        sobel_gradient *= 255.0 / np.max(sobel_gradient)\n\n        #Watershed algorithm\n        watershed = morphology.watershed(sobel_gradient, marker_watershed)\n\n        #Reducing the image created by the Watershed algorithm to its outline\n        outline = ndimage.morphological_gradient(watershed, size=(3,3))\n        outline = outline.astype(bool)\n\n        #Performing Black-Tophat Morphology for reinclusion\n        #Creation of the disk-kernel and increasing its size a bit\n        blackhat_struct = [[0, 0, 1, 1, 1, 0, 0],\n                           [0, 1, 1, 1, 1, 1, 0],\n                           [1, 1, 1, 1, 1, 1, 1],\n                           [1, 1, 1, 1, 1, 1, 1],\n                           [1, 1, 1, 1, 1, 1, 1],\n                           [0, 1, 1, 1, 1, 1, 0],\n                           [0, 0, 1, 1, 1, 0, 0]]\n        blackhat_struct = ndimage.iterate_structure(blackhat_struct, 8)\n        #Perform the Black-Hat\n        outline += ndimage.black_tophat(outline, structure=blackhat_struct)\n\n        #Use the internal marker and the Outline that was just created to generate the lungfilter\n        lungfilter = np.bitwise_or(marker_internal, outline)\n        #Close holes in the lungfilter\n        #fill_holes is not used here, since in some slices the heart would be reincluded by accident\n        lungfilter = ndimage.morphology.binary_closing(lungfilter, structure=np.ones((5,5)), iterations=3)\n\n        #Apply the lungfilter (note the filtered areas being assigned 30 HU)\n        segmented_scan[i] = np.where(lungfilter == 1, image, 30*np.ones((512, 512)))\n        \n    return segmented_scan\n\ndef threshold_and_normalize_scan (scan):\n    scan = scan.astype(np.float32)\n    scan [scan < THRESHOLD_LOW] = THRESHOLD_LOW\n    scan [scan > THRESHOLD_HIGH] = THRESHOLD_HIGH\n    \n    # Maximum absolute value of any pixel .\n    max_abs = abs (max(THRESHOLD_LOW, THRESHOLD_HIGH, key=abs))\n    \n    # This will bring values between -1 and 1\n    scan /= max_abs\n    \n    return scan\n\nif OUTPUT_FOLDER:\n    os.makedirs (OUTPUT_FOLDER+'stage1/', exist_ok=True)\n    \n# For full preprocessing you should to set demo=False\npreprocess_all_scans_mp (INPUT_SCAN_FOLDER, OUTPUT_FOLDER, demo=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a5e383ad-13e5-8338-b26c-992db53ec914"},"source":"Summary\n===========\nIn this post we built a working convolutional 3D neural network without programming. Please feel free to modify and experiment with it.\n\nCurrently I am working on following two more appoarches:\n\n1. Reduce dimensionality of scans using **autoencoders** to make it easy to process the dataset using some other neural network.\n\n2. Use **Convolutional LSTM neural network** that combines both CNN and LSTM for analyzing sequence of images.\n\nI hope to share more details about these experiments with you in coming days.\n\nIf you liked this post please give it a upvote!! - Thank you"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}