{"cells":[{"metadata":{"_cell_guid":"90110b59-afa6-f2f2-5cbb-3f6b78df238e","_uuid":"9e98c8a308eb229fc1d9173f167564813c7c6b39","trusted":true},"cell_type":"code","source":"import pydicom # for reading dicom files\nimport os # for doing directory operations \nimport pandas as pd # for some simple data analysis (right now, just to load in the labels data and quickly reference it)\n\n# Change this to wherever you are storing your data:\n# IF YOU ARE FOLLOWING ON KAGGLE, YOU CAN ONLY PLAY WITH THE SAMPLE DATA, WHICH IS MUCH SMALLER\n\ndata_dir = '../input/sample_images/'\npatients = os.listdir(data_dir)\n\nlabels_df = pd.read_csv('../input/stage1_labels.csv', index_col=0)\n\nlabels_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eb1643a6-c04a-6cb1-69a0-de96907a761d","_uuid":"bde26ffb7e8b03c0a1bdfe55f81b062c593d0ee9","trusted":true},"cell_type":"code","source":"for patient in patients[:1]:\n    label = labels_df.get_value(patient, 'cancer')\n    path = data_dir + patient\n    \n    # a couple great 1-liners from: https://www.kaggle.com/gzuidhof/data-science-bowl-2017/full-preprocessing-tutorial\n    slices = [pydicom.read_file(path + '/' + s) for s in os.listdir(path)]\n    slices.sort(key = lambda x: int(x.ImagePositionPatient[2]))\n    print(len(slices),label)\n    print(slices[0])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"50cfc820-2b8b-f781-283f-a7e96b81f1d8","_uuid":"3d0cf83d6c558f8ea708506dede86a048cf6ba60","trusted":true},"cell_type":"code","source":"for patient in patients[:3]:\n    label = labels_df.get_value(patient, 'cancer')\n    path = data_dir + patient\n    \n    # a couple great 1-liners from: https://www.kaggle.com/gzuidhof/data-science-bowl-2017/full-preprocessing-tutorial\n    slices = [pydicom.read_file(path + '/' + s) for s in os.listdir(path)]\n    slices.sort(key = lambda x: int(x.ImagePositionPatient[2]))\n    print(slices[0].pixel_array.shape, len(slices))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c5649892-5357-176a-d7b7-1b10af783f67","_uuid":"c74c03c2cab35df6fd39f644f7fd4aba06ed2f34","trusted":true},"cell_type":"code","source":"len(patients)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d7ce0958-6f85-d6df-e075-0cd0ada32530","_uuid":"6e744eb7d3fa6938db5a831739dc642d753b29c0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfor patient in patients[:1]:\n    label = labels_df.get_value(patient, 'cancer')\n    path = data_dir + patient\n    slices = [pydicom.read_file(path + '/' + s) for s in os.listdir(path)]\n    slices.sort(key = lambda x: int(x.ImagePositionPatient[2]))\n    \n    #          the first slice\n    plt.imshow(slices[50].pixel_array)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ba19962b-e38e-a445-382d-0d8f0bc4c05d","_uuid":"af67abdba7b2d841a0505eccadc9a7da8450a6a4"},"cell_type":"markdown","source":"# Section 2: Processing and viewing our Data #\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/lqhMTkouBx0?list=PLQVvvaa0QuDd5meH8cStO9cMi98tPT12_\" frameborder=\"0\" allowfullscreen></iframe>"},{"metadata":{"_cell_guid":"3f83e296-fb91-a340-c34c-a3ad324a957e","_uuid":"9484b3a0b66b1d03b3b5d53b3844723105e11adc","trusted":true},"cell_type":"code","source":"import cv2\nimport numpy as np\n\nIMG_PX_SIZE = 150\n\nfor patient in patients[:1]:\n    label = labels_df.get_value(patient, 'cancer')\n    path = data_dir + patient\n    slices = [pydicom.read_file(path + '/' + s) for s in os.listdir(path)]\n    slices.sort(key = lambda x: int(x.ImagePositionPatient[2]))\n    fig = plt.figure()\n    for num,each_slice in enumerate(slices[:12]):\n        y = fig.add_subplot(3,4,num+1)\n        new_img = cv2.resize(np.array(each_slice.pixel_array),(IMG_PX_SIZE,IMG_PX_SIZE))\n        y.imshow(new_img)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ba33920f-e452-dc45-7304-0010288435cc","_uuid":"bfa338b070dcfa94cf5cc7e0c2164110435f0185","trusted":true},"cell_type":"code","source":"import math\n\ndef chunks(l, n):\n    # Credit: Ned Batchelder\n    # Link: http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n\ndef mean(l):\n    return sum(l) / len(l)\n\nIMG_PX_SIZE = 150\nHM_SLICES = 20\n\ndata_dir = '../input/sample_images/'\npatients = os.listdir(data_dir)\nlabels_df = pd.read_csv('../input/stage1_labels.csv', index_col=0)\n\nfor patient in patients[:10]:\n    try:\n        label = labels_df.get_value(patient, 'cancer')\n        path = data_dir + patient\n        slices = [dicom.read_file(path + '/' + s) for s in os.listdir(path)]\n        slices.sort(key = lambda x: int(x.ImagePositionPatient[2]))\n        new_slices = []\n        slices = [cv2.resize(np.array(each_slice.pixel_array),(IMG_PX_SIZE,IMG_PX_SIZE)) for each_slice in slices]\n        chunk_sizes = math.ceil(len(slices) / HM_SLICES)\n        for slice_chunk in chunks(slices, chunk_sizes):\n            slice_chunk = list(map(mean, zip(*slice_chunk)))\n            new_slices.append(slice_chunk)\n\n        print(len(slices), len(new_slices))\n    except:\n        # some patients don't have labels, so we'll just pass on this for now\n        pass","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4bf7cd7c-b14a-ffae-9ff6-e926c61fa6ba","_uuid":"90b5809b12da13fe6172c23317bbe63148e87393"},"cell_type":"markdown","source":"The struggle is real. Okay, what you're about to see you shouldn't attempt if anyone else is watching, like if you're going to show your code to the public..."},{"metadata":{"_cell_guid":"c4114c53-9e7b-fd19-6e0c-914a58120451","_uuid":"3f54c01147431c42f84df1ea59fc5bdbadf5a618","trusted":true,"scrolled":true},"cell_type":"code","source":"for patient in patients[:10]:\n    try:\n        label = labels_df.get_value(patient, 'cancer')\n        path = data_dir + patient\n        slices = [pydicom.read_file(path + '/' + s) for s in os.listdir(path)]\n        slices.sort(key = lambda x: int(x.ImagePositionPatient[2]))\n        new_slices = []\n\n        slices = [cv2.resize(np.array(each_slice.pixel_array),(IMG_PX_SIZE,IMG_PX_SIZE)) for each_slice in slices]\n\n        chunk_sizes = math.ceil(len(slices) / HM_SLICES)\n\n\n        for slice_chunk in chunks(slices, chunk_sizes):\n            slice_chunk = list(map(mean, zip(*slice_chunk)))\n            new_slices.append(slice_chunk)\n\n        if len(new_slices) == HM_SLICES-1:\n            new_slices.append(new_slices[-1])\n\n        if len(new_slices) == HM_SLICES-2:\n            new_slices.append(new_slices[-1])\n            new_slices.append(new_slices[-1])\n\n        if len(new_slices) == HM_SLICES+2:\n            new_val = list(map(mean, zip(*[new_slices[HM_SLICES-1],new_slices[HM_SLICES],])))\n            del new_slices[HM_SLICES]\n            new_slices[HM_SLICES-1] = new_val\n\n        if len(new_slices) == HM_SLICES+1:\n            new_val = list(map(mean, zip(*[new_slices[HM_SLICES-1],new_slices[HM_SLICES],])))\n            del new_slices[HM_SLICES]\n            new_slices[HM_SLICES-1] = new_val\n\n        print(len(slices), len(new_slices))\n    except Exception as e:\n        # again, some patients are not labeled, but JIC we still want the error if something\n        # else is wrong with our code\n        print(str(e))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"00f3207b-8ec0-0f7e-5a98-2c1f53686f76","_uuid":"9939f890f2a632a7bc20bc6bd479fa15bdb213d5"},"cell_type":"markdown","source":"Okay, the Python gods are really not happy with me for that hacky solution. If any of you would like to improve this chunking/averaging code, feel free. Really, any of this code...if you have improvements, share them! This is going to stay pretty messy. But hey, we did it! We figured out a way to make sure our 3 dimensional data can be at any resolution we want or need. Awesome!\n\nThat's actually a decently large hurdle. Are we totally done? ...maybe not. One major issue is these colors and ranges of data. It's unclear to me whether or not a model would appreciate that. Even if we do a grayscale colormap in the imshow, you'll see that some scans are just darker overall than others. This might be problematic and we might need to actually normalize this dataset.\n\nI expect that, with a large enough dataset, this wouldn't be an actual issue, but, with this size of data, it might be of huge importance.\n\nIn effort to not turn this notebook into an actual book, however, we're going to move forward! We can now see our new data by doing:"},{"metadata":{"_cell_guid":"3d073af0-3fe8-bc99-9b61-4c35b1fc0745","_uuid":"d3ae0e146cd29d1a53d088e4f95074a4c8f7b4f7","trusted":true},"cell_type":"code","source":"for patient in patients[:1]:\n    label = labels_df.get_value(patient, 'cancer')\n    path = data_dir + patient\n    slices = [pydicom.read_file(path + '/' + s) for s in os.listdir(path)]\n    slices.sort(key = lambda x: int(x.ImagePositionPatient[2]))\n    new_slices = []\n\n    slices = [cv2.resize(np.array(each_slice.pixel_array),(IMG_PX_SIZE,IMG_PX_SIZE)) for each_slice in slices]\n    \n    chunk_sizes = math.ceil(len(slices) / HM_SLICES)\n    for slice_chunk in chunks(slices, chunk_sizes):\n        slice_chunk = list(map(mean, zip(*slice_chunk)))\n        new_slices.append(slice_chunk)\n\n    if len(new_slices) == HM_SLICES-1:\n        new_slices.append(new_slices[-1])\n\n    if len(new_slices) == HM_SLICES-2:\n        new_slices.append(new_slices[-1])\n        new_slices.append(new_slices[-1])\n\n    if len(new_slices) == HM_SLICES+2:\n        new_val = list(map(mean, zip(*[new_slices[HM_SLICES-1],new_slices[HM_SLICES],])))\n        del new_slices[HM_SLICES]\n        new_slices[HM_SLICES-1] = new_val\n        \n    if len(new_slices) == HM_SLICES+1:\n        new_val = list(map(mean, zip(*[new_slices[HM_SLICES-1],new_slices[HM_SLICES],])))\n        del new_slices[HM_SLICES]\n        new_slices[HM_SLICES-1] = new_val\n    \n    fig = plt.figure()\n    for num,each_slice in enumerate(new_slices):\n        y = fig.add_subplot(4,5,num+1)\n        y.imshow(each_slice, cmap='gray')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8a888510-5db1-bf6d-3fc5-754d245425c9","_uuid":"741546e2198e427c5078894e34f88db169be48fe"},"cell_type":"markdown","source":"# Section 3: Preprocessing our Data #\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_DAeMDMHgtY?list=PLQVvvaa0QuDd5meH8cStO9cMi98tPT12_\" frameborder=\"0\" allowfullscreen></iframe>\n\nOkay, so we know what we've got, and what we need to do with it.\n\nWe have a few options at this point, we could take the code that we have already and do the processing \"online.\" By this, I mean, while training the network, we can actually just loop over our patients, resize the data, then feed it through our neural network. We actually don't have to have all of the data prepared before we go through the network.\n\nIf you can preprocess all of the data into one file, and that one file doesn't exceed your available memory, then training should likely be faster, so you can more easily tweak your neural network and not be processing your data the same way over and over.\n\nIn many more realistic examples in the world, however, your dataset will be so large, that you wouldn't be able to read it all into memory at once anyway, but you could still maintain one big database or something.\n\nBottom line: There are tons of options here. Our dataset is only 1500 (even less if you are following in the Kaggle kernel) patients, and will be, for example, 20 slices of 150x150 image data if we went off the numbers we have now, but this will need to be even smaller for a typical computer most likely. \n\nRegardless, this much data wont be an issue to keep in memory or do whatever the heck we want.\n\nIf at all possible, I prefer to separate out steps in any big process like this, so I am going to go ahead and pre-process the data, so our neural network code is much simpler. Also, there's no good reason to maintain a network in GPU memory while we're wasting time processing the data which can be easily done on a CPU.\n\nNow, I will just make a slight modification to all of the code up to this point, and add some new final lines to preprocess this data and save the array of arrays to a file:"},{"metadata":{"_cell_guid":"7d11974b-cb01-6d5c-6214-df91955acf53","_uuid":"3c17d35f17b16a750da6cf6f0a1d318033c4a540","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pydicom\nimport os\nimport matplotlib.pyplot as plt\nimport cv2\nimport math\n\nIMG_SIZE_PX = 50\nSLICE_COUNT = 20\n\ndef chunks(l, n):\n    # Credit: Ned Batchelder\n    # Link: http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n\n\ndef mean(a):\n    return sum(a) / len(a)\n\n\ndef process_data(patient,labels_df,img_px_size=50, hm_slices=20, visualize=False):\n    \n    label = labels_df.get_value(patient, 'cancer')\n    path = data_dir + patient\n    slices = [pydicom.read_file(path + '/' + s) for s in os.listdir(path)]\n    slices.sort(key = lambda x: int(x.ImagePositionPatient[2]))\n\n    new_slices = []\n    slices = [cv2.resize(np.array(each_slice.pixel_array),(img_px_size,img_px_size)) for each_slice in slices]\n    \n    chunk_sizes = math.ceil(len(slices) / hm_slices)\n    for slice_chunk in chunks(slices, chunk_sizes):\n        slice_chunk = list(map(mean, zip(*slice_chunk)))\n        new_slices.append(slice_chunk)\n\n    if len(new_slices) == hm_slices-1:\n        new_slices.append(new_slices[-1])\n\n    if len(new_slices) == hm_slices-2:\n        new_slices.append(new_slices[-1])\n        new_slices.append(new_slices[-1])\n\n    if len(new_slices) == hm_slices+2:\n        new_val = list(map(mean, zip(*[new_slices[hm_slices-1],new_slices[hm_slices],])))\n        del new_slices[hm_slices]\n        new_slices[hm_slices-1] = new_val\n        \n    if len(new_slices) == hm_slices+1:\n        new_val = list(map(mean, zip(*[new_slices[hm_slices-1],new_slices[hm_slices],])))\n        del new_slices[hm_slices]\n        new_slices[hm_slices-1] = new_val\n\n    if visualize:\n        fig = plt.figure()\n        for num,each_slice in enumerate(new_slices):\n            y = fig.add_subplot(4,5,num+1)\n            y.imshow(each_slice, cmap='gray')\n        plt.show()\n\n    if label == 1: label=np.array([0,1])\n    elif label == 0: label=np.array([1,0])\n        \n    return np.array(new_slices),label\n\n#                                               stage 1 for real.\ndata_dir = '../input/sample_images/'\npatients = os.listdir(data_dir)\nlabels = pd.read_csv('../input/stage1_labels.csv', index_col=0)\n\nmuch_data = []\nfor num,patient in enumerate(patients):\n    if num % 100 == 0:\n        print(num)\n    try:\n        img_data,label = process_data(patient,labels,img_px_size=IMG_SIZE_PX, hm_slices=SLICE_COUNT)\n        #print(img_data.shape,label)\n        much_data.append([img_data,label])\n    except KeyError as e:\n        print('This is unlabeled data!')\n\nnp.save('muchdata-{}-{}-{}.npy'.format(IMG_SIZE_PX,IMG_SIZE_PX,SLICE_COUNT), much_data)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e75d64f1-4791-5f72-8d84-6ff88468ea8e","_uuid":"fdd4e62de83b349e71f478c0e30019b95d2348a5"},"cell_type":"markdown","source":"# Section 4: 3D Convolutional Neural Network #\n\n##Moment-o-truth##\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/CPZ5ihaNfJc?list=PLQVvvaa0QuDd5meH8cStO9cMi98tPT12_\" frameborder=\"0\" allowfullscreen></iframe>\n\nOkay, we've got preprocessed, normalized, data. Now we're ready to feed it through our 3D convnet and...see what happens!\n\nNow, I am not about to stuff a neural networks tutorial into this one. If you're already familiar with neural networks and TensorFlow, great! If not, as you might guess, I have a tutorial...or tutorials... for you!\n\nTo install the CPU version of TensorFlow, just do *pip install tensorflow*\n\nTo install the GPU version of TensorFlow, you need to get alllll the dependencies and such.\n\n**Installation tutorials:**\n\n[Installing the GPU version of TensorFlow in Ubuntu][1]\n\n[Installing the GPU version of TensorFlow on a Windows machine][2]\n\n**Using TensorFlow and concept tutorials:**\n\n[Introduction to deep learning with neural networks][3]\n\n[Introduction to TensorFlow][4] \n\n[Intro to Convolutional Neural Networks][5]\n\n[Convolutional Neural Network in TensorFlow tutorial][6]\n\nNow, the data we have is actually 3D data, not 2D data that's covered in most convnet tutorials, including mine above. So what changes? EVERYTHING! OMG IT'S THE END OF THE WORLD AS WE KNOW IT!!\n\nIt's not really all too bad. Your convolutional window/padding/strides need to change. Do note that, now, to have a bigger window, your processing penalty increases significantly as we increase in size, obviously much more than with 2D windows.\n\nOkay, let's begin.\n\n\n  [1]: https://pythonprogramming.net/how-to-cuda-gpu-tensorflow-deep-learning-tutorial/\n  [2]: https://www.youtube.com/watch?v=r7-WPbx8VuY\n  [3]: https://pythonprogramming.net/neural-networks-machine-learning-tutorial\n  [4]: https://pythonprogramming.net/tensorflow-introduction-machine-learning-tutorial/\n  [5]: https://pythonprogramming.net/convolutional-neural-network-cnn-machine-learning-tutorial/\n  [6]: https://pythonprogramming.net/cnn-tensorflow-convolutional-nerual-network-machine-learning-tutorial/"},{"metadata":{"_cell_guid":"450e3bc8-600f-a5cd-1002-efa46f3a16e0","_uuid":"557e834aeaaeec337160c86b5bf38d0939df001e","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\nIMG_SIZE_PX = 50\nSLICE_COUNT = 20\n\nn_classes = 2\nbatch_size = 10\n\nx = tf.placeholder('float')\ny = tf.placeholder('float')\n\nkeep_rate = 0.8","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ddd526a1-ab8e-b27d-545f-b788391b25fc","_uuid":"e6fae46aa46180f21e80d7e2a138d57013c31d0a","trusted":true},"cell_type":"code","source":"def conv3d(x, W):\n    return tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')\n\ndef maxpool3d(x):\n    #                        size of window         movement of window as you slide about\n    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3f371d51-500c-92c3-bc24-46ce421c5ef5","_uuid":"4820cc39444f6670a1ec0288f939c7cc699d9c5b"},"cell_type":"markdown","source":"Now we're ready for the network itself:"},{"metadata":{"_cell_guid":"1123bdcf-4f13-1805-4585-95b5f118fe80","_uuid":"76d4f4b54e5305a8da685d4723a9dd0f299b1697","trusted":true},"cell_type":"code","source":"def convolutional_neural_network(x):\n    #                # 5 x 5 x 5 patches, 1 channel, 32 features to compute.\n    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,1,32])),\n               #       5 x 5 x 5 patches, 32 channels, 64 features to compute.\n               'W_conv2':tf.Variable(tf.random_normal([3,3,3,32,64])),\n               #                                  64 features\n               'W_fc':tf.Variable(tf.random_normal([54080,1024])),\n               'out':tf.Variable(tf.random_normal([1024, n_classes]))}\n\n    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n               'b_conv2':tf.Variable(tf.random_normal([64])),\n               'b_fc':tf.Variable(tf.random_normal([1024])),\n               'out':tf.Variable(tf.random_normal([n_classes]))}\n\n    #                            image X      image Y        image Z\n    x = tf.reshape(x, shape=[-1, IMG_SIZE_PX, IMG_SIZE_PX, SLICE_COUNT, 1])\n\n    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])\n    conv1 = maxpool3d(conv1)\n\n\n    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])\n    conv2 = maxpool3d(conv2)\n\n    fc = tf.reshape(conv2,[-1, 54080])\n    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n    fc = tf.nn.dropout(fc, keep_rate)\n\n    output = tf.matmul(fc, weights['out'])+biases['out']\n\n    return output","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6d6fe928-e5f7-2c8d-b538-94d19bc5fc74","_uuid":"0a2fda909aa3d13a01e861320b6b4c99f0771df1"},"cell_type":"markdown","source":"Why 54080 magic number? To get this, I simply run the script once, and see what the error yells at me for the expected size multiple. This is certainly not the right way to go about it, but that's my 100% honest method, and my first time working in a 3D convnet. AFAIK, it's the padding that causes this to not be EXACTLY 50,000, (50 x 50 x 20 is the size of our actual input data, which is 50,000 total). \n\nSomeone feel free to enlighten me how one could actually calculate this number beforehand. \n\nNow we're set to train the network. I am not going to ask the Kaggle online kernel to even bother building this computation graph, so I will comment out the line to actually run this. Just uncomment it locally and it will run. When running locally, make sure your training data is NOT the sample images, it should be the stage1 images. Your training file should be ~700mb with ~1400 total labeled samples."},{"metadata":{"_cell_guid":"81f98243-d698-cc63-3623-3a54fee4f6d1","_uuid":"26822ec288c57b0f126012e81ee8bd37a81817b3","trusted":true,"scrolled":true},"cell_type":"code","source":"much_data = np.load('muchdata-50-50-20.npy')\n# If you are working with the basic sample data, use maybe 2 instead of 100 here... you don't have enough data to really do this\ntrain_data = much_data[:-100]\nvalidation_data = much_data[-100:]\n\n\ndef train_neural_network(x):\n    prediction = convolutional_neural_network(x)\n    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y) )\n    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\n    \n    hm_epochs = 10\n    with tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n        \n        successful_runs = 0\n        total_runs = 0\n        \n        for epoch in range(hm_epochs):\n            epoch_loss = 0\n            for data in train_data:\n                total_runs += 1\n                try:\n                    X = data[0]\n                    Y = data[1]\n                    _, c = sess.run([optimizer, cost], feed_dict={x: X, y: Y})\n                    epoch_loss += c\n                    successful_runs += 1\n                except Exception as e:\n                    pass\n            \n            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n\n            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n\n            print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n            \n        print('Done. Finishing accuracy:')\n        print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n        \n        #print('fitment percent:',successful_runs/total_runs)\n\n# Run this locally:\ntrain_neural_network(x)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bfbd3e51-89bd-df5e-7e79-6c183654fcd0","_uuid":"441b4154870903cb05810bf88b36f9254616a5b4"},"cell_type":"markdown","source":"Example output that I got:\n\n    Epoch 1 completed out of 10 loss: 195148607547.0\n\nAccuracy: 0.63\n\nEpoch 2 completed out of 10 loss: 14236109414.9\n\nAccuracy: 0.6\n\nEpoch 3 completed out of 10 loss: 5744945978.94\n\nAccuracy: 0.7\n\nEpoch 4 completed out of 10 loss: 3268944715.44\n\nAccuracy: 0.6\n\nEpoch 5 completed out of 10 loss: 1916325681.66\n\nAccuracy: 0.6\n\nEpoch 6 completed out of 10 loss: 1014763813.3\n\nAccuracy: 0.46\n\nEpoch 7 completed out of 10 loss: 680146186.953\n\nAccuracy: 0.54\n\nEpoch 8 completed out of 10 loss: 289082075.259\n\nAccuracy: 0.62\n\nEpoch 9 completed out of 10 loss: 122785997.913\n\nAccuracy: 0.57\n\nEpoch 10 completed out of 10 loss: 96427552.5371\n\nAccuracy: 0.51\n\nDone. Finishing accuracy:\n\nAccuracy: 0.69\n\nfitment percent: 0.9992289899768697\n"},{"metadata":{"_cell_guid":"45b2ba92-d97c-6520-4bd0-c7d6b9906cb5","_uuid":"90cd88684693c282a29ea166d7a23b4ef74d0bb8"},"cell_type":"markdown","source":"# Section 5: Concluding Remarks #\n\nSo how did we do? Well, we overfit almost certainly. How about our accuracy? Due to the lower amount of data on Kaggle, I have no idea what number you're seeing, just know it's probably not all that great. Even if it was, what was the number to beat? Was it 50%, since it's either cancer or not? Not quite. The real number we need to beat is if our network was to always predict a single class. Let's see what the best score our classifer could get is if it just always picked the most common class:"},{"metadata":{"_cell_guid":"7fa4a17a-55d0-8e06-5f5d-e002625c582e","_uuid":"b4b4b143c5ab5ace0e43ecdd13737e9b52915604","trusted":false},"cell_type":"code","source":"labels_df.cancer.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3fe2ad3e-7e05-bf93-5467-9566f255aa72","_uuid":"a7fc78c08603b55a0ff4fcd9224ffc39b881e383"},"cell_type":"markdown","source":"So, actually, our dataset has 1035 non-cancer examples and 362 cancerous examples. Thus, an algorithm that always predicted no-cancer with our model would be ~ 74% accurate (1035/1397).\n\nWe'd definitely want to confirm our testing set actually has this ratio before assuming anything. It might be the case our testing set has more cancerous examples, or maybe less, we really don't know. We can though:"},{"metadata":{"_cell_guid":"6d98db88-602e-b73d-d675-a702226f5352","_uuid":"fa7f1afac95e0ffe0bc68c33e51d8091dfdef01d","trusted":false},"cell_type":"code","source":"labels_df.ix[-100:].cancer.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ff71c422-7ae7-7956-0926-03c263230f46","_uuid":"0166cae97be8bbe0d7cbedcda2ef7485402170da"},"cell_type":"markdown","source":"In this case, it's pretty close. No cancer always would give us 72% accuracy. In our test, that's the real number to beat.\n\nAll this said, this specific competition isn't actually just \"right\" or wrong. Your job is to predict % chance of cancer, it's not so binary."},{"metadata":{"_cell_guid":"d9b790f5-778c-4ea1-3325-a8da96ba8b24","_uuid":"0874f812f2c6778dd772ddd829e4ba851e100e10"},"cell_type":"markdown","source":"## Now what? ##\n\nWell, likely the largest issue here is our size of data. If we're going to be successful with a neural network, we need more data. We can either hunt for more outside datasources, or we can engage in adding some noise to the data.\n\nIt might also just simply be the case that a neural network isn't the best model of choice here. Neural networks are capable of amazing things, only when we have the datasets to support it. We might just simply not have enough data.\n\n 1. Create new data from the existing data by adding noise. There are\n    many ways to add \"noise\" to data. With pictures, we can do all sorts\n    of things. Overlaying other pictures on top, rotating, messing with\n    edges, adding various transforms...the list goes on and on. Before\n    just adding any noise, it would be wise to research what exactly\n    doctors do when analyzing CT scans for cancerous tumors. For\n    example, you can check out this tutorial:\n    [https://www.kaggle.com/c/data-science-bowl-2017/discussion/27922][1]\n    for how an actual doctor reads a CT scan. You wouldn't want to add\n    noise that conflicts here most likely.\n 2. Bring in more outside data. I'd start here:\n    [https://www.kaggle.com/c/data-science-bowl-2017/discussion/27666][2]\n    \"official external data\" thread.\n 3. Consider neural network variables, like layers, learning rate,\n    activation functions, optimizer...etc.\n 4. Consider other models. \n 5. Keep hacking away!\n\n  [1]: https://www.kaggle.com/c/data-science-bowl-2017/discussion/27922\n  [2]: https://www.kaggle.com/c/data-science-bowl-2017/discussion/27666"},{"metadata":{"_cell_guid":"1aad7a07-c9aa-fe7b-6727-d1e570905b5f","_uuid":"8d999d471356ecca8ff4cef09e8c4dba1e92b6ac","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}