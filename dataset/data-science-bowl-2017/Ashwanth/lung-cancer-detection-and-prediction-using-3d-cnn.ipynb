{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"e6821b44-fc89-0e22-4160-8d1b025bbb97"},"source":"***3D Convolutional Neural Networks using 5 convolutional layers***\n\n**INTRODUCTION**\nThe project deals with the identification of lung cancer, which affects roughly about 225,000 people every year and accounts to about 12 billion in health care costs in the final stages. Therefore, early detection of cancer is critical to give the patients the best chance of recovery and survival.\n\nThe goal of this project is to evaluate the data (Slices of CT scans) provided with various pre-processing techniques and analyze the data using machine learning algorithms, in this case 3D Convolutional Neural Networks to train and validate the model, to create an accurate model which can be used to determine whether a person has cancer or not. This will greatly help in the identification and elimination of cancer cells in the early stages. Therefore, an automated method capable of determining whether the patient will be diagnosed with lung cancer is the aim of this project.\n\n**Dataset Information**\n            The dataset consists of thousands of CT images from high risk patients in DICOM format (detailed information given below). Each image contains a series with multiple axial slices of the chest cavity and a variable number of 2D slices, which can vary based on the machine taking the scan and the patient.\n\n**DICOM Standard**\n        DICOM is an acronym for Digital Imaging and Communications in Medicine. Files in this format are saved with either a DCM or DCM30 file extension, but some may not have an extension at all. It is used for both communication protocol and a file format i.e., it can store medical information, such as ultrasound and MRI images along with the patient’s information, all in one file.\n          The format ensures that all the data stays together and provides the ability to transfer information between devices that support the DICOM format. To view the information in the files there are two methods, they are \n*Opening DICOM files with Free viewer*\n                 I have used the MicroDicom File viewer, which is a free to use software exclusively for viewing the DICOM files. The image along with the metadata can be found making it easy and efficient to access the data for research and development purposes.\n*Using Libraries to extract the information* \n                 The DICOM standard is complex and there are number of different tools to work with DICOM files, I have used the pydicom package, which is a package that can be imported towards working with images in python.\n\n**File Description**\n        Each patient ID has an associated directory of DICOM files and is in the header of the file along with the patient name, the exact number of images per person varies with every file.\nstage1.7z - contains all images for the first stage of the competition, including both the training and test set. \nstage1_labels.csv - contains the cancer ground truth for the stage 1 training set images.\ndata_password.txt - contains the decryption key for the image files.\n\n**3D CNN Computation** \nThis section consists of the steps involved taking the processed data and using it to determine the accuracy of the model, to predict whether a person could be affected with cancer or not. To understand the composition of what goes into creating a Convolutional neural network and the layers that are used to process the data, a summary of every layer and its functions that we have used in the project is provided, the summary is an amalgamation of inferences that we have achieved based on various research papers whose links are also provided (references).\nWe should input the 3D image that we have created in the pre-processing section and set the various layers with values, there is no hard and fast rule that values are fixed, hence we had to test the layers’ multiple times with different values and arrive at a set of values that were optimal.\nSo, when we train the model, we send in a part of the data (majority) and keep the rest of it for validation purposes to check is the model is well trained. We had 1595 images in total, and we provided 1495 images as input for training the model and 100 images for validation.\nThe idea here is to complete training the model with 1495 images and then pass the 100 validation images to test the model to provide an accuracy as to how well the model is trained and can predict accurately. In short, we are providing the test data on the trained data model to identify the accuracy (final accuracy).\nWe also had setup the number of epochs to run, batch size of the input data specifying how many images are to processed at once, input size of the image was also specified as 50x50x20, it is very small compared to the original size of the image, but our systems could accommodate only such capacity, this in turn caused a drop in the quality and quantity of the dataset.\nThis drop in metrics lead us to observe that we are not using enough information for the model to train on and hence affect the accuracy of the model. After running the model for 100 epochs, we found that the percentage of accuracy was stagnant at the 83rd epoch at around 72%, which is the maximum accuracy we could achieve, we could not achieve more due to lack of more data causing the model to overfit, leading to lower accuracy. Before identifying the final accuracy, we had to mix and match the layers in between to get the optimal layer composition, i.e., At first we had set 3 layers and we had achieved an accuracy of 54% with just 3 epochs run, then we increased the number of epochs and observed that the accuracy increased, we kept increasing the layers and finally reached 6 layers which was optimal and 100 epochs was just fine to get the maximum accuracy.\n\n**Computation of confusion matrix and cancer prediction** \nThe confusion matrix is a way to determine whether the model is good or not. Here we have done the confusion matrix for predicted labels vs actual labels. The 0’s represent no cancer and the 1’s represent cancer. Ideally the false positives and the false negatives value should be low.\n\n**Dataset is not enough – overfitting occurs** \nThe total dataset size is 1595 out of which 1107 patients do not have cancer, 390 patients have cancer and 98 patients do not have labels. The dataset for number of patients having cancer is very low. This is the reason why overfitting occurs and cancer predictions are not that accurate. One way to overcome this problem is by adding more cancer patient datasets which is currently not available. Try adding LUNA16 dataset and also image rotation and zooming to the existing dataset.\n\n**Concluding Remarks**\nThe idea that we have presented is at the very beginning of its deployment phase and thus we wish to extend our project by implementing a model that can make valuable predictions based on the reports specified in the datasets and achieve a higher value of accuracy by using Microsoft ResNet approach to build a network, to provide much higher learning rate of the nodes increasing the performance of the model and minimizing the losses effectively.\n\n\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e46f9ab9-e390-ae1a-b65c-01dddd10785d"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport dicom\nimport os\nimport matplotlib.pyplot as plt\nimport cv2\nimport math\n\ndataDirectory = 'Lung_Cancer/stage1/stage1/'\nlungPatients = os.listdir(dataDirectory)\nlabels = pd.read_csv('Lung_Cancer/stage1_labels/stage1_labels.csv', index_col=0)\nsize = 50\nNoSlices = 20\n\n\ndef chunks(l, n):\n    count = 0\n    for i in range(0, len(l), n):\n        if (count < NoSlices):\n            yield l[i:i + n]\n            count = count + 1\n\n\ndef mean(l):\n    return sum(l) / len(l)\n\n\ndef dataProcessing(patient, labels_df, size=50, noslices=20, visualize=False):\n    label = labels_df.get_value(patient, 'cancer')\n    path = dataDirectory + patient\n    slices = [dicom.read_file(path + '/' + s) for s in os.listdir(path)]\n    slices.sort(key=lambda x: int(x.ImagePositionPatient[2]))\n\n    new_slices = []\n    slices = [cv2.resize(np.array(each_slice.pixel_array), (size, size)) for each_slice in slices]\n\n    chunk_sizes = math.floor(len(slices) / noslices)\n    for slice_chunk in chunks(slices, chunk_sizes):\n        slice_chunk = list(map(mean, zip(*slice_chunk)))\n        new_slices.append(slice_chunk)\n\n    if label == 1:\n        label = np.array([0, 1])\n    elif label == 0:\n        label = np.array([1, 0])\n    return np.array(new_slices), label\n\n\nimageData = []\nfor num, patient in enumerate(lungPatients):\n    if num % 100 == 0:\n        print('Saved -', num)\n    try:\n        img_data, label = dataProcessing(patient, labels, size=size, noslices=NoSlices)\n        imageData.append([img_data, label,patient])\n    except KeyError as e:\n        print('Data is unlabeled')\n\nnp.save('imageDataNew-{}-{}-{}.npy'.format(size, size, NoSlices), imageData)"},{"cell_type":"markdown","metadata":{"_cell_guid":"dd0658ff-c891-0c86-925e-3e14c26ee098"},"source":"Saved - 0\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nSaved - 100\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nSaved - 200\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nSaved - 300\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nSaved - 400\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nSaved - 500\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nSaved - 600\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nSaved - 700\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nSaved - 800\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nSaved - 900\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nSaved - 1000\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nSaved - 1100\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nSaved - 1200\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nSaved - 1300\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nSaved - 1400\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nSaved - 1500\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled\nData is unlabeled"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c9de3a07-a5ba-31a8-3c37-ce7c44e75a5b"},"outputs":[],"source":"import tensorflow as tf\nimport pandas as pd\nimport tflearn\nfrom tflearn.layers.conv import conv_3d, max_pool_3d\nfrom tflearn.layers.core import input_data, dropout, fully_connected\nfrom tflearn.layers.estimator import regression\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimageData = np.load('imageDataNew-50-50-20.npy')\ntrainingData = imageData[0:800]\nvalidationData = imageData[-200:-100]\n\nx = tf.placeholder('float')\ny = tf.placeholder('float')\nsize = 50\nkeep_rate = 0.8\nNoSlices = 20\n\n\ndef convolution3d(x, W):\n    return tf.nn.conv3d(x, W, strides=[1, 1, 1, 1, 1], padding='SAME')\n\n\ndef maxpooling3d(x):\n    return tf.nn.max_pool3d(x, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1], padding='SAME')\n\n\ndef cnn(x):\n    x = tf.reshape(x, shape=[-1, size, size, NoSlices, 1])\n    convolution1 = tf.nn.relu(\n        convolution3d(x, tf.Variable(tf.random_normal([3, 3, 3, 1, 32]))) + tf.Variable(tf.random_normal([32])))\n    convolution1 = maxpooling3d(convolution1)\n    convolution2 = tf.nn.relu(\n        convolution3d(convolution1, tf.Variable(tf.random_normal([3, 3, 3, 32, 64]))) + tf.Variable(\n            tf.random_normal([64])))\n    convolution2 = maxpooling3d(convolution2)\n    convolution3 = tf.nn.relu(\n        convolution3d(convolution2, tf.Variable(tf.random_normal([3, 3, 3, 64, 128]))) + tf.Variable(\n            tf.random_normal([128])))\n    convolution3 = maxpooling3d(convolution3)\n    convolution4 = tf.nn.relu(\n        convolution3d(convolution3, tf.Variable(tf.random_normal([3, 3, 3, 128, 256]))) + tf.Variable(\n            tf.random_normal([256])))\n    convolution4 = maxpooling3d(convolution4)\n    convolution5 = tf.nn.relu(\n        convolution3d(convolution4, tf.Variable(tf.random_normal([3, 3, 3, 256, 512]))) + tf.Variable(\n            tf.random_normal([512])))\n    convolution5 = maxpooling3d(convolution4)\n    fullyconnected = tf.reshape(convolution5, [-1, 1024])\n    fullyconnected = tf.nn.relu(\n        tf.matmul(fullyconnected, tf.Variable(tf.random_normal([1024, 1024]))) + tf.Variable(tf.random_normal([1024])))\n    fullyconnected = tf.nn.dropout(fullyconnected, keep_rate)\n    output = tf.matmul(fullyconnected, tf.Variable(tf.random_normal([1024, 2]))) + tf.Variable(tf.random_normal([2]))\n    return output\n\n\ndef network(x):\n    prediction = cnn(x)\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\n    epochs = 10\n    with tf.Session() as session:\n        session.run(tf.global_variables_initializer())\n        for epoch in range(epochs):\n            epoch_loss = 0\n            for data in trainingData:\n                try:\n                    X = data[0]\n                    Y = data[1]\n                    _, c = session.run([optimizer, cost], feed_dict={x: X, y: Y})\n                    epoch_loss += c\n                except Exception as e:\n                    pass\n\n            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n           # if tf.argmax(prediction, 1) == 0:\n            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n            print('Epoch', epoch + 1, 'completed out of', epochs, 'loss:', epoch_loss)\n            # print('Correct:',correct.eval({x:[i[0] for i in validationData], y:[i[1] for i in validationData]}))\n            print('Accuracy:', accuracy.eval({x: [i[0] for i in validationData], y: [i[1] for i in validationData]}))\n        print('Final Accuracy:', accuracy.eval({x: [i[0] for i in validationData], y: [i[1] for i in validationData]}))\n        patients = []\n        actual = []\n        predicted = []\n\n        finalprediction = tf.argmax(prediction, 1)\n        actualprediction = tf.argmax(y, 1)\n        for i in range(len(validationData)):\n            patients.append(validationData[i][2])\n        for i in finalprediction.eval({x: [i[0] for i in validationData], y: [i[1] for i in validationData]}):\n            if(i==1):\n                predicted.append(\"Cancer\")\n            else:\n                predicted.append(\"No Cancer\")\n        for i in actualprediction.eval({x: [i[0] for i in validationData], y: [i[1] for i in validationData]}):\n            if(i==1):\n                actual.append(\"Cancer\")\n            else:\n                actual.append(\"No Cancer\")\n        for i in range(len(patients)):\n            print(\"Patient: \",patients[i])\n            print(\"Actual: \", actual[i])\n            print(\"Predcited: \", predicted[i])\n\n        from sklearn.metrics import confusion_matrix\n        y_actual = pd.Series(\n            (actualprediction.eval({x: [i[0] for i in validationData], y: [i[1] for i in validationData]})),\n            name='Actual')\n        y_predicted = pd.Series(\n            (finalprediction.eval({x: [i[0] for i in validationData], y: [i[1] for i in validationData]})),\n            name='Predicted')\n        df_confusion = pd.crosstab(y_actual, y_predicted)\n        print(df_confusion)\n\n        def plot_confusion_matrix(df_confusion, title='Confusion matrix', cmap=plt.cm.gray_r):\\\n            \n            plt.matshow(df_confusion, cmap=cmap)\n            plt.colorbar()\n            tick_marks = np.arange(len(df_confusion.columns))\n            plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n            plt.yticks(tick_marks, df_confusion.index)\n            plt.ylabel(df_confusion.index.name)\n            plt.xlabel(df_confusion.columns.name)\n            plt.show()\n        plot_confusion_matrix(df_confusion)\nnetwork(x)"},{"cell_type":"markdown","metadata":{"_cell_guid":"2256d9dc-b1a2-23d0-9514-190a820fc4fb"},"source":"Epoch 1 completed out of 10 loss: 3.88795779023e+13\nAccuracy: 0.7\nEpoch 2 completed out of 10 loss: 1.24509060778e+13\nAccuracy: 0.6\nEpoch 3 completed out of 10 loss: 6.06095143603e+12\nAccuracy: 0.67\nEpoch 4 completed out of 10 loss: 3.88584119686e+12\nAccuracy: 0.71\nEpoch 5 completed out of 10 loss: 3.01922889866e+12\nAccuracy: 0.58\nEpoch 6 completed out of 10 loss: 1.83550548122e+12\nAccuracy: 0.48\nEpoch 7 completed out of 10 loss: 1.58098134931e+12\nAccuracy: 0.61\nEpoch 8 completed out of 10 loss: 1.02996348973e+12\nAccuracy: 0.67\nEpoch 9 completed out of 10 loss: 845322210816.0\nAccuracy: 0.52\nEpoch 10 completed out of 10 loss: 613924536640.0\nAccuracy: 0.6\nFinal Accuracy: 0.61\nPatient:  dc1ecce5e7f8a4be9082cb5650fa62bd\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  dc5cd907d9de1ed0609832f5bf1fc6e2\nActual:  No Cancer\nPredcited:  Cancer\nPatient:  dc66d11755fd073a59743d0df6b62ee2\nActual:  No Cancer\nPredcited:  Cancer\nPatient:  dc9854bcdcc71b690d9806438009001d\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  dcb426dd025b609489c8f520d6d644b7\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  dcde02d4757bb845376fa6dbb0351df6\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  dcdf0b64b314e08e8f71f3bec9ecb080\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  dcf5fd36b9fff9183f63df286bf8eef9\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  dcf75f484b2d2712e5033ba61fd6e2a0\nActual:  No Cancer\nPredcited:  Cancer\nPatient:  dd281294b34eb6deb76ef9f38169d50e\nActual:  No Cancer\nPredcited:  Cancer\nPatient:  dd571c3949cdae0b59fc0542bb23e06a\nActual:  No Cancer\nPredcited:  Cancer\nPatient:  dd5764803d51c71a27707d9db8c84aac\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  de04fbf5e6c2389f0d039398bdcda971\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  de4d3724030397e71d2ac2ab16df5fba\nActual:  No Cancer\nPredcited:  Cancer\nPatient:  de635c85f320131ee743733bb04e65b9\nActual:  No Cancer\nPredcited:  Cancer\nPatient:  de881c07adc8d53e52391fac066ccb9f\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  de9f65a7a70b73ce2ffef1e4a2613eee\nActual:  No Cancer\nPredcited:  Cancer\nPatient:  df015da931ad5312ee7b24b201b67478\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  df1354de25723c9a55e1241d4c40ffe2\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  df54dc42705decd3f75ec8fd8040e76e\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  df75d5a21b4289e8df6e2d0e135ac48f\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  df761dd787bfc439890740ccce934f36\nActual:  Cancer\nPredcited:  Cancer\nPatient:  df8614fd49a196123c5b88584dd5dd65\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e00832e96709eb85f8e0e608ca02c2b5\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  e10c2b829c39d4a500c09caf04d461a1\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  e127111e994be5f79bb0cea52c9d563e\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e129305f6d074d08cd2de0ebdfeaa576\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e1584618a0c72f124fe618e1ed9b3e55\nActual:  No Cancer\nPredcited:  Cancer\nPatient:  e163325ccf00afde107c80dfce2bce80\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e188bdeea72bb41d980dc2556dc8aafa\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e1c92d3f85a37bd8bb963345b6d66e03\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e1e47812eecd80466cf7f5b0160de446\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e1f3a01e73d706b7e9c30c0a17a4c0b5\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e2a7eaebd0830061e77690aa48f11936\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e2b7fe7fbb002029640c0e65e3051888\nActual:  Cancer\nPredcited:  Cancer\nPatient:  e2bcbfe1ab0f9ddc5d6234f819cd5df5\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e2ea2f046495909ff89e18e05f710fee\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e3034ac9c2799b9a9cee2111593d9853\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e3423505ef6b43f03c5d7bde52a5a78c\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  e38789c5eabb3005bfb82a5298055ba0\nActual:  Cancer\nPredcited:  Cancer\nPatient:  e3a9a6f8d21c6c459728066bcf18c615\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e3e518324e1a85b85f15d9127ed9ea89\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e414153d0e52f70cbe27c129911445a0\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e42815372aa308f5943847ad06f529de\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e43afa905c8e279f818b2d5104f6762b\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  e4421d2d5318845c1cccbc6fa308a96e\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e4436b5914162ff7efea2bdfb71c19ae\nActual:  No Cancer\nPredcited:  Cancer\nPatient:  e46973b13a7a6f421430d81fc1dda970\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e4a87107f94e4a8e32b735d18cef1137\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e4ff18b33b7110a64f497e177102f23d\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e537c91cdfa97d20a39df7ef04a52570\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  e5438d842118e579a340a78f3c5775cc\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e54b574a7e7c650edc224cbdede9e675\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  e56b9f25a47a42f4ae4085005c46109c\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  e572e978c2b50aca781e6302937e5b13\nActual:  Cancer\nPredcited:  Cancer\nPatient:  e58b78dc31d80a50285816f4ecd661e3\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  e58cc57cab8a1738041b72b156fedc56\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e5c68cfa0f33540da3098800f0daae2c\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  e5cf847e616cc2fe94816ffa547d2614\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  e608c0e6cf3adf3c9939593a3c322ef7\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e6214ef879c6d01ae598161e50e23c0c\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e63f43056330bc418a11208aa3a9e7f0\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e659f6517c4df17e86d4d87181396ea6\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  e67bc6cd24a71a486b626592d591a2da\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e6b3e750c6c7a70ca512d77defcfe615\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  e6d4a747235bfcc1feac759571c8485c\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e6d8b2631843a24e6761f2723ea30788\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e6f4757b8f315f31559c5c256cb8dead\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e709901da9ba15a95d4a29906edc01dd\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  e787e5fd289a9f1f6bba31569b7ad384\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e79f52e833ccca893509f0fdeeb26e9f\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e7adb2e4409683b9490e34b6b3604d9e\nActual:  No Cancer\nPredcited:  Cancer\nPatient:  e7cb27a5362a7098e1437bfb1224d2dc\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e7d76f0723911280b64f0f83a4990c97\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e858263b89f0bb57597bcff325eaeecf\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e8be143b9f5e352f71043b24f79f5a17\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e8eb842ee04bbad407f85fe671f24d4f\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  e92a2ed80510513497d5252b001cfa3e\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e977737394cee9abb19ad07310aae8eb\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  e9ccf1ce85c39779fafb9ec703c71555\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  ea7373271a2441b5864df2053c0f5c3e\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  eacb38abacf1214f3b456b6c9fa78697\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  ead64f9269f2200e1d439960a1e069b4\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  eaf753dc137e12fd06e96d27f3111043\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  eb008af181f3791fdce2376cf4773733\nActual:  Cancer\nPredcited:  Cancer\nPatient:  eb8d5136918d6859ca3cc3abafe369ac\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  eba18d04b18084ef64be8f22bb7905ca\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  eba4bfb93928d424ff21b5be96b5c09b\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  ebd601d40a18634b100c92e7db39f585\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  ed0f3c1619b2becec76ba5df66e1ea56\nActual:  Cancer\nPredcited:  No Cancer\nPatient:  ed49b57854f5580658fb3510676e03dd\nActual:  Cancer\nPredcited:  Cancer\nPatient:  ed83b655a1bbad40a782ad13cf27ce8f\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  eda58f4918c4b506cd156702bf8a56a3\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  edad1a7e85b5443e0ae9e654d2adbcba\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  edae2e1edd1217d0c9e20eff2a7b2dd8\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  edbf53a8478049de1494b213fdf942e6\nActual:  No Cancer\nPredcited:  Cancer\nPatient:  ee71210fa398cbb080f6c537a503e806\nActual:  No Cancer\nPredcited:  Cancer\nPatient:  ee88217bee233a3bfc971b450e3d8b85\nActual:  No Cancer\nPredcited:  No Cancer\nPatient:  ee984e8fba88691aac4992fbb14f6e97\nActual:  No Cancer\nPredcited:  Cancer\nPatient:  ee9c580272cd02741df7299892602ac7\nActual:  No Cancer\nPredcited:  No Cancer\nPredicted   0   1\nActual           \n0          56  17\n1          18   9"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}