{"cells":[{"metadata":{"_cell_guid":"82c732d7-4d5e-4b86-bffd-f1929547bf6c","_uuid":"a9d7332abbb64f50e23999d8588711e3605e10e9"},"cell_type":"markdown","source":"there is thread:https://www.kaggle.com/c/avito-demand-prediction/discussion/56079\n\nI think image_top_1 is a classify label, avito maybe use a CNN net classify image into  classes. image_top_1 represents these classes.\n\nTo prove this, I group data by image_top_1, and try use VGG16 to predict, to see if this image is almost same."},{"metadata":{"collapsed":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"\nSome code and sections of this notebook were adapted from:\n- https://www.kaggle.com/classtag/lightgbm-with-mean-encode-feature-0-233\n- https://keras.io/applications/#classify-imagenet-classes-with-resnet50\n"},{"metadata":{"_cell_guid":"e70e1894-5723-460b-8940-4bdfc18c60d9","_uuid":"299f1fe8ac83c837d29362d5431fa51472803f45"},"cell_type":"markdown","source":"# ResNet50 *vs* InceptionResNetV2 *vs* Xception\nLet's compare the performance of three pretrained deep learning models implmented for [Keras].\n\nThe models, [ResNet50], [InceptionV3] and [Xception], are all pre-trained on the [ImageNet] dataset.  Here we initialize them and plot a few images from our Avito's image set and the probability of their top classifications.\n\n**[ImageNet]** is a research project to develop a large image dataset with annotations, such as standard labels and descriptions.  The dataset has been used in the annual [ILSVRC] image classification challenge.  A few of the winners published their pretrained models with the research community, and we are going to use some of them here.\n\n[resnet50]: https://keras.io/applications/#resnet50\n[VGG16]: https://keras.io/applications/#vgg16\n[Xception]: https://keras.io/applications/#xception\n* [InceptionV3]: https://keras.io/applications/#inceptionv3\n[Keras]: https://keras.io/applications/\n[ImageNet]: http://www.image-net.org/\n[ILSVRC]: http://image-net.org/challenges/LSVRC/2017/index"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d28be55dde4ca6fa07ca71c2212a6f6e1c16c24a"},"cell_type":"code","source":"\"\"\"Copy Keras pre-trained model files to work directory from:\nhttps://www.kaggle.com/gaborfodor/keras-pretrained-models\n\nCode from: https://www.kaggle.com/classtag/extract-avito-image-features-via-keras-vgg16/notebook\n\"\"\"\nimport os\n\ncache_dir = os.path.expanduser(os.path.join('~', '.keras'))\nif not os.path.exists(cache_dir):\n    os.makedirs(cache_dir)\n\n# Create symbolic links for trained models.\n# Thanks to Lem Lordje Ko for the idea\n# https://www.kaggle.com/lemonkoala/pretrained-keras-models-symlinked-not-copied\nmodels_symlink = os.path.join(cache_dir, 'models')\nif not os.path.exists(models_symlink):\n    os.symlink('/kaggle/input/keras-pretrained-models/', models_symlink)\n\nimages_dir = os.path.expanduser(os.path.join('~', 'avito_images'))\nif not os.path.exists(images_dir):\n    os.makedirs(images_dir)","execution_count":1,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"_cell_guid":"d0a073a6-57e6-4ceb-9d81-4c65e5366635","_uuid":"f51951bb781a056f9de734a981854dfe66c8b16a","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom keras.preprocessing import image\nimport keras.applications.resnet50 as resnet50\nimport keras.applications.xception as xception\nimport keras.applications.inception_v3 as inception_v3\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\n%matplotlib inline","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"a82406f7-0f32-44dd-985b-e2ceed6ba078","_uuid":"ba4ba5aa664f025aca86921e2e1c3e708783959f","trusted":true},"cell_type":"code","source":"resnet_model = resnet50.ResNet50(weights='imagenet')\ninception_model = InceptionResNetV2(weights='imagenet')\nxception_model = xception.Xception(weights='imagenet')","execution_count":8,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"9dc04109-d250-4807-be75-2b361282a135","_uuid":"a5fb50d4db11db4d86381f52ca54b403a899711e","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/avito-demand-prediction/train.csv',usecols=['image_top_1','item_id','image','category_name','parent_category_name'],index_col='item_id')","execution_count":9,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c8cab812-da45-4bff-ad42-0211ad2c5743","_uuid":"805c20bceb3699f68c7b265980691d74d79821b4","trusted":true},"cell_type":"code","source":"groups = train_df.groupby(['image_top_1',])","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"4ed1c14c-a359-44ab-bcc3-62f3da27017b","_uuid":"223d28e52c81dd72f928b950fab899805dae9954"},"cell_type":"markdown","source":"I chose two groups, and show images and see "},{"metadata":{"collapsed":true,"_cell_guid":"adadf18d-c346-43a4-83a2-731cf41363a7","_uuid":"aceae809f679df674f86c7a7f988682edcb5746e","trusted":true},"cell_type":"code","source":"g0 = groups.get_group(0)\ng1 = groups.get_group(1)","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"da722ed2-ece6-4ee3-97ce-be4ed1ed1d09","_uuid":"1b3113876602d967a1b90cdfffb18e8e1e4230a4","trusted":true},"cell_type":"code","source":"uc = g0['category_name'].unique()\nuc","execution_count":12,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"7476f4b3-6db3-4a9a-8d72-e258885e248b","_uuid":"c310190c0f82eb6bbcec126346419966a34e988e","trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom zipfile import ZipFile\nzip_path = '../input/avito-demand-prediction/train_jpg.zip'\n# with ZipFile(zip_path) as myzip:\n#     files_in_zip = myzip.namelist()\n# with ZipFile(zip_path) as myzip:\n#     with myzip.open(files_in_zip[3]) as myfile:\n#         img = Image.open(myfile)","execution_count":13,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2753158d-d8f7-4d70-8b61-7b307ff1a253","_uuid":"c27dd3f8918e5f738e474284de64bdc1c9e1ae07","trusted":true},"cell_type":"code","source":"def image_classify(model, pak, img, top_n=3):\n    \"\"\"Classify image and return top matches.\"\"\"\n    target_size = (224, 224)\n    if img.size != target_size:\n        img = img.resize(target_size)\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = pak.preprocess_input(x)\n    preds = model.predict(x)\n    return pak.decode_predictions(preds, top=top_n)[0]\n\n\ndef plot_preds(img, preds_arr):\n    \"\"\"Plot image and its prediction.\"\"\"\n    sns.set_color_codes('pastel')\n    f, axarr = plt.subplots(1, len(preds_arr) + 1, figsize=(20, 5))\n    axarr[0].imshow(img)\n    axarr[0].axis('off')\n    for i in range(len(preds_arr)):\n        _, x_label, y_label = zip(*(preds_arr[i][1]))\n        plt.subplot(1, len(preds_arr) + 1, i + 2)\n        ax = sns.barplot(x=y_label, y=x_label)\n        plt.xlim(0, 1)\n        ax.set()\n        plt.xlabel(preds_arr[i][0])\n    plt.show()\n\n\ndef classify_and_plot(image_path):\n    \"\"\"Classify an image with different models.\n    Plot it and its predicitons.\n    \"\"\"\n    img = Image.open(image_path)\n    resnet_preds = image_classify(resnet_model, resnet50, img)\n    xception_preds = image_classify(xception_model, xception, img)\n    inception_preds = image_classify(inception_model, inception_v3, img)\n    preds_arr = [('Resnet50', resnet_preds), ('xception', xception_preds), ('Inception', inception_preds)]\n    plot_preds(img, preds_arr)","execution_count":14,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"9bd77ff3-6be1-419d-9fdc-964754759277","_uuid":"877b0fa977dcc34bd1dcab5876bc00f6ace513ed","trusted":true},"cell_type":"code","source":"def classify_and_plot0(img):\n    \"\"\"Classify an image with different models.\n    Plot it and its predicitons.\n    \"\"\"\n    resnet_preds = image_classify(resnet_model, resnet50, img)\n    xception_preds = image_classify(xception_model, xception, img)\n    inception_preds = image_classify(inception_model, inception_v3, img)\n    preds_arr = [('Resnet50', resnet_preds), ('xception', xception_preds), ('Inception', inception_preds)]\n    plot_preds(img, preds_arr)","execution_count":15,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"5dc08f22-07df-4fac-aace-22844b04ce80","_uuid":"3544a3bedc0f08b7f33ba6df17bf76128db35ba6","trusted":true},"cell_type":"code","source":"myzip = ZipFile(zip_path)\ndef classify_and_plot_path(img_path):\n    with myzip.open(img_path) as myfile:\n        img = Image.open(myfile)\n        classify_and_plot0(img)","execution_count":16,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b0b28bf3-3f83-4175-934b-c2c03868723c","_uuid":"efc36869645b8afd4ca9c05aaa5166e1b2c59b57","trusted":true},"cell_type":"code","source":"import math\ndef plot_images(imgs):\n    \"\"\"Plot image and its prediction.\"\"\"\n    sns.set_color_codes('pastel')\n    f, axarr = plt.subplots(math.ceil(len(imgs)/4.0), 4, figsize=(20, 5*(len(imgs)//4+1)))\n    axarr= axarr.reshape((-1))\n    for i,img in enumerate(imgs):\n        axarr[i].imshow(img)\n        axarr[i].axis('off')\n\n    plt.show()","execution_count":17,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"1f1eebcc-ea33-4d3b-ab3a-7ef7117677b2","_uuid":"09123e583607672b1818f77cb33f320bf2ed4893","trusted":true},"cell_type":"code","source":"def plot_images_ids(paths):\n    imgs = []\n    for p in paths:\n        myfile = myzip.open('data/competition_files/train_jpg/{}.jpg'.format(p))\n        img = Image.open(myfile)\n        imgs.append(img)\n    plot_images(imgs)","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"ed576161-d84d-4faa-b27a-328b0076f400","_uuid":"25f4b6ca262eaf118e2d8cb394b73493df946529","trusted":true},"cell_type":"code","source":"plot_images_ids(g0['image'][:40])","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"76086fb6dc44929b401938da92f8003db820e016"},"cell_type":"markdown","source":"# group0 maybe the class is earrings."},{"metadata":{"_cell_guid":"ccfde590-f241-40fa-a509-601d5b7f2f87","_uuid":"8706ab58a5591e4112ffb681b6283ce8491648cc","trusted":true},"cell_type":"code","source":"\nfor i in range(0,100,5):\n    p = 'data/competition_files/train_jpg/{}.jpg'.format(g0['image'][i])\n    classify_and_plot_path(p)","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"ed682d7b1b28ee5d490c3586b21ca05b1bb2e215"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"ff8ec799c0f7590096aeb96054038b4c72ecb006"},"cell_type":"markdown","source":"# this class mybe hair slide"},{"metadata":{"trusted":true,"_uuid":"9a32a3fe3558a14e2918b5c2b9a38f63430e43e9"},"cell_type":"code","source":"plot_images_ids(g1['image'][:40])","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"31dfeadf-84ca-4d02-ac4c-40579c11d5e8","_uuid":"e0c041c0ec0f1bc74227f8fbb8d08ffac88564d9","trusted":true},"cell_type":"code","source":"for i in range(0,100,5):\n    p = 'data/competition_files/train_jpg/{}.jpg'.format(g1['image'][i])\n    classify_and_plot_path(p)","execution_count":26,"outputs":[]},{"metadata":{"_cell_guid":"3cae9bc8-45f6-4330-900c-6e803a2ddff2","_uuid":"26246c926a4835d334c437c80d93af161c567119"},"cell_type":"markdown","source":"# Conclusion\n\nimage_top_1 maybe class label that predict by avito, And pretrain model accuracy is a bit low.\n"}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}