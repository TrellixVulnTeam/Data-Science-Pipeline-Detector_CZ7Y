{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import preprocessing ,metrics\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold, cross_validate, GridSearchCV\nimport xgboost as xgb\nimport lightgbm.sklearn as lgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sklearn\nfrom sklearn.utils.testing import all_estimators\nimport sys\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom scipy import sparse as sp\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7061ba024683b5907be803b692b70bc0d28752e9","collapsed":true},"cell_type":"code","source":"\n\nclass wrapped_logger():\n    def __init__(self,log_file_name = 'debug.log'):\n        from logging import getLogger, StreamHandler,DEBUG,Formatter, FileHandler\n        import psutil \n        import time\n        self.time=time.time\n        self.logger = getLogger(__name__)\n        self.START_TIME=self.time()\n        self.psutil=psutil\n\n        handler_format = Formatter(f'%(asctime)s -  %(name)s - %(levelname)s - %(message)s')\n\n        handler = StreamHandler()\n        handler.setFormatter(handler_format)\n        handler.setLevel(DEBUG)\n        file_handler = FileHandler(log_file_name)\n        file_handler.setFormatter(handler_format)\n        file_handler.setLevel(DEBUG)\n        self.logger.setLevel(DEBUG)\n        self.logger.addHandler(handler)\n        self.logger.addHandler(file_handler)\n    def info(self,message):\n        self.logger.info(self.__message(message))\n    def debug(self,message):\n        self.logger.debug(self.__message(message))\n    def warning(self,message):\n        self.logger.warning(self.__message(message))\n        \n    def __message(self,message):\n        return f'{self.time()-self.START_TIME}s - mem usage:{self.psutil.virtual_memory().used/1024/1024} - {message}'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7719640d5b915827f613691ce16cb9958c6043e","collapsed":true},"cell_type":"code","source":"logger=wrapped_logger()\nlogger.info('start logging')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11ddf5ffb48ded3648df3187bda5e59d558746a4","collapsed":true},"cell_type":"code","source":"pd.set_option(\"display.max_rows\",50)\n%env JOBLIB_TEMP_FOLDER=/tmp\nMY_DATASET='../input/avito-demand-prediction-challenge-private-dataset'\nDATASET='../input/avito-demand-prediction'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97541a6954334f493f3f589efd3882fb463d9eed"},"cell_type":"markdown","source":"**List of input files**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"from subprocess import check_output\nprint(check_output([\"ls\", \"../input/\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c0a76ab9cd3b4432f0fc204e9aaeb99cb24bc086"},"cell_type":"code","source":"FRAC = 0.1\ndef read_csv(frac,fanc):\n    data = fanc\n    if FRAC != 1: data = data.sample(frac = FRAC).reset_index(drop=True)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37a39eacdb505c9877050633ae2f9e69bce21e48","collapsed":true},"cell_type":"code","source":"train = read_csv(FRAC,pd.read_csv(f'{DATASET}/train.csv', parse_dates=[\"activation_date\"]))\ntest = read_csv(FRAC,pd.read_csv(f'{DATASET}/test.csv', parse_dates=[\"activation_date\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b17078d07cc4f0559eea6c0904d447bd8c9bb5d","collapsed":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87a547fa1ca66f243ce808667cb88e3f8a960690","collapsed":true},"cell_type":"code","source":"train.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"941819bb75edf59a174c51cd552c87e50eb1dd14","collapsed":true},"cell_type":"code","source":"train.sample(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"191466d8077f1ca575b79b9bd944eeac38300dbf","collapsed":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"97bee6b3f1649d79701c8600e9595ef23189f075"},"cell_type":"code","source":"def fill_na(col_names,train,test,fill_what):\n    for col_name in col_names:\n        train[col_name].fillna(fill_what, inplace=True)\n        test[col_name].fillna(fill_what, inplace=True)\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9b84b2be9fcc4f6f4d10dc6ceccca499e8b20da","collapsed":true},"cell_type":"code","source":"def get_len(col_names):\n    for col_name in col_names:\n        train[f'{col_name}_len'] = train[col_name].apply(lambda x: len(x.split()))\n        test[f'{col_name}_len'] = test[col_name].apply(lambda x: len(x.split()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb3ff2418b80f1679ff82cc803b3a2be20dce910","collapsed":true},"cell_type":"code","source":"#train['city'] = train['city'] + \"_\" + train['region']\n#test['city'] = test['city'] + \"_\" + test['region']\nfull_data = pd.concat([train, test], axis = 0)\ncol_names_fillna = [\n    'description',\n    'title',\n    \"param_1\", \n    \"param_2\", \n    \"param_3\",\n    'city',\n    \"region\",\n    \"parent_category_name\", \n    \"category_name\", \n    \"user_type\"\n    \n]\ntrain,test=fill_na(col_names_fillna,train,test,'NaN')\ntrain,test=fill_na(['price'],train,test,full_data[\"price\"].mean())\ntrain,test=fill_na(['image_top_1'],train,test,full_data[\"image_top_1\"].mode()[0])\n\ncol_names_len = [\n    'description',\n    'title',\n    \"param_1\", \n    \"param_2\", \n    \"param_3\"\n]\n\nget_len(col_names_len)\n\ny_train =train[\"deal_probability\"].ravel()\n\ntest_id = test[\"item_id\"].values\n\ncols_to_drop = [\"item_id\", 'image']\ntrain = train.drop(cols_to_drop + [\"deal_probability\"], axis = 1)\ntest = test.drop(cols_to_drop, axis = 1)\ndel full_data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1cd34d6cf53e1a001ae39e25bcb7a960f001975","scrolled":true,"collapsed":true},"cell_type":"code","source":"def get_svd(fit_data, transform_data, n_comp, col_name):\n    #get svd\n    svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n    svd_obj.fit(fit_data)\n    df_svd = pd.DataFrame(svd_obj.transform(transform_data))\n    df_svd.columns = ['svd_' + col_name + '_'+str(i+1) for i in range(n_comp)]\n    return df_svd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2ebe11369a5f1136d90a456307327a6483a210f","collapsed":true},"cell_type":"code","source":"def get_tfidf_svd(train, test, col_name):\n    #get tfidf\n    tfidf_vec = TfidfVectorizer(ngram_range=(1,1), max_features=100000)\n    full_tfidf = tfidf_vec.fit_transform(pd.concat([train[col_name], test[col_name]], axis=0))\n    train_tfidf = tfidf_vec.transform(train[col_name])\n    test_tfidf = tfidf_vec.transform(test[col_name])\n    \n    #get svd\n    n_comp = 3\n    train_svd = get_svd(full_tfidf, train_tfidf, n_comp,col_name)\n    test_svd = get_svd(full_tfidf, test_tfidf, n_comp,col_name)\n    \n    #clean\n    del full_tfidf, train_tfidf, test_tfidf, tfidf_vec\n    \n    return train_svd, test_svd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ceda0ea2df816a10c0e2cb1274f552c77d56bb4","collapsed":true},"cell_type":"code","source":"def engineering_tfidf(how,col_name):\n    if how == 'read': \n        logger.info(f'Read {col_name}')\n        train_svd = read_csv(FRAC,pd.read_csv(f'{MY_DATASET}/train_{col_name}_svd.csv'))\n        test_svd = read_csv(FRAC,pd.read_csv(f'{MY_DATASET}/test_{col_name}_svd.csv'))\n    else:\n        train_svd, test_svd = get_tfidf_svd(train = train, test = test, col_name = col_name)\n        train_svd.to_csv(f'train_{col_name}_svd.csv',index=False)\n        test_svd.to_csv(f'test_{col_name}_svd.csv',index=False)\n    return train_svd, test_svd\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8e46534285250fe3d33d5025686ea6989f82d0a","collapsed":true},"cell_type":"code","source":"train_title_svd, test_title_svd = engineering_tfidf('read','title')\ntrain_description_svd, test_description_svd = engineering_tfidf('read','description')\n\ntrain = pd.concat([train, train_title_svd,train_description_svd], axis=1)\ntest = pd.concat([test, test_title_svd,test_description_svd], axis=1)\n\ncols_to_drop = ['description', 'title']\ntrain = train.drop(cols_to_drop, axis = 1)\ntest = test.drop(cols_to_drop, axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f95ea46a37015fc257cb11c64326c18be239b1f","collapsed":true},"cell_type":"code","source":"def num_to_cat(col_names,train,test,n_comp=-1):\n    \n    for col_name in col_names:\n        full_col = pd.concat([train[col_name], test[col_name]])\n        n_unique=full_col.nunique(dropna=False)\n        le = preprocessing.LabelEncoder()\n        oe = preprocessing.OneHotEncoder()\n        le.fit(full_col.values.astype('str'))\n\n        train_le = le.transform(train[col_name].values.astype('str')).reshape(-1,1)\n        test_le = le.transform(test[col_name].values.astype('str')).reshape(-1,1)\n        full_le = np.append(train_le , test_le).reshape(-1,1)\n        oe.fit(full_le)\n        train_oe = oe.transform(train_le)\n        test_oe = oe.transform(test_le)\n        \n        full_oe =sp.vstack((train_oe  ,test_oe))\n        if n_unique <= n_comp or n_comp == -1: \n            col_names_svd=[f'le_{col_name}_{i}' for i in range(1, n_unique+1) ]\n            train_svd=pd.SparseDataFrame(train_oe, columns=col_names_svd)\n            test_svd=pd.SparseDataFrame(test_oe, columns=col_names_svd)\n            #train_svd,test_svd=fill_na(col_names_svd,train_svd,test_svd,0)\n        else:\n            train_svd=get_svd(full_oe, train_oe, n_comp, col_name)\n            test_svd=get_svd(full_oe, test_oe, n_comp, col_name)\n        train = pd.concat([train,train_svd], axis=1)\n        test = pd.concat([test, test_svd], axis=1)\n        \n        train = train.drop(col_name, axis = 1)\n        test = test.drop(col_name, axis = 1)\n\n    return train,test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54439880f10ea07f4b1141a9cd9095a891e181ff","collapsed":true},"cell_type":"code","source":"def num_to_label(col_names,train,test):\n    for col in col_names:\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n        train[col] = lbl.transform(list(train[col].values.astype('str')))\n        test[col] = lbl.transform(list(test[col].values.astype('str')))\n    return train,test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ab5a48c78d2f6358fb51b1907bcd53174e8699f","collapsed":true},"cell_type":"code","source":"train[\"activation_weekday\"] = train[\"activation_date\"].dt.weekday\ntest[\"activation_weekday\"] = test[\"activation_date\"].dt.weekday\ntrain[\"activation_month\"] = train[\"activation_date\"].dt.month\ntest[\"activation_month\"] = test[\"activation_date\"].dt.month\ncols_to_drop = [\n    \"activation_date\",\n    'user_id'\n]\ntrain = train.drop(cols_to_drop, axis = 1)\ntest = test.drop(cols_to_drop, axis = 1)\n#train.to_sparse(fill_value=0)\n#test.to_sparse(fill_value=0)\ncol_vars = [\n    #'user_id',\n    \"region\", \n    \"city\", \n    \"parent_category_name\", \n    \"category_name\", \n    \"user_type\", \n    \"param_1\", \n    \"param_2\", \n    \"param_3\"\n]\n#train = pd.get_dummies(train, columns = col_vars, dtype = 'int64')\n#test = pd.get_dummies(test, columns = col_vars, dtype = 'int64')\n#train,test=num_to_cat(col_vars,train,test)\ntrain,test=num_to_label(col_vars,train,test)\n\ntrain.info()\ntrain = train.to_sparse(fill_value=0)\ntest=test.to_sparse(fill_value=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0518d4f8a5e3fcac299d0015e75d7474a1457055"},"cell_type":"code","source":"def get_zscore(train,test,col_names):\n    full_data = pd.concat([train, test], axis = 0)\n    for col in col_names:\n        #full_data[col] =zscore(full_data[col])\n        full_data[col] =((full_data[col]-full_data[col].mean())/full_data[col].std())\n        train[col]=full_data[col][:train.shape[0]]\n        test[col] =full_data[col][train.shape[0]:]\n        \n    del full_data\n\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ca896f260f7a1d25cb6ee016da6a953d4bea4bf","collapsed":true},"cell_type":"code","source":"zscore_cols=[\n    'price',\n\n]\ntrain,test=get_zscore(train,test,zscore_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a857d426f73b24a8faf909ac9d0db1bed27ccabb","collapsed":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c9587054212c80d460688c44992f7eb280bd15c","collapsed":true},"cell_type":"code","source":"x_train = train.to_coo().tocsr()\nx_test = test.to_coo().tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"208d4684623d96bf09e3c1fd7e72426c20b1eb04","collapsed":true},"cell_type":"code","source":"# Some useful parameters which will come in handy later on\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNSPLITS = 5 # set folds for out-of-fold prediction\nkf = KFold(n_splits= NSPLITS, random_state=SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None, no_seed=False):\n        params['random_state'] = seed \n        if no_seed ==  True: del params['random_state']\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a7589145e90dbd0b74c21a7fe6e8a8c3eef5c07d"},"cell_type":"code","source":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NSPLITS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf.split(train)):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91f9442350c98386d74bbd66724b881813fa20a7","collapsed":true},"cell_type":"code","source":"\n\n# ElasticNet Regression Parameters\nenet_params = {\n    'n_estimators': 200,\n    'n_jobs': 4\n}\n\n# Lasso Regression parameters\nlasso_params = {\n    'min_samples_leaf': 2, \n    'min_samples_split': 3, \n    'n_estimators': 15, \n    'n_jobs': 4\n}\n\n# Ridge Regression parameters\nridge_params = {\n    'min_samples_leaf': 2, \n    'min_samples_split': 2, \n    'n_estimators': 15, \n    'n_jobs': 4\n}\n\n# Random Forest Regression parameters \nrf_params = {\n    'max_features': 1.0, \n    'max_samples': 0.5, \n    'n_estimators': 15, \n    'n_jobs': 4\n}\n    \n\nada_params={\n    \n}\nrd_params={\n    \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"961a44fb14faf10535a49eb3633b8612d9454310","collapsed":true},"cell_type":"code","source":"lgbm = SklearnHelper(clf=lgb.LGBMRegressor, seed=SEED, params=enet_params)\nrf = SklearnHelper(clf=sklearn.ensemble.forest.RandomForestRegressor, seed=SEED, params=lasso_params)\netr = SklearnHelper(clf=sklearn.ensemble.forest.ExtraTreesRegressor, seed=SEED, params=ridge_params)\nbr = SklearnHelper(clf=sklearn.ensemble.bagging.BaggingRegressor, seed=SEED, params=rf_params)\nada = SklearnHelper(clf=sklearn.ensemble.weight_boosting.AdaBoostRegressor, seed=SEED, params=ada_params)\nrd = SklearnHelper(clf=sklearn.linear_model.ridge.Ridge, seed=SEED, params=rd_params)\n\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"caf6f83e248befdff74e201487b74b4152b7ee74"},"cell_type":"code","source":"def engineering_oof(how,model_name,func,*args):\n    if how == 'read':\n        logger.info(f'start reading {model_name}*.csv')\n        oof_train = read_csv(FRAC,pd.DataFrame(np.loadtxt(f'{MY_DATASET}/{model_name}_train.csv',delimiter=','))).values\n        oof_test = read_csv(FRAC,pd.DataFrame(np.loadtxt(f'{MY_DATASET}/{model_name}_test.csv',delimiter=','))).values\n        logger.info(f'finish reading {model_name}*.csv')\n    else:\n        logger.info(f'start writing {model_name}*.csv')\n        oof_train, oof_test = func(*args)\n        if FRAC ==1:\n            np.savetxt(f'{model_name}_train.csv',oof_train,delimiter=',')\n            np.savetxt(f'{model_name}_test.csv',oof_test,delimiter=',')\n            logger.info(f'finish writing {model_name}*.csv')\n        else:\n            logger.info(f'skip writing {model_name}*.csv  FRAC!=1')\n\n    return  sp.csr_matrix(oof_train), sp.csr_matrix(oof_test)\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"910938f5293a1a3a751b23e5542a2005dc3e7693","collapsed":true},"cell_type":"code","source":"\n\n# Create our OOF train and test predictions. These base results will be used as new features\nlgbm_oof_train,lgbm_oof_test = engineering_oof('read', 'LightGBMRegressor',get_oof,*(lgbm,x_train, y_train, x_test))\nrf_oof_train,rf_oof_test = engineering_oof('read', 'RandomForestRegressor',get_oof,*(rf,x_train, y_train, x_test))\netr_oof_train,etr_oof_test = engineering_oof('read', 'ExtraTreesRegressor',get_oof,*(etr,x_train, y_train, x_test))\nbr_oof_train,br_oof_test = engineering_oof('read', 'BaggingRegressor',get_oof,*(br,x_train, y_train, x_test))\nnn_oof_train,nn_oof_test = engineering_oof('read', 'dae_nn',logger.debug,*(f'only reading'))\nada_oof_train,ada_oof_test = engineering_oof('read', 'AdaBoostRegressor',get_oof,*(ada,x_train, y_train, x_test))\nrd_oof_train,rd_oof_test = engineering_oof('read', 'Ridge',get_oof,*(rd,x_train, y_train, x_test))\ngd_oof_train,gd_oof_test =engineering_oof('read', 'GradientBoostingRegressor',logger.debug,*(f'only reading'))\nprint(\"Training is complete\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b380c352fd18953d4a913d6b3efa03aa3745f4e","collapsed":true},"cell_type":"code","source":"\nx_train = sp.hstack([\n    x_train, \n    lgbm_oof_train, \n    rf_oof_train,\n    etr_oof_train,\n    br_oof_train,\n    nn_oof_train,\n    ada_oof_train,\n    rd_oof_train,\n    gd_oof_train\n])\nx_test = sp.hstack([\n    x_test,\n    lgbm_oof_test,\n    rf_oof_test,\n    etr_oof_test,\n    br_oof_test,\n    nn_oof_test,\n    ada_oof_test,\n    rd_oof_test,\n    gd_oof_test\n])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7a8f88d6c8c371764bed673988d2e9f4bcfbf98","collapsed":true},"cell_type":"code","source":"x_train=x_train.tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fd955a6b896fd08d71ac90e7e6d7d59db6c72036"},"cell_type":"code","source":"def RMSLE(y, pred):\n    return metrics.mean_squared_error(y, pred)**0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"8e1bffff22f04484eb75d3980fafc5fc43aa3db4","collapsed":true},"cell_type":"code","source":"'''\nfrom catboost import CatBoostRegressor \nnum_fold=5\ny_test = np.zeros([num_fold, x_test.shape[0]])\ny_valid = np.zeros([x_train.shape[0]])\nfolds = list(KFold(n_splits=num_fold, shuffle=True, random_state=42).split(x_train))\nfor j, (ids_train_split, ids_valid_split) in enumerate(folds):\n    print(\"fold\", j+1, \"==================\")\n    cbr=CatBoostRegressor(loss_function='RMSE')\n    cat_cols=[0,1,2,3,4,5,6,8,9,10,36,37]\n    cbr.fit(x_train[ids_train_split].toarray(),y_train[ids_train_split],cat_cols)\n\n    \n    # Predict on train, val and test\n    y_valid[ids_valid_split] = cbr.predict(x_train[ids_valid_split].toarray())\n    y_test[j] = cbr.predict(x_test.toarray())\n\nscore = RMSLE(y_valid, y_train)\nlogger.info(f'valid score: {score}')\ny_test_mean = np.mean(y_test, axis=0)\ny_test_mean[y_test_mean>1] = 1\ny_test_mean[y_test_mean<0] = 0\nSubmission = pd.DataFrame({\"item_id\":test_id, 'deal_probability': y_test_mean})\nnp.savetxt(f'CatBoostRegressor_train.csv',y_valid,delimiter=',')\nnp.savetxt(f'CatBoostRegressor_test.csv',y_test_mean,delimiter=',')\n\nSubmission.to_csv('Submission.csv', index=False)\nlogger.info(f'finished')\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a05a9b5979bac8d897c63fa0bfff8a490e74465e","collapsed":true},"cell_type":"code","source":"import lightgbm \nnum_fold=10\ny_test = np.zeros([num_fold, x_test.shape[0]])\ny_valid = np.zeros([x_train.shape[0]])\nfolds = list(KFold(n_splits=num_fold, shuffle=True, random_state=42).split(x_train))\nfor j, (ids_train_split, ids_valid_split) in enumerate(folds):\n    print(\"fold\", j+1, \"==================\")\n    dat=lightgbm.Dataset(\n        x_train[ids_train_split],\n        label=y_train[ids_train_split],\n        categorical_feature=[0,1,2,3,4,5,6,8,9,10,36,37]\n    )\n    params={\n        #'max_bin':500,\n        #'learning_rate': 0.05,\n        #'num_iterations':200,\n        'n_estimators': 200,\n        'n_jobs': 4,\n        #'boosting': 'dart'\n    }\n    lgbm = lightgbm.train(params,dat)\n    \n    # Predict on train, val and test\n    y_valid[ids_valid_split] = lgbm.predict(x_train[ids_valid_split])\n    y_test[j] = lgbm.predict(x_test)\n\nscore = RMSLE(y_valid, y_train)\nlogger.info(f'valid score: {score}')\ny_test_mean = np.mean(y_test, axis=0)\ny_test_mean[y_test_mean>1] = 1\ny_test_mean[y_test_mean<0] = 0\nSubmission = pd.DataFrame({\"item_id\":test_id, 'deal_probability': y_test_mean})\n#np.savetxt(f'LightGBMRegressor_train.csv',y_valid,delimiter=',')\n#np.savetxt(f'LightGBMRegressor_test.csv',y_test_mean,delimiter=',')\n\nSubmission.to_csv('Submission.csv', index=False)\nlogger.info(f'finished')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99ce72420a41ad301fbfdf0cbf360e359addda55","collapsed":true},"cell_type":"code","source":"\nSubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5bf58427560c3b51098991247ecd729c6329422","collapsed":true},"cell_type":"code","source":"Submission.describe()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}