{"cells":[{"metadata":{"_uuid":"24969606d29c12150a1408915bccd6aed24f8aac"},"cell_type":"markdown","source":"# Review your feature before modeling.\n\nEvery kaggler much know that a good feature engineering is the key to get a best performance in your problem.\n\nThis is a brefly introduce my approce to review a feature before start modeliing.\n\nBasically,  linear model and tree base model are mostly choosen, so we'd better consider pearsonr correlation coefficient with target values for linear model, \nand check the gini, information gain, entropy with feature and target values  for building a tree base model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy.stats as st\nfrom sklearn.preprocessing import Imputer\nfrom textblob import TextBlob\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"tr = pd.read_csv('../input/train.csv', index_col='item_id', parse_dates=['activation_date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dbd91bfcba24153cb4f9b8c9bcb3577acfe422c","collapsed":true},"cell_type":"code","source":"tr.info()","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c64887bc964dc8fb6a22f121840573c6ab0ebf9","collapsed":true},"cell_type":"code","source":"## to see the uniquen number of each feature\nfor c in tr.columns:\n    print(\"%20s\"%c + \"\\t\" + str(len(tr[c].unique())))","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5a70b75b2bb44e2ee24fc8f9a445d27f7862a5b6"},"cell_type":"code","source":"tr['weekday'] = tr.activation_date.dt.weekday","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9cb7f38f71e51ef5db1be238aa23457154b53f4","collapsed":true},"cell_type":"code","source":"tr.activation_date.value_counts().sort_index()","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9007eec9effb9f22805246de77abb3ee75cf2d3e","collapsed":true},"cell_type":"code","source":"date_distribution = tr.activation_date.value_counts().sort_index()\ndate_distribution.index = [x.strftime(\"%m-%d\") for x in date_distribution.index]\ndate_distribution.plot.bar(figsize=(10,2))","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c761a5648b0e92703fc506b53c171159f032c8a","collapsed":true},"cell_type":"code","source":"date_target_stats = tr.groupby('activation_date')['deal_probability']\\\n                      .agg(['count','mean','std','var']).sort_index()\ndisplay(date_target_stats)\ndate_target_stats.index = [x.strftime(\"%m-%d\") for x in date_target_stats.index]\ndate_target_stats['mean'].plot.bar(figsize=(10,2))\nplt.show()\ndate_target_stats['std'].plot.bar(figsize=(10,2))\nplt.show()","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"361ef2ea093c05cc1b3add5876f623cf62580d34","collapsed":true},"cell_type":"code","source":"date_target_stats['std'].plot.hist(figsize=(10,3),bins=100)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"90d1b01f70f2c5cbaa4fb44937d2b61c293db96c"},"cell_type":"code","source":"normal_tr = tr.loc[tr.activation_date < pd.to_datetime('2017-03-29'), :] # ignore these special date data.","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"961c43783f9606397563d66de476c9258a6988e4"},"cell_type":"code","source":"# define some temp variable for store stats information. e.g: \nstat_map = {} # stats result\nentr_map = {} # each feature's entropy\ngini_map = {} # each feature's gini\ninfo_map = {} # each feature's information gain with target value\ncorr_map = {} # each feature's corr with target value","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a245df43a6e70c3800315f3709fe75428e549365","collapsed":true},"cell_type":"code","source":"from skfeature.utility.mutual_information import information_gain as info_gain\nfrom skfeature.utility.entropy_estimators import entropy","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d09ab578bb849091729492a7a68835c66ca29a47"},"cell_type":"code","source":"def plt_bar(stats, y):\n    stats.plot.bar(y='count',figsize=(10,3))\n    plt.legend('')\n    plt.xlabel('')\n    plt.title(y)\n    plt.show()\n\n\ndef caculate_gini(array):\n    \"\"\"Calculate the Gini coefficient of a numpy array.\"\"\"\n    # based on bottom eq:\n    # http://www.statsdirect.com/help/generatedimages/equations/equation154.svg\n    # from:\n    # http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n    # All values are treated equally, arrays must be 1d:\n    array = array.flatten()\n    if np.amin(array) < 0:\n        # Values cannot be negative:\n        array -= np.amin(array)\n    # Values cannot be 0:\n    array += 0.0000001\n    # Values must be sorted:\n    array = np.sort(array)\n    # Index per array element:\n    index = np.arange(1,array.shape[0]+1)\n    # Number of array elements:\n    n = array.shape[0]\n    # Gini coefficient:\n    return ((np.sum((2 * index - n  - 1) * array)) / (n * np.sum(array)))\n\n\ndef stats_view(df, cate, target, p=75, show_common=True):\n    df = df[~df[target].isnull()]\n    df.loc[df[cate].isnull(),cate] = 'NAN'\n    df[cate] = df[cate].astype('str')\n    \n    print('-------------------------------------------------------------------------')\n    y = df.deal_probability\n    ncat = len(df[cate].unique())\n    print(20*\"=\"+\"%20s\"%cate+\"\\t\"+\"%20s \"%target+\"=\"*20)\n    \n    if show_common:\n        print('ncat:%.2f'%ncat)\n        \n        catv = pd.factorize(df[cate].fillna('NAN'))[0].astype(float)\n        entr = st.entropy(catv)\n        gini = caculate_gini(catv)\n        info = info_gain(catv, y)\n        corr = st.pearsonr(catv, y)[0]\n        print('fact entr:%.6f'%entr)\n        print('fact gini:%.6f'%gini)\n        print('fact info:%.6f'%info)\n        print('fact corr:%.6f'%corr)\n        \n        entr_map[cate+\" fact\"] = entr\n        gini_map[cate+\" fact\"] = gini\n        info_map[cate+\" fact\"] = info\n        corr_map[cate+\" fact\"] = corr\n        \n        freq = df[cate].value_counts()/df.shape[0]\n        catf = df[cate].map(freq).values\n        entr = st.entropy(catf)\n        gini = caculate_gini(catf)\n        info = info_gain(catf, y)\n        corr = st.pearsonr(catf, y)[0]\n        print('freq entr:%.6f'%entr)\n        print('freq gini:%.6f'%gini)\n        print('freq info:%.6f'%info)\n        print('freq corr:%.6f'%corr)\n        entr_map[cate+\" freq\"] = entr\n        gini_map[cate+\" freq\"] = gini\n        info_map[cate+\" freq\"] = info\n        corr_map[cate+\" freq\"] = corr\n        \n    \n    target_gp = df.groupby(cate)[target]\n    stats = target_gp.agg(['count', 'mean', 'std', 'var']).sort_index()\n    stats['target_p%d'%p] = target_gp.agg(lambda x:np.percentile(x,p))\n    stats['uv'] = df.groupby(cate)['user_id'].agg(lambda x: len(np.unique(x)))\n    stats['puv'] = stats['uv'] / stats['count']\n    stats = stats.fillna(0)\n    display(stats.head(50))\n    \n    \n    base_ent = st.entropy(y)\n    for c in stats.columns:\n        if not show_common and c in ['count', 'uv', 'puv']:\n            continue\n        \n        dmap = stats[c].to_dict() \n        x = df[cate].apply(lambda x:dmap.get(x,-1))\n        n_miss = x.isnull().sum()\n        if  n_miss > len(df)*0.8:\n            print(\"Stats {} miss value too much, ignored.\".format(c), n_miss)\n            continue\n        \n        entr = st.entropy(stats[c])\n        gini = caculate_gini(x.astype(np.float).values)\n        info = info_gain(x.astype(np.float).values, y.values)\n        corr = st.pearsonr(x, y)[0]\n        \n        entr_map[cate+\" \"+ c] = entr\n        gini_map[cate+\" \"+ c] = gini\n        info_map[cate+\" \"+ c] = info\n        corr_map[cate+\" \"+ c] = corr\n        \n        print(\"{} {} {} entropy:{}, gini:{}, information_gain:{}  corrcoef:{}\"\n              .format(cate, target, c, round(entr,6), round(gini,6),round(info, 6), round(corr,6)))\n        if ncat>20:\n            plt_bar(stats[c].sort_values(ascending=False).head(20), c)\n        else:\n            plt_bar(stats[c].sort_values(ascending=False), c)\n    stat_map[cate+\" \"+ target] = stats","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95d52ab456854da8b0d75ae1f086db1737671c87","collapsed":true},"cell_type":"code","source":"# 原始统计特征分析\nfor c in ['image_top_1','region', 'city', 'parent_category_name', 'category_name', 'param_1',\n          'param_2', 'param_3', 'item_seq_number', 'user_type', 'weekday']:\n    stats_view(normal_tr, c, 'deal_probability') # analysis the target value and categorical feature.\n    print(100 * \"=\")\n    if c != 'image_top_1':\n        stats_view(normal_tr, c, 'image_top_1', show_common=False) # analysis image_top_1 value and categorical feature.\n        print(100 * \"=\")\n    stats_view(normal_tr, c, 'price', show_common=False) # analysis price value and categorical feature.\n    print(100 * \"=\")","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"290e790056635b456ae8809017fb72dd2d96b4a2"},"cell_type":"markdown","source":"# Results"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0dfc28fd3fd50326f969d8058b6c15722e5590ca"},"cell_type":"code","source":"base_feature_stats = pd.DataFrame([\n    pd.Series(entr_map, name='entripy'),\n    pd.Series(gini_map, name='gini'),\n    pd.Series(info_map, name='information gain'),\n    pd.Series(corr_map, name='corr')\n]).T\nbase_feature_stats['abs_corr'] = np.abs(base_feature_stats['corr'])\npd.set_option('display.max_rows',1000)\npd.options.display.float_format = '{:,.4f}'.format\ndisplay(base_feature_stats)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1fad326265dac9cf1184c39fe671444c1ae05cdb"},"cell_type":"markdown","source":"### You can quickly check your new features with my approach, welcome discuss here other efficient methods to check new feature's performance."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"02e6d4830d14c9d52dbaf59626ed00d077ea224d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a48b74953029b1620f094b21130e2d076caabdc"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"833ce0e9d87924d67e736e52370bade830384d63"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}