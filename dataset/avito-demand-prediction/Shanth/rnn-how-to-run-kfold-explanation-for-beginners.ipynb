{"cells":[{"metadata":{"_uuid":"b0c1f05aac25d8effe9bc441a31bd8527ff6fa9c"},"cell_type":"markdown","source":"# **OBJECTIVE OF KERNEL **\n\nThe kernel takes a step by step approach on how to run a SKLEARN KFOLD with RNN.  \n\n* I faced a variety of array mismatch issues  with the title_description columns while trying to make KFOLD work. \n* While the other variables where of shape (n,1), the title_description was of shape (n,100) where 100 was the max length \n   of the sequences of the title and description texts. \n\nI guessed some beginners may face the same issue. So this kernel provides one approach to work around this issue to run the KFOLD. ****"},{"metadata":{"_uuid":"7d303db39bc1ac6110b472d39af4152e2234e1d4"},"cell_type":"markdown","source":"# **WORK FLOW FOR KFOLD ****\n\n*   Generate train data frame with transformations for features \n*   Get train.values as array as input for KFOLD split \n*   Split train.values into train and test using sklearn KFOLD \n*   Arrange each column of the train and test arrays into a data frame for X_train and X_test ( A function is used )\n*  Convert the data frame into a Dictionary with relevant columns as \"Keys\"\n*   Use these Train and Test dictionary as inputs to the RNN "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport time \nimport gc \n\nnp.random.seed(42)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dropout, Dense, Embedding, SpatialDropout1D, concatenate\nfrom keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, BatchNormalization, Conv1D, MaxPooling1D, Flatten\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import Callback\nfrom keras import backend as K\nfrom keras.models import Model\n\nfrom keras import optimizers\n\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nos.environ['OMP_NUM_THREADS'] = '4'\n\nimport threading\nimport multiprocessing\nfrom multiprocessing import Pool, cpu_count\nfrom contextlib import closing\ncores = 4\n\nfrom keras import backend as K\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\n### rmse loss for keras\ndef root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_true - y_pred))) ","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"863ef916322f6e798dbb1a150b982b4a9dfca2d8"},"cell_type":"markdown","source":"# **GENERATE TRAIN DATA FRAME WITH FEATURES****"},{"metadata":{"trusted":true,"_uuid":"bec6d0c8b14b6288c090437320c2f31ccb5c3a72"},"cell_type":"code","source":"def preprocess_dataset(dataset):\n    \n    t1 = time.time()\n    print(\"Filling Missing Values.....\")\n    \n    dataset['price'] = dataset['price'].fillna(0).astype('float32')\n    print(\"Casting data types to type Category.......\")\n    dataset['category_name'] = dataset['category_name'].astype('category')\n    dataset['parent_category_name'] = dataset['parent_category_name'].astype('category')\n    dataset['region'] = dataset['region'].astype('category')\n    dataset['city'] = dataset['city'].astype('category')\n    print(\"PreProcessing Function completed.\")\n    \n    return dataset\n\ndef keras_fit(train):\n    \n    t1 = time.time()\n    train['title_description']= (train['title']+\" \"+train['description']).astype(str)\n    del train['description'], train['title']\n    gc.collect()\n    \n    print(\"Start Tokenization.....\")\n    tokenizer = text.Tokenizer(num_words = max_words_title_description)\n    all_text = np.hstack([train['title_description'].str.lower()])\n    tokenizer.fit_on_texts(all_text)\n    del all_text\n    gc.collect()\n    \n    print(\"Loading Test for Label Encoding on Train + Test\")\n    use_cols_test = ['region', 'city', 'parent_category_name', 'category_name', 'title', 'description', 'price']\n    test = pd.read_csv(\"../input/avito-demand-prediction/test.csv\", usecols = use_cols_test)\n\n    ntrain = train.shape[0]\n    DF = pd.concat([train, test], axis = 0)\n    del train, test\n    gc.collect()\n    print(DF.shape)\n    \n    print(\"Start Label Encoding process....\")\n    le_region = LabelEncoder()\n    le_region.fit(DF.region)\n    \n    le_city = LabelEncoder()\n    le_city.fit(DF.city)\n    \n    le_category_name = LabelEncoder()\n    le_category_name.fit(DF.category_name)\n    \n    le_parent_category_name = LabelEncoder()\n    le_parent_category_name.fit(DF.parent_category_name)\n\n    train = DF[0:ntrain]\n    del DF \n    gc.collect()\n    \n    train['price'] = np.log1p(train['price'])\n    \n    return train, tokenizer, le_region, le_city, le_category_name, le_parent_category_name\n\ndef keras_train_transform(dataset):\n    \n    t1 = time.time()\n    \n    dataset['seq_title_description']= tokenizer.texts_to_sequences(dataset.title_description.str.lower())\n    print(\"Transform done for test\")\n    print(\"Time taken for Sequence Tokens is\"+str(time.time()-t1))\n    del train['title_description']\n    gc.collect()\n\n    dataset['region'] = le_region.transform(dataset['region'])\n    dataset['city'] = le_city.transform(dataset['city'])\n    dataset['category_name'] = le_category_name.transform(dataset['category_name'])\n    dataset['parent_category_name'] = le_parent_category_name.transform(dataset['parent_category_name'])\n    print(\"Transform on test function completed.\")\n    \n    return dataset\n    \ndef keras_test_transform(dataset):\n    \n    t1 = time.time()\n    dataset['title_description']= (dataset['title']+\" \"+dataset['description']).astype(str)\n    del dataset['description'], dataset['title']\n    gc.collect()\n    \n    dataset['seq_title_description']= tokenizer.texts_to_sequences(dataset.title_description.str.lower())\n    print(\"Transform done for test\")\n    print(\"Time taken for Sequence Tokens is\"+str(time.time()-t1))\n    \n    del dataset['title_description']\n    gc.collect()\n\n    dataset['region'] = le_region.transform(dataset['region'])\n    dataset['city'] = le_city.transform(dataset['city'])\n    dataset['category_name'] = le_category_name.transform(dataset['category_name'])\n    dataset['parent_category_name'] = le_parent_category_name.transform(dataset['parent_category_name'])\n    dataset['price'] = np.log1p(dataset['price'])\n    \n    return dataset\n    \ndef get_keras_data(dataset):\n    X = {\n        'seq_title_description': pad_sequences(dataset.seq_title_description, maxlen=max_seq_title_description_length)\n        ,'region': np.array(dataset.region)\n        ,'city': np.array(dataset.city)\n        ,'category_name': np.array(dataset.category_name)\n        ,'parent_category_name': np.array(dataset.parent_category_name)\n        ,'price': np.array(dataset[[\"price\"]])\n\n    }\n    \n    print(\"Data ready for Vectorization\")\n    \n    return X\n\n# Loading Train data - No Params, No Image data \ndtypes_train = {\n                'price': 'float32',\n                'deal probability': 'float32',\n}\n\n# No user_id\nuse_cols = ['region', 'city', 'parent_category_name', 'category_name', 'title', 'description', 'price','deal_probability']\ntrain = pd.read_csv(\"../input/avito-demand-prediction/train.csv\", usecols = use_cols, dtype = dtypes_train, nrows = 10000)\n\ny_train = np.array(train['deal_probability'])\ndel train['deal_probability']\ngc.collect()\n\nmax_seq_title_description_length = 100\nmax_words_title_description = 200000\n\ntrain = preprocess_dataset(train)\ntrain, tokenizer, le_region, le_city, le_category_name, le_parent_category_name = keras_fit(train)\ntrain = keras_train_transform(train)\nprint(\"Tokenization done and TRAIN READY FOR Validation splitting\")\n\n# Calculation of max values for Categorical fields \n\nmax_region = np.max(train.region.max())+2\nmax_city= np.max(train.city.max())+2\nmax_category_name = np.max(train.category_name.max())+2\nmax_parent_category_name = np.max(train.parent_category_name.max())+2\n","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"9532bbc32979ace257e0a25336add8f554bee051"},"cell_type":"markdown","source":"# **** GENERATE EMBEDDING VECTORS ****"},{"metadata":{"trusted":true,"_uuid":"7ec6b72ba2f50b36c7e30222010d6e5801b167a4"},"cell_type":"code","source":"EMBEDDING_DIM1 = 300\nEMBEDDING_FILE1 = '../input/fasttest-common-crawl-russian/cc.ru.300.vec'\ndef get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index1 = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE1))\n\nvocab_size = len(tokenizer.word_index)+2\nEMBEDDING_DIM1 = 300# this is from the pretrained vectors\nembedding_matrix1 = np.zeros((vocab_size, EMBEDDING_DIM1))\nprint(embedding_matrix1.shape)\n# Creating Embedding matrix \nc = 0 \nc1 = 0 \nw_Y = []\nw_No = []\nfor word, i in tokenizer.word_index.items():\n    if word in embeddings_index1:\n        c +=1\n        embedding_vector = embeddings_index1[word]\n        w_Y.append(word)\n    else:\n        embedding_vector = None\n        w_No.append(word)\n        c1 +=1\n    if embedding_vector is not None:    \n        embedding_matrix1[i] = embedding_vector\n\nprint(c,c1, len(w_No), len(w_Y))\nprint(embedding_matrix1.shape)\ndel embeddings_index1\ngc.collect()\n\nprint(\" FAST TEXT DONE\")","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"78f8b4be7937ba6e0c7e5b175b436ef6d3b7a26e"},"cell_type":"markdown","source":"# ** FUNCTION FOR RNN MODEL GENERATION ****"},{"metadata":{"_uuid":"e74c68e6105ce951262e9f50b7da365660e6c59e","_cell_guid":"c0a3837a-e7ca-4618-a672-ba40210d005f","collapsed":true,"trusted":true},"cell_type":"code","source":"def RNN_model():\n\n    #Inputs\n    seq_title_description = Input(shape=[100], name=\"seq_title_description\")\n    region = Input(shape=[1], name=\"region\")\n    city = Input(shape=[1], name=\"city\")\n    category_name = Input(shape=[1], name=\"category_name\")\n    parent_category_name = Input(shape=[1], name=\"parent_category_name\")\n    price = Input(shape=[1], name=\"price\")\n    \n    #Embeddings layers\n\n    emb_seq_title_description = Embedding(vocab_size, EMBEDDING_DIM1, weights = [embedding_matrix1], trainable = False)(seq_title_description)\n    emb_region = Embedding(vocab_size, 10)(region)\n    emb_city = Embedding(vocab_size, 10)(city)\n    emb_category_name = Embedding(vocab_size, 10)(category_name)\n    emb_parent_category_name = Embedding(vocab_size, 10)(parent_category_name)\n    \n    rnn_layer1 = GRU(50) (emb_seq_title_description)\n    \n    #main layer\n    main_l = concatenate([\n          rnn_layer1\n        , Flatten() (emb_region)\n        , Flatten() (emb_city)\n        , Flatten() (emb_category_name)\n        , Flatten() (emb_parent_category_name)\n        , price\n    ])\n    \n    main_l = Dropout(0.1)(Dense(512,activation='relu') (main_l))\n    main_l = Dropout(0.1)(Dense(64,activation='relu') (main_l))\n    \n    #output\n    output = Dense(1,activation=\"sigmoid\") (main_l)\n    \n    #model\n    model = Model([seq_title_description, region, city, category_name, parent_category_name, price], output)\n    model.compile(optimizer = 'adam',\n                  loss= root_mean_squared_error,\n                  metrics = [root_mean_squared_error])\n    return model\n\ndef rmse(y, y_pred):\n\n    Rsum = np.sum((y - y_pred)**2)\n    n = y.shape[0]\n    RMSE = np.sqrt(Rsum/n)\n    return RMSE \n\ndef eval_model(model, X_test1):\n    val_preds = model.predict(X_test1)\n    y_pred = val_preds[:, 0]\n    \n    y_true = np.array(y_test1)\n    \n    yt = pd.DataFrame(y_true)\n    yp = pd.DataFrame(y_pred)\n    \n    print(yt.isnull().any())\n    print(yp.isnull().any())\n    \n    v_rmse = rmse(y_true, y_pred)\n    print(\" RMSE for VALIDATION SET: \"+str(v_rmse))\n    return v_rmse\n\nexp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"4582968f48f2d7b740505742de921fd03efa2a56"},"cell_type":"markdown","source":"# ** FUNCTION TO GENERATE PREDICTIONS FOR EACH KFOLD ****\n \n The function will be called after model training within each FOLD to generate the predictions for that fold. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dc5ded2811f4059b015dc87851758e8b563448de"},"cell_type":"code","source":"def predictions(model):\n    import time\n    t1 = time.time()\n    def load_test():\n        for df in pd.read_csv('../input/avito-demand-prediction/test.csv', chunksize= 250000):\n            yield df\n\n    item_ids = np.array([], dtype=np.int32)\n    preds= np.array([], dtype=np.float32)\n\n    i = 0 \n    \n    for df in load_test():\n    \n        i +=1\n        print(df.dtypes)\n        item_id = df['item_id']\n        print(\" Chunk number is \"+str(i))\n    \n        test = preprocess_dataset(df)\n        print(test.dtypes)\n\n        test = keras_test_transform(test)\n        del df\n        gc.collect()\n    \n        print(test.dtypes)\n    \n        X_test = get_keras_data(test)\n        del test \n        gc.collect()\n    \n        Batch_Size = 512*3\n        preds1 = modelRNN.predict(X_test, batch_size = Batch_Size, verbose = 1)\n        print(preds1.shape)\n        del X_test\n        gc.collect()\n        print(\"RNN Prediction is done\")\n\n        preds1 = preds1.reshape(-1,1)\n        #print(predsl.shape)\n        preds1 = np.clip(preds1, 0, 1)\n        print(preds1.shape)\n        item_ids = np.append(item_ids, item_id)\n        print(item_ids.shape)\n        preds = np.append(preds, preds1)\n        print(preds.shape)\n        \n    print(\"All chunks done\")\n    t2 = time.time()\n    print(\"Total time for Parallel Batch Prediction is \"+str(t2-t1))\n    return preds ","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffc8a220fc0b344f5a6e83d598e47182678511c8"},"cell_type":"code","source":"del train['description']\ndel train['title']\ntrain.dtypes","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"78e962ede17dabdc200f857502594b94b865716d"},"cell_type":"markdown","source":"# **STEP BY STEP BREAK DOWN FOR KFOLD****\n\n*  STEP 1  - GETTING INDEXES FOR TRAIN AND TEST \n\nfrom sklearn.model_selection import KFold\nskf = KFold(n_splits = 3)\nfor train_idx, test_idx in skf.split(train1,  y_train):\n\n-  In the above lines of code, train1  and y_train have to be numpy arrays\n- Using data frames directly does not work. The indexes are not identified when data frames are used.\n\nInstead, we use train.values() which will be a numpy array \n\ntrain1 = train.values() where train is the final data frame with all transformations applied \ny_train = train['deal_probability'].values()\n\n* STEP 2 - SPLITTING X AND Y INTO TEST AND TRAIN \n\n  #K Fold Split \n    \n  X_train1, X_test1 = train1[train_idx], train1[test_idx]\n  y_train1, y_test1 = y_train[train_idx], y_train[test_idx]\n\n- Once again train1, y_train have to be numpy arrays ( Data frames do not work in this step )\n\nSTEP 3  - USING TRAIN AND TEST VALUES FOR RNN \n\n for i in range(3):\n        hist = modelRNN.fit(X_train_1, y_train1, batch_size=batch_size+(batch_size*(2*i)), epochs=epochs, validation_data=(X_test_1, y_test1), verbose=1)\n        \n**This step gave me a multitude of ARRAY SIZE ERRORS. ****\n\nSOLUTION FOR PROBLEM IN STEP 3 \n\n-  Convert each of the two input arrays in Step 2 into Data frames. A function was written for that purpose \n    get_data_frame(dataset)\n-  Use these data frames in the RNN Fit call in Step 3 \n-  I must confess that this step looks a little messy but I simply could not get it to work any other way. I would be very   \n   happy to learn a cleaner way to make this work. "},{"metadata":{"trusted":true,"_uuid":"fe6ae6ae86c45274d91b0915afe4434dce009a86","collapsed":true},"cell_type":"code","source":"# Converting the TRAIN Data frame into array values \ntrain1 = np.array(train.values)\ndel train\ngc.collect()\n\n# Function to Convert the Train array values back into a data frame \n# The data frame produced by this function will be used as inputs for the RNN \n\ndef get_data_frame(dataset):\n    \n    DF = pd.DataFrame()\n    \n    DF['category_name'] = np.array(dataset[:,0])\n    DF['city'] = np.array(dataset[:,1])\n    DF['parent_category_name'] = np.array(dataset[:,2])\n    DF['price'] = np.array(dataset[:,3])\n    DF['region'] = np.array(dataset[:,4])\n    DF['seq_title_description'] = np.array(dataset[:,5])\n    \n    return DF ","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"4f795ba20696bf0924f5db13659ff9b099ee36c7"},"cell_type":"markdown","source":"# **K FOLD FOR RNN ********"},{"metadata":{"_uuid":"2484fc26cc0ecc6db58690d2b99c1feff53ccb95","_cell_guid":"c24f4ac2-e862-42b1-82ae-8d7ba4bfc73f","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nimport time \nskf = KFold(n_splits = 3)\nKfold_preds_final = []\nk = 0\nRMSE = []\n\nfor train_idx, test_idx in skf.split(train1, y_train):\n    \n    print(\"Number of Folds..\"+str(k+1))\n    \n    # Initialize a new Model for Current FOLD \n    \n    epochs = 1\n    batch_size = 512 * 3\n    steps = (int(train1.shape[0]/batch_size))*epochs\n    lr_init, lr_fin = 0.002, 0.001\n    lr_decay = exp_decay(lr_init, lr_fin, steps)\n    modelRNN = RNN_model()\n    K.set_value(modelRNN.optimizer.lr, lr_init)\n    K.set_value(modelRNN.optimizer.decay, lr_decay)\n\n    #K Fold Split \n    \n    X_train1, X_test1 = train1[train_idx], train1[test_idx]\n    print(X_train1.shape, X_test1.shape)\n    y_train1, y_test1 = y_train[train_idx], y_train[test_idx]\n    print(y_train1.shape, y_test1.shape)\n    gc.collect()\n    \n    # Getting the dataframes for Training and Test Arrays \n    X_train_final = get_data_frame(X_train1)\n    X_test_final = get_data_frame(X_test1)\n    \n    del X_train1, X_test1\n    gc.collect()\n    \n    # Getting the Dictionary for RNN input \n    X_train_f = get_keras_data(X_train_final)\n    X_test_f = get_keras_data(X_test_final)\n    \n    del X_train_final, X_test_final\n    gc.collect()\n\n    # Fit the NN Model \n    for i in range(3):\n        hist = modelRNN.fit(X_train_f, y_train1, batch_size=batch_size+(batch_size*(2*i)), epochs=epochs, validation_data=(X_test_f, y_test1), verbose=1)\n\n    del X_train_f\n    gc.collect()\n\n    # Print RMSE for Validation set for Kth Fold \n    v_rmse = eval_model(modelRNN, X_test_f)\n    RMSE.append(v_rmse)\n    \n    del X_test_f\n    del y_train1, y_test1\n    gc.collect()\n    \n    # Predict test set for Kth Fold \n    preds = predictions(modelRNN)\n    del modelRNN \n    gc.collect()\n\n    print(\"Predictions done for Fold \"+str(k))\n    print(preds.shape)\n    Kfold_preds_final.append(preds)\n    del preds\n    gc.collect()\n    print(\"Number of folds completed....\"+str(len(Kfold_preds_final)))\n    print(Kfold_preds_final[k][0:10])\n\nprint(\"All Folds completed\"+str(k+1))   \nprint(\"RNN FOLD MODEL Done\")","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"06892337d1b15feb05fb85ca3be362b34bd3148e"},"cell_type":"markdown","source":"# **HOW TO USE KFOLD OUTPUT ****\n\n* For each Fold, after the RNN training is completed, a call is made to a function to predict deal probability for test \n* RMSE of that fold is predicted and the RMSE value is put in a list\n* * The predicted value for that fold is put in a list \n\n* Approach 1  - A simple average of the output of each of the FOLDS \n* Approach 2 - Identify the fold with the least RMSE value.  Use the output of that FOLD as the final output \n* Approach 3 - Use the Output of FOLD 1 as the Target variable for Folds 2 and 3. \n* Approach 4  - Reuse the model trained in FOLD 1 for FOLDS 2 and 3 "},{"metadata":{"trusted":true,"_uuid":"31e33368c69a455ba099e341aad1ac8bf6975ab1"},"cell_type":"code","source":"pred_final1 = np.average(Kfold_preds_final, axis =0) # Average of all K Folds\nprint(pred_final1.shape)\n\n# Find lowest RMSE value \nmin_value = min(RMSE)\n# Which KFOLD has this lowest RMSE value \nRMSE_idx = RMSE.index(min_value)\nprint(RMSE_idx)\n\n# What is the prediction values corresponding to the lowest RMSE value \npred_final2 = Kfold_preds_final[RMSE_idx]\nprint(pred_final2.shape)\n\ndel Kfold_preds_final, train1\ngc.collect()","execution_count":28,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18a13c16b16ed8eed472abb512a26b1474332e79"},"cell_type":"code","source":"# Average Output \npred_final1[0:5]","execution_count":29,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e52defb29957170cd83926035df484b36f098d4"},"cell_type":"code","source":"# Output with lowest RMSE \npred_final2[0:5]","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"a720107aefb3a8e95c7b6c58f3483aa81b7b67bc"},"cell_type":"markdown","source":"# AVERAGE OF ALL FOLDS "},{"metadata":{"_uuid":"22eb2aa29ced6806f75b62a677c60a2449577c7d","_cell_guid":"e6833179-8039-44d9-867b-553c2d32dea3","trusted":true},"cell_type":"code","source":"test_cols = ['item_id']\ntest = pd.read_csv('../input/avito-demand-prediction/test.csv', usecols = test_cols)\n\n# using Average of KFOLD preds \n\nsubmission1 = pd.DataFrame( columns = ['item_id', 'deal_probability'])\n\nsubmission1['item_id'] = test['item_id']\nsubmission1['deal_probability'] = pred_final1\n\nprint(\"Check Submission NOW!!!!!!!!@\")\nsubmission1.to_csv(\"Avito_Shanth_RNN_AVERAGE.csv\", index=False)\n","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"63b8ecbc42418a989b0807d963f79686b150b8b8"},"cell_type":"markdown","source":"# PREDICTION FROM KFOLD WITH LOWEST RMSE "},{"metadata":{"trusted":true,"_uuid":"607052770f9d7670e586f7b9043638f9d96f56d4"},"cell_type":"code","source":"# Using KFOLD preds with Minimum value \nsubmission2 = pd.DataFrame( columns = ['item_id', 'deal_probability'])\n\nsubmission2['item_id'] = test['item_id']\nsubmission2['deal_probability'] = pred_final2\n\nprint(\"Check Submission NOW!!!!!!!!@\")\nsubmission2.to_csv(\"Avito_Shanth_RNN_MIN.csv\", index=False)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9f96c0662ee60339c48058788c7071e7527d5dd1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}