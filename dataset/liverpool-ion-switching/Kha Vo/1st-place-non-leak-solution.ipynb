{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os, numpy as np, pandas as pd, scipy.io as sio\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\nfrom hmmlearn.hmm import GaussianHMM\nfrom sklearn.linear_model import LinearRegression\nfrom scipy.stats import norm\n\ndef predict(Xp, catp, thres):\n    pred = np.zeros(len(Xp))\n    for i in range(6): \n        bins = [-99.0] + list(np.sort(thres[i])) + [99.0]\n        pred[catp==i] = np.digitize(Xp[catp==i], bins)-1\n    return pred\n\n\ndef calculate_matrix(transition_matrix, states, number_processes):\n    \"\"\"\n    Expand a transition matrix to model separate processes.\n    If max(open_channels) = K, then we assume K 0/1 processes. \n    E.g. our data category 3 corresponds to a maximum\n    of 3 open_channels, so 3 processes.\n    \n    We create model a combination_with_repetition(3, 4) = 20\n    transition matrix. The first row & col corresponds to all\n    processes being in the first hidden state (1, 1, 1). The\n    second row & col corresponds to (1, 1, 2), and so on until\n    (4, 4, 4).\n    \n    To calculate the transition probability from (1, 2, 2) to\n    (1, 1, 3), we calculate P(1->1) * P(2->1) * P(2->3). But\n    also for all permutations (e.g. (2, 1, 2) and (3, 1, 1)).\n    In the end, we normalize our transition matrix.\n    \"\"\"\n    # Fill in diagonals such that each row sums to 1\n    for i in range(transition_matrix.shape[0]):\n        transition_matrix[i, i] = 1 - np.sum(transition_matrix[i, :])\n\n    n0 = len(states)\n    new_transition_matrix = transition_matrix.copy()\n    new_states = [(x,) for x in range(n0)]\n    for process in range(1, number_processes):\n        # We expand our current transition matrix (that models up to `process` number\n        # of separate processes) its' dimensions by n0. We basically add another\n        # possible state transition for a new process.\n        nc = new_transition_matrix.shape[0]\n        temp_transition_matrix = np.zeros((n0*nc, n0*nc))\n        temp_states = []\n        for i in range(n0):\n            temp_states.extend([s + (i,) for s in new_states])\n            for j in range(n0):\n                # We add i -> j as our final transition\n                temp_transition_matrix[i*nc:(i+1)*nc, j*nc:(j+1)*nc] = transition_matrix[i][j] * new_transition_matrix\n              \n        # We now group similar processes together to reduce our matrix. \n        # E.g. (1, 2, 3) is the same as (2, 3, 1)\n        new_states = sorted(list(set([tuple(sorted(x)) for x in temp_states])))\n        new_transition_matrix = np.zeros((len(new_states), len(new_states)))\n        for i in range(len(new_states)):\n            ix_i = [k for k, x in enumerate(temp_states) if tuple(sorted(x)) == new_states[i]]\n            for j in range(len(new_states)):\n                ix_j = [k for k, x in enumerate(temp_states) if tuple(sorted(x)) == new_states[j]]\n                new_transition_matrix[i, j] = np.sum(temp_transition_matrix[ix_i, :][:, ix_j])\n                new_transition_matrix[i, j] /= len(ix_i)\n    \n    new_channels = []\n    for s in new_states:\n        new_channels.append(sum([states[x] for x in s]))\n    new_channels= np.array(new_channels)\n        \n    return new_transition_matrix, new_channels\n\n\n\ndef create_hmm(signal, predictions, transmat, states):\n    # Linear Regression to esimate the mean signal value\n    # per unique number of open channels\n    means = np.ones((len(states))) * np.NaN\n    for c in range(np.min(predictions), np.max(predictions) + 1):\n        mu = np.nanmedian(signal[predictions == c])\n        ix = np.where(states == c)[0]\n        for i in ix:\n            means[i] = mu\n    mask = ~np.isnan(means)\n    lr = LinearRegression()\n    lr.fit(states[mask].reshape(-1, 1), means[mask])\n    means = lr.predict(states.reshape(-1, 1))\n\n    # Defining our HMM\n    hmm = GaussianHMM(\n        n_components=len(states),           # Number of hidden states\n        n_iter=50,                         # Total number of iterations\n        verbose=False,                       # Show logs\n        algorithm='map',                    # Use maximum a posteriori instead of Viterbi\n        params='stmc',                      # Optimize start probs, transmat, means, covs\n        random_state=42,\n        init_params='s',                    # Manually initialize all but start probabilities\n        covariance_type='tied',             # Separate covariance per hidden state\n        tol=0.01                            # Convergence criterion\n    )\n\n    # Initialize the parameters of our HMM\n    hmm.n_features = 1\n    hmm.means_ = means.reshape(-1, 1)\n    covs = np.array([np.cov(signal[~np.isnan(signal)][predictions[~np.isnan(signal)] == c]) for c in states if np.sum(~np.isnan(signal) == c) > 1000])\n    hmm.covars_ = np.nanmean(covs).reshape(-1, 1)\n    hmm.transmat_ = transmat\n    \n    return hmm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def get_Ptran_cat(cat):\n    if cat==0:\n        mat = [[0     , 0.1713   , 0   , 0      ],\n              [0.3297, 0        , 0   , 0.01381],\n              [0     , 1        , 0   , 0      ],\n              [0     , 0.0002686, 0   , 0      ]]\n        \n    elif cat==1:\n        mat =  [[0     , 0.0121, 0     , 0     ],\n                [0.0424, 0     , 0.2766, 0.0101],\n                [0     , 0.2588, 0     , 0     ],\n                [0     , 0.0239, 0     , 0     ]]\n        \n    elif cat<=4:\n        mat =  [[0     , 0.0067, 0     , 0     ],\n                [0.0373, 0     , 0.2762, 0.0230],\n                [0     , 0.1991, 0     , 0     ],\n                [0     , 0.0050, 0     , 0     ]]\n        \n    \n    elif cat==5:\n        EPS = 0\n        mat =  [[0.        , EPS       , 0.34493706, 0.00287762, 0.00006045, EPS       ],\n                [EPS       , 0.        , 0.00040108, EPS       , EPS       , EPS       ],\n                [0.16435428, 0.00438756, 0.        , 0.01714043, 0.00023227, EPS       ],\n                [0.02920171, 0.00080145, 0.27065939, 0.        , 0.01805161, 0.00108684],\n                [0.00268151, 0.00000064, 0.06197474, 0.30666751, 0.        , 0.06625158],\n                [EPS       , EPS       , 0.00000136, 0.13616454, 0.51059444, EPS       ]]\n\n    return np.array(mat)\n\n\n\ndef get_hidden_states(cat):\n    if cat!=5: return [1, 1, 0, 0]\n    else: return [0, 0, 1, 2, 3, 4]\n\ndef get_Psig(signal, States, kexp):\n    Psig = np.zeros((len(signal), len(States)))\n    for i in range(len(Psig)):\n        Psig[i] = np.exp((-(signal[i] - States)**2)/(kexp))\n    return Psig\n\n\ndef forward(Psig, Ptran, etat_in=None, coef=1, normalize=True):\n    if etat_in is None: etat_in = np.ones(Psig.shape)/Psig.shape[1]\n    alpha = np.zeros(Psig.shape) # len(sig) x n_state\n    etat = np.zeros(Psig.shape) # len(sig) x n_state\n    C = np.zeros(Psig.shape[0]) # scale vector for each timestep\n    \n    etat[0] = etat_in[0]\n    alpha[0] = etat_in[0]\n    if normalize: \n        alpha[0] = etat_in[0]*Psig[0]\n        alpha[0]/=alpha[0].sum()\n\n    for j in range(1, Psig.shape[0]):\n        etat[j] = alpha[j-1]@Ptran\n        if normalize: etat[j] /= etat[j].sum()\n        etat[j] = (etat[j]**coef) * ((etat_in[j])**(1-coef))\n        if normalize: etat[j] /= etat[j].sum()\n        alpha[j] = etat[j]  * Psig[j]\n        alpha[j] /= alpha[j].sum()\n    return alpha, etat\n\n\ndef optimize_thres_unsupervised(pred):\n    y = pd.read_csv('/kaggle/input/liverpool-ion-switching/train.csv').open_channels.values\n    sig = np.load(PATH+'M301_sig.npy')\n    catbatches = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 2, 3, 5, 1, 4, 3, 4, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n    good_id = list(range(3600000))+list(range(3900000,5000000))\n    sY_all = [[0.26]*12,\n              [0.26]*12,\n              [0.26]*12,\n              [0.22]*12, #0.22\n              [0.26]*12,\n              [0.26]*12,\n             ]\n    L = 100000\n    Y = pred.copy()\n    Thres = {}\n    Yopt = pred.copy()\n\n    for b in range(70):\n        Thres[b] = np.zeros(12)\n        Thres[b][0] = -99\n        Thres[b][-1] = 99\n        poscat = range(L*b, L*(b+1))\n        catbatch = int(catbatches[b])\n        sY = sY_all[catbatch]\n\n        Yloc = Y[poscat]\n        floc = sig[poscat]\n\n        adaptive_sY = np.array([sY[int(np.round(item))] for item in floc])\n        floc2 = floc[np.abs(floc-np.round(floc)) - adaptive_sY < 0]\n\n        for i in range(10):\n            ni = len(floc2[np.round(floc2)<=i])\n            ni2 = np.round(ni*len(floc)/ max(1, len(floc2))).astype(int)\n            Ys = np.concatenate([np.sort(floc), [19]])\n            Thres[b][i+1] = 0.5*(Ys[max(0,ni2)]+Ys[min(len(Ys)-1,ni2)])\n\n        for i in range(11):\n            Yloc[(Yloc>=Thres[b][i])&(Yloc<Thres[b][i+1])] = i\n    #         print(Yloc.max(), Yloc.min(), i)\n        Yopt[poscat] = Yloc\n\n    print(f1_score(y[good_id], Yopt[:5000000][good_id], average='macro'))\n    return Yopt, Thres","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '/kaggle/input/ion-cleaned-data/'\nKexp = [.103, .120, .1307, .138, .267, .105] \nKexpp =  [1.8,  1.8,  1.8,   1.83, 1.807, 1.8]\nN_PROCESSES = [1, 1, 3, 5, 10, 1]\nCOEFS_BACK = [1, .9192, .9192, .8792, .9022, .9192]\nCOEFS_FOR = [1, .8869, .8869, .8869, .8849, .8869]\nCOEFS_FIN = [.618, 0.50, 0.50,  0.49, 0.509, 0.50]\nCOEFS_FIN3 = [0.3,  0.3,  0.3, 0.35, 0.335, 0.3]\nBATCHES = np.array([0, 5, 6, 10, 15, 20, 25, 30, 35, 40, 45, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 65])\nCATEGORIES = np.array([1, 1, 1, 2, 3, 5, 4, 2, 3, 4, 5, 6, 3, 4, 6, 2, 5, 4, 5, 6, 3, 6, 6])\n\nfull_pred = np.zeros(7000000)\ncleaned_signal = np.load(PATH+'sig_301_sinecleaned.npy')\noofs = pd.read_csv('../input/ion-oofs/YM257method2oofpredLB9446Proba/YM257method2oofpredLB9446Proba.txt', \n                   index_col=None, skiprows=1, header=None, sep=' ').values\npredictions = np.argmax(oofs, axis=1)\n\nfor c in [0,1,2,3,4,5]:\n\n    print(\"\\nTraining cat\", c)\n    kexp = Kexp[c]\n    kexpp = Kexpp[c]\n    coefback = COEFS_BACK[c]\n    coeffor = COEFS_FOR[c]\n    coef_fin = COEFS_FIN[c]\n    coef_fin3 = COEFS_FIN3[c]\n    Ptran, States = calculate_matrix(get_Ptran_cat(c), get_hidden_states(c), N_PROCESSES[c])\n\n    for jb, b in enumerate(BATCHES):\n        if CATEGORIES[jb]!=c+1: continue\n        end_b = BATCHES[jb+1] if b!=65 else 70\n        sig = cleaned_signal[100000*b:100000*end_b]\n        nstates = Ptran.shape[0]\n        Psig = get_Psig(sig, States, kexp)\n        \n        if c!=5:\n\n            alpha0, etat0 = forward(Psig, Ptran, normalize=False)\n            alpha1, etat1 = forward(Psig[::-1], np.transpose(Ptran), etat_in=etat0[::-1], coef=coefback)\n            alpha2, etat2 = forward(Psig, Ptran, etat_in=etat1[::-1], coef=coeffor)\n\n            alpha3 = etat1[::-1]*etat2*Psig**kexpp\n            for j, alp in enumerate(alpha3): alpha3[j] /= alp.sum()\n\n            pred = coef_fin*(alpha1[::-1]) + (1-coef_fin-coef_fin3)*alpha2 + coef_fin3*alpha3\n\n            full_pred[b*100000:b*100000+len(sig)] = pred@States\n            print('Max/min', (pred@States).max(), (pred@States).min())\n        \n        else: \n            for k in range(len(sig) // 100000):\n                sub_signal = sig[k*100000:(k+1)*100000]\n                oof = predictions[(b+k)*100000:(b+k+1)*100000]\n\n                hmm = create_hmm(sub_signal, oof, Ptran, States)\n                hmm.fit(sub_signal.reshape(-1, 1))\n                pred = hmm.predict_proba(sub_signal.reshape(-1, 1))\n                full_pred[(b+k)*100000:(b+k+1)*100000] = pred @ States\n                print('Max/min', (pred@States).max(), (pred@States).min())\n        \nYopt, Thres = optimize_thres_unsupervised(full_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Yopt_test = Yopt[5000000:]\n# cattest = np.load(PATH+\"cattest.npy\")\n# bestsub = pd.read_csv(PATH+\"subM312_0.94415_cat6_updated.txt\").open_channels.values # subM318_0.95481\n# Yopt_test[cattest==5] = bestsub[cattest==5]\nsub = pd.read_csv('/kaggle/input/liverpool-ion-switching/sample_submission.csv')\nsub['open_channels'] = Yopt_test.astype(np.int8)\nsub.to_csv('M318_Kha_withNewCat6.csv', index=None, float_format='%0.4f')\nplt.hist(Yopt[5000000:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}