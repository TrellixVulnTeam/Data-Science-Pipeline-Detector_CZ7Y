{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is a streamlined solution without any data augmentation. The model has two layers, which build upon each other and one layer uses CPU platform while the other uses GPU platform. We decided that the easiest will be to split this into three kernels, which will be three steps. Please run these three kernels in sequence one after the other as described below.\n \n\n(1) Liverppol_LGBM_oofs\n\n(1.a) This kernal has to be run first.\n\n(1.b) This kernal uses competition data and the public dataset Data Without Drift as input.\n\n(1.c) This kernal uses an LGBM based model to make baseline predictions. \n\n(1.d) This kernal runs on a CPU platform.\n\n(1.e) The runtime for this kernal is approximately 1.7 hrs (it took us 6133.8 sec).\n\n(2) Liverpool_NoiseRemoval\n\n(2.a) This kernal has to be run second.\n\n(2.b) This kernal takes output from the previous kernal as input.\n\n(2.c) This kernal removes 50Hz noise from the signal and we use that clean signal in the next step.\n\n(2.d) This kernal runs on a CPU platform.\n\n(2.d) This runtime for this kernal is approximately 1.2 min (it took us 72.9 sec).\n\n(3) Liverpool_Wavenet\n\n(3.a) This last kernal outputs two csv files, the one named final_submission_wavenet.csv can be submitted to the competition to get a score close to our final private score.\n\n(3.b) This kernal uses a Wavenet based model, which is trained on clean signal\nand makes the final predictions.\n\n(3.c) This kernal runs on a GPU platform.\n\n(3.d) The runtime for this kernal is approximately 2.5 hrs (it took us 8908.9 sec)\n\nDoing this iteratively, i.e generate better predictions and then generate cleaner \nsignal and then use that cleaner signal to make even better prediction captures \nthe basic strategy of our work.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/data-without-drift/train_clean.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/data-without-drift/test_clean.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,20):\n    for j in range(0,50):\n        test1=test_data[100000*i:(i+1)*100000]\n        test1.reset_index(inplace=True,drop=True)\n        train1=train_data[100000*j:(j+1)*100000]\n        train1.reset_index(inplace=True,drop=True)\n        corr=train1.signal.corr(test1.signal)\n        if corr>abs(0.5):\n            test_data[i*100000:(i+1)*100000]['signal']=test_data[i*100000:(i+1)*100000]['signal'].values - train_data[j*100000:(j+1)*100000]['signal'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.DataFrame()\ntrain['signal']=train_data['signal'].values\ntrain['open_channels']=train_data['open_channels'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfrom scipy import signal\n# signal processing features\n\ndef calc_power(s):\n    \n    power = pd.DataFrame()\n    power['s2'] = s**2\n    s2m = (s**2).mean()\n    power['s2_mean'] = s**2 - s2m\n    power = power.fillna(value=0)\n    return power\n    \ndef calc_gradients(s, n_grads=4):\n    '''\n    Calculate gradients for a pandas series. Returns the same number of samples\n    '''\n    grads = pd.DataFrame()\n    \n    g = s.values\n    for i in range(n_grads):\n        g = np.gradient(g)\n        grads['grad_' + str(i+1)] = g\n        \n    return grads\n\ndef calc_low_pass(s, n_filts=10):\n    '''\n    Applies low pass filters to the signal. Left delayed and no delayed\n    '''\n    wns = np.logspace(-2, -0.3, n_filts)\n    \n    low_pass = pd.DataFrame()\n    x = s.values\n    for wn in wns:\n        b, a = signal.butter(1, Wn=wn, btype='low')\n        zi = signal.lfilter_zi(b, a)\n        low_pass['lowpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n        low_pass['lowpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n        \n    return low_pass\n\ndef calc_high_pass(s, n_filts=10):\n    '''\n    Applies high pass filters to the signal. Left delayed and no delayed\n    '''\n    wns = np.logspace(-2, -0.1, n_filts)\n    \n    high_pass = pd.DataFrame()\n    x = s.values\n    for wn in wns:\n        b, a = signal.butter(1, Wn=wn, btype='high')\n        zi = signal.lfilter_zi(b, a)\n        high_pass['highpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n        high_pass['highpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n        \n    return high_pass\n\ndef calc_roll_stats(s, windows=[10, 50, 100, 500, 1000]):\n#def calc_roll_stats(s, windows=[4]):\n\n    '''\n    Calculates rolling stats like mean, std, min, max...\n    '''\n    roll_stats = pd.DataFrame()\n    for window in windows:\n        roll_stats['roll_mean_' + str(window)] = s.rolling(window=window, min_periods=1, center=True).mean()\n#        roll_stats['roll_mean_cen' + str(window)] = s.rolling(window=window, min_periods=1, center=True).mean()        \n        roll_stats['roll_std_' + str(window)] = s.rolling(window=window, min_periods=1, center=True).std()\n        roll_stats['roll_min_' + str(window)] = s.rolling(window=window, min_periods=1, center=True).min()\n        roll_stats['roll_max_' + str(window)] = s.rolling(window=window, min_periods=1, center=True).max()\n        roll_stats['roll_range_' + str(window)] = roll_stats['roll_max_' + str(window)] - roll_stats['roll_min_' + str(window)]\n#        roll_stats['roll_q10_' + str(window)] = s.rolling(window=window, min_periods=1, center=True).quantile(0.10)\n#        roll_stats['roll_q25_' + str(window)] = s.rolling(window=window, min_periods=1, center=True).quantile(0.25)\n#        roll_stats['roll_q50_' + str(window)] = s.rolling(window=window, min_periods=1, center=True).quantile(0.50)\n#        roll_stats['roll_q75_' + str(window)] = s.rolling(window=window, min_periods=1, center=True).quantile(0.75)\n#        roll_stats['roll_q90_' + str(window)] = s.rolling(window=window, min_periods=1, center=True).quantile(0.90)\n    \n    # add zeros when na values (std)\n    roll_stats = roll_stats.fillna(value=0)\n             \n    return roll_stats\n\ndef calc_roll_stats_back(s, windows=[10, 50, 100, 500, 1000]):\n#def calc_roll_stats(s, windows=[4]):\n\n    '''\n    Calculates rolling stats like mean, std, min, max...\n    '''\n    roll_stats_b = pd.DataFrame()\n    for window in windows:\n        roll_stats_b['roll_mean_b' + str(window)] = s.rolling(window=window, min_periods=1, center=False).mean()\n#        roll_stats_b['roll_mean_cen' + str(window)] = s.rolling(window=window, min_periods=1, center=True).mean()        \n        roll_stats_b['roll_std_b' + str(window)] = s.rolling(window=window, min_periods=1, center=False).std()\n        roll_stats_b['roll_min_b' + str(window)] = s.rolling(window=window, min_periods=1, center=False).min()\n        roll_stats_b['roll_max_b' + str(window)] = s.rolling(window=window, min_periods=1, center=False).max()\n        roll_stats_b['roll_range_b' + str(window)] = roll_stats_b['roll_max_b' + str(window)] - roll_stats_b['roll_min_b' + str(window)]\n#        roll_stats_b['roll_q10_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.10)\n#        roll_stats_b['roll_q25_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.25)\n#        roll_stats_b['roll_q50_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.50)\n#        roll_stats_b['roll_q75_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.75)\n#        roll_stats_b['roll_q90_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.90)\n    \n    # add zeros when na values (std)\n    roll_stats_b = roll_stats_b.fillna(value=0)\n             \n    return roll_stats_b\n\ndef calc_roll_stats_forward(s, windows=[10, 50, 100, 500, 1000]):\n#def calc_roll_stats(s, windows=[4]):\n\n    '''\n    Calculates rolling stats like mean, std, min, max...\n    '''\n    roll_stats_f = pd.DataFrame()\n    for window in windows:\n        roll_stats_f['roll_mean_f' + str(window)] = s.shift(periods=window, fill_value=0).rolling(window=window, min_periods=1, center=False).mean()\n#        roll_stats_f['roll_mean_cen' + str(window)] = s.rolling(window=window, min_periods=1, center=True).mean()        \n        roll_stats_f['roll_std_f' + str(window)] = s.shift(periods=window, fill_value=0).rolling(window=window, min_periods=1, center=False).std()\n        roll_stats_f['roll_min_f' + str(window)] = s.shift(periods=window, fill_value=0).rolling(window=window, min_periods=1, center=False).min()\n        roll_stats_f['roll_max_f' + str(window)] = s.shift(periods=window, fill_value=0).rolling(window=window, min_periods=1, center=False).max()\n        roll_stats_f['roll_range_f' + str(window)] = roll_stats_f['roll_max_f' + str(window)] - roll_stats_f['roll_min_f' + str(window)]\n#        roll_stats_f['roll_q10_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.10)\n#        roll_stats_f['roll_q25_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.25)\n#        roll_stats_f['roll_q50_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.50)\n#        roll_stats_f['roll_q75_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.75)\n#        roll_stats_f['roll_q90_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.90)\n    \n    # add zeros when na values (std)\n    roll_stats_f = roll_stats_f.fillna(value=0)\n             \n    return roll_stats_f\n\ndef calc_ewm(s, windows=[10, 50, 100, 500, 1000]):\n    '''\n    Calculates exponential weighted functions\n    '''\n    ewm = pd.DataFrame()\n    for w in windows:\n        ewm['ewm_mean_' + str(w)] = s.ewm(span=w, min_periods=1).mean()\n        ewm['ewm_std_' + str(w)] = s.ewm(span=w, min_periods=1).std()\n        \n    # add zeros when na values (std)\n    ewm = ewm.fillna(value=0)\n        \n    return ewm\n\ndef calc_grad_times_means(gradients, roll_stats, ewm, windows=[10, 50, 100, 500, 1000]):\n    '''\n    Calculates the gradient times means features\n    '''\n    grad_times = pd.DataFrame()\n    for w in windows:\n        grad_times['grad_1_times_roll_mean_' + str(w)] = gradients['grad_1']*roll_stats['roll_mean_' + str(w)]\n        grad_times['grad_2_times_roll_mean_' + str(w)] = gradients['grad_2']*roll_stats['roll_mean_' + str(w)]\n        grad_times['grad_3_times_roll_mean_' + str(w)] = gradients['grad_3']*roll_stats['roll_mean_' + str(w)]\n        grad_times['grad_4_times_roll_mean_' + str(w)] = gradients['grad_4']*roll_stats['roll_mean_' + str(w)]\n        grad_times['grad_1_times_ewm_mean_' + str(w)] = gradients['grad_1']*ewm['ewm_mean_' + str(w)]\n        grad_times['grad_2_times_ewm_mean_' + str(w)] = gradients['grad_2']*ewm['ewm_mean_' + str(w)]\n        grad_times['grad_3_times_ewm_mean_' + str(w)] = gradients['grad_3']*ewm['ewm_mean_' + str(w)]\n        grad_times['grad_4_times_ewm_mean_' + str(w)] = gradients['grad_4']*ewm['ewm_mean_' + str(w)]\n        grad_times['grad_1_times_ewm_std_' + str(w)] = gradients['grad_1']*ewm['ewm_std_' + str(w)]\n        grad_times['grad_2_times_ewm_std_' + str(w)] = gradients['grad_2']*ewm['ewm_std_' + str(w)]\n        grad_times['grad_3_times_ewm_std_' + str(w)] = gradients['grad_3']*ewm['ewm_std_' + str(w)]\n        grad_times['grad_4_times_ewm_std_' + str(w)] = gradients['grad_4']*ewm['ewm_std_' + str(w)]        \n        \n    grad_times = grad_times.fillna(value=0)\n    \n    return grad_times\n\ndef add_features(s):\n    '''\n    All calculations together\n    '''\n    power = calc_power(s)\n    gradients = calc_gradients(s)\n    low_pass = calc_low_pass(s)\n    high_pass = calc_high_pass(s)\n    roll_stats = calc_roll_stats(s)\n#    roll_stats_b = calc_roll_stats_back(s)\n#    roll_stats_f = calc_roll_stats_forward(s)\n    ewm = calc_ewm(s)\n#    grad_times_means = calc_grad_times_means(gradients, roll_stats, ewm)\n    return pd.concat([s, power, gradients, low_pass, high_pass, roll_stats, ewm], axis=1)\n\ndef divide_and_add_features(s, signal_size=100000):\n    '''\n    Divide the signal in bags of \"signal_size\".\n    Normalize the data dividing it by 15.0\n    '''\n    # normalize\n    s = s/15.0\n    \n    ls = []\n    for i in tqdm(range(int(s.shape[0]/signal_size))):\n        sig = s[i*signal_size:(i+1)*signal_size].copy().reset_index(drop=True)\n        sig_featured = add_features(sig)\n        ls.append(sig_featured)\n    \n    return pd.concat(ls, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        if col!='open_channels':\n            col_type = df[col].dtypes\n            if col_type in numerics:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs = []\nfor i in range(0,50):\n    df0 = divide_and_add_features(train[i*100000:(i+1)*100000]['signal'])\n    df0 = reduce_mem_usage(df0)\n    dfs.extend([df0])\ndf = pd.concat(dfs)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs = []\nfor i in range(0,20):\n    df0 = divide_and_add_features(test_data[i*100000:(i+1)*100000]['signal'])\n    df0 = reduce_mem_usage(df0)\n    dfs.extend([df0])\ndf_test = pd.concat(dfs)\ndf_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GroupKFold, StratifiedKFold, train_test_split\nfrom sklearn import metrics\nimport lightgbm as lgb\nimport time\nprint(time.ctime())\nkf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\ntarget = 'open_channels'\n#features = ['signal', 'grad_1', 'grad_2', 'grad_3', 'grad_4']\nfeatures = [col for col in df.columns if col not in ['roll_mean_b10', 'roll_std_b10', 'roll_min_b10', 'roll_max_b10', 'roll_range_b10', 'roll_mean_b50', 'roll_std_b50', 'roll_min_b50', 'roll_max_b50', 'roll_range_b50', 'roll_mean_b100', 'roll_std_b100', 'roll_min_b100', 'roll_max_b100', 'roll_range_b100', 'roll_mean_b500', 'roll_std_b500', 'roll_min_b500', 'roll_max_b500', 'roll_range_b500', 'roll_mean_b1000', 'roll_std_b1000', 'roll_min_b1000', 'roll_max_b1000', 'roll_range_b1000', 'roll_mean_f10', 'roll_std_f10', 'roll_min_f10', 'roll_max_f10', 'roll_range_f10', 'roll_mean_f50', 'roll_std_f50', 'roll_min_f50', 'roll_max_f50', 'roll_range_f50', 'roll_mean_f100', 'roll_std_f100', 'roll_min_f100', 'roll_max_f100', 'roll_range_f100', 'roll_mean_f500', 'roll_std_f500', 'roll_min_f500', 'roll_max_f500', 'roll_range_f500', 'roll_mean_f1000', 'roll_std_f1000', 'roll_min_f1000', 'roll_max_f1000', 'roll_range_f1000']]\n#features = [col for col in df.columns]\noof_pred = np.zeros(len(df))\ny_pred = np.zeros(len(df_test))\n#y_pred_b = np.zeros(len(df_test_b))\n#y_pred_c = np.zeros(len(df_test_c))\n#y_pred_d = np.zeros(len(df_test_d))\nfor fold, (tr_ind, val_ind) in enumerate(kf.split(df[features], train[target])):\n    x_train, x_val = df[features].iloc[tr_ind], df[features].iloc[val_ind]\n    y_train, y_val = train[target][tr_ind], train[target][val_ind]\n    train_set = lgb.Dataset(x_train, y_train)\n    val_set = lgb.Dataset(x_val, y_val)\n    params = {'boosting_type': 'gbdt',\n              'metric': 'rmse',\n              'objective': 'regression',\n              'n_jobs': -1,\n              'seed': 42,\n              'num_leaves': 280,\n              'learning_rate': 0.026623466966581126,\n              'max_depth': 73,\n              'lambda_l1': 2.959759088169741,\n              'lambda_l2': 1.331172832164913,\n              'bagging_fraction': 0.9655406551472153,\n              'bagging_freq': 9,\n              'colsample_bytree': 0.6867118652742716}\n        \n    model = lgb.train(params, train_set, num_boost_round = 10000, early_stopping_rounds = 50, \n                        valid_sets = [train_set, val_set], verbose_eval = 100)\n        \n    oof_pred[val_ind] = model.predict(x_val)\n        \n    y_pred += model.predict(df_test[features]) / kf.n_splits\n#    y_pred_b += model.predict(df_test_b[features]) / kf.n_splits\n#    y_pred_c += model.predict(df_test_c[features]) / kf.n_splits\n#    y_pred_d += model.predict(df_test_d[features]) / kf.n_splits\n        \nrmse_score = np.sqrt(metrics.mean_squared_error(train[target], oof_pred))\n# want to clip and then round predictions (you can get a better performance using optimization to found the best cuts)\noof_pred2 = np.round(np.clip(oof_pred, 0, 10)).astype(int)\nround_y_pred = np.round(np.clip(y_pred, 0, 10)).astype(int)\n#round_y_pred_b = np.round(np.clip(y_pred_b, 0, 10)).astype(int)\n#round_y_pred_c = np.round(np.clip(y_pred_c, 0, 5)).astype(int)\n#round_y_pred_d = np.round(np.clip(y_pred_d, 0, 5)).astype(int)\nf1 = metrics.f1_score(train[target], oof_pred2, average = 'macro')\n\npre_train = pd.DataFrame()\npre_train['oofs']=oof_pred\npre_train['oofs2']=oof_pred2\npre_train[['oofs','oofs2']].to_csv('oofs_train.csv', index = False)\n\nprint(f'Our oof rmse score is {rmse_score}')\nprint(f'Our oof  f1 score is {f1}')\n#f1_new = (f1[6]+f1[7]+f1[8]+f1[9]+f1[10])/5.0\n#print(f'Our oof  f1 score for channels 6-10 is {f1_new}')\nprint(time.ctime())\n#submission = pd.read_csv('/kaggle/input/liverpool-ion-switching/sample_submission.csv', dtype={'time':str})\n#submission['open_channels'] = round_y_pred\n#submission['oofs'] = y_pred\n\n#submission[['time','open_channels']].to_csv('submission.csv', index = False)\n#submission.to_csv('oofs_test.csv', index = False)\nsub = pd.DataFrame()\n#sub_b = pd.DataFrame()\n#sub_c = pd.DataFrame()\n#sub_d = pd.DataFrame()\nsub['oofs']=y_pred\n#sub_b['oofs']=y_pred_b\nsub['oofs2']=round_y_pred\n#sub_b['oofs2']=round_y_pred_b\n#sub_c['open_channels']=round_y_pred_c\n#sub_d['open_channels']=round_y_pred_d\nsub.to_csv('oofs_test.csv', index = False)\n#sub_b.to_csv('oofs_test_b.csv', index = False)\n#sub_c.to_csv('sub_i.csv', index = False)\n#sub_d.to_csv('sub_k.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}