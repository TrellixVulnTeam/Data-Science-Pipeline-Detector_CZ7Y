{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Credit to [jazivxt](https://www.kaggle.com/jazivxt) for original kernel. I only added the optimized rounder class, which shows some improvement on local CV"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom sklearn import *\nimport lightgbm as lgb\n\ntrain = pd.read_csv('/kaggle/input/liverpool-ion-switching/train.csv')\ntest = pd.read_csv('/kaggle/input/liverpool-ion-switching/test.csv')\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from functools import partial\nimport scipy as sp\nclass OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize F1 (Macro) score\n    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _f1_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n        return -metrics.f1_score(y, X_p, average = 'macro')\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._f1_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def features(df):\n    df = df.sort_values(by=['time']).reset_index(drop=True)\n    df.index = ((df.time * 10_000) - 1).values\n    df['batch'] = df.index // 50_000\n    df['batch_index'] = df.index  - (df.batch * 50_000)\n    df['batch_slices'] = df['batch_index']  // 5_000\n    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n    \n    for c in ['batch','batch_slices2']:\n        d = {}\n        d['mean'+c] = df.groupby([c])['signal'].mean()\n        d['median'+c] = df.groupby([c])['signal'].median()\n        d['max'+c] = df.groupby([c])['signal'].max()\n        d['min'+c] = df.groupby([c])['signal'].min()\n        d['std'+c] = df.groupby([c])['signal'].std()\n        d['mean_abs_chg'+c] = df.groupby([c])['signal'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = df.groupby([c])['signal'].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = df.groupby([c])['signal'].apply(lambda x: np.min(np.abs(x)))\n        for v in d:\n            df[v] = df[c].map(d[v].to_dict())\n        df['range'+c] = df['max'+c] - df['min'+c]\n        df['maxtomin'+c] = df['max'+c] / df['min'+c]\n        df['abs_avg'+c] = (df['abs_min'+c] + df['abs_max'+c]) / 2\n    \n    #add shifts\n    df['signal_shift_+1'] = [0,] + list(df['signal'].values[:-1])\n    df['signal_shift_-1'] = list(df['signal'].values[1:]) + [0]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+1'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-1'][i] = np.nan\n\n    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]:\n        df[c+'_msignal'] = df[c] - df['signal']\n        \n    return df\n\ntrain = features(train)\ntest = features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = [c for c in train.columns if c not in ['time', 'open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]\nx1, x2, y1, y2 = model_selection.train_test_split(train[col], train['open_channels'], test_size=0.3, random_state=7)\n\ndef lgb_Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = metrics.f1_score(labels, preds, average='macro')\n    return ('KaggleMetric', score, True)\n \nparams = {'learning_rate': 0.8, 'max_depth': 7, 'num_leaves':2**7+1, 'metric': 'rmse', 'random_state': 7, 'n_jobs':-1} \nmodel = lgb.train(params, lgb.Dataset(x1, y1), 3000,  lgb.Dataset(x2, y2), verbose_eval=50, early_stopping_rounds=100, feval=lgb_Metric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds = model.predict(train[col], num_iteration=model.best_iteration)\nmetrics.f1_score(train['open_channels'], np.round(np.clip(y_preds, 0, 10)), average = 'macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optR = OptimizedRounder()\noptR.fit(y_preds.reshape(-1,), train['open_channels'])\ncoefficients = optR.coefficients()\nprint(coefficients)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def optimize_prediction(prediction):\n    prediction[prediction <= coefficients[0]] = 0\n    prediction[np.where(np.logical_and(prediction > coefficients[0], prediction <= coefficients[1]))] = 1\n    prediction[np.where(np.logical_and(prediction > coefficients[1], prediction <= coefficients[2]))] = 2\n    prediction[np.where(np.logical_and(prediction > coefficients[2], prediction <= coefficients[3]))] = 3\n    prediction[np.where(np.logical_and(prediction > coefficients[3], prediction <= coefficients[4]))] = 4\n    prediction[np.where(np.logical_and(prediction > coefficients[4], prediction <= coefficients[5]))] = 5\n    prediction[np.where(np.logical_and(prediction > coefficients[5], prediction <= coefficients[6]))] = 6\n    prediction[np.where(np.logical_and(prediction > coefficients[6], prediction <= coefficients[7]))] = 7\n    prediction[np.where(np.logical_and(prediction > coefficients[7], prediction <= coefficients[8]))] = 8\n    prediction[np.where(np.logical_and(prediction > coefficients[8], prediction <= coefficients[9]))] = 9\n    prediction[prediction > coefficients[9]] = 10\n    \n    return prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.f1_score(train['open_channels'], optimize_prediction(y_preds), average = 'macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(test[col], num_iteration=model.best_iteration)\ntest['open_channels'] = optimize_prediction(preds).astype(int)\ntest[['time','open_channels']].to_csv('submission.csv', index=False, float_format='%.4f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb.plot_importance(model,importance_type='split', max_num_features=20)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}