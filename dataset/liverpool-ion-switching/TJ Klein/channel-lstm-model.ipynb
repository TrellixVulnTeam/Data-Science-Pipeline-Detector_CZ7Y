{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Channel LSTM Model\n\n### I have taken the kernel provided by [richardbj](https://www.kaggle.com/richardbj) and modified it to use the current competition dataset. Original kernel [here](https://www.kaggle.com/richardbj/channel-lstm-101)"},{"metadata":{"_uuid":"f175607e5a9152b7a9dda3732017a6281ba8e626","_cell_guid":"3c737fc3-71e9-482e-a09c-d3b64c65af5a","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.utils.np_utils import to_categorical\nimport matplotlib.pyplot as plt\nfrom pandas import read_csv, DataFrame\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout,Flatten, Reshape, Activation\nfrom keras.layers import LSTM\nfrom keras.layers import noise\nfrom keras.models import load_model\nfrom keras import optimizers\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import f1_score, accuracy_score\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First job; load the data"},{"metadata":{"_uuid":"fef6f081e475b8c0943257ce834991f40de19152","_cell_guid":"aa7b6857-a125-4f44-9670-f0a45d472102","trusted":true},"cell_type":"code","source":"#df_1=pd.read_csv('../input/2chan10dbnoise/output3.csv',header=None,nrows=1000)\ndf=pd.read_csv('../input/liverpool-ion-switching/train.csv')\ndataset=df.values\nprint(dataset[0:20,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Second job turn the current channel labels (which are the actual channel amplitudes in pA) into simpler classifier labels 0, 1 or 2."},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_labels = to_categorical(dataset[:,2], num_classes= 11)\nprint(categorical_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#does this help?\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset[:,0:2] = scaler.fit_transform(dataset[:,0:2])\nprint(dataset[:10,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\ndef precision_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    \ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aeee23d1c81dd792e3812880cf37c47a4f8a0c93","_cell_guid":"4d8ad450-48d2-4160-bbce-0898571cb101","trusted":true},"cell_type":"code","source":"batch_size=100\nmodel = Sequential()\ntimestep=1\ninput_dim=1\nmodel.add(LSTM(64, batch_input_shape=(batch_size, timestep, input_dim), stateful=True, return_sequences=True))\nmodel.add(Flatten())\nmodel.add(Dense(11))\nmodel.add(Activation('softmax'))\n#binary sinks like a stone! b/c not binary @@\n#model.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True), metrics=['accuracy'])\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True), metrics=[f1_m])\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c200c39387c8af3fd61f3c98e44b3864b0fb774","_cell_guid":"011220ce-35d7-4d6b-b30e-868cd21231f5","trusted":true},"cell_type":"code","source":"import math\ntrain_size = math.floor(len(dataset) * 0.80/100)\ntrain_size = int (train_size*100)\ntest_size = math.floor((len(dataset) - train_size)/100)\ntest_size = int(test_size*100)\nprint ('training set= ',train_size)\nprint('test set =', test_size)\nprint ('total length', test_size+train_size)\nprint ('Dataset= ', len(dataset))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4980144818f81e197d8562bdf5b658045de565d0","_cell_guid":"3ef2cd7c-9625-4225-8673-7d9be1164507"},"cell_type":"markdown","source":"in_train, in_test = dataset[0:train_size,1], dataset[train_size:len(dataset),1]\ntarget_train, target_test = categorical_labels[0:train_size,:], categorical_labels[train_size:len(dataset),:]\nin_train = in_train.reshape(len(in_train),1,1)\nin_test = in_test.reshape(len(in_test), 1,1)\nb=np.zeros([len(in_train),1,3])\nb[:,0,0]=in_train[:,0,0]\nin_train=b\nprint('in_train Shape',in_train.shape)\nprint('target train shape',target_train.shape)\nb=np.zeros([len(in_test),1,3])\nb[:,0,0]=in_test[:,0,0]\nin_test=b\nprint('in_test Shape',in_test.shape)\nprint('target_test Shape',in_test.shape)"},{"metadata":{"_uuid":"0e7790358cf9c8cafe385b3f361ceccff1b94d7d","_cell_guid":"e73ff743-3438-4c8b-8679-b671bd1d76d4","trusted":true},"cell_type":"code","source":"in_train, in_test = dataset[0:train_size,1], dataset[train_size:train_size+test_size,1]\ntarget_train, target_test = categorical_labels[0:train_size,:], categorical_labels[train_size:train_size+test_size,:]\n\nin_train = in_train.reshape(len(in_train),1,1)\nin_test = in_test.reshape(len(in_test), 1,1)\n\nprint('in_train Shape',in_train.shape)\nprint(in_train[0:2,:])\nprint('target_train Shape',target_train.shape)\nstate=np.argmax(target_train,axis=-1)\nprint(state)\n\nprint('in_test Shape',in_test.shape)\nprint(in_test[0:2,:])\nprint('target_test Shape',in_test.shape)\nstate=np.argmax(target_test,axis=-1)\nprint(state)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"7fc63eec20d1c24ac4433fc1f204e87c936d15ea","_cell_guid":"a601fdf0-0e8e-40b5-86a5-17ada91c4cfc","trusted":true},"cell_type":"code","source":"epochers=3\nhistory=model.fit(x=in_train,y=target_train, initial_epoch=0, epochs=epochers, batch_size=batch_size, verbose=2, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6776ae4ba71d219900926c4eed56c83d6c6754ef","_cell_guid":"b54583aa-0039-46e9-87d1-c3be2a143b2d","trusted":true},"cell_type":"code","source":"plt.plot(history.history['f1_m'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation"},{"metadata":{},"cell_type":"markdown","source":"#### Observations from Validation:\n- The Model does not do very well in terms of responding to signal noise. Example, We get a lot of misclassifications when the true target is 0 for long stretches in time, yet our prediction bounces back and forth between 0 and 1.\n- A better Validation scheme should be pursued instead of simply cutting off the last chunk of the training data to use for validation. There is a lot of data in the last part of the training data that the model hasn't seen yet, therefore it responds poorly to the validation set. "},{"metadata":{},"cell_type":"markdown","source":"### Training Data (First 80% of train)"},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = model.predict(in_train, batch_size=batch_size)\nprint(predict.shape)\nprint(predict[:5,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_train=np.argmax(target_train,axis=-1)\nclass_predict_train=np.argmax(predict,axis=-1)\nprint(state_train[:20])\nprint(class_predict_train[:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('F1_macro = ',f1_score(state_train,class_predict_train, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plotlen=test_size\nstarting_point = 15000\nlenny=1000\n#target_test = dataset[train_size:len(dataset),3]\n#target_test = target_test.reshape(plotlen, 1)\nplt.figure(figsize=(30,6))\nplt.subplot(2,1,1)\n#temp=scaler.inverse_transform(dataset)\n#plt.plot (temp[train_size:len(dataset),1], color='blue', label=\"some raw data\")\nplt.plot (dataset[starting_point:starting_point+lenny,1], color='blue', label=\"some raw data\")\nplt.title(\"The raw test\")\ndf=DataFrame(dataset[starting_point:starting_point+lenny,1])\nplt.subplot(2,1,2)\n#plt.plot(target_test.reshape(plotlen,1)*maxchannels, color='black', label=\"the actual idealisation\")\nplt.plot(state_train[starting_point:starting_point+lenny], color='black', label=\"the actual idealisation\")\n#plt.plot(spredict, color='red', label=\"predicted idealisation\")\nline,=plt.plot(class_predict_train[starting_point:starting_point+lenny], color='red', label=\"predicted idealisation\")\nplt.setp(line, linestyle='--')\nplt.xlabel('timepoint')\nplt.ylabel('current')\n#plt.savefig(name)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Validation Hold Out Data (Last 20% of train)"},{"metadata":{"_uuid":"590c063c98773bff7fa00ad69032694c3f1005b9","_cell_guid":"31aa71c7-13d8-4daf-9c2c-01ed85f338ec","trusted":true},"cell_type":"code","source":"predict_test = model.predict(in_test, batch_size=batch_size)\nprint(predict_test.shape)\nprint(predict_test[:5,:])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"914948221fc0a4254619033711fb42f7639b9f3c","_cell_guid":"9e61ad49-c792-4b05-893f-5f23df0ab45d","trusted":true},"cell_type":"code","source":"state_test=np.argmax(target_test,axis=-1)\nclass_predict=np.argmax(predict_test,axis=-1)\nprint(state[:20])\nprint(class_predict[:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('F1_macro = ',f1_score(state,class_predict, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24e5c18f7eb5b94304fe3abf84687c3a77e0ec6a","_cell_guid":"7f27d5a3-b0fa-423b-a481-4ba8132ae13e","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plotlen=test_size\nlenny=1000\n#target_test = dataset[train_size:len(dataset),3]\n#target_test = target_test.reshape(plotlen, 1)\nplt.figure(figsize=(30,6))\nplt.subplot(2,1,1)\n#temp=scaler.inverse_transform(dataset)\n#plt.plot (temp[train_size:len(dataset),1], color='blue', label=\"some raw data\")\nplt.plot (dataset[train_size:train_size+lenny,1], color='blue', label=\"some raw data\")\nplt.title(\"The raw test\")\ndf=DataFrame(dataset[train_size:train_size+lenny,1])\nplt.subplot(2,1,2)\n#plt.plot(target_test.reshape(plotlen,1)*maxchannels, color='black', label=\"the actual idealisation\")\nplt.plot(state[0:lenny], color='black', label=\"the actual idealisation\")\n#plt.plot(spredict, color='red', label=\"predicted idealisation\")\nline,=plt.plot(class_predict[:lenny], color='red', label=\"predicted idealisation\")\nplt.setp(line, linestyle='--')\nplt.xlabel('timepoint')\nplt.ylabel('current')\n#plt.savefig(name)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","name":"python","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3}}},"nbformat":4,"nbformat_minor":4}