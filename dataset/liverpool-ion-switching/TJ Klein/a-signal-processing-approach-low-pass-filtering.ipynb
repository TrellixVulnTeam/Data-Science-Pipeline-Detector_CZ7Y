{"cells":[{"metadata":{},"cell_type":"markdown","source":"# A signal processing approach - low pass filtering\n\nDue to the nature of the signal data we have been provided, a signal processing approach can provide advantages. In short, it can help us decouple the signal from the noise in order to provide our models with a less noisy signal. In this kernel I will be performing the following on each batch:\n1. Analyze frequency domain characteristics\n2. Characterize Signal-To-Noise Ratio (SNR) of the data\n3. Based on SNR, apply low pass filter to reduce signal noise\n4. Perform feature engineering and apply models based on filtered signal\n\n#### I am using the data without drift dataset for this kernel: https://www.kaggle.com/cdeotte/data-without-drift\n\n### If you find this kernel useful, please upvote!\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import datetime\nimport numpy as np\nimport scipy as sp\nimport scipy.fftpack\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.signal import butter,filtfilt,freqz\nfrom sklearn import *\nfrom sklearn.metrics import f1_score\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import Pool,CatBoostRegressor\nimport time\nimport datetime\nfrom sklearn.model_selection import KFold\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train = pd.read_csv('../input/data-without-drift/train_clean.csv')\ntest = pd.read_csv('../input/data-without-drift/test_clean.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"batch_size = 500000\nnum_batches = 10\nres = 1000 # Resolution of signal plots\n\nfs = 10000       # sample rate, 10kHz\nnyq = 0.5 * fs  # Nyquist Frequency\ncutoff_freq_sweep = range(250,4750,50) # Sweeping from 250 to 4750 Hz for SNR measurement\nlpf_cutoff = 600","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This is a good visualization of the training data signal and open channels."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(20,5));\nplt.plot(range(0,train.shape[0],res),train.signal[0::res])\nfor i in range(num_batches+1): plt.plot([i*batch_size,i*batch_size],[-5,12.5],'r')\nfor j in range(num_batches): plt.text(j*batch_size+200000,num_batches,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \nplt.title('Training Data Signal - 10 batches',size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(20,5));\nplt.plot(range(0,train.shape[0],res),train.open_channels[0::res])\nfor i in range(num_batches+1): plt.plot([i*batch_size,i*batch_size],[-5,12.5],'r')\nfor j in range(num_batches): plt.text(j*batch_size+200000,num_batches,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \nplt.title('Training Data Open Channels - 10 batches',size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The following functions can be used to create a high-pass, low-pass, or band-pass butterworth filter as well as measure SNR. I only used the low-pass filter in this kernel, but highly encourage others to experiment with different filter configurations."},{"metadata":{"trusted":true},"cell_type":"code","source":"def butter_lowpass_filter(data, cutoff, fs, order):\n    normal_cutoff = cutoff / nyq\n    # Get the filter coefficients \n    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n    y = filtfilt(b, a, data)\n    return y\n\ndef butter_highpass_filter(data, cutoff, fs, order):\n    normal_cutoff = cutoff / nyq\n    # Get the filter coefficients \n    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n    y = filtfilt(b, a, data)\n    return y\n\ndef butter_bandpass_filter(data, cutoff_low, cuttoff_high, fs, order):\n    normal_cutoff_low = cutoff_low / nyq\n    normal_cutoff_high = cutoff_high / nyq    \n    # Get the filter coefficients \n    b, a = butter(order, [normal_cutoff_low,normal_cutoff_high], btype='band', analog=False)\n    y = filtfilt(b, a, data)\n    return y\n\ndef signaltonoise(a, axis=0, ddof=0):\n    a = np.asanyarray(a)\n    m = a.mean(axis)\n    sd = a.std(axis=axis, ddof=ddof)\n    return np.where(sd == 0, 0, m/sd)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Frequency Domain Analysis\n\n### First off, we want to better understand the frequency content within each batch. We notice that there is considerable broadband noise and it isn't clear from this that there is a predominant frequency that differentiates the signal. There are some batches that have some interesting features at the low frequencies (Batch 1 and 2). There also appears to be some interesting noise scatter amonst higher frequencies in batch 8. Based on this, we may be able to attenuate a large portion of the higher frequencies to reduce signal noise. We will do this by applying a low-pass filter."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=5, ncols=2, figsize=(25, 15))\nfig.subplots_adjust(hspace = .5)\nax = ax.ravel()\ncolors = plt.rcParams[\"axes.prop_cycle\"]()\n\nfor batch in range(num_batches):\n    fft = sp.fftpack.fft(train.signal[batch_size*(batch):batch_size*(batch+1)])\n    psd = np.abs(fft) ** 2\n    fftfreq = sp.fftpack.fftfreq(len(psd),1/fs)\n    i = fftfreq > 0\n    \n    c = next(colors)[\"color\"]\n    ax[batch].plot(fftfreq[i], 10 * np.log10(psd[i]),color=c)\n    ax[batch].set_title(f'Batch {batch+1}')\n    ax[batch].set_xlabel('Frequency (Hz)')\n    ax[batch].set_ylabel('PSD (dB)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Signal-to-Noise (SNR) Characterization \n\n### We can apply low-pass filters with different cutoff frequencies in order to see at which cutoff frequency our signal begins to degrade. It looks like a cutoff frequency of 600Hz provides us with the best SNR for most batches. One thing to note is that the SNR for different batches is either positive or negative. Typically, a negative SNR means our noise is greater than our signal. However, I will show later that this isn't actual the case with some of our batches. \n\n**Positive SNR:** Batch 4, Batch 5, Batch 6, Batch 9, Batch 10\n\n**Negative SNR:** Batch 1, Batch 2, Batch 3, Batch 7, Batch 8"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(15,15));\n\n# Filter requirements.\norder = 20  \nSNR = np.zeros(len(cutoff_freq_sweep))\n\nfor batch in range(num_batches):\n    for index,cut in enumerate(cutoff_freq_sweep): \n        signal_lpf = butter_lowpass_filter(train.signal[batch_size*(batch):batch_size*(batch+1)], cut, fs, order)\n        SNR[index] = signaltonoise(signal_lpf)\n    \n    plt.plot(cutoff_freq_sweep,SNR)\n\nplt.title('Signal-to-Noise Ratio Per Batch')    \nplt.xlabel('Frequency')\nplt.ylabel('SNR')\nplt.legend(['Batch 1','Batch 2','Batch 3','Batch 4','Batch 5','Batch 6','Batch 7','Batch 8','Batch 9','Batch 10',])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Low Pass Filtering By Batch\n\n### In case you are unfamiliar with filtering, I am plotting the the low pass filter design with a cutoff frequency at 600Hz below. Notice how we are not attenuating between 0 and 600Hz, i.e. we are allowing low frequencies to 'pass' unaffected by the filter. But anything above 600Hz will be attenuated as you can see by how the amplitude falls away all the way out to our Nyquist frequency."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"b, a = butter(order, lpf_cutoff/nyq, btype='low', analog=False)\nw,h = freqz(b,a, fs=fs)\n\nplt.figure(figsize=(16,8));\nplt.plot(w, 20 * np.log10(abs(h)), 'b')\nplt.ylabel('Amplitude [dB]', color='b')\nplt.xlabel('Frequency [Hz]')\nplt.title('Low-pass Butterworth Filter, cutoff @ 600Hz')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### When we apply the filter to Batch 1, we see how our frequency response changes."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fft = sp.fftpack.fft(train.signal[batch_size*(batch-1):batch_size*batch])\npsd = np.abs(fft) ** 2\nfftfreq = sp.fftpack.fftfreq(len(psd),1/fs)\ni = fftfreq > 0\n\nfig, ax = plt.subplots(2, 1, figsize=(10, 6))\nfig.subplots_adjust(hspace = .5)\nax[0].plot(fftfreq[i], 10 * np.log10(psd[i]))\nax[0].set_xlabel('Frequency (1/10000 seconds)')\nax[0].set_ylabel('PSD (dB)')\nax[0].set_title('Unfiltered')\n\nbatch = 8\nsignal_lpf_batch_8 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfft = sp.fftpack.fft(signal_lpf_batch_8)\npsd = np.abs(fft) ** 2\nfftfreq = sp.fftpack.fftfreq(len(psd),1/fs)\ni = fftfreq > 0\n\nax[1].plot(fftfreq[i], 10 * np.log10(psd[i]))\nax[1].set_xlabel('Frequency (1/10000 seconds)')\nax[1].set_ylabel('PSD (dB)')\nax[1].set_title('Low pass filter - cutoff = 600 Hz')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we will apply this Low Pass Filter (LPF) to all batches and look at how that impacts the time domain signal."},{"metadata":{},"cell_type":"markdown","source":"## Batch 1\n\n### We can see that our noise floor has been reduced significantly. We also see that some of the spikes have been reduced in amplitude a bit, but overall not nearly as much as our noise floor."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"batch = 1\n\nsignal_lpf_batch_1 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_1[::res])\n\nax[0].legend(['open_channels'])\nax[1].legend(['signal', 'filtered signal'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Batch 2\n\n### Same story as Batch 1, more signal and less noise"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"batch = 2\n\nsignal_lpf_batch_2 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_2[::res])\n\nax[0].legend(['open_channels'])\nax[1].legend(['signal', 'filtered signal'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Batch 3\n\n### I think the LPF actually really helped with normalizing levels. Notice how now open_channel = 1 and it's associated levels in the filtered signal jump around much less than the unfiltered signal."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"batch = 3\n\nsignal_lpf_batch_3 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_3[::res])\n\nax[0].legend(['open_channels'])\nax[1].legend(['signal', 'filtered signal'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Batch 4\n\n### It isn't as clear whether the LPF helped out for this batch, but we will see when we check model performance"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"batch = 4\n\nsignal_lpf_batch_4 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_4[::res])\n\nax[0].legend(['open_channels'])\nax[1].legend(['signal', 'filtered signal'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Batch 5\n\n### We may have lost some signal when filtering on this batch. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"batch = 5\n\nsignal_lpf_batch_5 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_5[::res])\n\nax[0].legend(['open_channels'])\nax[1].legend(['signal', 'filtered signal'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Batch 6\n\n### Same as Batch 5, we may have lost some signal in the filtering"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"batch = 6\n\nsignal_lpf_batch_6 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_6[::res])\n\nax[0].legend(['open_channels'])\nax[1].legend(['signal', 'filtered signal'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Batch 7\n\n### This looks a lot like Batch 3 where it is clear that filtering helped reduce noise."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"batch = 7\n\nsignal_lpf_batch_7 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_7[::res])\n\nax[0].legend(['open_channels'])\nax[1].legend(['signal', 'filtered signal'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Batch 8\n\n### Batch 8 is unique in the sense that it looks like there was some measurement noise. The LPF was able to filter it out which will help us out a lot."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"batch = 8\n\nsignal_lpf_batch_8 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_8[::res])\n\nax[0].legend(['open_channels'])\nax[1].legend(['signal', 'filtered signal'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Batch 9\n\n### Unclear whether filtering helped or hurt"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"batch = 9\n\nsignal_lpf_batch_9 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_9[::res])\n\nax[0].legend(['open_channels'])\nax[1].legend(['signal', 'filtered signal'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Batch 10\n\n### Unclear whether filtering helped or hurt"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"batch = 10\n\nsignal_lpf_batch_10 = butter_lowpass_filter(train.signal[batch_size*(batch-1):batch_size*batch], lpf_cutoff, fs, order)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),signal_lpf_batch_10[::res])\n\nax[0].legend(['open_channels'])\nax[1].legend(['signal', 'filtered signal'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now just apply LPF to train and test data. We will start by applying our filter to one batch at a time, however we could see improvement by only applying the filter to batches we know were helped."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocess train data\nbatch = 8\ntrain['signal'][batch_size*(batch-1):batch_size*batch] = signal_lpf_batch_8\n\n# Train Data\ntrain['signal_undrifted'] = train['signal']\n# Test Data\ntest['signal_undrifted'] = test['signal']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering and Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"def features(df):\n    df = df.sort_values(by=['time']).reset_index(drop=True)\n    df.index = ((df.time * 10_000) - 1).values\n    df['batch'] = df.index // 50_000\n    df['batch_index'] = df.index  - (df.batch * 50_000)\n    df['batch_slices'] = df['batch_index']  // 5_000\n    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n    \n    for c in ['batch','batch_slices2']:\n        d = {}\n        d['mean'+c] = df.groupby([c])['signal_undrifted'].mean()\n        d['median'+c] = df.groupby([c])['signal_undrifted'].median()\n        d['max'+c] = df.groupby([c])['signal_undrifted'].max()\n        d['min'+c] = df.groupby([c])['signal_undrifted'].min()\n        d['std'+c] = df.groupby([c])['signal_undrifted'].std()\n        d['mean_abs_chg'+c] = df.groupby([c])['signal_undrifted'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = df.groupby([c])['signal_undrifted'].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = df.groupby([c])['signal_undrifted'].apply(lambda x: np.min(np.abs(x)))\n        for v in d:\n            df[v] = df[c].map(d[v].to_dict())\n        df['range'+c] = df['max'+c] - df['min'+c]\n        df['maxtomin'+c] = df['max'+c] / df['min'+c]\n        df['abs_avg'+c] = (df['abs_min'+c] + df['abs_max'+c]) / 2\n    \n    #add shifts\n    df['signal_shift_+1'] = [0,] + list(df['signal_undrifted'].values[:-1])\n    df['signal_shift_-1'] = list(df['signal_undrifted'].values[1:]) + [0]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+1'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-1'][i] = np.nan\n\n    # add shifts_2\n    df['signal_shift_+2'] = [0,] + [1,] + list(df['signal_undrifted'].values[:-2])\n    df['signal_shift_-2'] = list(df['signal_undrifted'].values[2:]) + [0] + [1]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==1].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-2'][i] = np.nan\n    for i in df[df['batch_index']==49998].index:\n        df['signal_shift_-2'][i] = np.nan \n        \n    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal_undrifted', 'open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]:\n        df[c+'_msignal'] = df[c] - df['signal_undrifted']\n        \n    return df\n\ntrain = features(train)\ntest = features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def f1_score_calc(y_true, y_pred):\n    return f1_score(y_true, y_pred, average=\"macro\")\n\ndef lgb_Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = f1_score(labels, preds, average=\"macro\")\n    return ('KaggleMetric', score, True)\n\n\ndef train_model_classification(X, X_test, y, params, model_type='lgb', eval_metric='f1score',\n                               columns=None, plot_feature_importance=False, model=None,\n                               verbose=50, early_stopping_rounds=200, n_estimators=2000):\n\n    columns = X.columns if columns == None else columns\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {\n                    'f1score': {'lgb_metric_name': lgb_Metric,}\n                   }\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros(len(X) )\n    \n    # averaged predictions on train data\n    prediction = np.zeros((len(X_test)))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    '''for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]'''\n            \n    if True:        \n        X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.3, random_state=7)    \n            \n        if model_type == 'lgb':\n            #model = lgb.LGBMClassifier(**params, n_estimators=n_estimators)\n            #model.fit(X_train, y_train, \n            #        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n            #       verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            model = lgb.train(params, lgb.Dataset(X_train, y_train),\n                              n_estimators,  lgb.Dataset(X_valid, y_valid),\n                              verbose_eval=verbose, early_stopping_rounds=early_stopping_rounds, feval=lgb_Metric)\n            \n            \n            preds = model.predict(X, num_iteration=model.best_iteration) #model.predict(X_valid) \n\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            \n        if model_type == 'xgb':\n            train_set = xgb.DMatrix(X_train, y_train)\n            val_set = xgb.DMatrix(X_valid, y_valid)\n            model = xgb.train(params, train_set, num_boost_round=2222, evals=[(train_set, 'train'), (val_set, 'val')], \n                                     verbose_eval=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            preds = model.predict(xgb.DMatrix(X)) \n\n            y_pred = model.predict(xgb.DMatrix(X_test))\n            \n\n        if model_type == 'cat':\n            # Initialize CatBoostRegressor\n            model = CatBoostRegressor(params)\n            # Fit model\n            model.fit(X_train, y_train)\n            # Get predictions\n            y_pred_valid = np.round(np.clip(preds, 0, 10)).astype(int)\n\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            y_pred = np.round(np.clip(y_pred, 0, 10)).astype(int)\n\n \n        oof = preds\n        \n        scores.append(f1_score_calc(y, np.round(np.clip(preds,0,10)).astype(int) ) )\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    #prediction /= folds.n_splits\n    \n    print('FINAL score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    result_dict['model'] = model\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= folds.n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"good_columns = [c for c in train.columns if c not in ['time', 'signal','open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]\n\nX = train[good_columns].copy()\ny = train['open_channels']\nX_test = test[good_columns].copy()\n\ndel train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_xgb = {'colsample_bytree': 0.375,'learning_rate': 0.1,'max_depth': 10, 'subsample': 1, 'objective':'reg:squarederror',\n          'eval_metric':'rmse'}\n\nresult_dict_xgb = train_model_classification(X=X[0:500000*8-1], X_test=X_test, y=y[0:500000*8-1], params=params_xgb, model_type='xgb', eval_metric='f1score', plot_feature_importance=False,\n                                                      verbose=50, early_stopping_rounds=250)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_lgb = {'learning_rate': 0.1, 'max_depth': 7, 'num_leaves':2**7+1, 'metric': 'rmse', 'random_state': 7, 'n_jobs':-1}\n\nresult_dict_lgb = train_model_classification(X=X[0:500000*8-1], X_test=X_test, y=y[0:500000*8-1], params=params_lgb, model_type='lgb', eval_metric='f1score', plot_feature_importance=False,\n                                                      verbose=50, early_stopping_rounds=250, n_estimators=3000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_ensemble = 0.50 * result_dict_lgb['prediction'] + 0.50 * result_dict_xgb['prediction']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/liverpool-ion-switching/sample_submission.csv')\nsub['open_channels'] =  np.array(np.round(preds_ensemble,0), np.int) \n\nsub.to_csv('submission.csv', index=False, float_format='%.4f')\nsub.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}