{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/liverpool-ion-switching/train.csv')\neval_data = pd.read_csv('/kaggle/input/liverpool-ion-switching/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will seperate the data set into different 10 subsets. In each parts, the subsets have 500 000 rows. They have the information of signals and numbers of open ion channels for each 50 seconds. "},{"metadata":{"trusted":true},"cell_type":"code","source":"subsets = []\neval_subsets = []\n\npart_number = int(data.shape[0]/500000)\neval_part_number = int(eval_data.shape[0]/500000)\n\nfor part in range(1, part_number+1):\n    subsets.append(part)\n    subsets[part-1] = data.iloc[((part-1)*500000):part*500000,:]\n    \nfor eval_part in range(1, eval_part_number+1):\n    eval_subsets.append(eval_part)\n    eval_subsets[eval_part-1] = eval_data.iloc[((eval_part-1)*500000):eval_part*500000,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nplt.figure(figsize=(20,5)); res = 1000\nplt.plot(range(0,data.shape[0],res),data.signal[0::res])\n#plt.plot(range(0,data.shape[0],res),data.open_channels[0::res])\n\nfor i in range(11): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(10): plt.text(j*500000+200000,10,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \nplt.title('Training Data Signal - 10 batches',size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the batches of 2, 7, 8, 9, and 10, drift was added. It is a signal bias which cause that the signal data change over time in unforeseen ways. The prediction is less accurate as time passes in this condition. So, we need to remove drif data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Crete a new clean data as copy of the data\n\nclean_data = data.copy()\n\n#Clean drift data in the batch 2 \n\na = 500000; b = 600000\nclean_data.loc[data.index[a:b],'signal'] = clean_data.signal[a:b].values - 3*(clean_data.time.values[a:b] - 50)/10.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We removed the slant drift for the batch 2. However, we need to remove the drift in the batches of 7, 8, 9, and 10. They have parabolic drift. We will assume it as a **sine** function. Then, we will try to remove it from the data. "},{"metadata":{},"cell_type":"markdown","source":"Firstly, let us define the sine function as \n\n\\begin{equation}\n    y = A \\sin(\\omega x + \\phi) + \\delta\n\\end{equation}  \nwhere $\\omega$ is the frequency, $x$ is avaliable **time** data, $\\phi$ represents the phase shift, and $\\delta$ denotes an addition noise. \n\nLet us make clear the equation. If we simplify the expression of $A \\sin(\\omega x + \\phi)$,\n\n\\begin{equation}\n    A \\sin(\\omega x + \\phi) = A \\sin(\\omega x) cos(\\phi) + A \\cos(\\omega x) sin(\\phi)\n\\end{equation} is obtained. Now, $y$ can be expressed as \n\n\\begin{equation}\n    y = A \\sin(\\omega x) cos(\\phi) + A \\cos(\\omega x) sin(\\phi) + \\delta\n\\end{equation}\n"},{"metadata":{},"cell_type":"markdown","source":"We can express it as a linear system: \n\n\n\\begin{equation}\n  \\begin{bmatrix}\n    sin(\\omega x_{1}) & cos(\\omega x_{1}) & 1 \\\\ \n    \\vdots & \\vdots & \\vdots \\\\ \n    sin(\\omega x_{n}) & cos(\\omega x_{n}) & 1\n  \\end{bmatrix}\n  %\n  \\begin{bmatrix}\n    Acos(\\phi) \\\\\n    Asin(\\phi) \\\\\n    \\delta \n  \\end{bmatrix}\n  =%\n  \\begin{bmatrix}\n    y_{1} \\\\\n    \\vdots \\\\\n    y_{n} \\\\\n  \\end{bmatrix}\n  %\n\\end{equation}\n\nwhere $n$ is 500000 because we work on the $\\sin$ function for a batch which has 500000 **time** data points. "},{"metadata":{},"cell_type":"markdown","source":"Let us define the matrix as $M$ and the vector as $\\theta$. If we simplify the linear system, $y$ can be stated as\n\n\\begin{equation}\n    M\\theta = y\n\\end{equation} \n\nNow, we can find the parameters of $A$ and $\\phi$ based on $\\theta$. \n\n"},{"metadata":{},"cell_type":"markdown","source":"\\begin{align}\n    \\theta &= (\\theta_{1}, \\theta_{2}, \\theta_{3})\\\\\n    \\theta_{1} &= Acos(\\phi)\\\\\n    \\theta_{2} &= Asin(\\phi)\\\\\n    \\theta_{3} &= \\delta\n\\end{align}"},{"metadata":{},"cell_type":"markdown","source":"\\begin{align}\n    \\theta_{1}^2 + \\theta_{2}^2 &= A^{2}{cos^{2}(\\phi)} + A^{2}{sin^{2}(\\phi)}\\\\\n                                &= A^{2}\n\\end{align}\n"},{"metadata":{},"cell_type":"markdown","source":"Then, we can find $A$ and $\\phi$, easily.\n\n\\begin{align}\n    A^{2} &= \\theta_{1}^2 + \\theta_{2}^2\\\\\n    A &= \\sqrt{\\theta_{1}^2 + \\theta_{2}^2} \n\\end{align} \n\nNow, let us find $\\phi$: \n\n\\begin{align}\n    \\frac{\\theta_{2}}{\\theta_{1}} &= \\frac{Asin(\\phi)}{Acos(\\phi)}\\\\\n                                 &= \\tan(\\phi)\\\\\n    \\phi &= \\tan^{-1}(\\frac{\\theta_{2}}{\\theta_{1}})\n\\end{align} "},{"metadata":{},"cell_type":"markdown","source":"Now, we will try to find optimal $A$ and $\\phi$ values for batches of 7,8,9, and 10. Then, we will use these optimal values on the $sine$ function in order to construct the parabolic drift function. "},{"metadata":{},"cell_type":"markdown","source":"Firstly, let us define $sin$ function. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def sin(x, A, phi, delta):\n    frequency = 0.01\n    omega = 2 * np.pi * frequency\n    return A * np.sin(omega * x + phi) + delta","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, we fit the parabolic drift function to get optimal $A$ and $\\phi$ values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\ndef parabolic_drift_fit(data):\n    x = data['time']\n    y = data['signal']\n\n    frequency = 0.01\n    omega = 2 * np.pi * frequency\n    \n    #construct matrix M for every data point t in the time data x\n    M = np.array([[np.sin(omega * time_point), np.cos(omega * time_point), 1] for time_point in x])\n    \n    #construct the signal data y\n    y = np.array(y).reshape(len(y), 1)\n\n    #linalg.lstsq function solves the equation a x = b by computing a vector x \n    #that minimizes the squared Euclidean 2-norm\n    (theta, _, _, _) = np.linalg.lstsq(M, y)\n    \n    #find A, phi, and delta parameters\n    A = np.sqrt(theta[0,0]**2 + theta[1,0]**2)\n    phi = math.atan2(theta[1,0], theta[0,0])\n    delta = theta[2,0]\n\n    optimal_parabol_drift = [A, phi, delta]\n    \n    \n    return optimal_parabol_drift","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find optimal parameter values for batch 7\na = 500000*6; b = 500000*7\nprint(parabolic_drift_fit(data.iloc[a:b]))\n\n#find optimal parameter values for batch 8\na = 500000*7; b = 500000*8\nprint(parabolic_drift_fit(data.iloc[a:b]))\n\n#find optimal parameter values for batch 9\na = 500000*8; b = 500000*9\nprint(parabolic_drift_fit(data.iloc[a:b]))\n\n#find optimal parameter values for batch 10\na = 500000*9; b = 500000*10\nprint(parabolic_drift_fit(data.iloc[a:b]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, optimal value of the amplitude, $A$, is $5$. We can say optimal values for the phase shift, $\\phi$, are $0$ and $\\pi$ for batches of 7,9 and batches of 8,10, respectively. More clearly,\n\n\\begin{align}\n    A_{optimal} &= 5 \\\\\n    \\phi_{optimal} &= \n    \\begin{cases}\n     0, & \\text{if batches of 7 and 9}\\\\\n    \\pi, & \\text{if batches 8 and 10}\n    \\end{cases}\n\\end{align}\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_parabolic_drift(x, A, opt_phi):\n    frequency = 0.01\n    omega = 2 * np.pi * frequency\n    y = A * np.sin(omega * x + opt_phi)\n    return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clean drift data in the batch 7\na = 500000*6; b = 500000*7\nclean_data.loc[data.index[a:b],'signal'] = data.signal.values[a:b] - remove_parabolic_drift(data.time[a:b].values, \n                                                                                            4.99, 0)\n\n#Clean drift data in the batch 8\na = 500000*7; b = 500000*8\nclean_data.loc[data.index[a:b],'signal'] = data.signal.values[a:b] - remove_parabolic_drift(data.time[a:b].values, \n                                                                                            5.07, 3.138)\n\n#Clean drift data in the batch 9\na = 500000*8; b = 500000*9\nclean_data.loc[data.index[a:b],'signal'] = data.signal.values[a:b] - remove_parabolic_drift(data.time[a:b].values, \n                                                                                            4.96, 0)\n\n#Clean drift data in the batch 10\na = 500000*9; b = 500000*10\nclean_data.loc[data.index[a:b],'signal'] = data.signal.values[a:b] - remove_parabolic_drift(data.time[a:b].values, \n                                                                                            5.07, 3.136)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.plot(data.time[::1000],data.signal[::1000])\nplt.title('Training Batches with Drift',size=16)\n\n\nplt.figure(figsize=(20,5))\nplt.plot(clean_data.time[::1000],clean_data.signal[::1000])\nplt.title('Training Batches without Drift',size=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nplt.figure(figsize=(20,5)); res = 1000\n#plt.plot(range(0,data.shape[0],res),data.signal[0::res])\nplt.plot(range(0,data.shape[0],res),data.open_channels[0::res])\n\nfor i in range(11): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(10): plt.text(j*500000+200000,10,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \nplt.title('Training Data Signal - 10 batches',size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will try feature scaling at each subset but will apply different scaling at these subsets. There are different numbers of ion channels at each subset. So, we will arrange the signal data in accordance with the numbers of ion channels at each subset. "},{"metadata":{},"cell_type":"markdown","source":"Now, let us view the numbers of ion channels in each subset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\npart_number = int(data.shape[0]/500000)\nion_channels_number_max = []\nion_channels_number_min = []\n\n\nfor sub_index in range(part_number):\n    sub_target_set = subsets[sub_index].iloc[:,-1]\n    ion_channels_number_max.append(max(Counter(sub_target_set).keys()))\n    ion_channels_number_min.append(min(Counter(sub_target_set).keys()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Maximum numbers of ion channels at each subset: "},{"metadata":{"trusted":true},"cell_type":"code","source":"ion_channels_number_max","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Minimum numbers of ion channels at each subset: "},{"metadata":{"trusted":true},"cell_type":"code","source":"ion_channels_number_min","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nplt.figure(figsize=(20,5)); res = 1000\nplt.plot(range(0,clean_data.shape[0],res),clean_data.signal[0::res])\nplt.plot(range(0,data.shape[0],res),data.open_channels[0::res])\n\nfor i in range(11): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(10): plt.text(j*500000+200000,10,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \nplt.title('Training Data Signal - 10 batches',size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_subsets = []\n#eval_subsets = []\n\npart_number = int(clean_data.shape[0]/500000)\n#eval_part_number = int(eval_data.shape[0]/500000)\n\nfor part in range(1, part_number+1):\n    clean_subsets.append(part)\n    clean_subsets[part-1] = clean_data.iloc[((part-1)*500000):part*500000,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"variance of batch 1 : \", clean_subsets[0].signal.var())\nprint(\"variance of batch 2 : \", clean_subsets[1].signal.var())\nprint(\"\\n\")\nprint(\"variance of batch 3 : \", clean_subsets[2].signal.var())\nprint(\"variance of batch 7 : \", clean_subsets[6].signal.var())\nprint(\"\\n\")\nprint(\"variance of batch 4 : \", clean_subsets[3].signal.var())\nprint(\"variance of batch 8 : \", clean_subsets[7].signal.var())\nprint(\"\\n\")\nprint(\"variance of batch 5 : \", clean_subsets[4].signal.var())\nprint(\"variance of batch 10 : \", clean_subsets[9].signal.var())\nprint(\"\\n\")\nprint(\"variance of batch 6 : \", clean_subsets[5].signal.var())\nprint(\"variance of batch 9 : \", clean_subsets[8].signal.var())\nprint(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM Autoencoders\n\nNow, we have clean data with removal drift. We will the clean data for the models. The models which we use are different architectures for ach batch because they have different distributions.\n\nIn last batch, we will use LSTM and Autoencoders together. An LSTM Autoencoder is an implementation of an autoencoder for sequence data using an Encoder-Decoder LSTM architecture. \n\nThe LSTM Autoencoder model consist of an encoder and en decoder. The encoder learns how to respresent the input into lower dimensions. The decoder is responsible for learning how to rebuild the smaller respresentations into the input again. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nfrom keras.utils import to_categorical\nfrom keras.layers import LSTM, Dense, Dropout, Flatten, BatchNormalization, Conv1D, Activation, RepeatVector, TimeDistributed\nfrom keras.models import Sequential, Input, Model\nfrom keras.regularizers import l1, l2, l1_l2\n\n\nclass lstm_model:\n    \n    def __init__(self, part1, part2):\n        self.part1 = part1\n        self.part2 = part2\n        #self.eval_subset_data = eval_subset_data\n        \n    def create_features(self):\n        part1 = self.part1\n        part2 = self.part2\n        \n        features = np.concatenate([clean_subsets[part1].signal, clean_subsets[part2].signal])\n        features = np.expand_dims(features, axis = 1)\n        \n        return features\n        \n    def create_target(self):\n        part1 = self.part1\n        part2 = self.part2\n        #target_set = self.subset_data.iloc[:,-1]\n        \n        target = np.concatenate([clean_subsets[part1].open_channels, clean_subsets[part2].open_channels])\n\n        \n        #target = np.array(target_set)\n        #target = to_categorical(target)\n        \n        return target\n    \n    def train_size(self, rate):\n    \n        all_size = 1000000\n        train_end = int(all_size*rate)\n        \n        return train_end\n        \n    \n    def create_train_features(self, train_rate):\n        features = self.create_features()\n        train_end = self.train_size(train_rate)\n        start = 0\n        \n        train_features = features[start:train_end]\n        train_features = np.expand_dims(train_features, axis = 1)\n        \n        return train_features\n    \n    \n    def create_test_features(self, train_rate):\n        features = self.create_features()\n        train_end = self.train_size(train_rate)\n        end = features.shape[0]\n        \n        test_features = features[train_end:end]\n        test_features = np.expand_dims(test_features, axis = 1)\n        \n        return test_features\n    \n    def create_train_target(self, train_rate):\n        target = self.create_target()\n        train_end = self.train_size(train_rate)\n        start = 0\n        \n        train_target = target[start:train_end]\n        train_target = to_categorical(train_target)\n        \n        return train_target\n    \n    def create_test_target(self, train_rate):\n        features = self.create_features()\n        target = self.create_target()\n        train_end = self.train_size(train_rate)\n        end = features.shape[0]\n        \n        test_target = target[train_end:end]\n        test_target = to_categorical(test_target)\n        \n        return test_target\n    \n    \n    def construct_model_for_batch_1_2(self):\n        features = self.create_features()\n        index = self.part1\n        \n        maximum_channel_number = ion_channels_number_max[index]\n        \n        model  = Sequential()\n\n        #model.add(Conv1D(1, 1, input_shape=(features.shape[1], 1), activation='relu'))\n        model.add(LSTM(1, input_shape=(features.shape[1], 1), return_sequences=False, activation='tanh'))\n        model.add(Dense(maximum_channel_number+1, activation='sigmoid'))\n        \n        return model\n    \n    def construct_model_for_batch_3_7(self):\n        features = self.create_features()\n        index = self.part1\n        \n        maximum_channel_number = ion_channels_number_max[index]\n        \n        model  = Sequential()\n        \n        #model.add(Conv1D(4, 1, input_shape=(features.shape[1], 1), activation='relu'))\n        model.add(LSTM(8, input_shape=(features.shape[1], 1), return_sequences=False, activation='tanh'))\n        model.add(Dense(maximum_channel_number+1, activation='sigmoid'))\n        \n        return model\n    \n    def construct_model_for_batch_4_8(self):\n        features = self.create_features()\n        index = self.part1\n        \n        maximum_channel_number = ion_channels_number_max[index]\n        \n        model  = Sequential()\n\n        model.add(Conv1D(16, 1, input_shape=(features.shape[1], 1), activation='relu'))\n        model.add(LSTM(16, return_sequences=False, activation='tanh'))\n        model.add(Dense(maximum_channel_number+1, activation='sigmoid'))\n        \n        return model\n    \n    def construct_model_for_batch_6_9(self):\n        features = self.create_features()\n        index = self.part1\n        \n        maximum_channel_number = ion_channels_number_max[index]\n        \n        model  = Sequential()\n\n        model.add(Conv1D(24, 1, input_shape=(features.shape[1], 1), activation='relu'))\n        model.add(LSTM(24, return_sequences=False, activation='tanh'))\n        model.add(Dense(maximum_channel_number+1, activation='sigmoid'))\n        \n        return model\n    \n    def construct_model_for_batch_5_10(self):\n        features = self.create_features()\n        index = self.part1\n        \n        maximum_channel_number = ion_channels_number_max[index]\n        \n        #model  = Sequential()\n\n        #model.add(Conv1D(128, 1, input_shape=(features.shape[1], 1), activation='relu'))\n        \n        input_data = Input(shape = (features.shape[1], 1))\n        encoded = LSTM(12, activation='tanh', return_sequences=True)(input_data)\n        encoded = LSTM(24, activation='tanh', return_sequences=True)(encoded)\n        encoded = LSTM(48, activation='tanh', return_sequences=False)(encoded)\n        \n        decoded = RepeatVector(features.shape[1])(encoded)\n        \n        decoded = LSTM(48, activation='tanh', return_sequences=True)(decoded)\n        decoded = LSTM(24, activation='tanh', return_sequences=True)(decoded)\n        decoded = LSTM(12, activation='tanh', return_sequences=True)(decoded)\n\n        output_data = LSTM(maximum_channel_number+1, activation = 'sigmoid', return_sequences=False)(decoded)\n        \n        model = Model(input_data, output_data)\n\n        \n        #model.add(LSTM(24, input_shape=(features.shape[1], 1), return_sequences=True, activation='tanh'))\n        #model.add(LSTM(12, return_sequences=False, activation='tanh'))\n        #model.add(RepeatVector(features.shape[1]))\n        #model.add(LSTM(maximum_channel_number+1, return_sequences = True, activation='tanh'))\n        \n        #model.add(LSTM(24, return_sequences=False, activation='tanh'))\n\n        #model.add(Dense(maximum_channel_number+1, activation='sigmoid'))\n        \n        return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_split_rate = 0.85\nmodel1 = lstm_model(part1=0, part2=1)\nprocessed_model1 = model1.construct_model_for_batch_1_2()\n\ntrain_feature_model1 = model1.create_train_features(train_split_rate)\ntrain_target_model1 = model1.create_train_target(train_split_rate)\n\ntest_feature_model1 = model1.create_test_features(train_split_rate)\ntest_target_model1 = model1.create_test_target(train_split_rate)\n\n\nprocessed_model1.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n\nrnn = processed_model1.fit(train_feature_model1, train_target_model1, batch_size=1000, epochs=20, \n                           validation_data=(test_feature_model1, test_target_model1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot the Loss Curves\nplt.figure(figsize=[8,6])\ntrain_loss, = plt.plot(rnn.history['loss'],'r',linewidth=3.0)\ntest_loss, = plt.plot(rnn.history['val_loss'],'b',linewidth=3.0)\nplt.legend([train_loss, test_loss], ['Training Loss', 'Test Loss'],fontsize=12)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Loss',fontsize=16)\nplt.title('Loss Curves',fontsize=16)\n \n#Plot the Accuracy Curves\nplt.figure(figsize=[8,6])\ntrain_accuracy, = plt.plot(rnn.history['accuracy'],'r',linewidth=3.0)\ntest_accuracy, = plt.plot(rnn.history['val_accuracy'],'b',linewidth=3.0)\nplt.legend([train_accuracy, test_accuracy], ['Training Accuracy', 'Test Accuracy'],fontsize=12)\n\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Accuracy',fontsize=16)\nplt.title('Accuracy Curves',fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#construct the lstm model for batch 3 and 7\n\ntrain_split_rate = 0.67\nmodel2 = lstm_model(2, 6)\nprocessed_model2 = model2.construct_model_for_batch_3_7()\n\ntrain_feature_model2 = model2.create_train_features(train_split_rate)\ntrain_target_model2 = model2.create_train_target(train_split_rate)\n\ntest_feature_model2 = model2.create_test_features(train_split_rate)\ntest_target_model2 = model2.create_test_target(train_split_rate)\n\n\nprocessed_model2.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n\nrnn = processed_model2.fit(train_feature_model2, train_target_model2, batch_size=1000, epochs=25, \n                           validation_data=(test_feature_model2, test_target_model2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot the Loss Curves\nplt.figure(figsize=[8,6])\ntrain_loss, = plt.plot(rnn.history['loss'],'r',linewidth=3.0)\ntest_loss, = plt.plot(rnn.history['val_loss'],'b',linewidth=3.0)\nplt.legend([train_loss, test_loss], ['Training Loss', 'Test Loss'],fontsize=12)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Loss',fontsize=16)\nplt.title('Loss Curves',fontsize=16)\n \n#Plot the Accuracy Curves\nplt.figure(figsize=[8,6])\ntrain_accuracy, = plt.plot(rnn.history['accuracy'],'r',linewidth=3.0)\ntest_accuracy, = plt.plot(rnn.history['val_accuracy'],'b',linewidth=3.0)\nplt.legend([train_accuracy, test_accuracy], ['Training Accuracy', 'Test Accuracy'],fontsize=12)\n\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Accuracy',fontsize=16)\nplt.title('Accuracy Curves',fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#construct the lstm model for batch 4 and 8\n\ntrain_split_rate = 0.75\nmodel3 = lstm_model(3, 7)\nprocessed_model3 = model3.construct_model_for_batch_4_8()\n\ntrain_feature_model3 = model3.create_train_features(train_split_rate)\ntrain_target_model3 = model3.create_train_target(train_split_rate)\n\ntest_feature_model3 = model3.create_test_features(train_split_rate)\ntest_target_model3 = model3.create_test_target(train_split_rate)\n\n\n#opt = keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\nprocessed_model3.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n\n\nrnn = processed_model3.fit(train_feature_model3, train_target_model3, batch_size=1000, epochs=40, \n                           validation_data=(test_feature_model3, test_target_model3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot the Loss Curves\nplt.figure(figsize=[8,6])\ntrain_loss, = plt.plot(rnn.history['loss'],'r',linewidth=3.0)\ntest_loss, = plt.plot(rnn.history['val_loss'],'b',linewidth=3.0)\nplt.legend([train_loss, test_loss], ['Training Loss', 'Test Loss'],fontsize=12)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Loss',fontsize=16)\nplt.title('Loss Curves',fontsize=16)\n \n#Plot the Accuracy Curves\nplt.figure(figsize=[8,6])\ntrain_accuracy, = plt.plot(rnn.history['accuracy'],'r',linewidth=3.0)\ntest_accuracy, = plt.plot(rnn.history['val_accuracy'],'b',linewidth=3.0)\nplt.legend([train_accuracy, test_accuracy], ['Training Accuracy', 'Test Accuracy'],fontsize=12)\n\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Accuracy',fontsize=16)\nplt.title('Accuracy Curves',fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#construct the lstm model for batch 6 and 9\n\ntrain_split_rate = 0.67\nmodel4 = lstm_model(5, 8)\nprocessed_model4 = model4.construct_model_for_batch_6_9()\n\ntrain_feature_model4 = model4.create_train_features(train_split_rate)\ntrain_target_model4 = model4.create_train_target(train_split_rate)\n\ntest_feature_model4 = model4.create_test_features(train_split_rate)\ntest_target_model4 = model4.create_test_target(train_split_rate)\n\n\nprocessed_model4.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n\nrnn = processed_model4.fit(train_feature_model4, train_target_model4, batch_size=1000, epochs=25, \n                           validation_data=(test_feature_model4, test_target_model4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot the Loss Curves\nplt.figure(figsize=[8,6])\ntrain_loss, = plt.plot(rnn.history['loss'],'r',linewidth=3.0)\ntest_loss, = plt.plot(rnn.history['val_loss'],'b',linewidth=3.0)\nplt.legend([train_loss, test_loss], ['Training Loss', 'Test Loss'],fontsize=12)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Loss',fontsize=16)\nplt.title('Loss Curves',fontsize=16)\n \n#Plot the Accuracy Curves\nplt.figure(figsize=[8,6])\ntrain_accuracy, = plt.plot(rnn.history['accuracy'],'r',linewidth=3.0)\ntest_accuracy, = plt.plot(rnn.history['val_accuracy'],'b',linewidth=3.0)\nplt.legend([train_accuracy, test_accuracy], ['Training Accuracy', 'Test Accuracy'],fontsize=12)\n\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Accuracy',fontsize=16)\nplt.title('Accuracy Curves',fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model5 = lstm_model(4, 9)\nprocessed_model5 = model5.construct_model_for_batch_5_10()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_model5.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#construct the lstm model for batch 5 and 10\nimport keras\nimport numpy as np\n\ntrain_split_rate = 0.75\nmodel5 = lstm_model(4, 9)\nprocessed_model5 = model5.construct_model_for_batch_5_10()\n\ntrain_feature_model5 = model5.create_train_features(train_split_rate)\ntrain_target_model5 = model5.create_train_target(train_split_rate)\n\ntest_feature_model5 = model5.create_test_features(train_split_rate)\ntest_target_model5 = model5.create_test_target(train_split_rate)\n\n\n#opt = keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\nprocessed_model5.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n\nrnn = processed_model5.fit(train_feature_model5, train_target_model5, batch_size=1000, epochs=30,\n                           validation_data=(test_feature_model5, test_target_model5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot the Loss Curves\nplt.figure(figsize=[8,6])\ntrain_loss, = plt.plot(rnn.history['loss'],'r',linewidth=3.0)\ntest_loss, = plt.plot(rnn.history['val_loss'],'b',linewidth=3.0)\nplt.legend([train_loss, test_loss], ['Training Loss', 'Test Loss'],fontsize=12)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Loss',fontsize=16)\nplt.title('Loss Curves',fontsize=16)\n \n#Plot the Accuracy Curves\nplt.figure(figsize=[8,6])\ntrain_accuracy, = plt.plot(rnn.history['accuracy'],'r',linewidth=3.0)\ntest_accuracy, = plt.plot(rnn.history['val_accuracy'],'b',linewidth=3.0)\nplt.legend([train_accuracy, test_accuracy], ['Training Accuracy', 'Test Accuracy'],fontsize=12)\n\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Accuracy',fontsize=16)\nplt.title('Accuracy Curves',fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nlet = ['A','B','C','D','E','F','G','H','I','J']\n\nr = eval_data.signal.rolling(30000).mean()\nplt.plot(eval_data.time.values,r)\n\nfor i in range(21): plt.plot([500+i*10,500+i*10],[-3,6],'r:')\nfor i in range(5): plt.plot([500+i*50,500+i*50],[-3,6],'r')\nfor k in range(4): plt.text(525+k*50,5.5,str(k+1),size=20)\nfor k in range(10): plt.text(505+k*10,4,let[k],size=16)\nplt.title('Test Signal Rolling Mean. Has Drift wherever plot is not horizontal line',size=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing drift for the subparts of A, B, and E in the batch 1 \n\n#removing drift in the A\nstart=500\na = 0; b = 100000\neval_data.loc[eval_data.index[a:b],'signal'] = eval_data.signal.values[a:b] - 3*(eval_data.time.values[a:b]-start)/10.\n\n#removing drift in the B\nstart=510\na = 100000; b = 200000\neval_data.loc[eval_data.index[a:b],'signal'] = eval_data.signal.values[a:b] - 3*(eval_data.time.values[a:b]-start)/10.\n\n#removing drift in the E\nstart=540\na = 400000; b = 500000\neval_data.loc[eval_data.index[a:b],'signal'] = eval_data.signal.values[a:b] - 3*(eval_data.time.values[a:b]-start)/10.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing drift for the subparts of G, H, and I in the batch 2 \n\n#removing drift in the G\nstart=560\na = 600000; b = 700000\neval_data.loc[eval_data.index[a:b],'signal'] = eval_data.signal.values[a:b] - 3*(eval_data.time.values[a:b]-start)/10.\n\n#removing drift in the H\nstart=570\na = 700000; b = 800000\neval_data.loc[eval_data.index[a:b],'signal'] = eval_data.signal.values[a:b] - 3*(eval_data.time.values[a:b]-start)/10.\n\n#removing drift in the I\nstart=580\na = 800000; b = 900000\neval_data.loc[eval_data.index[a:b],'signal'] = eval_data.signal.values[a:b] - 3*(eval_data.time.values[a:b]-start)/10.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = 1000000; b = 1500000\nprint(parabolic_drift_fit(eval_data.iloc[a:b]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing drift for the data in the batch 3\na = 1000000; b = 1500000\neval_data.loc[eval_data.index[a:b],'signal'] = eval_data.signal.values[a:b] - remove_parabolic_drift(eval_data.time[a:b].values, 4.99, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(20,5))\nr = eval_data.signal.rolling(30000).mean()\nplt.plot(eval_data.time.values,r)\nfor i in range(21): plt.plot([500+i*10,500+i*10],[-2,6],'r:')\nfor i in range(5): plt.plot([500+i*50,500+i*50],[-2,6],'r')\nfor k in range(4): plt.text(525+k*50,5.5,str(k+1),size=20)\nfor k in range(10): plt.text(505+k*10,4,let[k],size=16)\nplt.title('Test Signal Rolling Mean without Drift',size=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nres = 1000\nplt.plot(range(0,eval_data.shape[0],res),eval_data.signal[0::res])\nfor i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor i in range(21): plt.plot([i*100000,i*100000],[-5,12.5],'r:')\nfor k in range(4): plt.text(k*500000+250000,10,str(k+1),size=20)\nfor k in range(10): plt.text(k*100000+40000,7.5,let[k],size=16)\nplt.title('Test Signal without Drift',size=16)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let us remember the train data which has no drifting in order to analyze it and compare to test data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.plot(clean_data.time[::1000],clean_data.signal[::1000])\nplt.title('Training Batches 7-10 without Parabolic Drift',size=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will run different five models different part in the test data which has not drift data. As we see plot above;\n\n* we will use the model which is constructed batch 1 and 2 for the test data in the subparts of A, D, I, parts of 3 and 4\n* we will use the model which is constructed batch 3 and 7 for the test data in the subpart of E\n* we will use the model which is constructed batch 4 and 8 for the test data in the subparts of B and J\n* we will use the model which is constructed batch 6 and 9 for the test data in the subparts of C and G\n* we will use the model which is constructed batch 5 and 10 for the test data in the subparts of F and H\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/liverpool-ion-switching/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction of the subparts of A,D,I, the parts of 3 and 4\n\nfrom sklearn.metrics import f1_score, accuracy_score\n\neval_A = eval_data.signal.values[0:100000].reshape(-1,1)\neval_A = np.expand_dims(eval_A, axis = 2)\n\neval_D = eval_data.signal.values[300000:400000].reshape(-1,1)\neval_D = np.expand_dims(eval_D, axis = 2)\n\neval_I = eval_data.signal.values[800000:900000].reshape(-1,1)\neval_I = np.expand_dims(eval_I, axis = 2)\n\neval_3 = eval_data.signal.values[1000000:1500000].reshape(-1,1)\neval_3 = np.expand_dims(eval_3, axis = 2)\n\neval_4 = eval_data.signal.values[1500000:2000000].reshape(-1,1)\neval_4 = np.expand_dims(eval_4, axis = 2)\n\n\npred_A = processed_model1.predict_classes(eval_A)\npred_D = processed_model1.predict_classes(eval_D)\npred_I = processed_model1.predict_classes(eval_I)\npred_3 = processed_model1.predict_classes(eval_3)\npred_4 = processed_model1.predict_classes(eval_4)\n\n#assign the predictions to the submission data\nsubmission.iloc[0:100000, 1] = pred_A\nsubmission.iloc[300000:400000, 1] = pred_D\nsubmission.iloc[800000:900000, 1] = pred_I\nsubmission.iloc[1000000:1500000, 1] = pred_3\nsubmission.iloc[1500000:2000000, 1] = pred_4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction of the subpart of E\n\neval_E = eval_data.signal.values[400000:500000].reshape(-1,1)\neval_E = np.expand_dims(eval_E, axis = 2)\n\npred_E = processed_model2.predict_classes(eval_E)\n\n#assign the prediction to the submission data\nsubmission.iloc[400000:500000, 1] = pred_E","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction of the subparts of B and J\n\neval_B = eval_data.signal.values[100000:200000].reshape(-1,1)\neval_B = np.expand_dims(eval_B, axis = 2)\n\neval_J = eval_data.signal.values[900000:1000000].reshape(-1,1)\neval_J = np.expand_dims(eval_J, axis = 2)\n\npred_B = processed_model3.predict_classes(eval_B)\npred_J = processed_model3.predict_classes(eval_J)\n\n#assign the predictions to the submission data\nsubmission.iloc[100000:200000, 1] = pred_B\nsubmission.iloc[900000:1000000, 1] = pred_J","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction of the subparts of C and G\n\neval_C = eval_data.signal.values[200000:300000].reshape(-1,1)\neval_C = np.expand_dims(eval_C, axis = 2)\n\neval_G = eval_data.signal.values[600000:700000].reshape(-1,1)\neval_G = np.expand_dims(eval_G, axis = 2)\n\npred_C = processed_model4.predict_classes(eval_C)\npred_G = processed_model4.predict_classes(eval_G)\n\n#assign the predictions to the submission data\nsubmission.iloc[200000:300000, 1] = pred_C\nsubmission.iloc[600000:700000, 1] = pred_G","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction of the subparts of F and H\n\neval_F = eval_data.signal.values[500000:600000].reshape(-1,1)\neval_F = np.expand_dims(eval_F, axis = 2)\n\neval_H = eval_data.signal.values[700000:800000].reshape(-1,1)\neval_H = np.expand_dims(eval_H, axis = 2)\n\npred_F = processed_model5.predict_classes(eval_F)\npred_H = processed_model5.predict_classes(eval_H)\n\n#assign the predictions to the submission data\nsubmission.iloc[500000:600000, 1] = pred_F\nsubmission.iloc[700000:800000, 1] = pred_H","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nres = 1000\nplt.plot(range(0,eval_data.shape[0],res),submission.open_channels[0::res])\nfor i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor i in range(21): plt.plot([i*100000,i*100000],[-5,12.5],'r:')\nfor k in range(4): plt.text(k*500000+250000,10,str(k+1),size=20)\nfor k in range(10): plt.text(k*100000+40000,7.5,let[k],size=16)\nplt.title('Test Data Predictions',size=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('ion_switch_submission.csv', index = False, float_format='%.4f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}