{"cells":[{"metadata":{},"cell_type":"markdown","source":"## The purpose of this notebook\n\nI think most of us are familiar with [this great kernel](https://www.kaggle.com/cdeotte/rapids-knn-30-seconds-0-938) by Chris Deotte showing how to build a very promissing KNN model on the competition data using RAPIDS. A few days ago, I decided to play with this model and discovered a very annoying bug in the RAPIDS implementation of the KNN algorithm: its `predict_proba()` method returns a binary outcome of zeros and ones instead of actual probabilities. The good new is that Chris has already submitted a bug report, so hopefully it will be fixed soon. The bad news is that fixing it might take a while and the competition might be over by then. Fortunately, Chris suggested a clever workaround which I am going to share with you in this notebook. \n\nI will start by generating a simple data set for classification. Then I will train a RAPIDS KNN model on these data and try to make predictions using the `predict()` method first and the `predict_proba()` method second. The former will successfully return the class labels but the latter will only return binary outcomes. Then I will show you how to get actual KNN probabilities using the `kneighbors()` method of the RAPIDS `NearestNeighbors()` class. \n\n## Loading RAPIDS"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nimport sys\n!cp ../input/rapids/rapids.0.12.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.6/site-packages\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.6\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from cuml.neighbors import KNeighborsClassifier, NearestNeighbors\nimport cuml; cuml.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate data\n\nGenerate data for KNN classifier; then train the classifier on these data."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.datasets import make_blobs\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_blobs(n_samples=100, \n                  centers=5,\n                  cluster_std=5.0,\n                  n_features=4)\n\nknn = KNeighborsClassifier(n_neighbors=10)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80)\n\nknn.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is a gilimpse of our test data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions\n\nLet's try to predict classes (labels):"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predicting the labels seems to be working just fine. Let's try predicting probabilities using the `predict_proba` method."},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predicting probabilities fails: we only get binary zeros and ones, not the actual probabilities (in the case at hands we got only zeros -- it is possible to change the parameters of the `make_blobs()` function to get both zeros and ones).\n\n## Workaround\n\nThe idea of the workaround is very simple: let's find the train set indecies of the nearest neighbours for all points in the test set. Here is how it can be done: "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nKNN=10\nbatch=5\n\nclf = NearestNeighbors(n_neighbors=KNN)\n\nclf.fit(X_train)\n\ndistances, indices = clf.kneighbors(X_test)\n\nct = indices.shape[0]\n\npred = np.zeros((ct, KNN),dtype=np.int8)\n\nprobabilities = np.zeros((ct, len(np.unique(y_train))),dtype=np.float32)\n\nit = ct//batch + int(ct%batch!=0)\n\nfor k in range(it):\n    \n    a = batch*k; b = batch*(k+1); b = min(ct,b)\n    pred[a:b,:] = y_train[ indices[a:b].astype(int) ]\n    \n    for j in np.unique(y_train):\n        probabilities[a:b,j] = np.sum(pred[a:b,]==j,axis=1)/KNN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probabilities","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have successfully computed the probabilities! This method works pretty fast on the competition data -- it is possible to build it in a 5-fold cross-validation algorithm."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}