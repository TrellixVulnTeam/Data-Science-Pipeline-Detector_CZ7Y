{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Based on  https://www.kaggle.com/siavrez/wavenet-keras and https://www.kaggle.com/ragnar123/wavenet-with-1-more-feature\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport random\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv1D, Input, Dense, Add, Multiply\nfrom tensorflow.keras.callbacks import Callback, LearningRateScheduler\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import losses, models, optimizers\n\nfrom numba import cuda\n\nimport gc\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\"\"\" TF-Keras SWA: callback utility for performing stochastic weight averaging (SWA).\n\"\"\"\n\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.layers import BatchNormalization\n\nclass SWA(Callback):\n    \"\"\" Stochastic Weight Averging.\n    # Paper\n        title: Averaging Weights Leads to Wider Optima and Better Generalization\n        link: https://arxiv.org/abs/1803.05407\n    # Arguments\n        start_epoch:   integer, epoch when swa should start.\n        lr_schedule:   string, type of learning rate schedule.\n        swa_lr:        float, learning rate for swa sampling.\n        swa_lr2:       float, upper bound of cyclic learning rate.\n        swa_freq:      integer, length of learning rate cycle.\n        verbose:       integer, verbosity mode, 0 or 1.\n    \"\"\"\n    def __init__(self,\n                 start_epoch,\n                 lr_schedule='manual',\n                 swa_lr='auto',\n                 swa_lr2='auto',\n                 swa_freq=1,\n                 verbose=0):\n                 \n        super(SWA, self).__init__()\n        self.start_epoch = start_epoch - 1\n        self.lr_schedule = lr_schedule\n        self.swa_lr = swa_lr\n        self.swa_lr2 = swa_lr2\n        self.swa_freq = swa_freq\n        self.verbose = verbose\n\n        if start_epoch < 2:\n            raise ValueError('\"swa_start\" attribute cannot be lower than 2.')\n\n        schedules = ['manual', 'constant', 'cyclic']\n\n        if self.lr_schedule not in schedules:\n            raise ValueError('\"{}\" is not a valid learning rate schedule' \\\n                             .format(self.lr_schedule))\n\n        if self.lr_schedule == 'cyclic' and self.swa_freq < 2:\n            raise ValueError('\"swa_freq\" must be higher than 1 for cyclic schedule.')\n\n        if self.swa_lr == 'auto' and self.swa_lr2 != 'auto':\n            raise ValueError('\"swa_lr2\" cannot be manually set if \"swa_lr\" is automatic.') \n            \n        if self.lr_schedule == 'cyclic' and self.swa_lr != 'auto' \\\n           and self.swa_lr2 != 'auto' and self.swa_lr > self.swa_lr2:\n            raise ValueError('\"swa_lr\" must be lower than \"swa_lr2\".')\n\n    def on_train_begin(self, logs=None):\n\n        self.epochs = self.params.get('epochs')\n\n        if self.start_epoch >= self.epochs - 1:\n            raise ValueError('\"swa_start\" attribute must be lower than \"epochs\".')\n\n        self.init_lr = K.eval(self.model.optimizer.lr)\n\n        # automatic swa_lr\n        if self.swa_lr == 'auto':\n            self.swa_lr = 0.1*self.init_lr\n        \n        if self.init_lr < self.swa_lr:\n            raise ValueError('\"swa_lr\" must be lower than rate set in optimizer.')\n\n        # automatic swa_lr2 between initial lr and swa_lr   \n        if self.lr_schedule == 'cyclic' and self.swa_lr2 == 'auto':\n            self.swa_lr2 = self.swa_lr + (self.init_lr - self.swa_lr)*0.25\n\n        self._check_batch_norm()\n\n    def on_epoch_begin(self, epoch, logs=None):\n\n        self.current_epoch = epoch\n        self._scheduler(epoch)\n\n        # constant schedule is updated epoch-wise\n        if self.lr_schedule == 'constant' or self.is_batch_norm_epoch:\n            self._update_lr(epoch)\n\n        if self.is_swa_start_epoch:\n            self.swa_weights = self.model.get_weights()\n\n            if self.verbose > 0:\n                print('\\nEpoch %05d: starting stochastic weight averaging'\n                      % (epoch + 1))\n\n        if self.is_batch_norm_epoch:\n            self._set_swa_weights(epoch)\n\n            if self.verbose > 0:\n                print('\\nEpoch %05d: reinitializing batch normalization layers'\n                      % (epoch + 1))\n\n            self._reset_batch_norm()\n\n            if self.verbose > 0:\n                print('\\nEpoch %05d: running forward pass to adjust batch normalization'\n                      % (epoch + 1))\n\n    def on_batch_begin(self, batch, logs=None):\n\n        # update lr each batch for cyclic lr schedule\n        if self.lr_schedule == 'cyclic':\n            self._update_lr(self.current_epoch, batch)\n\n        if self.is_batch_norm_epoch:\n\n            batch_size = self.params['samples']\n            momentum = batch_size / (batch*batch_size + batch_size)\n\n            for layer in self.batch_norm_layers:\n                layer.momentum = momentum\n\n    def on_batch_end(self, batch, logs=None):\n        logs = logs or {}\n        logs['lr'] = K.eval(self.model.optimizer.lr)\n        for k, v in logs.items():\n            if k == 'lr':\n                self.model.history.history.setdefault(k, []).append(v)\n\n    def on_epoch_end(self, epoch, logs=None):\n\n        if self.is_swa_start_epoch:\n            self.swa_start_epoch = epoch\n\n        if self.is_swa_epoch and not self.is_batch_norm_epoch:\n            self.swa_weights = self._average_weights(epoch)\n\n    def on_train_end(self, logs=None):\n\n        if not self.has_batch_norm:\n            self._set_swa_weights(self.epochs)\n        else:\n            self._restore_batch_norm()\n\n    def _scheduler(self, epoch):\n\n        swa_epoch = (epoch - self.start_epoch)\n\n        self.is_swa_epoch = epoch >= self.start_epoch and swa_epoch % self.swa_freq == 0\n        self.is_swa_start_epoch = epoch == self.start_epoch\n        self.is_batch_norm_epoch = epoch == self.epochs - 1 and self.has_batch_norm\n\n    def _average_weights(self, epoch):\n\n        return [(swa_w * (epoch - self.start_epoch) + w)\n                / ((epoch - self.start_epoch) + 1)\n                for swa_w, w in zip(self.swa_weights, self.model.get_weights())]\n\n    def _update_lr(self, epoch, batch=None):\n\n        if self.is_batch_norm_epoch:\n            lr = 0\n            K.set_value(self.model.optimizer.lr, lr)\n        elif self.lr_schedule == 'constant':\n            lr = self._constant_schedule(epoch)\n            K.set_value(self.model.optimizer.lr, lr)\n        elif self.lr_schedule == 'cyclic':\n            lr = self._cyclic_schedule(epoch, batch)\n            K.set_value(self.model.optimizer.lr, lr)\n\n    def _constant_schedule(self, epoch):\n\n        t = epoch / self.start_epoch\n        lr_ratio = self.swa_lr / self.init_lr\n        if t <= 0.5:\n            factor = 1.0\n        elif t <= 0.9:\n            factor = 1.0 - (1.0 - lr_ratio) * (t - 0.5) / 0.4\n        else:\n            factor = lr_ratio\n        return self.init_lr * factor\n\n    def _cyclic_schedule(self, epoch, batch):\n        \"\"\" Designed after Section 3.1 of Averaging Weights Leads to\n        Wider Optima and Better Generalization(https://arxiv.org/abs/1803.05407)\n        \"\"\"\n        # steps are mini-batches per epoch, equal to training_samples / batch_size\n        steps = self.params.get('steps')\n        \n        #occasionally steps parameter will not be set. We then calculate it ourselves\n        if steps == None:\n            steps = self.params['samples'] // self.params['batch_size']\n        \n        swa_epoch = (epoch - self.start_epoch) % self.swa_freq\n        cycle_length = self.swa_freq * steps\n\n        # batch 0 indexed, so need to add 1\n        i = (swa_epoch * steps) + (batch + 1)\n        if epoch >= self.start_epoch:\n            t = (((i-1) % cycle_length) + 1)/cycle_length\n            return (1-t)*self.swa_lr2 + t*self.swa_lr\n        else:\n            return self._constant_schedule(epoch)\n\n    def _set_swa_weights(self, epoch):\n\n        self.model.set_weights(self.swa_weights)\n\n        if self.verbose > 0:\n            print('\\nEpoch %05d: final model weights set to stochastic weight average'\n                  % (epoch + 1))\n\n    def _check_batch_norm(self):\n\n        self.batch_norm_momentums = []\n        self.batch_norm_layers = []\n        self.has_batch_norm = False\n        self.running_bn_epoch = False\n\n        for layer in self.model.layers:\n            if issubclass(layer.__class__, BatchNormalization):\n                self.has_batch_norm = True\n                self.batch_norm_momentums.append(layer.momentum)\n                self.batch_norm_layers.append(layer)\n\n        if self.verbose > 0 and self.has_batch_norm:\n            print('Model uses batch normalization. SWA will require last epoch '\n                  'to be a forward pass and will run with no learning rate')\n\n    def _reset_batch_norm(self):\n\n        for layer in self.batch_norm_layers:\n\n            # to get properly initialized moving mean and moving variance weights\n            # we initialize a new batch norm layer from the config of the existing\n            # layer, build that layer, retrieve its reinitialized moving mean and\n            # moving var weights and then delete the layer\n            bn_config = layer.get_config()\n            new_batch_norm = BatchNormalization(**bn_config)\n            new_batch_norm.build(layer.input_shape)\n            new_moving_mean, new_moving_var = new_batch_norm.get_weights()[-2:]\n            # get rid of the new_batch_norm layer\n            del new_batch_norm\n            # get the trained gamma and beta from the current batch norm layer\n            trained_weights = layer.get_weights()\n            new_weights = []\n            # get gamma if exists\n            if bn_config['scale']:\n                new_weights.append(trained_weights.pop(0))\n            # get beta if exists\n            if bn_config['center']:\n                new_weights.append(trained_weights.pop(0))\n            new_weights += [new_moving_mean, new_moving_var]\n            # set weights to trained gamma and beta, reinitialized mean and variance\n            layer.set_weights(new_weights)\n\n    def _restore_batch_norm(self):\n\n        for layer, momentum in zip(self.batch_norm_layers, self.batch_norm_momentums):\n            layer.momentum = momentum","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# configurations and main hyperparammeters\nEPOCHS = 250\nNNBATCHSIZE = 16\nGROUP_BATCH_SIZE = 4000\nSEED = 321\nLR = 0.001\nSPLITS = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data():\n    train = pd.read_csv('../input/1-remove-drift-ac/train_feat_100k.csv.gz', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n    test  = pd.read_csv('../input/1-remove-drift-ac/test_feat_100k.csv.gz', dtype={'time': np.float32, 'signal': np.float32})\n    sub  = pd.read_csv('../input/liverpool-ion-switching/sample_submission.csv', dtype={'time': np.float32})\n    return train, test, sub\n\n# create batches of 4000 observations\ndef batching(df, batch_size):\n    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n    df['group'] = df['group'].astype(np.uint16)\n    return df\n\ndef normalize(train,test,features):\n    scal = preprocessing.StandardScaler()\n    scal.fit(train[features])\n    train[features] = scal.transform(train[features])\n    test[features] = scal.transform(test[features])\n    return train, test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df: pd.DataFrame,\n                     verbose: bool = True) -> pd.DataFrame:\n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n\n                if (c_min > np.iinfo(np.int32).min\n                      and c_max < np.iinfo(np.int32).max):\n                    df[col] = df[col].astype(np.int32)\n                elif (c_min > np.iinfo(np.int64).min\n                      and c_max < np.iinfo(np.int64).max):\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (c_min > np.finfo(np.float16).min\n                        and c_max < np.finfo(np.float16).max):\n                    df[col] = df[col].astype(np.float16)\n                elif (c_min > np.finfo(np.float32).min\n                      and c_max < np.finfo(np.float32).max):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    reduction = (start_mem - end_mem) / start_mem\n\n    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n    if verbose:\n        print(msg)\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def Classifier(shape_):\n    \n    def wave_block(x, filters, kernel_size, n):\n        dilation_rates = [2**i for i in range(n)]\n        x = Conv1D(filters = filters,\n                   kernel_size = 1,\n                   padding = 'same')(x)\n        res_x = x\n        for dilation_rate in dilation_rates:\n            tanh_out = Conv1D(filters = filters,\n                              kernel_size = kernel_size,\n                              padding = 'same', \n                              activation = 'tanh', \n                              dilation_rate = dilation_rate)(x)\n            sigm_out = Conv1D(filters = filters,\n                              kernel_size = kernel_size,\n                              padding = 'same',\n                              activation = 'sigmoid', \n                              dilation_rate = dilation_rate)(x)\n            x = Multiply()([tanh_out, sigm_out])\n            x = Conv1D(filters = filters,\n                       kernel_size = 1,\n                       padding = 'same')(x)\n            res_x = Add()([res_x, x])\n        return res_x\n    \n    inp = Input(shape = (shape_))\n    \n    x = wave_block(inp, 16, 3, 12)\n    x1 = wave_block(x, 32, 3, 8)\n    x2 = wave_block(x1, 64, 3, 4)\n    x3 = wave_block(x2, 128, 3, 1)\n    \n    \n    out = Dense(11, activation = 'softmax', name = 'out')(x3)\n    \n    model = models.Model(inputs = inp, outputs = out)\n    \n    opt = Adam(lr = LR)\n    model.compile(loss = losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n    return model\n\n# function that decrease the learning as epochs increase (i also change this part of the code)\ndef lr_schedule(epoch):\n    if epoch < 30:\n        lr = LR\n    elif epoch < 40:\n        lr = LR / 3\n    elif epoch < 50:\n        lr = LR / 5\n    elif epoch < 60:\n        lr = LR / 7\n    elif epoch < 70:\n        lr = LR / 9\n    elif epoch < 80:\n        lr = LR / 11\n    elif epoch < 90:\n        lr = LR / 13\n    elif epoch < 120:\n        lr = LR / 15\n    else:\n        lr = LR / 17\n    return lr\n\n# class to get macro f1 score. This is not entirely necessary but it's fun to check f1 score of each epoch (be carefull, if you use this function early stopping callback will not work)\nclass MacroF1(Callback):\n    def __init__(self, model, inputs, targets):\n        self.model = model\n        self.inputs = inputs\n        self.targets = np.argmax(targets, axis = 2).reshape(-1)\n        \n    def on_epoch_end(self, epoch, logs):\n        pred = np.argmax(self.model.predict(self.inputs), axis = 2).reshape(-1)\n        score = f1_score(self.targets, pred, average = 'macro')\n        print(f'F1 Macro Score: {score:.5f}')\n        \n\nstart_epoch = 200\n\n# define swa callback\nswa = SWA(start_epoch=start_epoch, \n          lr_schedule='manual',\n          swa_lr=0.001, \n          verbose=1)\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# main function to perfrom groupkfold cross validation (we have 1000 vectores of 4000 rows and 8 features (columns)). Going to make 5 groups with this subgroups.\ndef run_cv_model_by_batch(train, test, splits, batch_col, feats, sample_submission, nn_epochs, nn_batch_size):\n    seed_everything(SEED)\n    K.clear_session()\n    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n    tf.compat.v1.keras.backend.set_session(sess)\n      \n    oof_ = np.zeros((len(train), 11)) # build out of folds matrix with 11 columns, they represent our target variables classes (from 0 to 10)\n    preds_ = np.zeros((len(test), 11))\n    target = ['open_channels']\n    group = train['group']\n    kf = GroupKFold(n_splits=5)\n    splits = [x for x in kf.split(train, train[target], group)]\n\n    new_splits = []\n    for sp in splits:\n        new_split = []\n        new_split.append(np.unique(group[sp[0]]))\n        new_split.append(np.unique(group[sp[1]]))\n        new_split.append(sp[1])    \n        new_splits.append(new_split)\n    # pivot target columns to transform the net to a multiclass classification estructure (you can also leave it in 1 vector with sparsecategoricalcrossentropy loss function)\n    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n\n    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n    target_cols = ['target_'+str(i) for i in range(11)]\n    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values.astype(np.float32))))\n    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n    \n    gc.collect()\n    \n    for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n        print(f'Our training dataset shape is {train_x.shape}')\n        print(f'Our validation dataset shape is {valid_x.shape}')\n\n        shape_ = (None, train_x.shape[2]) # input is going to be the number of feature we are using (dimension 2 of 0, 1, 2)\n        model = Classifier(shape_)\n        # using our lr_schedule function\n        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n        model.fit(train_x,train_y,\n                  epochs = nn_epochs,\n                  callbacks = [swa,cb_lr_schedule],# MacroF1(model, valid_x, valid_y)], # adding custom evaluation metric for each epoch\n                  batch_size = nn_batch_size,verbose = 2,\n                  validation_data = (valid_x,valid_y))\n        gc.collect()\n        preds_f = model.predict(valid_x)\n        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  np.argmax(preds_f, axis=2).reshape(-1), average = 'macro') # need to get the class with the biggest probability\n        print(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n        oof_[val_orig_idx,:] += preds_f\n        te_preds = model.predict(test)\n        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n        preds_ += te_preds / SPLITS\n    \n        \n    # calculate the oof macro f1_score\n    f1_score_ = f1_score(np.argmax(train_tr, axis = 2).reshape(-1),  np.argmax(oof_, axis = 1), average = 'macro') # axis 2 for the 3 Dimension array and axis 1 for the 2 Domension Array (extracting the best class)\n    print(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n    \n    np.save(f'./oof.{f1_score_:1.5f}.npy',oof_)\n    np.save(f'./preds.{f1_score_:1.5f}.npy',preds_)\n    \n    \n    sample_submission['open_channels'] = np.argmax(preds_, axis = 1).astype(int)\n    sample_submission.to_csv(f'./submission_wavenet.{f1_score_:1.5f}.csv', index=False, float_format='%.4f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Reading Data Started...')\ntrain, test, sample_submission = read_data()\ntrain=train[0:5004000]\ngc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=['n','pt', 'm2am_50', 'm2am_100', 'm2am_500', 'P3', 'm2am_1000', 'signal', 'm2am_10', 'm2am_10000',\n     'P0', 'P1', 'P4', 'sdam_10000', 'P2', 'm2am_5', 'P6', 'P9', 'P5', 'P8', 'P7', 'P10', 'sdm_5000',\n     'sdm_500', 'sdam_1000', 'lma_2', 'lmi_3', 'lmi_2', 'sdam_5000', 'm2am_5000', 'm2_2', 'lmi_5', \n     'sdm_10000', 'lmed_2', 'lead_1', 'lma_3', 'm2am_4', 'pt1', 'lmi_4', 'leadm_1', 'vp', 'lmim_2',\n     'lma_50', 'lmi_10', 'lma_10', 'lmim_10', 'lmed_3', 'sdm_10', 'vp1', 'm2m_10000', 'sdam_50']\n\nGROUP_BATCH_SIZE = 4000\n\ntrain, test = normalize(train,test,['signal'])\ngc.collect()\n\ntest=test[features]\nfeatures.append('open_channels')\ntrain=train[features]\n\ntrain = batching(train, batch_size = GROUP_BATCH_SIZE)\ntest  = batching(test , batch_size = GROUP_BATCH_SIZE)\n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\n\nfeatures = [col for col in train.columns if col not in ['index', 'group', 'open_channels', 'time']]\n\ngc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started...')\nrun_cv_model_by_batch(train, test, SPLITS, 'group', features, sample_submission, EPOCHS, NNBATCHSIZE)\nprint('Training completed...')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}