{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nThis notebook is based on this clean implementation of the [Viterbi Algorithm](https://www.kaggle.com/miklgr500/viterbi-algorithm-without-segmentation-on-groups), which in turn was inspired by [this notebook](https://www.kaggle.com/friedchips/the-viterbi-algorithm-a-complete-solution). It shows how a relative high score can be achieved using only the provided signal by taking into account the sequential nature of the data. \n\nI made the following changes that improved my score locally:\n* The signal does not need to be discretized to calculate `p_signal`. Instead, estimate the mean and standard deviation using the available labeled data. This allows us to construct a gaussian distribution the signals of each `open_channels` value. We can then use the probability density function of this distribution to get a more accurate `p_signal`.\n* Changed the calculation of the Viterbi loop.\n* Fit different models for the different types of data. This is because both the `p_trans` and `p_signal` will differ in each batch.\n\nThe groups are made by eyeballing the plots in [this notebook](https://www.kaggle.com/cdeotte/one-feature-model-0-930) (Model 0, 1, 2, 3, 4 correspond to 1s, 1f, 3, 5 and 10 respectively.). While this notebook is not yet scoring > 0.940, it could be interesting to add `T1`, which is calculated during the Viterbi algorithm to your feature set.\n\n**I also have an implementation of the [forward-backward (posterior decoding) algorithm](https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm), which tends to achieve a little bit better results, but it is a lot slower. Let me know if you are interested!**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n\nfrom scipy.stats import norm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/ghost-drift-and-outliers/train_clean_kalman.csv')\ntest  = pd.read_csv('../input/ghost-drift-and-outliers/test_clean_kalman.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Viterbi algorithm (collapsed)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class ViterbiClassifier:\n    def __init__(self):\n        self._p_trans = None\n        self._p_signal = None\n        self._p_in = None\n    \n    def fit(self, x, y):\n        self._p_trans = self.markov_p_trans(y)\n        self._dists = []\n        self._states = len(np.unique(y))\n        for s in np.arange(y.min(), y.max() + 1):\n            self._dists.append((np.mean(x[y == s]), np.std(x[y == s])))\n        \n        return self\n        \n    def predict(self, x):\n        p_signal = self.markov_p_signal(x)\n        return self.viterbi(self._p_trans, p_signal, x)\n    \n    def markov_p_signal(self, signal):\n        p_signal = np.zeros((self._states, len(signal)))\n        for k, dist in enumerate(self._dists):\n            p_signal[k, :] = norm.pdf(signal, *dist)\n            \n        return p_signal\n    \n    def markov_p_trans(self, states):\n        # https://www.kaggle.com/friedchips/the-viterbi-algorithm-a-complete-solution\n        max_state = np.max(states)\n        states_next = np.roll(states, -1)\n        matrix = []\n        for i in range(max_state + 1):\n            current_row = np.histogram(states_next[states == i], bins=np.arange(max_state + 2))[0]\n            if np.sum(current_row) == 0: # if a state doesn't appear in states...\n                current_row = np.ones(max_state + 1) / (max_state + 1) # ...use uniform probability\n            else:\n                current_row = current_row / np.sum(current_row) # normalize to 1\n            matrix.append(current_row)\n        return np.array(matrix)\n    \n    def viterbi(self, p_trans, p_signal, signal):\n        # https://www.kaggle.com/friedchips/the-viterbi-algorithm-a-complete-solution\n        offset = 10**(-20) # added to values to avoid problems with log2(0)\n\n        p_trans_tlog  = np.transpose(np.log2(p_trans  + offset)) # p_trans, logarithm + transposed\n        p_signal_tlog = np.transpose(np.log2(p_signal + offset)) # p_signal, logarithm + transposed\n        \n        T1 = np.zeros(p_signal.shape)\n        T2 = np.zeros(p_signal.shape)\n\n        T1[:, 0] = p_signal_tlog[0, :]\n        T2[:, 0] = 0\n\n        for j in range(1, p_signal.shape[1]):\n            for i in range(len(p_trans)):\n                T1[i, j] = np.max(T1[:, j - 1] + p_trans_tlog[:, i] + p_signal_tlog[j, i])\n                T2[i, j] = np.argmax(T1[:, j - 1] + p_trans_tlog[:, i] + p_signal_tlog[j, i])\n\n        x = np.empty(p_signal.shape[1], 'B')\n        x[-1] = np.argmax(T1[:, p_signal.shape[1] - 1])\n        for i in reversed(range(1, p_signal.shape[1])):\n            x[i - 1] = T2[x[i], i]\n    \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['batch'] = (train['time'] - 0.0001) // 50\ncounts = train.groupby('batch').count()['time'].values\nmodels = [0, 0, 1, 2, 4, 3, 1, 2, 3, 4]\nblocks = [[], [], [], [], []]\ntotal = 0\nfor model, count in zip(models, counts):\n    blocks[model].extend(list(range(total, total + count)))\n    total += count\nprint([len(x) for x in blocks])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_state = train.open_channels.values\nsignal = train.signal.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's show the (gaussian) distributions of the signals\nf, ax = plt.subplots(1, len(blocks), figsize=(20, 5))\nfor i, ix in enumerate(blocks):\n    for label in set(true_state[ix]):\n        pd.Series(signal[ix][true_state[ix] == label]).plot(kind='hist', ax=ax[i], \n                                                            alpha=0.5, label=label)\n    ax[i].set_title('Data #{}'.format(i))\n    ax[i].legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\ntrain_predictions = np.zeros(len(signal))\nfor i, ix in enumerate(blocks):\n    sub_signal = signal[ix]\n    viterbi = ViterbiClassifier().fit(sub_signal, true_state[ix])\n    models.append(viterbi)\n    \n    train_predictions[ix] = viterbi.predict(sub_signal)\n    print('[Model #{}] F1 (macro) = {}'.format(i, f1_score(y_pred=train_predictions[ix], y_true=true_state[ix], average='macro')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total Accuracy =\", accuracy_score(y_pred=train_predictions, y_true=true_state))\nprint(\"Total F1 (macro) =\", f1_score(y_pred=train_predictions, y_true=true_state, average='macro'))\n\n# Total Accuracy = 0.9670930385544279\n# Total F1 (macro) = 0.9359432322559637\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_blocks = [\n    list(range(0, 100000)) + list(range(300000, 400000)) + list(range(800000, 900000)) + list(range(1000000, 2000000)),\n    list(range(400000, 500000)),\n    list(range(100000, 200000)) + list(range(900000, 1000000)),\n    list(range(200000, 300000)) + list(range(600000, 700000)),\n    list(range(500000, 600000)) + list(range(700000, 800000))\n]\n\n# Sanity check\nassert sum([len(x) for x in test_blocks]) == 2000000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_subm = pd.read_csv(\"../input/liverpool-ion-switching/sample_submission.csv\")\nfor i, ix in enumerate(test_blocks):\n    df_subm.loc[ix, 'open_channels'] = models[i].predict(test.signal.values[ix])\ndf_subm.to_csv(\"viterbi.csv\", float_format='%.4f', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sanity check \n# https://www.kaggle.com/cdeotte/one-feature-model-0-930\nplt.figure(figsize=(20,5))\nres = 1000; let = ['A','B','C','D','E','F','G','H','I','J']\nplt.plot(range(0,test.shape[0],res),df_subm.open_channels[0::res])\nfor i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor i in range(21): plt.plot([i*100000,i*100000],[-5,12.5],'r:')\nfor k in range(4): plt.text(k*500000+250000,10,str(k+1),size=20)\nfor k in range(10): plt.text(k*100000+40000,7.5,let[k],size=16)\nplt.title('Test Data Predictions',size=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}