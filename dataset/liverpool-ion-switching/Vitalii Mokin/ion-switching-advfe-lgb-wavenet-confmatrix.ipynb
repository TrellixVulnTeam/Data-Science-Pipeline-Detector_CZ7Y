{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# [University of Liverpool - Ion Switching](https://www.kaggle.com/c/liverpool-ion-switching)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## The kernel initially consist of a thorough overview of the parameters of all successful commits, the result (LB) obtained and their analysis (see section \"My upgrade\").\n\n## I developed a shell to build different models, check the result of their work, using Confusion matrices and weighted average of their predictions.\n\n## I am experimenting with different good kernels for FE, models and their tuning. Already used more 10 different basic kernels (in each commit their list is in the \"Acknowledgements\" section and - in the kernel's code when the code was taken from the basic kernel unchanged or almost unchanged).\n\nIn the section \"Acknowledgements\" I make a link to the basic kernel where I got the code from, and to where the code came from in the this basic kernel (the original source), after that I upvoted all these good kernels.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Acknowledgements\n### Notebooks:\n\n* [Physically Possible](https://www.kaggle.com/jazivxt/physically-possible)\n* [Simple EDA-Model](https://www.kaggle.com/siavrez/simple-eda-model)\n* [MM 2020 NCAAM: LGB, XGB, LogReg - Tuning&Merging](https://www.kaggle.com/vbmokin/mm-2020-ncaam-lgb-xgb-logreg-tuning-merging)\n* [Merging FE & Prediction - xgb, lgb, logr, linr](https://www.kaggle.com/vbmokin/merging-fe-prediction-xgb-lgb-logr-linr)\n* [BOD prediction in river - 15 regression models](https://www.kaggle.com/vbmokin/bod-prediction-in-river-15-regression-models)\n* [Automatic selection from 20 classifier models](https://www.kaggle.com/vbmokin/automatic-selection-from-20-classifier-models)\n* [ðŸ’¥3 Simple Ideas [Ensemble]](https://www.kaggle.com/teejmahal20/3-simple-ideas-ensemble)\n* [Single Model lgbm - Kalman Filter](https://www.kaggle.com/teejmahal20/single-model-lgbm-kalman-filter)\n* [Wavenet with SHIFTED-RFC Proba and CBR](https://www.kaggle.com/nxrprime/wavenet-with-shifted-rfc-proba-and-cbr)\n* [Wavenet with SHIFTED-RFC Proba & CBR - FE upgrade](https://www.kaggle.com/vbmokin/wavenet-with-shifted-rfc-proba-cbr-fe-upgrade)\n* [Wavenet with SHIFTED-RFC Proba](https://www.kaggle.com/sggpls/wavenet-with-shifted-rfc-proba)\n* [WaveNet-Keras](https://www.kaggle.com/siavrez/wavenet-keras)\n* [SHIFTED-RFC Pipeline](https://www.kaggle.com/sggpls/shifted-rfc-pipeline)\n\n### Datasets:\n* [Data Without Drift](https://www.kaggle.com/cdeotte/data-without-drift)\n* [ION-SHIFTED-RFC-PROBA](https://www.kaggle.com/sggpls/ion-shifted-rfc-proba)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.1\"></a>\n## Table of Contents\n\n1. [My upgrade](#1)\n    -  [Commit now](#1.1)\n    -  [Previous commits: LGB-1, FE-1,2](#1.2)\n    -  [Previous commits: LGB-2, FE-3](#1.3)\n    -  [Previous commits: LGB-2, MLP, LogReg, FE-3](#1.4)\n    -  [Previous commits: Advanced FE & LGB-2](#1.5)\n    -  [Previous commits: Advanced FE & LGB-2 & XGB-1](#1.6)\n1. [Import libraries](#2)\n1. [Download data & FE](#3)\n1. [Models tuning](#4)\n    -  [Ridge Regression](#4.1)\n    -  [SGDRegressor](#4.2)    \n    -  [Logistic Regression](#4.3)\n    -  [MLP](#4.4)\n    -  [LGB](#4.5)\n    -  [XGB](#4.6)\n    -  [Wavenet with SHIFTED-RFC Proba and CBR](#4.7)\n1. [Showing Confusion Matrices](#5)\n1. [Comparison and merging solutions](#6)\n1. [Submission](#7)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. My upgrade<a class=\"anchor\" id=\"1\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The using and tuning different tuning models, building Confusion matrices, selection of seed_random.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1.1. Commit now <a class=\"anchor\" id=\"1.1\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# FE-4\n\n# Seed for random_state\nseed_random = 42\n\n# FE-2 : window_sizes = [10, 25, 50, 100, 500, 1000, 5000, 10000, 25000, 50000, 100000, 500000]\n# About selection of these values see my post: https://www.kaggle.com/c/liverpool-ion-switching/discussion/135073\nwindow_sizes = [10, 50]\n# with new my features: signal_shift_+2, signal_shift_-2\n# without ratio-, diff-features\n# without 'medianbatch', 'abs_avgbatch', 'abs_maxbatch' with smallest feature importance\n\n# LGB-2 model tuning\nlr_lgb = 0.05\nnum_leaves = 200\nnum_iterations = 1500  # recommended 2000\n# 'metric'is 'logloss'\n\n# Wavenet with SHIFTED-RFC Proba and CBR model tuning (the pilot version)\nlr_wn = 0.0015  # recommended 0.0015\nEPOCHS_wn = 15  # recommended 180\nSPLITS = 2      # recommended 6\n\n# XGB-1 model tuning\n# lr_xgb = 0.05\n# max_depth_xgb = 10\n# num_boost_round_xgb = 1000\n#'eval_metric'is 'logloss'\n\n# Set weight of models\nw_lgb = 0.5\nw_wnet = 1 - w_lgb\nprint(w_wnet)\n# without LogReg, MLP, XGB, Ridge Regression and SGDRegressor models","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Variant 1 of FE (FE-1) from the kernels: https://www.kaggle.com/suicaokhoailang/an-embarrassingly-simple-baseline-0-960-lb and https://www.kaggle.com/tunguz/simple-ion-ridge-regression-starter\n* Variant 2 of FE (FE-2) from the kernel: https://www.kaggle.com/teejmahal20/regression-with-optimized-rounder and https://www.kaggle.com/pestipeti/eda-ion-switching\n* Variant 3 of FE (FE-3) from the kernels: https://www.kaggle.com/jazivxt/physically-possible and https://www.kaggle.com/siavrez/simple-eda-model\n* Variant 4 of FE (FE-4) from the kernels: https://www.kaggle.com/nxrprime/wavenet-with-shifted-rfc-proba-and-cbr","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* LGB model 1 (LGB-1) from the kernel https://www.kaggle.com/teejmahal20/regression-with-optimized-rounder\n* LGB model 2 (LGB-2) from the kernel https://www.kaggle.com/jazivxt/physically-possible","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* XGB model 1 (XGB-1) from the kernel https://www.kaggle.com/teejmahal20/3-simple-ideas-ensemble","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* \"Wavenet with SHIFTED-RFC Proba and CBR\" model 1 (WN-1) from the kernel https://www.kaggle.com/nxrprime/wavenet-with-shifted-rfc-proba-and-cbr","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Commits 1-12 (FE-1) are created under the first metric variant and are currently unsuccessful","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1.2. Previous commits: LGB-1, FE-1,2 <a class=\"anchor\" id=\"1.2\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 13\n\n* seed_random = 42\n* lr = 0.05\n* num_iterations = 2000\n* w_ridge = 0.05\n* w_sgd = 0.05\n* w_lgb = 0.85 <- basic LGB-1\n* w_logreg = 1 - w_ridge - w_sgd - w_lgb\n* FE-1\n* FE-2 (window_sizes = [20] replace of [10, 25, 50, 100, 500, 1000, 5000, 10000, 25000])\n\n**LB = 0.179**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 14\n\n* seed_random = 42\n* lr = 0.05\n* num_iterations = 2000\n* w_ridge = 0.05\n* w_sgd = 0.05\n* w_lgb = 0.85 <- basic LGB-1\n* w_logreg = 1 - w_ridge - w_sgd - w_lgb\n* FE-1\n\n**LB = 0.325**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1.3. Previous commits: LGB-2, FE-3 <a class=\"anchor\" id=\"1.3\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 21\n\n* seed_random = 42\n* num_leaves = 2**7+1\n* lr = 0.05\n* num_iterations = 2000\n* w_ridge = 0.001\n* w_sgd = 0.001\n* w_lgb = 0.996\n* w_logreg = 1 - w_ridge - w_sgd - w_lgb \n\n**LB = 0.936**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 22\n\n* seed_random = 42\n* num_leaves = 2**7+1\n* lr = 0.05\n* num_iterations = 2000\n* w_ridge = 0.001\n* w_sgd = 0.001\n* w_lgb = 0.499\n* w_logreg = 1 - w_ridge - w_sgd - w_lgb \n\n**LB = 0.860**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 23\n\n* seed_random = 42\n* num_leaves = 2**7+1\n* lr = 0.05\n* num_iterations = 10000\n* w_ridge = 0\n* w_sgd = 0\n* w_lgb = 0.998\n* w_logreg = 1 - w_ridge - w_sgd - w_lgb \n\n**LB = 0.936**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 24\n\n* seed_random = 42\n* num_leaves = 2**7+1\n* lr = 0.01\n* num_iterations = 2000\n* w_ridge = 0\n* w_sgd = 0\n* w_lgb = 0.998\n* w_logreg = 1 - w_ridge - w_sgd - w_lgb \n\n**LB = 0.936**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 24\n\n* seed_random = 42\n* num_leaves = 200\n* lr = 0.01\n* num_iterations = 2000\n* w_ridge = 0\n* w_sgd = 0\n* w_lgb = 0.998\n* w_logreg = 1 - w_ridge - w_sgd - w_lgb \n\n**LB = 0.936**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1.4. Previous commits: LGB-2, MLP, LogReg, FE-3 <a class=\"anchor\" id=\"1.4\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 27\n\n* seed_random = 42\n* num_leaves = 200\n* cv_mlp = 2\n* lr_mlp = 0.01\n* n_layer_0 = 2\n* n_layer_max = 10\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_ridge = 0\n* w_sgd = 0\n* w_lgb = 0.7\n* w_mlp = 0.2\n* w_logreg = 1 - w_ridge - w_sgd - w_lgb \n\n**LB = 0.935**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1.5. Previous commits: Advanced FE & LGB-2 <a class=\"anchor\" id=\"1.5\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 30\n\n* FE-3 = 51 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 4000\n* w_lgb = 1\n\n**LB = 0.936**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 32\n\n* FE-1 & FE-3 with 'group', without 'signal' = 52 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 4000\n* w_lgb = 1\n\n**LB = 0.882**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 33\n\n* FE-1 & FE-3 without 'group', with 'signal' = 52 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 4000\n* w_lgb = 1\n\n**LB = 0.898**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 35\n\n* FE-2 (window_sizes = [10, 50]) & FE-3 = 79 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.937**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 36\n\n* FE-2 (window_sizes = [25, 100, 5000]) & FE-3 = 79 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.936**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 37\n\n* FE-2 (window_sizes = [500, 1000, 10000]) & FE-3 = 79 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.935**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 39\n\n* FE-2 (window_sizes = [10, 25, 50, 5000, 10000], without some features) & FE-3 = 88 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.936**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 40\n\n* FE-2 (window_sizes = window_sizes = [10, 50, 5000]) & FE-3 = 96 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.937**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 41\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 84 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.937**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 42\n\n* FE-2 (window_sizes = window_sizes = [10, 50, 5000] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 99 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.937**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 44\n\n* FE-2 (window_sizes = window_sizes = [20, 30, 25000] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 99 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.936**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 45\n\n* FE-2 (window_sizes = window_sizes = [75, 2500, 15000] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 99 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.936**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 46\n\n* FE-2 (window_sizes = window_sizes = [25, 5000, 10000] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 99 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.936**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 47\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 84 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.937**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 48\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 84 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.938**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 49\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 84 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 200\n* num_iterations = 10000\n* w_lgb = 1\n\n**LB = 0.937**\n\nCalculation Time - 5.2 hours","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 50\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 84 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 511\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.937**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 51\n\n* FE-2 (window_sizes = window_sizes = [10, 50, 5000] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 99 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.937**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 53\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 99 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n* 'metrics': f1_score and 'logloss' (replace of f1_score and 'rmse' in previous commits)\n\n**LB = 0.938**\n\nThe kernel calculated faster by 300 seconds, but the LB was the same (see commit 48)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 54\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 99 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 250\n* num_iterations = 2000\n* w_lgb = 1\n* 'metrics': f1_score and 'logloss' (replace of f1_score and 'rmse' in previous commits)\n\n**LB = 0.937**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 55\n\n* FE-2 (window_sizes = window_sizes = [50000, 100000, 500000] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 99 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n* metrics are 'logloss' & f1_score\n\n**LB = 0.919**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 56\n\n* FE-2 (window_sizes = window_sizes = [10, 50, 50000] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 99 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n* metrics are 'logloss' & f1_score\n\n**LB = 0.937**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 57\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 84 features\n* seed_random = 0\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n* metrics are 'logloss' & f1_score\n\n**LB = 0.937**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 58\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 84 features\n* seed_random = 7\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n* metrics are 'logloss' & f1_score\n\n**LB = 0.937**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 59\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 84 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.015\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n* metrics are 'logloss' & f1_score\n\n**LB = 0.937**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1.6. Previous commits: Advanced FE & LGB-2 & XGB-1<a class=\"anchor\" id=\"1.6\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 63\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without ratio-, diff-, norm-features) & FE-3 = 72 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.1\n* num_leaves = 200\n* num_iterations = 200\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.1\n* max_depth_xgb = 10\n* num_boost_round_xgb = 22\n* w_xgb = 0.5\n* metrics is 'rmse'\n\n**LB = 0.937**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 64\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without ratio-, diff-, norm-features) & FE-3 = 72 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.1\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.1\n* max_depth_xgb = 10\n* num_boost_round_xgb = 2000\n* w_xgb = 0.5\n* metrics is 'rmse'\n\n**LB = 0.937**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 70\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without 'medianbatch', 'abs_avgbatch', 'abs_maxbatch', ratio-, diff-, norm-features) & FE-3 = 69 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.1\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.1\n* max_depth_xgb = 10\n* num_boost_round_xgb = 800\n* w_xgb = 0.5\n* metrics is 'rmse'\n\n**LB = 0.938**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 71\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without 'medianbatch', 'abs_avgbatch', 'abs_maxbatch', ratio-, diff-, norm-features) & FE-3 = 69 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.1\n* max_depth_xgb = 10\n* num_boost_round_xgb = 800\n* w_xgb = 0.5\n* metrics is 'rmse'\n\n**LB = 0.938**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 72\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without 'medianbatch', 'abs_avgbatch', 'abs_maxbatch', ratio-, diff-, norm-features) & FE-3 = 69 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.1\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.1\n* max_depth_xgb = 10\n* num_boost_round_xgb = 800\n* w_xgb = 0.5\n* metrics is 'logloss'\n\n**LB = 0.938**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 74\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without 'medianbatch', 'abs_avgbatch', 'abs_maxbatch', ratio-, diff-features) & FE-3 = 76 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.05\n* max_depth_xgb = 10\n* num_boost_round_xgb = 1000\n* w_xgb = 0.5\n* metrics is 'logloss'\n\n**LB = 0.939 (the best)**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 75\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without 'medianbatch', 'abs_avgbatch', 'abs_maxbatch', ratio-, diff-features) & FE-3 = 73 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.03\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.03\n* max_depth_xgb = 10\n* num_boost_round_xgb = 1000\n* w_xgb = 0.5\n* metrics is 'logloss'\n\n**CV = 0.9429**\n\n**LB = 0.939**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 76\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without 'medianbatch', 'abs_avgbatch', 'abs_maxbatch', ratio-, diff-features) & FE-3 = 73 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.02\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.03\n* max_depth_xgb = 12\n* num_boost_round_xgb = 1000\n* w_xgb = 0.5\n* metrics is 'logloss'\n\n**CV = 0.9463**\n\n**LB = 0.939**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 77\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without 'medianbatch', 'abs_avgbatch', 'abs_maxbatch', ratio-, diff-features) & FE-3 = 73 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.02\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 0.4\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.03\n* max_depth_xgb = 12\n* num_boost_round_xgb = 1000\n* w_xgb = 0.6\n* metrics is 'logloss'\n\n**CV = 0.9469**\n\n**LB = 0.938**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 78\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without 'medianbatch', 'abs_avgbatch', 'abs_maxbatch', ratio-, diff-features) & FE-3 = 73 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.02\n* max_depth_xgb = 12\n* num_boost_round_xgb = 600\n* w_xgb = 0.5\n* metrics is 'logloss'\n\n**CV = 0.9438**\n\n**LB = 0.939**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 80\n\n* with Kalman filter from https://www.kaggle.com/teejmahal20/single-model-lgbm-kalman-filter\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without 'medianbatch', 'abs_avgbatch', 'abs_maxbatch', ratio-, diff-features) & FE-3 = 73 features\n* seed_random = 42\n* others as in commit 74:\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.05\n* max_depth_xgb = 12\n* num_boost_round_xgb = 600\n* w_xgb = 0.5\n* metrics is 'logloss'\n\n**CV = 0.9459**\n\n**LB = 0.938**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 81\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without 'medianbatch', 'abs_avgbatch', 'abs_maxbatch', ratio-, diff-features) & FE-3 = 73 features\n* seed_random = 42\n* others as in commit 74:\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.05\n* max_depth_xgb = 12\n* num_boost_round_xgb = 600\n* w_xgb = 0.6\n* metrics is 'logloss'\n\n**CV = 0.9460**\n\n**LB = 0.938**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 83\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without 'medianbatch', 'abs_avgbatch', 'abs_maxbatch', ratio-, diff-features) & FE-3 = 73 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.02\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.02\n* max_depth_xgb = 12\n* num_boost_round_xgb = 1000\n* w_xgb = 0.4\n* metrics is 'logloss'\n\n**CV = 0.9465**\n\n**LB = 0.938**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Commit 84\n\n* FE-2 (window_sizes = window_sizes = [5000, 50000] with new features: signal_shift_+2, signal_shift_-2, without 'medianbatch', 'abs_avgbatch', 'abs_maxbatch', ratio-, diff-features) & FE-3 = 73 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.02\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.05\n* max_depth_xgb = 10\n* num_boost_round_xgb = 1000\n* w_xgb = 0.4\n* metrics is 'logloss'\n\n**CV = 0.9431**\n\n**LB = 0.933**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2. Import libraries <a class=\"anchor\" id=\"2\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n!pip install tensorflow_addons==0.9.1\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nimport pandas as pd\nimport numpy as np\nimport random\nfrom tensorflow.keras.callbacks import Callback, LearningRateScheduler\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import losses, models, optimizers\nimport tensorflow_addons as tfa\nimport math\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression, Ridge, SGDRegressor\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV, KFold, train_test_split, GroupKFold\nfrom sklearn.utils import shuffle\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, f1_score, mean_absolute_error, make_scorer\nimport lightgbm as lgb\nimport xgboost as xgb\n#from pykalman import KalmanFilter\n\nfrom functools import partial\nimport scipy as sp\n\nimport time\nimport datetime\n\nimport gc\n\nimport warnings\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 3. Download data & FE <a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        if col != 'time':\n            col_type = df[col].dtypes\n            if col_type in numerics:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/liverpool-ion-switching/train.csv')\ntest = pd.read_csv('/kaggle/input/liverpool-ion-switching/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE-2 - thanks to the kernels:\n* https://www.kaggle.com/teejmahal20/regression-with-optimized-rounder\n* https://www.kaggle.com/pestipeti/eda-ion-switching","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor window in window_sizes:\n    train[\"rolling_mean_\" + str(window)] = train['signal'].rolling(window=window).mean()\n    train[\"rolling_std_\" + str(window)] = train['signal'].rolling(window=window).std()\n    train[\"rolling_var_\" + str(window)] = train['signal'].rolling(window=window).var()\n    train[\"rolling_min_\" + str(window)] = train['signal'].rolling(window=window).min()\n    train[\"rolling_max_\" + str(window)] = train['signal'].rolling(window=window).max()\n    \n    #train[\"rolling_min_max_ratio_\" + str(window)] = train[\"rolling_min_\" + str(window)] / train[\"rolling_max_\" + str(window)]\n    #train[\"rolling_min_max_diff_\" + str(window)] = train[\"rolling_max_\" + str(window)] - train[\"rolling_min_\" + str(window)]\n    \n    a = (train['signal'] - train['rolling_min_' + str(window)]) / (train['rolling_max_' + str(window)] - train['rolling_min_' + str(window)])\n    train[\"norm_\" + str(window)] = a * (np.floor(train['rolling_max_' + str(window)]) - np.ceil(train['rolling_min_' + str(window)]))\n    \ntrain = train.replace([np.inf, -np.inf], np.nan)    \ntrain.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor window in window_sizes:\n    test[\"rolling_mean_\" + str(window)] = test['signal'].rolling(window=window).mean()\n    test[\"rolling_std_\" + str(window)] = test['signal'].rolling(window=window).std()\n    test[\"rolling_var_\" + str(window)] = test['signal'].rolling(window=window).var()\n    test[\"rolling_min_\" + str(window)] = test['signal'].rolling(window=window).min()\n    test[\"rolling_max_\" + str(window)] = test['signal'].rolling(window=window).max()\n    \n    #test[\"rolling_min_max_ratio_\" + str(window)] = test[\"rolling_min_\" + str(window)] / test[\"rolling_max_\" + str(window)]\n    #test[\"rolling_min_max_diff_\" + str(window)] = test[\"rolling_max_\" + str(window)] - test[\"rolling_min_\" + str(window)]\n\n    \n    a = (test['signal'] - test['rolling_min_' + str(window)]) / (test['rolling_max_' + str(window)] - test['rolling_min_' + str(window)])\n    test[\"norm_\" + str(window)] = a * (np.floor(test['rolling_max_' + str(window)]) - np.ceil(test['rolling_min_' + str(window)]))\n\ntest = test.replace([np.inf, -np.inf], np.nan)    \ntest.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE-3 - thanks to \n* https://www.kaggle.com/jazivxt/physically-possible\n* https://www.kaggle.com/siavrez/simple-eda-model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef features(df):\n    df = df.sort_values(by=['time']).reset_index(drop=True)\n    df.index = ((df.time * 10_000) - 1).values\n    df['batch'] = df.index // 25_000\n    df['batch_index'] = df.index  - (df.batch * 25_000)\n    df['batch_slices'] = df['batch_index']  // 2500\n    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n    \n    for c in ['batch','batch_slices2']:\n        d = {}\n        d['mean'+c] = df.groupby([c])['signal'].mean()\n        d['median'+c] = df.groupby([c])['signal'].median()\n        d['max'+c] = df.groupby([c])['signal'].max()\n        d['min'+c] = df.groupby([c])['signal'].min()\n        d['std'+c] = df.groupby([c])['signal'].std()\n        d['mean_abs_chg'+c] = df.groupby([c])['signal'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = df.groupby([c])['signal'].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = df.groupby([c])['signal'].apply(lambda x: np.min(np.abs(x)))\n        d['range'+c] = d['max'+c] - d['min'+c]\n        d['maxtomin'+c] = d['max'+c] / d['min'+c]\n        d['abs_avg'+c] = (d['abs_min'+c] + d['abs_max'+c]) / 2\n        for v in d:\n            df[v] = df[c].map(d[v].to_dict())\n\n    \n    # add shifts_1\n    df['signal_shift_+1'] = [0,] + list(df['signal'].values[:-1])\n    df['signal_shift_-1'] = list(df['signal'].values[1:]) + [0]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+1'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-1'][i] = np.nan\n    \n    # add shifts_2 - my upgrade\n    df['signal_shift_+2'] = [0,] + [1,] + list(df['signal'].values[:-2])\n    df['signal_shift_-2'] = list(df['signal'].values[2:]) + [0] + [1]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==1].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-2'][i] = np.nan\n    for i in df[df['batch_index']==49998].index:\n        df['signal_shift_-2'][i] = np.nan\n    \n    df = df.drop(columns=['batch', 'batch_index', 'batch_slices', 'batch_slices2'])\n\n    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels']]:\n        df[c+'_msignal'] = df[c] - df['signal']\n        \n    df = df.replace([np.inf, -np.inf], np.nan)    \n    df.fillna(0, inplace=True)\n    gc.collect()\n    return df\n\ntrain = features(train)\ntest = features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['open_channels']\ncol = [c for c in train.columns if c not in ['time', 'open_channels', 'group', 'medianbatch', 'abs_avgbatch', 'abs_maxbatch']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Model tuning <a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Standardization for regression model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# scaler = StandardScaler()\n# train_reg = pd.DataFrame(\n#     scaler.fit_transform(train[col]),\n#     columns=train[col].columns,\n#     index=train[col].index\n# )\n# test_reg = pd.DataFrame(\n#     scaler.transform(test[col]),\n#     columns=test[col].columns,\n#     index=test[col].index\n# )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Ridge Regression<a class=\"anchor\" id=\"4.1\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# ridge_reg = Ridge(tol=5e-5, fit_intercept=False, random_state=seed_random)\n# ridge_reg.fit(train_reg, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_train_ridge = ridge_reg.predict(train_reg)\n# y_preds_ridge = ridge_reg.predict(test_reg)\n# y_train_ridge = np.clip(y_train_ridge, 0, 10).astype(int)\n# y_preds_ridge = np.clip(y_preds_ridge, 0, 10).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 SGDRegressor <a class=\"anchor\" id=\"4.2\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# sgd = SGDRegressor(learning_rate = 'adaptive', fit_intercept=False, random_state=seed_random)\n# sgd.fit(train_reg, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_train_sgd = sgd.predict(train_reg)\n# y_preds_sgd = sgd.predict(test_reg)\n# y_train_sgd = np.clip(y_train_sgd, 0, 10).astype(int)\n# y_preds_sgd = np.clip(y_preds_sgd, 0, 10).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Logistic Regression <a class=\"anchor\" id=\"4.3\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# logreg = LogisticRegression(random_state=seed_random)\n# logreg.fit(train_reg, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_logreg_train = logreg.predict(train_reg)\n# y_logreg_pred = logreg.predict(test_reg)\n# y_logreg_train = np.clip(y_logreg_train, 0, 10).astype(int)\n# y_logreg_pred = np.clip(y_logreg_pred, 0, 10).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4 MLP <a class=\"anchor\" id=\"4.4\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # https://www.kaggle.com/vbmokin/automatic-selection-from-20-classifier-models\n# # https://www.kaggle.com/vbmokin/bod-prediction-in-river-15-regression-models\n# mlp = MLPClassifier()\n# f1scoring = make_scorer(f1_score, average = 'macro')\n# param_grid = {'hidden_layer_sizes': [i for i in range(n_layer_0, n_layer_max)],\n#               'activation': ['relu'],\n#               'solver': ['adam'],\n#               'learning_rate': ['constant'],\n#               'learning_rate_init': [lr_mlp],\n#               'power_t': [0.5],\n#               'alpha': [0.0001],\n#               'max_iter': [1000],\n#               'early_stopping': [True],\n#               'warm_start': [False],\n#               'random_state': [seed_random]}\n# mlp_GS = GridSearchCV(mlp, param_grid=param_grid, scoring=f1scoring,\n#                    cv=cv_mlp, verbose=True, pre_dispatch='2*n_jobs')\n# mlp_GS.fit(train_reg, y)\n# print(\"Best parameters set:\", mlp_GS.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_mlp_pred = mlp_GS.predict(test_reg)\n# y_pred_train_mlp = mlp_GS.predict(train_reg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del train_reg, test_reg\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.5 LGB <a class=\"anchor\" id=\"4.5\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/siavrez/simple-eda-model\ndef MacroF1Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = f1_score(labels, preds, average = 'macro')\n    return ('MacroF1Metric', score, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Thanks to https://www.kaggle.com/jazivxt/physically-possible with tuning from https://www.kaggle.com/siavrez/simple-eda-model and my tuning\nX_train, X_valid, y_train, y_valid = train_test_split(train[col], y, test_size=0.3, random_state=seed_random)\nparams = {'learning_rate': lr_lgb, \n          'max_depth': -1, \n          'num_leaves': num_leaves,\n          'metric': 'logloss', \n          'random_state': seed_random, \n          'n_jobs':-1, \n          'sample_fraction':0.33}\nmodel = lgb.train(params, lgb.Dataset(X_train, y_train), num_iterations, lgb.Dataset(X_valid, y_valid), verbose_eval=100, early_stopping_rounds=200, feval=MacroF1Metric)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ny_lgb_pred = model.predict(test[col], num_iteration=model.best_iteration)\ny_pred_train_lgb = model.predict(train[col], num_iteration=model.best_iteration)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('LGB score {0:.4f}'.format(np.mean(f1_score(y, np.round(np.clip(y_pred_train_lgb,0,10)).astype(int), average=\"macro\"))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgb.plot_importance(model,ax = axes,height = 0.5)\nplt.show();plt.close()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.6 XGB <a class=\"anchor\" id=\"4.6\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Thanks to https://www.kaggle.com/teejmahal20/3-simple-ideas-ensemble\n# train_set = xgb.DMatrix(X_train, y_train)\n# val_set = xgb.DMatrix(X_valid, y_valid)\n# del X_train, X_valid, y_train, y_valid\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# params_xgb = {'colsample_bytree': 0.375,\n#               'learning_rate': lr_xgb,\n#               'max_depth': max_depth_xgb, \n#               'subsample': 1, \n#               'objective':'reg:squarederror',\n#               'eval_metric':'logloss'}\n\n# modelx = xgb.train(params_xgb, train_set, num_boost_round=num_boost_round_xgb, evals=[(train_set, 'train'), (val_set, 'val')], \n#                                      verbose_eval=50, early_stopping_rounds=200)\n# del train_set, val_set\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# y_xgb_pred = modelx.predict(xgb.DMatrix(test[col]))\n# y_pred_train_xgb = modelx.predict(xgb.DMatrix(train[col]))\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('XGB score {0:.4f}'.format(np.mean(f1_score(y, np.round(np.clip(y_pred_train_xgb,0,10)).astype(int), average=\"macro\"))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.7 Wavenet with SHIFTED-RFC Proba and CBR <a class=\"anchor\" id=\"4.7\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/nxrprime/wavenet-with-shifted-rfc-proba-and-cbr\n# configurations and main hyperparammeters\nEPOCHS = EPOCHS_wn\nNNBATCHSIZE = 16\nGROUP_BATCH_SIZE = 4000\nSEED = 321\nLR = lr_wn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Thanks to https://www.kaggle.com/nxrprime/wavenet-with-shifted-rfc-proba-and-cbr - my upgrade: improved FE and simplified model\n# Create batches of 4000 observations\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\n# read data\ndef read_data():\n    train = pd.read_csv('/kaggle/input/data-without-drift/train_clean.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n    test  = pd.read_csv('/kaggle/input/data-without-drift/test_clean.csv', dtype={'time': np.float32, 'signal': np.float32})\n    sub  = pd.read_csv('/kaggle/input/liverpool-ion-switching/sample_submission.csv', dtype={'time': np.float32})\n    \n    Y_train_proba = np.load(\"/kaggle/input/ion-shifted-rfc-proba/Y_train_proba.npy\")\n    Y_test_proba = np.load(\"/kaggle/input/ion-shifted-rfc-proba/Y_test_proba.npy\")\n    \n    for i in range(11):\n        train[f\"proba_{i}\"] = Y_train_proba[:, i]\n        test[f\"proba_{i}\"] = Y_test_proba[:, i]\n\n    return train, test, sub\n\n# create batches of 4000 observations\ndef batching(df, batch_size):\n    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n    df['group'] = df['group'].astype(np.uint16)\n    return df\n\n# normalize the data (standard scaler). We can also try other scalers for a better score!\ndef normalize(train, test):\n    train_input_mean = train.signal.mean()\n    train_input_sigma = train.signal.std()\n    train['signal'] = (train.signal - train_input_mean) / train_input_sigma\n    test['signal'] = (test.signal - train_input_mean) / train_input_sigma\n    return train, test\n\n# get lead and lags features\ndef lag_with_pct_change(df, windows):\n    for window in windows:    \n        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n    return df\n\n# main module to run feature engineering. Here you may want to try and add other features and check if your score imporves :).\ndef run_feat_engineering(df, batch_size):\n    # create batches\n    df = batching(df, batch_size = batch_size)\n    # create leads and lags (1, 2, 3 making them 6 features)\n    df = lag_with_pct_change(df, [1, 2, 3])\n    # create signal ** 2 (this is the new feature)\n    df['signal_2'] = df['signal'] ** 2\n    \n    # 'batch_index' from https://www.kaggle.com/jazivxt/physically-possible and https://www.kaggle.com/siavrez/simple-eda-model\n    df['batch'] = df.index // 25_000\n    df['batch_index'] = df.index  - (df.batch * 25_000)\n    \n    # add shifts_2 - my upgrade\n    df['signal_shift_+2'] = [0,] + [1,] + list(df['signal'].values[:-2])\n    df['signal_shift_-2'] = list(df['signal'].values[2:]) + [0] + [1]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==1].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-2'][i] = np.nan\n    for i in df[df['batch_index']==49998].index:\n        df['signal_shift_-2'][i] = np.nan    \n    df = df.drop(columns=['batch', 'batch_index'])\n    \n    return df\n\n# fillna with the mean and select features for training\ndef feature_selection(train, test):\n    features = [col for col in train.columns if col not in ['index', 'group', 'open_channels', 'time']]\n    train = train.replace([np.inf, -np.inf], np.nan)\n    test = test.replace([np.inf, -np.inf], np.nan)\n    for feature in features:\n        feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n        train[feature] = train[feature].fillna(feature_mean)\n        test[feature] = test[feature].fillna(feature_mean)\n    return train, test, features\n\n# model function (very important, you can try different arquitectures to get a better score. I believe that top public leaderboard is a 1D Conv + RNN style)\ndef Classifier(shape_):\n    \n    def cbr(x, out_layer, kernel, stride, dilation):\n        x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        return x\n    \n    def wave_block(x, filters, kernel_size, n):\n        dilation_rates = [2**i for i in range(n)]\n        x = Conv1D(filters = filters,\n                   kernel_size = 1,\n                   padding = 'same')(x)\n        res_x = x\n        for dilation_rate in dilation_rates:\n            tanh_out = Conv1D(filters = filters,\n                              kernel_size = kernel_size,\n                              padding = 'same', \n                              activation = 'tanh', \n                              dilation_rate = dilation_rate)(x)\n            sigm_out = Conv1D(filters = filters,\n                              kernel_size = kernel_size,\n                              padding = 'same',\n                              activation = 'sigmoid', \n                              dilation_rate = dilation_rate)(x)\n            x = Multiply()([tanh_out, sigm_out])\n            x = Conv1D(filters = filters,\n                       kernel_size = 1,\n                       padding = 'same')(x)\n            res_x = Add()([res_x, x])\n        return res_x\n    \n    inp = Input(shape = (shape_))\n    x = cbr(inp, 64, 7, 1, 1)\n    x = BatchNormalization()(x)\n    x = wave_block(x, 64, 3, 4)\n    x = BatchNormalization()(x)\n    x = wave_block(x, 128, 3, 1)\n    x = cbr(x, 32, 7, 1, 1)\n    x = BatchNormalization()(x)\n    x = wave_block(x, 64, 3, 1)\n    x = cbr(x, 32, 7, 1, 1)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)  \n#     x = cbr(inp, 64, 7, 1, 1)\n#     x = BatchNormalization()(x)\n#     x = wave_block(x, 16, 3, 12)\n#     x = BatchNormalization()(x)\n#     x = wave_block(x, 32, 3, 8)\n#     x = BatchNormalization()(x)\n#     x = wave_block(x, 64, 3, 4)\n#     x = BatchNormalization()(x)\n#     x = wave_block(x, 128, 3, 1)\n#     x = cbr(x, 32, 7, 1, 1)\n#     x = BatchNormalization()(x)\n#     x = wave_block(x, 64, 3, 1)\n#     x = cbr(x, 32, 7, 1, 1)\n#     x = BatchNormalization()(x)\n#     x = Dropout(0.2)(x)\n    out = Dense(11, activation = 'softmax', name = 'out')(x)\n    \n    model = models.Model(inputs = inp, outputs = out)\n    \n    opt = Adam(lr = LR)\n    opt = tfa.optimizers.SWA(opt)\n    model.compile(loss = losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n    return model\n\n# function that decrease the learning as epochs increase (i also change this part of the code)\ndef lr_schedule(epoch):\n    if epoch < 3:\n        lr = LR\n    elif epoch < 8:\n        lr = LR / 3\n    elif epoch < 30:\n        lr = LR / 5\n    elif epoch < 60:\n        lr = LR / 7\n    elif epoch < 70:\n        lr = LR / 9\n    elif epoch < 80:\n        lr = LR / 11\n    elif epoch < 90:\n        lr = LR / 13\n    else:\n        lr = LR / 100\n    return lr\n\n# class to get macro f1 score. This is not entirely necessary but it's fun to check f1 score of each epoch (be carefull, if you use this function early stopping callback will not work)\nclass MacroF1(Callback):\n    def __init__(self, model, inputs, targets):\n        self.model = model\n        self.inputs = inputs\n        self.targets = np.argmax(targets, axis = 2).reshape(-1)\n        \n    def on_epoch_end(self, epoch, logs):\n        pred = np.argmax(self.model.predict(self.inputs), axis = 2).reshape(-1)\n        score = f1_score(self.targets, pred, average = 'macro')\n        print(f'F1 Macro Score: {score:.5f}')\n\n# main function to perfrom groupkfold cross validation (we have 1000 vectores of 4000 rows and 8 features (columns)). Going to make 5 groups with this subgroups.\ndef run_cv_model_by_batch(train, test, splits, batch_col, feats, sample_submission, nn_epochs, nn_batch_size):\n    \n    seed_everything(SEED)\n    K.clear_session()\n    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n    tf.compat.v1.keras.backend.set_session(sess)\n    oof_ = np.zeros((len(train), 11)) # build out of folds matrix with 11 columns, they represent our target variables classes (from 0 to 10)\n    preds_ = np.zeros((len(test), 11))\n    preds_train = np.zeros((len(train), 11))\n    target = ['open_channels']\n    group = train['group']\n    kf = GroupKFold(n_splits=SPLITS)\n    splits = [x for x in kf.split(train, train[target], group)]\n\n    new_splits = []\n    for sp in splits:\n        new_split = []\n        new_split.append(np.unique(group[sp[0]]))\n        new_split.append(np.unique(group[sp[1]]))\n        new_split.append(sp[1])    \n        new_splits.append(new_split)\n    # pivot target columns to transform the net to a multiclass classification estructure (you can also leave it in 1 vector with sparsecategoricalcrossentropy loss function)\n    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n\n    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n    target_cols = ['target_'+str(i) for i in range(11)]\n    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n\n    for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n        print(f'Our training dataset shape is {train_x.shape}')\n        print(f'Our validation dataset shape is {valid_x.shape}')\n\n        gc.collect()\n        shape_ = (None, train_x.shape[2]) # input is going to be the number of feature we are using (dimension 2 of 0, 1, 2)\n        model = Classifier(shape_)\n        # using our lr_schedule function\n        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n        model.fit(train_x,train_y,\n                  epochs = nn_epochs,\n                  callbacks = [cb_lr_schedule, MacroF1(model, valid_x, valid_y)], # adding custom evaluation metric for each epoch\n                  batch_size = nn_batch_size,verbose = 2,\n                  validation_data = (valid_x,valid_y))\n        preds_f = model.predict(valid_x)\n        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  np.argmax(preds_f, axis=2).reshape(-1), average = 'macro') # need to get the class with the biggest probability\n        print(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n        oof_[val_orig_idx,:] += preds_f\n        te_preds = model.predict(test)\n        te_train_preds = model.predict(train)\n        model.save(\"model-wavenet.h5\")\n        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n        preds_ += te_preds / SPLITS\n        te_train_preds = te_train_preds.reshape(-1, te_train_preds.shape[-1])           \n        preds_train += te_train_preds / SPLITS\n        \n    # calculate the oof macro f1_score\n    f1_score_ = f1_score(np.argmax(train_tr, axis = 2).reshape(-1),  np.argmax(oof_, axis = 1), average = 'macro') # axis 2 for the 3 Dimension array and axis 1 for the 2 Domension Array (extracting the best class)\n    print(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n    y_wnet_pred = np.argmax(preds_, axis = 1)\n    y_pred_train_wnet = np.argmax(preds_train, axis = 1)\n    #sample_submission.to_csv('submission_wavenet.csv', index=False, float_format='%.4f')\n    return y_wnet_pred, y_pred_train_wnet\n\n# this function run our entire program\nprint('Reading Data Started...')\ntrain, test, sample_submission = read_data()\ntrain, test = normalize(train, test)\nprint('Reading and Normalizing Data Completed')\n\nprint('Creating Features')\nprint('Feature Engineering Started...')\ntrain = run_feat_engineering(train, batch_size = GROUP_BATCH_SIZE)\ntest = run_feat_engineering(test, batch_size = GROUP_BATCH_SIZE)\ntrain, test, features = feature_selection(train, test)\nprint('Feature Engineering Completed...')\nprint(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started...')\ny_wnet_pred, y_pred_train_wnet = run_cv_model_by_batch(train, test, SPLITS, 'group', features, sample_submission, EPOCHS, NNBATCHSIZE)\nprint('Training Wavenet model completed...')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Wavenet score {0:.4f}'.format(np.mean(f1_score(y, np.round(np.clip(y_pred_train_wnet,0,10)).astype(int), average=\"macro\"))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Showing Confusion Matrices <a class=\"anchor\" id=\"5\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix\n# Thanks to https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud\ndef plot_cm(y_true, y_pred, title):\n    figsize=(14,14)\n    y_pred = y_pred.astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix for Ridge model\n#plot_cm(y, y_train_ridge, 'Confusion matrix for Ridge model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix for SGDRegressor model\n#plot_cm(y, y_train_sgd, 'Confusion matrix for SGDRegressor model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Showing Confusion Matrix for Logistic Regression\n# plot_cm(y, y_logreg_train, 'Confusion matrix for Logistic Regression')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix for LGB model\nplot_cm(y, y_pred_train_lgb, 'Confusion matrix for LGB model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix for Wavenet model\nplot_cm(y, y_pred_train_wnet, 'Confusion matrix for Wavenet model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Showing Confusion Matrix for XGB model\n# plot_cm(y, y_pred_train_xgb, 'Confusion matrix for XGB model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Showing Confusion Matrix for MLP model\n# plot_cm(y, y_pred_train_mlp, 'Confusion matrix for MLP model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Comparison and merging solutions <a class=\"anchor\" id=\"6\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Merging solution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix for the main solution before rounded\ny_train_preds = w_lgb*y_pred_train_lgb + w_wnet*y_pred_train_wnet\nplot_cm(y, y_train_preds, 'Confusion matrix for the main solution before rounded')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del y_pred_train_lgb, y_pred_train_wnet\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_preds = w_ridge*y_preds_ridge + w_sgd*y_preds_sgd + w_logreg*y_logreg_pred + w_lgb*y_lgb_pred\n#y_preds = w_logreg*y_logreg_pred + w_lgb*y_lgb_pred + w_mlp*y_mlp_pred\n#y_preds = w_lgb*y_lgb_pred + w_xgb*y_xgb_pred\ny_preds = w_lgb*y_lgb_pred + w_wnet*y_wnet_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del y_lgb_pred, y_wnet_pred\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Processing prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def pred_proc(pred):\n    pred = np.round(np.clip(pred, 0, 10))\n    return pred.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction processing for the main solution\ny_preds = pred_proc(y_preds)\ny_train_preds = pred_proc(y_train_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building Confusion matrices for processing solutions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix for the processing main solution\nplot_cm(y, y_train_preds, 'Confusion matrix for the processing main solution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The main solution score {0:.4f}'.format(np.mean(f1_score(y, y_train_preds, average=\"macro\"))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del y_train_preds\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Submission <a class=\"anchor\" id=\"7\"></a>\n\n[Back to Table of Contents](#0.1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test['open_channels'] = y_preds\ntest[['time','open_channels']].to_csv('submission.csv', index=False, float_format='%.4f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.time.values[:10])\nprint(test['open_channels'].mean())\nprint(test['open_channels'].hist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"I hope you find this kernel useful and enjoyable.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Your comments and feedback are most welcome.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"[Go to Top](#0)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}