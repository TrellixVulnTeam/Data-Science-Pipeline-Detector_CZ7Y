{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Setting package umum \nimport pandas as pd\nimport pandas_profiling as pp\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom tqdm import tqdm_notebook as tqdm\n%matplotlib inline\n\nfrom matplotlib.pylab import rcParams\n# For every plotting cell use this\nrcParams['figure.figsize'] = [10,5]\nplt.style.use('fivethirtyeight') \nsns.set_style('whitegrid')\n# fig, axes = plt.subplots()\n# grid = gridspec.GridSpec(n_row,n_col)\n# ax = plt.subplot(grid[i])\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom tqdm import tqdm\n\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 50)\npd.options.display.float_format = '{:.4f}'.format\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load dataset\ndf_train_clean = pd.read_csv('/kaggle/input/data-without-drift/train_clean.csv')\ndf_test_clean = pd.read_csv('/kaggle/input/data-without-drift/test_clean.csv')\ndf_train_kalman = pd.read_csv('/kaggle/input/data-without-drift-with-kalman-filter/train.csv')\ndf_test_kalman = pd.read_csv('/kaggle/input/data-without-drift-with-kalman-filter/test.csv')\ndf_train_rfc = pd.DataFrame(np.load('/kaggle/input/ion-shifted-rfc-proba/Y_train_proba.npy'))\ndf_test_rfc = pd.DataFrame(np.load('/kaggle/input/ion-shifted-rfc-proba/Y_test_proba.npy'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add `kalman` feature\ndf_train_clean['kalman'] = df_train_kalman['signal']\ndf_test_clean['kalman'] = df_test_kalman['signal']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add `shifted_prob_class` feature\nlist_shifted_var = []\nfor clas in list(df_train_rfc.columns) :\n    var_name = 'shited_proba_class' + str(clas)\n    df_train_clean[var_name] = df_train_rfc[clas]\n    df_test_clean[var_name] = df_test_rfc[clas]\n    list_shifted_var.append(var_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Make `previous_signal`\nlist_prev_var = []\ndef make_previous_signal(df, list_ts) :\n    \n    # Iterate to make signal\n    for ts in list_ts :\n        \n        # Make the signal\n        var_name = 'previous_signal_t'+ str(ts)\n        df[var_name] = df['signal'].shift(ts)\n        \n        # Fill missin values with mean\n        row_nan = list(range(ts))\n        df.loc[row_nan, var_name] = [np.mean(df.loc[row:ts+2, 'signal']) for row in row_nan]\n        \n        list_prev_var.append(var_name)\n        \n    return df\n        \ndf_train_clean = make_previous_signal(df_train_clean, [1,3,5,50,100])\ndf_test_clean = make_previous_signal(df_test_clean, [1,3,5,50,100])\n\nlist_prev_var = list(set(list_prev_var))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Make `future_signal`\nlist_fut_var = []\ndef make_future_signal(df, list_ts) :\n    \n    # Iterate to make signal\n    for ts in list_ts :\n        \n        # Make the signal\n        var_name = 'future_signal_t'+ str(ts)\n        df[var_name] = df['signal'].shift(-ts, fill_value=np.mean(df['signal']))\n        \n        list_fut_var.append(var_name)\n        \n    return df\n        \ndf_train_clean = make_future_signal(df_train_clean, [1,3,5,50,100])\ndf_test_clean = make_future_signal(df_test_clean, [1,3,5,50,100])\n\nlist_fut_var = list(set(list_fut_var))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Make 'moving average' :\nlist_ma_var = []\ndef moving_average(df, list_ts) :\n    \n    # Iterate to make MA\n    for ts in list_ts :\n        \n        # Make MA\n        var_name = 'ma_t' + str(ts)\n        df[var_name] = df['signal'].rolling(ts).mean()\n        \n        # Fill missing values with mean\n        row_nan = list(range(ts))\n        df.loc[row_nan, var_name] = [np.mean(df.loc[i+ts:2*(ts)+i, var_name]) for i in range(ts)]\n        \n        list_ma_var.append(var_name)\n        \n    return df\n\ndf_train_clean = moving_average(df_train_clean, [100,300,500])\ndf_test_clean = moving_average(df_test_clean, [100,300,500])\n\nlist_ma_var = list(set(list_ma_var))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Make `signal_power`\ndf_train_clean['signal_power'] = df_train_clean['signal'] ** 2\ndf_test_clean['signal_power'] = df_test_clean['signal'] ** 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Make batch data train\ntrain_batch = int(len(df_train_clean) / 10)\ntrain_group = [1,1,1,2,4,3,1,2,3,4]\ndf_train_clean['group'] = np.nan\n\nfor i,group in enumerate(train_group) :\n    df_train_clean.iloc[i*train_batch : (i+1)*train_batch, -1] = group\n        \n### Make batch data test\ntest_batch = int(len(df_test_clean) / 20)\ntest_group = [1,2,3,1,1,4,3,4,1,2,1,1,1,1,1,1,1,1,1,1]\ndf_test_clean['group'] = np.nan\n\nfor i,group in enumerate(test_group) :\n    df_test_clean.iloc[i*test_batch : (i+1)*test_batch, -1] = group","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Make `signal_deviate`\ntrain_batch = int(len(df_train_clean) / 10)\ndf_train_clean['signal_deviate'] = np.nan\n\nfor i,group in enumerate(train_group) :\n    df_train_clean.iloc[i*train_batch : (i+1)*train_batch, -1] = df_train_clean['signal_power'] - np.mean(df_train_clean.iloc[i*train_batch : (i+1)*train_batch, -3])\n    \ntest_batch = int(len(df_test_clean) / 20)\ndf_test_clean['signal_deviate'] = np.nan\n\nfor i,group in enumerate(test_group) :\n    df_test_clean.iloc[i*test_batch : (i+1)*test_batch, -1] = df_test_clean['signal_power'] - np.mean(df_test_clean.iloc[i*test_batch : (i+1)*test_batch, -3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Make `weight`\ndict_channels = (1 - (df_train_clean['open_channels'].value_counts() / len(df_train_clean))).to_dict()\ndf_train_clean['weight'] = df_train_clean['open_channels'].map(dict_channels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Make 'Tarun_wavelet'\nimport pywt\n\ndef maddest(d, axis=None):\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\ndef denoise_signal(x, wavelet='db4', level=1):\n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    sigma = (1/0.6745) * maddest(coeff[-level])\n\n    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n\n    return pywt.waverec(coeff, wavelet, mode='per')\n\ndf_train_clean['Tarun_wavelet'] = denoise_signal(df_train_clean['signal'])\ndf_test_clean['Tarun_wavelet'] = denoise_signal(df_test_clean['signal'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Make 'MProx_wavelet'\nimport pywt\n\ndef denoise_signal_mprox(x, wavelet='sym4') :\n    w = pywt.Wavelet('sym4')\n    maxlev = pywt.dwt_max_level(len(x), w.dec_len)\n    threshold = 0.04\n    \n    coeff = pywt.wavedec(x, 'sym4', level=maxlev)\n    coeff[1:] = (pywt.threshold(i, value=threshold*max(i)) for i in coeff[1:])\n    \n    return pywt.waverec(coeff, wavelet)\n\ndf_train_clean['MProx_wavelet'] = denoise_signal_mprox(df_train_clean['signal'])\ndf_test_clean['MProx_wavelet'] = denoise_signal_mprox(df_test_clean['signal'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Make 'Tarun_perm_ent'\ndef _embed(x, order=3, delay=1):\n    N = len(x)\n    if order * delay > N:\n        raise ValueError(\"Error: order * delay should be lower than x.size\")\n    if delay < 1:\n        raise ValueError(\"Delay has to be at least 1.\")\n    if order < 2:\n        raise ValueError(\"Order has to be at least 2.\")\n    Y = np.zeros((order, N - (order - 1) * delay))\n    for i in range(order):\n        Y[i] = x[i * delay:i * delay + Y.shape[1]]\n    return Y.T\n\nall = ['perm_entropy', 'spectral_entropy', 'svd_entropy', 'app_entropy',\n       'sample_entropy']\n\n\ndef perm_entropy(x, order=3, delay=1, normalize=False):\n    x = np.array(x)\n    ran_order = range(order)\n    hashmult = np.power(order, ran_order)\n    # Embed x and sort the order of permutations\n    sorted_idx = _embed(x, order=order, delay=delay).argsort(kind='quicksort')\n    # Associate unique integer to each permutations\n    hashval = (np.multiply(sorted_idx, hashmult)).sum(1)\n    # Return the counts\n    _, c = np.unique(hashval, return_counts=True)\n    # Use np.true_divide for Python 2 compatibility\n    p = np.true_divide(c, c.sum())\n    pe = -np.multiply(p, np.log2(p)).sum()\n    if normalize:\n        pe /= np.log2(factorial(order))\n    return pe\n\ndef make_perm_entropy(df) :\n    pe = [np.nan]*100\n    for i in range(100,len(df)) :\n        pe.append(perm_entropy(df.loc[i-100 : i, 'signal']))\n        \n    return pe\n\ndf_train_clean['Tarun_perm_ent'] = make_perm_entropy(df_train_clean)\ndf_train_clean['Tarun_perm_ent'] = df_train_clean['Tarun_perm_ent'].fillna(np.mean(df_train_clean['Tarun_perm_ent']))\ndf_test_clean['Tarun_perm_ent'] = make_perm_entropy(df_test_clean)\ndf_test_clean['Tarun_perm_ent'] = df_test_clean['Tarun_perm_ent'].fillna(np.mean(df_test_clean['Tarun_perm_ent']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Make 'Tarun_apx_ent'\nfrom sklearn.neighbors import KDTree\n\ndef _app_samp_entropy(x, order, metric='chebyshev', approximate=True):\n    \"\"\"Utility function for `app_entropy`` and `sample_entropy`.\n    \"\"\"\n    _all_metrics = KDTree.valid_metrics\n    if metric not in _all_metrics:\n        raise ValueError('The given metric (%s) is not valid. The valid '\n                         'metric names are: %s' % (metric, _all_metrics))\n    phi = np.zeros(2)\n    r = 0.2 * np.std(x, ddof=1)\n\n    # compute phi(order, r)\n    _emb_data1 = _embed(x, order, 1)\n    if approximate:\n        emb_data1 = _emb_data1\n    else:\n        emb_data1 = _emb_data1[:-1]\n    count1 = KDTree(emb_data1, metric=metric).query_radius(emb_data1, r,\n                                                           count_only=True\n                                                           ).astype(np.float64)\n    # compute phi(order + 1, r)\n    emb_data2 = _embed(x, order + 1, 1)\n    count2 = KDTree(emb_data2, metric=metric).query_radius(emb_data2, r,\n                                                           count_only=True\n                                                           ).astype(np.float64)\n    if approximate:\n        phi[0] = np.mean(np.log(count1 / emb_data1.shape[0]))\n        phi[1] = np.mean(np.log(count2 / emb_data2.shape[0]))\n    else:\n        phi[0] = np.mean((count1 - 1) / (emb_data1.shape[0] - 1))\n        phi[1] = np.mean((count2 - 1) / (emb_data2.shape[0] - 1))\n    return phi\n\n\ndef _numba_sampen(x, mm=2, r=0.2):\n    \"\"\"\n    Fast evaluation of the sample entropy using Numba.\n    \"\"\"\n    n = x.size\n    n1 = n - 1\n    mm += 1\n    mm_dbld = 2 * mm\n\n    # Define threshold\n    r *= x.std()\n\n    # initialize the lists\n    run = [0] * n\n    run1 = run[:]\n    r1 = [0] * (n * mm_dbld)\n    a = [0] * mm\n    b = a[:]\n    p = a[:]\n\n    for i in range(n1):\n        nj = n1 - i\n\n        for jj in range(nj):\n            j = jj + i + 1\n            if abs(x[j] - x[i]) < r:\n                run[jj] = run1[jj] + 1\n                m1 = mm if mm < run[jj] else run[jj]\n                for m in range(m1):\n                    a[m] += 1\n                    if j < n1:\n                        b[m] += 1\n            else:\n                run[jj] = 0\n        for j in range(mm_dbld):\n            run1[j] = run[j]\n            r1[i + n * j] = run[j]\n        if nj > mm_dbld - 1:\n            for j in range(mm_dbld, nj):\n                run1[j] = run[j]\n\n    m = mm - 1\n\n    while m > 0:\n        b[m] = b[m - 1]\n        m -= 1\n\n    b[0] = n * n1 / 2\n    a = np.array([float(aa) for aa in a])\n    b = np.array([float(bb) for bb in b])\n    p = np.true_divide(a, b)\n    return -log(p[-1])\n\n\ndef app_entropy(x, order=2, metric='chebyshev'):\n    phi = _app_samp_entropy(x, order=order, metric=metric, approximate=True)\n    return np.subtract(phi[0], phi[1])\n\ndef make_apx_entropy(df) :\n    ae = [np.nan]*100\n    for i in range(100,len(df)) :\n        ae.append(app_entropy(df.loc[i-100 : i, 'signal']))\n        \n    return ae\n\ndf_train_clean['Tarun_apx_ent'] = make_apx_entropy(df_train_clean)\ndf_train_clean['Tarun_apx_ent'] = df_train_clean['Tarun_apx_ent'].fillna(np.mean(df_train_clean['Tarun_apx_ent']))\ndf_test_clean['Tarun_apx_ent'] = make_apx_entropy(df_test_clean)\ndf_test_clean['Tarun_apx_ent'] = df_test_clean['Tarun_apx_ent'].fillna(np.mean(df_test_clean['Tarun_apx_ent']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Make 'Tarun_higuchi'\nfrom math import log, floor\n\ndef _log_n(min_n, max_n, factor):\n    max_i = int(floor(log(1.0 * max_n / min_n) / log(factor)))\n    ns = [min_n]\n    for i in range(max_i + 1):\n        n = int(floor(min_n * (factor ** i)))\n        if n > ns[-1]:\n            ns.append(n)\n    return np.array(ns, dtype=np.int64)\n\ndef _higuchi_fd(x, kmax):\n    n_times = x.size\n    lk = np.empty(kmax)\n    x_reg = np.empty(kmax)\n    y_reg = np.empty(kmax)\n    for k in range(1, kmax + 1):\n        lm = np.empty((k,))\n        for m in range(k):\n            ll = 0\n            n_max = floor((n_times - m - 1) / k)\n            n_max = int(n_max)\n            for j in range(1, n_max):\n                ll += abs(x[m + j * k] - x[m + (j - 1) * k])\n            ll /= k\n            ll *= (n_times - 1) / (k * n_max)\n            lm[m] = ll\n        # Mean of lm\n        m_lm = 0\n        for m in range(k):\n            m_lm += lm[m]\n        m_lm /= k\n        lk[k - 1] = m_lm\n        x_reg[k - 1] = log(1. / k)\n        y_reg[k - 1] = log(m_lm)\n    higuchi, _ = _linear_regression(x_reg, y_reg)\n    return higuchi\n\n\ndef higuchi_fd(x, kmax=10):\n    x = np.asarray(x, dtype=np.float64)\n    kmax = int(kmax)\n    return _higuchi_fd(x, kmax)\n\ndef _linear_regression(x, y):\n    n_times = x.size\n    sx2 = 0\n    sx = 0\n    sy = 0\n    sxy = 0\n    for j in range(n_times):\n        sx2 += x[j] ** 2\n        sx += x[j]\n        sxy += x[j] * y[j]\n        sy += y[j]\n    den = n_times * sx2 - (sx ** 2)\n    num = n_times * sxy - sx * sy\n    slope = num / den\n    intercept = np.mean(y) - slope * np.mean(x)\n    return slope, intercept\n\ndef make_higuchi(df) :\n    h = [np.nan]*100\n    for i in range(100,len(df)) :\n        h.append(higuchi_fd(df.loc[i-100 : i, 'signal']))\n        \n    return h\n\ndf_train_clean['Tarun_higuchi'] = make_higuchi(df_train_clean)\ndf_train_clean['Tarun_higuchi'] = df_train_clean['Tarun_higuchi'].fillna(np.mean(df_train_clean['Tarun_higuchi']))\ndf_test_clean['Tarun_higuchi'] = make_higuchi(df_test_clean)\ndf_test_clean['Tarun_higuchi'] = df_test_clean['Tarun_higuchi'].fillna(np.mean(df_test_clean['Tarun_higuchi']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Make 'Tarun_katz'\ndef katz_fd(x):\n    x = np.array(x)\n    dists = np.abs(np.ediff1d(x))\n    ll = dists.sum()\n    ln = np.log10(np.divide(ll, dists.mean()))\n    aux_d = x - x[0]\n    d = np.max(np.abs(aux_d[1:]))\n    return np.divide(ln, np.add(ln, np.log10(np.divide(d, ll))))\n\ndef make_katz(df) :\n    k = [np.nan]*100\n    for i in range(100,len(df)) :\n        k.append(katz_fd(df.loc[i-100 : i, 'signal']))\n        \n    return k\n\ndf_train_clean['Tarun_katz'] = make_katz(df_train_clean)\ndf_train_clean['Tarun_katz'] = df_train_clean['Tarun_katz'].fillna(np.mean(df_train_clean['Tarun_katz']))\ndf_test_clean['Tarun_katz'] = make_katz(df_test_clean)\ndf_test_clean['Tarun_katz'] = df_test_clean['Tarun_katz'].fillna(np.mean(df_test_clean['Tarun_katz']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the dataset\ndf_train_clean.to_csv('data_train_hope.csv' ,index=False)\ndf_test_clean.to_csv('data_test_hope.csv' ,index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}