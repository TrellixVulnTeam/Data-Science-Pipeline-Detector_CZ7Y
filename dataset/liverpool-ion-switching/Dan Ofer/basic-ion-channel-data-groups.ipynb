{"cells":[{"metadata":{},"cell_type":"raw","source":"##### Goal: \n* Predict the number of open_channels present, based on electrophysiological signal data. (at each time step and for each time series). \n\n* IMPORTANT: While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001.\n\nI'll do: \n\n* Simple baseline around min/max/average of having X channels opened. \n* Add the groups to segment the data (ech group is a seperate experiment) , for evaluation and further features "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport math","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/liverpool-ion-switching/train.csv\")\n\n\nprint(train.shape)\n\n# df  =train.copy()\n# data=df.values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/liverpool-ion-switching/test.csv\")\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_groups = 100\n# df[\"group\"] = 0\n# for i in range(n_groups):\n#     ids = np.arange(i*50000, (i+1)*50000)\n#     df.loc[ids,\"group\"] = i\n\ntrain['group'] = 0\ntrain.loc[:500000, 'group'] = 1 \ntrain.loc[500000:500000*2, 'group'] = 2\ntrain.loc[500000*2:500000*3, 'group'] = 3\ntrain.loc[500000*3:500000*4, 'group'] = 4\ntrain.loc[500000*4:500000*5, 'group'] = 5\ntrain.loc[500000*5:500000*6, 'group'] = 6\ntrain.loc[500000*6:500000*7, 'group'] = 7\ntrain.loc[500000*7:500000*8, 'group'] = 8\ntrain.loc[500000*8:500000*9, 'group'] = 9\ntrain.loc[500000*9:500000*10, 'group'] = 10\n\n\ntest['group'] = 0\ntest.loc[:500000, 'group'] = 11 \ntest.loc[500000:500000*2, 'group'] = 12\ntest.loc[500000*2:500000*3, 'group'] = 13\ntest.loc[500000*3:500000*4, 'group'] = 14","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## New Times\n* make new running \"time\" index pergroup = time that has passed, within the batch/group**\n*  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.cumcount.html#pandas.core.groupby.GroupBy.cumcount\n\n* We can also add a real datetime and add these values to it with `pd.to_TimeDelta()` , for time series methods that expect it, based on this "},{"metadata":{"trusted":true},"cell_type":"code","source":"test.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### I oerwrite the original time column, to make it easy to \"plug and play\" into other code that expect the time col \ntest[\"time\"] = test.groupby(\"group\").cumcount()+1\ntrain[\"time\"] = train.groupby(\"group\").cumcount()+1\n\ndisplay(test.groupby(\"group\")[\"time\"].head(2))\ndisplay(test.groupby(\"group\")[\"time\"].tail(2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in range(n_groups):\n#     sub = df[df.group == i]\n#     signals = sub.signal.values\n#     imax, imin = math.floor(np.max(signals)), math.ceil(np.min(signals))\n#     signals = (signals - np.min(signals))/(np.max(signals) - np.min(signals))\n#     signals = signals*(imax-imin)\n#     df.loc[sub.index,\"open_channels\"] = [0,] + list(np.array(signals[:-1],np.int))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features:\n* Z-score per group\n* coiuld add more , e.g. change vs minmax. \n\n* e.g. `df.groupby('batch')['signal'].rolling(r).mean().reset_index()['signal']`"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import zscore\ntest[\"signal_zscore\"] = test.groupby([\"group\"])[\"signal\"].transform(lambda x : zscore(x,ddof=1))\ntrain[\"signal_zscore\"] = train.groupby([\"group\"])[\"signal\"].transform(lambda x : zscore(x,ddof=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### add arbitrary start time\n* each step is 0.1 ms (10khz measurement).  We may want to arbitrarily scale up due to worries about precision , although it shouldn;'t be ap roblem.  The real times are important for easy feature engineering using biological prior knowledge"},{"metadata":{"trusted":true},"cell_type":"code","source":"arbitrary_time_start = pd.to_datetime(1490195805403502912, unit='ns')\nprint(arbitrary_time_start)\n\ntest[\"datetime\"] = arbitrary_time_start + pd.to_timedelta(test[\"time\"]*10,unit=\"ms\")\ntrain[\"datetime\"] = arbitrary_time_start + pd.to_timedelta(train[\"time\"]*10,unit=\"ms\")\n\ntest.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.to_csv(\"train_ionChannels.csv.gz\",index=False,compression=\"gzip\")\ntest.to_csv(\"test_ionChannels.csv.gz\",index=False,compression=\"gzip\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### forked code after htis. commented out for now"},{"metadata":{"trusted":true},"cell_type":"code","source":"## We are not going to bother with sample submission, let's just test this smoothing method against the labels in the training data. Should be fine.\n# print(data[:5,1])\n# print(data[:5,2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction = np.array(df.open_channels, np.int)\n# print(prediction[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(prediction.shape)\n# prediction.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks good!\nSo get the metrics, F1, Kappa, Quadratic Kappa and Accuracy;"},{"metadata":{"trusted":true},"cell_type":"code","source":"# #To check I am working the metrics right ;-)\n# gd=[1,2,3,4,5,6,7,8,9,0]\n# pr=[1,2,3,4,5,6,7,8,8,0]\n\n# from sklearn.metrics import cohen_kappa_score , accuracy_score ,f1_score\n\n# print(\"Regular Cohen's Kappa\", cohen_kappa_score(np.asarray(data[:,2],np.int),np.array(df.open_channels, np.int),weights=\"quadratic\"))\n# print(\"Quadratic Cohen's Kappa\", cohen_kappa_score(np.asarray(data[:,2],np.int),np.array(df.open_channels, np.int)))\n# print(\"Accuracy\", accuracy_score(data[:,2],np.array(df.open_channels, np.int)))\n# print(\"f1\", f1_score(data[:,2],np.array(df.open_channels, np.int)))\n# print(\"test Accuracy\", accuracy_score(gd,pr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}