{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\ninput_data=pd.read_csv('/kaggle/input/data-without-drift/test_clean.csv')\nprint(input_data.head())\ntrain_data=pd.read_csv('/kaggle/input/data-without-drift/train_clean.csv')\nprint(train_data.head())\n\ninput_data_drifted=pd.read_csv('/kaggle/input/liverpool-ion-switching/test.csv')\n\ntrain_data_drifted=pd.read_csv('/kaggle/input/liverpool-ion-switching/train.csv')\n\n'''input_data=pd.DataFrame(data={'time':[0,2,10],'signal':[0,3,11],'open_channels':[1,3,12]})\ninput_data_drifted=pd.DataFrame(data={'time':[0,5,17],'signal':[0,4,17],'open_channels':[0,1,17]})\n\ntrain_data=pd.DataFrame(data={'time':[0,1,19],'signal':[0,2,18],'open_channels':[1,7,18]})\ntrain_data_drifted=pd.DataFrame(data={'time':[0,5,11],'signal':[0,4,15],'open_channels':[0,1,16]})'''\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We add some plots in order to observe the continous batches, and check if the drift in the dataset was removed properly."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(input_data.time,input_data.signal)\nplt.show()\n\nplt.plot(train_data.time,train_data.signal)\nplt.show()\n\nprint(train_data.open_channels.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\nWe create sub-batches of the continous batches, and add some feature engineering on them. The reason for sub-batching is having some local information rather than the global operation of 10 concatenated continuous functions.\n\nWe add a drift feature because the data-cleaning process may have affected the correctnes of data, and including this in our model may help reducing the bias induced by improper drift removal.\n\nShifting helps us see how the continuous function evolves, taking some small discrete steps that try to evaluate the continous inferred graph.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['original']=train_data_drifted['signal']\ninput_data['original']=input_data_drifted['signal']\n\ntrain_data['drift']=train_data['signal']-train_data['original']\ninput_data['drift']=input_data['signal']-input_data['original']\n\nbatch_size=5*(10**4)\ntrain_data['index_in_batch']=[i%batch_size for i in range(train_data.shape[0])]\ninput_data['index_in_batch']=[i%batch_size for i in range(input_data.shape[0])]\n\ntrain_data['batch']=[i//batch_size for i in range(train_data.shape[0])]\ninput_data['batch']=[i//batch_size for i in range(input_data.shape[0])]\n\ntrain_data['max']=train_data.groupby('batch')['signal'].transform('max')\ninput_data['max']=input_data.groupby('batch')['signal'].transform('max')\n\ntrain_data['min']=train_data.groupby('batch')['signal'].transform('min')\ninput_data['min']=input_data.groupby('batch')['signal'].transform('min')\n\ntrain_data['mean']=train_data.groupby('batch')['signal'].transform('mean')\ninput_data['mean']=input_data.groupby('batch')['signal'].transform('mean')\n\ntrain_data['signal_shifted1']=train_data.groupby('batch').shift(1)['signal']\ninput_data['signal_shifted1']=input_data.groupby('batch').shift(1)['signal']\n\ntrain_data['signal_shifted-1']=train_data.groupby('batch').shift(-1)['signal']\ninput_data['signal_shifted-1']=input_data.groupby('batch').shift(-1)['signal']\n\ntrain_data['signal_shifted2']=train_data.groupby('batch').shift(2)['signal']\ninput_data['signal_shifted2']=input_data.groupby('batch').shift(2)['signal']\n\ntrain_data['signal_shifted-2']=train_data.groupby('batch').shift(-2)['signal']\ninput_data['signal_shifted-2']=input_data.groupby('batch').shift(-2)['signal']\n\ntrain_data['diff1']=train_data['signal']-train_data['signal_shifted1']\ninput_data['diff1']=input_data['signal']-input_data['signal_shifted1']\n\ntrain_data['diff-1']=train_data['signal']-train_data['signal_shifted-1']\ninput_data['diff-1']=input_data['signal']-input_data['signal_shifted-1']\n\ntrain_data['diff2']=train_data['signal']-train_data['signal_shifted2']\ninput_data['diff2']=input_data['signal']-input_data['signal_shifted2']\n\ntrain_data['diff-2']=train_data['signal']-train_data['signal_shifted-2']\ninput_data['diff-2']=input_data['signal']-input_data['signal_shifted-2']\n\ntrain_data['median']=train_data.groupby('batch')['signal'].transform('median')\ninput_data['median']=input_data.groupby('batch')['signal'].transform('median')\n\ntrain_data['mean_drift']=train_data.groupby('batch')['drift'].transform('mean')\ninput_data['mean_drift']=input_data.groupby('batch')['drift'].transform('mean')\n\n#train_data['roll']=train_data.groupby('batch')['drift'].transform('mean')\n#input_data['roll']=input_data.groupby('batch')['drift'].transform('mean')\nprint(train_data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also try adding some smaller batches, that may help us with some \"local\" information."},{"metadata":{"trusted":true},"cell_type":"code","source":"small_batch=5*(10**3)\n\ntrain_data['small_batch']=[i//batch_size for i in range(train_data.shape[0])]\ninput_data['small_batch']=[i//batch_size for i in range(input_data.shape[0])]\n\ntrain_data['small_batch_median']=train_data.groupby('batch')['signal'].transform('median')\ninput_data['small_batch_median']=input_data.groupby('batch')['signal'].transform('median')\n\ntrain_data['small_batch_mean']=train_data.groupby('batch')['signal'].transform('mean')\ninput_data['small_batch_mean']=input_data.groupby('batch')['signal'].transform('mean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model creation\nWe add to our model features which experimentally produced the accurate results and are diverse enough in terms of the insights they evaluate."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\n\n#think also about diff1 and diff-1\ncolumns_to_use=['signal','max','min','mean','median','diff1','diff-1','diff2','diff-2','index_in_batch','small_batch_median','small_batch_mean','small_batch','batch']\n\nprint(\"ok\")\n#TO DO:remove heads\nX=train_data[columns_to_use]\ny=train_data.open_channels\n\nX_final=input_data[columns_to_use]\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some deprecated model"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''from sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef get_error(y,actual_y):\n    return mean_absolute_error(y,actual_y)\n\npossible_depths=[10,15,20]\n\nbest_error=20\nbest_depth=20\n\nfor depth in possible_depths:\n    classifier=RandomForestClassifier(max_depth=depth,random_state=0)\n    classifier.fit(X_train,y_train)\n    predictions=classifier.predict(X_test)\n    error=get_error(predictions,y_test)\n    if error<best_error:\n        best_error=error\n        best_depth=depth\n    print(\"solved\")\nprint(best_depth,best_error)'''\nnothing_happens_here=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#inspired by: https://www.kaggle.com/siavrez/simple-eda-model/data\nimport lightgbm as lgb\nparams={'learning_rate':0.1,'max_depth':-1,'num_leaves':2**7+1,'metric':'mae','random_state':7,'n_jobs':-1,'sample_fraction':0.33,'verbose':-1}\nmodel1=lgb.train(params,lgb.Dataset(X_train,y_train),22222,lgb.Dataset(X_test,y_test),early_stopping_rounds=250)\npred1=model1.predict(X_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#inspired by: https://www.kaggle.com/siavrez/simple-eda-model/data\nfrom catboost import Pool,CatBoostRegressor\n    \nmodel2=CatBoostRegressor(task_type='GPU',iterations=22222,learning_rate=0.1,random_seed=7,depth=7,eval_metric='MAE')\ntrainer=Pool(X_train,y_train)\nvalidator=Pool(X_test,y_test)\nmodel2.fit(trainer,eval_set=validator,verbose=0,early_stopping_rounds=250)\npred2=model2.predict(X_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\n\nmodel3=XGBRegressor(n_estimators=50,learning_rat=0.1,n_jobs=4)\nmodel3.fit(X_train,y_train,eval_set=[(X_test,y_test)],verbose=False,early_stopping_rounds=250)\npred3=model3.predict(X_final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model combining\nAfter training 3 different gradient boosters, we try to evaluate the final result as the most accurate linear combination of the 3 evaluations. The function addition behaves linearly."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nbest_score=0\nbest_coef1=0\nbest_coef2=0\np1=model1.predict(X_test)\np2=model2.predict(X_test)\np3=model3.predict(X_test)\nfor coef1 in range(10):\n    for coef2 in range(10):\n        test_pred=np.around(((coef1)*np.array(p1)+(coef2)*np.array(p2)+(10-coef1-coef2)*np.array(p3))/10).astype(int)\n        score=f1_score(test_pred,y_test,average='macro')\n        if score>best_score:\n            best_score=score\n            best_coef1=coef1\n            best_coef2=coef2\ncoef1,coef2=best_coef1,best_coef2\nanswer=np.around(((coef1)*np.array(pred1)+(coef2)*np.array(pred2)+(10-coef1-coef2)*np.array(pred3))/10).astype(int)\nprint(best_score)\nprint(best_coef1,best_coef2,10-best_coef1-best_coef2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#classifier=RandomForestClassifier(max_depth=best_depth,random_state=0)\n#classifier.fit(X,y)\n#answer=[0 for i in range(Input.shape[0])]\n#answer=classifier.predict(X_final)\nOutput=pd.DataFrame({'time': input_data.time,'open_channels':answer})\nOutput.to_csv('submission.csv',index=False,float_format='%.4f')\nprint(Output.head())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}