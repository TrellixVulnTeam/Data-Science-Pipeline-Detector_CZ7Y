{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport glob\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.layers import Dense, Dropout, Reshape, Conv1D, BatchNormalization, Activation, AveragePooling1D, GlobalAveragePooling1D, Lambda, Input, Concatenate, Add, UpSampling1D, Multiply\nfrom keras.models import Model\nfrom keras.objectives import mean_squared_error\nfrom keras import backend as K\nfrom keras.losses import binary_crossentropy, categorical_crossentropy\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau,LearningRateScheduler\nfrom keras.initializers import random_normal\nfrom keras.optimizers import Adam, RMSprop, SGD\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import KFold, train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/liverpool-ion-switching/train.csv\")\ndf_test = pd.read_csv(\"../input/liverpool-ion-switching/test.csv\")\n\n# I don't use \"time\" feature\ntrain_input = df_train[\"signal\"].values.reshape(-1,4000,1)#number_of_data:1250 x time_step:4000\ntrain_input_mean = train_input.mean()\ntrain_input_sigma = train_input.std()\ntrain_input = (train_input-train_input_mean)/train_input_sigma\ntest_input = df_test[\"signal\"].values.reshape(-1,4000,1)\ntest_input = (test_input-train_input_mean)/train_input_sigma\n\ntrain_target = df_train[\"open_channels\"].values.reshape(-1,4000,1)\n\nidx = np.arange(train_input.shape[0])\ntrain_idx, val_idx = train_test_split(idx, random_state = 111,test_size = 0.2)\n\nval_input = train_input[val_idx]\ntrain_input = train_input[train_idx] \nval_target = train_target[val_idx]\ntrain_target = train_target[train_idx] \n\nprint(\"train_input:{}, val_input:{}, train_target:{}, val_target:{}\".format(train_input.shape, val_input.shape, train_target.shape, val_target.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cbr(x, out_layer, kernel, stride, dilation):\n    x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    return x\n\ndef se_block(x_in, layer_n):\n    x = GlobalAveragePooling1D()(x_in)\n    x = Dense(layer_n//8, activation=\"relu\")(x)\n    x = Dense(layer_n, activation=\"sigmoid\")(x)\n    x_out=Multiply()([x_in, x])\n    return x_out\n\ndef resblock(x_in, layer_n, kernel, dilation, use_se=True):\n    x = cbr(x_in, layer_n, kernel, 1, dilation)\n    x = cbr(x, layer_n, kernel, 1, dilation)\n    if use_se:\n        x = se_block(x, layer_n)\n    x = Add()([x_in, x])\n    return x  \n\ndef Unet(input_shape=(None,1)):\n    layer_n = 64\n    kernel_size = 7\n    depth = 2\n\n    input_layer = Input(input_shape)    \n    input_layer_1 = AveragePooling1D(5)(input_layer)\n    input_layer_2 = AveragePooling1D(25)(input_layer)\n    \n    ########## Encoder\n    x = cbr(input_layer, layer_n, kernel_size, 1, 1)#1000\n    for i in range(depth):\n        x = resblock(x, layer_n, kernel_size, 1)\n    out_0 = x\n\n    x = cbr(x, layer_n*2, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*2, kernel_size, 1)\n    out_1 = x\n\n    x = Concatenate()([x, input_layer_1])    \n    x = cbr(x, layer_n*3, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*3, kernel_size, 1)\n    out_2 = x\n\n    x = Concatenate()([x, input_layer_2])    \n    x = cbr(x, layer_n*4, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*4, kernel_size, 1)\n    \n    ########### Decoder\n    x = UpSampling1D(5)(x)\n    x = Concatenate()([x, out_2])\n    x = cbr(x, layer_n*3, kernel_size, 1, 1)\n\n    x = UpSampling1D(5)(x)\n    x = Concatenate()([x, out_1])\n    x = cbr(x, layer_n*2, kernel_size, 1, 1)\n\n    x = UpSampling1D(5)(x)\n    x = Concatenate()([x, out_0])\n    x = cbr(x, layer_n, kernel_size, 1, 1)    \n\n    x = Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\")(x)\n    out = Activation(\"sigmoid\")(x)\n    out = Lambda(lambda x: 12*x)(out)\n    \n    model = Model(input_layer, out)\n    \n    return model\n\n\ndef augmentations(input_data, target_data):\n    #flip\n    if np.random.rand()<0.5:    \n        input_data = input_data[::-1]\n        target_data = target_data[::-1]\n\n    return input_data, target_data\n\n\ndef Datagen(input_dataset, target_dataset, batch_size, is_train=False):\n    x=[]\n    y=[]\n  \n    count=0\n    idx_1 = np.arange(len(input_dataset))\n    #idx_2 = np.arange(len(input_dataset))\n    np.random.shuffle(idx_1)\n    #np.random.shuffle(idx_2)\n\n    while True:\n        for i in range(len(input_dataset)):\n            input_data = input_dataset[idx_1[i]]\n            target_data = target_dataset[idx_1[i]]\n            #input_data_mix = input_dataset[idx_2[i]]\n            #target_data_mix = target_dataset[idx_2[i]]\n\n            if is_train:\n                input_data, target_data = augmentations(input_data, target_data)\n                #input_data_mix, target_data_mix = augmentations(input_data_mix, target_data_mix)\n                \n            x.append(input_data)\n            y.append(target_data)\n            count+=1\n            if count==batch_size:\n                x=np.array(x, dtype=np.float32)\n                y=np.array(y, dtype=np.float32)\n                inputs = x\n                targets = y       \n                x = []\n                y = []\n                count=0\n                yield inputs, targets\n\n                \ndef model_fit(model, train_inputs, train_targets, val_inputs, val_targets, n_epoch, batch_size=32):\n    hist = model.fit_generator(\n        Datagen(train_inputs, train_targets, batch_size, is_train=True),\n        steps_per_epoch = len(train_inputs) // batch_size,\n        epochs = n_epoch,\n        validation_data=Datagen(val_inputs, val_targets, batch_size),\n        validation_steps = len(val_inputs) // batch_size,\n        callbacks = [lr_schedule],\n        shuffle = False,\n        verbose = 1\n        )\n    return hist\n\n\ndef lrs(epoch):\n    if epoch<35:\n        lr = learning_rate\n    elif epoch<50:\n        lr = learning_rate/10\n    else:\n        lr = learning_rate/100\n    return lr\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"K.clear_session()\nmodel = Unet()\n#print(model.summary())\n\nlearning_rate=0.0005\nn_epoch=60\nbatch_size=32\n\nlr_schedule = LearningRateScheduler(lrs)\nmodel.compile(loss=\"mean_squared_error\", \n              optimizer=Adam(lr=learning_rate),\n              metrics=[\"mean_absolute_error\"])\n\nhist = model_fit(model, train_input, train_target, val_input, val_target, n_epoch, batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = ((model.predict(val_input)+model.predict(val_input[:,::-1,:])[:,::-1,:])/2).reshape(-1)\nprint(\"VALIDATION_SCORE: \", cohen_kappa_score(val_target.reshape(-1), np.round(pred, 0), weights=\"quadratic\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = ((model.predict(test_input)+model.predict(test_input[:,::-1,:])[:,::-1,:])/2).reshape(-1)\n\ndf_sub = pd.read_csv(\"../input/liverpool-ion-switching/sample_submission.csv\", dtype={'time':str})\ndf_sub.open_channels = np.array(np.round(pred,0), np.int)\ndf_sub.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}