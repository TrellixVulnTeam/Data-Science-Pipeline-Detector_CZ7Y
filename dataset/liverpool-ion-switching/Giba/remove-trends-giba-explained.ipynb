{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [10, 10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load the train and test sets","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv( '../input/liverpool-ion-switching/train.csv' , dtype={'time':'str','signal':'float','open_channels':'int'} )\ntest  = pd.read_csv( '../input/liverpool-ion-switching/test.csv'  , dtype={'time':'str','signal':'float'} )\n\n# Split trainset in groups of 100k and 500k consecutive rows\ntrain['group']  = np.arange(train.shape[0])//100_000\ntrain['group2'] = np.arange(train.shape[0])//500_000\n\n# Split testset in groups of 100k consecutive rows\ntest['group']   = np.arange(test.shape[0])//100_000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fix train and test signal levels.\n## Train 500k groups 4 and 9 and test 100k groups 5 and 7 needs to be shifted by +2*exp(1)\n## Other groups are shifted by +exp(1)\n## All groups must be rescaled by 1.25 to match open_channels scale","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[ (train.group2==4)|(train.group2==9) ,'signal'] += np.exp(1)\ntest.loc[  ( test.group ==5)|( test.group ==7) ,'signal'] += np.exp(1)\n\ntrain['signal'] += np.exp(1)\ntrain['signal'] /= 1.25\n\ntest['signal'] += np.exp(1)\ntest['signal'] /= 1.25","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyzing the signal and target statistics we classified each train and test groups as xA and xB, where x is the maximum number of simultaneous open_channels per group","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MAP = {0:'1A', 1:'1A', 2:'1B', 3:'3B', 4:'10B', 5:'5B', 6:'1B', 7:'3B', 8:'5B', 9:'10B'}\ntrain['type'] = train['group2'].map( MAP )\n\nMAP = {0:'3A', 1:'3B', 2:'5B', 3:'3A', 4:'1B', 5:'10B', 6:'5B', 7:'10B', 8:'3A', 9:'3B',10:'3A', 11:'3A', 12:'4A', 13:'3A', 14:'3A', 15:'3A', 16:'3A', 17:'3A', 18:'3A', 19:'3A'}\ntest['type'] = test['group'].map( MAP )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Observe the drifts in trainset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n    train.loc[train.group2==i].signal.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(20):\n    test.loc[test.group==i].signal.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# After deep analisys we found that all the drifts come from the same equation:\n## 4 * sin(  2 * pi * range(500000) / 1000000  )","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SINE_DRIFT = 4*np.sin(  2*np.pi*np.arange(500000)/1000000 )\nplt.plot( SINE_DRIFT )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Remove the drift from train and test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[ train.group2==6, 'signal' ] -= SINE_DRIFT\ntrain.loc[ train.group2==7, 'signal' ] -= SINE_DRIFT\ntrain.loc[ train.group2==8, 'signal' ] -= SINE_DRIFT\ntrain.loc[ train.group2==9, 'signal' ] -= SINE_DRIFT\n\ntrain.signal.iloc[ 500000:600000 ]-= SINE_DRIFT[:100000]\n\ntest.signal.iloc[ :100000 ]       -= SINE_DRIFT[:100000]\ntest.signal.iloc[ 100000:200000 ] -= SINE_DRIFT[:100000]\ntest.signal.iloc[ 400000:500000 ] -= SINE_DRIFT[:100000]\ntest.signal.iloc[ 600000:700000 ] -= SINE_DRIFT[:100000]\ntest.signal.iloc[ 700000:800000 ] -= SINE_DRIFT[:100000]\ntest.signal.iloc[ 800000:900000 ] -= SINE_DRIFT[:100000]\n\ntest.loc[ (test.group>=10)&(test.group<=14), 'signal' ] -= SINE_DRIFT","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby(['group2','type'])['signal'].agg(['mean','std'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(50):\n    train.loc[train.group==i].signal.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(20):\n    test.loc[test.group==i].signal.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Remove outlier on train 100k batch group 4\n## this outlier pattern happens only on trainset so its safe to remove it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train.group==4].signal.plot()\n\ntrain['noise'] = train['signal'] - train['open_channels']\ntrain.loc[ (train.group==4)&(train.noise>1.0), 'signal' ] = np.random.normal( 0,0.2,1 ) #Just add gaussian noise with std 0.20\n\ntrain.loc[train.group==4].signal.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Remove/Decrease 50Hz power line interference","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Power line 50Hz interference is that small peak at axis 500 of the plot below\n# We used Fourier Transform to remove it precisely\nf = np.fft.fft( train.loc[train.group==45].signal )\nplt.plot( np.abs(f)[1:1000] )\nplt.plot([480, 480], [0, 8000], '--', lw=1)\nplt.plot([510, 510], [0, 8000], '--', lw=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import fftpack\n\n# Removing 100% of power line interference can remove also usefull information that lies in the 50Hz spectrum.\n# Thats why we removed only 45% of powerline interference in order to keep some useful signal present in the 50Hz.\n# Remove 50Hz in batches of 100k rows.\n\nfor g in range( train.group.max() ):\n    f = np.fft.fft( train.loc[train.group==g].signal )\n    freq = np.abs( fftpack.fftfreq( len(f) , d=0.0001) )\n    f[ (freq >= 49.8)& (freq <= 50.2) ] = 0.55*f[ (freq >= 49.8)& (freq <= 50.2) ]\n    train.loc[train.group==g,'signal'] = np.fft.ifft( f ).real\n    \nfor g in range( test.group.max() ):\n    f = np.fft.fft( test.loc[test.group==g].signal )\n    freq = np.abs( fftpack.fftfreq( len(f) , d=0.0001) )\n    f[ (freq >= 49.8)& (freq <= 50.2) ] = 0.55*f[ (freq >= 49.8)& (freq <= 50.2) ]\n    test.loc[test.group==g,'signal'] = np.fft.ifft( f ).real\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Note that the 50Hz frequency interference was decreased but not removed 100%.\nf = np.fft.fft( train.loc[train.group==45].signal )\nplt.plot( np.abs(f)[1:1000] )\nplt.plot([480, 480], [0, 8000], '--', lw=1)\nplt.plot([510, 510], [0, 8000], '--', lw=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now that signal and open_channels are in the same scale, to calculate the noise added to each row it is just a mater of subtract each other.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate real noise in train set.\n# This is only possible because signal and open_channels are in the same scale\ntrain['noise']       = train['signal'] - train['open_channels']\n\n# Subtracting signal from round(signal) is a pseudo way to calculate noise that can be applied both to train and test sets.\ntrain['noise_round'] = train['signal'] - train['signal'].round()\ntest ['noise_round'] = test ['signal'] - test ['signal'].round()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This is the bias offset and Standard deviation of the noise added to each group type.\ntrain.groupby(['group2','type'])['noise'].agg( ['mean','std'] )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Note that the std of the gaussian noise above is different for each type:\n## Type 1A sdt ~ 0.197\n## Type 1B sdt ~ 0.197\n## Type 3B sdt ~ 0.215\n## Type 5B sdt ~ 0.231\n## Type 10B sdt ~ 0.327","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adding 2x 5B std noise gives:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.random.normal( 0, 0.231, 1000000) + np.random.normal( 0, 0.231, 1000000)\nnp.std(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The result above makes us think that the 10 channels groups are the sum of two 5 channels groups","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Since we can't have the real noise in test set, we can approximate the noise in test using signal-signal.round() as a proxy for the noise.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby(['group2','type'])['noise_round'].agg( ['mean','std'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.groupby(['group','type'])['noise_round'].agg( ['mean','std'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # Removing outliers from 100k groups batches 36, 37 and 38 of trainset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## We can cleanly see that there is an interference in the 3B groups in trainset and the corresponding 3B groups in test doesn't have such interference.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[ train.group.isin([36,37,38]) ,'signal'].plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.loc[ test.type=='3B'].plot( x='time' ,y='signal' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( train.loc[ train.group==35 ].groupby('open_channels')['noise'].agg(['mean','std']) )\nprint( train.loc[ train.group==39 ].groupby('open_channels')['noise'].agg(['mean','std']) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## to remove the outliers, lets just add a gaussian noise to open_channels with a specific std level for the specific batches.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = train.loc[ train.group==36 ].copy()\ntmp.loc[ tmp.open_channels==0 ,'signal'] = 0 + np.random.normal(  -0.0130 , 0.2342, np.sum(tmp.open_channels==0) )\ntmp.loc[ tmp.open_channels==1 ,'signal'] = 1 + np.random.normal(  -0.0299 , 0.2356, np.sum(tmp.open_channels==1) )\ntmp.loc[ tmp.open_channels==2 ,'signal'] = 2 + np.random.normal(  -0.0460 , 0.2368, np.sum(tmp.open_channels==2) )\ntmp.loc[ tmp.open_channels==3 ,'signal'] = 3 + np.random.normal(  -0.0600 , 0.2344, np.sum(tmp.open_channels==3) )\ntrain.loc[ train.group==36 ] = tmp.copy()\n\ntmp = train.loc[ train.group==37 ].copy()\ntmp.loc[ tmp.open_channels==0 ,'signal'] = 0 + np.random.normal(  -0.0110 , 0.2307, np.sum(tmp.open_channels==0) )\ntmp.loc[ tmp.open_channels==1 ,'signal'] = 1 + np.random.normal(  -0.0299 , 0.2309, np.sum(tmp.open_channels==1) )\ntmp.loc[ tmp.open_channels==2 ,'signal'] = 2 + np.random.normal(  -0.0450 , 0.2368, np.sum(tmp.open_channels==2) )\ntmp.loc[ tmp.open_channels==3 ,'signal'] = 3 + np.random.normal(  -0.0580 , 0.2295, np.sum(tmp.open_channels==3) )\ntrain.loc[ train.group==37 ] = tmp.copy()\n\ntmp = train.loc[ train.group==38 ].copy()\ntmp.loc[ tmp.open_channels==0 ,'signal'] = 0 + np.random.normal(  -0.0100 , 0.2257, np.sum(tmp.open_channels==0) )\ntmp.loc[ tmp.open_channels==1 ,'signal'] = 1 + np.random.normal(  -0.0299 , 0.2275, np.sum(tmp.open_channels==1) )\ntmp.loc[ tmp.open_channels==2 ,'signal'] = 2 + np.random.normal(  -0.0440 , 0.2278, np.sum(tmp.open_channels==2) )\ntmp.loc[ tmp.open_channels==3 ,'signal'] = 3 + np.random.normal(  -0.0560 , 0.2245, np.sum(tmp.open_channels==3) )\ntrain.loc[ train.group==38 ] = tmp.copy()\n\ntrain['noise'] = train['signal'] - train['open_channels']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now all train groups looks fine\nfor i in range(50):\n    train.loc[train.group==i].signal.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now outlier group 7(3B) have similar statistics as group 3(3B)\ntrain.groupby(['group2','type'])['signal'].agg(['mean','std'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# There still some bias to be removed per group, both in train and in test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(50):\n    train.loc[train.group2==i].noise_round.rolling(10000).mean().plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(20):\n    test.loc[test.group==i].noise_round.rolling(10000).mean().plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Remove the bias","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove the bias offset using an iterative loop for both train and test sets\n\ntrain['noise_round'] = train['signal'] - train['signal'].round()\ntest ['noise_round'] = test ['signal'] - test ['signal'].round()\nfor i in range( 7 ):\n    train['bias'] = train.groupby('group')['noise_round'].transform('mean')\n    test ['bias'] = test.groupby( 'group')['noise_round'].transform('mean')\n    \n    train['signal'] = train['signal'] - train['bias']\n    test ['signal'] = test ['signal'] - test ['bias']\n    train['noise_round'] = train['signal'] - train['signal'].round()\n    test ['noise_round'] = test ['signal'] - test ['signal'].round()\n    \n    print( i, 'acc:',np.mean( train.open_channels == train.signal.round() ) , 'f1:',f1_score( train.open_channels , np.clip(train.signal.round(),0,10), average='macro' ) )\n\ntrain['noise'] = train['signal'] - train['open_channels']\ntrain['noise_round'] = train['signal'] - train['signal'].round()\ntest ['noise_round'] = test ['signal'] - test ['signal'].round()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check bias offsets now. Must be close to zero.\ntrain.groupby(['group','type'])['noise'].agg(['mean','std'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby(['group','type'])['noise_round'].agg(['mean','std'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.groupby(['group','type'])['noise_round'].agg(['mean','std'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now the bias offset are close to zero. But still a small bias offset in groups 10B.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(50):\n    train.loc[train.group==i].noise_round.rolling(10000).mean().plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The bias offset in groups 10B in train is at the same level as in test, so don't worry about it. \nfor i in range(10):\n    test.loc[test.group==i].noise_round.rolling(10000).mean().plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Accuracy and F1 using only round(signal) as a predictor.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print( 'ACC:', np.mean( train.open_channels == train.signal.round() )  )\nprint( 'F1:', f1_score( train.open_channels , np.clip(train.signal.round(),0,10), average='macro' ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calc ACC per group\ntrain['hit'] = train.open_channels == train.signal.round()\ntrain.groupby(['group','type'])['hit'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['time','signal','type','open_channels']].to_csv('train_clean_giba.csv', index=False )\ntest [['time','signal','type']].to_csv('test_clean_giba.csv', index=False )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}