{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport glob\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.layers import Dense, Dropout, Reshape, Conv1D, BatchNormalization, Activation, AveragePooling1D, GlobalAveragePooling1D, Lambda, Input, Concatenate, Add, UpSampling1D, Multiply\nfrom keras.models import Model\nfrom keras.objectives import mean_squared_error\nfrom keras import backend as K\nfrom keras.losses import binary_crossentropy, categorical_crossentropy,sparse_categorical_crossentropy\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau,LearningRateScheduler\nfrom keras.initializers import random_normal\nfrom keras.optimizers import Adam, RMSprop, SGD\nfrom keras.callbacks import Callback\nfrom keras.layers import Dense, Dropout, Reshape, Conv1D, BatchNormalization, Activation, AveragePooling1D, GlobalAveragePooling1D, Lambda, Input, Concatenate, Add, UpSampling1D, Multiply\nfrom sklearn.metrics import cohen_kappa_score, f1_score\nfrom sklearn.model_selection import KFold, train_test_split\n\n\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/liverpool-ion-switching/train.csv\")\ntest = pd.read_csv(\"../input/liverpool-ion-switching/test.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize(df):\n    arr = df['signal'].values\n    arr_mean = arr.mean()\n    arr_std = arr.std()\n    arr = (arr - arr_mean)/arr_std\n    df['signal'] = pd.DataFrame(arr)\n    return df\n\n\n# train = normalize(train)\n# test = normalize(test)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature(df):\n    df.index = (df.time*10000 - 1).values\n    df['batch'] = df.index // 50000 \n    df['mean'] = df.groupby('batch')['signal'].mean()\n    df['median'] = df.groupby('batch')['signal'].median()\n    df['max'] = df.groupby('batch')['signal'].max()\n    df['min'] = df.groupby('batch')['signal'].min()\n    df['std'] = df.groupby('batch')['signal'].std()\n    df = df.replace([np.inf, -np.inf], np.nan)    \n    df.fillna(0, inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = feature(train)\ntest = feature(test)\ncol = [c for c in train.columns if c not in ['time', 'open_channels']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n\ntrain_input = train[col].values.reshape(-1,4000,7)\n# train_input_mean = train_input.mean()\n# train_input_sigma = train_input.std()\n# train_input = (train_input-train_input_mean)/train_input_sigma\ntest_input = test[col].values.reshape(-1,10000,7)#\n# test_input = (test_input-train_input_mean)/train_input_sigma\n\n#train_target = df_train[\"open_channels\"].values.reshape(-1,4000,1)#regression\ntrain_target = pd.get_dummies(train[\"open_channels\"]).values.reshape(-1,4000,11)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x,valid_x,train_y,valid_y = train_test_split(train_input,train_target,random_state = 111,test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_x.shape)\nprint(valid_x.shape)\nprint(train_y.shape)\nprint(valid_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class macroF1(Callback):\n    def __init__(self, model, inputs, targets):\n        self.model = model\n        self.inputs = inputs\n        self.targets = np.argmax(targets, axis=2).reshape(-1)\n\n    def on_epoch_end(self, epoch, logs):\n        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n        f1_val = f1_score(self.targets, pred, average=\"macro\")\n        print(\"val_f1_macro_score: \", f1_val)\n        \n        \ndef augmentations(input_data, target_data):\n    #flip\n    if np.random.rand()<0.5:    \n        input_data = input_data[::-1]\n        target_data = target_data[::-1]\n\n    return input_data, target_data\n\n\ndef Datagen(input_dataset, target_dataset, batch_size, is_train=False):\n    x=[]\n    y=[]\n  \n    count=0\n    idx_1 = np.arange(len(input_dataset))\n    np.random.shuffle(idx_1)\n\n    while True:\n        for i in range(len(input_dataset)):\n            input_data = input_dataset[idx_1[i]]\n            target_data = target_dataset[idx_1[i]]\n            \n\n            if is_train:\n                input_data, target_data = augmentations(input_data, target_data)\n                \n                \n            x.append(input_data)\n            y.append(target_data)\n            count+=1\n            if count==batch_size:\n                x=np.array(x, dtype=np.float32)\n                y=np.array(y, dtype=np.float32)\n                inputs = x\n                targets = y       \n                x = []\n                y = []\n                count=0\n                yield inputs, targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cbr(x, out_layer, kernel, stride, dilation):\n    x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    return x\n\ndef se_block(x_in, layer_n):\n    x = GlobalAveragePooling1D()(x_in)\n    x = Dense(layer_n//8, activation=\"relu\")(x)\n    x = Dense(layer_n, activation=\"sigmoid\")(x)\n    x_out=Multiply()([x_in, x])\n    return x_out\n\ndef resblock(x_in, layer_n, kernel, dilation, use_se=True):\n    x = cbr(x_in, layer_n, kernel, 1, dilation)\n    x = cbr(x, layer_n, kernel, 1, dilation)\n    if use_se:\n        x = se_block(x, layer_n)\n    x = Add()([x_in, x])\n    return x  \n\ndef Unet(input_shape=(None,7)):\n    layer_n = 128\n    kernel_size = 7\n    depth = 2\n\n    input_layer = Input(input_shape)    \n    input_layer_1 = AveragePooling1D(5)(input_layer)\n    input_layer_2 = AveragePooling1D(25)(input_layer)\n    \n    ########## Encoder\n    x = cbr(input_layer, layer_n, kernel_size, 1, 1)#1000\n    for i in range(depth):\n        x = resblock(x, layer_n, kernel_size, 1)\n    out_0 = x\n\n    x = cbr(x, layer_n*2, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*2, kernel_size, 1)\n    out_1 = x\n\n    x = Concatenate()([x, input_layer_1])    \n    x = cbr(x, layer_n*3, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*3, kernel_size, 1)\n    out_2 = x\n\n    x = Concatenate()([x, input_layer_2])    \n    x = cbr(x, layer_n*4, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*4, kernel_size, 1)\n    \n    ########### Decoder\n    x = UpSampling1D(5)(x)\n    x = Concatenate()([x, out_2])\n    x = cbr(x, layer_n*3, kernel_size, 1, 1)\n\n    x = UpSampling1D(5)(x)\n    x = Concatenate()([x, out_1])\n    x = cbr(x, layer_n*2, kernel_size, 1, 1)\n\n    x = UpSampling1D(5)(x)\n    x = Concatenate()([x, out_0])\n    x = cbr(x, layer_n, kernel_size, 1, 1)    \n\n    #regressor\n    #x = Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\")(x)\n    #out = Activation(\"sigmoid\")(x)\n    #out = Lambda(lambda x: 12*x)(out)\n    \n    #classifier\n    x = Conv1D(11, kernel_size=kernel_size, strides=1, padding=\"same\")(x)\n    out = Activation(\"softmax\")(x)\n    \n    model = Model(input_layer, out)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Unet()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lrs(epoch):\n    if epoch<35:\n        lr = learning_rate\n    elif epoch<50:\n        lr = learning_rate/10\n    else:\n        lr = learning_rate/100\n    return lr\n\n\nlearning_rate=0.0015\nn_epoch=100\nbatch_size=32\n\nlr_schedule = LearningRateScheduler(lrs)\n\n#regressor\n#model.compile(loss=\"mean_squared_error\", \n#              optimizer=Adam(lr=learni'ng_rate),\n#              metrics=[\"mean_absolute_error\"])\n\n#classifier\nmodel.compile(loss=categorical_crossentropy, \n              optimizer=Adam(lr=learning_rate), \n              metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hist = model.fit_generator(Datagen(train_x, train_y, batch_size, is_train=True),\n#                             steps_per_epoch = len(train_x) // batch_size,\n#                             epochs = n_epoch,\n#                             validation_data=Datagen(valid_x, valid_y, batch_size),\n#                             validation_steps = len(valid_x) // batch_size,\n#                             callbacks = [lr_schedule, macroF1(model, valid_x, valid_y)],\n#                             shuffle = False,\n#                             verbose = 1 )\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = np.argmax((model.predict(valid_x)+model.predict(valid_x[:,::-1,:])[:,::-1,:])/2, axis=2).reshape(-1)\ngt = np.argmax(valid_y, axis=2).reshape(-1)\nprint(\"SCORE_oldmetric: \", cohen_kappa_score(gt, pred, weights=\"quadratic\"))\nprint(\"SCORE_newmetric: \", f1_score(gt, pred, average=\"macro\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = np.argmax((model.predict(test_input)+model.predict(test_input[:,::-1,:])[:,::-1,:])/2, axis=2).reshape(-1)\n\ndf_sub = pd.read_csv(\"../input/liverpool-ion-switching/sample_submission.csv\", dtype={'time':str})\ndf_sub.open_channels = np.array(np.round(pred,0), np.int)\ndf_sub.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p =model.predict(test_input)\np2 = model.predict(test_input[:,::-1,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}