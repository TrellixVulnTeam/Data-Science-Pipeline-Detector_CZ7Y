{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport os\nimport gc\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import f1_score\nfrom tqdm.notebook import tqdm\nfrom pykalman import KalmanFilter\nfrom scipy.misc import derivative\nfrom bayes_opt import BayesianOptimization\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch import optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.nn import functional as f\nfrom torch.utils.data import Dataset, DataLoader\nimport warnings\n\nsns.set_style(\"darkgrid\")\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nbase_path = '../input/clean-kalman/'\nbase_path0 = '../input/liverpool-ion-switching'\ntrain_path = os.path.join(base_path, 'train_clean_kalman.csv')\ntest_path = os.path.join(base_path, 'test_clean_kalman.csv')\nsub_path = os.path.join(base_path0, 'sample_submission.csv')\n\nfold_split = 10\nwindow_list = [20, 50]\nSAMPLE = 10000\nFOLD_GET = 5\nRANDOM_NOISE = 0.001\n\nkf = StratifiedKFold(n_splits=fold_split, random_state=37, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(train_path)\nsns.distplot(train_df['signal'].values)\nplt.show()\ntrain_df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add Noise","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['signal'] = train_df['signal'].apply(lambda x: x * np.random.uniform(1-RANDOM_NOISE, 1+RANDOM_NOISE))\nsns.distplot(train_df['signal'].values)\nplt.show()\ntrain_df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(test_path)\nsns.distplot(test_df['signal'].values)\nplt.show()\ntest_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef Kalman1D(observations,damping=1):\n    # To return the smoothed time series data\n    observation_covariance = damping\n    initial_value_guess = observations[0]\n    transition_matrix = 1\n    transition_covariance = 0.1\n    initial_value_guess\n    kf = KalmanFilter(\n            initial_state_mean=initial_value_guess,\n            initial_state_covariance=observation_covariance,\n            observation_covariance=observation_covariance,\n            transition_covariance=transition_covariance,\n            transition_matrices=transition_matrix\n        )\n    pred_state, state_cov = kf.smooth(observations)\n    return pred_state\n\nobservation_covariance = 0.0015\n\n# train_df['signal'] = Kalman1D(train_df['signal'].values,observation_covariance)\n# test_df['signal'] = Kalman1D(test_df['signal'].values,observation_covariance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize(train, test, col, type_='StandardScaler'):\n    \n    if type_ == 'StandardScaler':\n        clf = StandardScaler()\n    elif type_ == 'MinMaxScaler':\n        clf = MinMaxScaler()\n    elif type_ == 'RobustScaler':\n        clf = RobustScaler()\n    else:\n        print('Error type in!')\n    \n    train[col] = clf.fit_transform(train[col])\n    test[col] = clf.transform(test[col])\n    \n    return train, test\n\ntrain_df, test_df = normalize(train_df, test_df, ['signal'])\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyse","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"channel_signal = train_df.groupby('open_channels')['signal'].agg(['min', 'max', np.mean, np.std]).reset_index()\nchannel_signal.columns = ['signal_' + i if i!='open_channels' else i for i in channel_signal.columns]\n\nprint(channel_signal)\n\ntry:\n    del channel_signal\nexcept:\n    print('Variable channel_signal not defined!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(df, windows=window_list, batch_size=20000):    \n    \n    df = df.sort_values('time').reset_index(drop=True)\n    df.index = ((df.time*10000) - 1).values\n    df['batch'] = df.index // batch_size\n    df['batch_index'] = df.index % batch_size\n    df['batch_slices'] = df['batch_index'] // (batch_size//10)\n    df['batch_slices1'] = df.apply(lambda x: '_'.join([str(x['batch']).zfill(3), str(x['batch_slices']).zfill(3)]), axis=1)\n    \n    for c in ['batch', 'batch_slices1']:\n        d = {}\n        \n        d[f'min_{c}'] = df.groupby(c)['signal'].min()\n        d[f'max_{c}'] = df.groupby(c)['signal'].max()\n        d[f'mean_{c}'] = df.groupby(c)['signal'].mean()\n        d[f'std_{c}'] = df.groupby(c)['signal'].std()\n#         d[f'median_{c}'] = df.groupby(c)['signal'].median()\n        \n        for per in [10, 25, 75, 90]:\n            d[f'pct_{per}_{c}'] = df.groupby(c)['signal'].apply(lambda x: np.percentile(x, per))\n        \n        d[f'skew_{c}'] = df.groupby(c)['signal'].apply(lambda x: pd.Series(x).skew())\n        d[f'kurtosis_{c}'] = df.groupby(c)['signal'].apply(lambda x: pd.Series(x).kurtosis())\n        d[f'mean_abs_{c}'] = df.groupby(c)['signal'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d[f'min_abs_{c}'] =  df.groupby(c)['signal'].apply(lambda x: np.min(np.abs(x)))\n        d[f'max_abs_{c}'] =  df.groupby(c)['signal'].apply(lambda x: np.max(np.abs(x)))\n        d[f'range_{c}'] = d[f'max_{c}'] - d[f'min_{c}']\n        d[f'ratio_{c}'] = d[f'max_{c}'] / d[f'min_{c}']\n        d[f'avg_abs_{c}'] = (d[f'min_abs_{c}'] + d[f'max_abs_{c}'])/2\n        \n        for v in d:\n            df[v] = df[c].map(d[v].to_dict())\n    \n    # add shift_1\n    df['shift_+1'] = [0] + list(df['signal'].values[:-1])\n    df['shift_-1'] = list(df['signal'].values[1:]) + [0]\n    for i in df[df['batch_index']==0].index:\n        df['shift_+1'][i] = np.nan\n    for i in df[df['batch_index']==(batch_size-1)].index:\n        df['shift_-1'][i] = np.nan\n    \n    # add shift_2\n    df['shift_+2'] = [0,] + [1,] + list(df['signal'].values[:-2])\n    df['shift_-2'] = list(df['signal'].values[:-2]) + [0,] + [1,]\n    \n    for i in df[df['batch_index'].isin([0, 1])].index:\n        df['shift_+2'][i] = np.nan\n    for i in df[df['batch_index'].isin([batch_size-1, batch_size-2])].index:\n        df['shift_-2'][i] = np.nan\n        \n    df = df.drop(columns=['batch', 'batch_index', 'batch_slices', 'batch_slices1'])\n    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels']]:\n        df[f'{c}_msignal'] = df[f'{c}'] - df['signal']        \n        \n    df = df.replace([np.Inf, -np.Inf], np.nan)\n    df.fillna(0, inplace=True)\n    \n    gc.collect()\n    \n    return df\n\ntrain_df = process_data(train_df)\ntest_df = process_data(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n    if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.uint8).min and c_max < np.iinfo(np.uint8).max:\n                    df[col] = df[col].astype(np.uint8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.uint16).min and c_max < np.iinfo(np.uint16).max:\n                    df[col] = df[col].astype(np.uint16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.uint32).min and c_max < np.iinfo(np.uint32).max:\n                    df[col] = df[col].astype(np.uint32)                    \n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n                elif c_min > np.iinfo(np.uint64).min and c_max < np.iinfo(np.uint64).max:\n                    df[col] = df[col].astype(np.uint64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntrain_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_important(importances, title):\n    \n    data = pd.DataFrame({'feature': column_train, 'important': importances})\n    \n    plt.figure(figsize=(25, 25))\n    plt.title('Feature Importances')\n    sns.barplot(data=data.sort_values('important', ascending=False), x='important', y='feature')\n    plt.xlabel('Relative Importance')\n    plt.title(title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_train = [i for i in train_df.columns if i not in ['time', 'signal', 'open_channels']]\ncolumn_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_asymmetric_train(y_pred, y_true):\n    \n    y_true = y_true.get_label()\n    residual = (y_true - y_pred).astype('float')\n    grad = np.where(residual < 0, -2*1.1*residual, -2*residual)\n    hess = np.where(residual < 0, 2*1.1, 2)\n    \n    return grad, hess\n\n\n\ndef custom_asymmetric_valid(y_pred, y_true):\n    \n    y_true = y_true.get_label()\n    residual = (y_true - y_pred).astype('float')\n    loss = np.where(residual < 0, residual**2, 1.15*(residual**2))\n    \n    return 'custom_asymmetric_eval', np.mean(loss), False\n\n\n\n\ndef eval_gini(y_true, y_prob):\n    return np.sqrt(((y_prob - y_true) ** 2).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_train(model_type, params, train_df=train_df, test_df=test_df):    \n        \n    predict = np.zeros([len(test_df)])\n    importances = np.zeros([len(column_train), FOLD_GET])\n\n    for idx, (train_index, val_index) in enumerate(kf.split(train_df, train_df['open_channels'])):\n        if idx % 2 == 0:\n            print(f'Fold_{idx}:')\n            train_ = train_df.iloc[train_index]\n            val_ = train_df.iloc[val_index]\n\n            train_.reset_index(inplace=True, drop=True)\n            val_.reset_index(inplace=True, drop=True)            \n\n            if model_type=='xgb':\n                \n                train_dataset = lgb.DMatrix(train_.loc[:, column_train].values, label=train_.loc[:, 'open_channels'].values, feature_name=column_train)    \n                val_dataset = lgb.DMatrix(val_.loc[:, column_train].values, label=val_.loc[:, 'open_channels'].values, feature_name=column_train)\n                \n                clf = xgb.train(params, train_dataset, 500, valid_sets=[(val_dataset, 'valid')],\n                          verbose_eval=100, early_stopping_rounds = 100)\n                predict += clf.predict(test_df.loc[:, column_train], ntree_limit=clf.best_ntree_limit)/FOLD_GET\n                lgb.plot_importance(clf, max_num_features=10)\n                \n\n            elif model_type=='lgb':   \n                \n                train_dataset = lgb.Dataset(train_.loc[:, column_train].values, label=train_.loc[:, 'open_channels'].values, feature_name=column_train)    \n                val_dataset = lgb.Dataset(val_.loc[:, column_train].values, label=val_.loc[:, 'open_channels'].values, feature_name=column_train)\n            \n                clf = lgb.train(params, train_dataset, 4000, valid_sets=[train_dataset, val_dataset], feval=custom_asymmetric_valid,\n                                fobj=custom_asymmetric_train, verbose_eval=500, early_stopping_rounds = 400)\n                predict += clf.predict(test_df.loc[:, column_train], ntree_limit=clf.best_iteration)/FOLD_GET            \n\n            del clf\n            gc.collect()\n\n    return predict, importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_sample = train_df.sample(SAMPLE, replace=True)\n# train_sample.reset_index(inplace=True, drop=True)\n\ntrain_index, val_index = list(kf.split(train_df, train_df['open_channels']))[0]\ntrain_ = train_df.iloc[train_index]\nval_ = train_df.iloc[val_index]\n\ntrain_.reset_index(inplace=True, drop=True)\nval_.reset_index(inplace=True, drop=True)\n\n# del train_sample\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGB","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n\n# def xgb_bayesian(    \n#     max_depth,\n#     scale_pos_weight,\n#     gamma,\n#     eta,\n#     subsample,\n#     colsample_bytree,\n#     min_child_weight,\n#     max_delta_step\n# ):\n    \n#     max_depth = int(max_depth)\n#     scale_pos_weight = int(scale_pos_weight)\n#     min_child_weight = int(min_child_weight)\n\n#     assert type(max_depth) == int\n#     assert type(scale_pos_weight) == int\n#     assert type(min_child_weight) == int\n\n#     param = {\n#         'max_depth': max_depth,\n#         'gamma': gamma,\n#         'eta': eta,\n#         'objective': 'reg:squarederror',\n#         'nthread': 4,\n#         'eval_metric': 'rmse',\n#         'subsample': subsample,\n#         'colsample_bytree': colsample_bytree,\n#         'min_child_weight': min_child_weight,\n#         'max_delta_step': max_delta_step\n#     }    \n    \n    \n#     xgb_train = xgb.DMatrix(train_.loc[:, column_train].values, label=train_.loc[:, 'open_channels'].values)    \n#     xgb_valid = xgb.DMatrix(val_.loc[:, column_train].values, label=val_.loc[:, 'open_channels'].values)   \n\n#     clf = xgb.train(param, dtrain=xgb_train, num_boost_round=10, evals=[(xgb_valid, 'valid')],\n#                     verbose_eval=10, early_stopping_rounds = 400)\n\n#     predictions = clf.predict(val_.loc[:, column_train].values)[0]       \n#     score = eval_gini(val_.loc[:, \"open_channels\"].values, predictions)\n    \n#     return 1-score\n\n\n# bounds_xgb = {\n#     'gamma': (0.001, 10),\n#     'eta': (0.01, 0.2),\n#     'max_depth':(3,15),\n#     'subsample': (0, 0.5),\n#     'colsample_bytree': (0, 0.5),\n#     'min_child_weight': (0, 20),\n#     'max_delta_step': (0, 10),\n#     'scale_pos_weight': (1, 10)\n# }\n\n\n# xgb_bo = BayesianOptimization(xgb_bayesian, bounds_xgb, random_state=37)\n# print(xgb_bo.space.keys)\n\n# xgb_bo.maximize(init_points=5, n_iter=5, acq='ucb', xi=0.0, alpha=1e-6)\n# xgb_bo.max['params']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# base_param = {\n#         'max_depth': int(xgb_bo.max['params']['max_depth'])\n#         'gamma': xgb_bo.max['params']['gamma'],\n#         'eta': xgb_bo.max['params']['eta'],\n#         'objective': 'reg:squarederror',\n#         'nthread': 4,\n#         'eval_metric': 'rmse',\n#         'subsample': xgb_bo.max['params']['subsample'],\n#         'colsample_bytree': xgb_bo.max['params']['colsample_bytree'],\n#         'min_child_weight': int(xgb_bo.max['params']['min_child_weight']),\n#         'max_delta_step': int(xgb_bo.max['params']['max_depth'])\n#     }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n\n# pred_xgb, importances_xgb = model_train('xgb')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LBG","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef lgb_bayesian(\n    num_leaves,\n    min_data_in_leaf,\n    learning_rate,\n    min_sum_hessian_in_leaf,\n    feature_fraction,\n    lambda_l1,\n    lambda_l2,\n    min_gain_to_split,\n    max_depth,\n    scale_pos_weight):\n    \n    num_leaves = int(num_leaves)\n    min_data_in_leaf = int(min_data_in_leaf)\n    max_depth = int(max_depth)\n    scale_pos_weight = int(scale_pos_weight)\n\n    assert type(num_leaves) == int\n    assert type(min_data_in_leaf) == int\n    assert type(max_depth) == int\n    assert type(scale_pos_weight) == int\n\n    param = {\n        'num_leaves': num_leaves,\n        'max_bin': 63,\n        'min_data_in_leaf': min_data_in_leaf,\n        'learning_rate': learning_rate,\n        'min_sum_hessian_in_leaf': min_sum_hessian_in_leaf,\n        'bagging_fraction': 1.0,\n        'bagging_freq': 5,\n        'feature_fraction': feature_fraction,\n        'lambda_l1': lambda_l1,\n        'lambda_l2': lambda_l2,\n        'min_gain_to_split': min_gain_to_split,\n        'max_depth': max_depth,\n        'scale_pos_weight': scale_pos_weight,\n        'save_binary': True, \n        'objective': 'regression',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'mae',\n        'boost_from_average': False,\n    }    \n    \n    \n    lgb_train = lgb.Dataset(train_.loc[:, column_train].values, label=train_.loc[:, 'open_channels'].values)    \n    lgb_valid = lgb.Dataset(val_.loc[:, column_train].values, label=val_.loc[:, 'open_channels'].values)   \n\n    clf = lgb.train(param, lgb_train, 1000, valid_sets=[lgb_valid],feval=custom_asymmetric_valid,\n                    fobj=custom_asymmetric_train, verbose_eval=250, early_stopping_rounds = 400)\n    \n    predictions = clf.predict(val_.loc[:, column_train].values, num_iteration=clf.best_iteration)       \n    score = eval_gini(val_.loc[:, \"open_channels\"].values, predictions)\n    \n    return 1-score\n\n\nbounds_lgb = {\n    'num_leaves': (5, 20), \n    'min_data_in_leaf': (5, 20),  \n    'learning_rate': (0.01, 0.2),\n    'min_sum_hessian_in_leaf': (0.00001, 0.01),    \n    'feature_fraction': (0.05, 0.5),\n    'lambda_l1': (0, 5.0), \n    'lambda_l2': (0, 5.0), \n    'min_gain_to_split': (0, 1.0),\n    'max_depth':(3,15),\n    'scale_pos_weight': (1, 10)\n}\n\n\nlgb_bo = BayesianOptimization(lgb_bayesian, bounds_lgb, random_state=37)\nprint(lgb_bo.space.keys)\n\nlgb_bo.maximize(init_points=5, n_iter=5, acq='ucb', xi=0.0, alpha=1e-6)\nlgb_bo.max['params']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_param = {\n        'num_leaves': int(lgb_bo.max['params']['num_leaves']),\n        'max_bin': 63,\n        'min_data_in_leaf': int(lgb_bo.max['params']['min_data_in_leaf']),\n        'learning_rate': lgb_bo.max['params']['learning_rate'],\n        'min_sum_hessian_in_leaf': lgb_bo.max['params']['min_sum_hessian_in_leaf'],\n        'bagging_fraction': 1.0,\n        'bagging_freq': 5,\n        'feature_fraction': lgb_bo.max['params']['feature_fraction'],\n        'lambda_l1': lgb_bo.max['params']['lambda_l1'],\n        'lambda_l2': lgb_bo.max['params']['lambda_l2'],\n        'min_gain_to_split': lgb_bo.max['params']['min_gain_to_split'],\n        'max_depth': int(lgb_bo.max['params']['max_depth']),\n        'scale_pos_weight': int(lgb_bo.max['params']['scale_pos_weight']),\n        'save_binary': True, \n        'objective': 'regression',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'mae',\n        'boost_from_average': False,\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\npred_lgb, importances_lgb = model_train('lgb', base_param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df, test_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pred_proc(pred):\n    pred = np.round(np.clip(pred, 0, 10))\n    return pred.astype(int)\n\n# pred = 0.5*pred_xgb + 0.5*pred_lgb\npred = pred_proc(pred_lgb)\nset(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(sub_path)\n\nsubmission['open_channels'] = pred\nsubmission.to_csv(\"submission.csv\", index=False, float_format='%.4f')\n\nsubmission.tail()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}