{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport gc\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n\n# Any results you write to the current directory are saved as output.\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler\n\nfrom tqdm import tqdm\n\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.utils import class_weight\n\nfrom sklearn.metrics import accuracy_score, make_scorer\nfrom sklearn.metrics import roc_curve, auc, accuracy_score, cohen_kappa_score\nfrom sklearn.metrics import mean_squared_error, f1_score, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Introduction","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"Identify the number of channels open at each time point\n\nMany diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction. If scientists could better study ion channels, which may be possible with the aid of machine learning, it could have a far-reaching impact.\n\nWhen ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research. Scientists hope that technology could enable rapid automatic detection of ion channel current events in raw data.\n\nThe University of Liverpool’s Institute of Ageing and Chronic Disease is working to advance ion channel research. Their team of scientists have asked for your help. In this competition, you’ll use ion channel data to better model automatic identification methods. If successful, you’ll be able to detect individual ion channel events in noisy raw signals. The data is simulated and injected with real world noise to emulate what scientists observe in laboratory experiments.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Acknowledgements\n\n### This kernel used ideas and some code from excellent notebook:\n### https://www.kaggle.com/vbmokin/ion-switching-advanced-fe-lgb-xgb-confmatrix\n### and from this discussion: \n### https://www.kaggle.com/c/liverpool-ion-switching/discussion/143390\n\n\n### data from\n### https://www.kaggle.com/cdeotte/data-without-drift","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Utils","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        if col != 'time':\n            col_type = df[col].dtypes\n            if col_type in numerics:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndef get_stats(df):\n    stats = pd.DataFrame(index=df.columns, columns=['na_count', 'n_unique', 'type', 'memory_usage'])\n    for col in df.columns:\n        stats.loc[col] = [df[col].isna().sum(), df[col].nunique(dropna=False), df[col].dtypes, df[col].memory_usage(deep=True, index=False) / 1024**2]\n    stats.loc['Overall'] = [stats['na_count'].sum(), stats['n_unique'].sum(), None, df.memory_usage(deep=True).sum() / 1024**2]\n    return stats\n\ndef print_header():\n    print('col         conversion        dtype    na    uniq  size')\n    print()\n    \ndef print_values(name, conversion, col):\n    template = '{:10}  {:16}  {:>7}  {:2}  {:6}  {:1.2f}MB'\n    print(template.format(name, conversion, str(col.dtypes), col.isna().sum(), col.nunique(dropna=False), col.memory_usage(deep=True, index=False) / 1024 ** 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def display_set(df, column, n_sample, figsize ):\n    f, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize = figsize )\n    sns.lineplot(x= df.index[::n_sample], y = df[column][::n_sample], ax=ax1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Showing Confusion Matrix\n# Thanks to https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud\ndef plot_cm(y_true, y_pred, title):\n    figsize=(14,14)\n    y_pred = y_pred.astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_class_weight(classes, exp=1):\n    '''\n    Weight of the class is inversely proportional to the population of the class.\n    There is an exponent for adding more weight.\n    '''\n    hist, _ = np.histogram(classes, bins=np.arange(12)-0.5)\n    class_weight = hist.sum()/np.power(hist, exp)\n    \n    return class_weight","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load train and test datasets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**IMPORTANT: While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '/kaggle/input/data-without-drift/'\n#PATH = '/kaggle/input/liverpool-ion-switching/'\n\ntrain = pd.read_csv(PATH + 'train_clean.csv')\ntest = pd.read_csv(PATH + 'test_clean.csv')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Most of the FE have been taken from: \n## - https://www.kaggle.com/vbmokin/ion-switching-advanced-fe-lgb-xgb-confmatrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"WINDOW_SIZES = [3, 5, 10, 50, 100, 500]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"%%time\n\ndef gen_roll_features(full, win_sizes = WINDOW_SIZES):\n    for window in tqdm(win_sizes):\n        full[\"rolling_mean_\" + str(window)] = full['signal'].rolling(window=window).mean()\n        full[\"rolling_std_\" + str(window)] = full['signal'].rolling(window=window).std()\n        full[\"rolling_var_\" + str(window)] = full['signal'].rolling(window=window).var()\n        full[\"rolling_min_\" + str(window)] = full['signal'].rolling(window=window).min()\n        full[\"rolling_max_\" + str(window)] = full['signal'].rolling(window=window).max()\n\n        a = (full['signal'] - full['rolling_min_' + str(window)]) / (full['rolling_max_' + str(window)] - full['rolling_min_' + str(window)])\n        full[\"norm_\" + str(window)] = a * (np.floor(full['rolling_max_' + str(window)]) - np.ceil(full['rolling_min_' + str(window)]))\n    \n    full = full.replace([np.inf, -np.inf], np.nan)    \n    full.fillna(0, inplace=True)\n    return full\n\ntrain = gen_roll_features(train)\ntest = gen_roll_features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nGROUP_BATCH_SIZE = 8000\n\n# create batches of GROUP_BATCH_SIZE observations\ndef batching(df, batch_size, gr_name='group'):\n    df[gr_name] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n    df[gr_name] = df[gr_name].astype(np.uint16)\n    return df\n\ndef run_feat_engineering(df, batch_size, gr_name='group'):\n    df = batching(df, batch_size = batch_size, gr_name=gr_name)\n    df['signal_2'] = df['signal'] ** 2\n    df['signal_2-7500-mean'] = df['signal_2'] - df['signal_2'].rolling(window=7500).mean()    \n    return df\n\ntrain = run_feat_engineering(train, batch_size = GROUP_BATCH_SIZE, gr_name='group')\ntest = run_feat_engineering(test, batch_size = GROUP_BATCH_SIZE, gr_name='group')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n## add some noise\n\nSTD = 0.01\n\nold_data = train['signal']\nnew_data = old_data + np.random.normal(0,STD,size=len(train)) \ntrain['signal'] = new_data\n\nold_data = test['signal']\nnew_data = old_data + np.random.normal(0,STD,size=len(test)) \ntest['signal'] = new_data\n\ndel old_data, new_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef gen_roll_features(full, win_sizes = WINDOW_SIZES):\n    for window in tqdm(win_sizes):\n        full[\"rolling_mean_\" + str(window)] = full['signal'].rolling(window=window).mean()\n        full[\"rolling_std_\" + str(window)] = full['signal'].rolling(window=window).std()\n        full[\"rolling_var_\" + str(window)] = full['signal'].rolling(window=window).var()\n        full[\"rolling_min_\" + str(window)] = full['signal'].rolling(window=window).min()\n        full[\"rolling_max_\" + str(window)] = full['signal'].rolling(window=window).max()\n\n        a = (full['signal'] - full['rolling_min_' + str(window)]) / (full['rolling_max_' + str(window)] - full['rolling_min_' + str(window)])\n        full[\"norm_\" + str(window)] = a * (np.floor(full['rolling_max_' + str(window)]) - np.ceil(full['rolling_min_' + str(window)]))\n    return full\n\ntrain = gen_roll_features(train)\ntest = gen_roll_features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef gen_sig_features(df):\n    df = df.sort_values(by=['time']).reset_index(drop=True)\n    df.index = ((df.time * 10_000) - 1).values\n    df['batch'] = df.index // 25_000\n    df['batch_index'] = df.index  - (df.batch * 25_000)\n    df['batch_slices'] = df['batch_index']  // 2500\n    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n    \n    for c in tqdm(['batch','batch_slices2']):\n        d = {}\n        d['mean'+c] = df.groupby([c])['signal'].mean()\n        d['median'+c] = df.groupby([c])['signal'].median()\n        d['max'+c] = df.groupby([c])['signal'].max()\n        d['min'+c] = df.groupby([c])['signal'].min()\n        d['std'+c] = df.groupby([c])['signal'].std()\n        d['mean_abs_chg'+c] = df.groupby([c])['signal'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = df.groupby([c])['signal'].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = df.groupby([c])['signal'].apply(lambda x: np.min(np.abs(x)))\n        d['range'+c] = d['max'+c] - d['min'+c]\n        d['maxtomin'+c] = d['max'+c] / d['min'+c]\n        d['abs_avg'+c] = (d['abs_min'+c] + d['abs_max'+c]) / 2\n        for v in d:\n            df[v] = df[c].map(d[v].to_dict())\n    df = reduce_mem_usage(df)\n    gc.collect()\n    return df\n\ntrain = gen_sig_features(train)\ntest = gen_sig_features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef gen_shift_features(df):\n    # add shifts\n    df['signal_shift_+1'] = [0,] + list(df['signal'].values[:-1])\n    df['signal_shift_-1'] = list(df['signal'].values[1:]) + [0]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+1'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-1'][i] = np.nan\n    \n    df['signal_shift_+2'] = [0,] + [1,] + list(df['signal'].values[:-2])\n    df['signal_shift_-2'] = list(df['signal'].values[2:]) + [0] + [1]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==1].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-2'][i] = np.nan\n    for i in df[df['batch_index']==49998].index:\n        df['signal_shift_-2'][i] = np.nan\n    \n    df.drop(columns=['batch', 'batch_index', 'batch_slices', 'batch_slices2'], inplace=True)\n    gc.collect()\n\n    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels', 'group' 'category', 'index']]:\n        df[c+'_msignal'] = df[c] - df['signal']\n        \n    df = df.replace([np.inf, -np.inf], np.nan)    \n    df.fillna(0, inplace=True)\n    df = reduce_mem_usage(df)\n    gc.collect()\n    return df\n\ntrain = gen_shift_features(train)\ntest = gen_shift_features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ALL_FEATURES = [c for c in train.columns if c not in ['time', 'signal', 'open_channels', 'group' 'category', 'index']]\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display train and test signals","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"DATA_BATCH_SIZE = 500000\n\nTRAIN_SAMPLE_RATE = 100 ## for display\nTRAIN_BATCH_SIZE = int(len(train)/TRAIN_SAMPLE_RATE)\n\nf, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize = (20,4))\nsns.lineplot(data=train.signal[::TRAIN_SAMPLE_RATE], ax=ax1, hue=\"size\", size=\"size\")\nsns.lineplot(data=train.open_channels[::TRAIN_SAMPLE_RATE], ax=ax1, hue=\"size\", size=\"size\")\nax1.set_title(f'Full train signal')\n\nf, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize = (10,4))\nsns.lineplot(data=test.signal[::TRAIN_SAMPLE_RATE], ax=ax1, hue=\"size\", size=\"size\")\nax1.set_title(f'Full test signal')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## By batch","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f, axes = plt.subplots(nrows = 2, ncols = 5, figsize = (26,12))\nfor i in range(10):\n    XX = train.signal[i*DATA_BATCH_SIZE:(i+1)*DATA_BATCH_SIZE + 1]\n    yy = train.open_channels[i*DATA_BATCH_SIZE:(i+1)*DATA_BATCH_SIZE + 1]\n    sns.scatterplot(data=XX[::TRAIN_SAMPLE_RATE], ax=axes[int(i/5), i%5], hue=\"size\", size=\"size\")\n    sns.scatterplot(data=yy[::TRAIN_SAMPLE_RATE], ax=axes[int(i/5), i%5], hue=\"size\", size=\"size\")\n    axes[int(i/5), i%5].set_title(f'Train Batch# {i+1}')\n    \nf, axes = plt.subplots(nrows = 1, ncols = 5, figsize = (26,6))\nfor i in range(4):\n    XX = test.signal[i*DATA_BATCH_SIZE:(i+1)*DATA_BATCH_SIZE + 1]\n    sns.scatterplot(data=XX[::TRAIN_SAMPLE_RATE], ax=axes[i], hue=\"size\", size=\"size\")\n    axes[i].set_title(f'Test Batch# {i+1}')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Open channels value count","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 6))\nsns.countplot(x=\"open_channels\", data=train, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distributions for open channels for each batch","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f, axes = plt.subplots(nrows = 2, ncols = 5, figsize = (26,12))\nfor i in range(10):\n    y = pd.DataFrame()\n    sns.countplot( x = train.open_channels[i*DATA_BATCH_SIZE:(i+1)*DATA_BATCH_SIZE + 1], ax=axes[int(i/5), i%5])\n    axes[int(i/5), i%5].set_title(f'Train Batch# {i+1}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature selection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Original sizes: train: {train.shape}, test: {test.shape}' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### remove high corr values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n## this may take a lot of time\ncorr = train[ALL_FEATURES][::20].corr('spearman')\n\ncolumns = np.full((corr.shape[0],), True, dtype=bool)\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= 0.9:\n            if columns[j]:\n                columns[j] = False\n                \nselected_columns = train[ALL_FEATURES].columns[columns]\nprint(len(selected_columns))            \n#print(selected_columns)            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### remove all low-variance features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom sklearn.feature_selection import VarianceThreshold\n\nvt = VarianceThreshold(0.5)\ny = train['open_channels'].values\nX = train[selected_columns].values\nvt.fit(X, y)\n\n## let's take top 25\ntop_idx = np.argpartition(vt.variances_, -25)[-25:]\nSELECTED_FEATURES = [selected_columns[i] for i in top_idx]\nprint(SELECTED_FEATURES)     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LGB Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## reduce amount of data to speed things up\nX_train = train[SELECTED_FEATURES]\ny_train = train['open_channels'].values\n\nprint(f'Original sizes: train: {train.shape}, test: {test.shape}' )\nprint(f'Reduced train sizes: X_train: {X_train.shape}, y_train: {y_train.shape}' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/siavrez/simple-eda-model\ndef MacroF1Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = f1_score(labels, preds, average = 'macro')\n    return ('MacroF1Metric', score, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## started from here:\n## https://www.kaggle.com/vbmokin/ion-switching-advanced-fe-lgb-xgb-confmatrix\n\nNUM_BOOST_ROUND = 2000 \nEARLY_STOPPING_ROUNDS = 40\nVERBOSE_EVAL = 100\nRANDOM_SEED = 13\nLEARNING_RATE = 0.02\nMAX_DEPTH = -1\nNUM_LEAVES = 200\n\n\nX_train1, X_valid1, y_train1, y_valid1 = train_test_split(X_train, y_train, test_size=0.3, random_state=RANDOM_SEED)\n\nparams = {\n    'learning_rate': LEARNING_RATE, \n    'max_depth': MAX_DEPTH, \n    'num_leaves': NUM_LEAVES,\n    'metric': 'logloss', \n    'random_state': RANDOM_SEED, \n    'n_jobs':-1, \n    'sample_fraction':0.33\n    }\n\nevals_result = {}\nmodel = lgb.train(\n    params, \n    train_set=lgb.Dataset(X_train1, y_train1), \n    num_boost_round = NUM_BOOST_ROUND,\n    valid_sets = lgb.Dataset(X_valid1, y_valid1), \n    verbose_eval = VERBOSE_EVAL,\n    evals_result = evals_result,\n    early_stopping_rounds = EARLY_STOPPING_ROUNDS, \n    feval = MacroF1Metric)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize=(15, 6))\nlgb.plot_metric(evals_result, metric='MacroF1Metric', ax=ax1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LGB feature importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgb.plot_importance(model,ax = axes,height = 0.5)\nplt.show();plt.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LGB Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train_lgb = model.predict(X_train, num_iteration=model.best_iteration)\nprint('LGB score {0:.4f}'.format(np.mean(f1_score(y_train, np.round(np.clip(y_pred_train_lgb,0,10)).astype(int), average=\"macro\"))))\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cm(y_train, y_pred_train_lgb, 'LGB Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat = model.predict(test[SELECTED_FEATURES], num_iteration=model.best_iteration)\ny_pred = np.round(np.clip(y_hat,0,10)).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot test signal with predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize = (15,6))\nsns.scatterplot(x=test[SELECTED_FEATURES].index[::1000], y=test.signal[::1000], ax=ax1)\nsns.scatterplot(x=test[SELECTED_FEATURES].index[::1000], y=y_pred[::1000], ax=ax1 )\nax1.set_title(f'Full test signal with predictions')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/liverpool-ion-switching/sample_submission.csv')\nsub['open_channels'] = y_pred\nsub.to_csv('submission.csv', index=False, float_format='%.4f')\n\nsub.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}