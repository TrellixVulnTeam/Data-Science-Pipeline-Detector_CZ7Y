{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Import packages","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.manifold import TSNE\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\n!pip install tensorflow_addons==0.9.1\nfrom tensorflow.keras.callbacks import Callback, LearningRateScheduler\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import losses, models, optimizers\nimport tensorflow_addons as tfa\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import GroupKFold\nimport gc\nimport warnings\nimport os\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Check data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This time, I refered this way how to remove drift below URL.\n\nhttps://www.kaggle.com/c/liverpool-ion-switching/discussion/137537\n\nhttps://www.kaggle.com/cdeotte/data-without-drift","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"original_train = pd.read_csv('/kaggle/input/liverpool-ion-switching/train.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\nprint(\"train_data_shape : {}\".format(original_train.shape))\n\nplt.figure(figsize=(15,5), dpi=100)\nplt.title(\"train_with_drift\")\nplt.plot(original_train[\"time\"],original_train[\"signal\"])\nplt.ylabel(\"signal\")\nplt.xlabel(\"time\")\nplt.show()\n\n\ntrain_clean = pd.read_csv('/kaggle/input/data-without-drift/train_clean.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\nplt.figure(figsize=(15,5), dpi=100)\nplt.title(\"train_without_drift\")\nplt.plot(train_clean[\"time\"],train_clean[\"signal\"])\nplt.ylabel(\"signal\")\nplt.xlabel(\"time\")\nplt.show()\n\noriginal_test = pd.read_csv('/kaggle/input/liverpool-ion-switching/test.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\nprint(\"test_data_shape : {}\".format(original_test.shape))\n\nplt.figure(figsize=(15,5), dpi=100)\nplt.title(\"test_with_drift\")\nplt.plot(original_test[\"time\"],original_test[\"signal\"])\nplt.ylabel(\"signal\")\nplt.xlabel(\"time\")\nplt.show()\n\n\ntest_clean = pd.read_csv('/kaggle/input/data-without-drift/test_clean.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\nplt.figure(figsize=(15,5), dpi=100)\nplt.title(\"test_without_drift\")\nplt.plot(test_clean[\"time\"],test_clean[\"signal\"])\nplt.ylabel(\"signal\")\nplt.xlabel(\"time\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I tried three EDA processes !\n\nprocessA : remove unnecessary noize between 40s to 80s  \nprocessB : adjust signal range  \nprocessC : remove ghost drift","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"●PROCESS A","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# ①変なnoiseはsiganlが0以上なのにopen_channelsが0でおかしいので改ざん\nplt.figure(figsize=(15,5), dpi=100)\nplt.title(\"check_train_anomary_signals\")\nfo = train_clean[train_clean[\"time\"]<100]\nplt.plot(fo[\"time\"],fo[\"signal\"])\nplt.ylabel(\"signal\")\nplt.xlabel(\"time\")\nplt.show()\n\nfig = plt.figure(figsize=(10,4), dpi=72)\ntr_1 = fo[fo[\"open_channels\"]==0]\nax1 = fig.add_subplot(1,2,1)\nax1.plot(tr_1[\"time\"],tr_1[\"signal\"],color=\"blue\")\nax1.set_yticks(np.linspace(-5,5,5))\nax1.grid(axis = \"x\", color = \"black\",linestyle = \"--\", linewidth = 0.5)\nax1.grid(axis = \"y\", color = \"black\",linestyle = \"--\", linewidth = 0.5)\nax1.set_title(\"open_channel_0 (train_clean)\", fontsize = 16)\n\ntr_2 = fo[fo[\"open_channels\"]==1]\nax2 = fig.add_subplot(1,2,2)\nax2.plot(tr_2[\"time\"],tr_2[\"signal\"],color=\"red\")\nax2.set_yticks(np.linspace(-5,5,5))\nax2.grid(axis = \"x\", color = \"black\",linestyle = \"--\", linewidth = 0.5)\nax2.grid(axis = \"y\", color = \"black\",linestyle = \"--\", linewidth = 0.5)\nax2.set_title(\"open_channel_1 (train_clean)\", fontsize = 16)\nfig.tight_layout()\nfig.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [478587, 478609, 478610, 599999]:\n    train_clean.at[i,\"signal\"]=-2.7258763313293457\n    \nplt.figure(figsize=(15,5), dpi=100)\nplt.title(\"process_A_done\")\nplt.plot(train_clean[\"time\"],train_clean[\"signal\"])\nplt.ylabel(\"signal\")\nplt.xlabel(\"time\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"●PROCESS B","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fo = train_clean[(train_clean[\"time\"]>=200)&(train_clean[\"time\"]<300)]\ncolor_array = [\"blue\",\"green\",\"red\",\"orangered\",'sienna',\"cyan\",\"magenta\",\"yellow\",\"black\",\"gray\",\"lawngreen\"]\nfor i in range(0,11):\n    plt.scatter(fo[fo[\"open_channels\"]==i][\"time\"],fo[fo[\"open_channels\"]==i][\"signal\"],color=color_array[i],label=\"group_{}\".format(str(i)))\nplt.ylabel(\"signal\")\nplt.xlabel(\"time\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0, fontsize=10)\nplt.show()\n\nfo_normal = train_clean[(train_clean[\"time\"]>=250)&(train_clean[\"time\"]<300)]\nfo_abnormal = train_clean[(train_clean[\"time\"]>=200)&(train_clean[\"time\"]<250)]\nrange_name = [i for i in range(11)]\nrange_box_normal = []\nrange_box_abnormal = []\nfor i in range(11):\n    ma = round(np.max(fo_normal[fo_normal[\"open_channels\"]==i][\"signal\"]),3)\n    mi = round(np.min(fo_normal[fo_normal[\"open_channels\"]==i][\"signal\"]),3)\n    range_box_normal.append(str(mi)+\" ~ \"+str(ma))\n    \n    ma = round(np.max(fo_abnormal[fo_abnormal[\"open_channels\"]==i][\"signal\"]),3)\n    mi = round(np.min(fo_abnormal[fo_abnormal[\"open_channels\"]==i][\"signal\"]),3)\n    range_box_abnormal.append(str(mi)+\" ~ \"+str(ma))\n\nrange_df= pd.DataFrame()\nrange_df[\"Open_Channels\"] = range_name\nrange_df[\"Signal_Range_Normal\"] = range_box_normal\nrange_df[\"Signal_Range_Abnormal\"] = range_box_abnormal\ndisplay(range_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inde = train_clean[(train_clean[\"time\"]<=250)&(train_clean[\"time\"]>200)].index\ntrain_clean.loc[inde,\"signal\"] = train_clean.loc[inde,\"signal\"] + 2.7226785\n\ninde = train_clean[(train_clean[\"time\"]<=500)&(train_clean[\"time\"]>450)].index\ntrain_clean.loc[inde,\"signal\"] = train_clean.loc[inde,\"signal\"] + 2.7226785\n\ninde = test_clean[(test_clean[\"time\"]<=560)&(test_clean[\"time\"]>550)].index\ntest_clean.loc[inde,\"signal\"] = test_clean.loc[inde,\"signal\"] + 2.7226785\n\ninde = test_clean[(test_clean[\"time\"]<=580)&(test_clean[\"time\"]>570)].index\ntest_clean.loc[inde,\"signal\"] = test_clean.loc[inde,\"signal\"] + 2.7226785\n\nplt.figure(figsize=(15,5), dpi=100)\nplt.title(\"process_B_done\")\nplt.plot(train_clean[\"time\"],train_clean[\"signal\"])\nplt.ylabel(\"signal\")\nplt.xlabel(\"time\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool !","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"●PROCESS C\n\nI refered how to remove ghost drift below URL.  \nhttps://www.kaggle.com/fkubota/clean-batch7-signal","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_signal_mod(train):\n    left = 3641000\n    right = 3829000\n    thresh_dict = {\n        3: [0.1, 2.0],\n        2: [-1.1, 0.7],\n        1: [-2.3, -0.6],\n        0: [-3.8, -2],\n    }\n    \n    train['signal_mod'] = train['signal'].values\n    for ch in train[train['batch']==7]['open_channels'].unique():\n        idxs_noisy = (train['open_channels']==ch) & (left<train.index) & (train.index<right)\n        idxs_not_noisy = (train['open_channels']==ch) & ~idxs_noisy\n        mean = train[idxs_not_noisy]['signal'].mean()\n\n        idxs_outlier = idxs_noisy & (thresh_dict[ch][1]<train['signal'].values)\n        train['signal_mod'][idxs_outlier]  = mean\n        idxs_outlier = idxs_noisy & (train['signal'].values<thresh_dict[ch][0])\n        train['signal_mod'][idxs_outlier]  = mean\n    train[\"signal\"] = train[\"signal_mod\"]\n    train = train.drop([\"signal_mod\",\"batch\"],axis=1)\n    return train\n\nbatch_list = []\nfor n in range(10):\n    batchs = np.ones(500000)*n\n    batch_list.append(batchs.astype(int))\nbatch_list = np.hstack(batch_list)\ntrain_clean['batch'] = batch_list\ntrain_clean = create_signal_mod(train_clean)\n\nplt.figure(figsize=(15,5), dpi=100)\nplt.title(\"process_C_done\")\nplt.plot(train_clean[\"time\"],train_clean[\"signal\"])\nplt.ylabel(\"signal\")\nplt.xlabel(\"time\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fantastic !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df: pd.DataFrame,\n                     verbose: bool = True) -> pd.DataFrame:\n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n\n                if (c_min > np.iinfo(np.int32).min\n                      and c_max < np.iinfo(np.int32).max):\n                    df[col] = df[col].astype(np.int32)\n                elif (c_min > np.iinfo(np.int64).min\n                      and c_max < np.iinfo(np.int64).max):\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (c_min > np.finfo(np.float16).min\n                        and c_max < np.finfo(np.float16).max):\n                    df[col] = df[col].astype(np.float16)\n                elif (c_min > np.finfo(np.float32).min\n                      and c_max < np.finfo(np.float32).max):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    reduction = (start_mem - end_mem) / start_mem\n\n    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n    if verbose:\n        print(msg)\n\n    return df\n\ntrain_clean = reduce_mem_usage(train_clean)\ntest_clean = reduce_mem_usage(test_clean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Check data distribution","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It is important to check train and test distribution.  \nI use tSNE to compare with them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_make(train,test):\n    split_batch = 4000\n    train['group'] = train.groupby(train.index//split_batch, sort=False)['signal'].agg(['ngroup']).values\n    train['group'] = train['group'].astype(np.uint16)\n    test['group'] = test.groupby(test.index//split_batch, sort=False)['signal'].agg(['ngroup']).values\n    test['group'] = test['group'].astype(np.uint16)\n\n    train_data = pd.DataFrame()\n    \n    tr_num = len(train[\"group\"].value_counts())\n    for gro in range(0,tr_num):\n        try:\n            name = str(gro)\n            train_data[name] = train[train.group==gro][\"signal\"].values\n        except:\n            pass\n\n    train_data = train_data.T\n    train_data.columns = [\"time_\"+str(i) for i in range(0,split_batch)]\n\n    te_num = len(test[\"group\"].value_counts())\n    test_data = pd.DataFrame()\n    for gro in range(0,te_num):\n        name = str(gro)\n        test_data[name] = test[test.group==gro][\"signal\"].values\n\n    test_data = test_data.T \n    test_data.columns = [\"time_\"+str(i) for i in range(0,split_batch)]\n    \n    train_data[\"index\"] = train_data.index\n    test_data[\"index\"] = test_data.index\n    return train_data,test_data\n\n# グラフの可視化\ndef make_graph(data,data_length,boolen):\n    tr_1 = data[:data_length, 0]\n    tr_2 = data[:data_length, 1]\n    te_1 = data[data_length:, 0]\n    te_2 = data[data_length:, 1]\n    if boolen:\n        fig = plt.figure(figsize=(10,4), dpi=72)\n        ax1 = fig.add_subplot(1,2,1)\n        #ax2 = fig.add_subplot(1,2,3)\n\n        ax1.scatter(tr_1,tr_2,color=\"blue\",label=\"train\")\n        ax1.set_xticks(np.linspace(min(tr_1)*1.3,max(tr_1)*1.3, 5))\n        ax1.set_yticks(np.linspace(min(tr_2)*1.3,max(tr_2)*1.3, 5))\n        ax1.grid(axis = \"x\", color = \"black\",linestyle = \"--\", linewidth = 0.5)\n        ax1.grid(axis = \"y\", color = \"black\",linestyle = \"--\", linewidth = 0.5)\n        ax1.set_title(\"train_distribution\", fontsize = 16)\n\n    ax2 = fig.add_subplot(1,2,2)\n    ax2.scatter(te_1,te_2,color=\"red\",label=\"test\")\n    ax2.set_xticks(np.linspace(min(tr_1)*1.3,max(tr_1)*1.3, 5))\n    ax2.set_yticks(np.linspace(min(tr_2)*1.3,max(tr_2)*1.3, 5))\n    ax2.grid(axis = \"x\", color = \"black\",linestyle = \"--\", linewidth = 0.5)\n    ax2.grid(axis = \"y\", color = \"black\",linestyle = \"--\", linewidth = 0.5)\n    ax2.set_title(\"test_distribution\", fontsize = 16)\n    fig.tight_layout()\n    fig.legend()\n    plt.show()\n\n    fig = plt.figure(figsize=(5.1,4), dpi=72)\n    ax3 = fig.add_subplot(1,1,1)\n    ax3.scatter(tr_1,tr_2,color=\"blue\",label=\"train\")\n    ax3.scatter(te_1,te_2,color=\"red\",label=\"test\")\n    ax3.set_xticks(np.linspace(min(tr_1)*1.3,max(tr_1)*1.3, 5))\n    ax3.set_yticks(np.linspace(min(tr_2)*1.3,max(tr_2)*1.3, 5))\n    ax3.grid(axis = \"x\", color = \"black\",linestyle = \"--\", linewidth = 0.5)\n    ax3.grid(axis = \"y\", color = \"black\",linestyle = \"--\", linewidth = 0.5)\n    ax3.set_title(\"train_test_distribution\", fontsize = 16)\n    fig.tight_layout()\n    fig.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tsne,test_tsne = data_make(train_clean,test_clean)\ndata = pd.concat([train_tsne,test_tsne],axis=0).drop(\"index\",axis=1)\ntsne_data = TSNE(n_components=2,perplexity=30,learning_rate=300,random_state=1003).fit_transform(data)\nmake_graph(tsne_data,train_tsne.shape[0],True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great Fitting!!!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4. Feature engeneering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_feature(train,test):\n    #4000でデータをバッチ化する\n    train['group'] = train.groupby(train.index//4000, sort=False)['signal'].agg(['ngroup']).values\n    train['group'] = train['group'].astype(np.uint16)\n    test['group'] = test.groupby(test.index//4000, sort=False)['signal'].agg(['ngroup']).values\n    test['group'] = test['group'].astype(np.uint16)\n    \n    train_proba = pd.read_csv('/kaggle/input/cleandataajustedproba/train_proba.csv')\n    test_proba = pd.read_csv('/kaggle/input/cleandataajustedproba/test_proba.csv')\n\n    train = pd.concat([train, train_proba], axis=1)\n    test = pd.concat([test, test_proba], axis=1)\n    \n    #二乗要素追加\n    #※open_channelsが0の時、signalが代替0になるように調節する\n    train['signal**2'] = (train['signal']+2.694801092147827) ** 2\n    test['signal**2'] = (test['signal']+2.694801092147827) ** 2\n\n    #正規化\n    mi = np.min(pd.concat([train[\"signal\"],test[\"signal\"]],axis=0))\n    ma = np.max(pd.concat([train[\"signal\"],test[\"signal\"]],axis=0))\n    train[\"signal\"] = (train[\"signal\"]-mi)/(ma - mi)\n    test[\"signal\"] = (test[\"signal\"]-mi)/(ma - mi)\n    mi = np.min(pd.concat([train[\"signal**2\"],test[\"signal**2\"]],axis=0))\n    ma = np.max(pd.concat([train[\"signal**2\"],test[\"signal**2\"]],axis=0))\n    train[\"signal**2\"] = (train[\"signal**2\"]-mi)/(ma - mi)\n    test[\"signal**2\"] = (test[\"signal**2\"]-mi)/(ma - mi)\n    \n    \n    #欠損値の補完\n    def feature_selection(train, test):\n        features = [col for col in train.columns if col not in ['index', 'group', 'open_channels', 'time']]\n        train = train.replace([np.inf, -np.inf], np.nan)\n        test = test.replace([np.inf, -np.inf], np.nan)\n        for feature in features:\n            feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n            train[feature] = train[feature].fillna(feature_mean)\n            test[feature] = test[feature].fillna(feature_mean)\n        return train, test, features\n\n    train,test,features = feature_selection(train,test)\n    \n    train = reduce_mem_usage(train)\n    test = reduce_mem_usage(test)\n    \n    return train,test,features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test,features = make_feature(train_clean,test_clean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Select Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Thank you, Bryansky !  \nWavenet is strong for this competition.  \n(Probably wavenet is best model....!)  \nhttps://www.kaggle.com/sggpls/wavenet-with-shifted-rfc-proba","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"in this notebook, \n* EPOCHS = 1  \n* SPLITS = 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#設定\n\nEPOCHS = 1\nNNBATCHSIZE = 16\nGROUP_BATCH_SIZE = 4000\nSEED = 321\nLR = 0.0015\nSPLITS = 2\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(321)\nnp.random.seed(321)\nos.environ['PYTHONHASHSEED'] = str(321)\ntf.random.set_seed(321)\nK.clear_session()\nconfig = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\ntf.compat.v1.keras.backend.set_session(sess)\noof_ = np.zeros((len(train), 11)) # build out of folds matrix with 11 columns, they represent our target variables classes (from 0 to 10)\npreds_ = np.zeros((len(test), 11))\ntarget = ['open_channels']\nttt = train[\"open_channels\"]\ngroup = train['group']\n#もちろんGfold\nkf = GroupKFold(n_splits=SPLITS)\n\n# train,test * 5分割\nsplits = [x for x in kf.split(train, train[target], group)]\n\nnew_splits = []\nfor sp in splits:\n    # sp -> (train,test)\n    new_split = []\n    # trainで分けられたユニークなグループ\n    new_split.append(np.unique(group[sp[0]]))\n    # testで分けられたユニークなグループ\n    new_split.append(np.unique(group[sp[1]]))\n    # testで分けられたグループ\n    new_split.append(sp[1])    \n    new_splits.append(new_split)\n\ntr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\ntr.columns = ['target_'+str(i) for i in range(11)] + ['group']\ntarget_cols = ['target_'+str(i) for i in range(11)]\ntrain_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n#train : (Ntrain,4000,features)\ntrain = np.array(list(train.groupby('group').apply(lambda x: x[features].values)))\n#test : (Ntest,4000,features)\ntest = np.array(list(test.groupby('group').apply(lambda x: x[features].values)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Classifier(shape):\n    \n    def cbr(x, out_layer, kernel, stride, dilation):\n        x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        return x\n    \n    def wave_block(x, filters, kernel_size, n):\n        # https://qiita.com/MasaEguchi/items/cd5f7e9735a120f27e2a\n        # 層が深くなるにつれ、畳み込みのノードを離す（指数的に）\n        dilation_rates = [2**i for i in range(n)]\n        #Causal Conv\n        x = Conv1D(filters=filters,kernel_size=1,padding = 'same')(x)\n        res_x = x\n        for dilation_rate in dilation_rates:\n            tanh_out = Conv1D(filters = filters,\n                              kernel_size = kernel_size,\n                              padding = 'same', \n                              activation = 'tanh', \n                              dilation_rate = dilation_rate)(x)\n            sigm_out = Conv1D(filters = filters,\n                              kernel_size = kernel_size,\n                              padding = 'same',\n                              activation = 'sigmoid', \n                              dilation_rate = dilation_rate)(x)\n            x = Multiply()([tanh_out, sigm_out])\n            x = Conv1D(filters = filters,\n                       kernel_size = 1,\n                       padding = 'same')(x)\n            res_x = Add()([res_x, x])\n        return res_x\n    \n    inp = Input(shape = (shape_))\n    x = wave_block(inp, 16, 3, 12)\n    x = wave_block(x, 32, 3, 8)\n    x = wave_block(x, 64, 3, 4)\n    x = wave_block(x, 128, 3, 1)\n\n    out = Dense(11, activation = 'softmax', name = 'out')(x)\n    \n    model = models.Model(inputs = inp, outputs = out)\n    \n    opt = Adam(lr = LR)\n    opt = tfa.optimizers.SWA(opt)\n    model.compile(loss = losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n    return model\n\ndef lr_schedule(epoch):\n    if epoch < 30:\n        lr = LR\n    elif epoch < 40:\n        lr = LR / 3\n    elif epoch < 50:\n        lr = LR / 5\n    elif epoch < 60:\n        lr = LR / 7\n    elif epoch < 70:\n        lr = LR / 9\n    elif epoch < 80:\n        lr = LR / 11\n    elif epoch < 90:\n        lr = LR / 13\n    else:\n        lr = LR / 100\n    return lr\n\nclass MacroF1(Callback):\n    def __init__(self, model, inputs, targets):\n        self.model = model\n        self.inputs = inputs\n        self.targets = np.argmax(targets, axis = 2).reshape(-1)\n        \n    def on_epoch_end(self, epoch, logs):\n        pred = np.argmax(self.model.predict(self.inputs), axis = 2).reshape(-1)\n        score = f1_score(self.targets, pred, average = 'macro')\n        print(f'F1 Macro Score: {score:.5f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n    train_x, train_y = train[tr_idx], train_tr[tr_idx]\n    valid_x, valid_y = train[val_idx], train_tr[val_idx]\n    gc.collect()\n    shape_ = (None,len(features))#19:特徴量\n    model = Classifier(shape_)\n    #学習率を変化させる\n    cb_lr_schedule = LearningRateScheduler(lr_schedule)\n    hist = model.fit(train_x,train_y,\n          epochs=EPOCHS,\n          callbacks=[cb_lr_schedule, MacroF1(model, valid_x, valid_y)], # adding custom evaluation metric for each epoch\n          batch_size=16,verbose = 2,\n          validation_data = (valid_x,valid_y))\n    preds_f = model.predict(valid_x)\n    preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n    oof_[val_orig_idx,:] += preds_f\n    te_preds = model.predict(test)\n    te_preds = te_preds.reshape(-1, te_preds.shape[-1])\n    preds_ += te_preds / SPLITS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score_ = f1_score(ttt,  np.argmax(oof_, axis = 1), average = 'macro') # axis 2 for the 3 Dimension array and axis 1 for the 2 Domension Array (extracting the best class)\nprint(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This notebook below URL is great evaluation way !   \nhttps://www.kaggle.com/c/liverpool-ion-switching/discussion/150658\n\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2640832%2F4ac743209a357d4f4838b2fdab5eedd7%2FScreenshot%20at%2008-32-21.png?generation=1589326714780950&alt=media)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_train = pd.read_csv('/kaggle/input/data-without-drift/train_clean.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\ncompare_train[\"prediction\"] = np.argmax(oof_,axis=1)\n\n# 上記載のgroupを特徴量に入れる\ncompare_train[\"g\"] = 0\nnum = 100000\ncompare_train.loc[0:10*num,\"g\"] = 0\ncompare_train.loc[10*num:15*num,\"g\"] = 3\ncompare_train.loc[15*num:20*num,\"g\"] = 1\ncompare_train.loc[20*num:25*num,\"g\"] = 4\ncompare_train.loc[25*num:30*num,\"g\"] = 2\ncompare_train.loc[30*num:35*num,\"g\"] = 3\ncompare_train.loc[35*num:40*num,\"g\"] = 1\ncompare_train.loc[40*num:45*num,\"g\"] = 2\ncompare_train.loc[45*num:50*num,\"g\"] = 4\n\nfor i in range(0,5):\n    fo = compare_train[compare_train[\"g\"]==i]\n    score = f1_score(fo[\"open_channels\"], fo[\"prediction\"], average = 'micro')\n    print(\"グループ{} : \".format(str(i)) + str(score))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}