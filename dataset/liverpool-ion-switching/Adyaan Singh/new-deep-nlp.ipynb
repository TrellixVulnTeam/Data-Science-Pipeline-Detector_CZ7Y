{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# imports\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import signal\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, f1_score, plot_confusion_matrix\nfrom keras.models import Model\nimport keras.layers as L\nfrom keras.utils import to_categorical, plot_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's import the data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/liverpool-ion-switching/train.csv')\ntest = pd.read_csv('../input/liverpool-ion-switching/test.csv')\nprint('The shape of train data:', train.shape)\nprint('The shape of test data:', test.shape)      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the heads of data\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nplt.figure(figsize=(15,6))\n#scatter with positive examples\nplt.scatter(train.time[train.open_channels==1],\n           train.signal[train.open_channels==1],\n           c='salmon')\n\nplt.scatter(train.time[train.open_channels==2],\n           train.signal[train.open_channels==2],\n           c='red')\n\nplt.scatter(train.time[train.open_channels==3],\n           train.signal[train.open_channels==3],\n           c='green')\n\nplt.scatter(train.time[train.open_channels==4],\n           train.signal[train.open_channels==4],\n           c='blue')\n\nplt.scatter(train.time[train.open_channels==5],\n           train.signal[train.open_channels==5],\n           c='black')\n\nplt.scatter(train.time[train.open_channels==6],\n           train.signal[train.open_channels==6],\n           c='pink')\n\nplt.scatter(train.time[train.open_channels==7],\n           train.signal[train.open_channels==7],\n           c='lightgreen')\n\nplt.scatter(train.time[train.open_channels==8],\n           train.signal[train.open_channels==8],\n           c='yellow')\n\nplt.scatter(train.time[train.open_channels==9],\n           train.signal[train.open_channels==9],\n           c='orange')\n\nplt.scatter(train.time[train.open_channels==10],\n           train.signal[train.open_channels==10],\n           c='brown')\n\n\n\n# scatter with nagative examples\nplt.scatter(train.time[train.open_channels==0],\n           train.signal[train.open_channels==0],\n           c='lightblue')\n\n\n# add some usefull information\nplt.title('Time in function of Signal')\nplt.xlabel('Time')\nplt.ylabel(\"Signal\")\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature extraction\nLets add to each signal several other features like rolling stats, filters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the function\ndef calc_gradients(s, n_grads=4):\n    '''\n    Calculate the gradients for pandas series. Returns the same number of sampels\n    '''\n    grads = pd.DataFrame()\n    g = s.values\n    for i in range(n_grads):\n        g = np.gradient(g)\n        grads['grad_'+str(i+1)] = g\n        \n    return grads\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_low_pass(s, n_filters=10):\n    '''\n    Applies low pass filters to the signal. Left delayed and no delayed\n    '''\n    wns = np.logspace(-2,-0.3, n_filters)\n    \n    low_pass = pd.DataFrame()\n    for wn in wns:\n        b, a = signal.butter(1, Wn=wn, btype='low')\n        low_pass['lowpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, s.values)\n        low_pass['lowpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, s.values)\n        \n    return low_pass\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_high_pass(s, n_filters=10):\n    '''\n    Applies high pass filters to the signal. Left delayed and delayed\n    '''\n    wns = np.logspace(-2, -0.1, n_filters)\n    \n    high_pass = pd.DataFrame()\n    for wn in wns:\n        b, a = signal.butter(1, Wn=wn, btype='high')\n        high_pass['highpasslf_' + str('%4f' %wn)] = signal.lfilter(b, a, s.values)\n        high_pass['highpassff_' + str('%4f' %wn)] = signal.filtfilt(b, a, s.values)\n    return high_pass\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_roll_stats(s, windows=[10, 50, 100, 500, 1000]):\n    '''\n    Calculating rolling stats like mean std, min, max\n    '''\n    \n    roll_stats = pd.DataFrame()\n    for window in windows:\n        roll_stats['roll_mean_' + str(window)] = s.rolling(window=window, min_periods=1).mean()\n        roll_stats['roll_std_'+ str(window)] = s.rolling(window=window, min_periods=1).std()\n        roll_stats['roll_min_'+ str(window)] = s.rolling(window=window, min_periods=1).min()\n        roll_stats['roll_max_'+ str(window)] = s.rolling(window=window, min_periods=1).max()\n        roll_stats['roll_range_'+ str(window)] = roll_stats['roll_max_'+ str(window)] - roll_stats['roll_min_'+ str(window)]\n        roll_stats['roll_q10_'+ str(window)] = s.rolling(window=window, min_periods=1).quantile(0.10)\n        roll_stats['roll_q25_'+ str(window)] = s.rolling(window=window, min_periods=1).quantile(0.25)\n        roll_stats['roll_q50_'+ str(window)] = s.rolling(window=window, min_periods=1).quantile(0.50)\n        roll_stats['roll_q75_'+ str(window)] = s.rolling(window=window, min_periods=1).quantile(0.75)\n        roll_stats['roll_q90_'+ str(window)] = s.rolling(window=window, min_periods=1).quantile(0.90)\n        \n    # Let's add zero when creates na value (std)\n    roll_stats = roll_stats.fillna(value=0)\n    return roll_stats\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_ewm(s, windows=[10, 50, 100, 500, 1000]):\n    '''\n    calculates exponential weight function\n    '''\n    ewm = pd.DataFrame()\n    for window in windows:\n        ewm['ewm_mean_'+ str(window)] = s.ewm(span=window, min_periods=1).mean()\n        ewm['ewm_std_'+ str(window)] = s.ewm(span=window, min_periods=1).std()\n        \n        # Add zeros when creates na value(std)\n    ewm = ewm.fillna(value=0)\n    return ewm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_featurs(s):\n    '''\n    Keep all calculation together\n    '''\n    gradients = calc_gradients(s)\n    low_pass = calc_low_pass(s)\n    high_pass = calc_high_pass(s)\n    roll_stats = calc_roll_stats(s)\n    ewm = calc_ewm(s)\n    \n    return pd.concat([s, gradients, low_pass, high_pass, roll_stats, ewm], axis=1)\n\n\ndef divide_and_add_featurs(s, signal_size = 500000):\n    '''\n    Divide the signal in the bags of 'signal_size'\n    Normalize the data dividing it by 15.0    \n    \n    '''\n    # normalize\n    s = s/15.0\n    \n    ls = []\n    for i in tqdm(range(int(s.shape[0]/signal_size))):\n        sig = s[i*signal_size:(i+1)*signal_size].copy().reset_index(drop=True)\n        sig_featured = add_featurs(sig)\n        ls.append(sig_featured)\n        \n    return pd.concat(ls, axis=0)    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Let's apply every feature to the train data\ndf = divide_and_add_featurs(train['signal'])\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train, X_valid, y_train, y_valid = train_test_split(df.values, train['open_channels'].values, test_size=0.2)\n\n# X_train.shape, y_train.shape, X_valid.shape, y_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"def create_mlp(shape):\n    \n    \n    X_input = L.Input(shape)\n    X = L.Dense(350, activation='relu')(X_input)\n    X = L.Dense(300, activation='relu')(X)\n    X = L.Dense(200, activation='relu')(X)\n    X = L.Dense(100, activation='relu')(X)\n    X = L.Dense(125, activation='relu')(X)\n    X = L.Dense(75, activation='relu')(X)\n    X = L.Dense(25, activation='relu')(X)\n    X = L.Dense(11, activation='softmax')(X)\n    \n    \n    model = Model(inputs=X_input, outputs=X)\n    \n    return model\n    \n    \nmlp = create_mlp(X_train[0].shape)\nmlp.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\nprint(mlp.summary())\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"def get_class_weight(classes):\n    '''\n    Weight of the class is inversely proportional to the population of the class\n    '''\n    hist, _ = np.histogram(classes, bins=np.arange(12)-0.5)\n    class_weight = hist.sum()/hist\n    \n    return class_weight\n\nclass_weight = get_class_weight(y_train)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# # fit the model on train data\n# mlp.fit(x = X_train, y=y_train, epochs=30, batch_size=800, class_weight=class_weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"figsize=(10, 15)\nplt.figure(1)\nplt.plot(mlp.history.history['loss'], 'b', label='loss')\nplt.xlabel('epochs')\nplt.legend()\nplt.figure(2)\nplt.plot(mlp.history.history['sparse_categorical_accuracy'], 'g', label='sparse_categorical_accuracy')\nplt.xlabel('epochs')\nplt.legend();\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"# prediction on validation set\ny_pred = mlp.predict(X_valid)\ny_pred = np.argmax(y_pred, axis=-1)\npd.Series(y_pred).value_counts()\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"# Thanks to https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud\ndef plot_cm(y_true, y_pred, title):\n    figsize=(25,15)\n    y_pred = y_pred.astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual open channels'\n    cm.columns.name = 'Predicted open channels'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap='YlGnBu', annot=annot, fmt='', ax=ax)\n\n# f1 score\nf1 = f1_score(y_valid, y_pred, average='macro')\n\n# plot confusion matrix\nplot_cm(y_valid, y_pred, 'MLP f1_score=' + str('%.4f' %f1))\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"# create test DataFrame as X_test\nX_test = divide_and_add_featurs(test['signal'])\nprint('X_test.shape=', X_test.shape)\nX_test.head()\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"#Let's predict test dataset\ny_test = mlp.predict(X_test)\ny_test = np.argmax(y_test, axis=-1)\npd.Series(y_test).value_counts()\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"# create submission\nsubmission = pd.DataFrame()\nsubmission['time'] = test['time']\nsubmission['open_channels'] = y_test\n\n# write file\nsubmission.to_csv('final_if_imp.csv', index=False, float_format='%.4f')\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(df.values, train['open_channels'].values, test_size=0.2)\n\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_mlp(shape):\n    \n    \n    X_input = L.Input(shape)\n    X = L.Dense(1536, activation='relu')(X_input)\n    X = L.Dense(1024, activation='relu')(X)\n    X = L.Dense(512, activation='relu')(X)\n    X = L.Dense(11, activation='softmax')(X)\n        \n    model = Model(inputs=X_input, outputs=X)\n    \n    return model\n    \nmlp = create_mlp(X_train[0].shape)\n# mlp.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n# print(mlp.summary())    \n\n\noptimizer=tf.keras.optimizers.Adam(lr=0.001)\nmlp.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\nmlp.summary()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_class_weight(classes):\n    '''\n    Weight of the class is inversely proportional to the population of the class\n    '''\n    hist, _ = np.histogram(classes, bins=np.arange(12)-0.5)\n    class_weight = hist.sum()/hist\n    \n    return class_weight\n\nclass_weight = get_class_weight(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# fit the model on train data\nmlp.fit(x = X_train, y=y_train, epochs=20, batch_size=1536,validation_data = (X_valid, y_valid), class_weight=class_weight,callbacks = [reduce_lr])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(mlp.history.history['accuracy'])\nplt.plot(mlp.history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['Train','Valid'])\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(mlp.history.history['loss'])\nplt.plot(mlp.history.history['val_loss'])\nplt.title('Model Loss')\nplt.xlabel('Epochs')\nplt.ylabel('loss')\nplt.legend(['Train', 'Valid'])\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction on validation set\ny_pred = mlp.predict(X_valid)\ny_pred = np.argmax(y_pred, axis=-1)\npd.Series(y_pred).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud\ndef plot_cm(y_true, y_pred, title):\n    figsize=(25,15)\n    y_pred = y_pred.astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual open channels'\n    cm.columns.name = 'Predicted open channels'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap='YlGnBu', annot=annot, fmt='', ax=ax)\n\n# f1 score\nf1 = f1_score(y_valid, y_pred, average='macro')\n\n# plot confusion matrix\nplot_cm(y_valid, y_pred, 'MLP f1_score=' + str('%.4f' %f1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create test DataFrame as X_test\nX_test = divide_and_add_featurs(test['signal'])\nprint('X_test.shape=', X_test.shape)\nX_test.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's predict test dataset\ny_test = mlp.predict(X_test)\ny_test = np.argmax(y_test, axis=-1)\npd.Series(y_test).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create submission\nsubmission = pd.DataFrame()\nsubmission['time'] = test['time']\nsubmission['open_channels'] = y_test\n\n# write file\nsubmission.to_csv('final3_if_imp.csv', index=False, float_format='%.4f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}