{"cells":[{"metadata":{"id":"6BRLfFJ6llCH","outputId":"afda7265-bc50-4bbf-89ea-0fe7a0b59037","trusted":true},"cell_type":"code","source":"! pip install -q tensorflow\nimport tensorflow as tf\n# import tensorflow_io as tfio\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install py7zr\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls train/audio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_labels = ['_background_noise_', 'dog', 'four', 'left', 'off', 'seven', 'three', 'wow', 'bed', 'down', 'go', 'marvin', 'on', 'sheila', 'tree', 'yes', 'bird', 'eight', 'happy', 'nine', 'one', 'six', 'two', 'zero', 'cat', 'five', 'house', 'no', 'right', 'stop', 'up']\nclasses = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go','unknown']\n","execution_count":null,"outputs":[]},{"metadata":{"id":"W78yohTbbUFm","outputId":"55cd6375-c39c-4470-b4eb-22915aa01f7a","trusted":true},"cell_type":"code","source":"%cd ..\n!python -m py7zr x input/tensorflow-speech-recognition-challenge/train.7z \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#unzip test data\n!python -m py7zr x input/tensorflow-speech-recognition-challenge/test.7z ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls \nprint()\n!ls train","execution_count":null,"outputs":[]},{"metadata":{"id":"X6U2t63wV2AO","outputId":"9fe1463a-2430-4dcd-d55a-059ca280a853","trusted":true},"cell_type":"code","source":"#!ls ../input/tensorflow-speech-recognition-challenge/train.7z\n#!7z x ./input/tensorflow-speech-recognition-challenge/test.7z -o.","execution_count":null,"outputs":[]},{"metadata":{"id":"bLR9QF7FrTeU","trusted":true},"cell_type":"code","source":"### is this what you wanted ali?? better than linear search i guess\ndef load_test_file_set(test_file_path='./train/testing_list.txt'):\n    #open the txt file containing the pathes of files that should be added to test dataset\n    #convert the list to set datastructure\n    test_files = open(test_file_path).read().splitlines()\n    return set(test_files)\n\ndef load_validation_file_set(val_file_path='./train/validation_list.txt'):\n    #open the txt file containing the pathes of files that should be added to validation dataset\n    #convert the list to set datastructure\n    validation_files = open(val_file_path).read().splitlines()\n    return set(validation_files)\n\ndef is_test_file(test_set, file_path):\n    return file_path in test_set\n\ndef is_validation_file(val_set, file_path):\n    return file_path in val_set\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"QaGEjQ1xDbx9","trusted":true},"cell_type":"code","source":"def cut_into_one_sec_segment(audio , sampling_rate):\n    length = audio.shape[0]\n    #pad audio with zeros\n\n    num_segments = int(np.ceil(length/sampling_rate))\n    audio = np.pad(audio, ((0, sampling_rate * num_segments-audio.shape[0]), (0, 0)), 'constant', constant_values=(0, ))\n    #print(audio.shape)\n    #segments = np.zeros((sampling_rate,1))\n    segments = audio[0:  sampling_rate, :]\n    #print(segments.shape)\n    #print(segments.shape)\n    for i in range(1,num_segments):\n        seg = audio[ i * sampling_rate : ((i + 1) * sampling_rate ) , :]\n        segments = np.hstack((segments, seg))\n        #print(len(segments))\n    return segments","execution_count":null,"outputs":[]},{"metadata":{"id":"LadD_8eirRLq","trusted":true},"cell_type":"code","source":"import os\n#outputs  an array of shape(16000, some number)\ndef load_background_noise(sampling_rate = 16000):##Pads zeros to each background wave file\n    #read background music then cut then add to dataset\n    bg_noise_dirpath = './train/audio/_background_noise_/'\n    segments = None\n    first_iter = True\n    for filename in os.listdir(bg_noise_dirpath):\n        if (filename.endswith('.wav')):\n            full_file_path = os.path.join(bg_noise_dirpath, filename)\n            audio = tfio.audio.AudioIOTensor(full_file_path).to_tensor().numpy()\n            #print(audio.shape)\n            \n            #first iter return first segment into segments array\n            if first_iter == True:\n                first_iter = False\n                #convert noise audio to 1 second long audio segments with zero padding\n                segments = cut_into_one_sec_segment(audio, sampling_rate)\n                #print('firstsegshape')\n                #print(segments.shape)\n            else:#2nd+ iter stack the new segments\n                #convert noise audio to 1 second long audio segments with zero padding\n                new_segments = cut_into_one_sec_segment(audio, sampling_rate)\n                segments = np.hstack((segments, new_segments))\n                #print('stackedsegments shape')\n                #print(segments.shape)\n    #print(segments.shape)\n    \n    return segments\n\nbackground_dataset = load_background_noise()","execution_count":null,"outputs":[]},{"metadata":{"id":"VsFqYxVwwNld","outputId":"812c5206-b349-416c-bc48-283d4b0fa38d","trusted":true},"cell_type":"code","source":"print(background_dataset.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"RfYn5YN6g3Bq","outputId":"14918882-0780-45a2-8608-54173817e392","trusted":true},"cell_type":"code","source":"#CELL TO RUN to show output\n## this the cell to USe to load the train data\n\n# ALI ZEYAD RUN THIS\n\n\n# ! mv train/audio/_background_noise_ .\n###classes = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go','unknown']\nnum_wav_files_with_bgnoise = 64727 #(computed before using a forloop)\n#number of wav files not counting the background noise clips that have to broken down into smaller units\n#FS is 16000 for all inputs\n\nimport os\n\nX_train = []\nY_train = []\n\nX_val   = []\nY_val   = []\nX_test  = []\nY_test  = []\n\ndef read_train_audio(folder_path='./train/audio'):\n    \n    global X_train, X_val, Y_train, Y_val\n\n    classes = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go','unknown']\n    \n    all_labels = ['_background_noise_', 'dog', 'four', 'left', 'off', 'seven', 'three', 'wow', 'bed', 'down', 'go', 'marvin', 'on', 'sheila', 'tree', 'yes', 'bird', 'eight', 'happy', 'nine', 'one', 'six', 'two', 'zero', 'cat', 'five', 'house', 'no', 'right', 'stop', 'up']\n    label_id = {}\n    for i in range(len(all_labels)):\n        label_id[all_labels[i]] = i\n        \n    class_id = {}\n    for i in range(len(classes)):\n        class_id[classes[i]] = i\n    test_file_set = load_test_file_set()\n    validation_file_set = load_validation_file_set()\n    \n    for label in os.listdir(folder_path): #loop on all folders in the audio folder\n        dir_path = os.path.join(folder_path, label)\n        if os.path.isfile(dir_path):#skip any files\n            continue\n        for filename in os.listdir(dir_path): #loop on all files in folder\n            if label == '_background_noise_':\n                #background noise is loaded in a seperate\n                break\n            else:\n                \n                full_file_path = os.path.join(dir_path, filename)\n                if os.path.isdir(full_file_path):\n                    continue\n                #padded_audio = np.zeros(shape=(16000, 1))\n                audio = tfio.audio.AudioIOTensor(full_file_path).to_tensor().numpy()\n\n                # size_audio = audio.shape[0]\n                # padded_audio[0 : size_audio] = audio\n                file_rel_path = os.path.join(label, filename)\n                #print(file_rel_path)\n                if is_validation_file(validation_file_set, file_rel_path):\n                    X_val.append(np.pad(audio, ((0, 16000-audio.shape[0]), (0, 0)), 'constant', constant_values=(0, )))\n                    #Y_val.append(class_id.get(label, len(classes) - 1))\n                    \n                    Y_val.append(label_id.get(label, len(all_labels) - 1))\n                elif is_test_file(test_file_set, file_rel_path):\n                    #add to test data ??\n                    pass\n                else:\n                    #print(audio.shape)\n                    #print(full_file_path)\n                    #print(label)\n                    X_train.append(np.pad(audio, ((0, 16000-audio.shape[0]), (0, 0)), 'constant', constant_values=(0, )))\n                    #Y_train.append(class_id.get(label, len(classes) - 1))\n                    \n                    Y_train.append(label_id.get(label, len(all_labels) - 1))\n\n\n    \n   \n    X_train = np.array(X_train)\n    #print(X_train.shape)\n    X_val = np.array(X_val)\n    Y_val = np.array(Y_val)\n    Y_val = Y_val.reshape(Y_val.shape[0], 1)\n\n\n    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1])\n    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1])\n\n    bgnoise_train_data = load_background_noise()\n    bgnoise_train_data = bgnoise_train_data.T\n    print(bgnoise_train_data.shape)\n    #y_bgnoise = [class_id['unknown']] * bgnoise_train_data.shape[0]\n    y_bgnoise = [label_id['_background_noise_']] * bgnoise_train_data.shape[0]\n    Y_train.extend(y_bgnoise)\n    Y_train = np.array(Y_train)\n    Y_train = Y_train.reshape(Y_train.shape[0], 1)\n\n    X_train = np.vstack((X_train, bgnoise_train_data))\n    print(X_train.shape)\n\n\nread_train_audio()","execution_count":null,"outputs":[]},{"metadata":{"id":"uYN8Eb2sx1oB","outputId":"a09fb226-c57f-4e31-df28-236c49edb42d","trusted":true},"cell_type":"code","source":"print('X Train (words + background noise) Dimensions:', X_train.shape)\nprint('Y Train' , Y_train.shape)\n\nprint('X Validation' , X_val.shape)\nprint('Y Validation' , Y_val.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Ot8dJJ7AfAQp","trusted":false},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Dropout, Dense\nfrom tensorflow.keras.layers import Bidirectional, LSTM, TimeDistributed\n\n\ndef get_SR_Model(num_classes: int):\n    X_input = Input(shape=(16000, 1))\n    X = Conv1D(filters=256,kernel_size=15,strides=4)(X_input)\n    X = BatchNormalization()(X)\n    X = Dropout(0.2)(X)\n    X = Conv1D(filters=512,kernel_size=15,strides=4)(X_input)\n    X = BatchNormalization()(X)\n    X = Dropout(0.2)(X)\n    X = LSTM(units=512, return_sequences=True)(X)\n    X = LSTM(units=512, return_sequences=False)(X)\n    X = Dense(num_classes, activation='softmax')(X)\n    return Model(inputs=[X_input], outputs=[X])\n\nmodel = get_SR_Model(11)","execution_count":null,"outputs":[]},{"metadata":{"id":"li2CRYpgjUKb","outputId":"999c9c93-29fa-4bc9-ccea-b8030860e22a","trusted":false},"cell_type":"code","source":"from tensorflow.keras import optimizers\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import metrics\n#ana bahbed\nmodel.compile(loss=losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam'), metrics=metrics.SparseCategoricalCrossentropy(\n    name='sparse_categorical_crossentropy'))\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"id":"2HKQXiXuffa0","outputId":"220ecb88-de54-4e1e-d9a0-25e54462fa7b","trusted":false},"cell_type":"code","source":"## ana bahbed\nhistory = model.fit(X_train, Y_train, batch_size=64, epochs=1)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"eeo0_XOnt7Oq","outputId":"dac5496e-a73f-4573-ae12-a6d28464e200","trusted":false},"cell_type":"code","source":"print(history.history)","execution_count":null,"outputs":[]},{"metadata":{"id":"8AppSAtYNOa1","outputId":"0a75886e-82a0-4ead-fea9-2c24c2949b02","trusted":false},"cell_type":"code","source":"print(len(test_file_set))\nprint(len(validation_file_set))\nfolder_path='./train/audio'\nval_count = 0\ntest_count = 0\nfor label in os.listdir(folder_path):\n    labelpath = os.path.join(folder_path, label)\n    if os.path.isfile(labelpath):\n        continue\n    for filename in os.listdir(labelpath):\n        filepath = os.path.join(label,filename)\n        if is_validation_file(validation_file_set, filepath):\n            val_count += 1\n        elif is_test_file(test_file_set, filepath):\n            test_count += 1\n\nprint('Validation:' + str(val_count))\nprint('Test: ' +str(test_count))\nprint(test_file_set)","execution_count":null,"outputs":[]},{"metadata":{"id":"Tg2SFHBajwNC","trusted":false},"cell_type":"code","source":"read_train_audio_in_dir('train/audio')","execution_count":null,"outputs":[]},{"metadata":{"id":"D9wzpGbSqZgS","trusted":false},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}