{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyunpack\n!pip install patool\n!pip install py7zr\n!pip install sounddevice\n!pip install noisereduce\n!pip install librosa\n! pip install python_speech_features\n! pip install tensorflow==2.4\n! pip install malaya_speech\n! pip install webrtcvad","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:37:01.453457Z","iopub.execute_input":"2021-12-31T13:37:01.453727Z","iopub.status.idle":"2021-12-31T13:38:50.740084Z","shell.execute_reply.started":"2021-12-31T13:37:01.453702Z","shell.execute_reply":"2021-12-31T13:38:50.73915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Import the libraries**\n\nFirst, import all the necessary libraries into our notebook. LibROSA and SciPy are the Python libraries used for processing audio signals.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom py7zr import unpack_7zarchive\nimport shutil\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport librosa\nimport IPython.display as ipd\nfrom scipy.io import wavfile\n\nimport noisereduce as nr\nimport tensorflow \nfrom malaya_speech import Pipeline\n\nimport malaya_speech\nimport os\n\nfrom python_speech_features import mfcc\n\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sn","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:38:50.741885Z","iopub.execute_input":"2021-12-31T13:38:50.74224Z","iopub.status.idle":"2021-12-31T13:38:55.546456Z","shell.execute_reply.started":"2021-12-31T13:38:50.742201Z","shell.execute_reply":"2021-12-31T13:38:55.54527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shutil.register_unpack_format('7zip', ['.7z'], unpack_7zarchive)\nshutil.unpack_archive('/kaggle/input/tensorflow-speech-recognition-challenge/train.7z', '/kaggle/working/tensorflow-speech-recognition-challenge/train/')","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:38:55.548745Z","iopub.execute_input":"2021-12-31T13:38:55.549319Z","iopub.status.idle":"2021-12-31T13:41:37.429883Z","shell.execute_reply.started":"2021-12-31T13:38:55.54928Z","shell.execute_reply":"2021-12-31T13:41:37.42899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from pyunpack import Archive\n# import shutil\n# if not os.path.exists('/kaggle/working/tensorflow-speech-recognition-challenge/train/'):\n#     os.makedirs('/kaggle/working/tensorflow-speech-recognition-challenge/train/')\n# Archive('/kaggle/input/tensorflow-speech-recognition-challenge/train.7z').extractall('/kaggle/working/tensorflow-speech-recognition-challenge/train/')\n\n#for dirname, _, filenames in os.walk('/kaggle/working/tensorflow-speech-recognition-challenge/train/train/audio'):\n #   for filename in filename[:5]:\n  #      print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:41:37.431539Z","iopub.execute_input":"2021-12-31T13:41:37.431891Z","iopub.status.idle":"2021-12-31T13:41:37.437226Z","shell.execute_reply.started":"2021-12-31T13:41:37.431857Z","shell.execute_reply":"2021-12-31T13:41:37.436335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <center> Implementing the Speech-to-Text Model in Python\n**Understanding the Problem Statement for our Speech-to-Text Project**\n\nLet’s understand the problem statement of our project before we move into the implementation part.\n\nWe might be on the verge of having too many screens around us. It seems like every day, new versions of common objects are “re-invented” with built-in wifi and bright touchscreens. A promising antidote to our screen addiction is voice interfaces. \n\n__You can download the dataset from__ [here](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge).\n    \nTensorFlow recently released the Speech Commands Datasets. It includes 65,000 one-second long utterances of 30 short words, by thousands of different people. We’ll build a speech recognition system that understands simple spoken commands. <br>    ","metadata":{}},{"cell_type":"markdown","source":"**Data Exploration and Visualization**\n\nData Exploration and Visualization helps us to understand the data as well as pre-processing steps in a better way. \n\n**Visualization of Audio signal in time series domain**\n\nNow, we’ll visualize the audio signal in the time series domain:","metadata":{}},{"cell_type":"code","source":"train_audio_path = '/kaggle/working/tensorflow-speech-recognition-challenge/train/train/audio/'","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:41:37.438468Z","iopub.execute_input":"2021-12-31T13:41:37.438881Z","iopub.status.idle":"2021-12-31T13:41:37.448022Z","shell.execute_reply.started":"2021-12-31T13:41:37.438842Z","shell.execute_reply":"2021-12-31T13:41:37.447189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Accessing each file in data**","metadata":{}},{"cell_type":"code","source":"#!apt-get install -y p7zip-full\n#!7z x ../input/tensorflow-speech-recognition-challenge/train.7z","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:41:37.449556Z","iopub.execute_input":"2021-12-31T13:41:37.44999Z","iopub.status.idle":"2021-12-31T13:41:37.456737Z","shell.execute_reply.started":"2021-12-31T13:41:37.449954Z","shell.execute_reply":"2021-12-31T13:41:37.455788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samples, sample_rate = librosa.load(train_audio_path+'on/5a3712c9_nohash_1.wav', sr = 16000)\nfig = plt.figure(figsize=(14, 8))\nax1 = fig.add_subplot(211)\nax1.set_title('Raw wave of ' + '../input/train/audio/on/0a7c2a8d_nohash_0.wav')\nax1.set_xlabel('time')\nax1.set_ylabel('Amplitude')\nax1.plot(np.linspace(0, sample_rate/len(samples), sample_rate), samples)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:41:37.458055Z","iopub.execute_input":"2021-12-31T13:41:37.458453Z","iopub.status.idle":"2021-12-31T13:41:37.647259Z","shell.execute_reply.started":"2021-12-31T13:41:37.458414Z","shell.execute_reply":"2021-12-31T13:41:37.646483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sampling rate **\n\nLet us now look at the sampling rate of the audio signals","metadata":{}},{"cell_type":"code","source":"ipd.Audio(samples, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:41:37.649745Z","iopub.execute_input":"2021-12-31T13:41:37.650089Z","iopub.status.idle":"2021-12-31T13:41:37.659407Z","shell.execute_reply.started":"2021-12-31T13:41:37.650054Z","shell.execute_reply":"2021-12-31T13:41:37.658416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sample_rate)\nsig1=samples\nfs=sample_rate\nsr=fs","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:41:37.660904Z","iopub.execute_input":"2021-12-31T13:41:37.661153Z","iopub.status.idle":"2021-12-31T13:41:37.666495Z","shell.execute_reply.started":"2021-12-31T13:41:37.661123Z","shell.execute_reply":"2021-12-31T13:41:37.665532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time = np.linspace(0, len(sig1 - 1) / fs, len(sig1 - 1))\nreduced_noise1 = nr.reduce_noise(y=sig1, sr=fs,stationary=True)\nplt.plot(time, reduced_noise1)  # plot in seconds\n#reduced_noise2 = nr.reduce_noise(y=sig2, sr=fs,stationary=True)\n#plt.plot(time, reduced_noise2)  # plot in seconds\n#plt.title(\"Voice Signal\")\nplt.xlabel(\"Time [seconds]\")\nplt.ylabel(\"Voice amplitude\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:41:37.668081Z","iopub.execute_input":"2021-12-31T13:41:37.668936Z","iopub.status.idle":"2021-12-31T13:41:38.432666Z","shell.execute_reply.started":"2021-12-31T13:41:37.668892Z","shell.execute_reply":"2021-12-31T13:41:38.431758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ipd.Audio(reduced_noise1, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:41:38.436782Z","iopub.execute_input":"2021-12-31T13:41:38.437306Z","iopub.status.idle":"2021-12-31T13:41:38.449975Z","shell.execute_reply.started":"2021-12-31T13:41:38.437262Z","shell.execute_reply":"2021-12-31T13:41:38.449099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Silence Removal\nvad = malaya_speech.vad.webrtc()\ny=reduced_noise1\ny_= malaya_speech.resample(y, sr, 16000)\ny_ = malaya_speech.astype.float_to_int(y_)\nframes = malaya_speech.generator.frames(y, 30, sr)\nframes_ = list(malaya_speech.generator.frames(y_, 30, 16000, append_ending_trail = False))\nframes_webrtc = [(frames[no], vad(frame)) for no, frame in enumerate(frames_)]\ny_ = malaya_speech.combine.without_silent(frames_webrtc)\ny_","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:41:38.452617Z","iopub.execute_input":"2021-12-31T13:41:38.453831Z","iopub.status.idle":"2021-12-31T13:41:38.633713Z","shell.execute_reply.started":"2021-12-31T13:41:38.453793Z","shell.execute_reply":"2021-12-31T13:41:38.632897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ipd.Audio(y_, rate = sr )","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:41:38.635142Z","iopub.execute_input":"2021-12-31T13:41:38.635475Z","iopub.status.idle":"2021-12-31T13:41:38.644454Z","shell.execute_reply.started":"2021-12-31T13:41:38.63544Z","shell.execute_reply":"2021-12-31T13:41:38.643404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zero = np.zeros((1*sr-y_.shape[0]))\nsignal = np.concatenate((y_,zero))\nsignal.shape\ntime = np.linspace(0, len(signal - 1) / fs, len(signal - 1))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:41:38.645819Z","iopub.execute_input":"2021-12-31T13:41:38.646371Z","iopub.status.idle":"2021-12-31T13:41:38.654737Z","shell.execute_reply.started":"2021-12-31T13:41:38.646294Z","shell.execute_reply":"2021-12-31T13:41:38.653725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(time,signal)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:41:38.656125Z","iopub.execute_input":"2021-12-31T13:41:38.656667Z","iopub.status.idle":"2021-12-31T13:41:38.813113Z","shell.execute_reply.started":"2021-12-31T13:41:38.656618Z","shell.execute_reply":"2021-12-31T13:41:38.812324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels=os.listdir(train_audio_path)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:41:38.814552Z","iopub.execute_input":"2021-12-31T13:41:38.815016Z","iopub.status.idle":"2021-12-31T13:41:38.819921Z","shell.execute_reply.started":"2021-12-31T13:41:38.81498Z","shell.execute_reply":"2021-12-31T13:41:38.818742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#find count of each label and plot bar graph\nno_of_recordings=[]\nfor label in labels:\n    waves = [f for f in os.listdir(train_audio_path + '/'+ label) if f.endswith('.wav')]\n    no_of_recordings.append(len(waves))\n    \n#plot\nplt.figure(figsize=(30,5))\nindex = np.arange(len(labels))\nplt.bar(index, no_of_recordings)\nplt.xlabel('Commands', fontsize=12)\nplt.ylabel('No of recordings', fontsize=12)\nplt.xticks(index, labels, fontsize=15, rotation=60)\nplt.title('No. of recordings for each command')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:41:38.821444Z","iopub.execute_input":"2021-12-31T13:41:38.8218Z","iopub.status.idle":"2021-12-31T13:41:39.171531Z","shell.execute_reply.started":"2021-12-31T13:41:38.821765Z","shell.execute_reply":"2021-12-31T13:41:39.170728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels=[\"on\",\"off\",\"up\",\"down\"]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:43:45.881838Z","iopub.execute_input":"2021-12-31T13:43:45.882204Z","iopub.status.idle":"2021-12-31T13:43:45.886343Z","shell.execute_reply.started":"2021-12-31T13:43:45.882173Z","shell.execute_reply":"2021-12-31T13:43:45.88534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Duration of recordings**\n\nWhat’s next? A look at the distribution of the duration of recordings:","metadata":{}},{"cell_type":"markdown","source":"**Preprocessing the audio waves**\n\nIn the data exploration part earlier, we have seen that the duration of a few recordings is less than 1 second and the sampling rate is too high. So, let us read the audio waves and use the below-preprocessing steps to deal with this.\n\nHere are the two steps we’ll follow:\n\n* Noise Reduction\n* Silence Removal\n\nLet us define these preprocessing steps in the below code snippet:","metadata":{}},{"cell_type":"code","source":"sr=16000\nvad = malaya_speech.vad.webrtc()\nall_wave = []\nall_label = []\nfor label in labels:\n    print(label)\n    waves = [f for f in os.listdir(train_audio_path + '/'+ label) if f.endswith('.wav')]\n    for wav in waves:\n        samples, sample_rate = librosa.load(train_audio_path + '/' + label + '/' + wav, sr = 16000)\n        samples = nr.reduce_noise(y=samples, sr=sr,stationary=True)\n        y_= malaya_speech.resample(samples, sr, 16000)\n        y_ = malaya_speech.astype.float_to_int(y_)\n        frames = malaya_speech.generator.frames(samples, 30, sr)\n        frames_ = list(malaya_speech.generator.frames(y_, 30, 16000, append_ending_trail = False))\n        frames_webrtc = [(frames[no], vad(frame)) for no, frame in enumerate(frames_)]\n        y_ = malaya_speech.combine.without_silent(frames_webrtc)\n        zero = np.zeros(((1*sr+4000)-y_.shape[0]))\n        signal = np.concatenate((y_,zero))\n        all_wave.append(signal)\n        all_label.append(label)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:43:49.578892Z","iopub.execute_input":"2021-12-31T13:43:49.579229Z","iopub.status.idle":"2021-12-31T13:48:18.242332Z","shell.execute_reply.started":"2021-12-31T13:43:49.579198Z","shell.execute_reply":"2021-12-31T13:48:18.241447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.array(all_wave).shape)\nprint(np.array(all_label).shape)\ntime = np.linspace(0, len(signal - 1) / fs, len(signal - 1))\nplt.plot(time,np.array(all_wave)[2000,:])\nprint(np.array(all_label)[2000])\nipd.Audio(np.array(all_wave)[2000,:], rate = sr )","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:48:18.244189Z","iopub.execute_input":"2021-12-31T13:48:18.24466Z","iopub.status.idle":"2021-12-31T13:48:22.384612Z","shell.execute_reply.started":"2021-12-31T13:48:18.244598Z","shell.execute_reply":"2021-12-31T13:48:22.383523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_mfcc=[]\nfor wave in all_wave:\n    i=0\n    mfcc_feat = mfcc(wave , fs, winlen=256/fs, winstep=256/(2*fs), numcep=13, nfilt=26, nfft=256,\n                 lowfreq=0, highfreq=fs/2, preemph=0.97, ceplifter=22, appendEnergy=True, winfunc=np.hamming)\n    mfcc_feat= np.transpose(mfcc_feat)\n    all_mfcc.append(mfcc_feat)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:50:20.516514Z","iopub.execute_input":"2021-12-31T13:50:20.516878Z","iopub.status.idle":"2021-12-31T13:50:49.233875Z","shell.execute_reply.started":"2021-12-31T13:50:20.516847Z","shell.execute_reply":"2021-12-31T13:50:49.232763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.array(all_mfcc).shape)\nprint(np.array(all_label).shape)\nd1=np.array(all_mfcc).shape[1]\nd2=np.array(all_mfcc).shape[2]\nd=d1*d2\nprint(d)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:50:49.238952Z","iopub.execute_input":"2021-12-31T13:50:49.241204Z","iopub.status.idle":"2021-12-31T13:50:49.473381Z","shell.execute_reply.started":"2021-12-31T13:50:49.23944Z","shell.execute_reply":"2021-12-31T13:50:49.472384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"op_mfcc=np.array(all_mfcc)\nop_mfcc=op_mfcc.reshape(np.array(all_mfcc).shape[0],-1)\nop_mfcc.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:50:52.786337Z","iopub.execute_input":"2021-12-31T13:50:52.786694Z","iopub.status.idle":"2021-12-31T13:50:52.922366Z","shell.execute_reply.started":"2021-12-31T13:50:52.786656Z","shell.execute_reply":"2021-12-31T13:50:52.92147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#all_label = all_label.tolist()\n\nle = LabelEncoder()\ny=le.fit_transform(all_label)\nclasses= list(le.classes_)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:51:01.132017Z","iopub.execute_input":"2021-12-31T13:51:01.132342Z","iopub.status.idle":"2021-12-31T13:51:01.140879Z","shell.execute_reply.started":"2021-12-31T13:51:01.132311Z","shell.execute_reply":"2021-12-31T13:51:01.140009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Model based on ANN** ","metadata":{}},{"cell_type":"code","source":"! pip install --upgrade tensorflow\n! pip install --upgrade tensorflow-gpu\n! pip install keras==2.3.1","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:51:04.694372Z","iopub.execute_input":"2021-12-31T13:51:04.69472Z","iopub.status.idle":"2021-12-31T13:52:50.963523Z","shell.execute_reply.started":"2021-12-31T13:51:04.694688Z","shell.execute_reply":"2021-12-31T13:52:50.962592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.optimizers import SGD\nfrom keras.constraints import maxnorm\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv2D, Flatten, Dense,Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:52:50.965215Z","iopub.execute_input":"2021-12-31T13:52:50.965555Z","iopub.status.idle":"2021-12-31T13:52:51.017265Z","shell.execute_reply.started":"2021-12-31T13:52:50.965523Z","shell.execute_reply":"2021-12-31T13:52:51.01604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=tensorflow.keras.utils.to_categorical(y, num_classes=len(labels), dtype='float32')\ny.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:52:51.019292Z","iopub.execute_input":"2021-12-31T13:52:51.019707Z","iopub.status.idle":"2021-12-31T13:52:51.028686Z","shell.execute_reply.started":"2021-12-31T13:52:51.019666Z","shell.execute_reply":"2021-12-31T13:52:51.027595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_tr, x_val, y_tr, y_val= train_test_split(op_mfcc,np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:52:51.030426Z","iopub.execute_input":"2021-12-31T13:52:51.03112Z","iopub.status.idle":"2021-12-31T13:52:51.180602Z","shell.execute_reply.started":"2021-12-31T13:52:51.031083Z","shell.execute_reply":"2021-12-31T13:52:51.179624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_tr.shape)\nprint(y_tr.shape)\nprint(x_val.shape)\nprint(y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:52:51.181969Z","iopub.execute_input":"2021-12-31T13:52:51.182316Z","iopub.status.idle":"2021-12-31T13:52:51.190836Z","shell.execute_reply.started":"2021-12-31T13:52:51.18228Z","shell.execute_reply":"2021-12-31T13:52:51.189692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Model Architecture**","metadata":{}},{"cell_type":"code","source":"#from keras.models import Sequential\n#from keras.layers import Dense, Dropout, Activation\n\n#Model Architecture\nmodel = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape=(d,), kernel_constraint=maxnorm(3)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(80, activation='relu', kernel_constraint=maxnorm(3)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(len(classes), activation='softmax' , kernel_constraint=maxnorm(3)))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:51:34.453162Z","iopub.execute_input":"2021-12-31T14:51:34.453518Z","iopub.status.idle":"2021-12-31T14:51:34.503677Z","shell.execute_reply.started":"2021-12-31T14:51:34.45348Z","shell.execute_reply":"2021-12-31T14:51:34.50282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tensorflow.keras.utils.plot_model(model, 'model.png',show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:51:36.350002Z","iopub.execute_input":"2021-12-31T14:51:36.350347Z","iopub.status.idle":"2021-12-31T14:51:36.583226Z","shell.execute_reply.started":"2021-12-31T14:51:36.350315Z","shell.execute_reply":"2021-12-31T14:51:36.582242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',optimizer='adamax',metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:53:13.01893Z","iopub.execute_input":"2021-12-31T14:53:13.019283Z","iopub.status.idle":"2021-12-31T14:53:13.036918Z","shell.execute_reply.started":"2021-12-31T14:53:13.019248Z","shell.execute_reply":"2021-12-31T14:53:13.036135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=0.0001) \nmc = ModelCheckpoint('best_model.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:53:13.483579Z","iopub.execute_input":"2021-12-31T14:53:13.484048Z","iopub.status.idle":"2021-12-31T14:53:13.490411Z","shell.execute_reply.started":"2021-12-31T14:53:13.484012Z","shell.execute_reply":"2021-12-31T14:53:13.489381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x_tr=np.expand_dims(x_tr,axis=0)\n#y_tr=np.expand_dims(y_tr,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:53:13.923405Z","iopub.execute_input":"2021-12-31T14:53:13.923766Z","iopub.status.idle":"2021-12-31T14:53:13.927064Z","shell.execute_reply.started":"2021-12-31T14:53:13.923729Z","shell.execute_reply":"2021-12-31T14:53:13.926133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(x_tr, y_tr,validation_data=(x_val,y_val), epochs=120, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:53:14.546657Z","iopub.execute_input":"2021-12-31T14:53:14.547009Z","iopub.status.idle":"2021-12-31T14:54:44.456487Z","shell.execute_reply.started":"2021-12-31T14:53:14.546977Z","shell.execute_reply":"2021-12-31T14:54:44.455581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_score = model.evaluate(x_tr, y_tr, batch_size=12)\nprint(train_score)\n\nprint('----------------Training Complete-----------------')\n\ntest_score = model.evaluate(x_val, y_val, batch_size = 12)\nprint(test_score)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:54:50.91879Z","iopub.execute_input":"2021-12-31T14:54:50.919112Z","iopub.status.idle":"2021-12-31T14:54:52.453208Z","shell.execute_reply.started":"2021-12-31T14:54:50.919082Z","shell.execute_reply":"2021-12-31T14:54:52.452105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history.history.keys()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:54:59.739334Z","iopub.execute_input":"2021-12-31T14:54:59.739682Z","iopub.status.idle":"2021-12-31T14:54:59.745349Z","shell.execute_reply.started":"2021-12-31T14:54:59.73963Z","shell.execute_reply":"2021-12-31T14:54:59.74451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:55:02.677663Z","iopub.execute_input":"2021-12-31T14:55:02.678086Z","iopub.status.idle":"2021-12-31T14:55:02.810549Z","shell.execute_reply.started":"2021-12-31T14:55:02.678056Z","shell.execute_reply":"2021-12-31T14:55:02.80972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:55:06.942877Z","iopub.execute_input":"2021-12-31T14:55:06.943214Z","iopub.status.idle":"2021-12-31T14:55:07.082779Z","shell.execute_reply.started":"2021-12-31T14:55:06.943182Z","shell.execute_reply":"2021-12-31T14:55:07.081807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predict=model.predict(x_val)\nconf_mat=tensorflow.math.confusion_matrix(np.argmax(y_val,axis=1) , np.argmax(y_predict,axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:55:19.952173Z","iopub.execute_input":"2021-12-31T14:55:19.9525Z","iopub.status.idle":"2021-12-31T14:55:20.082685Z","shell.execute_reply.started":"2021-12-31T14:55:19.952469Z","shell.execute_reply":"2021-12-31T14:55:20.081829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cm = pd.DataFrame(np.array(conf_mat), index = [i for i in classes],\n                  columns = [i for i in classes])\nplt.figure(figsize = (13,7))\nax = sn.heatmap(df_cm, annot=True)\nplt.title(\"Confusion Matrix\", fontsize=20)\nplt.ylabel(\"True Class\"     , fontsize=20)\nplt.xlabel(\"Predicted Class\", fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:55:21.703852Z","iopub.execute_input":"2021-12-31T14:55:21.704175Z","iopub.status.idle":"2021-12-31T14:55:21.98795Z","shell.execute_reply.started":"2021-12-31T14:55:21.704145Z","shell.execute_reply":"2021-12-31T14:55:21.987124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_val[1].shape\nmodel.predict(x_val[1].reshape((1,d)))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:55:30.864612Z","iopub.execute_input":"2021-12-31T14:55:30.865045Z","iopub.status.idle":"2021-12-31T14:55:30.913701Z","shell.execute_reply.started":"2021-12-31T14:55:30.865009Z","shell.execute_reply":"2021-12-31T14:55:30.912725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the function that predicts text for the given audio:","metadata":{}},{"cell_type":"code","source":"def predict(audio):\n    print(samples.shape)\n    prob=model.predict(audio)\n    index=np.argmax(prob[0])\n    return classes[index]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:44:10.65094Z","iopub.execute_input":"2021-12-31T14:44:10.65126Z","iopub.status.idle":"2021-12-31T14:44:10.656098Z","shell.execute_reply.started":"2021-12-31T14:44:10.65123Z","shell.execute_reply":"2021-12-31T14:44:10.655066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prediction time! Make predictions on the validation data:","metadata":{}},{"cell_type":"code","source":"import random\nindex=random.randint(0,len(x_val)-1)\nprint(index)\nsamples=x_val[index]\nprint(\"Audio:\",classes[np.argmax(y_val[index])])\n#ipd.Audio(np.array(all_wave)[index,:], rate=16000)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:55:58.256778Z","iopub.execute_input":"2021-12-31T14:55:58.257109Z","iopub.status.idle":"2021-12-31T14:55:58.262873Z","shell.execute_reply.started":"2021-12-31T14:55:58.25708Z","shell.execute_reply":"2021-12-31T14:55:58.261817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Text:\",predict(samples.reshape(1,d)))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:55:59.68063Z","iopub.execute_input":"2021-12-31T14:55:59.681233Z","iopub.status.idle":"2021-12-31T14:55:59.726068Z","shell.execute_reply.started":"2021-12-31T14:55:59.681195Z","shell.execute_reply":"2021-12-31T14:55:59.725121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\nmodel.save(\"4W.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-12-31T13:58:53.968188Z","iopub.execute_input":"2021-12-31T13:58:53.968512Z","iopub.status.idle":"2021-12-31T13:58:54.006673Z","shell.execute_reply.started":"2021-12-31T13:58:53.968478Z","shell.execute_reply":"2021-12-31T13:58:54.005901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:25:10.189247Z","iopub.execute_input":"2021-12-31T14:25:10.189599Z","iopub.status.idle":"2021-12-31T14:25:10.218025Z","shell.execute_reply.started":"2021-12-31T14:25:10.189567Z","shell.execute_reply":"2021-12-31T14:25:10.216873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}