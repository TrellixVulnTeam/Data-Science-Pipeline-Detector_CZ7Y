{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Light-Weight CNN Tutorial (KOR)\n## Origin: https://www.kaggle.com/alphasis/light-weight-cnn-lb-0-74\n### Translated by siryuon","metadata":{}},{"cell_type":"markdown","source":"본 노트북의 목표 -> 경량 CNN 구축\n본 노트북에서의 Input -> resampling된 .wav file(rate 8000)의 specgrams\n\n본 캐글 노트북은 하드웨어 제한으로 인해 원래의 코드와는 다른 버전.\n\n본 노트북으로 얻은 결과를 똑같이 얻으려면, epoch를 5로 설정, chop_audio(num=1000)을 설정하고, 모든 Conv Layer의 매개변수를 두 배 늘려야 함.\n\n본 노트북은 Alex Ozerin의 baseline보다 약간 개선되었지만, 원본 .wav file(rate 16000)을 사용하면 더 높은 점수를 얻을 수 있을 것.","metadata":{}},{"cell_type":"markdown","source":"경량 CNN이기 때문에 성능이 제한됨.\n\n성능을 높이기 위해 할 수 있는 방법들\n  - resampling file 대신 원본 wav file 사용\n  - chop_audio를 사용해 더 많은 'silence' wav file 만들기\n  -  더 깊은 CNN을 사용하거나 RNN 사용\n  -  더 긴 EPOCH으로 훈련","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom scipy.fftpack import fft\nfrom scipy.io import wavfile\nfrom scipy import signal\nfrom glob import glob\nimport re\nimport pandas as pd\nimport gc\nfrom scipy.io import wavfile\n\nfrom keras import optimizers, losses, activations, models\nfrom keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization\nfrom sklearn.model_selection import train_test_split\nimport keras","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Original Sample rate는 16000. 여기서는 8000으로 줄여준다.","metadata":{}},{"cell_type":"code","source":"L = 16000\nlegal_labels = 'yes no up down left right on off stop go silence unknown'.split()\n\n#src folders\nroot_path = r'..'\nout_path = r'.'\nmodel_path = r'.'\ntrain_data_path = os.path.join(root_path, 'input', 'train', 'audio')\ntest_data_path = os.path.join(root_path, 'input', 'test', 'audio')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DavidS가 작성했던 function인 custom_fft, log_specgram을 불러온다.  \nkaggle notebook: https://www.kaggle.com/davids1992/speech-representation-and-data-exploration\n\n위 노트북의 1.1절(log_specgram), 1.5절(custom_fft)을 참고.","metadata":{}},{"cell_type":"markdown","source":"데이터 세트를 8000으로 리샘플링 가능 -> 중요하지 않은 정보를 버려 데이터 크기를 줄인다. -> fft 사용","metadata":{}},{"cell_type":"code","source":"#FFT(Fast Fourier Transform)\ndef custom_fft(y, fs):\n    T = 1.0 / fs\n    N = y.shape[0]\n    yf = fft(y)\n    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n    # FFT is simmetrical, so we take just the first half\n    # FFT is also complex, to we take just the real part (abs)\n    vals = 2.0/N * np.abs(yf[0:N//2])\n    return xf, vals\n\n#specgram? -> 스펙트로그램(spectrogram), 음성 데이터를 처리할 때 많이 볼 수 있다.\n#파형과 스펙트럼의 특징이 결합된 것으로, x축은 시간, y축은 주파수, z축은 진폭을 나타냄. -> 소리의 스펙트럼을 시각화하여 그래프로 표현하는 기법\n\n#specgram을 계산하는 함수(로그를 취한다)\n#로그를 취함으로써 얻는 이점? -> plot을 훨씬 깔끔하게 할 수 있고, 사람들이 듣는 방식과 비슷하게 매칭이 될 것.\n#로그에 대한 입력 값으로 0이 없는지 확인해야 한다.\n\ndef log_specgram(audio, sample_rate, window_size=20,\n                 step_size=10, eps=1e-10):\n    nperseg = int(round(window_size * sample_rate / 1e3))\n    noverlap = int(round(step_size * sample_rate / 1e3))\n    freqs, times, spec = signal.spectrogram(audio,\n                                    fs=sample_rate,\n                                    window='hann',\n                                    nperseg=nperseg,\n                                    noverlap=noverlap,\n                                    detrend=False)\n    return freqs, times, np.log(spec.T.astype(np.float32) + eps)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:18:24.996934Z","iopub.execute_input":"2021-09-23T04:18:24.997268Z","iopub.status.idle":"2021-09-23T04:18:25.008377Z","shell.execute_reply.started":"2021-09-23T04:18:24.997237Z","shell.execute_reply":"2021-09-23T04:18:25.006877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train data folder 내의 모든 wav file을 가져오는 utility function","metadata":{}},{"cell_type":"code","source":"def list_wavs_fname(dirpath, ext='wav'):\n    print(dirpath)\n    fpaths = glob(os.path.join(dirpath, r'*/*' + ext))\n    pat = r'.+/(\\w+)/\\w+\\.' + ext + '$'\n    labels = []\n    for fpath in fpaths:\n        r = re.match(pat, fpath)\n        if r:\n            labels.append(r.group(1))\n    pat = r'.+/(\\w+\\.' + ext + ')$'\n    fnames = []\n    for fpath in fpaths:\n        r = re.match(pat, fpath)\n        if r:\n            fnames.append(r.group(1))\n    return labels, fnames","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"pad_audio function은 16000(1초) 미만의 오디오를 모두 동일한 길이로 만들기 위해 제로 패딩을 하는 함수.  \n\nchop_audio function는 길이가 16000보다 큰 오디오(ex. 배경 소음 폴더의 wav file)을 16000으로 잘라준다. 또한, 매개변수 'num'이 지정된 하나의 큰 wav file에서 여러 chunk를 생성.\n\nlabel_transform function은 레이블을 더미 값으로 변환. 레이블을 예측하기 위해 softmax와 함께 사용된다.","metadata":{}},{"cell_type":"code","source":"def pad_audio(samples):\n    if len(samples) >= L: return samples\n    else: return np.pad(samples, pad_width=(L - len(samples), 0), mode='constant', constant_values=(0, 0))\n\ndef chop_audio(samples, L=16000, num=20):\n    for i in range(num):\n        beg = np.random.randint(0, len(samples) - L)\n        yield samples[beg: beg + L]\n\ndef label_transform(labels):\n    nlabels = []\n    for label in labels:\n        if label == '_background_noise_':\n            nlabels.append('silence')\n        elif label not in legal_labels:\n            nlabels.append('unknown')\n        else:\n            nlabels.append(label)\n    return pd.get_dummies(pd.Series(nlabels))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"위에서 선언한 함수를 이용해 X_train과 y_train 생성.\n\nlabel_index는 pandas가 더미 값을 생성하는 데 사용하는 index이므로 나중에 사용하기 위해 저장해야함.","metadata":{}},{"cell_type":"code","source":"labels, fnames = list_wavs_fname(train_data_path)\n\nnew_sample_rate = 8000\ny_train = []\nx_train = []\n\nfor label, fname in zip(labels, fnames):\n    sample_rate, samples = wavfile.read(os.path.join(train_data_path, label, fname))\n    samples = pad_audio(samples)\n    if len(samples) > 16000:\n        n_samples = chop_audio(samples)\n    else: n_samples = [samples]\n    for samples in n_samples:\n        resampled = signal.resample(samples, int(new_sample_rate / sample_rate * samples.shape[0]))\n        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n        y_train.append(label)\n        x_train.append(specgram)\nx_train = np.array(x_train)\nx_train = x_train.reshape(tuple(list(x_train.shape) + [1]))\ny_train = label_transform(y_train)\nlabel_index = y_train.columns.values\ny_train = y_train.values\ny_train = np.array(y_train)\ndel labels, fnames\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CNN 만들기.\n\n생성된 specgram들은 (99, 81)의 shape인데, Conv2D 연산을 위해서는 모양을 변경해야 한다.","metadata":{}},{"cell_type":"code","source":"input_shape = (99, 81, 1)\nnclass = 12\ninp = Input(shape=input_shape)\nnorm_inp = BatchNormalization()(inp)\nimg_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(norm_inp)\nimg_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(img_1)\nimg_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\nimg_1 = Dropout(rate=0.2)(img_1)\nimg_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\nimg_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\nimg_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\nimg_1 = Dropout(rate=0.2)(img_1)\nimg_1 = Convolution2D(32, kernel_size=3, activation=activations.relu)(img_1)\nimg_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\nimg_1 = Dropout(rate=0.2)(img_1)\nimg_1 = Flatten()(img_1)\n\ndense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(img_1))\ndense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(dense_1))\ndense_1 = Dense(nclass, activation=activations.softmax)(dense_1)\n\nmodel = models.Model(inputs=inp, outputs=dense_1)\nopt = optimizers.Adam()\n\nmodel.compile(optimizer=opt, loss=losses.binary_crossentropy)\nmodel.summary()\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=2017)\nmodel.fit(x_train, y_train, batch_size=16, validation_data=(x_valid, y_valid), epochs=3, shuffle=True, verbose=2)\n\nmodel.save(os.path.join(model_path, 'cnn.model'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test data가 너무 커서 RAM에 잘 돌아가지 않으므로 하나씩 처리해야 한다.\n\nGenerator인 test_data_generator는 CNN에 넣을 테스트 wav file의 배치를 생성.","metadata":{}},{"cell_type":"code","source":"def test_data_generator(batch=16):\n    fpaths = glob(os.path.join(test_data_path, '*wav'))\n    i = 0\n    for path in fpaths:\n        if i == 0:\n            imgs = []\n            fnames = []\n        i += 1\n        rate, samples = wavfile.read(path)\n        samples = pad_audio(samples)\n        resampled = signal.resample(samples, int(new_sample_rate / rate * samples.shape[0]))\n        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n        imgs.append(specgram)\n        fnames.append(path.split('\\\\')[-1])\n        if i == batch:\n            i = 0\n            imgs = np.array(imgs)\n            imgs = imgs.reshape(tuple(list(imgs.shape) + [1]))\n            yield fnames, imgs\n    if i < batch:\n        imgs = np.array(imgs)\n        imgs = imgs.reshape(tuple(list(imgs.shape) + [1]))\n        yield fnames, imgs\n    raise StopIteration()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"훈련된 모델을 사용해 예측 진행.\n\nKaggle은 테스트 데이터 제공 x -> 진행 종료.\n\n이 노트북은 wav file의 Conv 연산에 대한 개괄적인 내용이라고 볼 수 있음.","metadata":{}}]}