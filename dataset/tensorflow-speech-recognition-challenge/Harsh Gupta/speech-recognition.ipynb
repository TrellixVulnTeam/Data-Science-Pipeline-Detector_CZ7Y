{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom glob import glob # file handling\nimport librosa # audio manipulation\nfrom sklearn.utils import shuffle # shuffling of data\nimport os # interation with the OS\nfrom random import sample # random selection\nfrom tqdm import tqdm\nfrom scipy import signal # audio processing\nfrom scipy.io import wavfile # reading the wavfile\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"PATH = '../input/train/audio/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_files(path):\n    # write the complete file loading function here, this will return\n    # a dataframe having files and labels\n    # loading the files\n    train_labels = os.listdir(PATH)\n    train_labels.remove('_background_noise_')\n    \n    labels_to_keep = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', '_background_noise_']\n\n    train_file_labels = dict()\n    for label in train_labels:\n        files = os.listdir(PATH + '/' + label)\n        for f in files:\n            train_file_labels[label + '/' + f] = label\n\n    train = pd.DataFrame.from_dict(train_file_labels, orient='index')\n    train = train.reset_index(drop=False)\n    train = train.rename(columns={'index': 'file', 0: 'folder'})\n    train = train[['folder', 'file']]\n    train = train.sort_values('file')\n    train = train.reset_index(drop=True)\n\n    def remove_label_from_file(label, fname):\n        return path + label + '/' + fname[len(label)+1:]\n\n    train['file'] = train.apply(lambda x: remove_label_from_file(*x), axis=1)\n    train['label'] = train['folder'].apply(lambda x: x if x in labels_to_keep else 'unknown')\n\n    labels_to_keep.append('unknown')\n\n    return train, labels_to_keep","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, labels_to_keep = load_files(PATH)\n\n# making word2id dict\nword2id = dict((c,i) for i,c in enumerate(sorted(labels_to_keep)))\n\n# get some files which will be labeled as unknown\nunk_files = train.loc[train['label'] == 'unknown']['file'].values\nunk_files = sample(list(unk_files), 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unk_files[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from glob import glob\nfiles = glob(PATH + '_bac*/*.wav')\nprint(files)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_sil = []\nfor s in files:\n    sr, audio = wavfile.read(s)\n    # converting the file into samples of 1 sec each\n    len_ = int(len(audio)/sr)\n    print(len_)\n    for i in range(len_-1):\n        sample_ = audio[i*sr:(i+1)*sr]\n        all_sil.append(sample_)\nprint(len(all_sil))\nprint(all_sil[0].shape)\nsil_data =  np.zeros((392, 16000, ))\nfor i,d in enumerate(all_sil):\n    sil_data[i] = d\nprint(sil_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_feature(path):\n\tX, sample_rate = librosa.load(path)\n\tstft = np.abs(librosa.stft(X))\n\tmfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n\tchroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n\tmel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n\tcontrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n\ttonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n\treturn mfccs,chroma,mel,contrast,tonnetz\n\ndef parse_audio_files(files, word2id, unk = False):\n    # n: number of classes\n    features = np.empty((0,193))\n    one_hot = np.zeros(shape = (len(files), word2id[max(word2id)]))\n    print(one_hot.shape)\n    for i in tqdm(range(len(files))):\n        f = files[i]\n        mfccs, chroma, mel, contrast,tonnetz = extract_feature(f)\n        ext_features = np.hstack([mfccs,chroma,mel,contrast,tonnetz])\n        features = np.vstack([features,ext_features])\n        if unk == True:\n            l = word2id['unknown']\n            one_hot[i][l] = 1.\n        else:\n            l = word2id[f.split('/')[-2]]\n            one_hot[i][l] = 1.\n    return np.array(features), one_hot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files = train.loc[train['label'] != 'unknown']['file'].values\nprint(len(files))\nprint(files[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# playing around with the data for now\ntrain_audio_path = '../input/train/audio/'\nfilename = '/tree/24ed94ab_nohash_0.wav' # --> 'Yes'\nsample_rate, audio = wavfile.read(str(train_audio_path) + filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pylpot as plt\nplt.figure(figsize = (15, 4))\nplt.plot(audio)\nipd.Audio(audio, rate=sample_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"audio_chunks = []\nn_chunks = int(audio.shape[0]/320)\nfor i in range(n_chunks):\n    chunk = audio[i*320: (i+1)*320]\n    audio_chunks.append(chunk)\naudio_chunk = np.array(audio_chunks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_specgram(audio, sample_rate, window_size=10,\n                 step_size=10, eps=1e-10):\n    nperseg = int(round(window_size * sample_rate / 1e3))\n    noverlap = int(round(step_size * sample_rate / 1e3))\n    _, _, spec = signal.spectrogram(audio,\n                                    fs=sample_rate,\n                                    window='hann',\n                                    nperseg=nperseg,\n                                    noverlap=noverlap,\n                                    detrend=False)\n    return np.log(spec.T.astype(np.float32) + eps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nspectrogram = log_specgram(audio, sample_rate, 10, 0)\nspec = spectrogram.T\nprint(spec.shape)\nplt.figure(figsize = (15,4))\nplt.imshow(spec, aspect='auto', origin='lower')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = sorted(labels_to_keep)\nword2id = dict((c,i) for i,c in enumerate(labels))\nlabel = train['label'].values\nlabel = [word2id[l] for l in label]\nprint(labels)\ndef make_one_hot(seq, n):\n    # n --> vocab size\n    seq_new = np.zeros(shape = (len(seq), n))\n    for i,s in enumerate(seq):\n        seq_new[i][s] = 1.\n    return seq_new\none_hot_l = make_one_hot(label, 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(one_hot_l[10:15])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_l[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"paths = []\nfolders = train['folder']\nfiles = train['file']\nfor i in range(len(files)):\n    path = '../input/train/audio/' + str(folders[i]) + '/' + str(files[i])\n    paths.append(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def audio_to_data(path):\n    # we take a single path and convert it into data\n    sample_rate, audio = wavfile.read(path)\n    spectrogram = log_specgram(audio, sample_rate, 10, 0)\n    return spectrogram.T\n\ndef paths_to_data(paths,labels):\n    data = np.zeros(shape = (len(paths), 81, 100))\n    indexes = []\n    for i in tqdm(range(len(paths))):\n        audio = audio_to_data(paths[i])\n        if audio.shape != (81,100):\n            indexes.append(i)\n        else:\n            data[i] = audio\n    final_labels = [l for i,l in enumerate(labels) if i not in indexes]\n    print('Number of instances with inconsistent shape:', len(indexes))\n    return data[:len(data)-len(indexes)], final_labels, indexes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d,l,indexes = paths_to_data(paths,one_hot_l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = np.zeros(shape = [d.shape[0], len(l[0])])\nfor i,array in enumerate(l):\n    for j, element in enumerate(array):\n        labels[i][j] = element\nprint(labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(d.shape)\nprint(labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d,labels = shuffle(d,labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(d[0].shape)\nprint(labels[0].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(256, input_shape = (81, 100)))\n# model.add(Dense(1028))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(12, activation = 'softmax'))\nmodel.compile(optimizer = 'Adam', loss = 'mean_squared_error', metrics = ['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(d, labels, batch_size = 1024, epochs = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}