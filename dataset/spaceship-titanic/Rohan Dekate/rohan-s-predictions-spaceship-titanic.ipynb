{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-08T12:44:07.738198Z","iopub.execute_input":"2022-06-08T12:44:07.738532Z","iopub.status.idle":"2022-06-08T12:44:07.770863Z","shell.execute_reply.started":"2022-06-08T12:44:07.738437Z","shell.execute_reply":"2022-06-08T12:44:07.769611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# File and Data Field Descriptions\n\n**train.csv** - Personal records for about two-thirds (~8700) of the passengers, to be used as training data.\n* PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n* HomePlanet - The planet the passenger departed from, typically their planet of permanent residence.\n* CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n* Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n* Destination - The planet the passenger will be debarking to.\n* Age - The age of the passenger.\n* VIP - Whether the passenger has paid for special VIP service during the voyage.\n* RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n* Name - The first and last names of the passenger.\n* Transported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.\n\n**test.csv** - Personal records for the remaining one-third (~4300) of the passengers, to be used as test data. Your task is to predict the value of Transported for the passengers in this set.\n\n**sample_submission.csv** - A submission file in the correct format.\n* PassengerId - Id for each passenger in the test set.\n* Transported - The target. For each passenger, predict either True or False.","metadata":{}},{"cell_type":"markdown","source":"# Import Data & Check Dataframe","metadata":{}},{"cell_type":"code","source":"# Import training files\ntrain = pd.read_csv('../input/spaceship-titanic/train.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:45:42.875992Z","iopub.execute_input":"2022-06-08T12:45:42.876311Z","iopub.status.idle":"2022-06-08T12:45:42.957782Z","shell.execute_reply.started":"2022-06-08T12:45:42.876278Z","shell.execute_reply":"2022-06-08T12:45:42.957113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:45:46.541797Z","iopub.execute_input":"2022-06-08T12:45:46.5421Z","iopub.status.idle":"2022-06-08T12:45:46.547809Z","shell.execute_reply.started":"2022-06-08T12:45:46.542063Z","shell.execute_reply":"2022-06-08T12:45:46.546998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:45:48.565628Z","iopub.execute_input":"2022-06-08T12:45:48.566199Z","iopub.status.idle":"2022-06-08T12:45:48.60176Z","shell.execute_reply.started":"2022-06-08T12:45:48.566155Z","shell.execute_reply":"2022-06-08T12:45:48.601104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Test Files\ntest = pd.read_csv('../input/spaceship-titanic/test.csv')\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:45:50.051396Z","iopub.execute_input":"2022-06-08T12:45:50.052244Z","iopub.status.idle":"2022-06-08T12:45:50.100883Z","shell.execute_reply.started":"2022-06-08T12:45:50.052184Z","shell.execute_reply":"2022-06-08T12:45:50.10009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:45:51.635573Z","iopub.execute_input":"2022-06-08T12:45:51.636362Z","iopub.status.idle":"2022-06-08T12:45:51.6427Z","shell.execute_reply.started":"2022-06-08T12:45:51.636304Z","shell.execute_reply":"2022-06-08T12:45:51.642074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:45:53.322937Z","iopub.execute_input":"2022-06-08T12:45:53.323916Z","iopub.status.idle":"2022-06-08T12:45:53.341113Z","shell.execute_reply.started":"2022-06-08T12:45:53.323849Z","shell.execute_reply":"2022-06-08T12:45:53.34004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install fast-ml library\n# Fast-ML is a Python package with numerous inbuilt functionalities to make the life of a data scientist much easier\n!pip install fast_ml","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:45:59.346887Z","iopub.execute_input":"2022-06-08T12:45:59.347939Z","iopub.status.idle":"2022-06-08T12:46:11.057645Z","shell.execute_reply.started":"2022-06-08T12:45:59.347892Z","shell.execute_reply":"2022-06-08T12:46:11.05657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* df : Dataframe, refers to dataset used for analysis\n* variable : str, refers to a single variable. As required in the function it has to be passed ex 'V1'\n* variables : list type, refers to list of variables. Must be passed as list ex ['V1', 'V2]. Even a single variable has to be passed in list format. ex ['V1']\n* target : str, refers to target variable\n* model : str, ML problem type. use 'classification' or 'clf' for classification problems and 'regression' or 'reg' for regression problems\n* method : str, refers to various techniques available for Missing Value Imputation, Feature Engieering... as available in each module","metadata":{}},{"cell_type":"code","source":"# EDA using Fast-ML\nfrom fast_ml import eda\n\n# One of the most useful dataframe summary view\n# Returns a dataframe with useful summary - variables, datatype, number of unique values, \n# sample of unique values, missing count, missing percent\neda.df_info(train)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:47:04.155037Z","iopub.execute_input":"2022-06-08T12:47:04.155387Z","iopub.status.idle":"2022-06-08T12:47:05.419193Z","shell.execute_reply.started":"2022-06-08T12:47:04.155347Z","shell.execute_reply":"2022-06-08T12:47:05.418246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Data Copy","metadata":{}},{"cell_type":"code","source":"# Create a copy of train\ntrain1 = train\ntrain1.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:47:10.994556Z","iopub.execute_input":"2022-06-08T12:47:10.994855Z","iopub.status.idle":"2022-06-08T12:47:11.016883Z","shell.execute_reply.started":"2022-06-08T12:47:10.994822Z","shell.execute_reply":"2022-06-08T12:47:11.015978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a copy of test\ntest1 = test\ntest1.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:47:13.862838Z","iopub.execute_input":"2022-06-08T12:47:13.863593Z","iopub.status.idle":"2022-06-08T12:47:13.886008Z","shell.execute_reply.started":"2022-06-08T12:47:13.863546Z","shell.execute_reply":"2022-06-08T12:47:13.885353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"# Import Library\nfrom pandas_profiling import ProfileReport\n\n# Perform Pandas Profiling\ntrain_profile = ProfileReport(train1, title=\"EDA-Spaceship Titanic Training Data\")\ntrain_profile","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:48:09.854212Z","iopub.execute_input":"2022-06-08T12:48:09.854532Z","iopub.status.idle":"2022-06-08T12:48:32.465031Z","shell.execute_reply.started":"2022-06-08T12:48:09.854495Z","shell.execute_reply":"2022-06-08T12:48:32.464337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_profile = ProfileReport(test1, title=\"EDA-Spaceship Titanic Test Data\")\ntest_profile","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:48:57.856932Z","iopub.execute_input":"2022-06-08T12:48:57.857241Z","iopub.status.idle":"2022-06-08T12:49:17.982394Z","shell.execute_reply.started":"2022-06-08T12:48:57.85721Z","shell.execute_reply":"2022-06-08T12:49:17.980335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EDA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nsns.countplot(train1['HomePlanet'], hue=train1['Transported']);","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:54:38.429144Z","iopub.execute_input":"2022-06-08T12:54:38.429513Z","iopub.status.idle":"2022-06-08T12:54:38.684767Z","shell.execute_reply.started":"2022-06-08T12:54:38.429477Z","shell.execute_reply":"2022-06-08T12:54:38.684099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(train1['CryoSleep'], hue=train1['Transported']);","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:54:40.245763Z","iopub.execute_input":"2022-06-08T12:54:40.246482Z","iopub.status.idle":"2022-06-08T12:54:40.498973Z","shell.execute_reply.started":"2022-06-08T12:54:40.24643Z","shell.execute_reply":"2022-06-08T12:54:40.497884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(train1['Destination'], hue=train1['Transported']);","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:54:44.952157Z","iopub.execute_input":"2022-06-08T12:54:44.952439Z","iopub.status.idle":"2022-06-08T12:54:45.1968Z","shell.execute_reply.started":"2022-06-08T12:54:44.952408Z","shell.execute_reply":"2022-06-08T12:54:45.19583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(train1['Age']);","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:54:46.151174Z","iopub.execute_input":"2022-06-08T12:54:46.151446Z","iopub.status.idle":"2022-06-08T12:54:46.496954Z","shell.execute_reply.started":"2022-06-08T12:54:46.151419Z","shell.execute_reply":"2022-06-08T12:54:46.496244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(train1['VIP'], hue=train1['Transported']);","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:54:51.006198Z","iopub.execute_input":"2022-06-08T12:54:51.006875Z","iopub.status.idle":"2022-06-08T12:54:51.259141Z","shell.execute_reply.started":"2022-06-08T12:54:51.006831Z","shell.execute_reply":"2022-06-08T12:54:51.25748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(train1['Transported']);","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:54:53.046327Z","iopub.execute_input":"2022-06-08T12:54:53.047191Z","iopub.status.idle":"2022-06-08T12:54:53.213239Z","shell.execute_reply.started":"2022-06-08T12:54:53.047135Z","shell.execute_reply":"2022-06-08T12:54:53.212168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Treatment","metadata":{}},{"cell_type":"code","source":"# No need for PassengerId and Name as they won't help with model building\ntrain1 = train1.drop(['PassengerId','Name'], axis=1)\ntrain1.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:06.830324Z","iopub.execute_input":"2022-06-08T12:55:06.831343Z","iopub.status.idle":"2022-06-08T12:55:06.854125Z","shell.execute_reply.started":"2022-06-08T12:55:06.831292Z","shell.execute_reply":"2022-06-08T12:55:06.85284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform same operation on test dataframe\ntest1 = test1.drop(['PassengerId','Name'], axis=1)\ntest1.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:08.213481Z","iopub.execute_input":"2022-06-08T12:55:08.213756Z","iopub.status.idle":"2022-06-08T12:55:08.233886Z","shell.execute_reply.started":"2022-06-08T12:55:08.213725Z","shell.execute_reply":"2022-06-08T12:55:08.232936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add the amount spent by the passengers in RoomService, FoodCourt, ShoppingMall, Spa, VRDeck in a separate column\ntrain1['AmountSpent'] = train1['RoomService']+train1['FoodCourt']+train1['ShoppingMall']+train1['Spa']+train1['VRDeck']\n# Drop these columns since we have captured the data in them in AmountSpent column\ntrain1 = train1.drop(['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'], axis=1)\ntrain1.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:12.4697Z","iopub.execute_input":"2022-06-08T12:55:12.469992Z","iopub.status.idle":"2022-06-08T12:55:12.491507Z","shell.execute_reply.started":"2022-06-08T12:55:12.46996Z","shell.execute_reply":"2022-06-08T12:55:12.490632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(train1['AmountSpent']);","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:13.954518Z","iopub.execute_input":"2022-06-08T12:55:13.954793Z","iopub.status.idle":"2022-06-08T12:55:14.342107Z","shell.execute_reply.started":"2022-06-08T12:55:13.954764Z","shell.execute_reply":"2022-06-08T12:55:14.341074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add the amount spent by the passengers in RoomService, FoodCourt, ShoppingMall, Spa, VRDeck in a separate column\ntest1['AmountSpent'] = test1['RoomService']+test1['FoodCourt']+test1['ShoppingMall']+test1['Spa']+test1['VRDeck']\n# Drop these columns since we have captured the data in them in AmountSpent column\ntest1 = test1.drop(['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'], axis=1)\ntest1.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:17.250941Z","iopub.execute_input":"2022-06-08T12:55:17.251283Z","iopub.status.idle":"2022-06-08T12:55:17.272555Z","shell.execute_reply.started":"2022-06-08T12:55:17.251248Z","shell.execute_reply":"2022-06-08T12:55:17.271565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many unique cabin entires are there\ntrain1.Cabin.unique","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:18.596323Z","iopub.execute_input":"2022-06-08T12:55:18.596857Z","iopub.status.idle":"2022-06-08T12:55:18.604806Z","shell.execute_reply.started":"2022-06-08T12:55:18.596819Z","shell.execute_reply":"2022-06-08T12:55:18.604126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split Cabin column into respective constituents in the training data\ncabin_details = train1['Cabin'].str.split('/',expand=True)\ncabin_details","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:22.539749Z","iopub.execute_input":"2022-06-08T12:55:22.540598Z","iopub.status.idle":"2022-06-08T12:55:22.572092Z","shell.execute_reply.started":"2022-06-08T12:55:22.540512Z","shell.execute_reply":"2022-06-08T12:55:22.571503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split Cabin column into respective constituents in the test data\ncabin_details_test = test['Cabin'].str.split('/',expand=True)\ncabin_details_test","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:23.359705Z","iopub.execute_input":"2022-06-08T12:55:23.36051Z","iopub.status.idle":"2022-06-08T12:55:23.380278Z","shell.execute_reply.started":"2022-06-08T12:55:23.360457Z","shell.execute_reply":"2022-06-08T12:55:23.379588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add the constituent columns to training data as separate columns\n# SeatNumber is not required for modelling\ntrain1['Deck']=cabin_details[0]\n#train1['SeatNumber']=cabin_details[1]\ntrain1['Side']=cabin_details[2] # P= Port, S=Starboard\n# Drop Cabin column\ntrain1=train1.drop(['Cabin'],axis=1)\ntrain1","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:33.953702Z","iopub.execute_input":"2022-06-08T12:55:33.954027Z","iopub.status.idle":"2022-06-08T12:55:33.983827Z","shell.execute_reply.started":"2022-06-08T12:55:33.953977Z","shell.execute_reply":"2022-06-08T12:55:33.983081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(train1['Deck'], hue=train1['Transported']);","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:56:03.329065Z","iopub.execute_input":"2022-06-08T12:56:03.329358Z","iopub.status.idle":"2022-06-08T12:56:03.635328Z","shell.execute_reply.started":"2022-06-08T12:56:03.329325Z","shell.execute_reply":"2022-06-08T12:56:03.634224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(train1['Side'], hue=train1['Transported']);","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:56:34.779303Z","iopub.execute_input":"2022-06-08T12:56:34.780077Z","iopub.status.idle":"2022-06-08T12:56:34.996111Z","shell.execute_reply.started":"2022-06-08T12:56:34.780033Z","shell.execute_reply":"2022-06-08T12:56:34.995417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add the constituent columns to test data as separate columns\n# SeatNumber is not required for modelling\ntest1['Deck']=cabin_details_test[0]\n#test1['SeatNumber']=cabin_details_test[1]\ntest1['Side']=cabin_details_test[2] # P= Port, S=Starboard\n# Drop Cabin column\ntest1=test1.drop(['Cabin'],axis=1)\ntest1","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:42.698705Z","iopub.execute_input":"2022-06-08T12:55:42.699006Z","iopub.status.idle":"2022-06-08T12:55:42.725627Z","shell.execute_reply.started":"2022-06-08T12:55:42.698974Z","shell.execute_reply":"2022-06-08T12:55:42.724714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Data Treatment","metadata":{}},{"cell_type":"code","source":"eda.df_info(train1)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:57:21.947731Z","iopub.execute_input":"2022-06-08T12:57:21.948002Z","iopub.status.idle":"2022-06-08T12:57:21.999421Z","shell.execute_reply.started":"2022-06-08T12:57:21.947971Z","shell.execute_reply":"2022-06-08T12:57:21.998328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute Age with median, AmountSpent with median in training data\n# Import Library\nfrom sklearn.impute import SimpleImputer\n\n# Create Object\nmedian_impute = SimpleImputer(missing_values=np.nan, strategy='median')\n\n# Impute Age column\n#Simple imputer expects a column vector, so converting the pandas Series\ntrain1['Age'] = median_impute.fit_transform(train1['Age'].to_numpy().reshape(-1,1))\n\n# Prevent leakage - use same median to impute test data column\ntest1['Age'] = median_impute.transform(test1['Age'].to_numpy().reshape(-1,1))\n\n# Impute AmountSpent Column\ntrain1['AmountSpent'] = median_impute.fit_transform(train1['AmountSpent'].to_numpy().reshape(-1,1))\ntest1['AmountSpent'] = median_impute.transform(test1['AmountSpent'].to_numpy().reshape(-1,1))\n\n# For categorical columns we create a separate class 'Unknown' for missing values or nan\n# Source https://jamesrledoux.com/code/imputation\ntrain1 = train1.fillna(\"Unknown\")\ntest1 = test1.fillna(\"Unknown\")\n\n# Check if imputation worked in training data\neda.df_info(train1)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:57:38.148522Z","iopub.execute_input":"2022-06-08T12:57:38.148851Z","iopub.status.idle":"2022-06-08T12:57:38.524311Z","shell.execute_reply.started":"2022-06-08T12:57:38.148817Z","shell.execute_reply":"2022-06-08T12:57:38.523227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if imputation worked in test data\neda.df_info(test1)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:58:10.428631Z","iopub.execute_input":"2022-06-08T12:58:10.428926Z","iopub.status.idle":"2022-06-08T12:58:10.467981Z","shell.execute_reply.started":"2022-06-08T12:58:10.428895Z","shell.execute_reply":"2022-06-08T12:58:10.467089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encode Categorical Variables to Numerical","metadata":{}},{"cell_type":"code","source":"# Encode training data with dummies\ntrain2=pd.get_dummies(data=train1,drop_first=True)\ntrain2","metadata":{"execution":{"iopub.status.busy":"2022-06-08T13:19:50.56375Z","iopub.execute_input":"2022-06-08T13:19:50.564242Z","iopub.status.idle":"2022-06-08T13:19:50.609828Z","shell.execute_reply.started":"2022-06-08T13:19:50.564196Z","shell.execute_reply":"2022-06-08T13:19:50.609066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode training data with dummies\ntest2=pd.get_dummies(data=test1,drop_first=True)\ntest2","metadata":{"execution":{"iopub.status.busy":"2022-06-08T13:19:54.321132Z","iopub.execute_input":"2022-06-08T13:19:54.321748Z","iopub.status.idle":"2022-06-08T13:19:54.358443Z","shell.execute_reply.started":"2022-06-08T13:19:54.321696Z","shell.execute_reply":"2022-06-08T13:19:54.357618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Standardize Numerical Variables","metadata":{}},{"cell_type":"code","source":"# Segregate numerical variables\nnum_col = ['Age','AmountSpent']\n\n# Standardize Training Data\n\n# Import Library\nfrom sklearn.preprocessing import StandardScaler\n\n# Define method\nscaler = StandardScaler()\n\n# Perform standardization on training data\ntrain_scaled = scaler.fit_transform(train2[num_col])\n# Create dataframe\ntrain_scaled = pd.DataFrame(train_scaled, index=train2.index, columns=train2[num_col].columns)\n# Merge dataframe with training data\ntrain2 = train2.drop(num_col, axis = 1)\ntrain2 = pd.concat([train_scaled, train2], axis=1)\ntrain2.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T13:20:05.770821Z","iopub.execute_input":"2022-06-08T13:20:05.771393Z","iopub.status.idle":"2022-06-08T13:20:05.803986Z","shell.execute_reply.started":"2022-06-08T13:20:05.771354Z","shell.execute_reply":"2022-06-08T13:20:05.803186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform standardization on test data\ntest_scaled = scaler.transform(test2[num_col])\n# Create dataframe\ntest_scaled = pd.DataFrame(test_scaled, index=test2.index, columns=test2[num_col].columns)\n# Merge dataframe with training data\ntest2 = test2.drop(num_col, axis = 1)\ntest2 = pd.concat([test_scaled, test2], axis=1)\ntest2.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T13:21:21.651631Z","iopub.execute_input":"2022-06-08T13:21:21.651888Z","iopub.status.idle":"2022-06-08T13:21:21.68024Z","shell.execute_reply.started":"2022-06-08T13:21:21.65186Z","shell.execute_reply":"2022-06-08T13:21:21.679224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split Data into Train Test","metadata":{}},{"cell_type":"code","source":"# Split training data into dependent and independent variables\nX = train2.drop(columns = 'Transported', axis=1)\ny = train2['Transported']\n\n# Display training data file head\nX.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T13:21:29.6178Z","iopub.execute_input":"2022-06-08T13:21:29.618131Z","iopub.status.idle":"2022-06-08T13:21:29.63971Z","shell.execute_reply.started":"2022-06-08T13:21:29.618099Z","shell.execute_reply":"2022-06-08T13:21:29.638833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the data for training and testing out model\nfrom sklearn.model_selection import train_test_split \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T13:21:48.035589Z","iopub.execute_input":"2022-06-08T13:21:48.035868Z","iopub.status.idle":"2022-06-08T13:21:48.045785Z","shell.execute_reply.started":"2022-06-08T13:21:48.035839Z","shell.execute_reply":"2022-06-08T13:21:48.045108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if target variable is balanced\ny.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T13:21:51.423946Z","iopub.execute_input":"2022-06-08T13:21:51.424215Z","iopub.status.idle":"2022-06-08T13:21:51.431809Z","shell.execute_reply.started":"2022-06-08T13:21:51.424187Z","shell.execute_reply":"2022-06-08T13:21:51.430924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Target variable seems pretty balanced so no up/down sampling is necessary","metadata":{}},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"code","source":"import time\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n#from sklearn.metrics import f1_score\n\n# Define Function to Test Multiple Classification Models and Output Performance Metrics\ndef fit_n_print(model, X_train, X_test, y_train, y_test):  # take the model, train data and test data as input\n    \n    start = time.time()  # note the start time of training\n    \n    model.fit(X_train, y_train)   # fit the model using the train data\n    pred = model.predict(X_test)     # model predictions on the test data\n    \n    stop = time.time() # note end time for training\n    \n    duration = stop - start  # calculate the total duration\n    train_acc = model.score(X_train,y_train) # training accuracy score\n    test_acc = model.score(X_test,y_test) # test accuracy score\n    kfold = KFold(n_splits=10) # Perform cross validation\n    results = cross_val_score(model,X_train, y_train, cv=kfold)\n    mean_train_acc = np.mean(abs(results))\n    std_train_acc = results.std()\n    #f1score = f1_score(y_test,pred)\n           \n    return train_acc, mean_train_acc, std_train_acc, test_acc, duration, pred  # return all the metrics along with predictions\nprint(\"Function defined successfully!\")","metadata":{"execution":{"iopub.status.busy":"2022-06-08T13:54:31.470644Z","iopub.execute_input":"2022-06-08T13:54:31.470966Z","iopub.status.idle":"2022-06-08T13:54:31.48087Z","shell.execute_reply.started":"2022-06-08T13:54:31.470932Z","shell.execute_reply":"2022-06-08T13:54:31.479903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define Function to Create Model Objects from Algorithms and Output Final Results\ndef run_model(X_train, X_test, y_train, y_test):\n    \n    # Import Model Libraries\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.svm import SVC\n    import xgboost as xgb\n    from sklearn.ensemble import AdaBoostClassifier\n    from sklearn.ensemble import GradientBoostingClassifier\n    from sklearn.ensemble import BaggingClassifier\n\n    # Define Model Objects\n    logreg = LogisticRegression()\n    nb = GaussianNB()\n    sgd = SGDClassifier()\n    knn = KNeighborsClassifier()\n    dtree = DecisionTreeClassifier()\n    rfc = RandomForestClassifier()\n    svc = SVC()\n    xgbc = xgb.XGBClassifier()\n    ada_boost = AdaBoostClassifier()\n    gbcl = GradientBoostingClassifier()\n    bgcl = BaggingClassifier()\n    \n    result = {}   # Create an empty dictionary to later use to store metrics of each of the models\n    for model, name  in zip([logreg, nb, sgd, knn, dtree, rfc, svc,xgbc,ada_boost,gbcl,bgcl], \n                        ['Logistic Regression','Gaussian Naive Bayes', 'Stochastic Gradient Descent',\n                         'K-Nearest Neighbours', 'Decision Tree Classifier', 'Random Forest Classifier',\n                        'Support Vector Classifier','XGBoost Classifier','AdaBoost Classifier',\n                        'Gradient Boosting Classifier','Bagging Classifier']):\n        result[name] = fit_n_print(model,X_train, X_test, y_train, y_test)  \n    # store all the metrics in the result dict, with name as key\n    # make a dataframe out of the metrics from result dictionary \n    result1 = pd.DataFrame(np.array(list(result.values()))[:,:-1],    \n                       columns= ['Training Accuracy','CV-Mean Training Accuracy','CV-Sigma Training Accuracy', 'Test Accuracy', 'Duration'],\n                      index= result.keys())   # use the model names as index\n    result1.index.name = 'Model'   # name the index of the result1 dataframe as 'Model'\n    return result1\n\nbase_result = run_model(X_train, X_test, y_train, y_test)\nbase_result","metadata":{"execution":{"iopub.status.busy":"2022-06-08T13:54:34.172319Z","iopub.execute_input":"2022-06-08T13:54:34.173238Z","iopub.status.idle":"2022-06-08T13:55:25.555718Z","shell.execute_reply.started":"2022-06-08T13:54:34.173197Z","shell.execute_reply":"2022-06-08T13:55:25.554547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}