{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <b>1 <span style='color:lightseagreen'>|</span> Introduction</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.1 | Goal</b></p>\n</div>\n\nWelcome to the year 2912, where your data science skills are needed to solve a cosmic mystery. We've received a transmission from four lightyears away and things aren't looking good. The Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars. While rounding Alpha Centauri en route to its first destination—the torrid 55 Cancri E—the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!\n\nTo help rescue crews and retrieve the lost passengers, you are challenged to predict which passengers were transported by the anomaly using records recovered from the spaceship’s damaged computer system. Help save them and change history!","metadata":{"_uuid":"a66e5416-88ca-4896-a261-acb8f6cfeffc","_cell_guid":"206ebdaa-886e-428c-9765-f759cf3bdc1c","trusted":true}},{"cell_type":"code","source":"from IPython.display import clear_output\nimport os\nimport warnings\nfrom pathlib import Path\n\n# Basic libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nimport seaborn as sns\n\n# Clustering\nfrom sklearn.cluster import KMeans\n\n# Principal Component Analysis (PCA)\nfrom sklearn.decomposition import PCA\n\n#Mutual Information\nfrom sklearn.feature_selection import mutual_info_regression\n\n# Cross Validation\nfrom sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, learning_curve, train_test_split\n\n# Encoding\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom category_encoders import MEstimateEncoder\nfrom category_encoders import MEstimateEncoder\n\n# Algorithms\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\n# Optuna - Bayesian Optimization \nimport optuna\nfrom optuna.samplers import TPESampler\n\n# Plotly\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.offline as offline\nimport plotly.graph_objs as go\n\n# Spaceship Titanic Metric\nfrom sklearn.metrics import accuracy_score\n\nwarnings.filterwarnings('ignore')\n\ndef load_data():\n    data_dir = Path(\"../input/spaceship-titanic\")\n    df_train = pd.read_csv(data_dir / \"train.csv\")\n    df_test = pd.read_csv(data_dir / \"test.csv\")\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    return df\n\ndef plot_feature_importance(importance,names,model_type):\n    \n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n    \n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n    \n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    fi_df = fi_df[fi_df.feature_importance > 0]\n    fig = px.bar(fi_df, x='feature_names', y='feature_importance', color=\"feature_importance\",\n             color_continuous_scale='Blugrn')\n    # General Styling\n    fig.update_layout(height=400, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100,t=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Feature Importance Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)\n    fig.show()\n\ndf_data = load_data()\nclear_output()\npp.ProfileReport(df_data)","metadata":{"_uuid":"c3cc8e35-799d-4bec-8158-f09f9b71429a","_cell_guid":"c81a6bdb-b36f-4e80-814c-46ca194a47f1","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:31.607772Z","iopub.execute_input":"2022-04-17T00:31:31.608031Z","iopub.status.idle":"2022-04-17T00:31:54.16328Z","shell.execute_reply.started":"2022-04-17T00:31:31.608Z","shell.execute_reply":"2022-04-17T00:31:54.162338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>1.7 | Reducing Memory Usage</b></p>\n</div>\n\nIn order to not having **<span style='color:lightseagreen'>issues with memory</span>** in the kernel, we are going to reduce its memory usage with the following function. Below, we can appreciate that reduction was successful as we manage to make a **<span style='color:lightseagreen'>reduction of 20%</span>**.","metadata":{"_uuid":"41b7b3f0-2f7d-41c2-83ce-e4c3a8217883","_cell_guid":"f66d43eb-21b9-44f8-91f4-c7115988f7a1","trusted":true}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n \n    return df\n\ndf_data = reduce_mem_usage(df_data)","metadata":{"_uuid":"74997884-b360-4989-a1dc-4ed1a5cc049e","_cell_guid":"4401a8ba-ad9e-40b2-9cbd-452ceb2f3f2f","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.165266Z","iopub.execute_input":"2022-04-17T00:31:54.165707Z","iopub.status.idle":"2022-04-17T00:31:54.196057Z","shell.execute_reply.started":"2022-04-17T00:31:54.165658Z","shell.execute_reply":"2022-04-17T00:31:54.195285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>3 <span style='color:lightseagreen'>|</span> Missing Values</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.1 | Categorical Features</b></p>\n</div>\n\nFrom the starting profiling report we observe that there are plenty of features having missing values. We are going to focus on filling them along this section. We'll start with those belonging to object category. Let's take a quick look at those features.","metadata":{"_uuid":"894436ae-521d-4ec1-b2f3-a5fb8e06dfa4","_cell_guid":"1033c694-fc0c-4a81-be2f-5c54fe9a2f58","trusted":true}},{"cell_type":"code","source":"df_data.select_dtypes(['object']).head()","metadata":{"_uuid":"d950ed29-28ad-4067-b518-4474e7e5c9cc","_cell_guid":"9162af6c-2f97-4400-9a31-0878f0a9389b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.197403Z","iopub.execute_input":"2022-04-17T00:31:54.1982Z","iopub.status.idle":"2022-04-17T00:31:54.220505Z","shell.execute_reply.started":"2022-04-17T00:31:54.198157Z","shell.execute_reply":"2022-04-17T00:31:54.219828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.1 | HomePlanet\n\n> **<span style='color:gray'>HomePlanet description: the planet the passenger departed from, typically their planet of permanent residence.</span>**\n\nLet's focus first on HomePlanet. As it is shown in the report this feature is categorical, with three different values. Those are the following: Mars, Earth and Europa. We are going to calculate mode and we are going to fill missing values with it.","metadata":{"_uuid":"8d49796d-e39f-4751-9e75-92bd5940835d","_cell_guid":"fff800e2-8400-4c5e-a5e5-7f4d0ef7aa88","trusted":true}},{"cell_type":"code","source":"def filling_HomePlanet(df):\n    mode = df['HomePlanet'].value_counts().index[0]\n    df['HomePlanet'] = df['HomePlanet'].fillna(mode)\n    return df","metadata":{"_uuid":"76385be0-d2e8-4c7f-a1b7-82d0873a466f","_cell_guid":"2fb9b76c-8df8-47d7-9d9d-212f37b57f90","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.222386Z","iopub.execute_input":"2022-04-17T00:31:54.22306Z","iopub.status.idle":"2022-04-17T00:31:54.22782Z","shell.execute_reply.started":"2022-04-17T00:31:54.223021Z","shell.execute_reply":"2022-04-17T00:31:54.227092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.2 | CryoSleep\n\n> **<span style='color:gray'>CryoSleep description: indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.</span>**\n\nLet's focus now on CryoSleep. As it is shown in the report this feature is boolean. Due to the fact that, if passenger had elected to put himself into suspended animation rarely it would have a missing value, we are going to consider the option of replacing missing values with False in this case.","metadata":{"_uuid":"01a2dfc4-07d7-47a4-a086-9f200493b1fb","_cell_guid":"6e750b96-9da8-4730-ab5a-6c12f2fd106b","trusted":true}},{"cell_type":"code","source":"def filling_CryoSleep(df):\n    df['CryoSleep'] = df['CryoSleep'].fillna(False)\n    return df","metadata":{"_uuid":"a84838b2-38b3-4cbd-8af0-eeadea484ea3","_cell_guid":"91603584-826e-4478-b1e1-1523654e2349","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.229169Z","iopub.execute_input":"2022-04-17T00:31:54.229622Z","iopub.status.idle":"2022-04-17T00:31:54.236176Z","shell.execute_reply.started":"2022-04-17T00:31:54.229577Z","shell.execute_reply":"2022-04-17T00:31:54.235399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.3 | Cabin\n\n> **<span style='color:gray'>Cabin description: the cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.</span>**\n\nLet's focus now on Cabin. As it is shown in the report this feature is categorical. As it is almost impossible to estimate cabin number for a passenger with given format, we are going to split cabin number into three different features. Those are going to be describing: desk, number and side. Thus, we'll start Feature Engineering here (continued in detail subsequently). Next, we are going to replace missing values for deck type feature with F (most repeated value). Hereafter, we are going to fill side feature with most repeated value into decks of type F. Finally, we are going to fill cabin number with half of the maximum cabin number (as cabins belonging to one deck type could have more survival rate whether they are one of the first/last cabin).","metadata":{"_uuid":"9cfae2ea-984e-47ba-bcd7-30215da8b5d6","_cell_guid":"1de38f41-f223-4606-bee7-a8c4d0d671ad","trusted":true}},{"cell_type":"code","source":"def split_Cabin(df):\n    df['Deck'] = df['Cabin'].str.split(\"/\", n=2, expand=True)[0]\n    df['Number'] = df['Cabin'].str.split(\"/\", n=2, expand=True)[1]\n    df['Side'] = df['Cabin'].str.split(\"/\", n=2, expand=True)[2]\n    df.pop('Cabin')\n    return df\n\ndef filling_Cabin(df):\n    df['Deck'] = df['Deck'].fillna('F')\n    mode = df[df.Deck == 'F']['Side'].value_counts().index[0]\n    df['Side'] = mode\n    df['Number'] = df['Number'].astype(float)\n    df['Number'] = df['Number'].fillna(1796 / 2)\n    return df","metadata":{"_uuid":"f19185c0-bf8a-4547-ab66-313fd390f646","_cell_guid":"7512de7d-dcff-4771-a5b8-dedfde9a3f99","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.237506Z","iopub.execute_input":"2022-04-17T00:31:54.23808Z","iopub.status.idle":"2022-04-17T00:31:54.248393Z","shell.execute_reply.started":"2022-04-17T00:31:54.238041Z","shell.execute_reply":"2022-04-17T00:31:54.247578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.4 | Destination\n\n> **<span style='color:gray'>Destination description: the planet the passenger will be debarking to.</span>**\n\nLet's focus now on Destination. As it is shown in the report this feature is categorical. We are going to fill missing values with most repeated value.","metadata":{"_uuid":"5a378676-51e1-41a9-95b7-a99ed07ed591","_cell_guid":"83e1ab8d-e364-46bd-8e7a-71306c377638","trusted":true}},{"cell_type":"code","source":"def filling_Destination(df):\n    mode = df['Destination'].value_counts().index[0]\n    df['Destination'] = df['Destination'].fillna(mode)\n    return df","metadata":{"_uuid":"3dd12371-90c0-4a09-81aa-cc2159a387cc","_cell_guid":"a27be58c-194d-4c19-8ee6-1bb87524e0fb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.249923Z","iopub.execute_input":"2022-04-17T00:31:54.250377Z","iopub.status.idle":"2022-04-17T00:31:54.259155Z","shell.execute_reply.started":"2022-04-17T00:31:54.250337Z","shell.execute_reply":"2022-04-17T00:31:54.258218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.5 | VIP\n\n> **<span style='color:gray'>VIP description: whether the passenger has paid for special VIP service during the voyage.</span>**\n\nLet's focus now on VIP. It would seem strange that a customer who has paid for a VIP service deal has not been taken into account in the data collection. This is why I'm going to replace missing values with False.","metadata":{"_uuid":"a9014c8a-8b1e-4b54-b652-c3150cd58026","_cell_guid":"b6f30c5c-a7d6-4f24-824d-628e5cb8e832","trusted":true}},{"cell_type":"code","source":"def filling_VIP(df):\n    df['VIP'] = df['VIP'].fillna(False)\n    return df","metadata":{"_uuid":"27bb37cd-1ef1-465b-8439-06fa2b176470","_cell_guid":"748d7927-84ee-43c2-8218-6c0679b51dc8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.260486Z","iopub.execute_input":"2022-04-17T00:31:54.261033Z","iopub.status.idle":"2022-04-17T00:31:54.266277Z","shell.execute_reply.started":"2022-04-17T00:31:54.260991Z","shell.execute_reply":"2022-04-17T00:31:54.265523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.6 | Name\n\n> **<span style='color:gray'>Name description: the first and last names of the passenger.</span>**\n\nLastly, let's focus on Name Feature. We are going to replace it with None, as it is difficult to guess first and last name of a person as you could guess.","metadata":{"_uuid":"de5d2314-afbf-4171-af5b-7a3752c0e9c6","_cell_guid":"7a5dcb6a-2f94-4046-8ac5-188700acf93c","trusted":true}},{"cell_type":"code","source":"def filling_Name(df):\n    df['Name'] = df['Name'].fillna('None')\n    return df","metadata":{"_uuid":"33d5b681-af4d-484f-a243-235ab38c6802","_cell_guid":"e84358fd-06d4-41a2-89a1-e09c16ba5436","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.26756Z","iopub.execute_input":"2022-04-17T00:31:54.268099Z","iopub.status.idle":"2022-04-17T00:31:54.273363Z","shell.execute_reply.started":"2022-04-17T00:31:54.268063Z","shell.execute_reply":"2022-04-17T00:31:54.272595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.2 | Numerical Features</b></p>\n</div>\n\nFrom the starting profiling report we observe that there are quantitative features having missing values. We are going to focus on filling them along this section.\n\n### 3.2.1 | Age\n> **<span style='color:gray'>Age description: the age of the passenger.</span>**\n\nWe are going to start with Age Feature. We are going to replace missing values with median age.","metadata":{"_uuid":"1e896a3e-0816-4c99-9514-03ab18031d66","_cell_guid":"f8e8f354-8775-4ab0-b18f-7c7e45685874","trusted":true}},{"cell_type":"code","source":"def filling_Age(df):\n    median = df['Age'].describe()[5]\n    df['Age'] = df['Age'].fillna(median)\n    return df","metadata":{"_uuid":"fe61054f-2da8-4a4b-8d28-8862b51f8a7b","_cell_guid":"446a18c4-98f4-4715-9b23-0b34fdb02fa4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.276766Z","iopub.execute_input":"2022-04-17T00:31:54.277336Z","iopub.status.idle":"2022-04-17T00:31:54.282085Z","shell.execute_reply.started":"2022-04-17T00:31:54.2773Z","shell.execute_reply":"2022-04-17T00:31:54.281397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.2 | Luxury Features\n\n> **<span style='color:gray'>Luxury Features description: amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.</span>**\n\nNow, we are focusing into those VIP features that are related to the amount of money a passenger has paid. As it would be quite unusual to not have recorded payment data from a VIP passenger, we are going to consider that missing values refer to passengers who have not spent anything on those luxuries.","metadata":{"_uuid":"76791986-fe40-422a-b25d-7b6db374884f","_cell_guid":"830d1e8b-c44a-48b9-8355-957c61609acb","trusted":true}},{"cell_type":"code","source":"def filling_luxury_features(df):\n    luxury_features = ['RoomService','FoodCourt', 'ShoppingMall', 'Spa','VRDeck']\n    df[luxury_features] = df[luxury_features].fillna(0.0)\n    return df","metadata":{"_uuid":"f244ae67-ec44-4e80-b4cf-25b5ae829097","_cell_guid":"d468b5e9-4768-4e1c-ac11-cc75e82fa5e6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.283482Z","iopub.execute_input":"2022-04-17T00:31:54.284128Z","iopub.status.idle":"2022-04-17T00:31:54.289953Z","shell.execute_reply.started":"2022-04-17T00:31:54.284083Z","shell.execute_reply":"2022-04-17T00:31:54.289042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.3 | Missing Values Filling Function</b></p>\n</div>","metadata":{"_uuid":"ce22c0c8-d4c2-413c-89ed-45bba4177851","_cell_guid":"82190007-ed85-417b-bfff-bf83be867cee","trusted":true}},{"cell_type":"code","source":"def filling_numerical(df):\n    df = filling_Age(df)\n    df = filling_luxury_features(df)\n    return df\n\ndef filling_categorical(df):\n    df = filling_HomePlanet(df)\n    df = filling_CryoSleep(df)\n    df = split_Cabin(df)\n    df = filling_Cabin(df)\n    df = filling_Destination(df)\n    df = filling_VIP(df)\n    df = filling_Name(df)\n    return df\n\ndef filling_missing(df):\n    df = filling_categorical(df)\n    df = filling_numerical(df)\n    return df\n\ndf_data = filling_missing(df_data)","metadata":{"_uuid":"82e4d3cf-f796-4d7e-a08f-e247bd915587","_cell_guid":"a6d948a5-41ed-487f-b7ee-20f48f579d7e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.291572Z","iopub.execute_input":"2022-04-17T00:31:54.292253Z","iopub.status.idle":"2022-04-17T00:31:54.434176Z","shell.execute_reply.started":"2022-04-17T00:31:54.292185Z","shell.execute_reply":"2022-04-17T00:31:54.433424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>2 <span style='color:lightseagreen'>|</span> Exploratory Data Analysis</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.1 | Age Analysis</b></p>\n</div>\n\n### 2.1.1 | Age Distribution","metadata":{"_uuid":"b2e3576c-7105-42f9-b6b4-16e6e88b3488","_cell_guid":"23ef40dc-de7b-48ee-ae48-1bdaaa0b9c58","trusted":true}},{"cell_type":"code","source":"age_count = pd.DataFrame(df_data[df_data.Transported == False].Age.value_counts().sort_values()).reset_index()\nage_count.columns = ['age','count']\n\n# chart\nfig = make_subplots(rows=1, cols=2, column_widths=[0.5, 0.5], vertical_spacing=0.1, horizontal_spacing=0.1,\n                    subplot_titles=('Age Distribution over Training Set','Age Distribution over Test Set'))\n\n# Not transported People\nfig.add_trace(go.Bar(x=age_count['age'], y=age_count['count'], marker = dict(color=px.colors.sequential.Viridis[5]), name='Non Transported from Train'), row = 1, col = 1)\n\nage_count = pd.DataFrame(df_data[df_data.Transported == True].Age.value_counts().sort_values()).reset_index()\nage_count.columns = ['age','count']\n\nfig.add_trace(go.Bar(x=age_count['age'], y=age_count['count'], marker = dict(color=px.colors.qualitative.Plotly[4]), name='Transported from Train'), row = 1, col = 1)\n\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=1)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=1)\n\nage_count = pd.DataFrame(df_data[df_data.Transported.isnull() == True].Age.value_counts().sort_values()).reset_index()\nage_count.columns = ['age','count']\n\nfig.add_trace(go.Bar(x=age_count['age'], y=age_count['count'], marker = dict(color=px.colors.sequential.Viridis[5]), name='Age Distribution from Test'), row = 1, col = 2)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=2)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=2)\n\n# General Styling\nfig.update_layout(height=500, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Age Distribution Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=True)","metadata":{"_uuid":"5e972687-8b91-4ef9-ba03-3e573063cc65","_cell_guid":"cfd59639-c6eb-4fd1-8c2a-9521df19f891","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.435541Z","iopub.execute_input":"2022-04-17T00:31:54.435934Z","iopub.status.idle":"2022-04-17T00:31:54.509383Z","shell.execute_reply.started":"2022-04-17T00:31:54.435897Z","shell.execute_reply":"2022-04-17T00:31:54.508702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1.2 | Skewness and Kurtosis\n\n[Skewness and Kurtosis Tutorial](https://www.universoformulas.com/estadistica/descriptiva/asimetria-curtosis/)\n\nLet's now check **<span style='color:lightseagreen'>skewness</span>**. Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined.\n\nFor a unimodal distribution, negative skew commonly indicates that the tail is on the left side of the distribution, and positive skew indicates that the tail is on the right. In cases where one tail is long but the other tail is fat, skewness does not obey a simple rule. For example, a zero value means that the tails on both sides of the mean balance out overall; this is the case for a symmetric distribution, but can also be true for an asymmetric distribution where one tail is long and thin, and the other is short but fat. A skewness greater than 1 is generally judged to be skewed, so check mainly those greater than 1.","metadata":{"_uuid":"13e68d43-e62b-4554-ba51-aae5efb94dc3","_cell_guid":"f557d3b2-7c97-45e3-8c89-b05e83553e43","trusted":true}},{"cell_type":"code","source":"from scipy.stats import skew, kurtosis\nskew(df_data[df_data.Age.isnull() == False]['Age'])","metadata":{"_uuid":"66853ea9-e262-4b29-b8b0-10d0c48dfec2","_cell_guid":"d138b427-5b93-4f27-ab8b-1b09e0d8561f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.510561Z","iopub.execute_input":"2022-04-17T00:31:54.511338Z","iopub.status.idle":"2022-04-17T00:31:54.521356Z","shell.execute_reply.started":"2022-04-17T00:31:54.511301Z","shell.execute_reply":"2022-04-17T00:31:54.520391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now move into **<span style='color:lightseagreen'>kurtosis</span>**. In probability theory and statistics, kurtosis is a measure of the \"tailedness\" of the probability distribution of a real-valued random variable. Like skewness, kurtosis describes the shape of a probability distribution and there are different ways of quantifying it for a theoretical distribution and corresponding ways of estimating it from a sample from a population. Different measures of kurtosis may have different interpretations.\n\nThe standard measure of a distribution's kurtosis, originating with Karl Pearson, is a scaled version of the fourth moment of the distribution. This number is related to the tails of the distribution, not its peak; hence, the sometimes-seen characterization of kurtosis as \"peakedness\" is incorrect. For this measure, higher kurtosis corresponds to greater extremity of deviations (or outliers), and not the configuration of data near the mean.","metadata":{"_uuid":"f4e81273-8f15-402f-b88b-7dc730b8bf72","_cell_guid":"8b705c9d-7a3e-4875-bd1b-d946e6d870db","trusted":true}},{"cell_type":"code","source":"kurtosis(df_data[df_data.Age.isnull() == False]['Age'])","metadata":{"_uuid":"d95d49fa-32d3-47dd-94d3-05f508337010","_cell_guid":"17723166-73cf-4237-9c86-8d525db613bc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.523912Z","iopub.execute_input":"2022-04-17T00:31:54.524098Z","iopub.status.idle":"2022-04-17T00:31:54.533901Z","shell.execute_reply.started":"2022-04-17T00:31:54.524075Z","shell.execute_reply":"2022-04-17T00:31:54.532998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1.3 | Target vs Age\n\nIn the following chart we are going to plot transported mean per each of the ages. **<span style='color:lightseagreen'>Be careful</span>**, as we could observe previously in the histogram, **<span style='color:lightseagreen'>amounts of people with the same age vary from one age to another</span>**. Thus, results are more significantly reliable for those ages having high count values.","metadata":{"_uuid":"66125deb-7fe4-4b24-91e3-f14c41068f5f","_cell_guid":"b3c7baf4-36cf-4ba4-807d-54f3304c8558","trusted":true}},{"cell_type":"code","source":"temp = df_data[df_data.Age.isnull() == False][['Transported','Age']]\ntemp['Transported'].replace([False, True], [0,1], inplace = True)\ntemp['Transported'] = temp.groupby('Age')['Transported'].transform('mean')\n\nfig = px.scatter(temp, x='Age',y='Transported', color=\"Transported\", color_continuous_scale='Blugrn', size='Transported')\nfig.update_xaxes(showgrid = False, showline = True, gridwidth = 0.05, linecolor = 'gray', zeroline = False, linewidth = 2)\nfig.update_yaxes(showline = True, gridwidth = 0.05, linecolor = 'gray', linewidth = 2, zeroline = False)\n\n# General Styling\nfig.update_layout(height=400,\n              margin=dict(b=50,r=30,l=100,t=100),\n              title = \"<span style='font-size:36px; font-family:Times New Roman'>Transported Probability per Age Analysis</span>\",                  \n              plot_bgcolor='rgb(242,242,242)',\n              paper_bgcolor = 'rgb(242,242,242)',\n              font=dict(family=\"Times New Roman\", size= 14),\n              hoverlabel=dict(font_color=\"floralwhite\"),\n              showlegend=False)\nfig.show()","metadata":{"_uuid":"3ae07418-f645-4940-9e22-2a46dcd633fd","_cell_guid":"abeb313b-cd54-45a5-a86c-83eecc3c1421","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.535337Z","iopub.execute_input":"2022-04-17T00:31:54.535916Z","iopub.status.idle":"2022-04-17T00:31:54.659654Z","shell.execute_reply.started":"2022-04-17T00:31:54.535879Z","shell.execute_reply":"2022-04-17T00:31:54.659062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.2 | Room Service Analysis</b></p>\n</div>\n\n### 2.2.1 | Room Service Cumulative Distribution \n\nFrom the following chart we can conclude that most people do not spent money on Room Services. We can observe that the cumulative distribution tends to be almost an horizontal line when reaching 1000$. Before this amount curve is increasing. To sum up, the more money spent, the less amount of people.","metadata":{"_uuid":"5d1cf3fd-5b19-4b96-8e83-a9cf7b0f5fac","_cell_guid":"9790a02c-a107-4ee4-9265-62bfd9e4b372","trusted":true}},{"cell_type":"code","source":"# chart\nfig = make_subplots(rows=1, cols=2, column_widths=[0.5, 0.5], vertical_spacing=0.1, horizontal_spacing=0.1,\n                    subplot_titles=('RoomService Cumulative Distribution over Training Set','RoomService Cumulative Distribution over Test Set'))\n\nfig.add_trace(go.Histogram(x=df_data[df_data.Transported.isnull() == False]['RoomService'], marker = dict(color=px.colors.sequential.Viridis[5]), cumulative_enabled=True), row = 1, col = 1)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=1)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=1)\n\nfig.add_trace(go.Histogram(x=df_data[df_data.Transported.isnull() == True]['RoomService'], marker = dict(color=px.colors.sequential.Viridis[5]), cumulative_enabled=True), row = 1, col = 2)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=2)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=2)\n\n# General Styling\nfig.update_layout(height=400, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Room Service Cumulative Distribution Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_uuid":"ac31c0dd-6fbe-4de0-bdca-5e94b4736d0c","_cell_guid":"83e2d74f-c6e6-4ded-8dc9-2d1ba178f274","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-17T00:31:54.660723Z","iopub.execute_input":"2022-04-17T00:31:54.661206Z","iopub.status.idle":"2022-04-17T00:31:54.727081Z","shell.execute_reply.started":"2022-04-17T00:31:54.66117Z","shell.execute_reply":"2022-04-17T00:31:54.726296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.2 | Skewness and Kurtosis\n\nAs you can imagine, skewness and kurtosis are pretty predictable. Any idea of which kind of value are we going to obtain? For example, thinking about it, is well known that skewness is gonna be positive. Indeed, a large positive number (in terms of skewness values). Those are their respective values.","metadata":{"_uuid":"3783abc2-fac9-4498-b4e5-429b0d95aee4","_cell_guid":"cfd5523d-55ce-41e6-a266-02521ad9b4d2","trusted":true}},{"cell_type":"code","source":"print('Skewness: ', skew(df_data[df_data.RoomService.isnull() == False]['RoomService']))\nprint('Kurtosis: ', kurtosis(df_data[df_data.RoomService.isnull() == False]['RoomService']))","metadata":{"_uuid":"1ce77694-585a-46f3-9e85-b603998881b5","_cell_guid":"af47f21c-cde3-4575-9ec1-8cd840fa05d5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.72849Z","iopub.execute_input":"2022-04-17T00:31:54.728749Z","iopub.status.idle":"2022-04-17T00:31:54.741583Z","shell.execute_reply.started":"2022-04-17T00:31:54.728716Z","shell.execute_reply":"2022-04-17T00:31:54.740773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.3 | Target vs RoomService\n\nIn next chart we're going to plot the **<span style='color:lightseagreen'>mean value of being transported depending on which RoomService group a person belongs to</span>**. As we can observe below, people spending less money are more likely to be transported. By contrast, this value decreases when the amount of money spent on this kind of service increases.","metadata":{"_uuid":"3f3288ee-e8de-4b4a-b7fd-e086889baed7","_cell_guid":"b6292227-936d-4985-8b85-a9dc00915eac","trusted":true}},{"cell_type":"code","source":"temp = df_data[df_data.Transported.isnull() == False][['Transported','RoomService']].copy()\ntemp = temp[temp.RoomService.isnull() == False]\ntemp['Transported'].replace([False, True], [0,1], inplace = True)\ntemp['RoomService'] = pd.qcut(temp['RoomService'], 20, duplicates='drop')\ntemp['RoomService'] = temp['RoomService'].astype(str)\ntemp['Transported'] = temp.groupby('RoomService')['Transported'].transform('mean')\ntemp = pd.DataFrame(temp['RoomService'].unique(), temp['Transported'].unique()).reset_index()\ntemp.columns = ['Transported','RoomService']\ntemp = temp.sort_values(by='Transported')\n\nfig = px.bar(temp, x='RoomService',y='Transported', color=\"Transported\", color_continuous_scale = 'Blugrn')\nfig.update_xaxes(showgrid = False, showline = True, gridwidth = 0.05, linecolor = 'gray', zeroline = False, linewidth = 2)\nfig.update_yaxes(showline = True, gridwidth = 0.05, linecolor = 'gray', linewidth = 2, zeroline = False)\n\n# General Styling\nfig.update_layout(height=400,\n              margin=dict(b=50,r=30,l=100,t=100),\n              title = \"<span style='font-size:36px; font-family:Times New Roman'>Transported Probability per RoomService Analysis</span>\",                  \n              plot_bgcolor='rgb(242,242,242)',\n              paper_bgcolor = 'rgb(242,242,242)',\n              font=dict(family=\"Times New Roman\", size= 14),\n              hoverlabel=dict(font_color=\"floralwhite\"),\n              showlegend=False)\nfig.show()","metadata":{"_uuid":"80c06c6d-5cb2-4e96-9910-77f4193f27ff","_cell_guid":"253d0d25-4e05-487f-bce9-66c0906256b4","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.743041Z","iopub.execute_input":"2022-04-17T00:31:54.743323Z","iopub.status.idle":"2022-04-17T00:31:54.832092Z","shell.execute_reply.started":"2022-04-17T00:31:54.743288Z","shell.execute_reply":"2022-04-17T00:31:54.831289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.2 | Food Court Analysis</b></p>\n</div>\n\n### 2.2.1 | Room Service Cumulative Distribution \n\nFrom the following chart we can conclude that most people do not spent money on Food Court. We can observe that the cumulative distribution tends to be almost an horizontal line when reaching 1000$. Before this amount curve is increasing. To sum up, the more money spent, the less amount of people.","metadata":{"_uuid":"08ea90a3-0f7e-4673-b4ee-015908d19982","_cell_guid":"01de37a9-7032-4bb9-813b-7ce6cac1a432","trusted":true}},{"cell_type":"code","source":"# chart\nfig = make_subplots(rows=1, cols=2, column_widths=[0.5, 0.5], vertical_spacing=0.1, horizontal_spacing=0.1,\n                    subplot_titles=('Food Court Cumulative Distribution over Training Set','Food Court Cumulative Distribution over Test Set'))\n\nfig.add_trace(go.Histogram(x=df_data[df_data.Transported.isnull() == False]['FoodCourt'], marker = dict(color=px.colors.sequential.Viridis[5]), cumulative_enabled=True), row = 1, col = 1)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=1)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=1)\n\nfig.add_trace(go.Histogram(x=df_data[df_data.Transported.isnull() == True]['FoodCourt'], marker = dict(color=px.colors.sequential.Viridis[5]), cumulative_enabled=True), row = 1, col = 2)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=2)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=2)\n\n# General Styling\nfig.update_layout(height=400, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Food Court Cumulative Distribution Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_uuid":"dc2a4e2d-da81-4d28-943f-1935775faf65","_cell_guid":"c0104997-864d-4621-8456-7af9e2683c3c","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-17T00:31:54.833558Z","iopub.execute_input":"2022-04-17T00:31:54.833812Z","iopub.status.idle":"2022-04-17T00:31:54.897244Z","shell.execute_reply.started":"2022-04-17T00:31:54.833778Z","shell.execute_reply":"2022-04-17T00:31:54.896592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.2 | Skewness and Kurtosis\n\nAs you can imagine, skewness and kurtosis are pretty predictable. Any idea of which kind of value are we going to obtain? For example, thinking about it, is well known that skewness is gonna be positive. Indeed, a large positive number (in terms of skewness values). Those are their respective values.","metadata":{"_uuid":"82697479-92b1-4d79-8341-9138fd7d2192","_cell_guid":"234306b1-d038-47b3-a942-3e3e462efbd8","trusted":true}},{"cell_type":"code","source":"print('Skewness: ', skew(df_data[df_data.FoodCourt.isnull() == False]['FoodCourt']))\nprint('Kurtosis: ', kurtosis(df_data[df_data.FoodCourt.isnull() == False]['FoodCourt']))","metadata":{"_uuid":"15432394-c501-4077-b0f1-ead501e63bf6","_cell_guid":"f5780350-aea3-4457-9e6c-78cc42e09bb5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.898975Z","iopub.execute_input":"2022-04-17T00:31:54.899569Z","iopub.status.idle":"2022-04-17T00:31:54.91216Z","shell.execute_reply.started":"2022-04-17T00:31:54.899532Z","shell.execute_reply":"2022-04-17T00:31:54.911396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.3 | Target vs FoodCourt\n\nIn next chart we're going to plot the **<span style='color:lightseagreen'>mean value of being transported depending on which FoodCourt group a person belongs to</span>**. As we can observe below, result vary a bit from the one obtained for RoomService. In this case, people who has spent a few money are the ones having a lower Transported rate. On the other hand, people paying bigger amounts of money and people not paying anything are more likely to be transported.","metadata":{"_uuid":"68475a6f-e2e3-4066-a6ca-f0fb5da8e80d","_cell_guid":"a070b68e-8d44-4d32-8041-0f17e8c186a1","trusted":true}},{"cell_type":"code","source":"temp = df_data[df_data.Transported.isnull() == False][['Transported','FoodCourt']].copy()\ntemp = temp[temp.FoodCourt.isnull() == False]\ntemp['Transported'].replace([False, True], [0,1], inplace = True)\ntemp['FoodCourt'] = pd.qcut(temp['FoodCourt'], 20, duplicates='drop')\ntemp['FoodCourt'] = temp['FoodCourt'].astype(str)\ntemp['Transported'] = temp.groupby('FoodCourt')['Transported'].transform('mean')\ntemp = pd.DataFrame(temp['FoodCourt'].unique(), temp['Transported'].unique()).reset_index()\ntemp.columns = ['Transported','FoodCourt']\ntemp = temp.sort_values(by='Transported')\n\nfig = px.bar(temp, x='FoodCourt',y='Transported', color=\"Transported\", color_continuous_scale = 'Blugrn')\nfig.update_xaxes(showgrid = False, showline = True, gridwidth = 0.05, linecolor = 'gray', zeroline = False, linewidth = 2)\nfig.update_yaxes(showline = True, gridwidth = 0.05, linecolor = 'gray', linewidth = 2, zeroline = False)\n\n# General Styling\nfig.update_layout(height=400,\n              margin=dict(b=50,r=30,l=100,t=100),\n              title = \"<span style='font-size:36px; font-family:Times New Roman'>Transported Probability per FoodCourt Analysis</span>\",                  \n              plot_bgcolor='rgb(242,242,242)',\n              paper_bgcolor = 'rgb(242,242,242)',\n              font=dict(family=\"Times New Roman\", size= 14),\n              hoverlabel=dict(font_color=\"floralwhite\"),\n              showlegend=False)\nfig.show()","metadata":{"_uuid":"2e45752b-3912-47d5-8633-721658713b1a","_cell_guid":"b46dda52-9527-4e02-a2f7-5cc1cf60d2d9","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:54.91437Z","iopub.execute_input":"2022-04-17T00:31:54.915011Z","iopub.status.idle":"2022-04-17T00:31:55.015472Z","shell.execute_reply.started":"2022-04-17T00:31:54.914968Z","shell.execute_reply":"2022-04-17T00:31:55.014534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.3 | Shopping Mall Analysis</b></p>\n</div>\n\n### 2.3.1 | Shopping Mall Cumulative Distribution \n\nFrom the following chart we can conclude that most people do not spent money on Shopping Mall. We can observe that the cumulative distribution tends to be almost an horizontal line when reaching 1000$. Before this amount curve is increasing. To sum up, the more money spent, the less amount of people.","metadata":{"_uuid":"d6873f99-82a0-4f58-9875-be668c4f4a8e","_cell_guid":"bc463319-243d-4d28-a8f3-7374aa5d3587","trusted":true}},{"cell_type":"code","source":"# chart\nfig = make_subplots(rows=1, cols=2, column_widths=[0.5, 0.5], vertical_spacing=0.1, horizontal_spacing=0.1,\n                    subplot_titles=('Shopping Mall Cumulative Distribution over Training Set','Shopping Mall Cumulative Distribution over Test Set'))\n\nfig.add_trace(go.Histogram(x=df_data[df_data.Transported.isnull() == False]['ShoppingMall'], marker = dict(color=px.colors.sequential.Viridis[5]), cumulative_enabled=True), row = 1, col = 1)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=1)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=1)\n\nfig.add_trace(go.Histogram(x=df_data[df_data.Transported.isnull() == True]['ShoppingMall'], marker = dict(color=px.colors.sequential.Viridis[5]), cumulative_enabled=True), row = 1, col = 2)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=2)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=2)\n\n# General Styling\nfig.update_layout(height=400, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Shopping Mall Cumulative Distribution Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_uuid":"9fff6cf4-53bf-4077-802a-be6c49f325a4","_cell_guid":"40a5715d-167b-40f5-acfa-30d549c48b15","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-17T00:31:55.017052Z","iopub.execute_input":"2022-04-17T00:31:55.017397Z","iopub.status.idle":"2022-04-17T00:31:55.092554Z","shell.execute_reply.started":"2022-04-17T00:31:55.017358Z","shell.execute_reply":"2022-04-17T00:31:55.091648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3.2 | Skewness and Kurtosis\n\nAs you can imagine, skewness and kurtosis are pretty predictable. Any idea of which kind of value are we going to obtain? For example, thinking about it, is well known that skewness is gonna be positive. Indeed, a large positive number (in terms of skewness values). Those are their respective values.","metadata":{"_uuid":"9c2ed455-9afd-4d2b-a7de-2280504433c0","_cell_guid":"07528de1-0a4a-4873-9001-8b05917fd922","trusted":true}},{"cell_type":"code","source":"print('Skewness: ', skew(df_data[df_data.ShoppingMall.isnull() == False]['ShoppingMall']))\nprint('Kurtosis: ', kurtosis(df_data[df_data.ShoppingMall.isnull() == False]['ShoppingMall']))","metadata":{"_uuid":"cf04f8a0-fda1-46b1-b10e-a49d27aeb63f","_cell_guid":"7e20d840-eed6-41c0-8710-36ce97f1c765","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:55.094117Z","iopub.execute_input":"2022-04-17T00:31:55.094406Z","iopub.status.idle":"2022-04-17T00:31:55.10839Z","shell.execute_reply.started":"2022-04-17T00:31:55.094369Z","shell.execute_reply":"2022-04-17T00:31:55.107578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3.3 | Target vs ShoppingMall\n\nIn next chart we're going to plot the **<span style='color:lightseagreen'>mean value of being transported depending on which ShoppingMall group a person belongs to</span>**.","metadata":{"_uuid":"d44d6cec-dfde-45ba-8ebc-848ac32bc55a","_cell_guid":"9e1ace9f-b2f7-4381-beae-2cf19ea4d24b","trusted":true}},{"cell_type":"code","source":"temp = df_data[df_data.Transported.isnull() == False][['Transported','ShoppingMall']].copy()\ntemp = temp[temp.ShoppingMall.isnull() == False]\ntemp['Transported'].replace([False, True], [0,1], inplace = True)\ntemp['ShoppingMall'] = pd.qcut(temp['ShoppingMall'], 20, duplicates='drop')\ntemp['ShoppingMall'] = temp['ShoppingMall'].astype(str)\ntemp['Transported'] = temp.groupby('ShoppingMall')['Transported'].transform('mean')\ntemp = pd.DataFrame(temp['ShoppingMall'].unique(), temp['Transported'].unique()).reset_index()\ntemp.columns = ['Transported','ShoppingMall']\ntemp = temp.sort_values(by='Transported')\n\nfig = px.bar(temp, x='ShoppingMall',y='Transported', color=\"Transported\", color_continuous_scale = 'Blugrn')\nfig.update_xaxes(showgrid = False, showline = True, gridwidth = 0.05, linecolor = 'gray', zeroline = False, linewidth = 2)\nfig.update_yaxes(showline = True, gridwidth = 0.05, linecolor = 'gray', linewidth = 2, zeroline = False)\n\n# General Styling\nfig.update_layout(height=400,\n              margin=dict(b=50,r=30,l=100,t=100),\n              title = \"<span style='font-size:36px; font-family:Times New Roman'>Transported Probability per ShoppingMall Analysis</span>\",                  \n              plot_bgcolor='rgb(242,242,242)',\n              paper_bgcolor = 'rgb(242,242,242)',\n              font=dict(family=\"Times New Roman\", size= 14),\n              hoverlabel=dict(font_color=\"floralwhite\"),\n              showlegend=False)\nfig.show()","metadata":{"_uuid":"41879baa-2bb7-449e-866f-85b53fce47e6","_cell_guid":"88878af5-0c61-4999-9bed-095580b800ed","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:55.109824Z","iopub.execute_input":"2022-04-17T00:31:55.110083Z","iopub.status.idle":"2022-04-17T00:31:55.209871Z","shell.execute_reply.started":"2022-04-17T00:31:55.110048Z","shell.execute_reply":"2022-04-17T00:31:55.209135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.4 | Spa Analysis</b></p>\n</div>\n\n### 2.4.1 | Spa Cumulative Distribution \n\nFrom the following chart we can conclude that most people do not spent money on Spa. We can observe that the cumulative distribution tends to be almost an horizontal line when reaching 1000$. Before this amount curve is increasing. To sum up, the more money spent, the less amount of people.","metadata":{"_uuid":"a9ed888b-f384-47c6-9abb-09a100b76546","_cell_guid":"677d631e-ca7c-4f08-9f2d-0c6a8762a9c6","trusted":true}},{"cell_type":"code","source":"# chart\nfig = make_subplots(rows=1, cols=2, column_widths=[0.5, 0.5], vertical_spacing=0.1, horizontal_spacing=0.1,\n                    subplot_titles=('Spa Cumulative Distribution over Training Set','Spa Cumulative Distribution over Test Set'))\n\nfig.add_trace(go.Histogram(x=df_data[df_data.Transported.isnull() == False]['Spa'], marker = dict(color=px.colors.sequential.Viridis[5]), cumulative_enabled=True), row = 1, col = 1)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=1)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=1)\n\nfig.add_trace(go.Histogram(x=df_data[df_data.Transported.isnull() == True]['Spa'], marker = dict(color=px.colors.sequential.Viridis[5]), cumulative_enabled=True), row = 1, col = 2)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=2)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=2)\n\n# General Styling\nfig.update_layout(height=400, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Spa Cumulative Distribution Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_uuid":"1afd0524-9cd6-4bcf-ad0b-4caebd194d93","_cell_guid":"16202658-02c2-4adf-9d4d-0926cbe268c5","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-17T00:31:55.211184Z","iopub.execute_input":"2022-04-17T00:31:55.211619Z","iopub.status.idle":"2022-04-17T00:31:55.276296Z","shell.execute_reply.started":"2022-04-17T00:31:55.211581Z","shell.execute_reply":"2022-04-17T00:31:55.275566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4.2 | Skewness and Kurtosis\n\nAs you can imagine, skewness and kurtosis are pretty predictable. Any idea of which kind of value are we going to obtain? For example, thinking about it, is well known that skewness is gonna be positive. Indeed, a large positive number (in terms of skewness values). Those are their respective values.","metadata":{"_uuid":"4b0d32e1-a435-4776-8025-1bff68a20723","_cell_guid":"8888678d-5dfe-47c9-8def-6f4ad0e23a12","trusted":true}},{"cell_type":"code","source":"print('Skewness: ', skew(df_data[df_data.Spa.isnull() == False]['Spa']))\nprint('Kurtosis: ', kurtosis(df_data[df_data.Spa.isnull() == False]['Spa']))","metadata":{"_uuid":"cc5b303e-b923-4871-afab-dccf097a698f","_cell_guid":"538eb2e5-4766-4dc6-a8d1-2751dd8b6465","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:55.277731Z","iopub.execute_input":"2022-04-17T00:31:55.27816Z","iopub.status.idle":"2022-04-17T00:31:55.292194Z","shell.execute_reply.started":"2022-04-17T00:31:55.278123Z","shell.execute_reply":"2022-04-17T00:31:55.29147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4.3 | Target vs Spa\n\nIn next chart we're going to plot the **<span style='color:lightseagreen'>mean value of being transported depending on which Spa group a person belongs to</span>**.","metadata":{"_uuid":"f1bfab0e-0d78-45de-8709-f21f83728d4e","_cell_guid":"266a15e9-bd8b-4aa2-b226-589ab71de3a9","trusted":true}},{"cell_type":"code","source":"temp = df_data[df_data.Transported.isnull() == False][['Transported','Spa']].copy()\ntemp = temp[temp.Spa.isnull() == False]\ntemp['Transported'].replace([False, True], [0,1], inplace = True)\ntemp['Spa'] = pd.qcut(temp['Spa'], 20, duplicates='drop')\ntemp['Spa'] = temp['Spa'].astype(str)\ntemp['Transported'] = temp.groupby('Spa')['Transported'].transform('mean')\ntemp = pd.DataFrame(temp['Spa'].unique(), temp['Transported'].unique()).reset_index()\ntemp.columns = ['Transported','Spa']\ntemp = temp.sort_values(by='Transported')\n\nfig = px.bar(temp, x='Spa',y='Transported', color=\"Transported\", color_continuous_scale = 'Blugrn')\nfig.update_xaxes(showgrid = False, showline = True, gridwidth = 0.05, linecolor = 'gray', zeroline = False, linewidth = 2)\nfig.update_yaxes(showline = True, gridwidth = 0.05, linecolor = 'gray', linewidth = 2, zeroline = False)\n\n# General Styling\nfig.update_layout(height=400,\n              margin=dict(b=50,r=30,l=100,t=100),\n              title = \"<span style='font-size:36px; font-family:Times New Roman'>Transported Probability per Spa Analysis</span>\",                  \n              plot_bgcolor='rgb(242,242,242)',\n              paper_bgcolor = 'rgb(242,242,242)',\n              font=dict(family=\"Times New Roman\", size= 14),\n              hoverlabel=dict(font_color=\"floralwhite\"),\n              showlegend=False)\nfig.show()","metadata":{"_uuid":"c5372557-e35f-4838-95ea-e31950012947","_cell_guid":"93009844-81ee-4bcf-8b9a-3ac76f451404","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:55.297277Z","iopub.execute_input":"2022-04-17T00:31:55.297778Z","iopub.status.idle":"2022-04-17T00:31:55.395321Z","shell.execute_reply.started":"2022-04-17T00:31:55.297748Z","shell.execute_reply":"2022-04-17T00:31:55.394642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.5 | VRDeck Analysis</b></p>\n</div>\n\n### 2.5.1 | VRDeck Cumulative Distribution \n\nFrom the following chart we can conclude that most people do not spent money on VRDeck. We can observe that the cumulative distribution tends to be almost an horizontal line when reaching 1000$. Before this amount curve is increasing. To sum up, the more money spent, the less amount of people.","metadata":{"_uuid":"04f5b419-c5bf-4bd7-a8bd-9c40f75b707f","_cell_guid":"4ca8ab52-1c7d-44a1-8f60-e93106b8fbdc","trusted":true}},{"cell_type":"code","source":"# chart\nfig = make_subplots(rows=1, cols=2, column_widths=[0.5, 0.5], vertical_spacing=0.1, horizontal_spacing=0.1,\n                    subplot_titles=('VRDeck Cumulative Distribution over Training Set','VRDeck Cumulative Distribution over Test Set'))\n\nfig.add_trace(go.Histogram(x=df_data[df_data.Transported.isnull() == False]['VRDeck'], marker = dict(color=px.colors.sequential.Viridis[5]), cumulative_enabled=True), row = 1, col = 1)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=1)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=1)\n\nfig.add_trace(go.Histogram(x=df_data[df_data.Transported.isnull() == True]['VRDeck'], marker = dict(color=px.colors.sequential.Viridis[5]), cumulative_enabled=True), row = 1, col = 2)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=2)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=2)\n\n# General Styling\nfig.update_layout(height=400, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>VRDeck Cumulative Distribution Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_uuid":"c9fceec2-1cc2-4665-8548-cd83a4fd4ab0","_cell_guid":"64f8f7d4-ba66-46ad-bf19-5ffd1ab404f7","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-17T00:31:55.396592Z","iopub.execute_input":"2022-04-17T00:31:55.397008Z","iopub.status.idle":"2022-04-17T00:31:55.459027Z","shell.execute_reply.started":"2022-04-17T00:31:55.396973Z","shell.execute_reply":"2022-04-17T00:31:55.458215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5.2 | Skewness and Kurtosis\n\nAs you can imagine, skewness and kurtosis are pretty predictable. Any idea of which kind of value are we going to obtain? For example, thinking about it, is well known that skewness is gonna be positive. Indeed, a large positive number (in terms of skewness values). Those are their respective values.","metadata":{"_uuid":"7fd9d13b-a758-4014-af0c-ade9350794fe","_cell_guid":"19f03b02-38aa-418f-97a5-8bd682f05633","trusted":true}},{"cell_type":"code","source":"print('Skewness: ', skew(df_data[df_data.VRDeck.isnull() == False]['VRDeck']))\nprint('Kurtosis: ', kurtosis(df_data[df_data.VRDeck.isnull() == False]['VRDeck']))","metadata":{"_uuid":"c78293ce-76e7-42c8-8fac-0303ae78639f","_cell_guid":"7845db17-b907-44ea-8854-7c7dc153cc21","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:55.460378Z","iopub.execute_input":"2022-04-17T00:31:55.460624Z","iopub.status.idle":"2022-04-17T00:31:55.47363Z","shell.execute_reply.started":"2022-04-17T00:31:55.460591Z","shell.execute_reply":"2022-04-17T00:31:55.472837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5.3 | Target vs VRDeck\n\nIn next chart we're going to plot the **<span style='color:lightseagreen'>mean value of being transported depending on which VRDeck group a person belongs to</span>**.","metadata":{"_uuid":"5221a6e2-2880-423d-9a44-fb88af7b604f","_cell_guid":"229a9f15-2ac6-4282-af93-4c64c8b8b3cb","trusted":true}},{"cell_type":"code","source":"temp = df_data[df_data.Transported.isnull() == False][['Transported','VRDeck']].copy()\ntemp = temp[temp.VRDeck.isnull() == False]\ntemp['Transported'].replace([False, True], [0,1], inplace = True)\ntemp['VRDeck'] = pd.qcut(temp['VRDeck'], 20, duplicates='drop')\ntemp['VRDeck'] = temp['VRDeck'].astype(str)\ntemp['Transported'] = temp.groupby('VRDeck')['Transported'].transform('mean')\ntemp = pd.DataFrame(temp['VRDeck'].unique(), temp['Transported'].unique()).reset_index()\ntemp.columns = ['Transported','VRDeck']\ntemp = temp.sort_values(by='Transported')\n\nfig = px.bar(temp, x='VRDeck',y='Transported', color=\"Transported\", color_continuous_scale = 'Blugrn')\nfig.update_xaxes(showgrid = False, showline = True, gridwidth = 0.05, linecolor = 'gray', zeroline = False, linewidth = 2)\nfig.update_yaxes(showline = True, gridwidth = 0.05, linecolor = 'gray', linewidth = 2, zeroline = False)\n\n# General Styling\nfig.update_layout(height=400,\n              margin=dict(b=50,r=30,l=100,t=100),\n              title = \"<span style='font-size:36px; font-family:Times New Roman'>Transported Probability per VRDeck Analysis</span>\",                  \n              plot_bgcolor='rgb(242,242,242)',\n              paper_bgcolor = 'rgb(242,242,242)',\n              font=dict(family=\"Times New Roman\", size= 14),\n              hoverlabel=dict(font_color=\"floralwhite\"),\n              showlegend=False)\nfig.show()","metadata":{"_uuid":"edb8a77b-bca6-4c2b-bd13-c0cba6761a54","_cell_guid":"c5d81f31-d9b3-4837-aeb0-1cc05e9741fd","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:55.474957Z","iopub.execute_input":"2022-04-17T00:31:55.475283Z","iopub.status.idle":"2022-04-17T00:31:55.567624Z","shell.execute_reply.started":"2022-04-17T00:31:55.475249Z","shell.execute_reply":"2022-04-17T00:31:55.566916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.6 | VIP Analysis</b></p>\n</div>\n\nIn the chart I am showing below I've plotted both VIP feature value counts, and Transported mean per each of the possible values of VIP. As we can observe, VIP people are more likely to not being transported. By contrast, there is a 50% of chance that one Non-VIP person gets transported. On the left hand, we can appreciate that most people are Non-VIP.","metadata":{"_uuid":"bb147919-c6da-4b7f-9c6f-e71787cb704c","_cell_guid":"933da669-7773-4ae6-955f-9573f61631c2","trusted":true}},{"cell_type":"code","source":"temp = pd.DataFrame(df_data[df_data.VIP.isnull() == False]['VIP'].value_counts()).reset_index()\ntemp.columns = ['VIP', 'Count']\n\n# chart\nfig = make_subplots(rows=1, cols=2, column_widths=[0.5, 0.5], vertical_spacing=0.1, horizontal_spacing=0.1,\n                    subplot_titles=('VIP Count','Transported Mean vs VIP'))\n\nfig.add_trace(go.Bar(x=temp['VIP'], y=temp['Count'], marker = dict(color=[px.colors.sequential.Viridis[5],px.colors.qualitative.Plotly[4]])), row = 1, col = 1)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=1)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=1)\n\ntemp = df_data[(df_data.VIP.isnull() == False) & (df_data.Transported.isnull() == False)][['VIP','Transported']]\ntemp['Transported'] = temp.groupby('VIP')['Transported'].transform('mean')\ntemp = pd.DataFrame(temp['VIP'].unique(), temp['Transported'].unique()).reset_index()\ntemp.columns = ['Transported','VIP']\n\nfig.add_trace(go.Bar(x=temp['VIP'], y=temp['Transported'], marker = dict(color=[px.colors.sequential.Viridis[5],px.colors.qualitative.Plotly[4]])), row = 1, col = 2)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=2)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=2)\n\n# General Styling\nfig.update_layout(height=400, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>VIP Feature Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_uuid":"b5cc4345-1a28-4252-bee4-c6f46cea7bf9","_cell_guid":"874e33a7-59f0-4e23-948f-ac2f31f46a74","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:55.568925Z","iopub.execute_input":"2022-04-17T00:31:55.569376Z","iopub.status.idle":"2022-04-17T00:31:55.637827Z","shell.execute_reply.started":"2022-04-17T00:31:55.569335Z","shell.execute_reply":"2022-04-17T00:31:55.637011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.7 | HomePlanet Feature Analysis</b></p>\n</div>","metadata":{"_uuid":"7b807268-ff7e-4fb5-b257-8d69d0647ddb","_cell_guid":"6d1861e4-66b5-4559-9dfb-b1204f61f269","trusted":true}},{"cell_type":"code","source":"temp = pd.DataFrame(df_data[df_data.HomePlanet.isnull() == False]['HomePlanet'].value_counts()).reset_index()\ntemp.columns = ['HomePlanet', 'Count']\n\n# chart\nfig = make_subplots(rows=1, cols=2, column_widths=[0.5, 0.5], vertical_spacing=0.1, horizontal_spacing=0.1,\n                    subplot_titles=('HomePlanet Count','Transported Mean vs HomePlanet'))\n\nfig.add_trace(go.Bar(x=temp['HomePlanet'], y=temp['Count'], marker = dict(color=[px.colors.sequential.Blugrn[6], px.colors.sequential.Viridis[5],px.colors.qualitative.Plotly[4]])), row = 1, col = 1)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=1)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=1)\n\ntemp = df_data[(df_data.HomePlanet.isnull() == False) & (df_data.Transported.isnull() == False)][['HomePlanet','Transported']]\ntemp['Transported'] = temp.groupby('HomePlanet')['Transported'].transform('mean')\ntemp = pd.DataFrame(temp['HomePlanet'].unique(), temp['Transported'].unique()).reset_index()\ntemp.columns = ['Transported','HomePlanet']\n\nfig.add_trace(go.Bar(x=temp['HomePlanet'], y=temp['Transported'], marker = dict(color=[px.colors.sequential.Blugrn[6], px.colors.sequential.Viridis[5],px.colors.qualitative.Plotly[4]])), row = 1, col = 2)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=2)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=2)\n\n# General Styling\nfig.update_layout(height=400, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>HomePlanet Feature Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_uuid":"f4cecc40-fc17-4994-a780-e7de03e70ffb","_cell_guid":"856bf8c1-1b0e-4946-84f9-59a22d17b366","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:55.639264Z","iopub.execute_input":"2022-04-17T00:31:55.63951Z","iopub.status.idle":"2022-04-17T00:31:55.712293Z","shell.execute_reply.started":"2022-04-17T00:31:55.639477Z","shell.execute_reply":"2022-04-17T00:31:55.711647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.8 | Destination Feature Analysis</b></p>\n</div>","metadata":{"_uuid":"b74884ca-2cd4-4ceb-bb25-4648665d7045","_cell_guid":"ad9526bb-b14f-4c74-a4b4-5aad309ca97c","trusted":true}},{"cell_type":"code","source":"temp = pd.DataFrame(df_data[df_data.Destination.isnull() == False]['Destination'].value_counts()).reset_index()\ntemp.columns = ['Destination', 'Count']\n\n# chart\nfig = make_subplots(rows=1, cols=2, column_widths=[0.5, 0.5], vertical_spacing=0.1, horizontal_spacing=0.1,\n                    subplot_titles=('Destination Count','Transported Mean vs Destination'))\n\nfig.add_trace(go.Bar(x=temp['Destination'], y=temp['Count'], marker = dict(color=[px.colors.sequential.Blugrn[6], px.colors.sequential.Viridis[5],px.colors.qualitative.Plotly[4]])), row = 1, col = 1)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=1)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=1)\n\ntemp = df_data[(df_data.Destination.isnull() == False) & (df_data.Transported.isnull() == False)][['Destination','Transported']]\ntemp['Transported'] = temp.groupby('Destination')['Transported'].transform('mean')\ntemp = pd.DataFrame(temp['Destination'].unique(), temp['Transported'].unique()).reset_index()\ntemp.columns = ['Transported','Destination']\n\nfig.add_trace(go.Bar(x=temp['Destination'], y=temp['Transported'], marker = dict(color=[px.colors.sequential.Blugrn[6], px.colors.sequential.Viridis[5],px.colors.qualitative.Plotly[4]])), row = 1, col = 2)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=2)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=2)\n\n# General Styling\nfig.update_layout(height=400, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Destination Feature Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_uuid":"8ccf9ffd-81a5-480b-a401-436114b07bea","_cell_guid":"9501613b-573f-42ef-a6c5-52b8e6f1d48a","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:55.713603Z","iopub.execute_input":"2022-04-17T00:31:55.713833Z","iopub.status.idle":"2022-04-17T00:31:55.785779Z","shell.execute_reply.started":"2022-04-17T00:31:55.713801Z","shell.execute_reply":"2022-04-17T00:31:55.785155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.9 | CryoSleep Feature Analysis</b></p>\n</div>","metadata":{"_uuid":"0bb80507-0556-4604-a55d-c4e047823107","_cell_guid":"7d204de7-a447-4b9f-bdf4-03c82910c525","trusted":true}},{"cell_type":"code","source":"temp = pd.DataFrame(df_data[df_data.CryoSleep.isnull() == False]['CryoSleep'].value_counts()).reset_index()\ntemp.columns = ['CryoSleep', 'Count']\n\n# chart\nfig = make_subplots(rows=1, cols=2, column_widths=[0.5, 0.5], vertical_spacing=0.1, horizontal_spacing=0.1,\n                    subplot_titles=('CryoSleep Count','Transported Mean vs CryoSleep'))\n\nfig.add_trace(go.Bar(x=temp['CryoSleep'], y=temp['Count'], marker = dict(color=[px.colors.sequential.Viridis[5],px.colors.qualitative.Plotly[4]])), row = 1, col = 1)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=1)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=1)\n\ntemp = df_data[(df_data.CryoSleep.isnull() == False) & (df_data.Transported.isnull() == False)][['CryoSleep','Transported']]\ntemp['Transported'] = temp.groupby('CryoSleep')['Transported'].transform('mean')\ntemp = pd.DataFrame(temp['CryoSleep'].unique(), temp['Transported'].unique()).reset_index()\ntemp.columns = ['Transported','CryoSleep']\n\nfig.add_trace(go.Bar(x=temp['CryoSleep'], y=temp['Transported'], marker = dict(color=[px.colors.sequential.Viridis[5],px.colors.qualitative.Plotly[4]])), row = 1, col = 2)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=2)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=2)\n\n# General Styling\nfig.update_layout(height=400, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>CryoSleep Feature Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_uuid":"6277c1af-c782-4a52-a03c-a7655df35da8","_cell_guid":"fa96944b-6f21-4f25-be75-45a1c23f97f5","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:55.787769Z","iopub.execute_input":"2022-04-17T00:31:55.78802Z","iopub.status.idle":"2022-04-17T00:31:55.855155Z","shell.execute_reply.started":"2022-04-17T00:31:55.787988Z","shell.execute_reply":"2022-04-17T00:31:55.854549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.10 | Outliers</b></p>\n</div>\n\nBefore starting with this section, just mention that I have a massive Outlier Detection Tutorial here [Massiva PCA + Outlier Detection Tutorial](https://www.kaggle.com/code/javigallego/massive-pca-outlier-detection-tutorial#3-|-PCA-for-Data-Science). If you want to learn more about this, just check it.\n\n### 2.10.1 | Outliers Definition\nOutlier is an observation that is numerically distant from the rest of the data or in a simple word it is the value which is out of the range.let’s take an example to check what happens to a data set with and data set without outliers. Outlier can be of two types: Univariate and Multivariate. These outliers can be found when we look at distribution of a single variable. Multi-variate outliers are outliers in an n-dimensional space.\n\n### 2.10.2 | Univariate Outliers Detection\n\n We'll start by detecting whether there are outliers in our dataset or not. \n\n#### 2.10.2.1 | Grubbs Test\n\n$$\n\\begin{array}{l}{\\text { Grubbs' test is defined for the hypothesis: }} \\\\ {\\begin{array}{ll}{\\text { Ho: }}  {\\text { There are no outliers in the data set }} \\\\ {\\mathrm{H}_{\\mathrm{1}} :}  {\\text { There is exactly one outlier in the data set }}\\end{array}}\\end{array}\n$$\n$$\n\\begin{array}{l}{\\text {The Grubbs' test statistic is defined as: }} \\\\ {\\qquad G_{calculated}=\\frac{\\max \\left|X_{i}-\\overline{X}\\right|}{SD}} \\\\ {\\text { with } \\overline{X} \\text { and } SD \\text { denoting the sample mean and standard deviation, respectively. }} \\end{array}\n$$\n$$\nG_{critical}=\\frac{(N-1)}{\\sqrt{N}} \\sqrt{\\frac{\\left(t_{\\alpha /(2 N), N-2}\\right)^{2}}{N-2+\\left(t_{\\alpha /(2 N), N-2}\\right)^{2}}}\n$$\n$$\n\\begin{array}{l}{\\text { If the calculated value is greater than critical, you can reject the null hypothesis and conclude that one of the values is an outlier }}\\end{array}$$","metadata":{"_uuid":"265a6020-ca9a-4874-bc56-14b5c0314496","_cell_guid":"9568363f-4280-4297-9920-0150ed91dfa1","trusted":true}},{"cell_type":"code","source":"import scipy.stats as stats\ndef grubbs_test(x, feature):\n    n = len(x)\n    mean_x = np.mean(x)\n    sd_x = np.std(x)\n    numerator = max(abs(x-mean_x))\n    g_calculated = numerator/sd_x\n    print(\"Feature:\", feature)\n    print(\"Grubbs Calculated Value:\",g_calculated)\n    t_value = stats.t.ppf(1 - 0.05 / (2 * n), n - 2)\n    g_critical = ((n - 1) * np.sqrt(np.square(t_value))) / (np.sqrt(n) * np.sqrt(n - 2 + np.square(t_value)))\n    print(\"Grubbs Critical Value:\",g_critical)\n    if g_critical > g_calculated:\n        print(\"From grubbs_test we observe that calculated value is lesser than critical value, Accept null hypothesis and conclude that there is no outliers\\n\")\n    else:\n        print(\"From grubbs_test we observe that calculated value is greater than critical value, Reject null hypothesis and conclude that there is an outliers\\n\")\n    \n    print(\"==============================================================================================================================================\")\n    \nfor col in df_data.drop(['Age','Transported'],axis=1).select_dtypes(['float32']).columns:\n    grubbs_test(df_data[df_data.Transported.isnull() == False][col], col)","metadata":{"_uuid":"a392b4b1-d31c-4d7a-9504-6c55c17898f4","_cell_guid":"00c5f698-79a6-4086-a383-e9d78318096a","collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false},"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-17T00:31:55.856431Z","iopub.execute_input":"2022-04-17T00:31:55.856837Z","iopub.status.idle":"2022-04-17T00:31:55.894387Z","shell.execute_reply.started":"2022-04-17T00:31:55.856803Z","shell.execute_reply":"2022-04-17T00:31:55.893746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.10.2.3 | Z-score method\n\nUsing Z score method,we can find out how many standard deviations value away from the mean. \n\n![minipic](https://i.pinimg.com/originals/cd/14/73/cd1473c4c82980c6596ea9f535a7f41c.jpg)\n\n Figure in the left shows area under normal curve and how much area that standard deviation covers.\n* 68% of the data points lie between + or - 1 standard deviation.\n* 95% of the data points lie between + or - 2 standard deviation\n* 99.7% of the data points lie between + or - 3 standard deviation\n\n$\\begin{array}{l} {R.Z.score=\\frac{0.6745*( X_{i} - Median)}{MAD}}  \\end{array}$\n\nIf the z score of a data point is more than 3 (because it cover 99.7% of area), it indicates that the data value is quite different from the other values. It is taken as outliers.","metadata":{"_uuid":"4c99cba1-d83e-48b2-8eec-389b6ff15737","_cell_guid":"3b4aec77-74b6-4a5f-ae44-35c71bbebff4","trusted":true}},{"cell_type":"code","source":"out=[]\ndef Zscore_outlier(df):\n    m = np.mean(df)\n    sd = np.std(df)\n    row = 0\n    for i in df: \n        z = (i-m)/sd\n        if np.abs(z) > 3: \n            out.append(row)\n        row += 1\n    return out","metadata":{"_uuid":"6a4f9148-58e4-425a-8832-05f1b06aa630","_cell_guid":"cd4eb069-d961-45e0-9897-e95baa1f8fef","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:31:55.896138Z","iopub.execute_input":"2022-04-17T00:31:55.896407Z","iopub.status.idle":"2022-04-17T00:31:55.901265Z","shell.execute_reply.started":"2022-04-17T00:31:55.896373Z","shell.execute_reply":"2022-04-17T00:31:55.900573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.10.3 | Multivariate Outliers Detection\n#### 2.10.3.1 | DBSCAN\n\nWe are going to analyse outliers in two dimensions. One of our features is going to be the target one always. Let's start with **<span style='color:lightseagreen'>ShoppingMall</span>** feature.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\n\ndef dbscan(feature1):\n    # scale data first\n    cols=['Transported',feature1]\n    X = StandardScaler().fit_transform(df_data[df_data.Transported.isnull() == False][cols].copy().values)\n\n    db = DBSCAN(eps=3.0, min_samples=10).fit(X)\n    labels = db.labels_\n\n    plt.figure(figsize=(8,6))\n\n    unique_labels = set(labels)\n    colors = ['blue', 'red']\n\n    for color,label in zip(colors, unique_labels):\n        sample_mask = [True if l == label else False for l in labels]\n        plt.plot(X[:,0][sample_mask], X[:, 1][sample_mask], 'o', color=color);\n    plt.xlabel(cols[0]);\n    plt.ylabel(cols[1]);\n\n    print(pd.Series(labels).value_counts())\n    \ndbscan('ShoppingMall')","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:31:55.902491Z","iopub.execute_input":"2022-04-17T00:31:55.90272Z","iopub.status.idle":"2022-04-17T00:31:57.091855Z","shell.execute_reply.started":"2022-04-17T00:31:55.902688Z","shell.execute_reply":"2022-04-17T00:31:57.091093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"📌 **Interpret:** As we can observe, top5 people who have spent the most on this luxury service are detected as outliers. Thus, in the next cell we are going to drop them from our dataset. Hereafter, we'll keep our outliers detection task with next luxury service features. ","metadata":{}},{"cell_type":"code","source":"dropped_indexes = df_data[df_data.Transported.isnull() ==False][['ShoppingMall','Transported']].sort_values(by='ShoppingMall', ascending=False).head().index\naux = df_data[df_data.Transported.isnull() == True].copy()\ndf_data = df_data[df_data.Transported.isnull() == False].drop(dropped_indexes, axis=0)\ndf_data = pd.concat([df_data, aux], axis=0)\ndbscan('RoomService')","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:31:57.093441Z","iopub.execute_input":"2022-04-17T00:31:57.093705Z","iopub.status.idle":"2022-04-17T00:31:58.414101Z","shell.execute_reply.started":"2022-04-17T00:31:57.093668Z","shell.execute_reply":"2022-04-17T00:31:58.41342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dropped_indexes = df_data[df_data.Transported.isnull() ==False][['RoomService','Transported']].sort_values(by='RoomService', ascending=False).head(1).index\naux = df_data[df_data.Transported.isnull() == True].copy()\ndf_data = df_data[df_data.Transported.isnull() == False].drop(dropped_indexes, axis=0)\ndf_data = pd.concat([df_data, aux], axis=0)\ndbscan('VRDeck')","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:31:58.415288Z","iopub.execute_input":"2022-04-17T00:31:58.415547Z","iopub.status.idle":"2022-04-17T00:31:59.57788Z","shell.execute_reply.started":"2022-04-17T00:31:58.41551Z","shell.execute_reply":"2022-04-17T00:31:59.577036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dropped_indexes = df_data[df_data.Transported.isnull() ==False][['VRDeck','Transported']].sort_values(by='VRDeck', ascending=False).head(2).index\naux = df_data[df_data.Transported.isnull() == True].copy()\ndf_data = df_data[df_data.Transported.isnull() == False].drop(dropped_indexes, axis=0)\ndf_data = pd.concat([df_data, aux], axis=0)\ndbscan('Spa')","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:31:59.579422Z","iopub.execute_input":"2022-04-17T00:31:59.579685Z","iopub.status.idle":"2022-04-17T00:32:00.773933Z","shell.execute_reply.started":"2022-04-17T00:31:59.579648Z","shell.execute_reply":"2022-04-17T00:32:00.772646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dropped_indexes = df_data[df_data.Transported.isnull() ==False][['Spa','Transported']].sort_values(by='Spa', ascending=False).head(1).index\naux = df_data[df_data.Transported.isnull() == True].copy()\ndf_data = df_data[df_data.Transported.isnull() == False].drop(dropped_indexes, axis=0)\ndf_data = pd.concat([df_data, aux], axis=0)\ndbscan('FoodCourt')","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:32:00.775338Z","iopub.execute_input":"2022-04-17T00:32:00.775591Z","iopub.status.idle":"2022-04-17T00:32:02.499757Z","shell.execute_reply.started":"2022-04-17T00:32:00.775556Z","shell.execute_reply":"2022-04-17T00:32:02.499088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dropped_indexes = df_data[df_data.Transported.isnull() ==False][['FoodCourt','Transported']].sort_values(by='FoodCourt', ascending=False).head(4).index\naux = df_data[df_data.Transported.isnull() == True].copy()\ndf_data = df_data[df_data.Transported.isnull() == False].drop(dropped_indexes, axis=0)\ndf_data = pd.concat([df_data, aux], axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T00:32:02.500956Z","iopub.execute_input":"2022-04-17T00:32:02.502388Z","iopub.status.idle":"2022-04-17T00:32:02.52461Z","shell.execute_reply.started":"2022-04-17T00:32:02.502345Z","shell.execute_reply":"2022-04-17T00:32:02.523951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>4 <span style='color:lightseagreen'>|</span> Feature Engineering</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.1 | Local CV Scoring Dataset Function</b></p>\n</div>\n\nThe first step after EDA for us, is going to be building a reliable **<span style='color:lightseagreen'>local validation strategy</span>**. With reliable I mean a local CV score that **<span style='color:lightseagreen'>correlates</span>** with LB score. Because then we can use our local CV score to evaluate experiments or to tune (hyper)parameters. There are **<span style='color:lightseagreen'>two questions</span>** that I usually try to answer.\n\n- **<span style='color:lightseagreen'>How to split</span>** the data in train and validation (there are a lot of different strategies)?\n- Once a strategy is chosen does LB score moves in the direction of local CV score? If the answer is yes then probably the relationship between your local folds is the same relationship between Kaggle's train and test. If not, try other CV strategy and if you cannot find a reliable local CV then is it probably time to stop taking part in the competition because at the end you might be highly disappointed after the final shake-up.","metadata":{"_uuid":"bc0f9da6-4bc0-4e5c-981d-91eab21c67c9","_cell_guid":"93069ba9-bd5c-4b31-97c1-e6bd90f51a17","trusted":true}},{"cell_type":"code","source":"#def score_dataset(X, y, model=XGBRegressor(tree_method='gpu_hist', predictor='gpu_predictor'), model_2 = CatBoostRegressor(task_type = 'GPU', silent=True)):\n#def score_dataset(X, y, model=XGBRegressor(), model_2 = CatBoostRegressor(silent=True)):\ndef score_dataset(X,y,model=XGBClassifier(label_encoder=False)):\n    # Label encoding is good for XGBoost and RandomForest, but one-hot\n    # would be better for models like Lasso or Ridge. The `cat.codes`\n    # attribute holds the category levels.\n    for colname in X.select_dtypes([\"object\",\"bool\",\"category\"]).columns:\n        X[colname] = LabelEncoder().fit_transform(X[colname])\n    y['Transported'] = LabelEncoder().fit_transform(y['Transported'])\n    # Metric for Titanic SpaceShipt competition is MAE (Mean Absolute Error)\n    score_xgb = cross_val_score(\n        model, X, y, cv=5, scoring=\"accuracy\"\n    )\n    \n    score = score_xgb.mean()\n    return score\n\nX = df_data[df_data.Transported.isnull() == False].copy()\ny = pd.DataFrame(X.pop('Transported'))\nbaseline_score = score_dataset(X, y)\nclear_output()\nprint(f\"Baseline score: {baseline_score:.5f} Accuracy\")","metadata":{"_uuid":"4ccb78dc-b44b-458f-b3d8-84f79282a28d","_cell_guid":"369a738c-1517-4bc4-9ce9-cb58dd13a792","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:32:02.526004Z","iopub.execute_input":"2022-04-17T00:32:02.526267Z","iopub.status.idle":"2022-04-17T00:32:06.982974Z","shell.execute_reply.started":"2022-04-17T00:32:02.52622Z","shell.execute_reply":"2022-04-17T00:32:06.982444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.2 | Creating New Features</b></p>\n</div>\n\n### 4.2.1 | Age\nIn this case, what we'll do is making a distinction between several groups of ages. We are doing this, in order to make it easier to our classifier when making predictions and training.","metadata":{"_uuid":"12a0246c-365a-44d4-9142-db7a1d042652","_cell_guid":"ddd8a5bd-0fff-4632-bb7c-e67e5d38fb25","trusted":true}},{"cell_type":"code","source":"df_data['Age'] = pd.qcut(df_data['Age'], 10)\ndf_data.head().style.set_properties(subset=['Age'], **{'background-color': 'lightseagreen'})","metadata":{"_uuid":"cecf9d93-ba42-4ce0-b3c3-44af9a66fe52","_cell_guid":"e070470f-6206-454a-bee3-ee0c6f731da7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:32:06.985766Z","iopub.execute_input":"2022-04-17T00:32:06.987408Z","iopub.status.idle":"2022-04-17T00:32:07.00581Z","shell.execute_reply.started":"2022-04-17T00:32:06.987374Z","shell.execute_reply":"2022-04-17T00:32:07.005134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agebox = df_data[df_data.Transported.isnull() == False].copy()\nagebox['Transported'].replace([False, True], [0,1], inplace = True)\nagebox = agebox.groupby('Age').agg({'Transported':'mean'}).reset_index()\nagebox['Age'] = agebox['Age'].astype(str)\n\nfig = px.bar(agebox, x=\"Age\", y=\"Transported\", color=\"Transported\", color_continuous_scale = 'Blugrn')\n\nfig.update_xaxes(showgrid = False, showline = True, gridwidth = 0.05, linecolor = 'gray', zeroline = False, linewidth = 2)\nfig.update_yaxes(showline = True, gridwidth = 0.05, linecolor = 'gray', linewidth = 2, zeroline = False)\n\n# General Styling\nfig.update_layout(height=400, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100, t=80),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Age Groups Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_uuid":"fc94e739-da9b-456e-bafb-aab7803610fc","_cell_guid":"3612d67d-a723-49a1-a65e-a312488c71f2","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:32:07.006875Z","iopub.execute_input":"2022-04-17T00:32:07.007123Z","iopub.status.idle":"2022-04-17T00:32:07.103004Z","shell.execute_reply.started":"2022-04-17T00:32:07.007089Z","shell.execute_reply":"2022-04-17T00:32:07.102084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.2 | Family Features\n\nHereafter we are going to focus in **<span style='color:lightseagreen'>PassengerId</span>** and **<span style='color:lightseagreen'>Name</span>**. Firstly, we are going to take a brief view to both features, in order to study its relation. As we can see below, PassengerId feature is composed of **<span style='color:lightseagreen'>two parts</span>**\n- First one is related to **<span style='color:lightseagreen'>FamilyId</span>**\n- Second one is related to each member of the family. \n\nIn other words, we can appreciate that both Altark Susent and Solam Susent are from the same family. Therefore, in PassengerId feature they have the same FamilyId, concretely 0003. In order to **<span style='color:lightseagreen'>distinguish</span>** them into the family group, their second PassengerId part are 01 and 02 respectively. Thus, we are going to create some new features:\n- One for **<span style='color:lightseagreen'>FamilyId</span>**\n- One for **<span style='color:lightseagreen'>Family Name</span>**\n- One for **<span style='color:lightseagreen'>Family Size</span>**","metadata":{"_uuid":"32077c69-5438-4f66-b415-2d39c13ab59f","_cell_guid":"383da441-9f6d-496f-ab59-4e883c412b2d","trusted":true}},{"cell_type":"code","source":"df_data[['Name','PassengerId']].head().style.set_properties(subset=['PassengerId'], **{'background-color': 'lightseagreen'})","metadata":{"_uuid":"e8f4cb0a-87b1-4810-8a63-d1d1b05b5101","_cell_guid":"5b1e953c-433d-4108-ab99-4277dca25ccd","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:32:07.104642Z","iopub.execute_input":"2022-04-17T00:32:07.104921Z","iopub.status.idle":"2022-04-17T00:32:07.118117Z","shell.execute_reply.started":"2022-04-17T00:32:07.104884Z","shell.execute_reply":"2022-04-17T00:32:07.117169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data['FamilyId'] = df_data['PassengerId'].str.split(\"_\", n=2, expand=True)[0]\ndf_data['Family Name'] = df_data['Name'].str.split(' ', n=2, expand=True)[1]\ndf_data = df_data.set_index(['FamilyId','Family Name'])\ndf_data['Family Size'] = 1\nfor i in range(df_data.shape[0]):\n    fam_size = df_data.loc[df_data.index[i],:].shape[0]\n    df_data.loc[df_data.index[i],'Family Size'] = fam_size\n    \ndf_data = df_data.reset_index()\ndf_data[['FamilyId','PassengerId','Family Name','Name','Family Size']].head().style.set_properties(subset=['FamilyId', 'Family Name','Family Size'], **{'background-color': 'lightseagreen'})","metadata":{"_uuid":"1ec24cf0-9274-4231-a55e-cac822a32533","_cell_guid":"7c47c1c2-a955-41ba-a15a-46b386ca486a","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:32:07.119709Z","iopub.execute_input":"2022-04-17T00:32:07.120116Z","iopub.status.idle":"2022-04-17T00:32:17.70378Z","shell.execute_reply.started":"2022-04-17T00:32:07.120077Z","shell.execute_reply":"2022-04-17T00:32:17.703092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.2 | Luxury Features\n\nHereafter, we are going to focus our attention on **<span style='color:lightseagreen'>luxury features</span>**. Those features are: \n\n- Spa\n- VRDeck\n- Food Court\n- Room Service\n- Shopping Mall\n\nThey all reflect the amount of money a passenger has spent on it. We can create a feature for telling us the amount of money a passenger has spent in all these luxuries. Let's call it **<span style='color:lightseagreen'>Luxury Spending</span>**. Hereafter, as it's going to be a continuous numerical feature, in order to make it easier to our model, we are going to split it into 10 groups (each of them related to one of the percentiles).","metadata":{"_uuid":"474c6723-bad3-488c-ae28-2aa3e4196f54","_cell_guid":"3602a95b-ba0f-43c1-ac7a-e7f4358b128c","trusted":true}},{"cell_type":"code","source":"df_data['Luxury Spending'] = df_data['VRDeck'] + df_data['ShoppingMall'] + df_data['Spa'] + df_data['FoodCourt'] + df_data['RoomService']\nluxury = df_data[df_data.Transported.isnull() == False].copy()\nluxury['Luxury Spending'] = pd.qcut(luxury['Luxury Spending'], 6, duplicates = 'drop')\nluxury = luxury.groupby('Luxury Spending').agg({'Transported':'mean'}).reset_index()\nluxury['Luxury Spending'] = LabelEncoder().fit_transform(luxury['Luxury Spending'])\n\nfig = px.bar(luxury, x=\"Luxury Spending\", y=\"Transported\", color='Transported', color_continuous_scale='Blugrn')\n\nfig.update_xaxes(dtick=1, showgrid = False, showline = True, gridwidth = 0.05, linecolor = 'gray', zeroline = False, linewidth = 2)\nfig.update_yaxes(showline = True, gridwidth = 0.05, linecolor = 'gray', linewidth = 2, zeroline = False)\n\n# General Styling\nfig.update_layout(height=400, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100, t=80),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Transported Probability per Luxury Spending Group</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_uuid":"745156e5-9327-4bd7-ad5b-83ec0d5404a2","_cell_guid":"f24bed7c-0af5-44f4-8ef0-109a2674d40a","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:32:17.704945Z","iopub.execute_input":"2022-04-17T00:32:17.705206Z","iopub.status.idle":"2022-04-17T00:32:17.797386Z","shell.execute_reply.started":"2022-04-17T00:32:17.705171Z","shell.execute_reply":"2022-04-17T00:32:17.796697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check how our Local CV Scoring Value has changed. As we can observe, there's been a bit of improve. Let's keep working with Feature Engineering thought !","metadata":{"_uuid":"38930f1d-ed55-4783-9ebc-b03cfe885ae8","_cell_guid":"5c9f78bf-a308-4486-9aff-625ef6984dba","trusted":true}},{"cell_type":"code","source":"X = df_data[df_data.Transported.isnull() == False].copy()\ny = pd.DataFrame(X.pop('Transported'))\nbaseline_score = score_dataset(X, y)\nclear_output()\nprint(f\"Baseline score: {baseline_score:.5f} Accuracy\")","metadata":{"_uuid":"5ede1a06-7ccf-4e6a-916d-7f18d8263f65","_cell_guid":"21815685-8571-469e-a9ef-547e68a67b6a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:32:17.798741Z","iopub.execute_input":"2022-04-17T00:32:17.798975Z","iopub.status.idle":"2022-04-17T00:32:24.050746Z","shell.execute_reply.started":"2022-04-17T00:32:17.798942Z","shell.execute_reply":"2022-04-17T00:32:24.050166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data.head()","metadata":{"_uuid":"835e458a-2acc-41e3-afcf-e9342f77aa81","_cell_guid":"497249f3-e72e-4e52-aec9-aef3e805d0bf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:32:24.053708Z","iopub.execute_input":"2022-04-17T00:32:24.055145Z","iopub.status.idle":"2022-04-17T00:32:24.078869Z","shell.execute_reply.started":"2022-04-17T00:32:24.055112Z","shell.execute_reply":"2022-04-17T00:32:24.078071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>5 <span style='color:lightseagreen'>|</span> Feature Selection</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>5.1 | Scaling Features</b></p>\n</div>\n\nBefore entering into pure feature selection methods, I'm going to scale thos features which previously obtained a skewness values greater than 1.","metadata":{"_uuid":"4860ab50-a828-4c85-83e9-b03fc146432c","_cell_guid":"34e8b180-dcac-42f8-8062-41a9407a8b41","trusted":true}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler as ss\nskew_col = ['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','Luxury Spending']\nfor col in skew_col:\n    df_data[col] = np.log1p(df_data[col])","metadata":{"_uuid":"2abd3d8d-7204-41d0-bd69-ec1eca12e5d4","_cell_guid":"246134e5-23c9-493d-818b-5464e7258e19","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:32:24.080414Z","iopub.execute_input":"2022-04-17T00:32:24.08074Z","iopub.status.idle":"2022-04-17T00:32:24.0913Z","shell.execute_reply.started":"2022-04-17T00:32:24.080686Z","shell.execute_reply":"2022-04-17T00:32:24.090674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>5.2 | Mutual Information</b></p>\n</div>\n\nMutual information describes **<span style='color:lightseagreen'>relationships in terms of uncertainty</span>**. The mutual information (MI) between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other. If you knew the value of a feature, how much more **<span style='color:lightseagreen'>confident</span>** would you be about the target? Scikit-learn has two mutual information metrics in its feature_selection module: one for real-valued targets (mutual_info_regression) and one for categorical targets (mutual_info_classif). The next cell computes the MI scores for our features and wraps them up in a nice dataframe.","metadata":{"_uuid":"fe3f36db-85e7-4661-9a98-9234872f7a97","_cell_guid":"7c560a4c-f1e6-4e12-aef6-4fbca90b77ed","trusted":true}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\",\"bool\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    #discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_classif(X, y, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ndef uninformative_cols(df, mi_scores):\n    return df.loc[:, mi_scores == 0.0].columns\n    \nx = df_data[df_data.Transported.isnull() == False].copy()\nx['Transported'].replace([False, True], [0,1], inplace = True)\ny = x.pop('Transported')\n\nboolean_col = x.select_dtypes(['bool']).columns\nfor i in range(len(boolean_col)):\n    x[boolean_col[i]].replace([False, True], [0,1], inplace = True)\n\nfor colname in x.drop('PassengerId',axis=1).select_dtypes([\"object\",\"category\"]).columns:\n    x[colname] = LabelEncoder().fit_transform(x[colname])\n\nmi_scores = make_mi_scores(x, y)\nmi_scores = pd.DataFrame(mi_scores).reset_index().rename(columns={'index':'Feature'})\n\nfig = px.bar(mi_scores, x='MI Scores', y='Feature', color=\"MI Scores\", color_continuous_scale='darkmint')\n\nfig.update_xaxes(showgrid = False, showline = True, gridwidth = 0.05, linecolor = 'gray', zeroline = False, linewidth = 2)\nfig.update_yaxes(showline = True, gridwidth = 0.05, linecolor = 'gray', linewidth = 2, zeroline = False)\n\nfig.update_layout(height = 500, title_text=\"Mutual Information Scores\", plot_bgcolor='rgb(242, 242, 242)', paper_bgcolor = 'rgb(242, 242, 242)',\n                  title_font=dict(size=29, family=\"Lato, sans-serif\"), xaxis={'categoryorder':'category ascending'}, margin=dict(t=80))","metadata":{"_uuid":"e9ad5dc3-fab1-4868-a526-615487471d6e","_cell_guid":"d3f12462-e022-4664-9ac4-508e9c9bf4b3","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:32:24.093124Z","iopub.execute_input":"2022-04-17T00:32:24.09355Z","iopub.status.idle":"2022-04-17T00:32:24.843458Z","shell.execute_reply.started":"2022-04-17T00:32:24.093511Z","shell.execute_reply":"2022-04-17T00:32:24.842791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>5.3 | Heatmap</b></p>\n</div>","metadata":{"_uuid":"fb3cc5f6-75e5-401a-a7b5-3ea53920f7de","_cell_guid":"0fddacdb-b035-43e8-989c-0877de7d0863","trusted":true}},{"cell_type":"code","source":"x['Transported'] = df_data[df_data.Transported.isnull() == False]['Transported'].copy()\nx['Transported'].replace([False, True], [0,1], inplace = True)\ncorr = x.corr()\n\nfig = px.imshow(corr, color_continuous_scale='RdBu_r', origin='lower', text_auto=True, aspect='auto', color_continuous_midpoint=0.0)\nfig.update_layout(height = 500, title_text=\"Correlation Heatmap\", plot_bgcolor='rgb(242, 242, 242)', paper_bgcolor = 'rgb(242, 242, 242)',\n                  title_font=dict(size=29, family=\"Lato, sans-serif\"), margin=dict(t=90))","metadata":{"_uuid":"62e939e3-9264-4157-aef9-a1b2f8ec848d","_cell_guid":"951adc7e-34da-4a72-9930-acb3050ef5bc","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:32:24.84462Z","iopub.execute_input":"2022-04-17T00:32:24.84494Z","iopub.status.idle":"2022-04-17T00:32:24.912127Z","shell.execute_reply.started":"2022-04-17T00:32:24.844903Z","shell.execute_reply":"2022-04-17T00:32:24.911498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>5.4 | Feature Usefulness</b></p>\n</div>\n\nIn this section we'll analyse the usefulness of the features that we've just created. We'll plot how the target depends on every feature, that is, a diagram of $P(y=1|x)$. To get a meaningful plot, we apply two transformations:\n\n* The x axis is not the value of the feature, but its index (when sorted by feature value).\n* The y axis is not the target value (which can be only 0 or 1), but a rolling mean over 1000 targets.\n\nFeatures with an horizontal line as diagram (the probability of the positive target is 0.5 independently of the feature value), are going to be considered bad ones, not useful. On the other hand, good features would have a curve with high $y_{max}-y_ {min}$\n.","metadata":{"_uuid":"5c0df0e0-f165-43a1-a7d9-40fe0686d093","_cell_guid":"e806fc5e-ee34-466e-9738-a2ae73141a7c","trusted":true}},{"cell_type":"code","source":"fig = make_subplots(rows=6, cols=3, column_widths=[0.33, 0.34, 0.33], vertical_spacing=0.1, horizontal_spacing=0.1,\n                    subplot_titles=x.drop(['Transported', 'PassengerId'],axis=1).columns)\n    \n#plt.subplots(6, 4, sharey=True, sharex=True, figsize=(20, 25))\n\nrow_counter = 1\ncol_counter = 0\nfor col in x.drop(['Transported', 'PassengerId'],axis=1).columns:\n    temp = pd.DataFrame({col: x[col].values,\n                        'Transported': y.values})\n    temp = temp.sort_values(col)\n    temp.reset_index(inplace=True)\n    \n    aux = pd.DataFrame(temp.Transported.rolling(1000).mean())\n    aux.columns = ['Mean']\n    aux = aux[aux.Mean.isnull() == False]\n    \n    col_counter = col_counter +1\n    fig.add_trace(go.Scatter(x=aux.index, y=aux['Mean'], marker = dict(color = px.colors.sequential.Blugrn[6])), row = row_counter, col = col_counter)  \n    \n    fig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=row_counter, col=col_counter)\n    fig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=row_counter, col=col_counter)\n    \n    #plt.scatter(aux.index, aux.Mean, s=2)            \n    #plt.xlabel(col)\n    #plt.xticks([])\n    if col_counter % 3 == 0:\n        col_counter = 0\n        row_counter += 1\n\n# General Styling\nfig.update_layout(height=900, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100, t=90),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Feature Usefulness Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)        \nfig.show()","metadata":{"_uuid":"a50366da-ba64-4970-8793-f7af1766b11a","_cell_guid":"984d35ab-c54e-4403-93f7-1afb440f0810","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:32:24.913443Z","iopub.execute_input":"2022-04-17T00:32:24.913813Z","iopub.status.idle":"2022-04-17T00:32:25.461379Z","shell.execute_reply.started":"2022-04-17T00:32:24.913779Z","shell.execute_reply":"2022-04-17T00:32:25.46071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"📌 Interpret: we can conclude the following after analysing the previous chart plotted:\n\n- Both Luxury Services Features, and CryoSleep are the **<span style='color:lightseagreen'>best ones</span>**. \n- Side, Name, Age and some Family Features seem to be **<span style='color:lightseagreen'>useless</span>**.\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>5.3 | Permutation Importance</b></p>\n</div>\n\nBefore starting with the explanation, just mention that this will be included at the end of Modeling section, in order to reduce training time. Now let's go ahead with explanation: One of the most basic questions we might ask of a model is: What features have the biggest impact on predictions? This concept is called feature importance. There are multiple ways to measure feature importance. Some approaches answer subtly different versions of the question above. Other approaches have documented shortcomings. In this section, we'll focus on permutation importance. Compared to most other approaches, permutation importance is:\n\n* Fast to calculate,\n* Widely used and understood, and\n* Consistent with properties we would want a feature importance measure to have.","metadata":{"_uuid":"2fa98430-20fd-4a72-bffb-d906406065f5","_cell_guid":"31c29fe6-8fce-42a6-84a2-958992e0cc93","trusted":true}},{"cell_type":"code","source":"# Permutation Importance\nimport eli5\nfrom eli5.sklearn import PermutationImportance","metadata":{"_uuid":"90e7eee0-6539-4b6b-9180-c052c86dc109","_cell_guid":"28f985b6-ae31-4d5c-86fd-9bde8bbcd536","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:32:25.462847Z","iopub.execute_input":"2022-04-17T00:32:25.463222Z","iopub.status.idle":"2022-04-17T00:32:25.469632Z","shell.execute_reply.started":"2022-04-17T00:32:25.463186Z","shell.execute_reply":"2022-04-17T00:32:25.468448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>5.4 | Selecting and Encoding</b></p>\n</div>","metadata":{"_uuid":"fc9b41e8-0314-43ae-8d6f-f21493dcc62c","_cell_guid":"524c45aa-3291-496d-8771-80d2a5a4cc92","trusted":true}},{"cell_type":"code","source":"drop_columns = ['FamilyId','Family Name','Family Size','Name','Side','VIP','Age']\ndf_data = df_data.drop(drop_columns, axis=1)\nfor colname in df_data.select_dtypes([\"object\",\"bool\",\"category\"]).drop('Transported',axis=1):\n        df_data[colname], _ = df_data[colname].factorize()\ndf_data['Transported'].replace([False, True], [0,1], inplace = True)","metadata":{"_uuid":"9a20153d-d32f-4020-bb9e-0113e974167f","_cell_guid":"e37174a4-9d67-451b-93db-c605bbdcc05d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:32:25.472218Z","iopub.execute_input":"2022-04-17T00:32:25.47568Z","iopub.status.idle":"2022-04-17T00:32:25.517177Z","shell.execute_reply.started":"2022-04-17T00:32:25.475635Z","shell.execute_reply":"2022-04-17T00:32:25.516474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>6 <span style='color:lightseagreen'>|</span> Modeling</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>6.1 | Catboost</b></p>\n</div>\n\n### 6.1.1 | Hyperparameter Tuning - Optuna\n\nI will add the code for hyperparameter tuning below. However, for not wasting CPU time, since I have run it once, I will simply create the model with the specific features values. I will control whether making hyperparameter tuning or not with allow_optimize Finally, just say that code for tuning takes plenty of time. Due to that I enabled GPU technology.","metadata":{"_uuid":"153add29-c053-4759-960d-11c0263b09d1","_cell_guid":"f1ce39d6-376f-466b-8030-8894267adeea","trusted":true}},{"cell_type":"code","source":"X = df_data[df_data.Transported.isnull() == False].drop('PassengerId',axis=1)\ny = X.pop('Transported')\n\ndef objective(trial):\n    params = {\n        \"random_state\":trial.suggest_categorical(\"random_state\", [2022]),\n        'learning_rate' : trial.suggest_loguniform('learning_rate', 0.0001, 0.3),\n        'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n        \"n_estimators\": trial.suggest_int('n_estimators', 100, 500),\n        \"max_depth\":trial.suggest_int(\"max_depth\", 4, 16),\n        'random_strength' :trial.suggest_int('random_strength', 0, 100),\n        \"l2_leaf_reg\":trial.suggest_float(\"l2_leaf_reg\",1e-8,3e-5),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n        \"max_bin\": trial.suggest_int(\"max_bin\", 200, 500),\n        'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n        'task_type': trial.suggest_categorical('task_type', ['GPU']),\n        'eval_metric': trial.suggest_categorical('eval_metric', ['Accuracy'])\n    }\n\n    model = CatBoostClassifier(**params)\n    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X, y, test_size=0.3, random_state=42)\n    model.fit(\n        X_train_tmp, y_train_tmp,\n        eval_set=[(X_valid_tmp, y_valid_tmp)],\n        early_stopping_rounds=35, verbose=0\n    )\n        \n    y_train_pred = model.predict(X_train_tmp)\n    y_valid_pred = model.predict(X_valid_tmp)\n    train_ac_score = accuracy_score(y_train_tmp, y_train_pred)\n    valid_ac_score = accuracy_score(y_valid_tmp, y_valid_pred)\n    \n    print(f'Accuracy Score of Train: {train_ac_score}')\n    print(f'Accuracy Score of Validation: {valid_ac_score}')\n    \n    return valid_ac_score\n\nallow_optimize = 1\nTRIALS = 40\nTIMEOUT = 3600\n\nif allow_optimize:\n    sampler = TPESampler(seed=42)\n\n    study = optuna.create_study(\n        study_name = 'cat_parameter_opt',\n        direction = 'maximize',\n        sampler = sampler,\n    )\n    study.optimize(objective, n_trials=TRIALS)\n    print(\"Best Score:\",study.best_value)\n    print(\"Best trial\",study.best_trial.params)\n    \n    best_params = study.best_params\n    \n    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X, y, test_size=0.3, random_state=42)\n    model_cat = CatBoostClassifier(**best_params, verbose=1000).fit(X_train_tmp, y_train_tmp, eval_set=[(X_valid_tmp, y_valid_tmp)], early_stopping_rounds=35)\nelse:\n    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X, y, test_size=0.3, random_state=42)\n    model_cat = CatBoostClassifier(\n        verbose=1000,\n        early_stopping_rounds=10,\n        #iterations=5000,\n        random_state = 2022, learning_rate = 0.08665686887824392, bagging_temperature = 2.010272294890727, n_estimators = 806, max_depth = 7, \n        random_strength = 35, l2_leaf_reg = 1.2373460332766636e-05, min_child_samples = 69, max_bin = 317, od_type = 'IncToDec', \n        task_type = 'GPU', eval_metric = 'Accuracy'\n    ).fit(X_train_tmp, y_train_tmp, eval_set=[(X_valid_tmp, y_valid_tmp)], early_stopping_rounds=35)\n    \nclear_output()","metadata":{"_uuid":"dba39abf-ba14-489f-b38b-3946cd49c73f","_cell_guid":"a12ba748-30de-4798-9ef0-248d6a9e7140","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:32:25.518478Z","iopub.execute_input":"2022-04-17T00:32:25.518832Z","iopub.status.idle":"2022-04-17T00:33:58.53601Z","shell.execute_reply.started":"2022-04-17T00:32:25.518801Z","shell.execute_reply":"2022-04-17T00:33:58.535401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_feature_importance(model_cat.get_feature_importance(),X.columns,'CatBoost')","metadata":{"_uuid":"7850efe8-2fed-49ca-80f4-dc8652a965eb","_cell_guid":"df85e649-0887-450b-9449-d3cd9a135905","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:33:58.537357Z","iopub.execute_input":"2022-04-17T00:33:58.540974Z","iopub.status.idle":"2022-04-17T00:33:58.632971Z","shell.execute_reply.started":"2022-04-17T00:33:58.540935Z","shell.execute_reply":"2022-04-17T00:33:58.632364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = df_data[df_data.Transported.isnull() == True].drop(['PassengerId','Transported'],axis=1)\nperm = PermutationImportance(model_cat, random_state=1).fit(X, y)\npred = model_cat.predict(X_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","metadata":{"_uuid":"160b28e0-48fb-4834-bbdb-4b60d92ef5ba","_cell_guid":"df7156dd-e7f8-4f36-8404-71acc699b92d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:33:58.636592Z","iopub.execute_input":"2022-04-17T00:33:58.638558Z","iopub.status.idle":"2022-04-17T00:33:59.435626Z","shell.execute_reply.started":"2022-04-17T00:33:58.638517Z","shell.execute_reply":"2022-04-17T00:33:59.434968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"📌 **Interpret:** the values towards the top are the most important features, and those towards the bottom matter least. The first number in each row shows how much model performance decreased with a random shuffling (in this case, using \"accuracy\" as the performance metric). Like most things in data science, there is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after the ± measures how performance varied from one-reshuffling to the next. You'll occasionally see negative values for permutation importances. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn't matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. This is more common with small datasets, like the one in this example, because there is more room for luck/chance","metadata":{"_uuid":"9576229d-fa4b-424c-87c0-cff9f1d4cb3a","_cell_guid":"d69e361b-417d-4790-947f-be64de00bc7f","trusted":true}},{"cell_type":"code","source":"data_dir = Path(\"../input/spaceship-titanic\")\ndf_test = pd.read_csv(data_dir / \"test.csv\")\nsubmit = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Transported':pred}).set_index('PassengerId')\nsubmit['Transported'].replace([0,1], [False, True], inplace=True)\nsubmit.to_csv('./submission.csv')","metadata":{"_uuid":"aaf2049d-f1b2-4347-ba19-4139f0ca800a","_cell_guid":"7dd97d9d-2685-451d-8029-33bec3f507cf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:33:59.439368Z","iopub.execute_input":"2022-04-17T00:33:59.441606Z","iopub.status.idle":"2022-04-17T00:33:59.510757Z","shell.execute_reply.started":"2022-04-17T00:33:59.441568Z","shell.execute_reply":"2022-04-17T00:33:59.509872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>6.2 | LGBM</b></p>\n</div>","metadata":{"_uuid":"4d56c6a1-4bc2-45cd-a092-0e0dcc622378","_cell_guid":"1dae7dcc-7fd3-44a1-9921-241bb23297ff","trusted":true}},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        \"random_state\":trial.suggest_categorical(\"random_state\", [2022]),           \n        'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 1),   \n        \"n_estimators\": trial.suggest_int('n_estimators',50,2000),                 \n        \"max_depth\" : trial.suggest_int(\"max_depth\", 1, 20),\n        \"_min_child_weight\" : trial.suggest_float(\"_min_child_weight\", 0.1, 10),\n        \"reg_lambda\" : trial.suggest_float(\"reg_lambda\", 0.01, 10),\n        \"reg_alpha\" : trial.suggest_float('reg_alpha',0.01,10),\n        \"num_leaves\" : trial.suggest_int(\"num_leaves\", 50, 100),\n        'subsample' : trial.suggest_float('subsample', 0.01, 1)\n    }\n\n    model = LGBMClassifier(**params, device='GPU')\n    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X, y, test_size=0.3, random_state=42)\n    model.fit(\n        X_train_tmp, y_train_tmp,\n        eval_set=[(X_valid_tmp, y_valid_tmp)],\n        early_stopping_rounds=35, verbose=0\n    )\n        \n    y_train_pred = model.predict(X_train_tmp)\n    y_valid_pred = model.predict(X_valid_tmp)\n    train_ac_score = accuracy_score(y_train_tmp, y_train_pred)\n    valid_ac_score = accuracy_score(y_valid_tmp, y_valid_pred)\n    \n    print(f'Accuracy Score of Train: {train_ac_score}')\n    print(f'Accuracy Score of Validation: {valid_ac_score}')\n    \n    return valid_ac_score\n\nallow_optimize = 1\nTRIALS = 40\nTIMEOUT = 3600\n\nif allow_optimize:\n    sampler = TPESampler(seed=42)\n\n    study = optuna.create_study(\n        study_name = 'lgbm_parameter_opt',\n        direction = 'maximize',\n        sampler = sampler,\n    )\n    study.optimize(objective, n_trials=TRIALS)\n    print(\"Best Score:\",study.best_value)\n    print(\"Best trial\",study.best_trial.params)\n    \n    best_params = study.best_params\n    \n    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X, y, test_size=0.3, random_state=42)\n    model_lgbm = LGBMClassifier(**best_params, verbose=1000).fit(X_train_tmp, y_train_tmp, eval_set=[(X_valid_tmp, y_valid_tmp)], early_stopping_rounds=35)\nelse:\n    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X, y, test_size=0.3, random_state=42)\n    model_lgbm = LGBMClassifier(        \n        random_state = 2022, learning_rate = 0.010735756929247671, n_estimator = 1778, max_depth = 20, \n        min_child_weight = 8.703291969551636, reg_lambda = 6.849268797524583, reg_alpha = 4.290515300840544, \n        num_leaves = 90, subsample = 0.25248008329519706).fit(X_train_tmp, y_train_tmp, eval_set=[(X_valid_tmp, y_valid_tmp)], early_stopping_rounds=35)\n    \nclear_output()","metadata":{"_uuid":"dbba9f79-9695-4607-b09f-17f8ba61f594","_cell_guid":"5f48522b-ae96-41d7-9776-47959db64ccd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:33:59.515296Z","iopub.execute_input":"2022-04-17T00:33:59.515685Z","iopub.status.idle":"2022-04-17T00:34:38.605222Z","shell.execute_reply.started":"2022-04-17T00:33:59.515628Z","shell.execute_reply":"2022-04-17T00:34:38.604626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_feature_importance(model_lgbm.feature_importances_,X.columns,'LGBM')","metadata":{"_uuid":"3f2bb33a-4d95-4d6a-a9f2-321a49272c3e","_cell_guid":"040672e4-1a18-486b-8a4f-6dd430c433d1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:34:38.608137Z","iopub.execute_input":"2022-04-17T00:34:38.609617Z","iopub.status.idle":"2022-04-17T00:34:38.67548Z","shell.execute_reply.started":"2022-04-17T00:34:38.609584Z","shell.execute_reply":"2022-04-17T00:34:38.674806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = df_data[df_data.Transported.isnull() == True].drop(['PassengerId','Transported'],axis=1)\nperm = PermutationImportance(model_lgbm, random_state=1).fit(X, y)\npred = model_lgbm.predict(X_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","metadata":{"_uuid":"1171bf83-5158-4c80-8f05-6012f59db658","_cell_guid":"e11b82c9-912b-49bf-a77a-169d52014bd0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:34:38.676509Z","iopub.execute_input":"2022-04-17T00:34:38.679069Z","iopub.status.idle":"2022-04-17T00:34:39.717152Z","shell.execute_reply.started":"2022-04-17T00:34:38.679041Z","shell.execute_reply":"2022-04-17T00:34:39.716464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>6.3 | XGBoosting</b></p>\n</div>","metadata":{"_uuid":"a41026cf-0cf1-4d96-a796-6d84dd22d0df","_cell_guid":"a65e0ab4-6b86-4390-88ce-7ef8dce7213b","trusted":true}},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        \"random_state\":trial.suggest_categorical(\"random_state\", [2022]),           # categorical for concrete values\n        'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 1),   # loguniform for continuos values\n        \"n_estimators\": trial.suggest_int('n_estimators',50,2000),                 # int for discrete values. Interval between [100,2000]\n        \"max_depth\" : trial.suggest_int(\"max_depth\", 1, 20),\n        \"min_samples_split\" : trial.suggest_int(\"min_samples_split\", 2, 20),\n        \"min_samples_leaf\" : trial.suggest_int(\"min_samples_leaf\", 2, 20),\n        \"alpha\" : trial.suggest_loguniform('alpha',0.9,1),\n        \"max_features\" : trial.suggest_int(\"max_features\", 10, 50)\n    }\n\n    model = XGBClassifier(**params, tree_method='gpu_hist', predictor='gpu_predictor')\n    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X, y, test_size=0.3, random_state=42)\n    model.fit(\n        X_train_tmp, y_train_tmp,\n        eval_set=[(X_valid_tmp, y_valid_tmp)],\n        early_stopping_rounds=35, verbose=0\n    )\n        \n    y_train_pred = model.predict(X_train_tmp)\n    y_valid_pred = model.predict(X_valid_tmp)\n    train_ac_score = accuracy_score(y_train_tmp, y_train_pred)\n    valid_ac_score = accuracy_score(y_valid_tmp, y_valid_pred)\n    \n    print(f'Accuracy Score of Train: {train_ac_score}')\n    print(f'Accuracy Score of Validation: {valid_ac_score}')\n    \n    return valid_ac_score\n\nallow_optimize = 1\nTRIALS = 40\nTIMEOUT = 3600\n\nif allow_optimize:\n    sampler = TPESampler(seed=42)\n\n    study = optuna.create_study(\n        study_name = 'xgb_parameter_opt',\n        direction = 'maximize',\n        sampler = sampler,\n    )\n    study.optimize(objective, n_trials=TRIALS)\n    print(\"Best Score:\",study.best_value)\n    print(\"Best trial\",study.best_trial.params)\n    \n    best_params = study.best_params\n    \n    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X, y, test_size=0.3, random_state=42)\n    model_xgb = XGBClassifier(**best_params, tree_method='gpu_hist', predictor='gpu_predictor').fit(X_train_tmp, y_train_tmp, eval_set=[(X_valid_tmp, y_valid_tmp)], early_stopping_rounds=35)\nelse:\n    X_train_tmp, X_valid_tmp, y_train_tmp, y_valid_tmp = train_test_split(X, y, test_size=0.3, random_state=42)\n    model_xgb = XGBClassifier(        \n        random_state = 2022, learning_rate = 0.010735756929247671, n_estimator = 1778, max_depth = 20, \n        min_child_weight = 8.703291969551636, reg_lambda = 6.849268797524583, reg_alpha = 4.290515300840544, \n        num_leaves = 90, subsample = 0.25248008329519706, tree_method='gpu_hist', predictor='gpu_predictor').fit(X_train_tmp, y_train_tmp, eval_set=[(X_valid_tmp, y_valid_tmp)], early_stopping_rounds=35)\n    \nclear_output()","metadata":{"_uuid":"61bc3cd8-9886-4683-b610-df671f30b463","_cell_guid":"fab59791-ac30-4c35-9bd0-d636052be1f5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:34:39.720706Z","iopub.execute_input":"2022-04-17T00:34:39.720921Z","iopub.status.idle":"2022-04-17T00:35:18.436382Z","shell.execute_reply.started":"2022-04-17T00:34:39.720894Z","shell.execute_reply":"2022-04-17T00:35:18.435611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_feature_importance(model_xgb.feature_importances_,X.columns,'XGB')","metadata":{"_uuid":"52236e32-6e93-4cea-ab95-90170b8f7c95","_cell_guid":"aec4a4f7-3bb6-4399-bf02-bec934074aa5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:35:18.437841Z","iopub.execute_input":"2022-04-17T00:35:18.438088Z","iopub.status.idle":"2022-04-17T00:35:18.505477Z","shell.execute_reply.started":"2022-04-17T00:35:18.438053Z","shell.execute_reply":"2022-04-17T00:35:18.504779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = df_data[df_data.Transported.isnull() == True].drop(['PassengerId','Transported'],axis=1)\nperm = PermutationImportance(model_xgb, random_state=1).fit(X, y)\npred = model_xgb.predict(X_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","metadata":{"_uuid":"bb2fcdda-8169-479a-9201-3a3ce140fc22","_cell_guid":"b4618d90-683d-4d21-97e6-d51bd190d9e6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:35:18.506786Z","iopub.execute_input":"2022-04-17T00:35:18.507223Z","iopub.status.idle":"2022-04-17T00:35:18.94699Z","shell.execute_reply.started":"2022-04-17T00:35:18.507186Z","shell.execute_reply":"2022-04-17T00:35:18.946319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>6.4 | Voting Classifier</b></p>\n</div>\n\nThe idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.\n\n### 6.4.1 | Majority (Hard) Voting\n\nIn majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier. For example, if the prediction for a given sample is:\n\n* classifier 1 -> class 1\n* classifier 2 -> class 1\n* classifier 3 -> class 2\n\nthe VotingClassifier (with voting='hard') would classify the sample as “class 1” based on the majority class label. In the cases of a tie, the VotingClassifier will select the class based on the ascending sort order. For example, in the following scenario:\n\n* classifier 1 -> class 2\n* classifier 2 -> class 1\n\nthe class label 1 will be assigned to the sample.\n\n### 6.4.2 | Soft Voting\n\nIn contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities. Specific weights can be assigned to each classifier via the weights parameter. When weights are provided, the predicted class probabilities for each classifier are collected, multiplied by the classifier weight, and averaged. The final class label is then derived from the class label with the highest average probability. Suppose given some input to three models, the prediction probability for class A = (0.30, 0.47, 0.53) and B = (0.20, 0.32, 0.40). So the average for class A is 0.4333 and B is 0.3067, the winner is clearly class A because it had the highest probability averaged by each classifier.","metadata":{"_uuid":"ceeb1efe-c4cb-4d58-9a6a-8559ad3d9cf3","_cell_guid":"9d999e92-3f11-4551-bb17-4580a1b82ffa","trusted":true}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nvotingC = VotingClassifier(estimators=[('cat', model_cat), ('lgbm', model_lgbm),\n('xgb', model_xgb)], voting='hard', n_jobs=-1)\n\nX_test = df_data[df_data.Transported.isnull() == True].drop(['PassengerId','Transported'],axis=1)\nvotingC.fit(X,y)\nperm = PermutationImportance(votingC, random_state=1).fit(X, y)\npred = votingC.predict(X_test)\nclear_output()\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","metadata":{"_uuid":"c6193a73-8a4f-4018-9a47-9675061e9727","_cell_guid":"860f7c79-912b-4a13-bcd7-1812f6220a27","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:35:18.948265Z","iopub.execute_input":"2022-04-17T00:35:18.949108Z","iopub.status.idle":"2022-04-17T00:35:40.867791Z","shell.execute_reply.started":"2022-04-17T00:35:18.949072Z","shell.execute_reply":"2022-04-17T00:35:40.867079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = Path(\"../input/spaceship-titanic\")\ndf_test = pd.read_csv(data_dir / \"test.csv\")\nsubmit = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Transported':pred}).set_index('PassengerId')\nsubmit['Transported'].replace([0,1], [False, True], inplace=True)\nsubmit.to_csv('./submission.csv')","metadata":{"_uuid":"bf9a57b0-f4f9-4ef9-bb1a-b0cb346784eb","_cell_guid":"66d63543-aa6a-49ad-8f4e-b6793b4e0aaf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-17T00:35:40.868954Z","iopub.execute_input":"2022-04-17T00:35:40.869394Z","iopub.status.idle":"2022-04-17T00:35:40.897275Z","shell.execute_reply.started":"2022-04-17T00:35:40.869354Z","shell.execute_reply":"2022-04-17T00:35:40.896622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In progress ... \n\n* Working on EDA\n* Feature Engineering\n* Model Comparison\n* Ensembling","metadata":{"_uuid":"ac1092c7-6f31-474e-9ce3-aebdd7525083","_cell_guid":"90143d0a-90b4-4fa4-9f40-f6ebf89b2262","trusted":true}}]}