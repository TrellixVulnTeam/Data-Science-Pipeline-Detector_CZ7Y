{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.feature_selection import mutual_info_classif\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold\nfrom hyperopt import hp,fmin,tpe,Trials\nfrom hyperopt.pyll.stochastic import sample\nfrom hyperopt.pyll.base import scope\nfrom functools import partial","metadata":{"execution":{"iopub.status.busy":"2022-06-15T07:21:14.47066Z","iopub.execute_input":"2022-06-15T07:21:14.472554Z","iopub.status.idle":"2022-06-15T07:21:17.000426Z","shell.execute_reply.started":"2022-06-15T07:21:14.47243Z","shell.execute_reply":"2022-06-15T07:21:16.999387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some generally useful functions\n\ndef plotter(data,columns,fig_size):\n    \"\"\"function plots the density for bunch of columns in a data\"\"\"\n    rows=math.ceil(len(columns)/4)\n    i=0\n    sns.set_style('darkgrid')\n    plt.subplots(rows,4,figsize=fig_size)\n    plt.tight_layout()\n    for col in columns:\n        i+=1\n        plt.subplot(rows,4,i)\n        sns.kdeplot(data[col],shade=True)\n        \n# learning curve\nclass Learning_curve:\n    \"\"\"plots the learning curve\"\"\"\n    def __init__(self,train_x,train_y,val_x,val_y,model):\n        self.train_x=train_x\n        self.train_y=train_y\n        self.val_x=val_x\n        self.val_y=val_y\n        self.model=model\n        \n    def learning_curve(self):\n        loss_tr=[]\n        loss_val=[]\n        points=np.linspace(10,len(self.train_x),50)\n        for i in points:\n            i=math.ceil(i)\n            score=cross_val_score(self.model,self.train_x[:i],self.train_y[:i],cv=3,scoring='accuracy').mean()\n            self.model.fit(self.train_x[:i],self.train_y[:i])\n            loss1=1-score\n            loss2=1-accuracy_score(self.val_y,self.model.predict(self.val_x))\n            loss_tr.append(loss1)\n            loss_val.append(loss2)\n        return loss_tr,loss_val\n    def plot(self):\n        l1,l2=self.learning_curve()\n        plt.plot([i for i in range(len(l1))],l1)\n        plt.plot([i for i in range(len(l2))],l2)\n        plt.ylim((0,0.3))\n        plt.xlabel('Iterations')\n        plt.ylabel('Loss')\n        plt.title('Learning Curve')\n        plt.legend(['train','test'])\n\n\n        \ndef data_info(data):\n    \"\"\"returns some basic info on data\"\"\"\n    details={'info':data.info(),'description':data.describe(),'null':data.isna().sum()}\n    return details\n\n\ndef fetch_submission(predictions):\n    \"\"\"function that outputs the submission file given input the raw predictions\n    output from a model\"\"\"\n    \n    test_original=pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\n    test_original['Transported']=predictions.astype(bool)\n    submission=test_original[['PassengerId','Transported']]\n    submission.to_csv('submission.csv',index=False)\n    \ndef plotmi(mi): \n    \"\"\"plots the mi scores returned from function-mutual_information\"\"\"\n    sns.barplot(mi['mi_score'],mi.index)\n    plt.xlabel('Score')\n    plt.title('Mutual Information');","metadata":{"execution":{"iopub.status.busy":"2022-06-15T07:21:17.002738Z","iopub.execute_input":"2022-06-15T07:21:17.003259Z","iopub.status.idle":"2022-06-15T07:21:17.024725Z","shell.execute_reply.started":"2022-06-15T07:21:17.003212Z","shell.execute_reply":"2022-06-15T07:21:17.023744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preprocessing \n# problem specific functions for preprocessing\n\ndef data_load(train_path, test_path):\n    \"\"\"loads all data\"\"\"\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n    train, val = train_test_split(train, test_size=0.2, random_state=42)\n    return train, test, val\n\n\ndef column_transformer1(data):\n    \"\"\"expanding cabin and passeengerid\"\"\"\n    data.columns = [i.lower() for i in data.columns]\n    data[['deck', 'num', 'side']] = data['cabin'].str.split('/', expand=True)\n    data['num'].astype(float)\n    data.drop(['cabin', 'name','num','side'], axis=1, inplace=True)\n    data[['group', 'passenger']] = data['passengerid'].str.split(\n        '_', expand=True).astype(int)\n    data.drop('passengerid', axis=1, inplace=True)\n    return data\n\n\ndef transformation(data):\n    \"\"\"some categorical ordinal mapping\"\"\"\n    planet = {'Earth': 1, 'Europa': 2, 'Mars': 3}\n    data.homeplanet = data.homeplanet.map(planet)\n    destination = {'TRAPPIST-1e': 1, '55 Cancri e': 2, 'PSO J318.5-22': 3}\n    data.destination = data.destination.map(destination)\n    data[['cryosleep', 'vip', ]] = data[['cryosleep', 'vip', ]].astype(float)\n    deck = {'F': 1, 'C': 2, 'G': 3, 'B': 4, 'E': 5, 'D': 6, 'A': 7, 'T': 8}\n    data.deck = data.deck.map(deck)\n    # data.drop(['side', 'num'], 1, inplace=True)\n    try:\n        # cuz we gonna use train data in this fn as well which doesnt have transported column\n        data['transported'] = data['transported'].astype(float)\n        return data\n    except:\n        return data\n\n\ndef impute_split(data):\n    # this is used for training knn\n    data_good = data[data.isna().sum(axis=1).eq(0)]\n    data_transformable = data[data.isna().sum(axis=1).eq(1)]  # transformed wwith knn\n    data_non_transformable = data[data.isna().sum(axis=1).gt(1)]\n    return data_good, data_transformable, data_non_transformable\n\n\nclass Knn_imputation:\n    \"\"\"defining knn classifier and regressor \"\"\"\n    knn = KNeighborsClassifier(n_neighbors=100)\n    knn_r = KNeighborsRegressor(n_neighbors=100)\n\n    def __init__(self, training_data, transforming_data, rest_of_data, categorical, numerical):\n        \"\"\"instance takes input training and transforming data\"\"\"\n        self.training_data = training_data\n        self.transforming_data = transforming_data\n        self.rest_of_data = rest_of_data\n        self.categorical = categorical\n        self.numerical = numerical\n\n    def trans_cols(self):\n        cat = [i for i in self.transforming_data.columns[self.transforming_data.isna(\n        ).sum().gt(0)] if i in self.categorical]\n        num = [i for i in self.transforming_data.columns[self.transforming_data.isna(\n        ).sum().gt(0)] if i in self.numerical]\n        return cat, num\n\n    def knn_impute_cat(self):\n        \"\"\"columns must be categorical\n            columns are the columns which has non zero nan values\n           trasformable data are data you wanna impute using knn \n           but only has esxactly one nan value per row ,so that we \n           can train it efectively\"\"\"\n        cat_columns, _ = self.trans_cols()\n        for i in cat_columns:\n            train_x = self.training_data.drop(i, axis=1)\n            train_y = self.training_data[i]\n            test_x = self.transforming_data[self.transforming_data[i].isna()]\n            test_x = test_x.drop(i, axis=1)\n            self.knn.fit(train_x, train_y)\n            preds = self.knn.predict(test_x)\n            ind = test_x.index\n            self.transforming_data.loc[ind, i] = preds\n        return self.transforming_data\n\n    def knn_impute_num(self):\n        \"\"\"columns must be categorical\n            columns are the columns which has non zero nan values\n           trasformable data are data you wanna impute using knn \n           but only has esxactly one nan value per row ,so that we \n           can train it efectively\"\"\"\n        _, num_columns = self.trans_cols()\n        for i in num_columns:\n            train_x = self.training_data.drop(i, axis=1)\n            train_y = self.training_data[i]\n            test_x = self.transforming_data[self.transforming_data[i].isna()]\n            test_x = test_x.drop(i, axis=1)\n            self.knn_r.fit(train_x, train_y)\n            preds = self.knn_r.predict(test_x)\n            ind = test_x.index\n            self.transforming_data.loc[ind, i] = preds\n        return self.transforming_data\n\n    def knn_implement(self):\n        \"\"\"implementing knn imputation\"\"\"\n        cat, num = self.trans_cols()\n        data_transformable_im = self.knn_impute_cat()\n        data_transformable_im = self.knn_impute_num()\n        result = pd.concat([self.training_data, data_transformable_im,\n                            self.rest_of_data]).sort_index(ascending=True)\n        return result\n\n\ndef simple_im(train, test, val, categorical, numerical):\n    # we dont want to impute names\n    # but we'd impute the rest of the things based i=on median and equalent strategies\n\n    im_c = SimpleImputer(strategy='most_frequent')\n    im_n = SimpleImputer(strategy='median')\n\n    train[categorical] = im_c.fit_transform(train[categorical])\n    train[numerical] = im_n.fit_transform(train[numerical])\n\n    val[categorical] = im_c.transform(val[categorical])\n    val[numerical] = im_n.transform(val[numerical])\n\n    test['transported'] = np.zeros([test.shape[0]])\n    test[categorical] = im_c.transform(test[categorical])\n    test[numerical] = im_n.transform(test[numerical])\n    test.drop('transported', axis=1, inplace=True)\n    return train, test, val","metadata":{"execution":{"iopub.status.busy":"2022-06-15T07:21:17.026833Z","iopub.execute_input":"2022-06-15T07:21:17.027479Z","iopub.status.idle":"2022-06-15T07:21:17.072698Z","shell.execute_reply.started":"2022-06-15T07:21:17.027441Z","shell.execute_reply":"2022-06-15T07:21:17.071643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features\n# useful functions generally for feature engineering\n\ndef mutual_information(x,y,mask=None):\n    \"\"\"function calculates the mi score in descendinhg trend given x and y\"\"\"\n    if mask is not None:\n        mi=mutual_info_classif(x.iloc[:,:mask],y)\n        mi=pd.DataFrame(mi,columns=['mi_score'],index=x.columns[:mask])\n    elif mask is None:  \n        mi=mutual_info_classif(x,y)\n        mi=pd.DataFrame(mi,columns=['mi_score'],index=x.columns)\n        \n    mi=mi.sort_values(\"mi_score\",ascending=False)\n    return mi\n\n\ndef pca_ing(x,standardize=True):\n    \"\"\"function standardizes the data is not standardized and performs pca and outputs its componets in a df also loadings\"\"\"\n    if standardize:\n        sc=StandardScaler()\n        x_scaled=sc.fit_transform(x)\n        x=pd.DataFrame(x_scaled,columns=x.columns)\n    pca=PCA()\n    x_pca=pca.fit_transform(x)\n    components=[f'pca_{i}' for i in x.columns.values]\n    x_pca=pd.DataFrame(x_pca,columns=components)\n    loadings=pd.DataFrame(pca.components_.T,columns=components,index=x.columns)\n    return x_pca,loadings\n\ndef auto_best_features(x,y,other_data,n_features,standardize_on_pca=True):\n    \"\"\"best features(having most mi scores) among all of x and its pca \"\"\"\n    x_pca,_=pca_ing(x,standardize=standardize_on_pca)\n    x.reset_index(drop=True,inplace=True)\n    all_features=x.join(x_pca)\n    mutual_info=mutual_information(all_features,y)\n    selected_cols=mutual_info.index.values[:n_features]\n    other_data_selected=[]\n    for i in other_data:\n        i_pca,_=pca_ing(i,standardize=standardize_on_pca)\n        i.reset_index(drop=True,inplace=True)\n        i_all_features=i.join(i_pca)\n        other_data_selected.append(i_all_features[selected_cols])\n    return all_features[selected_cols],other_data_selected            \n        \n\n\n#new features\n# problem specific\n# here i'm not using this function much better features can be made after a good eda but,these are some potential good ones\ndef create_features(data):\n    \"\"\"function creates the following sorts of features for a given data\"\"\"\n    \n    #technical ones\n    data[['roomservice','foodcourt','shoppingmall','spa','vrdeck']]=data[['roomservice','foodcourt','shoppingmall','spa','vrdeck']].apply(np.log1p)\n    \n    #feature development basis \n    data['mall_avrg']= data.groupby(['deck','homeplanet'])['shoppingmall'].transform('mean')\n    data['food_avrg']= data.groupby(['deck','homeplanet'])['foodcourt'].transform('mean')\n    data['cnt_deckplanet']= data.groupby(['deck','homeplanet'])['homeplanet'].transform('count')\n    \n    # cryosleep feature (it has high mutual info)\n    data['cnt_cryodeckplnt']= data.groupby(['deck','homeplanet'])['cryosleep'].transform('count')\n    data['total_spend']=data[['roomservice','foodcourt','shoppingmall','spa','vrdeck']].sum(axis=1)\n    data['spend_sub1']=data[['foodcourt','shoppingmall']].sum(axis=1)\n    data['spend_sub2']=data[['roomservice','spa','vrdeck']].sum(axis=1)\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-06-15T07:21:17.076225Z","iopub.execute_input":"2022-06-15T07:21:17.076675Z","iopub.status.idle":"2022-06-15T07:21:17.109966Z","shell.execute_reply.started":"2022-06-15T07:21:17.076641Z","shell.execute_reply":"2022-06-15T07:21:17.108849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model\n\nmodel=RandomForestClassifier()\n\ndef results(trainx,trainy,valx,valy,test,params,model=model):\n    \"\"\"function that trains a model and validates it\"\"\"\n    model=model.set_params(**params)\n    folds=KFold(n_splits=5) #once set this seems reproduceable than settting cv in cross val score\n    tr_score=cross_val_score(model,trainx,trainy,cv=folds,scoring='accuracy').mean()\n    model.fit(trainx,trainy)\n    vl_score=accuracy_score(model.predict(valx),valy)\n    test_preds=model.predict(test)\n    return tr_score,vl_score,test_preds","metadata":{"execution":{"iopub.status.busy":"2022-06-15T07:21:17.111956Z","iopub.execute_input":"2022-06-15T07:21:17.112682Z","iopub.status.idle":"2022-06-15T07:21:17.124645Z","shell.execute_reply.started":"2022-06-15T07:21:17.112627Z","shell.execute_reply":"2022-06-15T07:21:17.123415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimization\n# bayesian search implementation\np_space={\n        'n_estimators':scope.int(hp.quniform('n_estimators',int(10),int(100),int(1))),\n        'max_depth':scope.int(hp.quniform('max_depth',1,50,1)),\n        'min_samples_split':scope.int(hp.quniform('min_samples_split',2,20,1)),\n        'min_samples_leaf':scope.int(hp.quniform('min_samples_leaf',2,20,1)),\n        'max_features':hp.quniform('max_features',0.1,1,0.1),\n        'bootstrap':hp.choice('bootstrap',[True,False]),\n        'criterion':hp.choice('criterion',['gini','entropy'])\n        }\n\nrf=model\ndef optimizer(param_space,trainx,trainy,valx,valy,model=rf):\n    model=rf.set_params(**param_space)\n    model.fit(trainx,trainy)\n    acc=accuracy_score(model.predict(valx),valy)\n    return -1*acc\n\ndef bayesian_search(trainx,trainy,valx,valy,param_space=p_space):\n    trials=Trials()\n    op_fn=partial(optimizer,trainx=trainx,trainy=trainy,valx=valx,valy=valy)\n    result=fmin(fn=op_fn,\n            space=param_space,\n            algo=tpe.suggest,\n            trials=trials,\n            max_evals=100\n            )\n    result['n_estimators']=int(result['n_estimators'])\n    result['min_samples_split']=int(result['min_samples_split'])\n    result[ 'min_samples_leaf']=int(result[ 'min_samples_leaf'])\n    result[ 'criterion']=['gini','entropy'][int(result[ 'criterion'])]\n    return result\n\n\ndef combining_models(preds1,preds2,preds3):\n    \"\"\"this function combines models predictions to make a final prediction\"\"\"\n    preds=preds1+preds2+preds3\n    preds[preds>1]=1\n    preds[preds==1]==0\n    return preds\n","metadata":{"execution":{"iopub.status.busy":"2022-06-15T07:21:17.126627Z","iopub.execute_input":"2022-06-15T07:21:17.127674Z","iopub.status.idle":"2022-06-15T07:21:17.171137Z","shell.execute_reply.started":"2022-06-15T07:21:17.127393Z","shell.execute_reply":"2022-06-15T07:21:17.169833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading\ntrain_path = '../input/spaceship-titanic/train.csv'\ntest_path = '../input/spaceship-titanic/test.csv'\ntrain, test, val = data_load(train_path, test_path)\n\n\n# transformation\ntrain = column_transformer1(train)\nval = column_transformer1(val)\ntest = column_transformer1(test)\ncategorical = train.select_dtypes('object', 'category')\nbool_columns = train.select_dtypes('bool')\ncategorical = categorical.join(bool_columns).columns\nnumerical = train.select_dtypes('number').columns\ntrain_copy = train.copy()\nval_copy = val.copy()\ntest_copy = test.copy()\n\n\n# more transformation\ntrain = transformation(train)\nval = transformation(val)\ntest = transformation(test)\ntrain_good, train_transformable, train_non_transformable = impute_split(train)\nval_good, val_transformable, val_non_transformable = impute_split(val)\ntest_good, test_transformable, test_non_transformable = impute_split(test)\n\n\n# imputing using knn\ntrain_imputer = Knn_imputation(\n    train_good, train_transformable, train_non_transformable, categorical, numerical)\ntrain = train_imputer.knn_implement()\nval_imputer = Knn_imputation(\n    val_good, val_transformable, val_non_transformable, categorical, numerical)\nval = val_imputer.knn_implement()\ntest_imputer = Knn_imputation(\n    test_good, test_transformable, test_non_transformable, categorical, numerical)\ntest = test_imputer.knn_implement()\ntrain, test, val = simple_im(train, test, val, categorical, numerical)\n\n\n# preparing data\ntrainx = train.drop(['transported'], axis=1)\ntrainy = train['transported'].astype(int)\nvalx = val.drop(['transported'], axis=1)\nvaly = val['transported'].astype(int)\n\n# selecting best features\ntrainx, other = auto_best_features(trainx,trainy,[valx, test], n_features=15, standardize_on_pca=True)\nvalx, test = other[0], other[1]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T07:21:17.174537Z","iopub.execute_input":"2022-06-15T07:21:17.177198Z","iopub.status.idle":"2022-06-15T07:21:19.093748Z","shell.execute_reply.started":"2022-06-15T07:21:17.175897Z","shell.execute_reply":"2022-06-15T07:21:19.092492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features density plot\nplotter(trainx,trainx.columns,(10,10))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T07:21:19.095506Z","iopub.execute_input":"2022-06-15T07:21:19.096346Z","iopub.status.idle":"2022-06-15T07:21:22.70836Z","shell.execute_reply.started":"2022-06-15T07:21:19.096297Z","shell.execute_reply":"2022-06-15T07:21:22.707567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features mutual info\nmi=mutual_information(trainx,trainy)\nplotmi(mi)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T07:21:22.709587Z","iopub.execute_input":"2022-06-15T07:21:22.710514Z","iopub.status.idle":"2022-06-15T07:21:23.509273Z","shell.execute_reply.started":"2022-06-15T07:21:22.710472Z","shell.execute_reply":"2022-06-15T07:21:23.508101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluating\nbest_params = bayesian_search(trainx,trainy,valx,valy)\ntrain_score, val_score,test_preds = results(trainx,trainy,valx,valy,test,best_params)\nprint('train score:', train_score)\nprint('val score:', val_score)\nfetch_submission(test_preds)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T07:21:23.511486Z","iopub.execute_input":"2022-06-15T07:21:23.511851Z","iopub.status.idle":"2022-06-15T07:25:47.336491Z","shell.execute_reply.started":"2022-06-15T07:21:23.51182Z","shell.execute_reply":"2022-06-15T07:25:47.335457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr=Learning_curve(trainx,trainy,valx,valy,RandomForestClassifier(**best_params))\nlr.plot()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T07:25:47.338016Z","iopub.execute_input":"2022-06-15T07:25:47.338579Z","iopub.status.idle":"2022-06-15T07:28:17.791896Z","shell.execute_reply.started":"2022-06-15T07:25:47.338531Z","shell.execute_reply":"2022-06-15T07:28:17.790698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}