{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"0\"></a>\n# Survival in space","metadata":{}},{"cell_type":"markdown","source":"<b>CONTENTS</b>\n<br>\n\n- [1. Problem statement](#1)<br></li>\n- [2. Cleaning data](#2)<br>\n    - [2.1 Managing null values](#2.1)<br>\n    - [2.2 Preprocessing data](#2.2)<br>\n- [3. Visualizing data](#3)<br>\n- [4. Classification](#4)<br>\n    - [4.1 Logistic regression](#4.1)<br>\n    - [4.2 Random forest](#4.2)<br>\n    - [4.3 LGBM](#4.3)<br>\n- [5. Model choice and submission](#5)<br>\n\n</p>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n## 1. Problem Statement\n[Back to top](#0)<br>\n\nWe have a dataset containing the data of passenger aboard *Spaceship Titanic*. \nWe need to predict whether passengers have been transported to another dimension or have \"survived\".\n\nWe can avail ourselves of a *labeled* training dataset, hence this is a typical **supervised classification problem**.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n## 2. Cleaning data\n[Back to top](#0)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random","metadata":{"execution":{"iopub.status.busy":"2022-06-13T15:15:02.013771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading datasets\ntrain_data, test_data = pd.read_csv(\"/kaggle/input/spaceship-titanic/train.csv\"), pd.read_csv(\"/kaggle/input/spaceship-titanic/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T09:26:29.334297Z","iopub.execute_input":"2022-06-11T09:26:29.335078Z","iopub.status.idle":"2022-06-11T09:26:29.351709Z","shell.execute_reply.started":"2022-06-11T09:26:29.33504Z","shell.execute_reply":"2022-06-11T09:26:29.350739Z"}}},{"cell_type":"markdown","source":"So we have quite some **categorical** as well as **numerical** columns. \n\n* Categorical: `HomePlanet`, `CryoSleep`, `Destination`, `VIP`\n* Numerical: `Age`, `RoomService`, `FoodCourt`, `ShoppingMall`, `Spa`, `VRDeck`\n\nThe 'Name' columns might be useful to check whether people belong to the same family together with the group information encoded in the 'PassengerId' columns (the idea being if two people share a last name and are in the same group they are probably related).\nNote that 'Age' might help with determining what family relation it is. But maybe it's an inference too far. \n\nSince \n> Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n\nit probably makes sense to split that column into \"Deck\" and \"Side\". I am still unsure whether the cabin number is relevant.\n\nThe *target* is encoded as a `bool` in the `Transported` column.\n\nFinally, we not that there are **null values** in all the columns except for `PassengerId` and `Transported`. However, these should be ok for the numerical columns as they represent the fact that some passenger havent' spent any money on extra services.","metadata":{}},{"cell_type":"code","source":"train_data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1\"></a>\n### 2.1 Managing null values\n[Back to top](#0)","metadata":{}},{"cell_type":"markdown","source":"Since we do have nulls, let's dig in and manage them.","metadata":{}},{"cell_type":"code","source":"train_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[train_data[\"Cabin\"].isnull() == True].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's fill the missing `Destination` `HomePlanet` and `CryoSleep` proportionally.","metadata":{}},{"cell_type":"code","source":"def fill_proportionally(col, dataset):\n    values = dataset[col].dropna().unique()\n    # getting weights for probability weighting\n    weights = dataset[col].value_counts().values / dataset[col].value_counts().values.sum()\n    # filling\n    dataset[col] = dataset[col].apply(lambda x: random.choices(values, weights=weights)[0] if pd.isnull(x) else x)\n    # checking\n    assert dataset[col].isna().sum() == 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in [\"Destination\", \"HomePlanet\", \"CryoSleep\"]:\n    fill_proportionally(column, train_data)\n    fill_proportionally(column, test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For simplicity let's `fillna` `VIP` as `False` as it's residual anyway and let's use the *median* for the `Age` columns and the other numerical ones.","metadata":{}},{"cell_type":"code","source":"for col in [\"RoomService\", \n            \"FoodCourt\", \n            \"ShoppingMall\",\n            \"Spa\", \n            \"VRDeck\",\n            \"Age\"]:\n                train_data[col].fillna(train_data[col].median(), inplace=True)\n                test_data[col].fillna(train_data[col].median(), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data[\"VIP\"].fillna(False, inplace=True)\ntrain_data[\"VIP\"].fillna(False, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.isna().sum())\nprint(\"Missing data %: \", train_data.isna().sum().sum()/train_data.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We still have ca. 400 NaNs. That's approx 4%, half of which in the `Name` column we are going to drop anyway. I could live with just filling those as empty strings and assigning the missing cabin to some dummy value such as `Z/0000/Z`.","metadata":{}},{"cell_type":"code","source":"train_data[\"Name\"].fillna(\"\", inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape, train_data.isna().sum().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"Cabin\"].fillna(\"Z/0000/Z\", inplace=True)\ntest_data[\"Cabin\"].fillna(\"Z/0000/Z\", inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"Cabin\"].value_counts(dropna=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly more than up to 8 passengers stayed in the same cabin. ","metadata":{}},{"cell_type":"code","source":"train_data[train_data[\"Cabin\"] == \"G/734/S\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Doesn't look like last name is giving any indication of parenthood worth pursuing.","metadata":{}},{"cell_type":"code","source":"train_data[\"Deck\"] = train_data[\"Cabin\"].apply(lambda x: str(x)[0])\ntest_data[\"Deck\"] = test_data[\"Cabin\"].apply(lambda x: str(x)[0]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"Deck\"].value_counts(dropna=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data[\"Deck\"].value_counts(dropna=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"Side\"] = train_data[\"Cabin\"].apply(lambda x: str(x).split(\"/\")[2])\ntest_data[\"Side\"] = test_data[\"Cabin\"].apply(lambda x: str(x).split(\"/\")[2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_data[\"Side\"].value_counts(dropna=False))\nprint(train_data[\"Side\"].value_counts(dropna=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's find out how many passengers were travelling in groups. The first part of `PassengerId` is in fact a Group ID, with the second being the progressive number of the passenger in that group.","metadata":{}},{"cell_type":"code","source":"train_data[\"GroupId\"] = train_data[\"PassengerId\"].apply(lambda x: x.split(\"_\")[0])\ntest_data[\"GroupId\"] = test_data[\"PassengerId\"].apply(lambda x: x.split(\"_\")[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"GroupIdProgNumber\"] = train_data[\"PassengerId\"].apply(lambda x: x.split(\"_\")[1])\ntest_data[\"GroupIdProgNumber\"] = test_data[\"PassengerId\"].apply(lambda x: x.split(\"_\")[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"groups = train_data[train_data[\"GroupId\"].duplicated()][\"GroupId\"]\ntrain_data[\"InGroup\"] = train_data[\"GroupId\"].apply(lambda x: x in groups.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"groups = test_data[test_data[\"GroupId\"].duplicated()][\"GroupId\"]\ntest_data[\"InGroup\"] = test_data[\"GroupId\"].apply(lambda x: x in groups.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"InGroup\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"GroupSize\"] = train_data[\"GroupId\"].apply(lambda x: train_data[\"GroupId\"].value_counts().loc[x])\ntest_data[\"GroupSize\"] = test_data[\"GroupId\"].apply(lambda x: test_data[\"GroupId\"].value_counts().loc[x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"GroupSize\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It might be interesting to find out how many of the passengers in the same group were also in the same cabin. These are possibly also a family.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\" class=\"anchor\"></a>\n## 3. Visualizing data\n[Back to top](#0)<br>\n\nLet's first take a look at the distribution of categorical values.","metadata":{}},{"cell_type":"code","source":"columns_to_plot = [\"Destination\", \"VIP\", \"HomePlanet\", \"InGroup\", \"CryoSleep\", \"Transported\"]\nrows = 3\ncolumns = 2\nix = 0\nfig, axes = plt.subplots(rows, columns, figsize=(9, 7))\nfor row in range(rows):\n    for col in range(columns):\n        try:\n            sns.countplot(data=train_data, x=columns_to_plot[ix], ax=axes[row][col])\n            sns.despine()\n            ix += 1\n        except Exception:\n            axes[row][col].set_visible(False)\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* By far the vast majority of people were heading to TRAPPIST-1e\n* Very few passengers were VIPs\n* Earth was the most common HomePlanet with Mars and Europa more or less equivalently represented\n* More than half of the passengers travelled alone\n* Approx. 35% of the passengers were in CryoSleep\n* Passengers had an overall even chance of being transported\n\nImportantly, the **target** column doesn't exhibit any noticeable **class imbalance** so we won't need to do extra work on that.\n\nNow let's look at relations between these and being transported.","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=train_data, x=\"HomePlanet\", hue=\"Transported\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You are less likely to be transported if you are from Earth.","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=train_data, x=\"Destination\", hue=\"Transported\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Passengers headed to TRAPPIST-1e are less likely to have been transported. Passengers headed to 55 Cancri e are more likely.","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=train_data, x=\"VIP\", hue=\"Transported\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Being a VIP doesn't seem to significantly affect your chances of being transported.","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=train_data, x=\"InGroup\", hue=\"Transported\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Passengers travelling alone are less likely to have been transported","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=train_data, x=\"CryoSleep\", hue=\"Transported\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Passengers in cryosleep had less chances of being transported","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=\"Side\", data=train_data, hue=\"Transported\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Being Starboard-side meant more likelihood of being transported.","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=\"Deck\", data=train_data, hue=\"Transported\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The choice of decks does seem to have some effect on being transported (e.g for deck B increasing likelihood of transportation and deck F decreasing it). \nWe'd expect:\n- passengers on decks B *and* port-side to be much _more_ likely to be transported. \n- passengers on decks F *and* starboard-side to be much _less_ likely to be transported.\n\nLet's see.\n\nIt  might also make sense to consider assigning passengers in deck \"T\" to deck \"F\" as it look like they're outliers or just a mistake.","metadata":{}},{"cell_type":"code","source":"train_data[\"Deck\"] = train_data[\"Deck\"].apply(lambda x: \"F\" if x==\"T\" else x)\ntest_data[\"Deck\"] = test_data[\"Deck\"].apply(lambda x: \"F\" if x==\"T\" else x)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(x=\"Deck\", data=train_data, hue=\"Transported\", col=\"Side\", kind=\"count\")\nsns.despine()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=\"GroupSize\", data=train_data, hue=\"Transported\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This fits with out previous finding up to an extent. \n\nNow let's take a look at numerical features.","metadata":{}},{"cell_type":"code","source":"sns.kdeplot(data=train_data, x=\"Age\", hue=\"Transported\", fill=True)\nplt.title(\"Age distribution\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Age does seem to affect chances of being transported but not to a great extent. To turn this into a useful feature we could add an `AgeGroup` feature.","metadata":{}},{"cell_type":"code","source":"min_age, max_age = train_data[\"Age\"].min(), train_data[\"Age\"].max()\nbins = np.linspace(min_age,max_age, 6)\nprint(bins)\nlabels = [\"Child\", \"Young\", \"Middle\", \"Senior\", \"Elder\"]\ntrain_data[\"AgeGroup\"] = pd.cut(train_data[\"Age\"], bins=bins, labels=labels, include_lowest=True)\nsns.countplot(data=train_data, x=\"AgeGroup\", hue=\"Transported\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data[\"AgeGroup\"] = pd.cut(test_data[\"Age\"], bins=bins, labels=labels, include_lowest=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"all\"] = \"\"\nsns.violinplot(data=train_data, y=\"Age\", x=\"all\", hue=\"Transported\", split=True);\ntrain_data.drop(\"all\", axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_to_plot = train_data.describe().columns\nrows=3\ncols=2\n\nfig, axes = plt.subplots(rows, cols, figsize=(12,8)) \nix = 0\nfor i in range(rows):\n    for j in range(cols):\n        sns.kdeplot(x=data_to_plot[ix], ax=axes[i][j], hue=\"Transported\", data=train_data, fill=True)\n        sns.despine()\n        ix += 1\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Expenditure does seem to affect chance of transportation, especially for certain types of expense.","metadata":{}},{"cell_type":"code","source":"data_to_plot = train_data.describe().columns\nrows=3\ncols=2\n\nfig, axes = plt.subplots(rows, cols, figsize=(12,8)) \nix = 0\nfor i in range(rows):\n    for j in range(cols):\n        sns.boxenplot(x=data_to_plot[ix], ax=axes[i][j], data=train_data)\n        sns.despine()\n        ix += 1\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It might make sense to get rid of at least some of those **outliers** in the numerical columns.","metadata":{}},{"cell_type":"code","source":"sns.kdeplot(data=train_data, x=\"Spa\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"capped = train_data.copy()\nupper_limit = train_data[\"RoomService\"].quantile(0.75)\nlower_limit = test_data[\"RoomService\"].quantile(0.25)\niqr = upper_limit - lower_limit\nupper_limit += iqr * 1.5\nlower_limit -= iqr * 1.5\ncapped[\"RoomService\"] = np.where(capped[\"RoomService\"] > upper_limit, upper_limit, capped[\"RoomService\"])\ncapped[\"RoomService\"] = np.where(capped[\"Spa\"] < lower_limit, lower_limit, capped[\"RoomService\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(capped[\"RoomService\"].skew(), train_data[\"RoomService\"].skew())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2)\nsns.kdeplot(data=capped, x=\"RoomService\", ax=axes[0])\naxes[0].set_title(\"With outliers\")\nsns.kdeplot(data=train_data, x=\"RoomService\", ax=axes[1]);\naxes[1].set_title(\"Without outliers\");\nplt.tight_layout()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxenplot(data=capped, x=\"RoomService\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nfor col in ['RoomService','FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']:\n    upper_limit = train_data[col].quantile(0.75)\n    lower_limit = test_data[col].quantile(0.25)\n    iqr = upper_limit - lower_limit\n    upper_limit += iqr * 1.5\n    lower_limit -= iqr * 1.5\n    train_data[col] = np.where(train_data[col] > upper_limit, upper_limit, train_data[col])\n    train_data[col] = np.where(train_data[col] < lower_limit, lower_limit, train_data[col])\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfor col in ['RoomService','FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']:\n    train_data[col] = np.log(1 + train_data[col])\n    test_data[col] = np.log(1+ test_data[col])\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's drop columns we won't need and reorder our dataframes to get to the modelling part.","metadata":{}},{"cell_type":"code","source":"train_data = train_data[['HomePlanet', 'CryoSleep', 'Destination', 'AgeGroup', 'VIP', 'RoomService',\n       'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Deck',\n       'Side', 'InGroup', 'GroupSize', 'Age', 'Transported']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = test_data[['HomePlanet', 'CryoSleep', 'Destination', 'AgeGroup', 'VIP', 'RoomService',\n       'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Deck',\n       'Side', 'InGroup', 'GroupSize', 'Age']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a><br>\n## 4. Classification\n[Back to top](#0)<br>","metadata":{}},{"cell_type":"markdown","source":"I am initializing an empty list to store (as tuples= the models together with accuracy and f1-scores.","metadata":{}},{"cell_type":"code","source":"models = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First I will make copies of the data. This will be useful to test different hypotheses without having to reprocess the data.","metadata":{}},{"cell_type":"code","source":"train_dataset = train_data.copy()\ntest_dataset = test_data.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categoricals = ['HomePlanet', 'CryoSleep', 'Destination', 'Deck', 'Side', 'InGroup', 'AgeGroup']\nnumericals = ['VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'GroupSize', 'Age']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.1\"></a><br>\n## 4.1 Logistic Regression\n[Back to top](#0)<br>","metadata":{}},{"cell_type":"markdown","source":"### Training the model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, learning_curve, KFold, GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, roc_curve","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.columns, test_data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer = ColumnTransformer([\n    (\"num\", StandardScaler(), numericals),\n    (\"cat\", OneHotEncoder(), categoricals),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline = Pipeline([\n    ('transformer', transformer),\n    ('classifier', LogisticRegression(max_iter=500))\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_data.iloc[:, :-1]\ny = train_data.iloc[:, -1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = KFold(n_splits=10, random_state=42, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Mean accuracy of K-fold cross validation: {:.2f} %\".format(np.mean(scores) * 100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analyzing the results","metadata":{}},{"cell_type":"code","source":"preds = pipeline.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Now we can look at the results of the model's predictions. Note that since the target values were balanced, we can use **f1-score** as the most relevant metric (even accuracy would be ok).","metadata":{}},{"cell_type":"code","source":"ConfusionMatrixDisplay.from_predictions(y_test, preds);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Training set accuracy score: { accuracy_score(y_train, pipeline.predict(X_train))*100:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Validation scores\")\nprint(\"=\"*25)\nprint(f\"Accuracy score: { accuracy_score(y_test, preds)*100:.2f}\")\nprint(f\"Precision score: { precision_score(y_test, preds)*100:.2f}\")\nprint(f\"Recall score: { recall_score(y_test, preds)*100:.2f}\")\nprint(f\"F-1 score: {f1_score(y_test, preds)*100:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logreg_pipeline = pipeline\nmodels.append((logreg_pipeline, accuracy_score(y_test, preds), f1_score(y_test, preds)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We could also analyze false positives and false negatives","metadata":{}},{"cell_type":"code","source":"FN = X_test[(preds==False) & (y_test==True)]\nFP = X_test[(preds==True) & (y_test==False)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diff_fn = FN.describe()-X_test.describe()\ndiff_fn.loc[[\"mean\", \"std\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diff_fp = FP.describe()-X_test.describe()\ndiff_fp.loc[[\"mean\", \"std\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see if there is any one feature patently responsible for the FN","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2)\nsns.countplot(data=FN, x=\"VIP\", ax=axes[0])\nsns.countplot(data=test_data, x=\"VIP\", ax=axes[1]);\naxes[0].set_title(\"False Negatives\")\naxes[1].set_title(\"Test Data\")\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = \"Deck\"\nfig, axes = plt.subplots(1,2)\nsns.countplot(data=FN, x=col, ax=axes[0], order=FN[col].value_counts().index)\nsns.countplot(data=test_data, x=col, ax=axes[1], order=train_data[col].value_counts().index)\naxes[0].set_title(\"False Negatives\")\naxes[1].set_title(\"Test Data\")\nplt.suptitle(col)\nplt.tight_layout();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col=\"CryoSleep\"\nfig, axes = plt.subplots(1,2)\nsns.countplot(data=FN, x=col, ax=axes[0], order=FN[col].value_counts().index)\nsns.countplot(data=test_data, x=col, ax=axes[1], order=train_data[col].value_counts().index)\naxes[0].set_title(\"False Negatives\")\naxes[1].set_title(\"Test Data\")\nplt.suptitle(col)\nplt.tight_layout();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This could actually be interesting. It's as though `Cryosleep` was underweighted. I have no idea how to manage this.... ","metadata":{}},{"cell_type":"markdown","source":"And finally let's see what were the most relevant features","metadata":{}},{"cell_type":"code","source":"coefficient_importance = list(zip(pipeline[\"transformer\"].get_feature_names_out(), pipeline[\"classifier\"].coef_[0]))\ncoefficient_importance.sort(key=lambda x: x[1])\ncoefficient_importance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of these results are consistent with our EDA. ","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=train_data, x=\"HomePlanet\", hue=\"Transported\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=train_data, x=\"Deck\", hue=\"Transported\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(10,5))\nsns.kdeplot(data=train_data, x=\"FoodCourt\", hue=\"Transported\", fill=True, ax=axes[0]);\nsns.kdeplot(data=train_data, x=\"Spa\", hue=\"Transported\", fill=True, ax=axes[1]);\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2\"></a>\n## 4.2 Random Forest Classifier\n[Back to top](#0)<br>","metadata":{}},{"cell_type":"markdown","source":"Reference: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom pprint import pprint","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y = train_dataset.copy().iloc[:, :-1], train_dataset.copy().iloc[:, -1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We are going to start with a base estimator for benchmarking\n- Then we are going to narrow down the sapce of hyperparams with RandomSearch\n- Finally we will finetune the hyperparams using GridSearch","metadata":{}},{"cell_type":"code","source":"base_pipeline = Pipeline([\n    ('transformer', \n         ColumnTransformer([\n            (\"cat\", OneHotEncoder(), categoricals),\n            (\"num\", StandardScaler(), numericals),\n        ])\n    ),\n    ('classifier', RandomForestClassifier(n_estimators=10))\n])\nbase_pipeline.fit(X_train, y_train)\nbase_preds = base_pipeline.predict(X_test)\nbase_accuracy = accuracy_score(y_test, base_preds)\nprint(\"Base model accuracy: {:.2f} %\".format(base_accuracy*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_pipeline = Pipeline([\n    ('transformer', transformer),\n    ('classifier', RandomForestClassifier())\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4, 6]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'classifier__n_estimators': n_estimators,\n               'classifier__max_features': max_features,\n               'classifier__max_depth': max_depth,\n               'classifier__min_samples_split': min_samples_split,\n               'classifier__min_samples_leaf': min_samples_leaf,\n               'classifier__bootstrap': bootstrap}\npprint(random_grid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_pipeline = Pipeline([\n    ('transformer', \n         ColumnTransformer([\n            (\"cat\", OneHotEncoder(handle_unknown = \"ignore\"), categoricals),\n            (\"num\", StandardScaler(), numericals),\n        ])\n    ),\n    ('classifier', RandomForestClassifier())\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_random = RandomizedSearchCV(estimator = rf_pipeline, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=1, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_random = rf_random.best_estimator_\nbest_random.fit(X_train, y_train)\nrandom_preds = best_random.predict(X_test)\nrandom_accuracy = accuracy_score(y_test, random_preds)\nprint(\"Random accuracy: {:.2f} %\".format(random_accuracy*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_random.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\n    'classifier__bootstrap': [True],\n    'classifier__max_depth': [110, 220, 80],\n    'classifier__max_features': ['auto'],\n    'classifier__min_samples_leaf': [4, 6, 8],\n    'classifier__min_samples_split': [2, 4, 6, 12],\n    'classifier__n_estimators': [377, 450, 800]\n}\n\n\ngrid_pipeline = Pipeline([\n    ('transformer', \n         ColumnTransformer([\n            (\"cat\", OneHotEncoder(handle_unknown = \"ignore\"), categoricals),\n            (\"num\", StandardScaler(), numericals),\n        ])\n    ),\n    ('classifier', RandomForestClassifier())\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search = GridSearchCV(estimator = grid_pipeline, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_grid = grid_search.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_preds = best_grid.predict(X_test)\ngrid_accuracy = accuracy_score(y_test, grid_preds)\ngrid_f1 = f1_score(y_test, grid_preds)\nprint(\"Grid accuracy: {:.2f} %\".format(grid_accuracy*100))\nprint(\"Grid f1: {:.2f} %\".format(grid_f1*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_pipeline = best_grid\nmodels.append((rf_pipeline, grid_accuracy, grid_f1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Improvement of {:0.2f}% vs. base.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))\nprint('Improvement of {:0.2f}% vs. random.'.format( 100 * (grid_accuracy - random_accuracy) / random_accuracy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.3\"></a>\n## 4.3 Gradient Boosting Classifier\n[Back to top](#0)<br>","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_pipeline = Pipeline([\n    (\"transformer\", transformer),\n    (\"classifier\", LGBMClassifier())\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_pipeline.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_preds = lgbm_pipeline.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_accuracy = accuracy_score(y_test, lgbm_preds)\nlgbm_f1 = f1_score(y_test, lgbm_preds)\nprint(lgbm_accuracy, lgbm_f1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"LGBM accuracy: {:.2f} %\".format(grid_accuracy*100))\nprint(\"LGBM f1: {:.2f} %\".format(grid_f1*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models.append((lgbm_pipeline, lgbm_accuracy, lgbm_f1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n## 5. Model choice and submission\n[Back to top](#0)<br>","metadata":{}},{"cell_type":"code","source":"models.sort(key=lambda x: x[2], reverse=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = models[0]\nfinal_predictor = best_model[0]\nfinal_predictor_name = str(final_predictor.get_params()[\"classifier\"])\nprint(final_predictor_name + \"\\nf1 score: \"+ str(best_model[2]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_df = pd.DataFrame.from_dict({\n    \"Models\": [str(model[0].get_params()[\"classifier\"]).split(\"(\")[0] for model in models],\n    \"Accuracy\":[ model[1] for model in models],\n    \"F1_score\": [model[2] for model in models]\n})\n\nmodels_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_data = test_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_submit = final_predictor.predict(final_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_submit = pd.DataFrame(to_submit, columns=[\"Transported\"])\nto_submit.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.concat([pd.read_csv(\"/kaggle/input/spaceship-titanic/test.csv\"), pd.DataFrame(to_submit)], axis=1)[[\"PassengerId\", \"Transported\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}