{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, we will use the technique of [ensembling](https://en.wikipedia.org/wiki/Ensemble_learning) to solve the [Spaceship Titanic](https://www.kaggle.com/competitions/spaceship-titanic) competition. Additionally, we will leverage the popular hyperparameter tuning package [Optuna](https://github.com/optuna/optuna) to tune our models.\n\nThe objective is to use ensembling in the form of [stacking](https://en.wikipedia.org/wiki/Ensemble_learning#Stacking), with the following models making up the stack:\n* [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)\n* [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost) with [Decision Tree](https://en.wikipedia.org/wiki/Decision_tree_learning) as its base classifier\n* [Random Forest](https://en.wikipedia.org/wiki/Random_forest)\n* [XGBoost](https://en.wikipedia.org/wiki/XGBoost)\n* [LightGBM](https://en.wikipedia.org/wiki/LightGBM)\n\nThis is Part 3 of a three part series:\n* Part 1: [Spaceship Titanic - Exploratory Data Analysis](https://www.kaggle.com/code/defcodeking/spaceship-titanic-exploratory-data-analysis)\n* Part 2: [Spaceship Titanic - Logistic Regression Baselines](https://www.kaggle.com/code/defcodeking/spaceship-titanic-logistic-regression-baselines)\n* Part 3: [Ensembling (And Optuna ðŸ˜‰) Is All You Need!](https://www.kaggle.com/code/defcodeking/ensembling-and-optuna-is-all-you-need) (you are here!)\n\n> Note: For a quickstart on Optuna, check out [Tutorial â€” Optuna](https://optuna.readthedocs.io/en/stable/tutorial/index.html).","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import functools\nimport os\nimport random\n\nimport lightgbm as lgb\nimport numpy as np\nimport optuna\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-10T13:25:55.901334Z","iopub.execute_input":"2022-06-10T13:25:55.901638Z","iopub.status.idle":"2022-06-10T13:25:55.908858Z","shell.execute_reply.started":"2022-06-10T13:25:55.901608Z","shell.execute_reply":"2022-06-10T13:25:55.907467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:55.937804Z","iopub.execute_input":"2022-06-10T13:25:55.938633Z","iopub.status.idle":"2022-06-10T13:25:55.94422Z","shell.execute_reply.started":"2022-06-10T13:25:55.938593Z","shell.execute_reply":"2022-06-10T13:25:55.943008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"markdown","source":"We will define a global `Config` class which holds common configuration.\n\nWe will also define a configuration class for each of the models that will be used in the ensemble. These classes will hold the default hyperparameters, static parameters (parameters that are not tuned), extra parameters to pass to `.fit()` etc., for the respective models.\n\nAdditionally, we will have a mapping between model names and their configuration classes so that we can get to them easily.\n\n> Note: The `get_fit_params()` method is required in the configuration classes of the models since there are subtle differences between what the `.fit()` method of each model accepts. For example, `LogisticRegression.fit()` has no parameter called `eval_set` while `XGBClassifier.fit()` does. `LogisticRegression` takes the `verbose` argument in the initializer while `XGBoost` takes it in `.fit()`. `XGBoost`, since version 1.6.0, has moved arguments like `callbacks` and `eval_metric` to the initializer while `LightGBM` continues to use them in `.fit()`. These differences are reconciled by dividing the arguments appropriately between `STATIC_PARAMS` and the output of `get_fit_params()`. ","metadata":{}},{"cell_type":"code","source":"class Config:\n    DATA_DIR = \"../input/spaceship-titanic-prepared-datasets\"\n    L1_N_TRIALS = 100\n    L2_N_TRIALS = 20\n    N_JOBS = 2\n    \n    # Map internal identifier to human-friendly name\n    MODELS = {\n        \"lr\": \"Logisitc Regression\",\n        \"ada\": \"AdaBoost\",\n        \"rf\": \"Random Forest\",\n        \"xgb\": \"XGBoost\",\n        \"lgb\": \"LightGBM\",\n    }\n    \n    @classmethod\n    def filepath(cls, filename):\n        return os.path.join(cls.DATA_DIR, filename)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:55.977481Z","iopub.execute_input":"2022-06-10T13:25:55.978417Z","iopub.status.idle":"2022-06-10T13:25:55.985532Z","shell.execute_reply.started":"2022-06-10T13:25:55.978371Z","shell.execute_reply":"2022-06-10T13:25:55.984111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration for Logistic Regression\nclass LRConfig:\n    DEFAULT_VALUES = {\n        \"tol\": 1e-4,\n        \"C\": 1.0,\n        \"solver\": \"lbfgs\",\n    }\n    STATIC_PARAMS = {\n        \"max_iter\": 1000,\n        \"verbose\": False,\n    }\n    \n    USE_PRUNER = False\n    \n    @classmethod\n    def get_fit_params(cls, X_train, y_train, X_val, y_val, params):\n        return {}","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.013562Z","iopub.execute_input":"2022-06-10T13:25:56.0143Z","iopub.status.idle":"2022-06-10T13:25:56.02129Z","shell.execute_reply.started":"2022-06-10T13:25:56.014263Z","shell.execute_reply":"2022-06-10T13:25:56.020299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration for AdaBoost\nclass AdaConfig:\n    DEFAULT_VALUES = {\n        \"base_estimator\": None,\n        \"n_estimators\": 50,\n        \"learning_rate\": 1.0,\n        \n    }\n    STATIC_PARAMS = {}\n    \n    USE_PRUNER = False\n    \n    @classmethod\n    def get_fit_params(cls, X_train, y_train, X_val, y_val, params):\n        return {}","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.054073Z","iopub.execute_input":"2022-06-10T13:25:56.054466Z","iopub.status.idle":"2022-06-10T13:25:56.060759Z","shell.execute_reply.started":"2022-06-10T13:25:56.054433Z","shell.execute_reply":"2022-06-10T13:25:56.059358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration for Random Forest\nclass RFConfig:\n    DEFAULT_VALUES = {\n        \"n_estimators\": 100,\n        \"max_depth\": None,\n        \"min_samples_split\": 2,\n        \"min_samples_leaf\": 1,\n        \"max_features\": \"sqrt\",\n        \"bootstrap\": True,\n        \"ccp_alpha\": 0.0,\n        \"max_samples\": None,\n    }\n\n    STATIC_PARAMS = {\n        \"n_jobs\": Config.N_JOBS,\n    }\n    \n    USE_PRUNER = False\n    \n    @classmethod\n    def get_fit_params(cls, X_train, y_train, X_val, y_val, params):\n        return {}","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.084052Z","iopub.execute_input":"2022-06-10T13:25:56.084305Z","iopub.status.idle":"2022-06-10T13:25:56.091937Z","shell.execute_reply.started":"2022-06-10T13:25:56.084261Z","shell.execute_reply":"2022-06-10T13:25:56.090714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration for XGBoost\nclass XGBConfig:\n    EVAL_METRIC = \"logloss\"\n    \n    DEFAULT_VALUES = {\n        \"max_depth\": 6,\n        \"n_estimators\": 100,\n        \"alpha\": 0.0,\n        \"lambda\": 1.0,\n        \"learning_rate\": 0.3,\n        \"colsample_bytree\": 1.0,\n        \"colsample_bylevel\": 1.0,\n        \"min_child_weight\": 1.0,\n        \"sampling_method\": \"uniform\",\n        \"early_stopping_rounds\": None,\n    }\n    \n    STATIC_PARAMS = {\n        \"tree_method\": \"gpu_hist\",\n        \"use_label_encoder\": False,\n        \"n_jobs\": Config.N_JOBS,\n        \"predictor\": \"gpu_predictor\",\n        \"max_bin\": 1024,\n        \"eval_metric\": EVAL_METRIC,\n    }\n    \n    USE_PRUNER = True\n    \n    @classmethod\n    def get_fit_params(cls, X_train, y_train, X_val, y_val, params):\n        return {\n            \"eval_set\": [(X_train, y_train), (X_val, y_val)],\n            \"verbose\": False,\n        }","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.120706Z","iopub.execute_input":"2022-06-10T13:25:56.121683Z","iopub.status.idle":"2022-06-10T13:25:56.130654Z","shell.execute_reply.started":"2022-06-10T13:25:56.121637Z","shell.execute_reply":"2022-06-10T13:25:56.129673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration for LightGBM\nclass LGBConfig:\n    DEFAULT_VALUES = {\n        \"num_leaves\": 31,\n        \"max_depth\": -1,\n        \"learning_rate\": 0.1,\n        \"n_estimators\": 100,\n        \"reg_alpha\": 0.0,\n        \"reg_lambda\": 0.0,\n        \"min_child_samples\": 20,\n        \"subsample_for_bin\": 200000,\n    }\n    \n    STATIC_PARAMS = {\n        \"n_jobs\": Config.N_JOBS,\n        \"verbose\": -1,\n        \"objective\": \"binary\",\n    }\n    \n    USE_PRUNER = True\n    \n    @classmethod\n    def get_fit_params(cls, X_train, y_train, X_val, y_val, params):\n        # To suppress training output\n        callbacks = params.get(\"callbacks\", []) + [lgb.log_evaluation(period=0)]\n        return {\n            \"eval_set\": [(X_train, y_train), (X_val, y_val)],\n            \"eval_metric\": \"logloss\",\n            \"callbacks\": callbacks,\n        }","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.147046Z","iopub.execute_input":"2022-06-10T13:25:56.147773Z","iopub.status.idle":"2022-06-10T13:25:56.156864Z","shell.execute_reply.started":"2022-06-10T13:25:56.147726Z","shell.execute_reply":"2022-06-10T13:25:56.15591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG_MAP = {\n    \"lr\": LRConfig,\n    \"ada\": AdaConfig,\n    \"rf\": RFConfig,\n    \"xgb\": XGBConfig,\n    \"lgb\": LGBConfig,\n}","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.179879Z","iopub.execute_input":"2022-06-10T13:25:56.180135Z","iopub.status.idle":"2022-06-10T13:25:56.185054Z","shell.execute_reply.started":"2022-06-10T13:25:56.180107Z","shell.execute_reply":"2022-06-10T13:25:56.183725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Datasets\n\n> Note: This notebook uses the datasets as prepared in [Spaceship Titanic - Logistic Regression Baselines](https://www.kaggle.com/code/defcodeking/spaceship-titanic-logistic-regression-baselines). Link to dataset: [Spaceship Titanic Prepared Datasets](https://www.kaggle.com/datasets/defcodeking/spaceship-titanic-prepared-datasets).\n\n> Note: While the experiments in Part 2 of this series suggest that using only `GroupId` (extracted from `PassengerId`) with label encoding gives the best model, we will use both `CabinNum` and `GroupId` label encoded since the former beats the latter only marginally (~0.23%).\n\nWe will load the datasets.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(Config.filepath(\"train_prepared_both_le.csv\"))\ntest_df = pd.read_csv(Config.filepath(\"test_prepared_both_le.csv\"))","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.205861Z","iopub.execute_input":"2022-06-10T13:25:56.206475Z","iopub.status.idle":"2022-06-10T13:25:56.298729Z","shell.execute_reply.started":"2022-06-10T13:25:56.206411Z","shell.execute_reply":"2022-06-10T13:25:56.297654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.300634Z","iopub.execute_input":"2022-06-10T13:25:56.300995Z","iopub.status.idle":"2022-06-10T13:25:56.336891Z","shell.execute_reply.started":"2022-06-10T13:25:56.300932Z","shell.execute_reply":"2022-06-10T13:25:56.33558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.339688Z","iopub.execute_input":"2022-06-10T13:25:56.340777Z","iopub.status.idle":"2022-06-10T13:25:56.540508Z","shell.execute_reply.started":"2022-06-10T13:25:56.340724Z","shell.execute_reply":"2022-06-10T13:25:56.539216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Map\n\nThe `MODEL_MAP` variable will be used as a mapping from the string identifier of a model to its associated class.","metadata":{}},{"cell_type":"code","source":"MODEL_MAP = {\n    \"lr\": LogisticRegression,\n    \"ada\": AdaBoostClassifier,\n    \"rf\": RandomForestClassifier,\n    \"xgb\": xgb.XGBClassifier,\n    \"lgb\": lgb.LGBMClassifier,\n}","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.542618Z","iopub.execute_input":"2022-06-10T13:25:56.543016Z","iopub.status.idle":"2022-06-10T13:25:56.548964Z","shell.execute_reply.started":"2022-06-10T13:25:56.542971Z","shell.execute_reply":"2022-06-10T13:25:56.54794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop\n\nThe `train()` function will be used as a generic training loop. It takes the training and test dataframes, a string identifier for the model that is being trained, the hyperparameters for the model and an optional verbosity argument. In the end, it returns the predictions for the training set, the test set and the overall accuracy.","metadata":{}},{"cell_type":"code","source":"def train(df, test_df, model, params, verbose=True):\n    # Create copies so that original datatsets do not change\n    df = df.copy()\n    test = test_df.drop(\"PassengerId\", axis=1)\n    \n    df[\"preds\"] = pd.NA\n    \n    drop = [\"Transported\", \"preds\", \"kfold\"]\n    \n    # Get the initializer class and the configuration class\n    klass = MODEL_MAP[model]\n    config = CONFIG_MAP[model]\n    \n    # Default values in the config class are for tuned parameters\n    # So, only those are filtered from params\n    # This is mainly added for AdaBoost, since it has a slightly different objective\n    params = {k: v for k, v in params.items() if k in config.DEFAULT_VALUES}\n    \n    # Add default values for parameters not defined in params\n    params.update({k: v for k, v in config.DEFAULT_VALUES.items() if k not in params})\n    \n    # Add static params - Parameters that are not tuned\n    params.update(config.STATIC_PARAMS)\n    \n    # For storing total accuracy across folds for averaging\n    total_acc = 0.0\n    \n    # Empty list for storing test predictions in each fold\n    test_preds = []\n    \n    for fold in range(5):\n        train = df[df[\"kfold\"] != fold]\n        \n        # Get training features and labels\n        y_train = train[\"Transported\"]\n        X_train = train.drop(drop, axis=1)\n        \n        val = df[df[\"kfold\"] == fold]\n        \n        # Get validation features and labels\n        y_val = val[\"Transported\"]\n        X_val = val.drop(drop, axis=1)\n        \n        # Initialize model\n        clf = klass(**params)\n        \n        # Get parameters for .fit() other than X and y\n        fit_params = config.get_fit_params(\n            X_train=X_train,\n            y_train=y_train,\n            X_val=X_val,\n            y_val=y_val,\n            params=params\n        )\n        \n        # Train model on training set\n        clf.fit(\n            X=X_train,\n            y=y_train,\n            **fit_params,\n        )\n        \n        # Make predictions on validation set\n        val_pred = clf.predict(X_val)\n        acc = metrics.accuracy_score(y_val, val_pred)\n        \n        # Report accuracy if verbose is True\n        if verbose is True:\n            print(f\"\\tFold {fold + 1} - Accuracy = {acc: .4f}\")\n        \n        # Add to total accuracy\n        total_acc += acc\n        \n        # Make predictions on validation set again\n        # But this time in terms of probabilities\n        # And store in the df\n        # These will be used in the meta model\n        df.loc[val.index, \"preds\"] = clf.predict_proba(X_val)[:, 1]\n        \n        # Get the test predictions for this fold in terms of probability\n        test_preds.append(clf.predict_proba(test)[:, 1])\n        \n    acc = total_acc / 5\n    \n    if verbose is True:\n        print(f\"\\tOverall accuracy = {acc: .4f}\")   \n    \n    # Calculate final test predictions\n    # These will be used in the meta model\n    test_preds = np.vstack(test_preds)\n    test_preds = test_preds.mean(axis=0)\n    \n    # Return val preds, test preds and overall accuracy\n    return df[\"preds\"].values, test_preds, acc","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.550731Z","iopub.execute_input":"2022-06-10T13:25:56.551308Z","iopub.status.idle":"2022-06-10T13:25:56.571829Z","shell.execute_reply.started":"2022-06-10T13:25:56.551265Z","shell.execute_reply":"2022-06-10T13:25:56.570911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optuna Objectives\n\nThe functions below will act as the objective functions that should be used for each of the models. These take the Optuna trial, the training dataframe and the test dataframe as arguments. They first use Optuna to get a dictionary of parameters and then call the `train()` function with the appropriate arguments. ","metadata":{}},{"cell_type":"code","source":"# Objective for Logistic Regression\ndef lr_objective(trial, train_df, test_df):\n    params = {\n        \"tol\": trial.suggest_float(\"tol\", 1e-6, 1e-4, log=True),\n        \"C\": trial.suggest_float(\"C\", 0.5, 2.0, log=True),\n        \"solver\": trial.suggest_categorical(\"solver\", [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]),\n    }\n    \n    _, _, acc = train(df=train_df, test_df=test_df, model=\"lr\", params=params, verbose=False)\n    return acc","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.575522Z","iopub.execute_input":"2022-06-10T13:25:56.577675Z","iopub.status.idle":"2022-06-10T13:25:56.588545Z","shell.execute_reply.started":"2022-06-10T13:25:56.577639Z","shell.execute_reply":"2022-06-10T13:25:56.587242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Objective for AdaBoost\ndef adaboost_objective(trial, train_df, test_df):\n    params = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500, log=True),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.1, 5.0, log=True),\n    }\n    \n    tune_estimator = trial.suggest_categorical(\"tune_estimator\", [True, False])\n    \n    if tune_estimator:\n        # Parameters for the Decision Tree in AdaBoost\n        max_depth = trial.suggest_int(\"max_depth\", 1, 50)\n        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10, log=True)\n        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 5, log=True)\n        ccp_alpha = trial.suggest_float(\"ccp_alpha\", 0.01, 1.0, log=True)\n        \n        params[\"base_estimator\"] = DecisionTreeClassifier(\n            max_depth=max_depth,\n            min_samples_split=min_samples_split,\n            min_samples_leaf=min_samples_leaf,\n            ccp_alpha=ccp_alpha,\n        )\n    \n    _, _, acc = train(df=train_df, test_df=test_df, model=\"ada\", params=params, verbose=False)\n    return acc","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.590338Z","iopub.execute_input":"2022-06-10T13:25:56.591384Z","iopub.status.idle":"2022-06-10T13:25:56.603069Z","shell.execute_reply.started":"2022-06-10T13:25:56.591335Z","shell.execute_reply":"2022-06-10T13:25:56.602097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Objectuve for Random Forest\ndef rf_objective(trial, train_df, test_df):\n    params = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 1, 500),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 50),\n        \"min_samples_split\": trial.suggest_int(\"min_samples_plit\", 2, 10, log=True),\n        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5, log=True),\n        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n        \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n        \"ccp_alpha\": trial.suggest_float(\"ccp_alpha\", 0.01, 1.0, log=True),\n    }\n    \n    if params[\"bootstrap\"] is True:\n        params[\"max_samples\"] = trial.suggest_float(\"max_samples\", 0.01, 1.0, log=True)\n    \n    _, _, acc = train(df=train_df, test_df=test_df, model=\"rf\", params=params, verbose=False)\n    return acc","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.605432Z","iopub.execute_input":"2022-06-10T13:25:56.606201Z","iopub.status.idle":"2022-06-10T13:25:56.619456Z","shell.execute_reply.started":"2022-06-10T13:25:56.606144Z","shell.execute_reply":"2022-06-10T13:25:56.618283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Objective for XGBoost\ndef xgb_objective(trial, train_df, test_df):\n    params = {\n        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 11),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 5, 500),\n        \"alpha\": trial.suggest_uniform(\"alpha\", 0.0, 5.0),\n        \"lambda\": trial.suggest_float(\"lambda\", 1.0, 5.0, log=True),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.03, 0.8, log=True),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.2, 1.0),\n        \"min_child_weight\": trial.suggest_uniform(\"min_child_weight\", 1, 100),\n        \"sampling_method\": trial.suggest_categorical(\"sampling_method\", [\"uniform\", \"gradient_based\"]),\n        \"early_stopping_rounds\": trial.suggest_int(\"early_stopping_rounds\", 5, 20, step=5)\n    }\n    \n    obs_k = f\"validation_1-{XGBConfig.EVAL_METRIC}\"\n    params[\"callbacks\"] = [optuna.integration.XGBoostPruningCallback(trial, obs_k)]\n    \n    _, _, acc = train(df=train_df, test_df=test_df, model=\"xgb\", params=params, verbose=False)\n    return acc","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.621441Z","iopub.execute_input":"2022-06-10T13:25:56.622167Z","iopub.status.idle":"2022-06-10T13:25:56.634568Z","shell.execute_reply.started":"2022-06-10T13:25:56.622104Z","shell.execute_reply":"2022-06-10T13:25:56.633338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Objective for LightGBM\ndef lgb_objective(trial, train_df, test_df):\n    params = {\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 100, log=True),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 100, log=True),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1.0),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 5.0),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 20, 50, log=True),\n        \"subsample_for_bin\": trial.suggest_int(\"subsample_for_bin\", 2000, 8000),\n    }\n    \n    params[\"callbacks\"] = [optuna.integration.LightGBMPruningCallback(trial, \"logloss\", \"valid_1\")]\n    \n    _, _, acc = train(df=train_df, test_df=test_df, model=\"lgb\", params=params, verbose=False)\n    return acc","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.636645Z","iopub.execute_input":"2022-06-10T13:25:56.637155Z","iopub.status.idle":"2022-06-10T13:25:56.650035Z","shell.execute_reply.started":"2022-06-10T13:25:56.63711Z","shell.execute_reply":"2022-06-10T13:25:56.649007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OBJECTIVE_MAP = {\n    \"lr\": lr_objective,\n    \"ada\": adaboost_objective,\n    \"rf\": rf_objective,\n    \"xgb\": xgb_objective,\n    \"lgb\": lgb_objective,\n}","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.652602Z","iopub.execute_input":"2022-06-10T13:25:56.652847Z","iopub.status.idle":"2022-06-10T13:25:56.661792Z","shell.execute_reply.started":"2022-06-10T13:25:56.652817Z","shell.execute_reply":"2022-06-10T13:25:56.660582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Search\n\nThe function `hyperparameter_search()` finds the best hyperparameters for the given model by utilizing the proper objective function. It takes the training dataframe, test dataframe and the string identifier of the model for which hyperparameters are required.","metadata":{}},{"cell_type":"code","source":"def hyperparameter_search(train_df, test_df, model, n_trials=Config.L1_N_TRIALS):\n    # Turn off verbose output\n    v = optuna.logging.get_verbosity()\n    optuna.logging.set_verbosity(optuna.logging.WARNING)\n    \n    objective = OBJECTIVE_MAP[model]\n    objective = functools.partial(objective, train_df=train_df, test_df=test_df)\n    \n    # Check if pruning is required\n    pruner = optuna.pruners.HyperbandPruner() if CONFIG_MAP[model].USE_PRUNER is True else None\n    \n    sampler = optuna.samplers.TPESampler(seed=42)\n    study = optuna.create_study(\n        direction=\"maximize\",\n        pruner=pruner,\n        sampler=sampler,\n    )\n    \n    study.optimize(objective, n_trials=n_trials)\n    \n    # Restore verbosity level\n    optuna.logging.set_verbosity(v)\n\n    return study.best_params","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.66569Z","iopub.execute_input":"2022-06-10T13:25:56.666259Z","iopub.status.idle":"2022-06-10T13:25:56.67709Z","shell.execute_reply.started":"2022-06-10T13:25:56.666212Z","shell.execute_reply":"2022-06-10T13:25:56.676142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Datasets for LGBMClassifier\n\nLightGBM is unique from other boosting algorithms in that it supports categorical features out of the box. We only need to make sure they are label encoded and have their datatype as `category`. The function below takes the two datasets and label encodes all the one-hot encoded columns.","metadata":{}},{"cell_type":"code","source":"def lgb_datasets(train_df, test_df):\n    # Make copies so that original datasets remain unchanged\n    train_df = train_df.copy()\n    test_df = test_df.copy()\n    \n    # Drop Transported and kfold\n    drop = [\"Transported\", \"kfold\"]\n    dropped = train_df[drop].values\n    train_df = train_df.drop(drop, axis=1)\n    \n    # Drop PassengerId\n    passenger_id = test_df[\"PassengerId\"].values\n    test_df = test_df.drop(\"PassengerId\", axis=1)\n    \n    # Add suffix to index and store indices\n    # So that the dataframes can be merged and split\n    train_df = train_df.rename(\"train_{}\".format)\n    test_df = test_df.rename(\"test_{}\".format)\n    \n    tr_idx = train_df.index\n    te_idx = test_df.index\n    \n    # Merge\n    df = pd.concat([train_df, test_df])\n    \n    oh_cols = [\"CabinDeck\", \"HomePlanet\", \"Destination\", \"GroupSize\"]\n    \n    for oh_col in oh_cols:\n        # Get all columns associated with the one-hot column\n        columns = [column for column in df.columns if column.startswith(f\"{oh_col}_\")]\n        \n        # .idxmax() returns that column name which has the maximum value in the row\n        values = df[columns].idxmax(axis=1)\n        \n        # Get all levels and make a mapping from level to index\n        levels = values.value_counts().index\n        mapping = {level: idx for idx, level in enumerate(levels)}\n        \n        # Add column with the mapping and specify type as category\n        df[oh_col] = values.map(mapping).astype(\"category\")\n        \n        # Drop one-hot columns\n        df = df.drop(columns, axis=1)\n        \n    # Make sure other categorical features have the correct type\n    missing = (col for col in df.columns if col.endswith(\"_missing\"))\n    others = [\"CryoSleep\", \"VIP\", \"Alone\", \"CabinNum\", \"GroupId\", *missing]\n    df[others] = df[others].astype(\"category\")\n        \n    # Split and add dropped columns\n    train_df = df.loc[tr_idx, :]\n    train_df[drop] = dropped\n    \n    test_df = df.loc[te_idx, :]\n    test_df[\"PassengerId\"] = passenger_id\n    \n    return train_df, test_df","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.679259Z","iopub.execute_input":"2022-06-10T13:25:56.679592Z","iopub.status.idle":"2022-06-10T13:25:56.695364Z","shell.execute_reply.started":"2022-06-10T13:25:56.679548Z","shell.execute_reply":"2022-06-10T13:25:56.694337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Example","metadata":{}},{"cell_type":"code","source":"tr, te = lgb_datasets(train_df, test_df)\ntr.columns, te.columns","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-10T13:25:56.697514Z","iopub.execute_input":"2022-06-10T13:25:56.698255Z","iopub.status.idle":"2022-06-10T13:25:56.861062Z","shell.execute_reply.started":"2022-06-10T13:25:56.698207Z","shell.execute_reply":"2022-06-10T13:25:56.860016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr.head()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-10T13:25:56.863677Z","iopub.execute_input":"2022-06-10T13:25:56.864264Z","iopub.status.idle":"2022-06-10T13:25:56.901583Z","shell.execute_reply.started":"2022-06-10T13:25:56.864198Z","shell.execute_reply":"2022-06-10T13:25:56.900287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble Class\n\nThe `Ensemble` class will implement all the logic required for stacking. It takes the training dataframe, the test dataframe and an optional list of strings which specifies the models that should be excluded from the ensemble. It has the following methods:\n\n- `fit_level_one_models()`: This method fits all the different models (Logistic Regression, XGBoost, etc.) on the original training data and also creates the training set and test sets for the meta-classifier that will generate the final predictions.\n- `fit_level_two_model()`: This method fits a logistic regression model on the dataset generated by `fit_level_one_models()` and gets the final predictions.","metadata":{}},{"cell_type":"code","source":"class Ensemble:\n    def __init__(self, train_df, test_df, exclude=None):\n        self.train_df = train_df\n        self.test_df = test_df\n        \n        models = Config.MODELS.keys()\n        \n        # Exclude models\n        if exclude is not None:\n            models = models - exclude\n            \n        self.models = list(models)\n        \n        # Create empty dataframe that will store\n        # The training set for the level 2 model\n        columns = [f\"{m}_preds\" for m in self.models]\n        extra_cols = [\"Transported\", \"kfold\"]\n        \n        meta_train_df = pd.DataFrame(columns=columns + extra_cols)\n        meta_train_df[extra_cols] = train_df[extra_cols]\n        \n        self.meta_train_df = meta_train_df\n        \n        # Create empty dataframe that will store\n        # The test set for the level 2 model\n        meta_test_df = pd.DataFrame(columns=[\"PassengerId\"] + columns)\n        meta_test_df[\"PassengerId\"] = test_df[\"PassengerId\"]\n        \n        self.meta_test_df = meta_test_df\n        \n    def fit_level_one_models(self):\n        print(\"Training level 1 models...\")\n        \n        for model in self.models:\n            if model == \"lgb\":\n                # Modify dataset for LGBMClassifier\n                train_df, test_df = lgb_datasets(self.train_df, self.test_df)\n            else:\n                train_df, test_df = self.train_df, self.test_df\n            \n            print(f\"{Config.MODELS[model]}:\")\n            \n            print(\"\\tFinding optimal hyperparameters using Optuna...\")\n            params = hyperparameter_search(\n                train_df=train_df, test_df=test_df, model=model\n            )\n            \n            print(f\"\\n\\tBest params: {params}\\n\")\n            \n            print(\"\\tTraining model with optimal parameters...\\n\")\n            val_preds, test_preds, acc = train(\n                df=train_df,\n                test_df=test_df,\n                model=model,\n                params=params\n            )\n            \n            print(\"\\tDone!\\n\")\n            \n            # Add predictions to the datasets for the level 2 model\n            self.meta_train_df[f\"{model}_preds\"] = val_preds\n            self.meta_test_df[f\"{model}_preds\"] = test_preds\n            \n    def fit_level_two_model(self):    \n        print(\"Training a Logistic Regression model as level 2 model...\")\n        \n        train_df = self.meta_train_df\n        test_df = self.meta_test_df\n        \n        print(\"\\tFinding optimal hyperparameters using Optuna...\")\n        params = hyperparameter_search(\n            train_df=train_df,\n            test_df=test_df,\n            model=\"lr\",\n            n_trials=Config.L2_N_TRIALS,\n        )\n\n        print(f\"\\n\\tBest params: {params}\\n\")\n\n        print(\"\\tTraining model with optimal parameters...\\n\")\n        \n        _, test_preds, _ = train(\n            df=train_df,\n            test_df=test_df,\n            model=\"lr\",\n            params=params\n        )\n        \n        print(\"\\tDone!\")\n        \n        self.meta_test_df[\"Transported\"] = test_preds >= 0.5\n        \n        return self.meta_test_df","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:25:56.903424Z","iopub.execute_input":"2022-06-10T13:25:56.904381Z","iopub.status.idle":"2022-06-10T13:25:56.922337Z","shell.execute_reply.started":"2022-06-10T13:25:56.904326Z","shell.execute_reply":"2022-06-10T13:25:56.921017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training\n\nFinally, we will use the `Ensemble` class to train our ensemble model.","metadata":{}},{"cell_type":"code","source":"# Initialize the ensemble\nensemble = Ensemble(train_df, test_df)\nensemble.models","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:26:50.081936Z","iopub.execute_input":"2022-06-10T13:26:50.08237Z","iopub.status.idle":"2022-06-10T13:26:50.110319Z","shell.execute_reply.started":"2022-06-10T13:26:50.082318Z","shell.execute_reply":"2022-06-10T13:26:50.109512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the level one models\nensemble.fit_level_one_models()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-10T13:26:51.967191Z","iopub.execute_input":"2022-06-10T13:26:51.967472Z","iopub.status.idle":"2022-06-10T13:27:02.660567Z","shell.execute_reply.started":"2022-06-10T13:26:51.967441Z","shell.execute_reply":"2022-06-10T13:27:02.659433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generated training set for level 2 model\nensemble.meta_train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T14:43:50.622265Z","iopub.execute_input":"2022-06-09T14:43:50.622823Z","iopub.status.idle":"2022-06-09T14:43:50.634633Z","shell.execute_reply.started":"2022-06-09T14:43:50.622782Z","shell.execute_reply":"2022-06-09T14:43:50.633789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generated test set for level 2 model\nensemble.meta_test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T14:43:54.078769Z","iopub.execute_input":"2022-06-09T14:43:54.079505Z","iopub.status.idle":"2022-06-09T14:43:54.090576Z","shell.execute_reply.started":"2022-06-09T14:43:54.079464Z","shell.execute_reply":"2022-06-09T14:43:54.089755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the level 2 model\ntest_predictions = ensemble.fit_level_two_model()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T14:43:58.413528Z","iopub.execute_input":"2022-06-09T14:43:58.414163Z","iopub.status.idle":"2022-06-09T14:44:02.348861Z","shell.execute_reply.started":"2022-06-09T14:43:58.414119Z","shell.execute_reply":"2022-06-09T14:44:02.346757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T14:44:05.451844Z","iopub.execute_input":"2022-06-09T14:44:05.452104Z","iopub.status.idle":"2022-06-09T14:44:05.467612Z","shell.execute_reply.started":"2022-06-09T14:44:05.452073Z","shell.execute_reply":"2022-06-09T14:44:05.4666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions[\"Transported\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-09T14:29:37.900768Z","iopub.execute_input":"2022-06-09T14:29:37.901333Z","iopub.status.idle":"2022-06-09T14:29:37.907962Z","shell.execute_reply.started":"2022-06-09T14:29:37.901292Z","shell.execute_reply":"2022-06-09T14:29:37.907186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n\nNow we will generate the submissions file.","metadata":{}},{"cell_type":"code","source":"submission = test_predictions[[\"PassengerId\", \"Transported\"]]\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T00:05:09.678458Z","iopub.execute_input":"2022-06-09T00:05:09.678716Z","iopub.status.idle":"2022-06-09T00:05:09.694394Z","shell.execute_reply.started":"2022-06-09T00:05:09.678685Z","shell.execute_reply":"2022-06-09T00:05:09.693765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nIn this notebook, we used the technique of ensembling in the form of stacking coupled with Optuna to solve the Spaceship Titanic competition.\n\nThank you for reading!","metadata":{}}]}