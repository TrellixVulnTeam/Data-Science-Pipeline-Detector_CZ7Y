{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nplt.style.use('ggplot')\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-22T22:23:26.500095Z","iopub.execute_input":"2022-05-22T22:23:26.500472Z","iopub.status.idle":"2022-05-22T22:23:27.702526Z","shell.execute_reply.started":"2022-05-22T22:23:26.50038Z","shell.execute_reply":"2022-05-22T22:23:27.701718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/spaceship-titanic/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/spaceship-titanic/test.csv\")\ntrain_copy = train.copy()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:23:27.704156Z","iopub.execute_input":"2022-05-22T22:23:27.704408Z","iopub.status.idle":"2022-05-22T22:23:27.785985Z","shell.execute_reply.started":"2022-05-22T22:23:27.704375Z","shell.execute_reply":"2022-05-22T22:23:27.785276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hypotheses\nWe have the following passenger information:\n- **PassengerId** - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n- **HomePlanet** - The planet the passenger departed from, typically their planet of permanent residence.\n- **CryoSleep** - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n- **Cabin** - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n- **Destination** - The planet the passenger will be debarking to.\n- **Age** - The age of the passenger.\n- **VIP** - Whether the passenger has paid for special VIP service during the voyage.\n- **RoomService**, **FoodCourt**, **ShoppingMall**, **Spa**, **VRDeck** - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n- **Name** - The first and last names of the passenger.\n- **Transported** - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.\n\n\n*What are some speculative reasons for getting transported to another dimension?*\n\n\n1. Location on the ship when collision occured. Is there a common **deck**, num or side with passengers who were transported? Out of the common location, was it primarily people who were in **cryosleep**?\n2. Was age a factor? Did younger people tend to get transported because they were up late at the time of the collision? Were older people transported because there was an event for older guests on at the time and location of the collision?\n3. If one member of a group (gggg part of the PassengerId) was transported, did this mean that others in the same group were too?","metadata":{}},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"print('Train test shape:', train_copy.shape)\nprint('Test test shape:', test.shape)\n\ntrain_copy.head(5)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-22T22:23:27.787208Z","iopub.execute_input":"2022-05-22T22:23:27.788017Z","iopub.status.idle":"2022-05-22T22:23:27.821531Z","shell.execute_reply.started":"2022-05-22T22:23:27.787978Z","shell.execute_reply":"2022-05-22T22:23:27.820673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Types","metadata":{}},{"cell_type":"code","source":"train_copy.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:23:27.824026Z","iopub.execute_input":"2022-05-22T22:23:27.824609Z","iopub.status.idle":"2022-05-22T22:23:27.833007Z","shell.execute_reply.started":"2022-05-22T22:23:27.824564Z","shell.execute_reply":"2022-05-22T22:23:27.832251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Missing Values","metadata":{}},{"cell_type":"code","source":"print('TRAINING SET MISSING VALUES:')\nprint(train_copy.isna().sum())\nprint('\\n')\nprint('TEST SET MISSING VALUES:')\nprint(test.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:23:27.834306Z","iopub.execute_input":"2022-05-22T22:23:27.835109Z","iopub.status.idle":"2022-05-22T22:23:27.859595Z","shell.execute_reply.started":"2022-05-22T22:23:27.835054Z","shell.execute_reply":"2022-05-22T22:23:27.858667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA\n\n**Target distribution**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10,7))\ntrain_copy['Transported'].value_counts().plot.pie(explode = [0.1, 0.1], autopct = '%1.1f%%')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:23:27.860648Z","iopub.execute_input":"2022-05-22T22:23:27.86129Z","iopub.status.idle":"2022-05-22T22:23:28.056573Z","shell.execute_reply.started":"2022-05-22T22:23:27.861256Z","shell.execute_reply":"2022-05-22T22:23:28.055601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because our target is evenly distributed, we don't have to worry about regarding sampling.","metadata":{}},{"cell_type":"markdown","source":"## Numerical Data:\n\n**Age**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10,7))\nsn.histplot(data = train_copy, x = 'Age', binwidth = 1, hue = 'Transported', kde = True)\nplt.title('Age Distribution (shows two histograms overlayed on each other)')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:23:28.058322Z","iopub.execute_input":"2022-05-22T22:23:28.058869Z","iopub.status.idle":"2022-05-22T22:23:28.858487Z","shell.execute_reply.started":"2022-05-22T22:23:28.058819Z","shell.execute_reply":"2022-05-22T22:23:28.857543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*:\n- 0-18 year olds were more likely to be transported compared to other age groups.\n- 18-25 year olds less likely to be transported than not.\n- Over 25s are equally likely \n\n*Insight*:\n- It would make sense to create a new feature which categorises people into either child, adolescent or adult.","metadata":{}},{"cell_type":"markdown","source":"**Spending on amenities**","metadata":{}},{"cell_type":"code","source":"amenities = ['RoomService', 'Spa', 'FoodCourt', 'VRDeck', 'ShoppingMall']\n\nfig = plt.figure(figsize = (30, 20))\n\nfor counter, amenity in enumerate(amenities):\n    ax = fig.add_subplot(5,2, 2*counter + 1)\n    sn.histplot(data = train_copy, x = amenity, axes = ax, bins = 30, kde = False, hue = 'Transported')\n    \n    ax = fig.add_subplot(5,2, 2*counter + 2)\n    sn.histplot(data = train_copy, x = amenity, axes = ax, bins = 30, kde = True, hue = 'Transported')\n    ax.set_ylim([0,100])","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:23:28.859981Z","iopub.execute_input":"2022-05-22T22:23:28.860393Z","iopub.status.idle":"2022-05-22T22:23:32.854773Z","shell.execute_reply.started":"2022-05-22T22:23:28.860322Z","shell.execute_reply":"2022-05-22T22:23:32.853861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*:\n- Across all amenity spending, those who spent less roughly less than £200 were more likely to be transported. Those who spent over this amount were more likely to not be transported.\n\n*Insight*:\n- Create two new categories: small spender (< 200£) vs big spender for each amenity.","metadata":{}},{"cell_type":"markdown","source":"## Categorical Data","metadata":{}},{"cell_type":"code","source":"cat_features = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\n\nfigure = plt.figure(figsize = (20,20))\n\nfor counter, feat in enumerate(cat_features):\n    ax = figure.add_subplot(4, 1, counter + 1)\n    sn.countplot(data = train_copy, x = feat, hue = 'Transported', ax = ax)\n    \nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:23:32.856385Z","iopub.execute_input":"2022-05-22T22:23:32.85711Z","iopub.status.idle":"2022-05-22T22:23:33.980845Z","shell.execute_reply.started":"2022-05-22T22:23:32.85706Z","shell.execute_reply":"2022-05-22T22:23:33.980235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*:\n\nHomePlanet: \n- Those coming from Europa had a higher chance of being transported.\n- Those coming from Earth were more likely to not be transported.\n- People from Mars had roughly a 50% chance of being transported.\n\nCryoSleep:\n- Those who were in CryoSleep were much more likely to be transported.\n- Those who weren't were more likely not to be transported.\n\nDestination and VIP categories were roughly equally likely to be transported or not.\n\n*Insights*:\n\n- We should probably consider dropping the VIP feature as does not look insightful. \n","metadata":{}},{"cell_type":"markdown","source":"## Qualitative features","metadata":{}},{"cell_type":"code","source":"qual_features = ['PassengerId', 'Cabin', 'Name']\n\ntrain_copy[qual_features].head()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:23:33.983284Z","iopub.execute_input":"2022-05-22T22:23:33.984112Z","iopub.status.idle":"2022-05-22T22:23:33.995118Z","shell.execute_reply.started":"2022-05-22T22:23:33.984061Z","shell.execute_reply":"2022-05-22T22:23:33.994544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Notes*\n\n- PassengerId takees the form gggg_pp where gggg indicates the group and pp indicates their number within the group.\n- Cabin takes the form deck/num/side. It could be interesting to see whether certain decks or sides are more prone to transportation.\n\n*Insights*\n\n- We can extract the group and group size from the PassengerId feature.\n- We can extract the deck, number and side from the cabin feature.\n- We could extract surnames from the name feature to identify families.","metadata":{}},{"cell_type":"markdown","source":"# Feature engineering\n\n**Age status**\n\nLet's group ages and hence convert this numerical feature into a categorical one.","metadata":{}},{"cell_type":"code","source":"train_age = train.copy()\n\ntrain_age['age_group'], bins = pd.cut(x = train_age['Age'],\n                                 bins = [0, 18, 25, 40, 100],\n                                 labels = ('age_0-18', 'age_18-25', 'age_25-40', 'age_over40'),\n                                 retbins = True)\n\n# Plot new feature distribution\\\nfigure = plt.figure(figsize = (10,8))\nsn.histplot(data = train_age, x = 'age_group', hue = 'Transported')\nplt.title('Age group distribution')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:23:33.996114Z","iopub.execute_input":"2022-05-22T22:23:33.997008Z","iopub.status.idle":"2022-05-22T22:23:34.251851Z","shell.execute_reply.started":"2022-05-22T22:23:33.996958Z","shell.execute_reply":"2022-05-22T22:23:34.251056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Expenditure**\n\nCalculate total expenditure and identify passengers with no expenditure.","metadata":{}},{"cell_type":"code","source":"train_exp = train_age.copy()\n\n# Create new feature for total expenditure on amenities\ntrain_exp['total_expenditure'] = train_exp[amenities].sum(axis = \"columns\", skipna = True)\n\n# Identify passengers with no expenditure\ntrain_exp['zero_expenditure'] = (train_exp['total_expenditure'] == 0).astype(int)\n\n# Distributions\nfigure, (ax1, ax2) = plt.subplots(2, 1, figsize = (20, 12))\nsn.histplot(data = train_exp, x = 'total_expenditure', binwidth = 200, \n            hue = 'Transported', kde = True, ax = ax1)\nsn.countplot(data = train_exp, x = \"zero_expenditure\", hue = 'Transported', ax = ax2)\nax2.set_xlabel('zero_expenditure (1 = True, 0 = False)')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:23:34.252938Z","iopub.execute_input":"2022-05-22T22:23:34.253147Z","iopub.status.idle":"2022-05-22T22:23:35.905435Z","shell.execute_reply.started":"2022-05-22T22:23:34.25312Z","shell.execute_reply":"2022-05-22T22:23:35.904428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Passenger group**\n\nExtract passenger group and group size from PassengerId","metadata":{}},{"cell_type":"code","source":"train_pass = train_exp.copy()\n\n# New feature - group\ntrain_pass['group'] = train_pass['PassengerId'].str.split('_').str[0]\n\n# New feature - group size\ngroup_size = pd.DataFrame(train_pass['group'].value_counts())\ntrain_pass = train_pass.merge(right = group_size, \n                             how = 'left',\n                             left_on = 'group',\n                             right_index = True)\n\ntrain_pass.rename(columns = {'group_x': 'group', 'group_y': 'group_size'}, inplace = True)\n\n# Visualise new features\nfigure, (ax1, ax2) = plt.subplots(1, 2, figsize = (20,4))\nsn.histplot(data = train_pass,\n            x = train_pass['group_size'],\n            hue = 'Transported', ax = ax1)\nsn.histplot(data = train_pass, \n            x = train_pass['group'],\n            hue = 'Transported',\n            ax = ax2,\n            binwidth = 5)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:23:35.907094Z","iopub.execute_input":"2022-05-22T22:23:35.907428Z","iopub.status.idle":"2022-05-22T22:25:20.220042Z","shell.execute_reply.started":"2022-05-22T22:23:35.907371Z","shell.execute_reply":"2022-05-22T22:25:20.21923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notes:\n- We can see that those travelling one their own are slightly more likely to be transported.\n\nInsight:\n- Create another feature which identifies whether someone is travelling on their own or not.","metadata":{}},{"cell_type":"code","source":"# New feature - solo travelling\ntrain_solo = train_pass.copy()\n\ntrain_solo['solo'] = (train_solo['group_size'] == 1).astype(int)\n\n# Visualise new feature\nplt.figure(figsize = (15,4))\nsn.countplot(data = train_solo,\n             x = 'solo',\n             hue = 'Transported')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:25:20.221519Z","iopub.execute_input":"2022-05-22T22:25:20.222485Z","iopub.status.idle":"2022-05-22T22:25:20.447773Z","shell.execute_reply.started":"2022-05-22T22:25:20.222436Z","shell.execute_reply":"2022-05-22T22:25:20.446907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Features for future work:**\n\n- Cabin location\n- Last name, family size ","metadata":{}},{"cell_type":"markdown","source":"# Missing values","metadata":{}},{"cell_type":"code","source":"train_missing = train_solo.copy()\n\nna_count = train_missing.isna().sum()\nna_pct = (na_count/train_missing.count()) * 100\n\nna_df = pd.concat([na_count, na_pct], axis = 'columns')\nna_df.rename(columns = {0: 'count', 1: 'pct'}, inplace = True)\nna_df","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:25:20.448793Z","iopub.execute_input":"2022-05-22T22:25:20.449011Z","iopub.status.idle":"2022-05-22T22:25:20.487362Z","shell.execute_reply.started":"2022-05-22T22:25:20.448983Z","shell.execute_reply":"2022-05-22T22:25:20.4864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Features going to be used in the model:**\n\n- Categorical features: HomePlanet, CryoSleep, Destination, age_group, zero_expenditure, group_size, solo\n- Numerical features: \n\n**Plan for missing values:**\n\nImpute missing values with mode for categorical features.","metadata":{}},{"cell_type":"markdown","source":"# Model\n\nThe features we will use in this model are:\n\n1. Age group\n2. Zero-expenditure\n3. Group-size","metadata":{}},{"cell_type":"markdown","source":"**Transformation Function**","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass NumericalAttributesAdder(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        return None\n    \n    def fit(self, X, y = None):\n        \n        return self\n    \n    def transform(self, X):\n        \n        age_ix, roomservice_ix, foodcourt_ix, shoppingmall_ix, spa_ix, vrdeck_id = 0, 1, 2, 3, 4, 5\n        \n        amenities = [1, 2, 3, 4, 5]\n        \n        X = pd.DataFrame(X)\n        \n        # Create new feature - age_group \n        age_group, bins = pd.cut(x = X[age_ix],\n                                     bins = [0, 18, 25, 40, 100],\n                                     labels = ('age_0-18', 'age_18-25', 'age_25-40', 'age_over40'),\n                                     retbins = True,\n                                     include_lowest = True)\n                \n        # Reshape so this can be One Hot Encoded\n        age_group = np.array(age_group).reshape(-1,1)\n        \n        # One Hot Encode age_group\n        from sklearn.preprocessing import OneHotEncoder\n        age_group_ohe = OneHotEncoder(sparse = False)\n        age_group_encoded = pd.DataFrame(age_group_ohe.fit_transform(age_group))\n        \n        # Create new feature for total expenditure on amenities\n        total_expenditure = X[amenities].sum(axis = 1)\n        \n        # Identify passengers with no expenditure\n        zero_expenditure = (total_expenditure == 0).astype(int)\n        \n        # Keep only the columns we want\n        X_enriched = pd.concat([age_group_encoded, zero_expenditure], axis = 'columns')\n        \n        return X_enriched\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:25:20.48862Z","iopub.execute_input":"2022-05-22T22:25:20.48883Z","iopub.status.idle":"2022-05-22T22:25:20.605054Z","shell.execute_reply.started":"2022-05-22T22:25:20.488803Z","shell.execute_reply":"2022-05-22T22:25:20.604318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CategoricalAttributesAdder(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        return None\n    \n    def fit(self, X, y = None):\n        \n        return self\n    \n    def transform(self, X):\n        \n        X = pd.DataFrame(X)\n        \n        passengerid_ix, homeplanet_ix, cryosleep_ix = 0, 1, 2\n        \n        # New feature - group\n        X['group'] = X[passengerid_ix].str.split('_').str[0]\n\n        # New feature - group size\n        group_size = pd.DataFrame(X['group'].value_counts())\n        \n        # Merge group size back to data\n        X = X.merge(right = group_size, \n                     how = 'left',\n                     left_on = 'group',\n                     right_index = True,\n                     suffixes = ('_', '_size'))\n        \n        # One Hot Encode: HomePlanet, CryoSleep\n        from sklearn.preprocessing import OneHotEncoder\n        cat_ohe = OneHotEncoder(sparse = False)\n        cat_encoded = pd.DataFrame(cat_ohe.fit_transform(X.loc[:, [homeplanet_ix, cryosleep_ix]]))\n        \n        # Keep only the columns we want\n        X_enriched = pd.concat([X['group_size'], cat_encoded], axis = 'columns')\n        \n        return np.array(X_enriched)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:25:20.606377Z","iopub.execute_input":"2022-05-22T22:25:20.606664Z","iopub.status.idle":"2022-05-22T22:25:20.616565Z","shell.execute_reply.started":"2022-05-22T22:25:20.606624Z","shell.execute_reply":"2022-05-22T22:25:20.615893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Testing**","metadata":{}},{"cell_type":"code","source":"# Numerical function testing\ntrain2 = train.copy()\n\nnum_cols = [col for col in train2.columns if train2[col].dtype == 'float64']\n\ntrain_num = train2[num_cols]\n\n# Impute data\nfrom sklearn.impute import SimpleImputer\nnum_imputer = SimpleImputer(strategy = 'mean')\n\ntrain_num = num_imputer.fit_transform(train_num)\n\nnum_class = NumericalAttributesAdder()\n\nnum_enriched = pd.DataFrame(num_class.fit_transform(train_num))\nnum_enriched","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:25:20.617665Z","iopub.execute_input":"2022-05-22T22:25:20.618143Z","iopub.status.idle":"2022-05-22T22:25:20.882749Z","shell.execute_reply.started":"2022-05-22T22:25:20.618107Z","shell.execute_reply":"2022-05-22T22:25:20.881838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Categorical function testing\ntrain3 = train.copy()\n\n# Get categorical columns\ncat_cols = ['PassengerId', 'HomePlanet', 'CryoSleep']\n\n# Reduce to categorical data\ntrain3 = train3[cat_cols]\n\n# Categorical imputer\ncat_imputer = SimpleImputer(strategy = 'most_frequent')\n\n# Fit_transform\ntrain_cat = cat_imputer.fit_transform(train3)\n\n# Enrich data\ncat_class = CategoricalAttributesAdder()\n\ncat_enriched = cat_class.fit_transform(train_cat)\ncat_enriched","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:25:20.883807Z","iopub.execute_input":"2022-05-22T22:25:20.884019Z","iopub.status.idle":"2022-05-22T22:25:20.985343Z","shell.execute_reply.started":"2022-05-22T22:25:20.883992Z","shell.execute_reply":"2022-05-22T22:25:20.984768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Pre-Processing Pipeline**","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Finding columns\nnum_cols = [col for col in train.columns if train[col].dtype == 'float64']\ncat_cols = ['PassengerId', 'HomePlanet', 'CryoSleep']\n\n# Numerical pipeline\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy = 'mean')),\n    ('attributes_adder', NumericalAttributesAdder())\n])\n\n# Categorical pipeline\ncat_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n    ('attributes_adder', CategoricalAttributesAdder()),\n])\n\n# Pre-processing pipeline\npre_processing = ColumnTransformer([\n    ('num', num_pipeline, num_cols),\n    ('cat1', cat_pipeline, cat_cols)\n])\n\n# Fit pipeline to training data\ntrain_prepared = pd.DataFrame(pre_processing.fit_transform(train))\ntrain_prepared","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:25:20.986579Z","iopub.execute_input":"2022-05-22T22:25:20.98703Z","iopub.status.idle":"2022-05-22T22:25:21.130549Z","shell.execute_reply.started":"2022-05-22T22:25:20.986991Z","shell.execute_reply":"2022-05-22T22:25:21.129727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Estimator (Logistic Regression)**","metadata":{}},{"cell_type":"code","source":"# Preparing data\ny_train = train['Transported'].astype('int')\nX_train = train.drop(labels = 'Transported', axis = 'columns')\n\n# Pre-processing\nX_train_prepared = pre_processing.fit_transform(X_train)\n\n# Selecting the model\nfrom sklearn.linear_model import LogisticRegression\nlogit_reg = LogisticRegression()\n\n# GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\nmy_params = [\n            {'solver' : ['newton-cg'], 'penalty': ['l2', 'none']},\n            {'solver' : ['lbfgs'], 'penalty' : ['l2', 'none']},\n            {'solver' : ['liblinear'], 'penalty' : ['l1', 'l2']},\n            {'solver' : ['sag'], 'penalty' : ['l2', 'none'], 'max_iter' : [500, 1000]}\n]\n\n# Fit model \nlogit_grid = GridSearchCV(estimator = logit_reg, param_grid = my_params, scoring = 'accuracy', cv = 4)\nlogit_grid.fit(X_train_prepared, y_train)\n\npd.DataFrame(logit_grid.cv_results_)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:25:21.13182Z","iopub.execute_input":"2022-05-22T22:25:21.132056Z","iopub.status.idle":"2022-05-22T22:25:24.160189Z","shell.execute_reply.started":"2022-05-22T22:25:21.132026Z","shell.execute_reply":"2022-05-22T22:25:24.15941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logit_estimator = logit_grid.best_estimator_\nprint(logit_estimator, logit_grid.best_score_)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:25:40.033392Z","iopub.execute_input":"2022-05-22T22:25:40.034342Z","iopub.status.idle":"2022-05-22T22:25:40.042505Z","shell.execute_reply.started":"2022-05-22T22:25:40.034293Z","shell.execute_reply":"2022-05-22T22:25:40.041595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Estimator (Support Vector Classifier)**","metadata":{}},{"cell_type":"code","source":"# Preparing data\ny_train = train['Transported'].astype('int')\nX_train = train.drop(labels = 'Transported', axis = 'columns')\n\n# Pre-processing\nX_train_prepared = pre_processing.fit_transform(X_train)\n\n# Selecting the model\nfrom sklearn.svm import SVC\nsvc = SVC()\n\n# GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\nmy_params = {'C': [0.25, 0.5, 0.75, 1, 1.25, 1.5],\n            'kernel': ['linear', 'rbf'],\n            'gamma': ['scale', 'auto']}\n\n# Fit model \nsvc_grid = GridSearchCV(estimator = svc, param_grid = my_params, scoring = 'accuracy', cv = 4)\nsvc_grid.fit(X_train_prepared, y_train)\n\npd.DataFrame(svc_grid.cv_results_)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:25:45.206153Z","iopub.execute_input":"2022-05-22T22:25:45.207279Z","iopub.status.idle":"2022-05-22T22:27:28.536166Z","shell.execute_reply.started":"2022-05-22T22:25:45.207159Z","shell.execute_reply":"2022-05-22T22:27:28.535573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc_estimator = svc_grid.best_estimator_\nprint(svc_estimator, svc_grid.best_score_)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:27:28.537472Z","iopub.execute_input":"2022-05-22T22:27:28.537789Z","iopub.status.idle":"2022-05-22T22:27:28.543106Z","shell.execute_reply.started":"2022-05-22T22:27:28.537761Z","shell.execute_reply":"2022-05-22T22:27:28.542138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Complete Pipeline**","metadata":{}},{"cell_type":"code","source":"# Complete pipeline\nfull_pipeline = Pipeline([\n    ('pre_processing', pre_processing),\n    ('estimator', svc_estimator)\n])\n\n# Fit the transformers, transform the data and then fit the estimator to the data\nfull_pipeline.fit(X_train, y_train)\n\n# Make predictions using the estimator\npred = full_pipeline.predict(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:27:28.54476Z","iopub.execute_input":"2022-05-22T22:27:28.544978Z","iopub.status.idle":"2022-05-22T22:27:32.422364Z","shell.execute_reply.started":"2022-05-22T22:27:28.544952Z","shell.execute_reply":"2022-05-22T22:27:32.421565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluation**","metadata":{}},{"cell_type":"code","source":"# Let's get a classification report\nfrom sklearn.metrics import classification_report\nreport = classification_report(y_train, pred, output_dict = True)\nreport","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:27:32.42395Z","iopub.execute_input":"2022-05-22T22:27:32.424159Z","iopub.status.idle":"2022-05-22T22:27:32.449057Z","shell.execute_reply.started":"2022-05-22T22:27:32.424133Z","shell.execute_reply":"2022-05-22T22:27:32.448269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predicting test set**","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv('../input/spaceship-titanic/sample_submission.csv')\n\nsubmission = sample_submission.copy()\n\n# Make transformations and predictions on the test set\ntest_pred = full_pipeline.predict(test)\n\n# Convert 1 & 0s to True/False\ntest_pred_tf = (test_pred == 1)\n\n# Make submission\nsubmission['Transported'] = test_pred_tf\n\n# Output to csv\nsubmission.to_csv('submission.csv', index = False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:27:32.450475Z","iopub.execute_input":"2022-05-22T22:27:32.45068Z","iopub.status.idle":"2022-05-22T22:27:33.036867Z","shell.execute_reply.started":"2022-05-22T22:27:32.450654Z","shell.execute_reply":"2022-05-22T22:27:33.035906Z"},"trusted":true},"execution_count":null,"outputs":[]}]}