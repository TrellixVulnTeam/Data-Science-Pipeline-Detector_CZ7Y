{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Spaceship Titanic Prediction","metadata":{}},{"cell_type":"markdown","source":"In this project, our task is to predict whether a passenger was transported to an alternate dimension during the Spaceship Titanic's collision with the spacetime anomaly. ","metadata":{}},{"cell_type":"markdown","source":"## File and Data Field Descriptions\n\n- train.csv - Personal records for about two-thirds (~8700) of the passengers, to be used as training data.\n    - PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n    - HomePlanet - The planet the passenger departed from, typically their planet of permanent residence.\n    - CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n    - Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n    - Destination - The planet the passenger will be debarking to.\n    - Age - The age of the passenger.\n    - VIP - Whether the passenger has paid for special VIP service during the voyage.\n    - RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n    - Name - The first and last names of the passenger.\n    - Transported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.\n- test.csv - Personal records for the remaining one-third (~4300) of the passengers, to be used as test data. Your task is to predict the value of Transported for the passengers in this set.\n- sample_submission.csv - A submission file in the correct format.\n    - PassengerId - Id for each passenger in the test set.\n    - Transported - The target. For each passenger, predict either True or False.\n","metadata":{}},{"cell_type":"markdown","source":"First of all, let's prepare libraries for analysis and then open files 'train.csv'. ","metadata":{}},{"cell_type":"markdown","source":"## Library importing","metadata":{}},{"cell_type":"code","source":"# Preparing Libraries \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd \nimport numpy as np\n\npd.options.display.max_columns = 200\nimport cufflinks as cf\ncf.go_offline(connected = True)\n\nimport plotly.express as px \nimport plotly.graph_objects as go \nimport plotly.offline as pyo\npyo.init_notebook_mode() ","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:51:29.929301Z","iopub.execute_input":"2022-04-01T00:51:29.930159Z","iopub.status.idle":"2022-04-01T00:51:34.486133Z","shell.execute_reply.started":"2022-04-01T00:51:29.930113Z","shell.execute_reply":"2022-04-01T00:51:34.484967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset importing","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/spaceship-titanic/train.csv', index_col = 'PassengerId')\ntest_df = pd.read_csv('../input/spaceship-titanic/test.csv', index_col = 'PassengerId')\n\ntrain_df.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:11.37005Z","iopub.execute_input":"2022-04-01T00:52:11.370368Z","iopub.status.idle":"2022-04-01T00:52:11.482342Z","shell.execute_reply.started":"2022-04-01T00:52:11.370336Z","shell.execute_reply":"2022-04-01T00:52:11.481687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Glimpse dataset","metadata":{}},{"cell_type":"markdown","source":"To get more details, we need to make function for viewing statistics of dataset. Our function 'data_glimpse(df)' shows dataset preview, column information, missing data, unique data, describe table and info table. ","metadata":{}},{"cell_type":"code","source":"def missing(df) : \n    \"\"\"\n    This function shows number of missing values and its percetages \n    \"\"\"\n    missing_number = df.isnull().sum().sort_values(ascending = False)\n    missing_percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending = False)\n    missing_values = pd.concat([missing_number, missing_percent], axis = 1, keys = ['Missing_number', 'Missing_percent'])\n    return missing_values \n\ndef categorize(df) :\n    \"\"\"\n    This function shows number of features by dtypes.\n    Result of function is not always accruate because this result estimate dtypes before preprocessing.\n    \"\"\"\n    Quantitive_features = df.select_dtypes([np.number]).columns.tolist()\n    Categorical_features = df.select_dtypes(exclude = [np.number]).columns.tolist()\n    Discrete_features = [col for col in Quantitive_features if len(df[col].unique()) < 10]\n    Continuous_features = [col for col in Quantitive_features if col not in Discrete_features]\n    print(f\"Quantitive feautres : {Quantitive_features} \\nDiscrete features : {Discrete_features} \\nContinous features : {Continuous_features} \\nCategorical features : {Categorical_features}\\n\")\n    print(f\"Number of quantitive feautres : {len(Quantitive_features)} \\nNumber of discrete features : {len(Discrete_features)} \\nNumber of continous features : {len(Continuous_features)} \\nNumber of categorical features : {len(Categorical_features)}\")\n    \ndef unique(df) : \n    \"\"\"\n    This function returns table storing number of unique values and its samples.\n    \"\"\"\n    tb1 = pd.DataFrame({'Columns' : df.columns, 'Number_of_Unique' : df.nunique().values.tolist(),\n                       'Sample1' : df.sample(1).values.tolist()[0], 'Sample2' : df.sample(1).values.tolist()[0], \n                       'Sample3' : df.sample(1).values.tolist()[0],\n                       'Sample4' : df.sample(1).values.tolist()[0], 'Sample5' : df.sample(1).values.tolist()[0]})\n    return tb1    \n\ndef data_glimpse(df) :   \n    \n    # Dataset preview \n    print(\"1. Dataset Preview \\n\")\n    display(df.head())\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    # Columns imformation\n    print(\"2. Column Information \\n\")\n    print(f\"Dataset have {df.shape[0]} rows and {df.shape[1]} columns\")\n    print(\"\\n\") \n    print(f\"Dataset Column name : {df.columns.values}\")\n    print(\"\\n\")\n    categorize(df)\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    # Basic imformation table \n    print(\"3. Missing data table : \\n\")\n    display(missing(df))\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    print(\"4. Number of unique value by column : \\n\")\n    display(unique(df))\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    print(\"5. Describe table : \\n\")\n    display(df.describe())\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    print(df.info())\n    print(\"-------------------------------------------------------------------------------\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:14.252296Z","iopub.execute_input":"2022-04-01T00:52:14.252742Z","iopub.status.idle":"2022-04-01T00:52:14.27038Z","shell.execute_reply.started":"2022-04-01T00:52:14.252707Z","shell.execute_reply":"2022-04-01T00:52:14.269729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_glimpse(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:15.513235Z","iopub.execute_input":"2022-04-01T00:52:15.51368Z","iopub.status.idle":"2022-04-01T00:52:15.65768Z","shell.execute_reply.started":"2022-04-01T00:52:15.513628Z","shell.execute_reply":"2022-04-01T00:52:15.656768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 8693 passengers in spaceship titanic with their 13 information. We can see there are 6 continuous features(‘Age’, ‘RoomService’, ‘FoodCourt’, ‘ShoppingMall’, ‘Spa’, ‘VRDeck’) and 8 categorical features(‘PassengerId’, ‘HomePlanet’, ‘CryoSleep’, ‘Cabin’, ‘Destination’, ‘VIP’, ‘Name’, ‘Transported’). \n\nThere are some missing values except columns ‘PassengerId’, ‘Transported’. We need to track why missing values occurs and need to impute them. \n\nAnd when we saw result of unique table, there are only three home planets and three destinations.","metadata":{}},{"cell_type":"markdown","source":"## Data cleaning for analysis","metadata":{}},{"cell_type":"markdown","source":"In data cleaning section, our goal is cleaning dataset, parsing values to get more insight in our dataset and also reducing data size.\n\nWhat we need to do saw the result of data_glimpse() is below : \n- parsing column 'Cabin' into Deck, DeckNumber, Side column\n- drop column 'Name'\n- filling missing values ","metadata":{}},{"cell_type":"markdown","source":"### Parsing column 'cabin'","metadata":{}},{"cell_type":"code","source":"# Parsing value from cabin\n\ndef parsing_from(dataset, idx) : \n    return dataset['Cabin'].str.split('/').str[idx]\n\ntrain_df['Deck'] = parsing_from(train_df, 0)\ntrain_df['DeckNumber'] = parsing_from(train_df, 1)\ntrain_df['Side'] = parsing_from(train_df, 2)\n\n# Check results\n\ntrain_df.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:21.223603Z","iopub.execute_input":"2022-04-01T00:52:21.224954Z","iopub.status.idle":"2022-04-01T00:52:21.295563Z","shell.execute_reply.started":"2022-04-01T00:52:21.224901Z","shell.execute_reply":"2022-04-01T00:52:21.294708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop exisiting column\n\ntrain_df.drop(['Cabin'], axis = 1, inplace = True)\n\ntrain_df.columns","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:21.398596Z","iopub.execute_input":"2022-04-01T00:52:21.398898Z","iopub.status.idle":"2022-04-01T00:52:21.40994Z","shell.execute_reply.started":"2022-04-01T00:52:21.398866Z","shell.execute_reply":"2022-04-01T00:52:21.40908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply process to test_df \n\ntest_df['Deck'] = parsing_from(test_df, 0)\ntest_df['DeckNumber'] = parsing_from(test_df, 1)\ntest_df['Side'] = parsing_from(test_df, 2)\ntest_df.drop(['Cabin'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:23.322326Z","iopub.execute_input":"2022-04-01T00:52:23.323095Z","iopub.status.idle":"2022-04-01T00:52:23.354692Z","shell.execute_reply.started":"2022-04-01T00:52:23.323053Z","shell.execute_reply":"2022-04-01T00:52:23.354029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop column 'Name'","metadata":{}},{"cell_type":"code","source":"train_df.drop(['Name'], axis = 1, inplace = True)\n\n# Apply process to test_df \n\ntest_df.drop(['Name'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:25.503837Z","iopub.execute_input":"2022-04-01T00:52:25.504553Z","iopub.status.idle":"2022-04-01T00:52:25.513562Z","shell.execute_reply.started":"2022-04-01T00:52:25.504507Z","shell.execute_reply":"2022-04-01T00:52:25.512882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### fill missing values","metadata":{}},{"cell_type":"code","source":"# automate layout for figure object\n\ndef fig_layout(title, xaxis, yaxis) : \n    fig.update_layout(\n    {\n        \"title\": {\n            \"text\": title,\n            \"x\": 0.5,\n            \"y\": 0.9,\n            \"font\": {\n                \"size\": 15\n            }\n        },\n        \"xaxis\": {\n            \"title\": xaxis,\n            \"showticklabels\":True,\n            \"tickfont\": {\n                \"size\": 9            \n            }\n        },\n        \"yaxis\": {\n            \"title\": yaxis,\n            \"tickfont\": {\n                \"size\": 10                \n            }\n        },\n        \"template\":'plotly_dark'\n    }\n    )\n    ","metadata":{"hide_input":false,"execution":{"iopub.status.busy":"2022-04-01T00:52:27.60741Z","iopub.execute_input":"2022-04-01T00:52:27.608337Z","iopub.status.idle":"2022-04-01T00:52:27.614801Z","shell.execute_reply.started":"2022-04-01T00:52:27.608296Z","shell.execute_reply":"2022-04-01T00:52:27.61393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot for viewing missing values \n\nmissing_val = train_df.isnull().sum().sort_values(ascending = False)\n\nfig = go.Figure()\n\nfig.add_trace(\n    go.Bar(\n        x = missing_val.index,\n        y = missing_val,\n        text = missing_val\n    )\n)\n\ntitle = \"<b>Count of missing values by features</b>\"\nxaxis = \"Variables\"\nyaxis = \"Count of missing values\"\n\nfig_layout(title, xaxis, yaxis)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:27.784944Z","iopub.execute_input":"2022-04-01T00:52:27.785251Z","iopub.status.idle":"2022-04-01T00:52:28.599179Z","shell.execute_reply.started":"2022-04-01T00:52:27.785215Z","shell.execute_reply":"2022-04-01T00:52:28.598189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Without our target variable 'Transported' and index 'Passenger Group' and 'Passenger Number', features in dataset have about 190 missing values. That amount is about only 2% of whole dataset size. We will impute missing values with most frequency values using SimpleImputer().","metadata":{}},{"cell_type":"code","source":"# We will drop 'Transported' from train_df to apply SimpleImputer\n\ny = train_df['Transported']\ntrain_df = train_df.drop(columns = ['Transported'], axis = 1)\n\nprint(f\"Shape of train dataset : {train_df.shape}\")\nprint(f\"Shape of test dataset : {test_df.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:30.730047Z","iopub.execute_input":"2022-04-01T00:52:30.730334Z","iopub.status.idle":"2022-04-01T00:52:30.73841Z","shell.execute_reply.started":"2022-04-01T00:52:30.730305Z","shell.execute_reply":"2022-04-01T00:52:30.737436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit SimpleImputer to train dataset and apply it to test dataset\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n\ntrain_df = pd.DataFrame(imputer.fit_transform(train_df), columns = train_df.columns, index = train_df.index)\ntest_df = pd.DataFrame(imputer.transform(test_df), columns = test_df.columns, index = test_df.index)\n\n# Change type object to numeric\n\ntrain_df[['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', \n          'Spa', 'VRDeck', 'DeckNumber']] = train_df[['Age', 'RoomService', 'FoodCourt', \n                                                       'ShoppingMall', 'Spa', 'VRDeck', 'DeckNumber']].apply(pd.to_numeric)\ntest_df[['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', \n          'Spa', 'VRDeck', 'DeckNumber']] = test_df[['Age', 'RoomService', 'FoodCourt', \n                                                       'ShoppingMall', 'Spa', 'VRDeck', 'DeckNumber']].apply(pd.to_numeric)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:30.928049Z","iopub.execute_input":"2022-04-01T00:52:30.928374Z","iopub.status.idle":"2022-04-01T00:52:31.357251Z","shell.execute_reply.started":"2022-04-01T00:52:30.92834Z","shell.execute_reply":"2022-04-01T00:52:31.356491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Missing values of train_df : {train_df.isnull().sum().sum()}\")\nprint(f\"Missing values of test_df : {test_df.isnull().sum().sum()}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:31.359081Z","iopub.execute_input":"2022-04-01T00:52:31.35941Z","iopub.status.idle":"2022-04-01T00:52:31.379565Z","shell.execute_reply.started":"2022-04-01T00:52:31.359367Z","shell.execute_reply":"2022-04-01T00:52:31.378598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concat target to train_df\n\ntrain_df = pd.concat([train_df, y], axis = 1)\ntrain_df.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:31.399795Z","iopub.execute_input":"2022-04-01T00:52:31.400067Z","iopub.status.idle":"2022-04-01T00:52:31.421459Z","shell.execute_reply.started":"2022-04-01T00:52:31.400039Z","shell.execute_reply":"2022-04-01T00:52:31.420869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We finished filling missing values with most frequent values. ","metadata":{}},{"cell_type":"markdown","source":"## Explore dataset ","metadata":{}},{"cell_type":"markdown","source":"After parsing and cleaning dataset, now we need to explore dataset to process outlier and missing values. To do this, we need to explore dataset with visualization or statistics. We will see visualizations of missing values, statistics and plots of each columns.","metadata":{}},{"cell_type":"markdown","source":"### Statistics and Visualizaitons of each columns","metadata":{}},{"cell_type":"markdown","source":"To find how we fill missing values with and is there any outlier in numerical features, we need to see statistics and visualization of each columns. First of all, we need to devide columns into continuous and categorical features.\n\n- continuous_fea = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']  \n- categorical_fea = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Deck Number', 'Side']\n- target_fea = 'Transported'\n\nLet's see distribution of target feature first and look at independent variables. ","metadata":{}},{"cell_type":"code","source":"fig = px.histogram(train_df, \"Transported\", color = \"Transported\")\n\ntitle = \"Histogram of Target feature\"\nxaxis = \"Transported\"\nyaxis = \"Count\"\nfig_layout(title, xaxis, yaxis)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:33.902235Z","iopub.execute_input":"2022-04-01T00:52:33.902501Z","iopub.status.idle":"2022-04-01T00:52:34.22134Z","shell.execute_reply.started":"2022-04-01T00:52:33.902472Z","shell.execute_reply":"2022-04-01T00:52:34.220296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have balanced target variables which false is 4315 and true is 4378.","metadata":{}},{"cell_type":"markdown","source":"### Continuous feature ","metadata":{}},{"cell_type":"code","source":"continuous_fea = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'] \ncategorical_fea = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side']\n\ntrain_df[continuous_fea].describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:34.893725Z","iopub.execute_input":"2022-04-01T00:52:34.894002Z","iopub.status.idle":"2022-04-01T00:52:34.927431Z","shell.execute_reply.started":"2022-04-01T00:52:34.893972Z","shell.execute_reply":"2022-04-01T00:52:34.926837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The standard deviation of 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck' is too big, so we need to deal with those outliers. ","metadata":{}},{"cell_type":"code","source":"# See distplot of two continuous features 'Age' and 'RoomService'\n\nimport plotly.figure_factory as ff \n\nfig = ff.create_distplot([train_df['Age']], ['Age'], bin_size = 5, \n                         curve_type = 'normal')\n\ntitle = \"<b>Distplot of Age</b>\"\nxaxis = \"Age\"\nyaxis = \"%\"\nfig_layout(title, xaxis, yaxis)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:35.458694Z","iopub.execute_input":"2022-04-01T00:52:35.459242Z","iopub.status.idle":"2022-04-01T00:52:35.610619Z","shell.execute_reply.started":"2022-04-01T00:52:35.4592Z","shell.execute_reply":"2022-04-01T00:52:35.609757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we saw at the desribe table, most age in spaceship is 28 years old. ","metadata":{}},{"cell_type":"code","source":"# Age by target\n\nfig = px.histogram(train_df, x = 'Age', color = 'Transported')\n\ntitle = \"<b>Distplot of Age by target</b>\"\nxaxis = \"Age\"\nyaxis = \"%\"\nfig_layout(title, xaxis, yaxis)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:37.42855Z","iopub.execute_input":"2022-04-01T00:52:37.42938Z","iopub.status.idle":"2022-04-01T00:52:37.580509Z","shell.execute_reply.started":"2022-04-01T00:52:37.429331Z","shell.execute_reply":"2022-04-01T00:52:37.579705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = ff.create_distplot([np.sqrt(train_df['RoomService'])], ['RoomService'], bin_size = 10, \n                         curve_type = 'normal')\n\ntitle = \"<b>Distplot of RoomService</b>\"\nxaxis = \"RoomService\"\nyaxis = \"%\"\nfig_layout(title, xaxis, yaxis)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:39.08636Z","iopub.execute_input":"2022-04-01T00:52:39.086978Z","iopub.status.idle":"2022-04-01T00:52:39.219619Z","shell.execute_reply.started":"2022-04-01T00:52:39.086935Z","shell.execute_reply":"2022-04-01T00:52:39.218721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import skew\n\nfor fea in continuous_fea : \n    print(f\"Skewness of {fea} is {skew(np.array(train_df[fea]))}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:39.221439Z","iopub.execute_input":"2022-04-01T00:52:39.221914Z","iopub.status.idle":"2022-04-01T00:52:39.230535Z","shell.execute_reply.started":"2022-04-01T00:52:39.221876Z","shell.execute_reply":"2022-04-01T00:52:39.229693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Diferrence of min and max values of 'RoomService' is so high that i need to apply np.sqrt() to view distplot. When we check skewness of other features of continuous_fea, without Age, all the other features's skewness is higher than 7. We will impute outliers using IQR range.","metadata":{}},{"cell_type":"markdown","source":"### Process Outlier","metadata":{}},{"cell_type":"code","source":"def impute_outlier(col) : \n    Q1 = train_df[col].quantile(0.25) \n    Q3 = train_df[col].quantile(0.75)\n    IQR = Q3 - Q1\n    \n    return Q3 + (1.5*IQR)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:44.189457Z","iopub.execute_input":"2022-04-01T00:52:44.190087Z","iopub.status.idle":"2022-04-01T00:52:44.194715Z","shell.execute_reply.started":"2022-04-01T00:52:44.190047Z","shell.execute_reply":"2022-04-01T00:52:44.193919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outlier_fea = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'] \n\nfor fea in outlier_fea : \n    max_val = impute_outlier(fea)\n    train_df[fea] = train_df[fea].map(lambda x : max_val if x > max_val else x)\n    # Apply result to test dataset\n    test_df[fea] = test_df[fea].map(lambda x : max_val if x > max_val else x)   ","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:44.467642Z","iopub.execute_input":"2022-04-01T00:52:44.467945Z","iopub.status.idle":"2022-04-01T00:52:44.524628Z","shell.execute_reply.started":"2022-04-01T00:52:44.467909Z","shell.execute_reply":"2022-04-01T00:52:44.523972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = ff.create_distplot([np.sqrt(train_df['RoomService'])], ['RoomService'], bin_size = 3, \n                         curve_type = 'normal')\n\ntitle = \"<b>Distplot of RoomService</b>\"\nxaxis = \"RoomService\"\nyaxis = \"%\"\nfig_layout(title, xaxis, yaxis)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:52:44.719487Z","iopub.execute_input":"2022-04-01T00:52:44.720061Z","iopub.status.idle":"2022-04-01T00:52:44.850071Z","shell.execute_reply.started":"2022-04-01T00:52:44.720013Z","shell.execute_reply":"2022-04-01T00:52:44.849497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now outlier is replaced by $Q3 + 1.5 \\times IQR$. ","metadata":{}},{"cell_type":"markdown","source":"### Categorical features","metadata":{}},{"cell_type":"markdown","source":"we will see univariate and coparision by target feature count plot of categorical features. \n\n- See univariate count plots of each color\n- See count plots of each color by target ","metadata":{}},{"cell_type":"code","source":"from plotly.subplots import make_subplots\n\ndef create_count_plot(fea) : \n    grouped_df = train_df.groupby(fea).size().reset_index()\n    grouped_df.columns = [fea, 'Count']\n    \n    grouped_df_target = train_df.groupby([fea, 'Transported']).size().reset_index()\n    grouped_df_target.columns = [fea, 'Transported', 'Count']\n    \n    fig = make_subplots(rows=1, cols=2)\n\n    fig.add_trace(go.Bar(\n        x = grouped_df[fea],\n        y = grouped_df[\"Count\"],\n        name = fea \n    ), row = 1, col = 1)\n    \n    for trans in train_df['Transported'].unique() : \n        plot_df = grouped_df_target[grouped_df_target['Transported'] == trans]\n        fig.add_trace(go.Bar(\n            x = plot_df[fea],\n            y = plot_df[\"Count\"],\n            name = f\"Transported {trans}\"\n        ), row = 1, col = 2)\n        \n    fig.update_layout(\n    {\n        \"title\": {\n            \"text\": f\"Countplots of {fea}\",\n            \"x\": 0.5,\n            \"y\": 0.9,\n            \"font\": {\n                \"size\": 15\n            }\n        },\n        \"yaxis\": {\n            \"title\": \"Count\",\n            \"tickfont\": {\n                \"size\": 10                \n            }\n        },\n        \"template\":'plotly_dark'\n    }\n    )  \n    \n    fig.update_xaxes(title_text=fea, row=1, col=1)\n    fig.update_xaxes(title_text=fea, row=1, col=2)\n        \n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:53:41.370524Z","iopub.execute_input":"2022-04-01T00:53:41.370907Z","iopub.status.idle":"2022-04-01T00:53:41.384221Z","shell.execute_reply.started":"2022-04-01T00:53:41.370871Z","shell.execute_reply":"2022-04-01T00:53:41.383285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of \"HomePlanet\"\n\ncreate_count_plot(\"HomePlanet\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:53:42.270395Z","iopub.execute_input":"2022-04-01T00:53:42.271357Z","iopub.status.idle":"2022-04-01T00:53:42.355406Z","shell.execute_reply.started":"2022-04-01T00:53:42.271314Z","shell.execute_reply":"2022-04-01T00:53:42.354184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Earth has the most numerous passengers in space titanic, following Europa and Mars. By the way, Earth also has the most numerous passengers didn't transported while Europa has the most numerous passengers transported.","metadata":{}},{"cell_type":"code","source":"# Distribution of \"CryoSleep\"\n\ncreate_count_plot(\"CryoSleep\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:53:44.283258Z","iopub.execute_input":"2022-04-01T00:53:44.283612Z","iopub.status.idle":"2022-04-01T00:53:44.365181Z","shell.execute_reply.started":"2022-04-01T00:53:44.283578Z","shell.execute_reply":"2022-04-01T00:53:44.364027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, the passengers who is in CryoSleep transported more than those who isn't in CryoSleep.","metadata":{}},{"cell_type":"code","source":"# Distribution of \"Destination\"\n\ncreate_count_plot(\"Destination\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:53:44.591062Z","iopub.execute_input":"2022-04-01T00:53:44.591396Z","iopub.status.idle":"2022-04-01T00:53:44.908543Z","shell.execute_reply.started":"2022-04-01T00:53:44.591362Z","shell.execute_reply":"2022-04-01T00:53:44.907755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_count_plot(\"VIP\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:53:44.910104Z","iopub.execute_input":"2022-04-01T00:53:44.910513Z","iopub.status.idle":"2022-04-01T00:53:44.989675Z","shell.execute_reply.started":"2022-04-01T00:53:44.910471Z","shell.execute_reply":"2022-04-01T00:53:44.988966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There isn't a big gap between passengers who have VIP or not. ","metadata":{}},{"cell_type":"code","source":"create_count_plot(\"Deck\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:53:44.991143Z","iopub.execute_input":"2022-04-01T00:53:44.991487Z","iopub.status.idle":"2022-04-01T00:53:45.072075Z","shell.execute_reply.started":"2022-04-01T00:53:44.991446Z","shell.execute_reply":"2022-04-01T00:53:45.070508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Passengers who uses deck B and C have been trasported more than other decks. ","metadata":{}},{"cell_type":"code","source":"create_count_plot(\"Side\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:53:47.309926Z","iopub.execute_input":"2022-04-01T00:53:47.31021Z","iopub.status.idle":"2022-04-01T00:53:47.389076Z","shell.execute_reply.started":"2022-04-01T00:53:47.310179Z","shell.execute_reply":"2022-04-01T00:53:47.387992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling ","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing ","metadata":{}},{"cell_type":"markdown","source":"Before we make a model for prediction, we need to choose some features improving our model's accuracy. First of all, we will divid target variable from train_df.","metadata":{}},{"cell_type":"code","source":"# Divide target features from train_df and change values with 0 and 1\n\ny_train = np.where(train_df['Transported'] == True, 1, 0)\nX = train_df.drop(columns = ['Transported'], axis = 1)\nX_test = test_df\n\nprint(f\"Size of each table : y = {y.shape}, X = {X.shape}, X_test = {X_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:53:49.699281Z","iopub.execute_input":"2022-04-01T00:53:49.699571Z","iopub.status.idle":"2022-04-01T00:53:49.709641Z","shell.execute_reply.started":"2022-04-01T00:53:49.699527Z","shell.execute_reply":"2022-04-01T00:53:49.708484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Selection\n\nWe will use correlation metrics to choose vairables from continuous features.","metadata":{}},{"cell_type":"code","source":"train_df['Transported'] = np.where(train_df['Transported'] == True, 1, 0)\ncorr_fea = continuous_fea\ncorr_fea.append('Transported')\ncorr = train_df[corr_fea].corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\ndf_mask = corr.mask(mask)\n\nfig = ff.create_annotated_heatmap(z=df_mask.round(3).to_numpy(), \n                                  x=df_mask.columns.tolist(),\n                                  y=df_mask.columns.tolist(),\n                                  colorscale=px.colors.diverging.RdBu,\n                                  hoverinfo=\"none\",\n                                  showscale=True, ygap=1, xgap=1\n                                 )\n\nfig.update_xaxes(side=\"bottom\")\n\nfig.update_layout(\n    title_text='Heatmap', \n    title_x=0.5, \n    width=1000, \n    height=1000,\n    xaxis_showgrid=False,\n    yaxis_showgrid=False,\n    xaxis_zeroline=False,\n    yaxis_zeroline=False,\n    yaxis_autorange='reversed',\n    template='plotly_dark'\n)\n\nfor i in range(len(fig.layout.annotations)):\n    if fig.layout.annotations[i].text == 'nan':\n        fig.layout.annotations[i].text = \"\"\n    fig.layout.annotations[i].font.size = 10\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:53:50.64792Z","iopub.execute_input":"2022-04-01T00:53:50.648271Z","iopub.status.idle":"2022-04-01T00:53:50.849592Z","shell.execute_reply.started":"2022-04-01T00:53:50.648236Z","shell.execute_reply":"2022-04-01T00:53:50.848426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# OneHotEncoding\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nOH_encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n\nOH_encoder.fit(X[categorical_fea])\n\nOH_X_train = pd.DataFrame(OH_encoder.transform(X[categorical_fea]))\nOH_X_train.index = X.index\n\nOH_X_test = pd.DataFrame(OH_encoder.transform(X_test[categorical_fea]))\nOH_X_test.index = X_test.index\n\nnum_X = X.drop(categorical_fea, axis = 1)\nnum_X_test = X_test.drop(categorical_fea, axis = 1)\n\nX = pd.concat([num_X, OH_X_train], axis = 1)\nX_test = pd.concat([num_X_test, OH_X_test], axis = 1)\n\n# MinMaxScaling \n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler() \n\nX = scaler.fit_transform(X)\nX_test = scaler.transform(X_test)\n\nprint(f\"Size of each table : y_train = {y.shape}, X = {X.shape}, X_test = {X_test.shape}\")\n\n# Divide dataset into train, valid dataset\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size = 0.8, test_size = 0.2)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:53:50.852177Z","iopub.execute_input":"2022-04-01T00:53:50.852556Z","iopub.status.idle":"2022-04-01T00:53:50.938699Z","shell.execute_reply.started":"2022-04-01T00:53:50.852495Z","shell.execute_reply":"2022-04-01T00:53:50.937537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use Logistic Regression, KNN, Decision Tree, RandomForest. we will measure accuracy of model using ROC score, F1-Score, Confusion matrix. ","metadata":{}},{"cell_type":"code","source":"# Importing libraries for metrics\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, roc_curve, accuracy_score, f1_score, roc_auc_score\n\n\n# Confusion matrix \n\ndef accuracy_plots(y_valid, preds, yscore) : \n    \n    fig = make_subplots(rows=1, cols=2)\n    \n    # Confusion matrix\n    \n    z = confusion_matrix(y_valid, preds)\n    x = ['Negative', 'Positive']\n    y = ['False', 'True']    \n    \n    fig.add_trace(go.Heatmap(\n        z=z,\n        x=x,\n        y=y,\n        text = z,\n        reversescale = True\n    ), row = 1, col = 1)\n    \n    # Roc-Auc curve\n    \n    fpr, tpr, thresholds = roc_curve(y_valid, yscore) \n    \n    fig.add_trace(go.Scatter(\n        x = fpr, y = tpr,\n        fill = 'tozeroy'\n    ), row = 1, col = 2)\n    \n    fig.update_layout(\n    {\n        \"title\": {\n            \"text\": f\"Confusion Matrix and ROC Curve\",\n            \"x\": 0.5,\n            \"y\": 0.9,\n            \"font\": {\n                \"size\": 15\n            }\n        },\n        \"template\":'plotly_dark'\n    }\n    )  \n    \n    fig.update_xaxes(title_text=\"Confusion Matrix\", row=1, col=1)\n    fig.update_yaxes(autorange=\"reversed\", row = 1, col = 1) \n    fig.update_xaxes(title_text=\"Roc Curve\", row=1, col=2)\n    \n    fig.show()\n    \n    \n# Accuracy Metrics\n\ndef metrics(estimators, params, X_train, y_train, X_valid, y_valid) : \n    metrics = []\n    \n    for name, model in estimators.items() : \n        grid_model = GridSearchCV(model, params[name], cv = 3)\n        grid_model.fit(X_train, y_train)\n        best_model = grid_model.best_estimator_\n        preds = best_model.predict(X_valid)\n        yscore = best_model.predict_proba(X_valid)[:, 1]\n        print(f\"Hyperparameter tuning of model : {grid_model.best_estimator_}\")\n            \n        scores = {}\n        scores['accuracy_score'] = accuracy_score(y_valid, preds)\n        scores['f1_score'] = f1_score(y_valid, preds)\n        scores['roc_auc_scor'] = roc_auc_score(y_valid, yscore)\n        \n        metrics.append(scores) \n        accuracy_plots(y_valid, preds, yscore)\n        \n    metrics_df = pd.DataFrame(metrics, index = ['Logisitc Regression', 'Support Vector Machine', 'Random Forest', 'Gradient Boosting Classifier', 'LGBM Classifier'])\n        \n    return metrics_df ","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:53:51.892925Z","iopub.execute_input":"2022-04-01T00:53:51.893259Z","iopub.status.idle":"2022-04-01T00:53:51.910558Z","shell.execute_reply.started":"2022-04-01T00:53:51.893225Z","shell.execute_reply":"2022-04-01T00:53:51.90956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model importing\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\n\nlgr = LogisticRegression(random_state = 0)\nsvc = SVC(random_state = 0, probability = True)\nrfc = RandomForestClassifier(random_state = 0)\ngbc = GradientBoostingClassifier(random_state = 0)\nlgbm = LGBMClassifier(random_state = 0) \n\nestimators = { \n    'LGR' : lgr,\n    'SVC' : svc,\n    'RFC' : rfc,\n    'GBC' : gbc,\n    'lgbm' : lgbm\n}\n\nparams = {\n    'LGR' : {\n        'C' : [0.01, 0.1, 0.5, 1],\n        'penalty' : ['l1', 'l2', 'elastic']\n    },\n    'SVC' : {\n        'C' : [0.1 ,1 ,2, 5],\n        'kernel' : ['linear', 'poly'],\n        'degree' : [2, 3]\n    },\n    'RFC' : {\n        'n_estimators' : [50, 100, 150, 200, 250],\n        'max_depth' : [5, 6, 7]   \n    },\n    'GBC' : { \n        \"n_estimators\": range(50, 100, 25), \n        \"max_depth\": [4, 5, 10]\n    },\n    'lgbm' : {\n        'num_leaves':[5, 10, 15], \n        'min_child_samples':[10,15, 20],\n        'max_depth':[5, 8, 10],\n        'reg_alpha':[0.01,0.03]\n\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:53:57.493664Z","iopub.execute_input":"2022-04-01T00:53:57.494008Z","iopub.status.idle":"2022-04-01T00:53:57.927077Z","shell.execute_reply.started":"2022-04-01T00:53:57.49397Z","shell.execute_reply":"2022-04-01T00:53:57.925833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics(estimators, params, X_train, y_train, X_valid, y_valid) ","metadata":{"execution":{"iopub.status.busy":"2022-04-01T00:54:01.307679Z","iopub.execute_input":"2022-04-01T00:54:01.307989Z","iopub.status.idle":"2022-04-01T01:01:57.227132Z","shell.execute_reply.started":"2022-04-01T00:54:01.307956Z","shell.execute_reply":"2022-04-01T01:01:57.226184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"The model have best score of accuracy is Gradient Boosting Classifier. To predict test dataset, we will use Gradient Boosting Classifier. ","metadata":{}},{"cell_type":"code","source":"final_model = GradientBoostingClassifier(max_depth=4, n_estimators=75, random_state=0)\nfinal_model.fit(X_train, y_train)\npredicts = final_model.predict(X_test)\n\nsubmissions = pd.DataFrame({'PassengerId' : test_df.index,\n                            'Transported' : predicts})\nsubmissions['Transported'] = np.where(submissions['Transported'] == 1, True, False) \nsubmissions.to_csv('./submissions.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T01:02:13.994316Z","iopub.execute_input":"2022-04-01T01:02:13.994913Z","iopub.status.idle":"2022-04-01T01:02:15.038024Z","shell.execute_reply.started":"2022-04-01T01:02:13.994866Z","shell.execute_reply":"2022-04-01T01:02:15.037246Z"},"trusted":true},"execution_count":null,"outputs":[]}]}