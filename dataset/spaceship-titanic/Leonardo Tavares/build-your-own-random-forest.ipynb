{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Build your own Random Forest\n\n![it's easy!!!](https://media.makeameme.org/created/its-easy-just-5bd65c.jpg)\n\n\nHave you ever thought about planting, I mean, implementing your own Random Forest? Yea? No? Well... let's implement one here!\n\n\nBefore starting, I would like to present some preliminary concepts:\n\n## Please upvote me if you like, ok? (this is really  really important to me)\n\n## 1.1 Preliminary concepts\n\n* **Every model** (model = machine learning method in which the data for training has already been presented) has some kind of **weakness** (either because of overfitting / or because of underfitting / or because of the complexity of the problem / problem dimension / data quality / whether by noise / ...)\n* There is a ceiling that the model alone can perform (by whatever metric we are using, whether for regression, classification, time series...). For example, a linear regression there is a limit to what it can represent, a decision tree as well...\n* There is still another component, which is the **irreducible error**: the one that we cannot overcome and that is inherent to the problem, regardless of the model...\n* So...what is the idea of the **ensemble**? combine **several weak models** and thus **create a strong model** (ensemble) so that they can obtain better performances..\n* For those who enjoy competitions on Kaggle, for example, the best ranked results are produced by this type of approach...\n\n## 1.2 Ensemble\n\n* The word ensemble, comes from French, means \"together\"...(cool huh?) \n* ... and the ensemble approach depends on the type of problem we are trying to address...\n* If we think about the bias and variance problem, we have 4 combinations\n    * What we want: a model that has a **good balance** between bias (low) and variance (low)\n    * If we have a weak model that has high variance and low bias then this model has learned too much and does not generalize (overfitting), then it is recommended to use ensembles that tend to reduce variance (**bagging**, for example)\n    * If we have in the other case, where we have low variance but high bias, then it is recommended to use an ensemble that tends to decrease the bias (**stacking or boosting**, for example)\n    * If the model has high bias and high variance, then it is incoherent/inconsistent...\n* Basically there are 3 forms of ensemble: Bagging, Boosting and Stacking\n\n### 1.2.1 Bagging (bootstrap aggregating)\n\n* Helps to create a model that is more robust than models alone\n* Each isolated model knows only part of the data (hence the name bootstrap, which is the generation of smaller bases given the larger base)\n* As each isolated model knows only part of the data, it tends to be less overfitting\n* The idea is to generate an ensemble model whose result is the \"average\" of the output of each isolated model (it can be by majority of votes (hard voting) for the classification case, for example OR the average probability of each class of each isolated model (soft voting))\n* Just remember the statistic that if we have a variable i.d.d (independently and identically distributed) and we calculate the mean, we have an estimator that preserves the expected value and low variance\n* We have a very good advantage here, which is the issue of **parallelism** (the training of each isolated model)\n* **Random Forest** is a successful example of bagging: you sample your database for some decision trees, and then average the outputs of each tree, so you have simpler trees (with less overfitting), reducing variance and overall more robust performance\n\n### 1.2.2 Boosting\n\n* Boosting is a **sequential** improvement technique, which aims to **reduce bias**\n* The idea is to sequentially adjust several weak models (iterative way), so that a model in a given step depends on the models of the previous steps\n* Each model in the sequence is fitted by giving more importance to observations in the data set that were **difficult to fit** by previous models in the sequence. Intuitively, each new model focuses its efforts on the most difficult-to-fit observations so far\n* We have a set of weights, where each weight is associated with an estimator\n* Here we have an iterative optimization process, **which is not easy to solve**, where we want to find the best set of weights\n* The most famous boosting methods use **gradient** (XGBooting, Light GBM) / or adaptive (Adaboosting) that each iteration penalizes the worst estimators so that they don't mess up the final result.\n* The **disadvantage** is that boosting cannot be parallelizable (which can be computationally expensive)\n\n### 1.2.3 Stacking\n\n* So far we have ensembles that are homogeneous (all the estimated ones are of the **same nature**)\n* But there is a way to join heterogeneous weak models, which is the case of Stacking, which is nothing more than layers of weak models\n* We can build a classifier that is, for example, formed by the first layer where we have (SVM, KNN, Decision Tree, Bayesian)\n* And in the second layer (meta-model) we can have a logistic regression\n* The second layer will combine the output of each of the models from the first layer, and generate a new result\n* And that's why he called stacking (stack of miscellaneous estimators)\n* It is possible to create stacking of different levels, 3, 4, 5, ...\n* The problem is that with each layer, it gets more **computationally expensive**\n\n### In conclusion...\n\n* Ensemble is a good output to help solve **bias** and **variance** problems\n* Each technique attacks a type of problem\n* And always be careful with the computational cost\n* Chapters 15 and 16 of The Elements of Statistical Learning (Hastie) deal with this subject in great depth\n\n# 2. Now let's build our own Random Forest\n\n![Random, Forest, Radom!!!](https://memegenerator.net/img/instances/55591858.jpg)\n\n\n## 2.1 What will we need?\n\n1. Data (of course)\n2. Some decision trees üå≥üå≥üå≥üå≥üå≥üå≥üå≥\n3. Water üíßüíßüíßüíßüíßüíß (for the trees)\n4. Some python code üêç\n\n\n\n**Hey ho, let's go! Hey ho, let's go!**\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# general imports\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# sklearn\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.base import BaseEstimator\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import VotingClassifier\n\nfrom mlxtend.feature_selection import ColumnSelector\n\nfrom yellowbrick.model_selection import ValidationCurve","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T00:49:09.238867Z","iopub.execute_input":"2022-06-06T00:49:09.239363Z","iopub.status.idle":"2022-06-06T00:49:10.402006Z","shell.execute_reply.started":"2022-06-06T00:49:09.239317Z","shell.execute_reply":"2022-06-06T00:49:10.400931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SOME CONSTANTS\nSEED = 123\n\n# so that you can reproduce the notebook\nnp.random.seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T00:49:10.405187Z","iopub.execute_input":"2022-06-06T00:49:10.405679Z","iopub.status.idle":"2022-06-06T00:49:10.411336Z","shell.execute_reply.started":"2022-06-06T00:49:10.405633Z","shell.execute_reply":"2022-06-06T00:49:10.409889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Importing the data**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\nprint(df.shape)\ndf.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T00:49:10.413079Z","iopub.execute_input":"2022-06-06T00:49:10.413834Z","iopub.status.idle":"2022-06-06T00:49:10.508994Z","shell.execute_reply.started":"2022-06-06T00:49:10.413777Z","shell.execute_reply":"2022-06-06T00:49:10.507591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's do a little exploratory data analysis + data engineering**","metadata":{}},{"cell_type":"code","source":"count = 1\nfor c in df.columns:\n    print(f'{count} - {c}')\n    print(f'- # of unique elements: {df[c].nunique()}')\n    print(f'- Sample: {df[c].unique()[0:20]}')\n    print(f'- Dtype: {df[c].dtype}')\n    print(f'- # of missing values: {df[c].isnull().sum()} of {df.shape[0]}')\n    print(f'- % of missing values: {np.round(df[c].isnull().sum() / df.shape[0], 3)}')\n    \n    \n    if df[c].dtype == int or df[c].dtype == float:\n        s = \"- Statistics:\\n\"\n\n        me = np.round(df[c].mean(), 2)\n        st = np.round(df[c].std(), 2)\n        s += f\"-- Mean (std): {me} ({st})\\n\"\n\n        q1 = np.round(df[c].quantile(0.25), 2)\n        q2 = np.round(df[c].quantile(0.5), 2)\n        q3 = np.round(df[c].quantile(0.75), 2)\n        s += f\"-- Quantiles: q1={q1}, q2={q2}, q3={q3}\\n\"\n        s += f\"-- Min {df[c].min()}\\n\"\n        s += f\"-- Max {df[c].max()}\"    \n        print(s)\n        \n    print('='*30)\n    count += 1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-06T00:49:10.511143Z","iopub.execute_input":"2022-06-06T00:49:10.511454Z","iopub.status.idle":"2022-06-06T00:49:10.604792Z","shell.execute_reply.started":"2022-06-06T00:49:10.511425Z","shell.execute_reply":"2022-06-06T00:49:10.603778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Please upvote me if you like, ok? (this is really  really important to me)\n\nWhat we have?\n\n* Numerical variables:\n    * Age\n    * RoomService\n    * FoodCourt\n    * ShoppingMall\n    * Spa\n    * VRDeck\n* Categorical variables:\n    * HomePlanet\n    * Destination\n* Binary variables:\n    * CryoSleep\n    * VIP\n* Columns to drop:\n    * PassengerId (Not interesting for the model)\n    * Cabin (I don't know if it's relevant to the predictive model)\n    * Name (I don't know if it's relevant to the predictive model)\n* Outcome:\n    * Transported\n    \n","metadata":{}},{"cell_type":"code","source":"# drop some columns\ndf.drop(columns=['PassengerId', 'Cabin', 'Name'], inplace=True)\nprint(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T00:49:10.606333Z","iopub.execute_input":"2022-06-06T00:49:10.606957Z","iopub.status.idle":"2022-06-06T00:49:10.633731Z","shell.execute_reply.started":"2022-06-06T00:49:10.606912Z","shell.execute_reply":"2022-06-06T00:49:10.632787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create a (very very very simple) pipeline for the predictor variables, which performs the following tasks:\n\n* Numeric features:\n    * Imputer: KNNImputer (k=5)\n    * Scaler: StandardScaler\n* Categorial features:\n    * Imputer: Most frequent\n    * Encoder: One Hot Encoder\n* Binary features:\n    * Imputer: Most frequent\n    * Encoder: Ordinal Encoder\n\nThis pipeline will be used later, at the time of the experiments, ok?","metadata":{}},{"cell_type":"code","source":"# numerical features\nnumeric_features = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\nnumeric_transformer = Pipeline(\n    steps=[(\"imputer\", KNNImputer(n_neighbors=5)), \n           (\"scaler\", StandardScaler())]\n)\n\nPipeline(\n    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), \n           (\"scaler\", StandardScaler())])\n\n# categorial features\ncategorical_features = [\"HomePlanet\", \"Destination\"]\ncategorical_transformer = Pipeline(\n    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n           (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))])\n    \n# binary features\nbinary_features = [\"CryoSleep\", \"VIP\"]\nbinary_transformer = Pipeline(\n    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n           (\"ohe\", OrdinalEncoder())])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_features),\n        (\"cat\", categorical_transformer, categorical_features),\n        (\"bin\", binary_transformer, binary_features),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T00:49:10.635333Z","iopub.execute_input":"2022-06-06T00:49:10.635954Z","iopub.status.idle":"2022-06-06T00:49:10.64741Z","shell.execute_reply.started":"2022-06-06T00:49:10.63591Z","shell.execute_reply":"2022-06-06T00:49:10.646617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's separate the predictor variables ($X$) and the outcome ($y$)...","metadata":{}},{"cell_type":"code","source":"X = df.drop(columns=['Transported'])\ny = df['Transported']\n\n# As the outcome is with boolean type, I must change it to 0 and 1\ny = LabelEncoder().fit_transform(y)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T00:49:10.64885Z","iopub.execute_input":"2022-06-06T00:49:10.649467Z","iopub.status.idle":"2022-06-06T00:49:10.667255Z","shell.execute_reply.started":"2022-06-06T00:49:10.649424Z","shell.execute_reply":"2022-06-06T00:49:10.666092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's separate training and test sets, being 70% and 30% respectively. These sets will be used by the following experiments...","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED, stratify=y)\nprint(f'X_train shape {X_train.shape}')\nprint(f'y_train shape {y_train.shape}')\nprint('-'*20)\nprint(f'X_test shape {X_test.shape}')\nprint(f'y_test shape {y_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-06-06T00:49:10.670533Z","iopub.execute_input":"2022-06-06T00:49:10.6729Z","iopub.status.idle":"2022-06-06T00:49:10.692329Z","shell.execute_reply.started":"2022-06-06T00:49:10.672863Z","shell.execute_reply":"2022-06-06T00:49:10.691316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Weak Model - Decision Tree\n\n\nNow we are going to train several decision trees (from sklearn itself) with different depths (1, 2, 3, ..., 18, 19, 20) so that we can observe **how the accuracy** is affected by this parameter (all others decision tree parameters will be kept at their default setting).\n\n## Please upvote me if you like, ok? (this is really  really important to me)","metadata":{}},{"cell_type":"code","source":"acc_dt = {\n  'max_depth':[],\n  'acc':[],\n  'train_test':[],\n}\n\nmax_depth = np.arange(2, 21, 1)\nfor md in tqdm(max_depth):\n    pipe_dt = Pipeline(\n        [('preprocessor', preprocessor), \n         ('estimator', DecisionTreeClassifier(random_state=SEED, max_depth=md))])    \n    \n    pipe_dt.fit(X_train, y_train)\n    # first predict on X_train set\n    y_pred = pipe_dt.predict(X_train)\n    # store the result\n    acc_dt['max_depth'].append(md)\n    acc_dt['acc'].append(accuracy_score(y_train, y_pred))\n    acc_dt['train_test'].append('train_dt')\n    \n\n    # now predict on X_test set\n    y_pred = pipe_dt.predict(X_test)\n    # store the result\n    acc_dt['max_depth'].append(md)\n    acc_dt['acc'].append(accuracy_score(y_test, y_pred))\n    acc_dt['train_test'].append('test_dt')\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T00:49:10.693903Z","iopub.execute_input":"2022-06-06T00:49:10.695073Z","iopub.status.idle":"2022-06-06T00:49:29.643832Z","shell.execute_reply.started":"2022-06-06T00:49:10.695026Z","shell.execute_reply":"2022-06-06T00:49:29.642579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_dt = pd.DataFrame(acc_dt)\nfig = plt.figure(figsize=(12, 8))\nsns.lineplot(\n    data=result_dt,\n    x=\"max_depth\", \n    y=\"acc\", \n    hue=\"train_test\"\n)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T00:49:29.646839Z","iopub.execute_input":"2022-06-06T00:49:29.647213Z","iopub.status.idle":"2022-06-06T00:49:29.992547Z","shell.execute_reply.started":"2022-06-06T00:49:29.64718Z","shell.execute_reply":"2022-06-06T00:49:29.991617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_dt[result_dt['train_test'] == 'test_dt'][['acc']].describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T00:49:29.99419Z","iopub.execute_input":"2022-06-06T00:49:29.994673Z","iopub.status.idle":"2022-06-06T00:49:30.013793Z","shell.execute_reply.started":"2022-06-06T00:49:29.994628Z","shell.execute_reply":"2022-06-06T00:49:30.012615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**What can we see in the graph above?**\n\nNote in the graph that the training and testing accuracy curves grow \"together\" until at a certain depth (around 5 or 6) they start to take off... and, from there, they get more and more separated: for training continues to grow; and for testing it dropped and then remains stagnant. This phenomenon is known as **overfitting** and is characterized by the loss of generalization of the predictive model. In other words: the model learns so much about the training set that it becomes unable to do well on the test set.\n\nAnother phenomenon that we can observe here is the ceiling effect (which I mentioned at the beginning of the text). The \"increase in the complexity\" of the tree does not necessarily translate into a performance increase in the test suite, so it's better that we have simpler trees... you know? **simpler trees**....\n\nGood, now what? what do we do?\n\nHow about we build our own random forest and see what happens...\n\n## Please upvote me if you like, ok? (this is really  really important to me)","metadata":{}},{"cell_type":"markdown","source":"# 4. Ensemble Model: My Own Random Forest\n\n**In essence: what is a random forest?**\n\nin a very very very simplified way a random forest can be thought of as a set of simple trees, that is, shallower trees that know only part of the data.\n\nSpeaking of shallower trees, how about listening to a beautiful song?  \n\n**Lady Gaga, Bradley Cooper - Shallow (from A Star Is Born) (Official Music Video)**\n\n\n[![Lady Gaga, Bradley Cooper - Shallow (from A Star Is Born) (Official Music Video)](http://img.youtube.com/vi/bo_efYhYU2A/0.jpg)](https://www.youtube.com/watch?v=bo_efYhYU2A)\n\n\n**Curiosities**: here in Brazil we have this version of the song...\n\n**Paula Fernandes - Juntos (Ao Vivo Em Sete Lagoas, Brazil / 2019 / Origens)**\n\n\n[![Paula Fernandes - Juntos (Ao Vivo Em Sete Lagoas, Brazil / 2019 / Origens)](http://img.youtube.com/vi/xd8xxl222Ls/0.jpg)](https://www.youtube.com/watch?v=xd8xxl222Ls)\n\n\n\nok... let's move on...\n\n\nAnd how do we make each tree know **only part** of the data?\n\n\nIn the same way that we can build a pipeline to transform the data, we can build a pipeline that, after transforming the columns, **randomly selects** some of them to present to the trees.\n\n\nThen we will build a committee (**VotingClassifier**) so that each tree can vote at the time of prediction what the outcome is. **The majority vote wins.**","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Now it's hands-on!!!\n\nThe code below is a simplification of what is actually a Random Forest, but the main ideas are contained here: majority vote trees know only part of the dataset.\n\nEach tree will only know 70% of the columns (I chose 70% arbitrarily, it could be any amount). But how many columns do we have after transforming the data? Let's check!!!","metadata":{}},{"cell_type":"code","source":"preprocessor.fit_transform(X_train).shape","metadata":{"execution":{"iopub.status.busy":"2022-06-06T00:49:30.015197Z","iopub.execute_input":"2022-06-06T00:49:30.015546Z","iopub.status.idle":"2022-06-06T00:49:30.42673Z","shell.execute_reply.started":"2022-06-06T00:49:30.015514Z","shell.execute_reply":"2022-06-06T00:49:30.42551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are $14$ columns (since some of them undergo the One Hot Encoding transformation). There are 14 columns (since some of them undergo the One Hot Encoding transformation). Now let's calculate 70% of 14 columns is approximately 9.79... let's round up to 10. That is, of the 14 possible columns, each tree will only know 10.\n\nDon't think this is not enough... with this configuration we would be able to create a forest of up to 1001 different combinations (just think that it is a combinatorial analysis of 14, grouped 10 in 10, without repetition in which the order does not matter).\n\nLet's use the way sklearn works (with the fit and predict methods). As our forest will be very simple, we will only have 2 parameters: number of estimators and maximum depth of each tree. All other parameters will be kept at their default settings. ","metadata":{}},{"cell_type":"code","source":"class MyOwnRandomForest(BaseEstimator):\n    # Class responsible for simulating a RandomForest.\n\n    def __init__(self, n_estimators, max_depth):\n        \"\"\"\n        Here we will define the pipeline for each tree.\n        \n        :params:\n        n_estimators: The number of trees in our own forest.\n        \n        max_depth: The maximum depth of each tree. \n        \"\"\"\n               \n        self.estimators_list = []\n        # for each estimator we will create a pipeline that selects the columns and then do its work\n        for i in range(n_estimators):\n            # the numpy package itself has a function that randomly selects numbers from an array\n            # here of the 14, let's randomly select 10, (without repetition)\n            columns = tuple(np.random.choice(14, 10, replace=False))\n            \n            # building the pipeline\n            pipe_dt = make_pipeline(ColumnSelector(cols=columns),\n                                     DecisionTreeClassifier(random_state=SEED, max_depth=max_depth))\n            self.estimators_list.append((f'pipe_dt_{i}', pipe_dt))\n            \n        # Here we use the \"voting classifier\" itself    \n        self.voting = VotingClassifier(estimators=self.estimators_list)\n        \n    \n    def fit(self, X, y):\n        # fits all\n        self.voting.fit(X, y)\n\n    def predict(self, X):\n        # predicts all\n        return self.voting.predict(X)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-06T00:49:30.429017Z","iopub.execute_input":"2022-06-06T00:49:30.429731Z","iopub.status.idle":"2022-06-06T00:49:30.440895Z","shell.execute_reply.started":"2022-06-06T00:49:30.429659Z","shell.execute_reply":"2022-06-06T00:49:30.439617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Does it work? Let's try!!!**\n\nIn our first test we are going to create a forest with only 10 trees, and with the same settings as in the previous experiment. Let's see what happens...","metadata":{}},{"cell_type":"code","source":"acc_rn = {\n  'max_depth':[],\n  'acc':[],\n  'train_test':[],\n}\n\nmax_depth = np.arange(2, 21, 1)\nfor md in tqdm(max_depth):\n    pipe_dt = Pipeline(\n        [('preprocessor', preprocessor), \n         ('estimator', MyOwnRandomForest(max_depth=md, n_estimators=10))])    \n    \n    pipe_dt.fit(X_train, y_train)\n    # first predict on X_train set\n    y_pred = pipe_dt.predict(X_train)\n    # store the result\n    acc_rn['max_depth'].append(md)\n    acc_rn['acc'].append(accuracy_score(y_train, y_pred))\n    acc_rn['train_test'].append('train_rn')\n    \n\n    # now predict on X_test set\n    y_pred = pipe_dt.predict(X_test)\n    # store the result\n    acc_rn['max_depth'].append(md)\n    acc_rn['acc'].append(accuracy_score(y_test, y_pred))\n    acc_rn['train_test'].append('test_rn')","metadata":{"execution":{"iopub.status.busy":"2022-06-06T00:49:30.442643Z","iopub.execute_input":"2022-06-06T00:49:30.443124Z","iopub.status.idle":"2022-06-06T00:49:52.624451Z","shell.execute_reply.started":"2022-06-06T00:49:30.443077Z","shell.execute_reply":"2022-06-06T00:49:52.623759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_rn = pd.DataFrame(acc_rn)\nfig = plt.figure(figsize=(12, 8))\nsns.lineplot(\n    data=result_rn,\n    x=\"max_depth\", \n    y=\"acc\", \n    hue=\"train_test\", \n    dashes=True\n)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T00:49:52.625523Z","iopub.execute_input":"2022-06-06T00:49:52.626449Z","iopub.status.idle":"2022-06-06T00:49:52.909453Z","shell.execute_reply.started":"2022-06-06T00:49:52.626416Z","shell.execute_reply":"2022-06-06T00:49:52.908416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_rn[result_rn['train_test'] == 'test_rn'][['acc']].describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T00:49:52.910724Z","iopub.execute_input":"2022-06-06T00:49:52.911092Z","iopub.status.idle":"2022-06-06T00:49:52.928394Z","shell.execute_reply.started":"2022-06-06T00:49:52.911058Z","shell.execute_reply":"2022-06-06T00:49:52.927264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**What do we perceive here?**\n\nApparently our Random Forest took a little longer than the decision tree to go into overfitting (and has a more irregular behavior too). Is it possible to see this graphically?","metadata":{}},{"cell_type":"code","source":"result = pd.concat([result_dt, result_rn], axis=0).reset_index().drop(columns=['index'])\nfig = plt.figure(figsize=(12, 8))\nsns.lineplot(\n    data=result[result['train_test'].isin(['test_dt', 'test_rn'])],\n    x=\"max_depth\", \n    y=\"acc\",\n    hue='train_test',\n    dashes=True\n)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T00:49:52.929614Z","iopub.execute_input":"2022-06-06T00:49:52.929991Z","iopub.status.idle":"2022-06-06T00:49:53.235657Z","shell.execute_reply.started":"2022-06-06T00:49:52.929959Z","shell.execute_reply":"2022-06-06T00:49:53.234622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**What if** we simultaneously change the number of estimators and the number of estimators? What will be the result of this?","metadata":{}},{"cell_type":"code","source":"max_depth = np.arange(2, 21, 1)\nn_estimators = np.arange(2, 41, 2)\n\nresults = {\n    'n_estimators':[],\n    'max_depth':[],\n    'acc':[]\n}\n\nfor est in tqdm(n_estimators):\n    for md in max_depth:\n        pipe_dt = Pipeline(\n            [('preprocessor', preprocessor), \n             ('estimator', MyOwnRandomForest(max_depth=md, n_estimators=est))])    \n\n        pipe_dt.fit(X_train, y_train)\n        # now predict on X_test set\n        y_pred = pipe_dt.predict(X_test)\n        # store the result\n        results['n_estimators'].append(est)\n        results['max_depth'].append(md)\n        results['acc'].append(accuracy_score(y_test, y_pred))\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T00:49:53.237152Z","iopub.execute_input":"2022-06-06T00:49:53.237842Z","iopub.status.idle":"2022-06-06T00:55:30.510644Z","shell.execute_reply.started":"2022-06-06T00:49:53.237796Z","shell.execute_reply":"2022-06-06T00:55:30.509582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = pd.DataFrame(results)\nresults_pivot = results.pivot(\"n_estimators\", \"max_depth\", \"acc\")\nfig = plt.figure(figsize=(20, 8))\nsns.heatmap(results_pivot, annot=True, fmt=\".3f\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T00:55:30.511749Z","iopub.execute_input":"2022-06-06T00:55:30.512038Z","iopub.status.idle":"2022-06-06T00:55:32.428534Z","shell.execute_reply.started":"2022-06-06T00:55:30.51201Z","shell.execute_reply":"2022-06-06T00:55:32.427465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Which is the best result?","metadata":{}},{"cell_type":"code","source":"results.sort_values(['acc'], ascending=False).head(1)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T00:55:32.429621Z","iopub.execute_input":"2022-06-06T00:55:32.430105Z","iopub.status.idle":"2022-06-06T00:55:32.442554Z","shell.execute_reply.started":"2022-06-06T00:55:32.430072Z","shell.execute_reply":"2022-06-06T00:55:32.441623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Which is the worst result?","metadata":{}},{"cell_type":"code","source":"results.sort_values(['acc'], ascending=False).tail(1)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T00:55:32.443679Z","iopub.execute_input":"2022-06-06T00:55:32.444326Z","iopub.status.idle":"2022-06-06T00:55:32.460161Z","shell.execute_reply.started":"2022-06-06T00:55:32.444292Z","shell.execute_reply":"2022-06-06T00:55:32.459207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Some conclusions\n\n1. A simple way to build a Random Forest solution was presented...\n1. Obviously the purpose here is not to build to be competitive or put into production, my purpose is always to present complex ideas in a simple way...\n1. Empirically, it was possible to observe that an ensemble model like this one can overcome the obstacle of overfitting...\n1. There are certainly a lot of improvements to be made in the code above like parallelism and other things... if you have any ideas, leave them in the comments, ok?\n1. If you want to know the origin of the origin of Random Forest, read the original article at https://link.springer.com/article/10.1023/A:1010933404324 or https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf\n1. And finally...\n\n![](https://memecreator.org/static/images/memes/5153568.jpg)\n\n\n\nThank you very much for your attention, and feel free to make suggestions!!! Bye!!!\n\n![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQeTlBWPzFucVr0vMMgbWtuF1iX_Ja16zMiuUzpx41OHktuj_PeeGQht8qiof2LWZZfv4g&usqp=CAU)","metadata":{}}]}