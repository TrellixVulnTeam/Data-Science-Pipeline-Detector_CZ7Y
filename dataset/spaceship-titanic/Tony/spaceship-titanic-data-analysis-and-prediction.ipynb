{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing the libraries and dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport copy\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:32.648842Z","iopub.execute_input":"2022-06-20T07:55:32.649629Z","iopub.status.idle":"2022-06-20T07:55:34.183688Z","shell.execute_reply.started":"2022-06-20T07:55:32.64949Z","shell.execute_reply":"2022-06-20T07:55:34.182712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_train_df = pd.read_csv('../input/spaceship-titanic/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:34.185148Z","iopub.execute_input":"2022-06-20T07:55:34.185536Z","iopub.status.idle":"2022-06-20T07:55:34.245886Z","shell.execute_reply.started":"2022-06-20T07:55:34.185504Z","shell.execute_reply":"2022-06-20T07:55:34.245176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Analysis","metadata":{}},{"cell_type":"code","source":"# dimensions of the original training dataset\ntitanic_train_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:34.24725Z","iopub.execute_input":"2022-06-20T07:55:34.247795Z","iopub.status.idle":"2022-06-20T07:55:34.25488Z","shell.execute_reply.started":"2022-06-20T07:55:34.247763Z","shell.execute_reply":"2022-06-20T07:55:34.254236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset has 8693 records and 14 attributes.","metadata":{}},{"cell_type":"code","source":"# training data column information\ntitanic_train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:34.256792Z","iopub.execute_input":"2022-06-20T07:55:34.257129Z","iopub.status.idle":"2022-06-20T07:55:34.296636Z","shell.execute_reply.started":"2022-06-20T07:55:34.257099Z","shell.execute_reply":"2022-06-20T07:55:34.295652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# examining the first few rows of the training data\ntitanic_train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:34.298115Z","iopub.execute_input":"2022-06-20T07:55:34.298815Z","iopub.status.idle":"2022-06-20T07:55:34.330422Z","shell.execute_reply.started":"2022-06-20T07:55:34.298771Z","shell.execute_reply":"2022-06-20T07:55:34.329722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing the null values in each columns","metadata":{}},{"cell_type":"code","source":"plt.bar(titanic_train_df.columns, titanic_train_df.isna().sum())\nplt.xticks(rotation = 90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:34.331528Z","iopub.execute_input":"2022-06-20T07:55:34.332171Z","iopub.status.idle":"2022-06-20T07:55:34.601971Z","shell.execute_reply.started":"2022-06-20T07:55:34.332135Z","shell.execute_reply":"2022-06-20T07:55:34.601277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On observing the above bar chart it can be concluded that except PassengerId and Transported columns, the rest of the columns in the dataset have missing values.\nSince the number of missing values in most of these columns are nearly around 200, they are further examined for the presence of any pattern among them.","metadata":{}},{"cell_type":"markdown","source":"### Using a seaborn library heatmap to visually identify any patterns in missing values","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 10))\nsns.heatmap(titanic_train_df.loc[:, ~titanic_train_df.columns.isin(['PassengerId', 'Transported'])].isna(), cmap = 'YlGnBu')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:34.603073Z","iopub.execute_input":"2022-06-20T07:55:34.605476Z","iopub.status.idle":"2022-06-20T07:55:35.647663Z","shell.execute_reply.started":"2022-06-20T07:55:34.605439Z","shell.execute_reply":"2022-06-20T07:55:35.646533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On analysing the heatmap of missing values, there are no signs of patterns among the records. To further support the claim, a heatmap from missingno package is used. This package contains various charts and dendograms that can be used to analyse the missing data in a dataset.","metadata":{}},{"cell_type":"markdown","source":"### Analysing missing data using heatmap from missingno package","metadata":{}},{"cell_type":"code","source":"msno.heatmap(titanic_train_df, figsize = (15, 10))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:35.648885Z","iopub.execute_input":"2022-06-20T07:55:35.649263Z","iopub.status.idle":"2022-06-20T07:55:36.139341Z","shell.execute_reply.started":"2022-06-20T07:55:35.649225Z","shell.execute_reply":"2022-06-20T07:55:36.138379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is evident from the above heatmap that there is no pattern among the missing values in the dataset. Therefore, further analysis needs to be performed to impute with proper values in place of the missing information in the dataset.","metadata":{}},{"cell_type":"markdown","source":"### Distribution of numerical values in the dataset","metadata":{}},{"cell_type":"code","source":"numerical_cols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n\nplt.figure(figsize = (15, 15))\nplt.subplots_adjust(hspace = 0.25)\nfor i in range(0, len(numerical_cols)):\n    plt.subplot(3, 2, i + 1)\n    plt.title(numerical_cols[i])\n    plt.hist(titanic_train_df.loc[:, numerical_cols[i]])","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:36.140497Z","iopub.execute_input":"2022-06-20T07:55:36.140875Z","iopub.status.idle":"2022-06-20T07:55:36.984175Z","shell.execute_reply.started":"2022-06-20T07:55:36.140842Z","shell.execute_reply":"2022-06-20T07:55:36.983225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On examining the numerical records in the dataset, it can be noticed that the columns - RoomService, FoodCourt, ShoppingMall, Spa and VRDeck are more skewed than the age column. Mean is an efficient method for imputation if the data follows normal distribution. On the other hand, median can be used for skewed data.","metadata":{}},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"markdown","source":"During the analysis stage, it was observed that the PassengerId column had both the group number and the passenger number within the group. It can be used to extract the number of applicants within each group. This feature could aid as the ratio of number of applicants to accepted applicants might play a role in overall result.\n\nSimilarly, the Cabin column has 3 informations namely Deck, Deck Number and Deck Side. They are extracted into individual columns.\n\nThe Last Name is extracted from the Name column because of two reasons. They are,\n1. Number of family members in the same group influencing decision\n2. Previlege to members from certain family","metadata":{}},{"cell_type":"code","source":"def cabin_transform(cabin, index):\n    if cabin is np.nan:\n        return cabin\n    else:\n        return str(cabin).split('/')[index]\n    \ndef modify_features(orig_df):\n    df = copy.deepcopy(orig_df)\n    \n    df.insert(0, 'PassengerGroup', df['PassengerId'].transform(lambda passengerId: int(passengerId.split('_')[0])))\n    df.insert(1, 'GroupCount', df.groupby('PassengerGroup')['PassengerId'].transform('count'))\n\n    df['Deck'] = df['Cabin'].transform(lambda cabin: cabin_transform(cabin, 0))\n    df['DeckNumber'] = df['Cabin'].transform(lambda cabin: cabin_transform(cabin, 1))\n    df['DeckSide'] = df['Cabin'].transform(lambda cabin: cabin_transform(cabin, 2))\n    df['FamilyName'] = df['Name'].transform(lambda name: name if (name is np.nan) else str(name).split(' ')[-1])\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:36.986986Z","iopub.execute_input":"2022-06-20T07:55:36.987415Z","iopub.status.idle":"2022-06-20T07:55:36.997635Z","shell.execute_reply.started":"2022-06-20T07:55:36.987383Z","shell.execute_reply":"2022-06-20T07:55:36.996662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The numerical columns such as Age, RoomService are converted to categories by binning the values. This will also help to mitigate the effect of data imputation in these columns.","metadata":{}},{"cell_type":"code","source":"def bin_numerical_values(orig_df):\n    df = copy.deepcopy(orig_df)\n    \n    ageLabels = ['children', 'youth', 'adult', 'senior']\n    amountLabels = ['< 1000', '< 2000', '> 2000']\n    \n    df['Age'] = pd.cut(df['Age'], bins = [0, 15, 24, 64, np.inf], labels = ageLabels, include_lowest=True)\n    \n    for col in ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']:\n        df[col] = pd.cut(df[col], bins = [0, 1000, 2000, np.inf], labels = amountLabels, include_lowest = True)\n    \n    return df\n ","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:36.999126Z","iopub.execute_input":"2022-06-20T07:55:36.999603Z","iopub.status.idle":"2022-06-20T07:55:37.009579Z","shell.execute_reply.started":"2022-06-20T07:55:36.999561Z","shell.execute_reply":"2022-06-20T07:55:37.008742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The missing values are now imputed using the above mentioned strategy for numerical values. On the other hand, for all the other columns the missing values are marked as 'Unknown' and grouped into a separate category.\n\nAs a part of preprocessing the dataset, all the categorical values are encoded into numerical values using OrdinalEncoder while the output column is encoded using LabelEncoder.\n\nThe encoders and the imputer functions are stored in separate dictonaries so that they could be used to transform the test data with the information fitted against the training data.","metadata":{}},{"cell_type":"code","source":"def encode_output(y):\n    encoder = LabelEncoder()\n    return encoder.fit_transform(y)\n\ndef preprocess_training_data(orig_df):\n    df = copy.deepcopy(orig_df)\n    \n    df = modify_features(df)\n    y = encode_output(df['Transported'])\n    df = df.drop(['PassengerGroup', 'PassengerId', 'Cabin', 'Name', 'Transported'], axis = 1)\n    \n    cols = df.columns\n    \n    medianImputer = SimpleImputer(strategy = 'median')\n    constantImputer = SimpleImputer(strategy = 'constant', fill_value = 'unknown')\n    meanImputer = SimpleImputer(strategy = 'mean')\n    \n    df[['Age']] = meanImputer.fit_transform(df[['Age']])\n    df[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']] = medianImputer.fit_transform(df[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']])\n    df = constantImputer.fit_transform(df)\n    \n    imputers = {\n        'constant' : constantImputer, \n        'median' : medianImputer,\n        'mean' : meanImputer\n    }\n    \n    df = pd.DataFrame(df, columns = cols)\n    df = df.convert_dtypes()\n    \n    df = bin_numerical_values(df)\n        \n    encoder_models = {}\n    \n    for col in df.columns:\n        encoder = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = -1)\n        if df[col].dtypes != 'object':\n            df[[col]] = encoder.fit_transform(df[[col]].astype('category'))\n        else:\n            df[[col]] = encoder.fit_transform(df[[col]].astype('string'))\n\n        encoder_models[col] = encoder\n        \n    return df, y, imputers, encoder_models","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:37.010588Z","iopub.execute_input":"2022-06-20T07:55:37.010909Z","iopub.status.idle":"2022-06-20T07:55:37.162579Z","shell.execute_reply.started":"2022-06-20T07:55:37.010882Z","shell.execute_reply":"2022-06-20T07:55:37.161443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, y, imputers, encoders = preprocess_training_data(titanic_train_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:37.163781Z","iopub.execute_input":"2022-06-20T07:55:37.164178Z","iopub.status.idle":"2022-06-20T07:55:37.431955Z","shell.execute_reply.started":"2022-06-20T07:55:37.164147Z","shell.execute_reply":"2022-06-20T07:55:37.430994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:37.433126Z","iopub.execute_input":"2022-06-20T07:55:37.433503Z","iopub.status.idle":"2022-06-20T07:55:37.45811Z","shell.execute_reply.started":"2022-06-20T07:55:37.433472Z","shell.execute_reply":"2022-06-20T07:55:37.457406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:37.459337Z","iopub.execute_input":"2022-06-20T07:55:37.459808Z","iopub.status.idle":"2022-06-20T07:55:37.473173Z","shell.execute_reply.started":"2022-06-20T07:55:37.459765Z","shell.execute_reply":"2022-06-20T07:55:37.472559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The imputers and encoders obtained while preprocessing the training data are used to transform the test data.","metadata":{}},{"cell_type":"code","source":"def preprocess_test_data(orig_df, imputers, encoders):\n    \n    df = copy.deepcopy(orig_df)\n    \n    df = modify_features(df)\n    df = df.drop(['PassengerGroup', 'PassengerId', 'Cabin', 'Name'], axis = 1)\n    \n    cols = df.columns\n    \n    df[['Age']] = imputers['mean'].transform(df[['Age']])\n    df[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']] = imputers['median'].transform(df[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']])\n    df = imputers['constant'].transform(df)\n    \n    df = pd.DataFrame(df, columns = cols)\n    df = df.convert_dtypes()\n    \n    df = bin_numerical_values(df)\n    \n    for col in encoders:\n        if df[col].dtypes != 'object':\n            df[[col]] = encoders[col].transform(df[[col]].astype('category'))\n        else:\n            df[[col]] = encoders[col].transform(df[[col]].astype('string'))\n        \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:37.476434Z","iopub.execute_input":"2022-06-20T07:55:37.477504Z","iopub.status.idle":"2022-06-20T07:55:37.487704Z","shell.execute_reply.started":"2022-06-20T07:55:37.47746Z","shell.execute_reply":"2022-06-20T07:55:37.486823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_test_df = pd.read_csv('../input/spaceship-titanic/test.csv')\ntest_df = preprocess_test_data(titanic_test_df, imputers, encoders)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:37.488747Z","iopub.execute_input":"2022-06-20T07:55:37.489155Z","iopub.status.idle":"2022-06-20T07:55:37.670724Z","shell.execute_reply.started":"2022-06-20T07:55:37.489122Z","shell.execute_reply":"2022-06-20T07:55:37.670031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:37.671929Z","iopub.execute_input":"2022-06-20T07:55:37.672995Z","iopub.status.idle":"2022-06-20T07:55:37.701551Z","shell.execute_reply.started":"2022-06-20T07:55:37.672947Z","shell.execute_reply":"2022-06-20T07:55:37.700443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:37.703093Z","iopub.execute_input":"2022-06-20T07:55:37.703631Z","iopub.status.idle":"2022-06-20T07:55:37.712855Z","shell.execute_reply.started":"2022-06-20T07:55:37.703587Z","shell.execute_reply":"2022-06-20T07:55:37.712092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"markdown","source":"Since there is no test output to evaluate the performance of the model, the training data is split into train and test (hold-out cross validation set) sets and accuracy is used as the metric to evaluate different models.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\ntrain_X, test_X, train_y, test_y = train_test_split(train_df, y, test_size = 0.10, shuffle = True, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:37.714092Z","iopub.execute_input":"2022-06-20T07:55:37.714779Z","iopub.status.idle":"2022-06-20T07:55:37.724687Z","shell.execute_reply.started":"2022-06-20T07:55:37.714744Z","shell.execute_reply":"2022-06-20T07:55:37.723691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('X train shape : ', train_X.shape)\nprint('y train shape : ', train_y.shape)\nprint('X test shape : ', test_X.shape)\nprint('y test shape : ', test_y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:37.726018Z","iopub.execute_input":"2022-06-20T07:55:37.726701Z","iopub.status.idle":"2022-06-20T07:55:37.734708Z","shell.execute_reply.started":"2022-06-20T07:55:37.726655Z","shell.execute_reply":"2022-06-20T07:55:37.733721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"Since the model was underfitting the max_iter parameter was increased to 2000 among other hyperparameters that were tuned but then removed as they did not increase the performance of the model.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nclf = LogisticRegression(max_iter = 2000)\nclf.fit(train_X, train_y)\ny_train_pred = clf.predict(train_X)\nprint('Training accuracy : ', accuracy_score(train_y, y_train_pred))\ny_pred = clf.predict(test_X)\nprint('Test accuracy : ', accuracy_score(test_y, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:37.735985Z","iopub.execute_input":"2022-06-20T07:55:37.736354Z","iopub.status.idle":"2022-06-20T07:55:38.19695Z","shell.execute_reply.started":"2022-06-20T07:55:37.736323Z","shell.execute_reply":"2022-06-20T07:55:38.195932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree Classifier\n\nOn fitting the Decision Tree Classifier with the default hyperparameters the training accuracy was close to 99% while the testing result was at 73%. This is clearly a problem of overfitting. Therefore, to reduce the variance, the max_depth value was adjusted till the model did not improve anymore.","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(max_depth = 14, random_state = 0)\nclf.fit(train_X, train_y)\ny_train_pred = clf.predict(train_X)\nprint('Training accuracy : ', accuracy_score(train_y, y_train_pred))\ny_pred = clf.predict(test_X)\nprint('Test accuracy : ', accuracy_score(test_y, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:38.19873Z","iopub.execute_input":"2022-06-20T07:55:38.199472Z","iopub.status.idle":"2022-06-20T07:55:38.30176Z","shell.execute_reply.started":"2022-06-20T07:55:38.199423Z","shell.execute_reply":"2022-06-20T07:55:38.300635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On comparing all the above two methods, the decision tree classifier is chosen for the final prediction with the test data.","metadata":{}},{"cell_type":"code","source":"test_pred = clf.predict(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:38.303419Z","iopub.execute_input":"2022-06-20T07:55:38.304168Z","iopub.status.idle":"2022-06-20T07:55:38.314225Z","shell.execute_reply.started":"2022-06-20T07:55:38.30412Z","shell.execute_reply":"2022-06-20T07:55:38.313213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'PassengerId' : titanic_test_df['PassengerId'],\n    'Transported' : test_pred.astype(bool)\n})\nsubmission.to_csv('./submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:55:38.31556Z","iopub.execute_input":"2022-06-20T07:55:38.315866Z","iopub.status.idle":"2022-06-20T07:55:38.334086Z","shell.execute_reply.started":"2022-06-20T07:55:38.315837Z","shell.execute_reply":"2022-06-20T07:55:38.333096Z"},"trusted":true},"execution_count":null,"outputs":[]}]}