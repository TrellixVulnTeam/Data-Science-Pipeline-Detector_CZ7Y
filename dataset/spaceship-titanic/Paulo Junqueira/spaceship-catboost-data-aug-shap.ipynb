{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install deep_tabular_augmentation","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-11T16:02:41.283522Z","iopub.execute_input":"2022-03-11T16:02:41.284031Z","iopub.status.idle":"2022-03-11T16:02:55.70904Z","shell.execute_reply.started":"2022-03-11T16:02:41.28393Z","shell.execute_reply":"2022-03-11T16:02:55.707786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Models\n#Logistic Regression\nfrom catboost import CatBoostClassifier\nfrom scipy.stats import t\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n#Dicision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n#Ensenble\nfrom sklearn.ensemble import VotingClassifier,RandomForestClassifier\nfrom  sklearn import preprocessing\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score,roc_auc_score,f1_score,precision_score,recall_score\nfrom sklearn.model_selection import KFold,StratifiedKFold\nimport copy \nfrom sklearn.preprocessing import PowerTransformer, QuantileTransformer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import uniform\nimport os\nimport shap \nsns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport torch\n\nimport warnings\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\n\n\n# from https://github.com/lschmiddey/deep_tabular_augmentation/tree/main/deep_tabular_augmentation\nimport deep_tabular_augmentation as dta \n\nfrom functools import partial\nfrom sklearn.preprocessing import StandardScaler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-11T16:02:55.712021Z","iopub.execute_input":"2022-03-11T16:02:55.71232Z","iopub.status.idle":"2022-03-11T16:03:00.847226Z","shell.execute_reply.started":"2022-03-11T16:02:55.71229Z","shell.execute_reply":"2022-03-11T16:03:00.846295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SpaceShip Classification Task\n\nThe main objective of this notebook is a classification task on the SpaceShip database. \nTo achieve this goal, the following steps are implemented:\n- Loading Data: Prepare the training and validation datasets\n- EDA: Generate insights using visualization tools to understand the variables\n- Feature Engineering: Apply Transformation on data\n- Data Augmentation: Apply a tabular augmentation with deep_tabular_augmentation model: (https://github.com/lschmiddey/deep_tabular_augmentation/tree/main/deep_tabular_augmentation)\n- Model: Test diferent models: Catboost, Decision Trees, Random Forests\n- Run the Model: Run the models in different combinations of variables with cross validation\n- Analysis of results: Check the results of all models and SHAP\n- Submission: Use the best trained model to predict the test set","metadata":{}},{"cell_type":"code","source":"# The objective for this class is to allow easy configuration of hyperparameters in this notebook\nclass Configuration():\n    \"\"\"Configuration Class for easy parametrization\"\"\"\n    #Random Seed\n    random_state = 666\n    #Validation Ratio\n    val_ratio = 0.2\n    \n    #Null values Inputer strategy\n    imputer = 'median'\n    \n    #Select a set of top correlatated variables (person) to test \n    top_vars = 15\n    \n    #Data Augmentation Parameters:\n    epochs = 1000\n    increase = 1000 # half 1 half 0\n    \n    # Cross validation K folds\n    folds = 10\n    \n    #Criteria to select best model for submission by\n    selection = 'accuracy_mean'\n    \n    \nCFG = Configuration()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:00.848611Z","iopub.execute_input":"2022-03-11T16:03:00.849066Z","iopub.status.idle":"2022-03-11T16:03:00.855309Z","shell.execute_reply.started":"2022-03-11T16:03:00.849033Z","shell.execute_reply":"2022-03-11T16:03:00.854262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data\n- This part is simple. Just to load the data set and take a first glance at the data and columns","metadata":{}},{"cell_type":"code","source":"# Loading the competions data\ntrain_df = pd.read_csv('../input/spaceship-titanic/train.csv')\ntest_df = pd.read_csv('../input/spaceship-titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:00.857566Z","iopub.execute_input":"2022-03-11T16:03:00.857834Z","iopub.status.idle":"2022-03-11T16:03:00.959768Z","shell.execute_reply.started":"2022-03-11T16:03:00.857796Z","shell.execute_reply":"2022-03-11T16:03:00.959044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First take a look in some samples of the dataset\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:00.961505Z","iopub.execute_input":"2022-03-11T16:03:00.96232Z","iopub.status.idle":"2022-03-11T16:03:00.996832Z","shell.execute_reply.started":"2022-03-11T16:03:00.962267Z","shell.execute_reply":"2022-03-11T16:03:00.995888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the categories and some sample of the values\nprint(f'Spaceshipe dataset:\\nTotal Samples: {train_df.shape[0]} - Total Variables: {train_df.shape[1]} \\n')\nfor column in train_df.columns:\n    print(f'Variable: {column} - Unique values {train_df[column].nunique()} - Sample:')\n    print(train_df[column].unique(), '\\n')","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:00.998293Z","iopub.execute_input":"2022-03-11T16:03:00.998719Z","iopub.status.idle":"2022-03-11T16:03:01.049076Z","shell.execute_reply.started":"2022-03-11T16:03:00.998685Z","shell.execute_reply":"2022-03-11T16:03:01.047809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA\n- In this section is time for a deeper look at the variables ","metadata":{}},{"cell_type":"code","source":"# Checking if the target is balanced\nnames = [ (train_df['Transported'].value_counts().index[i],train_df['Transported'].value_counts().values[i] ) for i in range(2)]\nsize_of_groups= train_df['Transported'].value_counts().values\n\nplt.figure(figsize = (9,5))\n# Create a pieplot\nplt.pie(size_of_groups, labels = names)\n\n# add a circle at the center to transform it in a donut chart\nmy_circle=plt.Circle( (0,0), 0.7, color='white')\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('Target proportion')","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:01.050595Z","iopub.execute_input":"2022-03-11T16:03:01.050964Z","iopub.status.idle":"2022-03-11T16:03:01.225641Z","shell.execute_reply.started":"2022-03-11T16:03:01.050919Z","shell.execute_reply":"2022-03-11T16:03:01.224659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking graphicaly the null values in dataset \nplt.figure(figsize = (12,8))\nsns.heatmap(train_df.isnull())\nplt.title('Null Values in the dataset')","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:01.227659Z","iopub.execute_input":"2022-03-11T16:03:01.22838Z","iopub.status.idle":"2022-03-11T16:03:02.232782Z","shell.execute_reply.started":"2022-03-11T16:03:01.228325Z","shell.execute_reply":"2022-03-11T16:03:02.231815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the percentage of null values for each variable\nplt.figure(figsize = (12,8))\n(train_df.isnull().mean().sort_values(ascending = False) *100).plot(kind = 'barh')\nplt.title('Percentage of Null Values in the dataset')","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:02.234269Z","iopub.execute_input":"2022-03-11T16:03:02.234497Z","iopub.status.idle":"2022-03-11T16:03:02.650631Z","shell.execute_reply.started":"2022-03-11T16:03:02.23447Z","shell.execute_reply":"2022-03-11T16:03:02.649492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we chose to drop all rows with any nulls in the dataset, the total size of the dataset will be\nchanged to: 6606","metadata":{}},{"cell_type":"code","source":"print(f'Removed percentage: {(1 - train_df.dropna().shape[0]/train_df.shape[0] ) * 100}%\\n{train_df.dropna().shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:02.65429Z","iopub.execute_input":"2022-03-11T16:03:02.654565Z","iopub.status.idle":"2022-03-11T16:03:02.688501Z","shell.execute_reply.started":"2022-03-11T16:03:02.654536Z","shell.execute_reply":"2022-03-11T16:03:02.687701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the variables and the target \ndef customSummary(data, label, var):\n    \n    #size of data\n    N = len(data)\n    \n    # Grouping the data and calculating the mean std and confidence inverval\n    data = data[[var,label]].groupby(var).agg([len, \\\n                                               np.mean, \\\n                                               np.std, \\\n                                               lambda x: (t.ppf(0.95/2.0 + .5, len(x)-1) \\\n                                                          * (np.std(x) / np.sqrt(len(x))))])\n    \n    \n    #Reseting index\n    data.reset_index(inplace=True)\n    data.columns = data.columns.droplevel()\n    \n    #Renaming Variables\n    data.rename(columns = {'<lambda_0>':'interval'}, inplace = True)\n    data.rename(columns = {'':var}, inplace = True)\n    \n    #Calculating the proportion\n    data['proportion'] = data['len']/N\n    \n    return data\n\n\ndef plot_(data, label, variable, size = (8,5)):\n    x = customSummary(data,label, variable)\n    palette = sns.color_palette('Blues', len(x[variable]))\n\n    fig, ax = plt.subplots(figsize = size)\n    ax.bar(x[variable], x['proportion'], color = list(palette))\n    ax.errorbar(x[variable], x['mean'], yerr=x['interval'], fmt='-o', color = 'red',capsize=6)\n    ax.set_ylabel(ylabel ='Percentage')\n    axes2 = ax.twinx()   \n    axes2.set_ylabel(ylabel ='Frequence')\n    axes2.set_ylim(ymin = 0, ymax = max(x['len']))\n    axes2.grid(visible=False)\n    fig.suptitle(variable)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:02.689776Z","iopub.execute_input":"2022-03-11T16:03:02.690055Z","iopub.status.idle":"2022-03-11T16:03:02.704809Z","shell.execute_reply.started":"2022-03-11T16:03:02.690023Z","shell.execute_reply":"2022-03-11T16:03:02.703705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for categorical values, lets check the number of classes and the the correspondent target values\nvar = ['HomePlanet','CryoSleep','Destination','VIP']\n\nfor i in var:\n    plot_(train_df, 'Transported', i)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:02.708354Z","iopub.execute_input":"2022-03-11T16:03:02.708664Z","iopub.status.idle":"2022-03-11T16:03:04.608589Z","shell.execute_reply.started":"2022-03-11T16:03:02.70863Z","shell.execute_reply":"2022-03-11T16:03:04.607418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The numeric variables have a possible difficult distribution. So lets try some different approaches to handle it\n- The gauss transformation seems to have a better result","metadata":{}},{"cell_type":"code","source":"# A custon binarization transformation. Where, anything higer than = 0 goes to 1:\ndef binarization(sample):\n    transformed_sample = 0\n    if sample > 0:\n        transformed_sample = 1\n    return transformed_sample\n\n# List of variables to check\nvar = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n\n#Creating different datasets to check the distribuiton of the transformed variables\ntrain_df_t0 = train_df.copy()\ntrain_df_t1 = train_df.copy()\ntrain_df_t2 = train_df.copy()\n\n# gaussing linear transformation\ntransform_gauss = PowerTransformer()\n# a Quantile linear transformation\ntransform_qt = QuantileTransformer()\n# Applying The transformations\ntrain_df_t1[var] = transform_gauss.fit_transform(train_df[var])\ntrain_df_t2[var] = transform_qt.fit_transform(train_df[var])\n\n#Ploting the comparison of the transformations\nfor v in var:\n    fig, ax = plt.subplots(ncols = 3, nrows =1, figsize= (16,5))\n    train_df_t0[v] = train_df_t0[v].apply(binarization)\n    \n    sns.histplot(x = v, data = train_df_t0, ax=ax[0], hue = 'Transported', kde = True)\n    sns.histplot(x = v, data = train_df_t1, ax=ax[1], hue = 'Transported', kde = True)\n    sns.histplot(x = v, data = train_df_t2, ax=ax[2], hue = 'Transported', kde = True)\n    ax[0].set_title('Binary')\n    ax[1].set_title('Gauss')\n    ax[2].set_title('Quantile')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:04.610505Z","iopub.execute_input":"2022-03-11T16:03:04.610854Z","iopub.status.idle":"2022-03-11T16:03:10.872045Z","shell.execute_reply.started":"2022-03-11T16:03:04.610806Z","shell.execute_reply":"2022-03-11T16:03:10.871268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Now, lets categorize the age variable to try to find some groups that could be better separeted by the target variable\n- It is possible to see that the youngest have a higher probability to survive","metadata":{}},{"cell_type":"code","source":"# Categorizing age\n\ntrain_df_t= train_df\ntrain_df_t['Age_d'] = pd.cut(train_df['Age'],bins =[0,10,18,25,40,60,100],\n                               labels=[ \"child\", \"teen\", \"youg adult\",\"adult\", \"mature\", \"elder\"])\n\n\nplt.figure(figsize = (12,8))\nsns.histplot(x = 'Age', data =train_df, hue = 'Transported')\nplot_(train_df_t, 'Transported', 'Age_d', (12,8))","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:10.873379Z","iopub.execute_input":"2022-03-11T16:03:10.873648Z","iopub.status.idle":"2022-03-11T16:03:11.820014Z","shell.execute_reply.started":"2022-03-11T16:03:10.873615Z","shell.execute_reply":"2022-03-11T16:03:11.81909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Now, it is time to check the correlation matrix between all variables \n- Higher values means positive correlation - if variable increase, Transported = 1\n- Lower values means negative correlation - if varible increase, Transported = 0\n- 0 means no correlation (Person)","metadata":{}},{"cell_type":"code","source":"def corr_mat(data, annot = True):\n# Compute the correlation matrix\n\n    corr = data.corr()\n\n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(15, 9))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap='coolwarm',vmin=-1, vmax=1, center=0,\n    square=True, linewidths=1, cbar_kws={\"shrink\": .7}, annot = annot)\n\ncorr_mat(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:11.821368Z","iopub.execute_input":"2022-03-11T16:03:11.821591Z","iopub.status.idle":"2022-03-11T16:03:12.447376Z","shell.execute_reply.started":"2022-03-11T16:03:11.821563Z","shell.execute_reply":"2022-03-11T16:03:12.446297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sns.pairplot(train_df[var])","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:12.448739Z","iopub.execute_input":"2022-03-11T16:03:12.449578Z","iopub.status.idle":"2022-03-11T16:03:12.454019Z","shell.execute_reply.started":"2022-03-11T16:03:12.449534Z","shell.execute_reply":"2022-03-11T16:03:12.452952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\n- In this section, we will create different set of variables applying the transformations  in the numeric set, as seen previously. Afterwards, this set is going to be teste separately in the model.\n- Also, categorical variables are going to betransformed into dummies (0 or 1) or econded with one hot enconding process (0,0,1,...)","metadata":{}},{"cell_type":"markdown","source":"# Inputing Variables","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/spaceship-titanic/train.csv')\n\ndef inputer(data, kind = 'median'):\n    print(f'inputing null values with {kind}')\n    for i in data.columns:\n        value = 0 \n        \n        if kind == 'media':\n            value = data[i].median()\n        elif kind == 'mean':\n            value = data[i].mean()\n\n        data[i] = data[i].fillna(value)\n        \n    return data\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:12.456726Z","iopub.execute_input":"2022-03-11T16:03:12.457262Z","iopub.status.idle":"2022-03-11T16:03:12.502482Z","shell.execute_reply.started":"2022-03-11T16:03:12.457216Z","shell.execute_reply":"2022-03-11T16:03:12.501833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#All feature engineering \ndef feat_eng(data, train =False):\n    #imputing null values \n    data = inputer(data, kind =CFG.imputer )\n    \n    sub = data[['PassengerId','Name']]\n    \n#     remove = ['']\n#     data = data.drop(remove, axis = 1)\n    \n   #Variables Transformations\n    tf = {'_b':[],'_g':[],'_q':[]}\n    var  = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n    for k,j in tf.items():\n        for v in var:\n            j.append((v+k))\n\n#   Applying some transformations in the var variables list          \n    transform_gauss = PowerTransformer()\n    transform_qt = QuantileTransformer()\n    \n    data[tf['_b']] = np.vectorize(binarization)(data[var])\n    data[tf['_g']] = transform_gauss.fit_transform(data[var])\n    data[tf['_q']] = transform_qt.fit_transform(data[var])\n    \n    tf['_ori'] = var\n    tf['_All'] = [ j for k, i in tf.items() for j in i]\n    \n    \n    \n    # Creating categorical age\n    data['Age_d'] = pd.cut(data['Age'],bins =[0,10,18,100],\n                               labels=[ \"child\", \"teen\",\"adult\"])\n    #transform into dummies categorical variables\n    data = pd.get_dummies(data,columns = ['HomePlanet','CryoSleep','Destination','VIP'],dummy_na=False, prefix_sep = '_D_', drop_first=True)\n    data = pd.get_dummies(data,columns = ['Age_d'],dummy_na=False)\n    \n    \n    #Spling cabin letter and applying label enconding\n    data['Cabin_enc'] = data['Cabin'].apply(lambda x: str(x).split('/')[0])\n    le = preprocessing.LabelEncoder()\n    data['Cabin_enc'] = le.fit_transform(data['Cabin_enc'])\n    \n    #Drops\n    data = data.drop((['Age','Cabin']), axis = 1 )\n    data = data.drop(['PassengerId','Name'], axis = 1)\n    if train:\n        data['Transported'] = (data['Transported']).apply(int)\n            \n    \n#   Return the data transformed (data), the passengerId (sub for submission) and the dicitionarie of vairables transformed ( to select in the model section)     \n    return data,sub, tf\n\ntrain_df_a,_, tf = feat_eng(train_df,train = True)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:12.503593Z","iopub.execute_input":"2022-03-11T16:03:12.504573Z","iopub.status.idle":"2022-03-11T16:03:12.675471Z","shell.execute_reply.started":"2022-03-11T16:03:12.504531Z","shell.execute_reply":"2022-03-11T16:03:12.674602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_a.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:12.676711Z","iopub.execute_input":"2022-03-11T16:03:12.677073Z","iopub.status.idle":"2022-03-11T16:03:12.70327Z","shell.execute_reply.started":"2022-03-11T16:03:12.677038Z","shell.execute_reply":"2022-03-11T16:03:12.702252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Total of variables:{ train_df_a.shape[1]}')","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:12.704712Z","iopub.execute_input":"2022-03-11T16:03:12.705569Z","iopub.status.idle":"2022-03-11T16:03:12.718506Z","shell.execute_reply.started":"2022-03-11T16:03:12.705492Z","shell.execute_reply":"2022-03-11T16:03:12.717466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the new variables correlations\ncorr_mat(train_df_a,False)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:12.7195Z","iopub.execute_input":"2022-03-11T16:03:12.719837Z","iopub.status.idle":"2022-03-11T16:03:13.899166Z","shell.execute_reply.started":"2022-03-11T16:03:12.719804Z","shell.execute_reply":"2022-03-11T16:03:13.898074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the correlation of all variables to the transported (target) (Zoom in)\ncorr = train_df_a.corr()['Transported']\ncorr = corr.reset_index()\nplt.figure(figsize = (12,10))\nsns.barplot(y = 'index', x = 'Transported',data = corr.sort_values('Transported',ascending = False)[1:], palette = 'magma')\nplt.title('Variables Correlation to the Transported Target')","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:13.903298Z","iopub.execute_input":"2022-03-11T16:03:13.904641Z","iopub.status.idle":"2022-03-11T16:03:14.452197Z","shell.execute_reply.started":"2022-03-11T16:03:13.904579Z","shell.execute_reply":"2022-03-11T16:03:14.451195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a top most correlataed variables set for training (abs)\ncorr['Transported'] = np.vectorize(abs)(corr['Transported'])\ntf['_topVars'] =  list(corr.sort_values('Transported', ascending = False)[1:(CFG.top_vars+1)]['index'].values)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:03:14.454003Z","iopub.execute_input":"2022-03-11T16:03:14.454899Z","iopub.status.idle":"2022-03-11T16:03:14.464019Z","shell.execute_reply.started":"2022-03-11T16:03:14.45482Z","shell.execute_reply":"2022-03-11T16:03:14.462634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation\n- Experimenting with a  tabular data augmentation from the project found in Git: https://github.com/lschmiddey/deep_tabular_augmentation/tree/main/deep_tabular_augmentation","metadata":{}},{"cell_type":"code","source":"X = train_df_a.drop('Transported', axis = 1)\ny = train_df_a['Transported']\n\ntrain_df_aug = train_df_a.copy()\nfor i in [0,1]:\n\n    X_train, X_val, y_train, y_val = train_test_split(X,y, \n                                                  test_size = CFG.val_ratio, \n                                                  random_state = CFG.random_state,\n                                                  stratify =y)\n    \n    # from https://github.com/lschmiddey/deep_tabular_augmentation/tree/main/deep_tabular_augmentation\n    target_class = i\n    x_scaler = StandardScaler()\n\n    X_train_scaled = x_scaler.fit_transform(X_train)\n\n    X_val_scaled = x_scaler.transform(X_val)\n\n    X_train = X_train_scaled[np.where(y_train==target_class)[0]]\n    X_val = X_val_scaled[np.where(y_val==target_class)[0]]\n\n    y_train = y_train.values[np.where(y_train==target_class)[0]]\n    y_val = y_val.values[np.where(y_val==target_class)[0]]\n\n    datasets = dta.create_datasets(X_train, y_train, X_val, y_val)\n    data = dta.DataBunch(*dta.create_loaders(datasets, bs=1024))\n\n    D_in = X_train.shape[1]\n    VAE_arch = [27,20,20]\n    target_name = 'Transported'\n\n    df_cols = list(X.columns)\n    device = 'cpu'\n    model = dta.Autoencoder(D_in, VAE_arch, latent_dim =3)\n    opt = torch.optim.Adam(model.parameters(), lr=0.01)\n    loss_func = dta.customLoss()\n\n    learn = dta.Learner(model, opt, loss_func, data, target_name, target_class, df_cols)\n\n    run = dta.Runner(cb_funcs=[dta.LR_Find, dta.Recorder])\n\n    run.fit(CFG.epochs, learn)\n\n    # run.recorder.plot(skip_last=5)\n\n    sched = dta.combine_scheds([0.3, 0.7], [dta.sched_cos(0.01, 0.1), dta.sched_cos(0.1, 0.01)])\n\n    cbfs = [partial(dta.LossTracker, show_every=50), dta.Recorder, partial(dta.ParamScheduler, 'lr', sched)]\n    model = dta.Autoencoder(D_in, VAE_arch, latent_dim=20).to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=0.01)\n    learn = dta.Learner(model, opt, loss_func, data, target_name, target_class, df_cols)\n    run = dta.Runner(cb_funcs=cbfs)\n    run.fit(CFG.epochs, learn)\n    \n    difference_in_class_occurences = int((CFG.increase/2))\n    df_fake = run.predict_df(learn, no_samples=difference_in_class_occurences, scaler=x_scaler)\n    std_list = list(train_df_a[train_df_a['Transported']==1][df_cols].std()/10)\n    df_fake_with_noise = run.predict_with_noise_df(learn, no_samples=difference_in_class_occurences, mu=0, sigma=std_list, scaler=x_scaler)\n    \n    x_aug = x_scaler.inverse_transform(df_fake_with_noise.drop(['Transported'],axis =1 ))\n    x_aug = pd.DataFrame(x_aug, columns = df_cols)\n    x_aug['Transported'] = df_fake_with_noise['Transported']\n    \n    train_df_aug = pd.concat([train_df_aug, x_aug])","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-11T16:03:14.466088Z","iopub.execute_input":"2022-03-11T16:03:14.466537Z","iopub.status.idle":"2022-03-11T16:06:13.256461Z","shell.execute_reply.started":"2022-03-11T16:03:14.466489Z","shell.execute_reply":"2022-03-11T16:06:13.255527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_aug = train_df_aug.reset_index().drop('index',axis =1)\ntrain_df_aug.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:06:13.257943Z","iopub.execute_input":"2022-03-11T16:06:13.25816Z","iopub.status.idle":"2022-03-11T16:06:13.272975Z","shell.execute_reply.started":"2022-03-11T16:06:13.258134Z","shell.execute_reply":"2022-03-11T16:06:13.271866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sns.pairplot(train_df_aug.reset_index().drop('index',axis =1)[['RoomService', 'FoodCourt']])","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:06:13.274305Z","iopub.execute_input":"2022-03-11T16:06:13.274991Z","iopub.status.idle":"2022-03-11T16:06:13.279231Z","shell.execute_reply.started":"2022-03-11T16:06:13.274956Z","shell.execute_reply":"2022-03-11T16:06:13.27842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\n- For model training, we are going to use a stratifyed K fold validaditon and a hyper parameter pre-tuning","metadata":{}},{"cell_type":"code","source":"\ndef training_cv(model,X, y, split = CFG.folds):\n\n    kf = StratifiedKFold(n_splits=split, shuffle = True, random_state = CFG.random_state)\n    acc_best = 0\n    acc_list = []\n    rec_list = []\n    auc_list = []\n    prec_list = []\n    best_model = 0\n    \n    for train_, val_df in kf.split(X,y):\n        X_train, X_val = X.iloc[train_], X.iloc[val_df]\n        y_train, y_val = y.iloc[train_], y.iloc[val_df]\n\n        model.fit(X_train,y_train)\n        pred = model.predict(X_val)\n        \n        \n        acc = accuracy_score(y_val,pred)\n        prec = precision_score(y_val,pred)\n        rec = recall_score(y_val,pred)\n        auc = roc_auc_score(y_val,pred)\n        \n        print(round(accuracy_score(y_train,model.predict(X_train)),3),round(acc,3) )\n        \n        acc_list.append(acc)\n        prec_list.append(prec)\n        rec_list.append(rec)\n        auc_list.append(auc)\n        \n        \n        if acc > acc_best:\n            best_model = copy.deepcopy(model)\n            acc_best = acc\n            \n    results = {'acc':acc_list,'precision':prec_list,'recall':rec_list,'auc':auc_list}\n    return best_model, results","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:06:13.280634Z","iopub.execute_input":"2022-03-11T16:06:13.280918Z","iopub.status.idle":"2022-03-11T16:06:13.295514Z","shell.execute_reply.started":"2022-03-11T16:06:13.280862Z","shell.execute_reply":"2022-03-11T16:06:13.294638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tuning models\ndef tuning_models(models, parameters, X, y):\n#     models = { i:k for i,k in models.items() if i != 'Ensemble'}\n    tuning_results = {}\n    for name, m in models.items():\n        print(f'>Tuning:{name}')\n        clf = RandomizedSearchCV(m, parameters[name], random_state=CFG.random_state)\n        search = clf.fit(X.values, y,)\n        tuning_results[name] = {'best_model':search.best_estimator_}\n        \n    return tuning_results","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:06:13.302336Z","iopub.execute_input":"2022-03-11T16:06:13.303019Z","iopub.status.idle":"2022-03-11T16:06:13.313677Z","shell.execute_reply.started":"2022-03-11T16:06:13.30298Z","shell.execute_reply":"2022-03-11T16:06:13.312977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shap model for model explainability\ndef SHAP_(model, X):\n    \n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X)\n    \n    return  explainer, shap_values","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:06:13.315028Z","iopub.execute_input":"2022-03-11T16:06:13.315439Z","iopub.status.idle":"2022-03-11T16:06:13.328254Z","shell.execute_reply.started":"2022-03-11T16:06:13.315399Z","shell.execute_reply":"2022-03-11T16:06:13.327429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Data\nX = train_df_aug.drop('Transported', axis = 1)\ny = train_df_aug['Transported']\n\n\n\n# # spliting train validation set\n# X_train, X_val, y_train, y_val = train_test_split(X,y, \n#                                                   test_size = CFG.val_ratio, \n#                                                   random_state = CFG.random_state,\n#                                                   stratify =y)\n\n\n# MODELS parameters for tunning\nparams = {'depth':[3,1,2,6,4,5,7,8,9,10,12,13],\n          'iterations':[100,250,500, 700,1000],\n          'learning_rate':[0.03,0.001,0.01,0.1,0.2,0.3,0.5], \n          'l2_leaf_reg':[3,1,5,10,100],\n          'border_count':[32,5,10,20,50,100,200]}\n\n\n#Parameters to tunning\nparameters = {'RandomForest':{'max_depth':[2,3,4,5,10,20,30,50],'min_samples_leaf' : np.linspace(0.1, 0.5, 5, endpoint=True),'n_estimators':[10,20,50,75,100,200,400]} ,\n          'DecisionTree': {'max_depth':[3,5,10,20],'min_samples_leaf' : np.linspace(0.1, 0.5, 5, endpoint=True)} ,\n          'SVC': {'kernel':['linear', 'rbf', 'poly']},\n          'LogisticRegression':{'C': uniform(loc=0, scale=4)},\n           'CatBoost':params } \n\ntf_vars = list(tf.values())\nbase_vars = [i for i in train_df_a.columns if i not in [j for i in tf.values() for j in i]]\nbase_vars.remove('Transported')\n\nresults    = {}\nbest_model = {}\nexplainer_l  = {}\nshap_values_l = {}\n#Testing different sets of variables\nfor i in tf.keys():\n# for i in ['_g']:\n    if i == '_topVars':\n        var  = tf[i]\n        X_ = X[var].copy()\n    else:\n        var = base_vars + tf[i]\n        X_ = X[var].copy()\n    \n#     models = {'CatBoost':CatBoostClassifier(verbose = False)}\n    \n    #       Other models to test  \n    models = {'RandomForest': RandomForestClassifier(random_state = CFG.random_state,oob_score = True),\n              'DecisionTree': DecisionTreeClassifier(random_state = CFG.random_state),\n               'CatBoost':CatBoostClassifier(verbose = False)}\n\n\n    \n    # Tuning\n    tuning_results = tuning_models(models, parameters, X_, y) \n    models = {model:d['best_model'] for model, d in tuning_results.items()}\n    \n    #Adding ensemble\n#     vclf = VotingClassifier(estimators = [(model,d['best_model']) for model, d in tuning_results.items() ],\n#                         voting = 'soft')\n\n#     models = dict(models, **{'Ensemble': vclf})\n\n    #Training\n    for name, model in models.items():\n        print(f'>Evaluating Model: {(name+i)}')\n        best_model[(name+i)], metrics = training_cv(model, X_, y)\n        results[(name+i)] = {'best_model':best_model, 'accuracy': metrics['acc'],'recall': metrics['recall'],'precision': metrics['precision'], 'auc': metrics['auc']}\n        explainer_l[(name+i)], shap_values_l[(name+i)] = SHAP_(best_model[(name+i)], X_)\n        \n#generate results table    \ndata_results = pd.DataFrame.from_dict(results, orient='index',).reset_index().rename(columns={'index':'models'}).explode(['accuracy','recall','precision','auc'])\ndata_table = pd.melt(data_results,id_vars = ['models'], value_vars = ['accuracy','recall','precision','auc'],var_name='metric')\nprint('End')\n","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-11T16:33:53.723344Z","iopub.execute_input":"2022-03-11T16:33:53.723647Z","iopub.status.idle":"2022-03-11T17:14:59.96311Z","shell.execute_reply.started":"2022-03-11T16:33:53.723617Z","shell.execute_reply":"2022-03-11T17:14:59.961967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results Analysis","metadata":{}},{"cell_type":"code","source":"#Generating table of results\ndf = data_results.drop('best_model', axis = 1).groupby('models').agg(['mean', 'median'])\ndf.columns = ['_'.join(col) for col in df.columns]","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:14:59.964862Z","iopub.execute_input":"2022-03-11T17:14:59.965131Z","iopub.status.idle":"2022-03-11T17:15:00.014906Z","shell.execute_reply.started":"2022-03-11T17:14:59.965101Z","shell.execute_reply":"2022-03-11T17:15:00.014154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Printing comparision of results\nmean_data = data_results.drop('best_model', axis = 1).groupby('models').mean()\n\nmean_data = data_results.drop('best_model', axis = 1).groupby('models').agg(['mean', 'median'])\nmean_data.columns = ['_'.join(col) for col in mean_data.columns]\n\nfor i in mean_data.columns:\n    metric = mean_data[i].max()\n    model = mean_data[mean_data[i] == metric ].index[0]\n    \n    print(f'best {i}: {model} \\ {metric}')","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:15:00.016037Z","iopub.execute_input":"2022-03-11T17:15:00.016823Z","iopub.status.idle":"2022-03-11T17:15:00.096602Z","shell.execute_reply.started":"2022-03-11T17:15:00.016785Z","shell.execute_reply":"2022-03-11T17:15:00.095277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ploting mean and median of results\nfig, ax = plt.subplots(figsize = (15,8))\nmean_data.plot(kind = 'bar', ax = ax)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:15:00.099283Z","iopub.execute_input":"2022-03-11T17:15:00.099768Z","iopub.status.idle":"2022-03-11T17:15:01.25906Z","shell.execute_reply.started":"2022-03-11T17:15:00.099714Z","shell.execute_reply":"2022-03-11T17:15:01.258093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Boxplot of metrics\nplt.subplots(figsize = (20,8))\nsns.boxplot(x = 'models',y = 'value', data = data_table, hue = 'metric')\nplt.ylabel('Metrics',fontdict ={'size':16})\nplt.xlabel('Models',fontdict ={'size':16})\nplt.title('Comparison',fontdict ={'size':16})\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:15:01.260524Z","iopub.execute_input":"2022-03-11T17:15:01.260793Z","iopub.status.idle":"2022-03-11T17:15:03.420148Z","shell.execute_reply.started":"2022-03-11T17:15:01.260761Z","shell.execute_reply":"2022-03-11T17:15:03.419137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysis - CatBoost","metadata":{}},{"cell_type":"code","source":"#Feature importance of the variables in all models\nfig, ax = plt.subplots(nrows = 6, figsize =(10,25))\nfor f, i in enumerate(tf.keys()):\n    \n    if i == '_topVars':\n        var  = tf[i]\n    else:\n        var = base_vars + tf[i]\n    \n    ax[f].set_title(('CatBoost'+i))\n    sns.barplot(x = best_model[('CatBoost'+i)].feature_importances_, y = X[var].columns, ax = ax[f])","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:15:03.421465Z","iopub.execute_input":"2022-03-11T17:15:03.421759Z","iopub.status.idle":"2022-03-11T17:15:05.70158Z","shell.execute_reply.started":"2022-03-11T17:15:03.421686Z","shell.execute_reply":"2022-03-11T17:15:05.700568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SHAP Values Analysis - CatBoost","metadata":{}},{"cell_type":"code","source":"# Shap values of variables in the model \nfor v, m in zip(tf.keys(), ['CatBoost_b','CatBoost_g','CatBoost_q','CatBoost_ori','CatBoost_All', 'CatBoost_topVars']):\n    if v == '_topVars':\n        var  = tf[v]\n    else:\n        var = base_vars + tf[v]\n    \n\n    X_ = X[var]\n    plt.title(m)\n    shap.summary_plot(shap_values_l[m], X_)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T17:17:47.293398Z","iopub.execute_input":"2022-03-11T17:17:47.294661Z","iopub.status.idle":"2022-03-11T17:18:00.885972Z","shell.execute_reply.started":"2022-03-11T17:17:47.294606Z","shell.execute_reply":"2022-03-11T17:18:00.884958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Get best model based on accuracy\nsub_model = mean_data[mean_data[CFG.selection] == mean_data[CFG.selection].max() ].index[0]","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:14:56.586097Z","iopub.status.idle":"2022-03-11T16:14:56.586512Z","shell.execute_reply.started":"2022-03-11T16:14:56.586307Z","shell.execute_reply":"2022-03-11T16:14:56.586334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(print(sub_model))","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:14:56.587987Z","iopub.status.idle":"2022-03-11T16:14:56.588521Z","shell.execute_reply.started":"2022-03-11T16:14:56.588222Z","shell.execute_reply":"2022-03-11T16:14:56.588249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/spaceship-titanic/test.csv')\n\ntest_df_a, id_df, tf = feat_eng(test_df)\n\ni = '_'+sub_model.split('_')[1]\n\nif i == '_topVars':\n    var  = tf[i]\nelse:\n    var = base_vars + tf[i]\n\ntest_df_a = test_df_a[var]\n\npred = best_model[sub_model].predict(test_df_a)\n\npred = (pred ==1) \nid_df['Transported'] = pred\nid_df[['PassengerId','Transported']].to_csv('./submission.csv', index = False)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-11T16:14:56.590126Z","iopub.status.idle":"2022-03-11T16:14:56.590482Z","shell.execute_reply.started":"2022-03-11T16:14:56.590295Z","shell.execute_reply":"2022-03-11T16:14:56.59032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id_df.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T16:14:56.591691Z","iopub.status.idle":"2022-03-11T16:14:56.592097Z","shell.execute_reply.started":"2022-03-11T16:14:56.5919Z","shell.execute_reply":"2022-03-11T16:14:56.591926Z"},"trusted":true},"execution_count":null,"outputs":[]}]}