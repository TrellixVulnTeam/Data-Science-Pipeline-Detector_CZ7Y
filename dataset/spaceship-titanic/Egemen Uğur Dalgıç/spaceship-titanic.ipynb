{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport shap\nfrom termcolor import colored\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport category_encoders as ce\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer, SimpleImputer\nfrom sklearn.compose import make_column_selector,make_column_transformer\nfrom scipy.stats import mode, chi2_contingency\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold,train_test_split\n\nimport lightgbm\nfrom lightgbm import LGBMClassifier","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-18T08:22:45.089334Z","iopub.execute_input":"2022-05-18T08:22:45.089615Z","iopub.status.idle":"2022-05-18T08:22:45.098471Z","shell.execute_reply.started":"2022-05-18T08:22:45.089586Z","shell.execute_reply":"2022-05-18T08:22:45.097455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/spaceship-titanic/train.csv\")\ntest =  pd.read_csv(\"../input/spaceship-titanic/test.csv\")\nX_train = train.drop(\"Transported\", axis = 1)\ny_train = train.Transported.astype(np.int8)\nX_test = test.copy()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:21:50.814802Z","iopub.execute_input":"2022-05-18T08:21:50.815139Z","iopub.status.idle":"2022-05-18T08:21:50.911897Z","shell.execute_reply.started":"2022-05-18T08:21:50.815072Z","shell.execute_reply":"2022-05-18T08:21:50.910988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:21:52.956292Z","iopub.execute_input":"2022-05-18T08:21:52.956626Z","iopub.status.idle":"2022-05-18T08:21:52.983256Z","shell.execute_reply.started":"2022-05-18T08:21:52.956589Z","shell.execute_reply":"2022-05-18T08:21:52.98256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Feature Engineeering & EDA","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:21:55.165562Z","iopub.execute_input":"2022-05-18T08:21:55.166225Z","iopub.status.idle":"2022-05-18T08:21:55.198017Z","shell.execute_reply.started":"2022-05-18T08:21:55.166185Z","shell.execute_reply":"2022-05-18T08:21:55.197078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"object_features = [col for col in train.columns if train[col].dtype == \"object\"]\nnumeric_features = [col for col in train.columns if train[col].dtype != \"object\"]\nobject_features.remove(\"PassengerId\") \nnumeric_features.remove(\"Transported\") ","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:21:56.159861Z","iopub.execute_input":"2022-05-18T08:21:56.16067Z","iopub.status.idle":"2022-05-18T08:21:56.167861Z","shell.execute_reply.started":"2022-05-18T08:21:56.160616Z","shell.execute_reply":"2022-05-18T08:21:56.166565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.1. Defining Functions Used in This Notebook","metadata":{}},{"cell_type":"code","source":"# missing values\n\ndef count_missing(data):\n    \"\"\"Function for counting missing values in a dataset\"\"\"\n    \n    for col in data.columns:\n        missing = data[col].isna().sum()\n        perc = missing/len(data)\n        print(f\"Feature {col} - Missing Values: {missing} ({perc*100:.2f}%)\")\n\ndef visualize_missing(data):\n    \"\"\" Function to analyze missing value dynamics\"\"\"\n    fig,axes = plt.subplots(ncols = 2, figsize = (12,5))\n    msno.bar(data, ax = axes[0])\n    msno.heatmap(data, ax = axes[1])\n    plt.tight_layout()\n    plt.show()\n    \ndef get_dataset():\n    train = pd.read_csv(\"../input/spaceship-titanic/train.csv\")\n    test =  pd.read_csv(\"../input/spaceship-titanic/test.csv\")\n    X_train = train.drop(\"Transported\", axis = 1)\n    y_train = train.Transported.astype(np.int8)\n    return X_train,y_train,test\n\ndef fill_missing(data,impute_type = \"most_frequent\"):\n    \n    if impute_type == \"most_frequent\":\n        \n    \n        for obj in object_features:\n            mfv = mode(data[obj])[0][0]\n            print(f\"Feature {obj} - Most Frequent Variable: {mfv} ({data[obj].value_counts()[mfv]*100/len(data):.2f}%)\")\n            data[obj] = data[obj].fillna(mfv)\n        print(\"Replaced All Missing Values with the Most Frequent Variable\\n\")\n        del mfv\n        \n        si = SimpleImputer(strategy = \"median\")\n        data[numeric_features] = si.fit_transform(data[numeric_features])\n        data[['Age','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']] = data[['Age','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].astype(np.float32)\n\n        return data\n    \n    elif impute_type == \"iterative\":\n        imputer_obj = SimpleImputer(strategy = 'most_frequent')\n        imputer_num = IterativeImputer(initial_strategy = 'median',max_iter = 100)\n        data_cols = ['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination','VIP','Name','Age',\"RoomService\",\"FoodCourt\",'ShoppingMall','Spa','VRDeck']\n        col_trans = make_column_transformer((imputer_obj,make_column_selector(dtype_include = \"object\",dtype_exclude = \"float64\")),(imputer_num,make_column_selector(dtype_include = \"float64\")))\n        col_trans.fit(data)\n        data = col_trans.transform(data)\n        data = pd.DataFrame(data, columns = data_cols)\n        data[['Age','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']] = data[['Age','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].astype(np.float32)\n        return data,col_trans\n    \ndef preprocessing_function(data,keep_id = False):\n\n        if keep_id:\n            IDs = data[\"PassengerId\"]\n        else:\n            pass\n        \n        # convert binary\n        data[\"VIP\"] = data[\"VIP\"].astype(np.int8)\n        data[\"CryoSleep\"] = data[\"CryoSleep\"].astype(np.int8)\n        \n        # extract Surnames\n        data[\"Surnames\"] = data[\"Name\"].str.split(expand = True).values[:,1] \n        data.drop(\"Name\", axis = 1, inplace = True)\n        \n        # separate Cabin variable to 3 distinct features\n        data[\"Deck\"] = data[\"Cabin\"].str.split(\"/\", expand = True)[0]\n        data[\"Num\"] = data[\"Cabin\"].str.split(\"/\", expand = True)[1]\n        data[\"Side\"] = data[\"Cabin\"].str.split(\"/\", expand = True)[2]\n        data.drop(\"Cabin\", axis = 1, inplace = True)\n        \n        # make more features\n        data[\"Total_Lux_Expense\"] = data[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis = 1)\n        \n        # separate groupID from passengerID\n        data[\"Group\"] = data[\"PassengerId\"].str.split(\"_\",expand = True)[0].astype(\"category\")\n        data.drop(\"PassengerId\", axis = 1, inplace = True)\n        \n        if keep_id:\n        \n            return IDs,data\n        else:\n            return data\n\ndef target_encoder(cols_encode,X_train,y_train,X_test, smooth = 0.22):\n    print(\"--- Before encoding ---\\n\")\n    for col in cols_encode:\n        \n        print(f\"Training set - {col} - # of unique variables: {X_train[col].nunique()}\")\n        print(f\"Test set - {col} - # of unique variables: {X_test[col].nunique()}\")\n        print()\n    oof = pd.DataFrame()\n    skf = StratifiedKFold(n_splits = 5, random_state = 1, shuffle = True)\n    for train_idx,val_idx in skf.split(X_train,y_train):\n        target_encoder = ce.TargetEncoder(cols = cols_encode,smoothing = smooth)\n        target_encoder.fit(X_train.loc[train_idx,cols_encode], y_train.loc[train_idx])\n        oof = oof.append(target_encoder.transform(X_train.loc[val_idx,cols_encode]))\n    target_encoder = ce.TargetEncoder(cols = cols_encode,smoothing = smooth)\n    target_encoder.fit(X_train,y_train)\n    X_train[cols_encode] = oof.sort_index()\n    X_test = target_encoder.transform(X_test)\n    for col in cols_encode:\n        print(\"--- After encoding ---\\n\")\n        print(f\"Training set - {col} - # of unique variables: {X_train[col].nunique()}\")\n        print(f\"Test set - {col} - # of unique variables: {X_test[col].nunique()}\")\n    return X_train,X_test\n\n# H0: Two categorical features are not dependent.\n# H1: Two categorical features are dependent.\ndef dependency_control(col,col2 = None, with_target = True):\n    if with_target:\n        \n        array1 = X_train[col].values\n        transport_array = y_train.values.astype(str)\n\n        stat_df = pd.DataFrame()\n        stat_df[col] = array1\n        stat_df[\"Transported\"] = transport_array\n\n        #convert cross tab format\n        cr_tab = pd.crosstab(index = stat_df[col],\n                            columns = stat_df[\"Transported\"],\n                            )\n\n        test_stats,p,_,_ = chi2_contingency(cr_tab)\n        print(f\"P value: {p}\")\n        if p < 0.05:\n            print(\"Independent from Target\")\n        else:\n            print(\"Dependent to Target\")\n    else:\n        array1 = X_train[col].values\n        array2 = X_train[col2].values\n        stat_df = pd.DataFrame()\n        stat_df[col] = array1\n        stat_df[col2] = array2\n        cr_tab = pd.crosstab(index = stat_df[col],\n                            columns = stat_df[col2],\n                            )\n\n        test_stats,p,_,_ = chi2_contingency(cr_tab)\n        print(f\"P value: {p}\")\n        if p < 0.05:\n            print(f\"{col} and {col2} are independent\")\n        else:\n            print(f\"{col} and {col2} are dependent\")\n            \n\ndef experiment_cols(categorical_features = np.array([\"Group\",\"HomePlanet\",\"Destination\",\"Deck\",\"Num\",\"Side\"])):\n    seed = [1,3,5]\n    mean_losses = []\n    for i in range (6):\n        if i == 0:\n            \n            print(colored(\"No categorical columns\\n\",attrs = [\"bold\"]))\n            loss = []\n            for s in seed:\n                print(f\"Seed: {s}\")\n                model = LGBMClassifier(random_state = 42)\n                X_trn,X_val,y_trn,y_val = train_test_split(X_train.drop(X_train.select_dtypes(include = \"category\").columns,axis = 1),y_train,stratify = y_train,random_state = s)\n                model.fit(X_trn,y_trn, eval_set = [(X_val,y_val)],callbacks = [lightgbm.log_evaluation(period = 0)])\n                min_loss = np.min(model.evals_result_[\"valid_0\"][\"binary_logloss\"])\n                print(f'Minimum Logloss: {min_loss}')\n                loss.append(min_loss)\n            mean_loss = np.mean(loss)\n            print(colored(f\"\\nAverage Logloss: {mean_loss}\",attrs=[\"bold\"]))\n            mean_losses.append(mean_loss)\n                  \n        else:\n            loss = []\n            exp_cols = categorical_features[:i]\n            print(colored(f\"\\nCategorical Column in: {exp_cols}\\n\", attrs = [\"bold\"]))\n            for s in seed:\n                print(f\"Seed: {s}\")\n                exclude_cols = np.setdiff1d(categorical_features,exp_cols)\n                model = LGBMClassifier(random_state = 42, categorical_features = exp_cols)\n                X_tra = X_train.drop(exclude_cols, axis = 1)\n                X_trn,X_val,y_trn,y_val = train_test_split(X_tra,y_train,stratify = y_train,random_state = s)\n                model = LGBMClassifier(categorical_features = exp_cols)\n                model.fit(X_trn,y_trn, eval_set = [(X_val,y_val)],callbacks = [lightgbm.log_evaluation(period = 0)])\n                min_loss = np.min(model.evals_result_[\"valid_0\"][\"binary_logloss\"])\n                print(f'Minimum Logloss: {min_loss}')\n                loss.append(min_loss)\n            mean_loss = np.mean(loss)\n            print(colored(f\"\\nAverage Logloss: {mean_loss}\",attrs=[\"bold\"]))\n            mean_losses.append(mean_loss)\n        loss_reduction = -np.diff(np.array(mean_losses))\n    print()\n    for i,(col,loss) in enumerate(zip(categorical_features,loss_reduction)):\n        print(f\"Adding Feature {col} - Reduced Validation Loss: {loss_reduction[i]}\") ","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:21:57.949588Z","iopub.execute_input":"2022-05-18T08:21:57.950349Z","iopub.status.idle":"2022-05-18T08:21:58.001672Z","shell.execute_reply.started":"2022-05-18T08:21:57.950286Z","shell.execute_reply":"2022-05-18T08:21:58.000612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2. Missing Data Analysis\n\n* Except for a couple of columns, almost all features have missing values. Therefore they have to be handled before putting data into the model. \n* Missing values can be considered as MCAR. Because any feature does not depend on another.","metadata":{}},{"cell_type":"code","source":"count_missing(train)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:22:00.482849Z","iopub.execute_input":"2022-05-18T08:22:00.483479Z","iopub.status.idle":"2022-05-18T08:22:00.504132Z","shell.execute_reply.started":"2022-05-18T08:22:00.483429Z","shell.execute_reply":"2022-05-18T08:22:00.503039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_missing(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:22:01.70016Z","iopub.execute_input":"2022-05-18T08:22:01.700673Z","iopub.status.idle":"2022-05-18T08:22:01.717659Z","shell.execute_reply.started":"2022-05-18T08:22:01.700638Z","shell.execute_reply":"2022-05-18T08:22:01.716295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_missing(train)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:22:02.637931Z","iopub.execute_input":"2022-05-18T08:22:02.638724Z","iopub.status.idle":"2022-05-18T08:22:04.635863Z","shell.execute_reply.started":"2022-05-18T08:22:02.638683Z","shell.execute_reply":"2022-05-18T08:22:04.634828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_missing(test)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:22:04.637332Z","iopub.execute_input":"2022-05-18T08:22:04.637595Z","iopub.status.idle":"2022-05-18T08:22:06.493575Z","shell.execute_reply.started":"2022-05-18T08:22:04.637557Z","shell.execute_reply":"2022-05-18T08:22:06.492342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3. Continuous Feature Analysis\n\n* Train and Test data have similar continuous feature distributions.\n* Peak points differ a bit. \n* Age is bimodal other ones are positively skewed.","metadata":{}},{"cell_type":"code","source":"fig,axes = plt.subplots(nrows = 2, ncols = 3, figsize = (16,9))\ncol_no = 0\nrow_no = 0\nfor col in numeric_features:\n    sns.kdeplot(train[col], ax = axes[row_no][col_no], fill = True, label = \"Train\")\n    sns.kdeplot(X_test[col], ax = axes[row_no][col_no], fill = True, label = \"Test\")\n    axes[row_no][col_no].set_yticks([])\n    axes[row_no][col_no].legend()\n    if (col_no+1)%3 == 0:\n        col_no = 0\n        row_no += 1\n    else:\n        col_no += 1\nplt.tight_layout()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:22:07.049417Z","iopub.execute_input":"2022-05-18T08:22:07.050331Z","iopub.status.idle":"2022-05-18T08:22:08.502278Z","shell.execute_reply.started":"2022-05-18T08:22:07.050263Z","shell.execute_reply":"2022-05-18T08:22:08.501339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.4. Iterative Filling\n\nIn this section the missing values,\n\n* At categorical columns were filled with the most frequent ones.\n* At numerical columns filled iteratively. The iteration process starts with selecting the feature which has the least missing values. The missing value is filled according to the similarity of the other instances.\n\n**Other Preprocessing Techniques Used in This Section**\n\n**1. Target Encoding:** Surnames have lot less unique number of elements than names yet it is still too much (2217 - 1726). Therefore they have handled via TargetEncoding. Target encoding for \"Groups\" feature works well in training set but same thing is not true for test set. Regularization parameter becomes really dominant so cardinality of test set decreses to 1 even in small regularization coefficients.\n\n**2. Split Cabin Feature**\n\n**3. Split PassengerId Feature**\n\n**4. Add Total Expenses**\n\n**5. Drop ID Column**\n\n**6. Convert Booleans to binary features** ","metadata":{}},{"cell_type":"code","source":"for col in X_train[X_train.select_dtypes(include = [\"object\",\"category\"]).columns]:\n    print(f\"Feature: {col} - # of Unique Elements: {X_train[col].nunique()}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:22:09.772405Z","iopub.execute_input":"2022-05-18T08:22:09.772843Z","iopub.status.idle":"2022-05-18T08:22:09.794587Z","shell.execute_reply.started":"2022-05-18T08:22:09.772811Z","shell.execute_reply":"2022-05-18T08:22:09.793567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# iterative filling\nX_train,y_train,X_test = get_dataset()\nX_train,col_trans = fill_missing(X_train,\"iterative\")\ndata_cols = ['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination','VIP','Name','Age',\"RoomService\",\"FoodCourt\",'ShoppingMall','Spa','VRDeck']\nX_test = col_trans.transform(X_test)\nX_test = pd.DataFrame(X_test,columns = data_cols)\nX_train = preprocessing_function(X_train)\nIDs,X_test = preprocessing_function(X_test,keep_id = True)\nX_train,X_test = target_encoder([\"Surnames\"],X_train,y_train,X_test)\nX_train[X_train.select_dtypes(include = \"object\").columns] = X_train[X_train.select_dtypes(include = \"object\").columns].astype(\"category\")\n","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:22:16.89801Z","iopub.execute_input":"2022-05-18T08:22:16.898368Z","iopub.status.idle":"2022-05-18T08:22:17.669472Z","shell.execute_reply.started":"2022-05-18T08:22:16.898332Z","shell.execute_reply":"2022-05-18T08:22:17.668176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.4.1. Categorical Column Dependency Check \n\nIn this section categorical columns and target feature are considered. Since corr() method is not applicable to those, chi2 test was used to check dependency.\n\n**Findings:** Besides the relation between side and destination, all other combination of features are independent from each other.","metadata":{}},{"cell_type":"code","source":"dependency_control(\"Side\")","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:22:19.443872Z","iopub.execute_input":"2022-05-18T08:22:19.444302Z","iopub.status.idle":"2022-05-18T08:22:19.481774Z","shell.execute_reply.started":"2022-05-18T08:22:19.444261Z","shell.execute_reply":"2022-05-18T08:22:19.480692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dependency_control(\"Num\")","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:22:20.380425Z","iopub.execute_input":"2022-05-18T08:22:20.380757Z","iopub.status.idle":"2022-05-18T08:22:20.436895Z","shell.execute_reply.started":"2022-05-18T08:22:20.380715Z","shell.execute_reply":"2022-05-18T08:22:20.435881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dependency_control(\"Deck\")","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:22:21.085731Z","iopub.execute_input":"2022-05-18T08:22:21.086015Z","iopub.status.idle":"2022-05-18T08:22:21.117591Z","shell.execute_reply.started":"2022-05-18T08:22:21.085985Z","shell.execute_reply":"2022-05-18T08:22:21.116514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dependency_control(\"Destination\")","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:22:21.651189Z","iopub.execute_input":"2022-05-18T08:22:21.651453Z","iopub.status.idle":"2022-05-18T08:22:21.68225Z","shell.execute_reply.started":"2022-05-18T08:22:21.651425Z","shell.execute_reply":"2022-05-18T08:22:21.681306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dependency_control(\"Deck\",\"Destination\",0)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:22:22.234016Z","iopub.execute_input":"2022-05-18T08:22:22.234316Z","iopub.status.idle":"2022-05-18T08:22:22.260259Z","shell.execute_reply.started":"2022-05-18T08:22:22.234285Z","shell.execute_reply":"2022-05-18T08:22:22.258577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dependency_control(\"Side\",\"Destination\",0)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:22:22.829156Z","iopub.execute_input":"2022-05-18T08:22:22.829482Z","iopub.status.idle":"2022-05-18T08:22:22.853854Z","shell.execute_reply.started":"2022-05-18T08:22:22.829418Z","shell.execute_reply":"2022-05-18T08:22:22.853181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dependency_control(\"Deck\",\"Side\",0)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:22:23.486621Z","iopub.execute_input":"2022-05-18T08:22:23.486919Z","iopub.status.idle":"2022-05-18T08:22:23.510512Z","shell.execute_reply.started":"2022-05-18T08:22:23.486886Z","shell.execute_reply":"2022-05-18T08:22:23.509638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.4.2. Model Performance on Iterative Filling\n\n* For categorical columns whether a feature is helpful for prediction seems vague. Therefore in this section LGBM Model includes them one by one and shows which feature helps to which extent.\n* \"Deck\" and \"HomePlanet\" features are the ones which are significantly improve performances. \"Destination\" and \"Side\" improves a bit. On the other hand \"Num\" feature worsen the model performance and \"Group\" feature has no effect; therefore it shouldn't be included in final prediction. ","metadata":{}},{"cell_type":"code","source":"experiment_cols()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:22:53.462576Z","iopub.execute_input":"2022-05-18T08:22:53.46343Z","iopub.status.idle":"2022-05-18T08:23:11.432013Z","shell.execute_reply.started":"2022-05-18T08:22:53.463369Z","shell.execute_reply":"2022-05-18T08:23:11.431296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.5. Simple Filling\n\nIn this section the missing values,\n\n* At categorical columns were filled with the most frequent ones.\n* At numerical columns were filled with median. \n\n**Other Preprocessing Techniques Used in This Section**\n\n**1. Target Encoding:** Surnames have lot less unique number of elements than names yet it is still too much (2217 - 1725). Therefore they have handled via TargetEncoding.\n\n**2. Split Cabin Feature**\n\n**3. Add Total Expenses**\n\n**4. Drop ID Column**\n\n**5. Convert Booleans to binary features** ","metadata":{}},{"cell_type":"code","source":"# simple filling\nX_train,y_train,X_test = get_dataset()\nX_train = fill_missing(X_train)\nX_test = fill_missing(X_test)\ndata_cols = ['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination','VIP','Name','Age',\"RoomService\",\"FoodCourt\",'ShoppingMall','Spa','VRDeck']\nX_train = preprocessing_function(X_train)\nIDs,X_test = preprocessing_function(X_test,keep_id = True)\nX_train,X_test = target_encoder([\"Surnames\"],X_train,y_train,X_test)\nX_train[X_train.select_dtypes(include = \"object\").columns] = X_train[X_train.select_dtypes(include = \"object\").columns].astype(\"category\")","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:23:16.215195Z","iopub.execute_input":"2022-05-18T08:23:16.215524Z","iopub.status.idle":"2022-05-18T08:23:24.960063Z","shell.execute_reply.started":"2022-05-18T08:23:16.21549Z","shell.execute_reply":"2022-05-18T08:23:24.95888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.5.2. Model Performance on Simple Filling\n\n* Iterative filling is slightly better on model performance.\n* Features have the same effect discussed in the previous section.","metadata":{}},{"cell_type":"code","source":"experiment_cols()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:23:24.961781Z","iopub.execute_input":"2022-05-18T08:23:24.96217Z","iopub.status.idle":"2022-05-18T08:23:41.261613Z","shell.execute_reply.started":"2022-05-18T08:23:24.962128Z","shell.execute_reply":"2022-05-18T08:23:41.260581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Final Model","metadata":{}},{"cell_type":"code","source":"# data preparation\nX_train,y_train,X_test = get_dataset()\nX_train,col_trans = fill_missing(X_train,\"iterative\")\ndata_cols = ['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination','VIP','Name','Age',\"RoomService\",\"FoodCourt\",'ShoppingMall','Spa','VRDeck']\nX_test = col_trans.transform(X_test)\nX_test = pd.DataFrame(X_test,columns = data_cols)\nX_test[['Age','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']] = X_test[['Age','RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].astype(np.float32)\nX_train = preprocessing_function(X_train)\nIDs,X_test = preprocessing_function(X_test,keep_id = True)\nX_train,X_test = target_encoder([\"Surnames\"],X_train,y_train,X_test)\nX_train[X_train.select_dtypes(include = \"object\").columns] = X_train[X_train.select_dtypes(include = \"object\").columns].astype(\"category\")\nX_test[X_test.select_dtypes(include = \"object\").columns] = X_test[X_test.select_dtypes(include = \"object\").columns].astype(\"category\")\nX_train.drop([\"Num\",\"Group\"], axis = 1, inplace = True)\nX_test.drop([\"Num\",\"Group\"], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:23:46.118915Z","iopub.execute_input":"2022-05-18T08:23:46.119227Z","iopub.status.idle":"2022-05-18T08:23:46.898552Z","shell.execute_reply.started":"2022-05-18T08:23:46.119191Z","shell.execute_reply":"2022-05-18T08:23:46.897627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = []\nmodel = LGBMClassifier(categorical_features = list(X_train.select_dtypes(include = \"category\").columns), random_state = 42)\nmodel.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:23:53.377792Z","iopub.execute_input":"2022-05-18T08:23:53.378763Z","iopub.status.idle":"2022-05-18T08:23:54.218058Z","shell.execute_reply.started":"2022-05-18T08:23:53.378672Z","shell.execute_reply":"2022-05-18T08:23:54.217159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(X_test)\npreds = preds == 1","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:23:55.37813Z","iopub.execute_input":"2022-05-18T08:23:55.378956Z","iopub.status.idle":"2022-05-18T08:23:55.406844Z","shell.execute_reply.started":"2022-05-18T08:23:55.378894Z","shell.execute_reply":"2022-05-18T08:23:55.405802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission[\"PassengerId\"] = IDs\nsubmission[\"Transported\"] = preds\nsubmission.to_csv(\"Submission.csv\",index = False)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T08:23:56.888659Z","iopub.execute_input":"2022-05-18T08:23:56.889276Z","iopub.status.idle":"2022-05-18T08:23:56.908626Z","shell.execute_reply.started":"2022-05-18T08:23:56.889222Z","shell.execute_reply":"2022-05-18T08:23:56.907231Z"},"trusted":true},"execution_count":null,"outputs":[]}]}