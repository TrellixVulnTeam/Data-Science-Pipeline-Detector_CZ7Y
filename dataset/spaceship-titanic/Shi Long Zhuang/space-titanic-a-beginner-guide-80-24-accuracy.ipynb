{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center>\n<img src=\"https://2.bp.blogspot.com/-mNmwKeTuZZg/XR76sYkpbpI/AAAAAAAAELA/5DTQjesqBqMdMIlYwe1uOYTmdLUoBRpvACKgBGAs/w1920-h1080-c/spaceship-minimalist-sci-fi-digital-art-uhdpaper.com-4K-138.jpg\" />\n</center>","metadata":{"execution":{"iopub.status.busy":"2022-04-30T10:50:49.0531Z","iopub.execute_input":"2022-04-30T10:50:49.05339Z","iopub.status.idle":"2022-04-30T10:50:49.059236Z","shell.execute_reply.started":"2022-04-30T10:50:49.053362Z","shell.execute_reply":"2022-04-30T10:50:49.058135Z"}}},{"cell_type":"markdown","source":"# Spaceship Titanic Solution Walkthrough \n\nThe purpose of the notebook is to practice Exploratory Data Analysis, Visualization, and Machine Learning as well as show you how I have applied a systematic Data Science workflow as I navigate through this project.\n\n\n## Data Science Workflow\n> The foundation of this workflow was based on the author's citations in this [notebook](https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy/notebook). I figured there was still room for more C's so I bothered making a more sophisticated and obssessive-compulsive framework.\n- **Comprehend.** *Exploratory Data Analysis.* Understand the nature and relationships among each features in the datasets through data analyses and visualization.\n- **Correlate.** *Feature Selection* Validate the strength of association across features with the appopriate statistical tools and metrics, and to select the features that are significantly relevant with the solution goal.\n- **Clean.** *Data Cleaning.* Identify and remedy missing/null values by imputing them with reasonable inputs.  \n- **Create.** *Feature Engineering.* Create new features out of the existing ones which can make better predictions while also reducing noise in the number of features.\n- **Convert.** *Data Preprocessing.* Perform the necessary adjustments (one-hot encoding) and data transformations (i.e. sqrt, log trasformations) to make the data fit for modelling.\n- **Complete.** *Training Model.* Completion of a working and cleaned dataset in preparation for training the model and predicting solutions out of it. \n- **Configure.** *Hyperparameter Tuning.* Further optimize our learning algorithms by determining and running the optimal parameters. \n- **Combine.** *Ensemble Learning.* Combine multiple algorithms into one that can leverage the strengths and compensates the weaknesses of the tested models.","metadata":{}},{"cell_type":"markdown","source":"Credits to the creator who made this awesome package *mplcyberpunk*, which allows us to create visualizations surrounding the 'cyberpunk' theme. You can check more about the repository [HERE](https://github.com/dhaitz/mplcyberpunk/blob/master/README.md).","metadata":{}},{"cell_type":"code","source":"pip install mplcyberpunk","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:47:22.443545Z","iopub.execute_input":"2022-06-02T13:47:22.44427Z","iopub.status.idle":"2022-06-02T13:47:34.256298Z","shell.execute_reply.started":"2022-06-02T13:47:22.444097Z","shell.execute_reply":"2022-06-02T13:47:34.255185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data analysis\nimport pandas as pd\nimport numpy as np\n\n# data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n%matplotlib inline\n\n# Breathtaking visuals\nimport mplcyberpunk","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:47:34.258677Z","iopub.execute_input":"2022-06-02T13:47:34.259335Z","iopub.status.idle":"2022-06-02T13:47:38.839713Z","shell.execute_reply.started":"2022-06-02T13:47:34.259292Z","shell.execute_reply":"2022-06-02T13:47:38.838722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use(\"cyberpunk\")","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:47:38.842136Z","iopub.execute_input":"2022-06-02T13:47:38.84238Z","iopub.status.idle":"2022-06-02T13:47:38.849191Z","shell.execute_reply.started":"2022-06-02T13:47:38.84235Z","shell.execute_reply":"2022-06-02T13:47:38.84803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\ntest_df = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\ntest_df_copy = test_df.copy()\ndf = [train_df, test_df]","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:47:38.85279Z","iopub.execute_input":"2022-06-02T13:47:38.853745Z","iopub.status.idle":"2022-06-02T13:47:38.93219Z","shell.execute_reply.started":"2022-06-02T13:47:38.853561Z","shell.execute_reply":"2022-06-02T13:47:38.931354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:47:38.933761Z","iopub.execute_input":"2022-06-02T13:47:38.934174Z","iopub.status.idle":"2022-06-02T13:47:38.973273Z","shell.execute_reply.started":"2022-06-02T13:47:38.934085Z","shell.execute_reply":"2022-06-02T13:47:38.97209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:47:38.975414Z","iopub.execute_input":"2022-06-02T13:47:38.975724Z","iopub.status.idle":"2022-06-02T13:47:38.998449Z","shell.execute_reply.started":"2022-06-02T13:47:38.975681Z","shell.execute_reply":"2022-06-02T13:47:38.997161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:47:39.000536Z","iopub.execute_input":"2022-06-02T13:47:39.001382Z","iopub.status.idle":"2022-06-02T13:47:39.047353Z","shell.execute_reply.started":"2022-06-02T13:47:39.001318Z","shell.execute_reply":"2022-06-02T13:47:39.045515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:47:39.048953Z","iopub.execute_input":"2022-06-02T13:47:39.049323Z","iopub.status.idle":"2022-06-02T13:47:39.069865Z","shell.execute_reply.started":"2022-06-02T13:47:39.049281Z","shell.execute_reply":"2022-06-02T13:47:39.068721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:47:39.07176Z","iopub.execute_input":"2022-06-02T13:47:39.072078Z","iopub.status.idle":"2022-06-02T13:47:39.111536Z","shell.execute_reply.started":"2022-06-02T13:47:39.072035Z","shell.execute_reply":"2022-06-02T13:47:39.110359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def missing_values(df):\n    # Calculate missing value and their percentage for each feature\n    missing_percent = df.isnull().sum() * 100 / df.shape[0]\n    df_missing_percent = pd.DataFrame(missing_percent).round(2)\n    df_missing_percent = df_missing_percent.reset_index().rename(\n                    columns={\n                            'index':'Feature',\n                            0:'Missing Percentage (%)'\n                    }\n                )\n    df_missing_value = df.isnull().sum()\n    df_missing_value = df_missing_value.reset_index().rename(\n                    columns={\n                            'index':'Feature',\n                            0:'Missing Values'\n                    }\n                )\n\n    Final = df_missing_value.merge(df_missing_percent, how = 'inner', left_on = 'Feature', right_on = 'Feature')\n    Final = Final.sort_values(by = 'Missing Percentage (%)',ascending = False)\n    return Final\n\nmissing_values(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:47:39.116194Z","iopub.execute_input":"2022-06-02T13:47:39.116428Z","iopub.status.idle":"2022-06-02T13:47:39.165581Z","shell.execute_reply.started":"2022-06-02T13:47:39.116394Z","shell.execute_reply":"2022-06-02T13:47:39.164327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:47:39.167235Z","iopub.execute_input":"2022-06-02T13:47:39.167656Z","iopub.status.idle":"2022-06-02T13:47:39.197864Z","shell.execute_reply.started":"2022-06-02T13:47:39.167601Z","shell.execute_reply":"2022-06-02T13:47:39.196182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Missing Values**\n- The proportion of missing values to the total entries in each feature are relatively small, ranging from 0% to 2.5%.\n\n**Data Types**\n- Numerical. *Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck*\n- Categorical. *HomePlanet, CryoSleep, Destination, VIP*\n- Mixed/Alphanumeric. *Cabin, Name*\n- Target Categorical. *Transported*\n\n### Interesting Questions and Hypotheses\n- **Cabin vs Cryosleep.** Does a cryosleep facility have its designated cabin area?\n- **PassengerId group** (gggg=group, pp=number) **vs HomePlanet and Destination.** Did passengers within their groups travel together, which means coming from the same HomePlanet and debarking to the same Destination.\n- **CryoSleep vs Services.** Did passengers who elected to cryosleep have lower expenditures?\n- **CryoSleep vs PassengerId group.** Were those who traveled alone in the group likely to undergo CryoSleep?\n- **VIP vs Services.** How the services and expenditures from VIP members differ from non-VIPs?","metadata":{"execution":{"iopub.status.busy":"2022-04-25T07:06:12.795404Z","iopub.execute_input":"2022-04-25T07:06:12.795741Z","iopub.status.idle":"2022-04-25T07:06:12.802596Z","shell.execute_reply.started":"2022-04-25T07:06:12.79571Z","shell.execute_reply":"2022-04-25T07:06:12.801206Z"}}},{"cell_type":"code","source":"df_num = train_df[['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Transported']]\ndf_cat = train_df[['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Transported']]","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:47:39.199353Z","iopub.execute_input":"2022-06-02T13:47:39.2002Z","iopub.status.idle":"2022-06-02T13:47:39.209604Z","shell.execute_reply.started":"2022-06-02T13:47:39.200155Z","shell.execute_reply":"2022-06-02T13:47:39.208498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Numerical Variables","metadata":{"execution":{"iopub.status.busy":"2022-04-25T07:11:25.435054Z","iopub.execute_input":"2022-04-25T07:11:25.435397Z","iopub.status.idle":"2022-04-25T07:11:25.43976Z","shell.execute_reply.started":"2022-04-25T07:11:25.435363Z","shell.execute_reply":"2022-04-25T07:11:25.438917Z"}}},{"cell_type":"code","source":"sns.pairplot(df_num, hue='Transported')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:47:39.211357Z","iopub.execute_input":"2022-06-02T13:47:39.212063Z","iopub.status.idle":"2022-06-02T13:48:04.137564Z","shell.execute_reply.started":"2022-06-02T13:47:39.212016Z","shell.execute_reply":"2022-06-02T13:48:04.136021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nax = sns.histplot(data=train_df, x=\"Age\", hue=\"Transported\", binwidth=1, kde=True)\nplt.title('Age',\n          fontsize = 18,\n          fontweight = 'bold',\n          fontfamily = 'serif',\n          loc = 'left')\n\nax.set(xlabel=None, ylabel=None)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:04.138811Z","iopub.execute_input":"2022-06-02T13:48:04.139212Z","iopub.status.idle":"2022-06-02T13:48:05.019372Z","shell.execute_reply.started":"2022-06-02T13:48:04.139095Z","shell.execute_reply":"2022-06-02T13:48:05.018441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations\n- Passengers (<18) were more likely to be transported than other age groups.\n- Passengers (21-28) were less likely to be transported.\n- Age seems to follow a normal distribution, but a little skewed to the right.\n\n#### Decisions\n- Complete the missing values in 'Age'\n- Normalize our 'Age' distribution through data transformations.","metadata":{}},{"cell_type":"code","source":"def strip_plot(df, x, y):\n    ax = sns.stripplot(x=df[x], y=df[y])\n    plt.title(str(y),\n          fontsize = 18,\n          fontweight = 'bold',\n          fontfamily = 'serif',\n          loc = 'left')\n    \n    ax.set(xlabel=None, ylabel=None)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:05.021196Z","iopub.execute_input":"2022-06-02T13:48:05.021564Z","iopub.status.idle":"2022-06-02T13:48:05.028321Z","shell.execute_reply.started":"2022-06-02T13:48:05.021519Z","shell.execute_reply":"2022-06-02T13:48:05.027168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15, 15))\n\nplt.subplot(3, 2, 1)\nstrip_plot(train_df, 'Transported', 'RoomService')\n\nplt.subplot(3, 2, 2)\nstrip_plot(train_df, 'Transported', 'FoodCourt')\n\nplt.subplot(3, 2, 3)\nstrip_plot(train_df, 'Transported', 'Spa')\n\nplt.subplot(3, 2, 4)\nstrip_plot(train_df, 'Transported', 'ShoppingMall')\n\nplt.subplot(3, 2, 5)\nstrip_plot(train_df, 'Transported', 'VRDeck')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:05.030745Z","iopub.execute_input":"2022-06-02T13:48:05.031577Z","iopub.status.idle":"2022-06-02T13:48:06.158021Z","shell.execute_reply.started":"2022-06-02T13:48:05.031528Z","shell.execute_reply":"2022-06-02T13:48:06.157093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations\n\n- The distributions on RoomService, Spa, and VRDeck follow very similar patterns, the same is observed with those of FoodCourt and ShoppingMall.\n- The bills spent by transported passengers appear to be concentrated and approaching to 0. Either they spent very little or spent no amount at all.\n- Passengers who spent more on **RoomService, Spa,** and **VRDeck** services were less likely to get transported.\n- Passengers who spent more on **FoodCourt** and **ShoppingMall** services were more likely to get transported.\n\n#### Decisions\n\n- Create a new feature called *Premium* that sums the bills spent on **RoomService, Spa,** and **VRDeck**\n- Create a new feature called *Basic* feature that sums those from **FoodCourt** and **ShoppingMall**.","metadata":{}},{"cell_type":"markdown","source":"## Correlating Numerical Variables\nThis corr matrix will mark as our baseline to understand our numerical variables and see what we can play with for feature engineering. ","metadata":{}},{"cell_type":"code","source":"plt.subplots(figsize=(10,10))\nmask = np.triu(np.ones_like(df_num.corr()))\nsns.heatmap(df_num.corr(), mask=mask, cmap='cool', annot=True, annot_kws={\"fontsize\":13}, square=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:06.161257Z","iopub.execute_input":"2022-06-02T13:48:06.161489Z","iopub.status.idle":"2022-06-02T13:48:06.571262Z","shell.execute_reply.started":"2022-06-02T13:48:06.16146Z","shell.execute_reply":"2022-06-02T13:48:06.570178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations\n- Despite following a normal distribution, *Age* has an underwhelmingly low correlation with 'Transported'.\n- So far, *RoomService, Spa, and VRDeck* have some of the highest correlation with our target variable.\n\n#### Decisions\n- Create a categorical feature *AgeGroup* as a function of *Age*.","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering Numerical Features","metadata":{}},{"cell_type":"code","source":"# Create Basic, Premium, and All Spent\nfor dataset in df:\n    dataset['Premium'] = dataset['RoomService'] + dataset['Spa'] + dataset['VRDeck']\n    dataset['Basic'] = dataset['FoodCourt'] + dataset['ShoppingMall']\n    dataset['All_Services'] = dataset['RoomService'] + dataset['Spa'] + dataset ['VRDeck'] + dataset['FoodCourt'] + dataset['ShoppingMall']\n\ndf_num = train_df[['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Basic', 'Premium', 'All_Services', 'Transported']]\n\nplt.subplots(figsize=(15,10))\nmask = np.triu(np.ones_like(df_num.corr()))\nsns.heatmap(df_num.corr(), mask=mask, cmap='cool', annot=True, annot_kws={\"fontsize\":13}, center=0, square=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:06.573209Z","iopub.execute_input":"2022-06-02T13:48:06.573527Z","iopub.status.idle":"2022-06-02T13:48:07.264256Z","shell.execute_reply.started":"2022-06-02T13:48:06.573482Z","shell.execute_reply":"2022-06-02T13:48:07.263144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Decisions\n- I am rooting *Premium* for now, since its correlation -0.36 seems promising. Should I choose to keep it, other features that are multicolliner with *Premium* will be removed.\n- Let's try normalizing the highly skewed numerical features and see if it improves the correlation.","metadata":{}},{"cell_type":"markdown","source":"## Missing Numerical Values\nNormally, the easier and simpler way is to resort to Simple Imputation technique usually through the measures of central tendency: **mean, median, or mode**, but a more advanced technique can be employed by filling up missing values based on the insights I can find out by conducting Exploratory Data Analysis on my features.","metadata":{}},{"cell_type":"code","source":"def scatter(df, x, y, h):\n    ax = sns.scatterplot(x=df[x], y=df[y], hue=df[h])\n    plt.title(str(y),\n          fontsize = 18,\n          fontweight = 'bold',\n          fontfamily = 'serif',\n          loc = 'left')\n    \n    ax.set(xlabel=None, ylabel=None)\n\n\nfig = plt.figure(figsize=(15, 15))\n\nplt.subplot(3, 2, 1)\nscatter(train_df, 'Age', 'RoomService', 'Transported')\n\nplt.subplot(3, 2, 2)\nscatter(train_df, 'Age', 'FoodCourt', 'Transported')\n\nplt.subplot(3, 2, 3)\nscatter(train_df, 'Age', 'Spa', 'Transported')\n\nplt.subplot(3, 2, 4)\nscatter(train_df, 'Age', 'ShoppingMall', 'Transported')\n\nplt.subplot(3, 2, 5)\nscatter(train_df, 'Age', 'VRDeck', 'Transported')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:07.266552Z","iopub.execute_input":"2022-06-02T13:48:07.267284Z","iopub.status.idle":"2022-06-02T13:48:10.598759Z","shell.execute_reply.started":"2022-06-02T13:48:07.267239Z","shell.execute_reply":"2022-06-02T13:48:10.596782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def box(df, x, y):\n    ax = sns.boxplot(x=df[x], y=df[y], width = .3)\n    plt.title(str(y) + ' by ' + str(x),\n          fontsize = 18,\n          fontweight = 'bold',\n          fontfamily = 'serif',\n          loc = 'left')\n    \n    ax.set(xlabel=None, ylabel=None)\n\n    \nfig = plt.figure(figsize=(15, 15))\n\nplt.subplot(3, 2, 1)\nbox(train_df, 'VIP', 'Age')\n\nplt.subplot(3, 2, 2)\nbox(train_df, 'HomePlanet', 'Age')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:10.60075Z","iopub.execute_input":"2022-06-02T13:48:10.60127Z","iopub.status.idle":"2022-06-02T13:48:11.032255Z","shell.execute_reply.started":"2022-06-02T13:48:10.601229Z","shell.execute_reply":"2022-06-02T13:48:11.031077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights\n- Easily read like a book! It shows that children (<12) don't have buying power as their expenditures amounted to 0 across every service there is.\n- It is also likely that an age requirement is laid out when applying for *VIP*, this explains the slight variation in the distribution of *Age* by *VIP*.\n- Visible variation in the age distributions is also evident across *HomePlanet.*\n- Impute missing age based from *VIP* and *HomePlanet*.","metadata":{}},{"cell_type":"markdown","source":"Let's now examine *Expenditures* by *CryoSleep*.","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(15, 15))\n\nplt.subplot(3, 2, 1)\nstrip_plot(train_df, 'CryoSleep', 'RoomService')\n\nplt.subplot(3, 2, 2)\nstrip_plot(train_df, 'CryoSleep', 'FoodCourt')\n\nplt.subplot(3, 2, 3)\nstrip_plot(train_df, 'CryoSleep', 'Spa')\n\nplt.subplot(3, 2, 4)\nstrip_plot(train_df, 'CryoSleep', 'ShoppingMall')\n\nplt.subplot(3, 2, 5)\nstrip_plot(train_df, 'CryoSleep', 'VRDeck')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:11.034251Z","iopub.execute_input":"2022-06-02T13:48:11.034995Z","iopub.status.idle":"2022-06-02T13:48:12.232384Z","shell.execute_reply.started":"2022-06-02T13:48:11.034931Z","shell.execute_reply":"2022-06-02T13:48:12.231321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insights\n- As expected, Cryosleeping members don't spend much on services.\n- We can impute *Expenditures* based from *Age* and *CryoSleep*.","metadata":{}},{"cell_type":"code","source":"for dataset in df:\n    dataset['IsChild'] = 0\n    dataset.loc[dataset.Age <= 12, 'IsChild'] = 1\n    dataset.loc[dataset.Age > 12, 'IsChild'] = 0\n    \nservices = ['RoomService', 'FoodCourt', 'Spa', 'ShoppingMall', 'VRDeck']\nfor s in services: \n    print(train_df.groupby(['IsChild', 'CryoSleep'])[s].median().fillna(0))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:12.233966Z","iopub.execute_input":"2022-06-02T13:48:12.235036Z","iopub.status.idle":"2022-06-02T13:48:12.271235Z","shell.execute_reply.started":"2022-06-02T13:48:12.234989Z","shell.execute_reply":"2022-06-02T13:48:12.269994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute Expenditures\nfor dataset in df:\n    dataset['RoomService'] = dataset['RoomService'].fillna(dataset.groupby(['IsChild', 'CryoSleep'])['RoomService'].transform('median'))\n    dataset['FoodCourt'] = dataset['FoodCourt'].fillna(dataset.groupby(['IsChild', 'CryoSleep'])['FoodCourt'].transform('median'))\n    dataset['Spa'] = dataset['Spa'].fillna(dataset.groupby(['IsChild', 'CryoSleep'])['Spa'].transform('median'))\n    dataset['VRDeck'] = dataset['VRDeck'].fillna(dataset.groupby(['IsChild', 'CryoSleep'])['VRDeck'].transform('median'))\n    dataset['ShoppingMall'] = dataset['ShoppingMall'].fillna(dataset.groupby(['IsChild', 'CryoSleep'])['ShoppingMall'].transform('median'))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:12.272453Z","iopub.execute_input":"2022-06-02T13:48:12.272748Z","iopub.status.idle":"2022-06-02T13:48:12.311553Z","shell.execute_reply.started":"2022-06-02T13:48:12.272692Z","shell.execute_reply":"2022-06-02T13:48:12.310676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:12.313103Z","iopub.execute_input":"2022-06-02T13:48:12.314269Z","iopub.status.idle":"2022-06-02T13:48:12.35203Z","shell.execute_reply.started":"2022-06-02T13:48:12.314198Z","shell.execute_reply":"2022-06-02T13:48:12.351075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update Basic, Premium, and All_Services\nfor dataset in df:\n    dataset['Premium'] = dataset['RoomService'] + dataset['Spa'] + dataset['VRDeck']\n    dataset['Basic'] = dataset['FoodCourt'] + dataset['ShoppingMall']\n    dataset['All_Services'] = dataset['RoomService'] + dataset['Spa'] + dataset ['VRDeck'] + dataset['FoodCourt'] + dataset['ShoppingMall']","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:12.353821Z","iopub.execute_input":"2022-06-02T13:48:12.354195Z","iopub.status.idle":"2022-06-02T13:48:12.368318Z","shell.execute_reply.started":"2022-06-02T13:48:12.354149Z","shell.execute_reply":"2022-06-02T13:48:12.367021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reference table for imputing *Age*:\n- it appears that there were no VIPs that aboarded from Earth.","metadata":{}},{"cell_type":"code","source":"train_df.groupby(['HomePlanet', 'VIP'])['Age'].median().fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:12.370506Z","iopub.execute_input":"2022-06-02T13:48:12.370968Z","iopub.status.idle":"2022-06-02T13:48:12.388747Z","shell.execute_reply.started":"2022-06-02T13:48:12.370888Z","shell.execute_reply":"2022-06-02T13:48:12.387753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute Age\nfor dataset in df:\n    dataset.Age = dataset.groupby(['HomePlanet', 'VIP']).Age.apply(lambda x: x.fillna(x.median()))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:12.390218Z","iopub.execute_input":"2022-06-02T13:48:12.391258Z","iopub.status.idle":"2022-06-02T13:48:12.415056Z","shell.execute_reply.started":"2022-06-02T13:48:12.391196Z","shell.execute_reply":"2022-06-02T13:48:12.414062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:12.656474Z","iopub.execute_input":"2022-06-02T13:48:12.656756Z","iopub.status.idle":"2022-06-02T13:48:12.694884Z","shell.execute_reply.started":"2022-06-02T13:48:12.656724Z","shell.execute_reply":"2022-06-02T13:48:12.69362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transforming Numerical Variables\nThe **probability plot** or [**quantile-quantile plot (QQplot)**](https://www.statisticshowto.com/q-q-plots/) allows us to plot our sample data against the quantiles of a normal distribution. In a nutshell, the objective is to have all the points lie along the line in the QQplot.\n\nBefore doing that, let's first fill up our missing numerical values with median.","metadata":{}},{"cell_type":"code","source":"import scipy.stats as stats\n\n# Defining the function to generate the distribution plot alongside QQplot\ndef QQplot(df, col):\n    plt.figure(figsize = (15, 5))\n    plt.subplot(1,2,1)\n    ax = sns.histplot(x=df[col], kde=True)\n    plt.title(str(col),\n          fontsize = 18,\n          fontweight = 'bold',\n          fontfamily = 'serif',\n          loc = 'left')\n    \n    ax.set(xlabel=None, ylabel=None)\n    \n    plt.subplot(1,2,2)\n    stats.probplot(df[col].dropna(), dist=\"norm\", plot=plt)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:12.696668Z","iopub.execute_input":"2022-06-02T13:48:12.697104Z","iopub.status.idle":"2022-06-02T13:48:12.706218Z","shell.execute_reply.started":"2022-06-02T13:48:12.697045Z","shell.execute_reply":"2022-06-02T13:48:12.704917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will present the analysis the *Premium, Basic,* and *All_Services* features as working examples for visualizing and interpreting the QQplots. As shown below is the baseline of the their distributions in the form of QQplot.","metadata":{}},{"cell_type":"code","source":"QQplot(train_df, 'Premium')\nQQplot(train_df, 'All_Services')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:12.707872Z","iopub.execute_input":"2022-06-02T13:48:12.708547Z","iopub.status.idle":"2022-06-02T13:48:15.393897Z","shell.execute_reply.started":"2022-06-02T13:48:12.7085Z","shell.execute_reply":"2022-06-02T13:48:15.392522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Afterwards, we can proceed to transform our data and assess its fit in the QQplots once more. Here, I chose to try the following data transformations.\n- square root\n- cube root\n- logarithmic ( log(x+1))","metadata":{}},{"cell_type":"code","source":"_ = train_df[['Premium', 'Transported']]\n\n_[\"sqrt\"] = _['Premium']**(1./2)\n_[\"4rt\"] = _['Premium']**(1./4)\n_[\"log\"] = np.log(_['Premium']+1)\n\nQQplot(_, 'sqrt')\nQQplot(_, '4rt')\nQQplot(_, 'log')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:15.395742Z","iopub.execute_input":"2022-06-02T13:48:15.39607Z","iopub.status.idle":"2022-06-02T13:48:17.604527Z","shell.execute_reply.started":"2022-06-02T13:48:15.396017Z","shell.execute_reply":"2022-06-02T13:48:17.603659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After transformation, it is usually a good practice to check the correlation once more if there are any improvements. As shown below, both 4th root and log transformations improved the correlation from **-0.35 to -0.56.**","metadata":{}},{"cell_type":"code","source":"plt.subplots(figsize=(8,6))\nmask = np.triu(np.ones_like(_.corr()))\nsns.heatmap(_.corr(), mask=mask, cmap='cool', annot=True, annot_kws={\"fontsize\":13}, center=0, square=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:17.609307Z","iopub.execute_input":"2022-06-02T13:48:17.611659Z","iopub.status.idle":"2022-06-02T13:48:18.096012Z","shell.execute_reply.started":"2022-06-02T13:48:17.611615Z","shell.execute_reply":"2022-06-02T13:48:18.095199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = train_df[['All_Services', 'Transported']]\n\n_[\"sqrt\"] = _['All_Services']**(1./2)\n_[\"4rt\"] = _['All_Services']**(1./4)\n_[\"log\"] = np.log(_['All_Services']+1)\n\nQQplot(_, 'sqrt')\nQQplot(_, '4rt')\nQQplot(_, 'log')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:18.100885Z","iopub.execute_input":"2022-06-02T13:48:18.103284Z","iopub.status.idle":"2022-06-02T13:48:20.04197Z","shell.execute_reply.started":"2022-06-02T13:48:18.103239Z","shell.execute_reply":"2022-06-02T13:48:20.04096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(figsize=(8,6))\nmask = np.triu(np.ones_like(_.corr()))\nsns.heatmap(_.corr(), mask=mask, cmap='cool', annot=True, annot_kws={\"fontsize\":13}, center=0, square=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:20.043587Z","iopub.execute_input":"2022-06-02T13:48:20.044125Z","iopub.status.idle":"2022-06-02T13:48:20.373686Z","shell.execute_reply.started":"2022-06-02T13:48:20.044081Z","shell.execute_reply":"2022-06-02T13:48:20.372646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating *Spent*.","metadata":{}},{"cell_type":"code","source":"for dataset in df:\n    dataset['Spent'] = 0\n    dataset.loc[dataset['All_Services'] > 0, 'Spent'] = 1","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:20.375391Z","iopub.execute_input":"2022-06-02T13:48:20.375917Z","iopub.status.idle":"2022-06-02T13:48:20.387709Z","shell.execute_reply.started":"2022-06-02T13:48:20.375873Z","shell.execute_reply":"2022-06-02T13:48:20.3864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Categorical Features\n\nBefore we visualize them, let's fill up our missing values in each feature with their corresponding **mode**, which is the most common label in the existing feature.","metadata":{}},{"cell_type":"code","source":"def count_plot(df, x, y):\n    plt.subplots(1,2, figsize = (15, 5))\n    plt.subplot(1,2,1)\n    ax = sns.countplot( x=df[x].dropna(), hue=df[y])\n    plt.title(str(x),\n          fontsize = 18,\n          fontweight = 'bold',\n          fontfamily = 'serif',\n          loc = 'left')\n    ax.set(xlabel=None, ylabel=None)\n    \n    plt.subplot(1,2,2)\n    plt.ylim(0,1)\n    ax = sns.lineplot( x=df[x], y=df[y], data=df, ci=None, linewidth=3, marker=\"o\")\n    ax.set(xlabel=None, ylabel=None)\n    plt.show()\n\n\ncount_plot(train_df, 'HomePlanet', 'Transported')\ncount_plot(train_df, 'CryoSleep', 'Transported')\ncount_plot(train_df, 'Destination', 'Transported')\ncount_plot(train_df, 'VIP', 'Transported')\ncount_plot(train_df, 'IsChild', 'Transported')\ncount_plot(train_df, 'Spent', 'Transported')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:20.390538Z","iopub.execute_input":"2022-06-02T13:48:20.390881Z","iopub.status.idle":"2022-06-02T13:48:22.575771Z","shell.execute_reply.started":"2022-06-02T13:48:20.39085Z","shell.execute_reply":"2022-06-02T13:48:22.574612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mixed/Alphanumeric Features\nLet's extract relevant information from 'PassengerId', 'Cabin', and 'Name'\n- **PassengerId** = gggg_pp (gggg=group, pp=number within group)\n- **Cabin** = deck/num/side (side: P=Port, S=Starboard)\n- **Name** = First Name + Last Name","metadata":{}},{"cell_type":"code","source":"# Splitting PassengerId into 'Group' and 'GroupSize'\n\nfor dataset in df:\n    dataset['Group'] = dataset['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\n    dataset['GroupSize'] = dataset['Group'].map(lambda x: dataset['Group'].value_counts()[x])\n    \n    dataset['withGroup'] = 1\n    dataset.loc[dataset['GroupSize'] == 1, 'withGroup'] = 0\n\ncount_plot(train_df, 'GroupSize', 'Transported')\ncount_plot(train_df, 'withGroup', 'Transported')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:22.577617Z","iopub.execute_input":"2022-06-02T13:48:22.57832Z","iopub.status.idle":"2022-06-02T13:48:34.296198Z","shell.execute_reply.started":"2022-06-02T13:48:22.578091Z","shell.execute_reply":"2022-06-02T13:48:34.295063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is more likely that larger group sizes have children with them because they are family. Let's try exploring that.","metadata":{}},{"cell_type":"code","source":"count_plot(train_df, 'GroupSize', 'IsChild')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:34.297755Z","iopub.execute_input":"2022-06-02T13:48:34.299372Z","iopub.status.idle":"2022-06-02T13:48:34.718113Z","shell.execute_reply.started":"2022-06-02T13:48:34.299321Z","shell.execute_reply":"2022-06-02T13:48:34.717167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GroupSize**\n- Large number of passengers traveled alone, the volume exponentially decreases with larger group sizes.\n- GroupSize of 4 followed by 6 had the highest transport rates.\n- GroupSize of 8 followed by alone passengers had the lowest transport rates.\n- As expected, group sizes (>1) tend to have children with them, but with the exception of 7, and 8. This likely explains the transport rate pattern in *GroupSize*.","metadata":{}},{"cell_type":"code","source":"# Splitting Cabin into Deck, Num, and Side\n\nfor dataset in df:\n    dataset['Deck'] = dataset['Cabin'].apply(lambda x: x.split('/')[0] if (str(x)) != 'nan' else x)\n    dataset['Num'] = dataset['Cabin'].apply(lambda x: x.split('/')[1] if (str(x)) != 'nan' else x)\n    dataset['Side'] = dataset['Cabin'].apply(lambda x: x.split('/')[2] if (str(x)) != 'nan' else x)\n\ncount_plot(train_df, 'Deck', 'Transported')\ncount_plot(train_df, 'Side', 'Transported')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:34.719521Z","iopub.execute_input":"2022-06-02T13:48:34.72062Z","iopub.status.idle":"2022-06-02T13:48:35.789153Z","shell.execute_reply.started":"2022-06-02T13:48:34.720571Z","shell.execute_reply":"2022-06-02T13:48:35.788037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Deck**\n- Majority of passengers resided in Cabins F and G.\n- Highest transport rate among Cabin B passengers, followed by those of Cabin C.\n- Lowest transort rate among Cabin T passengers, but data is not representative enough.\n\n**Side**\n- Side S dominates transport rate by a small margin over Side P.","metadata":{}},{"cell_type":"code","source":"# Splitting Name into First and Last Names\nfor dataset in df:\n    dataset['FirstName'] = dataset['Name'].apply(lambda x: x.split(' ')[0] if (str(x)) != 'nan' else x)\n    dataset['LastName'] = dataset['Name'].apply(lambda x: x.split(' ')[1] if (str(x)) != 'nan' else x)\n    \n    # Creating 'FamilySize' from 'LastName'\n    dataset['FamilySize'] = dataset['LastName'].map(lambda x: dataset['LastName'].value_counts()[x] if (str(x)) != 'nan' else x)\n\n    \ndef count_plot_adj(df, x, y):\n    plt.subplots(1,2, figsize = (15, 5))\n    plt.subplot(1,2,1)\n    ax = sns.countplot( x=df[x].dropna(), hue=df[y])\n    plt.title(str(x),\n          fontsize = 18,\n          fontweight = 'bold',\n          fontfamily = 'serif',\n          loc = 'left')\n    ax.set(xlabel=None, ylabel=None)\n    \n    plt.subplot(1,2,2)\n    plt.ylim(0,1)\n    ax = sns.lineplot( x=df[x], y=df[y], data=df, ci=None, linewidth=3, marker=\"o\")\n    ax.set(xlabel=None, ylabel=None, xlim = (1, 18))\n    \n    plt.show()\n    \ncount_plot_adj(train_df, 'FamilySize', 'Transported')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:48:35.790955Z","iopub.execute_input":"2022-06-02T13:48:35.791237Z","iopub.status.idle":"2022-06-02T13:49:05.623051Z","shell.execute_reply.started":"2022-06-02T13:48:35.791201Z","shell.execute_reply":"2022-06-02T13:49:05.622184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_plot_adj(train_df, 'FamilySize', 'IsChild')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:49:05.624701Z","iopub.execute_input":"2022-06-02T13:49:05.625251Z","iopub.status.idle":"2022-06-02T13:49:06.173998Z","shell.execute_reply.started":"2022-06-02T13:49:05.625192Z","shell.execute_reply":"2022-06-02T13:49:06.173015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**FamilySize**\n- *FamilySize* of 3, 4, and 5 comprise the majority.\n- Transport rate appears to steadily decline with increasing FamilySize, but with the exception of 15.\n- Similar pattern was observed in the increasing distribution of children with larger *FamilySize*, although there seems to be exceptions as it goes greater than 10.","metadata":{}},{"cell_type":"markdown","source":"## Associating Categorical Variables\n\nIn terms of statistics terminologies, it is not usually appropriate to use the term *correlation* when testing with categorical variables. We can't really assess the magnitude or the strength of correlation between predictor and response categorical variables, unless if they are either dichotomous (categorical variables having 2 categories like 'Sex') or ordinal variables then it is allowed to use **Pearson's correlation.**\n> This means that we can still evaluate *CryoSleep, VIP* against our target *Transported* with Pearson's correlation.\n\nThe rest of our categorical features are either non-ordinal or contain more than 2 categories. Instead of conducting correlation tests, it is more appropriate to use tests of independency to assess the strength of association between cateorical variables. The one I will use is Cramer's V which is based from **[Chi-square test](https://towardsdatascience.com/chi-square-test-for-feature-selection-in-machine-learning-206b1f0b8223#:~:text=In%20feature%20selection%2C%20we%20aim,hypothesis%20of%20independence%20is%20incorrect.).**\n\nBefore we can conduct Chi-square tests, we must ensure that our categorical data are numerically encoded first using `LabelEncoder()`.","metadata":{}},{"cell_type":"code","source":"# Encoding categorical labels\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encode = LabelEncoder()\n\ndf1 = train_df.copy()\n\ndf_cat = df1[['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'IsChild', 'Spent', 'Group', 'withGroup', 'Deck', 'Num', 'Side', 'LastName', 'Transported']]\n\nlabel = LabelEncoder()\ndf_cat_encoded = pd.DataFrame()\n\nfor i in df_cat.columns:\n    df_cat_encoded[i] = label.fit_transform(df_cat[i])\n    \ndf_cat_encoded.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:49:06.175871Z","iopub.execute_input":"2022-06-02T13:49:06.176539Z","iopub.status.idle":"2022-06-02T13:49:06.240582Z","shell.execute_reply.started":"2022-06-02T13:49:06.176493Z","shell.execute_reply":"2022-06-02T13:49:06.239552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats.contingency import association       \n    \ndef Cramers_V(var1, var2) :\n  crosstab = np.array(pd.crosstab(index=var1, columns=var2)) # Cross Tab\n  return (association(crosstab, method='cramer'))            # Return Cramer's V\n\n# Create the dataFrame matrix with the returned Cramer's V\nrows = []\n\nfor var1 in df_cat_encoded:\n    col = []\n\n    for var2 in df_cat_encoded:\n        V = Cramers_V(df_cat_encoded[var1], df_cat_encoded[var2]) # Return Cramer's V\n        col.append(V)                                             # Store values to subsequent columns  \n    rows.append(col)                                              # Store values to subsequent rows\n  \nCramersV_results = np.array(rows)\nCramersV_df = pd.DataFrame(CramersV_results, columns = df_cat_encoded.columns, index = df_cat_encoded.columns)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:49:06.24222Z","iopub.execute_input":"2022-06-02T13:49:06.242745Z","iopub.status.idle":"2022-06-02T13:49:55.125248Z","shell.execute_reply.started":"2022-06-02T13:49:06.242688Z","shell.execute_reply":"2022-06-02T13:49:55.124178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(figsize=(20,15))\ncorr = np.corrcoef(np.random.randn(13, 13))\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(CramersV_df, mask=mask, cmap='cool', annot=True, annot_kws={\"fontsize\":13}, center=0, square=True, cbar=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:49:55.127073Z","iopub.execute_input":"2022-06-02T13:49:55.127392Z","iopub.status.idle":"2022-06-02T13:49:55.940926Z","shell.execute_reply.started":"2022-06-02T13:49:55.127352Z","shell.execute_reply":"2022-06-02T13:49:55.939853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations.** There is a lot to dig in here because my approach to feature engineering was rather exhaustive. As a result, multicollinearity tend to occur, this happens when two or more independent variables are highly associated with one another, rendering them redundant.","metadata":{}},{"cell_type":"markdown","source":"## Missing Categorical Values\n\nBy answering these questions, we also stand to gain relevant insights and patterns which we can apply to our imputation method. Guess what, since we created our association matrix above, we already know which features and other correspoding interrelated features to look out for. Below are just some questions that we are particularly interested in.\n\n- **PassengerId group vs Name** (last name). Assuming they are family, it is more probable that most have similar last names.\n- **PassengerId group** (gggg=group, pp=number) **vs HomePlanet and Destination.** Did passengers within their groups travel together, which means coming from the same HomePlanet and debarking to the same Destination.\n- **PassengerId group vs Cabin**. Do people of the same group stay in the same cabin?\n- **Cryosleep vs Cabin.** Does a cryosleep facility have its designated cabin area?\n- **CryoSleep vs Services.** Did passengers who elected to cryosleep have lower expenditures?\n- **CryoSleep vs withGroup.** Were those who traveled alone in the group likely to undergo CryoSleep?\n- **CryoSleep vs HomePlanet and Destination.** Is there a pattern among passengers who cryoslept with respect to longer travels (can possibly infer that distances are farther between HomePlanet vs Destination)","metadata":{}},{"cell_type":"code","source":"# Define function to impute based on a feature\ndef impute_cat(var1, var2):\n    print('Before %s Train:' %var2, train_df[var2].isnull().sum())\n    print('Before %s Test:' %var2, test_df[var2].isnull().sum())\n\n    test_df['Transported'] = np.NaN\n    df_full = pd.concat([train_df, test_df])\n\n    reference = df_full.groupby([var1, var2])[var2].size().unstack().fillna(0)\n\n    for dataset in df:          \n        index = dataset[dataset[var2].isnull()][(dataset.loc[dataset[var2].isnull()][var1]).isin(reference.index)].index\n        dataset.loc[index, var2] = dataset.loc[index, var1].map(lambda x: reference.idxmax(axis=1)[x])\n    \n    print('After %s Train:' %var2, train_df[var2].isnull().sum())\n    print('After %s Test:' %var2, test_df[var2].isnull().sum())\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:49:55.942665Z","iopub.execute_input":"2022-06-02T13:49:55.943727Z","iopub.status.idle":"2022-06-02T13:49:55.958519Z","shell.execute_reply.started":"2022-06-02T13:49:55.943678Z","shell.execute_reply":"2022-06-02T13:49:55.956409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imputing CryoSleep","metadata":{}},{"cell_type":"markdown","source":"Since Cryosleep is strongly associted with Spent, we will impute Cryosleep as False if Spent is 1, then True otherwise.","metadata":{}},{"cell_type":"code","source":"print('Before Train:', train_df['CryoSleep'].isnull().sum())\nprint('Before Test:', test_df['CryoSleep'].isnull().sum())\n\nfor dataset in df:\n    dataset.loc[(dataset.CryoSleep.isnull()) & (dataset.Spent == 0), 'CryoSleep' ] = True\n    dataset.loc[(dataset.CryoSleep.isnull()) & (dataset.Spent == 1), 'CryoSleep' ] = False\n\n\nprint('After Train:', train_df['CryoSleep'].isnull().sum())\nprint('After Test:', test_df['CryoSleep'].isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:49:55.961826Z","iopub.execute_input":"2022-06-02T13:49:55.962603Z","iopub.status.idle":"2022-06-02T13:49:56.014669Z","shell.execute_reply.started":"2022-06-02T13:49:55.962511Z","shell.execute_reply":"2022-06-02T13:49:56.013383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute Remaining Expenditures from CryoSleep\nfor dataset in df:\n    dataset['RoomService'] = dataset['RoomService'].fillna(dataset.groupby(['IsChild', 'CryoSleep'])['RoomService'].transform('median'))\n    dataset['FoodCourt'] = dataset['FoodCourt'].fillna(dataset.groupby(['IsChild', 'CryoSleep'])['FoodCourt'].transform('median'))\n    dataset['Spa'] = dataset['Spa'].fillna(dataset.groupby(['IsChild', 'CryoSleep'])['Spa'].transform('median'))\n    dataset['VRDeck'] = dataset['VRDeck'].fillna(dataset.groupby(['IsChild', 'CryoSleep'])['VRDeck'].transform('median'))\n    dataset['ShoppingMall'] = dataset['ShoppingMall'].fillna(dataset.groupby(['IsChild', 'CryoSleep'])['ShoppingMall'].transform('median'))\n    \n    # Update Basic, Premium, and All_Services\n    dataset['Premium'] = dataset['RoomService'] + dataset['Spa'] + dataset['VRDeck']\n    dataset['Basic'] = dataset['FoodCourt'] + dataset['ShoppingMall']\n    dataset['All_Services'] = dataset['RoomService'] + dataset['Spa'] + dataset ['VRDeck'] + dataset['FoodCourt'] + dataset['ShoppingMall']\n    \n    # Update Spent\n    dataset['Spent'] = 0\n    dataset.loc[dataset['All_Services'] > 0, 'Spent'] = 1","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:49:56.023204Z","iopub.execute_input":"2022-06-02T13:49:56.025591Z","iopub.status.idle":"2022-06-02T13:49:56.102426Z","shell.execute_reply.started":"2022-06-02T13:49:56.025545Z","shell.execute_reply":"2022-06-02T13:49:56.10134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imputing VIP","metadata":{}},{"cell_type":"code","source":"# How VIP spend on services\nplt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nsns.violinplot(data=train_df, x=\"VIP\", y='Premium')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:49:56.103856Z","iopub.execute_input":"2022-06-02T13:49:56.104957Z","iopub.status.idle":"2022-06-02T13:49:56.498955Z","shell.execute_reply.started":"2022-06-02T13:49:56.104913Z","shell.execute_reply":"2022-06-02T13:49:56.498113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation.** It appears that VIP passengers tend to spend more on premium services. For now, I will impute by mode.","metadata":{}},{"cell_type":"code","source":"print('Before Train:', train_df['VIP'].isnull().sum())\nprint('Before Test:', test_df['VIP'].isnull().sum())\n\nfor dataset in df:\n    dataset['VIP'].fillna(False, inplace=True)\n    \nprint('After Train:', train_df['VIP'].isnull().sum())\nprint('After Test:', test_df['VIP'].isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:49:56.500198Z","iopub.execute_input":"2022-06-02T13:49:56.500655Z","iopub.status.idle":"2022-06-02T13:49:56.529192Z","shell.execute_reply.started":"2022-06-02T13:49:56.500609Z","shell.execute_reply":"2022-06-02T13:49:56.528165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imputing Cabin Deck and Side","metadata":{}},{"cell_type":"code","source":"CD_PG = train_df.groupby(['Group', 'Deck'])['Deck'].size().unstack().fillna(0)\nCD_PG.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:49:56.531049Z","iopub.execute_input":"2022-06-02T13:49:56.536428Z","iopub.status.idle":"2022-06-02T13:49:56.589656Z","shell.execute_reply.started":"2022-06-02T13:49:56.536365Z","shell.execute_reply":"2022-06-02T13:49:56.58874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion.** We discovered that passengers of the same group stay in the same cabin deck.","metadata":{}},{"cell_type":"code","source":"# Imputing Deck\nimpute_cat('Group', 'Deck')\nimpute_cat('Group', 'Side')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:49:56.594176Z","iopub.execute_input":"2022-06-02T13:49:56.597132Z","iopub.status.idle":"2022-06-02T13:50:01.112476Z","shell.execute_reply.started":"2022-06-02T13:49:56.597083Z","shell.execute_reply":"2022-06-02T13:50:01.111553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Decision.** Impute *Deck* based on 2nd most highly associated feature *LastName*. Navigate [here](#impute-deck-lastname).","metadata":{}},{"cell_type":"markdown","source":"### Imputing Homeplanet and Destination","metadata":{}},{"cell_type":"code","source":"HP_PG = train_df.groupby(['Group', 'HomePlanet'])['HomePlanet'].size().unstack().fillna(0)\nHP_PG.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:50:01.113832Z","iopub.execute_input":"2022-06-02T13:50:01.114483Z","iopub.status.idle":"2022-06-02T13:50:01.140509Z","shell.execute_reply.started":"2022-06-02T13:50:01.114434Z","shell.execute_reply":"2022-06-02T13:50:01.139535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"D_PG = train_df.groupby(['Group', 'Destination'])['Destination'].size().unstack().fillna(0)\nD_PG.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:50:01.141832Z","iopub.execute_input":"2022-06-02T13:50:01.142752Z","iopub.status.idle":"2022-06-02T13:50:01.168251Z","shell.execute_reply.started":"2022-06-02T13:50:01.142708Z","shell.execute_reply":"2022-06-02T13:50:01.167182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion.** The passengers within each group have the same HomePlanet and Destinations. Given this premise, we can impute missing values in HomePlanet by returning the column name (Earth, Europa, Mars) with the positive number of passengers, as a function of 'Group'. The same imputation process can be done for missing values in 'Destination'.","metadata":{}},{"cell_type":"code","source":"# Impute HomePlanet and Destination\nimpute_cat('Group', 'HomePlanet')\nimpute_cat('Group', 'Destination')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:50:01.171479Z","iopub.execute_input":"2022-06-02T13:50:01.172066Z","iopub.status.idle":"2022-06-02T13:50:05.205292Z","shell.execute_reply.started":"2022-06-02T13:50:01.172034Z","shell.execute_reply":"2022-06-02T13:50:05.202812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are still remaining missing values that weren't filled, so my strategy now is to  impute them based on the succeeding features that are highly associated to *HomePlanet* and *Destination.*","metadata":{}},{"cell_type":"markdown","source":"**Decision.** Impute *HomePlanet* and *Destination* based from 2nd most associated feature *LastName*","metadata":{}},{"cell_type":"code","source":"LN_HP = train_df.groupby(['LastName', 'HomePlanet'])['HomePlanet'].size().unstack().fillna(0)\nLN_HP.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:50:05.207005Z","iopub.execute_input":"2022-06-02T13:50:05.207329Z","iopub.status.idle":"2022-06-02T13:50:05.237175Z","shell.execute_reply.started":"2022-06-02T13:50:05.207293Z","shell.execute_reply":"2022-06-02T13:50:05.236089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"impute_cat('LastName', 'HomePlanet')\nimpute_cat('LastName', 'Destination')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:50:05.238851Z","iopub.execute_input":"2022-06-02T13:50:05.239222Z","iopub.status.idle":"2022-06-02T13:50:06.663389Z","shell.execute_reply.started":"2022-06-02T13:50:05.239169Z","shell.execute_reply":"2022-06-02T13:50:06.662188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Decision.** Impute *HomePlanet* and *Destination* based from 3rd most associated feature *Deck*","metadata":{}},{"cell_type":"code","source":"CD_HP = train_df.groupby(['Deck', 'HomePlanet'])['HomePlanet'].size().unstack().fillna(0)\nCD_HP.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:50:06.665367Z","iopub.execute_input":"2022-06-02T13:50:06.665684Z","iopub.status.idle":"2022-06-02T13:50:06.689944Z","shell.execute_reply.started":"2022-06-02T13:50:06.665624Z","shell.execute_reply":"2022-06-02T13:50:06.68906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"impute_cat('Deck', 'HomePlanet')\nimpute_cat('Deck', 'Destination')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:50:06.691389Z","iopub.execute_input":"2022-06-02T13:50:06.691727Z","iopub.status.idle":"2022-06-02T13:50:06.770754Z","shell.execute_reply.started":"2022-06-02T13:50:06.691671Z","shell.execute_reply":"2022-06-02T13:50:06.769449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analyzing Cabin Deck, Side vs HomePlanet and Destination","metadata":{}},{"cell_type":"code","source":"HP_D_CS_CD = train_df.groupby(['HomePlanet', 'Destination', 'Spent', 'Deck'])['Deck'].size().unstack().fillna(0)\nHP_D_CS_CD.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:50:06.773147Z","iopub.execute_input":"2022-06-02T13:50:06.77437Z","iopub.status.idle":"2022-06-02T13:50:06.816029Z","shell.execute_reply.started":"2022-06-02T13:50:06.774307Z","shell.execute_reply":"2022-06-02T13:50:06.815161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(figsize=(15,18))\nsns.heatmap(HP_D_CS_CD, cmap='cool', annot=True, annot_kws={\"fontsize\":13}, fmt='g', center=0, square=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:50:06.81754Z","iopub.execute_input":"2022-06-02T13:50:06.818037Z","iopub.status.idle":"2022-06-02T13:50:08.136992Z","shell.execute_reply.started":"2022-06-02T13:50:06.81799Z","shell.execute_reply":"2022-06-02T13:50:08.136032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion.** Out of all the comparisons, HomePlanet-Destination-Spent vs CabinDeck yielded the best patterns.\n- Cabins E, F, and G are mostly reserved by passengers that embarked from Earth.\n- Cabins A, B, C, D, and E resereved by those that embarked from Europa.\n- Cabins D, E, and F resereved by those that embarked from Mars.\n\nIf you are interested, you may check the proposed solution in Stackoverflow [HERE](https://stackoverflow.com/questions/45741879/can-i-replace-nans-with-the-mode-of-a-column-in-a-grouped-data-frame) if you want to proceed the imputation. No matter how I editted and approached, I wasn't able to make it work : (","metadata":{}},{"cell_type":"markdown","source":"### Imputing Surname and Family Size\n- Passengers of the same group are likely to be families (having the same surnames)\n- The purpose of filling up *LastName* is to update the missing data in *FamilySize* later on.","metadata":{}},{"cell_type":"code","source":"PG_SN = train_df.groupby(['Group', 'LastName'])['LastName'].size().fillna(0)\nPG_SN.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:50:08.138623Z","iopub.execute_input":"2022-06-02T13:50:08.139422Z","iopub.status.idle":"2022-06-02T13:50:08.159332Z","shell.execute_reply.started":"2022-06-02T13:50:08.139377Z","shell.execute_reply":"2022-06-02T13:50:08.158329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Decision.** It appears that most, but not all the passengers in the group have the same surnames, as in the case of Group 20, so we can just impute as the *LastName* with the highest occurences.","metadata":{}},{"cell_type":"code","source":"impute_cat('Group', 'LastName')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:50:08.160601Z","iopub.execute_input":"2022-06-02T13:50:08.161497Z","iopub.status.idle":"2022-06-02T13:50:34.228797Z","shell.execute_reply.started":"2022-06-02T13:50:08.161452Z","shell.execute_reply":"2022-06-02T13:50:34.22694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Decision.** Impute *LastName* based on 2nd most highly associated feature *HomePlanet*","metadata":{}},{"cell_type":"code","source":"impute_cat('HomePlanet', 'LastName')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:50:34.230427Z","iopub.execute_input":"2022-06-02T13:50:34.231022Z","iopub.status.idle":"2022-06-02T13:50:35.596662Z","shell.execute_reply.started":"2022-06-02T13:50:34.230976Z","shell.execute_reply":"2022-06-02T13:50:35.595577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update the 'FamilySize' column\nfor dataset in df:\n    dataset['FamilySize'] = dataset['LastName'].map(lambda x: dataset['LastName'].value_counts()[x] if (str(x)) != 'nan' else x)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:50:35.598378Z","iopub.execute_input":"2022-06-02T13:50:35.598901Z","iopub.status.idle":"2022-06-02T13:51:06.501232Z","shell.execute_reply.started":"2022-06-02T13:50:35.598855Z","shell.execute_reply":"2022-06-02T13:51:06.500153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"impute-deck-lastname\"></a>\n**Decision.** Impute remaining missing *Deck* and *Side* based on *LastName*, followed by *HomePlanet*.","metadata":{}},{"cell_type":"code","source":"impute_cat('LastName', 'Deck')\nimpute_cat('HomePlanet', 'Deck')\n\nimpute_cat('LastName', 'Side')\nimpute_cat('HomePlanet', 'Side')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:51:06.502709Z","iopub.execute_input":"2022-06-02T13:51:06.503062Z","iopub.status.idle":"2022-06-02T13:51:08.120261Z","shell.execute_reply.started":"2022-06-02T13:51:06.503006Z","shell.execute_reply":"2022-06-02T13:51:08.1192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We want finalize if our missing data have all been filled before we proceed to preprocessing.","metadata":{}},{"cell_type":"code","source":"missing_values(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:51:08.121761Z","iopub.execute_input":"2022-06-02T13:51:08.122063Z","iopub.status.idle":"2022-06-02T13:51:08.16888Z","shell.execute_reply.started":"2022-06-02T13:51:08.122023Z","shell.execute_reply":"2022-06-02T13:51:08.167864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:51:08.170485Z","iopub.execute_input":"2022-06-02T13:51:08.170779Z","iopub.status.idle":"2022-06-02T13:51:08.207295Z","shell.execute_reply.started":"2022-06-02T13:51:08.17074Z","shell.execute_reply":"2022-06-02T13:51:08.206167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Now, I will be dropping features that are highly cardinal, weakly correlated/associated with the target variable, and multicollinearity-inducing.","metadata":{}},{"cell_type":"code","source":"# Change VIP data type from bool to object\nfor dataset in df:\n    dataset['VIP'] = dataset['VIP'].astype(object)\n    \n    # 4th Root transform Premium\n    dataset['Premium'] = dataset['Premium']**(1./4)\n    dataset['All_Services'] = dataset['All_Services']**(1./4)\n    dataset['Basic'] = dataset['Basic']**(1./4)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:51:08.208581Z","iopub.execute_input":"2022-06-02T13:51:08.209593Z","iopub.status.idle":"2022-06-02T13:51:08.223895Z","shell.execute_reply.started":"2022-06-02T13:51:08.209547Z","shell.execute_reply":"2022-06-02T13:51:08.222495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.drop('Transported', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:51:08.226044Z","iopub.execute_input":"2022-06-02T13:51:08.226695Z","iopub.status.idle":"2022-06-02T13:51:08.235266Z","shell.execute_reply.started":"2022-06-02T13:51:08.226636Z","shell.execute_reply":"2022-06-02T13:51:08.234052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train_df['Transported']\nX_train = train_df[['CryoSleep', 'Premium', 'Basic', 'IsChild', 'withGroup', 'HomePlanet', 'Destination', 'Deck', 'Side']]\n\nX_test = test_df[['CryoSleep', 'Premium', 'Basic', 'IsChild', 'withGroup', 'HomePlanet', 'Destination', 'Deck', 'Side']]\n\nX = [X_train, X_test]","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:54:22.961409Z","iopub.execute_input":"2022-06-02T13:54:22.961697Z","iopub.status.idle":"2022-06-02T13:54:22.972357Z","shell.execute_reply.started":"2022-06-02T13:54:22.961665Z","shell.execute_reply":"2022-06-02T13:54:22.971212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\n\n# Label encode categorical variables\nfor dataset in X:\n    dataset['Side'] = label.fit_transform(dataset['Side'])\n    dataset['CryoSleep'] = label.fit_transform(dataset['CryoSleep'])\n\n# Scale num features\nX_train[['Premium']] = scale.fit_transform(X_train[['Premium']])\nX_test[['Premium']] = scale.transform(X_test[['Premium']])\n\nX_train[['Basic']] = scale.fit_transform(X_train[['Basic']])\nX_test[['Basic']] = scale.transform(X_test[['Basic']])","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:54:24.050063Z","iopub.execute_input":"2022-06-02T13:54:24.050764Z","iopub.status.idle":"2022-06-02T13:54:24.087959Z","shell.execute_reply.started":"2022-06-02T13:54:24.050728Z","shell.execute_reply":"2022-06-02T13:54:24.087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating dummy indicator columns for categorical variables\nX_train = pd.get_dummies(X_train, columns=['HomePlanet', 'Destination', 'Deck'])\nX_test = pd.get_dummies(X_test, columns=['HomePlanet', 'Destination', 'Deck'])","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:55:19.355625Z","iopub.execute_input":"2022-06-02T13:55:19.355979Z","iopub.status.idle":"2022-06-02T13:55:19.379324Z","shell.execute_reply.started":"2022-06-02T13:55:19.355947Z","shell.execute_reply":"2022-06-02T13:55:19.378327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:55:23.227665Z","iopub.execute_input":"2022-06-02T13:55:23.227954Z","iopub.status.idle":"2022-06-02T13:55:23.258041Z","shell.execute_reply.started":"2022-06-02T13:55:23.22792Z","shell.execute_reply":"2022-06-02T13:55:23.257075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training\nI will run my model through some common supervised model algorithms.\n- Logistic Regression\n- Support Vector Classifier\n- Decision Tree\n- Random Forest\n- XGBoost","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n#Common Model Algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\n# Defining a list of Machine Learning Algorithms I will be running\nMLA = [\n    LogisticRegression(max_iter = 2000),\n    SVC(),\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    XGBClassifier(),\n    LGBMClassifier(),\n#     CatBoostClassifier(verbose=False)\n]\n\nrow_index = 0\n\n# Setting up the table to compare the performances of each model\nMLA_cols = ['Model', 'Accuracy']\nMLA_compare = pd.DataFrame(columns = MLA_cols)\n\n# Iterate and store scores in the table\nfor model in MLA:\n    MLA_compare.loc[row_index, 'Model'] = model.__class__.__name__\n    cv_results = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy')\n    MLA_compare.loc[row_index, 'Accuracy'] = cv_results.mean()\n    \n    row_index+=1\n\n# Present table\nMLA_compare.sort_values(by=['Accuracy'], ascending=False, inplace=True)\nMLA_compare","metadata":{"execution":{"iopub.status.busy":"2022-06-02T13:55:32.118291Z","iopub.execute_input":"2022-06-02T13:55:32.119093Z","iopub.status.idle":"2022-06-02T13:56:20.384106Z","shell.execute_reply.started":"2022-06-02T13:55:32.11904Z","shell.execute_reply":"2022-06-02T13:56:20.383146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion.** Among the model algorithms with default parameters, the top 3 best perorming classifiers turned out to be LGBM, followed by SVC, followed by XGB.","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameters Tuning\nThe top 3 best performing model algorithms will be chosen as candidates for hyperparameter tuning. I already ran the code below for hypertuning parameters and found the optimal parameters as listed below, so the code was commented out in the interest of time. ","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n\n# Hypertune XGBoost\ndef objective(trial, data=X_train , target=y_train):\n\n    param = {\n        'n_estimators': 1000,\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n        'max_depth': trial.suggest_categorical('max_depth', [3, 4, 5, 6, 7, 8, 9, 10]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 2, 10),\n        'subsample': trial.suggest_categorical('subsample', [0.5, 0.6, 0.7, 0.8, 0.9])\n    }    \n    \n    cv = StratifiedKFold( n_splits=5, shuffle=True, random_state=42)\n    \n    for idx, (train_idx, test_idx) in enumerate(cv.split(X_train, y_train)):\n        \n        x_trn, x_val = X_train.iloc[train_idx], X_train.iloc[test_idx]\n        y_trn, y_val = y_train.iloc[train_idx], y_train.iloc[test_idx]\n    \n    \n        model = XGBClassifier(**param)\n        \n        model.fit(x_trn,\n                  y_trn,\n                  eval_set = [(x_val, y_val)],\n                  early_stopping_rounds = 100,\n                  eval_metric = 'logloss',\n                  verbose = False\n                 )\n    \n        preds = model.predict(x_val)\n        scores = accuracy_score(y_val, preds)\n        \n    return np.mean(scores)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:06:23.640009Z","iopub.execute_input":"2022-06-02T14:06:23.64053Z","iopub.status.idle":"2022-06-02T14:06:23.651983Z","shell.execute_reply.started":"2022-06-02T14:06:23.640493Z","shell.execute_reply":"2022-06-02T14:06:23.651064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=10)\n\n# print(f\"\\tBest score: {study.best_value:.5f}\")\n# print(f\"\\tBest params:\", study.best_trial.params)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:06:24.29814Z","iopub.execute_input":"2022-06-02T14:06:24.298882Z","iopub.status.idle":"2022-06-02T14:08:54.485208Z","shell.execute_reply.started":"2022-06-02T14:06:24.298847Z","shell.execute_reply":"2022-06-02T14:08:54.484182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hypertune LGBM\nfrom optuna.integration import LightGBMPruningCallback\nfrom lightgbm import early_stopping\n\ndef objective(trial, data=X_train , target=y_train):\n\n    param = {\n        'n_estimators': 5000,\n        'num_leaves': trial.suggest_int('num_leaves', 20, 3000, step=20),\n#         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 200, 10000, step=100),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 15),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10),\n        'subsample': trial.suggest_categorical('subsample', [0.5, 0.6, 0.7, 0.8, 0.9])\n    }    \n    \n    cv = StratifiedKFold( n_splits=5, shuffle=True, random_state=42)\n    \n    for idx, (train_idx, test_idx) in enumerate(cv.split(X_train, y_train)):\n        \n        x_trn, x_val = X_train.iloc[train_idx], X_train.iloc[test_idx]\n        y_trn, y_val = y_train.iloc[train_idx], y_train.iloc[test_idx]\n\n    \n        model = LGBMClassifier(**param)\n        \n        model.fit(x_trn,\n                  y_trn,\n                  eval_set = [(x_val, y_val)],\n                  eval_metric = 'logloss',\n                  callbacks = [early_stopping(stopping_rounds=100,\n                                              verbose = False)],\n                  verbose = False\n                 )\n    \n        preds = model.predict(x_val)\n        scores = accuracy_score(y_val, preds)\n        \n    return np.mean(scores)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:08:54.487383Z","iopub.execute_input":"2022-06-02T14:08:54.488189Z","iopub.status.idle":"2022-06-02T14:08:54.512572Z","shell.execute_reply.started":"2022-06-02T14:08:54.488117Z","shell.execute_reply":"2022-06-02T14:08:54.511361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=30)\n\n# print(f\"\\tBest score: {study.best_value:.5f}\")\n# print(f\"\\tBest params:\", study.best_trial.params)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:08:54.514332Z","iopub.execute_input":"2022-06-02T14:08:54.514951Z","iopub.status.idle":"2022-06-02T14:10:40.6157Z","shell.execute_reply.started":"2022-06-02T14:08:54.514902Z","shell.execute_reply":"2022-06-02T14:10:40.614601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tuned models\nxgb_params = {'learning_rate': 0.03824797490742583, 'max_depth': 10, 'min_child_weight': 9, 'subsample': 0.5}\nlgbm_params = {'num_leaves': 940, 'learning_rate': 0.02496339867162098, 'max_depth': 5, 'min_child_samples': 7, 'reg_alpha': 1.1993698456266415, 'reg_lambda': 0.0012967298146930883, 'subsample': 0.6}","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:14:26.584355Z","iopub.execute_input":"2022-06-02T14:14:26.58464Z","iopub.status.idle":"2022-06-02T14:14:26.593556Z","shell.execute_reply.started":"2022-06-02T14:14:26.584607Z","shell.execute_reply":"2022-06-02T14:14:26.590277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble Model","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nxgb_optimal = XGBClassifier(**xgb_params)\nlgbm_optimal = LGBMClassifier(**lgbm_params)\n\n# Create Hard Voting Classifier\nEnsemble_HV = VotingClassifier(estimators= [('SVC', SVC()),\n                                           ('XBG', xgb_optimal),\n                                           ('LGBM', lgbm_optimal)],\n                              voting = 'hard')\n\n# Create Soft Voting Classifier\nEnsemble_SV = VotingClassifier(estimators= [('SVC', SVC()),\n                                           ('XBG', xgb_optimal),\n                                           ('LGBM', lgbm_optimal)],\n                              voting = 'soft')\n\n# Return Accuracy Scores\ncv_HV = cross_val_score(Ensemble_HV, X_train, y_train, scoring='accuracy')\ncv_SV = cross_val_score(Ensemble_SV, X_train, y_train, scoring='accuracy')\n\nprint('Hard Voting Classifier:' , cv_HV.mean())\nprint('Soft Voting Classifier:' , cv_SV.mean())","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:14:59.144938Z","iopub.execute_input":"2022-06-02T14:14:59.145368Z","iopub.status.idle":"2022-06-02T14:15:30.929882Z","shell.execute_reply.started":"2022-06-02T14:14:59.145305Z","shell.execute_reply":"2022-06-02T14:15:30.928814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Defining a function to predict solutions\ndef predict(model):\n    model.fit(X_train, y_train)\n    Y_pred = model.predict(X_test)\n    pred = pd.DataFrame({\n    'PassengerId': test_df_copy['PassengerId'],\n    'Transported': Y_pred\n})\n    return pred","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:15:44.615914Z","iopub.execute_input":"2022-06-02T14:15:44.616213Z","iopub.status.idle":"2022-06-02T14:15:44.622298Z","shell.execute_reply.started":"2022-06-02T14:15:44.61618Z","shell.execute_reply":"2022-06-02T14:15:44.621242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb = XGBClassifier()\nlgbm = LGBMClassifier()\ncat = CatBoostClassifier()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:15:55.862972Z","iopub.execute_input":"2022-06-02T14:15:55.863866Z","iopub.status.idle":"2022-06-02T14:15:55.874846Z","shell.execute_reply.started":"2022-06-02T14:15:55.863828Z","shell.execute_reply":"2022-06-02T14:15:55.873063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(lgbm).to_csv('submission_lgbm.csv', index=False)\npredict(xgb).to_csv('submission_xgb.csv', index=False)\npredict(lgbm_optimal).to_csv('submission_lgbm_optimal.csv', index=False)\npredict(xgb_optimal).to_csv('submission_xgb_optimal.csv', index=False)\npredict(Ensemble_HV).to_csv('submission_Ensemble_HV.csv', index=False)\npredict(Ensemble_SV).to_csv('submission_Ensemble_SV.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:16:47.508861Z","iopub.execute_input":"2022-06-02T14:16:47.509189Z","iopub.status.idle":"2022-06-02T14:17:01.094899Z","shell.execute_reply.started":"2022-06-02T14:16:47.509153Z","shell.execute_reply":"2022-06-02T14:17:01.093641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## My Other Works\nCheck out my KAGGLE profile and my other contributions\n- [Attack-on-Titanic Solution Walkthrough | Kaggle](https://www.kaggle.com/code/shilongzhuang/attack-on-titanic-solution-walkthrough-top-8)\n- [Plotly Advanced Charts: EDA on Unicorn Startups](https://www.kaggle.com/code/shilongzhuang/plotly-advanced-charts-eda-on-unicorn-startups)","metadata":{}},{"cell_type":"markdown","source":"---\n# References\n\nSpecial thanks and credits to these awesome and comprehensively informative resources (notebooks) and guides created by talented professionals in the field, which were able to guide me while creating this work.\n\n- [A Data Science Framework: To achieve 99% Accuracy](https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy)\n- [Spaceship Titanic: A complete guide](https://www.kaggle.com/code/samuelcortinhas/spaceship-titanic-a-complete-guide)","metadata":{}}]}