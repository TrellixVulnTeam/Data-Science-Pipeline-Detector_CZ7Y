{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Table of Contents\n<a id=\"table-of-contents\"></a>\n- [1 What is TrueFoundry?](#1)\n- [2 Spaceship Titanic](#2)\n- [3 Preparation](#3)\n    - [3.1 Essential Packages](#3.1)\n    - [3.2 TrueFoundry Packages & Login](#3.2)\n- [4 Data Loading and Preprocessing](#4)\n- [5 Models](#5)\n    - [5.1 XGBoost Classifier](#5.1)\n    - [5.2 LGBM Classifier](#5.2)\n    - [5.3 Catboost Classifier](#5.3)\n- [6 TrueFoundry Results](#6)\n    - [6.1 Projects](#6.1)\n    - [6.2 Runs](#6.2)\n        - [6.2.1 Overview](#6.2.1)\n        - [6.2.2 Run Metrics](#6.2.2)\n        - [6.2.3 Data & Feature Metrics](#6.2.3)\n        - [6.2.4 General Artifact](#6.2.4)\n    - [6.3 Models Comparison](#6.3)","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"1\"></a>\n# 1 What is TrueFoundry? \n\nTruefoundry aims to provide the different components in a Machine learning stack - all wound together in a way that they talk to each other seamlessly and teams don't have to spend time glueing pieces together. While all the pieces are knit tightly together, we also design all the components in a way that they can be seamlessly integrated with other tools in the future. TrueFoundry can be accessed at https://app.truefoundry.com/mlfoundry.\n\nTruefoundry comprises of the following pieces to tie together things seamlessly:\n1. `MlFoundry`: used during model training to log your model artifacts, parameters, data & code so as to be able to collaborate with your team and reproduce Machine Learning Experiments.\n2. `ServiceFoundry`: single API which containerizes and deploys your model to a managed Kubernetes Cluster. This also generates a Grafana cluster with complete visibility of your Service Health, System Logs, and Kubernetes Workspace.\n3. `Monitoring`: model input-output monitoring, data drift charts, and root-cause analysis when things break. Coming soon! ","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"2\"></a>\n# 2 Spaceship Titanic \nThe Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars.\n\nWhile rounding Alpha Centauri en route to its first destination—the torrid 55 Cancri E—the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!\n\nTo help rescue crews and retrieve the lost passengers, you are challenged to predict which passengers were transported by the anomaly using records recovered from the spaceship’s damaged computer system.\n\nHelp save them and change history!","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"3\"></a>\n# 3 Preparation\nPrepare packages and data that will be used in the analysis process and we will use `TrueFoundry` to track our experiments and `Essential packages` that will be loaded are mainly for data manipulation, data visualization and modeling. There are 2 datasets that are used in the analysis, they are train and test dataset. The main use of train dataset is to train models and use it to predict test dataset. While sample submission file is used to informed participants on the expected submission for the competition.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.1\"></a>\n## 3.1 Essential Packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn\nfrom IPython.display import Image\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LinearRegression\nfrom scipy.stats import boxcox\nfrom xgboost import XGBClassifier \nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier","metadata":{"execution":{"iopub.status.busy":"2022-06-20T08:05:56.61777Z","iopub.execute_input":"2022-06-20T08:05:56.618158Z","iopub.status.idle":"2022-06-20T08:05:56.624702Z","shell.execute_reply.started":"2022-06-20T08:05:56.618126Z","shell.execute_reply":"2022-06-20T08:05:56.623538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"api_key\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-20T01:57:12.89041Z","iopub.execute_input":"2022-06-20T01:57:12.890948Z","iopub.status.idle":"2022-06-20T01:57:13.684013Z","shell.execute_reply.started":"2022-06-20T01:57:12.890901Z","shell.execute_reply":"2022-06-20T01:57:13.672533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n## 3.2 TrueFoundry Packages & Login\nIn the first step we will install `TrueFoundry` using `pip install mlfoundry` and import it is as `mlf`.","metadata":{}},{"cell_type":"code","source":"!pip install mlfoundry\nimport mlfoundry as mlf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-20T01:57:13.695824Z","iopub.execute_input":"2022-06-20T01:57:14.050388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Login to `TrueFoundry` platform and get the API key. `Create New API keys` if we don't have it or copy the generated API keys before. API key can be found at https://app.truefoundry.com/settings.","metadata":{}},{"cell_type":"code","source":"Image(\"../input/experimenttracking-truefoundry/API_Key.png\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will login using `mlf.get_client` and use pass our API Key on `api_key`. We also create a `project name`.","metadata":{}},{"cell_type":"code","source":"client = mlf.get_client(api_key=secret_value_0)\nproject_name = 's-titanic'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"4\"></a>\n# 4 Data Loading and Preprocessing\nThere are 3 thing that will be done in this section:\n1. Load `train` dataset.\n2. Convert target variable `Transported`, from `True` and `False` to `1` and `0` using `label encoder`. \n3. `Categorical` features will also be converted using `label encoder`.","metadata":{}},{"cell_type":"code","source":"folds = 5\ntrain_load = pd.read_csv(\"../input/spaceship-titanic/train.csv\")\n\nfeatures = [col for col in train_load.columns if col not in ['PassengerId', 'Transported', 'Name']]\ncat_features = ['HomePlanet', 'CryoSleep', 'VIP', 'Destination', 'Cabin']\ncont_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n\n# convert Transported into 0 and 1\nle = preprocessing.LabelEncoder()\ntrain_load['Transported'] = le.fit_transform(train_load['Transported'])\n\n# convert categorical features\nfor feature in cat_features:\n    le = preprocessing.LabelEncoder()\n    train_load[feature] = le.fit_transform(train_load[feature])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"5\"></a>\n# 5 Models\nWe are going to use 3 models: `XGBoost Classifier`, `Catboost Classifier` and `LGBM Classifier`. We will also create `5 folds` cross validation. We will not perform any hyperparameters tuning. \n\n**Observations:**\n- It seems that `Catboost Classifer` has the best OOF score of `0.8029448981939492`.\n- The second best performer is `LGBM Classifier` with OOF score of `0.8019095824226389`.\n- `Catboost Classifer` and `LGBM Classifer` performance are really close. If we inspect in each folds, `Catboost Classifier` has beaten `LGBM Classifier` in fold `0, 1 and 2` while in fold `3 and 4` `LGBM Classifier` performed better than `Catboost Classifier`.\n\nWe can improve the results by performing `features engineering` and `hyperparameters tuning`.\n\n**TrueFoundry Experiment Tracking**\n\nWe will track our experiment using `TrueFoundry`. Below are some explanations in the code related to `TrueFoundry` experiment tracking. We are using `xgboost classifier` as an example and consistently being used for other models:\n- Create `run = client.create_run(project_name=project_name, run_name=\"xgboost\")` to start logging our experiment by creating project name and the run name. In this case we log the project name as `s-titanic` that has been setup before and naming our run as `xgboost` for XGBoost Classifier model.\n- We can track our dataset including `target prediction` and `target actual` using `run.log_dataset(features=train_df[features], dataset_name=\"full\", actuals=train_df['Transported'], predictions=train_oof)`. Logging our `actual` and `prediction` target will help us to compare them in `TrueFoundry` platform. This line of codes are putted the end of the code as we will need to wait until all the prediction in each fold finished.   \n- We will also do the same thing for each fold-dataset, we use`run.log_dataset(dataset_name=\"fold_\"+str(fold), features=X_valid, actuals=y_valid, predictions=temp_oof)` but we will perform this after a fold prediction finished.\n- To log hyperparameters from the model, we use `run.log_params(model.get_xgb_params())`. We can only log 1 set hyperparameters, that's why we put it at the end of the code.\n- We can also log our validation accuracy metrics over time using below code:<br>\n    `results = model.evals_result()`<br>\n    `epochs = len(results['validation_0']['error'])`<br>\n    `accuracy_fold = [1-err for err in results['validation_0']['error']]`<br>\n    `for global_step in range(epochs):`<br>\n    `run.log_metrics(metric_dict={f'Accuracy_fold_{fold}':accuracy_fold[global_step]}, step=global_step)`<br>\n  We can compare the `accuracy` of each `fold` and `OOF` accuracy across all model over time.\n- The last but not least, we need to end our run using `run.end()`.\n\nIt may be easier to understand after seeing the code in practice and a comment of `#TrueFoundry` at the end of the code.","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"5.1\"></a>\n## 5.1 XGBoost Classifier\nBelow are the results from XGBoost Classifier:\n\n- Fold 0 Accuracy:  0.79700977573318\n- Fold 1 Accuracy:  0.7786083956296722\n- Fold 2 Accuracy:  0.7912593444508338\n- Fold 3 Accuracy:  0.7957422324510932\n- Fold 4 Accuracy:  0.7957422324510932\n- OOF AUC:  0.7916714597952376","metadata":{}},{"cell_type":"code","source":"run = client.create_run(project_name=project_name, run_name=\"xgboost\") #TrueFoundry\n\n# Load and Prep Data\ntrain_df = train_load.copy()\n\ntrain_oof = np.zeros((8693,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['Transported'])):\n    train, valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = train['Transported']\n    y_valid = valid['Transported']\n    X_train = train[features]\n    X_valid = valid[features]\n    \n    model = XGBClassifier(random_state=42, verbosity=0)\n    model = model.fit(X_train, y_train, eval_metric=\"error\", eval_set=[(X_valid, y_valid)], verbose=0)\n    \n    temp_oof = model.predict(X_valid)\n    train_oof[valid_idx] = temp_oof    \n    \n    print(f'Fold {fold} Accuracy: ', accuracy_score(y_valid, temp_oof))\n    \n    results = model.evals_result()\n    epochs = len(results['validation_0']['error'])\n    accuracy_fold = [1-err for err in results['validation_0']['error']]\n    for global_step in range(epochs):\n        run.log_metrics(metric_dict={f'Accuracy_fold_{fold}':accuracy_fold[global_step]}, step=global_step) #TrueFoundry\n    run.log_dataset(dataset_name=\"fold_\"+str(fold), features=X_valid, actuals=y_valid, predictions=temp_oof) #TrueFoundry\n    \naccuracy_final = accuracy_score(train_df['Transported'], train_oof)    \nprint(f'OOF AUC: ', accuracy_final)\n\nrun.log_params(model.get_params()) #TrueFoundry\nrun.log_metrics(metric_dict={'Accuracy_OOF':accuracy_final}) #TrueFoundry\nrun.log_dataset(features=train_df[features], dataset_name=\"full\", actuals=train_df['Transported'], predictions=train_oof) #TrueFoundry\n\nrun.end() #TrueFoundry","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"5.2\"></a>\n## 5.2 LGBM Classifier\nBelow are the results from LGBM Classifier:\n\n- Fold 0 Accuracy:  0.8021851638872916\n- Fold 1 Accuracy:  0.7906843013225991\n- Fold 2 Accuracy:  0.8062104657849338\n- Fold 3 Accuracy:  0.8072497123130035\n- Fold 4 Accuracy:  0.8032220943613348\n- OOF AUC:  0.8019095824226389","metadata":{}},{"cell_type":"code","source":"run = client.create_run(project_name=project_name, run_name=\"lgbm\") #TrueFoundry\n\n# Load and Prep Data\ntrain_df = train_load.copy()\n\ntrain_oof = np.zeros((8693,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['Transported'])):\n    train, valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = train['Transported']\n    y_valid = valid['Transported']\n    X_train = train[features]\n    X_valid = valid[features]\n\n    model = LGBMClassifier(random_state=42, verbosity=0)\n    model = model.fit(X_train, y_train, eval_metric=\"binary_error\", eval_set=[(X_valid, y_valid)], verbose=0)\n\n    temp_oof = model.predict(X_valid)\n    train_oof[valid_idx] = temp_oof\n    \n    print(f'Fold {fold} Accuracy: ', accuracy_score(y_valid, temp_oof))\n    \n    results = model.evals_result_\n    epochs = len(results['valid_0']['binary_error'])\n    accuracy_fold = [1-err for err in results['valid_0']['binary_error']]\n    for global_step in range(epochs):\n        run.log_metrics(metric_dict={f'Accuracy_fold_{fold}':accuracy_fold[global_step]}, step=global_step) #TrueFoundry\n    run.log_dataset(dataset_name=\"fold_\"+str(fold), features=X_valid, actuals=y_valid, predictions=temp_oof) #TrueFoundry\n    \naccuracy_final = accuracy_score(train_df['Transported'], train_oof)    \nprint(f'OOF AUC: ', accuracy_final)\n\nrun.log_params(model.get_params()) #TrueFoundry\nrun.log_metrics(metric_dict={'Accuracy_OOF':accuracy_final}) #TrueFoundry\nrun.log_dataset(features=train_df[features], dataset_name=\"full\", actuals=train_df['Transported'], predictions=train_oof) #TrueFoundry\n\nrun.end() #TrueFoundry","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"5.3\"></a>\n## 5.3 Catboost Classifier\nBelow are the results from Catboost Classifier:\n\n- Fold 0 Accuracy:  0.8113858539390454\n- Fold 1 Accuracy:  0.8021851638872916\n- Fold 2 Accuracy:  0.8102357676825762\n- Fold 3 Accuracy:  0.7968929804372842\n- Fold 4 Accuracy:  0.7940161104718066\n- OOF AUC:  0.8029448981939492","metadata":{}},{"cell_type":"code","source":"run = client.create_run(project_name=project_name, run_name=\"catboost\") #TrueFoundry\n\n# Load and Prep Data\ntrain_df = train_load.copy()\n\ntrain_oof = np.zeros((8693,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['Transported'])):\n    train, valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = train['Transported']\n    y_valid = valid['Transported']\n    X_train = train[features]\n    X_valid = valid[features]\n    \n    model = CatBoostClassifier(iterations=100, eval_metric='Accuracy', random_state=42)\n    model = model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], use_best_model=False, verbose=0)\n    \n    temp_oof = model.predict(X_valid)\n    train_oof[valid_idx] = temp_oof    \n    \n    print(f'Fold {fold} Accuracy: ', accuracy_score(y_valid, temp_oof))\n    \n    results = model.get_evals_result()\n    epochs = len(results['validation']['Accuracy'])\n    for global_step in range(epochs):\n        run.log_metrics(metric_dict={f'Accuracy_fold_{fold}':results['validation']['Accuracy'][global_step]}, step=global_step) #TrueFoundry\n    run.log_dataset(dataset_name=\"fold_\"+str(fold), features=X_valid, actuals=y_valid, predictions=temp_oof) #TrueFoundry\n    \naccuracy_final = accuracy_score(train_df['Transported'], train_oof)    \nprint(f'OOF AUC: ', accuracy_final)\n\nrun.log_params(model.get_params()) #TrueFoundry\nrun.log_metrics(metric_dict={'Accuracy_OOF':accuracy_final}) #TrueFoundry\nrun.log_dataset(features=train_df[features], dataset_name=\"full\", actuals=train_df['Transported'], predictions=train_oof) #TrueFoundry\n\nrun.end() #TrueFoundry","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"6\"></a>\n# 6 TrueFoundry Platform\nIn this section we will see how our `dataset`, `hyperparameters` and `metrics` have been logged in TrueFoundry. We will see into 2 sections: `Projects` and `Runs`.","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"6.1\"></a>\n## 6.1 Projects\n\nWe can check all of our projects in `ML Foundry` section. In this case we are looking for our `s-titanic` projects.","metadata":{}},{"cell_type":"code","source":"Image(\"../input/experimenttracking-truefoundry/all_projects.png\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T08:06:26.488366Z","iopub.execute_input":"2022-06-20T08:06:26.488774Z","iopub.status.idle":"2022-06-20T08:06:26.50687Z","shell.execute_reply.started":"2022-06-20T08:06:26.488741Z","shell.execute_reply":"2022-06-20T08:06:26.506041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By clicking our project we will see our `Run`. We will see 3 runs that have been logged before which are `catboost`, `lgbm` and `xgboost`.","metadata":{}},{"cell_type":"code","source":"Image(\"../input/experimenttracking-truefoundry/all_runs.png\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T08:06:35.727911Z","iopub.execute_input":"2022-06-20T08:06:35.728776Z","iopub.status.idle":"2022-06-20T08:06:35.781678Z","shell.execute_reply.started":"2022-06-20T08:06:35.728724Z","shell.execute_reply":"2022-06-20T08:06:35.780614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"6.2\"></a>\n## 6.2 Runs\nLet's check our `xgboost` run. In the upper side, we can see our `Run Name`, `Run Id`, `Author`, `Status`, `Last Updated On`, `Tags` and `Run Duration`. We can also put `tags` and `notes`.\n\n<a id=\"6.2.1\"></a>\n### 6.2.1 Overview\n- In the left side, we can see our log `Key Metrics` which is `accuracy`. It logs `fold_0` through `fold_4` including `OOF` accuracy metrics.\n- In the right side, we can see our `hyperparameters` that has been logged. It also represents the latest hyperparameters and it would be the same accross the folds.","metadata":{}},{"cell_type":"code","source":"Image(\"../input/experimenttracking-truefoundry/xgboost_overview.png\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T08:06:44.619921Z","iopub.execute_input":"2022-06-20T08:06:44.620508Z","iopub.status.idle":"2022-06-20T08:06:44.692405Z","shell.execute_reply.started":"2022-06-20T08:06:44.620475Z","shell.execute_reply":"2022-06-20T08:06:44.691182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.2.2\"></a>\n### 6.2.2 Run Metrics\nWe can see each of our `validation` fold (0 through 4) accuracy in a line graph. Meaning we can see how it performs in each steps.","metadata":{}},{"cell_type":"code","source":"Image(\"../input/experimenttracking-truefoundry/xgboost_run_metrics.png\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T08:08:33.899071Z","iopub.execute_input":"2022-06-20T08:08:33.899498Z","iopub.status.idle":"2022-06-20T08:08:33.978805Z","shell.execute_reply.started":"2022-06-20T08:08:33.899466Z","shell.execute_reply":"2022-06-20T08:08:33.977855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.2.3\"></a>\n### 6.2.3 Data & Feature Metrics\nWe can see our folds dataset, in this case we can see `fold_0` through `fold_4` and `full` dataset as we have logged them before.\nWe can also see more details on each features by clicking the `Details`.","metadata":{}},{"cell_type":"code","source":"Image(\"../input/experimenttracking-truefoundry/xgboost_data.png\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T08:10:53.343949Z","iopub.execute_input":"2022-06-20T08:10:53.344372Z","iopub.status.idle":"2022-06-20T08:10:53.403189Z","shell.execute_reply.started":"2022-06-20T08:10:53.344332Z","shell.execute_reply":"2022-06-20T08:10:53.401254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also see the comparsion between our predictions and actual performance for each fold (0 to 4) and also in the full dataset.","metadata":{}},{"cell_type":"code","source":"Image(\"../input/experimenttracking-truefoundry/xgboost_data_2.png\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T08:12:02.851197Z","iopub.execute_input":"2022-06-20T08:12:02.851616Z","iopub.status.idle":"2022-06-20T08:12:02.929766Z","shell.execute_reply.started":"2022-06-20T08:12:02.851585Z","shell.execute_reply":"2022-06-20T08:12:02.928738Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6.2.4\"></a>\n### 6.2.4 General Artifact\nIn here we can see our dataset that have been stored in `csv format` and can be re-downloaded.","metadata":{}},{"cell_type":"code","source":"Image(\"../input/experimenttracking-truefoundry/xgboost_artifact.png\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T08:12:23.123789Z","iopub.execute_input":"2022-06-20T08:12:23.12419Z","iopub.status.idle":"2022-06-20T08:12:23.180711Z","shell.execute_reply.started":"2022-06-20T08:12:23.124159Z","shell.execute_reply":"2022-06-20T08:12:23.17983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"6.3\"></a>\n## 6.3 Models Comparison\nWe can perform model comparison by clicking all the models that we want to compare. In this case we choose all of them.","metadata":{}},{"cell_type":"code","source":"Image(\"../input/experimenttracking-truefoundry/models_comparison_1.png\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T08:12:29.295064Z","iopub.execute_input":"2022-06-20T08:12:29.296032Z","iopub.status.idle":"2022-06-20T08:12:29.318697Z","shell.execute_reply.started":"2022-06-20T08:12:29.295961Z","shell.execute_reply":"2022-06-20T08:12:29.317996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the comparison of our models (catboost, lgbm and xgboost) in a line graph based on the `validation accuracy` metric for each fold. By hovering into the line, we can see the `accuracy` for each model steps in each fold.","metadata":{}},{"cell_type":"code","source":"Image(\"../input/experimenttracking-truefoundry/models_comparison_2.png\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-20T08:14:45.309666Z","iopub.execute_input":"2022-06-20T08:14:45.310144Z","iopub.status.idle":"2022-06-20T08:14:45.379028Z","shell.execute_reply.started":"2022-06-20T08:14:45.310107Z","shell.execute_reply":"2022-06-20T08:14:45.378159Z"},"trusted":true},"execution_count":null,"outputs":[]}]}