{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle Competition: Space Titanic","metadata":{}},{"cell_type":"markdown","source":"## Look at the Big Picture","metadata":{}},{"cell_type":"markdown","source":"### Frame the Problem","metadata":{}},{"cell_type":"markdown","source":"Let us first define the nature of the problem. Since we want to predict which passengers were transported to an alternate dimension, this is a binary classification problem. Since the dataset involves both features and labels, supervising machine learning techniques are used. Batch learning is used since the dataset is prepared to be used.","metadata":{}},{"cell_type":"markdown","source":"### Select a Performance Metric","metadata":{}},{"cell_type":"markdown","source":"Since this is a binary classification problem, we can use accuracy or ROC-AUC score to assess the performance of the models.\n\n- ROC-AUC score is a more comprehensive performance metric, since it is calculated independent of the threshold.","metadata":{}},{"cell_type":"markdown","source":"### Check the Assumptions","metadata":{}},{"cell_type":"markdown","source":"Now Let's check the assumptions. By looking at the training data and the test data, we can see that the label are True/False values, that means it is a binary classification task.","metadata":{}},{"cell_type":"markdown","source":"## Get the Data","metadata":{}},{"cell_type":"markdown","source":"We obtain the data from Kaggle competition website.","metadata":{}},{"cell_type":"markdown","source":"## Discover and Visualize the Data to Gain Insights","metadata":{}},{"cell_type":"markdown","source":"First we load the data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ntrain = pd.read_csv('../input/spaceship-titanic/train.csv')\ntest = pd.read_csv('../input/spaceship-titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:45.042667Z","iopub.execute_input":"2022-06-15T01:00:45.043119Z","iopub.status.idle":"2022-06-15T01:00:45.113438Z","shell.execute_reply.started":"2022-06-15T01:00:45.04308Z","shell.execute_reply":"2022-06-15T01:00:45.111818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:45.115846Z","iopub.execute_input":"2022-06-15T01:00:45.116301Z","iopub.status.idle":"2022-06-15T01:00:45.142151Z","shell.execute_reply.started":"2022-06-15T01:00:45.116264Z","shell.execute_reply":"2022-06-15T01:00:45.140596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We use info() method to check the details of each variable. We can see that there are 14 columns altogether of which 13 of them are features and 1 of them is label. 7 of the data types of the features are 'object', while 6 of them are 'float64'.","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:45.144078Z","iopub.execute_input":"2022-06-15T01:00:45.144508Z","iopub.status.idle":"2022-06-15T01:00:45.175831Z","shell.execute_reply.started":"2022-06-15T01:00:45.144471Z","shell.execute_reply":"2022-06-15T01:00:45.174737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:45.17821Z","iopub.execute_input":"2022-06-15T01:00:45.178872Z","iopub.status.idle":"2022-06-15T01:00:45.219755Z","shell.execute_reply.started":"2022-06-15T01:00:45.178831Z","shell.execute_reply":"2022-06-15T01:00:45.218495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:45.221367Z","iopub.execute_input":"2022-06-15T01:00:45.221821Z","iopub.status.idle":"2022-06-15T01:00:45.229367Z","shell.execute_reply.started":"2022-06-15T01:00:45.221784Z","shell.execute_reply":"2022-06-15T01:00:45.228411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the histogram below, we can see that the 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', and 'VRDeck' are right skewed.","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nfrom matplotlib import pyplot as plt\ntrain.hist(log=True)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:45.230532Z","iopub.execute_input":"2022-06-15T01:00:45.23219Z","iopub.status.idle":"2022-06-15T01:00:46.465032Z","shell.execute_reply.started":"2022-06-15T01:00:45.232108Z","shell.execute_reply":"2022-06-15T01:00:46.463973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:46.466348Z","iopub.execute_input":"2022-06-15T01:00:46.466757Z","iopub.status.idle":"2022-06-15T01:00:46.487071Z","shell.execute_reply.started":"2022-06-15T01:00:46.466716Z","shell.execute_reply":"2022-06-15T01:00:46.485753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Construct a correlation matrix. If there are null values, corr() remove the pairwise correlations.\n\n- https://stackoverflow.com/questions/57155427/how-does-corr-remove-na-and-null-values","metadata":{}},{"cell_type":"code","source":"train.corr()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:46.488868Z","iopub.execute_input":"2022-06-15T01:00:46.489607Z","iopub.status.idle":"2022-06-15T01:00:46.513208Z","shell.execute_reply.started":"2022-06-15T01:00:46.489553Z","shell.execute_reply":"2022-06-15T01:00:46.511708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix = train.corr()\ncorr_matrix['Transported'].sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:46.515292Z","iopub.execute_input":"2022-06-15T01:00:46.516147Z","iopub.status.idle":"2022-06-15T01:00:46.53013Z","shell.execute_reply.started":"2022-06-15T01:00:46.516104Z","shell.execute_reply":"2022-06-15T01:00:46.529054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Construct scatter matrix for further inspection","metadata":{}},{"cell_type":"code","source":"from pandas.plotting import scatter_matrix\nattributes = ['Age', 'VRDeck', 'Spa', 'RoomService']\nscatter_matrix(train[attributes], figsize=(12,12))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:46.534013Z","iopub.execute_input":"2022-06-15T01:00:46.534972Z","iopub.status.idle":"2022-06-15T01:00:48.218128Z","shell.execute_reply.started":"2022-06-15T01:00:46.534928Z","shell.execute_reply":"2022-06-15T01:00:48.216846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the Data for Machine Learning Algorithms","metadata":{}},{"cell_type":"markdown","source":"Separate the features and labels\n\nDrop the Passenger Id and the Name since they are irrelevant.\n\nThe 'Cabin' variable is dropped for now. It will be used later.","metadata":{}},{"cell_type":"code","source":"X_train = train.drop(columns=['Transported','Name'])\ny_train = train['Transported']","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:48.219596Z","iopub.execute_input":"2022-06-15T01:00:48.220497Z","iopub.status.idle":"2022-06-15T01:00:48.229604Z","shell.execute_reply.started":"2022-06-15T01:00:48.220444Z","shell.execute_reply":"2022-06-15T01:00:48.228153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Cleaning\n\nFirst, we should decide how we would manage the missing data. From above we see that except for passenger ID and the label, each of the variables contains missing values.\n\n- For quantiative data, we fill the missing data during the data transformation process using SimpleImputer. Here we fill the missing values with the median.\n- For qualitative data, we also fill the missing data during the data transfomraiton process. We use Imputer class to fill the missing values with the most frequent category.","metadata":{}},{"cell_type":"code","source":"train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:48.231556Z","iopub.execute_input":"2022-06-15T01:00:48.232374Z","iopub.status.idle":"2022-06-15T01:00:48.26375Z","shell.execute_reply.started":"2022-06-15T01:00:48.232314Z","shell.execute_reply":"2022-06-15T01:00:48.262949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling Text and Categorical Variables\n\n- To handle categorigal variables, we can use one hot encoding during the data transformation process.\n- The 'Deck' and 'Side' categorical variables are extracted from the 'Cabin' variable.","metadata":{}},{"cell_type":"code","source":"X_train_copy = X_train.copy()\n\ndef df_transform(X):\n    X['Group'] = X['PassengerId'].str.split('_', expand=True).iloc[:,0]\n    group_group = X.groupby('Group')\n    group_size = group_group.apply(len)\n    X['GroupSize'] = X['Group'].replace(list(group_size.index), list(group_size.values))\n    X = X.drop(columns=['Group'])\n    X['Deck'] = X['Cabin'].str.split('/', expand=True).iloc[:,0]\n    X['Side'] = X['Cabin'].str.split('/', expand=True).iloc[:,2]\n    return X","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:48.265799Z","iopub.execute_input":"2022-06-15T01:00:48.266154Z","iopub.status.idle":"2022-06-15T01:00:48.276926Z","shell.execute_reply.started":"2022-06-15T01:00:48.266124Z","shell.execute_reply":"2022-06-15T01:00:48.275446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Custom Transformer","metadata":{}},{"cell_type":"markdown","source":"### Feature Scaling\n\n- All quantitative features will be standardized.","metadata":{}},{"cell_type":"markdown","source":"### Transformation Pipelines\n\n- After listing the strategies to handle missing data and categorical variales, we can now build a data transformation pipeline for preprocessing. we first build separate pipelines for quantiative and qualitative data, then we use column Transformer to combine the two pipelines into a full one.","metadata":{}},{"cell_type":"code","source":"num_attribs = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'GroupSize']\ncat_attribs = ['HomePlanet','Destination', 'CryoSleep', 'VIP', 'Deck', 'Side']","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:48.27885Z","iopub.execute_input":"2022-06-15T01:00:48.280109Z","iopub.status.idle":"2022-06-15T01:00:48.292536Z","shell.execute_reply.started":"2022-06-15T01:00:48.280059Z","shell.execute_reply":"2022-06-15T01:00:48.291572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('std_scaler', StandardScaler())\n])\n\ncat_pipeline = Pipeline([\n    ('oh_encoder', OneHotEncoder())\n])\n\nfull_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_attribs),\n    ('cat', cat_pipeline, cat_attribs)\n])\n\n# non-selected columns such as 'PassengerId' and 'Cabin' will be\n# dropped after passing through the column transformer\nX_train_copy_transformed = df_transform(X_train_copy)\nprint(X_train_copy_transformed.columns)\nX_train_copy_prepared = full_pipeline.fit_transform(X_train_copy_transformed)\nprint(X_train_copy_prepared[0])","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:48.294036Z","iopub.execute_input":"2022-06-15T01:00:48.29437Z","iopub.status.idle":"2022-06-15T01:00:50.75408Z","shell.execute_reply.started":"2022-06-15T01:00:48.29434Z","shell.execute_reply":"2022-06-15T01:00:50.752618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Select and Train a Model","metadata":{}},{"cell_type":"markdown","source":"### Training and Evaluating on the Training Set","metadata":{}},{"cell_type":"markdown","source":"Now we choose which classification models to use. Here we use the following classifiers:\n\n1. Random Forest Classifier\n2. Support Vector Classifier with linear kernel\n3. Support Vector Classifier with polynomial kernel\n4. Support Vector classifier with Guassian RBF kernel\n5. K Neighbours Classifier\n6. Logistic Regression\n7. SGD Classifier (Linear Support Vector Machine)\n8. Soft Voting Classifier (Random Forest, Logistic Regression, and Linear SVM)\n9. AdaBoost Classifier\n\nNote: for SVM, probability=True to generate prediction probability for voting classifier for the calculation of the ROC-AUC score.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nforest_clf = RandomForestClassifier(n_estimators=200, random_state=42)\nlinear_svm_clf = SVC(C=5, random_state=42)\npoly_svm_clf = SVC(kernel='poly', degree=3, coef0=1, probability=True)\nrbf_svm_clf = SVC(kernel='rbf', gamma='scale', C=5)\nknn_clf = KNeighborsClassifier()\nlog_clf = LogisticRegression(solver='sag', random_state=42)\nsgd_clf = SGDClassifier(loss='hinge', alpha=0.017, max_iter=1000, tol=1e-3, random_state=42)\nvoting_clf = VotingClassifier([\n    ('svm', poly_svm_clf),\n    ('log', log_clf),\n    ('for', forest_clf)\n], voting='soft')\nada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200,\n    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n\nalgorithms = [forest_clf, linear_svm_clf, poly_svm_clf, rbf_svm_clf,\n knn_clf, log_clf, sgd_clf, voting_clf, ada_clf]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:50.755323Z","iopub.status.idle":"2022-06-15T01:00:50.756365Z","shell.execute_reply.started":"2022-06-15T01:00:50.756033Z","shell.execute_reply":"2022-06-15T01:00:50.756065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculate cross validation scores for each of the classifiers\n\nAgain, ROC-AUC is used because this is a binary classification task.","metadata":{}},{"cell_type":"code","source":"X_train_transformed = df_transform(X_train)\nX_train_prepared = full_pipeline.fit_transform(X_train_transformed)\n\nfrom sklearn.model_selection import cross_val_score\n\nbest_mean = 0\nfor alg in algorithms:\n    alg_scores = cross_val_score(alg, X_train_prepared, y_train, scoring='roc_auc', cv=10)\n    print(f'Classifier: {str(alg)}')\n    print(f'Mean Score: {alg_scores.mean()}')\n    print(f'Standard Deviation: {alg_scores.std()}')\n    if alg_scores.mean() > best_mean:\n        best_mean = alg_scores.mean()\n        best_classifier = str(alg)\n\nprint(f'Best Model: {best_classifier}')\nprint(f'Best Model Mean Score: {best_mean}')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:50.758316Z","iopub.status.idle":"2022-06-15T01:00:50.759116Z","shell.execute_reply.started":"2022-06-15T01:00:50.758829Z","shell.execute_reply":"2022-06-15T01:00:50.758859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the cross validation results above, we found that the Voting Classifier performs the best. Therefore, we will fine tune the model using Grid Search.","metadata":{}},{"cell_type":"markdown","source":"## Fine Tune Your Model","metadata":{}},{"cell_type":"markdown","source":"### Grid Search\n\nLogistic Regression does not require grid search as it has no hyperparameters.","metadata":{}},{"cell_type":"markdown","source":"### Evaluate Your System on the Test Set","metadata":{}},{"cell_type":"code","source":"X_test = test.drop(columns=['Name'])\nX_test_transformed = df_transform(X_test)\nX_test_prepared = full_pipeline.fit_transform(X_test_transformed)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:50.760564Z","iopub.status.idle":"2022-06-15T01:00:50.761181Z","shell.execute_reply.started":"2022-06-15T01:00:50.760913Z","shell.execute_reply":"2022-06-15T01:00:50.760941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voting_clf.fit(X_train_prepared, y_train)\ny_test_pred = voting_clf.predict(X_test_prepared)\ny_test_pred","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:50.762528Z","iopub.status.idle":"2022-06-15T01:00:50.763107Z","shell.execute_reply.started":"2022-06-15T01:00:50.762832Z","shell.execute_reply":"2022-06-15T01:00:50.762864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final = pd.read_csv('../input/spaceship-titanic/sample_submission.csv')\nfinal.iloc[:,1] = y_test_pred\nfinal.to_csv('final_submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T01:00:50.765097Z","iopub.status.idle":"2022-06-15T01:00:50.765838Z","shell.execute_reply.started":"2022-06-15T01:00:50.765608Z","shell.execute_reply":"2022-06-15T01:00:50.765632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comments\n\n**Results without cabin information**\n\nLogistic Regression 0.79027\n\nVoting Classifier (Linear SVM, Logistic Regression, Random Forest): 0.78980\n\n**Results with cabin information**\n\nVoting Classifier (Linear SVM, Logistic Regression, Random Forest): 0.80289\n\n**Results with cabin information and groupsize**\n\nVoting Classifier (Polynomial SVM, Logistic Regression, Random Forest (n_estimators=200)): 0.80430","metadata":{}}]}