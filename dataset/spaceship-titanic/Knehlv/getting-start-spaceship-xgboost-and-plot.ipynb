{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport xgboost as xgb\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Dropout, Dense\nfrom keras.models import Sequential, load_model\nfrom keras.callbacks import EarlyStopping, Callback, LearningRateScheduler\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-30T03:02:11.535397Z","iopub.execute_input":"2022-06-30T03:02:11.535898Z","iopub.status.idle":"2022-06-30T03:02:24.732304Z","shell.execute_reply.started":"2022-06-30T03:02:11.53585Z","shell.execute_reply":"2022-06-30T03:02:24.731117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# overview \ntrain_data=pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\ntest_data=pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\ntest_id=test_data['PassengerId']\n\ntrain_data.drop(labels='Name',axis=1, inplace=True)\n#train_data.drop(labels='PassengerId',axis=1, inplace=True)\n\ntest_data.drop(labels='Name',axis=1, inplace=True)\n#test_data.drop(labels='PassengerId',axis=1, inplace=True)\n\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:31:09.000153Z","iopub.execute_input":"2022-06-30T03:31:09.000549Z","iopub.status.idle":"2022-06-30T03:31:09.073948Z","shell.execute_reply.started":"2022-06-30T03:31:09.000516Z","shell.execute_reply":"2022-06-30T03:31:09.072661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"要舍去的数据列为id，名字，也可以不舍去，没时间分析了就舍去吧","metadata":{}},{"cell_type":"code","source":"# show null presentage\n# train_data.dropna(axis=0, how='any', inplace=True)\nprint((train_data.isnull().sum()).sort_values(ascending=False))\nprint(train_data.shape)\nprint(test_data.info())","metadata":{"execution":{"iopub.status.busy":"2022-06-30T04:06:08.391859Z","iopub.execute_input":"2022-06-30T04:06:08.392227Z","iopub.status.idle":"2022-06-30T04:06:08.420172Z","shell.execute_reply.started":"2022-06-30T04:06:08.392196Z","shell.execute_reply":"2022-06-30T04:06:08.419363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"缺失的数据极少，可以用平均值,众数补全","metadata":{}},{"cell_type":"code","source":"# fix missing data\n# numeric use mean\n\ndef fix_missing(data):\n    numeric_cols = [column for column in data.select_dtypes([\"int\", \"float\"])]\n    for j in numeric_cols:\n        data[j].fillna(data[j].mean(), inplace=True)\n    categoric_cols = [column for column in data.select_dtypes(exclude = [\"int\", \"float\"])]\n    for j in categoric_cols:\n        data[j].fillna(data[j].value_counts().index[0], inplace=True)\n    return data\n        \ntrain_data_nonull = fix_missing(train_data)\ntest_data_nonull = fix_missing(test_data)\n#print((train_data.isnull().sum()).sort_values(ascending=False))\ntrain_data_nonull.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:07:04.274435Z","iopub.execute_input":"2022-06-30T03:07:04.274845Z","iopub.status.idle":"2022-06-30T03:07:04.358514Z","shell.execute_reply.started":"2022-06-30T03:07:04.274814Z","shell.execute_reply":"2022-06-30T03:07:04.357347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"处理数据，转化为int类型，某些特征因子化变成onehot形式，有些值太多暂时保留","metadata":{}},{"cell_type":"code","source":"\n# processing \ndef processing_data(data):\n    destination = pd.get_dummies(data['Destination'],prefix='Des')\n    data = pd.concat([data, destination], axis=1)\n    \n    home = pd.get_dummies(data['HomePlanet'], prefix='Home')\n    data = pd.concat([data, home], axis=1)\n    \n    data.drop(labels='Destination', axis=1, inplace=True)\n    data.drop(labels='HomePlanet', axis=1, inplace=True)\n    \n    data['group_num']=data['PassengerId'].apply(lambda x:x.split('_')[0]).astype('int')\n    data['in_group_num']=data['PassengerId'].apply(lambda x:x.split('_')[1]).astype('int')\n    \n    data['deck']=data['Cabin'].apply(lambda x:x.split('/')[0])\n    data['num']=data['Cabin'].apply(lambda x:x.split('/')[1])\n    data['side']=data['Cabin'].apply(lambda x:x.split('/')[2])\n    \n    data['num']=data['num'].astype('int')\n    \n    data['CryoSleep']=data['CryoSleep'].map({False:0, True:1})\n    data['VIP']=data['VIP'].map({False:0, True:1})\n    data['side']=data['side'].map({'P':0, 'S':1})\n    \n    data.drop(labels='Cabin', axis=1, inplace=True)\n    data.drop(labels='PassengerId', axis=1, inplace=True)\n    \n    deck = pd.get_dummies(data['deck'], prefix='deck')\n    data = pd.concat([data, deck], axis=1)\n    data.drop(labels='deck', axis=1, inplace=True)\n    \n    \n    return data\n\ndata_new_train = processing_data(train_data_nonull)\ndata_new_test = processing_data(test_data_nonull)\n\ndata_new_train.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:18:03.444636Z","iopub.execute_input":"2022-06-30T03:18:03.445025Z","iopub.status.idle":"2022-06-30T03:18:03.568178Z","shell.execute_reply.started":"2022-06-30T03:18:03.444994Z","shell.execute_reply":"2022-06-30T03:18:03.566838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"正则化","metadata":{}},{"cell_type":"code","source":"def scaler_data(train_data,test_data):\n    target_data = train_data['Transported']\n    feature_data = train_data.copy()\n    feature_data_t = test_data.copy()\n    feature_data.drop(labels='Transported', axis=1, inplace=True)\n    all_data = pd.concat([feature_data, feature_data_t])\n    \n    float_col=[col for col in all_data.select_dtypes([\"float\"])]\n    scaler = StandardScaler()\n    for j in float_col:\n        scaler_params = scaler.fit(all_data[j].values.reshape(-1, 1))\n        feature_data[j] = scaler.transform(feature_data[j].values.reshape(-1, 1), scaler_params)\n        feature_data_t[j] = scaler.transform(feature_data_t[j].values.reshape(-1, 1), scaler_params)\n    return feature_data,target_data,feature_data_t\n    \n    \nscaled_train_feature, scaled_train_target, scaled_test_feature = scaler_data(data_new_train,data_new_test)\nscaled_train_feature.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:07:12.825607Z","iopub.execute_input":"2022-06-30T03:07:12.826Z","iopub.status.idle":"2022-06-30T03:07:12.873164Z","shell.execute_reply.started":"2022-06-30T03:07:12.825967Z","shell.execute_reply":"2022-06-30T03:07:12.871962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"xgboost try","metadata":{}},{"cell_type":"code","source":"def xgboost_train_try(x,y):\n    x_train, x_test, y_train, y_test = train_test_split(x, y, \n                                                        test_size=0.1, \n                                                        shuffle=True, \n                                                        random_state = 1)\n    model = xgb.XGBClassifier(gamma = 1.5,\n                              subsample = 1.0,\n                              max_depth = 5,\n                              colsample_bytree = 1.0,\n                              n_estimators = 200,\n                              silent=1,\n                              objective='binary:logistic'\n                             )\n    model.fit(x_train,y_train,early_stopping_rounds=10, eval_metric='logloss', eval_set=[(x_test, y_test)],\n              verbose=True)\n    y_pred = model.predict(x_test)\n    acc = accuracy_score(y_test, y_pred)\n    xgb.plot_importance(model)\n    plt.show()\n    print(acc)\n    return model\ntry_model=xgboost_train_try(scaled_train_feature, scaled_train_target)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-30T03:07:16.320737Z","iopub.execute_input":"2022-06-30T03:07:16.321215Z","iopub.status.idle":"2022-06-30T03:07:18.103432Z","shell.execute_reply.started":"2022-06-30T03:07:16.321169Z","shell.execute_reply":"2022-06-30T03:07:18.102084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fully_xgboost(x,y):\n    model = xgb.XGBClassifier(gamma = 1.5,\n                              subsample = 1.0,\n                              max_depth = 5,\n                              colsample_bytree = 1.0,\n                              n_estimators = 50,\n                              objective='binary:logistic'\n                             )\n    model.fit(x,y)\n    return model\n\nxgbmodel=fully_xgboost(scaled_train_feature, scaled_train_target)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T01:55:25.039528Z","iopub.execute_input":"2022-06-30T01:55:25.03992Z","iopub.status.idle":"2022-06-30T01:55:25.457317Z","shell.execute_reply.started":"2022-06-30T01:55:25.039888Z","shell.execute_reply":"2022-06-30T01:55:25.456243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict with xgboost\nprediction=xgbmodel.predict(scaled_test_feature)\npred = pd.Series(prediction).map({0:False, 1:True})\nsubmission=pd.DataFrame({\"PassengerId\": test_id.values, \"Transported\": pred})\nsubmission.to_csv('submission_xgboost.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T01:55:35.661592Z","iopub.execute_input":"2022-06-30T01:55:35.661985Z","iopub.status.idle":"2022-06-30T01:55:35.688742Z","shell.execute_reply.started":"2022-06-30T01:55:35.661953Z","shell.execute_reply":"2022-06-30T01:55:35.687533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}