{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Space Titanic","metadata":{}},{"cell_type":"markdown","source":"This notebook will be done under the **OSEMN** framework. <br>\nHere is a quick recap:<br>\n**O** - Obtaining the data (Collect the data and transform it into suitable format) <br>\n**S** - Scrubbing / cleaning the data (Try to understand errors and handle missing values) <br>\n**E** - Exploring the data (Statistical analysis, visualisation, feature engineering) <br>\n**M** - Model training (Model training) <br>\n**N** - Interpretation (Evaluation of the model performance) <br>","metadata":{}},{"cell_type":"markdown","source":"# Obtaining the data\nAll the datasets on the Kaggle come in handy so this step is omitted.","metadata":{}},{"cell_type":"markdown","source":"# S is for scrubbing","metadata":{}},{"cell_type":"code","source":"space_titanic = pd.read_csv('../input/spaceship-titanic/train.csv')\nspace_titanic.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:00.525733Z","iopub.execute_input":"2022-04-24T14:41:00.526025Z","iopub.status.idle":"2022-04-24T14:41:00.572436Z","shell.execute_reply.started":"2022-04-24T14:41:00.525993Z","shell.execute_reply":"2022-04-24T14:41:00.571564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"space_titanic.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:00.776753Z","iopub.execute_input":"2022-04-24T14:41:00.777035Z","iopub.status.idle":"2022-04-24T14:41:00.81245Z","shell.execute_reply.started":"2022-04-24T14:41:00.777002Z","shell.execute_reply":"2022-04-24T14:41:00.811606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# count the proportion of zeros in all numerical columns\nspace_titanic.isnull().sum() / len(space_titanic)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:01.16013Z","iopub.execute_input":"2022-04-24T14:41:01.160402Z","iopub.status.idle":"2022-04-24T14:41:01.174777Z","shell.execute_reply.started":"2022-04-24T14:41:01.160374Z","shell.execute_reply":"2022-04-24T14:41:01.173972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that proportion of null values is about 2% in all numerical columns. At the same time we see that 75% quartile is low for all the numerical columns except ```Age``` which means that it's more than reasonable to impute null values insted of missing values. For the categorical columns we will use more sophisticated approach described below.\n","metadata":{}},{"cell_type":"code","source":"space_titanic.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:01.543825Z","iopub.execute_input":"2022-04-24T14:41:01.544115Z","iopub.status.idle":"2022-04-24T14:41:01.551302Z","shell.execute_reply.started":"2022-04-24T14:41:01.544082Z","shell.execute_reply":"2022-04-24T14:41:01.550547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\ndf = space_titanic.copy()\n\n\ndf['Age'] = df['Age'].fillna(df['Age'].mean())\n\nnum_cols = df.select_dtypes(include=['float64']).columns.tolist()\nnum_cols.remove(\"Age\")\n\ndf[num_cols] = df[num_cols].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:01.769506Z","iopub.execute_input":"2022-04-24T14:41:01.769792Z","iopub.status.idle":"2022-04-24T14:41:01.780435Z","shell.execute_reply.started":"2022-04-24T14:41:01.769757Z","shell.execute_reply":"2022-04-24T14:41:01.779622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Time to do some feature engineering. First what comes to mind is to split ```PassengerId``` into 2 columns and ```Cabin``` into 3 columns. <br>\nWe get feature names out of dataset description: <br>\n1. ```PassengerNum``` - just the ranking parameter for all groups of passengers (groups = families, etc.). \n2. ```PassengerGroup``` - number of the passenger within the group.\n3. ```CabinDeck``` - the deck on which passenger is staying.\n4. ```CabinNum``` - the cabin number where the passenger is staying.\n5. ```CabinSide``` - can be either port or starboard.","metadata":{}},{"cell_type":"code","source":"df[['PassengerNum', 'PassengerGroup']] = df.PassengerId.str.split('_', expand=True)\ndf[['CabinDeck', 'CabinNum', 'CabinSide']] = df.Cabin.str.split('/', expand=True)\ndf[['FName', 'SName']] = df.Name.str.split(' ', expand=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:02.009777Z","iopub.execute_input":"2022-04-24T14:41:02.010039Z","iopub.status.idle":"2022-04-24T14:41:02.056145Z","shell.execute_reply.started":"2022-04-24T14:41:02.010011Z","shell.execute_reply":"2022-04-24T14:41:02.055256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the total number of group members within each group and drop all the used columns. ","metadata":{}},{"cell_type":"code","source":"df['GroupCount'] = df['PassengerNum'].map(lambda x: (df['PassengerNum'] == x).sum())\n\ndf = df.drop(columns=['PassengerId', 'Cabin', 'Name'])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:02.256232Z","iopub.execute_input":"2022-04-24T14:41:02.259199Z","iopub.status.idle":"2022-04-24T14:41:08.919856Z","shell.execute_reply.started":"2022-04-24T14:41:02.259117Z","shell.execute_reply":"2022-04-24T14:41:08.919113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.SName.nunique() / len(df))\nprint(df.FName.nunique() / len(df))","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:08.921147Z","iopub.execute_input":"2022-04-24T14:41:08.921533Z","iopub.status.idle":"2022-04-24T14:41:08.932351Z","shell.execute_reply.started":"2022-04-24T14:41:08.921503Z","shell.execute_reply":"2022-04-24T14:41:08.931609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The proportion of unique names is low so I impute missing values in the ```FName``` & ```SName``` columns using the most frequent strategy.","metadata":{}},{"cell_type":"code","source":"name_cols = ['FName', 'SName']\nimputer = SimpleImputer(strategy='most_frequent').fit(df[name_cols])\ndf[name_cols] = imputer.transform(df[name_cols])","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:08.933363Z","iopub.execute_input":"2022-04-24T14:41:08.934148Z","iopub.status.idle":"2022-04-24T14:41:08.949294Z","shell.execute_reply.started":"2022-04-24T14:41:08.934112Z","shell.execute_reply":"2022-04-24T14:41:08.948525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adding an additional column to calculate the total expence \ndf['TotalExpence'] = df.RoomService + df.FoodCourt + df.ShoppingMall + df.Spa + df.VRDeck","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:08.950946Z","iopub.execute_input":"2022-04-24T14:41:08.951705Z","iopub.status.idle":"2022-04-24T14:41:08.965995Z","shell.execute_reply.started":"2022-04-24T14:41:08.951667Z","shell.execute_reply":"2022-04-24T14:41:08.964886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add total group expence\n# df_temp = df.copy()\n# df_temp['GroupExpence'] = df_temp['PassengerNum'].map(lambda x: x ** 2 if df_temp['PassengerNum'] == x)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:08.967311Z","iopub.execute_input":"2022-04-24T14:41:08.967881Z","iopub.status.idle":"2022-04-24T14:41:08.975058Z","shell.execute_reply.started":"2022-04-24T14:41:08.967837Z","shell.execute_reply":"2022-04-24T14:41:08.974346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data is now more clean and meaningful so let's move onto next step.","metadata":{}},{"cell_type":"markdown","source":"# E for exploartion and visualization","metadata":{}},{"cell_type":"code","source":"def zero_expences(row):\n    if row['RoomService'] == 0 and row['FoodCourt'] == 0 \\\n    and row['ShoppingMall'] == 0 and row[\"Spa\"] == 0 and row['VRDeck'] == 0:\n        return True\n    else:\n        return False\n\ndf['ZeroExpences'] = df.apply(lambda row: zero_expences(row), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:08.976148Z","iopub.execute_input":"2022-04-24T14:41:08.976365Z","iopub.status.idle":"2022-04-24T14:41:09.127877Z","shell.execute_reply.started":"2022-04-24T14:41:08.976331Z","shell.execute_reply":"2022-04-24T14:41:09.127016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add a specific paramter that displays family size\ndef family_size_splitter(row):\n    family_size = ''\n    if row == 1:\n        family_size = 'Solo'\n    elif row <= 3:\n        family_size = 'Small'\n    elif row <= 5:\n        family_size = 'Medium'\n    else:\n        family_size = 'Large'\n    return family_size\n\ndf['FamilySize'] = df.apply(lambda row: family_size_splitter(row['GroupCount']), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:09.128881Z","iopub.execute_input":"2022-04-24T14:41:09.129087Z","iopub.status.idle":"2022-04-24T14:41:09.217707Z","shell.execute_reply.started":"2022-04-24T14:41:09.129062Z","shell.execute_reply":"2022-04-24T14:41:09.216965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncols = ['HomePlanet',\n        'CryoSleep',\n        'VIP',\n        'Destination',\n        'CabinDeck',\n        'CabinSide',\n        'PassengerGroup',\n        'GroupCount',\n        'ZeroExpences',\n        'FamilySize']\n\nfig, axs = plt.subplots(2, 5, figsize=(20, 15))\nfor i in range(len(cols)):\n    plt.subplot(2, 5, i + 1)\n    sns.countplot(data = df, x = cols[i], hue = 'Transported')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:09.218925Z","iopub.execute_input":"2022-04-24T14:41:09.219135Z","iopub.status.idle":"2022-04-24T14:41:10.684603Z","shell.execute_reply.started":"2022-04-24T14:41:09.219111Z","shell.execute_reply":"2022-04-24T14:41:10.683626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 15))\nsns.displot(data = df,\n            x = 'Age',\n            hue = 'Transported',\n            kde = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:10.685893Z","iopub.execute_input":"2022-04-24T14:41:10.686504Z","iopub.status.idle":"2022-04-24T14:41:11.414Z","shell.execute_reply.started":"2022-04-24T14:41:10.686439Z","shell.execute_reply":"2022-04-24T14:41:11.413005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"isVip = df[df.VIP == True]\nnotVip = df[df.VIP == False]\n\nisVip.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:11.416358Z","iopub.execute_input":"2022-04-24T14:41:11.417226Z","iopub.status.idle":"2022-04-24T14:41:11.460642Z","shell.execute_reply.started":"2022-04-24T14:41:11.417174Z","shell.execute_reply":"2022-04-24T14:41:11.459794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"notVip.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:11.461843Z","iopub.execute_input":"2022-04-24T14:41:11.462157Z","iopub.status.idle":"2022-04-24T14:41:11.500897Z","shell.execute_reply.started":"2022-04-24T14:41:11.462114Z","shell.execute_reply":"2022-04-24T14:41:11.500209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nsns.heatmap(df.corr())","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:11.50189Z","iopub.execute_input":"2022-04-24T14:41:11.502212Z","iopub.status.idle":"2022-04-24T14:41:11.87235Z","shell.execute_reply.started":"2022-04-24T14:41:11.502185Z","shell.execute_reply":"2022-04-24T14:41:11.871552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Further preprocessing","metadata":{}},{"cell_type":"markdown","source":"The number of the total expence is not random - it is slightly bigger then the 75% quartile for those who are not in the VIP.","metadata":{}},{"cell_type":"code","source":"def fill_vip(row):\n    if row['VIP'] != None:\n        if row['TotalExpence'] <= 1500:\n            return False\n        else:\n            return True\n    else:\n        return row['VIP']\n\ndf['VIP'] = df.apply(lambda row: fill_vip(row), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:11.873777Z","iopub.execute_input":"2022-04-24T14:41:11.874231Z","iopub.status.idle":"2022-04-24T14:41:11.994081Z","shell.execute_reply.started":"2022-04-24T14:41:11.87419Z","shell.execute_reply":"2022-04-24T14:41:11.993323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:11.995203Z","iopub.execute_input":"2022-04-24T14:41:11.995419Z","iopub.status.idle":"2022-04-24T14:41:12.010591Z","shell.execute_reply.started":"2022-04-24T14:41:11.995394Z","shell.execute_reply":"2022-04-24T14:41:12.00964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_cols = ['HomePlanet',\n            'CryoSleep',\n            'Destination',\n            'CabinDeck',\n            'CabinSide']\n\nnum_cols = ['CabinNum']\n\n\npre_transformer = ColumnTransformer([\n    ('cat', SimpleImputer(strategy='most_frequent'), cat_cols),\n    ('num', SimpleImputer(strategy='mean'), num_cols)\n], remainder = 'passthrough')\n\ncols = df.columns.tolist()\ndf = pd.DataFrame(pre_transformer.fit_transform(df),\n                  columns = cols)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:12.012197Z","iopub.execute_input":"2022-04-24T14:41:12.012585Z","iopub.status.idle":"2022-04-24T14:41:12.051039Z","shell.execute_reply.started":"2022-04-24T14:41:12.012545Z","shell.execute_reply":"2022-04-24T14:41:12.050214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_transformer.get_feature_names_out","metadata":{"execution":{"iopub.status.busy":"2022-04-24T15:00:49.585367Z","iopub.execute_input":"2022-04-24T15:00:49.586384Z","iopub.status.idle":"2022-04-24T15:00:49.605154Z","shell.execute_reply.started":"2022-04-24T15:00:49.58633Z","shell.execute_reply":"2022-04-24T15:00:49.603837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:41:24.563607Z","iopub.execute_input":"2022-04-24T14:41:24.564021Z","iopub.status.idle":"2022-04-24T14:41:24.588013Z","shell.execute_reply.started":"2022-04-24T14:41:24.563989Z","shell.execute_reply":"2022-04-24T14:41:24.587352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# M is for model building\n1. Split data into train & test dataset to eliminate bias\n2. Create preprocessing pipeline to work with numerical and categorical columns separately\n3. Transform data and restore index & column names","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import (\n    OrdinalEncoder, \n    MinMaxScaler,\n    StandardScaler,\n    OneHotEncoder\n)\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\n\nX = df.copy()\ny = X.pop('Transported')\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, train_size = 0.8)\n\nnum_cols = list(X.select_dtypes(include = ['float', 'int']))\ncat_cols = list(X.select_dtypes(include = ['object']))\ncat_cols.remove('FName')\ncat_cols.remove('SName')\n\nnumerical_pipe = Pipeline(steps=[\n    ('transformer', StandardScaler())\n])\n\ncategorical_pipe = Pipeline(steps=[\n    ('trasformer', OrdinalEncoder())\n])\n\ntransformer_pipe = ColumnTransformer([\n    ('num', numerical_pipe, num_cols),\n    ('cat', categorical_pipe, cat_cols)\n], remainder='drop')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:40:44.847602Z","iopub.execute_input":"2022-04-24T14:40:44.847859Z","iopub.status.idle":"2022-04-24T14:40:44.875298Z","shell.execute_reply.started":"2022-04-24T14:40:44.847833Z","shell.execute_reply":"2022-04-24T14:40:44.874413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cat_cols)\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:40:45.117812Z","iopub.execute_input":"2022-04-24T14:40:45.118068Z","iopub.status.idle":"2022-04-24T14:40:45.141Z","shell.execute_reply.started":"2022-04-24T14:40:45.118039Z","shell.execute_reply":"2022-04-24T14:40:45.140202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transform via imputer\n\nX_train_imputed = pd.DataFrame(transformer_pipe.fit_transform(X_train))\nX_test_imputed = pd.DataFrame(transformer_pipe.transform(X_test))\n\nX_train_imputed.index = X_train.index\nX_train_imputed.columns = num_cols + cat_cols\n\nX_test_imputed.index = X_test.index\nX_test_imputed.columns = num_cols + cat_cols","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:40:45.362062Z","iopub.execute_input":"2022-04-24T14:40:45.362341Z","iopub.status.idle":"2022-04-24T14:40:45.55514Z","shell.execute_reply.started":"2022-04-24T14:40:45.362313Z","shell.execute_reply":"2022-04-24T14:40:45.554046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_imputed.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:40:45.603179Z","iopub.execute_input":"2022-04-24T14:40:45.603928Z","iopub.status.idle":"2022-04-24T14:40:45.628131Z","shell.execute_reply.started":"2022-04-24T14:40:45.603882Z","shell.execute_reply":"2022-04-24T14:40:45.627263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Further preprocessing ideas:\n1. Introduce new features to the dataset (split several first columns)\n2. Evaluate current features via permutation importance","metadata":{}},{"cell_type":"markdown","source":"# Model build\n## Random forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nn_leaves = range(10, 100, 10)\n\nfor n in n_leaves:\n    clf = RandomForestClassifier(max_leaf_nodes = n, random_state = 42)\n    clf.fit(X_train_imputed, y_train)\n    score = clf.score(X_test_imputed, y_test)\n    print(f'For {n} leaves model score is \\t {score}')    ","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:40:45.845039Z","iopub.execute_input":"2022-04-24T14:40:45.845331Z","iopub.status.idle":"2022-04-24T14:40:45.95512Z","shell.execute_reply.started":"2022-04-24T14:40:45.845298Z","shell.execute_reply":"2022-04-24T14:40:45.953789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The peak performance is around 60, 70 leaves.\nTo evaluate lets try to build k-means classifier.\n## K-Neighbors","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nfor i in range(1, 10):\n    knn = KNeighborsClassifier(n_neighbors = i).fit(X_train_imputed, y_train)\n    train_score = knn.score(X_train_imputed, y_train)\n    test_score = knn.score(X_test_imputed, y_test)\n    print(f'For {i} neighbors \\t train score: {train_score} \\t test_score: {train_score}')\nprint('===============')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:40:46.106778Z","iopub.execute_input":"2022-04-24T14:40:46.107448Z","iopub.status.idle":"2022-04-24T14:40:46.131314Z","shell.execute_reply.started":"2022-04-24T14:40:46.10741Z","shell.execute_reply":"2022-04-24T14:40:46.130309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparams = {'n_neighbors': range(1, 10),\n         'weights': ['uniform', 'distance']}\n\nmodel = KNeighborsClassifier()\nknc_grid = GridSearchCV(model,\n                       params,\n                       cv=5,\n                       n_jobs=5,\n                       verbose=True)\n\nknc_grid.fit(X_train_imputed, y_train)\nprint(knc_grid.best_score_)\nprint(knc_grid.best_params_)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:40:46.344766Z","iopub.execute_input":"2022-04-24T14:40:46.345396Z","iopub.status.idle":"2022-04-24T14:40:48.65264Z","shell.execute_reply.started":"2022-04-24T14:40:46.345361Z","shell.execute_reply":"2022-04-24T14:40:48.650985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nclf = KNeighborsClassifier(n_neighbors = 5).fit(X_train_imputed, y_train)\nperm = PermutationImportance(clf).fit(X_train_imputed, y_train)\neli5.show_weights(perm, feature_names = X_train_imputed.columns.tolist())","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:40:48.654052Z","iopub.status.idle":"2022-04-24T14:40:48.654896Z","shell.execute_reply.started":"2022-04-24T14:40:48.654671Z","shell.execute_reply":"2022-04-24T14:40:48.654697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model gets slightly bigger score with 2 neighbors but it's probably due overfitting to the data, so we choose 3 neighbors in the final version.","metadata":{}},{"cell_type":"markdown","source":"# Final pipeline creation\nIn future I want to avoid the rewriting of the code - very demoralizing and frustrating, write a pipeline with thoughts of final evaluation. <br>\nIn other words data can be split in the end of the program.","metadata":{"execution":{"iopub.status.busy":"2022-03-29T19:03:32.186702Z","iopub.execute_input":"2022-03-29T19:03:32.187571Z","iopub.status.idle":"2022-03-29T19:03:32.210547Z","shell.execute_reply.started":"2022-03-29T19:03:32.187526Z","shell.execute_reply":"2022-03-29T19:03:32.209317Z"}}},{"cell_type":"code","source":"first_num_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\nfirst_cat_cols = ['HomePlanet', 'CryoSleep', 'VIP', 'Destination', 'Cabin']\nsecond_num_cols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', \n                       'VRDeck', 'PassengerNum', 'CabinNum', 'RelativesNum', 'TotalExpence']\nsecond_cat_cols = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'CabinFL', 'CabinSL']\n\n\ndef split_drop_df(X):\n    X[['PassengerNum', 'PassengerClass']] = X.PassengerId.str.split('_', expand=True)\n    X[['CabinFL', 'CabinNum', 'CabinSL']] = X.Cabin.str.split('/', expand=True)\n    X[['FName', 'SName']] = X.Name.str.split(' ', expand=True)\n    \n    X['RelativesNum'] = X['PassengerNum'].map(lambda x: (X['PassengerNum'] == x).sum())\n    X = X.drop(columns=['PassengerId', 'PassengerClass', 'Cabin', 'Name'])\n    X = X.RoomService + X.FoodCourt + X.ShoppingMall + X.Spa + X.VRDeck\n    return X\n\n\ndef preprocess_df(X):\n    X['Age'] = X['Age'].fillna(X['Age'].mean())\n    X[first_num_cols] = X[first_num_cols].fillna(0)\n    X[first_cat_cols] = SimpleImputer(strategy='most_frequent').fit_transform(X[first_cat_cols])\n    X = split_drop_df(X)\n    \n    \n    numerical_pipe = Pipeline(steps=[\n        ('transformer', MinMaxScaler())\n    ])\n\n    categorical_pipe = Pipeline(steps=[\n        ('trandformer', OrdinalEncoder())\n    ])\n\n    transformer_pipe = ColumnTransformer([\n        ('num', numerical_pipe, second_num_cols),\n        ('cat', categorical_pipe, second_cat_cols)\n    ])\n    \n    return X, transformer_pipe\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:40:48.655805Z","iopub.status.idle":"2022-04-24T14:40:48.656379Z","shell.execute_reply.started":"2022-04-24T14:40:48.656196Z","shell.execute_reply":"2022-04-24T14:40:48.656217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"X_train = pd.read_csv('../input/spaceship-titanic/train.csv')\nX_test = pd.read_csv('../input/spaceship-titanic/test.csv')\n\ny_train = X_train.pop('Transported')\npassenger_id = X_test['PassengerId']\n\nX_train, _ = preprocess_df(X_train)\nX_test, transformer_pipe = preprocess_df(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:40:48.657192Z","iopub.status.idle":"2022-04-24T14:40:48.657487Z","shell.execute_reply.started":"2022-04-24T14:40:48.657327Z","shell.execute_reply":"2022-04-24T14:40:48.657342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_new = pd.DataFrame(transformer_pipe.fit_transform(X_train))\nX_test_new = pd.DataFrame(transformer_pipe.transform(X_test))\n\n\nX_train_new.columns = second_num_cols + second_cat_cols\nX_test_new.columns = second_num_cols + second_cat_cols\n\nknn = KNeighborsClassifier(n_neighbors = 2)\nknn.fit(X_train_new, y_train)\n\npredictions = pd.Series(clf.predict(X_test_new), name='Transported')\nfinal_result = pd.concat([passenger_id, predictions], axis=1)\n\nfinal_result.to_csv('/kaggle/working/submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T14:40:48.658641Z","iopub.status.idle":"2022-04-24T14:40:48.658923Z","shell.execute_reply.started":"2022-04-24T14:40:48.658775Z","shell.execute_reply":"2022-04-24T14:40:48.65879Z"},"trusted":true},"execution_count":null,"outputs":[]}]}