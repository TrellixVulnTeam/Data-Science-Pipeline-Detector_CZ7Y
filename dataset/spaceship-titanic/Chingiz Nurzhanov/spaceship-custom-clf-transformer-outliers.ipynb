{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install feature_engine","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:38:33.865079Z","iopub.execute_input":"2022-06-15T10:38:33.865482Z","iopub.status.idle":"2022-06-15T10:38:42.837436Z","shell.execute_reply.started":"2022-06-15T10:38:33.865447Z","shell.execute_reply":"2022-06-15T10:38:42.836607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Libraries","metadata":{"ExecuteTime":{"end_time":"2022-06-14T14:20:14.337834Z","start_time":"2022-06-14T14:20:14.335292Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport re\nfrom collections import Counter\n# Data split\nfrom sklearn.model_selection import StratifiedKFold\n\n# Preprocessing\nimport missingno as msno\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\n\n\n# Custom Transformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.impute import SimpleImputer\nfrom feature_engine.encoding import RareLabelEncoder\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# Custom Classifier\nfrom imblearn import FunctionSampler\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_array, check_is_fitted\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.model_selection import HalvingGridSearchCV\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.feature_selection import SelectPercentile, f_classif\nfrom feature_engine.selection import DropCorrelatedFeatures\nfrom sklearn.ensemble import BaggingClassifier\n\n# Pipeline\nfrom imblearn.pipeline import Pipeline\n\n# Metrics \nfrom sklearn.metrics import brier_score_loss, roc_curve, precision_recall_curve \nfrom sklearn.metrics import balanced_accuracy_score, accuracy_score, confusion_matrix, classification_report\n\n# Feature Enginering\nfrom sklearn.inspection import permutation_importance\nimport shap\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import BaggingClassifier","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:02.390508Z","start_time":"2022-06-15T10:33:59.87504Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:42.839027Z","iopub.execute_input":"2022-06-15T10:38:42.839378Z","iopub.status.idle":"2022-06-15T10:38:42.857685Z","shell.execute_reply.started":"2022-06-15T10:38:42.839341Z","shell.execute_reply":"2022-06-15T10:38:42.85632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import set_config\nset_config(display=\"diagram\")\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom sklearn.exceptions import FitFailedWarning\nwarnings.filterwarnings(action='ignore', category=FitFailedWarning)\nwarnings.filterwarnings(action='ignore', category=UserWarning)","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:02.395111Z","start_time":"2022-06-15T10:34:02.391528Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:42.85865Z","iopub.execute_input":"2022-06-15T10:38:42.858972Z","iopub.status.idle":"2022-06-15T10:38:42.875806Z","shell.execute_reply.started":"2022-06-15T10:38:42.858943Z","shell.execute_reply":"2022-06-15T10:38:42.874894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis(EDA)\n\n- Analysis of the features.\n\n- Finding any relations or trends considering multiple features.","metadata":{}},{"cell_type":"markdown","source":"RoomService, FoodCourt, ShoppingMall, Spa, VRDeckRoomService, FoodCourt, ShoppingMall, Spa, VRDeckRoomService, FoodCourt, ShoppingMall, Spa, VRDeck## train.csv - Personal records for about two-thirds (~8700) of the passengers, to be used as training data.\n- PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n- HomePlanet - The planet the passenger departed from, typically their planet of permanent residence.\n- CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n- Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\nDestination - The planet the passenger will be debarking to.\n- Age - The age of the passenger.\n- VIP - Whether the passenger has paid for special VIP service during the voyage.\nRoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n- Name - The first and last names of the passenger.\nTransported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/spaceship-titanic/train.csv')\ntest = pd.read_csv('../input/spaceship-titanic/test.csv')","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:02.426727Z","start_time":"2022-06-15T10:34:02.397147Z"},"run_control":{"marked":true},"execution":{"iopub.status.busy":"2022-06-15T10:38:42.877668Z","iopub.execute_input":"2022-06-15T10:38:42.878011Z","iopub.status.idle":"2022-06-15T10:38:42.968Z","shell.execute_reply.started":"2022-06-15T10:38:42.877983Z","shell.execute_reply":"2022-06-15T10:38:42.967274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6,6))\ndf.Transported.value_counts().plot.pie(explode=[0.1,0.1], autopct='%1.1f%%', shadow=True, textprops={'fontsize':16}).set_title(\"Transported\")","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:02.519013Z","start_time":"2022-06-15T10:34:02.428255Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:42.969125Z","iopub.execute_input":"2022-06-15T10:38:42.969469Z","iopub.status.idle":"2022-06-15T10:38:43.169833Z","shell.execute_reply.started":"2022-06-15T10:38:42.969436Z","shell.execute_reply":"2022-06-15T10:38:43.168726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:02.534288Z","start_time":"2022-06-15T10:34:02.520546Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:43.175016Z","iopub.execute_input":"2022-06-15T10:38:43.17848Z","iopub.status.idle":"2022-06-15T10:38:43.221557Z","shell.execute_reply.started":"2022-06-15T10:38:43.178414Z","shell.execute_reply":"2022-06-15T10:38:43.220439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:02.54757Z","start_time":"2022-06-15T10:34:02.535307Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:43.227455Z","iopub.execute_input":"2022-06-15T10:38:43.228766Z","iopub.status.idle":"2022-06-15T10:38:43.251035Z","shell.execute_reply.started":"2022-06-15T10:38:43.228705Z","shell.execute_reply":"2022-06-15T10:38:43.250036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##From https://www.kaggle.com/code/arootda/pycaret-visualization-optimization-0-81/notebook\nfrom IPython.core.display import HTML\n\ndef multi_table(table_list):\n    return HTML(\n        f\"<table><tr> {''.join(['<td>' + table._repr_html_() + '</td>' for table in table_list])} </tr></table>\")\n\nmulti_table([pd.DataFrame(df[i].value_counts()) for i in df.columns if i != \"I'm serious this could be your ad here\"])","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:02.579652Z","start_time":"2022-06-15T10:34:02.549608Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:43.253686Z","iopub.execute_input":"2022-06-15T10:38:43.254143Z","iopub.status.idle":"2022-06-15T10:38:43.30899Z","shell.execute_reply.started":"2022-06-15T10:38:43.254108Z","shell.execute_reply":"2022-06-15T10:38:43.308065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing data","metadata":{}},{"cell_type":"markdown","source":"In statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation. Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data. [https://en.wikipedia.org/wiki/Missing_data]","metadata":{}},{"cell_type":"code","source":"# from https://cjasn.asnjournals.org/content/early/2014/02/06/CJN.10141013?versioned=true\nfrom IPython.display import Image\nImage(url = 'https://cjasn.asnjournals.org/content/clinjasn/early/2014/02/06/CJN.10141013/F2.large.jpg?width=800&height=600&carousel=1', width=700)","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:02.585781Z","start_time":"2022-06-15T10:34:02.581189Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:43.310223Z","iopub.execute_input":"2022-06-15T10:38:43.311167Z","iopub.status.idle":"2022-06-15T10:38:43.317795Z","shell.execute_reply.started":"2022-06-15T10:38:43.311124Z","shell.execute_reply":"2022-06-15T10:38:43.317156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Missing Completely at Random (MCAR)\n\nThe missing values on a given variable (Y) are not associated with other variables in a given data set or with the variable (Y) itself. In other words, there is no particular reason for the missing values.\n\n- Missing at Random (MAR)\n\nMAR occurs when the missingness is not random, but where missingness can be fully accounted for by variables where there is complete information.\n\n- Missing Not at Random (MNAR)\n\nMissingness depends on unobserved data or the value of the missing data itself.\n\nWikipedia: https://en.wikipedia.org/wiki/Missing_data\n\nGood guide\n\nsource: https://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python\n","metadata":{"ExecuteTime":{"end_time":"2022-06-14T14:22:43.754014Z","start_time":"2022-06-14T14:22:43.748906Z"}}},{"cell_type":"markdown","source":"#### Search null","metadata":{"ExecuteTime":{"end_time":"2022-06-14T14:22:50.350761Z","start_time":"2022-06-14T14:22:50.348209Z"}}},{"cell_type":"code","source":"def null_detector(df):\n    # number of all null values in dataframe columns\n    \n    print(f'number of all null values in df columns')\n\n    print(\"\\n\"f'{df.isnull().sum()}')\n\n    # Nullity matrix to find null value in dataframe\n    plt.figure(figsize=(15,10))\n    sns.heatmap(df.isnull(),\n            cmap=\"YlGnBu\",\n            cbar_kws={'label': 'Missing Data'})\n    plt.show()","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:02.590358Z","start_time":"2022-06-15T10:34:02.586798Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:43.319987Z","iopub.execute_input":"2022-06-15T10:38:43.320425Z","iopub.status.idle":"2022-06-15T10:38:43.329931Z","shell.execute_reply.started":"2022-06-15T10:38:43.320396Z","shell.execute_reply":"2022-06-15T10:38:43.329042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"null_detector(df)\nnull_detector(test)","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:03.889793Z","start_time":"2022-06-15T10:34:02.591386Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:43.331145Z","iopub.execute_input":"2022-06-15T10:38:43.331626Z","iopub.status.idle":"2022-06-15T10:38:45.238529Z","shell.execute_reply.started":"2022-06-15T10:38:43.331588Z","shell.execute_reply":"2022-06-15T10:38:45.237655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop duplicats\ndf.drop_duplicates(inplace=True)\n","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:03.902007Z","start_time":"2022-06-15T10:34:03.891319Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:45.239664Z","iopub.execute_input":"2022-06-15T10:38:45.240056Z","iopub.status.idle":"2022-06-15T10:38:45.261452Z","shell.execute_reply.started":"2022-06-15T10:38:45.240025Z","shell.execute_reply":"2022-06-15T10:38:45.260747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:04.012776Z","start_time":"2022-06-15T10:34:03.90506Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:45.263499Z","iopub.execute_input":"2022-06-15T10:38:45.263887Z","iopub.status.idle":"2022-06-15T10:38:45.286372Z","shell.execute_reply.started":"2022-06-15T10:38:45.263858Z","shell.execute_reply":"2022-06-15T10:38:45.285408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:04.025507Z","start_time":"2022-06-15T10:34:04.014301Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:45.287391Z","iopub.execute_input":"2022-06-15T10:38:45.28779Z","iopub.status.idle":"2022-06-15T10:38:45.305872Z","shell.execute_reply.started":"2022-06-15T10:38:45.287759Z","shell.execute_reply":"2022-06-15T10:38:45.304881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include='all').T","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:04.061754Z","start_time":"2022-06-15T10:34:04.026532Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:45.307236Z","iopub.execute_input":"2022-06-15T10:38:45.307768Z","iopub.status.idle":"2022-06-15T10:38:45.364943Z","shell.execute_reply.started":"2022-06-15T10:38:45.307736Z","shell.execute_reply":"2022-06-15T10:38:45.363893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.dtypes[(df.dtypes == \"float64\") |\n             (df.dtypes == \"int64\")].index.values].hist(figsize=[24, 24])","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:04.591857Z","start_time":"2022-06-15T10:34:04.062777Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:45.366688Z","iopub.execute_input":"2022-06-15T10:38:45.367459Z","iopub.status.idle":"2022-06-15T10:38:46.189965Z","shell.execute_reply.started":"2022-06-15T10:38:45.367414Z","shell.execute_reply":"2022-06-15T10:38:46.189032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pearson correlation map\ncols = df.corr().index\ncm = df[cols].corr()\nplt.figure(figsize=(20, 10))\nsns.heatmap(cm, annot=True, cmap='viridis')","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:04.88883Z","start_time":"2022-06-15T10:34:04.592871Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:46.191525Z","iopub.execute_input":"2022-06-15T10:38:46.19199Z","iopub.status.idle":"2022-06-15T10:38:46.602516Z","shell.execute_reply.started":"2022-06-15T10:38:46.191948Z","shell.execute_reply":"2022-06-15T10:38:46.601623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In statistics, correlation or dependence is any statistical relationship between two variables. \"correlation\" may indicate any type of association, in statistics it refers to the degree to which a pair of variables are linearly related. We can only measure the numerical features.\nThe values range between -1.0 and 1.0. A calculated number greater than 1.0 or less than -1.0 means that there was an error in the correlation measurement.\n\nThe two main types of correlation:\n- positive correlation and\n- negative correlation.\n\nPositive correlation: A value 1.0 means perfect positive correlation, it's mean that increase in this feature lead in increase in another feature. i.e if we have features A, B and if feature A leads to increase in feature B, then they are positive correlated. If a value -1 that mean positive correlation\n\nNegative correlation: A value -1.0 means perfect negative correlation, it's mean that decrease in this feature lead in increase in another feature. i.e if we have features A, B and if feature A leads to decrease in feature B, then they are negatively correlated. If a value -1 that mean negative correlation\n\nIf the value is 0, there is no correlation between the two variables. This means that the variables changes in a random manner with respect to each other.\nA value 0.7 to 1 are strong positive correlation or perfectly correlated, so the increase in one leads to increase in the other\nGenerally speaking, a Pearson correlation coefficient value greater than 0.7 indicates the presence of multi-collinearity. So we looking for correlation <0.7. A value -0.7 to -1 also strong negative correlation or negative correlated indicates the presence of multi-collinearity.\nThis means that both the features are containing highly similar information is known as MultiColinearity when both of them contains almost the same information.\n\nAlso, a correlation near 0 may also be relevant, but it may be different from the correlation.","metadata":{}},{"cell_type":"markdown","source":"Pearson correlation\n- The Pearson product-moment correlation coefficient (or Pearson correlation coefficient, for short) is a measure of the strength of a linear association between two variables and is denoted by r. Basically, a Pearson product-moment correlation attempts to draw a line of best fit through the data of two variables, and the Pearson correlation coefficient, r, indicates how far away all these data points are to this line of best fit (i.e., how well the data points fit this new model/line of best fit).\n\nsource:https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide.php\n\n\nKendall correlation\n- It is a measure of rank correlation: the similarity of the orderings of the data when ranked by each of the quantities.\n\nsource:https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient\n\nSpearman correlation\n- The Spearman's rank-order correlation is the nonparametric version of the Pearson product-moment correlation. Spearman's correlation coefficient, (ρ, also signified by rs) measures the strength and direction of association between two ranked variables.\n\nsource:https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php\n","metadata":{}},{"cell_type":"code","source":"def correlation_map(df):\n    fig, axs = plt.subplots(nrows=3, figsize=(20, 40))\n    cols = df.corr().index\n    # Pearson correlation map\n    pearson = df[cols].corr(method='pearson')\n    # Kendall correlation map\n    kendall = df[cols].corr(method='kendall')\n    # Spearman correlation map\n    spearman = df[cols].corr(method='spearman')\n    sns.heatmap(pearson, annot=True, cmap='viridis', ax=axs[0])\n    sns.heatmap(kendall, annot=True, cmap='viridis', ax=axs[1])\n    sns.heatmap(spearman, annot=True, cmap='viridis', ax=axs[2])\n\n\ncorrelation_map(df)","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.002702Z","start_time":"2022-06-15T10:34:04.890369Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:46.603809Z","iopub.execute_input":"2022-06-15T10:38:46.604675Z","iopub.status.idle":"2022-06-15T10:38:48.360196Z","shell.execute_reply.started":"2022-06-15T10:38:46.604632Z","shell.execute_reply":"2022-06-15T10:38:48.359486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We looking for relation between two variables. \nSo we may observe all type of correlation map","metadata":{}},{"cell_type":"markdown","source":"### Box plot for searchin outliers","metadata":{}},{"cell_type":"markdown","source":"Plot boxplots to have a better understanding of the distribution of features. A boxplot is a standardized way of displaying the distribution of data based on a five number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”). It can tell you about your outliers and what their values are. It can also tell you if your data is symmetrical, how tightly your data is grouped, and if and how your data is skewed. [https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51]\n","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\n# from [https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51]\nImage(url='https://miro.medium.com/max/1400/1*2c21SkzJMf3frPXPAR_gZA.png', width=700)","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.008301Z","start_time":"2022-06-15T10:34:06.004225Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.361316Z","iopub.execute_input":"2022-06-15T10:38:48.362096Z","iopub.status.idle":"2022-06-15T10:38:48.36906Z","shell.execute_reply.started":"2022-06-15T10:38:48.362062Z","shell.execute_reply":"2022-06-15T10:38:48.368167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- median (Q2/50th Percentile): the middle value of the dataset.\n\n- first quartile (Q1/25th Percentile): the middle number between the smallest number (not the “minimum”) and the median of the dataset.\n\n- third quartile (Q3/75th Percentile): the middle value between the median and the highest value (not the “maximum”) of the dataset.\n\n- interquartile range (IQR): 25th to the 75th percentile.\n\n- whiskers (shown in blue)\n\n- outliers (shown as green circles)\n\n- “maximum”: Q3 + 1.5*IQR\n\n- “minimum”: Q1 -1.5*IQR","metadata":{"run_control":{"marked":true}}},{"cell_type":"code","source":"df.boxplot(figsize=(10, 10))","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.163583Z","start_time":"2022-06-15T10:34:06.009316Z"},"scrolled":true,"execution":{"iopub.status.busy":"2022-06-15T10:38:48.370295Z","iopub.execute_input":"2022-06-15T10:38:48.370788Z","iopub.status.idle":"2022-06-15T10:38:48.661602Z","shell.execute_reply.started":"2022-06-15T10:38:48.370755Z","shell.execute_reply":"2022-06-15T10:38:48.660694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Detect outliers","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nnumerical_features = df.select_dtypes(include=['int64', 'float64']).columns\n\n\ndef detect_outliers(df, features):\n    outlier_indices = []\n\n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c], 25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c], 75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) |\n                              (df[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n\n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n\n    return multiple_outliers\n\n\nOutliers_to_drop = detect_outliers(df, numerical_features)","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.175816Z","start_time":"2022-06-15T10:34:06.165113Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.662839Z","iopub.execute_input":"2022-06-15T10:38:48.663162Z","iopub.status.idle":"2022-06-15T10:38:48.680961Z","shell.execute_reply.started":"2022-06-15T10:38:48.663134Z","shell.execute_reply":"2022-06-15T10:38:48.680198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Outlier shape\ndf.loc[Outliers_to_drop]","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.184479Z","start_time":"2022-06-15T10:34:06.177345Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.682359Z","iopub.execute_input":"2022-06-15T10:38:48.682922Z","iopub.status.idle":"2022-06-15T10:38:48.697524Z","shell.execute_reply.started":"2022-06-15T10:38:48.68288Z","shell.execute_reply":"2022-06-15T10:38:48.69607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop outliers","metadata":{}},{"cell_type":"code","source":"# Need to understand your data to know if outliers should be removed.\n# Drop outliers\ndf = df.drop(Outliers_to_drop, axis=0).reset_index(drop=True)","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.190588Z","start_time":"2022-06-15T10:34:06.186006Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.698556Z","iopub.execute_input":"2022-06-15T10:38:48.699262Z","iopub.status.idle":"2022-06-15T10:38:48.706811Z","shell.execute_reply.started":"2022-06-15T10:38:48.699211Z","shell.execute_reply":"2022-06-15T10:38:48.706107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"hmm IQR method is not that effective here","metadata":{}},{"cell_type":"markdown","source":"### check_freq","metadata":{}},{"cell_type":"code","source":"def check_freq(df=df, percent=0.05):\n    # show all categorical data and their frequency, you can control percentage of category. Default = 5%\n    df_object_type = df.select_dtypes(include=['object', 'category'])\n    for col in df_object_type.columns:\n        label_freq = df_object_type[col].value_counts() / len(df_object_type)\n        fig = label_freq.sort_values(ascending=False).plot.bar()\n        fig.axhline(y=percent, color='red')\n        fig.set_ylabel(f'Percentage of destination category of {col}')\n        fig.set_xlabel('Destination')\n        fig.set_title(f'{col} rare categories')\n        plt.show()","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.196181Z","start_time":"2022-06-15T10:34:06.192112Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.707915Z","iopub.execute_input":"2022-06-15T10:38:48.708452Z","iopub.status.idle":"2022-06-15T10:38:48.715221Z","shell.execute_reply.started":"2022-06-15T10:38:48.708421Z","shell.execute_reply":"2022-06-15T10:38:48.714566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check_freq(df = df, percent = 0.05)","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.199742Z","start_time":"2022-06-15T10:34:06.197197Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.716553Z","iopub.execute_input":"2022-06-15T10:38:48.717138Z","iopub.status.idle":"2022-06-15T10:38:48.729591Z","shell.execute_reply.started":"2022-06-15T10:38:48.717098Z","shell.execute_reply":"2022-06-15T10:38:48.728799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering\nFeature engineering is the process of using domain knowledge to choose and transform the variables that will feed into your machine learning model. The process involves a combination of understanding the problem, data analysis, and applying good judgement. Feature Engineering is as much an art as it is a science.\n\nFeature engineering happens prior to modeling, and is an essential part of building a machine learning solution. Appropriate well-designed features are often the deciding factor of the performance of your final algorithm. For this reason, data scientists often spend 70%-80% of their time on the pre-modelling phase, which largely consists of feature engineering.[https://zindi.africa/learn/what-is-feature-engineering-a-tutorial-from-mohamed-salem-jedidi]","metadata":{}},{"cell_type":"markdown","source":"## Create new columns PasId_group and PasId_number","metadata":{}},{"cell_type":"code","source":"df[['PasId_group', 'PasId_number']] = df.PassengerId.str.split('_', expand=True)\ntest[['PasId_group', 'PasId_number']] = test.PassengerId.str.split('_', expand=True)","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.328343Z","start_time":"2022-06-15T10:34:06.201267Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.73071Z","iopub.execute_input":"2022-06-15T10:38:48.731147Z","iopub.status.idle":"2022-06-15T10:38:48.760034Z","shell.execute_reply.started":"2022-06-15T10:38:48.73112Z","shell.execute_reply":"2022-06-15T10:38:48.759334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create new columns Cabin_deck, Cabin_num and PasId_number","metadata":{}},{"cell_type":"code","source":"df[['Cabin_deck', 'Cabin_num','Cabin_side']] = df.Cabin.str.split('/', expand=True)\ntest[['Cabin_deck', 'Cabin_num','Cabin_side']] = test.Cabin.str.split('/', expand=True)","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.347233Z","start_time":"2022-06-15T10:34:06.32988Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.761195Z","iopub.execute_input":"2022-06-15T10:38:48.761643Z","iopub.status.idle":"2022-06-15T10:38:48.793262Z","shell.execute_reply.started":"2022-06-15T10:38:48.761613Z","shell.execute_reply":"2022-06-15T10:38:48.792564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create new columns Age_type","metadata":{"ExecuteTime":{"end_time":"2022-06-15T08:54:43.616599Z","start_time":"2022-06-15T08:54:43.613536Z"}}},{"cell_type":"code","source":"df['CategoricalAge'] =  pd.cut(df.Age, bins=4)","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.354878Z","start_time":"2022-06-15T10:34:06.348258Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.79679Z","iopub.execute_input":"2022-06-15T10:38:48.797109Z","iopub.status.idle":"2022-06-15T10:38:48.811647Z","shell.execute_reply.started":"2022-06-15T10:38:48.797081Z","shell.execute_reply":"2022-06-15T10:38:48.810906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['CategoricalAge'] =  pd.cut(df.Age, bins=4)","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.360998Z","start_time":"2022-06-15T10:34:06.355904Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.812678Z","iopub.execute_input":"2022-06-15T10:38:48.813109Z","iopub.status.idle":"2022-06-15T10:38:48.820807Z","shell.execute_reply.started":"2022-06-15T10:38:48.81308Z","shell.execute_reply":"2022-06-15T10:38:48.819933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['PasId_group'] = df['PasId_group'].astype('float64')\ntest['PasId_group'] = test['PasId_group'].astype('float64')\n\ndf['PasId_number'] = df['PasId_group'].astype('float64')\ntest['PasId_number'] = test['PasId_group'].astype('float64')\n\ndf['Cabin_num'] = df['Cabin_num'].astype('float64')\ntest['Cabin_num'] = test['Cabin_num'].astype('float64')","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.373233Z","start_time":"2022-06-15T10:34:06.362013Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.822121Z","iopub.execute_input":"2022-06-15T10:38:48.823197Z","iopub.status.idle":"2022-06-15T10:38:48.838172Z","shell.execute_reply.started":"2022-06-15T10:38:48.823154Z","shell.execute_reply":"2022-06-15T10:38:48.837406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data split","metadata":{}},{"cell_type":"code","source":"X = df.copy(deep=True)\nX = X.drop(['Transported','Name', 'PassengerId'], axis=1)\ny = df.copy(deep=True)\ny = y.Transported\n# I hear boolean type better than the numerical type. Because computer can  understand it better\ny = y.astype('bool')","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.382413Z","start_time":"2022-06-15T10:34:06.374258Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.839314Z","iopub.execute_input":"2022-06-15T10:38:48.83977Z","iopub.status.idle":"2022-06-15T10:38:48.852259Z","shell.execute_reply.started":"2022-06-15T10:38:48.839733Z","shell.execute_reply":"2022-06-15T10:38:48.851466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nkf = StratifiedKFold(n_splits=3)\n\ntrain_index, val_index = next(iter(kf.split(X, y)))\nX_train, X_test = X.iloc[train_index], X.iloc[val_index]\ny_train, y_test = y.iloc[train_index], y.iloc[val_index]","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.391077Z","start_time":"2022-06-15T10:34:06.383948Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.853405Z","iopub.execute_input":"2022-06-15T10:38:48.853967Z","iopub.status.idle":"2022-06-15T10:38:48.865368Z","shell.execute_reply.started":"2022-06-15T10:38:48.853926Z","shell.execute_reply":"2022-06-15T10:38:48.864341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.396188Z","start_time":"2022-06-15T10:34:06.392102Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.86687Z","iopub.execute_input":"2022-06-15T10:38:48.867682Z","iopub.status.idle":"2022-06-15T10:38:48.874938Z","shell.execute_reply.started":"2022-06-15T10:38:48.86764Z","shell.execute_reply":"2022-06-15T10:38:48.874034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Observe y_train we can see that data is very balanced that good\nfrom collections import Counter\ncouter = Counter(y_train)\nfor k, v in couter.items():\n    dist = v / len(y_train) * 100\n    print(f'(Class={k}, n={v}, {dist}% )')","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.402799Z","start_time":"2022-06-15T10:34:06.397714Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.876263Z","iopub.execute_input":"2022-06-15T10:38:48.877471Z","iopub.status.idle":"2022-06-15T10:38:48.886102Z","shell.execute_reply.started":"2022-06-15T10:38:48.877423Z","shell.execute_reply":"2022-06-15T10:38:48.885158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Same here in y_test\nfrom collections import Counter\ncouter = Counter(y_test)\nfor k, v in couter.items():\n    dist = v / len(y_test) * 100\n    print(f'(Class={k}, n={v}, {dist}% )')","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.408396Z","start_time":"2022-06-15T10:34:06.404321Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.887397Z","iopub.execute_input":"2022-06-15T10:38:48.888398Z","iopub.status.idle":"2022-06-15T10:38:48.900333Z","shell.execute_reply.started":"2022-06-15T10:38:48.888356Z","shell.execute_reply":"2022-06-15T10:38:48.899339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Good data set very balanced","metadata":{}},{"cell_type":"markdown","source":"# Initiate classes","metadata":{}},{"cell_type":"markdown","source":"## DataFrameMapper","metadata":{"run_control":{"marked":true}}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.impute import SimpleImputer\nfrom feature_engine.encoding import RareLabelEncoder\nfrom feature_engine.encoding import CountFrequencyEncoder\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nclass DataFramePreprocessing(TransformerMixin, BaseEstimator):\n    '''DataFramePreprocessing class fit and transform all feature and return Pandas Dataframe'''\n    def __init__(self):\n        pass\n\n    # Find all categorical features. Find features with <= 7 unique value\n    def categorical_features(self):\n        return [\n            col for col in X.select_dtypes(include=['object']).columns\n            if len(X[col].unique()) <= 7\n        ]\n\n    # Find all ordinal features. Find features with > 7 unique value\n    def ordinal_features(self):\n        return [\n            col for col in X.select_dtypes(include=['object']).columns\n            if len(X[col].unique()) > 7\n        ]\n\n    # Find all numerical features.\n    def numerical_features(self):\n        return X.select_dtypes(include=['int64', 'float64']).columns\n\n    # Find all boolean features.\n    def boolean_features(self):\n        return X.select_dtypes(include=['bool']).columns\n\n    # Use SimpleImputer, RareLabelEncoder, and  OneHotEncoder for all categorical features.\n    # The RareLabelEncoder() groups rare or infrequent categories in\n    # a new category called \"Rare\", or any other name entered by the user. By default 5%\n    def cat(self):\n        return [([c], [\n            SimpleImputer(strategy='most_frequent'),\n            RareLabelEncoder(),\n            OneHotEncoder(sparse=False, handle_unknown='ignore')\n        ]) for c in self.categorical_features()]\n\n    # Use SimpleImputer and OrdinalEncoder for all ordinal features\n    def ordin(self):\n        from sklearn.preprocessing import OrdinalEncoder\n        return [([o], [\n            SimpleImputer(strategy='most_frequent'),\n            OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n                           unknown_value=-999)\n        ]) for o in self.ordinal_features()]\n\n    # Use SimpleImputer and OneHotEncoder for all boolean features\n    def boolean(self):\n        return [([b], [\n            SimpleImputer(strategy='most_frequent'),\n            OneHotEncoder(sparse=False, handle_unknown='error')\n        ]) for b in self.boolean_features()]\n\n    # Use IterativeImputer some times better than the SimpleImputer and StandardScaler for all numerical features\n    def num(self):\n        from sklearn.preprocessing import MinMaxScaler\n        return [([n], [\n            IterativeImputer(IsolationForest(n_estimators=100,\n                            max_samples='auto',\n                            max_features=1.0,\n                            bootstrap=False,\n                            n_jobs=-1,\n                            random_state=0,\n                            verbose=0,\n                            contamination=0.3\n                           ),initial_strategy ='median'),\n            StandardScaler(with_mean=False, with_std=True)\n        ]) for n in self.numerical_features()]\n\n    # Mapper transform all feature and return Pandas DataFrame\n    def mapper(self):\n        from sklearn_pandas import DataFrameMapper\n        return DataFrameMapper(self.num() + self.ordin() + self.cat() +\n                               self.boolean(),\n                               df_out=True)\n\n    # Fit\n    def fit(self, X, y=None):\n        # Standart fitter, recomended use Pandas DataFrame\n        X = check_array(X, accept_sparse=False)\n\n        self.n_features_in_ = X.shape[1]\n        self.n_features_ = X.shape[1]\n        self.is_fitted_ = True\n\n        return self\n\n    # Transform\n    def transform(self, X):\n        # Standart transform\n        check_is_fitted(self, ['is_fitted_'])\n\n        X = check_array(X, accept_sparse=True)\n\n        if X.shape[1] != self.n_features_:\n            raise ValueError(\n                'Shape of input is different from what was seen in `fit`')\n        return self","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.421656Z","start_time":"2022-06-15T10:34:06.409924Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.90183Z","iopub.execute_input":"2022-06-15T10:38:48.902646Z","iopub.status.idle":"2022-06-15T10:38:48.92212Z","shell.execute_reply.started":"2022-06-15T10:38:48.902602Z","shell.execute_reply":"2022-06-15T10:38:48.921008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## outlier_rejection","metadata":{}},{"cell_type":"code","source":"#source https://imbalanced-learn.org/stable/auto_examples/applications/plot_outlier_rejections.html\nfrom sklearn.ensemble import IsolationForest\n\ndef outlier_rejection(X, y):\n    \"\"\"This function for outliers and anomaly\"\"\"\n    model = IsolationForest(n_estimators=100,\n                            max_samples='auto',\n                            max_features=1.0,\n                            bootstrap=False,\n                            n_jobs=-1,\n                            random_state=0,\n                            verbose=0,\n                            contamination=0.01\n                           ) #1% of data that IsolationForest think as outliers\n    model.fit(X)\n    y_pred = model.predict(X)\n    return X[y_pred == 1], y[y_pred == 1]","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.427278Z","start_time":"2022-06-15T10:34:06.423186Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.923232Z","iopub.execute_input":"2022-06-15T10:38:48.924108Z","iopub.status.idle":"2022-06-15T10:38:48.937608Z","shell.execute_reply.started":"2022-06-15T10:38:48.92407Z","shell.execute_reply":"2022-06-15T10:38:48.936797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CustomClassifiere","metadata":{}},{"cell_type":"code","source":"from imblearn import FunctionSampler\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_array, check_is_fitted\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.model_selection import HalvingGridSearchCV\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn import FunctionSampler\nfrom sklearn.feature_selection import SelectPercentile, f_classif, chi2, f_oneway, mutual_info_classif\nfrom sklearn.ensemble import BaggingClassifier\nfrom feature_engine.selection import DropCorrelatedFeatures\n\n\nclass CustomClassifier(ClassifierMixin, BaseEstimator):\n    '''CustomClassifier update to outliers and anomaly\n    Custom Classifier have 1 goal give a good pipeline with hyperparameters tuning.\n    Hyperparameters to tune as a part of the fit() function and not the constructor.\n    Template from https://learn-scikit.oneoffcoder.com/customized-estimators.html'''\n    \n    param_grid = None\n\n    def __init__(self, base_estimator, scaler=None):\n        # base_estimator model\n        self.base_estimator = base_estimator\n        self.scaler = scaler\n\n    def set_base_estimator(self, base_estimator):\n        self.base_estimator = base_estimator\n        return self\n\n    def __get_pipeline(self):\n        scal = self.scaler\n        classifier = self.base_estimator\n        \n        steps = ([\n            ('scaler', scal), #My CustomTransformer DataFrame. \n            ('outliers',FunctionSampler(func=outlier_rejection)), # search anomaly and outliers\n            ('smote', SMOTE(random_state=0, n_jobs=-1)), #this data is very balanced but SMOTE improves accuracy so why not\n            ('drop', DropCorrelatedFeatures()), #good stuff find correlated features and drop them. and can be tuned.\n            #Remember correlated features not always bad.\n            ('select', SelectPercentile()), # I check all feature selectors from sklearn and this is my favorit\n            ('model',\n             BaggingClassifier(\n                 base_estimator=classifier,\n                 bootstrap_features=True,\n                 n_jobs=-1,\n                 random_state=0,\n             )),\n        ])\n\n        pipeline = Pipeline(steps=steps)\n        return pipeline\n\n    def __get_model(self):\n        if self.param_grid is None:\n            return self.__get_pipeline()\n        model = HalvingGridSearchCV(\n            **{\n                'estimator': self.__get_pipeline(),\n                'cv': self.cv,\n                'param_grid': self.param_grid,\n                'scoring': 'neg_brier_score',\n                'n_jobs': -1,\n                'random_state': 0,\n                'factor': 2,\n            })\n        return model\n\n    def predict_proba(self, X):\n        check_is_fitted(self, 'is_fitted_')\n\n        return self.model_.predict_proba(X)\n\n    def set_param_grid(self, param_grid=None):\n        self.param_grid = param_grid\n        return self\n\n    def set_cv(self, cv=5):\n        self.cv = cv\n        return self\n\n    def set_scaler(self, scaler):\n        self.scaler = scaler\n        return self.scaler\n\n    def fit(self, X, y, *args, **kwargs):\n        if y is None:\n            raise ValueError(\n                'requires y to be passed, but the target y is None')\n\n        self.is_fitted_ = True\n        self.n_features_in_ = X.shape[1]\n\n        self.model_ = self.__get_model()\n\n        self.model_.fit(X, y)\n\n        return self\n\n    def predict(self, X):\n        check_is_fitted(self, 'is_fitted_')\n        X = check_array(X)\n\n        return self.model_.predict(X)","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.440025Z","start_time":"2022-06-15T10:34:06.428807Z"},"run_control":{"marked":true},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.938991Z","iopub.execute_input":"2022-06-15T10:38:48.939602Z","iopub.status.idle":"2022-06-15T10:38:48.956842Z","shell.execute_reply.started":"2022-06-15T10:38:48.939557Z","shell.execute_reply":"2022-06-15T10:38:48.955699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Param Grid","metadata":{"ExecuteTime":{"end_time":"2022-06-14T14:37:21.748283Z","start_time":"2022-06-14T14:37:21.745738Z"}}},{"cell_type":"code","source":"# Here you can use anything that you want to tune\n# lgbm_param_grid = {\n#     'select__percentile': np.arange(2, 45, 1),\n#     'smote__k_neighbors': np.arange(5, 13, 1),\n#     # Belong to BaggingClassifier\n#     'model__n_estimators': np.arange(20, 31, 1),\n#     # Belong to LGBMClassifier\n#     'model__base_estimator__max_depth': np.arange(5, 13, 1),\n# }","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.444095Z","start_time":"2022-06-15T10:34:06.441549Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.958367Z","iopub.execute_input":"2022-06-15T10:38:48.9588Z","iopub.status.idle":"2022-06-15T10:38:48.971475Z","shell.execute_reply.started":"2022-06-15T10:38:48.958758Z","shell.execute_reply":"2022-06-15T10:38:48.970342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_param_grid = {\n    #'select__percentile': [46],\n    'select__percentile': [65],\n    'smote__k_neighbors': [7],\n    'drop__threshold':[0.9],\n    # Belong to BaggingClassifier\n    'model__n_estimators': [148],\n    #'model__max_samples': [0.90],\n    'model__max_samples': [0.12],\n    #'model__max_features': np.arange(0.1, 0.7, 0.05),\n    # Belong to LGBMClassifier\n    #'model__base_estimator__max_depth': [7],\n    #'model__base_estimator__learning_rate': np.arange(0.01, 0.31, 0.05)\n}","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.451723Z","start_time":"2022-06-15T10:34:06.448163Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.972496Z","iopub.execute_input":"2022-06-15T10:38:48.973147Z","iopub.status.idle":"2022-06-15T10:38:48.98414Z","shell.execute_reply.started":"2022-06-15T10:38:48.973111Z","shell.execute_reply":"2022-06-15T10:38:48.983438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import balanced_accuracy_score, accuracy_score, confusion_matrix, classification_report\n\n\ndef evaluate(estimator, X_train, y_train, X_test, y_test):\n\n    # Accuracy for X_train and y_train\n    score_train = estimator.score(X_train, y_train)\n    print(f'(Accuracy for score(X_train,y_train) {score_train})\\n')\n\n    # Accuracy for X_test and y_test\n\n    score_test = estimator.score(X_test, y_test)\n    print(f'(Accuracy for score(X_test,y_test) {score_test})\\n')\n\n    # Prediction on Testing Data\n    y_pred = estimator.predict(X_test)\n\n    # Accuracy for y_test and y_pred\n    classifier_accuracy_score = accuracy_score(y_test, y_pred)\n    print(\"Accuracy Score:\\n\", classifier_accuracy_score, \"\\n\")\n\n    # Accuracy for y_test and y_pred\n    classifier_accuracy_score = balanced_accuracy_score(y_test, y_pred)\n    print(\"Balanced Accuracy Score:\\n\", classifier_accuracy_score, \"\\n\")\n\n    # Confusion Matrix\n    conf_mtx = confusion_matrix(y_test, y_pred)\n    print(\"Confusion Matrix:\\n\", conf_mtx, \"\\n\")\n\n    # Classification Report\n    class_rep = classification_report(y_test, y_pred)\n    print(\"Classification Report:\\n\", class_rep, \"\\n\")","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.458349Z","start_time":"2022-06-15T10:34:06.45375Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:48.985039Z","iopub.execute_input":"2022-06-15T10:38:48.985589Z","iopub.status.idle":"2022-06-15T10:38:49.00296Z","shell.execute_reply.started":"2022-06-15T10:38:48.985524Z","shell.execute_reply":"2022-06-15T10:38:49.001944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve, roc_curve\n\n\ndef visual(estimator, X_test, y_test):\n    # Precision - Recall Curve\n    yhat = estimator.predict_proba(X_test)\n    precision, recall, _ = precision_recall_curve(y_test, yhat[:, 1])\n    plt.figure(dpi=100, figsize=(15, 6))\n    plt.subplot(121)\n    sns.lineplot([0, 1], [1, 0], linestyle='--', label='No Skill')\n    sns.lineplot(recall, precision, marker='.', label='skill')\n    plt.title(\"Recall vs Precision Curve\")\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.legend()\n\n    # ROC Curve\n    plt.subplot(122)\n    sns.lineplot([0, 1], [0, 1], linestyle='--', label='No Skill')\n    fpr, tpr, _ = roc_curve(y_test, yhat[:, 1])\n    sns.lineplot(fpr, tpr, marker='.', label='Skill')\n    plt.title(\"ROC Curve\")\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend()\n    plt.show()\n\n    # Calibrate curve\n    plt.subplot()\n    from sklearn.calibration import calibration_curve\n    sns.lineplot([0, 1], [0, 1], linestyle='--', label='No Skill')\n    fop, mpv = calibration_curve(y_test, yhat[:, 1], n_bins=10)\n    sns.lineplot(fop, mpv, marker='.', label='Skill')\n    plt.title(\"Calibrate curve\")\n    plt.legend()\n    plt.show()","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.46651Z","start_time":"2022-06-15T10:34:06.459877Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:49.006572Z","iopub.execute_input":"2022-06-15T10:38:49.007294Z","iopub.status.idle":"2022-06-15T10:38:49.018883Z","shell.execute_reply.started":"2022-06-15T10:38:49.007258Z","shell.execute_reply":"2022-06-15T10:38:49.018054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the context of binary classification, calibration refers to the process of transforming the output scores from a binary classifier to class probabilities. If we think of the classifier as a “black box” that transforms input data into a score, we can think of calibration as a post-processing step that converts the score into a probability of the observation belonging to class 1.\n\nsource: https://statisticaloddsandends.wordpress.com/2020/10/07/what-is-calibration/","metadata":{"ExecuteTime":{"end_time":"2022-06-14T14:38:04.922504Z","start_time":"2022-06-14T14:38:04.917409Z"},"run_control":{"marked":true}}},{"cell_type":"code","source":"from IPython.display import Image\n\nImage(\n    url=\n    'https://statisticaloddsandends.files.wordpress.com/2020/10/pipeline.png?w=768&h=86',\n    width=700)","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.473645Z","start_time":"2022-06-15T10:34:06.468546Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:49.019815Z","iopub.execute_input":"2022-06-15T10:38:49.020165Z","iopub.status.idle":"2022-06-15T10:38:49.03711Z","shell.execute_reply.started":"2022-06-15T10:38:49.020135Z","shell.execute_reply":"2022-06-15T10:38:49.036162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\n\n\ndef calibration(estimator, X_train, y_train, method='sigmoid'):\n    calibration = CalibratedClassifierCV(base_estimator=estimator.model_,\n                                         n_jobs=-1,\n                                         cv='prefit',\n                                         method=method)\n    calibration.fit(X_train, y_train)\n    return calibration","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.479238Z","start_time":"2022-06-15T10:34:06.474661Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:49.038402Z","iopub.execute_input":"2022-06-15T10:38:49.039456Z","iopub.status.idle":"2022-06-15T10:38:49.050423Z","shell.execute_reply.started":"2022-06-15T10:38:49.03941Z","shell.execute_reply":"2022-06-15T10:38:49.049471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def brier_score(estimator, X_test, y_test):\n    from sklearn.metrics import brier_score_loss\n    yhat = estimator.predict_proba(X_test)\n    print(f'brier_score_loss = {brier_score_loss(y_test, yhat[:, 1])}')","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.483813Z","start_time":"2022-06-15T10:34:06.480761Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:49.051665Z","iopub.execute_input":"2022-06-15T10:38:49.052252Z","iopub.status.idle":"2022-06-15T10:38:49.060034Z","shell.execute_reply.started":"2022-06-15T10:38:49.052209Z","shell.execute_reply":"2022-06-15T10:38:49.059324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def full_evaluate(estimator, X_train, y_train, X_test, y_test):\n    evaluate(estimator, X_train, y_train, X_test, y_test)\n    brier_score(estimator, X_test, y_test)\n    visual(estimator, X_test, y_test)","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.489424Z","start_time":"2022-06-15T10:34:06.485337Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:49.061131Z","iopub.execute_input":"2022-06-15T10:38:49.062113Z","iopub.status.idle":"2022-06-15T10:38:49.070845Z","shell.execute_reply.started":"2022-06-15T10:38:49.062069Z","shell.execute_reply":"2022-06-15T10:38:49.07003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{"ExecuteTime":{"end_time":"2022-06-14T14:38:57.747934Z","start_time":"2022-06-14T14:38:57.745384Z"}}},{"cell_type":"code","source":"def modeling(estimator, param_grid=None):\n    estimator = CustomClassifier(estimator)\n    if param_grid == None:\n        pass\n    else:\n        estimator.set_param_grid(param_grid)\n\n    estimator.set_scaler(DataFramePreprocessing().mapper())\n\n    estimator.set_cv(cv=kf)\n    return estimator","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.494515Z","start_time":"2022-06-15T10:34:06.49044Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:49.072224Z","iopub.execute_input":"2022-06-15T10:38:49.072665Z","iopub.status.idle":"2022-06-15T10:38:49.08645Z","shell.execute_reply.started":"2022-06-15T10:38:49.072514Z","shell.execute_reply":"2022-06-15T10:38:49.085631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def full_evaluate_fit(estimator):\n    estimator.fit(X_train, y_train)\n    try:\n        print(estimator.model_.best_params_)\n    except:\n        pass\n    cal = calibration(estimator, X_train, y_train, method='isotonic')\n    cal_sig = calibration(estimator, X_train, y_train)\n    print('full_evaluate(estimator.model_)')\n    full_evaluate(estimator.model_, X_train, y_train, X_test, y_test)\n    print('full_evaluate(cal)')\n    full_evaluate(cal, X_train, y_train, X_test, y_test)\n    print('full_evaluate(cal_sig)')\n    full_evaluate(cal_sig, X_train, y_train, X_test, y_test)\n    return estimator.model_, cal, cal_sig","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.500118Z","start_time":"2022-06-15T10:34:06.49604Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:49.088662Z","iopub.execute_input":"2022-06-15T10:38:49.089838Z","iopub.status.idle":"2022-06-15T10:38:49.097856Z","shell.execute_reply.started":"2022-06-15T10:38:49.089793Z","shell.execute_reply":"2022-06-15T10:38:49.097083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimator = modeling(\n    LGBMClassifier(\n        boosting_type='dart',\n        n_jobs=-1,\n        random_state=0,\n        class_weight= 'balanced'\n    ), lgbm_param_grid)","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.511798Z","start_time":"2022-06-15T10:34:06.502146Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:49.099177Z","iopub.execute_input":"2022-06-15T10:38:49.099941Z","iopub.status.idle":"2022-06-15T10:38:49.124498Z","shell.execute_reply.started":"2022-06-15T10:38:49.099902Z","shell.execute_reply":"2022-06-15T10:38:49.12369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mapper = DataFramePreprocessing().mapper()\n\nX_train_mapped= mapper.fit_transform(X_train)\nX_train_mapped","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:06.628079Z","start_time":"2022-06-15T10:34:06.513323Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:49.125704Z","iopub.execute_input":"2022-06-15T10:38:49.126109Z","iopub.status.idle":"2022-06-15T10:38:49.276315Z","shell.execute_reply.started":"2022-06-15T10:38:49.126071Z","shell.execute_reply":"2022-06-15T10:38:49.275426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vf,gf = outlier_rejection(X_train_mapped,y_train)\nvf.shape","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:07.051548Z","start_time":"2022-06-15T10:34:06.629099Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:49.277866Z","iopub.execute_input":"2022-06-15T10:38:49.278516Z","iopub.status.idle":"2022-06-15T10:38:50.006405Z","shell.execute_reply.started":"2022-06-15T10:38:49.278473Z","shell.execute_reply":"2022-06-15T10:38:50.00574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#raise()","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:34:07.056118Z","start_time":"2022-06-15T10:34:07.05358Z"},"execution":{"iopub.status.busy":"2022-06-15T10:38:50.007398Z","iopub.execute_input":"2022-06-15T10:38:50.007734Z","iopub.status.idle":"2022-06-15T10:38:50.01146Z","shell.execute_reply.started":"2022-06-15T10:38:50.007705Z","shell.execute_reply":"2022-06-15T10:38:50.010843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"code","source":"model, model_calib, model_calib_sig = full_evaluate_fit(estimator)","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:35:18.312146Z","start_time":"2022-06-15T10:34:07.057642Z"},"scrolled":true,"execution":{"iopub.status.busy":"2022-06-15T10:38:50.012396Z","iopub.execute_input":"2022-06-15T10:38:50.013027Z","iopub.status.idle":"2022-06-15T10:41:35.909827Z","shell.execute_reply.started":"2022-06-15T10:38:50.012993Z","shell.execute_reply":"2022-06-15T10:41:35.908878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('../input/spaceship-titanic/sample_submission.csv')\ny_preds = model_calib.predict(test)\nsub['Transported'] = y_preds\nsub.to_csv('submission.csv', index=False)","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:35:19.892332Z","start_time":"2022-06-15T10:35:18.31367Z"},"run_control":{"marked":true},"execution":{"iopub.status.busy":"2022-06-15T10:41:35.911146Z","iopub.execute_input":"2022-06-15T10:41:35.911743Z","iopub.status.idle":"2022-06-15T10:41:39.010192Z","shell.execute_reply.started":"2022-06-15T10:41:35.911702Z","shell.execute_reply":"2022-06-15T10:41:39.009326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_calib why i always use it? calibration with method='isotonic'. In my experience, almost always better than Sigmoid.","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:35:19.897416Z","start_time":"2022-06-15T10:35:19.893856Z"},"execution":{"iopub.status.busy":"2022-06-15T10:41:39.011352Z","iopub.execute_input":"2022-06-15T10:41:39.012209Z","iopub.status.idle":"2022-06-15T10:41:39.016815Z","shell.execute_reply.started":"2022-06-15T10:41:39.012165Z","shell.execute_reply":"2022-06-15T10:41:39.015774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6,6))\nsub['Transported'].value_counts().plot.pie(explode=[0.1,0.1], autopct='%1.1f%%', shadow=True, textprops={'fontsize':16}).set_title(\"Prediction distribution\")","metadata":{"ExecuteTime":{"end_time":"2022-06-15T10:35:19.969825Z","start_time":"2022-06-15T10:35:19.898938Z"},"scrolled":true,"execution":{"iopub.status.busy":"2022-06-15T10:41:39.01828Z","iopub.execute_input":"2022-06-15T10:41:39.01899Z","iopub.status.idle":"2022-06-15T10:41:39.391281Z","shell.execute_reply.started":"2022-06-15T10:41:39.018948Z","shell.execute_reply":"2022-06-15T10:41:39.389981Z"},"trusted":true},"execution_count":null,"outputs":[]}]}