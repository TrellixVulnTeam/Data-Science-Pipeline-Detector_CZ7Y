{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"I am a student from Indonesia that is currently learning Machine Learning and Data Science.\n\nIn this code, I want to show how I approach this problem.","metadata":{}},{"cell_type":"markdown","source":"Spaceship Titanic is what we can say as Titanic - Machine Learning from Disaster 2.0.\n\nIt has some similiarities with that competition, the background, the features, etc.","metadata":{}},{"cell_type":"markdown","source":"# Importing Library","metadata":{}},{"cell_type":"markdown","source":"So we start by Importing necessary library","metadata":{}},{"cell_type":"code","source":"import numpy as np # Dealing with array manipulation\nimport pandas as pd # Data manipulation\nfrom matplotlib import pyplot as plt # Making graph\nplt.rcParams['figure.figsize'] = (17, 11)\nplt.style.use('seaborn-darkgrid')\nimport seaborn as sns # Same as matplotlib, but more simple\nsns.set_style('darkgrid')\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, LabelEncoder # Preprocessing the data\nfrom sklearn.model_selection import KFold, RepeatedKFold, RepeatedStratifiedKFold, StratifiedKFold, train_test_split # To split the data for train and validation\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score, balanced_accuracy_score # Metrics (To measure how accurate our predictions are)\n\nfrom catboost import CatBoostClassifier # The algorithm we use\n\nfrom warnings import filterwarnings, simplefilter\nfilterwarnings('ignore') # To shut the warnings (Red-y thing but not an Error)\nsimplefilter('ignore')\nimport gc # Garbage collector, to free memory\ngc.enable()\nfrom tqdm.auto import tqdm # To give us a progress bar\nfrom IPython.display import clear_output # To clear the output when it's too much","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-20T10:29:47.593679Z","iopub.execute_input":"2022-04-20T10:29:47.594767Z","iopub.status.idle":"2022-04-20T10:29:49.15799Z","shell.execute_reply.started":"2022-04-20T10:29:47.594654Z","shell.execute_reply":"2022-04-20T10:29:49.157351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading the Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/spaceship-titanic/train.csv') # Train, the data we have\ntest = pd.read_csv('../input/spaceship-titanic/test.csv') # Test, the data we have to predict\nsub = pd.read_csv('../input/spaceship-titanic/sample_submission.csv') # Sample submission, how we should submit our predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:29:49.160584Z","iopub.execute_input":"2022-04-20T10:29:49.160895Z","iopub.status.idle":"2022-04-20T10:29:49.269863Z","shell.execute_reply.started":"2022-04-20T10:29:49.160853Z","shell.execute_reply":"2022-04-20T10:29:49.269079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Taking the Group out of the PassengerId column","metadata":{}},{"cell_type":"markdown","source":"We're gonna take the group of the people from the PassengerId to use that to impute missing value\n\n> People in a group are often family members, but not always.\n\nThat's quoted from Spaceship Titanic data description. Which means that same group tend to have same \"something\" and we'll see what this something is","metadata":{}},{"cell_type":"code","source":"train[['Group', 'Id']] = train['PassengerId'].str.split('_', expand = True)\ntest[['Group', 'Id']] = test['PassengerId'].str.split('_', expand = True)\ntrain.drop(['PassengerId'], axis = 1, inplace = True)\ntest.drop(['PassengerId'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:29:49.271038Z","iopub.execute_input":"2022-04-20T10:29:49.271251Z","iopub.status.idle":"2022-04-20T10:29:49.326803Z","shell.execute_reply.started":"2022-04-20T10:29:49.271224Z","shell.execute_reply":"2022-04-20T10:29:49.325906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We concatenate train and test because there are some people with same group but separated by train and test split","metadata":{}},{"cell_type":"code","source":"data = train.append(test).reset_index(drop = True)\ndata = data.sort_values('Group') # We sort by group so the imputing would be easire\ndata","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:29:49.330267Z","iopub.execute_input":"2022-04-20T10:29:49.330773Z","iopub.status.idle":"2022-04-20T10:29:49.40158Z","shell.execute_reply.started":"2022-04-20T10:29:49.330724Z","shell.execute_reply":"2022-04-20T10:29:49.400756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making columns that tell if some columns has NAN Value","metadata":{}},{"cell_type":"markdown","source":"So before we impute the missing value, we give some **mark** for the missing value\n\nBecause we're not sure if our imputer is going to be accurate, so we add an indicator","metadata":{}},{"cell_type":"code","source":"data_na = data.isna().astype(int)\ndata_na.drop(['Transported', 'Group', 'Id'], axis = 1, inplace = True) # Drop these columns because none of these columns have NaN value\ndata = data.join(data_na, rsuffix = '_nan')\ndata","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:29:49.4032Z","iopub.execute_input":"2022-04-20T10:29:49.403758Z","iopub.status.idle":"2022-04-20T10:29:49.467011Z","shell.execute_reply.started":"2022-04-20T10:29:49.403713Z","shell.execute_reply":"2022-04-20T10:29:49.466093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making a new DataFrame for the Group that is not alone","metadata":{}},{"cell_type":"markdown","source":"So here we are, we'll start imputing by taking group that has more than one person","metadata":{}},{"cell_type":"code","source":"new_df = pd.DataFrame()\nfor i, r in tqdm(data.iterrows(), total = len(data)) :\n    try :\n        if data.iloc[i, 13] == data.iloc[i+1, 13] :\n            new_df = new_df.append(data.iloc[i])\n            new_df = new_df.append(data.iloc[i+1])\n    except :\n        print('End of the row.')\ndisplay(new_df)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:29:49.468646Z","iopub.execute_input":"2022-04-20T10:29:49.468952Z","iopub.status.idle":"2022-04-20T10:30:54.494359Z","shell.execute_reply.started":"2022-04-20T10:29:49.468911Z","shell.execute_reply":"2022-04-20T10:30:54.493439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have to drop any duplicatijng value because my code above isn't effective and efficient (not good).\n\nYou guys may fix it so it'll be more effective and efficient","metadata":{}},{"cell_type":"code","source":"new_df = new_df.drop_duplicates().sort_values('Group') # Again, we sort by group to make things right\nnew_df","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:30:54.495564Z","iopub.execute_input":"2022-04-20T10:30:54.495786Z","iopub.status.idle":"2022-04-20T10:30:54.572554Z","shell.execute_reply.started":"2022-04-20T10:30:54.495756Z","shell.execute_reply":"2022-04-20T10:30:54.571623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, before we drop_duplicate the data, we had 7380 rows. Now we have 5825, indicating that there WERE duplicated values.","metadata":{}},{"cell_type":"markdown","source":"Now, we'll make a list of columns that we will impute","metadata":{}},{"cell_type":"code","source":"inspected_col = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Name', 'VIP']\ninspected_col_spend = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:30:54.573595Z","iopub.execute_input":"2022-04-20T10:30:54.573804Z","iopub.status.idle":"2022-04-20T10:30:54.57888Z","shell.execute_reply.started":"2022-04-20T10:30:54.573777Z","shell.execute_reply":"2022-04-20T10:30:54.577965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I make one for categorical columns and one for continuous columns.","metadata":{}},{"cell_type":"markdown","source":"And also, we take the unique value of the group because we're going to do an iteration on it","metadata":{}},{"cell_type":"code","source":"group = new_df['Group'].unique()\ngroup","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:30:54.580478Z","iopub.execute_input":"2022-04-20T10:30:54.580785Z","iopub.status.idle":"2022-04-20T10:30:54.593113Z","shell.execute_reply.started":"2022-04-20T10:30:54.580744Z","shell.execute_reply":"2022-04-20T10:30:54.592314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Imputing","metadata":{}},{"cell_type":"markdown","source":"And now the time comes, this is how the imputing works","metadata":{}},{"cell_type":"code","source":"for col in tqdm(inspected_col) : # Make an iteration on the wanted columns\n    for g in tqdm(group) : # Make an iteration (again) on the unique value\n        try :\n            tofill = data.loc[data['Group'] == g, col].mode()[0] # We'll try imputing the NaN value on those group by taking the mode\n        except : # We add except syntax here in case in one group, everything is NaN\n            print(f'in {col} One Group has all NAN Value, inputing most frequent')\n            tofill = data[col].mode()[0] # If a group doesn't have any value (All NaN), then we'll fill with the mode of the whole data\n        data.loc[data['Group'] == g, col] = data.loc[data['Group'] == g, col].fillna(tofill) # We apply the change to our \"data\" DataFrame\n        new_df.loc[data['Group'] == g, col] = new_df.loc[data['Group'] == g, col].fillna(tofill) # And new_df as well","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:30:54.59697Z","iopub.execute_input":"2022-04-20T10:30:54.597375Z","iopub.status.idle":"2022-04-20T10:34:13.366361Z","shell.execute_reply.started":"2022-04-20T10:30:54.597338Z","shell.execute_reply":"2022-04-20T10:34:13.365576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This one goes the same","metadata":{}},{"cell_type":"code","source":"for col in tqdm(inspected_col_spend) : # Make an iteration on the wanted columns (Continuous column)\n    for g in tqdm(group) : # Make an iteration on the group\n        try :\n            tofill = data.loc[data['Group'] == g, col].min() # We try imputing the NaN value with the minimal value in the group\n                                                             # You actually could try using .mean(), .max(), .median() whatever you want\n        except : # As before, if one group is just filled with NaN value\n            print(f'in {col} One Group has all NAN Value, imputing most frequent')\n            tofill = data[col].median() # We impute that group with the median of the whole data.\n                                        # Again, you could use mean, max, median, whatever you want.\n        data.loc[data['Group'] == g, col] = data.loc[data['Group'] == g, col].fillna(tofill) # Apply the change to variable data\n        new_df.loc[data['Group'] == g, col] = new_df.loc[data['Group'] == g, col].fillna(tofill) # And new_df","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:34:13.367777Z","iopub.execute_input":"2022-04-20T10:34:13.368005Z","iopub.status.idle":"2022-04-20T10:36:48.74143Z","shell.execute_reply.started":"2022-04-20T10:34:13.367962Z","shell.execute_reply":"2022-04-20T10:36:48.740661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.isna().sum() # Checking the NaN value on the new_df DataFrame","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:36:48.74258Z","iopub.execute_input":"2022-04-20T10:36:48.742811Z","iopub.status.idle":"2022-04-20T10:36:48.757956Z","shell.execute_reply.started":"2022-04-20T10:36:48.742782Z","shell.execute_reply":"2022-04-20T10:36:48.757218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great, all the columns that we wanted to impute has no longer NaN value","metadata":{}},{"cell_type":"code","source":"data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:36:48.759437Z","iopub.execute_input":"2022-04-20T10:36:48.759853Z","iopub.status.idle":"2022-04-20T10:36:48.781757Z","shell.execute_reply.started":"2022-04-20T10:36:48.759781Z","shell.execute_reply":"2022-04-20T10:36:48.781201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, in `data`, tnere are still some NaN value","metadata":{}},{"cell_type":"markdown","source":"This is because data has NaN values in the person that is alone in their group.\n\nIn this case, we'll just use some simple imputing method","metadata":{}},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:36:48.782561Z","iopub.execute_input":"2022-04-20T10:36:48.782754Z","iopub.status.idle":"2022-04-20T10:36:48.817812Z","shell.execute_reply.started":"2022-04-20T10:36:48.782729Z","shell.execute_reply":"2022-04-20T10:36:48.81728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we impute those value, I want to split the Cabin and the Name first","metadata":{}},{"cell_type":"code","source":"data[['deck', 'num', 'side']] = data['Cabin'].str.split('/', expand = True) # We split the cabin to be 3 part, deck, num, and side\ndata[['FirstName', 'LastName']] = data['Name'].str.split(' ', expand = True) # We split the name to be 2 part, First and Last Name\ndata.drop(['Cabin', 'FirstName', 'Name', 'Group', 'Id'], axis = 1, inplace = True) # We drop the unnecessary column\ndata['num'] = data['num'].astype(float) # We make num as a float (Cause they are high-cardinality)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:36:48.818785Z","iopub.execute_input":"2022-04-20T10:36:48.819305Z","iopub.status.idle":"2022-04-20T10:36:49.00166Z","shell.execute_reply.started":"2022-04-20T10:36:48.819265Z","shell.execute_reply":"2022-04-20T10:36:49.000729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We make a variable group of columns where one is continuous and the other is categorical","metadata":{}},{"cell_type":"code","source":"cont = data.select_dtypes(float).columns.tolist()\ncat = [col for col in data.columns if not col in cont and not col == 'Transported']","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:36:49.002784Z","iopub.execute_input":"2022-04-20T10:36:49.00301Z","iopub.status.idle":"2022-04-20T10:36:49.009845Z","shell.execute_reply.started":"2022-04-20T10:36:49.002982Z","shell.execute_reply":"2022-04-20T10:36:49.009092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple Imputing","metadata":{}},{"cell_type":"markdown","source":"We use the regular imputing method. \n\n* We fill continuous with its median\n* And categorical with its mode","metadata":{}},{"cell_type":"code","source":"for col in cont :\n    data[col].fillna(data[col].median(), inplace = True)\n\nfor col in cat :\n    data[col].fillna(data[col].mode()[0], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:36:49.010741Z","iopub.execute_input":"2022-04-20T10:36:49.01136Z","iopub.status.idle":"2022-04-20T10:36:49.060711Z","shell.execute_reply.started":"2022-04-20T10:36:49.011325Z","shell.execute_reply":"2022-04-20T10:36:49.060133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making a \"Relatives\" Feature","metadata":{}},{"cell_type":"markdown","source":"This feature just tells how many relatives people have by their Last Name","metadata":{}},{"cell_type":"code","source":"tqdm.pandas()\ndata['Relatives'] = data['LastName'].progress_apply(lambda x : sum(x == data['LastName']) - 1)\ndata.drop(['LastName'], axis = 1, inplace = True)\ncat.remove('LastName')\ndata","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:36:49.061781Z","iopub.execute_input":"2022-04-20T10:36:49.062011Z","iopub.status.idle":"2022-04-20T10:37:40.234795Z","shell.execute_reply.started":"2022-04-20T10:36:49.061981Z","shell.execute_reply":"2022-04-20T10:37:40.234005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Label Encoding","metadata":{}},{"cell_type":"markdown","source":"This is just making the categorical data type to be integer, so It'll be readable to our Machine Learning Algorithm. No big deal!","metadata":{}},{"cell_type":"code","source":"for col in cat :\n    le = LabelEncoder()\n    data[col] = le.fit_transform(data[col])","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:37:40.236235Z","iopub.execute_input":"2022-04-20T10:37:40.236795Z","iopub.status.idle":"2022-04-20T10:37:40.267438Z","shell.execute_reply.started":"2022-04-20T10:37:40.236756Z","shell.execute_reply":"2022-04-20T10:37:40.266624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Total Spending","metadata":{}},{"cell_type":"markdown","source":"We also make a feature that is just the summation of every bill that each passenger paid.","metadata":{}},{"cell_type":"code","source":"spending = ['RoomService', 'FoodCourt', 'ShoppingMall', 'VRDeck', 'Spa']\ndata['TotalSpend'] = data[spending].sum(axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:37:40.268643Z","iopub.execute_input":"2022-04-20T10:37:40.26887Z","iopub.status.idle":"2022-04-20T10:37:40.276962Z","shell.execute_reply.started":"2022-04-20T10:37:40.268842Z","shell.execute_reply":"2022-04-20T10:37:40.276337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And this feature basically to indicate whether each passenger is Adult or not","metadata":{}},{"cell_type":"code","source":"data['Adult'] = (data['Age'] > 18).astype(int)\ncat = cat + ['Adult']","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:37:40.278124Z","iopub.execute_input":"2022-04-20T10:37:40.27843Z","iopub.status.idle":"2022-04-20T10:37:40.289082Z","shell.execute_reply.started":"2022-04-20T10:37:40.278399Z","shell.execute_reply":"2022-04-20T10:37:40.288487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We sort the index cause we sorted by the Group before, we do this so we'll be able to split the data to be train and test again","metadata":{}},{"cell_type":"code","source":"data = data.sort_index()\ndata","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:37:40.2903Z","iopub.execute_input":"2022-04-20T10:37:40.29073Z","iopub.status.idle":"2022-04-20T10:37:40.333727Z","shell.execute_reply.started":"2022-04-20T10:37:40.290701Z","shell.execute_reply":"2022-04-20T10:37:40.333186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Re-splitting the variable \"data\"","metadata":{}},{"cell_type":"markdown","source":"As we have done many things with the concatenate of train and test, we separate them again","metadata":{}},{"cell_type":"code","source":"train = data.loc[0:len(train)-1]\ntest = data.loc[len(train):len(train) + len(test)].reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:37:40.33498Z","iopub.execute_input":"2022-04-20T10:37:40.335367Z","iopub.status.idle":"2022-04-20T10:37:40.341763Z","shell.execute_reply.started":"2022-04-20T10:37:40.335336Z","shell.execute_reply":"2022-04-20T10:37:40.341203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking for NaN Values","metadata":{}},{"cell_type":"code","source":"train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:37:40.343122Z","iopub.execute_input":"2022-04-20T10:37:40.34343Z","iopub.status.idle":"2022-04-20T10:37:40.357191Z","shell.execute_reply.started":"2022-04-20T10:37:40.3434Z","shell.execute_reply":"2022-04-20T10:37:40.356605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:37:40.358442Z","iopub.execute_input":"2022-04-20T10:37:40.359019Z","iopub.status.idle":"2022-04-20T10:37:40.367081Z","shell.execute_reply.started":"2022-04-20T10:37:40.358988Z","shell.execute_reply":"2022-04-20T10:37:40.366348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hooray! No more NaN values.\n\n`test` DataFrame has Transported as NaN value because that's what we are trying to predict","metadata":{}},{"cell_type":"markdown","source":"So we'll drop them.","metadata":{}},{"cell_type":"code","source":"test.drop(['Transported'], axis = 1, inplace= True)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:37:40.368349Z","iopub.execute_input":"2022-04-20T10:37:40.368721Z","iopub.status.idle":"2022-04-20T10:37:40.374725Z","shell.execute_reply.started":"2022-04-20T10:37:40.368691Z","shell.execute_reply":"2022-04-20T10:37:40.374095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we pop the Transported columns on `train`, just to make things easier","metadata":{}},{"cell_type":"code","source":"y = train.pop('Transported').astype(int)\ny","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:37:40.376077Z","iopub.execute_input":"2022-04-20T10:37:40.376369Z","iopub.status.idle":"2022-04-20T10:37:40.389314Z","shell.execute_reply.started":"2022-04-20T10:37:40.376336Z","shell.execute_reply":"2022-04-20T10:37:40.388547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KFold","metadata":{}},{"cell_type":"markdown","source":"What I am using here is a RepeatedKFold, I'm not just using KFold because I want my algorithm to have different combination of train-and-validation so that it won't have a high variance","metadata":{}},{"cell_type":"code","source":"kf = RepeatedKFold(n_splits = 10, n_repeats = 50, random_state = 27)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:37:40.393575Z","iopub.execute_input":"2022-04-20T10:37:40.394455Z","iopub.status.idle":"2022-04-20T10:37:40.399552Z","shell.execute_reply.started":"2022-04-20T10:37:40.394411Z","shell.execute_reply":"2022-04-20T10:37:40.398989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm using 10 folds and 50 repeats here, so we'll do 500 train-test which sounds bonker xD\n\nBut whatever you guys want to use tho, it's okay...","metadata":{}},{"cell_type":"markdown","source":"Note : I am using default setting of CatBoost. Because when I tried using Hyper-Parameter-optimized CatBoost with Optuna, it did not do well :(","metadata":{}},{"cell_type":"code","source":"scores = []\ntest_preds = []\nfor i, (t, v) in tqdm(enumerate(kf.split(train)), total = 500) :\n    xtrain = train.iloc[t, :] # Take the indices that KFold return as train set\n    xval = train.iloc[v, :] # Take the indices that KFold return as validation set\n    xtest = test.copy() # Make a copy of the test\n    ytrain = y.iloc[t] # Take the indices that KFold return as y-train set\n    yval = y.iloc[v] # Take the indices that KFold return as y-validation set\n    \n    model = CatBoostClassifier( # Define the model\n        iterations = 2000, # How many iterations we should do\n        random_state = 0, # The Reproducibility\n        verbose = 0, # Progress bar (in case you want to see)\n        boost_from_average = True, # The initial guess will start from the average of the data\n        eval_metric = 'Accuracy', # Measurement that we use to stop the training\n        cat_features = cat # The categorical feature\n    )\n    model.fit(\n        xtrain, ytrain, # We fit the model to xtrain and ytrain\n        early_stopping_rounds = 1000, # If there's not any improvement in 1000 iterations, the model will stop\n        use_best_model = True, # After the iterations stop, the model will use the best score\n        eval_set = (xval, yval) # The validation to stop the training\n    ) \n    yhat = model.predict(xval) # We predict the validation set\n    score = balanced_accuracy_score(yval, yhat) # And then measure it with accuracy metrics\n    scores.append(score) # We add the score to the list of scores that is defined outside the KFold iterations\n    ypred = model.predict_proba(xtest)[:, 1] # Predicting the probability of xtest (Cause we'll take the mean of those prediction)\n    test_preds.append(ypred) # Append the prediction to test_preds variable\n    print(f'FOLD {i} : {score}')\n    del xtrain, xval, xtest, ytrain, yval, model # We delete all the variable to free our memory\n    gc.collect() # using gc to free memory","metadata":{"execution":{"iopub.status.busy":"2022-04-20T10:37:40.40184Z","iopub.execute_input":"2022-04-20T10:37:40.40214Z","iopub.status.idle":"2022-04-20T12:46:22.256744Z","shell.execute_reply.started":"2022-04-20T10:37:40.402098Z","shell.execute_reply":"2022-04-20T12:46:22.256172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that, we see the mean and the standard deviation of our scores in validation.","metadata":{}},{"cell_type":"code","source":"print(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T12:46:22.257783Z","iopub.execute_input":"2022-04-20T12:46:22.258232Z","iopub.status.idle":"2022-04-20T12:46:22.263829Z","shell.execute_reply.started":"2022-04-20T12:46:22.258199Z","shell.execute_reply":"2022-04-20T12:46:22.26291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Update :\n\nHere, I just want to mess with the distribution of the score, is it normal or not.","metadata":{}},{"cell_type":"code","source":"_ = sns.distplot(scores)\nplt.axvline(np.median(scores), color = 'red')\nplt.annotate('Median', (np.median(scores) + .0001, 30), (np.median(scores) + .005, 32), arrowprops = {'color' : 'black', 'arrowstyle' : 'simple'})\n\nplt.axvline(np.mean(scores), color = 'red')\nplt.annotate(\n    'Mean', (np.mean(scores) - .0001, 27), (np.mean(scores) - .005, 24), arrowprops = {'color' : 'black', 'arrowstyle' : '->'}\n)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T12:46:22.264962Z","iopub.execute_input":"2022-04-20T12:46:22.265214Z","iopub.status.idle":"2022-04-20T12:46:22.747128Z","shell.execute_reply.started":"2022-04-20T12:46:22.265177Z","shell.execute_reply":"2022-04-20T12:46:22.74634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Turns out it's a normal distribution.\n\nNow, I'll try make a PDF out of it","metadata":{}},{"cell_type":"code","source":"# The PDF of Normal Distribution\ndef normalcurve(data) :\n    data = np.array(data)\n    z = (data - np.mean(data)) / np.std(data)\n    numerator = np.exp(-.5*z**2)\n    denominator = np.std(data)*np.sqrt(2*np.pi)\n    return numerator/denominator","metadata":{"execution":{"iopub.status.busy":"2022-04-20T12:46:22.748613Z","iopub.execute_input":"2022-04-20T12:46:22.749362Z","iopub.status.idle":"2022-04-20T12:46:22.756527Z","shell.execute_reply.started":"2022-04-20T12:46:22.749317Z","shell.execute_reply":"2022-04-20T12:46:22.75549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_pdf = normalcurve(scores)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T12:46:22.757738Z","iopub.execute_input":"2022-04-20T12:46:22.758037Z","iopub.status.idle":"2022-04-20T12:46:22.770135Z","shell.execute_reply.started":"2022-04-20T12:46:22.758006Z","shell.execute_reply":"2022-04-20T12:46:22.769344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = sns.lineplot(x = scores, y = score_pdf)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T12:46:22.77122Z","iopub.execute_input":"2022-04-20T12:46:22.771441Z","iopub.status.idle":"2022-04-20T12:46:23.212013Z","shell.execute_reply.started":"2022-04-20T12:46:22.771414Z","shell.execute_reply":"2022-04-20T12:46:23.211117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DANG!!! That's a perfect curve right there...","metadata":{}},{"cell_type":"markdown","source":"We can approximate our score by taking mean +/- (2 * std). (95 % value that is possible)","metadata":{}},{"cell_type":"markdown","source":"Our worst score could be :","metadata":{}},{"cell_type":"code","source":"worst = np.mean(scores) - 2 * np.std(scores)\nprint('Worst Score Approximation :', worst)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T12:46:23.213167Z","iopub.execute_input":"2022-04-20T12:46:23.213422Z","iopub.status.idle":"2022-04-20T12:46:23.219648Z","shell.execute_reply.started":"2022-04-20T12:46:23.213385Z","shell.execute_reply":"2022-04-20T12:46:23.218732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And our best score could be :","metadata":{}},{"cell_type":"code","source":"best = np.mean(scores) + 2 * np.std(scores)\nprint('Best Score Approximation :', best)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T12:46:23.220827Z","iopub.execute_input":"2022-04-20T12:46:23.221051Z","iopub.status.idle":"2022-04-20T12:46:23.233956Z","shell.execute_reply.started":"2022-04-20T12:46:23.221025Z","shell.execute_reply":"2022-04-20T12:46:23.233106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note : They are not actually **worst** or **best**, they're just approximation.\n\nThe chance of the scores could beyond those is low, but not zero. If you ask me, it's around 5%","metadata":{}},{"cell_type":"markdown","source":"And note that our likelihood could reach the value of 30 because the range of our value is sooo narrow.\n\nIn the graph, it's only shown between 0.79 and 0.85. And the narrower the range of the value, the taller the curve.","metadata":{}},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"markdown","source":"## OVERFITTING ALERT !1!!1!","metadata":{}},{"cell_type":"markdown","source":"Know that we have 500 predictions for the test, so we average them with `np.mean(*, axis = 0)` numpy syntax\n\nAnd I want to make the distribution in the submission will be the SAME as the distribution in test set","metadata":{}},{"cell_type":"markdown","source":"But... how?\n\nThis is what called Leaderboard Probing and many kagglers have done this (I recall last year's TPS April people do it to have good scores)","metadata":{}},{"cell_type":"markdown","source":"So, I've accidentally submitted a bunch of ones and get 0.50689\n\nAnd because the metric is **accuracy**, I know that **0.50689** of the test set will be ones, rest is zero.","metadata":{}},{"cell_type":"markdown","source":"So, let's do that.\nLet's do this nasty thing","metadata":{}},{"cell_type":"markdown","source":"We'll make a pandas Series that has regular threshold to determine whether it's True or False (0.5)","metadata":{}},{"cell_type":"code","source":"regular = pd.Series(np.round(np.mean(test_preds, axis = 0)).astype(bool)) # You can just do this by using np.round\nregular","metadata":{"execution":{"iopub.status.busy":"2022-04-20T13:02:22.819956Z","iopub.execute_input":"2022-04-20T13:02:22.821048Z","iopub.status.idle":"2022-04-20T13:02:22.843899Z","shell.execute_reply.started":"2022-04-20T13:02:22.820989Z","shell.execute_reply":"2022-04-20T13:02:22.842921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we'll see the distribution","metadata":{}},{"cell_type":"code","source":"print(regular.value_counts() / len(regular))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T13:02:25.997922Z","iopub.execute_input":"2022-04-20T13:02:25.998278Z","iopub.status.idle":"2022-04-20T13:02:26.007406Z","shell.execute_reply.started":"2022-04-20T13:02:25.998239Z","shell.execute_reply":"2022-04-20T13:02:26.006104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can adjust the distribution in our submission so that it'll match with the test distribution (0.50689 and 0.48311) by adjusting the threshold","metadata":{}},{"cell_type":"code","source":"thresh = .512 # Adjustable!\nour_preds = np.where(np.mean(test_preds, axis = 0) > thresh, True, False) \n#Basically saying 'average the fold prediction, if the value is more than thresh, set it to True. If not, then False'\nour_preds_series = pd.Series(our_preds) # We wrap it in pandas Series so we can count the distribution\nprint(our_preds_series.value_counts() / len(our_preds_series)) # And show our distribution!","metadata":{"execution":{"iopub.status.busy":"2022-04-20T13:03:40.031086Z","iopub.execute_input":"2022-04-20T13:03:40.03141Z","iopub.status.idle":"2022-04-20T13:03:40.04635Z","shell.execute_reply.started":"2022-04-20T13:03:40.031368Z","shell.execute_reply":"2022-04-20T13:03:40.045727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Code above is the code that shows the distribution in the submission. If you want to make it the same as train's distribution, just adjust the `thresh` variable!\n\nIn this case, mine is around 0.512","metadata":{}},{"cell_type":"markdown","source":"And now, we just input the series to the sample submission as in format!","metadata":{}},{"cell_type":"code","source":"sub['Transported'] = our_preds_series\nsub.to_csv('submission.csv', index = False)\nsub","metadata":{"execution":{"iopub.status.busy":"2022-04-20T13:03:51.990897Z","iopub.execute_input":"2022-04-20T13:03:51.9919Z","iopub.status.idle":"2022-04-20T13:03:52.020733Z","shell.execute_reply.started":"2022-04-20T13:03:51.991837Z","shell.execute_reply":"2022-04-20T13:03:52.019916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('HUGE OVERFITTING!')","metadata":{"execution":{"iopub.status.busy":"2022-04-01T01:43:25.0579Z","iopub.execute_input":"2022-04-01T01:43:25.058231Z","iopub.status.idle":"2022-04-01T01:43:25.06386Z","shell.execute_reply.started":"2022-04-01T01:43:25.058197Z","shell.execute_reply":"2022-04-01T01:43:25.063023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note : I said that our worst score could be 0.799 something and best score 0.846 by plotting the normal curve above.\n\nThis is a different story with our threshold changing, cause in KFold we only used `predict` method, not `predict_proba`. And by default, any algorithm will take 0.5 as the threshold. So those worst/best scores approximation won't apply here.","metadata":{}},{"cell_type":"markdown","source":"And I truly apologize if my explanation is confusing :\".","metadata":{}}]}