{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport sklearn as sk\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nfrom numpy.random import seed\nseed(1)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-19T15:54:15.801206Z","iopub.execute_input":"2022-05-19T15:54:15.802423Z","iopub.status.idle":"2022-05-19T15:54:15.808975Z","shell.execute_reply.started":"2022-05-19T15:54:15.802358Z","shell.execute_reply":"2022-05-19T15:54:15.807345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Today I'm going to complete an Exploratory Data Analysis of the Spaceship Titanic Dataset, after cleaning the data. I will then prepare a machine learning pipeline, using a few different types of models, and comparing their efficacy. Finally, I will submit the model with the most accurate predictions to the competition, and see how I do. ","metadata":{}},{"cell_type":"code","source":"trainPath = \"../input/spaceship-titanic/train.csv\"\ntrainingData = pd.read_csv(trainPath)\ntrainingData.head()\ntestPath = \"../input/spaceship-titanic/test.csv\"\ntestingData = pd.read_csv(testPath)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:54:15.811172Z","iopub.execute_input":"2022-05-19T15:54:15.811741Z","iopub.status.idle":"2022-05-19T15:54:15.881392Z","shell.execute_reply.started":"2022-05-19T15:54:15.811692Z","shell.execute_reply":"2022-05-19T15:54:15.880256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Data is Formated as following according to the competition:\n* PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n* HomePlanet - The planet the passenger departed from, typically their planet of permanent residence.\n* CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n* Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n* Destination - The planet the passenger will be debarking to.\n* Age - The age of the passenger.\n* VIP - Whether the passenger has paid for special VIP service during the voyage.\n* RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n* Name - The first and last names of the passenger.\n* Transported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.\n\nAs we can see in these descriptions, some further work is required in order to fully process the data set, so lets add some columns to include the additional data that can be gleaned from what we have. ","metadata":{}},{"cell_type":"code","source":"trainingData[[\"GroupId\", \"GroupPassengerId\"]] = trainingData[\"PassengerId\"].str.split(\"_\", expand = True).astype(int)\ntrainingData[[\"Deck\", \"CabinNum\", \"Side\"]] = trainingData['Cabin'].str.split(\"/\", expand = True)\ntrainingData[\"TotalSpending\"] = trainingData[[\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]].sum(axis = 1)\ntestingData[[\"GroupId\", \"GroupPassengerId\"]] = testingData[\"PassengerId\"].str.split(\"_\", expand = True)\ntestingData[[\"Deck\", \"CabinNum\", \"Side\"]] = testingData['Cabin'].str.split(\"/\", expand = True)\ntestingData[\"TotalSpending\"] = testingData[[\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]].sum(axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:54:15.883457Z","iopub.execute_input":"2022-05-19T15:54:15.883699Z","iopub.status.idle":"2022-05-19T15:54:15.962145Z","shell.execute_reply.started":"2022-05-19T15:54:15.88367Z","shell.execute_reply":"2022-05-19T15:54:15.961082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. **Univariate Data Analysis**\n\nLets start with the quantitative variables, and then analyze the categorical variables \n","metadata":{}},{"cell_type":"code","source":"from matplotlib import figure\nsns.displot(trainingData, x = trainingData['TotalSpending'], binwidth = 1000, bins = 35)\nsns.displot(trainingData, x = trainingData['Age'], binwidth = 5, bins = 20 )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:54:15.963783Z","iopub.execute_input":"2022-05-19T15:54:15.964064Z","iopub.status.idle":"2022-05-19T15:54:16.626178Z","shell.execute_reply.started":"2022-05-19T15:54:15.964031Z","shell.execute_reply":"2022-05-19T15:54:16.624996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of the spending is skewed heavily right with several large outliers, while the age distribution is pretty evenly distributed by comparison. Now that we've seen the distributions of the quantitative variables, lets move to the categorical variables. ","metadata":{}},{"cell_type":"code","source":"num_cols = trainingData._get_numeric_data().columns\ncat_cols = list(set(trainingData.columns) - set(num_cols))\nprint(cat_cols)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:54:16.629572Z","iopub.execute_input":"2022-05-19T15:54:16.629857Z","iopub.status.idle":"2022-05-19T15:54:16.63743Z","shell.execute_reply.started":"2022-05-19T15:54:16.629824Z","shell.execute_reply":"2022-05-19T15:54:16.636146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(3,2, figsize = (15,10))\nsns.countplot(trainingData['Transported'], palette='Paired_r', ax = ax[0][0])\nsns.countplot(trainingData['HomePlanet'], palette='Set2', ax = ax[0][1])\nsns.countplot(trainingData['Destination'], palette='Paired_r', ax = ax[1][0])\nsns.countplot(trainingData['Side'], palette='Set2', ax = ax[1][1])\nsns.countplot(trainingData['Deck'], palette='Paired_r', ax = ax[2][0])\nsns.countplot(trainingData['VIP'], palette='Set2', ax = ax[2][1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:54:16.639769Z","iopub.execute_input":"2022-05-19T15:54:16.640413Z","iopub.status.idle":"2022-05-19T15:54:17.345774Z","shell.execute_reply.started":"2022-05-19T15:54:16.640362Z","shell.execute_reply":"2022-05-19T15:54:17.344716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. **Multivariate Data Analysis**\n\nThere are a few relationships that I am interested in in this segment: \n* VIP and Transported \n* Deck and Transported\n* Side and Transported\n* Age and Transported\n* TotalSpending and Transported \n\nNow I will create a plot that can visualize each of these relationships. ","metadata":{}},{"cell_type":"code","source":"sns.countplot(x = trainingData['VIP'], hue = trainingData['Transported'])\nplt.show()\nsns.countplot(x = trainingData.Deck, hue = trainingData.Transported)\nplt.show()\nsns.displot(x = trainingData.TotalSpending, hue = trainingData.Transported)\nplt.show()\nsns.kdeplot(x=trainingData.Age , hue=trainingData.Transported, palette='Paired_r', multiple = 'stack')\nplt.show()\nsns.countplot(x = trainingData.Side, hue = trainingData.Transported)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-19T15:54:17.347631Z","iopub.execute_input":"2022-05-19T15:54:17.347969Z","iopub.status.idle":"2022-05-19T15:54:20.697782Z","shell.execute_reply.started":"2022-05-19T15:54:17.347903Z","shell.execute_reply":"2022-05-19T15:54:20.696513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given that we expect roughly even amounts of people who are transported and those that aren't, we can see which variables have a major impact on chances of survival. For example, being on the starboard side of the ship is not exactly great for your health, as there is a larger proportion of people who get transported from that side of the ship than the other. Lets really quickly utilize a chi-squared test for independence to determine if the differences in these distributions are statistically significant. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ninitialobserved = trainingData[['Side', 'Transported']]\nfor col in initialobserved.columns:\n    initialobserved[col] = LabelEncoder().fit_transform(initialobserved[col])\n\ncount = pd.DataFrame(index = ['Transported', 'Not Transported'], columns = ['Port', 'Starboard'], dtype = int)\n\ndef createCount (row):\n    counttp = 0 \n    countts = 0\n    countntp = 0 \n    countnts = 0\n    if row['Side'] == 0 and row['Transported'] == 1:\n        counttp += 1\n    if row['Side'] == 1 and row['Transported'] == 1:\n        countts += 1\n    if row['Side'] == 0 and row['Transported'] == 0:\n        countntp += 1\n    if row['Side'] == 1 and row['Transported'] == 0:\n        countnts += 1\n    return counttp, countts, countntp, countnts\n\ninitialobserved = initialobserved.apply(lambda row: createCount(row), axis = 1)\n\nobserved = pd.DataFrame(initialobserved.tolist(), index = initialobserved.index)\n\ncount['Port']['Transported'] = observed[0].sum(axis = 0).astype(int)\ncount['Starboard']['Transported'] = observed[1].sum(axis = 0).astype(int)\ncount['Port']['Not Transported'] = observed[2].sum(axis = 0).astype(int)\ncount['Starboard']['Not Transported'] = observed[3].sum(axis = 0).astype(int)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-05-19T15:54:20.699503Z","iopub.execute_input":"2022-05-19T15:54:20.69975Z","iopub.status.idle":"2022-05-19T15:54:21.114086Z","shell.execute_reply.started":"2022-05-19T15:54:20.699722Z","shell.execute_reply":"2022-05-19T15:54:21.112972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the counts I have created lets create a heatmap and complete the chi squared test. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,8)) \nsns.heatmap(count, annot=True, cmap=\"YlGnBu\")","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:54:21.115872Z","iopub.execute_input":"2022-05-19T15:54:21.116148Z","iopub.status.idle":"2022-05-19T15:54:21.373344Z","shell.execute_reply.started":"2022-05-19T15:54:21.116117Z","shell.execute_reply":"2022-05-19T15:54:21.37219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import chi2_contingency \n\nc, p, dof, expected = chi2_contingency(count) \n\nif p < .05:\n    print('p = ' + str(p) + ' which is less than 5%, thus the variables of side of the ship and transported are not independent')\nif p > .05: \n    print('p = ' + str(p) + ' which is greater than 5%, thus the variables of side of the ship and transported are independent')","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:54:21.374521Z","iopub.execute_input":"2022-05-19T15:54:21.37473Z","iopub.status.idle":"2022-05-19T15:54:21.383524Z","shell.execute_reply.started":"2022-05-19T15:54:21.374704Z","shell.execute_reply":"2022-05-19T15:54:21.382313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have these correlations and have shown that the difference between sides of the ship is statistically significant, lets move into data cleaning and model creation, imputing missing values, and encoding the categorical data into numerical values so that it is usable in the modelling process. \n\n**Data Cleaning**","metadata":{}},{"cell_type":"code","source":"print(\"Null Count for all Columns\")\nprint(trainingData.isnull().sum(axis = 0))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:54:21.386353Z","iopub.execute_input":"2022-05-19T15:54:21.386665Z","iopub.status.idle":"2022-05-19T15:54:21.412798Z","shell.execute_reply.started":"2022-05-19T15:54:21.386634Z","shell.execute_reply":"2022-05-19T15:54:21.41176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets build the Preprocessor to handle all of these null values and to encode the data to numerical values. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import KNNImputer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\n\nCategorical_Cleaner = Pipeline(steps=[('imputer', KNNImputer())])\n\nNumerical_Cleaner = Pipeline(steps=[('imputer', KNNImputer())])","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:54:21.414124Z","iopub.execute_input":"2022-05-19T15:54:21.414342Z","iopub.status.idle":"2022-05-19T15:54:21.421318Z","shell.execute_reply.started":"2022-05-19T15:54:21.414315Z","shell.execute_reply":"2022-05-19T15:54:21.420322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have defined the preprocessor, lets split the data into a training and a testing set, and begin to assemble a few seperate model pipelines to train the models, select the most important features, and then retrain a new model. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ny = trainingData.Transported\ny = pd.DataFrame(LabelEncoder().fit_transform(y))\n\nfeatures = ['Deck', 'VIP', 'GroupPassengerId', \n            'CryoSleep', 'GroupId', 'Destination', \n            'HomePlanet', 'Side', 'Age',\n            'TotalSpending']\n\nX = trainingData[features]\ns = (X.dtypes == 'object')\nobject_cols = list(s[s].index)\nnum_cols = X._get_numeric_data().columns\nfor object_col in object_cols:\n    X[object_col] = LabelEncoder().fit_transform(X[object_col])\n\n\nX[object_cols] = Categorical_Cleaner.fit_transform(X[object_cols])\nX[num_cols] = Numerical_Cleaner.fit_transform(X[num_cols])\nX['TotalSpending'] = StandardScaler().fit_transform(X['TotalSpending'].array.reshape(-1,1))\n\n\ntrainX, testX, trainy, testy = train_test_split(X, y, test_size = .25, random_state=42) \n\ntargetX = testingData[features]\ns = (targetX.dtypes == 'object')\nobject_cols = list(s[s].index)\nnum_cols = targetX._get_numeric_data().columns\nfor object_col in object_cols:\n    targetX[object_col] = LabelEncoder().fit_transform(targetX[object_col])\n\n\ntargetX[object_cols] = Categorical_Cleaner.fit_transform(targetX[object_cols])\ntargetX[num_cols] = Numerical_Cleaner.fit_transform(targetX[num_cols])\ntargetX['TotalSpending'] = StandardScaler().fit_transform(targetX['TotalSpending'].array.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:54:21.422473Z","iopub.execute_input":"2022-05-19T15:54:21.422685Z","iopub.status.idle":"2022-05-19T15:54:21.732576Z","shell.execute_reply.started":"2022-05-19T15:54:21.422657Z","shell.execute_reply":"2022-05-19T15:54:21.73158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\n\nNaive_Bayes_Model = GaussianNB()\nLogisticModel = LogisticRegression(max_iter=10000, tol=0.1)\n\nfrom xgboost import XGBClassifier\n\nXGB_model = XGBClassifier()\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngradientBoost = GradientBoostingClassifier(n_estimators = 100, random_state = 42)\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\ndef KerasBuilder():\n    model = Sequential()\n    model.add(Dense(10, input_dim=10, activation = 'relu'))\n    model.add(Dense(5, activation = 'relu'))\n    model.add(Dense(2, activation = 'relu'))\n    model.compile(loss = 'binary_crossentropy', \n                  optimizer = 'adam', \n                  metrics = ['accuracy'])\n    return model\n\nKerasEstimator = KerasClassifier(build_fn = KerasBuilder, \n                                 epochs = 200, \n                                 batch_size = 5, \n                                 verbose = False)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:54:21.734152Z","iopub.execute_input":"2022-05-19T15:54:21.734892Z","iopub.status.idle":"2022-05-19T15:54:21.750397Z","shell.execute_reply.started":"2022-05-19T15:54:21.734841Z","shell.execute_reply":"2022-05-19T15:54:21.749486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have the models defined and the pipelines built, lets train the models and evaluate them on the test set. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\n\nbayespreds = Naive_Bayes_Model.fit(trainX, trainy).predict(testX)\n\nlogpreds = LogisticModel.fit(trainX, trainy).predict(testX)\n\ngradientboostpreds = gradientBoost.fit(trainX, trainy).predict(testX)\n\nXGB_Model = XGB_model.fit(trainX, trainy, eval_metric = 'error')\n#KerasEstimator.fit(trainX, trainy, epochs=150, batch_size=10, verbose=0)\n#Keraspreds = pd.DataFrame(KerasEstimator.predict(testX))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:54:21.751844Z","iopub.execute_input":"2022-05-19T15:54:21.752592Z","iopub.status.idle":"2022-05-19T15:54:23.438115Z","shell.execute_reply.started":"2022-05-19T15:54:21.752541Z","shell.execute_reply":"2022-05-19T15:54:23.437256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now the moment we've all been waiting for, the scoring of the initial models. Lets see which of thes survives through the first round of eliminations. ","metadata":{}},{"cell_type":"code","source":"#print(\"Keras Scores: \" + str(accuracy_score(Keraspreds, testy)))\nprint(\"Bayes score: \" + str(accuracy_score(bayespreds, testy)))\nprint(\"Logistic score: \" + str(accuracy_score(logpreds, testy)))\nprint('Ensemble learning score: ' + str(accuracy_score(gradientboostpreds, testy)))\nprint(\"XGB score: \" + str(XGB_Model.score(testX, testy)))\nprint('XGB training score: ' + str(XGB_Model.score(trainX, trainy)))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:54:23.439506Z","iopub.execute_input":"2022-05-19T15:54:23.440108Z","iopub.status.idle":"2022-05-19T15:54:23.481235Z","shell.execute_reply.started":"2022-05-19T15:54:23.440069Z","shell.execute_reply":"2022-05-19T15:54:23.480258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the end, the XGB Classifier is the winner, slipping past the ensemble learning algrithm by roughly .003 accuracy. The next step is to complete a search for the best hyperparameters, and complete feature selection. This is going to be interesting as I have never done this before, so here goes nothing. ","metadata":{}},{"cell_type":"code","source":"#The parameter grid is defined below for XGBClassifier\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\n\nfrom datetime import datetime\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\n#Because this is going to take a while, I'm stealing this timer function to time it. \ndef timer(start_time = None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time: \n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, \n                                                                      tmin, \n                                                                      round(tsec, 2)))\n                                                                      ","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:54:23.484606Z","iopub.execute_input":"2022-05-19T15:54:23.485102Z","iopub.status.idle":"2022-05-19T15:54:23.495376Z","shell.execute_reply.started":"2022-05-19T15:54:23.485062Z","shell.execute_reply":"2022-05-19T15:54:23.494282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I've placed the random search Below so that it can be commented out, because it is going to take a while. ","metadata":{}},{"cell_type":"code","source":"xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n                    silent=True, nthread=1)\n\n\nfolds = 5\nparam_comb = 50\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X,y), verbose=0, random_state=42 )\n\n\nstart_time = timer(None) \nrandom_search.fit(X, y)\ntimer(start_time) \n\nprint('\\n All results:')\nprint(random_search.cv_results_)\nprint('\\n Best estimator:')\nprint(random_search.best_estimator_)\nprint('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\nprint(random_search.best_score_ * 2 - 1)\nprint('\\n Best hyperparameters:')\nprint(random_search.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:54:23.496641Z","iopub.execute_input":"2022-05-19T15:54:23.496907Z","iopub.status.idle":"2022-05-19T16:00:17.353788Z","shell.execute_reply.started":"2022-05-19T15:54:23.496878Z","shell.execute_reply":"2022-05-19T16:00:17.352857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One of the best ways to increase model accuracy is to increase the amount of training data available to the model, so in the next code block, I will fit the model first to the training data to evaluate its accuracy, and then to the entire dataset. After some testing, I've found that the randomized search hurt performance quite considerably, so I will simply use the initial XGB model that is created. ","metadata":{}},{"cell_type":"code","source":"optimizedXGB = random_search.best_estimator_\noptimizedXGB.fit(trainX, trainy)\nprint(optimizedXGB.score(trainX, trainy))\nprint(optimizedXGB.score(testX, testy))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:00:50.856402Z","iopub.execute_input":"2022-05-19T16:00:50.857079Z","iopub.status.idle":"2022-05-19T16:00:54.790589Z","shell.execute_reply.started":"2022-05-19T16:00:50.857029Z","shell.execute_reply":"2022-05-19T16:00:54.789788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the model's performance on the training data suffers considerably, while the testing data only moderately improves. Because of this, I will use the original Naive bayes model, and repeat the process of parameter optimization, and see if we get better results. ","metadata":{}},{"cell_type":"code","source":"#Parameter matrix for a gaussian NB model \nNBParams = {'var_smoothing': np.logspace(0,-9, num=100)}\n\nstart_timeGNB = timer(None) \ntunedNB = GridSearchCV(estimator = Naive_Bayes_Model, param_grid = NBParams, cv = skf.split(X,y),verbose=1, scoring='accuracy')\ntunedNB.fit(X,y)\ntimer(start_timeGNB)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T20:50:35.183108Z","iopub.execute_input":"2022-05-19T20:50:35.183437Z","iopub.status.idle":"2022-05-19T20:50:35.260797Z","shell.execute_reply.started":"2022-05-19T20:50:35.183369Z","shell.execute_reply":"2022-05-19T20:50:35.259273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nTunedNB = tunedNB.best_estimator_\nTunedNB.fit(trainX,trainy)\nprint(TunedNB.score(trainX, trainy))\nprint(TunedNB.score(testX, testy))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following code finally saves the model in a pickle repository so that I don't have to do this again.","metadata":{}},{"cell_type":"code","source":"import pickle\n\nwith open('GNB_tuned.pkl', 'wb') as file:\n    pickle.dump(TunedNB, file)\nwith open('GNB_tuned.pkl', 'rb') as model:\n    finalmodel = pickle.load(model)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:06:29.189683Z","iopub.execute_input":"2022-05-19T16:06:29.190661Z","iopub.status.idle":"2022-05-19T16:06:29.208358Z","shell.execute_reply.started":"2022-05-19T16:06:29.190598Z","shell.execute_reply":"2022-05-19T16:06:29.207052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The below code finally predicts and submits the predictions of the best model. ","metadata":{}},{"cell_type":"code","source":"preds = pd.DataFrame(TunedNB.predict(targetX))\nprint(preds)\nEXSUB = pd.read_csv('../input/spaceship-titanic/sample_submission.csv')\nfinalPreds = pd.DataFrame(preds.astype(bool))\nfinalPreds.insert(0,\"PassngerId\", testingData.PassengerId)\nfinalPreds.columns = EXSUB.columns\nprint(finalPreds)\nfinalPreds.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T16:06:34.110249Z","iopub.execute_input":"2022-05-19T16:06:34.110806Z","iopub.status.idle":"2022-05-19T16:06:34.158389Z","shell.execute_reply.started":"2022-05-19T16:06:34.110759Z","shell.execute_reply":"2022-05-19T16:06:34.157682Z"},"trusted":true},"execution_count":null,"outputs":[]}]}