{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import graphviz\nimport itertools\nimport matplotlib.pyplot as plt # graphing with insane defaults\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport seaborn as sns # graphing with sane defaults\nimport scipy.stats as stats\nfrom sklearn import linear_model\nfrom sklearn import preprocessing # Preprocess data (e.g. scale numerical data to 0-1\nfrom sklearn import tree\nfrom sklearn.base import BaseEstimator\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, AdaBoostRegressor, HistGradientBoostingRegressor, IsolationForest, AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nimport sklearn.metrics as metrics\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, PrecisionRecallDisplay\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.utils._testing import ignore_warnings\nfrom termcolor import colored, cprint\nimport typing # Apply common types to objects)\nimport warnings\nfrom yellowbrick.classifier.rocauc import roc_auc\nfrom yellowbrick.classifier import precision_recall_curve\nfrom yellowbrick.features import PCA as yellowPCA, Manifold\nfrom yellowbrick.regressor import ResidualsPlot, PredictionError\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfeature_engineering = typing.TypeVar('rabbitml.feature_engineering')\n\nclass rabbitml:\n    \"\"\"\n    An automl library designed for tabular data\n\n    @Taran Sean Marley \n    https://www.kaggle.com/taranmarley\n    \"\"\"\n    class feature_engineering:\n        \"\"\"\n        A class intended to move through and improve the features of a dataset.\n        \"\"\"\n        \n        def auto_casefold(self, df : pd.DataFrame) -> pd.DataFrame:\n            \"\"\"\n            Take a dataframe, find the string columns and convert them all to lower case through casefold\n\n            Parameters\n            ----------\n            dataframe : pd.DataFrame\n                The dataframe to casefold over to convert to lower case\n\n            Returns\n            -------\n            pd.DataFrame\n                The same dataframe given with the new lorrrwer case values if applied\n\n            \"\"\"\n            for col in df.columns:\n                if self.is_string_type(df[col]):\n                    df[col] = df[col].astype(str).str.casefold()\n            return df\n\n        def break_up_by_string(self, df_temp : pd.DataFrame, splitting_string : str, cols : typing.List = None) -> pd.DataFrame:\n            \"\"\"\n            Break up columns by string to create new columns from each split.\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to start splitting up object columns\n            splitting_string : str\n                String to split up columns by\n            cols : typing.List\n                Optional parameter that if provided will be the only columns that will be considered for splitting\n\n            Returns\n            -------\n            pd.DataFrame\n                modified dataframe with extra columns containing split up values\n            \"\"\"\n            obj_cols = df_temp.select_dtypes(include=[object])\n            if cols is not None:\n                obj_cols = cols\n            # count spaces\n            for col in obj_cols:\n                if df_temp[col].str.contains(splitting_string).sum() > 0:\n                    df2 = df_temp[col].str.split(splitting_string, expand=True)\n                    # Rename columns\n                    rename_dict = {}\n                    for rename_col in df2.columns:\n                        if (splitting_string != \" \"):\n                            rename_dict[rename_col] = col + splitting_string + str(rename_col)\n                        else:\n                            rename_dict[rename_col] = col + str(rename_col)\n                    df2 = df2.rename(columns=rename_dict)\n                    df2 = df2.fillna(0)\n                    df_temp = pd.concat([df_temp,df2], axis=1) \n            return df_temp\n        \n        \n        def compare_object_columns(self, df_temp : pd.DataFrame, df_temp_2 : pd.DataFrame, silent = False, replace = False) -> None:\n            \"\"\"\n            Compare object columns and print out the if there is a difference between them. This helps determining the differences between a test dataframe and a training dataframe\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                First dataframe to compare columns with\n            df_temp_2 : pd.DataFrame\n                Second dataframe to compare columns with\n            silent : bool\n                Print the results or not\n            replace : bool\n                Replace bad values in df_temp with NaN values\n            \"\"\"\n            for col in df_temp.select_dtypes(include=\"object\").columns:\n                if col in df_temp_2.columns:\n                    unique_df_list = df_temp[col].unique().tolist()\n                    test_df_list = df_temp_2[col].unique().tolist()\n                    if set(unique_df_list) != set(test_df_list):\n                        unique_df_list = [\"nan\" if x is np.nan else x for x in unique_df_list]\n                        test_df_list = [\"nan\" if x is np.nan else x for x in test_df_list] \n                        unique_df_list.sort()\n                        test_df_list.sort()\n                        # Print lists if requested\n                        if not silent:\n                            print(\"***\",col)\n                            print(unique_df_list)\n                            print(test_df_list)\n                        # Replace with NaN if requested by parameter \n                        for x in unique_df_list:\n                            if x not in test_df_list:\n                                df_temp[col].replace({x:np.nan})\n                                \n        def create_anomaly_scores_preds(self, df_temp : pd.DataFrame, estimator : BaseEstimator, df_test_temp : pd.DataFrame = None, target_col : str = None) -> typing.Tuple[pd.DataFrame, pd.DataFrame]:\n            \"\"\"\n            Create anomaly predictions and scores and add them to the given dataFrame\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                DataFrame that will be examined for outliers and added to\n            estimator : BaseEstimator\n                Estimator \n            df_test_temp : pd.DataFrame \n            \n            Returns\n            -------\n            typing.Tuple[pd.DataFrame, pd.DataFrame]\n                The main dataframe and the optional testing dataframe if it exists\n            \"\"\"\n            df_temp = df_temp.copy()\n            len_of_train = len(df_temp)\n            if df_test_temp is not None:\n                df_temp = pd.concat([df_temp.copy(), df_test_temp.copy()], ignore_index=True)\n            X = df_temp.copy()\n            estimator = estimator.fit(X)\n            df_temp[\"anomaly_\" + estimator.__class__.__name__] = estimator.predict(X)\n            df_temp[\"anomaly_\" + estimator.__class__.__name__] = df_temp[\"anomaly_\" + estimator.__class__.__name__].replace({-1:0})\n            df_temp[\"anomaly_score_\" + estimator.__class__.__name__] = estimator.score_samples(X)\n            if df_test_temp is not None:\n                df_test_temp = df_temp[len_of_train:]\n                df_temp = df_temp[:len_of_train]\n            return df_temp, df_test_temp\n        \n        def create_interactions(self, df_temp : pd.DataFrame, column_list : typing.List) -> pd.DataFrame:\n            \"\"\"\n            Create interactions by totalling and multiplying columns within a dataframe\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to create interactions in\n            column_list : typing.List\n                List of columns to create interactions from\n\n            Returns\n            ----------\n            pd.DataFrame\n                Dataframe with interactions added\n            \"\"\"\n            # Cross wise multiplication interactions\n            for x in itertools.combinations(column_list, 2):\n                df_temp[x[0]+\"_X_\"+x[1]] = df_temp[x[0]] * df_temp[x[1]]\n                df_temp = df_temp.copy()\n            # Iterative Totals\n            iterative_total = 0\n            i = 0\n            for j in (column_list):\n                iterative_total = iterative_total + df_temp[j]\n                if i > 0:\n                    df_temp[\"A\" + str(i) + \"_iter_score\"] = iterative_total\n                    df_temp = df_temp.copy()\n                i = i + 1\n            return df_temp\n            \n        def detect_continous_columns(self, df_temp : pd.DataFrame, ratio : float = 0.05, continous_columns : typing.List = []) -> typing.List:\n            \"\"\"\n            Detect the continous columns in a dataframe. Columns that have more than the given ratio by total length of dataframe will be considered continous.\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to detect continous columns in. This is assumed to already be encoded to a numerical format\n            ratio : float / int\n                Ratio of the total length of dataframe that will be used to cull continous from discrete data, if given as an int then this is consider to be a discrete number instead of a ratio\n            continous_columns : typing.List\n                Continous columns that can be given to the function without checking\n\n            Returns\n            ----------\n            typing.List\n                List of columns found\n            \"\"\"\n            continous_cutoff : int = round(ratio * len(df_temp))\n            if ratio > 1:\n                continous_cutoff = ratio\n            for col in df_temp.columns:\n                if not self.is_string_type(df_temp[col]):\n                    if col not in continous_columns:\n                        if df_temp[col].nunique() > continous_cutoff:\n                            continous_columns.append(col)\n            return continous_columns\n\n        def detect_duplicates(self, df_temp : pd.DataFrame, silent : bool = False, id_cols : typing.List = []) -> None: \n            \"\"\"\n            Detect duplicates in data and return the columns in which duplicates where detected.\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to detect duplicates in\n            silent : bool\n                Whether to run print statements \n            id_cols : typing.List\n                Given id cols that aren't auto detected - Useful if there is an obvious ID column that also wants to be detected for duplication\n            \"\"\"\n            # Filter out identity columns\n            cols_to_use = []\n            for col in df_temp.columns:\n                if len(df_temp[col].unique()) != len(df_temp[col]):\n                    cols_to_use.append(col)\n            id_cols = self.detect_id_columns(df_temp)\n            id_temp = df_temp.copy()[id_cols]\n            df_temp = df_temp.copy()[cols_to_use]    \n            count_dupes = df_temp.duplicated().sum()\n            count_dupes_in_ID = id_temp.duplicated().sum()\n            if not silent:\n                print('Duplicates in data: ', str(count_dupes))\n                print('Duplicates in id columns: ', str(count_dupes_in_ID))\n                print('When filtering out id columns: ', str(id_cols))\n\n        def detect_nans(self, df_temp : pd.DataFrame, name = '', silent : bool = False, plot : bool = True) -> typing.List:\n            \"\"\"\n            Detect NaNs in a provided dataframe and return the columns that NaNs were detected in     \n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to detect NaN values in\n            name : str\n                Name of the dataframe which helps give a more descriptive read out\n            silent : bool\n                Whether the print statements should fire\n            plot : bool\n                Whether to return a plot of the counts of NaNs in the data\n\n            Returns\n            -------\n            typing.List\n                List of columns in the provided dataframe that contain NaN values\n            \"\"\"\n            plt.rcParams[\"figure.figsize\"] = (9,9)\n            \n            count_nulls = df_temp.isnull().sum().sum()\n            columns_with_NaNs = []\n            # Count NaNs by column\n            if count_nulls > 0:\n                for col in df_temp.columns:\n                    if df_temp[col].isnull().sum().sum() > 0:\n                        columns_with_NaNs.append(col)\n            # Print out the NaN values\n            if not silent:            \n                if name != '': \n                    print('******')\n                    cprint('Detecting NaNs in ' + str(name), attrs=['bold'])\n                    print('******')\n                print('NaNs in data:', count_nulls)\n                if count_nulls > 0:\n                    print('******')\n                    for col in columns_with_NaNs:\n                        print('NaNs in', col + \": \", df_temp[col].isnull().sum().sum())\n                    print('******')\n            print('')\n            # Plot the NaN values in columns in bar plot\n            if plot and count_nulls > 0:\n                sns.barplot(y=df_temp[columns_with_NaNs].isnull().sum().index, x=df_temp[columns_with_NaNs].isnull().sum().values).set_title(str(name) + \" NaNs\")\n                plt.show()\n            return columns_with_NaNs\n        \n        def detect_id_columns(self, df_temp : pd.DataFrame) -> typing.List:\n            \"\"\"\n            Detect which columns are ID columns, those for which one unique value exists for each row.\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to detect ID columns\n\n            Returns\n            -------\n            typing.List\n                List of Identity columns that were detected\n            \"\"\"\n            id_cols = []\n            for col in df_temp.columns:\n                if df_temp[col].nunique() == len(df_temp[col]):\n                    id_cols.append(col)\n            return id_cols\n        \n        def detect_uncorrelated_columns(self, df_temp : pd.DataFrame) -> typing.List:\n            \"\"\"\n            Detect which columns are very uncorrelated columns\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to detect ID columns\n\n            Returns\n            -------\n            typing.List\n                List of Identity columns that were detected\n            \"\"\"\n            id_cols = []\n            # Create correlation dataframe\n            corr_df = pd.DataFrame(columns=list(df_temp.columns))\n            for col_from in df_temp.columns:\n                for col_to in df_temp.columns:\n                    corr_df.loc[col_from, col_to] = df_temp[col_from].corr(df_temp[col_to])\n            corr_df = corr_df.abs()\n            corr_df[\"sum\"] = corr_df.sum(axis=0) - 1\n            # Add to id_cols if correlation very low\n            for i, col in enumerate(df_temp.columns):\n                if corr_df.iloc[i,-1] < (0.04 * len(df_temp.columns)) and corr_df.iloc[i,-1] != -1:\n                    id_cols.append(col)\n            return id_cols\n\n        def drop_unshared_columns(self, df_temp : pd.DataFrame, df_temp_2 : pd.DataFrame, exclude_columns : typing.List) -> None:\n            \"\"\"\n            Detect which columns are not shared between the two dataframes excepting for a target_col if provided.\n            Delete in place.\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to check for shared columns        \n            df_temp_2 : pd.DataFrame\n                Second dataframe to check for shared columns\n            exclude_columns : typing.List\n                Columns not to remove in this process\n            \"\"\"    \n            drop_cols : typing.List = []\n            for col in df_temp_2.columns:\n                if col not in df_temp.columns:\n                    if col not in exclude_columns:\n                        drop_cols.append(col)\n            df_temp_2.drop(columns=drop_cols, axis=1, inplace=True)\n            drop_cols : typing.List = []\n            for col in df_temp.columns:\n                if col not in df_temp_2.columns:\n                    if col not in exclude_columns:\n                        drop_cols.append(col)\n            df_temp.drop(columns=drop_cols, axis=1, inplace=True)\n                        \n        def encode_binary_object(self, series : pd.Series) -> pd.Series:\n            \"\"\"\n            Encode a binary object series\n\n            Parameters\n            ----------\n            series : pd.Series\n                The series to be encoded. \n\n            Returns\n            -------\n            pd.Series\n                The encoded series\n            \"\"\"\n            map_dict = {}\n            series_list = series.unique().tolist()\n            series_list.sort()\n            for i, x in enumerate(series_list):\n                map_dict[x] = i\n            series = series.map(map_dict)\n            return series\n        \n        def encode_columns(self, df : pd.DataFrame, columns : pd.Series, test_df : pd.DataFrame = None, cutoff : int = 20) -> typing.Tuple[pd.DataFrame, pd.DataFrame]:\n            \"\"\"\n            Encode columns based on the number of unique values in each column\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to encode columns in \n            columns : pd.Series\n                Columns to encode\n            test_df : pd.DataFrame\n                Test dataframe to encode based on classes in the Dataframe\n            cut_off : int\n                The cut off number of classes to choose between label encoding and get dummies. This keeps the dimensionality under control\n\n            Returns\n            -------\n            (pd.DataFrame, pd.DataFrame)\n                Original dataframe and the test dataframe\n            \"\"\"    \n            for col in columns:\n                le = preprocessing.LabelEncoder()\n                classes_to_encode = df[col].astype(str).unique().tolist()\n                classes_to_encode.sort()\n                classes_to_encode.append('None')\n                le.fit(classes_to_encode)\n                # Get dummies except for binary variables which are handled by len(le.classes) != 3\n                if len(le.classes_) < cutoff and len(le.classes_) != 3:\n                    df = pd.get_dummies(df, columns = [col])\n                    if test_df is not None:\n                        test_df = pd.get_dummies(test_df, columns = [col])\n                else:\n                    # First test for binary variables that should be encoded and change things if that is the case\n                    binary_detected = False\n                    if df[col].nunique() == 2 and df[col].isnull().sum().sum() == 0:\n                        # Detect test_df exists and is binary and the unqiue values of test compare the unique values in regular df\n                        if test_df is not None and test_df[col].nunique() == 2 and test_df[col].isnull().sum().sum() == 0 and set(test_df[col].unique()) == set(df[col].unique()): \n                            binary_detected = True\n                        elif test_df is None:\n                            binary_detected = True\n                    if binary_detected:\n                        classes_to_encode.remove('None')\n                        le.fit(classes_to_encode)\n                    # Test that the column isn't too unique to be useful\n                    if df[col].nunique() > (len(df[col]) * 0.95):\n                        print(\"Dropping column:\", col, \"due to high uniqueness that would lead to overfitting\")\n                        df = df.drop(columns=col)\n                        if test_df is not None:\n                            test_df = test_df.drop(columns=col)\n                        continue\n                    # If no test dataframe encode as normal else we should clear out classes not found in test\n                    if test_df is None:\n                        df[col] = le.transform(df[col].astype(str))\n                    else:\n                        check_col = df.copy()[col]\n                        # Clean out labels in train that aren't in test\n                        input_dict = {}\n                        for unique in df[col].unique():\n                            if unique not in pd.unique(test_df[col]) and not binary_detected:\n                                input_dict[unique] = 'None'\n                        df[col] = df[col].replace(input_dict)     \n                        # Check whether there is little crossover between test and df \n                        if len(df[col]) * 0.9 < df[col].tolist().count('None'):\n                            print(\"Dropping column:\", col, \"due to little to no crossover with test dataframe\")\n                            df = df.drop(columns=col)\n                            test_df = test_df.drop(columns=col)\n                            continue\n                        df[col] = le.transform(df[col].astype(str))\n                        #Clean out unseen labels in test\n                        input_dict = {}\n                        for unique in test_df[col].unique():\n                            if unique not in pd.unique(check_col) and not binary_detected:\n                                input_dict[unique] = 'None'\n                        test_df[col] = test_df[col].replace(input_dict)\n                        test_df[col] = le.transform(test_df[col].astype(str))\n            return df, test_df\n\n        def fill_nans_create_columns(self, df_temp : pd.DataFrame, columns : typing.List, value : float = 0) -> pd.DataFrame:\n            \"\"\"\n            Fill NaN of provided columns and create columns to signify they weren't there.\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to modify\n            columns : typing.List\n                Columns of the provided dataframe to modify\n            value : float\n                Value to replace the NaN values with\n\n            Returns\n            -------\n            pd.DataFrame\n                Modified Dataframe with NaNs filled and new columns signifying the rows that contained NaNs\n            \"\"\"\n            for col in columns:\n                df_temp[col + \"_was_null\"] = df_temp[col].isnull().astype(int)\n                df_temp[col] = df_temp[col].fillna(value)\n            return(df_temp)\n        \n        def is_string_type(self, series: pd.Series) -> bool:\n            \"\"\"\n            Detect if a series contains is a string type \n\n            Parameters\n            ----------\n            series : pd.Series\n                The series to detect the presence of a string type\n\n            Returns\n            -------\n            bool\n                Whether a string type was detected or not\n\n            @Inspired by work by https://stackoverflow.com/users/3876599/yourstruly\n            \"\"\"\n            if pd.StringDtype.is_dtype(series.dtype):\n                # Is a string extension type\n                return True\n\n            if series.dtype != \"object\":\n                # No object column - definitely not a string\n                return False\n\n            try:\n                series.str\n            except AttributeError:\n                return False\n\n            return True        \n\n        def quantile_transform_column_wise(self, df_temp : pd.DataFrame, target_col : str = \"\", output_distribution=\"uniform\") -> pd.DataFrame:\n            \"\"\"\n            Transform values in dataframe to quantile uniform distribution\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to quantile transform \n            target_col : str\n                This is the target col and is not transformed\n\n            Returns\n            -------\n            pd.DataFrame\n                Modified dataframe\n            \"\"\"    \n            df_temp = df_temp.copy()\n            # find n_samples\n            n_samples : int = 1000\n            if len(df_temp) < 1000:\n                n_samples = len(df_temp)\n            for col in df_temp.columns:\n                if col != target_col:\n                    transformed = preprocessing.QuantileTransformer(random_state=1, n_quantiles=n_samples, output_distribution=output_distribution).fit_transform(df_temp[col].values.reshape(-1, 1))\n                    df_temp[col] = pd.Series(transformed[:,0], index=df_temp[col].index, name=df_temp[col].name)\n            return df_temp\n        \n        def min_max_column_wise(self, df_temp : pd.DataFrame, target_col : str = \"\"):\n            df_temp = df_temp.copy()\n            for col in df_temp.columns:\n                if col != target_col:\n                    df_temp[col] = preprocessing.MinMaxScaler().fit_transform(df_temp[col].values.reshape(-1, 1))\n            return df_temp\n        \n        def pipeline(self, df_temp : pd.DataFrame, test_df_temp : pd.DataFrame = None, target_col : str = None, create_interactions : bool = True, id_cols : typing.List = None, break_up_cols : typing.List[typing.Tuple[str, str]] = None) -> typing.Tuple[pd.DataFrame, pd.DataFrame]:\n            \"\"\"\n            A pipeline through which the data is processed and feature engineered\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to process features of \n            test_df_temp : pd.DataFrame\n                The test dataframe to process features of \n            target_col : str\n                The optional target column that won't be processed due to this being problematic for the end result\n            create_interactions : bool\n                Whether to multiply and add columns together. Defaults to true\n            id_cols : typing.List \n                Id columns if given manually will eliminate auto testing and likely is more important \n            break_up_cols : typing.List(typing.Tuple[str, str])\n                A list of tuples the first element of which is the column name and the second element is the string with which to split up the elements.\n                \n            Returns \n            -------\n            (pd.DataFrame, pd.DataFrame)\n                \n            \"\"\"            \n            df_temp = df_temp.copy()\n            if test_df_temp is not None:\n                test_df_temp = test_df_temp.copy()\n            # Remove ID Column if provided\n            if id_cols is not None:\n                print(\"Provided id columns dropped:\", id_cols)\n                df_temp = df_temp.drop(columns=id_cols)\n                if test_df_temp is not None:\n                    test_df_temp = test_df_temp.drop(columns=id_cols)\n            target = None\n            if target_col != \"\":\n                target = df_temp[target_col]\n                df_temp = df_temp.drop(columns=target_col) \n            # Detect potential error, is test exists and its columns don't match the original dataframe minus target then there is \n            if test_df_temp is not None:\n                if set(df_temp.columns) != set(test_df_temp.columns):\n                    cprint(\"Unfortunately the columns of the testing dataframe and training dataframe do not match, this may yield errors or bad interactions. This is not a recommended format.\", 'red', attrs=['bold'])\n            # Break up columns if requested\n            if break_up_cols is not None:\n                for break_up in break_up_cols:\n                    df_temp = self.break_up_by_string(df_temp, break_up[1], cols=[break_up[0]])\n                    if test_df_temp is not None:\n                        test_df_temp = self.break_up_by_string(test_df_temp, break_up[1], cols=[break_up[0]])\n            self.detect_nans(df_temp, \"Training Data\")\n            self.fill_nans_create_columns(df_temp, df_temp.columns, -1)\n            self.detect_duplicates(df_temp)\n            # Detect Ids and completely uncorrelated columns and remove them\n            continous_columns = self.detect_continous_columns(df_temp, 20, continous_columns=[])\n            # Encode columns so all are of a numerical type - This helps detect and remove uncorrelated continous columns from the interactions\n            df_correlation_test, _ = self.encode_columns(df_temp, df_temp.select_dtypes(include=\"object\").columns)\n            # Detect completely uncorrelated columns and remove them from the continous_columns detected\n            uncorrelated_cols = self.detect_uncorrelated_columns(df_correlation_test)\n            for c in uncorrelated_cols:\n                if c in continous_columns:\n                    continous_columns.remove(c)\n            # Process test dataframe if it exists\n            if test_df_temp is not None:\n                self.detect_nans(test_df_temp, \"Testing Data\" )\n                self.fill_nans_create_columns(test_df_temp, test_df_temp.columns, -1)\n                self.detect_duplicates(test_df_temp)\n                # Create Interactions by adding and multiplying columns together \n                if create_interactions:\n                    test_df_temp = self.create_interactions(test_df_temp, continous_columns).copy()\n            # Encode columns so all are of a numerical type\n            df_temp, test_df_temp = self.encode_columns(df_temp, df_temp.select_dtypes(include=\"object\").columns, test_df_temp)\n            # Drop unshared columns\n            if test_df_temp is not None:\n                self.drop_unshared_columns(df_temp, test_df_temp, target_col)\n            # Detect Ids and completely uncorrelated columns and remove them\n            uncorrelated_cols = self.detect_uncorrelated_columns(df_temp)\n            print(\"Removed Uncorrelated: \", uncorrelated_cols)\n            # Create Interactions by adding and multiplying columns together\n            if create_interactions:\n                #continous_columns = self.detect_continous_columns(df_temp, 20, continous_columns=[])\n                df_temp = self.create_interactions(df_temp, continous_columns).copy()\n                if test_df_temp is not None:\n                    test_df_temp = self.create_interactions(test_df_temp, continous_columns).copy()\n            if uncorrelated_cols is not None and len(uncorrelated_cols) > 0:\n                df_temp = df_temp.drop(columns=uncorrelated_cols)\n                # Drop unshared columns due to the drop of uncorrelated above\n                if test_df_temp is not None:\n                    self.drop_unshared_columns(df_temp, test_df_temp, target_col)\n            # Create Anomaly Scores\n            df_temp, test_df_temp = self.create_anomaly_scores_preds(df_temp, IsolationForest(random_state=0), test_df_temp, target_col)\n            if target_col != \"\":\n                df_temp[target_col] = target\n            return df_temp, test_df_temp\n        \n    class eda:\n        \"\"\"\n        A set of tools for Exploratory Data Analysis\n        \"\"\"\n        \n        def box_plots(self, df_temp : pd.DataFrame, columns : typing.List) -> None:\n            \"\"\"\n            Make box plots of different continous columns\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to make box plots of\n            columns : typing.List\n                A list of continous columns to use\n            \"\"\"\n            if len(columns) > 25:\n                columns = columns[:25]\n            fig = plt.figure(figsize = (15, 9))\n            colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n            palette = itertools.cycle(colors)\n\n            for index,col in enumerate(df_temp[columns]):\n                plt.subplot(5, 5, index + 1)\n                sns.boxplot(y = col, data = df_temp[columns], color=next(palette))\n                plt.tight_layout()\n            plt.show()\n        \n        def box_plot_correlated_columns(self, df_temp : pd.DataFrame, target_col : str, fe : feature_engineering) -> None:\n            \"\"\"\n            Create and display box plots of categorical columns based on the most correlated columns in a dataset\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                The feature dataset to find the correlations from and make box plots from\n            target_col : str\n                The column that is the aim for prediction\n            fe : feature_engineering\n                The feature engineering library\n            \"\"\"\n            scaled_df = fe.quantile_transform_column_wise(df_temp, \"\")\n            correlated_cols = self.calculate_correlations(scaled_df, target_col, 7, silent=True, visualise=False)\n            correlated_cols.remove(target_col)\n            fig = plt.figure(figsize = (16, 16))\n            for index, col in enumerate(correlated_cols):\n                plt.subplot(3, 2, index + 1)\n                ax = sns.boxplot(x=target_col, y=col, data=df_temp, palette=\"Set3\")\n            plt.show()\n\n        def class_balance(self, df_temp : pd.DataFrame, target_col : str) -> None:\n            \"\"\"\n            Display and show a plot of the target categorical value\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to find class balance in \n            target_col : str\n                Name of column with which to find the target categorical value\n            \"\"\"\n            sns.countplot(x=df_temp[target_col])\n            plt.show()\n            column_values = df_temp[target_col].values.ravel()\n            unique_values = pd.unique(column_values)\n            unique_values = np.sort(unique_values)\n            for value in unique_values:\n                print(value,\":\",(len(df_temp.loc[df_temp[target_col] == value]) / len(df_temp)) * 100, \"%\")\n\n        def pca_dimension_reduction_info(self, df_temp : pd.DataFrame, target_col : str) -> None:\n            \"\"\"\n            Examine the results of dimensionality reduction on the dataset\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                DataFrame to conduct PCA on\n            target_col : str\n                target column to remove before conducting PCA         \n            \"\"\"\n            df_temp = df_temp.copy()\n            y = df_temp[target_col]\n            X = df_temp.drop(columns=target_col, axis=1).values\n            X_scaled = X\n            print(str(len(X_scaled[0])) + \" initial feature components\")\n            pca = PCA(n_components=0.95)\n            X_p = pca.fit(X_scaled).transform(X_scaled)\n            print(\"95% variance explained by \" + str(len(X_p[0])) + \" components by principle component analysis\")\n            pca = PCA(n_components=2)\n            TwoX_p = pca.fit(X_scaled).transform(X_scaled)\n            print(str(round(pca.explained_variance_ratio_.sum() * 100)) + \"% variance explained by 2 components by principle component analysis\")\n            # 2D plot\n            fig = px.scatter(TwoX_p, x=0, y=1, color=y, width=600, height=600, title=\"Two Component PCA\")\n            fig.show()\n            pca = PCA(n_components=3)\n            ThreeX_p = pca.fit(X_scaled).transform(X_scaled)\n            print(str(round(pca.explained_variance_ratio_.sum() * 100)) + \"% variance explained by 3 components by principle component analysis\")\n            # 3D plot \n            fig = px.scatter_3d(ThreeX_p, x=0, y=1, z=2, color=y, width=600, height=600, title=\"Three Component PCA\")\n            fig.show()\n            \n        def pca_four_component(self, df_temp : pd.DataFrame, target_col : str, fe : feature_engineering) -> pd.DataFrame:\n            \"\"\"\n            Do a four component PCA and show the resulting pair wise plot\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                features to run PCA on\n            target_col : str\n                target column to PCA against\n            fe : feature_engineering\n                Feature engineering library to use for scaling\n\n            Returns\n            -------\n            pd.DataFrame\n                A dataframe with target column and PCA features only. \n            \"\"\"\n            pca = PCA(n_components=4)\n            y = df_temp[target_col]\n            X = df_temp.drop(columns=target_col)\n            fourX_p = pca.fit(X).transform(X)\n            print(str(round(pca.explained_variance_ratio_.sum() * 100)) + \"% variance explained by 4 components by principle component analysis\")\n\n            labels = {\n                str(i): f\"PC {i+1} ({var:.1f}%)\"\n                for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n            }\n\n            fig = px.scatter_matrix(\n                fourX_p,\n                title=\"4 Component PCA\",\n                labels=labels,\n                dimensions=range(4),\n                color=y,\n                width=800,\n                height=800\n            )\n            fig.update_traces(diagonal_visible=False)\n            fig.show()\n\n            return_df = pd.DataFrame(fourX_p, columns=[\"PCA1\",\"PCA2\",\"PCA3\",\"PCA4\"])\n            return_df[target_col] = y\n            return return_df\n        \n        def pca_visualisation_2d(self, df_temp : pd.DataFrame, target_col : str, plot_title = \"Principle Component Plot\") -> None:\n            \"\"\"\n            Visualize 2d\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                The dataframe to use features from for embedding\n            target_col : str\n                The target variable to be dropped from dataframe\n            \"\"\"\n            if len(df_temp) > 3000:\n                df_temp = df_temp.copy().sample(n=2999, random_state=1)\n            y = df_temp[target_col]\n            X = df_temp.drop(columns=[target_col])\n            visualizer = yellowPCA(scale=True, projection=2, alpha=0.4, title=plot_title)\n            visualizer.fit_transform(X, y)\n            visualizer.show()\n            plt.show()\n            \n        def pca_visualisation_3d(self, df_temp : pd.DataFrame, target_col : str) -> None:\n            \"\"\"\n            Visualize 3d PCA embedding\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                The dataframe to use features from for embedding\n            target_col : str\n                The target variable to be dropped from dataframe\n            \"\"\"\n            if len(df_temp) > 3000:\n                df_temp = df_temp.copy().sample(n=2999, random_state=1)\n            y = df_temp[target_col]\n            X = df_temp.drop(columns=[target_col])\n            visualizer = yellowPCA(scale=True, projection=3, alpha=0.4, size=(700,700))\n            visualizer.fit_transform(X, y)\n            visualizer.show()\n            plt.show()\n            \n        def line_plots(self, df_temp : pd.DataFrame, columns : typing.List) -> None:\n            \"\"\"\n            Make line plots of different continous columns\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to make line plots of\n            columns : typing.List\n                A list of continous columns to use\n            \"\"\"    \n            pltdf = df_temp.copy()\n            pltdf = pltdf[columns]\n            pltdf = pltdf.sample(frac=1, random_state=42).reset_index(drop=True)\n            pltdf.iloc[:50, :25].plot(subplots=True, layout=(5,5), figsize=(15,10))\n            plt.show()\n        \n        def calculate_correlations(self, df_temp : pd.DataFrame, target_col : str, n_cols : int = 10, silent : bool = False, visualise : bool = False) -> typing.List:\n            \"\"\"\n            Calculate the pearson correlations between the target variable and the dataframe and returns columns that are beyond a certain ratio correlation \n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                dataframe to examine\n            target_col : str\n                the target column to measure correlation against\n            n_cols : int\n                number of columns to return, this amount of columns with the highest correlation\n            silent : bool\n                whether to print to console\n            visualise : bool\n                whether to display a heatmap of correlations\n            \"\"\"\n            df_temp = df_temp.copy()\n            if not silent:\n                print(\"Correlations with\",target_col + \":\")\n            # Generate correlation list\n            correlations_list = []\n            for col_one in df_temp.iloc[:,:].columns:\n                correlation_value =  abs(df_temp[col_one].corr(df_temp[target_col]))\n                # Check for NaN\n                if (correlation_value == correlation_value):\n                    correlations_list.append((correlation_value,col_one))\n            # Sort List\n            correlations_list = sorted(correlations_list, key=lambda tup: tup[0], reverse=True)\n            # Go through list to find columns to return\n            cols = []\n            for i, row in enumerate(correlations_list):\n                correlation = row[0]\n                col = row[1]\n                if i < n_cols:\n                    cols.append(col)\n                    # print the correlation\n                    if not silent:\n                        print(col, \":\", correlation)            \n            corrdf = df_temp.copy()\n            corrdf = corrdf[cols].corr()\n            if visualise == True:\n                sns.heatmap(abs(corrdf), annot=True, cmap=\"Blues\")\n            return cols\n\n        def decision_tree(self, df_temp : pd.DataFrame, depth : int, target_col : str, class_names : typing.List = None) -> None:\n            \"\"\"\n            Draw a decision_tree from the given dataframe to the given depth and display it\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                The dataframe to create a decision tree from\n            depth : int\n                The depth of the decision tree to create\n            target_col : str\n                The target column to make a decision tree towards\n            \"\"\"\n            tree_set = df_temp.copy()\n            target = tree_set[target_col]\n            tree_set.drop([target_col], axis=1, inplace=True)\n            tree_clf = DecisionTreeClassifier(max_depth=depth, random_state=1)\n            tree_clf.fit(tree_set, target)\n            text_representation = tree.export_text(tree_clf, feature_names=tree_set.columns.tolist())\n            # print(text_representation)\n            print(\"accuracy: \" + str(tree_clf.score(tree_set, target)))    \n            plt.rcParams[\"figure.figsize\"] = (18,18)\n            # tree.plot_tree(tree_clf, feature_names=tree_set.columns, filled=True)\n            class_unique_values = class_names\n            if class_names == None:\n                class_column_values = df_temp[target_col].values.ravel()\n                class_unique_values = pd.unique(class_column_values)\n                class_unique_values = np.sort(class_unique_values)\n                class_unique_values = class_unique_values.astype('str')\n            dot_data = tree.export_graphviz(tree_clf, out_file=None, \n                                            feature_names=tree_set.columns,  \n                                            class_names=class_unique_values,\n                                            filled=True)\n            return graphviz.Source(dot_data, format=\"png\")\n            return None\n        \n        def manifold_embedding(self, df_temp : pd.DataFrame, target_col : str, manifold_type=\"tsne\", classes=['very low', 'low', 'med', 'high', 'very high']) -> None:\n            \"\"\"\n            Create a manifold embedding from a feature set based on a target value. This will take the dataset, scale it, bin the target and assign classes to it.\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                The features and target to create a manifold embedding from\n            target_col : str\n                The target column to create the manifold to. Must be label encoded. Default requires 5 unique values\n            manifold_type : str \n                The type of manifold \n            \"\"\"\n            y = df_temp[target_col]\n            X = df_temp.drop(columns=[target_col], axis=1)\n            standard_scaler = preprocessing.StandardScaler()\n            X = standard_scaler.fit_transform(X)\n            viz = Manifold(manifold=manifold_type, classes=classes)\n\n            viz.fit_transform(X[:4000], y[:4000])  # Fit the data to the visualizer\n            viz.show()               # Finalize and render the figure\n        \n        def pair_grid_plot(self, df_temp : pd.DataFrame, cols : typing.List) -> None:\n            \"\"\"\n            Pair grid plots of given columns\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Data to plot from\n            cols : typing.List\n                Columns to make a pairgrid from\n            \"\"\"\n            df_temp = df_temp.copy()\n            # Deal with seaborn bug on boolean datatype\n            for col in df_temp.columns:\n                if df_temp[col].dtype == \"bool\":\n                    df_temp[col] = df_temp[col].astype(int)\n            # Create PairGrid\n            g = sns.PairGrid(df_temp[cols].iloc[:500,:], diag_sharey=False)\n            g.map_upper(sns.histplot, multiple=\"stack\")\n            g.map_lower(sns.kdeplot)\n            g.map_diag(sns.kdeplot, lw=2)\n        \n        def pivot_table(self, df_temp : pd.DataFrame, target_col : str, number_to_display : int = 15, display : bool = True) -> pd.DataFrame:\n            \"\"\"\n            Create a pivot table based on the target variable and if requested plot it. So that the mean of each variable can be compared to others.\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                DataFrame that contains the features to create a pivot table from\n            target_col : str\n                The target column that will be pivoted on - Must be categorical \n            display : bool\n                Display plot of table\n            number_to_display : int\n                The number of rows to aim for in the pivot table\n\n            Returns\n            -------\n            pd.DataFrame\n                Pivot Table that was created from the features provided\n            \"\"\"\n            df_temp = df_temp.copy()\n            y = df_temp[target_col]\n            X = df_temp.drop(columns=target_col, axis=1)\n            df_temp = pd.DataFrame(X)  \n            df_temp.columns = X.columns\n            df_temp[target_col] = y\n            table = pd.pivot_table(data=df_temp,index=[target_col]).T\n            # Cut out rows with no real difference\n            table_copy = table.copy()\n            differences_tuple_list = []\n            for idx, row in table_copy.iterrows():\n                # Find difference\n                difference = max(row)-min(row)\n                # Record difference\n                differences_tuple_list.append((idx, difference))\n            differences_tuple_list = sorted(differences_tuple_list, key=lambda tup: tup[0], reverse=True)\n            # Get indexes of top 15 sorted tuples\n            diff_index = []\n            i = 0\n            for t in differences_tuple_list:\n                if t[1] != 0 and i < number_to_display:\n                    i = i + 1\n                    diff_index.append(t[0])\n            diff_index = [x for x in table_copy.index.tolist() if x not in diff_index]\n            table = table.drop(diff_index)\n            if display:\n                sns.heatmap(table, annot=True, cmap=\"Blues\")\n            return table\n        \n        def pipeline(self, df_temp, target_col : str, fe : feature_engineering, kind=\"categorical\"):\n            \"\"\"\n            A pipeline to apply a bunch of different EDA procedures against the \n            \"\"\"\n            print(\"***\")\n            cprint(\"Class Balance\", attrs=['bold'])\n            print(\"***\")\n            if kind == \"categorical\":\n                self.class_balance(df_temp, target_col)\n                plt.show()\n            else: \n                sns.displot(x=df_temp[target_col])\n                plt.title(\"Class Balance\")\n                plt.show()\n            print(\"***\")\n            cprint(\"Dimensional Reduction\", attrs=['bold'])\n            print(\"***\")\n            self.pca_dimension_reduction_info(df_temp, target_col)\n            plt.show()\n            self.pca_four_component(df_temp, target_col, fe)\n            plt.show()            \n            print(\"***\")\n            cprint(\"Box Plots\", attrs=['bold'])\n            print(\"***\")\n            continous_columns = fe.detect_continous_columns(df_temp, 20, continous_columns=[])\n            self.box_plots(df_temp, continous_columns)\n            plt.show()\n            print(\"***\")\n            cprint(\"Line Plots\", attrs=['bold'])\n            print(\"***\")\n            continous_columns = fe.detect_continous_columns(df_temp, 20, continous_columns=[])\n            self.line_plots(df_temp, continous_columns)\n            plt.show()\n            print(\"***\")\n            cprint(\"Correlations\", attrs=['bold'])\n            print(\"***\")\n            scaled_df = fe.quantile_transform_column_wise(df_temp, target_col)\n            correlations = self.calculate_correlations(scaled_df, target_col, 10, visualise=True)\n            plt.show()\n            print(\"***\")\n            cprint(\"Pair Grid Plot\", attrs=['bold'])\n            print(\"***\")\n            self.pair_grid_plot(df_temp, self.calculate_correlations(df_temp, target_col, 5))\n            plt.show()\n            print(\"***\")\n            cprint(\"Decision Tree\", attrs=['bold'])\n            print(\"***\")  \n            tree_graph = None\n            if kind == \"regression\":\n                df_temp_copy = df_temp.copy()\n                df_temp_copy[\"TargetBin\"] = pd.qcut(df_temp_copy[target_col], 5, [\"very low\", \"low\",\"med\",\"high\",\"very high\"]).cat.codes\n                tree_graph = self.decision_tree(df_temp_copy.drop(columns=target_col), 2, \"TargetBin\", [\"very low\", \"low\",\"med\",\"high\",\"very high\"])\n            else:\n                tree_graph = self.decision_tree(df_temp, 2, target_col)\n            if tree_graph is not None:\n                display(tree_graph)\n                plt.show()\n            print(\"***\")\n            cprint(\"Anomalies\", attrs=['bold'])\n            print(\"***\")    \n            plt.rcParams[\"figure.figsize\"] = (9,9)\n            eda.pca_visualisation_2d(fe.quantile_transform_column_wise(df_temp, \"anomaly_IsolationForest\", \"normal\"), \"anomaly_IsolationForest\", \"Anomalies in Data\")\n            plt.show()\n            if kind == \"regression\":\n                print(\"***\")\n                cprint(\"Normality Test\", attrs=['bold'])\n                print(\"***\")    \n                stats.probplot(df[target_col], dist=\"norm\", plot=plt)\n                plt.show()\n            print(\"***\")\n            cprint(\"Pivot Table\", attrs=['bold'])\n            print(\"***\")                \n            if kind == \"regression\":\n                scaled_df[\"TargetBin\"] = pd.qcut(scaled_df[target_col], 5, [\"very low\", \"low\",\"med\",\"high\",\"very high\"])\n                self.pivot_table(scaled_df.drop(columns=target_col), \"TargetBin\")\n            else:\n                self.pivot_table(scaled_df, target_col)                \n            plt.show()\n            print(\"***\")\n            cprint(\"Manifold Embedding\", attrs=['bold'])\n            print(\"***\")                                  \n            if kind == \"regression\":\n                scaled_df[\"TargetBin\"] = pd.qcut(scaled_df[target_col], 5, [\"very low\", \"low\",\"med\",\"high\",\"very high\"]).cat.codes\n                self.manifold_embedding(scaled_df.drop(columns=target_col), \"TargetBin\")\n            else:\n                scaled_df_copy = scaled_df.copy()\n                le = preprocessing.LabelEncoder()\n                unique_values = scaled_df_copy[target_col].unique()\n                unique_values.sort()\n                unique_values = unique_values[::-1]\n                le.fit(scaled_df_copy[target_col].unique())\n                scaled_df_copy[target_col] = le.transform(scaled_df_copy[target_col])\n                self.manifold_embedding(scaled_df_copy, target_col, classes=unique_values.tolist())    \n            plt.show()    \n    \n    class feature_selection:\n        \"\"\"\n        Determine which features to use for training. \n        \"\"\"\n        def kendall_tau_feature_elimination(df_temp : pd.DataFrame, columns : typing.List, target_col : str, test_p_value : float = 0.001) -> typing.List:\n            \"\"\"\n            Eliminate features that don't pass a Kendall Tau test in regards to the target variable. \n            This would tend to eliminate useless or unhelpful features from the dataset while retaining appropriate ones.\n            A list of features the do pass the test is created and returned. \n\n            Should be used with continous columns only.\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                DataFrame to conduct Kendall Tau on features \n            columns : typing.List\n                A list of continous columns\n            target_col : str\n                Target column to conduct Kendall Tau towards\n            test_p_value : float\n                P value to test against. The sensitivity for \n                \n            Returns\n            -------\n            typing.List\n                features that passed the Kendall Tau features\n            \"\"\"\n            new_features = []\n            for feature in columns:\n                tau, p_value = stats.kendalltau(df_temp[target_col], df_temp[feature])\n                if p_value <= test_p_value:\n                    new_features.append(feature)\n            return new_features    \n\n        def select_from_model_features(df_temp: pd.DataFrame, target_col : str, estimator : BaseEstimator, threshold : float = None) -> typing.Tuple[typing.List, np.ndarray]:\n            \"\"\"\n            Use a compatible sklearn estimator for determining a list of features that are important to the dataset\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                dataframe to find the features in\n            target_col : str\n                the name of the target column so it can be selected\n            estimator : BaseEstimator\n                The sklearn estimator to fit on. Must have coef_\n            threshold : float\n                Threshold to decide which features to keep or not\n\n            Returns\n            -------\n            typing.Tuple[typing.List, np.ndarray]\n                List of features selected, array of the coefficients that have been selected\n            \"\"\"\n            X = df_temp.drop(columns=target_col)\n            y = df_temp[target_col]\n            estimator.fit(X, y)\n            model = SelectFromModel(estimator, threshold = threshold, prefit=True) \n            feature_names = np.array(df_temp.drop(columns=target_col).columns)\n            return feature_names[model.get_support()].tolist(), model.estimator.coef_\n    \n    class prediction:\n        \"\"\"\n        Create and compare predictors then analyse them\n        \"\"\"\n        def categorical_model_analysis(self, clf : BaseEstimator, X : np.ndarray, y : np.ndarray):\n            \"\"\"\n            Plot and display various graphs that will increase model explainability\n            Parameters\n            ----------\n            clf : BaseEstimator\n                The sklearn compatible estimator to investigate\n            X : np.ndarray\n                The features to use the estimator on\n            y : np.ndarray\n                The target to use the estimator on\n\n            \"\"\"\n            X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n            clf.fit(X_train, y_train)\n            predictions = clf.predict(X_test)\n            print(\"***\")\n            cprint(\"Confusion Matrix\", attrs=['bold'])\n            print(\"***\")\n            cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n            disp.plot()\n            plt.grid(False)\n            plt.show()\n            print(\"***\")\n            cprint(\"Precision Recall Curve\", attrs=['bold'])\n            print(\"***\")\n            precision_recall_curve(clf, X_train, y_train, X_test, y_test)\n            print(\"***\")\n            cprint(\"ROC AUC\", attrs=['bold'])\n            print(\"***\")\n            roc_auc(clf, X_train, y_train, X_test=X_test, y_test=y_test)\n            print(\"***\")\n            cprint(\"Classification Report\", attrs=['bold'])\n            print(\"***\")\n            print(classification_report(y_test, predictions, labels=clf.classes_, zero_division=False))\n            \n        def classification_compare(self, X : np.ndarray, y : np.ndarray) -> typing.List:\n            \"\"\"\n            Classify the target col using a number of classifiers\n\n            Parameters\n            ----------\n            X : np.ndarray\n                The features to use for classification\n            y : np.ndarray\n                The target to use the classifiers on\n\n            Returns\n            -------\n            typing.List\n                A list of classifiers that have been fitted on the data provided\n            \"\"\"\n\n            classifiers = [\n                (\"AdaBoostClassifier\", AdaBoostClassifier()),\n                (\"DecisionTreeClassifier\", DecisionTreeClassifier()),\n                (\"GaussianNB\", GaussianNB()),\n                #(\"GaussianProcessClassifier\", GaussianProcessClassifier()),\n                (\"GradientBoostingClassifier\", GradientBoostingClassifier()),\n                (\"HistGradientBoostingClassifier\", HistGradientBoostingClassifier(random_state=1)),\n                (\"KNeighborsClassifier\", KNeighborsClassifier()),\n                (\"RandomForestClassifier\", RandomForestClassifier(random_state=1)),\n                (\"SVC\", SVC())\n            ]\n            classification_results = []\n            kfold = KFold(n_splits=5)\n            for entry in classifiers:\n                name : str = entry[0]\n                classifier = entry[1]\n                scores = []\n                for train_index, test_index in kfold.split(X, y):\n                    classifier.fit(X[train_index], y[train_index])\n                    scores.append(classifier.score(X[test_index], y[test_index]))\n                classification_results.append((classifier, (sum(scores) / len(scores))))\n                print(name, \"Score:\", sum(scores) / len(scores))        \n            return classification_results\n        \n        def confusion_matrix_display(self, clf : BaseEstimator, X : np.ndarray, y : np.ndarray) -> None:\n            \"\"\"\n            Shown the confusion matrix for a given sklearn estimator\n            \n            Parameters\n            ----------\n            clf : BaseEstimator\n                The classifier to use for the confusion matrix\n            X : np.ndarray\n                The numpy array of features\n            y : np.ndarray\n                The numpy array of the target variable\n            \"\"\"\n            X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n            predictions = clf.predict(X_test)\n            cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n            disp.plot()\n            plt.grid(False)\n            plt.show()\n            \n        @ignore_warnings(category=ConvergenceWarning)\n        def ensemble_regression_compare(self, X : np.ndarray, y : np.ndarray) -> typing.List:\n            \"\"\"\n            Regress on the target col using a number of ensemble models\n\n            Parameters\n            ----------\n            X : np.ndarray\n                The features to use for regression\n            y : np.ndarray\n                The target to use the regression on\n\n            Returns\n            -------\n            typing.List\n                A list of regressor that have been fitted on the data provided\n            \"\"\"\n            ensembles = [(\"GradientBoostingRegressor\", GradientBoostingRegressor()), (\"RandomForestRegressor\", RandomForestRegressor()), (\"ExtraTreesRegressor\", ExtraTreesRegressor()), (\"AdaBoostRegressor\", AdaBoostRegressor()), (\"HistGradientBoostingRegressor\", HistGradientBoostingRegressor())]\n            ensemble_results = []\n            kfold = KFold(n_splits=5)\n            for entry in ensembles:\n                name : str = entry[0]\n                model = entry[1]\n                scores = []\n                mae_scores = []\n                for train_index, test_index in kfold.split(X, y):\n                    model.fit(X[train_index], y[train_index])\n                    scores.append(model.score(X[test_index], y[test_index]))\n                    mae_scores.append(mean_absolute_error(y[test_index], model.predict(X[test_index])))\n                ensemble_results.append((model, (sum(scores) / len(scores))))\n                print(name, \"Score:\", sum(scores) / len(scores))        \n                print(name, \"MAE Accuracy:\", sum(mae_scores) / len(mae_scores))        \n                print(\"***\")        \n            return ensemble_results\n\n        def linear_classification_compare(self, X : np.ndarray, y : np.ndarray) -> typing.List:\n            \"\"\"\n            Classify the target col using a number of linear classifiers\n            \n            Parameters\n            ----------\n            X : np.ndarray\n                The features to use for classification\n            y : np.ndarray\n                The target to use the classifiers on\n                \n            Returns\n            -------\n            typing.List\n                A list of classifiers that have been fitted on the data provided\n            \"\"\"\n            classifiers = [(\"RidgeClassifier\", linear_model.RidgeClassifier()), (\"SGDClassifier\", linear_model.SGDClassifier()), (\"LogisticRegression\", linear_model.LogisticRegression(solver=\"liblinear\"))]\n            classification_results = []\n            kfold = KFold(n_splits=5)\n            for entry in classifiers:\n                name : str = entry[0]\n                classifier = entry[1]\n                scores = []\n                for train_index, test_index in kfold.split(X, y):\n                    classifier.fit(X[train_index], y[train_index])\n                    scores.append(classifier.score(X[test_index], y[test_index]))\n                classification_results.append((classifier, (sum(scores) / len(scores))))\n                print(name, \"Score:\", sum(scores) / len(scores))        \n            return classification_results\n        \n        @ignore_warnings(category=ConvergenceWarning)\n        def linear_regression_compare(self, X : np.ndarray, y : np.ndarray) -> typing.List:\n            \"\"\"\n            Regress on the target col using a number of linear regressors\n\n            Parameters\n            ----------\n            X : np.ndarray\n                The features to use for regression\n            y : np.ndarray\n                The target to use the regression on\n\n            Returns\n            -------\n            typing.List\n                A list of regressor that have been fitted on the data provided\n            \"\"\"\n            regressors = [(\"LinearRegression\", linear_model.LinearRegression()), (\"RidgeRegression\", linear_model.Ridge()), (\"Lasso\", linear_model.Lasso(max_iter=4000)), (\"ElasticNet\", linear_model.ElasticNet()),(\"LassoLARS\", linear_model.LassoLars(normalize=True)),(\"BayesianRidge\", linear_model.BayesianRidge()), (\"ARDRegression\", linear_model.ARDRegression()), (\"Stochastic Gradient Descent\", linear_model.SGDRegressor()), (\"Huber Regressor\", linear_model.HuberRegressor())]\n            regressor_results = []\n            kfold = KFold(n_splits=5)\n            for entry in regressors:\n                name : str = entry[0]\n                regressor = entry[1]\n                scores = []\n                mae_scores = []\n                for train_index, test_index in kfold.split(X, y):\n                    regressor.fit(X[train_index], y[train_index])\n                    scores.append(regressor.score(X[test_index], y[test_index]))\n                    mae_scores.append(mean_absolute_error(y[test_index], regressor.predict(X[test_index])))\n                regressor_results.append((regressor, (sum(scores) / len(scores))))\n                print(name, \"Score:\", sum(scores) / len(scores))        \n                print(name, \"MAE Accuracy:\", sum(mae_scores) / len(mae_scores))        \n                print(\"***\")        \n            return regressor_results\n        \n        def residuals_plot(self, model : BaseEstimator, X_train : np.ndarray, y_train : np.ndarray, X_test : np.ndarray, y_test : np.ndarray) -> None:\n            \"\"\"\n            Create a residuals plot for a given model\n\n            Parameters\n            ----------\n            model : BaseEstimator\n                The trained regression model that will be used to make a regression model from\n            X_train : np.ndarray\n                The features to train against\n            y_train : np.ndarray\n                The target to train against\n            X_test : np.ndarray\n                The features to score against\n            y_test : np.ndarray\n                The target to score against\n            \"\"\"\n            plt.rcParams[\"figure.figsize\"] = (9,9)\n            visualizer = ResidualsPlot(model)\n            visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n            visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n            visualizer.show()                 # Finalize and render the figure\n            \n        def prediction_error(self, model : BaseEstimator, X_train : np.ndarray, y_train : np.ndarray, X_test : np.ndarray, y_test : np.ndarray) -> None:\n            \"\"\"\n            Create a prediction error plot for a given model\n\n            Parameters\n            ----------\n            model : BaseEstimator\n                The trained regression model that will be used to make a regression visualisation from\n            X_train : np.ndarray\n                The features to train against\n            y_train : np.ndarray\n                The target to train against\n            X_test : np.ndarray\n                The features to score against\n            y_test : np.ndarray\n                The target to score against\n            \"\"\"\n            plt.rcParams[\"figure.figsize\"] = (9,9)\n            visualizer = PredictionError(model)\n            visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n            visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n            visualizer.show()                 # Finalize and render the figure\n        \n        def regression_results(self, y_true : np.ndarray, y_pred : np.ndarray) -> None:\n            \"\"\"\n            Give a read out of common regressor metrics\n\n            Parameters\n            ----------\n            y_true : np.ndarray\n                actual results\n            y_pred : np.ndarray\n                predicted results\n            \"\"\"\n            # Regression metrics\n            explained_variance=metrics.explained_variance_score(y_true, y_pred)\n            mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n            mse=metrics.mean_squared_error(y_true, y_pred) \n            median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n            r2=metrics.r2_score(y_true, y_pred)\n            print('Explained Variance:     ', round(explained_variance,4))    \n            if np.amin(y_true) > 0 and np.amin(y_pred) > 0:\n                mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n                print('Mean Squared Log Error: ', round(mean_squared_log_error,4))\n            print('R2:                     ', round(r2,4))\n            print('Mean Absolute Error:    ', round(mean_absolute_error,4))\n            print('Mean Squared Error:     ', round(mse,4))\n            print('Root Mean Square Error: ', round(np.sqrt(mse),4))\n            \n        def categorical_pipeline(self, train_df : pd.DataFrame, target_col : str) -> typing.List:\n            \"\"\"\n            A pipeline that goes through and finds a list of classifiers and visualises the best one\n\n            Parameters\n            ----------\n            train_df : pd.DataFrame\n                The training data frame \n            target_col : str\n                The name of the target column\n                \n            Returns\n            -------\n            typing.List\n                A list of classifiers with score\n            \"\"\"\n            # Define X and y\n            X = train_df.drop(columns=target_col).values\n            y = train_df[target_col].values\n            # Compare Linear Classifiers\n            classifiers = self.linear_classification_compare(X, y)\n            # Compare Ensemble Classifiers\n            classifiers.extend(self.classification_compare(X, y))\n            # Sort list\n            classifiers = sorted(classifiers, key=lambda tup: tup[1], reverse=True)\n            # Get best Classifier\n            classifier = classifiers[0]\n            # Print best Classifier\n            print(\"******\")\n            print(\"Best Classifier\")\n            print(\"******\")\n            print(classifier[0])\n            # Create the train and test data\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n            # Print metrics\n            self.categorical_model_analysis(classifier[0], X , y)\n            return classifiers\n            \n        def regression_pipeline(self, train_df : pd.DataFrame, target_col : str) -> typing.List:\n            \"\"\"\n            A pipeline that goes through and finds a list of regressors and visualises the best one\n            \n            Parameters\n            ----------\n            train_df : pd.DataFrame\n                The training data frame \n            target_col : str\n                The name of the target column\n                  \n            Returns\n            -------\n            typing.List\n                A list of regressors with score\n            \"\"\"\n            # Define X and y\n            X = train_df.drop(columns=target_col).values\n            y = train_df[target_col].values\n            # Compare Linear Regressors\n            regressors = self.linear_regression_compare(X, y)\n            # Compare Ensemble Regressors\n            regressors.extend(self.ensemble_regression_compare(X, y))\n            # Sort list\n            regressors = sorted(regressors, key=lambda tup: tup[1], reverse=True)\n            # Get best Regressor\n            regressor = regressors[0]\n            # Print best Regressor\n            print(\"******\")\n            print(\"Best Regressor\")\n            print(\"******\")\n            print(regressor[0])\n            # Create the train and test data\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n            # Print metrics\n            self.regression_results(y_test, regressor[0].predict(X_test))\n            # Plot prediction error of X and y of the data\n            self.prediction_error(regressor[0], X_train, y_train, X_test, y_test)\n            # Plot residuals of X and y of the data\n            self.residuals_plot(regressor[0], X_train, y_train, X_test, y_test)\n            return regressors","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-08T07:55:31.744612Z","iopub.execute_input":"2022-04-08T07:55:31.744915Z","iopub.status.idle":"2022-04-08T07:55:31.940583Z","shell.execute_reply.started":"2022-04-08T07:55:31.744883Z","shell.execute_reply":"2022-04-08T07:55:31.939888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n\nIn this notebook I will be using an AutoML library of my own development to do feature engineering, EDA and prediction. This library is entirely contained in the hidden cell above. \n\nThis has been an ongoing development and today I will add the LightGBM and XGBoost classifiers and work on feature selection.\n\nNotebooks on the development of the RabbitML library are below: \n\n[AutoML from Scratch #1](https://www.kaggle.com/code/taranmarley/automl-from-scratch-1/notebook)\n\n[AutoML from Scratch #2](https://www.kaggle.com/code/taranmarley/automl-from-scratch-2/notebook)\n\n[AutoML from Scratch #3](https://www.kaggle.com/code/taranmarley/automl-from-scratch-3/notebook)\n\n[AutoML from Scratch #4](https://www.kaggle.com/code/taranmarley/automl-from-scratch-4/notebook)\n\n[AutoML from Scratch #5](https://www.kaggle.com/code/taranmarley/automl-from-scratch-5/notebook)\n\n[AutoML from Scratch #6](https://www.kaggle.com/code/taranmarley/automl-from-scratch-6/notebook)\n\n[AutoML from Scratch #7](https://www.kaggle.com/code/taranmarley/automl-from-scratch-7/notebook)\n\n[AutoML from Scratch #8](https://www.kaggle.com/code/taranmarley/automl-from-scratch-8/notebook)","metadata":{}},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/spaceship-titanic/train.csv\")\ntest_df = pd.read_csv(\"../input/spaceship-titanic/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-08T07:55:31.942298Z","iopub.execute_input":"2022-04-08T07:55:31.943033Z","iopub.status.idle":"2022-04-08T07:55:31.98607Z","shell.execute_reply.started":"2022-04-08T07:55:31.942988Z","shell.execute_reply":"2022-04-08T07:55:31.985325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Look at data","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T07:55:32.141909Z","iopub.execute_input":"2022-04-08T07:55:32.142401Z","iopub.status.idle":"2022-04-08T07:55:32.164803Z","shell.execute_reply.started":"2022-04-08T07:55:32.142351Z","shell.execute_reply":"2022-04-08T07:55:32.16384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Describe all the data","metadata":{}},{"cell_type":"code","source":"df.describe(include='all').T","metadata":{"execution":{"iopub.status.busy":"2022-04-08T07:55:32.550003Z","iopub.execute_input":"2022-04-08T07:55:32.550272Z","iopub.status.idle":"2022-04-08T07:55:32.607866Z","shell.execute_reply.started":"2022-04-08T07:55:32.550245Z","shell.execute_reply":"2022-04-08T07:55:32.607169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"fe = rabbitml.feature_engineering()\ndf, test_df = fe.pipeline(df, test_df, target_col=\"Transported\", id_cols=[\"PassengerId\"], break_up_cols=[(\"Cabin\", \"/\")])\ndf.head(4)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T07:55:32.970077Z","iopub.execute_input":"2022-04-08T07:55:32.970379Z","iopub.status.idle":"2022-04-08T07:55:43.899748Z","shell.execute_reply.started":"2022-04-08T07:55:32.970339Z","shell.execute_reply":"2022-04-08T07:55:43.898753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"eda = rabbitml.eda()\neda.pipeline(df, \"Transported\", fe, \"categorical\")\ndisplay()","metadata":{"execution":{"iopub.status.busy":"2022-04-08T07:55:43.901197Z","iopub.execute_input":"2022-04-08T07:55:43.901408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Observation Selection\n\nIt is useful to chuck out observations from training that are seen as anomolous based on the test dataframe. ","metadata":{}},{"cell_type":"code","source":"def iforest_test_elimination(df_temp : pd.DataFrame, test_df_temp : pd.DataFrame, target_col : str) -> pd.DataFrame:\n    \"\"\"\n    Eliminate observations from train that are seen as anomolous from the point of view of the test dataset\n    \n    Parameters\n    ----------\n    df_temp : pd.DataFrame\n        Training Dataframe containing the training features. This is the dataframe we will be deleting from\n    test_df_temp : pd.DataFrame\n        Test DataFrame containing the test features. This is the dataframe the isolation forest will be trained on \n    \"\"\"\n    iforest = IsolationForest(random_state=1, contamination=0.01)\n    iforest.fit(test_df_temp.values)\n    anomaly_removed_df = df_temp.copy()\n    anomaly_removed_df[\"anomaly_delete\"] = iforest.predict(anomaly_removed_df.drop(columns=target_col).values)\n    anomaly_removed_df = anomaly_removed_df.drop(anomaly_removed_df[anomaly_removed_df.anomaly_delete == -1].index)\n    return anomaly_removed_df.drop(columns=\"anomaly_delete\")\n\nanomaly_removed_df = iforest_test_elimination(df, test_df, \"Transported\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"markdown","source":"Detect the columns that have a similar distribution to the test set","metadata":{}},{"cell_type":"code","source":"from scipy.stats import ks_2samp\n\ndef check_distributions(df_temp : pd.DataFrame, df_test_temp : pd.DataFrame) -> typing.List[str]:\n    \"\"\"\n    Check the distributions between columns between two dataframes \n    \n    Parameters\n    ----------\n    df_temp : pd.DataFrame\n        The first dataframe to check the distributions by column \n    df_test_temp : pd.DataFrame\n        The second dataframe to check the distributions by column\n        \n    Returns\n    -------\n    typing.List[str]\n        The list of columns with the same distribution between dataFrames\n    \"\"\"\n    same_cols = []\n    for col in df_temp.columns:\n        if col in df_test_temp.columns:\n            if (ks_2samp(df_temp[col], df_test_temp[col])[1]) > 0.85:\n                same_cols.append(col)\n    return same_cols\n\nsame_cols = check_distributions(anomaly_removed_df, test_df)\nsamed_df = anomaly_removed_df.copy()[same_cols]\nsamed_df[\"Transported\"] = anomaly_removed_df[\"Transported\"]\nsamed_test_df = test_df[same_cols]\nsamed_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled_df = fe.quantile_transform_column_wise(samed_df, \"Transported\")\nscaled_test_df = fe.quantile_transform_column_wise(samed_test_df, \"Transported\")\nfinal_scaled_df = scaled_df.copy()\nfinal_scaled_test_df = scaled_test_df.copy()\nnew_features = rabbitml.feature_selection.kendall_tau_feature_elimination(scaled_df, scaled_df.columns, \"Transported\", 0.001)\ncprint(\"Features to keep: \" + str(new_features))\neliminated_df = scaled_df.copy()[new_features]\nnew_features.remove(\"Transported\")\neliminated_test_df = scaled_test_df.copy()[new_features]\neliminated_df.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"scaled_test_df# Prediction","metadata":{}},{"cell_type":"code","source":"predictor = rabbitml.prediction()\nscaled_df = fe.min_max_column_wise(samed_df, \"Transported\")\nscaled_test_df = fe.min_max_column_wise(samed_test_df, \"Transported\")\nclassifiers = predictor.categorical_pipeline(scaled_df, \"Transported\")\ndisplay()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Add LightGBM","metadata":{}},{"cell_type":"markdown","source":"That's a good rundown of what we have so far but I'd like to add LightGBM to the mix and will do so below:","metadata":{}},{"cell_type":"code","source":"has_lightgbm = True\ntry:\n   import lightgbm\nexcept ImportError:\n   has_lightgbm = False\n\ndef lightgbm_classifier(X : np.ndarray, y : np.ndarray) -> typing.Tuple:\n    \"\"\"\n    Apply a lightgbm classifier to the give features and target\n    \n    Parameters\n    ----------\n    X : np.ndarray\n        The features of the data as a numpy array\n    y : np.ndarray\n        The target to classify the given features against\n        \n    Returns\n    -------\n    typing.Tuple\n        The lightgbm classifier and its score\n    \"\"\"\n    if has_lightgbm:\n        classifier = lightgbm.LGBMClassifier(random_state = 0)\n        kfold = KFold(n_splits=5)\n        scores = []\n        for train_index, test_index in kfold.split(X, y):\n            classifier.fit(X[train_index], y[train_index])\n            scores.append(classifier.score(X[test_index], y[test_index]))\n        print(\"LightGBM\", \"Score:\", sum(scores) / len(scores))    \n        return (classifier, sum(scores) / len(scores))\nlightgbmclf = lightgbm_classifier(scaled_df.drop(columns=\"Transported\").values, scaled_df[\"Transported\"].values)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor.categorical_model_analysis(lightgbmclf[0], scaled_df.drop(columns=\"Transported\").values, scaled_df[\"Transported\"].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Add XGBoost","metadata":{}},{"cell_type":"code","source":"has_xgboost = True\ntry:\n   import xgboost\nexcept ImportError:\n   has_xgboost = False\n\ndef xgboost_classifier(X : np.ndarray, y : np.ndarray) -> typing.Tuple:\n    \"\"\"\n    Apply a xgboost classifier to the give features and target\n    \n    Parameters\n    ----------\n    X : np.ndarray\n        The features of the data as a numpy array\n    y : np.ndarray\n        The target to classify the given features against\n        \n    Returns\n    -------\n    typing.Tuple\n        The xgboost classifier and its score\n    \"\"\"\n    if has_xgboost:\n        classifier = xgboost.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n        kfold = KFold(n_splits=5)\n        scores = []\n        for train_index, test_index in kfold.split(X, y):\n            classifier.fit(X[train_index], y[train_index])\n            scores.append(classifier.score(X[test_index], y[test_index]))\n        print(\"XGBoost\", \"Score:\", sum(scores) / len(scores))    \n        return (classifier, sum(scores) / len(scores))\n\nxgbclassifier = xgboost_classifier(scaled_df.drop(columns=\"Transported\").values, scaled_df[\"Transported\"].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor.categorical_model_analysis(xgbclassifier[0], scaled_df.drop(columns=\"Transported\").values, scaled_df[\"Transported\"].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\n\ndef stacked_classifier(X : np.ndarray, y : np.ndarray, estimators : typing.List, final_estimator : BaseEstimator) -> typing.Tuple:\n    \"\"\"\n    Apply stacked estimators as a classifier to the given features and target\n    \n    Parameters\n    ----------\n    X : np.ndarray\n        The features of the data as a numpy array\n    y : np.ndarray\n        The target to classify the given features against\n    estimators : typing.List\n        List of instantiated estimators to use\n    final_estimator : BaseEstimator\n        The estimator to use against the results of the stack\n        \n    Returns\n    -------\n    typing.Tuple\n        The stacked classifier and its score\n    \"\"\"\n    classifier = StackingClassifier(estimators=estimators, final_estimator=final_estimator)\n    kfold = KFold(n_splits=5)\n    scores = []\n    for train_index, test_index in kfold.split(X, y):\n        classifier.fit(X[train_index], y[train_index])\n        scores.append(classifier.score(X[test_index], y[test_index]))\n    print(\"Stacked Classifier\", \"Score:\", sum(scores) / len(scores))    \n    return (classifier, sum(scores) / len(scores))\n\nstacked = stacked_classifier(scaled_df.drop(columns=\"Transported\").values, scaled_df[\"Transported\"].values, estimators=[(\"histgradientbooster\", classifiers[0][0]),(\"XGB\", xgbclassifier[0])], final_estimator=linear_model.LogisticRegression())\ndisplay()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor.categorical_model_analysis(stacked[0], scaled_df.drop(columns=\"Transported\").values, scaled_df[\"Transported\"].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"scaled_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifiers[0][0].fit(final_scaled_df.drop(columns=\"Transported\").values, final_scaled_df[\"Transported\"].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.read_csv(\"../input/spaceship-titanic/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df[\"Transported\"] = classifiers[0][0].predict(final_scaled_test_df.values)\nsubmission_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nI am happy with what I have achieved here. The feature selection and prediction for classification has improved markedly, I will add the work today to the categorical pipeline. I can now expect reasonably good performance out the box for categorical tasks. I think it would be interesting to work more on model interpretability in the future. Potentially with work similar to what is being done with the EDA. A comprehensive feature selection pipeline would be a good addition as well.","metadata":{}}]}