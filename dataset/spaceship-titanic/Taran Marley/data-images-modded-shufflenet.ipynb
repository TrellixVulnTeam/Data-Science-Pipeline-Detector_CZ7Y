{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tab2img\n!pip install gender_guesser","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-10T00:23:29.362484Z","iopub.execute_input":"2022-03-10T00:23:29.363014Z","iopub.status.idle":"2022-03-10T00:23:54.637499Z","shell.execute_reply.started":"2022-03-10T00:23:29.36292Z","shell.execute_reply":"2022-03-10T00:23:54.636643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom PIL import Image\nfrom dateutil.parser import parse\nfrom typing import List\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom torch import optim\nimport torch.nn as nn\nimport sklearn.preprocessing as preprocessing","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-10T00:23:54.640392Z","iopub.execute_input":"2022-03-10T00:23:54.640863Z","iopub.status.idle":"2022-03-10T00:23:57.248797Z","shell.execute_reply.started":"2022-03-10T00:23:54.640813Z","shell.execute_reply":"2022-03-10T00:23:57.247812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n\nThis is a fun competition and I want to see how converting the data to an image format performs. I enjoy this approach and I do believe there could be an advantage to this idea if I can develop it further. I generally find this is a middle of the pack prediction method but perhaps there will be surprise in store for us.","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering\n\nThe aim here is to deal with the object columns and convert them to different numerical types so that we can apply ML models to them. I'm going to rush through this as if you want a more detailed approach I have more at my other notebook: https://www.kaggle.com/taranmarley/feature-engineering-eda-and-lightgbm","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/spaceship-titanic/train.csv\")\ntest_df = pd.read_csv(\"../input/spaceship-titanic/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:23:57.250381Z","iopub.execute_input":"2022-03-10T00:23:57.250685Z","iopub.status.idle":"2022-03-10T00:23:57.339588Z","shell.execute_reply.started":"2022-03-10T00:23:57.25064Z","shell.execute_reply":"2022-03-10T00:23:57.338811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Transported\"] = df[\"Transported\"].astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:23:57.342073Z","iopub.execute_input":"2022-03-10T00:23:57.342781Z","iopub.status.idle":"2022-03-10T00:23:57.352986Z","shell.execute_reply.started":"2022-03-10T00:23:57.342736Z","shell.execute_reply":"2022-03-10T00:23:57.352117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seperate_passenger_id(df_temp):\n    passenger_class = []\n    for idx, row in df_temp.iterrows():\n        passengerid = str(row[\"PassengerId\"])\n        if \"_\" in passengerid:\n            passenger_class.append(int(passengerid.split(\"_\")[1]))\n        else:\n            passenger_class.append(0)\n    df_temp[\"Passenger Class\"] = passenger_class\n    return df_temp\ndf = seperate_passenger_id(df)\ntest_df = seperate_passenger_id(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:23:57.353937Z","iopub.execute_input":"2022-03-10T00:23:57.354164Z","iopub.status.idle":"2022-03-10T00:23:58.036679Z","shell.execute_reply.started":"2022-03-10T00:23:57.354137Z","shell.execute_reply":"2022-03-10T00:23:58.035861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cabin Details Seperated**","metadata":{}},{"cell_type":"code","source":"def seperate_cabin(df_temp):\n    letters = []\n    numbers = []\n    final_letters = []\n    for idx, row in df_temp.iterrows():\n        cabin = str(row[\"Cabin\"])\n        if \"/\" in cabin:\n            letters.append(cabin.split(\"/\")[0])\n            numbers.append(cabin.split(\"/\")[1])\n            final_letters.append(cabin.split(\"/\")[2])\n        else:\n            letters.append(None)\n            numbers.append(-1)\n            final_letters.append(None)\n    df_temp[\"letters\"] = letters\n    df_temp[\"numbers\"] = numbers\n    df_temp[\"final_letters\"] = final_letters\n    return df_temp\ndf = seperate_cabin(df)\ntest_df = seperate_cabin(test_df)\ndf = df.drop(columns=\"Cabin\")\ntest_df = test_df.drop(columns=\"Cabin\")","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:23:58.038799Z","iopub.execute_input":"2022-03-10T00:23:58.039561Z","iopub.status.idle":"2022-03-10T00:23:58.731764Z","shell.execute_reply.started":"2022-03-10T00:23:58.039509Z","shell.execute_reply":"2022-03-10T00:23:58.731083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"numbers\"] = pd.to_numeric(df[\"numbers\"], errors = 'ignore')\ntest_df[\"numbers\"] = pd.to_numeric(test_df[\"numbers\"], errors = 'ignore')","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:23:58.733472Z","iopub.execute_input":"2022-03-10T00:23:58.73414Z","iopub.status.idle":"2022-03-10T00:23:58.755335Z","shell.execute_reply.started":"2022-03-10T00:23:58.734094Z","shell.execute_reply":"2022-03-10T00:23:58.754439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Gender from Name**","metadata":{}},{"cell_type":"code","source":"import gender_guesser.detector as gender\ndef predict_gender(df):\n    d = gender.Detector()\n    gender_predicted = []\n    for idx, row in df.iterrows():\n        name = str(row[\"Name\"])\n        if \" \" in name:\n            predicted = d.get_gender(name.split(\" \")[0])\n            if predicted == \"mostly_male\":\n                predicted = \"male\"\n            elif predicted == \"mostly_female\":\n                predicted = \"female\"\n            gender_predicted.append(predicted)\n        else:\n            gender_predicted.append(\"unknown\")\n    df[\"gender\"] = gender_predicted\n    df = pd.get_dummies(df, columns = [\"gender\"])\n    return df\n\ndf = predict_gender(df)\ntest_df = predict_gender(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:23:58.756661Z","iopub.execute_input":"2022-03-10T00:23:58.757419Z","iopub.status.idle":"2022-03-10T00:24:00.425283Z","shell.execute_reply.started":"2022-03-10T00:23:58.75738Z","shell.execute_reply":"2022-03-10T00:24:00.424319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Change the last names**","metadata":{}},{"cell_type":"code","source":"def last_names(df):\n    Last_Names = []\n    for idx, row in df.iterrows():\n        name = str(row[\"Name\"])\n        if \" \" in name:\n            Last_Names.append(name.split(\" \")[-1])\n        else:\n            Last_Names.append(None)\n    df[\"Name\"] = Last_Names\n    return df\ndf = last_names(df)\ntest_df = last_names(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:00.426587Z","iopub.execute_input":"2022-03-10T00:24:00.42687Z","iopub.status.idle":"2022-03-10T00:24:01.118116Z","shell.execute_reply.started":"2022-03-10T00:24:00.426838Z","shell.execute_reply":"2022-03-10T00:24:01.117186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Count Number of Family Members On Ship**","metadata":{}},{"cell_type":"code","source":"df_temp = pd.concat([df.copy(), test_df.copy()], ignore_index=True)\ndf_temp['Num_Family_Members'] = df_temp.groupby(['Name'])['PassengerId'].transform('nunique')\ndf['Num_Family_Members'] = df_temp['Num_Family_Members'][:8693].values\ntest_df['Num_Family_Members'] = df_temp['Num_Family_Members'][8693:].values","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:01.121385Z","iopub.execute_input":"2022-03-10T00:24:01.121714Z","iopub.status.idle":"2022-03-10T00:24:01.155001Z","shell.execute_reply.started":"2022-03-10T00:24:01.121677Z","shell.execute_reply":"2022-03-10T00:24:01.15408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Remove the PassengerId Column**","metadata":{}},{"cell_type":"code","source":"df = df.drop(columns=[\"PassengerId\"])\ntest_df = test_df.drop(columns=[\"PassengerId\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:01.156881Z","iopub.execute_input":"2022-03-10T00:24:01.157244Z","iopub.status.idle":"2022-03-10T00:24:01.16796Z","shell.execute_reply.started":"2022-03-10T00:24:01.157187Z","shell.execute_reply":"2022-03-10T00:24:01.167054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Encode columns to one hot encoding**","metadata":{}},{"cell_type":"code","source":"# df.drop(columns=\"Name\", inplace=True)\n# test_df.drop(columns=\"Name\", inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:01.169361Z","iopub.execute_input":"2022-03-10T00:24:01.169818Z","iopub.status.idle":"2022-03-10T00:24:01.177049Z","shell.execute_reply.started":"2022-03-10T00:24:01.169781Z","shell.execute_reply":"2022-03-10T00:24:01.175954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_columns(df, columns, test_df = None):\n    for col in columns:\n        le = preprocessing.LabelEncoder()\n        le.fit(df[col].astype(str))\n        if len(le.classes_) < 30:\n            df = pd.get_dummies(df, columns = [col])\n            if test_df is not None:\n                test_df = pd.get_dummies(test_df, columns = [col])\n        else:\n            check_col = df.copy()[col]\n            df[col] = le.transform(df[col].astype(str))\n            if test_df is not None:\n                #Clean out unseen labels\n                inputs = []\n                for idx, row in test_df.iterrows():\n                    if row[col] in pd.unique(check_col):\n                        inputs.append(row[col])\n                    else:\n                        inputs.append(None)\n                test_df[col] = inputs\n                test_df[col] = le.transform(test_df[col].astype(str))\n    return df, test_df\n#encode_columns(df, [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Name\", \"letters\", \"final_letters\"], test_df)\ndf, test_df = encode_columns(df, [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Name\", \"letters\", \"final_letters\"], test_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:01.178572Z","iopub.execute_input":"2022-03-10T00:24:01.179009Z","iopub.status.idle":"2022-03-10T00:24:07.164168Z","shell.execute_reply.started":"2022-03-10T00:24:01.178974Z","shell.execute_reply":"2022-03-10T00:24:07.163208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Fill in NaNs**\n\nI will also record where there was a NaN in case that proves useful","metadata":{}},{"cell_type":"code","source":"Age_Recorded = []\ndef fillna_create_column(df_temp, columns, value = 0):\n    \"\"\"\n    Fill na of provided columns and create columns to signify they weren't there\n    \"\"\"\n    for col in columns:\n        temp_col = []\n        for idx, row in df_temp.iterrows():\n            if row[col] != row[col]:\n                temp_col.append(0)\n            else:\n                temp_col.append(1)\n        df_temp[col + \"_exists\"] = temp_col\n        df_temp[col] = df_temp[col].fillna(0)\n    return(df_temp)\ndf = fillna_create_column(df, [\"Age\",\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\",\"Num_Family_Members\"])\ntest_df = fillna_create_column(test_df, [\"Age\",\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\",\"Num_Family_Members\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:07.165464Z","iopub.execute_input":"2022-03-10T00:24:07.165987Z","iopub.status.idle":"2022-03-10T00:24:11.822339Z","shell.execute_reply.started":"2022-03-10T00:24:07.165953Z","shell.execute_reply":"2022-03-10T00:24:11.821251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check that there are NaNs still in data","metadata":{}},{"cell_type":"code","source":"def detect_NaNs(df_temp): \n    print('NaNs in data: ', df_temp.isnull().sum().sum())\n    count_nulls = df_temp.isnull().sum().sum()\n    if count_nulls > 0:\n        print('******')\n        for col in df_temp.columns:\n            print('NaNs in', col + \": \", df_temp[col].isnull().sum().sum())\n        print('******')\n    print('')\ndetect_NaNs(df)\ndetect_NaNs(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:11.82406Z","iopub.execute_input":"2022-03-10T00:24:11.824374Z","iopub.status.idle":"2022-03-10T00:24:11.84598Z","shell.execute_reply.started":"2022-03-10T00:24:11.82433Z","shell.execute_reply":"2022-03-10T00:24:11.845021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create Interactions**","metadata":{}},{"cell_type":"code","source":"import itertools\ndef create_interactions(df_temp, column_list):\n    # Cross wise interactions\n    for x in itertools.combinations(column_list, 2):\n        df_temp[x[0]+\"+\"+x[1]] = df_temp[x[0]]+df_temp[x[1]]\n    # Iterative Totals\n    iterative_total = 0\n    i = 0\n    for j in (column_list):\n        iterative_total = iterative_total + df_temp[j]\n        if i > 0:\n            df_temp[\"A\" + str(i) + \"_iter_score\"] = iterative_total\n        i = i + 1\n    return df_temp\ndf = create_interactions(df, [\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\"])\ntest_df = create_interactions(test_df, [\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:11.847398Z","iopub.execute_input":"2022-03-10T00:24:11.847655Z","iopub.status.idle":"2022-03-10T00:24:11.874541Z","shell.execute_reply.started":"2022-03-10T00:24:11.847611Z","shell.execute_reply":"2022-03-10T00:24:11.8734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"TotalSpend\"] = df[\"A4_iter_score\"]\ndf = df.drop(columns=\"A4_iter_score\")\ntest_df[\"TotalSpend\"] = test_df[\"A4_iter_score\"]\ntest_df = test_df.drop(columns=\"A4_iter_score\")","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:11.875985Z","iopub.execute_input":"2022-03-10T00:24:11.876237Z","iopub.status.idle":"2022-03-10T00:24:11.891216Z","shell.execute_reply.started":"2022-03-10T00:24:11.876208Z","shell.execute_reply":"2022-03-10T00:24:11.890054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def spend_by_age(df_temp):\n    spending_by_age = []\n    for idx, row in df_temp.iterrows():\n        if row[\"Age\"] != 0:\n            spending_by_age.append((row[\"TotalSpend\"] / row[\"Age\"]))\n        else:\n            spending_by_age.append(0)    \n    return spending_by_age\ndf[\"spending_by_age\"] = spend_by_age(df)\ntest_df[\"spending_by_age\"] = spend_by_age(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:11.893001Z","iopub.execute_input":"2022-03-10T00:24:11.893672Z","iopub.status.idle":"2022-03-10T00:24:12.638863Z","shell.execute_reply.started":"2022-03-10T00:24:11.893568Z","shell.execute_reply":"2022-03-10T00:24:12.637817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_interactions_based_on_total(df_temp, column_list, total_col_name):\n    \"\"\"\n    Determine ratio of columns based on a total\n    \"\"\"\n    # Cross wise interactions\n    for j in (column_list):\n        df_temp[j + \" per \" + total_col_name] = df_temp[j] / df_temp[total_col_name]\n        df_temp[j + \" per \" + total_col_name] = df_temp[j + \" per \" + total_col_name].replace([np.inf, -np.inf], np.nan)\n        df_temp[j + \" per \" + total_col_name] = df_temp[j + \" per \" + total_col_name].fillna(0)\n    \n    return df_temp\ndf = create_interactions_based_on_total(df, [\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\"], \"TotalSpend\")\ntest_df = create_interactions_based_on_total(test_df, [\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\"], \"TotalSpend\")","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:12.640285Z","iopub.execute_input":"2022-03-10T00:24:12.640924Z","iopub.status.idle":"2022-03-10T00:24:12.664719Z","shell.execute_reply.started":"2022-03-10T00:24:12.640882Z","shell.execute_reply":"2022-03-10T00:24:12.663677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = create_interactions_based_on_total(df, [\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\"], \"Age\")\ntest_df = create_interactions_based_on_total(test_df, [\"RoomService\",\"FoodCourt\",\"ShoppingMall\",\"Spa\",\"VRDeck\"], \"Age\")","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:12.666096Z","iopub.execute_input":"2022-03-10T00:24:12.666972Z","iopub.status.idle":"2022-03-10T00:24:12.68809Z","shell.execute_reply.started":"2022-03-10T00:24:12.666924Z","shell.execute_reply":"2022-03-10T00:24:12.687234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create PCA Features**","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\n\ndf_temp = pd.concat([df.copy(), test_df.copy()], ignore_index=True)\ny = df_temp[\"Transported\"]\nX = df_temp.drop(columns=\"Transported\", axis=1)\nX_scaled = MinMaxScaler().fit_transform(X)\npca = PCA(n_components=10)\nX_p = pca.fit(X_scaled).transform(X_scaled)\nfor i in range(10):\n    df[\"PCA_\" + str(i)] = X_p[:8693,i]\n    test_df[\"PCA_\" + str(i)] = X_p[8693:,i]","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:12.6893Z","iopub.execute_input":"2022-03-10T00:24:12.689644Z","iopub.status.idle":"2022-03-10T00:24:13.264391Z","shell.execute_reply.started":"2022-03-10T00:24:12.689581Z","shell.execute_reply":"2022-03-10T00:24:13.263399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create Polynomial Features**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\ndef PolynomialFeatures_labeled(input_df,power, target_col, interaction_only=False):\n    '''Basically this is a cover for the sklearn preprocessing function. \n    The problem with that function is if you give it a labeled dataframe, it ouputs an unlabeled dataframe with potentially\n    a whole bunch of unlabeled columns. \n\n    Inputs:\n    input_df = Your labeled pandas dataframe (list of x's not raised to any power) \n    power = what order polynomial you want variables up to. (use the same power as you want entered into pp.PolynomialFeatures(power) directly)\n\n    Ouput:\n    Output: This function relies on the powers_ matrix which is one of the preprocessing function's outputs to create logical labels and \n    outputs a labeled pandas dataframe   \n    \n    Heavily modified from: https://stackoverflow.com/users/3633522/afflatus\n    '''\n    # Remove target_col\n    input_df = input_df.copy()\n    if target_col is not None:\n        target_col_saved = input_df[target_col]\n        input_df.drop(columns=target_col, inplace=True)\n    \n    poly = PolynomialFeatures(power, interaction_only=interaction_only)\n    output_nparray = poly.fit_transform(input_df)\n    powers_nparray = poly.powers_\n\n    input_feature_names = list(input_df.columns)\n    target_feature_names = [\"Constant Term\"]\n    for feature_distillation in powers_nparray[1:]:\n        intermediary_label = \"\"\n        final_label = \"\"\n        for i in range(len(input_feature_names)):\n            if feature_distillation[i] == 0:\n                continue\n            else:\n                variable = input_feature_names[i]\n                power = feature_distillation[i]\n                intermediary_label = \"%s^%d\" % (variable,power)\n                if final_label == \"\":         #If the final label isn't yet specified\n                    final_label = intermediary_label\n                else:\n                    final_label = final_label + \" x \" + intermediary_label\n        target_feature_names.append(final_label)\n    output_df = pd.DataFrame(output_nparray, columns = target_feature_names)\n    if target_col is not None:\n        output_df[target_col] = target_col_saved\n    return output_df\n\npoly_df = PolynomialFeatures_labeled(df, 2, \"Transported\")\npoly_test_df = PolynomialFeatures_labeled(test_df, 2, None)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:35:48.329847Z","iopub.execute_input":"2022-03-10T00:35:48.330364Z","iopub.status.idle":"2022-03-10T00:35:49.056867Z","shell.execute_reply.started":"2022-03-10T00:35:48.330326Z","shell.execute_reply":"2022-03-10T00:35:49.056024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Split Train Test**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\ntrain_ratio = 0.90\n\ny = poly_df[\"Transported\"]\npoly_df = poly_df.drop(columns=\"Transported\")\nX = poly_df\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=1 - train_ratio, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:35:53.596157Z","iopub.execute_input":"2022-03-10T00:35:53.59645Z","iopub.status.idle":"2022-03-10T00:35:53.839667Z","shell.execute_reply.started":"2022-03-10T00:35:53.59642Z","shell.execute_reply":"2022-03-10T00:35:53.838245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Scale Train Test**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\nscaler = preprocessing.MinMaxScaler().fit(X_train)\nscaled_poly = scaler.transform(X_train)\nscaled_poly_val = scaler.transform(X_val)\nscaled_poly_test = scaler.transform(poly_test_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:35:55.274179Z","iopub.execute_input":"2022-03-10T00:35:55.274519Z","iopub.status.idle":"2022-03-10T00:35:55.860634Z","shell.execute_reply.started":"2022-03-10T00:35:55.274484Z","shell.execute_reply":"2022-03-10T00:35:55.859792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Image Dataset","metadata":{}},{"cell_type":"markdown","source":"**Convert tabular data to images**","metadata":{}},{"cell_type":"code","source":"from tab2img.converter import Tab2Img\nmodel = Tab2Img()\ntrain_images = model.fit_transform(scaled_poly, y_train.values)\nval_images = model.transform(scaled_poly_val)\ntest_images = model.transform(scaled_poly_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:35:58.697287Z","iopub.execute_input":"2022-03-10T00:35:58.697567Z","iopub.status.idle":"2022-03-10T00:36:00.63133Z","shell.execute_reply.started":"2022-03-10T00:35:58.697535Z","shell.execute_reply":"2022-03-10T00:36:00.630482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualize the Images**","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(2,5)\nfor i in range(10):\n    nparray = test_images[i].reshape(55,55)\n    image = Image.fromarray(nparray * 255)\n    ax[i%2][i//2].imshow(image)\nfig.show()\n\nfig,ax = plt.subplots(2,5)\nfor i in range(10):\n    nparray = train_images[i].reshape(55,55)\n    image = Image.fromarray(nparray * 255)\n    ax[i%2][i//2].imshow(image)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:36:02.273831Z","iopub.execute_input":"2022-03-10T00:36:02.274231Z","iopub.status.idle":"2022-03-10T00:36:04.283321Z","shell.execute_reply.started":"2022-03-10T00:36:02.274195Z","shell.execute_reply":"2022-03-10T00:36:04.282488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create the Custom Dataset Class**\n\nWe need this to be able to load the image and label into the model we will create. So we will create a custom dataset to handle this.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n  def __init__(self, X, y, BatchSize, transform):\n    super().__init__()\n    self.BatchSize = BatchSize\n    self.y = y\n    self.X = X\n    self.transform = transform\n    \n  def num_of_batches(self):\n    \"\"\"\n    Detect the total number of batches\n    \"\"\"\n    return math.floor(len(self.list_IDs) / self.BatchSize)\n\n  def __getitem__(self,idx):\n    class_id = self.y[idx]\n    img = self.transform(np.nan_to_num(self.X[idx]))\n    return img, torch.tensor(class_id)\n\n  def __len__(self):\n    return len(self.X)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:18.974598Z","iopub.execute_input":"2022-03-10T00:24:18.974886Z","iopub.status.idle":"2022-03-10T00:24:18.983599Z","shell.execute_reply.started":"2022-03-10T00:24:18.974853Z","shell.execute_reply":"2022-03-10T00:24:18.982665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Instantiate the Datasets**\n\nWe will form them into torch dataloaders to make the data easier to work with. ","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torchvision import transforms\n\ntransform = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize([0.5], [0.5])\n            ])\n\ndataset_stages = ['train', 'val', 'test']\n\nbatch_size = 32\nimage_datasets = {'train' : CustomDataset(train_images, y_train.values, batch_size, transform), 'val' : CustomDataset(val_images, y_val.values, batch_size, transform), 'test' : CustomDataset(test_images, range(0,len(test_df)), batch_size, transform)}\ndataloaders = {'train' : DataLoader(image_datasets['train'], batch_size=image_datasets['train'].BatchSize, shuffle=True, num_workers=0), \n               'val' : DataLoader(image_datasets['val'], batch_size=image_datasets['val'].BatchSize, shuffle=True, num_workers=0), \n               'test' : DataLoader(image_datasets['test'], batch_size=image_datasets['test'].BatchSize, shuffle=False, num_workers=0)}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:18.987792Z","iopub.execute_input":"2022-03-10T00:24:18.988133Z","iopub.status.idle":"2022-03-10T00:24:19.235645Z","shell.execute_reply.started":"2022-03-10T00:24:18.988085Z","shell.execute_reply":"2022-03-10T00:24:19.234702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check an image from the dataset","metadata":{}},{"cell_type":"code","source":"image = transforms.ToPILImage()(image_datasets['train'][1][0].cpu()).convert(\"RGB\")\ndisplay(image)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:19.237028Z","iopub.execute_input":"2022-03-10T00:24:19.2373Z","iopub.status.idle":"2022-03-10T00:24:19.284684Z","shell.execute_reply.started":"2022-03-10T00:24:19.237269Z","shell.execute_reply":"2022-03-10T00:24:19.284011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Neural Network\n\n**Create Training Function**","metadata":{}},{"cell_type":"code","source":"import time\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=10, early_stop_value=0, categorical=True):\n    since = time.time()\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n            num_batches = 0\n            outputs = None\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                # Loading Bar\n                if (phase == 'train'):\n                    num_batches += 1\n                    percentage_complete = ((num_batches * batch_size) / (dataset_sizes[phase])) * 100\n                    percentage_complete = np.clip(percentage_complete, 0, 100)\n                    print(\"{:0.2f}\".format(percentage_complete), \"% complete\", end='\\r')\n\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n                \n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    labels = labels.type(torch.LongTensor)\n                    labels = labels.to(device)\n                    outputs = outputs.float().to(device)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        # TODO: try removal\n                        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                if categorical:\n                    predicted = torch.max(outputs.data, 1)[1] \n                    running_correct = (predicted == labels).sum()\n                    running_corrects += running_correct\n                else:\n                    running_loss += loss.item() * inputs.size(0)\n                    running_correct = 0\n                    for i in  range(0,len(outputs)):\n                        label = labels.unsqueeze(1).float()[i]\n                        running_correct += abs(abs(outputs[i]) -  abs(label))\n                    running_corrects += running_correct\n                    \n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            \n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n            #epoch_acc = sum(epoch_acc) / len(epoch_acc)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc.item()))\n            # Early Stop\n            if early_stop_value > 0:\n                if phase == 'val':\n                    val_accuracy = epoch_acc.item()\n        if early_stop_value > 0 and val_accuracy > early_stop_value:\n            print(\"*** EARLY STOP ***\")\n            break\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:19.286049Z","iopub.execute_input":"2022-03-10T00:24:19.286263Z","iopub.status.idle":"2022-03-10T00:24:19.30827Z","shell.execute_reply.started":"2022-03-10T00:24:19.286237Z","shell.execute_reply":"2022-03-10T00:24:19.307237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create Model**","metadata":{}},{"cell_type":"code","source":"from torchvision import models\nfrom torch.optim import lr_scheduler\n\nshufflenet = models.shufflenet_v2_x1_0()\nshufflenet.conv1[0] = nn.Conv2d(1, 24, kernel_size=(2, 2), stride=(1, 1))\nshufflenet.fc = nn.Linear(in_features=1024, out_features=2, bias=True)\nmodel_ft = shufflenet","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:19.309189Z","iopub.execute_input":"2022-03-10T00:24:19.309422Z","iopub.status.idle":"2022-03-10T00:24:19.385567Z","shell.execute_reply.started":"2022-03-10T00:24:19.309394Z","shell.execute_reply":"2022-03-10T00:24:19.384876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train Model**","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\noptimizer_ft = optim.Adam(model_ft.parameters(), lr=0.01)\n\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.01)\n\nmodel_ft = train_model(model_ft.to(device), criterion, optimizer_ft, exp_lr_scheduler, 16)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:24:19.38654Z","iopub.execute_input":"2022-03-10T00:24:19.387125Z","iopub.status.idle":"2022-03-10T00:35:42.284467Z","shell.execute_reply.started":"2022-03-10T00:24:19.387092Z","shell.execute_reply":"2022-03-10T00:35:42.282429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run on Test Set","metadata":{}},{"cell_type":"code","source":"predictions = []\n\noutputs = None\n\nfor inputs, labels in dataloaders['test']:\n    model_ft.eval()\n    model_ft.eval()\n    \n    inputs = inputs.to(device)\n    labels = labels.to(device)\n    outputs = model_ft(inputs)\n    \n    for o in torch.max(outputs.data, 1)[1]:\n        predictions.append(o.cpu().item())    ","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:35:42.285802Z","iopub.status.idle":"2022-03-10T00:35:42.287074Z","shell.execute_reply.started":"2022-03-10T00:35:42.286765Z","shell.execute_reply":"2022-03-10T00:35:42.286802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert back to boolean as required for submission","metadata":{}},{"cell_type":"code","source":"new_predictions = []\nfor p in predictions:\n    if p == 0:\n        new_predictions.append(False)\n    else:\n        new_predictions.append(True)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:35:42.288905Z","iopub.status.idle":"2022-03-10T00:35:42.289814Z","shell.execute_reply.started":"2022-03-10T00:35:42.289494Z","shell.execute_reply":"2022-03-10T00:35:42.289537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Submission","metadata":{}},{"cell_type":"code","source":"submissiondf = pd.read_csv(\"../input/spaceship-titanic/sample_submission.csv\")\nsubmissiondf[\"Transported\"] = new_predictions\nsubmissiondf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:35:42.291488Z","iopub.status.idle":"2022-03-10T00:35:42.292381Z","shell.execute_reply.started":"2022-03-10T00:35:42.292094Z","shell.execute_reply":"2022-03-10T00:35:42.292124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submissiondf.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T00:35:42.294092Z","iopub.status.idle":"2022-03-10T00:35:42.294971Z","shell.execute_reply.started":"2022-03-10T00:35:42.294688Z","shell.execute_reply":"2022-03-10T00:35:42.294718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nThese are much better results than I expected. A very good result from image recognition has been achieved here and to me implies this could be a very strong method going forward for this dataset. With all image recognition the disadvantage can be in interpretability, I do think this result is strongly driven by the polynomial features and that means to me that interactions can be a very strong path forward in this dataset and possibly power transforms should be explored. ","metadata":{}}]}