{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome to Spaceship Titanic!\n\nWe will do a quick clean-up of the data and then show you how to stack models to better your score. If you want to see my EDA go here:\nhttps://www.kaggle.com/code/crained/spaceship-titanic-pandas-profiling-eda","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn import (\n    ensemble,\n    model_selection,    \n    preprocessing,\n    tree,\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-04T11:55:35.219558Z","iopub.execute_input":"2022-04-04T11:55:35.220553Z","iopub.status.idle":"2022-04-04T11:55:36.675656Z","shell.execute_reply.started":"2022-04-04T11:55:35.220418Z","shell.execute_reply":"2022-04-04T11:55:36.674914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load dataset\ntrain=pd.read_csv(\"../input/spaceship-titanic/train.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:55:36.677295Z","iopub.execute_input":"2022-04-04T11:55:36.677764Z","iopub.status.idle":"2022-04-04T11:55:36.759592Z","shell.execute_reply.started":"2022-04-04T11:55:36.67772Z","shell.execute_reply":"2022-04-04T11:55:36.758753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Drop columns","metadata":{}},{"cell_type":"code","source":"train = train.drop(['PassengerId','Cabin', 'Name'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:55:36.760832Z","iopub.execute_input":"2022-04-04T11:55:36.761049Z","iopub.status.idle":"2022-04-04T11:55:36.773068Z","shell.execute_reply.started":"2022-04-04T11:55:36.761024Z","shell.execute_reply":"2022-04-04T11:55:36.772389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Features","metadata":{}},{"cell_type":"markdown","source":"We need to create dummy columns from string columns. This will create new columns for sex and embarked. Pandas has a convenient get_dummies function for that.","metadata":{}},{"cell_type":"code","source":"train = pd.get_dummies(train)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:55:36.77532Z","iopub.execute_input":"2022-04-04T11:55:36.775637Z","iopub.status.idle":"2022-04-04T11:55:36.805389Z","shell.execute_reply.started":"2022-04-04T11:55:36.775595Z","shell.execute_reply":"2022-04-04T11:55:36.804386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:55:36.810274Z","iopub.execute_input":"2022-04-04T11:55:36.810622Z","iopub.status.idle":"2022-04-04T11:55:36.821034Z","shell.execute_reply.started":"2022-04-04T11:55:36.810581Z","shell.execute_reply":"2022-04-04T11:55:36.820207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this point the \"VIP_True\" and \"CryoSleep_True\" columns are perfectly inverse correlated with False columns. Typically we remove any columns with perfect or very high positive or negative correlation. Multicollinearity can impact interpretation of feature importance and coefficients in some models.","metadata":{}},{"cell_type":"code","source":"train = train.drop(columns=[\"VIP_True\",\n                            \"CryoSleep_True\"])","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:55:36.822272Z","iopub.execute_input":"2022-04-04T11:55:36.822687Z","iopub.status.idle":"2022-04-04T11:55:36.830739Z","shell.execute_reply.started":"2022-04-04T11:55:36.822652Z","shell.execute_reply":"2022-04-04T11:55:36.830005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transported is what we are trying to predict so we will make it our y variable\ny = train.Transported\nX = train.drop(columns=\"Transported\")","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:55:36.831823Z","iopub.execute_input":"2022-04-04T11:55:36.832353Z","iopub.status.idle":"2022-04-04T11:55:36.847082Z","shell.execute_reply.started":"2022-04-04T11:55:36.832317Z","shell.execute_reply":"2022-04-04T11:55:36.845569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = model_selection.train_test_split(\n    X, y, test_size=0.3, random_state=42\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:55:36.849685Z","iopub.execute_input":"2022-04-04T11:55:36.851394Z","iopub.status.idle":"2022-04-04T11:55:36.861049Z","shell.execute_reply.started":"2022-04-04T11:55:36.851341Z","shell.execute_reply":"2022-04-04T11:55:36.860176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Many of the columns have missing values. We need to impute the numeric values. We only want to impute on the training set and then use that imputer to fill in the date for the test set. Otherwise we are leaking data (cheating by giving future information to the model).","metadata":{}},{"cell_type":"code","source":"# we can look at the data once more to see missing values\ntrain.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:55:36.862627Z","iopub.execute_input":"2022-04-04T11:55:36.863512Z","iopub.status.idle":"2022-04-04T11:55:36.874215Z","shell.execute_reply.started":"2022-04-04T11:55:36.863462Z","shell.execute_reply":"2022-04-04T11:55:36.873125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.experimental import (\n    enable_iterative_imputer,\n)\nfrom sklearn import impute\nnum_cols = [\n    \"Age\",\n    \"RoomService\",\n    \"FoodCourt\",\n    \"ShoppingMall\",\n    \"Spa\",\n    \"VRDeck\",\n]","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:55:36.877359Z","iopub.execute_input":"2022-04-04T11:55:36.877873Z","iopub.status.idle":"2022-04-04T11:55:36.891078Z","shell.execute_reply.started":"2022-04-04T11:55:36.877841Z","shell.execute_reply":"2022-04-04T11:55:36.890114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use Sklearn impute to fill in the missing data.","metadata":{}},{"cell_type":"code","source":"imputer = impute.IterativeImputer()\nimputed = imputer.fit_transform(\n    X_train[num_cols]\n)\nX_train.loc[:, num_cols] = imputed\nimputed = imputer.transform(X_test[num_cols])\nX_test.loc[:, num_cols] = imputed","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:55:36.892559Z","iopub.execute_input":"2022-04-04T11:55:36.892986Z","iopub.status.idle":"2022-04-04T11:55:37.127794Z","shell.execute_reply.started":"2022-04-04T11:55:36.892954Z","shell.execute_reply":"2022-04-04T11:55:37.126687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normalize the data\n\nNormalizing or preprocessing the data will help many models perform better after this is done. Particularly those that depend on a distance metric to determine similarity.","metadata":{}},{"cell_type":"code","source":"cols = [\"Age\",\n        \"RoomService\",\n        \"FoodCourt\",\n        \"ShoppingMall\",\n        \"Spa\",\n        \"VRDeck\",\n        \"HomePlanet_Earth\",\n        \"HomePlanet_Europa\",\n       \"HomePlanet_Mars\",\n        \"CryoSleep_False\",\n        \"Destination_55 Cancri e\",\n        \"Destination_PSO J318.5-22\",\n        \"Destination_TRAPPIST-1e\",\n        \"VIP_False\"\n]","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:55:37.133747Z","iopub.execute_input":"2022-04-04T11:55:37.137659Z","iopub.status.idle":"2022-04-04T11:55:37.159341Z","shell.execute_reply.started":"2022-04-04T11:55:37.137568Z","shell.execute_reply":"2022-04-04T11:55:37.149895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\n\nWe are going to standardize the data for the preprocessing. Standardizing is translating the data so that it has a mean value of zero and a standard deviation of one. This way models don’t treat variables with larger scales as more important than smaller scaled variables. I’m going to stick the result (numpy array) back into a pandas DataFrame for easier manipulation (and to keep column names).","metadata":{}},{"cell_type":"code","source":"sca = preprocessing.StandardScaler()\nX_train = sca.fit_transform(X_train)\nX_train = pd.DataFrame(X_train, columns=cols)\nX_test = sca.transform(X_test)\nX_test = pd.DataFrame(X_test, columns=cols)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:55:37.165498Z","iopub.execute_input":"2022-04-04T11:55:37.168175Z","iopub.status.idle":"2022-04-04T11:55:37.203501Z","shell.execute_reply.started":"2022-04-04T11:55:37.168104Z","shell.execute_reply":"2022-04-04T11:55:37.202598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline Model\n\nCreating a baseline model that does something really simple can give us something to compare our model to. Note that using the default .score result gives us the accuracy which can be misleading. A problem where a positive case is 1 in 10,000 can easily get over 99% accuracy by always predicting negative.","metadata":{}},{"cell_type":"code","source":"from sklearn.dummy import DummyClassifier\nbm = DummyClassifier()\nbm.fit(X_train, y_train)\nbm.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:55:37.204799Z","iopub.execute_input":"2022-04-04T11:55:37.205358Z","iopub.status.idle":"2022-04-04T11:55:37.217165Z","shell.execute_reply.started":"2022-04-04T11:55:37.205309Z","shell.execute_reply":"2022-04-04T11:55:37.216332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nmetrics.precision_score(\ny_test, bm.predict(X_test))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:55:37.21835Z","iopub.execute_input":"2022-04-04T11:55:37.218936Z","iopub.status.idle":"2022-04-04T11:55:37.229024Z","shell.execute_reply.started":"2022-04-04T11:55:37.2189Z","shell.execute_reply":"2022-04-04T11:55:37.228057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Tests\n\nThis code tries a variety of algorithm families. The “No Free Lunch” theorem states that no algorithm performs well on all data. However, for some finite set of data, there may be an algorithm that does well on that set. (A popular choice for structured learning these days is a tree-boosted algorithm such as XGBoost.)","metadata":{}},{"cell_type":"code","source":"# Because we are using k-fold cross-validation, \n# we will feed the model all of X and y:\nX = pd.concat([X_train, X_test])\ny = pd.concat([y_train, y_test])","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:55:37.23048Z","iopub.execute_input":"2022-04-04T11:55:37.231257Z","iopub.status.idle":"2022-04-04T11:55:37.239726Z","shell.execute_reply.started":"2022-04-04T11:55:37.2311Z","shell.execute_reply":"2022-04-04T11:55:37.238737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import (\n    LogisticRegression,\n)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import (\n    KNeighborsClassifier,\n)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import (\n    RandomForestClassifier,\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:55:37.241294Z","iopub.execute_input":"2022-04-04T11:55:37.241791Z","iopub.status.idle":"2022-04-04T11:55:37.359507Z","shell.execute_reply.started":"2022-04-04T11:55:37.241746Z","shell.execute_reply":"2022-04-04T11:55:37.358677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build our models","metadata":{}},{"cell_type":"code","source":"for model in [\n    DummyClassifier,\n    LogisticRegression,\n    DecisionTreeClassifier,\n    KNeighborsClassifier,\n    GaussianNB,\n    SVC,\n    RandomForestClassifier\n]:\n    cls = model()\n    kfold = model_selection.KFold(\n        n_splits=10\n    )\n    s = model_selection.cross_val_score(\n        cls, X, y, scoring=\"accuracy\", cv=kfold\n    )\n    print(\n        f\"{model.__name__:22} Accuracy: \"\n        f\"{s.mean():.3f} STD: {s.std():.2f}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2022-04-04T12:29:24.252729Z","iopub.execute_input":"2022-04-04T12:29:24.253143Z","iopub.status.idle":"2022-04-04T12:30:12.390432Z","shell.execute_reply.started":"2022-04-04T12:29:24.253111Z","shell.execute_reply":"2022-04-04T12:30:12.389587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stacking \n\nWe will now take the models above and stack them to build a more accurate model.\n\nTo learn more about this method see the documentation here:\nhttp://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/","metadata":{}},{"cell_type":"code","source":"from mlxtend.classifier import (\n    StackingClassifier,\n)\n\nclfs = [\n    x()\n    for x in [\n        LogisticRegression,\n        DecisionTreeClassifier,\n        KNeighborsClassifier,\n        GaussianNB,\n        SVC,\n        RandomForestClassifier\n    ]\n    \n    \n]\n\nstack = StackingClassifier(\n    classifiers=clfs,\n    meta_classifier=LogisticRegression(),\n)\nkfold = model_selection.KFold(\n    n_splits=10\n)\n\ns = model_selection.cross_val_score(\n    stack, X, y, scoring=\"accuracy\", cv=kfold\n)\n\nprint(\n    f\"{stack.__class__.__name__} \"\n    f\"Accuracy: {s.mean():.3f}  STD: {s.std():.2f}\"\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T12:30:30.868646Z","iopub.execute_input":"2022-04-04T12:30:30.868988Z","iopub.status.idle":"2022-04-04T12:31:47.644374Z","shell.execute_reply.started":"2022-04-04T12:30:30.868958Z","shell.execute_reply":"2022-04-04T12:31:47.642616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GridSearch\n\nLets try GridSearch to better our hyperparameters. ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Initializing models\n\nclf1 = KNeighborsClassifier(n_neighbors=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\nlr = LogisticRegression()\nsclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n                          meta_classifier=lr)\n\nparams = {'kneighborsclassifier__n_neighbors': [1, 5],\n          'randomforestclassifier__n_estimators': [10, 50],\n          'meta_classifier__C': [0.1, 10.0]}\n\ngrid = GridSearchCV(estimator=sclf, \n                    param_grid=params, \n                    cv=5,\n                    refit=True)\ngrid.fit(X, y)\n\ncv_keys = ('mean_test_score', 'std_test_score', 'params')\n\nfor r, _ in enumerate(grid.cv_results_['mean_test_score']):\n    print(\"%0.3f +/- %0.2f %r\"\n          % (grid.cv_results_[cv_keys[0]][r],\n             grid.cv_results_[cv_keys[1]][r] / 2.0,\n             grid.cv_results_[cv_keys[2]][r]))\n\nprint('Best parameters: %s' % grid.best_params_)\nprint('Accuracy: %.2f' % grid.best_score_)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T12:31:47.646387Z","iopub.execute_input":"2022-04-04T12:31:47.646675Z","iopub.status.idle":"2022-04-04T12:32:35.439795Z","shell.execute_reply.started":"2022-04-04T12:31:47.646643Z","shell.execute_reply":"2022-04-04T12:32:35.438699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If this helped you please don't forget to upvote :)","metadata":{}}]}