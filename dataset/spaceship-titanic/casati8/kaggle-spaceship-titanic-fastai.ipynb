{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is a quick demonstration, who to use the Fastai v2 library for a Kaggle tabular competition. Fastai v2 is based on pytorch and allows you, to build a decent machine learning application. For more information please visit the Fastai documentation: https://docs.fast.ai/. I will link to \"Chapter 9, Tabular Modelling Deep Dive\" and the notebook \"09_tabular.ipynb\".\n\nThis competition is a binary classification problem: find the correct state, wheter a passenger is transported. The offered dataset has 14 differend features and for many rows, some values are missing.\nIn this notebook i will use a neural network approach and i will train this network with the traing data set.\n\nLet's start and import the needed stuff ..","metadata":{}},{"cell_type":"code","source":"from fastai.tabular.all import * \nfrom fastai.test_utils import show_install\nfrom IPython.display import display, clear_output\nimport seaborn as sns\nshow_install()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-22T17:42:21.045102Z","iopub.execute_input":"2022-06-22T17:42:21.045847Z","iopub.status.idle":"2022-06-22T17:42:22.31529Z","shell.execute_reply.started":"2022-06-22T17:42:21.045697Z","shell.execute_reply":"2022-06-22T17:42:22.313679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:22.318129Z","iopub.execute_input":"2022-06-22T17:42:22.319461Z","iopub.status.idle":"2022-06-22T17:42:22.332816Z","shell.execute_reply.started":"2022-06-22T17:42:22.319421Z","shell.execute_reply":"2022-06-22T17:42:22.331468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed_value(seed=718):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed_value()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:22.334681Z","iopub.execute_input":"2022-06-22T17:42:22.335654Z","iopub.status.idle":"2022-06-22T17:42:22.346001Z","shell.execute_reply.started":"2022-06-22T17:42:22.335599Z","shell.execute_reply":"2022-06-22T17:42:22.344777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = Path('../input/spaceship-titanic/')\nPath.BASE_PATH = path\npath.ls()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:22.349588Z","iopub.execute_input":"2022-06-22T17:42:22.35055Z","iopub.status.idle":"2022-06-22T17:42:22.363555Z","shell.execute_reply.started":"2022-06-22T17:42:22.350496Z","shell.execute_reply":"2022-06-22T17:42:22.362238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the datasets and define the depending variable: Transported","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(path, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(path, 'test.csv'))\nsample_submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n\ndep_var = 'Transported'","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:22.365874Z","iopub.execute_input":"2022-06-22T17:42:22.366699Z","iopub.status.idle":"2022-06-22T17:42:22.430155Z","shell.execute_reply.started":"2022-06-22T17:42:22.366649Z","shell.execute_reply":"2022-06-22T17:42:22.429162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see the first rows of our training data set to get an overview:","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:22.431801Z","iopub.execute_input":"2022-06-22T17:42:22.4324Z","iopub.status.idle":"2022-06-22T17:42:22.471419Z","shell.execute_reply.started":"2022-06-22T17:42:22.432348Z","shell.execute_reply":"2022-06-22T17:42:22.470114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see the columns and their types. The function info() shows the number of rows with values for each column. If these numbers differ from row to row and the total amount of rows, we have a dataset with missing values, mostly NaN named.","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:22.47266Z","iopub.execute_input":"2022-06-22T17:42:22.473013Z","iopub.status.idle":"2022-06-22T17:42:22.505189Z","shell.execute_reply.started":"2022-06-22T17:42:22.472951Z","shell.execute_reply":"2022-06-22T17:42:22.504173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:22.507113Z","iopub.execute_input":"2022-06-22T17:42:22.507437Z","iopub.status.idle":"2022-06-22T17:42:22.555795Z","shell.execute_reply.started":"2022-06-22T17:42:22.507406Z","shell.execute_reply":"2022-06-22T17:42:22.554737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's print the number of NaN rows for each column.","metadata":{}},{"cell_type":"code","source":"print(train_df.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:22.557306Z","iopub.execute_input":"2022-06-22T17:42:22.557624Z","iopub.status.idle":"2022-06-22T17:42:22.572077Z","shell.execute_reply.started":"2022-06-22T17:42:22.557593Z","shell.execute_reply":"2022-06-22T17:42:22.570907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The name values are mostly unique and should'nt have any influence on the 'Transpored' value, therefore i will remove them.","metadata":{}},{"cell_type":"code","source":"train_df.drop(['Name'], axis=1, inplace=True)\ntest_df.drop(['Name'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:22.576856Z","iopub.execute_input":"2022-06-22T17:42:22.578066Z","iopub.status.idle":"2022-06-22T17:42:22.589244Z","shell.execute_reply.started":"2022-06-22T17:42:22.578014Z","shell.execute_reply":"2022-06-22T17:42:22.587923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As mentioned the description of the data, the values inn the columns 'Cabin' and 'PassengerId' are combined string values, The values for 'Cabin' constist of the value for the deck, a number and s side value. The values for 'PassengerId' are are the string concatenation of a group value and unique number inside this group. \nI will define a function to replace the values in the columns 'Cabin' and 'PassengerId' with these sub values. The original columns can be droped.\nThe name values are mostly unique and should'nt have any influence on the 'Transpored' value, therefore i will remove them.","metadata":{}},{"cell_type":"code","source":"def split_columns_with_combinded_data(df, drop_orgin:bool=False):\n    df[['Deck','Num', 'Side']] = df['Cabin'].str.split('/', expand=True)\n    df[['PGroup','PNr']] = df['PassengerId'].str.split('_', expand=True)\n    if drop_orgin:\n        df.drop(['Cabin', 'PassengerId'], axis=1, inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:22.591216Z","iopub.execute_input":"2022-06-22T17:42:22.5917Z","iopub.status.idle":"2022-06-22T17:42:22.600553Z","shell.execute_reply.started":"2022-06-22T17:42:22.591651Z","shell.execute_reply":"2022-06-22T17:42:22.599222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = split_columns_with_combinded_data(train_df, drop_orgin=True)\ntest_df = split_columns_with_combinded_data(test_df, drop_orgin=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:22.602608Z","iopub.execute_input":"2022-06-22T17:42:22.603213Z","iopub.status.idle":"2022-06-22T17:42:22.843399Z","shell.execute_reply.started":"2022-06-22T17:42:22.603162Z","shell.execute_reply":"2022-06-22T17:42:22.842368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that the columns 'Destination' and 'HomePlanet' are enumeration types. I will print the number of thier unique values to check my assumption.","metadata":{}},{"cell_type":"code","source":"train_df['Destination'].nunique(), train_df['HomePlanet'].nunique(),","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:22.844833Z","iopub.execute_input":"2022-06-22T17:42:22.845322Z","iopub.status.idle":"2022-06-22T17:42:22.85529Z","shell.execute_reply.started":"2022-06-22T17:42:22.845276Z","shell.execute_reply":"2022-06-22T17:42:22.854053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay there a a handful unique values in both columns. That's lighten handling of the missing values in these columns. We can convert them into 'one-hot-encoded' values. If a row contains a Nan value in the orginal column, none of the derived rows contain the value 1, all rows have the value 0.","metadata":{}},{"cell_type":"code","source":"def convert_to_dummies(df):\n    df = pd.get_dummies(df, columns=['Destination'], prefix=\"D\")\n    df = pd.get_dummies(df, columns=['HomePlanet'], prefix=\"H\")\n    df = pd.get_dummies(df, columns=['Side'])\n    df = pd.get_dummies(df, columns=['VIP'])\n    df = pd.get_dummies(df, columns=['CryoSleep'])\n    df = pd.get_dummies(df, columns=['Deck'])\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:22.856896Z","iopub.execute_input":"2022-06-22T17:42:22.857492Z","iopub.status.idle":"2022-06-22T17:42:22.867869Z","shell.execute_reply.started":"2022-06-22T17:42:22.857443Z","shell.execute_reply":"2022-06-22T17:42:22.867049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = convert_to_dummies(train_df)\ntest_df = convert_to_dummies(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:22.868908Z","iopub.execute_input":"2022-06-22T17:42:22.869549Z","iopub.status.idle":"2022-06-22T17:42:22.943619Z","shell.execute_reply.started":"2022-06-22T17:42:22.869498Z","shell.execute_reply":"2022-06-22T17:42:22.942599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The range of the values for the different service values is quite high, they are in the ramng from 0 to 24000. To provide issues in the later training process and because these values are belong to the independent columns, i scale them down and i use the ln(x+1) function from numpy. ","metadata":{}},{"cell_type":"code","source":"def scale_service_values(df):\n    for s in ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']:\n        # df[s] = np.log1p(df[s])\n        df[s] = df[s]/1024.0\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:22.944855Z","iopub.execute_input":"2022-06-22T17:42:22.945273Z","iopub.status.idle":"2022-06-22T17:42:22.95126Z","shell.execute_reply.started":"2022-06-22T17:42:22.94524Z","shell.execute_reply":"2022-06-22T17:42:22.950053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = scale_service_values(train_df)\ntest_df = scale_service_values(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:22.952876Z","iopub.execute_input":"2022-06-22T17:42:22.953696Z","iopub.status.idle":"2022-06-22T17:42:22.967539Z","shell.execute_reply.started":"2022-06-22T17:42:22.953647Z","shell.execute_reply":"2022-06-22T17:42:22.966375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will change the type for some columns, that needed for the neural network later ..","metadata":{}},{"cell_type":"code","source":"def change_column_type(df):\n    df['Num'] = df['Num'].astype('float')\n    df['PNr'] = df['PNr'].astype('int')\n    df['PGroup'] = df['PGroup'].astype('float')\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:22.969575Z","iopub.execute_input":"2022-06-22T17:42:22.970262Z","iopub.status.idle":"2022-06-22T17:42:22.977375Z","shell.execute_reply.started":"2022-06-22T17:42:22.970212Z","shell.execute_reply":"2022-06-22T17:42:22.976181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = change_column_type(train_df)\ntestn_df = change_column_type(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:22.979194Z","iopub.execute_input":"2022-06-22T17:42:22.980113Z","iopub.status.idle":"2022-06-22T17:42:23.007247Z","shell.execute_reply.started":"2022-06-22T17:42:22.98006Z","shell.execute_reply":"2022-06-22T17:42:23.00604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's recheck the number of NaN rows for each column after the preprocessing:","metadata":{}},{"cell_type":"code","source":"print(train_df.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:23.008995Z","iopub.execute_input":"2022-06-22T17:42:23.009661Z","iopub.status.idle":"2022-06-22T17:42:23.020684Z","shell.execute_reply.started":"2022-06-22T17:42:23.009612Z","shell.execute_reply":"2022-06-22T17:42:23.019524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First i will look at the correlation matrix to verify how important a feature is.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\n\ncorr=train_df.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, robust=True, center=0,square=True, linewidths=.6,cmap='rainbow')\nplt.title('Correlation')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:23.022379Z","iopub.execute_input":"2022-06-22T17:42:23.023072Z","iopub.status.idle":"2022-06-22T17:42:23.964957Z","shell.execute_reply.started":"2022-06-22T17:42:23.023022Z","shell.execute_reply":"2022-06-22T17:42:23.96387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I need a list of the column names, which are candidates for category variables and which are no candidates, also called continous variables. The Fastai library offers the function 'cont_cat_split' to do this for us. Our training data set contains only floating values for the independed variables, therefore we expect that no category variables are available.","metadata":{}},{"cell_type":"code","source":"cont_vars, cat_vars = cont_cat_split(train_df, dep_var=dep_var)\ncont_vars, cat_vars","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:23.96651Z","iopub.execute_input":"2022-06-22T17:42:23.967401Z","iopub.status.idle":"2022-06-22T17:42:23.986395Z","shell.execute_reply.started":"2022-06-22T17:42:23.967351Z","shell.execute_reply":"2022-06-22T17:42:23.985256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next step is to create a data loader. The Fastai library offers a powerful helper called 'TabularPandas'. It needs the data frame, list of the category and continous variables, the depened variable and a splitter. The splitter divides the data set into two parts: one for the training and one for the validation and for internal optimization step in each epoch. The batch size is set to 1024, because we have a large data set. We can use a random split because the rows in the data set are independed.","metadata":{}},{"cell_type":"code","source":"def getData(df, batchSize=128):\n    \n    to_train = TabularPandas(df, \n                           [Normalize, Categorify, FillMissing],\n                           cat_vars,\n                           cont_vars, \n                           splits=RandomSplitter(valid_pct=0.2)(df),  \n                           device = device,\n                           y_block=CategoryBlock(),\n                           y_names=dep_var) \n\n    return to_train.dataloaders(bs=batchSize)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:23.988162Z","iopub.execute_input":"2022-06-22T17:42:23.988829Z","iopub.status.idle":"2022-06-22T17:42:23.996697Z","shell.execute_reply.started":"2022-06-22T17:42:23.98878Z","shell.execute_reply":"2022-06-22T17:42:23.995619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls = getData(train_df)\nlen(dls.train), len(dls.valid)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:23.998347Z","iopub.execute_input":"2022-06-22T17:42:23.999029Z","iopub.status.idle":"2022-06-22T17:42:24.245051Z","shell.execute_reply.started":"2022-06-22T17:42:23.998946Z","shell.execute_reply":"2022-06-22T17:42:24.243587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show me the transformed data, which will be used in the network later.","metadata":{}},{"cell_type":"code","source":"dls.show_batch()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:24.246681Z","iopub.execute_input":"2022-06-22T17:42:24.247219Z","iopub.status.idle":"2022-06-22T17:42:24.331107Z","shell.execute_reply.started":"2022-06-22T17:42:24.247169Z","shell.execute_reply":"2022-06-22T17:42:24.329571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At least i create a learner pasing the dataloader into it. The default settings are two hidden layers with 200 and 100 elements. Increasing the number of parameters in the neural network will improve the accuarcy and score, hopefully: Change number and the depth of the hidden layers, use a a batch normalization and/or a dropout layer, etc.","metadata":{}},{"cell_type":"code","source":"my_config = tabular_config(y_range=(0,1), use_bn=True, ps=0.1, embed_p=0.1)\n\nlearn = tabular_learner(dls,\n                        config = my_config,\n                        layers=[200,100],\n                        metrics=[accuracy])\n\nlearn.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:24.333155Z","iopub.execute_input":"2022-06-22T17:42:24.333654Z","iopub.status.idle":"2022-06-22T17:42:24.388453Z","shell.execute_reply.started":"2022-06-22T17:42:24.333605Z","shell.execute_reply":"2022-06-22T17:42:24.387249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We needd a proper leraning rate. The The Fastai library offers the funtcion lr_find() for this job.","metadata":{}},{"cell_type":"code","source":"lr_min,lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:24.390163Z","iopub.execute_input":"2022-06-22T17:42:24.391057Z","iopub.status.idle":"2022-06-22T17:42:27.630704Z","shell.execute_reply.started":"2022-06-22T17:42:24.390991Z","shell.execute_reply":"2022-06-22T17:42:27.62944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:27.636117Z","iopub.execute_input":"2022-06-22T17:42:27.636818Z","iopub.status.idle":"2022-06-22T17:42:27.643455Z","shell.execute_reply.started":"2022-06-22T17:42:27.636742Z","shell.execute_reply":"2022-06-22T17:42:27.642286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will use a maximum learning rate of 5e-3. Starting the learning process is quite easy, i will run for 30 epochs. I will save the model with the best, with the lowest validation lost value. The Fastai library offers the SaveModelCallback callback. You must specify the file name only. The option with_opt=True stores the values of the optimizer also. You will find the new file in the subdirectory 'models'.","metadata":{}},{"cell_type":"code","source":"learn.fit_one_cycle(30, 5e-3, wd=0.01, cbs=SaveModelCallback(fname='kaggle_spaceship_titanic', with_opt=True))","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:42:27.645094Z","iopub.execute_input":"2022-06-22T17:42:27.646362Z","iopub.status.idle":"2022-06-22T17:43:21.852943Z","shell.execute_reply.started":"2022-06-22T17:42:27.646311Z","shell.execute_reply":"2022-06-22T17:43:21.85181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The confusion matrix below shows us the quality of data prediction during the learning phase.","metadata":{}},{"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(normalize=True, norm_dec=3)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:43:21.854574Z","iopub.execute_input":"2022-06-22T17:43:21.854914Z","iopub.status.idle":"2022-06-22T17:43:22.31451Z","shell.execute_reply.started":"2022-06-22T17:43:21.854881Z","shell.execute_reply":"2022-06-22T17:43:22.313225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it's time to calculate the predictions for the test data set. Therefor i load the 'best' model with the lowest validation loss value","metadata":{}},{"cell_type":"code","source":"learn.load('kaggle_spaceship_titanic')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:43:22.316682Z","iopub.execute_input":"2022-06-22T17:43:22.317542Z","iopub.status.idle":"2022-06-22T17:43:22.358701Z","shell.execute_reply.started":"2022-06-22T17:43:22.317469Z","shell.execute_reply":"2022-06-22T17:43:22.357554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.show_results()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:43:22.360669Z","iopub.execute_input":"2022-06-22T17:43:22.361472Z","iopub.status.idle":"2022-06-22T17:43:22.447459Z","shell.execute_reply.started":"2022-06-22T17:43:22.361421Z","shell.execute_reply":"2022-06-22T17:43:22.446188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I got the 'one hot encoded' prediction values, which are probabilities for the different target values. np.argmax returns the index with the maximum probability value, like 0 or 1.","metadata":{}},{"cell_type":"code","source":"dlt = learn.dls.test_dl(test_df) \nnn_preds,_ ,preds = learn.get_preds(dl=dlt , with_decoded=True) \n\nnn_preds","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:43:22.449311Z","iopub.execute_input":"2022-06-22T17:43:22.450049Z","iopub.status.idle":"2022-06-22T17:43:22.795589Z","shell.execute_reply.started":"2022-06-22T17:43:22.449995Z","shell.execute_reply":"2022-06-22T17:43:22.794827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission[dep_var] = np.argmax(nn_preds, axis=1) == 1\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:43:22.79689Z","iopub.execute_input":"2022-06-22T17:43:22.79748Z","iopub.status.idle":"2022-06-22T17:43:22.81933Z","shell.execute_reply.started":"2022-06-22T17:43:22.797445Z","shell.execute_reply":"2022-06-22T17:43:22.818069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -la","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:43:22.821303Z","iopub.execute_input":"2022-06-22T17:43:22.821821Z","iopub.status.idle":"2022-06-22T17:43:23.597381Z","shell.execute_reply.started":"2022-06-22T17:43:22.82177Z","shell.execute_reply":"2022-06-22T17:43:23.596329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see i achieve an accuracy value roughly 0.802 - 0.805 with the default Fastai settings and with a minimal features engineering. That is a great result and is the baseline to investigate in more feature engineering and/or modeling to get a better final result. At this point you can start your own experience. Fell free and use my notebokk if you like, or tell me your concerns. Feedback is wellcome!","metadata":{}}]}