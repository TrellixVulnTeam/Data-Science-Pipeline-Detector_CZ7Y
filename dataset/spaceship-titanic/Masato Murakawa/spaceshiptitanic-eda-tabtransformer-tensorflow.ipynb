{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n# [Spaceship Titanic][1]\n\n- We are challenged to predict which passengers were transported by the anomaly using records recovered from the spaceshipâ€™s damaged computer system.\n\n---\n#### **The aim of this notebook is to**\n- **1. Conduct exploratory data analysis (EDA).**\n- **2. Converting numerical features into categorical features by binning.**\n- **3. Conduct feature engineering on 'Cabin' feature.**\n- **4. Build and train a TabTransformer model.**\n\n---\n**References:** Thanks to previous great codes and notebooks.\n- [ðŸ”¥ðŸ”¥[TensorFlow]TabTransformerðŸ”¥ðŸ”¥][2]\n- [Structured data learning with TabTransformer][3]\n- [Sachin's Blog Tensorflow Learning Rate Finder][4]\n\n---\n### **If you find this notebook useful, please do give me an upvote. It helps me keep up my motivation.**\n#### **Also, I would appreciate it if you find any mistakes and help me correct them.**\n\n---\n[1]: https://www.kaggle.com/competitions/spaceship-titanic/overview\n[2]: https://www.kaggle.com/code/usharengaraju/tensorflow-tabtransformer\n[3]: https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/structured_data/ipynb/tabtransformer.ipynb\n[4]: https://sachinruk.github.io/blog/tensorflow/learning%20rate/2021/02/15/Tensorflow-Learning-Rate-Finder.html","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>0. TABLE OF CONTENTS</center></h1>\n\n<ul class=\"list-group\" style=\"list-style-type:none;\">\n    <li><a href=\"#1\" class=\"list-group-item list-group-item-action\">1. Settings</a></li>\n    <li><a href=\"#2\" class=\"list-group-item list-group-item-action\">2. Data Loading</a></li>\n    <li><a href=\"#3\" class=\"list-group-item list-group-item-action\">3. Exploratory Data Analysis</a>\n        <ul class=\"list-group\" style=\"list-style-type:none;\">\n            <li><a href=\"#3.1\" class=\"list-group-item list-group-item-action\">3.1 Feature Engineering</a></li>\n            <li><a href=\"#3.2\" class=\"list-group-item list-group-item-action\">3.2 Target Distribution</a></li>\n            <li><a href=\"#3.3\" class=\"list-group-item list-group-item-action\">3.3 Numerical Features</a>\n                <ul class=\"list-group\" style=\"list-style-type:none;\">\n                    <li><a href=\"#3.3.1\" class=\"list-group-item list-group-item-action\">3.3.1 Statistics of Numerical Features</a></li>\n                    <li><a href=\"#3.3.2\" class=\"list-group-item list-group-item-action\">3.3.2 Binning for Numerical Features</a></li>\n                </ul>\n            </li>\n            <li><a href=\"#3.4\" class=\"list-group-item list-group-item-action\">3.4 Categorical Feature</a></li>\n            <li><a href=\"#3.5\" class=\"list-group-item list-group-item-action\">3.5  Data Processing Complete</a></li>\n            <li><a href=\"#3.6\" class=\"list-group-item list-group-item-action\">4.6 Validation Split</a></li>\n        </ul>\n    </li>\n    <li><a href=\"#4\" class=\"list-group-item list-group-item-action\">4. Model Building</a>\n        <ul class=\"list-group\" style=\"list-style-type:none;\">\n            <li><a href=\"#4.1\" class=\"list-group-item list-group-item-action\">4.1 Dataset</a></li>\n            <li><a href=\"#4.2\" class=\"list-group-item list-group-item-action\">4.2 Preprocessing Model</a></li>\n            <li><a href=\"#4.3\" class=\"list-group-item list-group-item-action\">4.3 Training Model</a></li>\n        </ul>\n    </li>\n    <li><a href=\"#5\" class=\"list-group-item list-group-item-action\">5. Model Training</a>\n        <ul class=\"list-group\" style=\"list-style-type:none;\">\n            <li><a href=\"#5.1\" class=\"list-group-item list-group-item-action\">5.1 Learning Rate Finder</a></li>\n            <li><a href=\"#5.2\" class=\"list-group-item list-group-item-action\">5.2 Model Training</a></li>\n        </ul>\n    </li>\n    <li><a href=\"#6\" class=\"list-group-item list-group-item-action\">6. Inference</a>\n        <ul class=\"list-group\" style=\"list-style-type:none;\">\n            <li><a href=\"#6.1\" class=\"list-group-item list-group-item-action\">6.1 Finalize Model</a></li>\n            <li><a href=\"#6.2\" class=\"list-group-item list-group-item-action\">6.2 Test Inference</a></li>\n        </ul>\n    </li>\n    <li><a href=\"#7\" class=\"list-group-item list-group-item-action\">7. Cross Validation and Ensebmling</a></li>\n</ul>\n","metadata":{}},{"cell_type":"markdown","source":"<a id =\"1\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>1. Settings</center></h1>","metadata":{}},{"cell_type":"code","source":"## Import dependencies \nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport matplotlib.pyplot as plt \n%matplotlib inline\n\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport os\nimport pathlib\nimport gc\nimport sys\nimport re\nimport math \nimport random\nimport time \nimport datetime as dt\nfrom tqdm import tqdm \n\nimport sklearn\nfrom sklearn.model_selection import StratifiedKFold\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint('import done!')","metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-05-24T04:41:12.726887Z","iopub.execute_input":"2022-05-24T04:41:12.727579Z","iopub.status.idle":"2022-05-24T04:41:12.740611Z","shell.execute_reply.started":"2022-05-24T04:41:12.727532Z","shell.execute_reply":"2022-05-24T04:41:12.739791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## For reproducible results    \ndef seed_all(s):\n    random.seed(s)\n    np.random.seed(s)\n    tf.random.set_seed(s)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    os.environ['PYTHONHASHSEED'] = str(s) \n    print('Seeds setted!')\n    \nglobal_seed = 42\nseed_all(global_seed)\n\n\n## Limit GPU Memory in TensorFlow\n## Because TensorFlow, by default, allocates the full amount of available GPU memory when it is launched. \nphysical_devices = tf.config.list_physical_devices('GPU')\nif len(physical_devices) > 0:\n    for device in physical_devices:\n        tf.config.experimental.set_memory_growth(device, True)\n        print('{} memory growth: {}'.format(device, tf.config.experimental.get_memory_growth(device)))\nelse:\n    print(\"Not enough GPU hardware devices available\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:12.747522Z","iopub.execute_input":"2022-05-24T04:41:12.748169Z","iopub.status.idle":"2022-05-24T04:41:12.87013Z","shell.execute_reply.started":"2022-05-24T04:41:12.748135Z","shell.execute_reply":"2022-05-24T04:41:12.869233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Parameters\ndata_config = {'train_csv_path': '../input/spaceship-titanic/train.csv',\n               'test_csv_path': '../input/spaceship-titanic/test.csv',\n               'sample_submission_path': '../input/spaceship-titanic/sample_submission.csv',\n              }\n\nexp_config = {'n_bins': 10,\n              'n_splits': 5,\n              'batch_size': 512,\n              'num_columns': 13,\n              'learning_rate': 2e-4,\n              'weight_decay': 0.0001,\n              'train_epochs': 50,\n              'checkpoint_filepath': './tmp/model/exp.ckpt',\n              'finalize': True,\n              'cross_validation': True,\n             }\n\nmodel_config = {'cat_embedding_dim': 12,\n                'num_transformer_blocks': 4,\n                'num_heads': 3,\n                'tf_dropout_rates': [0., 0., 0., 0.,],\n                'ff_dropout_rates': [0., 0., 0., 0.,],\n                'mlp_dropout_rates': [0.2, 0.1],\n                'mlp_hidden_units_factors': [2, 1],\n                'label_smoothing': 0.0,\n               }\n\nprint('Parameters setted!')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:12.878152Z","iopub.execute_input":"2022-05-24T04:41:12.878758Z","iopub.status.idle":"2022-05-24T04:41:12.888589Z","shell.execute_reply.started":"2022-05-24T04:41:12.878721Z","shell.execute_reply":"2022-05-24T04:41:12.887601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"2\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>2. Data Loading</center></h1>","metadata":{}},{"cell_type":"markdown","source":"---\n### [File and Data Field Descriptions](https://www.kaggle.com/competitions/spaceship-titanic/data)\n\n- **train.csv** - Personal records for about two-thirds (~8700) of the passengers, to be used as training data.\n - `PassengerId` - A unique Id for each passenger. Each Id takes the form `gggg_pp` where `gggg` indicates a group the passenger is travelling with and `pp` is their number within the group. People in a group are often family members, but not always.\n - `HomePlanet` - The planet the passenger departed from, typically their planet of permanent residence.\n - `CryoSleep` - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n - `Cabin` - The cabin number where the passenger is staying. Takes the form `deck/num/side`, where `side` can be either `P` for *Port* or `S` for *Starboard*.\n - `Destination` - The planet the passenger will be debarking to.\n - `Age` - The age of the passenger.\n - `VIP` - Whether the passenger has paid for special VIP service during the voyage.\n - `RoomService`, `FoodCourt`, `ShoppingMall`, `Spa`, `VRDeck` - Amount the passenger has billed at each of the *Spaceship Titanic*'s many luxury amenities.\n - `Name` - The first and last names of the passenger.\n - `Transported` - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.\n\n\n- **test.csv** - Personal records for the remaining one-third (~4300) of the passengers, to be used as test data. Your task is to predict the value of `Transported` for the passengers in this set.\n\n\n- **sample_submission.csv** - A submission file in the correct format.\n - `PassengerId` - Id for each passenger in the test set.\n - `Transported` - The target. For each passenger, predict either *True* or *False*.\n\n---\n### [Submission & Evaluation](https://www.kaggle.com/competitions/spaceship-titanic/overview/evaluation)\n\n- Submissions are evaluated based on their classification accuracy, the percentage of predicted labels that are correct.\n\n---","metadata":{}},{"cell_type":"code","source":"## Data Loading\ntrain_df = pd.read_csv(data_config['train_csv_path'])\ntest_df = pd.read_csv(data_config['test_csv_path'])\nsubmission_df = pd.read_csv(data_config['sample_submission_path'])\n\nprint(f'train_length: {len(train_df)}')\nprint(f'test_lenght: {len(test_df)}')\nprint(f'submission_length: {len(submission_df)}')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:12.890373Z","iopub.execute_input":"2022-05-24T04:41:12.89095Z","iopub.status.idle":"2022-05-24T04:41:12.951257Z","shell.execute_reply.started":"2022-05-24T04:41:12.890872Z","shell.execute_reply":"2022-05-24T04:41:12.950477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Null Value Check\nprint('train_df.info()'); print(train_df.info(), '\\n')\nprint('test_df.info()'); print(test_df.info(), '\\n')\n\n## train_df Check\ntrain_df.head()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:12.954918Z","iopub.execute_input":"2022-05-24T04:41:12.95527Z","iopub.status.idle":"2022-05-24T04:41:13.008787Z","shell.execute_reply.started":"2022-05-24T04:41:12.955227Z","shell.execute_reply":"2022-05-24T04:41:13.007943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>3. Exploratory Data Analysis</center></h1>","metadata":{}},{"cell_type":"markdown","source":"<a id =\"3.1\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>3.1 Feature Engineering</center></h2>","metadata":{}},{"cell_type":"code","source":"## Feature Selection\nnumerical_columns = ['Age', 'RoomService', 'FoodCourt',\n                     'ShoppingMall', 'Spa', 'VRDeck']\ncategorical_columns = ['PassengerId', 'HomePlanet', 'CryoSleep',\n                       'Cabin', 'Destination', 'VIP', 'Name']\ntarget = 'Transported'\n\n## Number of unique values in each categorical features.\ncategorical_n_unique = {cc: train_df[cc].nunique() for cc in categorical_columns}\ncategorical_n_unique","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:13.010247Z","iopub.execute_input":"2022-05-24T04:41:13.011073Z","iopub.status.idle":"2022-05-24T04:41:13.033914Z","shell.execute_reply.started":"2022-05-24T04:41:13.011027Z","shell.execute_reply":"2022-05-24T04:41:13.03317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_df(dataframe):\n    df = dataframe.copy()\n    \n    ## Drop 'Name'\n    df = df.drop(['Name'], axis=1)\n    \n    ## Transform 'Transported' column to 0 or 1.\n    if 'Transported' in df.columns:\n        df.loc[df['Transported']==True, 'Transported'] = 1.\n        df.loc[df['Transported']==False, 'Transported'] = 0.\n        df['Transported'] = df['Transported'].astype('int64')\n    \n    ## Transform True-False features (CryoSleep and VIP) to 'Yes' or 'No'.\n    df.loc[df['CryoSleep']==True, 'CryoSleep'] = 'Yes'\n    df.loc[df['CryoSleep']==False, 'CryoSleep'] = 'No'\n    df['CryoSleep'] = df['CryoSleep'].astype(str)\n    \n    df.loc[df['VIP']==True, 'VIP'] = 'Yes'\n    df.loc[df['VIP']==False, 'VIP'] = 'No'\n    df['VIP'] = df['VIP'].astype(str)\n    \n    ## Transform the dtypes of HomePlanet and Destination to str\n    df['HomePlanet'] = df['HomePlanet'].astype(str)\n    df['Destination'] = df['Destination'].astype(str)\n    \n    return df\n\ntrain = preprocess_df(train_df)\ntrain.head()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:13.035246Z","iopub.execute_input":"2022-05-24T04:41:13.035603Z","iopub.status.idle":"2022-05-24T04:41:13.084099Z","shell.execute_reply.started":"2022-05-24T04:41:13.035562Z","shell.execute_reply":"2022-05-24T04:41:13.083267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Caution: After `astype(str)`, null values (np.nan) are replaced by the string 'nan'.**","metadata":{}},{"cell_type":"code","source":"## Handle 'Cabin' feature\ndef cabin_split(dataframe):\n    df = dataframe.copy()\n    \n    df['Cabin'] = df['Cabin'].astype(str)\n    cabins = df['Cabin'].str.split('/', expand=True)\n    cabins.columns = ['Cabin_0', 'Cabin_1', 'Cabin_2']\n    \n    df = pd.concat([df, cabins], axis=1)\n    df = df.drop(['Cabin'], axis=1)\n    df['Cabin_0'].astype(str)\n    df['Cabin_1'] = pd.to_numeric(df['Cabin_1'], errors='coerce')\n    df['Cabin_2'].astype(str)\n    df['Cabin_2'] = df['Cabin_2'].map(lambda x: 'nan' if x is None else x)\n    \n    return df\n\ntrain = cabin_split(train)\ntrain.head()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:13.085595Z","iopub.execute_input":"2022-05-24T04:41:13.085927Z","iopub.status.idle":"2022-05-24T04:41:13.150465Z","shell.execute_reply.started":"2022-05-24T04:41:13.085885Z","shell.execute_reply":"2022-05-24T04:41:13.14954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_columns = ['HomePlanet', 'CryoSleep',\n                       'Destination', 'VIP']\n\ntrain_pos = train.query('Transported==1').reset_index(drop=True)\ntrain_neg = train.query('Transported==0').reset_index(drop=True)\nprint(f'positive samples: {len(train_pos)}, negative samples: {len(train_neg)}')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:13.151912Z","iopub.execute_input":"2022-05-24T04:41:13.152261Z","iopub.status.idle":"2022-05-24T04:41:13.169897Z","shell.execute_reply.started":"2022-05-24T04:41:13.15222Z","shell.execute_reply":"2022-05-24T04:41:13.168846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3.2\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>3.2 Target Distribution</center></h2>","metadata":{}},{"cell_type":"code","source":"## Target Distribution\ntarget_count = train.groupby(['Transported'])['PassengerId'].count()\ntarget_percent = target_count / target_count.sum()\n\n## Make Figure object\nfig = go.Figure()\n\n## Make trace (graph object)\ndata = go.Bar(x=target_count.index.astype(str).values, \n              y=target_count.values)\n\n## Add the trace to the Figure\nfig.add_trace(data)\n\n## Setting layouts\nfig.update_layout(title = dict(text=\"Target distribution\"),\n                  xaxis = dict(title=\"Transported' values\"),\n                  yaxis = dict(title='counts'))\n\n## Show the Figure\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:13.171902Z","iopub.execute_input":"2022-05-24T04:41:13.172186Z","iopub.status.idle":"2022-05-24T04:41:13.191659Z","shell.execute_reply.started":"2022-05-24T04:41:13.172146Z","shell.execute_reply":"2022-05-24T04:41:13.190803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3.3\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>3.3 Numerical Features</center></h2>","metadata":{}},{"cell_type":"markdown","source":"<a id =\"3.3.1\"></a><h2 style=\"background:#D4F1F4; border:0; border-radius: 12px; color:black\"><center>3.3.1 Statistics of Numerical Features </center></h2>","metadata":{}},{"cell_type":"code","source":"train.describe().T.style.bar(subset=['mean'],)\\\n                        .background_gradient(subset=['std'], cmap='coolwarm')\\\n                        .background_gradient(subset=['50%'], cmap='coolwarm')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:13.196499Z","iopub.execute_input":"2022-05-24T04:41:13.197131Z","iopub.status.idle":"2022-05-24T04:41:13.243373Z","shell.execute_reply.started":"2022-05-24T04:41:13.197085Z","shell.execute_reply":"2022-05-24T04:41:13.242557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby('Transported').describe().T","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:13.244808Z","iopub.execute_input":"2022-05-24T04:41:13.245091Z","iopub.status.idle":"2022-05-24T04:41:13.306731Z","shell.execute_reply.started":"2022-05-24T04:41:13.245043Z","shell.execute_reply":"2022-05-24T04:41:13.305745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quantiles = [0, 0.9, 0.95, 0.98, 0.99, 1]\ntrain_quantile_values = train[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].quantile(quantiles)\ntrain_quantile_values","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:13.308486Z","iopub.execute_input":"2022-05-24T04:41:13.308806Z","iopub.status.idle":"2022-05-24T04:41:13.331629Z","shell.execute_reply.started":"2022-05-24T04:41:13.308764Z","shell.execute_reply":"2022-05-24T04:41:13.330611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n#### There seems to be outliers...\n\n---","metadata":{}},{"cell_type":"code","source":"## Clipping outliers on 99% quantile\ndef clipping_quantile(dataframe, quantile_values=None, quantile=0.99):\n    df = dataframe.copy()\n    if quantile_values is None:\n        quantile_values = df[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].quantile(quantile)\n    \n    for num_column in ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']:\n        num_values = df[num_column].values\n        threshold = quantile_values[num_column]\n        num_values = np.where(num_values > threshold, threshold, num_values)\n        df[num_column] = num_values    \n    return df\n\ntrain = clipping_quantile(train, quantile_values=None, quantile=0.99)\n\ntrain.describe().T.style.bar(subset=['mean'],)\\\n                        .background_gradient(subset=['std'], cmap='coolwarm')\\\n                        .background_gradient(subset=['50%'], cmap='coolwarm')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:13.333474Z","iopub.execute_input":"2022-05-24T04:41:13.333779Z","iopub.status.idle":"2022-05-24T04:41:13.386899Z","shell.execute_reply.started":"2022-05-24T04:41:13.333736Z","shell.execute_reply":"2022-05-24T04:41:13.386002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## After clipping outliers on 99% quantile\ntrain.groupby('Transported').describe().T","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:13.388498Z","iopub.execute_input":"2022-05-24T04:41:13.388796Z","iopub.status.idle":"2022-05-24T04:41:13.448532Z","shell.execute_reply.started":"2022-05-24T04:41:13.388757Z","shell.execute_reply":"2022-05-24T04:41:13.447555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## After clipping outliers on 99% quantile\nn_cols = 2\nn_rows = int(np.ceil(len(numerical_columns) / n_cols))\n\nfig, axes = plt.subplots(nrows=n_rows,ncols=n_cols,figsize=(20,15))\n\nbins = 50\nfor i, column in enumerate(numerical_columns):\n    q, mod = divmod(i, n_cols)\n    sns.histplot(x=column, data=train, hue='Transported', ax=axes[q][mod], bins=bins, stat=\"percent\", legend=True)\n    axes[q][mod].set_title(f'Distribution of {numerical_columns[i]}',size=15)\n    \nfig.suptitle('Blue: Transported=0, Red: Transported=1', fontsize=20)\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:13.450302Z","iopub.execute_input":"2022-05-24T04:41:13.450629Z","iopub.status.idle":"2022-05-24T04:41:16.474143Z","shell.execute_reply.started":"2022-05-24T04:41:13.450589Z","shell.execute_reply":"2022-05-24T04:41:16.47338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Heat map of Correlation Matrix\nfig = px.imshow(train.corr(),\n                color_continuous_scale='RdBu_r',\n                color_continuous_midpoint=0, \n                aspect='auto')\nfig.update_layout(height=500, \n                  width=500,\n                  title = \"Heatmap\",                  \n                  showlegend=False)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:16.475423Z","iopub.execute_input":"2022-05-24T04:41:16.476028Z","iopub.status.idle":"2022-05-24T04:41:16.531872Z","shell.execute_reply.started":"2022-05-24T04:41:16.475987Z","shell.execute_reply":"2022-05-24T04:41:16.530948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3.3.2\"></a><h2 style=\"background:#D4F1F4; border:0; border-radius: 12px; color:black\"><center>3.3.2 Binning for Numerical Features </center></h2>","metadata":{}},{"cell_type":"markdown","source":"### Binning Method\n- `Age`: 0 to 100 at intervals of 5.\n\n- `other numerical features`: Split into 10 bins.\n - 1. Value=0 is the first bin ( get by (-1, 0] ).\n - 2. Get quantiles at [ 0, 0.9, 0.95, 0.99, 1 ].\n - 3. Split between quantiles_0 and quantiles_0.9 into 6 bins.\n - 4. Use quantiles_0.95, _0.99, _1 for the rest boundary.","metadata":{}},{"cell_type":"code","source":"def bin_split(dataframe, column, n_bins, thresholds=None):\n    if thresholds is None:\n        if column == 'Age':\n            bins = np.array([i*5 for i in range(21)])\n        else:\n            bins = np.array([-1, ])\n            x = dataframe[column]\n            x_quantiles = x.quantile([0, 0.9, 0.95, 0.99, 1])\n            bins = np.append(bins, [i * ((x_quantiles.iloc[1] - x_quantiles.iloc[0]) / (n_bins-4)) for i in range(n_bins-4)])\n            bins = np.append(bins, [x_quantiles.iloc[1], x_quantiles.iloc[2], x_quantiles.iloc[3], x_quantiles.iloc[4]+1])\n    else:\n        bins = thresholds[column]\n        \n    splits = pd.cut(dataframe[column], bins=bins, labels=False, right=True)\n    return splits, bins\n\ndef binning(dataframe, numerical_columns, n_bins, thresholds=None):\n    df = dataframe.copy()\n    df_split_bins = {}\n    for num_column in numerical_columns:\n        splits, bins = bin_split(df, num_column, n_bins, thresholds)\n        df[num_column] = splits\n        df_split_bins[num_column] = bins    \n    return df, df_split_bins\n\nn_bins = exp_config['n_bins']\ntrain, train_split_bins = binning(train, numerical_columns, n_bins, thresholds=None)\n\nfor key in train_split_bins:\n    print(f'{key} bins: \\n{train_split_bins[key]}\\n\\n')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:16.533518Z","iopub.execute_input":"2022-05-24T04:41:16.533807Z","iopub.status.idle":"2022-05-24T04:41:16.567737Z","shell.execute_reply.started":"2022-05-24T04:41:16.533767Z","shell.execute_reply":"2022-05-24T04:41:16.566897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## After Binning\nn_cols = 2\nn_rows = int(np.ceil(len(numerical_columns) / n_cols))\n\nfig, axes = plt.subplots(nrows=n_rows,ncols=n_cols,figsize=(20,15))\n\nbins = 50\nfor i, column in enumerate(numerical_columns):\n    q, mod = divmod(i, n_cols)\n    sns.histplot(x=column, data=train, hue='Transported', ax=axes[q][mod], bins=bins, stat=\"percent\", legend=True)\n    axes[q][mod].set_title(f'Distribution of {numerical_columns[i]}',size=15)\n    \nfig.suptitle('Blue: Transported=0, Red: Transported=1', fontsize=20)\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:16.569794Z","iopub.execute_input":"2022-05-24T04:41:16.570284Z","iopub.status.idle":"2022-05-24T04:41:19.384214Z","shell.execute_reply.started":"2022-05-24T04:41:16.570239Z","shell.execute_reply":"2022-05-24T04:41:19.383473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3.4\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>3.4 Categorical Features</center></h2>","metadata":{}},{"cell_type":"code","source":"## Make Figure object\nfig = make_subplots(rows=2, cols=2,\n                    subplot_titles=categorical_columns,\n                    shared_yaxes='all')\n\nfor i in range(2):\n    for j in range(2):\n        n = i*2 + j\n        ## Make trace (graph object)\n        data0 = go.Histogram(x=train_neg[categorical_columns[n]],\n                             marker = dict(color='#0000FF'), ## Blue\n                             name='Transporetd=0')\n        data1 = go.Histogram(x=train_pos[categorical_columns[n]],\n                             marker = dict(color='#FF0000'), ## Red\n                             name='Transported=1')\n        \n        ## Add the trace to the Figure\n        fig.add_trace(data0, row=i+1, col=j+1)\n        fig.add_trace(data1, row=i+1, col=j+1)\n        \n        fig.update_traces(opacity=0.75, histnorm='probability')\n        #fig.update_layout(barmode='overlay')\n\n## Setting layouts\nfig.update_layout(title = dict(text='Blue: Transported=0, Red: Transported=1'),\n                  showlegend=False,)\nfig.update_yaxes(title='probability', row=1, col=1)\nfig.update_yaxes(title='probability', row=2, col=1)\n\n## Show the Figure\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:19.385547Z","iopub.execute_input":"2022-05-24T04:41:19.386058Z","iopub.status.idle":"2022-05-24T04:41:19.598062Z","shell.execute_reply.started":"2022-05-24T04:41:19.386016Z","shell.execute_reply":"2022-05-24T04:41:19.597254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cabin Features","metadata":{}},{"cell_type":"code","source":"## 'Cabin_0'\nsns.countplot(x='Cabin_0', data=train, hue='Transported')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:19.599537Z","iopub.execute_input":"2022-05-24T04:41:19.599814Z","iopub.status.idle":"2022-05-24T04:41:19.874811Z","shell.execute_reply.started":"2022-05-24T04:41:19.599779Z","shell.execute_reply":"2022-05-24T04:41:19.874116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 'Cabin_1'\nsns.histplot(x='Cabin_1', data=train, hue='Transported', kde=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:19.87622Z","iopub.execute_input":"2022-05-24T04:41:19.876474Z","iopub.status.idle":"2022-05-24T04:41:20.237228Z","shell.execute_reply.started":"2022-05-24T04:41:19.876439Z","shell.execute_reply":"2022-05-24T04:41:20.236536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 'Cabin_2'\nsns.countplot(x='Cabin_2', data=train, hue='Transported')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:20.239052Z","iopub.execute_input":"2022-05-24T04:41:20.239543Z","iopub.status.idle":"2022-05-24T04:41:20.440218Z","shell.execute_reply.started":"2022-05-24T04:41:20.239503Z","shell.execute_reply":"2022-05-24T04:41:20.439473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Binning 'Cabin_1'","metadata":{}},{"cell_type":"code","source":"## Histogram of 'Cabin_1' by Plotly (interactive)\nfig = go.Figure()\n\ndata0 = go.Histogram(x=train_neg['Cabin_1'],\n                             marker = dict(color='#0000FF'), # Blue\n                             opacity=0.6,\n                             name='Transporetd=0')\ndata1 = go.Histogram(x=train_pos['Cabin_1'],\n                             marker = dict(color='#FF0000'), # Red\n                             opacity=0.6,\n                             name='Transported=1')\n\nfig.add_trace(data0)\nfig.add_trace(data1)\n\nfig.update_layout(xaxis = dict(title='Cabin_1'),\n                  yaxis = dict(title='Count'))\nfig.update_layout(barmode='overlay')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:20.441703Z","iopub.execute_input":"2022-05-24T04:41:20.44197Z","iopub.status.idle":"2022-05-24T04:41:20.462531Z","shell.execute_reply.started":"2022-05-24T04:41:20.441932Z","shell.execute_reply":"2022-05-24T04:41:20.461618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Binning 'Cabin_1' based on the above graph\ncabin_1_bins = np.array([0, 300, 600, 1150, 1500, 1700, 2000])\ntrain['Cabin_1'] = pd.cut(train['Cabin_1'], bins=cabin_1_bins, labels=False, right=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:20.464377Z","iopub.execute_input":"2022-05-24T04:41:20.464968Z","iopub.status.idle":"2022-05-24T04:41:20.475347Z","shell.execute_reply.started":"2022-05-24T04:41:20.464924Z","shell.execute_reply":"2022-05-24T04:41:20.474498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 'Cabin_1' after binning\nsns.countplot(x='Cabin_1', data=train, hue='Transported')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:20.477323Z","iopub.execute_input":"2022-05-24T04:41:20.477762Z","iopub.status.idle":"2022-05-24T04:41:20.725227Z","shell.execute_reply.started":"2022-05-24T04:41:20.477607Z","shell.execute_reply":"2022-05-24T04:41:20.724471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3.5\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>3.5 Data Processing Complete </center></h2>","metadata":{}},{"cell_type":"code","source":"numerical_columns_0 = ['Age', 'RoomService', 'FoodCourt',\n                     'ShoppingMall', 'Spa', 'VRDeck']\nnumerical_columns_1 = ['Age', 'RoomService', 'FoodCourt',\n                     'ShoppingMall', 'Spa', 'VRDeck', 'Cabin_1']\ncategorical_columns_0 = ['PassengerId', 'HomePlanet', 'CryoSleep',\n                       'Cabin', 'Destination', 'VIP', 'Name']\ncategorical_columns_1 = ['PassengerId', 'HomePlanet', 'CryoSleep',\n                       'Cabin', 'Destination', 'VIP', 'Name',\n                       'Cabin_0', 'Cabin_2']","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:20.726656Z","iopub.execute_input":"2022-05-24T04:41:20.727139Z","iopub.status.idle":"2022-05-24T04:41:20.734225Z","shell.execute_reply.started":"2022-05-24T04:41:20.727096Z","shell.execute_reply":"2022-05-24T04:41:20.733392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Before filling null values,ã€€making the string 'nan' (transformed by astype(str) in preprocess_df() function) back to np.nan.\nfor column in ['CryoSleep', 'VIP', 'HomePlanet', 'Destination', 'Cabin_0', 'Cabin_2']:\n    train[column] = train[column].map(lambda x: np.nan if x=='nan' else x)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:20.73603Z","iopub.execute_input":"2022-05-24T04:41:20.736366Z","iopub.status.idle":"2022-05-24T04:41:20.761076Z","shell.execute_reply.started":"2022-05-24T04:41:20.736323Z","shell.execute_reply":"2022-05-24T04:41:20.760339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Filling null values with mode\ntrain = train.fillna(train.mode().iloc[0])\n\nfor numerical in numerical_columns_1:\n    train[numerical] = train[numerical].astype('int64')\n\ntrain.info()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:20.766026Z","iopub.execute_input":"2022-05-24T04:41:20.76624Z","iopub.status.idle":"2022-05-24T04:41:20.833089Z","shell.execute_reply.started":"2022-05-24T04:41:20.766214Z","shell.execute_reply":"2022-05-24T04:41:20.831101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Test Data Processing\ntest = preprocess_df(test_df)\ntest = cabin_split(test)\n\ntest = clipping_quantile(test, quantile_values=train_quantile_values.loc[0.99])\ntest, _ = binning(test, numerical_columns_0, n_bins, thresholds=train_split_bins)\ntest['Cabin_1'] = pd.cut(test['Cabin_1'], bins=cabin_1_bins, labels=False, right=False)\n\nfor column in ['CryoSleep', 'VIP', 'HomePlanet', 'Destination', 'Cabin_0', 'Cabin_2']:\n    test[column] = test[column].map(lambda x: np.nan if x=='nan' else x)\n\ntest = test.fillna(train.mode().iloc[0])\n\nfor numerical in numerical_columns_1:\n    test[numerical] = test[numerical].astype('int64')\n\ntest.info()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:20.834725Z","iopub.execute_input":"2022-05-24T04:41:20.835039Z","iopub.status.idle":"2022-05-24T04:41:20.942689Z","shell.execute_reply.started":"2022-05-24T04:41:20.834996Z","shell.execute_reply":"2022-05-24T04:41:20.941747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3.6\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>3.6 Validation Split </center></h2>","metadata":{}},{"cell_type":"code","source":"## Split train samples for cross-validation\nn_splits = exp_config['n_splits']\nskf = StratifiedKFold(n_splits=n_splits)\ntrain['k_folds'] = -1\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(X=train,\n                                                        y=train['Transported'])):\n    train['k_folds'][valid_idx] = fold\n    \n## Check split samples\nfor i in range(n_splits):\n    print(f\"fold {i}: {len(train.query('k_folds==@i'))} samples\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:20.944249Z","iopub.execute_input":"2022-05-24T04:41:20.94478Z","iopub.status.idle":"2022-05-24T04:41:20.979347Z","shell.execute_reply.started":"2022-05-24T04:41:20.944734Z","shell.execute_reply":"2022-05-24T04:41:20.978355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Hold-out validation\nvalid_fold = train.query(f'k_folds == 0').reset_index(drop=True)\ntrain_fold = train.query(f'k_folds != 0').reset_index(drop=True)\nprint(len(train_fold), len(valid_fold))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:20.98078Z","iopub.execute_input":"2022-05-24T04:41:20.981284Z","iopub.status.idle":"2022-05-24T04:41:21.000206Z","shell.execute_reply.started":"2022-05-24T04:41:20.981246Z","shell.execute_reply":"2022-05-24T04:41:20.999134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"4\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>4. Model</center></h1>","metadata":{}},{"cell_type":"markdown","source":"<a id =\"4.1\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>4.1 Dataset </center></h2>","metadata":{}},{"cell_type":"code","source":"def df_to_dataset(dataframe, num_columns, target=None,\n                  shuffle=False, repeat=False,\n                  batch_size=5, drop_remainder=False):\n    df = dataframe.copy()\n    if target is not None:\n        labels = df.pop(target)\n        data = {key: value[:, tf.newaxis] for key, value in df.items()}\n        data = dict(data)\n        \n        column_indices = tf.range(start=0, limit=num_columns,\n                                          delta=1, dtype='int64')\n        column_indices = tf.expand_dims(column_indices, axis=0)\n        column_indices = tf.repeat(column_indices, repeats=len(dataframe), axis=0)\n        data['column_indices'] = column_indices\n        \n        ds = tf.data.Dataset.from_tensor_slices((data, labels))\n    else:\n        data = {key: value[:, tf.newaxis] for key, value in df.items()}\n        data = dict(data)\n        \n        column_indices = tf.range(start=0, limit=num_columns,\n                                          delta=1, dtype='int64')\n        column_indices = tf.expand_dims(column_indices, axis=0)\n        column_indices = tf.repeat(column_indices, repeats=len(dataframe), axis=0)\n        data['column_indices'] = column_indices\n        \n        ds = tf.data.Dataset.from_tensor_slices(data)\n    \n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(df))\n    if repeat:\n        ds = ds.repeat()\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n    ds = ds.prefetch(batch_size)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:21.002064Z","iopub.execute_input":"2022-05-24T04:41:21.002381Z","iopub.status.idle":"2022-05-24T04:41:21.016367Z","shell.execute_reply.started":"2022-05-24T04:41:21.002338Z","shell.execute_reply":"2022-05-24T04:41:21.015166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create datasets\nnum_columns = exp_config['num_columns']\nbatch_size = exp_config['batch_size']\n\ntrain_ds = df_to_dataset(train_fold, num_columns,\n                         target='Transported',\n                         shuffle=True,\n                         repeat=False,\n                         batch_size=batch_size,\n                         drop_remainder=False)\n\nvalid_ds = df_to_dataset(valid_fold, num_columns,\n                         target='Transported',\n                         shuffle=False,\n                         repeat=False,\n                         batch_size=batch_size,\n                         drop_remainder=False)\n\n## Display a batch sample\nexample = next(iter(train_ds))[0]\ninput_dtypes = {}\nfor key in example:\n    input_dtypes[key] = example[key].dtype\n    print(f'{key}, shape:{example[key].shape}, {example[key].dtype}')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:21.017841Z","iopub.execute_input":"2022-05-24T04:41:21.018567Z","iopub.status.idle":"2022-05-24T04:41:21.135264Z","shell.execute_reply.started":"2022-05-24T04:41:21.018526Z","shell.execute_reply":"2022-05-24T04:41:21.134351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"4.2\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>4.2 Preprocessing Model </center></h2>","metadata":{}},{"cell_type":"code","source":"## After binning, all features are categorical.\nnumerical_columns = []\ncategorical_columns = ['Age', 'RoomService', 'FoodCourt',\n                       'ShoppingMall', 'Spa', 'VRDeck',\n                       'HomePlanet', 'CryoSleep',\n                       'Destination', 'VIP', \n                       'Cabin_0', 'Cabin_1', 'Cabin_2']","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:21.136726Z","iopub.execute_input":"2022-05-24T04:41:21.137014Z","iopub.status.idle":"2022-05-24T04:41:21.142102Z","shell.execute_reply.started":"2022-05-24T04:41:21.136974Z","shell.execute_reply":"2022-05-24T04:41:21.14115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Preprocessing model inputs\ndef create_preprocess_inputs(numerical, categorical, num_columns, input_dtypes):\n    preprocess_inputs = {}\n    numerical_inputs = {key: layers.Input(shape=(1,),\n                                          dtype=input_dtypes[key]) for key in numerical}\n    categorical_inputs = {key: layers.Input(shape=(1,), \n                                            dtype=input_dtypes[key]) for key in categorical}\n    preprocess_inputs.update(**numerical_inputs, **categorical_inputs)\n    \n    column_indices_inputs = layers.Input(shape=(num_columns, ),\n                                         dtype='int64')\n    preprocess_inputs['column_indices']  =column_indices_inputs\n    return preprocess_inputs\n\n\npreprocess_inputs = create_preprocess_inputs(numerical_columns,\n                                             categorical_columns,\n                                             num_columns,\n                                             input_dtypes)\npreprocess_inputs","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:21.14365Z","iopub.execute_input":"2022-05-24T04:41:21.144134Z","iopub.status.idle":"2022-05-24T04:41:21.172959Z","shell.execute_reply.started":"2022-05-24T04:41:21.144087Z","shell.execute_reply":"2022-05-24T04:41:21.171898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create Preprocessing model\ndef create_preprocessing_model(numerical, categorical,\n                               num_columns, input_dtypes, df):\n    \n    ## Create inputs\n    preprocess_inputs = create_preprocess_inputs(numerical,\n                                                 categorical,\n                                                 num_columns,\n                                                 input_dtypes)\n    \n    ## Preprocessing layers for numerical_features\n    normalize_layers = {}\n    for nc in numerical:\n        normalize_layer = layers.Normalization(mean=df[nc].mean(),\n                                               variance=df[nc].var())\n        normalize_layers[nc] = normalize_layer\n        \n    ## Preprocessing layers for categorical_features\n    lookup_layers = {}\n    for cc in categorical:\n        if input_dtypes[cc] is tf.string:\n            lookup_layer = layers.StringLookup(vocabulary=df[cc].unique(),\n                                               output_mode='int')\n        elif input_dtypes[cc] is tf.int64:\n            lookup_layer = layers.IntegerLookup(vocabulary=df[cc].unique(),\n                                                output_mode='int')\n        lookup_layers[cc] = lookup_layer\n    \n    ## Create outputs\n    preprocess_outputs = {}\n    for key in preprocess_inputs:\n        if key in normalize_layers:\n            output = normalize_layers[key](preprocess_intputs[key])\n            preprocess_outputs[key] = output\n        elif key in lookup_layers:\n            output = lookup_layers[key](preprocess_inputs[key])\n            preprocess_outputs[key] = output\n        elif key is 'column_indices':\n            preprocess_outputs[key] = preprocess_inputs[key]\n            \n    ## Create model\n    preprocessing_model = tf.keras.Model(preprocess_inputs,\n                                         preprocess_outputs)\n    \n    return preprocessing_model, lookup_layers\n\n\npreprocessing_model, lookup_layers = create_preprocessing_model(numerical_columns,\n                                                             categorical_columns,\n                                                             num_columns,\n                                                             input_dtypes,\n                                                             train_fold)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:21.174958Z","iopub.execute_input":"2022-05-24T04:41:21.175447Z","iopub.status.idle":"2022-05-24T04:41:21.289781Z","shell.execute_reply.started":"2022-05-24T04:41:21.175404Z","shell.execute_reply":"2022-05-24T04:41:21.28889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Apply the preprocessing model in tf.data.Dataset.map\ntrain_ds = train_ds.map(lambda x, y: (preprocessing_model(x), y),\n                        num_parallel_calls=tf.data.AUTOTUNE)\nvalid_ds = valid_ds.map(lambda x, y: (preprocessing_model(x), y),\n                        num_parallel_calls=tf.data.AUTOTUNE)\n\n## Display a preprocessed input sample\nexample = next(train_ds.take(1).as_numpy_iterator())[0]\nfor key in example:\n    print(f'{key}, shape:{example[key].shape}, {example[key].dtype}')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:21.29178Z","iopub.execute_input":"2022-05-24T04:41:21.292221Z","iopub.status.idle":"2022-05-24T04:41:21.532962Z","shell.execute_reply.started":"2022-05-24T04:41:21.292181Z","shell.execute_reply":"2022-05-24T04:41:21.532031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"4.3\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>4.3 Training Model </center></h2>","metadata":{}},{"cell_type":"code","source":"## Training model inputs\ndef create_model_inputs(numerical, categorical, input_dtypes):\n    model_inputs = {}\n    \n    normalized_inputs = {key: layers.Input(shape=(1,),\n                                           dtype=input_dtypes[key]) for key in numerical}\n    lookup_inputs = {key: layers.Input(shape=(1,),\n                                       dtype='int64') for key in categorical}\n    column_indices_inputs = layers.Input(shape=(len(categorical),),\n                                        dtype='int64')\n    \n    model_inputs.update(**normalized_inputs, **lookup_inputs)\n    model_inputs['column_indices'] = column_indices_inputs\n    return model_inputs\n\nmodel_inputs = create_model_inputs(numerical_columns,\n                                   categorical_columns,\n                                   input_dtypes)\nmodel_inputs","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:21.536494Z","iopub.execute_input":"2022-05-24T04:41:21.536752Z","iopub.status.idle":"2022-05-24T04:41:21.565833Z","shell.execute_reply.started":"2022-05-24T04:41:21.53672Z","shell.execute_reply":"2022-05-24T04:41:21.564562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create Embedding layers\ndef create_embedding_layers(model_inputs, numerical, categorical,\n                            lookup_layers, emb_dim):\n    numerical_feature_list = []\n    encoded_categorical_feature_list = []\n    \n    for key in model_inputs:\n        if key in numerical:\n            numerical_feature_list.append(model_inputs[key])\n        elif key in categorical:\n            ## Create Embeddings for categorical features\n            embedding = layers.Embedding(input_dim=lookup_layers[key].vocabulary_size(),\n                                         output_dim=emb_dim)\n            encoded_categorical_feature = embedding(model_inputs[key])\n            encoded_categorical_feature_list.append(encoded_categorical_feature)\n        elif key is 'column_indices':\n            ## Create positional embedding (column embedding)\n            column_embedding = layers.Embedding(\n                input_dim=len(categorical), output_dim=emb_dim, \n                name='column_embedding')\n            column_embeddings = column_embedding(model_inputs[key])\n    \n    if len(numerical_feature_list) != 0:\n        numerical_features = tf.concat(numerical_feature_list, axis=1)\n    else:\n        numerical_features = tf.stack(numerical_feature_list)\n    \n    encoded_categorical_features = tf.concat(encoded_categorical_feature_list, axis=1)\n    encoded_categorical_features = encoded_categorical_features + column_embeddings\n    \n    return numerical_features, encoded_categorical_features\n\ncat_embedding_dim = model_config['cat_embedding_dim']\nnumerical_features, encoded_categorical_features = create_embedding_layers(model_inputs,\n                                                                           numerical_columns,\n                                                                           categorical_columns,\n                                                                           lookup_layers,\n                                                                           cat_embedding_dim)\nnumerical_features.shape, encoded_categorical_features.shape","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:21.567516Z","iopub.execute_input":"2022-05-24T04:41:21.567984Z","iopub.status.idle":"2022-05-24T04:41:21.666258Z","shell.execute_reply.started":"2022-05-24T04:41:21.56794Z","shell.execute_reply":"2022-05-24T04:41:21.66547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tab Transformer\n\nThe TabTransformer architecture works as follows:\n\n- All the categorical features are encoded as embeddings, using the same embedding_dims. This means that each value in each categorical feature will have its own embedding vector.\n\n- A column embedding, one embedding vector for each categorical feature, is added (point-wise) to the categorical feature embedding.\n\n- The embedded categorical features are fed into a stack of Transformer blocks. Each Transformer block consists of a multi-head self-attention layer followed by a feed-forward layer.\n\n- The outputs of the final Transformer layer, which are the contextual embeddings of the categorical features, are concatenated with the input numerical features, and fed into a final MLP block.\n\n<img src=\"https://raw.githubusercontent.com/keras-team/keras-io/master/examples/structured_data/img/tabtransformer/tabtransformer.png\" width=\"500\"/>\n","metadata":{}},{"cell_type":"code","source":"## TabTransformer's module\ndef create_mlp(hidden_units, dropout_rates,\n               activation, normalization_layer,\n               name=None):\n    mlp_layers = []\n    for i, units in enumerate(hidden_units):\n        mlp_layers.append(normalization_layer)\n        mlp_layers.append(layers.Dense(units,\n                                       activation=activation))\n        mlp_layers.append(layers.Dropout(dropout_rates[i]))\n    return keras.Sequential(mlp_layers, name=name)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:21.66749Z","iopub.execute_input":"2022-05-24T04:41:21.668386Z","iopub.status.idle":"2022-05-24T04:41:21.675119Z","shell.execute_reply.started":"2022-05-24T04:41:21.668318Z","shell.execute_reply":"2022-05-24T04:41:21.674098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create TabTransformer model\ndef create_tabtransformer(num_transformer_blocks,\n                          num_heads,\n                          emb_dim,\n                          tf_dropout_rates,\n                          ff_dropout_rates,\n                          mlp_dropout_rates,\n                          mlp_hidden_units_factors,\n                          numerical_columns,\n                          categorical_columns,\n                          num_columns,\n                          input_dtypes,\n                          lookup_layers,):\n    \n    model_inputs = create_model_inputs(numerical_columns,\n                                       categorical_columns,\n                                       input_dtypes)\n    \n    numerical_features, encoded_categorical_features = create_embedding_layers(model_inputs,\n                                                                           numerical_columns,\n                                                                           categorical_columns,\n                                                                           lookup_layers,\n                                                                           emb_dim)\n    \n    for block_idx in range(num_transformer_blocks):\n        ## Create a multi-head attention layer\n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads,\n            key_dim=emb_dim,\n            dropout=tf_dropout_rates[block_idx],\n            name=f'multi-head_attention_{block_idx}'\n        )(encoded_categorical_features, encoded_categorical_features)\n        ## Skip connection 1\n        x = layers.Add(\n            name=f'skip_connection1_{block_idx}'\n        )([attention_output, encoded_categorical_features])\n        ## Layer normalization 1\n        x = layers.LayerNormalization(\n            name=f'layer_norm1_{block_idx}', \n            epsilon=1e-6\n        )(x)\n        ## Feedforward\n        feedforward_output = keras.Sequential([\n            layers.Dense(emb_dim, activation=keras.activations.gelu),\n            layers.Dropout(ff_dropout_rates[block_idx]),\n        ], name=f'feedforward_{block_idx}'\n        )(x)\n        ## Skip_connection 2\n        x = layers.Add(\n            name=f'skip_connection2_{block_idx}'\n        )([feedforward_output, x])\n        ## Layer normalization 2\n        encoded_categorical_features = layers.LayerNormalization(\n            name=f'layer_norm2_{block_idx}', \n            epsilon=1e-6\n        )(x)\n        \n    contextualized_categorical_features = layers.Flatten(\n    )(encoded_categorical_features)\n    \n    ## Numerical features\n    if len(numerical_columns) > 0:\n        numerical_features = layers.LayerNormalization(\n            name=f'numerical_norm', \n            epsilon=1e-6\n        )(numerical_features)\n        \n        ## Concatenate categorical features with numerical features\n        features = layers.Concatenate()([\n            contextualized_categorical_features,\n            numerical_features\n        ])\n    else:\n        features = contextualized_categorical_features\n        \n    ## Final MLP\n    mlp_hidden_units = [\n        int(factor * features.shape[-1]) for factor in mlp_hidden_units_factors]\n    features = create_mlp(\n        hidden_units=mlp_hidden_units,\n        dropout_rates=mlp_dropout_rates,\n        activation=keras.activations.selu,\n        normalization_layer=layers.BatchNormalization(),\n        name='MLP'\n    )(features)\n    \n    ## Add a sigmoid to cat the output from 0 to 1\n    model_outputs = layers.Dense(\n        units=1,\n        activation='sigmoid',\n        name='sigmoid'\n    )(features)\n    \n    ## Create model\n    training_model = keras.Model(inputs=model_inputs,\n                                 outputs=model_outputs)\n    \n    return training_model\n    \n## Settings for TabTransformer\nnum_transformer_blocks = model_config['num_transformer_blocks']\nnum_heads = model_config['num_heads']\ntf_dropout_rates = model_config['tf_dropout_rates']\nff_dropout_rates = model_config['ff_dropout_rates']\nmlp_dropout_rates = model_config['mlp_dropout_rates']\nmlp_hidden_units_factors = model_config['mlp_hidden_units_factors']\n\n## Create TabTransformer\ntraining_model = create_tabtransformer(num_transformer_blocks,\n                                       num_heads,\n                                       cat_embedding_dim,\n                                       tf_dropout_rates,\n                                       ff_dropout_rates,\n                                       mlp_dropout_rates,\n                                       mlp_hidden_units_factors,\n                                       numerical_columns,\n                                       categorical_columns,\n                                       num_columns,\n                                       input_dtypes,\n                                       lookup_layers)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:21.676886Z","iopub.execute_input":"2022-05-24T04:41:21.67717Z","iopub.status.idle":"2022-05-24T04:41:22.377621Z","shell.execute_reply.started":"2022-05-24T04:41:21.67713Z","shell.execute_reply":"2022-05-24T04:41:22.376888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## model compile and build\nlr = exp_config['learning_rate']\nwd = exp_config['weight_decay']\n\noptimizer = tfa.optimizers.AdamW(\n    learning_rate=lr, \n    weight_decay=wd)\n\nloss_fn = keras.losses.BinaryCrossentropy(\n    from_logits=False, \n    label_smoothing=model_config['label_smoothing'])\n\ntraining_model.compile(optimizer=optimizer,\n                       loss=loss_fn,\n                       metrics=['accuracy', keras.metrics.AUC()])\n\ntraining_model.summary()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:22.378759Z","iopub.execute_input":"2022-05-24T04:41:22.379383Z","iopub.status.idle":"2022-05-24T04:41:22.43443Z","shell.execute_reply.started":"2022-05-24T04:41:22.379335Z","shell.execute_reply":"2022-05-24T04:41:22.433795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"5\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>5. Model Training</center></h1>","metadata":{}},{"cell_type":"markdown","source":"<a id =\"5.1\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>5.1 Learning Rate Finder </center></h2>","metadata":{}},{"cell_type":"code","source":"class LRFind(tf.keras.callbacks.Callback):\n    def __init__(self, min_lr, max_lr, n_rounds):\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.step_up = tf.constant((max_lr / min_lr) ** (1 / n_rounds))\n        self.lrs = []\n        self.losses = []\n        \n    def on_train_begin(self, logs=None):\n        self.weights = self.model.get_weights()\n        self.model.optimizer.lr = self.min_lr\n        \n    def on_train_batch_end(self, batch, logs=None):\n        self.lrs.append(self.model.optimizer.lr.numpy())\n        self.losses.append(logs['loss'])\n        self.model.optimizer.lr = self.model.optimizer.lr * self.step_up\n        if self.model.optimizer.lr > self.max_lr:\n            self.model.stop_training = True \n    \n    def on_train_end(self, logs=None):\n        self.model.set_weights(self.weights)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:22.436925Z","iopub.execute_input":"2022-05-24T04:41:22.437107Z","iopub.status.idle":"2022-05-24T04:41:22.445244Z","shell.execute_reply.started":"2022-05-24T04:41:22.437084Z","shell.execute_reply":"2022-05-24T04:41:22.444446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_lr = 1e-6\nmax_lr = 5e-2\nlr_find_epochs = 1\nlr_find_steps = 100\nlr_find_batch_size = 512\n\nlr_find = LRFind(min_lr, max_lr, lr_find_steps)\nlr_find_ds = df_to_dataset(train_fold, num_columns,\n                           target='Transported',\n                           repeat=True,\n                           batch_size=lr_find_batch_size)\nlr_find_ds = lr_find_ds.map(lambda x, y: (preprocessing_model(x), y),\n                            num_parallel_calls=tf.data.AUTOTUNE)\n\ntraining_model.fit(lr_find_ds,\n                   steps_per_epoch=lr_find_steps,\n                   epochs=lr_find_epochs,\n                   callbacks=[lr_find])\n\nplt.plot(lr_find.lrs, lr_find.losses)\nplt.xscale('log')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:22.446841Z","iopub.execute_input":"2022-05-24T04:41:22.447129Z","iopub.status.idle":"2022-05-24T04:41:35.996742Z","shell.execute_reply.started":"2022-05-24T04:41:22.447058Z","shell.execute_reply":"2022-05-24T04:41:35.995955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"5.2\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>5.2 Model Training </center></h2>","metadata":{}},{"cell_type":"code","source":"## Settings for Training\nepochs = exp_config['train_epochs']\nbatch_size = exp_config['batch_size']\nsteps_per_epoch = len(train_fold)//batch_size\n\n## Re-construct the model\ntraining_model_config = training_model.get_config()\ntraining_model = tf.keras.Model.from_config(training_model_config)\n\n## Model compile\nlearning_rate = exp_config['learning_rate']\nweight_decay = exp_config['weight_decay']\n\nlearning_schedule = tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate=learning_rate,\n    decay_steps=epochs*steps_per_epoch, \n    alpha=0.0)\n\noptimizer = tfa.optimizers.AdamW(\n    learning_rate=learning_schedule,\n    weight_decay=weight_decay)\n\nloss_fn = keras.losses.BinaryCrossentropy(\n    from_logits=False, \n    label_smoothing=model_config['label_smoothing'])\n\ntraining_model.compile(optimizer=optimizer,\n                       loss=loss_fn,\n                       metrics=['accuracy', keras.metrics.AUC()])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:41:35.998061Z","iopub.execute_input":"2022-05-24T04:41:35.999832Z","iopub.status.idle":"2022-05-24T04:41:36.718813Z","shell.execute_reply.started":"2022-05-24T04:41:35.999785Z","shell.execute_reply":"2022-05-24T04:41:36.718076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Checkpoint callback\ncheckpoint_filepath = exp_config['checkpoint_filepath']\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath, \n    save_weights_only=True, \n    monitor='val_loss', \n    mode='min', \n    save_best_only=True)\n\n## Model training\nhistory = training_model.fit(train_ds,\n                  epochs=epochs,\n                  shuffle=True,\n                  validation_data=valid_ds,\n                  callbacks=[model_checkpoint_callback])\n\n## Load the best parameters\ntraining_model.load_weights(checkpoint_filepath)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:41:36.720232Z","iopub.execute_input":"2022-05-24T04:41:36.720528Z","iopub.status.idle":"2022-05-24T04:42:46.065253Z","shell.execute_reply.started":"2022-05-24T04:41:36.720493Z","shell.execute_reply":"2022-05-24T04:42:46.064535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plot the train and valid losses\ndef plot_history(hist, title=None, valid=True):\n    plt.figure(figsize=(7, 5))\n    plt.plot(np.array(hist.index), hist['loss'], label='Train Loss')\n    if valid:\n        plt.plot(np.array(hist.index), hist['val_loss'], label='Valid Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title(title)\n    plt.show()\n    \nhist = pd.DataFrame(history.history)\nplot_history(hist)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:42:46.066722Z","iopub.execute_input":"2022-05-24T04:42:46.06698Z","iopub.status.idle":"2022-05-24T04:42:46.268672Z","shell.execute_reply.started":"2022-05-24T04:42:46.066945Z","shell.execute_reply":"2022-05-24T04:42:46.267985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"6\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>6. Inference</center></h1>","metadata":{}},{"cell_type":"markdown","source":"<a id =\"6.1\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>6.1 Finalize Model </center></h2>","metadata":{}},{"cell_type":"code","source":"## Finalize with all training data\nif exp_config['finalize']:\n    \n    ## Create datasets    \n    train_all_ds = df_to_dataset(train, num_columns,\n                         target='Transported',\n                         shuffle=True,\n                         repeat=False,\n                         batch_size=batch_size,\n                         drop_remainder=False)\n    \n    ## Create preprocessing model\n    preprocessing_model, lookup_layers = create_preprocessing_model(numerical_columns,\n                                                             categorical_columns,\n                                                             num_columns,\n                                                             input_dtypes,\n                                                             train)\n    \n    ## Apply the preprocessing model in tf.data.Dataset.map\n    train_all_ds = train_all_ds.map(lambda x, y: (preprocessing_model(x), y),\n                        num_parallel_calls=tf.data.AUTOTUNE)\n    \n    ## Re-construct the training model\n    #training_model_config = training_model.get_config()\n    #training_model = tf.keras.Model.from_config(training_model_config)\n    training_model = create_tabtransformer(num_transformer_blocks,\n                                       num_heads,\n                                       cat_embedding_dim,\n                                       tf_dropout_rates,\n                                       ff_dropout_rates,\n                                       mlp_dropout_rates,\n                                       mlp_hidden_units_factors,\n                                       numerical_columns,\n                                       categorical_columns,\n                                       num_columns,\n                                       input_dtypes,\n                                       lookup_layers)\n\n    ## Model compile\n    learning_rate = exp_config['learning_rate']\n    weight_decay = exp_config['weight_decay']\n    \n    learning_schedule = tf.keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate=learning_rate,\n        decay_steps=epochs*steps_per_epoch, \n        alpha=0.0)\n    \n    optimizer = tfa.optimizers.AdamW(\n        learning_rate=learning_schedule,\n        weight_decay=weight_decay)\n    \n    loss_fn = keras.losses.BinaryCrossentropy(\n        from_logits=False, \n        label_smoothing=model_config['label_smoothing'])\n    \n    training_model.compile(optimizer=optimizer,\n                       loss=loss_fn,\n                       metrics=['accuracy', keras.metrics.AUC()])\n    \n    ## Model training\n    final_hist = training_model.fit(train_all_ds, \n                                    epochs=epochs,\n                                    shuffle=True)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:42:46.269874Z","iopub.execute_input":"2022-05-24T04:42:46.270118Z","iopub.status.idle":"2022-05-24T04:43:48.540523Z","shell.execute_reply.started":"2022-05-24T04:42:46.270083Z","shell.execute_reply":"2022-05-24T04:43:48.539727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plot the training loss\nif exp_config['finalize']:\n    final_hist = pd.DataFrame(final_hist.history)\n    plot_history(final_hist, valid=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:43:48.542931Z","iopub.execute_input":"2022-05-24T04:43:48.543198Z","iopub.status.idle":"2022-05-24T04:43:48.733117Z","shell.execute_reply.started":"2022-05-24T04:43:48.543162Z","shell.execute_reply":"2022-05-24T04:43:48.732463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"6.2\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>6.2 Test Inference </center></h2>","metadata":{}},{"cell_type":"code","source":"## Inference_model = preprocessing_model + training_model\ninference_inputs = preprocessing_model.input\ninference_outputs = training_model(preprocessing_model(inference_inputs))\ninference_model = tf.keras.Model(inputs=inference_inputs,\n                                 outputs=inference_outputs)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:43:48.734294Z","iopub.execute_input":"2022-05-24T04:43:48.735811Z","iopub.status.idle":"2022-05-24T04:43:49.068522Z","shell.execute_reply.started":"2022-05-24T04:43:48.735771Z","shell.execute_reply":"2022-05-24T04:43:49.067822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Test Dataset\ntest_ds = df_to_dataset(test, num_columns,\n                        target=None,\n                        shuffle=False,\n                        repeat=False,\n                        batch_size=batch_size,\n                        drop_remainder=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:43:49.069655Z","iopub.execute_input":"2022-05-24T04:43:49.069912Z","iopub.status.idle":"2022-05-24T04:43:49.095833Z","shell.execute_reply.started":"2022-05-24T04:43:49.06988Z","shell.execute_reply":"2022-05-24T04:43:49.095187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Inference and submission\nprobas = inference_model.predict(test_ds)\nprobas = np.squeeze(probas)\n\npreds = np.where(probas > 0.5, True, False)\n\nsubmission_df['Transported'] = preds\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df.head()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T04:43:49.09716Z","iopub.execute_input":"2022-05-24T04:43:49.097548Z","iopub.status.idle":"2022-05-24T04:43:49.827334Z","shell.execute_reply.started":"2022-05-24T04:43:49.09751Z","shell.execute_reply":"2022-05-24T04:43:49.826602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"7\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>7. Cross Validation and Ensebmling</center></h1>","metadata":{}},{"cell_type":"code","source":"if exp_config['cross_validation']:\n    ## Settings for Training\n    epochs = exp_config['train_epochs']\n    batch_size = exp_config['batch_size']\n    steps_per_epoch = len(train)//batch_size\n    \n    submission_df['probas_mean'] = 0.\n    \n    ## Create cross validation samples\n    for fold in range(exp_config['n_splits']):\n        valid_fold = train.query(f'k_folds == {fold}').reset_index(drop=True)\n        train_fold = train.query(f'k_folds != {fold}').reset_index(drop=True)\n        \n        ## Create datasets    \n        train_ds = df_to_dataset(train_fold, num_columns,\n                         target='Transported',\n                         shuffle=True,\n                         repeat=False,\n                         batch_size=batch_size,\n                         drop_remainder=True)\n        \n        valid_ds = df_to_dataset(valid_fold, num_columns,\n                         target='Transported',\n                         shuffle=False,\n                         repeat=False,\n                         batch_size=batch_size,\n                         drop_remainder=True)\n        \n        ## Create preprocessing model\n        preprocessing_model, lookup_layers = create_preprocessing_model(numerical_columns,\n                                                             categorical_columns,\n                                                             num_columns,\n                                                             input_dtypes,\n                                                             train_fold)\n        \n        ## Apply the preprocessing model in tf.data.Dataset.map\n        train_ds = train_ds.map(lambda x, y: (preprocessing_model(x), y),\n                                num_parallel_calls=tf.data.AUTOTUNE)\n        valid_ds = valid_ds.map(lambda x, y: (preprocessing_model(x), y),\n                                num_parallel_calls=tf.data.AUTOTUNE)\n        \n        ## Re-construct the training model\n        training_model = create_tabtransformer(num_transformer_blocks,\n                                       num_heads,\n                                       cat_embedding_dim,\n                                       tf_dropout_rates,\n                                       ff_dropout_rates,\n                                       mlp_dropout_rates,\n                                       mlp_hidden_units_factors,\n                                       numerical_columns,\n                                       categorical_columns,\n                                       num_columns,\n                                       input_dtypes,\n                                       lookup_layers)\n        \n        ## Model compile\n        learning_rate = exp_config['learning_rate']\n        weight_decay = exp_config['weight_decay']\n        \n        learning_schedule = tf.keras.optimizers.schedules.CosineDecay(\n            initial_learning_rate=learning_rate,\n            decay_steps=epochs*steps_per_epoch, \n            alpha=0.0)\n        \n        optimizer = tfa.optimizers.AdamW(\n            learning_rate=learning_schedule,\n            weight_decay=weight_decay)\n        \n        loss_fn = keras.losses.BinaryCrossentropy(\n            from_logits=False, \n            label_smoothing=model_config['label_smoothing'])\n        \n        training_model.compile(\n            optimizer=optimizer,\n            loss=loss_fn,\n            metrics=['accuracy', keras.metrics.AUC()])\n        \n        ## Checkpoint callback\n        checkpoint_filepath = f\"./tmp/model_{fold}/exp.ckpt\"\n        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n            filepath=checkpoint_filepath, \n            save_weights_only=True, \n            monitor='val_loss', \n            mode='min', \n            save_best_only=True)\n        \n        ## Model training\n        history = training_model.fit(train_ds,\n                                     epochs=epochs,\n                                     shuffle=True,\n                                     validation_data=valid_ds,\n                                     callbacks=[model_checkpoint_callback],\n                                     verbose=0)\n        \n        ## Plot the train and valid losses\n        hist = pd.DataFrame(history.history)\n        plot_history(hist, title=f'fold: {fold}')\n        \n        ## Load the best parameters\n        training_model.load_weights(checkpoint_filepath)\n        \n        ## Test Dataset \n        test_ds = df_to_dataset(test, num_columns,\n                        target=None,\n                        shuffle=False,\n                        repeat=False,\n                        batch_size=batch_size,\n                        drop_remainder=False)\n        test_ds = test_ds.map(lambda x: preprocessing_model(x),\n                        num_parallel_calls=tf.data.AUTOTUNE)\n        \n        ## Inference\n        probas = training_model.predict(test_ds)\n        probas = np.squeeze(probas)\n        submission_df[f'probas_{fold}'] = probas\n        submission_df['probas_mean'] += probas\n    \n    ## Ensebmle the inferences of cvs\n    submission_df['probas_mean'] /= exp_config['n_splits']\n    probas_mean = submission_df['probas_mean'].values\n    preds = np.where(probas_mean > 0.5, True, False)\n    submission_df['Transported'] = preds\n    \n    ## Create Submission file\n    for fold in range(exp_config['n_splits']):\n        submission_df = submission_df.drop([f'probas_{fold}'], axis=1)\n    submission_df = submission_df.drop(['probas_mean'], axis=1)\n    submission_df.to_csv('submission_cv.csv', index=False)\n    submission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T04:43:49.828731Z","iopub.execute_input":"2022-05-24T04:43:49.828965Z","iopub.status.idle":"2022-05-24T04:43:49.849276Z","shell.execute_reply.started":"2022-05-24T04:43:49.828931Z","shell.execute_reply":"2022-05-24T04:43:49.84842Z"},"trusted":true},"execution_count":null,"outputs":[]}]}