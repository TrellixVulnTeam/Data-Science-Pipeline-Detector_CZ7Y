{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Let's build a Decision Tree and a Random Forest from scratch!\nThat will be awsome. We don't need any stinkin' Pandas. And we want to implement our own classifier/regressor so we don't want any Scikit-Learn either. (sklearn doesn't stink though - It is actually a useful piece of software) This is of course written for the learning. Other software developers has already implemeted decision trees probably better than me a long time ago. However implemetning this make me sure that I do indeed understand the process.\n\nThe Random Forest is also very simple implemented, and the improvents of my code is more or less just waiting be implemented - say for instance threading in the tree generation of a forest. \n\n* No Pandas\n* No Scikit learn\n* ... and **very** slow...","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-31T12:41:21.985189Z","iopub.execute_input":"2022-03-31T12:41:21.985665Z","iopub.status.idle":"2022-03-31T12:41:22.01741Z","shell.execute_reply.started":"2022-03-31T12:41:21.985552Z","shell.execute_reply":"2022-03-31T12:41:22.015705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport csv\nfrom collections import defaultdict\n\nINFILE = \"/kaggle/input/spaceship-titanic/train.csv\"\n\ndef is_float(string):\n    try:\n        float(string)\n        return True\n    except ValueError:\n        return False\n\ndef string_to_float( s , default=0 ):\n    # This accepts error like \"N/A\", \"\", \"nan\" and so on...\n    try:\n        f = float(s)\n        return f\n    except ValueError:\n        return float(default)\n\ntrain_data = defaultdict(list)\nwith open(INFILE) as f:\n    reader = csv.DictReader(f, delimiter=',') # read rows into a dictionary format\n    for row in reader: # read a row as {column1: value1, column2: value2,...}\n        for (k,v) in row.items(): # go over each column name and value\n            train_data[k].append(v) # append the value into the appropriate list\n                                 # based on column name k","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:41:37.055725Z","iopub.execute_input":"2022-03-31T12:41:37.056285Z","iopub.status.idle":"2022-03-31T12:41:37.190912Z","shell.execute_reply.started":"2022-03-31T12:41:37.056224Z","shell.execute_reply":"2022-03-31T12:41:37.190307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We don't need the ID\ntrain_data.pop(\"PassengerId\", None)\n\n# split Cabin\nfor elem in train_data[\"Cabin\"]:\n    try:\n        a,b,c = elem.split('/')\n    except:\n        a,b,c = [\"unknown\"] * 3\n    train_data[\"Cabin_a\"].append(a)\n    train_data[\"Cabin_b\"].append(b)\n    train_data[\"Cabin_c\"].append(c)\n\n# split name\nfor elem in train_data[\"Name\"]:\n    try:\n        fname, lname = elem.split()\n    except:\n        fname, lname = [\"unknown\"] * 2\n    train_data[\"FirstName\"].append(fname)\n    train_data[\"LastName\"].append(lname)\n    \n# drop the original columns of 'Name' and 'Cabin'\ntrain_data.pop(\"Cabin\", None)\ntrain_data.pop(\"Name\", None)\nprint(\" \".join(train_data.keys()))","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:41:44.873688Z","iopub.execute_input":"2022-03-31T12:41:44.874021Z","iopub.status.idle":"2022-03-31T12:41:44.905134Z","shell.execute_reply.started":"2022-03-31T12:41:44.873982Z","shell.execute_reply":"2022-03-31T12:41:44.904025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def is_feature_categorical(arr):\n    arr = np.array(arr)\n    unique_val = np.unique(arr)    \n    if (not is_float(unique_val[1])) or (len(unique_val) <= 15):\n        return True\n    else:\n        return False     \n\nis_categorical = { k: is_feature_categorical(v)  for (k,v) in train_data.items() }\n\n# convert the lists to numpy arrays. \nfor k,v in train_data.items():\n    train_data[k] = np.array([ string_to_float( x, default=-1 ) for x in v]) if not is_categorical[k] else np.array(v)\n\n# del is_categorical[target_col]\ntrain_data = dict(train_data)  # make it a normal dictionary\n\ndef info( data, target ):\n    print(\"Features:\")\n    for feat in data.keys():\n        if feat == target: continue\n        print(\"  {:20} => {:15}\".format(feat, \"Categorical\" if is_categorical[feat] else \"Numerical\" ), end='')\n        if is_categorical[feat]:\n            uniq, c = np.unique(data[feat], return_counts=True)            \n            print( \"Cardinality: {:5} Mode: {:12}  Modecount: {:5}\".format(len(uniq), data[feat][c.argmax()], c.max() ))\n        else:\n            arr = data[feat]\n            print(\"Min: {}  Max: {}  Mean: {:5.1f}\".format(arr.min(), arr.max(), arr.mean()) )\n    print(\"Target: \", target)\n    if is_categorical[target]:\n        print(\"  Classification task. Classifing to {} classes. Classes are:\\n  * \".format(len(np.unique(data[target]))), end='')\n        print(\"\\n  * \".join(np.unique(data[target])))\n    else:\n        print(\"  Regression task. Target: {}\".format(target))","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:42:16.209131Z","iopub.execute_input":"2022-03-31T12:42:16.209989Z","iopub.status.idle":"2022-03-31T12:42:16.359992Z","shell.execute_reply.started":"2022-03-31T12:42:16.209931Z","shell.execute_reply":"2022-03-31T12:42:16.359094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_col = \"Transported\"\ninfo( train_data, target_col)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:42:20.427364Z","iopub.execute_input":"2022-03-31T12:42:20.427651Z","iopub.status.idle":"2022-03-31T12:42:20.455632Z","shell.execute_reply.started":"2022-03-31T12:42:20.42762Z","shell.execute_reply":"2022-03-31T12:42:20.454802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k,v in train_data.items():\n    print(\"{:20} length: {}  Unique count: {}\".format(k, len(v), len(np.unique(v))))","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:42:29.198594Z","iopub.execute_input":"2022-03-31T12:42:29.199418Z","iopub.status.idle":"2022-03-31T12:42:29.225594Z","shell.execute_reply.started":"2022-03-31T12:42:29.199378Z","shell.execute_reply":"2022-03-31T12:42:29.224599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make some usable functions","metadata":{}},{"cell_type":"code","source":"def is_pure(data):\n    return len(np.unique(data))==1","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:46:42.713122Z","iopub.execute_input":"2022-03-31T12:46:42.713447Z","iopub.status.idle":"2022-03-31T12:46:42.718437Z","shell.execute_reply.started":"2022-03-31T12:46:42.713411Z","shell.execute_reply":"2022-03-31T12:46:42.717528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is actually th mode of a (categorical) column\ndef classify(data):\n    u, c = np.unique(data, return_counts=True)    \n    return u[c.argmax()]","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:46:43.099139Z","iopub.execute_input":"2022-03-31T12:46:43.099916Z","iopub.status.idle":"2022-03-31T12:46:43.103993Z","shell.execute_reply.started":"2022-03-31T12:46:43.099876Z","shell.execute_reply":"2022-03-31T12:46:43.10332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_split_values(data, target):\n    split_vals = {}        \n    for col,arr in data.items():\n        if col == target: continue\n        if is_categorical[col]: continue\n        values = np.unique(arr)\n        split_vals[col] = np.array([(x1+x2)/2 for x1,x2 in zip(values[:-1], values[1:])])\n\n    return split_vals","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:46:43.50639Z","iopub.execute_input":"2022-03-31T12:46:43.506893Z","iopub.status.idle":"2022-03-31T12:46:43.51473Z","shell.execute_reply.started":"2022-03-31T12:46:43.506854Z","shell.execute_reply":"2022-03-31T12:46:43.513709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def entropy(data):\n    _, counts = np.unique(data, return_counts=True)\n    p = counts / counts.sum()\n    return - (p * np.log2(p)).sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:46:44.017997Z","iopub.execute_input":"2022-03-31T12:46:44.018523Z","iopub.status.idle":"2022-03-31T12:46:44.024944Z","shell.execute_reply.started":"2022-03-31T12:46:44.018486Z","shell.execute_reply":"2022-03-31T12:46:44.024179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def overall_entropy(lhs, rhs): \n    tot = lhs.shape[0] + rhs.shape[0]\n    p_lhs = lhs.shape[0] / tot\n    p_rhs = rhs.shape[0] / tot\n    return (p_lhs * entropy(lhs)) + (p_rhs * entropy(rhs))","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:46:44.465048Z","iopub.execute_input":"2022-03-31T12:46:44.466116Z","iopub.status.idle":"2022-03-31T12:46:44.471844Z","shell.execute_reply.started":"2022-03-31T12:46:44.466066Z","shell.execute_reply":"2022-03-31T12:46:44.470707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stddev( a ):\n    return a.var() if a.shape[0] != 0 else 0.0\n        \ndef weighted_mean_squared_error(lhs, rhs):\n    tot = lhs.shape[0] + rhs.shape[0]\n    p_lhs = lhs.shape[0] / tot\n    p_rhs = rhs.shape[0] / tot\n    # I don't understand why, but it seems to work better with std.dev than variance?\n    return ( p_lhs * stddev(lhs) ) + ( p_rhs * stddev(rhs))","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:46:45.43459Z","iopub.execute_input":"2022-03-31T12:46:45.435076Z","iopub.status.idle":"2022-03-31T12:46:45.44269Z","shell.execute_reply.started":"2022-03-31T12:46:45.435039Z","shell.execute_reply":"2022-03-31T12:46:45.441664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_best_split(data, potential_splits, target):\n    # I have to calculate the best split without actually doing the split.\n    # This will be interesting, indeed.\n\n    criterion = overall_entropy if is_categorical[target] else weighted_mean_squared_error\n\n    best_overall_entropy = 999999999999999999999\n    for col,arr in data.items():\n        if col == target: continue\n        \n        if is_categorical[col]:\n            iter = np.unique(data[col])            \n        else: \n            iter = potential_splits[col]\n\n        for value in iter:\n            if is_categorical[col]:\n                l_slice = arr == value\n                r_slice = arr != value\n            else:\n                l_slice = arr <= value\n                r_slice = arr  > value\n\n            d_left  = data[target][l_slice]\n            d_right = data[target][r_slice]\n\n            # if it is no split this is not the best split obviously.\n            if d_left.shape[0]==0 or d_right.shape[0]==0:\n                continue\n\n            c_entropy = criterion(d_left, d_right)\n\n            if c_entropy <= best_overall_entropy:\n                best_overall_entropy = c_entropy\n                best_split_column = col\n                best_split_value = value\n\n    #print( best_split_column, best_split_value, best_overall_entropy )\n    return best_split_column, best_split_value","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:46:47.247061Z","iopub.execute_input":"2022-03-31T12:46:47.247404Z","iopub.status.idle":"2022-03-31T12:46:47.258455Z","shell.execute_reply.started":"2022-03-31T12:46:47.247365Z","shell.execute_reply":"2022-03-31T12:46:47.257462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_data( data, column, value):\n    \n    if is_categorical[column]:\n        l_slice = data[column] == value\n        r_slice = data[column] != value\n    else:\n        l_slice = data[column] <= value\n        r_slice = data[column] >  value\n        \n    l_data = { k:v[l_slice] for (k,v) in data.items()}\n    r_data = { k:v[r_slice] for (k,v) in data.items()}\n    return l_data, r_data","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:46:48.505338Z","iopub.execute_input":"2022-03-31T12:46:48.506291Z","iopub.status.idle":"2022-03-31T12:46:48.51416Z","shell.execute_reply.started":"2022-03-31T12:46:48.506184Z","shell.execute_reply":"2022-03-31T12:46:48.512912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define our two classes - Decison and DecisionTree\n`Decision` is defined by namedtuple. `DecisionTree` is a real class.\n","metadata":{}},{"cell_type":"code","source":"from collections import namedtuple\nDecision = namedtuple(\"Decision\", [\"column\", \"value\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:46:50.410614Z","iopub.execute_input":"2022-03-31T12:46:50.411065Z","iopub.status.idle":"2022-03-31T12:46:50.414868Z","shell.execute_reply.started":"2022-03-31T12:46:50.41101Z","shell.execute_reply":"2022-03-31T12:46:50.414217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DecisionTree(object):\n    def __init__(self, data, target, decision=None):\n        self.data = data\n        self.target = target\n        self.decision = decision\n        self.left = None\n        self.right = None\n\n    # Discuss: Maybe this method could be renamed 'fit' to mimic the sklearn interface?\n    def build_tree(self, depth=0, leaf_size_limit=6, max_depth=100):\n        tgt_col = self.data[self.target]\n        # base cases\n        if is_pure(tgt_col):\n            return classify(tgt_col) if is_categorical[self.target] else tgt_col.mean()\n\n        if tgt_col.shape[0] < leaf_size_limit:\n            return classify(tgt_col) if is_categorical[self.target] else tgt_col.mean()\n\n        if depth > max_depth:\n            return classify(tgt_col) if is_categorical[self.target] else tgt_col.mean()\n\n        # recurse\n        sv = find_split_values(self.data, self.target)\n        self.decision = Decision( *find_best_split(self.data, sv, self.target) )        \n        l_data, r_data = split_data(self.data, *self.decision )\n        self.left = DecisionTree( l_data, self.target )\n        self.right = DecisionTree( r_data, self.target )\n        self.left.build_tree ( depth=depth+1, leaf_size_limit=leaf_size_limit, max_depth=max_depth )\n        self.right.build_tree( depth=depth+1, leaf_size_limit=leaf_size_limit, max_depth=max_depth )        \n\n    def predict(self, sample):\n        tgt_col = self.data[self.target]\n\n        # Pure or leaf\n        if  (self.decision is None) or is_pure(tgt_col):\n            return classify(tgt_col) if is_categorical[self.target] else tgt_col.mean()\n        \n        col, val = self.decision\n\n        if is_categorical[col]:\n            return self.left.predict( sample ) if sample[col] == val else self.right.predict( sample )\n        else:\n            return self.left.predict( sample ) if sample[col] <= val else self.right.predict( sample )","metadata":{"execution":{"iopub.status.busy":"2022-03-31T12:46:52.858681Z","iopub.execute_input":"2022-03-31T12:46:52.859132Z","iopub.status.idle":"2022-03-31T12:46:52.874241Z","shell.execute_reply.started":"2022-03-31T12:46:52.859098Z","shell.execute_reply":"2022-03-31T12:46:52.873533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's try out the DecisionTree.\n\nIn version 1, I got a score 0.77928 w/o any cross validation or anything - just guessing some hyperparameters. Let's at least try to do some validation.","metadata":{}},{"cell_type":"code","source":"split_ratio = 0.10   # validation ratio. So with 0.10 we get 10% validation and 90% train\nn_samples = train_data[\"Transported\"].shape[0]\nn_train = int((1-split_ratio) * n_samples)\nn_valid = n_samples - n_train\nnew_train_data = {}\nvalidation_data = {}\nfor k,v in train_data.items():\n    new_train_data[k] = v[:n_train]\n    validation_data[k] = v[n_train:]","metadata":{"execution":{"iopub.status.busy":"2022-03-31T13:19:27.881614Z","iopub.execute_input":"2022-03-31T13:19:27.881897Z","iopub.status.idle":"2022-03-31T13:19:27.888651Z","shell.execute_reply.started":"2022-03-31T13:19:27.881866Z","shell.execute_reply":"2022-03-31T13:19:27.88778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This cell is used for experiments only.\ndo_not_run = True\nfor max_d in [20,22,24]:\n    if do_not_run: continue\n    my_dt = DecisionTree(new_train_data, target_col)\n    my_dt.build_tree(leaf_size_limit=53, max_depth=max_d)  # Takes some time.\n    \n    y_pred = []\n    y_real = []    \n    for i in range(n_valid):\n        sample = { k:v[i] for (k,v) in validation_data.items()}\n        y_pred.append(my_dt.predict(sample))\n        y_real.append( sample[target_col] )\n    accuracy_array = np.array(y_pred) == np.array(y_real)\n    print(\"Validation accuracy (max_depth={:2d}): {:5.5f} \".format(max_d, accuracy_array.mean()))","metadata":{"execution":{"iopub.status.busy":"2022-03-31T14:31:33.229332Z","iopub.execute_input":"2022-03-31T14:31:33.229969Z","iopub.status.idle":"2022-03-31T14:31:33.236866Z","shell.execute_reply.started":"2022-03-31T14:31:33.229929Z","shell.execute_reply":"2022-03-31T14:31:33.236285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After some experiments it looks like this dataset validates best at about 53 leafnode limit and a depth limit of 20.","metadata":{}},{"cell_type":"markdown","source":"## Let's make a very simple Random Forest.","metadata":{}},{"cell_type":"code","source":"class RandomForest(object):\n    def __init__(self, n_trees=10):\n        self.n_trees = n_trees\n        self._trees = []\n        \n    def fit( self, data, target, leaf_size_limit=6, max_depth=100):\n        self.data = data\n        self.target = target\n        n_total_samples = data[target].shape[0]\n        # I guess this loop can be threaded?\n        for i in range(self.n_trees):\n            idx = np.unique(np.random.randint(n_total_samples, size=(n_total_samples)))\n            slicer = np.eye(n_total_samples)[idx].sum(axis=0).astype(bool)\n            #print(slicer.mean())  #debug\n            this_tree_data = { k:v[slicer] for (k,v) in data.items() }\n            dt = DecisionTree( this_tree_data, target )\n            print('Building tree: {:4d} / {:4d}.'.format(i+1,self.n_trees), end='')\n            dt.build_tree( leaf_size_limit=leaf_size_limit, max_depth=max_depth )\n            print('', end='\\r')\n            self._trees.append( dt )\n        print(\"Done!\")          \n        \n    def predict( self, sample ):\n        arr = np.array( [ dt.predict(sample) for dt in self._trees ])        \n        if is_categorical[self.target]:\n            return classify(arr)\n        else:\n            return arr.mean()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T14:38:56.178784Z","iopub.execute_input":"2022-03-31T14:38:56.179307Z","iopub.status.idle":"2022-03-31T14:38:56.192011Z","shell.execute_reply.started":"2022-03-31T14:38:56.17924Z","shell.execute_reply":"2022-03-31T14:38:56.191265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Try it out with the train data.","metadata":{}},{"cell_type":"code","source":"rf = RandomForest(n_trees=3) # Pick an odd number so the tally of trees doesn't end in a tie.\nrf.fit( train_data, target_col, leaf_size_limit=53, max_depth=19 )","metadata":{"execution":{"iopub.status.busy":"2022-03-31T14:38:58.674426Z","iopub.execute_input":"2022-03-31T14:38:58.674887Z","iopub.status.idle":"2022-03-31T14:46:57.550123Z","shell.execute_reply.started":"2022-03-31T14:38:58.674834Z","shell.execute_reply":"2022-03-31T14:46:57.549282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read the test data","metadata":{}},{"cell_type":"code","source":"INFILE = \"/kaggle/input/spaceship-titanic/test.csv\"\n\ntest_data = defaultdict(list)\nwith open(INFILE) as f:\n    reader = csv.DictReader(f, delimiter=',') # read rows into a dictionary format\n    for row in reader: # read a row as {column1: value1, column2: value2,...}\n        for (k,v) in row.items(): # go over each column name and value\n            test_data[k].append(v) # append the value into the appropriate list\n                                 # based on column name k\n# split Cabin\nfor elem in test_data[\"Cabin\"]:\n    try:\n        a,b,c = elem.split('/')\n    except:\n        a,b,c = [\"unknown\"] * 3\n    test_data[\"Cabin_a\"].append(a)\n    test_data[\"Cabin_b\"].append(b)\n    test_data[\"Cabin_c\"].append(c)\n\n# split name\nfor elem in test_data[\"Name\"]:\n    try:\n        fname, lname = elem.split()\n    except:\n        fname, lname = [\"unknown\"] * 2\n    test_data[\"FirstName\"].append(fname)\n    test_data[\"LastName\"].append(lname)\n    \n# drop the original columns of 'Name' and 'Cabin'\ntest_data.pop(\"Cabin\", None)\ntest_data.pop(\"Name\", None)\nprint(\" \".join(test_data.keys()))\n\n# convert the lists to numpy arrays.\nis_categorical[\"PassengerId\"] = True\nfor k,v in test_data.items():\n    test_data[k] = np.array([ string_to_float( x, default=-1 ) for x in v]) if not is_categorical[k] else np.array(v)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:38:06.955668Z","iopub.execute_input":"2022-03-31T11:38:06.955967Z","iopub.status.idle":"2022-03-31T11:38:07.070319Z","shell.execute_reply.started":"2022-03-31T11:38:06.955936Z","shell.execute_reply":"2022-03-31T11:38:07.069404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heading = open(\"/kaggle/input/spaceship-titanic/sample_submission.csv\").readlines()[0]","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:38:09.106445Z","iopub.execute_input":"2022-03-31T11:38:09.106707Z","iopub.status.idle":"2022-03-31T11:38:09.111592Z","shell.execute_reply.started":"2022-03-31T11:38:09.106679Z","shell.execute_reply":"2022-03-31T11:38:09.110964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"submission.csv\", \"w\") as f:\n    f.write(heading)\n    for i in range(test_data[\"PassengerId\"].shape[0]):\n        p = rf.predict( { k:v[i] for (k,v) in test_data.items() } )\n        f.write(\"{},{}\\n\".format(test_data[\"PassengerId\"][i], p))","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:38:11.227703Z","iopub.execute_input":"2022-03-31T11:38:11.228123Z","iopub.status.idle":"2022-03-31T11:39:14.861032Z","shell.execute_reply.started":"2022-03-31T11:38:11.228091Z","shell.execute_reply":"2022-03-31T11:39:14.860081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}