{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-22T02:54:33.424994Z","iopub.execute_input":"2022-05-22T02:54:33.425612Z","iopub.status.idle":"2022-05-22T02:54:33.438063Z","shell.execute_reply.started":"2022-05-22T02:54:33.425565Z","shell.execute_reply":"2022-05-22T02:54:33.43689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/spaceship-titanic/train.csv\")\ndata2 = pd.read_csv(\"../input/spaceship-titanic/test.csv\")\ndf = [data,data2]\ndata.head()\ndf[0].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T02:54:33.622748Z","iopub.execute_input":"2022-05-22T02:54:33.62325Z","iopub.status.idle":"2022-05-22T02:54:33.693542Z","shell.execute_reply.started":"2022-05-22T02:54:33.623201Z","shell.execute_reply":"2022-05-22T02:54:33.692279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data cleaning\n\nimport seaborn as sns\nimport math\nfrom sklearn import preprocessing\n\n#replacing NA cabin info\ndf[0]['Cabin'].fillna('C/2/P',inplace=True)\ndf[1]['Cabin'].fillna('C/2/P',inplace=True)\n\n#replacing NA age info\ndf[0]['Age'].fillna(df[0]['Age'].mean(),inplace=True)\ndf[1]['Age'].fillna(df[1]['Age'].mean(),inplace=True)\n\n#splitting cabin into various fields\ndf[0][['Deck','Cabin Number','Ship Side']] = (df[0]['Cabin'].str.split('/',expand=True))\ndf[1][['Deck','Cabin Number','Ship Side']] = (df[1]['Cabin'].str.split('/',expand=True))\n\n#split last name from first and then assign an int value to last name to see if there is a corrilation with surviving\ndf[0][['First Name','Last Name']] = (df[0]['Name'].str.split(expand=True))\ndf[1][['First Name','Last Name']] = (df[1]['Name'].str.split(expand=True))\n\n#removing NA values in money related columns by assuming spent money is 0\ncols_to_process = ['RoomService','ShoppingMall','FoodCourt','Spa','VRDeck']\nfor i in df:\n    for x in cols_to_process:\n        i[x].fillna(0,inplace=True)\n\n\n#adding a total spent column\na = df[0]['RoomService'].values\nb = df[0]['ShoppingMall'].values\nc = df[0]['FoodCourt'].values\nd = df[0]['Spa'].values\ne = df[0]['VRDeck'].values\ndf[0].insert(10, 'TotalSpent', a+b+c+d+e)\n\na = df[1]['RoomService'].values\nb = df[1]['ShoppingMall'].values\nc = df[1]['FoodCourt'].values\nd = df[1]['Spa'].values\ne = df[1]['VRDeck'].values\ndf[1].insert(10, 'TotalSpent', a+b+c+d+e)\n\n\n#replacing missing names\n\n\n\n#dropping cols that will not be used\nPID = df[1]['PassengerId']\ndf[0].drop(['PassengerId'],axis=1,inplace=True)\ndf[1].drop(['PassengerId'],axis=1,inplace=True)\n\ndf[0].drop(['First Name'],axis=1,inplace=True)\ndf[1].drop(['First Name'],axis=1,inplace=True)\n\ndf[0].drop(['Cabin'],axis=1,inplace=True)\ndf[1].drop(['Cabin'],axis=1,inplace=True)\n\ndf[0].drop(['Name'],axis=1,inplace=True)\ndf[1].drop(['Name'],axis=1,inplace=True)\n\n\n\ncols_to_process = ['VIP', 'CryoSleep', 'Destination', 'HomePlanet', 'Deck', 'Cabin Number', 'Ship Side', 'Last Name']\nle = preprocessing.LabelEncoder()\nfor i in df:\n    for col in cols_to_process:\n        i[col] = le.fit_transform(i[col])\n\n\nimport matplotlib.pyplot as plt\n#plt.figure(figsize=(16,10))\n#heatmap = sns.heatmap(data.corr()[['Transported']].sort_values(by='Transported',ascending=False), vmin=-1, vmax=1, annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T02:54:33.807975Z","iopub.execute_input":"2022-05-22T02:54:33.809016Z","iopub.status.idle":"2022-05-22T02:54:33.986804Z","shell.execute_reply.started":"2022-05-22T02:54:33.808965Z","shell.execute_reply":"2022-05-22T02:54:33.985534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16, 8))\nnumeric_cols = df[0].select_dtypes(include=np.number).columns.tolist()\nsns.heatmap(df[0][numeric_cols + [\"Transported\"]].corr(), annot=True, ax=ax, \n            xticklabels=numeric_cols + [\"Transported\"],\n            yticklabels=numeric_cols + [\"Transported\"]);","metadata":{"execution":{"iopub.status.busy":"2022-05-22T02:54:33.989556Z","iopub.execute_input":"2022-05-22T02:54:33.989845Z","iopub.status.idle":"2022-05-22T02:54:35.343847Z","shell.execute_reply.started":"2022-05-22T02:54:33.989814Z","shell.execute_reply":"2022-05-22T02:54:35.342825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Expenditure features\n\"\"\"\nexp_feats=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\nimport matplotlib.pyplot as plt\n# Plot expenditure features\nfig=plt.figure(figsize=(10,20))\nfor i, var_name in enumerate(exp_feats):\n    # Left plot\n    ax=fig.add_subplot(5,2,2*i+1)\n    sns.histplot(data=df[0], x=var_name, axes=ax, bins=30, kde=False, hue='Transported')\n    ax.set_title(var_name)\n\n    # Right plot (truncated)\n    ax=fig.add_subplot(5,2,2*i+2)\n    sns.histplot(data=df[0], x=var_name, axes=ax, bins=30, kde=True, hue='Transported')\n    plt.ylim([0,100])\n    ax.set_title(var_name)\nfig.tight_layout()  # Improves appearance a bit\nplt.show()\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-05-22T02:54:35.345731Z","iopub.execute_input":"2022-05-22T02:54:35.346427Z","iopub.status.idle":"2022-05-22T02:54:35.354666Z","shell.execute_reply.started":"2022-05-22T02:54:35.346366Z","shell.execute_reply":"2022-05-22T02:54:35.353732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[0].head()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T02:54:35.356485Z","iopub.execute_input":"2022-05-22T02:54:35.357455Z","iopub.status.idle":"2022-05-22T02:54:35.386323Z","shell.execute_reply.started":"2022-05-22T02:54:35.357394Z","shell.execute_reply":"2022-05-22T02:54:35.385284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#splitting training/test data\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(df[0].drop(['Transported'], axis=1), df[0]['Transported'].values)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T02:54:35.389026Z","iopub.execute_input":"2022-05-22T02:54:35.390433Z","iopub.status.idle":"2022-05-22T02:54:35.403799Z","shell.execute_reply.started":"2022-05-22T02:54:35.390375Z","shell.execute_reply":"2022-05-22T02:54:35.40257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#basic nn model\nimport os\nimport cv2\nimport tensorflow as tf\ntf.random.set_seed(15)\nNNM = tf.keras.models.Sequential()\n    \nNNM.add(tf.keras.layers.Dense(15, activation='relu'))\n\nNNM.add(tf.keras.layers.Dense(128, activation='relu'))\n\nNNM.add(tf.keras.layers.Dense(64, activation='relu'))\n\nNNM.add(tf.keras.layers.Dense(32, activation='relu'))\n\nNNM.add(tf.keras.layers.Dense(16, activation='relu'))\n\nNNM.add(tf.keras.layers.Dense(2, activation='softmax'))\n    \nNNM.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nNNM.fit(X_train, Y_train, epochs=10)\nNNM.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T02:54:35.405525Z","iopub.execute_input":"2022-05-22T02:54:35.40586Z","iopub.status.idle":"2022-05-22T02:54:44.907465Z","shell.execute_reply.started":"2022-05-22T02:54:35.405824Z","shell.execute_reply":"2022-05-22T02:54:44.906511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, Y_train)\nY_pred = classifier.predict(X_test)\ncm = confusion_matrix(Y_test, Y_pred)\nsvm_acc = accuracy_score(Y_test,Y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T02:54:44.909288Z","iopub.execute_input":"2022-05-22T02:54:44.909859Z","iopub.status.idle":"2022-05-22T02:54:47.481759Z","shell.execute_reply.started":"2022-05-22T02:54:44.909803Z","shell.execute_reply":"2022-05-22T02:54:47.480478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(max_iter=4000).fit(X_train, Y_train)\na = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nlog_acc = accuracy_score(Y_test, a)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T02:54:47.485339Z","iopub.execute_input":"2022-05-22T02:54:47.485758Z","iopub.status.idle":"2022-05-22T02:54:48.745294Z","shell.execute_reply.started":"2022-05-22T02:54:47.485709Z","shell.execute_reply":"2022-05-22T02:54:48.744052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nbayes_model = GaussianNB().fit(X_train, Y_train)\nnaive_pred = bayes_model.predict(X_test)\nnaive_acc = accuracy_score(Y_test, naive_pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T02:54:48.752133Z","iopub.execute_input":"2022-05-22T02:54:48.755409Z","iopub.status.idle":"2022-05-22T02:54:48.778576Z","shell.execute_reply.started":"2022-05-22T02:54:48.755325Z","shell.execute_reply":"2022-05-22T02:54:48.777512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn_model = KNeighborsClassifier().fit(X_train, Y_train)\nknn_pred = knn_model.predict(X_test)\nknn_acc = accuracy_score(Y_test, knn_pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T02:54:48.78489Z","iopub.execute_input":"2022-05-22T02:54:48.788084Z","iopub.status.idle":"2022-05-22T02:54:48.9528Z","shell.execute_reply.started":"2022-05-22T02:54:48.787992Z","shell.execute_reply":"2022-05-22T02:54:48.952014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nforest_model = RandomForestClassifier(random_state=0, n_estimators=475, class_weight='balanced').fit(X_train, Y_train)\nforest_pred = forest_model.predict(X_test)\nforest_acc = accuracy_score(Y_test, forest_pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T02:54:48.955694Z","iopub.execute_input":"2022-05-22T02:54:48.956048Z","iopub.status.idle":"2022-05-22T02:54:54.150033Z","shell.execute_reply.started":"2022-05-22T02:54:48.956003Z","shell.execute_reply":"2022-05-22T02:54:54.148863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nXGB_model = XGBClassifier(n_estimators=230, seed=0, scale_pos_weight=1.5).fit(X_train, Y_train)\nXGB_pred = XGB_model.predict(X_test)\nXGB_acc = accuracy_score(Y_test, XGB_pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T02:54:54.151557Z","iopub.execute_input":"2022-05-22T02:54:54.151851Z","iopub.status.idle":"2022-05-22T02:54:56.162785Z","shell.execute_reply.started":"2022-05-22T02:54:54.151818Z","shell.execute_reply":"2022-05-22T02:54:56.162077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('logicistic: ', log_acc)\nprint('naive_bayes: ', naive_acc)\nprint('svm: ', svm_acc)\nprint('knn: ', knn_acc)\nprint('forest: ', forest_acc)\nprint('XGB: ', XGB_acc)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T02:54:56.164502Z","iopub.execute_input":"2022-05-22T02:54:56.165064Z","iopub.status.idle":"2022-05-22T02:54:56.174661Z","shell.execute_reply.started":"2022-05-22T02:54:56.165023Z","shell.execute_reply":"2022-05-22T02:54:56.173639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = forest_model = RandomForestClassifier(random_state=0, n_estimators=475, class_weight='balanced').fit(X_train, Y_train)\n\npred_final = model.predict(df[1])\n\nfinal_dict = {'PassengerId': PID, 'Transported':pred_final}\n\nsubmission_csv = pd.DataFrame(final_dict)\nsubmission_csv.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T03:05:31.43139Z","iopub.execute_input":"2022-05-22T03:05:31.431681Z","iopub.status.idle":"2022-05-22T03:05:36.715856Z","shell.execute_reply.started":"2022-05-22T03:05:31.43165Z","shell.execute_reply":"2022-05-22T03:05:36.714537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}