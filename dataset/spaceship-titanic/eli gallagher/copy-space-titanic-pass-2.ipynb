{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-22T22:15:01.051535Z","iopub.execute_input":"2022-05-22T22:15:01.051995Z","iopub.status.idle":"2022-05-22T22:15:01.05981Z","shell.execute_reply.started":"2022-05-22T22:15:01.051946Z","shell.execute_reply":"2022-05-22T22:15:01.059123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/spaceship-titanic/train.csv\")\ndata2 = pd.read_csv(\"../input/spaceship-titanic/test.csv\")\ndf = [data,data2]\ndata.head()\ndf[0].isna().sum()\n\n\n\n#data cleaning\n\nimport seaborn as sns\nimport math\nfrom sklearn import preprocessing\n\n#replacing NA age info\ndf[0]['Age'].fillna(df[0]['Age'].mean(),inplace=True)\ndf[1]['Age'].fillna(df[1]['Age'].mean(),inplace=True)\n\n#splitting cabin into various fields\ndf[0][['Deck','CabinNumber','ShipSide']] = (df[0]['Cabin'].str.split('/',expand=True))\ndf[1][['Deck','CabinNumber','ShipSide']] = (df[1]['Cabin'].str.split('/',expand=True))\n\n#replacing NA cabin info\ndf[0]['CabinNumber'] = df[0]['CabinNumber'].astype(float)\ndf[1]['CabinNumber'] = df[1]['CabinNumber'].astype(float)\n\ndf[0]['CabinNumber'].fillna(df[0]['CabinNumber'].mean(),inplace=True)\ndf[1]['CabinNumber'].fillna(df[1]['CabinNumber'].mean(),inplace=True)\n\ndf[0]['Deck'].fillna('C',inplace=True)\ndf[1]['Deck'].fillna('C',inplace=True)\n\ndf[0]['ShipSide'].fillna('C',inplace=True)\ndf[1]['ShipSide'].fillna('C',inplace=True)\n\n\n#split last name from first and then assign an int value to last name to see if there is a corrilation with surviving\ndf[0][['First Name','Last Name']] = (df[0]['Name'].str.split(expand=True))\ndf[1][['First Name','Last Name']] = (df[1]['Name'].str.split(expand=True))\n\n#removing NA values in money related columns by assuming spent money is 0\ncols_to_process = ['RoomService','ShoppingMall','FoodCourt','Spa','VRDeck']\nfor i in df:\n    for x in cols_to_process:\n        i[x].fillna(0,inplace=True)\n\n\n#adding a total spent column\na = df[0]['RoomService'].values\nb = df[0]['ShoppingMall'].values\nc = df[0]['FoodCourt'].values\nd = df[0]['Spa'].values\ne = df[0]['VRDeck'].values\ndf[0].insert(10, 'TotalSpent', a+b+c+d+e)\n\na = df[1]['RoomService'].values\nb = df[1]['ShoppingMall'].values\nc = df[1]['FoodCourt'].values\nd = df[1]['Spa'].values\ne = df[1]['VRDeck'].values\ndf[1].insert(10, 'TotalSpent', a+b+c+d+e)\n\n\n#replacing missing names\n\n\n\n#dropping cols that will not be used\nPID = df[1]['PassengerId']\ndf[0].drop(['PassengerId'],axis=1,inplace=True)\ndf[1].drop(['PassengerId'],axis=1,inplace=True)\n\ndf[0].drop(['First Name'],axis=1,inplace=True)\ndf[1].drop(['First Name'],axis=1,inplace=True)\n\ndf[0].drop(['Cabin'],axis=1,inplace=True)\ndf[1].drop(['Cabin'],axis=1,inplace=True)\n\ndf[0].drop(['Name'],axis=1,inplace=True)\ndf[1].drop(['Name'],axis=1,inplace=True)\n\n\n\ncols_to_process = ['VIP', 'CryoSleep', 'Destination', 'HomePlanet', 'Deck', 'CabinNumber', 'ShipSide', 'Last Name']\nle = preprocessing.LabelEncoder()\nfor i in df:\n    for col in cols_to_process:\n        i[col] = le.fit_transform(i[col])\n\n\nfor i in df:\n    i['Under18'] = i['Age'].apply(lambda x: 1 if x < 16 else 0)\n    \n#testing dropping the lower corr feilds #, 'CabinNumber', 'ShoppingMall', 'Age', 'FoodCourt', 'Last Name', 'ShipSide', 'TotalSpent'\ncol_for_drop = ['VIP']\nfor i in df:\n    for x in col_for_drop:\n        i.drop([x],axis=1,inplace=True)\n        \n        \n        \n#splitting training/test data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(df[0].drop(['Transported'], axis=1), df[0]['Transported'].values)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:15:01.061547Z","iopub.execute_input":"2022-05-22T22:15:01.062002Z","iopub.status.idle":"2022-05-22T22:15:01.232817Z","shell.execute_reply.started":"2022-05-22T22:15:01.061971Z","shell.execute_reply":"2022-05-22T22:15:01.231911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#splitting training/test data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(df[0].drop(['Transported'], axis=1), df[0]['Transported'].values)\n\n#SVC model\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, Y_train)\nY_pred = classifier.predict(X_test)\ncm = confusion_matrix(Y_test, Y_pred)\nsvm_acc = accuracy_score(Y_test,Y_pred)\n\n#Logistical reg\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(max_iter=4000).fit(X_train, Y_train)\na = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nlog_acc = accuracy_score(Y_test, a)\n\n#GNB\nfrom sklearn.naive_bayes import GaussianNB\nbayes_model = GaussianNB().fit(X_train, Y_train)\nnaive_pred = bayes_model.predict(X_test)\nnaive_acc = accuracy_score(Y_test, naive_pred)\n\n#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_model = KNeighborsClassifier().fit(X_train, Y_train)\nknn_pred = knn_model.predict(X_test)\nknn_acc = accuracy_score(Y_test, knn_pred)\n\n#RF\nfrom sklearn.ensemble import RandomForestClassifier\nforest_model = RandomForestClassifier(random_state=0, n_estimators=475, class_weight='balanced').fit(X_train, Y_train)\nforest_pred = forest_model.predict(X_test)\nforest_acc = accuracy_score(Y_test, forest_pred)\n\n#XGB (kinda like a tree)\nfrom xgboost import XGBClassifier\nXGB_model = XGBClassifier(n_estimators=230, seed=0, scale_pos_weight=1.5).fit(X_train, Y_train)\nXGB_pred = XGB_model.predict(X_test)\nXGB_acc = accuracy_score(Y_test, XGB_pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:15:01.234654Z","iopub.execute_input":"2022-05-22T22:15:01.235063Z","iopub.status.idle":"2022-05-22T22:15:10.510797Z","shell.execute_reply.started":"2022-05-22T22:15:01.23502Z","shell.execute_reply":"2022-05-22T22:15:10.510142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(df[0].drop(['Transported'], axis=1), df[0]['Transported'].values)\nimport os\nimport cv2\nimport tensorflow as tf\ntf.random.set_seed(15)\nNNM = tf.keras.models.Sequential()\n    \nNNM.add(tf.keras.layers.Dense(10, activation='relu'))\nNNM.add(tf.keras.layers.Dense(512, activation='relu'))\nNNM.add(tf.keras.layers.Dense(256, activation='relu'))\nNNM.add(tf.keras.layers.Dense(128, activation='relu'))\nNNM.add(tf.keras.layers.Dense(64, activation='relu'))\nNNM.add(tf.keras.layers.Dense(32, activation='relu'))\nNNM.add(tf.keras.layers.Dense(16, activation='relu'))\nNNM.add(tf.keras.layers.Dense(2, activation='softmax'))\n    \nNNM.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nNNM.fit(X_train, Y_train, epochs=7)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:15:10.512154Z","iopub.execute_input":"2022-05-22T22:15:10.512596Z","iopub.status.idle":"2022-05-22T22:15:17.426233Z","shell.execute_reply.started":"2022-05-22T22:15:10.512563Z","shell.execute_reply":"2022-05-22T22:15:17.425403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('logicistic: ', log_acc)\nprint('naive_bayes: ', naive_acc)\nprint('svm: ', svm_acc)\nprint('knn: ', knn_acc)\nprint('forest: ', forest_acc)\nprint('XGB: ', XGB_acc)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T22:15:17.428358Z","iopub.execute_input":"2022-05-22T22:15:17.428662Z","iopub.status.idle":"2022-05-22T22:15:17.437291Z","shell.execute_reply.started":"2022-05-22T22:15:17.428621Z","shell.execute_reply":"2022-05-22T22:15:17.436485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = forest_model = RandomForestClassifier(random_state=0, n_estimators=475, class_weight='balanced').fit(X_train, Y_train)\n\npred_final = model.predict(df[1])\n\nfinal_dict = {'PassengerId': PID, 'Transported':pred_final}\n\nsubmission_csv = pd.DataFrame(final_dict)\nsubmission_csv.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}