{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain_df = pd.read_csv('../input/spaceship-titanic/train.csv')\ntest_df = pd.read_csv('../input/spaceship-titanic/test.csv')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-06-03T04:35:51.077589Z","iopub.execute_input":"2022-06-03T04:35:51.078099Z","iopub.status.idle":"2022-06-03T04:35:51.173369Z","shell.execute_reply.started":"2022-06-03T04:35:51.078057Z","shell.execute_reply":"2022-06-03T04:35:51.17218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Filling Null Values**","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:35:51.175615Z","iopub.execute_input":"2022-06-03T04:35:51.176171Z","iopub.status.idle":"2022-06-03T04:35:51.212718Z","shell.execute_reply.started":"2022-06-03T04:35:51.176123Z","shell.execute_reply":"2022-06-03T04:35:51.211517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:35:51.214042Z","iopub.execute_input":"2022-06-03T04:35:51.214414Z","iopub.status.idle":"2022-06-03T04:35:51.251346Z","shell.execute_reply.started":"2022-06-03T04:35:51.214383Z","shell.execute_reply":"2022-06-03T04:35:51.250268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking for null values\nprint(train_df.isnull().sum(), '\\n')\ntest_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:35:51.254042Z","iopub.execute_input":"2022-06-03T04:35:51.254779Z","iopub.status.idle":"2022-06-03T04:35:51.279998Z","shell.execute_reply.started":"2022-06-03T04:35:51.254727Z","shell.execute_reply":"2022-06-03T04:35:51.278807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# filling null values for columns containing categorical variables\ncat_cols = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\n\nfor col in cat_cols:\n    train_df[col].fillna(train_df[col].mode(), inplace = True)\n    test_df[col].fillna(test_df[col].mode(), inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:35:51.281762Z","iopub.execute_input":"2022-06-03T04:35:51.282283Z","iopub.status.idle":"2022-06-03T04:35:51.314716Z","shell.execute_reply.started":"2022-06-03T04:35:51.282237Z","shell.execute_reply":"2022-06-03T04:35:51.31387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Filling of null values in the categorical columns was important as *pd.get_dummies* (which will be used after this) fills them with 0, by default. This can lead to misleading results.","metadata":{}},{"cell_type":"code","source":"# converting categorical variables into dummy variables for train set\nhome_train_new = pd.get_dummies(train_df['HomePlanet'], drop_first = True)\ncryo_train_new = pd.get_dummies(train_df['CryoSleep'], drop_first = True)\ndest_train_new = pd.get_dummies(train_df['Destination'], drop_first = True)\nvip_train_new = pd.get_dummies(train_df['VIP'], drop_first = True)\ntrans_train_new = pd.get_dummies(train_df['Transported'], drop_first = True)\ntrain_df = pd.concat([train_df, home_train_new, cryo_train_new, dest_train_new, vip_train_new, trans_train_new], axis = 1)\ntrain_df.drop(train_df.columns[[1, 2, 3, 4, 6, 12, 13, 14, 17]], axis = 1, inplace = True)\ntrain_df.columns = ['PassengerId', 'Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Transported']","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:35:51.316372Z","iopub.execute_input":"2022-06-03T04:35:51.317031Z","iopub.status.idle":"2022-06-03T04:35:51.349658Z","shell.execute_reply.started":"2022-06-03T04:35:51.316981Z","shell.execute_reply":"2022-06-03T04:35:51.348445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting categorical variables into dummy variables for test set\nhome_test_new = pd.get_dummies(test_df['HomePlanet'], drop_first = True)\ncryo_test_new = pd.get_dummies(test_df['CryoSleep'], drop_first = True)\ndest_test_new = pd.get_dummies(test_df['Destination'], drop_first = True)\nvip_test_new = pd.get_dummies(test_df['VIP'], drop_first = True)\ntest_df = pd.concat([test_df, home_test_new, cryo_test_new, dest_test_new, vip_test_new], axis = 1)\ntest_df.drop(test_df.columns[[1, 2, 3, 4, 6, 12, 13, 16]], axis = 1, inplace = True)\ntest_df.columns = ['PassengerId', 'Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'HomePlanet', 'CryoSleep', 'Destination', 'VIP']","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:35:51.351058Z","iopub.execute_input":"2022-06-03T04:35:51.35144Z","iopub.status.idle":"2022-06-03T04:35:51.371477Z","shell.execute_reply.started":"2022-06-03T04:35:51.35141Z","shell.execute_reply":"2022-06-03T04:35:51.370337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above two cells, we created dataframes with dummy variables for the columns HomePlanet, CryoSleep, Destination, VIP and, Transported. Then we concatenated them to their respective dataframes and deleted the extra columns that were created, along with the columns: Name and Cabin. We finally renamed the columns as some had the same name.","metadata":{}},{"cell_type":"code","source":"# filling null values of numeric variables with median scores for both train and test sets\ncols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\nfor col in cols:\n    train_df[col].fillna(np.nanmedian(train_df[col]), inplace = True)\n    test_df[col].fillna(np.nanmedian(test_df[col]), inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:35:51.373228Z","iopub.execute_input":"2022-06-03T04:35:51.375295Z","iopub.status.idle":"2022-06-03T04:35:51.391127Z","shell.execute_reply.started":"2022-06-03T04:35:51.37524Z","shell.execute_reply":"2022-06-03T04:35:51.390019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No more null values are now remaining.","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:35:51.392793Z","iopub.execute_input":"2022-06-03T04:35:51.393682Z","iopub.status.idle":"2022-06-03T04:35:51.416885Z","shell.execute_reply.started":"2022-06-03T04:35:51.393628Z","shell.execute_reply":"2022-06-03T04:35:51.416033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:35:51.419309Z","iopub.execute_input":"2022-06-03T04:35:51.420053Z","iopub.status.idle":"2022-06-03T04:35:51.443824Z","shell.execute_reply.started":"2022-06-03T04:35:51.420014Z","shell.execute_reply":"2022-06-03T04:35:51.442968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We plot a correlation heatmap for checking relation between all the variables.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf_num = train_df[['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Transported']]\ndataplot = sns.heatmap(df_num.corr(), annot = True)\nplt.show","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:35:51.444913Z","iopub.execute_input":"2022-06-03T04:35:51.445609Z","iopub.status.idle":"2022-06-03T04:35:51.945067Z","shell.execute_reply.started":"2022-06-03T04:35:51.445574Z","shell.execute_reply":"2022-06-03T04:35:51.943991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that most of the features are non-correlated, as their correlation coefficients lie very close to zero.","metadata":{}},{"cell_type":"markdown","source":"**Removing Outliers**","metadata":{}},{"cell_type":"markdown","source":"We first make boxplots using the columns containing numeric variables, to get a rough estimate of the outliers are present.","metadata":{}},{"cell_type":"code","source":"num_cols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n\nfor col in num_cols:\n    sns.boxplot(x = 'Transported', y = col, data = train_df)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:35:51.946713Z","iopub.execute_input":"2022-06-03T04:35:51.947242Z","iopub.status.idle":"2022-06-03T04:35:53.070108Z","shell.execute_reply.started":"2022-06-03T04:35:51.947198Z","shell.execute_reply":"2022-06-03T04:35:53.068821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(data = df_num, orient = 'h')\nplt.xticks(rotation = 45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:35:53.071568Z","iopub.execute_input":"2022-06-03T04:35:53.071973Z","iopub.status.idle":"2022-06-03T04:35:53.365977Z","shell.execute_reply.started":"2022-06-03T04:35:53.07194Z","shell.execute_reply":"2022-06-03T04:35:53.364725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now run a loop that sets a column-wise condition on the train and test sets, and creates a new dataframe containing only the outliers. This dataframe will then be used to as a filter to create the final, clean datasets.","metadata":{}},{"cell_type":"code","source":"# the outliers will be removed using the interquartile range (IQR)\nfor col in num_cols:\n    Q1 = train_df[col].quantile(0.10)\n    Q3 = train_df[col].quantile(0.90)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - (1.5*IQR)\n    upper_bound = Q3 + (1.5*IQR)\n\n    temp_train = train_df[(train_df[col] < lower_bound) | (train_df[col] > upper_bound)]         # creates a dataframe containing the outliers\n\ntrain_df = pd.merge(train_df, temp_train, indicator = True, how = 'outer').query('_merge == \"left_only\"').drop('_merge', axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:35:53.367306Z","iopub.execute_input":"2022-06-03T04:35:53.367656Z","iopub.status.idle":"2022-06-03T04:35:53.443061Z","shell.execute_reply.started":"2022-06-03T04:35:53.367625Z","shell.execute_reply":"2022-06-03T04:35:53.442106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The datasets are now clean. They can now be used for modelling purposes.","metadata":{}},{"cell_type":"markdown","source":"**Modelling**","metadata":{}},{"cell_type":"code","source":"# importing required libraries for modelling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# splitting the dataframes into train and test sets\nX = train_df.drop(['Transported', 'PassengerId'], axis = 1).values\ny = train_df['Transported'].values.reshape(-1, 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify = y)\n\n# preparing the test data for prediction\ntest_data = test_df.drop('PassengerId', axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:35:53.444588Z","iopub.execute_input":"2022-06-03T04:35:53.446295Z","iopub.status.idle":"2022-06-03T04:35:54.075566Z","shell.execute_reply.started":"2022-06-03T04:35:53.446245Z","shell.execute_reply":"2022-06-03T04:35:54.07465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a common function to run all models\ndef common(model):\n    cv = cross_val_score(model, X_train, y_train, cv = 10)\n    print('Cross Validation Score: {}'.format(cv))\n    print('Mean Cross Validation Score: {}'.format(cv.mean()))\n    \n    # fitting the classifier to the training data\n    model.fit(X_train, y_train)\n    \n    # predicting the labels of the test set: y_pred\n    y_pred = model.predict(X_test)\n    \n    # calculating testing accuracy of model\n    model.score(X_test, y_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print('Accuracy: ', accuracy)\n    \n    # creating submission predictions\n    predictions = model.predict(test_data.values)\n    \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:35:54.077295Z","iopub.execute_input":"2022-06-03T04:35:54.077986Z","iopub.status.idle":"2022-06-03T04:35:54.087948Z","shell.execute_reply.started":"2022-06-03T04:35:54.077915Z","shell.execute_reply":"2022-06-03T04:35:54.086515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_pred = common(KNeighborsClassifier(n_neighbors = 50))","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:35:54.089565Z","iopub.execute_input":"2022-06-03T04:35:54.09006Z","iopub.status.idle":"2022-06-03T04:35:55.455701Z","shell.execute_reply.started":"2022-06-03T04:35:54.09002Z","shell.execute_reply":"2022-06-03T04:35:55.454449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logreg_pred = common(LogisticRegression())","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:35:55.457154Z","iopub.execute_input":"2022-06-03T04:35:55.457575Z","iopub.status.idle":"2022-06-03T04:35:56.581247Z","shell.execute_reply.started":"2022-06-03T04:35:55.457542Z","shell.execute_reply":"2022-06-03T04:35:56.580034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_pred = common(XGBClassifier())","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:35:56.58308Z","iopub.execute_input":"2022-06-03T04:35:56.583894Z","iopub.status.idle":"2022-06-03T04:36:03.724888Z","shell.execute_reply.started":"2022-06-03T04:35:56.583845Z","shell.execute_reply":"2022-06-03T04:36:03.724001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_pred = common(RandomForestClassifier(n_estimators = 400))","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:36:03.726434Z","iopub.execute_input":"2022-06-03T04:36:03.727111Z","iopub.status.idle":"2022-06-03T04:36:35.722234Z","shell.execute_reply.started":"2022-06-03T04:36:03.72707Z","shell.execute_reply":"2022-06-03T04:36:35.721041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that among the above models, LogisticRegression offers the best accuracy and CV score. Hence, we use that for creating our final predictions.","metadata":{}},{"cell_type":"code","source":"# preparing the submission csv file\ndf = pd.DataFrame({'PassengerId':test_df['PassengerId'], 'Transported':logreg_pred})\n\n# the values of the Transported columns are in 0s and 1s, so we convert them back to False and True form, respectively\ndf['Transported'] = df['Transported'].replace([0], False)\ndf['Transported'] = df['Transported'].replace([1], True)\n\ndf.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T04:36:35.723785Z","iopub.execute_input":"2022-06-03T04:36:35.724212Z","iopub.status.idle":"2022-06-03T04:36:35.746562Z","shell.execute_reply.started":"2022-06-03T04:36:35.724176Z","shell.execute_reply":"2022-06-03T04:36:35.745659Z"},"trusted":true},"execution_count":null,"outputs":[]}]}