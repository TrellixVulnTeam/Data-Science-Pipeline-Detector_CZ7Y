{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction \n`V2.0.0`\n### Who am I\nJust a fellow Kaggle learner. I was creating this Notebook as practice and thought it could be useful to some others \n### Who is this for\nThis Notebook is for people that learn from examples. Forget the boring lectures and follow along for some fun/instructive time :)\n### What can I learn here\nYou learn all the basics needed to create a rudimentary XGBoost model with hyperparameter tuning. I go over a multitude of steps with explanations. Hopefully with these building blocks,you can go ahead and build much more complex models.\n\n### Things to remember\n+ Please Upvote/Like the Notebook so other people can learn from it\n+ Feel free to give any recommendations/changes. \n+ I will be continuously updating the notebook. Look forward to many more upcoming changes in the future.\n\n### You can also refer to these notebooks that have helped me as well:\n+ https://www.kaggle.com/code/sanjaylalwani/spaceship-titanic-eda-ensemble-with-80-5#Feature-Selection","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# Python Imports\nimport os\nimport numpy as np   # Library for n-dimensional arrays\nimport pandas as pd  # Library for dataframes (structured data)\nfrom pathlib import Path\nfrom datetime import datetime\n\n# ML imports\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, f1_score\nfrom sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom lightgbm import LGBMClassifier\n\n# Plotting imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# I like to disable my Notebook Warnings.\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seeds to make the experiment more reproducible.\nfrom numpy.random import seed\nseed(1)\n\n# Allows us to see more information regarding the DataFrame\npd.set_option(\"display.max_rows\", 500)\npd.set_option(\"display.max_columns\", 500)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Data\n1. Since data is in form of csv file we have to use pandas read_csv to load the data\n2. After loading it is important to check the complete information of data. It is important to get a general feel of the data that we are going to be using.","metadata":{}},{"cell_type":"code","source":"DATA_DIR = Path(\"../input/spaceship-titanic\")\n\nTRAIN_DATA_PATH = DATA_DIR / \"train.csv\"\nTEST_DATA_PATH = DATA_DIR / \"test.csv\"\nSUBMISSION_PATH = DATA_DIR / \"sample_submission.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_DATA_PATH)\ntest_df = pd.read_csv(TEST_DATA_PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Tip:</b> We can use the .head() method to obtain the first 5 rows of the DataFrame.\n</div>","metadata":{}},{"cell_type":"code","source":"train_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Tip:</b> We can use the .sample() method to obtain 5 random rows in the DataFrame.\n</div>","metadata":{}},{"cell_type":"code","source":"test_df.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA/Visualizations\nThe goal is to try and gain insights from the data prior to modeling","metadata":{}},{"cell_type":"markdown","source":"## Explorating the Dataframe\nIt is useful to use .info() method to quickly have a glance on the general information about the DataFrame. It displays info such as the type of the columnd and also the # of non-null count. In this case there is 8693 entries. Some columns have less values than 8693 which mean they have some missing values that we will have to take care of.","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The describe() method gives a quick summary of the statistical information of the numerical columns. We get descriptions for the mean, standard deviation and max value for example.","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are defining a function that returns are categorical, numerical and feature columns. We will be using it consistenly across the notebook.","metadata":{}},{"cell_type":"code","source":"def get_all_cols(df, target_col, exclude=None):\n        \n    if exclude is None:\n        exclude = []\n        \n    # Select categorical columns\n    object_cols = [cname for cname in df.columns \n                   if df[cname].dtype == \"object\"]\n\n    # Select numerical columns\n    num_cols = [cname for cname in df.columns \n                if df[cname].dtype in ['int64', 'float64', 'uint8']]\n    \n    all_cols = object_cols + num_cols\n    exclude_cols = exclude + [target_col]\n    feature_cols = [col for col in all_cols if col not in exclude_cols]\n    \n    return object_cols, num_cols, feature_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TARGET = \"Transported\"\nobject_cols, num_cols, feature_cols = get_all_cols(train_df, TARGET)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\">  \n<b>Note:</b> We assign a constant variable TARGET that we can refer to throughout the notebook as the target variable. Makes it much easier than always typing \"Transported\".\n</div>","metadata":{}},{"cell_type":"markdown","source":"We can also explore unique values for our feature columns using the unique() method.","metadata":{}},{"cell_type":"code","source":"for object_col in object_cols:\n    train_df_unique_list = train_df[object_col].unique()\n    print(f'{object_col}:{train_df_unique_list}\\n')","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The value_counts() method allows us to get unique value counts that exist in a specific column. In this case, we will get the unique values and count of the three feature columns.","metadata":{}},{"cell_type":"code","source":"for object_col in object_cols:\n    obj_val_counts = train_df[object_col].value_counts()\n    print(f'LENGTH: {len(obj_val_counts)}\\n',f'{obj_val_counts}\\n')","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyzing the categorical columns, we notice that some of the colums (True/False) we can transform into a boolean column, some we can one-hot encode (the ones that have less than 7 unique values).","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering / Prepare the data","metadata":{}},{"cell_type":"markdown","source":"## Handling Missing Values\nIn this section, we will take care of the missing values before starting to train the model. ","metadata":{}},{"cell_type":"markdown","source":"Let's start with the categorical columns. We notice that for HomePlanet, the most common category is Earth and for Destination it is TRAPPIST-1e. We fill the missing values with these common value.","metadata":{}},{"cell_type":"code","source":"train_df['HomePlanet']= train_df['HomePlanet'].fillna('Earth')\ntest_df['HomePlanet']= test_df['HomePlanet'].fillna('Earth')\n\ntrain_df['Destination']= train_df['Destination'].fillna('TRAPPIST-1e')\ntest_df['Destination']= test_df['Destination'].fillna('TRAPPIST-1e')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then take care of the boolean columns and transform the True/False into 1/0.","metadata":{}},{"cell_type":"code","source":"train_df['CryoSleep']= train_df['CryoSleep'].fillna(False).astype(int)\ntest_df['CryoSleep']= test_df['CryoSleep'].fillna(False).astype(int)\n\ntrain_df['VIP']= train_df['VIP'].fillna(False).astype(int)\ntest_df['VIP']= test_df['VIP'].fillna(False).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will take care of the numerical columns. We fill the missing values with the mean.","metadata":{}},{"cell_type":"code","source":"train_df[\"Age\"] = train_df[\"Age\"].fillna(train_df[\"Age\"].mean())\ntrain_df[\"RoomService\"] = train_df[\"RoomService\"].fillna(train_df[\"RoomService\"].mean())\ntrain_df[\"FoodCourt\"] = train_df[\"FoodCourt\"].fillna(train_df[\"FoodCourt\"].mean())\ntrain_df[\"ShoppingMall\"] = train_df[\"ShoppingMall\"].fillna(train_df[\"ShoppingMall\"].mean())\ntrain_df[\"Spa\"] = train_df[\"Spa\"].fillna(train_df[\"Spa\"].mean())\ntrain_df[\"VRDeck\"] = train_df[\"VRDeck\"].fillna(train_df[\"VRDeck\"].mean())\n\ntest_df[\"Age\"] = test_df[\"Age\"].fillna(test_df[\"Age\"].mean())\ntest_df[\"RoomService\"] = test_df[\"RoomService\"].fillna(test_df[\"RoomService\"].mean())\ntest_df[\"FoodCourt\"] = test_df[\"FoodCourt\"].fillna(test_df[\"FoodCourt\"].mean())\ntest_df[\"ShoppingMall\"] = test_df[\"ShoppingMall\"].fillna(test_df[\"ShoppingMall\"].mean())\ntest_df[\"Spa\"] = test_df[\"Spa\"].fillna(test_df[\"Spa\"].mean())\ntest_df[\"VRDeck\"] = test_df[\"VRDeck\"].fillna(test_df[\"VRDeck\"].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There seems to only be two columns with missing values. These are two columns that we will transform first before taking care of the missing valuues.","metadata":{}},{"cell_type":"markdown","source":"## Treating the outliers\nLet's first define a helper function that will allow us to quickly plot box plots for our numerical columns.","metadata":{}},{"cell_type":"code","source":"def box_plots(df):\n    plt.figure(figsize=(10,5))\n    plt.title(\"Box Plot\")\n    sns.boxplot(df)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TARGET = \"Transported\"\nobject_cols, num_cols, feature_cols = get_all_cols(train_df, TARGET)\n\n# Remove CryoSleep and VIP column because they are binary columns\nnum_cols.remove('CryoSleep')\nnum_cols.remove('VIP')\ncontinous_num_cols = num_cols.copy()\ncontinous_num_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the Data\nIn this subsection, we look into preparing the feature columns. That can be done by transforming the type of the column to a proper one, creating datetime features from our date column or even adding more valuable feature column (such as holidays) to our dataframe. This is the first step before going to other feature engineering steps.","metadata":{}},{"cell_type":"markdown","source":"Let's create three new columns derived from the `Cabin` column. Dividing this column in three would potentially allow us to get more information that can help us achieve better accuracy. We can then take care of the missing values of the three new columns.","metadata":{}},{"cell_type":"code","source":"train_df[['Deck', 'Num', 'Side']] = train_df['Cabin'].str.split('/', expand=True)   \ntest_df[['Deck', 'Num', 'Side']] = test_df['Cabin'].str.split('/', expand=True)   \n\ntrain_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"object_cols, num_cols, feature_cols = get_all_cols(train_df, TARGET, [\"Cabin\"])\n\nfor object_col in object_cols:\n    obj_val_counts = train_df[object_col].value_counts()\n    print(f'LENGTH: {len(obj_val_counts)}\\n',f'{obj_val_counts}\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now take care of the missing values of the three new columns introduced: `Deck` :Categorical, `Num` :Numerical, `Side` :Categorical","metadata":{}},{"cell_type":"code","source":"train_df['Deck']= train_df['Deck'].fillna('F')\ntrain_df['Num'] = train_df['Num'].astype(float)\ntrain_df['Num']= train_df['Num'].fillna(train_df['Num'].mean())\ntrain_df['Side']= train_df['Side'].fillna('S')\n\ntest_df['Deck']= test_df['Deck'].fillna('F')\ntest_df['Num'] = test_df['Num'].astype(float)\ntest_df['Num']= test_df['Num'].fillna(train_df['Num'].mean())\ntest_df['Side']= test_df['Side'].fillna('S')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now create a new column `group_id` that will be derived from the `Passenger_ID`. We don't really care about the passenger ID, but we care more about the Group_ID because common group ids can represent a family or a common group of friends. This column has no missing values, therefore no missing value handling is required.","metadata":{}},{"cell_type":"code","source":"def create_group_id(passenger_id):\n    splitted_id = passenger_id.split(\"_\")\n    group_id = splitted_id[0]\n    return group_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"group_id\"] = train_df[\"PassengerId\"].apply(create_group_id)\ntrain_df[\"group_id\"] = train_df[\"group_id\"].astype(int)\n\ntest_df[\"group_id\"] = test_df[\"PassengerId\"].apply(create_group_id)\ntest_df[\"group_id\"] = test_df[\"group_id\"].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\">  \nDon't forget to do the same for our test data!\n</div>","metadata":{}},{"cell_type":"markdown","source":"Let's now remove the unwanted columns. We are removing PassengerId and Cabin because they have been transformed into other columns. We are removing Name because it contains too many unique values and we deduce it would have not much impact in the prediction. \n\nIf you have more time, you can keep Name or transform it into another columns and see the real impact it has in the predictions.","metadata":{}},{"cell_type":"code","source":"DROP_COLS = ['PassengerId', 'Name', 'Cabin']\nPassengerId = test_df['PassengerId']\ntrain_df.drop(DROP_COLS,axis=1, inplace=True)\ntest_df.drop(DROP_COLS,axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Handling Categorical Data\nSo that the model can understand categorical data, we must transform them in a numerical form. There is various ways to do that. ","metadata":{}},{"cell_type":"markdown","source":"Some of them categorical data are,\n<div class=\"alert alert-block alert-info\">\n<b>Nominal Data</b> --> data are not in any order --> OneHotEncoder or pandas.get_dummies() is used in this case\n</div>\n<div class=\"alert alert-block alert-info\">\n<b>Ordinal data </b> --> data are in order --> LabelEncoder is used in this case\n</div>","metadata":{}},{"cell_type":"markdown","source":"We can one hot encode Homeplanete, Destination and Side because they have less than 8 unique values. We will label encode deck.","metadata":{}},{"cell_type":"code","source":"object_cols, num_cols, feature_cols = get_all_cols(train_df, TARGET)\n\nfor object_col in object_cols:\n    obj_val_counts = train_df[object_col].value_counts()\n    print(f'LENGTH: {len(obj_val_counts)}\\n',f'{obj_val_counts}\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ONE_HOT_CATEGORICAL = ['HomePlanet', 'Destination', 'Side']\ndef create_one_hot(df, categ_colums = ONE_HOT_CATEGORICAL):\n    \"\"\"\n    Creates one_hot encoded fields for the specified categorical columns...\n    Args\n        df\n        categ_colums\n    Returns\n        df\n    \"\"\"\n    df = pd.get_dummies(df, columns=categ_colums)\n    return df\n\nLABEL_CATEGORICAL = ['Deck']\ndef encode_categ_features(df, categ_colums = LABEL_CATEGORICAL):\n    \"\"\"\n    Use the label encoder to encode categorical features...\n    Args\n        df\n        categ_colums\n    Returns\n        df\n    \"\"\"\n    le = LabelEncoder()\n    for col in categ_colums:\n        df['enc_'+col] = le.fit_transform(df[col])\n    df.drop(categ_colums, axis=1, inplace=True)\n    return df\n\ntrain_df = encode_categ_features(train_df)\ntest_df = encode_categ_features(test_df)\n\ntrain_df = create_one_hot(train_df)\ntest_df = create_one_hot(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CATEGORICAL = ONE_HOT_CATEGORICAL + LABEL_CATEGORICAL\n\nobject_cols, num_cols, feature_cols = get_all_cols(train_df, TARGET)\nobject_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info(), test_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No more object columns... we have done our job :)","metadata":{}},{"cell_type":"markdown","source":"## Feature Selection\n\nFinding out the best feature which will contribute and have good relation with target variable.\nFollowing are some of the feature selection methods,","metadata":{}},{"cell_type":"markdown","source":"\n<div class=\"alert alert-block alert-info\">\n<b>1. heatmap</b> \n</div>\n<div class=\"alert alert-block alert-info\">\n<b>2. feature_importance_</b> \n</div>\n<div class=\"alert alert-block alert-info\">\n<b>3. SelectKBest</b> \n</div>","metadata":{}},{"cell_type":"markdown","source":"### Correlation \nTo see the correlation between the various features and also with the target value, we will use a heatmap.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (18,18))\nsns.heatmap(train_df.corr(), annot = True, cmap = \"RdYlGn\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We notice some features are heavily correlated. We will remove two to reduce the dimensionality of our model:**\n1.  enc_Deck and HomePlanet_Europa are heavily negatively correlated. I decide to remove enc_Deck since it has more unique values than HomePlanet_Europa.","metadata":{}},{"cell_type":"code","source":"train_df.drop(\"enc_Deck\", axis=1, inplace=True)\ntest_df.drop(\"enc_Deck\", axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['Under15'] = train_df['Age'].apply(lambda x: 1 if x < 15 else 0)\ntest_df['Under15'] = test_df['Age'].apply(lambda x: 1 if x < 15 else 0)\n\ntrain_df = train_df.drop(['Age'], axis=1)\ntest_df = test_df.drop(['Age'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the data\nIn this section, we will split the data in train and test set. Do not confuse test set with our test data. Test set is just a subsample of train_df.","metadata":{}},{"cell_type":"code","source":"X = train_df.drop(TARGET, axis=1)\ny = train_df[TARGET]\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models\nIn this section, we will explore one model:\n\n1. XGBClassifier","metadata":{}},{"cell_type":"markdown","source":"## Training\nWe've prepared the food (data), time to... FEED THE MACHINE.","metadata":{}},{"cell_type":"code","source":"xgb_model = XGBClassifier()\nmodel = xgb_model.fit(X_train, y_train, eval_metric='logloss')\n\nprint(\"Performance on train data:\", model.score(X_train, y_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predicting & Evaluating\nIn this subsection, we evaluate using plots and metrics to see if our predictions are good or not.","metadata":{}},{"cell_type":"code","source":"y_valid_pred = model.predict(X_valid)\n\nprint(\"Performance on validation data:\", f1_score(y_valid, y_valid_pred, average='micro'))\n\ncm = confusion_matrix(y_valid, y_valid_pred) \nprint (\"Confusion Matrix : \\n\", cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning\n\n\n* Choose following method for hyperparameter tuning\n    1. **RandomizedSearchCV**: Faster when there are many combinations of hyperparameter\n    2. **GridSearchCV**: Tries all combinations\n* Assign hyperparameters in form of dictionary\n* Fit the model\n* Check best paramters and best score","metadata":{}},{"cell_type":"markdown","source":"## Search for best hyperparameters\nI have already run the hyperparameter optimization and found an optimized model. The code has been commented out, but you can use it to then find an even more optimal model with different hyperparameters.","metadata":{}},{"cell_type":"code","source":"# # A parameter grid for XGBoost\n# params = {\n#         'min_child_weight': [1, 5, 10],\n#         'gamma': [0.5, 1, 1.5, 2, 5],\n#         'subsample': [0.6, 0.8, 1.0],\n#         'colsample_bytree': [0.6, 0.8, 1.0],\n#         'max_depth': [3, 4, 5]\n#         }\n\n# xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n#                     silent=True, use_label_encoder=False, nthread=1)\n\n# def timer(start_time=None):\n#     if not start_time:\n#         start_time = datetime.now()\n#         return start_time\n#     elif start_time:\n#         thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n#         tmin, tsec = divmod(temp_sec, 60)\n#         print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n        \n# folds = 10\n# param_comb = 120\n\n# skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\n# random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X,y), verbose=1, random_state=42 )\n\n# # Here we go\n# start_time = timer(None) # timing starts from this point for \"start_time\" variable\n# random_search.fit(X, y)\n# timer(start_time) # timing ends here for \"start_time\" variable\n\n# print('\\n All results:')\n# print(random_search.cv_results_)\n# print('\\n Best estimator:')\n# print(random_search.best_estimator_)\n# print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n# print(random_search.best_score_ * 2 - 1)\n# print('\\n Best hyperparameters:')\n# print(random_search.best_params_)\n# results = pd.DataFrame(random_search.cv_results_)\n# results.to_csv('xgb-random-grid-search-results-01.csv', index=False)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is the model with the optimized hyperparameters","metadata":{}},{"cell_type":"markdown","source":"## Predicting with tuned model\nLet us used our tuned model to predict the Target price and see if it does better than our untuned model.","metadata":{}},{"cell_type":"code","source":"# optimized_xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#               colsample_bynode=1, colsample_bytree=0.6,\n#               enable_categorical=False, gamma=1, gpu_id=-1,\n#               importance_type=None, interaction_constraints='',\n#               learning_rate=0.02, max_delta_step=0, max_depth=5,\n#               min_child_weight=1, monotone_constraints='()',\n#               n_estimators=600, n_jobs=1, nthread=1, num_parallel_tree=1,\n#               predictor='auto', random_state=0, reg_alpha=0, reg_lambda=1,\n#               scale_pos_weight=1, silent=True, subsample=0.6,\n#               tree_method='exact', validate_parameters=1, verbosity=None)\n\n\noptimized_xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.8,\n              enable_categorical=False, gamma=0.5, gpu_id=-1,\n              importance_type=None, interaction_constraints='',\n              learning_rate=0.02, max_delta_step=0, max_depth=5,\n              min_child_weight=1, monotone_constraints='()',\n              n_estimators=600, n_jobs=1, nthread=1, num_parallel_tree=1,\n              predictor='auto', random_state=0, reg_alpha=0, reg_lambda=1,\n              scale_pos_weight=1, silent=True, subsample=0.6,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\n# optimized_xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#               colsample_bynode=1, colsample_bytree=0.6,\n#               enable_categorical=False, gamma=1.5, gpu_id=-1,\n#               importance_type=None, interaction_constraints='',\n#               learning_rate=0.02, max_delta_step=0, max_depth=5,\n#               min_child_weight=1, monotone_constraints='()',\n#               n_estimators=600, n_jobs=1, nthread=1, num_parallel_tree=1,\n#               predictor='auto', random_state=0, reg_alpha=0, reg_lambda=1,\n#               scale_pos_weight=1, silent=True, subsample=0.8,\n#               tree_method='exact', validate_parameters=1, verbosity=None)\n\n# optimized_xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#               colsample_bynode=1, colsample_bytree=0.8,\n#               enable_categorical=False, gamma=5, gpu_id=-1,\n#               importance_type=None, interaction_constraints='',\n#               learning_rate=0.02, max_delta_step=0, max_depth=5,\n#               min_child_weight=5, monotone_constraints='()',\n#               n_estimators=600, n_jobs=1, nthread=1, num_parallel_tree=1,\n#               predictor='auto', random_state=0, reg_alpha=0, reg_lambda=1,\n#               scale_pos_weight=1, silent=True, subsample=0.8,\n#               tree_method='exact', validate_parameters=1, verbosity=None)\n\nxgb_optimal = optimized_xgb.fit(X, y, eval_metric='logloss')\n\nprint(\"Performance on train data:\", xgb_optimal.score(X, y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble of models\nHere we will create an ensemble of models to increase accuracy and generability of the model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nfrom sklearn.svm import SVC\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nsvc_optimal = SVC(C=1.5, gamma='scale', kernel='rbf')\nlgbm_optimal = LGBMClassifier(learning_rate=0.05, max_depth=8, n_estimators=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Hard Voting Classifier\nEnsemble_HV = VotingClassifier(estimators= [('SVC', svc_optimal),\n                                           ('XBG', xgb_optimal),\n                                           ('LGBM', lgbm_optimal)],\n                              voting = 'hard')\n\n# Return Accuracy Scores\ncv_HV = cross_val_score(Ensemble_HV, X, y, scoring='accuracy')\n\nprint('Hard Voting Classifier:' , cv_HV.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble_model = Ensemble_HV.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T03:53:17.957339Z","iopub.execute_input":"2022-06-10T03:53:17.957714Z","iopub.status.idle":"2022-06-10T03:53:29.72279Z","shell.execute_reply.started":"2022-06-10T03:53:17.957675Z","shell.execute_reply":"2022-06-10T03:53:29.721875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predicting & Evaluating tuned model","metadata":{}},{"cell_type":"code","source":"y_valid_pred = ensemble_model.predict(X_valid)\n\nprint(\"Performance on validation data:\", f1_score(y_valid, y_valid_pred, average='micro'))\n\ncm = confusion_matrix(y_valid, y_valid_pred) \nprint (\"Confusion Matrix : \\n\", cm)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T03:53:46.50127Z","iopub.execute_input":"2022-06-10T03:53:46.501546Z","iopub.status.idle":"2022-06-10T03:53:47.096168Z","shell.execute_reply.started":"2022-06-10T03:53:46.501516Z","shell.execute_reply":"2022-06-10T03:53:47.095266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save the model to reuse it again\nThere's various ways to save the model. We decided to go forward with pickling. It is very easy and straighforward. ","metadata":{}},{"cell_type":"code","source":"import pickle\n# open a file, where you ant to store the data\nwith open('ensemble_model.pkl', 'wb') as file:\n    pickle.dump(ensemble_model, file)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T03:53:53.017311Z","iopub.execute_input":"2022-06-10T03:53:53.017582Z","iopub.status.idle":"2022-06-10T03:53:53.172221Z","shell.execute_reply.started":"2022-06-10T03:53:53.017554Z","shell.execute_reply":"2022-06-10T03:53:53.171343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('ensemble_model.pkl', 'rb') as model:\n    loaded_model = pickle.load(model)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T03:53:54.057324Z","iopub.execute_input":"2022-06-10T03:53:54.057678Z","iopub.status.idle":"2022-06-10T03:53:54.236985Z","shell.execute_reply.started":"2022-06-10T03:53:54.057637Z","shell.execute_reply":"2022-06-10T03:53:54.236156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submitting\nIn this section, we get the predictions for the TEST DATA and save our dataframe into a csv file to then be submitted.","metadata":{}},{"cell_type":"code","source":"y_pred = loaded_model.predict(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T03:53:56.421189Z","iopub.execute_input":"2022-06-10T03:53:56.42151Z","iopub.status.idle":"2022-06-10T03:53:58.01804Z","shell.execute_reply.started":"2022-06-10T03:53:56.421479Z","shell.execute_reply":"2022-06-10T03:53:58.016898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create a  DataFrame with the passengers ids and our prediction\nsubmission_df = pd.read_csv(SUBMISSION_PATH)\nsubmission_df[\"Transported\"] = y_pred\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T03:53:58.808957Z","iopub.execute_input":"2022-06-10T03:53:58.809272Z","iopub.status.idle":"2022-06-10T03:53:58.835544Z","shell.execute_reply.started":"2022-06-10T03:53:58.80924Z","shell.execute_reply":"2022-06-10T03:53:58.83484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Remarks\nThank you for going through this notebook. Please feel free to show support and comment on the notebooks with advice or improvements. If you found it useful, please let me know as well :)","metadata":{}}]}