{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Core\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)\nfrom imblearn.over_sampling import SMOTE\nimport itertools\nimport warnings\nwarnings.filterwarnings('ignore')\nimport plotly.express as px\nimport time","metadata":{"execution":{"iopub.status.busy":"2022-04-05T05:33:19.492996Z","iopub.execute_input":"2022-04-05T05:33:19.49377Z","iopub.status.idle":"2022-04-05T05:33:22.667253Z","shell.execute_reply.started":"2022-04-05T05:33:19.493626Z","shell.execute_reply":"2022-04-05T05:33:22.666306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/spaceship-titanic/train.csv')\ntest = pd.read_csv('../input/spaceship-titanic/test.csv')\nprint('train shape : ', train.shape)\nprint('test shape : ', test.shape)\ntrain.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:01.941749Z","iopub.execute_input":"2022-04-03T14:45:01.942843Z","iopub.status.idle":"2022-04-03T14:45:02.100017Z","shell.execute_reply.started":"2022-04-03T14:45:01.942797Z","shell.execute_reply":"2022-04-03T14:45:02.099246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## feature_description\n- PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n- HomePlanet - The planet the passenger departed from, typically their planet of permanent residence.\n- CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n- Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n- Destination - The planet the passenger will be debarking to.\n- Age - The age of the passenger.\n- VIP - Whether the passenger has paid for special VIP service during the voyage.\n- RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n- Name - The first and last names of the passenger.\n- Transported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.","metadata":{}},{"cell_type":"code","source":"# check missing values of train\nmissing = pd.DataFrame(train.isnull().sum().sort_values(), columns = ['sum_missing'])\nmissing['percentage_null'] = missing['sum_missing']/train.shape[0]*100\nmissing","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:02.10224Z","iopub.execute_input":"2022-04-03T14:45:02.102556Z","iopub.status.idle":"2022-04-03T14:45:02.132695Z","shell.execute_reply.started":"2022-04-03T14:45:02.102515Z","shell.execute_reply":"2022-04-03T14:45:02.13199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check missing values of test\nmissing = pd.DataFrame(test.isnull().sum().sort_values(), columns = ['sum_missing'])\nmissing['percentage_null'] = missing['sum_missing']/test.shape[0]*100\nmissing","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:02.133981Z","iopub.execute_input":"2022-04-03T14:45:02.134429Z","iopub.status.idle":"2022-04-03T14:45:02.152005Z","shell.execute_reply.started":"2022-04-03T14:45:02.134396Z","shell.execute_reply":"2022-04-03T14:45:02.15142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking duplicated rows\nprint(f'duplicated rows in train : {train.duplicated().sum()}  {train.duplicated().sum()/train.shape[0]*100}%')\nprint(f'duplicated rows in test : {test.duplicated().sum()}  {test.duplicated().sum()/test.shape[0]*100}%')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:02.153653Z","iopub.execute_input":"2022-04-03T14:45:02.153897Z","iopub.status.idle":"2022-04-03T14:45:02.206265Z","shell.execute_reply.started":"2022-04-03T14:45:02.153867Z","shell.execute_reply":"2022-04-03T14:45:02.205268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking datatypes\ntrain.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:02.207599Z","iopub.execute_input":"2022-04-03T14:45:02.2084Z","iopub.status.idle":"2022-04-03T14:45:02.234155Z","shell.execute_reply.started":"2022-04-03T14:45:02.208352Z","shell.execute_reply":"2022-04-03T14:45:02.233501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We will need to transform the data to be numeric (int64 or float64) so that we can train machine learning models. These models (in general) don't work on text.**","metadata":{}},{"cell_type":"code","source":"num_data = train.select_dtypes(include = 'number')\nnum_data","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:02.235141Z","iopub.execute_input":"2022-04-03T14:45:02.235623Z","iopub.status.idle":"2022-04-03T14:45:02.25926Z","shell.execute_reply.started":"2022-04-03T14:45:02.235588Z","shell.execute_reply":"2022-04-03T14:45:02.257999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:02.260739Z","iopub.execute_input":"2022-04-03T14:45:02.261348Z","iopub.status.idle":"2022-04-03T14:45:02.286765Z","shell.execute_reply.started":"2022-04-03T14:45:02.261309Z","shell.execute_reply":"2022-04-03T14:45:02.285896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There are 6 continuous features, 4 categorical features (excluding the target) and 3 descriptive/qualitative features.**","metadata":{}},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"# target distribution\nplt.figure(figsize = (6, 6))\nplt.pie(train['Transported'].value_counts(), explode = [0, 0.1], shadow = True, autopct = '%.2f%%', labels = ['True', 'False'])\nplt.title('target_distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:02.287866Z","iopub.execute_input":"2022-04-03T14:45:02.288283Z","iopub.status.idle":"2022-04-03T14:45:02.470091Z","shell.execute_reply.started":"2022-04-03T14:45:02.288247Z","shell.execute_reply":"2022-04-03T14:45:02.469136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The target is highly balanced, so we luckily don't have to consider techniques like under/over-sampling**","metadata":{}},{"cell_type":"markdown","source":"### cotinuous features","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.histplot(data = train, x = 'Age', hue = 'Transported', kde = True)\nplt.title('Age Distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:02.471653Z","iopub.execute_input":"2022-04-03T14:45:02.472213Z","iopub.status.idle":"2022-04-03T14:45:03.07397Z","shell.execute_reply.started":"2022-04-03T14:45:02.472145Z","shell.execute_reply":"2022-04-03T14:45:03.073007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Notes:**\n- 0-18 year olds were more likely to be transported than not.\n- 18-25 year olds were less likely to be transported than not.\n- Over 25 year olds were equally likely to be transported than not.\n\n**Insight:**\n- Create a new feature that indicates whether the passanger is a child (under 18), adolescent (18-25) or adult (over 25). Then we can drop the age feature to prevent overfitting.","metadata":{}},{"cell_type":"code","source":"# Expenditures Features\nexp_features=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n\nfig = plt.figure(figsize = (10, 20))\nfor i, col in enumerate(exp_features):\n    # left figure\n    ax = fig.add_subplot(5,2,2*i+1)\n    sns.histplot(data = train, x = col, bins = 30, hue = 'Transported', ax = ax)\n    # right figure\n    ax = fig.add_subplot(5,2,2*i+2)\n    sns.histplot(data = train, x = col, bins = 30, hue = 'Transported', kde = True, ax = ax)\n    plt.ylim([0, 100])\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:03.077007Z","iopub.execute_input":"2022-04-03T14:45:03.077282Z","iopub.status.idle":"2022-04-03T14:45:07.068129Z","shell.execute_reply.started":"2022-04-03T14:45:03.077252Z","shell.execute_reply":"2022-04-03T14:45:07.067243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Notes:**\n\n- Most people don't spend any money (as we can see on the left).\n- The distribution of spending decays exponentially (as we can see on the right).\n- There are a small number of outliers.\n- People who were transported tended to spend less.\n- RoomService, Spa and VRDeck have different distributions to FoodCourt and ShoppingMall - we can think of this as luxury vs essential amenities.\n\n**Insight:**\n\n- Create a new feature that tracks the total expenditure across all 5 amenities.\n- Create a binary feature to indicate if the person has not spent anything. (i.e. total expenditure is 0).","metadata":{}},{"cell_type":"markdown","source":"### Categorical features","metadata":{}},{"cell_type":"code","source":"# Categorical features\ncat_features=['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\nfig = plt.figure(figsize=(10, 20))\nfor i, col in enumerate(cat_features):\n    ax = fig.add_subplot(4, 1, i+1)\n    sns.countplot(data = train, x = col, hue = 'Transported', ax = ax, palette = 'viridis')\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:07.069667Z","iopub.execute_input":"2022-04-03T14:45:07.070461Z","iopub.status.idle":"2022-04-03T14:45:07.901843Z","shell.execute_reply.started":"2022-04-03T14:45:07.070424Z","shell.execute_reply":"2022-04-03T14:45:07.900956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Notes:**\n\n- VIP does not appear to be a useful feature; the target split is more or less equal.\n- CryoSleep appears the be a very useful feature in contrast.\n\n**Insights:**\n\n- We might consider dropping the VIP column to prevent overfitting.","metadata":{}},{"cell_type":"markdown","source":"### Qualitative features\n\nWe can't plot this data (yet). We need to transform it into more useful features.","metadata":{}},{"cell_type":"code","source":"# Qualitative features\nqual_features=['PassengerId', 'Cabin' ,'Name']\n\n# Preview qualitative features\ntrain[qual_features].head()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:07.90323Z","iopub.execute_input":"2022-04-03T14:45:07.90359Z","iopub.status.idle":"2022-04-03T14:45:07.917072Z","shell.execute_reply.started":"2022-04-03T14:45:07.90355Z","shell.execute_reply":"2022-04-03T14:45:07.916225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notes:\n\n- PassengerId takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group.\n- Cabin takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n\nInsights:\n\n- We can extract the group and group size from the PassengerId feature.\n- We can extract the deck, number and side from the cabin feature.\n- We could extract the surname from the name feature to identify families.","metadata":{}},{"cell_type":"markdown","source":"## Missing values","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (12, 6))\nsns.heatmap(train.isna().T, cmap = 'flare')\nplt.title('Heatmap of missing values')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:07.918948Z","iopub.execute_input":"2022-04-03T14:45:07.919329Z","iopub.status.idle":"2022-04-03T14:45:08.987918Z","shell.execute_reply.started":"2022-04-03T14:45:07.919285Z","shell.execute_reply":"2022-04-03T14:45:08.986885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Missing values make up about 2% of the data, which is a relatively small amount. For the most part, they don't seem to be happening at the same time, but let's inspect closer.","metadata":{}},{"cell_type":"code","source":"train['na_counts'] = train.isna().sum(axis = 1)\nplt.figure(figsize = (10, 6))\nsns.countplot(data = train, x = 'na_counts', hue = 'Transported')\nplt.title('Number of missing entries by passenger')\nplt.show()\ntrain.drop('na_counts', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:08.989562Z","iopub.execute_input":"2022-04-03T14:45:08.989901Z","iopub.status.idle":"2022-04-03T14:45:09.247986Z","shell.execute_reply.started":"2022-04-03T14:45:08.989858Z","shell.execute_reply":"2022-04-03T14:45:09.247188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This shows that missing values don't favour either outcome in the target and for the most part are isolated. This means it is reasonable to smartly 'guess' alternatives to the missing values as opposed to dropping these passengers entirely and losing a lot of training data.","metadata":{}},{"cell_type":"markdown","source":"### Continuous data","metadata":{}},{"cell_type":"code","source":"# Impute median (for continuous data)\ntrain['Age'].fillna(train['Age'].median(), inplace=True)\ntest['Age'].fillna(train['Age'].median(), inplace=True) # be careful of data leakage","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.249141Z","iopub.execute_input":"2022-04-03T14:45:09.249413Z","iopub.status.idle":"2022-04-03T14:45:09.255963Z","shell.execute_reply.started":"2022-04-03T14:45:09.249384Z","shell.execute_reply":"2022-04-03T14:45:09.255201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical data","metadata":{}},{"cell_type":"code","source":"# Find mode of each categorical feature\ntrain[['HomePlanet','CryoSleep','Destination','VIP']].mode()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.257268Z","iopub.execute_input":"2022-04-03T14:45:09.257508Z","iopub.status.idle":"2022-04-03T14:45:09.286284Z","shell.execute_reply.started":"2022-04-03T14:45:09.257472Z","shell.execute_reply":"2022-04-03T14:45:09.285495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute most frequent category (for categorical data)\ntrain['HomePlanet'].fillna('Earth', inplace=True)\ntest['HomePlanet'].fillna('Earth', inplace=True)\n\ntrain['CryoSleep'].fillna(False, inplace=True)\ntest['CryoSleep'].fillna(False, inplace=True)\n\ntrain['Destination'].fillna('TRAPPIST-1e', inplace=True)\ntest['Destination'].fillna('TRAPPIST-1e', inplace=True)\n\ntrain['VIP'].fillna(False, inplace=True)\ntest['VIP'].fillna(False, inplace=True)\n\n# Impute 0's (mode) to Exp_features, because we will create a categorical column from this later\nfor col in exp_features:\n    train.loc[train[col].isna(),col]=0\n    test.loc[test[col].isna(),col]=0","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.287534Z","iopub.execute_input":"2022-04-03T14:45:09.287738Z","iopub.status.idle":"2022-04-03T14:45:09.316117Z","shell.execute_reply.started":"2022-04-03T14:45:09.287712Z","shell.execute_reply":"2022-04-03T14:45:09.31545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Qualitative data","metadata":{}},{"cell_type":"code","source":"# Impute outliers (for qualitative data)\ntrain['Cabin'].fillna('Z/9999/Z', inplace=True)\ntest['Cabin'].fillna('Z/9999/Z', inplace=True)\n\ntrain['Name'].fillna('No Name', inplace=True)\ntest['Name'].fillna('No Name', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.317414Z","iopub.execute_input":"2022-04-03T14:45:09.318097Z","iopub.status.idle":"2022-04-03T14:45:09.328884Z","shell.execute_reply.started":"2022-04-03T14:45:09.318044Z","shell.execute_reply":"2022-04-03T14:45:09.32825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"\n### Age status\n\nIndentify children, adolescents and adults.","metadata":{}},{"cell_type":"code","source":"# New features - training set\ntrain['Under_18']=(train['Age']<18).astype(int)\ntrain['18_to_25']=((train['Age']>=18) & (train['Age']<=25)).astype(int)\ntrain['Over_25']=(train['Age']>25).astype(int)\n\n# New features - test set\ntest['Under_18']=(test['Age']<18).astype(int)\ntest['18_to_25']=((test['Age']>=18) & (test['Age']<=25)).astype(int)\ntest['Over_25']=(test['Age']>25).astype(int)\n\n# Plot distribution of new features\ntrain['Age_plot']=train['Under_18']+2*train['18_to_25']+3*train['Over_25']\nplt.figure(figsize=(10, 6))\ng=sns.countplot(data=train, x='Age_plot', hue='Transported')\nplt.title('Age status distribution')\ng.set_xticklabels(['Under 18', '18-25', 'Over 25'])\ntrain.drop('Age_plot', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.329966Z","iopub.execute_input":"2022-04-03T14:45:09.330254Z","iopub.status.idle":"2022-04-03T14:45:09.602919Z","shell.execute_reply.started":"2022-04-03T14:45:09.330222Z","shell.execute_reply":"2022-04-03T14:45:09.602047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Expenditure\n\nCalculate total expenditure and identify passengers with no expenditure.","metadata":{}},{"cell_type":"code","source":"# New features - training set\ntrain['Expenditure']=train[exp_features].sum(axis=1)\ntrain['No_spending']=(train['Expenditure']==0).astype(int)\n\n# New features - test set\ntest['Expenditure']=test[exp_features].sum(axis=1)\ntest['No_spending']=(test['Expenditure']==0).astype(int)\n\n# Plot distribution of new features\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nsns.histplot(data=train, x='Expenditure', hue='Transported', kde = True, bins=200)\nplt.title('Total expenditure (truncated)')\nplt.ylim([0,200])\nplt.xlim([0,15000])\n\nplt.subplot(1,2,2)\nsns.countplot(data=train, x='No_spending', hue='Transported', palette = 'viridis')\nplt.title('No spending indicator')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:50:10.20223Z","iopub.execute_input":"2022-04-03T14:50:10.202518Z","iopub.status.idle":"2022-04-03T14:50:11.582589Z","shell.execute_reply.started":"2022-04-03T14:50:10.202487Z","shell.execute_reply":"2022-04-03T14:50:11.581461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Passenger group\n\nExtract passenger group and group size from PassengerId.","metadata":{}},{"cell_type":"code","source":"# New features - training set\ntrain['Group'] = train['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\ntrain['Group_size']=train['Group'].map(lambda x: train['Group'].value_counts()[x])\n\n# New features - test set\ntest['Group'] = test['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\ntest['Group_size']=test['Group'].map(lambda x: test['Group'].value_counts()[x])\n\n# Plot distribution of new features\nplt.figure(figsize=(12,12))\nplt.subplot(2,1,1)\nsns.histplot(data=train, x='Group', hue='Transported', bins = 200, kde = True)\n\nplt.subplot(2,1,2)\nsns.countplot(data=train, x='Group_size', hue='Transported')\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.767719Z","iopub.status.idle":"2022-04-03T14:45:09.768581Z","shell.execute_reply.started":"2022-04-03T14:45:09.768327Z","shell.execute_reply":"2022-04-03T14:45:09.768358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can't really use the Group feature because it has too big of a cardinality (6217) and would explode the number of dimensions with one-hot encoding.\n\nThe Group size on the other hand should be a useful feature. In fact, we can compress the feature further by creating a 'Solo' column that tracks whether someone is travelling on their own or not. The figure on the right shows that group size=1 is less likely to be transported than group size>1.","metadata":{}},{"cell_type":"code","source":"# New feature\ntrain['Solo']=(train['Group_size']==1).astype(int)\ntest['Solo']=(test['Group_size']==1).astype(int)\n\n# New feature distribution\nplt.figure(figsize=(10,6))\nsns.countplot(data=train, x='Solo', hue='Transported')\nplt.title('Passenger travelling solo or not')\nplt.ylim([0,3000])","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.769596Z","iopub.status.idle":"2022-04-03T14:45:09.769905Z","shell.execute_reply.started":"2022-04-03T14:45:09.769738Z","shell.execute_reply":"2022-04-03T14:45:09.769755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cabin location\n\nExtract deck, number and side from cabin feature.","metadata":{}},{"cell_type":"code","source":"# New features - training set\ntrain['Cabin_deck'] = train['Cabin'].apply(lambda x: x.split('/')[0])\ntrain['Cabin_number'] = train['Cabin'].apply(lambda x: x.split('/')[1]).astype(int)\ntrain['Cabin_side'] = train['Cabin'].apply(lambda x: x.split('/')[2])\n\n# New features - test set\ntest['Cabin_deck'] = test['Cabin'].apply(lambda x: x.split('/')[0])\ntest['Cabin_number'] = test['Cabin'].apply(lambda x: x.split('/')[1]).astype(int)\ntest['Cabin_side'] = test['Cabin'].apply(lambda x: x.split('/')[2])\n\n# Plot distribution of new features\nfig=plt.figure(figsize=(12,18))\nplt.subplot(3,1,1)\nsns.countplot(data=train, x='Cabin_deck', hue='Transported', order=['A','B','C','D','E','F','G','T','Z'])\nplt.title('Cabin deck')\n\nplt.subplot(3,1,2)\nsns.histplot(data=train, x='Cabin_number', hue='Transported',binwidth=20, kde = True)\nplt.vlines(300, ymin=0, ymax=200, color='black')\nplt.vlines(600, ymin=0, ymax=200, color='black')\nplt.vlines(900, ymin=0, ymax=200, color='black')\nplt.vlines(1200, ymin=0, ymax=200, color='black')\nplt.vlines(1500, ymin=0, ymax=200, color='black')\nplt.vlines(1800, ymin=0, ymax=200, color='black')\nplt.title('Cabin number')\nplt.xlim([0,2000])\n\nplt.subplot(3,1,3)\nsns.countplot(data=train, x='Cabin_side', hue='Transported')\nplt.title('Cabin side')\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.770752Z","iopub.status.idle":"2022-04-03T14:45:09.771087Z","shell.execute_reply.started":"2022-04-03T14:45:09.770915Z","shell.execute_reply":"2022-04-03T14:45:09.770939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Wow, this is interesting!** It appears that Cabin_number is grouped into chunks of 300 cabins. This means we can compress this feature into a categorical one, which indicates which chunk each passenger is in.\n\n**Other notes:** The cabin deck 'T' seems to be an outlier (there are only 5 samples).","metadata":{}},{"cell_type":"code","source":"# New features - training set\ntrain['Cabin_region1']=(train['Cabin_number']<300).astype(int)   # one-hot encoding\ntrain['Cabin_region2']=((train['Cabin_number']>=300) & (train['Cabin_number']<600)).astype(int)\ntrain['Cabin_region3']=((train['Cabin_number']>=600) & (train['Cabin_number']<900)).astype(int)\ntrain['Cabin_region4']=((train['Cabin_number']>=900) & (train['Cabin_number']<1200)).astype(int)\ntrain['Cabin_region5']=((train['Cabin_number']>=1200) & (train['Cabin_number']<1500)).astype(int)\ntrain['Cabin_region6']=((train['Cabin_number']>=1500) & (train['Cabin_number']<1800)).astype(int)\ntrain['Cabin_region7']=(train['Cabin_number']>=1800).astype(int)\n\n# New features - test set\ntest['Cabin_region1']=(test['Cabin_number']<300).astype(int)   # one-hot encoding\ntest['Cabin_region2']=((test['Cabin_number']>=300) & (test['Cabin_number']<600)).astype(int)\ntest['Cabin_region3']=((test['Cabin_number']>=600) & (test['Cabin_number']<900)).astype(int)\ntest['Cabin_region4']=((test['Cabin_number']>=900) & (test['Cabin_number']<1200)).astype(int)\ntest['Cabin_region5']=((test['Cabin_number']>=1200) & (test['Cabin_number']<1500)).astype(int)\ntest['Cabin_region6']=((test['Cabin_number']>=1500) & (test['Cabin_number']<1800)).astype(int)\ntest['Cabin_region7']=(test['Cabin_number']>=1800).astype(int)\n\n# Plot distribution of new features\nplt.figure(figsize=(12,6))\ntrain['Cabin_regions_plot']=(train['Cabin_region1']+2*train['Cabin_region2']+3*train['Cabin_region3']+4*train['Cabin_region4']+5*train['Cabin_region5']+6*train['Cabin_region6']+7*train['Cabin_region7']).astype(int)\nsns.countplot(data=train, x='Cabin_regions_plot', hue='Transported', palette = 'viridis')\nplt.title('Cabin regions')\ntrain.drop('Cabin_regions_plot', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.772255Z","iopub.status.idle":"2022-04-03T14:45:09.772575Z","shell.execute_reply.started":"2022-04-03T14:45:09.772402Z","shell.execute_reply":"2022-04-03T14:45:09.772425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Last name\n\nCalculate family size from last name.","metadata":{}},{"cell_type":"code","source":"# New features - training set\ntrain['Surname']=train['Name'].str.split().str[-1]\ntrain['Family_size']=train['Surname'].map(lambda x: train['Surname'].value_counts()[x])\n\n# New features - test set\ntest['Surname']=test['Name'].str.split().str[-1]\ntest['Family_size']=test['Surname'].map(lambda x: test['Surname'].value_counts()[x])\n\n# Set outliers (no name) to have no family\ntrain.loc[train['Family_size']==200,'Family_size']=0\ntest.loc[test['Family_size']==200,'Family_size']=0\n\n# New feature distribution\nplt.figure(figsize=(12,6))\nsns.countplot(data=train, x='Family_size', hue='Transported')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.77445Z","iopub.status.idle":"2022-04-03T14:45:09.775078Z","shell.execute_reply.started":"2022-04-03T14:45:09.774892Z","shell.execute_reply":"2022-04-03T14:45:09.774914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"markdown","source":"**Drop unwanted features**","metadata":{}},{"cell_type":"code","source":"# Drop qualitative/redundant/high cardinality features\ntrain.drop(['PassengerId', 'Cabin', 'Name', 'Surname', 'VIP', 'Group', 'Cabin_number'], axis=1, inplace=True)\ntest.drop(['PassengerId', 'Cabin', 'Name', 'Surname', 'VIP', 'Group', 'Cabin_number'], axis=1, inplace=True)\n\n# Preview resulting training set\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.776162Z","iopub.status.idle":"2022-04-03T14:45:09.776526Z","shell.execute_reply.started":"2022-04-03T14:45:09.776351Z","shell.execute_reply":"2022-04-03T14:45:09.776375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We still need to encode the categorical columns.\n\n### Labels and features","metadata":{}},{"cell_type":"code","source":"# Sklearn\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score\nfrom sklearn.metrics import roc_auc_score, plot_confusion_matrix, plot_roc_curve, roc_curve\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.decomposition import PCA\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.utils import resample","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.777938Z","iopub.status.idle":"2022-04-03T14:45:09.778611Z","shell.execute_reply.started":"2022-04-03T14:45:09.778416Z","shell.execute_reply":"2022-04-03T14:45:09.778443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=train['Transported'].copy().astype(int)\nX=train.drop('Transported', axis=1).copy()\nX_test=test.copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.779711Z","iopub.status.idle":"2022-04-03T14:45:09.780226Z","shell.execute_reply.started":"2022-04-03T14:45:09.780001Z","shell.execute_reply":"2022-04-03T14:45:09.780024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding and scaling\n\nWe will use column transformers to be more professional. It's also good practice.","metadata":{}},{"cell_type":"code","source":"# Indentify numerical and categorical columns\nnumerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\ncategorical_cols = [cname for cname in X.columns if X[cname].dtype == \"object\"]\n\n# Scale numerical data to have mean=0 and variance=1\nnumerical_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n\n# One-hot encode categorical data\ncategorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(drop='if_binary', handle_unknown='ignore',sparse=False))])\n\n# Combine preprocessing\nct = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)],\n        remainder='passthrough')\n\n# Apply preprocessing\nX = ct.fit_transform(X)\nX_test = ct.transform(X_test)\n\n# Print new shape\nprint('Training set shape:', X.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.781445Z","iopub.status.idle":"2022-04-03T14:45:09.781764Z","shell.execute_reply.started":"2022-04-03T14:45:09.781593Z","shell.execute_reply":"2022-04-03T14:45:09.781615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PCA\n\nJust for fun, let's look at the transformed data in PCA space. This gives a low dimensional representation of the data, which preserves local and global structure.","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=3)\ncomponents = pca.fit_transform(X)\n\ntotal_var = pca.explained_variance_ratio_.sum() * 100\n\nfig = px.scatter_3d(\n    components, x=0, y=1, z=2, color=y, size=0.1*np.ones(len(X)), opacity = 1,\n    title=f'Total Explained Variance: {total_var:.2f}%',\n    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'},\n    width=800, height=500\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.783471Z","iopub.status.idle":"2022-04-03T14:45:09.783986Z","shell.execute_reply.started":"2022-04-03T14:45:09.783781Z","shell.execute_reply":"2022-04-03T14:45:09.783807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explained variance (how important each additional principal component is)\npca = PCA().fit(X)\nfig, ax = plt.subplots(figsize=(15,6))\nxi = np.arange(1, 1+X.shape[1], step=1)\nyi = np.cumsum(pca.explained_variance_ratio_)\nplt.plot(xi, yi, marker='o', linestyle='--', color='b')\n\n# Aesthetics\nplt.ylim(0.0,1.1)\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(1, 1+X.shape[1], step=1))\nplt.ylabel('Cumulative variance (%)')\nplt.title('Explained variance by each component')\nplt.axhline(y=1, color='r', linestyle='-')\nplt.text(0.5, 0.85, '100% cut-off threshold', color = 'red')\nax.grid(axis='x')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.785123Z","iopub.status.idle":"2022-04-03T14:45:09.785507Z","shell.execute_reply.started":"2022-04-03T14:45:09.785335Z","shell.execute_reply":"2022-04-03T14:45:09.785357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a validation set\n\nWe will use this to choose which model(s) to use","metadata":{}},{"cell_type":"code","source":"# Train-validation split\nX_train, X_valid, y_train, y_valid = train_test_split(X,y,stratify=y,train_size=0.8,test_size=0.2,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.786645Z","iopub.status.idle":"2022-04-03T14:45:09.786986Z","shell.execute_reply.started":"2022-04-03T14:45:09.786807Z","shell.execute_reply":"2022-04-03T14:45:09.78683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model selection","metadata":{}},{"cell_type":"markdown","source":"### To briefly mention the algorithms we will use,\n\n**Logistic Regression:** Unlike regression which uses Least Squares, the model uses Maximum Likelihood to fit a sigmoid-curve on the target variable distribution. It uses a logistic function, and most commonly used when the data in question has binary output.\n\n**K-Nearest Neighbors (KNN):** KNN predicts by selecting the majority class of the k-nearest neighbours. The metric used is usually Euclidean distance. It is a simple and effective algorithm but can be sensitive by many factors, e.g. the value of k, the preprocessing done to the data and the metric used.\n\n**Support Vector Machine (SVM):** SVM finds the optimal hyperplane that seperates the data in the feature space. Predictions are made by looking at which side of the hyperplane the test point lies on. Ordinary SVM assumes the data is linearly separable, which is not always the case. A kernel trick can be used when this assumption fails to transform the data into a higher dimensional space where it is linearly seperable. SVM is a popular algorithm because it is computationally effecient and produces very good results.\n\n**Random Forest (RF):** RF is a reliable ensemble of decision trees, which can be used for regression or classification problems. Here, the individual trees are built via bagging (i.e. aggregation of bootstraps which are nothing but multiple train datasets created via sampling with replacement) and split using fewer features. The resulting diverse forest of uncorrelated trees exhibits reduced variance; therefore, is more robust towards change in data and carries its prediction accuracy to new data. It works well with both continuous & categorical data.\n\n**Extreme Gradient Boosting (XGBoost):** XGBoost is similar to RF in that it is made up of an ensemble of decision-trees. The difference arises in how those trees as derived. XGboost uses extreme gradient boosting when optimising its objective function. It often produces the best results but is relatively slow compared to other gradient boosting algorithms.\n\n**Light Gradient Boosting Machine (LGBM):** LGBM works essentially the same as XGBoost but with a different boosting technique. It usually produces similar results to XGBoost but is significantly faster.\n\n**Categorical Boosting (CatBoost):** CatBoost is an open source algorithm based on gradient boosted decision trees. It supports numerical, categorical and text features. It works well with heterogeneous data and even relatively small data. Informally, it tries to take the best of both worlds from XGBoost and LGBM.\n\n**Naive Bayes (NB):** Naive Bayes learns how to classify samples by using Bayes' Theorem, which uses prior information to 'update' the probability of an event by incoorporateing this information in a clever way. The algorithm is quite fast but a downside is that it assumes the input features are independent, which is not always the case.\n\n\nWe will train these models and evaluate them on the validation set to then choose which ones to carry through to the next stage (cross validation).","metadata":{}},{"cell_type":"markdown","source":"### Define classifiers","metadata":{}},{"cell_type":"code","source":"# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.787944Z","iopub.status.idle":"2022-04-03T14:45:09.788276Z","shell.execute_reply.started":"2022-04-03T14:45:09.788081Z","shell.execute_reply":"2022-04-03T14:45:09.788098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classifiers\nclassifiers = {\n    \"LogisticRegression\" : LogisticRegression(random_state=0),\n    \"KNN\" : KNeighborsClassifier(),\n    \"SVC\" : SVC(random_state=0, probability=True),\n    \"RandomForest\" : RandomForestClassifier(random_state=0),\n    #\"XGBoost\" : XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='logloss'), # XGBoost takes too long\n    \"LGBM\" : LGBMClassifier(random_state=0),\n    \"CatBoost\" : CatBoostClassifier(random_state=0, verbose=False),\n    \"NaiveBayes\": GaussianNB()\n}\n\n# Grids for grid search\nLR_grid = {'penalty': ['l1','l2'],\n           'C': [0.25, 0.5, 0.75, 1, 1.25, 1.5],\n           'max_iter': [50, 100, 150]}\n\nKNN_grid = {'n_neighbors': [3, 5, 7, 9],\n            'p': [1, 2]}\n\nSVC_grid = {'C': [0.25, 0.5, 0.75, 1, 1.25, 1.5],\n            'kernel': ['linear', 'rbf'],\n            'gamma': ['scale', 'auto']}\n\nRF_grid = {'n_estimators': [50, 100, 150, 200, 250, 300],\n        'max_depth': [4, 6, 8, 10, 12]}\n\nboosted_grid = {'n_estimators': [50, 100, 150, 200],\n        'max_depth': [4, 8, 12],\n        'learning_rate': [0.05, 0.1, 0.15]}\n\nNB_grid={'var_smoothing': [1e-10, 1e-9, 1e-8, 1e-7]}\n\n# Dictionary of all grids\ngrid = {\n    \"LogisticRegression\" : LR_grid,\n    \"KNN\" : KNN_grid,\n    \"SVC\" : SVC_grid,\n    \"RandomForest\" : RF_grid,\n    \"XGBoost\" : boosted_grid,\n    \"LGBM\" : boosted_grid,\n    \"CatBoost\" : boosted_grid,\n    \"NaiveBayes\": NB_grid\n}","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.789316Z","iopub.status.idle":"2022-04-03T14:45:09.789608Z","shell.execute_reply.started":"2022-04-03T14:45:09.789453Z","shell.execute_reply":"2022-04-03T14:45:09.78947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train and evaluate models**\n\nTrain models with grid search (but no cross validation so it doesn't take too long) to get a rough idea of which are the best models for this dataset.","metadata":{}},{"cell_type":"code","source":"i=0\nclf_best_params=classifiers.copy()\nvalid_scores=pd.DataFrame({'Classifer':classifiers.keys(), 'Validation accuracy': np.zeros(len(classifiers)), 'Training time': np.zeros(len(classifiers))})\nfor key, classifier in classifiers.items():\n    start = time.time()\n    clf = GridSearchCV(estimator=classifier, param_grid=grid[key], n_jobs=-1, cv=None)\n\n    # Train and score\n    clf.fit(X_train, y_train)\n    valid_scores.iloc[i,1]=clf.score(X_valid, y_valid)\n\n    # Save trained model\n    clf_best_params[key]=clf.best_params_\n    \n    # Print iteration and training time\n    stop = time.time()\n    valid_scores.iloc[i,2]=np.round((stop - start)/60, 2)\n    \n    print('Model:', key)\n    print('Training time (mins):', valid_scores.iloc[i,2])\n    print('')\n    i+=1","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.791137Z","iopub.status.idle":"2022-04-03T14:45:09.791475Z","shell.execute_reply.started":"2022-04-03T14:45:09.791309Z","shell.execute_reply":"2022-04-03T14:45:09.791327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show results\nvalid_scores","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.792285Z","iopub.status.idle":"2022-04-03T14:45:09.792896Z","shell.execute_reply.started":"2022-04-03T14:45:09.792422Z","shell.execute_reply":"2022-04-03T14:45:09.792438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Motivated by this, we will take RandomForest, LGBM and CatBoost to the final stage of modelling","metadata":{}},{"cell_type":"code","source":"# Show best parameters from grid search\nclf_best_params","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.793756Z","iopub.status.idle":"2022-04-03T14:45:09.794058Z","shell.execute_reply.started":"2022-04-03T14:45:09.793902Z","shell.execute_reply":"2022-04-03T14:45:09.793919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"markdown","source":"We can finally train our best model on the whole training set using cross validation and ensembling predictions together to produce the most confident predictions.\n\n**Define best models**","metadata":{}},{"cell_type":"code","source":"# Classifiers\nbest_classifiers = {\n    \"RandomForest\" : RandomForestClassifier(**clf_best_params[\"RandomForest\"], random_state=0),\n    \"LGBM\" : LGBMClassifier(**clf_best_params[\"LGBM\"], random_state=0),\n    \"CatBoost\" : CatBoostClassifier(**clf_best_params[\"CatBoost\"], verbose=False, random_state=0),\n}","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.795707Z","iopub.status.idle":"2022-04-03T14:45:09.796031Z","shell.execute_reply.started":"2022-04-03T14:45:09.795855Z","shell.execute_reply":"2022-04-03T14:45:09.79588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cross validation and ensembling predictions**\n\nPredictions are ensembled together using soft voting. This averages the predicted probabilies to produce the most confident predictions.","metadata":{}},{"cell_type":"code","source":"# Number of folds in cross validation\nFOLDS=10\n\npreds=np.zeros(len(X_test))\nfor key, classifier in best_classifiers.items():\n    start = time.time()\n    \n    # 5-fold cross validation\n    cv = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=0)\n    \n    score=0\n    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n        # Get training and validation sets\n        X_train, X_valid = X[train_idx], X[val_idx]\n        y_train, y_valid = y[train_idx], y[val_idx]\n\n        # Train model\n        clf = classifier\n        clf.fit(X_train, y_train)\n\n        # Make predictions and measure accuracy\n        preds += clf.predict_proba(X_test)[:,1]\n        score += clf.score(X_valid, y_valid)\n\n    # Average accuracy    \n    score=score/FOLDS\n    \n    # Stop timer\n    stop = time.time()\n\n    # Print accuracy and time\n    print('Model:', key)\n    print('Average validation accuracy:', np.round(100*score,2))\n    print('Training time (mins):', np.round((stop - start)/60,2))\n    print('')\n    \n# Ensemble predictions\npreds=preds/(FOLDS*len(best_classifiers))","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.796883Z","iopub.status.idle":"2022-04-03T14:45:09.797221Z","shell.execute_reply.started":"2022-04-03T14:45:09.79702Z","shell.execute_reply":"2022-04-03T14:45:09.797037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"markdown","source":"### Post processing","metadata":{}},{"cell_type":"code","source":"# Round predictions to nearest integer\npreds=np.round(preds).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.798332Z","iopub.status.idle":"2022-04-03T14:45:09.798648Z","shell.execute_reply.started":"2022-04-03T14:45:09.798491Z","shell.execute_reply":"2022-04-03T14:45:09.798507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submit predictions","metadata":{}},{"cell_type":"code","source":"# Sample submission (to get right format)\nsub=pd.read_csv('../input/spaceship-titanic/sample_submission.csv')\n\n# Add predictions\nsub['Transported']=preds\n\n# Replace 0 to False and 1 to True\nsub=sub.replace({0:False,1:True})\n\n# Prediction distribution\nplt.figure(figsize=(6,6))\nsub['Transported'].value_counts().plot.pie(explode=[0.1,0.1], autopct='%1.1f%%', shadow=True, textprops={'fontsize':16}).set_title(\"Prediction distribution\")","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.79967Z","iopub.status.idle":"2022-04-03T14:45:09.800027Z","shell.execute_reply.started":"2022-04-03T14:45:09.799809Z","shell.execute_reply":"2022-04-03T14:45:09.799825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Output to csv\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T14:45:09.800954Z","iopub.status.idle":"2022-04-03T14:45:09.801304Z","shell.execute_reply.started":"2022-04-03T14:45:09.801093Z","shell.execute_reply":"2022-04-03T14:45:09.801116Z"},"trusted":true},"execution_count":null,"outputs":[]}]}