{"cells":[{"metadata":{"_uuid":"818c07d546724ff29299926397a63c5814980b85"},"cell_type":"markdown","source":"**Google Analytics Customer Revenue Prediction**\n\nTo predict - How much GStore's Revenue from each customers, using EDA and ML models.\n\nGiven in problem description, the 80/20 rule - 80% of revenue comes from only 20% of the customers.So our task is to predict the Gstore's revenue from those customers (using natural log of sum of all transactions per user).\n\n**Features** of the given dataset: \n* fullVisitorId- A unique identifier for each user of the Google Merchandise Store.\n* channelGrouping - The channel via which the user came to the Store.\n* date - The date on which the user visited the Store.\n* device - The specifications for the device used to access the Store.\n* geoNetwork - This section contains information about the geography of the user.\n* sessionId - A unique identifier for this visit to the store.\n* socialEngagementType - Engagement type, either \"Socially Engaged\" or \"Not Socially Engaged\".\n* totals - This section contains aggregate values across the session.\n* trafficSource - This section contains information about the Traffic Source from which the session originated.\n* visitId - An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId.\n* visitNumber - The session number for this user. If this is the first session, then this is set to 1.\n* visitStartTime - The timestamp (expressed as POSIX time)."},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true,"_uuid":"5ffd9f80ec36e2ac2ac1335040a3b02f848586de"},"cell_type":"code","source":"import json\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import model_selection, preprocessing, metrics\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dfa0da1144dce3f2bd5032d78dcb402c5d95395"},"cell_type":"markdown","source":"**Note:** \nSome columns contain serialised JSON as strings which should be deserialised and converted to seperate columns.\n"},{"metadata":{"_uuid":"72754b71c17b37e08d47962b400904ead29692df","trusted":true},"cell_type":"code","source":"def load_df(csv_path='./train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, #json.loads takes in a string and converts to dict or list object.\n                     dtype={'fullVisitorId': 'str'}, #convert id to string\n                     nrows=nrows)\n    \n\n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])  #converts semi-structured json to flat table.\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Shape: {df.shape}\")\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37d1ce822f9f4bbdb2284ff8bfcd131f2b556700","trusted":true},"cell_type":"code","source":"train_df = load_df(\"../input/train.csv\")\ntest_df = load_df(\"../input/test.csv\")\n#train_df.to_csv(\"train_sep_cols\")\n#test_df.to_csv(\"test_sep_cols\")\n#train_df = load_df(\"./train_sep_cols\")\n#test_df = load_df(\"./test_sep_cols\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83debb392630d73f482d25fb1fafe917a93117ad"},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c97ec016bf171edbb22878b7b2ef808cee9d72ed"},"cell_type":"markdown","source":"**Lets perform EDA**:\n\nEach row in the dataset is one visit to the store.\n\nLet us look at what all are the numerical variables and categorical variables we have in train dataset."},{"metadata":{"trusted":true,"_uuid":"160711ed778e4a732490364b14288a9043eb780d"},"cell_type":"code","source":"numeric_features = train_df.select_dtypes(include=[np.number])\nprint(numeric_features.columns)\n\ncategorical_features = train_df.select_dtypes(include=[np.object])\nprint(categorical_features.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3080b04a2b83baa6e9aad7299d5b8bb522c09480"},"cell_type":"markdown","source":"Note:  There are some variables in categorical_features which can be converted to fload and can be used as numeric variables, such as ['totals.bounces', 'totals.hits', 'totals.newVisits', 'totals.pageviews', 'totals.transactionRevenue',\n'totals.visits']."},{"metadata":{"_uuid":"d47ae7d3baba549539adc896419dcb0baaa5e465"},"cell_type":"markdown","source":"Now lets check whether the given dataset conforms 80/20 rule :\n\n* First convert values of totals.transactionRevenue to float \n* Group values according to fullVisitorId ( i.e we are calculating revenue from each customer )\n* Then consider only those customers who have revenue more than zero and find out the ratio. "},{"metadata":{"_uuid":"870d95b29045174fb4aa777952b04a01c48340a8","trusted":true},"cell_type":"code","source":"train_df[\"totals.transactionRevenue\"] = train_df[\"totals.transactionRevenue\"].astype('float')\ngrouped_revenue = train_df.groupby(\"fullVisitorId\")[\"totals.transactionRevenue\"].sum().reset_index()\n\nnon_zero_customers = (grouped_revenue[\"totals.transactionRevenue\"]>0).sum()\nprint(\"Number of unique customers with non-zero revenue : \", non_zero_customers, \"and the ratio is : \", non_zero_customers / grouped_revenue.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"So the ratio of revenue generating customers to total number of customers is 1.3%. From the above analysis it is confirmed that only 1.3% of the customers bring in revenue to Gstore."},{"metadata":{"_uuid":"3397da66a795e1e032cbb928df0c85ca0f64bb55"},"cell_type":"markdown","source":"**Data pre-processing:**\n\nSome columns have constant values and missing values. So, Lets examine that and drop those columns from feature set which would make our dataset more useful while training models.\n\n"},{"metadata":{"trusted":true,"_uuid":"798b02e13b95f98f02816b81bc627eac1a82de2d"},"cell_type":"code","source":"# convert the 'date' column values to datetime object\nimport datetime\ntrain_df['date'] = train_df['date'].apply(lambda x: datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])))\ntest_df['date'] = test_df['date'].apply(lambda x: datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21ffadde8972b409bbbd862dfce5cdd573ba2859"},"cell_type":"code","source":"consts = [c for c in train_df.columns if train_df[c].nunique(dropna=False)==1 ] #lets include nan values in the count(nunique())\nconsts","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c14480fe6ae613a6b254b6219503fe88c902ef71"},"cell_type":"markdown","source":"As we know that train set has 55 columns whereas test set has 53 ,lets examine what are those two extra variables in train set and which of those two can be target variable."},{"metadata":{"trusted":true,"_uuid":"37842a63f589c3bc61c75254c6fd38e2407f450a"},"cell_type":"code","source":"set(train_df.columns).difference(set(test_df.columns))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59ae9a092f50293243c26560967eeeb852362c30"},"cell_type":"markdown","source":"As 'trafficSource.campaignCode' is an extra feature not in test set (other than target variable 'totals.transactionRevenue' ).Lets drop this also. Even sessionId can be removed as this is just a unique number for each visit.\n\nSo lets drop consts (columns with constant values declared above),  'sessionId', 'trafficSource.campaignCode'."},{"metadata":{"trusted":true,"_uuid":"22280fbf84468dab80cc8fc15f0f1dd0cc33bba0"},"cell_type":"code","source":"cols_to_drop = consts + [\"sessionId\"] + [\"trafficSource.campaignCode\"]\ntrain_df = train_df.drop(cols_to_drop, axis=1)\ntest_df = test_df.drop(cols_to_drop[:-1], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf6442e295de858d14e3561a07c637cb60f0e8ce"},"cell_type":"markdown","source":"* Fill in missing values to 0.\n* Now, Identify categorical variables and convert to numbers i.e label encode them.\n* Identify numeric variables and convert them to floats.\n\n**Note**: Do not include IDs and dates to any of the above operations."},{"metadata":{"trusted":true,"_uuid":"d511ef3a9e350c0c048fd38dbdf8ab93431f76a4","scrolled":false},"cell_type":"code","source":"#train_df.head()\n#train_df.info()\n#train_df.describe()\n\ntrain_df[\"totals.transactionRevenue\"].fillna(0, inplace=True)\ntrain_y = train_df[\"totals.transactionRevenue\"].values\n\n#identify categorical variables and label encode them.\ncategorical_cols = [\"channelGrouping\", \"device.browser\", \n            \"device.deviceCategory\", \"device.operatingSystem\", \n            \"geoNetwork.city\", \"geoNetwork.continent\", \n            \"geoNetwork.country\", \"geoNetwork.metro\",\n            \"geoNetwork.networkDomain\", \"geoNetwork.region\", \n            \"geoNetwork.subContinent\", \"trafficSource.adContent\", \n            \"trafficSource.adwordsClickInfo.adNetworkType\", \n            \"trafficSource.adwordsClickInfo.gclId\", \n            \"trafficSource.adwordsClickInfo.page\", \n            \"trafficSource.adwordsClickInfo.slot\", \"trafficSource.campaign\",\n            \"trafficSource.keyword\", \"trafficSource.medium\", \n            \"trafficSource.referralPath\", \"trafficSource.source\",\n            'trafficSource.adwordsClickInfo.isVideoAd', 'trafficSource.isTrueDirect']\n\nfor col in categorical_cols:\n    print(col)\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))\n    train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))\n    test_df[col] = lbl.transform(list(test_df[col].values.astype('str')))\n\n\nnumeric_cols = [\"totals.hits\", \"totals.pageviews\", \"visitNumber\", \"visitStartTime\", 'totals.bounces',  'totals.newVisits']    \nfor col in numeric_cols:\n    train_df[col] = train_df[col].astype(float)\n    test_df[col] = test_df[col].astype(float)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c727df0fcd963153c59d59235a8a2a19342baae4"},"cell_type":"markdown","source":"As the training set contains data from August 1st 2016 to August 1st 2017. Then take cross-validation set as last three month's data which makes the ratio of train set to cross-val set  roughly 7.5 : 2.5 .(This is considering months but not no of examples in trainset , so might not be exactly 7.5 : 2.5)."},{"metadata":{"trusted":true,"_uuid":"ef002df940bf4403b5960ba003bcc7ed77427c57"},"cell_type":"code","source":"# Split the train dataset into development and valid based on time \ndev_df = train_df[train_df['date']<=datetime.date(2017,5,31)]\nval_df = train_df[train_df['date']>datetime.date(2017,5,31)]\n\ndev_y = np.log1p(dev_df[\"totals.transactionRevenue\"].values)\nval_y = np.log1p(val_df[\"totals.transactionRevenue\"].values)\n\ndev_X = dev_df[categorical_cols + numeric_cols] \nval_X = val_df[categorical_cols + numeric_cols] \ntest_X = test_df[categorical_cols + numeric_cols] ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4786d952c50855d552e25df9aa37bbe198e6dc7c"},"cell_type":"markdown","source":"**Choosing a model to train:**\n\nAs we have large dataset (with more columns >30 and rows > 10000)its better to use ensemble learning (gradient boosting models).we'll use LightGBM to train our model and predict revenue from test set."},{"metadata":{"trusted":true,"_uuid":"83aeb1f37378a8b06c2f7ec0b5e4b923b97b8064"},"cell_type":"code","source":"def run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\"objective\" : \"regression\",\"metric\" : \"rmse\", \n            \"subsample\" : 0.9,\"colsample_bytree\" : 0.9,\n            \"num_leaves\" : 31,\"min_child_samples\" : 100,\n            \"learning_rate\" : 0.03,\"bagging_fraction\" : 0.7,\n            \"feature_fraction\" : 0.5,\"bagging_frequency\" : 5,\n            \"bagging_seed\" : 2018,\"verbosity\" : -1}\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    model = lgb.train(params, lgtrain, 1000,early_stopping_rounds=100, valid_sets=[lgval],  verbose_eval=100)\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    pred_val_y = model.predict(val_X, num_iteration=model.best_iteration)\n    return pred_test_y, model, pred_val_y\n\n# Training the model #\npred_test, model, pred_val = run_lgb(dev_X, dev_y, val_X, val_y, test_X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e59b8f65ad4db19549d5d9cbad014358160c79b"},"cell_type":"markdown","source":" Let us compute the evaluation metric on the validation data. \n * Assign zero if predicted value is less than 0.\n * Prepare a validation dataframe. As predicted values are logarithmic values apply np.exp ( we'll convert this to log values again after grouping values acc to 'fullVisitorId' and calculating sum of revenue per user )\n * Apply log on sum for all the transactions per user (or apply sum first on grouped data and then apply log).\n * Calculate rms error."},{"metadata":{"trusted":true,"_uuid":"3a991ff90c88726993c105a31f03cdc7dcd2e72b"},"cell_type":"code","source":"from sklearn import metrics\n\npred_val[pred_val<0] = 0\nval_pred_df = pd.DataFrame({\"fullVisitorId\":val_df[\"fullVisitorId\"].values})\nval_pred_df[\"transactionRevenue\"] = val_df[\"totals.transactionRevenue\"].values\nval_pred_df[\"PredictedRevenue\"] = np.expm1(pred_val) #exp(x) -1 can also be used but expm1 gives greater precision when converting log\n\nval_pred_df = val_pred_df.groupby(\"fullVisitorId\")[\"transactionRevenue\", \"PredictedRevenue\"].sum().reset_index()\nval_pred_df[\"transactionRevenue\"] = np.log1p(val_pred_df[\"transactionRevenue\"].values)\nval_pred_df[\"PredictedRevenue\"] =  np.log1p(val_pred_df[\"PredictedRevenue\"].values)\n\n#Now apply rms to find out error\nprint(np.sqrt(metrics.mean_squared_error(val_pred_df[\"transactionRevenue\"].values, val_pred_df[\"PredictedRevenue\"].values)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e04f8e58fa07429833549bd6afa254d3bd61abe"},"cell_type":"markdown","source":"Prepare submission.csv :"},{"metadata":{"trusted":true,"_uuid":"cae624b3de3a1ac1ab4948a1f7deb8622351ee3e"},"cell_type":"code","source":"train_id = train_df[\"fullVisitorId\"].values\ntest_id = test_df[\"fullVisitorId\"].values   \nsubmit_df = pd.DataFrame({\"fullVisitorId\":test_id})\n\n#Repeat same steps as we did for cross-validation\npred_test[pred_test<0] = 0\nsubmit_df[\"PredictedLogRevenue\"] = np.expm1(pred_test)\nsubmit_df = submit_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsubmit_df[\"PredictedLogRevenue\"] = np.log1p(submit_df[\"PredictedLogRevenue\"])\n\nsubmit_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nprint(submit_df.head())\nsubmit_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6adc016ab7a60b6faf1dd9e23c07b96ae33aa13"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}