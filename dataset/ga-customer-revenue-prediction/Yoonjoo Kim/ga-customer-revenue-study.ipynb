{"cells":[{"metadata":{"_uuid":"bb32cc5d2722e96bb02725aaece54c5509bdacb9"},"cell_type":"markdown","source":"_Copied and edited from sudalairajkumar's notebook._\n\n**Objective of the notebook:**\n\nIn this notebook, let us explore the given dataset and make some inferences along the way. Also finally we will build a baseline light gbm model to get started. \n\n**Objective of the competition:**\n\nIn this competition, we a’re challenged to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics\nimport lightgbm as lgb\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6868c4286025cdb431e9f6c51e4ee714d594fca"},"cell_type":"markdown","source":"**About the dataset:**\n\nSimilar to most other kaggle competitions, we are given two datasets\n* train.csv\n* test.csv\n\nEach row in the dataset is one visit to the store. We are predicting the natural log of the sum of all transactions per user. \n    \nThe data fields in the given files are \n* fullVisitorId- A unique identifier for each user of the Google Merchandise Store.\n* channelGrouping - The channel via which the user came to the Store.\n* date - The date on which the user visited the Store.\n* device - The specifications for the device used to access the Store.\n* geoNetwork - This section contains information about the geography of the user.\n* sessionId - A unique identifier for this visit to the store.\n* socialEngagementType - Engagement type, either \"Socially Engaged\" or \"Not Socially Engaged\".\n* totals - This section contains aggregate values across the session.\n* trafficSource - This section contains information about the Traffic Source from which the session originated.\n* visitId - An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId.\n* visitNumber - The session number for this user. If this is the first session, then this is set to 1.\n* visitStartTime - The timestamp (expressed as POSIX time).\n\nAlso it is important to note that some of the fields are in json format. \n\nThanks to this [wonderful kernel](https://www.kaggle.com/julian3833/1-quick-start-read-csv-and-flatten-json-fields/notebook) by [Julian](https://www.kaggle.com/julian3833), we can convert all the json fields in the file to a flattened csv format which generally use in other competitions."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def load_df(csv_path='../input/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e08fe7f8ec7419ba476cd52664140ed6ae736a8c"},"cell_type":"code","source":"%%time\ntrain_df = load_df()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e08fe7f8ec7419ba476cd52664140ed6ae736a8c"},"cell_type":"code","source":"test_df = load_df(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"302c2b9449b6173ed44ad1d27d3547573368afd4"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcaa5e0839118e5e2d9192f450569309e8c3798d"},"cell_type":"markdown","source":"**Target Variable Exploration:**\n\nSince we are predicting the natural log of sum of all transactions of the user, let us sum up the transaction revenue at user level and take a log and then do a scatter plot."},{"metadata":{"trusted":true,"_uuid":"09744db3d19167bcb29c298ee5b248004bdb01c1"},"cell_type":"code","source":"train_df[\"totals.transactionRevenue\"] = train_df[\"totals.transactionRevenue\"].astype('float')\ngdf = train_df.groupby(\"fullVisitorId\")[\"totals.transactionRevenue\"].sum().reset_index()\n\nplt.figure(figsize=(8,6))\nplt.scatter(range(gdf.shape[0]), np.sort(np.log1p(gdf[\"totals.transactionRevenue\"].values)))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('TransactionRevenue', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45d9283ef187997aaf8e16acf8dbe47b7fc0a046"},"cell_type":"markdown","source":"Wow, This confirms the first two lines of the competition overview.\n    * The 80/20 rule has proven true for many businesses–only a small percentage of customers produce most of the revenue. As such, marketing teams are challenged to make appropriate investments in promotional strategies.\nInfact in this case, the ratio is even less.     "},{"metadata":{"trusted":true,"_uuid":"4a86c2c2d3a8191845bd799e1124c6d94fcc04ff"},"cell_type":"code","source":"nzi = pd.notnull(train_df[\"totals.transactionRevenue\"]).sum()\nnzr = (gdf[\"totals.transactionRevenue\"]>0).sum()\nprint(\"Number of instances in train set with non-zero revenue : \", nzi, \" and ratio is : \", nzi / train_df.shape[0])\nprint(\"Number of unique customers with non-zero revenue : \", nzr, \"and the ratio is : \", nzr / gdf.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"569d56cf56f6134872a47e89c8654aff2ca7562c"},"cell_type":"markdown","source":"So the ratio of revenue generating customers to customers with no revenue is in the ratio os 1.3%\n\nSince most of the rows have non-zero revenues, in the following plots let us have a look at the count of each category of the variable along with the number of instances where the revenue is not zero.\n\n** Number of visitors and common visitors:**\n\nNow let us look at the number of unique visitors in the train and test set and also the number of common visitors."},{"metadata":{"trusted":true,"_uuid":"dcc528a15dda0f853516375bd7e2e73aec09ac75"},"cell_type":"code","source":"print(\"Number of unique visitors in train set : \",train_df.fullVisitorId.nunique(), \" out of rows : \",train_df.shape[0])\nprint(\"Number of unique visitors in test set : \",test_df.fullVisitorId.nunique(), \" out of rows : \",test_df.shape[0])\nprint(\"Number of common visitors in train and test set : \",len(set(train_df.fullVisitorId.unique()).intersection(set(test_df.fullVisitorId.unique())) ))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"961119f995a317c8b937571890cf30ac66bd1969"},"cell_type":"markdown","source":"**Columns with constant values: **\n\nLooks like there are quite a few features with constant value in the train set. Let us get the list of these features. As pointed by Svitlana in the comments below, let us not include the columns which has constant value and some null values. "},{"metadata":{"trusted":true,"_uuid":"5eed694270d93fd7c2bc0a16a620fd329d59c2c1"},"cell_type":"code","source":"const_cols = [c for c in train_df.columns if train_df[c].nunique(dropna=False)==1 ]\nconst_cols","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66245f8c2294398511b88d68538578a962cbdf07"},"cell_type":"markdown","source":"They are quite a few. Since the values are constant, we can just drop them from our feature list and save some memory and time in our modeling process. \n\n**Device Information:**"},{"metadata":{"trusted":true,"_uuid":"c57e7e05554e2e19a694a934ffcef530baef47f4","_kg_hide-input":false},"cell_type":"code","source":"def horizontal_bar_chart(cnt_srs, color):\n    trace = go.Bar(\n        y=cnt_srs.index[::-1],\n        x=cnt_srs.values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n# Device Browser\ncnt_srs = train_df.groupby('device.browser')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace1 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'rgba(50, 171, 96, 0.6)')\ntrace2 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10), 'rgba(50, 171, 96, 0.6)')\ntrace3 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10), 'rgba(50, 171, 96, 0.6)')\n\n# Device Category\ncnt_srs = train_df.groupby('device.deviceCategory')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace4 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'rgba(71, 58, 131, 0.8)')\ntrace5 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10), 'rgba(71, 58, 131, 0.8)')\ntrace6 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10), 'rgba(71, 58, 131, 0.8)')\n\n# Operating system\ncnt_srs = train_df.groupby('device.operatingSystem')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace7 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'rgba(246, 78, 139, 0.6)')\ntrace8 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10),'rgba(246, 78, 139, 0.6)')\ntrace9 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10),'rgba(246, 78, 139, 0.6)')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=3, vertical_spacing=0.04, \n                          subplot_titles=[\"Device Browser - Count\", \"Device Browser - Non-zero Revenue Count\", \"Device Browser - Mean Revenue\",\n                                          \"Device Category - Count\",  \"Device Category - Non-zero Revenue Count\", \"Device Category - Mean Revenue\", \n                                          \"Device OS - Count\", \"Device OS - Non-zero Revenue Count\", \"Device OS - Mean Revenue\"])\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 3)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace5, 2, 2)\nfig.append_trace(trace6, 2, 3)\nfig.append_trace(trace7, 3, 1)\nfig.append_trace(trace8, 3, 2)\nfig.append_trace(trace9, 3, 3)\n\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Device Plots\")\npy.iplot(fig, filename='device-plots')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"832da86bcdc9fd153d4142f4bd5430c37915e271"},"cell_type":"markdown","source":"Inferences:\n* Device browser distribution looks similar on both the count and count of non-zero revenue plots\n* On the device category front, desktop seem to have higher percentage of non-zero revenue counts compared to mobile devices.\n* In device operating system, though the number of counts is more from windows, the number of counts where revenue is not zero is more for Macintosh.\n* Chrome OS also has higher percentage of non-zero revenue counts\n* On the mobile OS side, iOS has more percentage of non-zero revenue counts compared to Android \n\n**Date Exploration:**"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"59eb3e903949d8345fd8747aac85eab66c0f42cd"},"cell_type":"code","source":"import datetime\n\ndef scatter_plot(cnt_srs, color):\n    trace = go.Scatter(\n        x=cnt_srs.index[::-1],\n        y=cnt_srs.values[::-1],\n        showlegend=False,\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\ntrain_df['date'] = train_df['date'].apply(lambda x: datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])))\ncnt_srs = train_df.groupby('date')['totals.transactionRevenue'].agg(['size', 'count'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\"]\ncnt_srs = cnt_srs.sort_index()\n#cnt_srs.index = cnt_srs.index.astype('str')\ntrace1 = scatter_plot(cnt_srs[\"count\"], 'red')\ntrace2 = scatter_plot(cnt_srs[\"count of non-zero revenue\"], 'blue')\n\nfig = tools.make_subplots(rows=2, cols=1, vertical_spacing=0.08,\n                          subplot_titles=[\"Date - Count\", \"Date - Non-zero Revenue count\"])\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 2, 1)\nfig['layout'].update(height=800, width=800, paper_bgcolor='rgb(233,233,233)', title=\"Date Plots\")\npy.iplot(fig, filename='date-plots')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f35771ef1b28f298d1a266d24e8d2a77626ccf0"},"cell_type":"markdown","source":"Inferences:\n* We have data from 1 Aug, 2016 to 31 July, 2017 in our training dataset\n* In Nov 2016, though there is an increase in the count of visitors, there is no increase in non-zero revenue counts during that time period (relative to the mean)."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"08f4a090a5928029e4b47b660afd71d952002146"},"cell_type":"code","source":"test_df['date'] = test_df['date'].apply(lambda x: datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])))\ncnt_srs = test_df.groupby('date')['fullVisitorId'].size()\n\n\ntrace = scatter_plot(cnt_srs, 'red')\n\nlayout = go.Layout(\n    height=400,\n    width=800,\n    paper_bgcolor='rgb(233,233,233)',\n    title='Dates in Test set'\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"ActivationDate\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f98518587981622a86b31c3e3ce7ab70a2a349e"},"cell_type":"markdown","source":"In the test set, we have dates from 2 Aug, 2017 to 30 Apr, 2018. So there are no common dates between train and test set. So it might be a good idea to do time based validation for this dataset.  \n\n\n* Session date"},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [train_df, test_df]:\n    df['date_visit'] = pd.to_datetime(df['visitStartTime'], unit='s')\n    df['sess_date_dow'] = df['date_visit'].dt.dayofweek\n    df['sess_date_hours'] = df['date_visit'].dt.hour\n    df['sess_date_day'] = df['date_visit'].dt.day\n    \ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnt_srs1 = train_df.groupby('sess_date_dow')['totals.transactionRevenue'].agg(['size', 'count'])\ncnt_srs1.columns = [\"count\", \"count of non-zero revenue\"]\ncnt_srs1 = cnt_srs1.sort_index()\n#cnt_srs.index = cnt_srs.index.astype('str')\ntrace1 = scatter_plot(cnt_srs1[\"count\"], 'red')\ntrace2 = scatter_plot(cnt_srs1[\"count of non-zero revenue\"], 'blue')\n\ncnt_srs2 = train_df.groupby('sess_date_hours')['totals.transactionRevenue'].agg(['size', 'count'])\ncnt_srs2.columns = [\"count\", \"count of non-zero revenue\"]\ncnt_srs2 = cnt_srs2.sort_index()\ntrace3 = scatter_plot(cnt_srs2[\"count\"], 'red')\ntrace4 = scatter_plot(cnt_srs2[\"count of non-zero revenue\"], 'blue')\n\ncnt_srs3 = train_df.groupby('sess_date_day')['totals.transactionRevenue'].agg(['size', 'count'])\ncnt_srs3.columns = [\"count\", \"count of non-zero revenue\"]\ncnt_srs3 = cnt_srs3.sort_index()\ntrace5 = scatter_plot(cnt_srs3[\"count\"], 'red')\ntrace6 = scatter_plot(cnt_srs3[\"count of non-zero revenue\"], 'blue')\n\nfig = tools.make_subplots(rows=3, cols=2, vertical_spacing=0.1,\n                          subplot_titles=[\"Day of Week - count\", \"Day of Week - revenue\",\n                                          \"Date Hours - count\", \"Date Hours - revenue\",\n                                          \"Day of Month - count\", \"Day of Month - revenue\"])\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 2, 1)\nfig.append_trace(trace4, 2, 2)\nfig.append_trace(trace5, 3, 1)\nfig.append_trace(trace6, 3, 2)\n\n\nfig['layout'].update(height=800, width=800, paper_bgcolor='rgb(233,233,233)', title=\"Session Date Plots\")\npy.iplot(fig, filename='date-plots-sess')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that most sessions and transactions are being made in the weekdays, especially during the afternoon and night times.  \n- Which day of the month does not seem to be very effective."},{"metadata":{"_uuid":"3f98518587981622a86b31c3e3ce7ab70a2a349e"},"cell_type":"markdown","source":"**Geographic Information:**"},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":true,"_uuid":"fcb64598e3f9d0931e140f56c70d4dd5dc6f7c75"},"cell_type":"code","source":"# Continent\ncnt_srs = train_df.groupby('geoNetwork.continent')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace1 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'rgba(58, 71, 80, 0.6)')\ntrace2 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10), 'rgba(58, 71, 80, 0.6)')\ntrace3 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10), 'rgba(58, 71, 80, 0.6)')\n\n# Sub-continent\ncnt_srs = train_df.groupby('geoNetwork.subContinent')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace4 = horizontal_bar_chart(cnt_srs[\"count\"], 'orange')\ntrace5 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"], 'orange')\ntrace6 = horizontal_bar_chart(cnt_srs[\"mean\"], 'orange')\n\n# Network domain\ncnt_srs = train_df.groupby('geoNetwork.networkDomain')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace7 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'blue')\ntrace8 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10), 'blue')\ntrace9 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=3, vertical_spacing=0.08, horizontal_spacing=0.15, \n                          subplot_titles=[\"Continent - Count\", \"Continent - Non-zero Revenue Count\", \"Continent - Mean Revenue\",\n                                          \"Sub Continent - Count\",  \"Sub Continent - Non-zero Revenue Count\", \"Sub Continent - Mean Revenue\",\n                                          \"Network Domain - Count\", \"Network Domain - Non-zero Revenue Count\", \"Network Domain - Mean Revenue\"])\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 3)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace5, 2, 2)\nfig.append_trace(trace6, 2, 3)\nfig.append_trace(trace7, 3, 1)\nfig.append_trace(trace8, 3, 2)\nfig.append_trace(trace9, 3, 3)\n\nfig['layout'].update(height=1500, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Geography Plots\")\npy.iplot(fig, filename='geo-plots')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acba2fcee5fe10d5985a7daf9cc88aaa7a55f4b0"},"cell_type":"markdown","source":"Inferences:\n* On the continent plot, we can see that America has both higher number of counts as well as highest number of counts where the revenue is non-zero\n* Though Asia and Europe has high number of counts, the number of non-zero revenue counts from these continents are comparatively low. \n* We can infer the first two points from the sub-continents plot too.\n* If the network domain is \"unknown.unknown\" rather than \"(not set)\", then the number of counts with non-zero revenue tend to be lower. \n\n**Traffic Source:**\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"be180a4dbb76e68b449487a6dbc214a7fac35f61","scrolled":true},"cell_type":"code","source":"# Continent\ncnt_srs = train_df.groupby('trafficSource.source')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace1 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'green')\ntrace2 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10), 'green')\ntrace3 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10), 'green')\n\n# Sub-continent\ncnt_srs = train_df.groupby('trafficSource.medium')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace4 = horizontal_bar_chart(cnt_srs[\"count\"], 'purple')\ntrace5 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"], 'purple')\ntrace6 = horizontal_bar_chart(cnt_srs[\"mean\"], 'purple')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=2, cols=3, vertical_spacing=0.08, horizontal_spacing=0.15, \n                          subplot_titles=[\"Traffic Source - Count\", \"Traffic Source - Non-zero Revenue Count\", \"Traffic Source - Mean Revenue\",\n                                          \"Traffic Source Medium - Count\",  \"Traffic Source Medium - Non-zero Revenue Count\", \"Traffic Source Medium - Mean Revenue\"\n                                          ])\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 3)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace5, 2, 2)\nfig.append_trace(trace6, 2, 3)\n\nfig['layout'].update(height=1000, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Traffic Source Plots\")\npy.iplot(fig, filename='traffic-source-plots')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f36fe7649aba0b26a3026dd0fb6ec8e8b9b69168"},"cell_type":"markdown","source":"Inferences:\n* In the traffic source plot, though Youtube has high number of counts in the dataset, the number of non-zero revenue counts are very less. \n* Google plex has a high ratio of non-zero revenue count to total count in the traffic source plot. \n* On the traffic source medium, \"referral\" has more number of non-zero revenue count compared to \"organic\" medium.\n\n**Visitor Profile:**\n\nNow let us look at the visitor profile variables like number of pageviews by the visitor, number of hits by the visitor and see how they look."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"6b4042af5aa1c3b5aa79ba6d60d4af3554e510e7"},"cell_type":"code","source":"\n# Page views\ncnt_srs = train_df.groupby('totals.pageviews')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace1 = horizontal_bar_chart(cnt_srs[\"count\"].head(60), 'cyan')\ntrace2 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(60), 'cyan')\ntrace5 = horizontal_bar_chart(cnt_srs[\"mean\"].head(60), 'cyan')\n\n# Hits\ncnt_srs = train_df.groupby('totals.hits')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", 'mean']\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace3 = horizontal_bar_chart(cnt_srs[\"count\"].head(60), 'black')\ntrace4 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(60), 'black')\ntrace6 = horizontal_bar_chart(cnt_srs[\"mean\"].head(60), 'black')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=2, cols=3, vertical_spacing=0.08, horizontal_spacing=0.15, \n                          subplot_titles=[\"Total Pageviews - Count\", \"Total Pageviews - Non-zero Revenue Count\", \"Total Pageviews - Mean Revenue\",\n                                          \"Total Hits - Count\",  \"Total Hits - Non-zero Revenue Count\", \"Total Hits - Mean Revenue\"])\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace5, 1, 3)\nfig.append_trace(trace3, 2, 1)\nfig.append_trace(trace4, 2, 2)\nfig.append_trace(trace6, 2, 3)\n\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Visitor Profile Plots\")\npy.iplot(fig, filename='visitor-profile-plots')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa7d921a36f47ed34e55fc566f9781e625432db0"},"cell_type":"markdown","source":"Inferences:\n* Both these variables look very predictive\n* Count plot shows decreasing nature i.e. we have  a very high total count for less number of hits and page views per visitor transaction and the overall count decreases when the number of hits per visitor transaction increases.\n* On the other hand, we can clearly see that when the number of hits / pageviews per visitor transaction increases, we see that there is a high number of non-zero revenue counts. \n\n**Baseline Model:**\n\nNow let us build a baseline model on this dataset. Before we start building models, let us look at the variable names which are there in train dataset and not in test dataset. "},{"metadata":{"trusted":true,"_uuid":"d008bbab982f36a922928f1dbe079202c380df33"},"cell_type":"code","source":"print(\"Variables not in test but in train : \", set(train_df.columns).difference(set(test_df.columns)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0235902dbb12f2ac27c2c0442ce6b6ab50e9e61"},"cell_type":"markdown","source":"So apart from target variable, there is one more variable \"trafficSource.campaignCode\" not present in test dataset. So we need to remove this variable while building models. Also we can drop the constant variables which we got earlier.\n\nAlso we can remove the \"sessionId\" as it is a unique identifier of the visit."},{"metadata":{"trusted":true,"_uuid":"13d15eed12437ae974f3ec03af6a2b6fba75ec4c"},"cell_type":"code","source":"cols_to_drop = const_cols + ['sessionId'] + ['date_visit']\n\ntrain_df = train_df.drop(cols_to_drop + [\"trafficSource.campaignCode\"], axis=1)\ntest_df = test_df.drop(cols_to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f5ae722a5beceacaf32f52a6691a57f05884be0"},"cell_type":"markdown","source":"Now let us create development and validation splits based on time to build the model. We can take the last two months as validation sample."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b4cc2fa9b1580c9d8a7ad43b5f75657e150fba3"},"cell_type":"code","source":"# Impute 0 for missing target values\ntrain_df[\"totals.transactionRevenue\"].fillna(0, inplace=True)\ntrain_y = train_df[\"totals.transactionRevenue\"].values\ntrain_id = train_df[\"fullVisitorId\"].values\ntest_id = test_df[\"fullVisitorId\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"excluded_features = [\n    'date', 'fullVisitorId', 'sessionId', 'totals.transactionRevenue', 'visitId', \n    'visitStartTime', 'totals.bounces', 'totals.hits', 'totals.newVisits', 'totals.pageviews'\n]\n\ncategorical_features = [\n    _f for _f in train_df.columns\n    if (_f not in excluded_features) & (train_df[_f].dtype == 'object')\n]\n#categorical_features = categorical_features + ['sess_date_dow', ' sess_date_hours', 'sess_date_day']\ncategorical_features\ncat_features = categorical_features + ['sess_date_dow', 'sess_date_hours', 'sess_date_day']\ncat_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b4cc2fa9b1580c9d8a7ad43b5f75657e150fba3"},"cell_type":"code","source":"# label encode the categorical variables and convert the numerical variables to float\n\nfor col in cat_features:\n    print(col)\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))\n    train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))\n    test_df[col] = lbl.transform(list(test_df[col].values.astype('str')))\n\nnum_cols = [\"totals.hits\", \"totals.pageviews\", \"visitNumber\", \"visitStartTime\", 'totals.bounces',  'totals.newVisits']    \n#num_cols = [\"totals.hits\", \"totals.pageviews\", 'totals.bounces',  'totals.newVisits']    \nfor col in num_cols:\n    train_df[col] = train_df[col].astype(float)\n    test_df[col] = test_df[col].astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b4cc2fa9b1580c9d8a7ad43b5f75657e150fba3"},"cell_type":"code","source":"# Split the train dataset into train and validation sets based on time \ntrn_df = train_df[train_df['date']<=datetime.date(2017,5,31)]\nval_df = train_df[train_df['date']>datetime.date(2017,5,31)]\ntrn_y = np.log1p(trn_df[\"totals.transactionRevenue\"].values)\nval_y = np.log1p(val_df[\"totals.transactionRevenue\"].values)\n\ntrn_X = trn_df[cat_features + num_cols] \nval_X = val_df[cat_features + num_cols] \ntest_X = test_df[cat_features + num_cols] \n\nprint(trn_X.shape, val_X.shape, test_X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3de965f248a0bf1a536d9230ef0f3cb9789b0763"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\n\n\n#lgtrain = lgb.Dataset(trn_X, label=trn_y)\n#lgval = lgb.Dataset(val_X, label=val_y)\n#num_round = 1000\n#param1 = {'objective': 'binary', 'metric' : 'rmse'}\n#model = lgb.train(param1, lgtrain, num_round, valid_sets=[lgval], early_stopping_rounds=100, \n#                  verbose_eval=100)\n\n\"\"\"\nmodel = lgb.LGBMRegressor( \n    objective='regression',\n    random_state=10, \n    n_estimators=1000,\n    bagging_fraction=0.7,\n    early_stoping_rounds=100,\n    verbose_eval=100,\n    verbosity=-1,\n    learning_rate=0.1\n)\n\nparams_opt = {\n        \"num_leaves\" : [30, 200],\n        \"min_child_samples\" : [20,100],\n        \"feature_fraction\" : [0.5, 0.9],\n        \"bagging_frequency\" : [1,5],\n        \"bagging_seed\" : [100, 2000],\n}\n\ngridSearchCV = GridSearchCV(estimator = model, param_grid = params_opt, cv=2)\ngridSearchCV.fit(trn_X,trn_y)\ngridSearchCV.grid_scores_, gridSearchCV.best_params_, gridSearchCV.best_score_\n\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LightGBM Information\n\n- Light GBM is a gradient boosting framework that uses tree based learning algorithm.\n- Light GBM grows tree vertically (leaf-wise), unlike others that grow tree horizontally.\n- Using gradient-based method, it chooses the leaf with max delta loss to grow. This can reduce more loss than a level-wise algorithm.\n- High speed, Can handle large size of data, focuses on accuracy\n- Sensitive to overfitting, Would not recommend with small size of data\n\n**Core Parameters**\n\n1. task (default: 'train'): It specifies the task you want to do (training or prediction).\n2. objective (default: 'regression'): Specifies the application of your model. It can be regression, binary, multiclass etc.\n3. boosting (default: 'gbdt'): Specifies the type of boosting algorithm. It can be gbdt, rf, dart or goss. You can read more about them here.\n4. num_boost_round (default: 100): Number of boosting iterations.\n5. learning_rate (default: 0.1): Determines the impact of each tree on the final outcome.\n6. num_leaves (default: 31): Specifies the maximum number of trees in one tree.\n7. device_type (default: 'cpu'): Specifies the type of device you want to use (cpu or gpu).\n\n**Control parameters**\n1. max_depth (default: -1): It describes the maximum depth of the tree you want. If you think your model is overfitting just reduce the max_depth.\n2. min_data_in_leaf (default: 20): It describes minimum samples in a leaf. For eg., if it is 20 then it will not split. Increase its value when you feel your model is overfitting.\n3. feature_fraction (default: 1.0): Its value is <= 1.0. For eg., if it is 0.7 then the model will select 70% of the total features randomly in each iteration for building trees. Also used to avoid overfitting and better generalization of data.\n4. bagging_fraction (default: 1.0): Similar to feature_fration but used to rows i.e. it will randomly select part of data without resampling. Can reduce training time and overfitting.\n5. early_stopping_round (default: 0): Model will stop training if one metric of one validation data doesn't improve in last 'early_stopping_round' rounds.\n6. min_gain_to_split (default: 0.): It specifies the minimum gain required to split a leaf.\n7. lambda_l1 (default: 0.0): Parameter for L1 regularization\n8. lambda_l2 (default: 0.0): Parameter for L2 regularization.\n9. max_cat_to_onehot(default: 4): When number of categories for a particular columns is <= 'max_cat_to_onehot', one-vs-other split algorithm will be used.\n\n**Metric Parameters**\n1. metric (default: \"\"): Specifies the metric to use as a loss function. In the default case, the model will choose the metric relevant to the objective. You can choose from auc, binary_logloss, softmax, mae, mse and many more.\n2. is_provide_training_metric (default: False): Set this to true to output metric result over training dataset.\n3. metric_freq (default: 1): Defines frequency for metric output.\n\nTo see more, refer to the site below.\n- https://lightgbm.readthedocs.io/en/latest/Parameters.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"# custom function to run light gbm model\nparams1 = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\", \n        \"num_leaves\" : 30,\n        \"min_child_samples\" : 100,\n        \"learning_rate\" : 0.1,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.5,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 2018,\n        \"verbosity\" : -1\n    }\nparams2 = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\", \n        \"num_leaves\" : 200,\n        \"min_child_samples\" : 20,\n        \"learning_rate\" : 0.05,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.9,\n        \"bagging_frequency\" : 1,\n        \"bagging_seed\" : 100,\n        \"verbosity\" : -1\n    }\nparams3 = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\", \n        \"num_leaves\" : 200,\n        \"min_child_samples\" : 100,\n        \"learning_rate\" : 0.05,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.5,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 100,\n        \"verbosity\" : -1\n    }\nparams4 = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\", \n        \"num_leaves\" : 30,\n        \"min_child_samples\" : 20,\n        \"learning_rate\" : 0.05,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.9,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 100,\n        \"verbosity\" : -1\n    }\n\ndef run_lgb(train_X, train_y, val_X, val_y, test_X, params):\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    model = lgb.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=100)\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    pred_val_y = model.predict(val_X, num_iteration=model.best_iteration)\n    return pred_test_y, model, pred_val_y\n\n# Training the model #\npred_test1, model1, pred_val1 = run_lgb(trn_X, trn_y, val_X, val_y, test_X, params1)\npred_test2, model2, pred_val2 = run_lgb(trn_X, trn_y, val_X, val_y, test_X, params2)\npred_test3, model3, pred_val3 = run_lgb(trn_X, trn_y, val_X, val_y, test_X, params3)\npred_test4, model4, pred_val4 = run_lgb(trn_X, trn_y, val_X, val_y, test_X, params4)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a69ff5f2cb2b33b59056acd8b3048a189a20cd5a"},"cell_type":"markdown","source":"Now let us compute the evaluation metric on the validation data as mentioned in [this new discussion thread](https://www.kaggle.com/c/ga-customer-revenue-prediction/discussion/66737). So we need to do a sum for all the transactions of the user and then do a log transformation on top. Let us also make the values less than 0 to 0 as transaction revenue can only be 0 or more. "},{"metadata":{"trusted":true,"_uuid":"360ffb4d501ac2e35b274ac15efd031495c85ec3"},"cell_type":"code","source":"from sklearn import metrics\npred_val4[pred_val4<0] = 0\nval_pred_df = pd.DataFrame({\"fullVisitorId\":val_df[\"fullVisitorId\"].values})\nval_pred_df[\"transactionRevenue\"] = val_df[\"totals.transactionRevenue\"].values\nval_pred_df[\"PredictedRevenue\"] = np.expm1(pred_val4)\n#print(np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"transactionRevenue\"].values), np.log1p(val_pred_df[\"PredictedRevenue\"].values))))\nval_pred_df = val_pred_df.groupby(\"fullVisitorId\")[\"transactionRevenue\", \"PredictedRevenue\"].sum().reset_index()\nprint(np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"transactionRevenue\"].values), np.log1p(val_pred_df[\"PredictedRevenue\"].values))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"574dae973bc73351f6325db2a596a008fc8f8338"},"cell_type":"markdown","source":"\nNow let us prepare the submission file similar to validation set."},{"metadata":{"trusted":true,"_uuid":"5a1b1b6dd5b277d59f8a4a06752c23bb8bd7a5ea"},"cell_type":"code","source":"sub_df = pd.DataFrame({\"fullVisitorId\":test_id})\npred_test4[pred_test4<0] = 0\nsub_df[\"PredictedLogRevenue\"] = np.expm1(pred_test4)\nsub_df = sub_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsub_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsub_df[\"PredictedLogRevenue\"] = np.log1p(sub_df[\"PredictedLogRevenue\"])\nsub_df.to_csv(\"baseline_lgb.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34da0a2ac754fb22062cd99f44f2422da7b35450"},"cell_type":"code","source":"sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4cbf691d8f8ce6bebe84b17f729cf7c7a0ad8c9b"},"cell_type":"markdown","source":"**Feature Importance:**\n\nNow let us have a look at the important features of the light gbm model."},{"metadata":{"trusted":true,"_uuid":"7a0da4195ac3e9683c29579b7700a21eb58fdcb8"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,18))\nlgb.plot_importance(model4, max_num_features=50, height=0.8, ax=ax)\nax.grid(False)\nplt.title(\"LightGBM - Feature Importance\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0458610aa52e885eaeaf83db91312d379424d15"},"cell_type":"markdown","source":"\"totals.pageviews\" turn out to be the most important feature followed by \"totals.hits\" and \"visitStartTime\". "},{"metadata":{"_uuid":"e2ed97686a372b89e874864f271765cdcbe9129b"},"cell_type":"markdown","source":"**More to come. Stay tuned.!**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}