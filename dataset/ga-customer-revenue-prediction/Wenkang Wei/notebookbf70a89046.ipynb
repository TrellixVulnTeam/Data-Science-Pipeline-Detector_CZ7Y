{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport json\nimport os\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\nimport plotly.graph_objs as go\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import TimeSeriesSplit, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\npd.options.display.max_columns = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch \nfrom torch import nn\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Google Analytics Customer Revenue Prediction\n"},{"metadata":{},"cell_type":"markdown","source":"## 1. Load Data and Format data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_path = \"../input/ga-customer-revenue-prediction/\"\ntrain_df = pd.read_csv(data_path+ \"train.csv\")\ntest_df = pd.read_csv(data_path+ \"test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extract json data from csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_df(csv_path='kaggle/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    df = pd.read_csv(csv_path,converters={column: json.loads for column in JSON_COLUMNS},dtype={'fullVisitorId': 'str'},nrows=nrows)\n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df\ntrain = load_df(csv_path = data_path+ \"train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Exploratory Data Analysis (EDA)"},{"metadata":{},"cell_type":"markdown","source":"Since the goal is to predict the total revenue per user, so we want to explore the total_revenue grouped by each user"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"totals.transactionRevenue\"] = train[\"totals.transactionRevenue\"].astype('float')\ngdf = train.groupby(\"fullVisitorId\")[\"totals.transactionRevenue\"].sum().reset_index()\n\nplt.figure(figsize=(8,6))\nplt.scatter(range(gdf.shape[0]), np.sort(np.log1p(gdf[\"totals.transactionRevenue\"].values)))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('TransactionRevenue', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"not_none_transaction = sum(gdf[\"totals.transactionRevenue\"]>0)\nprint(\"There are %d transactions and %d transactions have consumptions, %.3f%% are transactions are >0\"%( \n                                                                                len(gdf), not_none_transaction,\n                                                                                100*not_none_transaction/len(gdf)))\nnot_none_records = pd.notnull(train[\"totals.transactionRevenue\"]).sum()\nprint(\"There are %d out of %d records with NaN values. %.3f%% transactions are Nan \"%(not_none_records, len(train[\"totals.transactionRevenue\"]),\n                                                                                      100*not_none_records/len(train[\"totals.transactionRevenue\"])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visit_cnt = train.groupby(\"fullVisitorId\")[[\"totals.hits\"]].sum().reset_index()\ndf = visit_cnt.sample(1000)\nplt.figure(figsize=(8,6))\nplt.scatter(range(df.shape[0]), np.sort(df[\"totals.hits\"]))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('hits', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(20,8))\nplt.hist( x= np.sort(df[\"totals.hits\"]),orientation= \"vertical\",bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\n\ndef scatter_plot(cnt_srs, color):\n    trace = go.Scatter(\n        x=cnt_srs.index[::-1],\n        y=cnt_srs.values[::-1],\n        showlegend=False,\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\ntrain['date'] = train['date'].apply(lambda x: datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])))\ncnt_srs = train.groupby('date')['totals.transactionRevenue'].agg(['size', 'count'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\"]\ncnt_srs = cnt_srs.sort_index()\n#cnt_srs.index = cnt_srs.index.astype('str')\ntrace1 = scatter_plot(cnt_srs[\"count\"], 'red')\ntrace2 = scatter_plot(cnt_srs[\"count of non-zero revenue\"], 'blue')\n\nfig = tools.make_subplots(rows=2, cols=1, vertical_spacing=0.08,\n                          subplot_titles=[\"Date - Count\", \"Date - Non-zero Revenue count\"])\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 2, 1)\nfig['layout'].update(height=800, width=800, paper_bgcolor='rgb(233,233,233)', title=\"Date Plots\")\npy.iplot(fig, filename='date-plots')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True, inplace=True)\ntrain['trafficSource.isTrueDirect'].fillna(False, inplace=True)\n\n# remove columns with only one distinct value\ncols_to_drop = [col for col in train.columns if train[col].nunique(dropna=False) == 1]\ntrain.drop(cols_to_drop, axis=1, inplace=True)\n\n#only one not null value\ntrain.drop(['trafficSource.campaignCode'], axis=1, inplace=True)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = ['visitNumber', 'totals.hits', 'totals.pageviews', 'totals.bounces', 'totals.newVisits', 'totals.transactionRevenue']\n\nfor col in num_cols:\n    train[col] = train[col].fillna(0)\n    train[col] = train[col].astype(float)\n    train[col] = np.log1p(train[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['trafficSource.adContent'] = train['trafficSource.adContent'].fillna(0)\ntrain['trafficSource.keyword'] = train['trafficSource.keyword'].fillna(0)\ntrain['trafficSource.adwordsClickInfo.adNetworkType'] = train['trafficSource.adwordsClickInfo.adNetworkType'].fillna(0)\ntrain['trafficSource.adwordsClickInfo.gclId'] = train['trafficSource.adwordsClickInfo.gclId'].fillna(0)\ntrain['trafficSource.adwordsClickInfo.page'] = train['trafficSource.adwordsClickInfo.page'].fillna(0)\ntrain['trafficSource.adwordsClickInfo.slot'] = train['trafficSource.adwordsClickInfo.slot'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['device.browser'].nunique(),train['device.deviceCategory'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['browser_category'] = train['device.browser'] + '_' + train['device.deviceCategory']\ntrain['browser_operatingSystem'] = train['device.browser'] + '_' + train['device.operatingSystem']\ntrain['source_country'] = train['trafficSource.source'] + '_' + train['geoNetwork.country']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_use = [\"date\", \"fullVisitorId\", \"sessionId\", \"visitId\", \"visitStartTime\", 'totals.transactionRevenue', 'trafficSource.referralPath']\ncat_cols = [col for col in train.columns if col not in num_cols and col not in no_use]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Label Encoding to convert strings to labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_values = {}\nfor col in cat_cols:\n    print(col)\n    lbl = LabelEncoder()\n    lbl.fit(list(train[col].values.astype('str')))\n    train[col] = lbl.transform(list(train[col].values.astype('str')))\n    max_values[col] = train[col].max() + 2  # 根据经验，比真实值大一点，效果较好","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_col_labels1 = [\"channelGrouping\", \"device.deviceCategory\", \"device.operatingSystem\", \"geoNetwork.continent\",\n                   \"geoNetwork.subContinent\", \"trafficSource.adContent\", \"trafficSource.adwordsClickInfo.adNetworkType\",\n                   \"trafficSource.adwordsClickInfo.isVideoAd\", \"trafficSource.adwordsClickInfo.page\", \"trafficSource.adwordsClickInfo.slot\",\n                   \"trafficSource.campaign\", \"trafficSource.medium\", \"geoNetwork.region\"]\n\ncat_col_labels2 = [\"browser_category\", \"browser_operatingSystem\", \"source_country\", \"device.browser\", \"geoNetwork.city\",\n                   \"trafficSource.source\", \"trafficSource.keyword\", \"trafficSource.adwordsClickInfo.gclId\", \"geoNetwork.networkDomain\",\n                   \"geoNetwork.country\", \"geoNetwork.metro\", \"geoNetwork.region\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Split dataset into trainset and validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\n\ntrain = train.sort_values('date')\n\nx_train = train[train[\"date\"] <= pd.Timestamp(2017,5,31)]\nx_val = train[train[\"date\"] > pd.Timestamp(2017,5,31)]\n\ny_train = x_train['totals.transactionRevenue']\ny_val = x_val['totals.transactionRevenue']\n\nx_train = x_train.drop(no_use, axis=1)\nx_val = x_val.drop(no_use, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols.remove(\"totals.transactionRevenue\")\nnum_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emb_dims1 = []\nemb_dims2 = []\nfor i in cat_col_labels1:\n    emb_dims1.append((max_values[i], min((max_values[i]+1)//2, 50)))\nfor i in cat_col_labels2:\n    emb_dims2.append((max_values[i], min((max_values[i]+1)//2, 50)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Modeling"},{"metadata":{},"cell_type":"markdown","source":"# 5.1  LGBM model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\ntrain_set = lgb.Dataset(x_train, y_train)\nvalid_set = lgb.Dataset(x_val, y_val)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'boosting': 'gbdt',\n        'learning_rate': 0.3 ,\n        'verbose': 0,\n        'num_leaves': 90,\n        'bagging_fraction': 0.95,\n        'bagging_freq': 1,\n        'bagging_seed': 1,\n        'feature_fraction': 0.9,\n        'feature_fraction_seed': 1,\n        'max_bin': 256,\n        'max_depth': 15,\n        'num_rounds': 200,\n        'metric' : 'auc'\n    }\n\n%time model_f1 = lgb.train(params, train_set=train_set,  valid_sets=valid_set, verbose_eval=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.1 Wide and Deep Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TabularDataset(Dataset):\n    def __init__(self, x_data, y_data, cat_cols1, cat_cols2, num_cols):\n        \n        \"\"\"\n        data: pandas data frame;\n        cat_cols: list of string, the names of the categorical columns in the data, will be passed through the embedding layers;\n        num_cols: list of string\n        y_data: the target\n        \"\"\"\n        self.n = x_data.shape[0]\n        self.y = y_data.astype(np.float32).values.reshape(-1, 1)\n       \n        self.cat_cols1 = cat_cols1\n        self.cat_cols2 = cat_cols2\n        self.num_cols = num_cols\n        \n        self.num_X = x_data[self.num_cols].astype(np.float32).values\n        self.cat_X1 = x_data[self.cat_cols1].astype(np.int64).values\n        self.cat_X2 = x_data[self.cat_cols2].astype(np.int64).values\n        \n    \n    def print_data(self):\n        return self.num_X, self.cat_X1, self.cat_X2, self.y\n    \n    def __len__(self):\n        \"\"\"\n        total number of samples\n        \"\"\"\n        return self.n\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        Generates one sample of data.\n        \"\"\"\n        return [self.y[idx], self.num_X[idx], self.cat_X1[idx], self.cat_X2[idx]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FeedForwardNN(nn.Module):\n    def __init__(self, emb_dims1, emb_dims2, no_of_num, lin_layer_sizes, output_size, emb_dropout, lin_layer_dropouts):\n        \"\"\"\n        emb_dims:           List of two element tuples;\n        no_of_num:          Integer, the number of continuous features in the data;\n        lin_layer_sizes:    List of integers. The size of each linear layer;\n        output_size:        Integer, the size of the final output;\n        emb_dropout:        Float, the dropout to be used after the embedding layers.\n        lin_layer_dropouts: List of floats, the dropouts to be used after each linear layer.\n        \"\"\"\n        super().__init__()\n        \n        # embedding layers\n        self.emb_layers1 = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims1])\n        self.emb_layers2 = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims2])\n        \n        # 计算各个emb参数数量，为后续Linear layer的输入做准备\n        self.no_of_embs1 = sum([y for x, y in emb_dims1])\n        self.no_of_embs2 = sum([y for x, y in emb_dims2])\n        self.no_of_num = no_of_num\n        \n        # 分支1\n        self.branch1 = nn.Linear(self.no_of_embs1, lin_layer_sizes[0])\n        self.branch1_2 = nn.Linear(lin_layer_sizes[0], lin_layer_sizes[1])\n        nn.init.kaiming_normal_(self.branch1.weight.data)\n        nn.init.kaiming_normal_(self.branch1_2.weight.data)\n        \n        # 分支2\n        self.branch2 = nn.Linear(self.no_of_embs2, lin_layer_sizes[0] * 2)\n        self.branch2_2 = nn.Linear(lin_layer_sizes[0] * 2, lin_layer_sizes[1] * 2)\n        nn.init.kaiming_normal_(self.branch2.weight.data)\n        nn.init.kaiming_normal_(self.branch2_2.weight.data)\n        \n        # 主分支\n        self.main_layer1 = nn.Linear(lin_layer_sizes[1] * 3 + self.no_of_num, lin_layer_sizes[2])\n        self.main_layer2 = nn.Linear(lin_layer_sizes[2], lin_layer_sizes[3])\n        \n        # batch normal\n        self.branch_bn_layers1 = nn.BatchNorm1d(lin_layer_sizes[0])\n        self.branch_bn_layers2 = nn.BatchNorm1d(lin_layer_sizes[0] * 2)\n        self.main_bn_layer = nn.BatchNorm1d(lin_layer_sizes[2])\n        \n        # Dropout Layers\n        self.emb_dropout_layer = nn.Dropout(emb_dropout)\n        self.dropout_layers = nn.ModuleList([nn.Dropout(size) for size in lin_layer_dropouts])\n        \n        # Output layer\n        self.output_layer = nn.Linear(lin_layer_sizes[-1], output_size)\n        nn.init.kaiming_normal_(self.output_layer.weight.data)\n        \n    def forward(self, num_data, cat_data1, cat_data2):\n        # embedding categorical feature and cat them together\n        x1 = [emb_layer(torch.tensor(cat_data1[:, i])) for i, emb_layer in enumerate(self.emb_layers1)]\n        x1 = torch.cat(x1, 1)\n        \n        x1 = self.emb_dropout_layer(F.relu(self.branch1(x1)))\n        x1 = self.branch_bn_layers1(x1)\n        x1 = self.dropout_layers[0](F.relu(self.branch1_2(x1)))\n\n        x2 = [emb_layer(torch.tensor(cat_data2[:, i])) for i, emb_layer in enumerate(self.emb_layers2)]\n        x2 = torch.cat(x2, 1)\n        \n        x2 = self.emb_dropout_layer(F.relu(self.branch2(x2)))\n        x2 = self.branch_bn_layers2(x2)\n        x2 = self.dropout_layers[0](F.relu(self.branch2_2(x2)))\n\n        main = torch.cat([x1, x2, num_data], 1)\n\n        main = self.dropout_layers[1](F.relu(self.main_layer1(main)))\n        main = self.main_bn_layer(main)\n        main = self.dropout_layers[2](F.relu(self.main_layer2(main)))\n\n        out = self.output_layer(main)\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = TabularDataset(x_data=x_train, y_data=y_train, cat_cols1=cat_col_labels1, cat_cols2=cat_col_labels2, num_cols=num_cols)\nval_dataset = TabularDataset(x_data=x_val, y_data=y_val, cat_cols1=cat_col_labels1, cat_cols2=cat_col_labels2, num_cols=num_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batchsize = 64\ntrain_dataloader = DataLoader(train_dataset, batchsize, shuffle=True, num_workers=0)\nval_dataloder = DataLoader(val_dataset, 64, shuffle=True, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = FeedForwardNN(emb_dims1=emb_dims1, \n                      emb_dims2=emb_dims2, \n                      no_of_num=len(num_cols),\n                      lin_layer_sizes=[128,64,32,16],\n                      output_size=1,\n                      lin_layer_dropouts=[0.1, 0.1, 0.05],\n                      emb_dropout=0.05).to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"no_of_epochs = 5\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\ntotal_data = train_dataset.__len__()\n\nprint_every = 500\nsteps = 0\nrunning_loss = 0\nbest_val_score = 0\nbest_model = None\n\nfor epoch in range(no_of_epochs):\n    model.train()\n    for index, datas in enumerate(train_dataloader):\n        steps += 1\n        y, num_x, cat_x1, cat_x2 = datas\n        cat_x1 = cat_x1.to(device)\n        cat_x2 = cat_x2.to(device)\n        num_x = num_x.to(device)\n        y  = y.to(device)\n        \n        # Forward Pass\n        optimizer.zero_grad()\n        preds = model.forward(num_x, cat_x1, cat_x2)\n        loss = criterion(preds, y)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        \n        if steps % print_every == 0:\n            val_loss = 0\n            model.eval()\n            with torch.no_grad():\n                for val_index, val_datas in enumerate(val_dataloder):\n                    y, num_x, cat_x1, cat_x2 = val_datas\n                    cat_x1 = cat_x1.to(device)\n                    cat_x2 = cat_x2.to(device)\n                    num_x = num_x.to(device)\n                    y  = y.to(device)\n                    \n                    out = model.forward(num_x, cat_x1, cat_x2)\n                    val_acc = ((out>0.5)==y ).sum().detach().to('cpu').numpy()/len(out)\n                    \n                    batch_loss = criterion(out, y)\n                    val_loss += batch_loss.item()\n                    \n                    if val_acc> best_val_score:\n                        best_val_score = val_acc\n                        torch.save(model,\"checkpoint.pt\")\n            \n            print(f\"Epoch {epoch+1}/{no_of_epochs}..\"\n                     f\"Train loss:{running_loss/print_every:.3f}..\"\n                     f\"Validation loss:{val_loss/len(val_dataloder):.3f}..\")\n            running_loss = 0\n            model.train()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.2 LGBM model"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}