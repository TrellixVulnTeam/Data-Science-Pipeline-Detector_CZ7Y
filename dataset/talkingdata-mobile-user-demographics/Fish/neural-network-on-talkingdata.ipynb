{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd3bc0a6-598e-00b5-1a05-ceb9c1b1e2d6"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n%matplotlib inline\n#import seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn.metrics import log_loss\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n#from subprocess import check_output\n#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2828fb5a-3fa2-fd06-3e47-c9479303c9a7"},"outputs":[],"source":"datadir = '../input'\n# 把device_id作为行号，后面避免了搜索device的程序\n# 注意这里的ga_train和ga_test要一起处理，因为app_events等文件没有区分train和test，因此索引都是全部一起索引\nga_train = pd.read_csv(os.path.join(datadir,'gender_age_train.csv'), index_col='device_id')\nga_test = pd.read_csv(os.path.join(datadir,'gender_age_test.csv'), index_col='device_id') \n# 将行指标设置为event_id，这样就可以通过app_events找到app对应的event_id，然后通过event_id指标直接索引到device_id，最后通过ga_train将device_id的指标直接索引到gender和age\nevents = pd.read_csv(os.path.join(datadir,'events.csv'), index_col='event_id', parse_dates=['timestamp']) \n# 所有的app的is_installed都为1，因此此列无效，将其删去\napp_events = pd.read_csv(os.path.join(datadir,'app_events.csv'), usecols=['event_id','app_id','is_active'])\n# phone_brand里面有重复（对应同一个大品牌下面的不同device_model）\n##### 此处需要分析一下重复的数据\ndevice_brand = pd.read_csv(os.path.join(datadir,'phone_brand_device_model.csv'))\ndevice_brand = device_brand.drop_duplicates('device_id').set_index('device_id')\napp_labels = pd.read_csv(os.path.join(datadir,'app_labels.csv'))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"87df8cb2-e014-f07d-fe5b-a4643d5af93a"},"outputs":[],"source":"ga_train['trainrow'] = np.arange(ga_train.shape[0])\nga_test['testrow'] = np.arange(ga_test.shape[0])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cf881e18-31dc-9945-81d1-1f0561faa407"},"outputs":[],"source":"# 将brand编码成整数，可使用transform和inverse_transform进行brand和编码的转换\nbrand_encoder = LabelEncoder().fit(device_brand['phone_brand'])\ndevice_brand['brand'] = brand_encoder.transform(device_brand['phone_brand'])\nga_train['brand'] = device_brand['brand'] # 由于行标为device_id，因此device_id自动匹配\nga_test['brand'] = device_brand['brand']\n# 建立一个稀疏矩阵，行是device（对应的trainrow）,列是各个brand，值为1代表某个device对应是某个brand\nXtr_brand = csr_matrix((np.ones(ga_train.shape[0]), (ga_train['trainrow'], ga_train['brand'])))\nXte_brand = csr_matrix((np.ones(ga_test.shape[0]), (ga_test['testrow'], ga_test['brand'])))\nprint(Xtr_brand.shape, Xte_brand.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e09894e8-2a49-b3e6-9b29-26de2bef0b9b"},"outputs":[],"source":"m = device_brand.phone_brand.str.cat(device_brand.device_model)\nmodelencoder = LabelEncoder().fit(m)\ndevice_brand['model'] = modelencoder.transform(m)\nga_train['model'] = device_brand['model']\nga_test['model'] = device_brand['model']\nXtr_model = csr_matrix((np.ones(ga_train.shape[0]), \n                       (ga_train.trainrow, ga_train.model)))\nXte_model = csr_matrix((np.ones(ga_test.shape[0]), \n                       (ga_test.testrow, ga_test.model)))\nprint('Model features: train shape {}, test shape {}'.format(Xtr_model.shape, Xte_model.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee2aa2c7-5c45-b978-29cf-374273b3edba"},"outputs":[],"source":"# 将所有的app编码\napp_encoder = LabelEncoder().fit(app_events['app_id'])\napp_events['app'] = app_encoder.transform(app_events['app_id'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"11786c7d-1910-6234-ae0e-00142d1ce5d7"},"outputs":[],"source":"# 将app_events与events合并\ndevice_apps = app_events.merge(events[['device_id']], how='left',left_on='event_id',right_index=True)\n# 将相同的device合并，并记录每个device使用app的次数\ndevice_apps = device_apps.groupby(['device_id','app'])['app'].agg(['size'])\n# 将device_apps继续与ga_train和ga_test合并（仅合并行标），从而可以通过trainrow和testrow得到它们对应的分类\ndevice_apps = device_apps.merge(ga_train[['trainrow']], how='left', left_index=True, right_index=True)\ndevice_apps = device_apps.merge(ga_test[['testrow']], how='left', left_index=True, right_index=True)\ndevice_apps = device_apps.reset_index() # 原来是将device_id和app都设为行标，现在将其恢复为属性"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"053f4229-e31a-89a3-6692-97e9880931fe"},"outputs":[],"source":"napps = len(app_encoder.classes_)\n# 建立一个稀疏矩阵，行是device（对应的trainrow/testrow）,列是各个app，值为1代表某个device对应安装了某个app\nd = device_apps.dropna(subset=['trainrow']) # 取出有trainrow（testrow为NaN）的数据\nXtr_app = csr_matrix((np.ones(d.shape[0]), (d['trainrow'], d['app'])), shape=[ga_train.shape[0],napps])\nd = device_apps.dropna(subset=['testrow']) \nXte_app = csr_matrix((np.ones(d.shape[0]), (d['testrow'], d['app'])), shape=[ga_test.shape[0],napps])\n# 对应有app信息的设备数量大于有品牌信息的设备数量，说明不是所有的device都有对应的brand\nprint(Xtr_app.shape, Xte_app.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ba3e40f-15ad-07be-fcd5-b855af4227e8"},"outputs":[],"source":"# 将app编号加入到app_labels中\n# 因为app_labels里面有一些app是在events中没有出现的，因此只取出那些出现了的\napp_labels = app_labels.loc[app_labels.app_id.isin(app_events.app_id.unique())]\napp_labels['app'] = app_encoder.transform(app_labels['app_id'])\n# 将label重新编号\nlabel_encoder = LabelEncoder().fit(app_labels['label_id'])\napp_labels['label'] = label_encoder.transform(app_labels['label_id'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7ec10bb5-b8a5-1bab-c0a3-b6990f6f3ca4"},"outputs":[],"source":"device_labels = (device_apps[['device_id','app']]\n                .merge(app_labels[['app','label']])\n                .groupby(['device_id','label'])['app'].agg(['size'])\n                .merge(ga_train[['trainrow']], how='left', left_index=True, right_index=True)\n                .merge(ga_test[['testrow']], how='left', left_index=True, right_index=True)\n                .reset_index())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d2950201-7e48-76af-6b23-ee55a86d2232"},"outputs":[],"source":"nlabels = len(label_encoder.classes_) # 下面csr_matrix后面要加一个shape，不然可能由于中间函数筛选的原因使得大小不一致\nd = device_labels.dropna(subset=['trainrow'])\nXtr_label = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.label)), shape=(ga_train.shape[0],nlabels))\nd = device_labels.dropna(subset=['testrow'])\nXte_label = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.label)), shape=(ga_test.shape[0],nlabels))\nprint(Xtr_label.shape, Xte_label.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"cd4a06e9-b2b8-c560-6ab6-5589ee19117c"},"source":"Xtrain = hstack((Xtr_brand, Xtr_app, Xtr_label), format='csr')\nXtest =  hstack((Xte_brand, Xte_app, Xte_label), format='csr')\nprint(Xtrain.shape, Xtest.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f242b9a4-0a76-85cb-a2ee-4ed37567a253"},"outputs":[],"source":"X = hstack((Xtr_brand, Xtr_model, Xtr_app, Xtr_label), format='csr')\nX_test = hstack((Xte_brand, Xte_model, Xte_app, Xte_label), format='csr')\nprint(X.shape, X_test.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"71556256-df07-8be6-21e2-eb744194aa33"},"outputs":[],"source":"from sklearn.preprocessing import LabelBinarizer\ntarget_encoder = LabelEncoder().fit(ga_train['group'])\ntarget_encoder1 = LabelBinarizer(sparse_output=True).fit(ga_train['group'])\n#Y1 = target_encoder1.transform(ga_train['group'])\n#target_encoder2 = LabelBinarizer().fit(Y1)\nY = target_encoder1.transform(ga_train['group'])\nnclasses = len(target_encoder1.classes_)\n#app_labels\nprint(nclasses)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c0b9e7b9-78ff-ae8f-c75b-550c55294d79"},"source":"clf = LogisticRegression(C=0.02)\nclf.fit(Xtrain[:100], y[:100])"},{"cell_type":"markdown","metadata":{"_cell_guid":"8b4b4b34-6f43-873a-dc28-281dd450e84d"},"source":"pred = clf.predict_proba(Xtrain[70000:])\nlog_loss(y[70000:], pred)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c4ae5bff-40b2-b2ec-db4a-f88767e1b71a"},"source":"pred = pd.DataFrame(clf.predict_proba(Xtest), index=ga_test.index, columns=target_encoder.classes_)\npred.head()\npred.to_csv('logreg_subm.csv',index=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3a2a0e42-fc4c-b59b-12e8-58e13611772f"},"source":"#from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nj=5\ni = 650\nrdf = RandomForestClassifier(n_estimators=i, max_depth=None, #max_features=19860,#int(19860**0.5),\n      min_samples_split=j, random_state=0)\nrdf.fit(Xtrain[:70000], y[:70000])\npred = rdf.predict_proba(Xtrain[70000:])\nprint(log_loss(y[70000:], pred),'est:',i,'split:',j)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5abf914d-31dc-699b-e685-66b49adf67ad"},"source":"#from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nj=5\ni = 450\nrdf = RandomForestClassifier(n_estimators=i, max_depth=None, #max_features=19860,#int(19860**0.5),\n      min_samples_split=j, random_state=0)\nrdf.fit(Xtrain,y)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e8809248-e949-b528-ca9f-a4aae6ecc2c8"},"source":"2.26080338867 est: 380 split: 5\n2.26088936719 est: 400 split: 5\n2.26069933976 est: 420 split: 5\n2.26086483248 est: 440 split: 5\n2.26044390546 est: 450 split: 5\n2.2606303388 est: 460 split: 5\n2.26082674649 est: 470 split: 5\n2.2608811594 est: 500 split: 5\n2.26127479135 est: 600 split: 5\n2.26122217128 est: 700 split: 5\n2.261 est:1000 split:5\n\n2.26278305032 est: 600 split: 2\n2.26127479135 est: 600 split: 5"},{"cell_type":"markdown","metadata":{"_cell_guid":"0a2c9031-58fa-038e-4575-7094ad894fe4"},"source":"pred1 = rdf.predict_proba(Xtrain[70000:])\nlog_loss(y[70000:], pred1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0f93e2bc-a951-a1f3-1d28-87d00da34762"},"source":"pred = pd.DataFrame(rdf.predict_proba(Xtest), index=ga_test.index, columns=target_encoder.classes_)\npred.head()\npred.to_csv(os.path.join(datadir,'logreg_subm_randomforest.csv'),index=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0a354bb6-7435-50f0-4627-bc17d39c059b"},"source":"from sklearn.ensemble import GradientBoostingClassifier\ni = 90\nj = 0.05\ngbc = GradientBoostingClassifier(n_estimators=i, learning_rate=j,\n        max_depth=1, random_state=0).fit(Xtrain[:70000], y[:70000])\npred = gbc.predict_proba(Xtrain[70000:])\nprint(log_loss(y[70000:], pred), i, j)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3127228a-405f-3521-eedc-be7c2f444af3"},"source":"pred_gbc = pd.DataFrame(gbc.predict_proba(Xtest), index=ga_test.index, columns=target_encoder.classes_)\npred_gbc.head()\npred_gbc.to_csv('logreg_subm_gradientboosting.csv',index=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a6cef24-6842-1d54-69f2-ff9ad872bc9d"},"outputs":[],"source":"Xtest = X[70001:]\nytest = Y[70001:]\n\nXtrain = X[:70001]\nytrain = Y[:70001]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8bf79d8-7452-e4ee-c1f1-8c1e40841382"},"outputs":[],"source":"import tensorflow as tf\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\nN = 100\nM = 30\n# Create the model\nx = tf.placeholder(tf.float32, [None, 21527])\nW1 = weight_variable([21527, N])\nb1 = bias_variable([N])\ny1 = tf.nn.relu(tf.matmul(x, W1) + b1)\nkeep_prob1 = tf.placeholder(\"float\")\nfc1_drop = tf.nn.dropout(y1, keep_prob1)\n\nW2 = weight_variable([N, M])\nb2 = bias_variable([M])\ny2 = tf.nn.relu(tf.matmul(y1, W2) + b2)\nkeep_prob2 = tf.placeholder(\"float\")\nfc2_drop = tf.nn.dropout(y1, keep_prob2)\n\nW3 = weight_variable([M, 12])\nb3 = bias_variable([12])\ny = tf.matmul(y2, W3) + b3\n\n# Define loss and optimizer\ny_ = tf.placeholder(tf.float32, [None, 12])\n\n# The raw formulation of cross-entropy,\n#\n#   tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)),\n#                                 reduction_indices=[1]))\n#\n# can be numerically unstable.\n#\n# So here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n# outputs of 'y', and then average across the batch.\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\n#reg = tf.contrib.layers.apply_regularization(tf.contrib.layers.l2_regularizer(0.0001), \n                                             #weights_list=[W1, W2])\nloss = cross_entropy\n#train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n\nsess = tf.InteractiveSession()\ntf.global_variables_initializer().run()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c305e0c6-9602-6ef1-35d7-6298f84855d2"},"outputs":[],"source":"j0 = 30\ni0 = 100\nn0 = 100\n# Train\nfor j in range(j0):\n    for i in range(i0):\n        shuffle = np.random.choice(70000, size=100, replace=False)\n        sess.run(train_step, feed_dict={x: Xtrain[shuffle].toarray(), \n                                        y_: ytrain[shuffle].toarray(), \n                                        keep_prob1:0.5, \n                                        keep_prob2:0.5})\n    if j%5==0:\n        # Test trained model\n        #shuffle_test = np.random.choice(74645, size=10000, replace=False)\n        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n        #print('epoch:', j, sess.run(cross_entropy, feed_dict={x: X[shuffle_test].toarray(),\n                                                              #y_: Y[shuffle_test].toarray()}))\n        print('epoch:', j, sess.run(cross_entropy, feed_dict={x: Xtest.toarray(),\n                                                              y_: ytest.toarray(), \n                                                              keep_prob1:1.0, \n                                                              keep_prob2:1.0}))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eeb01c78-0986-6316-813e-75c13ba078c7"},"outputs":[],"source":"logloss = tf.nn.softmax(y)\ni1 = 1121\nn0 = 100\n\nfor i in range(i1):\n    if n0*(i+1)>112071:\n        m0 = 112071\n    else:\n        m0 = n0*(i+1)\n    if i==0:\n        pred_nn = sess.run(logloss, feed_dict={x: X_test[n0*i:m0].toarray()})\n    else:\n        pred_nn = np.row_stack((pred_nn, sess.run(logloss, feed_dict={x: X_test[n0*i:m0].toarray(), \n                                                                     keep_prob1:1.0, \n                                                                     keep_prob2:1.0})))\n\n#pred_nn = pd.DataFrame(sess.run(log_loss, feed_dict={x: X_test.toarray()}), index=ga_test.index, columns=targetencoder.classes) \n#pred_nn.to_csv('logreg_subm_neuralnetwork.csv',index=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"932ec5de-92b4-4c46-96d9-8207ce47c66d"},"source":"logloss = tf.nn.softmax(y)\ni1 = 47\nn0 = 100\n\nfor i in range(i1):\n    if n0*(i+1)>4644:\n        m0 = 4644\n    else:\n        m0 = n0*(i+1)\n    if i==0:\n        pred_nn = sess.run(logloss, feed_dict={x: Xtest[n0*i:m0].toarray()})\n    else:\n        pred_nn = np.row_stack((pred_nn, sess.run(logloss, feed_dict={x: Xtest[n0*i:m0].toarray()})))\n\n#pred_nn = pd.DataFrame(sess.run(log_loss, feed_dict={x: X_test.toarray()}), index=ga_test.index, columns=targetencoder.classes) \n#pred_nn.to_csv('logreg_subm_neuralnetwork.csv',index=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"be9a71d7-0b26-6498-a6a6-bed9a1f9d3ed"},"source":"target_encoder = LabelEncoder().fit(ga_train['group'])\npred = pd.DataFrame(pred_nn, index=ga_test.index, columns=target_encoder1.classes_) \npred.to_csv('logreg_subm_neuralnetwork.csv',index=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b2bada6-4ac6-aa56-e3aa-73f5eb4a99de"},"outputs":[],"source":"target_encoder = LabelEncoder().fit(ga_train['group'])\npred = pd.DataFrame(pred_nn, index=ga_test.index, columns=target_encoder1.classes_) \npred.to_csv('testnew_nn.csv',index=True)\npred.head()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}