{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"markdown","metadata":{},"source":"# File processing\n# Use LabelEncoder to map labels to integers\n# Output the trained LabelEncoder for 'group' to obtain the mapping\n# Note: need to drop duplicates in the brand_device_model.csv file!"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"__author__ = 'HeartburntXiaoxue'\n'''\nthis starter uses only phone_brand and device_model to predict both the gender and age\n'''\nimport datetime\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\nimport random\nimport zipfile\nimport time\nimport shutil\nfrom sklearn.metrics import log_loss, precision_score\nfrom sklearn.datasets import make_hastie_10_2\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\n\nrandom.seed(2016)\n\ndef read_train_test():\n    # read events\n    print('read events...')\n    events = pd.read_csv('../input/events.csv',\n                         dtype={'event_id': np.int, 'device_id': np.str, 'timestamp':np.str,\n                               'longitude': np.float, 'latitude': np.float})  # force the device_id to be a string\n    events['counts'] = events.groupby(['device_id'])['event_id'].transform('count')  # number of events for each device\n    \n    # read brand and model\n    print('read brands...')\n    br = pd.read_csv('../input/phone_brand_device_model.csv', dtype={'device_id': np.str, 'phone_brand':np.str, 'device_model':np.str})\n    br.drop_duplicates('device_id', inplace = True)\n    le = LabelEncoder()\n    br.phone_brand = le.fit_transform(br.phone_brand)\n    br.device_model = le.fit_transform(br.device_model)\n    \n    # train\n    print('read train...')\n    train = pd.read_csv('../input/gender_age_train.csv', dtype={'device_id': np.str})\n    train.group = le.fit_transform(train.group)\n    # train = train.drop(['group'])  # will drop the group info, but keep gender and age\n    train = train.replace({'gender': {'M': 0, 'F': 1}})\n    train = pd.merge(train, br, how='left', on='device_id', left_index=True)\n\n    # test\n    print('read test...')\n    test = pd.read_csv(\"../input/gender_age_test.csv\", dtype={'device_id': np.str})\n    test = pd.merge(test, br, how='left', on='device_id', left_index=True)\n    test.drop_duplicates() # why are there duplicates after merging?\n    \n    # features\n    features = list(test.columns.values)\n    features.remove('device_id')\n\n    return train, test, features, le"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train,test,features, group_encoder = read_train_test()"},{"cell_type":"markdown","metadata":{},"source":"# Prediction: be careful that we are to predict the probabilities of each label, so we need to use .predict_proba"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"\ndef run_classifier(clf, train,test,features,target, random_state = 0):\n    start_time = time.time()\n    \n    X_train, X_valid = train_test_split(train,test_size=0.3,random_state=random_state)\n    #print('Length train:', len(X_train.index))\n    #print('Length test:', len(X_valid.index))\n    y_train = X_train[target]\n    y_valid = X_valid[target]\n    \n    clf = clf.fit(X_train[features], y_train)\n    \n    #print('validating...')\n    check = clf.predict_proba(X_valid[features])\n    score = log_loss(y_valid.tolist(),check)\n    \n    #print('predict test set...')\n    test_prediction = clf.predict_proba(test[features])\n    \n    #print('training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n    return test_prediction, score\n    \nclfs = [LogisticRegression(),DecisionTreeClassifier(),RandomForestClassifier(),AdaBoostClassifier(),GaussianNB()]\nnames = ['logit','DecisionTree', 'RandomForest', 'AdaBoost', 'GaussianNaiveBayes']\n\nfor i, clf in enumerate(clfs):\n    print(names[i])\n    test_pred, score = run_classifier(clf,train,test,features,'group')\n    print('score: {}'.format( score,2))"},{"cell_type":"markdown","metadata":{},"source":"# Try tuning parameters"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#clfs = [LogisticRegression(),DecisionTreeClassifier(),RandomForestClassifier(),AdaBoostClassifier(),GaussianNB()]\n#names = ['logit','DecisionTree', 'RandomForest', 'AdaBoost', 'GaussianNaiveBayes']\nfrom sklearn.grid_search import GridSearchCV\ndef run_classifier_tuned(clf, grid, train,test,features,target, random_state = 0):\n    start_time = time.time()\n    \n    X_train, X_valid = train_test_split(train,test_size=0.3,random_state=random_state)\n    #print('Length train:', len(X_train.index))\n    #print('Length test:', len(X_valid.index))\n    y_train = X_train[target]\n    y_valid = X_valid[target]\n    clf_grid = GridSearchCV(clf, grid, cv=5,\n                       scoring='log_loss')\n    \n    \n    clf_grid = clf_grid.fit(X_train[features], y_train)\n    print(\"Best parameters set found on development set:\")\n    print(clf_grid.best_params_)\n    \n    #print('validating...')\n    check = clf_grid.predict_proba(X_valid[features])\n    score = log_loss(y_valid.tolist(),check)\n    \n    #print('predict test set...')\n    test_prediction = clf_grid.predict_proba(test[features])\n    \n    #print('training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n    return test_prediction, score\n    "},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"clf = AdaBoostClassifier()\nnames = 'RF'\nparam_grid = {\"base_estimator\" : [RandomForestClassifier(), DecisionTreeClassifier()]\n             }\ntest_pred, score = run_classifier_tuned(clf,param_grid,train,test,features,'group')\nprint('score: {}'.format( score,2))"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"print('read events...')\nevents = pd.read_csv('../input/events.csv',\n                     dtype={'event_id': np.int, 'device_id': np.str, 'timestamp':np.str,\n                            'longitude': np.float, 'latitude': np.float})  # force the device_id to be a string\nevents['counts'] = events.groupby(['device_id'])['event_id'].transform('count')  # number of events for each device\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"events.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}