{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"000cafb8-4c30-8565-753b-410884fe698c"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport datetime\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\nimport xgboost as xgb\nimport random\nimport zipfile\nimport time\nimport shutil\nfrom sklearn.metrics import log_loss\nfrom pandas import DataFrame\n\nrandom.seed(2016)\n\ndef run_xgb(train, test, features, target, random_state=0):\n    eta = 0.1\n    max_depth = 3\n    subsample = 0.7\n    colsample_bytree = 0.7\n    start_time = time.time()\n\n    print('XGBoost params. ETA: {}, MAX_DEPTH: {}, SUBSAMPLE: {}, COLSAMPLE_BY_TREE: {}'.format(eta, max_depth, subsample, colsample_bytree))\n    params = {\n        \"objective\": \"multi:softprob\",\n        \"num_class\": 12,\n        \"booster\" : \"gbtree\",\n        \"eval_metric\": \"mlogloss\",\n        \"eta\": eta,\n        \"max_depth\": max_depth,\n        \"subsample\": subsample,\n        \"colsample_bytree\": colsample_bytree,\n        \"silent\": 1,\n        \"seed\": random_state,\n    }\n    num_boost_round = 500\n    early_stopping_rounds = 50\n    test_size = 0.3\n\n    X_train, X_valid = train_test_split(train, test_size=test_size, random_state=random_state)\n    print('Length train:', len(X_train.index))\n    print('Length valid:', len(X_valid.index))\n    y_train = X_train[target]\n    y_valid = X_valid[target]\n    dtrain = xgb.DMatrix(X_train[features], y_train)\n    dvalid = xgb.DMatrix(X_valid[features], y_valid)\n\n    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n    gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n\n    print(\"Validating...\")\n    check = gbm.predict(xgb.DMatrix(X_valid[features]), ntree_limit=gbm.best_iteration)\n    score = log_loss(y_valid.tolist(), check)\n\n    print(\"Predict test set...\")\n    test_prediction = gbm.predict(xgb.DMatrix(test[features]), ntree_limit=gbm.best_iteration)\n\n    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n    return test_prediction.tolist(), score\n\n\ndef create_submission(score, test, prediction):\n    # Make Submission\n    now = datetime.datetime.now()\n    sub_file = 'submission_' + str(score) + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n    print('Writing submission: ', sub_file)\n    f = open(sub_file, 'w')\n    f.write('device_id,F23-,F24-26,F27-28,F29-32,F33-42,F43+,M22-,M23-26,M27-28,M29-31,M32-38,M39+\\n')\n    total = 0\n    test_val = test['device_id'].values\n    for i in range(len(test_val)):\n        str1 = str(test_val[i])\n        for j in range(12):\n            str1 += ',' + str(prediction[i][j])\n        str1 += '\\n'\n        total += 1\n        f.write(str1)\n    f.close()\n\n\n\ndef map_column(table, f):\n    labels = sorted(table[f].unique())\n    mappings = dict()\n    for i in range(len(labels)):\n        mappings[labels[i]] = i\n    table = table.replace({f: mappings})\n    return table\n\n\ndef read_data():\n    # Events\n    print('Read events...')\n    events = pd.read_csv(\"../input/events.csv\", dtype={'device_id': np.str})\n    events['counts'] = events.groupby(['device_id'])['event_id'].transform('count')\n    events_small = events[['device_id', 'counts', 'timestamp']].drop_duplicates('device_id', keep='first')\n\n    # App events\n    print('Read app_events...')\n    app_events = pd.read_csv(\"../input/app_events.csv\", dtype={'app_id': np.str})\n    app_events.drop_duplicates('event_id', keep='first', inplace=True)\n\n    # App labels\n    print('Read app_labels...')\n    app_labels = pd.read_csv(\"../input/app_labels.csv\", dtype={'app_id': np.str})\n\n    # Phone brand\n    print('Read brands...')\n    models = pd.read_csv(\"../input/phone_brand_device_model.csv\", dtype={'device_id': np.str})\n    models.drop_duplicates('device_id', keep='first', inplace=True)\n    map_column(models, 'device_model')\n    map_column(models, 'phone_brand')\n    \n    # training data\n    print('Read training data...')\n    train = pd.read_csv(\"../input/gender_age_train.csv\", dtype={'device_id': np.str})\n    \n    # test data\n    print('Read test data...')\n    test = pd.read_csv('../input/gender_age_test.csv', dtype={'device_id': np.str})\n\n    nevents = events.event_id.nunique()\n    nlabels = app_labels.label_id.nunique()\n    napps = app_labels.app_id.nunique()\n    ntrain = train.shape[0]\n    ntest = test.shape[0]\n\n    print('Altogether %d events' % nevents)\n    print('Altogether %d different apps' %napps)\n    print('Altogether %d labels' %nlabels)\n    print('Altogether %d devices in the training data' %ntrain)\n    print('Altogether %d devices in the test data' % ntest)\n\n    return events_small,app_events , app_labels, models, train, test\n\n\n# raw data from the given datasets\nevents, app_events, app_labels, models, otrain, otest = read_data()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"69729798-3ef5-5c9c-ee10-db9abb2b0afe"},"outputs":[],"source":"\n# create train and test sets\n# feature space: brand, model, frequency of events, number of involved apps,\n# distribution of events during the day, distribution of app labels\nimport time\nfrom time import mktime\nfrom datetime import datetime\ndef create_train_test(events, app_events, app_labels, models, otrain, otest):\n    train = otrain\n    test = otest\n    \n    # model and brand\n    train = pd.merge(train, models, how='left', on='device_id')\n    train.set_index('device_id')\n      \n    test = pd.merge(test, models, how='left', on='device_id')\n    test.set_index('device_id')\n    \n    # number of events\n    ec = events.drop_duplicate('device_id')['device_id','counts']\n    ec = ec.set_index('device_id')\n    \n    # average longitude and latitude\n    lo = events.groupby('device_id')['longitude'].mean()\n    la = events.groupby('device_id')['latitude'].mean()\n    \n    # distribution of event time during the day\n    def timestamp_to_hour(row):\n        return datetime.strptime(row.timestamp[0], \"%Y-%m-%d %H:%M:%S\").hour\n    hour = events.apply(timestamp_to_hour, axis=1)\n    events['hour'] = hour\n    et = events.groupby['device_id']['hour'].count()\n\n    train = pd.concat([train, ec, lo, la, et], axis =1)\n    test = pd.concat([test, ec, lo, la, et], axis=1)\n    \n    return train, test"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}