{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# coding=utf8\n\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom scipy import sparse\nfrom sklearn.feature_extraction import FeatureHasher\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\nimport gzip\n\n# Create bag-of-apps in character string format\n# first by event\n# then merge to generate larger bags by device\n\n##################\n#   App Labels\n##################\n\nprint(\"# Read App Labels\")\napp_lab = pd.read_csv(\"../input/app_labels.csv\", dtype={'device_id': np.str})\napp_lab = app_lab.groupby(\"app_id\")[\"label_id\"].apply(\n    lambda x: \" \".join(str(s) for s in x))\n\n##################\n#   App Events\n##################\nprint(\"# Read App Events\")\napp_ev = pd.read_csv(\"../input/app_events.csv\", dtype={'device_id': np.str})\napp_ev[\"app_lab\"] = app_ev[\"app_id\"].map(app_lab)\napp_ev = app_ev.groupby(\"event_id\")[\"app_lab\"].apply(\n    lambda x: \" \".join(str(s) for s in x))\n\ndel app_lab\n\n##################\n#     Events\n##################\nprint(\"# Read Events\")\nevents = pd.read_csv(\"../input/events.csv\", dtype={'device_id': np.str})\nevents[\"app_lab\"] = events[\"event_id\"].map(app_ev)\nevents = events.groupby(\"device_id\")[\"app_lab\"].apply(\n    lambda x: \" \".join(str(s) for s in x))\n    \n\n\n#with gzip.open('events.csv', 'wt') as write_file:\n#    events.to_csv(write_file, index=False)\n\ndel app_ev\n\n##################\n#   Phone Brand\n##################\nprint(\"# Read Phone Brand\")\npbd = pd.read_csv(\"../input/phone_brand_device_model.csv\",\n                  dtype={'device_id': np.str})\npbd.drop_duplicates('device_id', keep='first', inplace=True)\n\n\n##################\n#  Train and Test\n##################\nprint(\"# Generate Train and Test\")\ntrain = pd.read_csv(\"../input/gender_age_train.csv\",\n                    dtype={'device_id': np.str})\ntrain[\"app_lab\"] = train[\"device_id\"].map(events)\ntrain = pd.merge(train, pbd, how='left',\n                 on='device_id', left_index=True)\n\ntest = pd.read_csv(\"../input/gender_age_test.csv\",\n                   dtype={'device_id': np.str})\ntest[\"app_lab\"] = test[\"device_id\"].map(events)\ntest = pd.merge(test, pbd, how='left',\n                on='device_id', left_index=True)\n\ndel pbd\ndel events\n\n# train.to_csv(\"train.csv\", index=False)\n# test.to_csv(\"test.csv\", index=False)\n\n##################\n#   Build Model\n##################\n\n# def get_hash_data(df):\n#     hasher = FeatureHasher(input_type='string')\n#     # hasher = DictVectorizer(sparse=False)\n#     df = df[[\"phone_brand\", \"device_model\", \"app_id\"]].apply(\n#         lambda x: \",\".join(str(s) for s in x), axis=1)\n#     df = hasher.transform(df.apply(lambda x: x.split(\",\")))\n#     return df\n\n\ndef get_hash_data(train, test):\n    df = pd.concat((train, test), axis=0, ignore_index=True)\n    split_len = len(train)\n\n    # TF-IDF Feature\n    tfv = TfidfVectorizer(min_df=1)\n    df = df[[\"phone_brand\", \"device_model\", \"app_lab\"]].astype(np.str).apply(\n        lambda x: \" \".join(s for s in x), axis=1).fillna(\"Missing\")\n    df_tfv = tfv.fit_transform(df)\n\n    train = df_tfv[:split_len, :]\n    test = df_tfv[split_len:, :]\n    return train, test\n\n# Group Labels\nY = train[\"group\"]\nlable_group = LabelEncoder()\nY = lable_group.fit_transform(Y)\n\ndevice_id = test[\"device_id\"].values\ntrain, test = get_hash_data(train,test)\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from scipy import sparse, io\n\nm = sparse.csr_matrix([[0,0,0],[1,0,0],[0,1,0]])\nm              # <3x3 sparse matrix of type '<type 'numpy.int64'>' with 2 stored elements in Compressed Sparse Row format>\n\nio.mmwrite(\"train.mtx\", train)\nio.mmwrite(\"test.mtx\", test)\n\n#newm = io.mmread(\"test.mtx\")\n#newm           # <3x3 sparse matrix of type '<type 'numpy.int32'>' with 2 stored elements in COOrdinate format>\n#newm.tocsr()   # <3x3 sparse matrix of type '<type 'numpy.int32'>' with 2 stored elements in Compressed Sparse Row format>\n#newm.toarray() # array([[0, 0, 0], [1, 0, 0], [0, 1, 0]], dtype=int32)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"\nX_train, X_val, y_train, y_val = train_test_split(train, Y, train_size=.80)\n\n##################\n#     XGBoost\n##################\n\ndtrain = xgb.DMatrix(X_train, y_train)\ndvalid = xgb.DMatrix(X_val, y_val)\n\nparams = {\n    \"objective\": \"multi:softprob\",\n    \"num_class\": 12,\n    \"booster\": \"gbtree\",\n    \"eval_metric\": \"mlogloss\",\n    \"eta\": 0.15,\n    \"silent\": 1,\n}\nwatchlist = [(dtrain, 'train'), (dvalid, 'eval')]\ngbm = xgb.train(params, dtrain, 30, evals=watchlist, verbose_eval=True)\n\ny_pre = gbm.predict(xgb.DMatrix(test), ntree_limit=gbm.best_iteration)\n\n# Write results\nresult = pd.DataFrame(y_pre, columns=lable_group.classes_)\nresult[\"device_id\"] = device_id\nresult = result.set_index(\"device_id\")\nresult.to_csv('test_Result.csv', index=True, index_label='device_id')"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}