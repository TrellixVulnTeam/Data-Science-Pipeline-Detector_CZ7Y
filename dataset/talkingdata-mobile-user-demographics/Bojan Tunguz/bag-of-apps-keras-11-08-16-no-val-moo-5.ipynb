{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5aca3dbe-cd4b-dc77-77aa-0e3fabe18739"},"outputs":[],"source":"\n# Bag of apps categories\n# Bag of labels categories\n# Include phone brand and model device\n\nprint(\"Initialize libraries\")\n\nimport pandas as pd\nimport sys\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nfrom sklearn.cross_validation import StratifiedKFold, KFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics as skmetrics\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom keras.layers.advanced_activations import PReLU\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn import ensemble\nfrom sklearn.decomposition import PCA\nimport os\nimport gc\nfrom scipy import sparse\nfrom sklearn.cross_validation import train_test_split, cross_val_score\nfrom sklearn.feature_selection import SelectPercentile, f_classif, chi2, SelectKBest\nfrom sklearn import ensemble\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nfrom keras.optimizers import SGD\n\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.cross_validation import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import log_loss\n\n#------------------------------------------------- Write functions ----------------------------------------\n\ndef rstr(df): return df.dtypes, df.head(3) ,df.apply(lambda x: [x.unique()]), df.apply(lambda x: [len(x.unique())]),df.shape\n\ndef batch_generator(X, y, batch_size, shuffle):\n    #chenglong code for fiting from generator (https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices)\n    number_of_batches = np.ceil(X.shape[0]/batch_size)\n    counter = 0\n    sample_index = np.arange(X.shape[0])\n    if shuffle:\n        np.random.shuffle(sample_index)\n    while True:\n        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n        X_batch = X[batch_index,:].toarray()\n        y_batch = y[batch_index]\n        counter += 1\n        yield X_batch, y_batch\n        if (counter == number_of_batches):\n            if shuffle:\n                np.random.shuffle(sample_index)\n            counter = 0\n\ndef batch_generatorp(X, batch_size, shuffle):\n    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n    counter = 0\n    sample_index = np.arange(X.shape[0])\n    while True:\n        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n        X_batch = X[batch_index, :].toarray()\n        counter += 1\n        yield X_batch\n        if (counter == number_of_batches):\n            counter = 0\n\n#------------------------------------------------ Read data from source files ------------------------------------\n\nseed = 700\nnp.random.seed(seed)\ndatadir = '../input'\n\nprint(\"### ----- PART 1 ----- ###\")\n\n# Data - Events data\n# Bag of apps\nprint(\"# Read app events\")\napp_events = pd.read_csv(os.path.join(datadir,'app_events.csv'), dtype={'device_id' : np.str})\n#app_events.head(5)\n#app_events.info()\n#print(rstr(app_events))\n\n# remove duplicates(app_id)\napp_events= app_events.groupby(\"event_id\")[\"app_id\"].apply(\n    lambda x: \" \".join(set(\"app_id:\" + str(s) for s in x)))\n#app_events.head(5)\n\nprint(\"# Read Events\")\nevents = pd.read_csv(os.path.join(datadir,'events.csv'), dtype={'device_id': np.str})\n#events.head(5)\nevents[\"app_id\"] = events[\"event_id\"].map(app_events)\nevents = events.dropna()\ndel app_events\n\nevents = events[[\"device_id\", \"app_id\"]]\n#events.info()\n# 1Gb reduced to 34 Mb\n\n# remove duplicates(app_id)\nevents.loc[:,\"device_id\"].value_counts(ascending=True)\n\nevents = events.groupby(\"device_id\")[\"app_id\"].apply(\n    lambda x: \" \".join(set(str(\" \".join(str(s) for s in x)).split(\" \"))))\nevents = events.reset_index(name=\"app_id\")\n\n# expand to multiple rows\nevents = pd.concat([pd.Series(row['device_id'], row['app_id'].split(' '))\n                    for _, row in events.iterrows()]).reset_index()\nevents.columns = ['app_id', 'device_id']\n#events.head(5)\nf3 = events[[\"device_id\", \"app_id\"]]    # app_id\n\nprint(\"#Part1 formed\")\n\n##################\n#   App labels\n##################\n\nprint(\"### ----- PART 2 ----- ###\")\n\nprint(\"# Read App labels\")\napp_labels = pd.read_csv(os.path.join(datadir,'app_labels.csv'))\nlabel_cat = pd.read_csv(os.path.join(datadir,'label_categories.csv'))\n#app_labels.info()\n#label_cat.info()\nlabel_cat=label_cat[['label_id','category']]\n\napp_labels=app_labels.merge(label_cat,on='label_id',how='left')\n#app_labels.head(3)\n#events.head(3)\n#app_labels = app_labels.loc[app_labels.smaller_cat != \"unknown_unknown\"]\n\n#app_labels = app_labels.groupby(\"app_id\")[\"category\"].apply(\n#    lambda x: \";\".join(set(\"app_cat:\" + str(s) for s in x)))\napp_labels = app_labels.groupby([\"app_id\",\"category\"]).agg('size').reset_index()\napp_labels = app_labels[['app_id','category']]\nprint(\"# App labels done\")\n\n\n# Remove \"app_id:\" from column\nprint(\"## Handling events data for merging with app lables\")\nevents['app_id'] = events['app_id'].map(lambda x : x.lstrip('app_id:'))\nevents['app_id'] = events['app_id'].astype(str)\napp_labels['app_id'] = app_labels['app_id'].astype(str)\n#app_labels.info()\n\nprint(\"## Merge\")\n\nevents= pd.merge(events, app_labels, on = 'app_id',how='left').astype(str)\n#events['smaller_cat'].unique()\n\n# expand to multiple rows\nprint(\"#Expand to multiple rows\")\n#events= pd.concat([pd.Series(row['device_id'], row['category'].split(';'))\n#                    for _, row in events.iterrows()]).reset_index()\n#events.columns = ['app_cat', 'device_id']\n#events.head(5)\n#print(events.info())\n\nevents= events.groupby([\"device_id\",\"category\"]).agg('size').reset_index()\nevents= events[['device_id','category']]\n#events.head(10)\nprint(\"# App labels done\")\n\nf5 = events[[\"device_id\", \"category\"]]    # app_id\n# Can % total share be included as well?\nprint(\"# App category part formed\")\n\n##################\n#   Phone Brand\n##################\nprint(\"### ----- PART 3 ----- ###\")\n\nprint(\"# Read Phone Brand\")\npbd = pd.read_csv(os.path.join(datadir,'phone_brand_device_model.csv'),\n                  dtype={'device_id': np.str})\npbd.drop_duplicates('device_id', keep='first', inplace=True)\n\n##################\n#  Train and Test\n##################\nprint(\"# Generate Train and Test\")\n\ntrain = pd.read_csv(os.path.join(datadir,'gender_age_train.csv'),\n                    dtype={'device_id': np.str})\ntrain['dl'] = pd.Series(train.index/len(train.index),index=train.index)\n                    \ntrain.drop([\"age\", \"gender\"], axis=1, inplace=True)\ntrain.info()\ntest = pd.read_csv(os.path.join(datadir,'gender_age_test.csv'),\n                   dtype={'device_id': np.str})\ntest['dl'] = pd.Series(test.index/len(test.index),index=test.index)\n                   \ntest[\"group\"] = np.nan\ntest.info()\nsplit_len = len(train)\n\n# Group Labels\nY = train[\"group\"]\nlable_group = LabelEncoder()\nY = lable_group.fit_transform(Y)\ndevice_id = test[\"device_id\"]\n\n# Concat\nDf = pd.concat((train, test), axis=0, ignore_index=True)\n\nprint(\"### ----- PART 4 ----- ###\")\n\nDf = pd.merge(Df, pbd, how=\"left\", on=\"device_id\")\nDf[\"phone_brand\"] = Df[\"phone_brand\"].apply(lambda x: \"phone_brand:\" + str(x))\nDf[\"device_model\"] = Df[\"device_model\"].apply(\n    lambda x: \"device_model:\" + str(x))\n\n\n###################\n#  Concat Feature\n###################\n\nprint(\"# Concat all features\")\n\nf1 = Df[[\"device_id\", \"phone_brand\"]]   # phone_brand\nf2 = Df[[\"device_id\", \"device_model\"]]  # device_model\nf6 = Df[[\"device_id\", \"dl\"]]  # \n\nevents = None\nDf = None\n\nf1.columns.values[1] = \"feature\"\nf2.columns.values[1] = \"feature\"\nf5.columns.values[1] = \"feature\"\nf3.columns.values[1] = \"feature\"\nf6.columns.values[1] = \"dl\"\n\nFLS = pd.concat((f1, f2, f3, f5), axis=0, ignore_index=True)\nprint(FLS.head(10))\nprint(f1.head(10))\nprint(f2.head(10))\nprint(f3.head(10))\nprint(f5.head(10))\nprint(f6.head(10))\nFLS.info()\nFLS6 = pd.concat((f1, f2, f3, f5,f6), axis=0, ignore_index=True)\nFLS6.info()\nprint(FLS6.head(10))\n###################\n# User-Item Feature\n###################\nprint(\"# User-Item-Feature\")\n\ndevice_ids = FLS6[\"device_id\"].unique()\nfeature_cs = FLS[\"feature\"].unique()\nmoo = f6[\"dl\"]\n\ndata = np.hstack((np.ones(len(FLS)),moo.data)) # numpy ndarray\nprint(len(data))\n\ndec = LabelEncoder().fit(FLS6[\"device_id\"])\nrow = dec.transform(FLS6[\"device_id\"])\ncol = np.hstack((LabelEncoder().fit_transform(FLS[\"feature\"])+1,np.ones(len(moo))))\nsparse_matrix = sparse.csr_matrix(\n    (data, (row, col)), shape=(len(device_ids), len(feature_cs)+1))\nprint(sparse_matrix.shape)\nsys.getsizeof(sparse_matrix)\n\nsparse_matrix = sparse_matrix[:, sparse_matrix.getnnz(0) > 0]\nprint(\"# Sparse matrix done\")\n\ndel FLS\ndel data\nf1 = [1]\nf5 = [1]\nf2 = [1]\nf3 = [1]\n\nevents = [1]\n\n##################\n#      Data\n##################\n\nprint(\"# Split data\")\ntrain_row = dec.transform(train[\"device_id\"])\ntrain_sp = sparse_matrix[train_row, :]\n\ntest_row = dec.transform(test[\"device_id\"])\ntest_sp = sparse_matrix[test_row, :]\n\nX_train, X_val, y_train, y_val = train_test_split(\n    train_sp, Y, train_size=0.999, random_state=10)\n\n##################\n#   Feature Sel\n##################\nprint(\"# Feature Selection\")\n#selector = SelectPercentile(f_classif, percentile=53)\n\n#selector.fit(X_train, y_train)\n#X_train.shape\n#X_train = selector.transform(X_train)\n#X_train.shape\n#X_val = selector.transform(X_val)\n#X_val.shape\n\n# Selection using chi-square\n# selector = SelectKBest(chi2, k=11155).fit(X_train, y_train)\n# X_train.shape\n# X_train = selector.transform(X_train)\n# X_train.shape\n# X_val = selector.transform(X_val)\n# X_val.shape\n\nprint(\"# Num of Features: \", X_train.shape[1])\n\n##################\n#  Build Model\n##################\n\n\n#act = keras.layers.advanced_activations.PReLU(init='zero', weights=None)\n\ndef baseline_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(150, input_dim=X_train.shape[1], init='normal'))\n    model.add(PReLU())\n    model.add(Dropout(0.4))\n    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n    model.add(PReLU())\n    model.add(Dropout(0.2))\n    model.add(Dense(12, init='normal', activation='softmax'))\n    # Compile model\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n    return model\n\nmodel=baseline_model()\n\nfit= model.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n                         nb_epoch=16,\n                         samples_per_epoch=69984,\n                         validation_data=(X_val.todense(), y_val), verbose=2\n                         )\n\n# evaluate the model\nscores_val = model.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\nprint('logloss val {}'.format(log_loss(y_val, scores_val)))\n\nprint(\"# Final prediction\")\nscores = model.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\nresult = pd.DataFrame(scores , columns=lable_group.classes_)\nresult[\"device_id\"] = device_id\nprint(result.head(1))\nresult = result.set_index(\"device_id\")\n\n#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n#Drop out 0.2\n#Validation 2.3017\nresult.to_csv('sub_bagofapps7_keras_150_pt4_50_pt2_15epoch_prelu_softmax.csv', index=True, index_label='device_id')\n\n\nprint(\"Done\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4be8df35-9da1-07c2-d6b7-8282e4fe8e33"},"outputs":[],"source":"from scipy import sparse, io\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ae91c58-6f92-9dcc-8c77-d1da7b2dc713"},"outputs":[],"source":"io.mmwrite(\"train.mtx\", train_sp)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9431089-f242-6ac0-5f7e-ad3992e864f7"},"outputs":[],"source":"train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"256bf245-9959-2f3a-bc4e-e57805689b29"},"outputs":[],"source":"io.mmwrite(\"test_sp.mtx\", test_sp)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eee45037-2de9-f883-6428-ccbe8f449a6a"},"outputs":[],"source":"lable_group.classes_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9d7605b2-5e7e-24a7-efc7-915df9c997ce"},"outputs":[],"source":"device_id"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"54472632-ea7b-5bc3-3b4f-6c63d1ab58ee"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}