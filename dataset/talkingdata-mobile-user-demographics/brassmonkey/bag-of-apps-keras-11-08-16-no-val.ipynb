{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"54284994-cde4-1e6a-2875-51ceed8ed24b"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"abbe2b5d-547c-faaf-eb5d-babc81cea4db"},"outputs":[],"source":"\n# Bag of apps categories\n# Bag of labels categories\n# Include phone brand and model device\n\nprint(\"Initialize libraries\")\n\nimport pandas as pd\nimport sys\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nfrom sklearn.cross_validation import StratifiedKFold, KFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics as skmetrics\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom keras.layers.advanced_activations import PReLU\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn import ensemble\nfrom sklearn.decomposition import PCA\nimport os\nimport gc\nfrom scipy import sparse\nfrom sklearn.cross_validation import train_test_split, cross_val_score\nfrom sklearn.feature_selection import SelectPercentile, f_classif, chi2, SelectKBest\nfrom sklearn import ensemble\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nfrom keras.optimizers import SGD\n\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.cross_validation import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import log_loss\n\n#------------------------------------------------- Write functions ----------------------------------------\n\ndef rstr(df): return df.dtypes, df.head(3) ,df.apply(lambda x: [x.unique()]), df.apply(lambda x: [len(x.unique())]),df.shape\n\ndef batch_generator(X, y, batch_size, shuffle):\n    #chenglong code for fiting from generator (https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices)\n    number_of_batches = np.ceil(X.shape[0]/batch_size)\n    counter = 0\n    sample_index = np.arange(X.shape[0])\n    if shuffle:\n        np.random.shuffle(sample_index)\n    while True:\n        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n        X_batch = X[batch_index,:].toarray()\n        y_batch = y[batch_index]\n        counter += 1\n        yield X_batch, y_batch\n        if (counter == number_of_batches):\n            if shuffle:\n                np.random.shuffle(sample_index)\n            counter = 0\n\ndef batch_generatorp(X, batch_size, shuffle):\n    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n    counter = 0\n    sample_index = np.arange(X.shape[0])\n    while True:\n        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n        X_batch = X[batch_index, :].toarray()\n        counter += 1\n        yield X_batch\n        if (counter == number_of_batches):\n            counter = 0\n\n#------------------------------------------------ Read data from source files ------------------------------------\n\nseed = 7\nnp.random.seed(seed)\ndatadir = '../input'\n\nprint(\"### ----- PART 1 ----- ###\")\n\n# Data - Events data\n# Bag of apps\nprint(\"# Read app events\")\napp_events = pd.read_csv(os.path.join(datadir,'app_events.csv'), dtype={'device_id' : np.str})\napp_events.head(5)\napp_events.info()\n#print(rstr(app_events))\n\n# remove duplicates(app_id)\napp_events= app_events.groupby(\"event_id\")[\"app_id\"].apply(\n    lambda x: \" \".join(set(\"app_id:\" + str(s) for s in x)))\napp_events.head(5)\n\nprint(\"# Read Events\")\nevents = pd.read_csv(os.path.join(datadir,'events.csv'), dtype={'device_id': np.str})\nevents.head(5)\nevents[\"app_id\"] = events[\"event_id\"].map(app_events)\nevents = events.dropna()\ndel app_events\n\nevents = events[[\"device_id\", \"app_id\"]]\nevents.info()\n# 1Gb reduced to 34 Mb\n\n# remove duplicates(app_id)\nevents.loc[:,\"device_id\"].value_counts(ascending=True)\n\nevents = events.groupby(\"device_id\")[\"app_id\"].apply(\n    lambda x: \" \".join(set(str(\" \".join(str(s) for s in x)).split(\" \"))))\nevents = events.reset_index(name=\"app_id\")\n\n# expand to multiple rows\nevents = pd.concat([pd.Series(row['device_id'], row['app_id'].split(' '))\n                    for _, row in events.iterrows()]).reset_index()\nevents.columns = ['app_id', 'device_id']\nevents.head(5)\nf3 = events[[\"device_id\", \"app_id\"]]    # app_id\n\nprint(\"#Part1 formed\")\n\n##################\n#   App labels\n##################\n\nprint(\"### ----- PART 2 ----- ###\")\n\nprint(\"# Read App labels\")\napp_labels = pd.read_csv(os.path.join(datadir,'app_labels.csv'))\nlabel_cat = pd.read_csv(os.path.join(datadir,'label_categories.csv'))\napp_labels.info()\nlabel_cat.info()\nlabel_cat=label_cat[['label_id','category']]\n\napp_labels=app_labels.merge(label_cat,on='label_id',how='left')\napp_labels.head(3)\nevents.head(3)\n#app_labels = app_labels.loc[app_labels.smaller_cat != \"unknown_unknown\"]\n\n#app_labels = app_labels.groupby(\"app_id\")[\"category\"].apply(\n#    lambda x: \";\".join(set(\"app_cat:\" + str(s) for s in x)))\napp_labels = app_labels.groupby([\"app_id\",\"category\"]).agg('size').reset_index()\napp_labels = app_labels[['app_id','category']]\nprint(\"# App labels done\")\n\n\n# Remove \"app_id:\" from column\nprint(\"## Handling events data for merging with app lables\")\nevents['app_id'] = events['app_id'].map(lambda x : x.lstrip('app_id:'))\nevents['app_id'] = events['app_id'].astype(str)\napp_labels['app_id'] = app_labels['app_id'].astype(str)\napp_labels.info()\n\nprint(\"## Merge\")\n\nevents= pd.merge(events, app_labels, on = 'app_id',how='left').astype(str)\n#events['smaller_cat'].unique()\n\n# expand to multiple rows\nprint(\"#Expand to multiple rows\")\n#events= pd.concat([pd.Series(row['device_id'], row['category'].split(';'))\n#                    for _, row in events.iterrows()]).reset_index()\n#events.columns = ['app_cat', 'device_id']\n#events.head(5)\n#print(events.info())\n\nevents= events.groupby([\"device_id\",\"category\"]).agg('size').reset_index()\nevents= events[['device_id','category']]\nevents.head(10)\nprint(\"# App labels done\")\n\nf5 = events[[\"device_id\", \"category\"]]    # app_id\n# Can % total share be included as well?\nprint(\"# App category part formed\")\n\n##################\n#   Phone Brand\n##################\nprint(\"### ----- PART 3 ----- ###\")\n\nprint(\"# Read Phone Brand\")\npbd = pd.read_csv(os.path.join(datadir,'phone_brand_device_model.csv'),\n                  dtype={'device_id': np.str})\npbd.drop_duplicates('device_id', keep='first', inplace=True)\n\n##################\n#  Train and Test\n##################\nprint(\"# Generate Train and Test\")\n\ntrain = pd.read_csv(os.path.join(datadir,'gender_age_train.csv'),\n                    dtype={'device_id': np.str})\ntrain.drop([\"age\", \"gender\"], axis=1, inplace=True)\n\ntest = pd.read_csv(os.path.join(datadir,'gender_age_test.csv'),\n                   dtype={'device_id': np.str})\ntest[\"group\"] = np.nan\n\n\nsplit_len = len(train)\n\n# Group Labels\nY = train[\"group\"]\nlable_group = LabelEncoder()\nY = lable_group.fit_transform(Y)\ndevice_id = test[\"device_id\"]\n\n# Concat\nDf = pd.concat((train, test), axis=0, ignore_index=True)\n\nprint(\"### ----- PART 4 ----- ###\")\n\nDf = pd.merge(Df, pbd, how=\"left\", on=\"device_id\")\nDf[\"phone_brand\"] = Df[\"phone_brand\"].apply(lambda x: \"phone_brand:\" + str(x))\nDf[\"device_model\"] = Df[\"device_model\"].apply(\n    lambda x: \"device_model:\" + str(x))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"82887109-2fe1-906b-c812-0846cd9fdfa0"},"outputs":[],"source":"###################\n#  Concat Feature\n###################\n\nprint(\"# Concat all features\")\n\nf1 = Df[[\"device_id\", \"phone_brand\"]]   # phone_brand\nf2 = Df[[\"device_id\", \"device_model\"]]  # device_model\n\nevents = None\n#Df = None\n\nf1.columns.values[1] = \"feature\"\nf2.columns.values[1] = \"feature\"\nf5.columns.values[1] = \"feature\"\nf3.columns.values[1] = \"feature\"\n\nFLS = pd.concat((f1, f2, f3, f5), axis=0, ignore_index=True)\n\nFLS.info()\n\n###################\n# User-Item Feature\n###################\nprint(\"# User-Item-Feature\")\n\ndevice_ids = FLS[\"device_id\"].unique()\nfeature_cs = FLS[\"feature\"].unique()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30e20ee9-e14e-6069-303e-97a667aa76b0"},"outputs":[],"source":"data = np.ones(len(FLS))\nlen(data)\ndec = LabelEncoder().fit(FLS[\"device_id\"])\nrow = dec.transform(FLS[\"device_id\"])\ncol = LabelEncoder().fit_transform(FLS[\"feature\"])\nsparse_matrix = sparse.csr_matrix(\n    (data, (row, col)), shape=(len(device_ids), len(feature_cs)))\nsparse_matrix.shape\nsys.getsizeof(sparse_matrix)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d1e92b0-c604-7b5e-7a94-18b25e57639d"},"outputs":[],"source":"f6 = device_ids.astype(float)\nf6 = sparse.csr_matrix(f6)\nf6 = f6.transpose()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f4bbc8b-a977-9603-16c4-ec8e1dabdb8b"},"outputs":[],"source":"from scipy.sparse import coo_matrix, hstack\nsparse_matrix = hstack((sparse_matrix, f6))\nsparse_matrix = sparse.csr_matrix(sparse_matrix)\n\nsparse_matrix = sparse_matrix[:, sparse_matrix.getnnz(0) > 0]\nprint(\"# Sparse matrix done\")\nsparse_matrix.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f23b60bc-df8c-1000-31c0-8a6b57f96a7c"},"outputs":[],"source":"del FLS\ndel data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3c651a28-8136-a219-0923-2dbfd1526088"},"outputs":[],"source":"f1 = [1]\nf5 = [1]\nf2 = [1]\nf3 = [1]\n\nevents = [1]\n\n##################\n#      Data\n##################\n\nprint(\"# Split data\")\ntrain_row = dec.transform(train[\"device_id\"])\ntrain_sp = sparse_matrix[train_row, :]\n\ntest_row = dec.transform(test[\"device_id\"])\ntest_sp = sparse_matrix[test_row, :]\n\nX_train, X_val, y_train, y_val = train_test_split(\n    train_sp, Y, train_size=0.999, random_state=10)\n\n##################\n#   Feature Sel\n##################\nprint(\"# Feature Selection\")\n#selector = SelectPercentile(f_classif, percentile=53)\n\n#selector.fit(X_train, y_train)\n#X_train.shape\n#X_train = selector.transform(X_train)\n#X_train.shape\n#X_val = selector.transform(X_val)\n#X_val.shape\n\n# Selection using chi-square\n# selector = SelectKBest(chi2, k=11155).fit(X_train, y_train)\n# X_train.shape\n# X_train = selector.transform(X_train)\n# X_train.shape\n# X_val = selector.transform(X_val)\n# X_val.shape\n\nprint(\"# Num of Features: \", X_train.shape[1])\n\n##################\n#  Build Model\n##################\n\n\n#act = keras.layers.advanced_activations.PReLU(init='zero', weights=None)\n\ndef baseline_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(150, input_dim=X_train.shape[1], init='normal'))\n    model.add(PReLU())\n    model.add(Dropout(0.4))\n    model.add(Dense(50, input_dim=X_train.shape[1], init='normal'))\n    model.add(PReLU())\n    model.add(Dropout(0.2))\n    model.add(Dense(12, init='normal', activation='softmax'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n    return model\n\nmodel=baseline_model()\n\nfit= model.fit_generator(generator=batch_generator(X_train, y_train, 400, True),\n                         nb_epoch=16,\n                         samples_per_epoch=69984,\n                         validation_data=(X_val.todense(), y_val), verbose=2\n                         )\n\n# evaluate the model\nscores_val = model.predict_generator(generator=batch_generatorp(X_val, 400, False), val_samples=X_val.shape[0])\nprint('logloss val {}'.format(log_loss(y_val, scores_val)))\n\nprint(\"# Final prediction\")\nscores = model.predict_generator(generator=batch_generatorp(test_sp, 800, False), val_samples=test_sp.shape[0])\nresult = pd.DataFrame(scores , columns=lable_group.classes_)\nresult[\"device_id\"] = device_id\nprint(result.head(1))\nresult = result.set_index(\"device_id\")\n\n#result.to_csv('./sub_bagofapps7_keras_10_50_pt2_10epoch.csv', index=True, index_label='device_id')\n#Drop out 0.2\n#Validation 2.3017\nresult.to_csv('sub_bagofapps7_keras_150_pt4_50_pt2_15epoch_prelu_softmax.csv', index=True, index_label='device_id')\n\n\nprint(\"Done\")"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}