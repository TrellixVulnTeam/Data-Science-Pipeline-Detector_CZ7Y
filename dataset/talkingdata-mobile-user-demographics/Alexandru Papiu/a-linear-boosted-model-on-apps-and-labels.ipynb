{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"932ebb10-ffd5-4181-52ae-7ed367a85a6e"},"source":"**Forked from dune_dweller's excellent notebook. I try using xgboost gblinear instead of logistic regression but get slightly worse results.\n\nThis notebook shows how to build a linear model on features from apps, app labels, phone brands and device models. It uses LogisticRegression classifier from sklearn. \n\nIt also shows an efficient way of constructing bag-of-apps and bag-of-labels features without concatenating a bunch of strings."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"10e55365-83e8-f7b6-a26e-aaa376cad62a"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn.metrics import log_loss"},{"cell_type":"markdown","metadata":{"_cell_guid":"29ab3f35-c097-465e-e2a5-4efe1aa836aa"},"source":"## Load data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e76e779c-b6b2-d800-1150-b920c610e9ff"},"outputs":[],"source":"datadir = '../input'\ngatrain = pd.read_csv(os.path.join(datadir,'gender_age_train.csv'),\n                      index_col='device_id')\ngatest = pd.read_csv(os.path.join(datadir,'gender_age_test.csv'),\n                     index_col = 'device_id')\nphone = pd.read_csv(os.path.join(datadir,'phone_brand_device_model.csv'))\n# Get rid of duplicate device ids in phone\nphone = phone.drop_duplicates('device_id',keep='first').set_index('device_id')\nevents = pd.read_csv(os.path.join(datadir,'events.csv'),\n                     parse_dates=['timestamp'], index_col='event_id')\nappevents = pd.read_csv(os.path.join(datadir,'app_events.csv'), \n                        usecols=['event_id','app_id','is_active'],\n                        dtype={'is_active':bool})\napplabels = pd.read_csv(os.path.join(datadir,'app_labels.csv'))"},{"cell_type":"markdown","metadata":{"_cell_guid":"456cc4a1-26fc-0248-9cec-e786adcb35c7"},"source":"## Feature engineering\n\nThe features I'm going to use include:\n\n* phone brand\n* device model\n* installed apps\n* app labels\n\nI'm going to one-hot encode everything and sparse matrices will help deal with a very large number of features.\n\n### Phone brand\n\nAs preparation I create two columns that show which train or test set row a particular device_id belongs to."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f81678cf-e762-19b5-0a67-cf6430689886"},"outputs":[],"source":"gatrain['trainrow'] = np.arange(gatrain.shape[0])\ngatest['testrow'] = np.arange(gatest.shape[0])"},{"cell_type":"markdown","metadata":{"_cell_guid":"c8d7224e-7680-789a-fd8c-633ac1a9093b"},"source":"A sparse matrix of features can be constructed in various ways. I use this constructor:\n\n    csr_matrix((data, (row_ind, col_ind)), [shape=(M, N)])\n    where ``data``, ``row_ind`` and ``col_ind`` satisfy the\n    relationship ``a[row_ind[k], col_ind[k]] = data[k]``\n    \nIt lets me specify which values to put into which places in a sparse matrix. For phone brand data the `data` array will be all ones, `row_ind` will be the row number of a device and `col_ind` will be the number of brand."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1e93897e-518c-8ea3-4e55-b204f0d08985"},"outputs":[],"source":"brandencoder = LabelEncoder().fit(phone.phone_brand)\nphone['brand'] = brandencoder.transform(phone['phone_brand'])\ngatrain['brand'] = phone['brand']\ngatest['brand'] = phone['brand']\nXtr_brand = csr_matrix((np.ones(gatrain.shape[0]), \n                       (gatrain.trainrow, gatrain.brand)))\nXte_brand = csr_matrix((np.ones(gatest.shape[0]), \n                       (gatest.testrow, gatest.brand)))\nprint('Brand features: train shape {}, test shape {}'.format(Xtr_brand.shape, Xte_brand.shape))"},{"cell_type":"markdown","metadata":{"_cell_guid":"7668f876-da68-06aa-d9ab-5317cfbd2329"},"source":"### Device model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"850876c4-f13a-7854-f875-9f68617a6333"},"outputs":[],"source":"m = phone.phone_brand.str.cat(phone.device_model)\nmodelencoder = LabelEncoder().fit(m)\nphone['model'] = modelencoder.transform(m)\ngatrain['model'] = phone['model']\ngatest['model'] = phone['model']\nXtr_model = csr_matrix((np.ones(gatrain.shape[0]), \n                       (gatrain.trainrow, gatrain.model)))\nXte_model = csr_matrix((np.ones(gatest.shape[0]), \n                       (gatest.testrow, gatest.model)))\nprint('Model features: train shape {}, test shape {}'.format(Xtr_model.shape, Xte_model.shape))"},{"cell_type":"markdown","metadata":{"_cell_guid":"617f26d5-0bcb-1071-6cbc-02231e171c48"},"source":"### Installed apps features\n\nFor each device I want to mark which apps it has installed. So I'll have as many feature columns as there are distinct apps.\n\nApps are linked to devices through events. So I do the following:\n\n- merge `device_id` column from `events` table to `app_events`\n- group the resulting dataframe by `device_id` and `app` and aggregate\n- merge in `trainrow` and `testrow` columns to know at which row to put each device in the features matrix"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d4e074f1-a1cd-c69e-f2e1-d2ea6aae8a27"},"outputs":[],"source":"appencoder = LabelEncoder().fit(appevents.app_id)\nappevents['app'] = appencoder.transform(appevents.app_id)\nnapps = len(appencoder.classes_)\ndeviceapps = (appevents.merge(events[['device_id']], how='left',left_on='event_id',right_index=True)\n                       .groupby(['device_id','app'])['app'].agg(['size'])\n                       .merge(gatrain[['trainrow']], how='left', left_index=True, right_index=True)\n                       .merge(gatest[['testrow']], how='left', left_index=True, right_index=True)\n                       .reset_index())\ndeviceapps.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"445dc656-f877-743e-6eec-f2c217404d78"},"source":"Now I can build a feature matrix where the `data` is all ones, `row_ind` comes from `trainrow` or `testrow` and `col_ind` is the label-encoded `app_id`."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74a7f87d-6eb8-7553-addb-d32b72382506"},"outputs":[],"source":"d = deviceapps.dropna(subset=['trainrow'])\nXtr_app = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.app)), \n                      shape=(gatrain.shape[0],napps))\nd = deviceapps.dropna(subset=['testrow'])\nXte_app = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.app)), \n                      shape=(gatest.shape[0],napps))\nprint('Apps data: train shape {}, test shape {}'.format(Xtr_app.shape, Xte_app.shape))"},{"cell_type":"markdown","metadata":{"_cell_guid":"c48ad7c3-e0c1-8b11-da35-11f4b7f2d6b2"},"source":"### App labels features\n\nThese are constructed in a way similar to apps features by merging `app_labels` with the `deviceapps` dataframe we created above."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b9bd3b7c-9146-ae81-ef83-e92e5bcd6868"},"outputs":[],"source":"applabels = applabels.loc[applabels.app_id.isin(appevents.app_id.unique())]\napplabels['app'] = appencoder.transform(applabels.app_id)\nlabelencoder = LabelEncoder().fit(applabels.label_id)\napplabels['label'] = labelencoder.transform(applabels.label_id)\nnlabels = len(labelencoder.classes_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a2f082a8-8e42-a914-aa0d-f1b53ca17788"},"outputs":[],"source":"devicelabels = (deviceapps[['device_id','app']]\n                .merge(applabels[['app','label']])\n                .groupby(['device_id','label'])['app'].agg(['size'])\n                .merge(gatrain[['trainrow']], how='left', left_index=True, right_index=True)\n                .merge(gatest[['testrow']], how='left', left_index=True, right_index=True)\n                .reset_index())\ndevicelabels.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c4897b6-877a-2dc4-a546-a61dda27eb65"},"outputs":[],"source":"d = devicelabels.dropna(subset=['trainrow'])\nXtr_label = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.label)), \n                      shape=(gatrain.shape[0],nlabels))\nd = devicelabels.dropna(subset=['testrow'])\nXte_label = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.label)), \n                      shape=(gatest.shape[0],nlabels))\nprint('Labels data: train shape {}, test shape {}'.format(Xtr_label.shape, Xte_label.shape))"},{"cell_type":"markdown","metadata":{"_cell_guid":"4d900504-996c-3bf1-9ff6-251166b6dc36"},"source":"### Concatenate all features"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"31c19090-6cab-8935-c0dd-5dc7af47d5c9"},"outputs":[],"source":"Xtrain = hstack((Xtr_brand, Xtr_model, Xtr_app, Xtr_label), format='csr')\nXtest =  hstack((Xte_brand, Xte_model, Xte_app, Xte_label), format='csr')\nprint('All features: train shape {}, test shape {}'.format(Xtrain.shape, Xtest.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8340b287-1c39-c0e8-92e1-0a97a63a426f"},"outputs":[],"source":"Xtrain"},{"cell_type":"markdown","metadata":{"_cell_guid":"bc5e2845-0597-4bab-18c2-75deace40f40"},"source":"## Try xgboost:\ndune_dweller's original script uses a regularized logistic regression to tackle the problem so I was curious how a linear boosted model would do. To my surprise I haven't been able to beat the score of one linear model (granted I haven't been spent too much time on the parameter tuning)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"100a586a-d05b-5c3c-fb80-3cd20f07505e"},"outputs":[],"source":"targetencoder = LabelEncoder().fit(gatrain.group)\ny = targetencoder.transform(gatrain.group)\nnclasses = len(targetencoder.classes_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"308a18b8-1c9c-09f9-6c93-3cdddd5319d4"},"outputs":[],"source":"import xgboost as xgb"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff30dab7-55f3-0ab9-203a-dadab733c59b"},"outputs":[],"source":"dtrain = xgb.DMatrix(Xtrain, y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0bdff9e6-b69c-ace3-d797-f4900d471275"},"outputs":[],"source":"params = {\n        \"eta\": 0.1,\n        \"booster\": \"gblinear\",\n        \"objective\": \"multi:softprob\",\n        \"alpha\": 4,\n        \"lambda\": 0,\n        \"silent\": 1,\n        \"seed\": 1233,\n        \"num_class\": 12,\n        \"eval_metric\": \"mlogloss\"\n    }"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"98579bdc-cf00-1cc7-fbfa-56b4335e4f16"},"outputs":[],"source":"xgb.cv(params, dtrain, \n       num_boost_round=50, \n       #early_stopping_rounds = 5, \n       maximize = False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"91ef2026-4b6e-a46d-63a4-e1658b90283b"},"source":"It seems we can only get around 2.279 using xgboost. "},{"cell_type":"markdown","metadata":{"_cell_guid":"636bc986-feac-7785-132c-c55216b6e674"},"source":"## Try Random Forests:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0fc61778-b4d8-715f-d844-ddf890ba67dd"},"outputs":[],"source":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\n#model.fit(Xtrain, y) hmm this is going to take too long."},{"cell_type":"markdown","metadata":{"_cell_guid":"6cf9883e-62c3-27dd-71af-ef765c59bafb"},"source":"## Predict on test data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b9a67b06-83ec-cbce-d810-1a4ce78f3a9d"},"outputs":[],"source":"model = xgb.train(params, dtrain, num_boost_round=25)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6b6c501e-bc9a-86bb-0374-8f390039aee9"},"outputs":[],"source":"dtest = xgb.DMatrix(Xtest)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6147b3c6-457e-d046-6064-4b494ff9c302"},"outputs":[],"source":"model.predict(dtest)\npred = pd.DataFrame(model.predict(dtest), index = gatest.index, columns=targetencoder.classes_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"478c75c7-1366-a234-d190-7a038d39815f"},"outputs":[],"source":"pred.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"88cf5ad8-e1ba-874f-89f4-4011be29ea52"},"outputs":[],"source":"pred.to_csv('xgb_subm.csv',index=True) "},{"cell_type":"markdown","metadata":{"_cell_guid":"ac0f998c-0093-ce33-31dc-d7c1e60da87f"},"source":"The submission above scores 2.26735 on the LB, slightly worse than \t2.26508 for dune_dweller's original script. I guess this should a good lesson to not ignore logistic regression in the future :)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60071691-e879-9ca0-f7ff-12256a9f2e7d"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}