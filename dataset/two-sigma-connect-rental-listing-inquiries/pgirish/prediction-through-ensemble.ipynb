{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"cd833648-167f-6f1b-1f21-547e2e2c7387"},"source":"I am predicting the interest level of rental listing through Ensembles since these algorithms can give  boost in accuracy on the dataset. In this project i have used Boosting, Bagging and Majority Voting and compare the accuracy of the models on the datasets.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d503fb10-78cf-1384-68b1-0d9e38a09ce9"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f52ab76-201d-75d1-86e5-01d26df91372"},"outputs":[],"source":"# Supress unnecessary warnings so that presentation looks clean\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\n# Python version\nimport sys\nprint('Python: {}'.format(sys.version))\n# scipy\nimport scipy\nprint('scipy: {}'.format(scipy.__version__))\n# numpy\nimport numpy\nprint('numpy: {}'.format(numpy.__version__))\n# matplotlib\nimport matplotlib\nprint('matplotlib: {}'.format(matplotlib.__version__))\n# pandas\nimport pandas\nprint('pandas: {}'.format(pandas.__version__))\n# scikit-learn\nimport sklearn\nprint('sklearn: {}'.format(sklearn.__version__))"},{"cell_type":"markdown","metadata":{"_cell_guid":"4dc8bc4c-ea52-ddb3-43c2-51df2ff2a9a9"},"source":"## Load the Data ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42d818b3-457c-9d67-9e9c-d1bf66d9570d"},"outputs":[],"source":"#provides data structures to quickly analyze data\n#Read the train dataset\n\ndataset = pandas.read_json(\"../input/train.json\") \n"},{"cell_type":"markdown","metadata":{"_cell_guid":"b51f6b32-da47-9896-c961-ed5e754d32e8"},"source":"## Summarizing the Dataset ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2a963b08-6407-6df6-3190-6d5430150e9d"},"outputs":[],"source":"# Size of the dataframe\nprint(dataset.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ff9ec6a-1cd4-0f25-0f8f-bda01e74eff1"},"outputs":[],"source":"#view sample data\nprint (dataset.head(3))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dafa22e3-9655-74c0-5a77-b9f8649a49ff"},"outputs":[],"source":"#view the columns\nlist(dataset)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6448d0be-9cb1-c369-9d26-d814f75f77ed"},"outputs":[],"source":"# Now we can take a look at a summary of each attribute.\n# This includes the count, mean, the min and max values as well as some percentiles.\n# descriptions are only for continuous variable\nprint(dataset.describe())"},{"cell_type":"markdown","metadata":{"_cell_guid":"24c3ae9d-96d9-63e5-45ec-f33a164d67e9"},"source":"## Prepare data ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7edfc5b7-ca14-83dd-9365-a5ff3261d051"},"outputs":[],"source":"data = dataset.loc[:, ['bathrooms','bedrooms','created','description','display_address','features',\n                        'latitude','longitude','photos','price','street_address']] \n\ndata['created'] = pandas.to_datetime(data['created'], coerce=True)\ndata['Year'] = data['created'].dt.year\ndata['Month'] = data['created'].dt.month\ndata['Day'] = data['created'].dt.day\ndata[\"description\"] = data[\"description\"].apply(len)\ndata[\"display_address\"] = data[\"display_address\"].apply(len)\ndata[\"features\"] = data[\"features\"].apply(len)\ndata[\"photos\"] = data[\"photos\"].apply(len)\ndata[\"street_address\"] = data[\"street_address\"].apply(len)\ndata = data.drop('created', 1)\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d750e986-a211-f95a-9b02-7f107b48f0ee"},"outputs":[],"source":"dataset.interest_level.replace(['low','medium', 'high'], [1, 2, 3], inplace=True)\nY = dataset.loc[:,'interest_level']"},{"cell_type":"markdown","metadata":{"_cell_guid":"ee6d69a3-d7d2-ed6a-fbd2-5cf353466579"},"source":"## Evaluate Ensemble Algorithms ##"},{"cell_type":"markdown","metadata":{"_cell_guid":"e252f80f-6f9c-f2e2-cf8c-a17a6edcf0c3"},"source":"### Bagging###"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d315e89-cccd-0eb7-21f4-5b4244ac3baf"},"outputs":[],"source":"#Bagging\n#Bagged Decision Trees for Classification\nfrom sklearn import model_selection\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nseed = 7\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\ncart = DecisionTreeClassifier()\nnum_trees = 100\nmodel = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\nresults = model_selection.cross_val_score(model, data, Y, cv=kfold)\nprint(results.mean())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b05f9937-13c8-50bb-d00a-f75827099752"},"outputs":[],"source":"#Bagging\n# Random Forest Classification\nfrom sklearn.ensemble import RandomForestClassifier\nseed = 7\nnum_trees = 100\nmax_features = 8\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\nmodel = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\nresults = model_selection.cross_val_score(model,data, Y, cv=kfold)\nprint(results.mean())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"76cde301-4fef-ea28-f78a-c43d18010072"},"outputs":[],"source":"#Bagging\n# Extra Trees Classification\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nseed = 7\nnum_trees = 100\nmax_features = 8\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\nmodel = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\nresults = model_selection.cross_val_score(model, data, Y, cv=kfold)\nprint(results.mean())"},{"cell_type":"markdown","metadata":{"_cell_guid":"d2e67635-d398-9323-005d-03d0ec885eed"},"source":"###Boosting###"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b241ec75-5337-aa3a-bac8-10525a1205f5"},"outputs":[],"source":"#Boosting\n# AdaBoost Classification\nfrom sklearn.ensemble import AdaBoostClassifier\n\nseed = 7\nnum_trees = 30\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\nmodel = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\nresults = model_selection.cross_val_score(model, data, Y, cv=kfold)\nprint(results.mean())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e360581e-3e61-bc60-9fe0-9d7e98b1b882"},"outputs":[],"source":"#Boosting\n# Stochastic Gradient Boosting Classification\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nseed = 7\nnum_trees = 100\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\nmodel = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\nresults = model_selection.cross_val_score(model, data, Y, cv=kfold)\nprint(results.mean())"},{"cell_type":"markdown","metadata":{"_cell_guid":"4c37ed8f-9f1f-714b-cd9a-c03d1e121add"},"source":"###Voting Ensemble ###"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4985f493-b71e-9b98-bb7a-eb6d9b8fb3ab"},"outputs":[],"source":"# Voting Ensemble for Classification\nimport pandas\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\n\nseed = 7\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\n# create the sub models\nestimators = []\nmodel1 = LogisticRegression()\nestimators.append(('logistic', model1))\nmodel2 = DecisionTreeClassifier()\nestimators.append(('cart', model2))\nmodel3 = LinearDiscriminantAnalysis()\nestimators.append(('LDA', model3))\nmodel4 = KNeighborsClassifier()\nestimators.append(('KNN', model4))\n\n# create the ensemble model\nensemble = VotingClassifier(estimators)\nresults = model_selection.cross_val_score(ensemble, data, Y, cv=kfold)\nprint(results.mean())"},{"cell_type":"markdown","metadata":{"_cell_guid":"9f726bb1-0d92-d066-fe65-5ac40eeb7bab"},"source":"**By comparing all the models we can see Random Forest Classifier is the best.**"},{"cell_type":"markdown","metadata":{"_cell_guid":"da57402e-4bb1-a4cc-f753-996609980910"},"source":"## Make Predictions ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"22ba5b80-74c5-9ee3-581a-323e270a74e4"},"outputs":[],"source":"test_dataset = pandas.read_json(\"../input/test.json\") \n\ntest_data = test_dataset.loc[:, ['bathrooms','bedrooms','created','description','display_address','features',\n                        'latitude','longitude','photos','price','street_address']] \n\ntest_data['created'] = pandas.to_datetime(test_data['created'], coerce=True)\n\ntest_data['Year'] = test_data['created'].dt.year\ntest_data['Month'] = test_data['created'].dt.month\ntest_data['Day'] = test_data['created'].dt.day\n\ntest_data[\"description\"] = test_data[\"description\"].apply(len)\ntest_data[\"display_address\"] = test_data[\"display_address\"].apply(len)\ntest_data[\"features\"] = test_data[\"features\"].apply(len)\ntest_data[\"photos\"] = test_data[\"photos\"].apply(len)\ntest_data[\"street_address\"] = test_data[\"street_address\"].apply(len)\n\ntest_data = test_data.drop('created', 1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3da73c11-fce6-b301-6bbe-01d892f9499f"},"outputs":[],"source":"from sklearn.ensemble import RandomForestClassifier\nseed = 7\nnum_trees = 100\nmax_features = 8\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\nmodel = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\nclassifier = model.fit(data,Y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf1eb222-6e84-0b4c-7d88-95de5633dca9"},"outputs":[],"source":"results = model_selection.cross_val_score(model,data, Y, cv=kfold)\nprint(results.mean())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5e3b20e2-b768-c8a6-d92d-02c4d99a05ff"},"outputs":[],"source":"prediction = classifier.predict(test_data) \nprediction_prob = classifier.predict_proba(test_data)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aaf7496d-f36f-4a84-2941-d2db7ff1539b"},"outputs":[],"source":"output = pandas.DataFrame()\noutput['listing_id'] = test_dataset['listing_id']\noutput['interest_level'] = prediction\noutput.interest_level.replace([1, 2, 3], ['low', 'medium', 'high'], inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e128ba7d-687b-7b59-390d-5ab083e51df1"},"outputs":[],"source":"output_prob = pandas.DataFrame()\noutput_prob['listing_id'] = test_dataset['listing_id']\n\noutput_prob['high'] = prediction_prob[:,2]\noutput_prob['medium'] = prediction_prob[:,1]\noutput_prob['low'] = prediction_prob[:,0]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"800c3ee0-3996-8afb-994e-2fa2f69084aa"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}