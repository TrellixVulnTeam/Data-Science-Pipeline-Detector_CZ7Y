{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Two Sigma Connect: Rental Listing Inquiries\n\nI got this exercise as a hiring test.<br>\nThe limitations I have:\n* images are not availble anymore (no peers)\n* for some resons I had only two days\n\nConsidering the above I decided to use Catboost as multipurpose tool with powerful built-in modules for categorical and text data preprocessing.","metadata":{}},{"cell_type":"markdown","source":"### Content\n1. Data loading\n1. Defining pipeline\n1. Playing with features\n1. Catboost baseline\n1. Catbost grid search\n1. Augmentation (not used)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom catboost import Pool, CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom numpy.random import seed\nseed(17)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-28T09:11:17.435131Z","iopub.execute_input":"2022-01-28T09:11:17.435771Z","iopub.status.idle":"2022-01-28T09:11:18.403698Z","shell.execute_reply.started":"2022-01-28T09:11:17.435662Z","shell.execute_reply":"2022-01-28T09:11:18.402972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_raw = pd.read_json('../input/two-sigma-connect-rental-listing-inquiries/train.json.zip').reset_index(drop = True)\ntest_raw = pd.read_json('../input/two-sigma-connect-rental-listing-inquiries/test.json.zip').reset_index(drop = True)\nfull_raw = pd.concat([train_raw, test_raw])\n\nprint(train_raw.shape, test_raw.shape, full_raw.shape)\ntrain_raw.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T09:11:18.405398Z","iopub.execute_input":"2022-01-28T09:11:18.40565Z","iopub.status.idle":"2022-01-28T09:11:24.204043Z","shell.execute_reply.started":"2022-01-28T09:11:18.405616Z","shell.execute_reply":"2022-01-28T09:11:24.20337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After a quick vivew at the data I decided to make the following features transformations:\n\n\n```bathrooms```       convert to int<br>\n```bedrooms```        no changes<br>\n```building_id```     drop<br>\n```created```         extract day and month<br>\n```description```     clean text<br>\n```display_address``` clean text<br>\n```features```        extract from list and clean text<br>\n```latitude```        no changes<br>\n```listing_id```      drop<br>\n```longitude```       no changes<br>\n```manager_id```      no changes<br>\n```photos```          count links (as images itself are not availble)<br>\n```price```           round<br>\n```street_address```  drop<br>\n```interest_level```  convert to labels\n\nI do not care about outliers and frequency encoding as Catboost has perfect built-in [algorithms](https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic) for this, especially for GPU.<br>And the best thing is that it process [text](https://catboost.ai/en/docs/concepts/algorithm-main-stages_text-to-numeric) data automatically as well, it makes life much more easier. The only thing I have to do is to set feature types.<br>\n*Honestly I tried to remove outliers manually, but result was worse...*","metadata":{}},{"cell_type":"markdown","source":"### Process data","metadata":{}},{"cell_type":"code","source":"#Drop unused\nfull = full_raw.drop(['building_id', 'listing_id'], axis=1)\n\n#Extract from list\nfull['features'] = [','.join(map(str, i)) for i in full['features']]\n\n#Convert dtypes\nfull[['bathrooms', 'bedrooms']] = full[['bathrooms', 'bedrooms']].astype(int)\n\n#Extract day/month and drop original date\nfull['day'] = pd.to_datetime(full.created).dt.day.astype('object')\nfull['month'] = pd.to_datetime(full.created).dt.day.astype('object')\nfull = full.drop('created', axis=1)\n\n#Count web links\nfull['photos'] = full.photos.apply(len)\n\n#Round price\nfull.price = full.price // 50 * 50\nfull.loc[full.price > 10000, 'price'] = full.loc[full.price > 10000, 'price'] // 500 * 500\n\n#Replace classes by labels\nfull = full.replace({'interest_level' : { 'high' : 0, 'medium' : 1, 'low' : 2 }})","metadata":{"execution":{"iopub.status.busy":"2022-01-28T09:11:24.205248Z","iopub.execute_input":"2022-01-28T09:11:24.205568Z","iopub.status.idle":"2022-01-28T09:11:24.760311Z","shell.execute_reply.started":"2022-01-28T09:11:24.20553Z","shell.execute_reply":"2022-01-28T09:11:24.759522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text cleaning","metadata":{}},{"cell_type":"markdown","source":"Common things for all text features","metadata":{}},{"cell_type":"code","source":"#Columns to clean text\ncols_text = ['description', 'display_address', 'street_address', 'features']\n#To string\nfull[cols_text] = full[cols_text].astype(str)\n#To lower case\nfull[cols_text] = full[cols_text].apply(lambda x: x.str.lower())\n\n#Remove punctuation\nwith_whitespace = ['&', '(', ')', \"-\", \"_\", ':', '=', '\"', ',']\nwith_empty = ['.', \"'\", '`', '!', '*', '#', '/', '<', '>', 'br',\n              ';', '$', '%', '|', '+', '?']\n\n\ndef replace_symbol(df, to_replace, replace_by):\n    for symbol in to_replace:\n        df = df.apply(lambda x: x.str.replace(symbol, replace_by, regex = True)) \n    return df\n\nfull[cols_text] = replace_symbol(full[cols_text], with_whitespace, ' ')\nfull[cols_text] = replace_symbol(full[cols_text], with_empty, '')","metadata":{"execution":{"iopub.status.busy":"2022-01-28T09:11:24.762822Z","iopub.execute_input":"2022-01-28T09:11:24.763138Z","iopub.status.idle":"2022-01-28T09:11:35.054002Z","shell.execute_reply.started":"2022-01-28T09:11:24.7631Z","shell.execute_reply":"2022-01-28T09:11:35.053203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Text cleaning in ```display_address``` and ```street_address```","metadata":{}},{"cell_type":"code","source":"adr_feat = ['display_address', 'street_address']\n\n#Correct 'street' and 'avenue' cuts\nfull[adr_feat] = full[adr_feat].replace(['\\sst\\s', '\\sst$'], ' street', regex = True)\nfull[adr_feat] = full[adr_feat].replace(['\\save\\s', '\\save$'], ' avenue', regex = True)\n\n#Correct 'east' and 'west' cuts\nfull[adr_feat] = full[adr_feat].replace(['\\se\\s', '^e\\s'], ' east ', regex = True)\nfull[adr_feat] = full[adr_feat].replace(['\\sw\\s', '^w\\s'], ' west ', regex = True)\nfull[adr_feat].sample()","metadata":{"execution":{"iopub.status.busy":"2022-01-28T09:11:35.056079Z","iopub.execute_input":"2022-01-28T09:11:35.056281Z","iopub.status.idle":"2022-01-28T09:11:39.725205Z","shell.execute_reply.started":"2022-01-28T09:11:35.056256Z","shell.execute_reply":"2022-01-28T09:11:39.724525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Text cleaning in ```description``` and ```features```","metadata":{}},{"cell_type":"code","source":"desc_feat = ['description', 'features']\n#web links\nfull[desc_feat] = full[desc_feat].replace('\\swww.\\S*', ' weblink ', regex = True)\n#emails\nfull[desc_feat] = full[desc_feat].replace('\\s\\S*@\\S*', ' emailaddress', regex = True)\n#time\nfull[desc_feat] = full[desc_feat].replace('\\s\\d{1,2}\\s\\d\\d[ap]m', ' ampmtime', regex = True)\n#phone numbers\nfull[desc_feat] = full[desc_feat].replace('\\s\\d{2,4}\\s\\d{2,4}\\s\\d{2,4}', ' phonenumber', regex = True)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T09:11:39.72657Z","iopub.execute_input":"2022-01-28T09:11:39.726849Z","iopub.status.idle":"2022-01-28T09:11:50.474595Z","shell.execute_reply.started":"2022-01-28T09:11:39.726813Z","shell.execute_reply":"2022-01-28T09:11:50.473852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Trim leading, tailing and multiple whitespaces","metadata":{"execution":{"iopub.status.busy":"2022-01-27T09:16:07.041899Z","iopub.execute_input":"2022-01-27T09:16:07.042238Z","iopub.status.idle":"2022-01-27T09:16:07.082312Z","shell.execute_reply.started":"2022-01-27T09:16:07.042204Z","shell.execute_reply":"2022-01-27T09:16:07.08126Z"}}},{"cell_type":"code","source":"#Reduce multiple whitespaces\nfull[cols_text] = full[cols_text].replace('\\s+', ' ', regex = True)\n#Trim leading and tailing whitespaces\nfull[cols_text] = full[cols_text].replace(['^\\s', '\\s$'], '', regex = True)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T09:11:50.476078Z","iopub.execute_input":"2022-01-28T09:11:50.476321Z","iopub.status.idle":"2022-01-28T09:12:02.908788Z","shell.execute_reply.started":"2022-01-28T09:11:50.476287Z","shell.execute_reply":"2022-01-28T09:12:02.908028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Catboost","metadata":{}},{"cell_type":"markdown","source":"Split back concatenated dataframe to train / test and additionally split train for ```X``` and ```y```","metadata":{}},{"cell_type":"code","source":"X = full.iloc[:-1 * len(test_raw)].drop('interest_level', axis=1)\ny = full.iloc[:-1 * len(test_raw)].interest_level\ntest = full.iloc[-1 * len(test_raw):].drop('interest_level', axis=1)\nprint(full.shape, '->', X.shape, y.shape, test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T09:12:02.910172Z","iopub.execute_input":"2022-01-28T09:12:02.910419Z","iopub.status.idle":"2022-01-28T09:12:02.932225Z","shell.execute_reply.started":"2022-01-28T09:12:02.910386Z","shell.execute_reply":"2022-01-28T09:12:02.93151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train / validation split and create data pools.<br>\n*[Pool](https://catboost.ai/en/docs/concepts/python-reference_pool) is a special Catboost data constructor that increases training performance*<br>\nAs you can see, I set ```display_address``` as a category and the ```street_address``` as a text. Experiments have demonstrated that this was the right approach.","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid =  train_test_split(\n                                      X, y, test_size=0.2, stratify=y, random_state=17)\n\ncat_features = ['day', 'month', 'manager_id', 'display_address']\ntext_features = ['description','street_address' , 'features']\n\nTrain = Pool(data=X_train,\n             label=y_train,\n             cat_features=cat_features,\n             text_features = text_features)\n            \nValid = Pool(data=X_valid,\n             label=y_valid,\n             cat_features=cat_features,\n             text_features = text_features)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T09:28:11.819407Z","iopub.execute_input":"2022-01-28T09:28:11.819655Z","iopub.status.idle":"2022-01-28T09:28:11.981344Z","shell.execute_reply.started":"2022-01-28T09:28:11.819627Z","shell.execute_reply":"2022-01-28T09:28:11.980562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Baseline\nSimple out-of-the-box Catboost model.<br>\nThere is not log_loss **metric** in Catboost, but as it has the same nature as MultiClass **loss_function** I didn't set any metric and left it by default.","metadata":{}},{"cell_type":"code","source":"model = CatBoostClassifier( random_seed = 17,     \n                            thread_count = -1, \n                            verbose = 100,  \n                            loss_function='MultiClass',\n                            task_type = \"GPU\" )\n# Fit model\nmodel.fit(Train, eval_set=Valid)\npreds_class = model.predict(Valid)\npreds_proba = model.predict_proba(Valid)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T09:28:14.303286Z","iopub.execute_input":"2022-01-28T09:28:14.303539Z","iopub.status.idle":"2022-01-28T09:28:44.07968Z","shell.execute_reply.started":"2022-01-28T09:28:14.303509Z","shell.execute_reply":"2022-01-28T09:28:44.07891Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature importances\nYou can see how it was imprtant to manage text features","metadata":{}},{"cell_type":"code","source":"FE = model.get_feature_importance(data=Valid,\n                       thread_count=-1,\n                       verbose=False)\nFEG = pd.DataFrame(FE, index = X_valid.columns ).sort_values(0, ascending = False)\nFEG.plot.bar(figsize = (20,5), rot = 0)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T09:29:07.342137Z","iopub.execute_input":"2022-01-28T09:29:07.342399Z","iopub.status.idle":"2022-01-28T09:29:08.979569Z","shell.execute_reply.started":"2022-01-28T09:29:07.34237Z","shell.execute_reply":"2022-01-28T09:29:08.978882Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Validation scores","metadata":{}},{"cell_type":"code","source":"result = pd.DataFrame()\nresult['Real'] = y_valid.values\nresult['Pred'] = preds_class[:,0]\n\nfor i in range(3):\n    print('Accuracy for Class', str(i), ':', \"%.4f\" %\n          (result.loc[result.Real == i, 'Real'] ==\n           result.loc[result.Real == i, 'Pred']).mean())\n    print('Class', str(i), 'in observations:', \"%.0f\" %\n          (result.loc[result.Real == i].shape[0] / len(result) * 100),\n          '%', \"\\n\")\nprint ('Mean accuracy:',  \"%.4f\" % (result.Real == result.Pred).mean(), \"\\n\")\nfor i in range(3):\n    print('Class', str(i), 'observations count:', \"%.0f\" % y[y==i].shape[0])","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is the **main problem**: the data set is very imbalanced, this is why scores for minority classes is too low.<br>\nOne of possible solution is to set weights for classes, I will try it in grid search.","metadata":{}},{"cell_type":"code","source":"model.get_all_params()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-28T09:13:42.776632Z","iopub.execute_input":"2022-01-28T09:13:42.776878Z","iopub.status.idle":"2022-01-28T09:13:42.787348Z","shell.execute_reply.started":"2022-01-28T09:13:42.776846Z","shell.execute_reply":"2022-01-28T09:13:42.786429Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Baseline model predictions","metadata":{}},{"cell_type":"code","source":"#Generate new pool from all train data\nTest = Pool(data=test,\n            cat_features=cat_features,\n            text_features = text_features)\n\npreds_proba = model.predict_proba(Test)\npredictions = pd.DataFrame(preds_proba)\n\nsub = pd.read_csv('../input/two-sigma-connect-rental-listing-inquiries/sample_submission.csv.zip')\n\nsub['high'] = predictions[0]\nsub['medium'] = predictions[1]\nsub['low'] = predictions[2]\n\nsub.to_csv('submission_baseline.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T09:31:54.975517Z","iopub.execute_input":"2022-01-28T09:31:54.976335Z","iopub.status.idle":"2022-01-28T09:32:04.782015Z","shell.execute_reply.started":"2022-01-28T09:31:54.97629Z","shell.execute_reply":"2022-01-28T09:32:04.781261Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Baseline model score on validation was 0.5593, when I made submissions I got 0.56646 in private LB.<br>\nIt is not good score compare to others...","metadata":{}},{"cell_type":"markdown","source":"#### Grid search","metadata":{}},{"cell_type":"markdown","source":"Here are my trying with the grid search. Although it is possible to use sklearn GridSearchCV with Catboost models I kindly recommend you to use built-in ```grid_search``` as it works much faster. The main reason that built-in module support GPU.\nTo save notebook commiting time I kept only the best parameters, but you can see all other values I played with commented.<br>\nMy major disappointment is that ```auto_class_weights``` did not work as I hoped. Class manual setting (Catboost allows to do this) dicreased scores as well.<br>\nActually the main thing I could play with is number of trees, all other parameters Catboost picks up well automatically, depending on what I set manually.","metadata":{}},{"cell_type":"code","source":"Train_gs = Pool(data=X,\n             label=y,\n             cat_features=cat_features,\n             text_features = text_features)\n\nmodel_gs = CatBoostClassifier(random_seed = 17,     \n                            thread_count = -1, \n                            verbose = 1000,  \n                            loss_function='MultiClass',\n                            task_type = \"GPU\",\n                            )\nparams_gs = {\n            'iterations': [10000],    #  [1000,2500,5000, 10000]\n            #'learning_rate': [0.01, 0.1, 0.15, 0.3, 0.5], \n            #'auto_class_weights': ['None', 'Balanced', 'SqrtBalanced']\n            #'depth': [4, 6, 8, 10]\n            #'l2_leaf_reg': [2,3,4]\n            #'min_data_in_leaf': [1, 2, 3,4]\n            }\ngs_result = model_gs.grid_search(params_gs, \n                              Train_gs, \n                              partition_random_seed = 17,\n                              stratified = True,\n                              verbose = 1000,\n                              plot=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T10:49:20.082571Z","iopub.execute_input":"2022-01-28T10:49:20.083273Z","iopub.status.idle":"2022-01-28T11:02:02.913421Z","shell.execute_reply.started":"2022-01-28T10:49:20.083234Z","shell.execute_reply":"2022-01-28T11:02:02.912741Z"},"scrolled":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_gs.get_all_params()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Final model predictions and submissions","metadata":{}},{"cell_type":"code","source":"Test = Pool(data=test,\n            cat_features=cat_features,\n            text_features = text_features)\n\npreds_proba_gs = model_gs.predict_proba(Test)\npredictions_gs = pd.DataFrame(preds_proba_gs)\n\nsub_gs = pd.read_csv('../input/two-sigma-connect-rental-listing-inquiries/sample_submission.csv.zip')\n\nsub_gs['high'] = predictions_gs[0]\nsub_gs['medium'] = predictions_gs[1]\nsub_gs['low'] = predictions_gs[2]\n\nsub_gs.head()\nsub_gs.to_csv('submission_final.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T11:02:02.950779Z","iopub.execute_input":"2022-01-28T11:02:02.951565Z","iopub.status.idle":"2022-01-28T11:02:25.016606Z","shell.execute_reply.started":"2022-01-28T11:02:02.951531Z","shell.execute_reply":"2022-01-28T11:02:25.015762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Final submission score is 0.55789.<br>\n**Thank you for your attention !**","metadata":{}},{"cell_type":"markdown","source":"#### Augmentation (optional reading)\nOne of the ways to manage imbalanced data is data augmentation - adding synthetic data for minor classes. The issue is that in this competition you need to augment text data as well, which takes a lot of time, which I did not have. *(in my algorithm, the augmentation of 10 text cells took about a minute due to google translate lag and slow pandas.apply() method)*<br>\nBy the way  in the hidden cell you can find my algorithm, maybe it will be useful for your experiments. The algorithm uses translation to random language with further retranslation to English. I tried augmenting the numeric data only, but it expectedly worsened the situation.","metadata":{}},{"cell_type":"code","source":"# !pip uninstall googletrans -y\n# !pip install googletrans==4.0.0rc1\n# import googletrans\n# from googletrans import Translator\n\n# data = X_train.join(y_train)\n\n# data_0 = data[data.interest_level == 0]\n# data_1 = data[data.interest_level == 1]\n\n# data_0_aug = data_1_aug = pd.DataFrame()\n\n# data_0_aug = pd.concat([data_0]*(Class_0_add-1))\n# data_1_aug = pd.concat([data_1]*(Class_1_add-1))\n\n# data_aug = pd.concat([data_0_aug, data_1_aug]).reset_index(drop = True)\n# for i in range(3):\n#     print('Class', str(i), 'count:', \"%.0f\" % data_aug[data_aug.interest_level == i].shape[0])\n\n\n# length = data_aug.shape[0]\n\n# rand_price = np.random.uniform(0.98, 1.02, length)\n# rand_long = np.random.uniform(0.999, 1.001, length)\n# rand_lat = np.random.uniform(0.9995, 1.0005, length)\n\n# data_aug['price'] = (data_aug['price'] * rand_price).astype(int) // 10 * 10\n# data_aug['longitude'] = round(data_aug['longitude'] * rand_long,4)\n# data_aug['latitude'] = round(data_aug['latitude'] * rand_lat,4)\n\n# train_new = pd.concat([data, data_aug])\n# print('New train set:')\n# for i in range(3):\n#     print('Class', str(i), 'count:', \"%.0f\" % train_new[train_new.interest_level == i].shape[0])\n\n\n# #*****Text Data distortion*****\n\n# languages = list(googletrans.LANGUAGES.keys())\n# def text_aug (text):\n#     rand = np.random.randint(0, len(languages))\n#     translated = translator.translate(text, dest = languages[rand])\n#     return str.lower(translator.translate\n#                     (translated.text, dest = 'en').text)\n\n# # Can be used only column by column, like this:\n# for col in df.columns:\n#     df[col] = df[col].apply(text_aug)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T09:15:55.685377Z","iopub.status.idle":"2022-01-28T09:15:55.687276Z","shell.execute_reply.started":"2022-01-28T09:15:55.687047Z","shell.execute_reply":"2022-01-28T09:15:55.687071Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}