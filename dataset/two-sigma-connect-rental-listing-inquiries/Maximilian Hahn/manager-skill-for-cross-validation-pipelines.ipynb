{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"ba01e1ef-4a94-10da-587e-398677c2157c"},"source":"Based on the insightful script by den3b (https://www.kaggle.com/den3b81/two-sigma-connect-rental-listing-inquiries/improve-perfomances-using-manager-features/notebook) and others, we know that a manager skill feature can improve the performance of our models.\n\nHowever, since the target variable is encoded in this feature, it can be hard to use this feature in cross-validation. Fortunately, scikit-learn provides us with so-called pipelines (http://scikit-learn.org/stable/modules/pipeline.html), where feature construction and estimation can be chained during cross-validation.\n\nFor this to work, you need the feature construction in the fit-transform scheme of sklearn. I reused the code of den3b to do exactly this. Maybe it is useful for people who did not know this before or are just too lazy ;)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"226947f8-7539-9883-abb0-e0f60eafc0f3"},"outputs":[],"source":"#Let us import some modules\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split # Train-Test-Split"},{"cell_type":"markdown","metadata":{"_cell_guid":"3dcebae6-abb5-ffe6-eb46-a18dd55dd8a0"},"source":"Now let us load the data just like in the script of den3b and perform a train-test-split."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9d054dcc-224d-4e51-1982-6a58dc396406"},"outputs":[],"source":"df = pd.read_json(open(\"../input/train.json\", \"r\"))\n\n#Encode label\nfrom sklearn import preprocessing\n\nlbl = preprocessing.LabelEncoder()\nlbl.fit(list(df['manager_id'].values))\ndf['manager_id'] = lbl.transform(list(df['manager_id'].values))\n\n\nX = df.drop([\"interest_level\"], axis = 1)\ny = df[\"interest_level\"]\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)\n\nX_train.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a7e9e390-1ca2-29bf-8ad0-1756913377e1"},"source":"Now it is time do define our function or class respectively. It will inherit from two base sklearn classes, but you do not have to think about this. Most notably, this will give us a fit-transform method later."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d60f212-1f7a-4290-cd8c-a2103f24ce7c"},"outputs":[],"source":"#Our feature construction class will inherit from these two base classes of sklearn.\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\n\nclass manager_skill(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Adds the column \"manager_skill\" to the dataset, based on the Kaggle kernel\n    \"Improve Perfomances using Manager features\" by den3b. The function should\n    be usable in scikit-learn pipelines.\n    \n    Parameters\n    ----------\n    threshold : Minimum count of rental listings a manager must have in order\n                to get his \"own\" score, otherwise the mean is assigned.\n\n    Attributes\n    ----------\n    mapping : pandas dataframe\n        contains the manager_skill per manager id.\n        \n    mean_skill : float\n        The mean skill of managers with at least as many listings as the \n        threshold.\n    \"\"\"\n    def __init__(self, threshold = 5):\n        \n        self.threshold = threshold\n        \n    def _reset(self):\n        \"\"\"Reset internal data-dependent state of the scaler, if necessary.\n        \n        __init__ parameters are not touched.\n        \"\"\"\n        # Checking one attribute is enough, becase they are all set together\n        # in fit        \n        if hasattr(self, 'mapping_'):\n            \n            self.mapping_ = {}\n            self.mean_skill_ = 0.0\n        \n    def fit(self, X,y):\n        \"\"\"Compute the skill values per manager for later use.\n        \n        Parameters\n        ----------\n        X : pandas dataframe, shape [n_samples, n_features]\n            The rental data. It has to contain a column named \"manager_id\".\n            \n        y : pandas series or numpy array, shape [n_samples]\n            The corresponding target values with encoding:\n            low: 0.0\n            medium: 1.0\n            high: 2.0\n        \"\"\"        \n        self._reset()\n        \n        temp = pd.concat([X.manager_id,pd.get_dummies(y)], axis = 1).groupby('manager_id').mean()\n        temp.columns = ['low_frac', 'medium_frac', 'high_frac']\n        temp['count'] = X.groupby('manager_id').count().iloc[:,1]\n        \n        print(temp.head())\n        \n        temp['manager_skill'] = temp['high_frac']*2 + temp['medium_frac']\n        \n        mean = temp.loc[temp['count'] >= self.threshold, 'manager_skill'].mean()\n        \n        temp.loc[temp['count'] < self.threshold, 'manager_skill'] = mean\n        \n        self.mapping_ = temp[['manager_skill']]\n        self.mean_skill_ = mean\n            \n        return self\n        \n    def transform(self, X):\n        \"\"\"Add manager skill to a new matrix.\n        \n        Parameters\n        ----------\n        X : pandas dataframe, shape [n_samples, n_features]\n            Input data, has to contain \"manager_id\".\n        \"\"\"        \n        X = pd.merge(left = X, right = self.mapping_, how = 'left', left_on = 'manager_id', right_index = True)\n        X['manager_skill'].fillna(self.mean_skill_, inplace = True)\n        \n        return X"},{"cell_type":"markdown","metadata":{"_cell_guid":"d6bdfcd7-c04e-93f6-e323-794f6acd3063"},"source":"You can use this method just like any other scikit-learn transformation method:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d5b67e84-a103-f276-0c48-33684de50e5d"},"outputs":[],"source":"#Initialize the object\ntrans = manager_skill()\n#First, fit it to the training data:\ntrans.fit(X_train, y_train)\n#Now transform the training data\nX_train_transformed = trans.transform(X_train)\n#You can also do fit and transform in one step:\nX_train_transformed = trans.fit_transform(X_train, y_train)\nX_train_transformed.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"90b2eb6a-0b5e-2c25-eec5-1a3f181eac01"},"source":"After fitting on the training data, you can apply the method on the test data (or your hold-out set in cross-validation, pipelines will do this automatically):"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"15fe0e2c-5e17-8182-4b47-b5db49434b10"},"outputs":[],"source":"X_val_transformed = trans.transform(X_val)\nX_val_transformed.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"e8bdfa2c-deb7-749f-0583-4818b65a1d3f"},"source":"Now you can get \"real\" cross-validation results with this feature. I hope this is helpful for you. If you have any suggestions or find any mistakes, please let me know."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}