{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"4a05bdab-000c-8dda-fad1-8d38353a65d9"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e895db36-5d8c-da4a-72eb-4996c206d504"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2aec64d6-8e1c-53b6-5b53-2c769af8e9a2"},"outputs":[],"source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss"},{"cell_type":"markdown","metadata":{"_cell_guid":"cd274cd8-9b9d-55e7-6868-522212e00c1b"},"source":"# read data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"195645e9-f275-0f6b-7bda-a15469a12670"},"outputs":[],"source":"df = pd.read_json(open(\"../input/train.json\", \"r\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b276aa60-6294-9a7a-2964-692cd924ba03"},"outputs":[],"source":"print(df.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"58b09b32-445d-572d-9550-235defff1d62"},"outputs":[],"source":"df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"48237d07-fc8f-fb51-81a9-28e5052c0fd5"},"outputs":[],"source":"print(df.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6ce4d7dc-e9f0-25c5-77cd-4274e477ef95"},"source":"## Classifying Building ID\n\nThe text information is very worthwhile but not the easiest to address. Ideally, we'd parse the information in the displayed address, building description, and the building ID (which is stored as a string). We'll start with the easiest one first (building ID) because it's just one element. After we incorporate that, we can burn the other bridges as we come to them. \n\nBuilding ID could be interesting to analyze, but it's not in the right format for Sklearn so let's try building ID"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f770d6b-716d-36fd-608a-326ad7f422b0"},"outputs":[],"source":"#first lets get a sense of the most popular building IDs, how many are there? \n#this uses two methods, seperated onto different lines for clarity \nprint(\n    df['building_id']\n          .value_counts()\n          .nlargest(50)\n     )\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2c63447c-51da-cb94-0cc0-b5a8f09c591d"},"outputs":[],"source":"#This is a little more than I care to look it! I'll plot it - a picture is worth at least 1000 words. \n\ndf['street_address'].value_counts().plot(kind = 'hist', bins = 50)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ebf5f086-e936-c1a9-be75-cf3a85e07d03"},"source":"Let's try the preprocessing tool! - Note, this example commits a cardinal sin, it configures the label encoder with test and training data. If we think this is a good approach, we'll have to move some things around and fix them in the future! "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f4242daa-0ec8-7981-3384-5c01feb5c3b5"},"outputs":[],"source":"from sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\nle.fit(df['building_id'])\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"1b50f3de-38cb-70d0-2d12-1597d3d37bcd"},"source":"We'll have to transform our data so that the system can use it - "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0d482459-62fd-cd6d-b411-90051b04329b"},"outputs":[],"source":"df['building_id'] = le.fit_transform(df['building_id']) \n\ndf['building_id'].head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"13852bd6-5613-a204-13ed-40dc40336d8d"},"source":"## Text analysis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"76099eaf-00e8-1177-960e-75e3571baeeb"},"outputs":[],"source":"##make models / stuff for each interest level? \nimport re\nimport nltk\n\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nenglish_stemmer=nltk.stem.SnowballStemmer('english')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6077f937-317f-70b3-ed68-1f13c25bb333"},"outputs":[],"source":"## first we need to split the data and get the features so that we can train and validate this process\n\n\n(\n   X_train_low\n , X_val_low\n , y_train_low\n , y_val_low\n) = train_test_split(df[\"description\"][df[\"interest_level\"] == \"low\"], df[\"interest_level\"][df[\"interest_level\"] == \"low\"], test_size=0.60)\n\n(\n   X_train_medium\n , X_val_medium\n , y_train_medium\n , y_val_medium\n) = train_test_split(df[\"description\"][df[\"interest_level\"] == \"low\"], df[\"interest_level\"][df[\"interest_level\"] == \"low\"], test_size=0.60)\n\n\n(\n   X_train_high\n , X_val_high\n , y_train_high\n , y_val_high\n) = train_test_split(df[\"description\"][df[\"interest_level\"] == \"low\"], df[\"interest_level\"][df[\"interest_level\"] == \"low\"], test_size=0.60)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5e3fd45e-fe84-ff14-558c-8d1bb1254bd8"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aabefa0c-a441-a1c5-b0c3-8e2fce9401c0"},"outputs":[],"source":"def description_to_wordlist( description, remove_stopwords=True):\n    # Function to convert a document to a sequence of words,\n    # optionally removing stop words.  Returns a list of words.\n\n    # 1. Remove non-letters\n    description_text = re.sub(\"[^a-zA-Z]\",\" \", description)\n    #\n    # 2. Convert words to lower case and split them\n    words = review_text.lower().split()\n    #\n    # 3. Optionally remove stop words (True by default)\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n\n    b=[]\n    stemmer = english_stemmer #PorterStemmer()\n    for word in words:\n        b.append(stemmer.stem(word))\n\n    # 5. Return a list of words\n    return(b)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b00b40e-e82c-f94e-dda9-4a9c64bfd3e9"},"outputs":[],"source":"description_low = []\nfor description in X_train_low['description']:\n    description_low.append( \" \".join(description_to_wordlist(review)))\n   \ndescription_med = []\nfor description in X_train_med['description']:\n    description_med.append( \" \".join(description_to_wordlist(review)))\n\ndescription_high = []\nfor description in X_train_high['description']:\n    description_high.append( \" \".join(description_to_wordlist(review)))\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"1938afbe-c4de-3818-5777-167744aaa566"},"source":"# naive feature engineering"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c29fe75-fcad-6681-e78d-ff88fb785fb2"},"outputs":[],"source":"df[\"num_photos\"] = df[\"photos\"].apply(len)\ndf[\"num_features\"] = df[\"features\"].apply(len)\ndf[\"num_description_words\"] = df[\"description\"].apply(lambda x: len(x.split(\" \")))\ndf[\"created\"] = pd.to_datetime(df[\"created\"])\ndf[\"created_year\"] = df[\"created\"].dt.year\ndf[\"created_month\"] = df[\"created\"].dt.month\ndf[\"created_day\"] = df[\"created\"].dt.day\ndf[\"price_per_bedroom\"] =  df[\"bedrooms\"] / df[\"price\"] \ndf[\"price_per_bathroom\"] = df[\"bathrooms\"] / df[\"price\"] "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ebc84cd4-a2f7-36f4-4cfe-263509615704"},"outputs":[],"source":"df.columns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d9c6d2b-3109-1d27-f3a7-6df4330aee32"},"outputs":[],"source":"num_feats = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",\n             \"num_photos\", \"num_features\", \"num_description_words\",\n             \"created_year\", \"created_month\", \"created_day\", \"building_id\",\n              \"price_per_bedroom\", \"price_per_bathroom\"]\n\nX = df[num_feats]\ny = df[\"interest_level\"]\nX.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a66624cf-ed69-c745-449c-60ac80da57a6"},"source":"# train model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c39a7de-821a-e570-23e3-d9ea46d5d04c"},"outputs":[],"source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.45)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3173b2c8-a451-97a9-4aa2-2144c34f9b96"},"outputs":[],"source":"clf = RandomForestClassifier(n_estimators=1500, )\nclf.fit(X_train, y_train)\ny_val_pred = clf.predict_proba(X_val)\nlog_loss(y_val, y_val_pred)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f07e6451-e1b4-43d3-2e40-907e530fbb36"},"source":"# Train another model\n\nLet's try a bagging approachd!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b2a6e53-449a-64b3-91ed-b7b87f8562ac"},"outputs":[],"source":"from sklearn.ensemble import BaggingClassifier\nb1 = BaggingClassifier(n_estimators=2000, )\nb1.fit(X_train, y_train)\ny_val_pred = b1.predict_proba(X_val)\nlog_loss(y_val, y_val_pred)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79f0ded1-3190-2cbc-858f-ce13d19db5cd"},"outputs":[],"source":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\ny_val_pred = gbc.predict_proba(X_val)\nlog_loss(y_val, y_val_pred)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1a14d829-bcc8-a63b-3001-30653b31bf84"},"outputs":[],"source":"from sklearn import svm\nclf = svm.SVC()\nclf.fit(X_train, y_train)\ny_val_pred = clf.predict_proba(X_val)\nlog_loss(y_val, y_val_pred)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2cd86abf-e3cf-49f0-da1b-b0acbeba5bea"},"outputs":[],"source":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X_train, y_train)\ny_val_pred = neigh.predict_proba(X_val)\nlog_loss(y_val, y_val_pred)"},{"cell_type":"markdown","metadata":{"_cell_guid":"692a48a7-73e4-ff08-d9aa-eb79c035bdb6"},"source":"# make prediction"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd806a9a-d8ce-fabf-ae12-ace6a70c3bde"},"outputs":[],"source":"df = pd.read_json(open(\"../input/test.json\", \"r\"))\nprint(df.shape)\ndf[\"num_photos\"] = df[\"photos\"].apply(len)\ndf[\"num_features\"] = df[\"features\"].apply(len)\ndf[\"num_description_words\"] = df[\"description\"].apply(lambda x: len(x.split(\" \")))\ndf[\"created\"] = pd.to_datetime(df[\"created\"])\ndf[\"created_year\"] = df[\"created\"].dt.year\ndf[\"created_month\"] = df[\"created\"].dt.month\ndf[\"created_day\"] = df[\"created\"].dt.day\nX = df[num_feats]\n\ny = clf.predict_proba(X)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a07942f-4c8b-1a08-859c-6d3f5a934edf"},"outputs":[],"source":"labels2idx = {label: i for i, label in enumerate(clf.classes_)}\nlabels2idx"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2a6a7430-56b3-e875-0741-79568dbd2558"},"outputs":[],"source":"sub = pd.DataFrame()\nsub[\"listing_id\"] = df[\"listing_id\"]\nfor label in [\"high\", \"medium\", \"low\"]:\n    sub[label] = y[:, labels2idx[label]]\nsub.to_csv(\"submission_rf.csv\", index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"313a883b-7dd2-71ec-d3bf-468370965eae"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}