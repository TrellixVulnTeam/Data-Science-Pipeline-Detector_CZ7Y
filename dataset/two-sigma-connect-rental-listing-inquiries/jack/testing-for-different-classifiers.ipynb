{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"5847e130-a092-14a0-9153-5233e9949f55"},"source":"Trying Different Classifiers"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"554d6e91-8810-cb15-742e-08f7915130d4"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cab8133c-5003-284b-5eb2-cf62a14c3f06"},"outputs":[],"source":"import os\nimport sys\nimport operator\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nimport xgboost as xgb\nfrom sklearn import model_selection, preprocessing, ensemble\nfrom sklearn.metrics import log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"480e2606-8271-1415-3748-4465b6fbaea4"},"outputs":[],"source":"data_path = \"../input/\"\ntrain_file = data_path + \"train.json\"\ntest_file = data_path + \"test.json\"\ntrain_df = pd.read_json(train_file)\ntest_df = pd.read_json(test_file)\nprint(train_df.shape)\nprint(test_df.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"abc95421-1cb7-da85-4b07-4a779fe339b0"},"outputs":[],"source":"features_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\"]\n\n# count of photos #\ntrain_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\ntest_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n\n# count of \"features\" #\ntrain_df[\"num_features\"] = train_df[\"features\"].apply(len)\ntest_df[\"num_features\"] = test_df[\"features\"].apply(len)\n\n# count of words present in description column #\ntrain_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\ntest_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n\n# convert the created column to datetime object so as to extract more features \ntrain_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\ntest_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n\n# Let us extract some features like year, month, day, hour from date columns #\ntrain_df[\"created_year\"] = train_df[\"created\"].dt.year\ntest_df[\"created_year\"] = test_df[\"created\"].dt.year\ntrain_df[\"created_month\"] = train_df[\"created\"].dt.month\ntest_df[\"created_month\"] = test_df[\"created\"].dt.month\ntrain_df[\"created_day\"] = train_df[\"created\"].dt.day\ntest_df[\"created_day\"] = test_df[\"created\"].dt.day\ntrain_df[\"created_hour\"] = train_df[\"created\"].dt.hour\ntest_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n\n# adding all these new features to use list #\nfeatures_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\",\"created_year\", \"created_month\", \"created_day\", \"listing_id\", \"created_hour\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"94221816-c9ad-224a-4cdc-39f2319ad39b"},"outputs":[],"source":"categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\nfor f in categorical:\n        if train_df[f].dtype=='object':\n            #print(f)\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n            train_df[f] = lbl.transform(list(train_df[f].values))\n            test_df[f] = lbl.transform(list(test_df[f].values))\n            features_to_use.append(f)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b47a7472-f212-fa61-4206-c9d075272b69"},"outputs":[],"source":"#We have features column which is a list of string values. \n#So we can first combine all the strings together to get a single string and then apply count vectorizer on top of it.\ntrain_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.lower().split(\" \")) for i in x]))\ntest_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.lower().split(\" \")) for i in x]))\nprint(train_df[\"features\"].head())\ntfidf = CountVectorizer(stop_words='english', max_features=200)\ntr_sparse = tfidf.fit_transform(train_df[\"features\"])\nte_sparse = tfidf.transform(test_df[\"features\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7718d4c8-ff5a-ca28-44bf-425ae210836d"},"outputs":[],"source":"train_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()\ntest_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()\n\ntarget_num_map = {'high':0, 'medium':1, 'low':2}\ntrain_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\nprint(train_X.shape, test_X.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"51abf52f-21b3-44a0-548f-a7f024e8f2c4"},"outputs":[],"source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\n#clf.fit(train_X, train_y)\nscores = cross_val_score(clf, train_X.toarray(), train_y, cv=3,scoring='neg_log_loss')\nprint (scores)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"21f49410-2a38-b48c-7d82-cddb3983c738"},"outputs":[],"source":"clf.fit(train_X.toarray(), train_y)\nsubmission = clf.predict_proba(test_X.toarray())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6d3c3775-c004-c37a-1109-160854075618"},"outputs":[],"source":"out_df = pd.DataFrame(submission)\nout_df.columns = [\"high\", \"medium\", \"low\"]\nout_df[\"listing_id\"] = test_df.listing_id.values\nout_df.to_csv(\"xgb_starter2.csv\", index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}