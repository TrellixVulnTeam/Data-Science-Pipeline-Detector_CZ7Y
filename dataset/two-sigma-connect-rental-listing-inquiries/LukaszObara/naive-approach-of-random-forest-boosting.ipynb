{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"81fdba6a-697f-9ef0-2d00-e85d37e1c83e"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d90be9a-4525-2c19-4a9b-da99a2118874"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f927b668-1661-0ac4-0943-0bfd52e139dd"},"outputs":[],"source":"import seaborn as sns\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\nimport sklearn\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\n# import xgboost as xgb"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"708463d2-6cc1-8b75-1b17-ceb1dcb76ffd"},"outputs":[],"source":"data = pd.read_json(open(\"../input/train.json\", \"r\"))\ndata.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e4e6efee-551c-2856-53d6-27b38e45d694"},"outputs":[],"source":"print(data.shape)\ndata.info()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2870f584-5a57-039c-5d55-1f08d9b080f3"},"outputs":[],"source":"print(len(data[\"manager_id\"].unique()))\nprint(len(data[\"building_id\"].unique()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03a7ce12-1497-f687-5613-50640145ef0c"},"outputs":[],"source":"data[\"manager_id\"] = pd.factorize(data[\"manager_id\"])[0]\ndata[\"building_id\"] = pd.factorize(data[\"building_id\"])[0]\n# data[\"interest_level\"] = pd.factorize(data[\"interest_level\"])[0]\ndata[\"num_description_words\"] = data[\"description\"].apply(lambda x: len(x.split(\" \")))\ndata[\"num_features\"] = data[\"features\"].apply(lambda x: len(x))\ndata[\"num_photos\"] = data[\"photos\"].apply(lambda x: len(x))\ndata[\"created\"] = pd.to_datetime(data[\"created\"])\ndata[\"created_year\"] = data[\"created\"].dt.year\ndata[\"created_month\"] = data[\"created\"].dt.month\ndata[\"created_day\"] = data[\"created\"].dt.day\n\nranking = {\"high\": 0, \"medium\": 1, \"low\": 2}\ndata[\"interest_level\"] = np.array(data['interest_level'].apply(lambda x: ranking[x]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ffdbe856-6db8-bf2b-cb50-7065456f205d"},"outputs":[],"source":"data.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a66df496-61f2-807c-2eb5-80f088182995"},"source":"# Naive Prediction"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd4ab0b2-b5f3-39bd-3583-c90630544da1"},"outputs":[],"source":"list(data.columns.values)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3a2e90b9-673b-bbf3-d2bd-b33bc4d49214"},"outputs":[],"source":"features = [\"bathrooms\", \"bedrooms\", \"building_id\", \"latitude\", \"longitude\", \"manager_id\", \"price\", \n            \"num_description_words\", \"created_year\", \"created_month\", \"num_features\", \"num_photos\"]\ntarget = \"interest_level\""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7bf2a74d-b031-f9cf-3173-138729c63562"},"outputs":[],"source":"X = data[features]\ny = data[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=35)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"941aece2-b95f-7bfa-b10b-b05e9c279626"},"outputs":[],"source":"plt.figure(figsize=(15, 10))\n\n# N Estimators\nplt.subplot(2, 3, 1)\nfeature_param = range(100, 150)\nscores=[]\nfor feature in feature_param:\n    clf = RandomForestClassifier(n_estimators=feature)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict_proba(X_test)\n    score = log_loss(y_test, y_pred)\n    scores.append(score)\nplt.plot(scores, \".-\")\nplt.axis(\"tight\")\nplt.xlabel(\"parameter\")\nplt.ylabel(\"score\")\nplt.title(\"N Estimators\")\nplt.grid()\n\n# Criterion\nplt.subplot(2, 3, 2)\nfeature_param = [\"gini\",\"entropy\"]\nscores=[]\nfor feature in feature_param:\n    clf = RandomForestClassifier(criterion=feature)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict_proba(X_test)\n    score = log_loss(y_test, y_pred)\n    scores.append(score)\nplt.plot(scores, \".-\")\nplt.title(\"Criterion\")\nplt.xticks(range(len(feature_param)), feature_param)\nplt.grid()\n\n# Max Features\nplt.subplot(2, 3, 3)\nfeature_param = [\"auto\", \"sqrt\", \"log2\", None]\nscores=[]\nfor feature in feature_param:\n    clf = RandomForestClassifier(max_features=feature)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict_proba(X_test)\n    score = log_loss(y_test, y_pred)\n    scores.append(score)\nplt.plot(scores, \".-\")\nplt.axis(\"tight\")\nplt.title(\"Max Features\")\nplt.xticks(range(len(feature_param)), feature_param)\nplt.grid()\n\n# Max Depth\nplt.subplot(2, 3, 4)\nfeature_param = range(1, 21)\nscores=[]\nfor feature in feature_param:\n    clf = RandomForestClassifier(max_depth=feature)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict_proba(X_test)\n    score = log_loss(y_test, y_pred)\n    scores.append(score)\nplt.plot(feature_param, scores, \".-\")\nplt.axis(\"tight\")\nplt.title(\"Max Depth\")\nplt.grid()\n\n# Min Weight Fraction Leaf\nplt.subplot(2, 3, 5)\nfeature_param = np.linspace(0, 0.5, 10)\nscores=[]\nfor feature in feature_param:\n    clf = RandomForestClassifier(min_weight_fraction_leaf=feature)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict_proba(X_test)\n    score = log_loss(y_test, y_pred)\n    scores.append(score)\nplt.plot(feature_param, scores, \".-\")\nplt.axis(\"tight\")\nplt.title(\"Min Weight Fraction Leaf\")\nplt.grid()\n\n# Max Leaf Nodes\nplt.subplot(2, 3, 6)\nfeature_param = range(2, 21)\nscores=[]\nfor feature in feature_param:\n    clf = RandomForestClassifier(max_leaf_nodes=feature)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict_proba(X_test)\n    score = log_loss(y_test, y_pred)\n    scores.append(score)\nplt.plot(feature_param, scores, \".-\")\nplt.axis(\"tight\")\nplt.title(\"Max Leaf Nodes\")\nplt.grid()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"caef6118-5bab-1819-9708-126ce0fb4591"},"outputs":[],"source":"plt.figure(figsize=(15, 10))\n\n# N Estimators\nplt.subplot(2, 3, 1)\nfeature_param = range(1, 21)\nscores=[]\nfor feature in feature_param:\n    clf = GradientBoostingClassifier(n_estimators=feature)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict_proba(X_test)\n    score = log_loss(y_test, y_pred)\n    scores.append(score)\nplt.plot(scores, \".-\")\nplt.axis(\"tight\")\nplt.title(\"N Estimators\")\nplt.grid()\n\n# Learning Rate\nplt.subplot(2, 3, 2)\nfeature_param = np.linspace(0.1, 1, 10)\nscores=[]\nfor feature in feature_param:\n    clf = GradientBoostingClassifier(learning_rate=feature)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict_proba(X_test)\n    score = log_loss(y_test, y_pred)\n    scores.append(score)\nplt.plot(scores, \".-\")\nplt.title(\"Learning Rate\")\nplt.grid()\n\n# Max Features\nplt.subplot(2, 3, 3)\nfeature_param = [\"auto\", \"sqrt\", \"log2\", None]\nscores=[]\nfor feature in feature_param:\n    clf = GradientBoostingClassifier(max_features=feature)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict_proba(X_test)\n    score = log_loss(y_test, y_pred)\n    scores.append(score)\nplt.plot(scores, \".-\")\nplt.axis(\"tight\")\nplt.title(\"Max Features\")\nplt.grid()\n\n# Max Depth\nplt.subplot(2, 3, 4)\nfeature_param = range(1, 11)\nscores=[]\nfor feature in feature_param:\n    clf = GradientBoostingClassifier(max_depth=feature)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict_proba(X_test)\n    score = log_loss(y_test, y_pred)\n    scores.append(score)\nplt.plot(feature_param, scores, \".-\")\nplt.axis(\"tight\")\nplt.title(\"Max Depth\")\nplt.grid()\n\n# Min Weight Fraction Leaf\nplt.subplot(2, 3, 5)\nfeature_param = np.linspace(0, 0.5, 10)\nscores=[]\nfor feature in feature_param:\n    clf = GradientBoostingClassifier(min_weight_fraction_leaf =feature)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict_proba(X_test)\n    score = log_loss(y_test, y_pred)\n    scores.append(score)\nplt.plot(feature_param, scores, \".-\")\nplt.axis(\"tight\")\nplt.title(\"Min Weight Fraction Leaf\")\nplt.grid()\n\n# Max Leaf Nodes\nplt.subplot(2, 3, 6)\nfeature_param = range(2, 21)\nscores=[]\nfor feature in feature_param:\n    clf = GradientBoostingClassifier(max_leaf_nodes=feature)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict_proba(X_test)\n    score = log_loss(y_test, y_pred)\n    scores.append(score)\nplt.plot(feature_param, scores, \".-\")\nplt.axis(\"tight\")\nplt.title(\"Max Leaf Nodes\")\nplt.grid()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7abca162-6784-1912-0533-6fabcc3d86e0"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}