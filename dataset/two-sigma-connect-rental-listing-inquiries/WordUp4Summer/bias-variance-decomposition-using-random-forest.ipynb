{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Get your hands dirty!\n\nI have been asked about bias & variance at job interveiws for **many, many times,** and I still feel I still haven't completely grasp the concepts. So I decide to just use a dataset as an example to actually capture the dynamics of bias and variances. \n\n","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://images.pexels.com/photos/4255819/pexels-photo-4255819.jpeg?cs=srgb&dl=pexels-jonathan-borba-4255819.jpg&fm=jpg\" width=\"500px\"/>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-29T19:32:29.051504Z","iopub.execute_input":"2021-09-29T19:32:29.052085Z","iopub.status.idle":"2021-09-29T19:32:29.093535Z","shell.execute_reply.started":"2021-09-29T19:32:29.051995Z","shell.execute_reply":"2021-09-29T19:32:29.092447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from subprocess import check_output\n#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nimport zipfile","metadata":{"execution":{"iopub.status.busy":"2021-09-29T19:32:29.097308Z","iopub.execute_input":"2021-09-29T19:32:29.097569Z","iopub.status.idle":"2021-09-29T19:32:30.306428Z","shell.execute_reply.started":"2021-09-29T19:32:29.097541Z","shell.execute_reply":"2021-09-29T19:32:30.30557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-09-29T19:32:30.307342Z","iopub.execute_input":"2021-09-29T19:32:30.307562Z","iopub.status.idle":"2021-09-29T19:32:30.312644Z","shell.execute_reply.started":"2021-09-29T19:32:30.307538Z","shell.execute_reply":"2021-09-29T19:32:30.312026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","metadata":{"execution":{"iopub.status.busy":"2021-09-29T19:32:30.314442Z","iopub.execute_input":"2021-09-29T19:32:30.314874Z","iopub.status.idle":"2021-09-29T19:32:30.323306Z","shell.execute_reply.started":"2021-09-29T19:32:30.314843Z","shell.execute_reply":"2021-09-29T19:32:30.322582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('seaborn-poster')","metadata":{"execution":{"iopub.status.busy":"2021-09-29T19:32:30.324527Z","iopub.execute_input":"2021-09-29T19:32:30.324954Z","iopub.status.idle":"2021-09-29T19:32:30.335551Z","shell.execute_reply.started":"2021-09-29T19:32:30.324924Z","shell.execute_reply":"2021-09-29T19:32:30.334951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Give credits to...\n\n[Thanks to Li Li's awesome feature engineering](https://www.kaggle.com/aikinogard/random-forest-starter-with-numerical-features)\n\n[Thanks to this fantastic explanation. Hands down the best explanation I've read so far](http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/)\n","metadata":{}},{"cell_type":"markdown","source":"## Thereotically speaking..\n\nSuppose we want to estimate some function $\\theta$ by using an estimator $\\hat{\\theta}$\n\n**Bias is the difference between the expected value of the estimator and the true parameter $\\theta$ we want to estimate**\n\ntherefore,\n\n$$bias = E[\\hat{\\theta}] - \\theta $$\n\n**Variance is the difference between the expected value of the squared estimator and squared expectation of the estimator**\n\nin a more convenient form\n\n$$variance = E[E[\\hat{\\theta}] - \\hat{\\theta}]]^2$$\n\n#### Now if we have multiple training sets drawn from the unknown distribution...\n\nWe can define these training sets as **true function plus noise**\n\n\nWe will use an linear regression model to illustrate the idea\n\n","metadata":{}},{"cell_type":"code","source":"reg = pd.read_csv('../input/random-linear-regression/train.csv')\nreg.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-29T19:32:30.33684Z","iopub.execute_input":"2021-09-29T19:32:30.337294Z","iopub.status.idle":"2021-09-29T19:32:30.37866Z","shell.execute_reply.started":"2021-09-29T19:32:30.337264Z","shell.execute_reply":"2021-09-29T19:32:30.377548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## remove outliner\nreg = reg[reg.x <= 75]","metadata":{"execution":{"iopub.status.busy":"2021-09-29T19:32:30.380062Z","iopub.execute_input":"2021-09-29T19:32:30.380376Z","iopub.status.idle":"2021-09-29T19:32:30.394119Z","shell.execute_reply.started":"2021-09-29T19:32:30.380334Z","shell.execute_reply":"2021-09-29T19:32:30.392949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg.fillna(np.mean(reg.y), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T19:32:30.395216Z","iopub.execute_input":"2021-09-29T19:32:30.395434Z","iopub.status.idle":"2021-09-29T19:32:30.404495Z","shell.execute_reply.started":"2021-09-29T19:32:30.395409Z","shell.execute_reply":"2021-09-29T19:32:30.403749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##draw samples from the dataset\n\nsample1 = reg.sample(frac=.01,replace=False, random_state=6)\nsample2 = reg.sample(frac=.01,replace=False, random_state=4)\nsample3 = reg.sample(frac=.01,replace=False, random_state=9)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T19:32:30.405919Z","iopub.execute_input":"2021-09-29T19:32:30.406446Z","iopub.status.idle":"2021-09-29T19:32:30.421604Z","shell.execute_reply.started":"2021-09-29T19:32:30.406398Z","shell.execute_reply":"2021-09-29T19:32:30.420525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because we want to artificially create a high bias situtation, here we just use y = mean(x) as a uniform function to evaluate the true function.","metadata":{}},{"cell_type":"code","source":"lm = LinearRegression()","metadata":{"execution":{"iopub.status.busy":"2021-09-29T19:40:29.750178Z","iopub.execute_input":"2021-09-29T19:40:29.750465Z","iopub.status.idle":"2021-09-29T19:40:29.755283Z","shell.execute_reply.started":"2021-09-29T19:40:29.750434Z","shell.execute_reply":"2021-09-29T19:40:29.754503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nnum=1\ncolors = ['red', 'green', 'yellow']\ncolors_dot= ['darkred', 'darkgreen', 'orange']\nfor sample in [sample1, sample2, sample3]:\n    mean_y = np.mean(sample['y'])\n    plt.scatter(sample.x, sample.y,color= colors_dot[num-1])\n    plt.hlines(xmin= 0, xmax = 75, y=mean_y, color= colors[num-1], label= 'sample' + str(num))\n    num +=1\n## fit true distribution\nlm.fit(X = pd.DataFrame(reg['x']), y=reg['y'])\nplt.plot(reg.x, lm.predict(pd.DataFrame(reg['x'])), color='black', label='True function f(x)')\nplt.legend()\nplt.title('high bias');","metadata":{"execution":{"iopub.status.busy":"2021-09-29T19:45:39.334789Z","iopub.execute_input":"2021-09-29T19:45:39.335499Z","iopub.status.idle":"2021-09-29T19:45:39.665282Z","shell.execute_reply.started":"2021-09-29T19:45:39.335436Z","shell.execute_reply":"2021-09-29T19:45:39.66417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we can say that the **bias is large** because the difference between the true value and the predicted value,\non average (here, average means **expectation of the training sets** not expectation over examples in the training set), is large:\n\nNow to create high variance scenario, we can create overfitting models which are just lines between the dots.","metadata":{"execution":{"iopub.status.busy":"2021-09-29T17:53:35.838346Z","iopub.execute_input":"2021-09-29T17:53:35.838698Z","iopub.status.idle":"2021-09-29T17:53:35.846146Z","shell.execute_reply.started":"2021-09-29T17:53:35.838657Z","shell.execute_reply":"2021-09-29T17:53:35.844984Z"}}},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nnum=1\ncolors = ['red', 'green', 'yellow']\ncolors_dot= ['darkred', 'darkgreen', 'orange']\nfor sample in [sample1, sample2, sample3]:\n    plt.plot(sample['x'], sample['y'], 'o-', label='sample' + str(num))\n    num +=1\n## fit true distribution\nlm.fit(X = pd.DataFrame(reg['x']), y=reg['y'])\nplt.plot(reg.x, lm.predict(pd.DataFrame(reg['x'])), color='black', label='True function f(x)')\nplt.legend()\nplt.title('high variance');","metadata":{"execution":{"iopub.status.busy":"2021-09-29T19:45:42.242711Z","iopub.execute_input":"2021-09-29T19:45:42.243088Z","iopub.status.idle":"2021-09-29T19:45:42.500292Z","shell.execute_reply.started":"2021-09-29T19:45:42.243043Z","shell.execute_reply":"2021-09-29T19:45:42.49961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the variance is very large, since **on average, a prediction differs a lot from the expectation value of the prediction**:\n\nNow we can use mlextend package to quantify the bias and variance using a real-world dataset","metadata":{"execution":{"iopub.status.busy":"2021-09-29T17:42:31.331938Z","iopub.execute_input":"2021-09-29T17:42:31.332273Z","iopub.status.idle":"2021-09-29T17:42:31.33997Z","shell.execute_reply.started":"2021-09-29T17:42:31.332225Z","shell.execute_reply":"2021-09-29T17:42:31.338918Z"}}},{"cell_type":"code","source":"with zipfile.ZipFile(\"../input/two-sigma-connect-rental-listing-inquiries/\"+'train.json'+\".zip\",\"r\") as z:\n    z.extractall(\"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2021-09-29T19:45:46.266263Z","iopub.execute_input":"2021-09-29T19:45:46.267384Z","iopub.status.idle":"2021-09-29T19:45:47.758969Z","shell.execute_reply.started":"2021-09-29T19:45:46.267323Z","shell.execute_reply":"2021-09-29T19:45:47.758003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_json(open(\"/kaggle/working/train.json\", \"r\"))\n## sample data\ndf = df.sample(frac=.01, replace=False, random_state=3)\n## naive feature engineering\n## referencede from Li Li's notebook\ndf[\"num_photos\"] = df[\"photos\"].apply(len)\ndf[\"num_features\"] = df[\"features\"].apply(len)\ndf[\"num_description_words\"] = df[\"description\"].apply(lambda x: len(x.split(\" \")))\ndf[\"created\"] = pd.to_datetime(df[\"created\"])\ndf[\"created_year\"] = df[\"created\"].dt.year\ndf[\"created_month\"] = df[\"created\"].dt.month\ndf[\"created_day\"] = df[\"created\"].dt.day\nnum_feats = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",\n             \"num_photos\", \"num_features\", \"num_description_words\",\n             \"created_year\", \"created_month\", \"created_day\"]\nX = df[num_feats]\ny = df[\"interest_level\"]\nX.head()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-29T19:45:49.003115Z","iopub.execute_input":"2021-09-29T19:45:49.00342Z","iopub.status.idle":"2021-09-29T19:45:50.92529Z","shell.execute_reply.started":"2021-09-29T19:45:49.00339Z","shell.execute_reply":"2021-09-29T19:45:50.92428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.interest_level.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-29T19:45:50.927464Z","iopub.execute_input":"2021-09-29T19:45:50.927833Z","iopub.status.idle":"2021-09-29T19:45:50.939329Z","shell.execute_reply.started":"2021-09-29T19:45:50.927789Z","shell.execute_reply":"2021-09-29T19:45:50.938316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_dict = {'low': 1, 'medium':2, 'high':3}\ny = df['interest_level'].apply(lambda x: y_dict[x])","metadata":{"execution":{"iopub.status.busy":"2021-09-29T19:45:51.82346Z","iopub.execute_input":"2021-09-29T19:45:51.824121Z","iopub.status.idle":"2021-09-29T19:45:51.830951Z","shell.execute_reply.started":"2021-09-29T19:45:51.824064Z","shell.execute_reply":"2021-09-29T19:45:51.83003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## train test split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-29T19:45:53.396319Z","iopub.execute_input":"2021-09-29T19:45:53.396617Z","iopub.status.idle":"2021-09-29T19:45:53.403437Z","shell.execute_reply.started":"2021-09-29T19:45:53.396588Z","shell.execute_reply":"2021-09-29T19:45:53.40169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now let's build a random forrest \n\nWe can change the number of estimators (change the model complexity) and then estimate the bias and variances at each point","metadata":{"execution":{"iopub.status.busy":"2021-09-29T15:42:11.471986Z","iopub.execute_input":"2021-09-29T15:42:11.47234Z","iopub.status.idle":"2021-09-29T15:42:11.519825Z","shell.execute_reply.started":"2021-09-29T15:42:11.47231Z","shell.execute_reply":"2021-09-29T15:42:11.518948Z"}}},{"cell_type":"code","source":"from mlxtend.evaluate import bias_variance_decomp","metadata":{"execution":{"iopub.status.busy":"2021-09-29T19:45:54.938234Z","iopub.execute_input":"2021-09-29T19:45:54.938522Z","iopub.status.idle":"2021-09-29T19:45:55.005969Z","shell.execute_reply.started":"2021-09-29T19:45:54.938495Z","shell.execute_reply":"2021-09-29T19:45:55.005009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_arr = X_train.values\ny_train_arr = y_train.values\nX_val_arr = X_val.values\ny_val_arr = y_val.values","metadata":{"execution":{"iopub.status.busy":"2021-09-29T19:45:56.471346Z","iopub.execute_input":"2021-09-29T19:45:56.471707Z","iopub.status.idle":"2021-09-29T19:45:56.478698Z","shell.execute_reply.started":"2021-09-29T19:45:56.471664Z","shell.execute_reply":"2021-09-29T19:45:56.477687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimators_nums = [10, 30, 50, 100, 300, 500]\nrf = RandomForestClassifier(n_estimators=30)\n#rf.fit(X_train, y_train)\n\navg_expected_losses = []\navg_biases = []\navg_vars = []\n\nfor n in estimators_nums:\n    print('estimators: ', n)\n    rf = RandomForestClassifier(n_estimators=n)\n    avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n        rf, X_train_arr, y_train_arr, X_val_arr, y_val_arr, loss='0-1_loss',\n        random_seed=123)\n    \n    avg_expected_losses.append(avg_expected_loss)\n    avg_biases.append(avg_bias)\n    avg_vars.append(avg_var)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-29T20:29:00.114782Z","iopub.execute_input":"2021-09-29T20:29:00.115067Z","iopub.status.idle":"2021-09-29T20:36:13.858008Z","shell.execute_reply.started":"2021-09-29T20:29:00.115039Z","shell.execute_reply":"2021-09-29T20:36:13.857161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.plot(estimators_nums, avg_expected_losses, label='avg expected loss')\nplt.plot(estimators_nums, avg_biases, label='avg bias')\nplt.plot(estimators_nums, avg_vars, label='avg var')\nplt.xlabel('n estimator')\nplt.legend();","metadata":{"execution":{"iopub.status.busy":"2021-09-29T20:57:57.287127Z","iopub.execute_input":"2021-09-29T20:57:57.287752Z","iopub.status.idle":"2021-09-29T20:57:57.542745Z","shell.execute_reply.started":"2021-09-29T20:57:57.287715Z","shell.execute_reply":"2021-09-29T20:57:57.541868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lm.fit(np.array(estimators_nums).reshape(-1,1), np.array(avg_vars))\nlm.coef_","metadata":{"execution":{"iopub.status.busy":"2021-09-29T21:00:45.742951Z","iopub.execute_input":"2021-09-29T21:00:45.743769Z","iopub.status.idle":"2021-09-29T21:00:45.752743Z","shell.execute_reply.started":"2021-09-29T21:00:45.743727Z","shell.execute_reply":"2021-09-29T21:00:45.751886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nestimators_nums_inv = [1/i for i in estimators_nums]\nplt.plot(estimators_nums_inv, avg_vars)\nplt.xlabel('1 / number_of_estimator')\nplt.ylabel('avg var')\nplt.title('variances and inverse number of estimators');","metadata":{"execution":{"iopub.status.busy":"2021-09-29T21:05:59.109029Z","iopub.execute_input":"2021-09-29T21:05:59.109505Z","iopub.status.idle":"2021-09-29T21:05:59.30474Z","shell.execute_reply.started":"2021-09-29T21:05:59.109471Z","shell.execute_reply":"2021-09-29T21:05:59.303762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As shown above, since we are using random forrest, as the number of trees increases, average variances decreases. The average variance and the number of trees are negatively correlated. A further investigation shows that average variances is almost proportional to the **inverse of the number of estimators**.\n\nThis is because random forrest has used bagging **AKA. bootstrap samples**. As we increase the number of bootstrapped samples, the mean remains the same, but the variances will be smaller. Suppose we have p bootstrap samples, then\n\n$$\\frac{var(x_1) + var(x_2) + .. var(x_p)}{p^2} = \\frac{p \\sigma^2}{p^2} = \\frac{\\sigma^2}{p}$$","metadata":{}},{"cell_type":"markdown","source":"For more information, please refer to:\n\n[Ensemble Learning on Bias and Variance](https://www.section.io/engineering-education/ensemble-bias-var/)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T18:20:54.711506Z","iopub.execute_input":"2021-09-29T18:20:54.711834Z","iopub.status.idle":"2021-09-29T18:20:54.783042Z","shell.execute_reply.started":"2021-09-29T18:20:54.7118Z","shell.execute_reply":"2021-09-29T18:20:54.781846Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}