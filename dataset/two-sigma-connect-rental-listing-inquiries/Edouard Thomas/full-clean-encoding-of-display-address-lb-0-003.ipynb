{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"f8ddae87-ddae-4230-cd2f-527ce0fcf3e1"},"source":"**OBJECTIVE**\nThe objective is to encode the address feature better than one-hot, ie. with less columns but without information loss.\nThe objective is to get a sparse matrix whose each column corresponds to a street name, an avenue name, or more generally any element of interest.\nFor this purpose we create a dictionnary with all important places (Broadway, 5th street,...), and important keywords (ex : rd, st, blvd...). To build this dictionnary we begin by cleaning the address field and we select the expressions that appear the most.\nIf an address contains an element of the dictionnary it gets a one in the corresponding column.\nTherefore, this encoding of the address field is not stricty one-hot, as there can be more than 1 one per row. It is cleaner than one-hot encoding (less columns) and gives the results a nice boost :)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6895ec0-05bb-31f1-7e68-5c59390dedd9"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nfrom scipy.sparse import csr_matrix"},{"cell_type":"markdown","metadata":{"_cell_guid":"72c73fd0-d424-2e9d-fe27-28bf352c30bd"},"source":"Now time for cleaning the addresses :"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f9bc3053-7e90-afba-ae1d-0a5976e22e80"},"outputs":[],"source":"train = pd.read_json(\"../input/train.json\")\ntest = pd.read_json(\"../input/test.json\")\ndata=pd.concat([train.drop(\"interest_level\",1),test],0)\n\n#To select the rows corresponding to the train and the test set\ntrainSel=np.array([True]*len(train)+[False]*len(test))\ntestSel=np.array([False]*len(train)+[True]*len(test))\n\ndata=data.reset_index(drop=True)\n\n#We need the field of interest\ndisadr=data.display_address    \n\n#What does this feature look like ? \ndisadr.head(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d3b67af8-0b24-4889-d37c-6259754048b5"},"outputs":[],"source":"def cleanAdr(adress):\n    adress=adress.lower()\n    adress=adress.strip()\n    adress=re.sub('\\.','',adress)\n    adress=re.sub(',','',adress)\n    adress=re.sub('\\'','',adress)\n    adress=re.sub('(?P<num>[0-9]+)(th|st|rd|nd)','\\g<1>',adress) \n    adress=re.sub('(?P<num>[0-9]+) (th|nd)','\\g<1>',adress) \n    adress=re.sub('3 rd (?P<num>(st|rd|ave))','3 \\g<1>',adress) \n    adress=re.sub('^[0-9]+-[0-9]+ ','',adress)\n    adress=re.sub('street','st',adress) \n    adress=re.sub(' av$',' ave',adress)\n    adress=re.sub('avenue','ave',adress)\n    adress=re.sub('place','pl',adress)\n    adress=re.sub('boulevard','blvd',adress)\n    adress=re.sub(' lane',' ln',adress)\n    adress=re.sub(' road',' rd',adress)\n    adress=re.sub(' parkway',' pkwy',adress)\n    adress=re.sub(' square',' sq',adress)\n    adress=re.sub(' drive',' dr',adress)\n    adress=re.sub(' park',' pk',adress)\n    adress=re.sub('east','e',adress) \n    adress=re.sub('west','w',adress) \n    adress=re.sub('north','n',adress)\n    adress=re.sub('south ','s',adress)\n    adress=re.sub(' +',' ',adress)\n    #special case of the abcx avenues\n    adress=re.sub('(?P<num>(a|b|c|x)) ave','ave \\g<1>',adress) \n    #Pb with streets that contain pk or st in their name (ex : pk avenue)\n    adress=re.sub('.*pk ave','pk ave',adress)\n    adress=re.sub('.*pk pl','pk pl',adress)\n    adress=re.sub('.*st marks','st marks',adress)\n    adress=re.sub('.*(?P<num>ave (x|a|b))','\\g<1>',adress)\n    #famous places without their specification (avenue,street,park...)\n    #We choose the most common\n    adress=re.sub('ave of the [a-z]+','6 ave',adress)\n    adress=re.sub('^([0-9]+ )?madison$','madison ave',adress)\n    adress=re.sub('^([0-9]+ )?thompson$','thompson st',adress)\n    adress=re.sub('^([0-9]+ )?columbus$','columbus ave',adress)\n    adress=re.sub('^([0-9]+ )?sullivan$','sullivan st',adress)\n    adress=re.sub('^([0-9]+ )?john$','john st',adress)\n    adress=re.sub('^([0-9]+ )?putnam$','putnam ave',adress)\n    adress=re.sub('^([0-9]+ )?union$','union sq',adress)\n    adress=re.sub('^([0-9]+ )?st marks$','st marks pl',adress)\n    adress=re.sub('^([0-9]+ )?saint nicholas$','saint nicholas ave',adress)\n    #streets without the \"st\"\n    adress=re.sub('(?P<num>(e|w) [0-9]+)$','\\g<1> st',adress)\n    adress=re.sub('^(?P<num>[0-9][0-9])$','\\g<1> st',adress)\n    #The \"and\" without specifications\n    adress=re.sub('(?P<num>[0-9]+) (and|&) (?P<n>([a-z]+|[0-9]+))','\\g<1> st and \\g<2> ave',adress)    \n    adress=adress.strip()\n    return adress"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0d894ee-3a43-5744-4374-c3db6bd17b06"},"outputs":[],"source":"#We apply this cleaning to the address column\ndisadr=disadr.apply(cleanAdr)\n\n#The addresses of the training set, for the dictionnaries\ndisadrTr=disadr[trainSel].reset_index(drop=True)\n\n#What does the cleaned feature look like ?\ndisadr.head(10)"},{"cell_type":"markdown","metadata":{"_cell_guid":"07d4b5ff-5815-166b-bc47-a25d077b59e3"},"source":"Now that we have the cleaned address we can build the dictionnary :"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff582277-3b1d-26a4-131d-3a46e0091d1d"},"outputs":[],"source":"def createDico(regex,serieofad,withdrawuniques):\n    sel=serieofad.apply(lambda x:True if re.search(regex,x) else False)\n    d=serieofad[sel]\n    d=d.apply(lambda x:re.search(regex,x).group(0))\n    dico=d.value_counts()\n    if withdrawuniques:\n        dico=dico[dico>2]\n    dico=list(dico.index)\n    return dico\n\n#dictionnary of the streets\nregex=re.compile('([0-9]+|[a-z]+) st')\ndicost=createDico(regex,disadr,True)\ndicostTr=createDico(regex,disadrTr,False)\n\n#dictionnary of the avenues\nregex=re.compile('(([0-9]+|[a-z]+) ave|ave (a|b|c|x))')\ndicoave=createDico(regex,disadr,True)\ndicoaveTr=createDico(regex,disadrTr,False)\n\n#dictionnary of the other stuffs\nregex=re.compile('([0-9]+|[a-z]+)( pkwy| rd| ln| pl| blvd| sq| dr| pk| terrace| heights)')\ndicorest=createDico(regex,disadr,True)\ndicorestTr=createDico(regex,disadrTr,False)\n\n\n\n#is the word only in the testing set?\nstsel1=list(map(lambda x:1 if x in dicostTr else 0,dicost))\navesel1=list(map(lambda x:1 if x in dicoaveTr else 0,dicoave))\nrestsel1=list(map(lambda x:1 if x in dicorestTr else 0,dicorest))\n\n\n#We select the elements outside the intersections\ndicost2=list(map(lambda x:re.sub(' st','',x),dicost))\ndicoave2=list(map(lambda x:re.sub(' ave','',x),dicoave))\ndicorest2=list(map(lambda x:re.sub('( pkwy| rd| ln| pl| blvd| sq| dr| pk| terrace| heights)','',x),dicorest))\nstsel2=list(map(lambda x:0 if x in dicoave2+dicorest2 else 1,dicost2))\navesel2=list(map(lambda x:0 if x in dicost2+dicorest2 else 1,dicoave2))\nrestsel2=list(map(lambda x:0 if x in dicoave2+dicost2 else 1,dicorest2))\n\n\n\nfor i in range(len(dicost)):\n    if stsel2[i]==1:\n        dicost[i]=re.sub(' st','',dicost[i])\nfor i in range(len(dicoave)):\n    if avesel2[i]==1:\n        dicoave[i]=re.sub(' ave','',dicoave[i])\nfor i in range(len(dicorest)):\n    if restsel2[i]==1:\n        dicorest[i]=re.sub('( pkwy| rd| ln| pl| blvd| sq| dr| pk| terrace| heights)','',dicorest[i])\n\n\n\n#Now we keep in the dictionnaries only the words present in the training set\ndicost=[obj for ind,obj in enumerate(dicost) if stsel1[ind]==1]\ndicoave=[obj for ind,obj in enumerate(dicoave) if avesel1[ind]==1]\ndicorest=[obj for ind,obj in enumerate(dicorest) if restsel1[ind]==1]"},{"cell_type":"markdown","metadata":{"_cell_guid":"636909a6-5169-5c9c-ce2a-0d9203de7974"},"source":"We can add special elements to the dictionnary :"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4827ccee-8553-31b6-7456-ef3bd2fa4718"},"outputs":[],"source":"#Additional dictionnaries - you can add as many words as you want\ndicosp=[\"central pk\",\"flatiron\",\"broadway\"]\ndicodis=[\"financial district\",\"hells kitchen\",\"midtown\",\"upper\",\"soho\",\"murray hill\",\"village\",\n         \"chelsea\",\"tribeca\",\"harlem\",\"lower\",\"astoria\",\"kips bay\",\"turtle bay\",\"williamsburg\"]\ndiconewfeatures=[\"st\",\"ave\",\"blvd\",\"pkwy\",\"rd\",\"terrace\",\"e\",\"w\"] \n\n#The final dictionnary !! -> dico\ndico=dicost+dicoave+dicorest+dicosp+dicodis+diconewfeatures\ndico=list(set(dico))\n\n#Control of the dictionnary (we want to withdraw abnormal words)\nl=pd.Series(list(map(len,dico)))\nind=l[l==1].index  #OK\ndel dico[410]\n\n#An extract of the dictionnary\ndico[165:175]"},{"cell_type":"markdown","metadata":{"_cell_guid":"edb04f09-e575-8f66-41de-26930eff7073"},"source":"For each address we want a list of all the dictionnary words it contains :"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1970c4a0-452d-c9b8-2599-90015e7eaced"},"outputs":[],"source":"#We check the presence of each expression of the dictionnary in each address\ndef searchDico(x,dico):\n    out=[]\n    for i in range(len(dico)):\n        if \" \"+dico[i]+\" \" in x:   #spaces are here to avoid '3 st' being found in '33 st'\n            out.append(i)\n    return out\n\n\ndisadr=disadr.apply(lambda x:' '+x+' ') \nstreets=disadr.apply(lambda x:searchDico(x,dico)) "},{"cell_type":"markdown","metadata":{"_cell_guid":"942aece1-43a6-1849-880c-fd65cc6a5320"},"source":"We can finally build the sparse matrix"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a4f834b-a52a-83d1-620f-a5d8f8f952e2"},"outputs":[],"source":"#Direct building of the sparse matrix - much more efficient than building a DF and convert to sparse :)\nnbof1=sum(streets.apply(len))\nl=list(streets)\ncolumns=[e for liste in l for e in liste]\nind=list(streets.apply(len))\nind=list(np.cumsum(ind))\nind=[0]+ind\ndisplay_address_sparse=csr_matrix(([1]*nbof1,columns,ind))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b1deb11-49b9-ffa5-46b4-5713d577aa19"},"outputs":[],"source":"#Final check : small cleaning of the dictionnary\n\ndas=display_address_sparse.toarray()\ndas=pd.DataFrame(das)\ns=das.apply(sum)\nbadwords=s[s==0].index\n\ndico=list(np.delete(np.array(dico),badwords))  \ndisplay_address_sparse=display_address_sparse[:,np.nonzero(display_address_sparse.sum(axis=0))[1]]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"757bb595-2ed2-c9c1-26f8-74a26514fffc"},"outputs":[],"source":"display_address_sparse"},{"cell_type":"markdown","metadata":{"_cell_guid":"62946a80-c49a-00a7-f36e-c9236a70f838"},"source":"Appendix : a good feature, the presence of the street number"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8d4c31e-ebff-fdee-904f-666207ef5624"},"outputs":[],"source":"r=r\"^([0-9]+) \"\ndisadr2=disadr.apply(lambda x:re.sub('[0-9]+ (st|ave|rd|blvd)','',x))\nsel=disadr2.apply(lambda x:True if re.search(r,x.strip()) else False)\nstreet_number=pd.Series(np.zeros(len(data)))\nstreet_number[sel]=1"},{"cell_type":"markdown","metadata":{"_cell_guid":"7100d72e-663e-68ba-6e0d-051101396fe2"},"source":"A big part of the dictionary is built automatically, but you can add or remove expressions and see the effects :)\nPersonally, as there are still many columns, I used this matrix to reinforce my stacking only at the second level and it gave me a cool boost !\nIt was a great competition and I hope you enjoyed this kernel, :)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}