{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"da6d5932-5d5d-d980-2368-76417237c2cf"},"source":"Update 2017-03-11: Kaggle added sklearn_pandas to kernels, yeah !\n\n# Why this kernel\n\nThe purpose of this kernel is to provide you with a hackable baseline for this competition.\n\nThe unofficial secondary purpose (shhh!) is to get a shiny Kaggle medal. If you find it useful vote!\n\n\n## Introduction\nSo far all kernels I see are not doing feature engineering in a maintainable and scalable way:\n* Feature engineering has to be done twice, once for training, one for testing.\n* It's contamination prone for cross-validation and GridSearch. (E.G. You compute the mean of the whole dataset and use it as a feature even though you cross-validate on 80% only for example).\n* Feature testing and scaling is all over the code.\n* You have to leave Pandas at one point and use NumPy array, meaning you lose context and label of data.\n* You can't find useful features in an easy automated way, especially after OneHotEncoding or LabelBinarizer.\n\n## Learning outcomes\nYou will learn:\n* How to scale feature engineering with a Pipeline\n* How to debug easily any step in your feature engineering Pipeline\n* How to structure your code to enable/disable features in a single place (aka Command Center) and preprocess them properly (StandardScaler, LabelBinarizer, OneHotEncoder ...)\n* How to extract the most useful features from a feature set, even after OneHotEncoding or Binarization\n\n**The end goal is to have a very clean code that allows to test features very rapidly**\n\nWhat you will not learn:\n* Data exploration and visualization\n* Stacking in 2 liners, use mlxtend for that: https://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/\n* Imputing missing values with advanced techniques (beyond mean/median/mode): check my Titanic kernels in [Python](https://www.kaggle.com/mratsim/titanic/titanic-end-to-end-pipeline-stacking-gridsearch) and [Julia](https://www.kaggle.com/mratsim/titanic/titanic-julia-end-to-end-pipelining) for examples.\n\n\n## Notes\nThis is a port of [Li Li's kernel](https://www.kaggle.com/aikinogard/two-sigma-connect-rental-listing-inquiries/random-forest-starter-with-numerical-features) to Scikit's Pipeline. Thank you Li Li for some clean and to the point code.\n\nUnfortunately this kernel does not run completely at Kaggle kernel due to the lack of sklearn-pandas library that allows to use Pandas' dataframes with ScikitLearn\n\nThe Baseline score is : 0.63"},{"cell_type":"markdown","metadata":{"_cell_guid":"8f958a0a-949a-3406-9a59-71ff8615490b"},"source":"# Import libraries\n* Numerical libraries\n* ScikitLearn Tools\n* Classifier: XGBoost, using the Scikit Learn API\n* time: to name the output files\n\nsklearn-pandas is imported at a later time as it won't run in Kaggle anyway"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f90512b-8f79-7c0f-125c-a0fbd99d70a9"},"outputs":[],"source":"import numpy as np\nimport pandas as pd"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0d15cdd6-bbee-0231-309c-82f03b1e060e"},"outputs":[],"source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelBinarizer, RobustScaler, Binarizer, StandardScaler, OneHotEncoder"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"727faec5-0dbc-20f8-118b-6e2a8ff28ce9"},"outputs":[],"source":"from xgboost import XGBClassifier"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"67278049-360e-64d1-755e-264c49c5d4d4"},"outputs":[],"source":"import time"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"41d20820-fff9-c752-7ecf-9d970cb00a6d"},"outputs":[],"source":"# Update 2017-03-11 - Kaggle added sklearn_pandas support\nfrom sklearn_pandas import DataFrameMapper"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0306a776-ab91-af49-b496-b9caf80b86b4"},"outputs":[],"source":"# Update 2017-03-11: Kaggle added sklearn_pandas to kernels, yeah !\nfrom sklearn_pandas import DataFrameMapper"},{"cell_type":"markdown","metadata":{"_cell_guid":"ab8a3702-8a2e-43c9-ba0c-a12cf4dd2ca4"},"source":"# Import and display data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"130ff55c-ccee-c91e-eb80-0070804cbe32"},"outputs":[],"source":"df_train = pd.read_json(open(\"../input/train.json\", \"r\"))\ndf_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"edd512b4-8162-e065-fcca-3c7caff8b9e7"},"outputs":[],"source":"df_test = pd.read_json(open(\"../input/test.json\", \"r\"))\ndf_test.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"6fde7eb7-39f5-3509-1137-f0db8c71f204"},"source":"# Transformers\n\nThis is the main lesson of this code. Transformers allow your code to be easily maintainable by wrapping eachpreprocessing step in an easily testable class :\n* You will always be sure that transformations are applied to train and test data\n* You won't have to track matrix sizes, column names\n* You can store temporary data in the training phase to reuse in the testing phase in a clean way, for example the number of occurences of a manager id.\n\nTransformers have 4 methods:\n\n**\\_\\_init\\_\\_**\n\nUsually at \"pass\", it's useful:\n* for temporary variables, like a dictionary to do some mapping\n* if you need to store data from the training phase to reuse during the predict phase. Check my [Titanic Kernel](https://www.kaggle.com/mratsim/titanic/titanic-end-to-end-pipeline-stacking-gridsearch)\n\n**fit**\n\nIf the transformer don't learn from the input data (same transformation at training and testing phase) it should return self\nIf the transformer \"learns\" from the data, i.e. tehre is one or more internal properties in the __init__ methods, update the properties and then return self. Those properties can then be used in the transform method\n\n**transform**\n\nHow the transformer transform the data\n\n**predict**\n\nThis is used if you build [your custom classifier](http://scikit-learn.org/dev/developers/contributing.html#rolling-your-own-estimator)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d118f42-c32b-fa2c-ab4c-0f9017f56ef5"},"outputs":[],"source":"# This transformer extracts the number of photos\nclass PP_NumPhotTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n    def transform(self, X, **transform_params):\n        df = pd.DataFrame(X) #Python thinks X is a list input by default instead of a Dataframe\n        return df.assign(\n            NumPhotos = df['photos'].str.len()\n            )\n    \n# This transformer extracts the number of features\nclass PP_NumFeatTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n    def transform(self, X, **transform_params):\n        df = pd.DataFrame(X) #Python thinks X is a list input by default instead of a Dataframe\n        return df.assign(\n            NumFeat = df['features'].str.len()\n            )\n    \n# This transformer extracts the number of words in the description\nclass PP_NumDescWordsTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n    def transform(self, X, **transform_params):\n        df = pd.DataFrame(X) #Python thinks X is a list input by default instead of a Dataframe\n        return df.assign(\n            NumDescWords = df[\"description\"].apply(lambda x: len(x.split(\" \")))\n            )\n    \n# This transformer extracts the date/month/year and timestamp in a neat package\nclass PP_DateTimeTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n    def transform(self, X, **transform_params):\n        df = pd.DataFrame(X) #Python thinks X is a list input by default instead of a Dataframe\n        df = df.assign(\n            Created_TS = pd.to_datetime(df[\"created\"])\n        )\n        return df.assign(\n            Created_Year = df[\"Created_TS\"].dt.year,\n            Created_Month = df[\"Created_TS\"].dt.month,\n            Created_Day = df[\"Created_TS\"].dt.day\n            )\n\n####### Debug Transformer ###########\n# Use this transformer anywhere in your Pipeline to dump your dataframe to CSV\nclass DebugTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n    def transform(self, X, **transform_params):\n        X.to_csv('./debug.csv')\n        return X"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d3f0dfc-2fac-d2a4-86ec-a2ca563de6b1"},"outputs":[],"source":"# Impact sklearn_pandas which Pandas DataFrame compatibility with Scikit's classifiers and Pipeline\nfrom sklearn_pandas import DataFrameMapper"},{"cell_type":"markdown","metadata":{"_cell_guid":"f5bc3ee3-9bb5-13d1-5681-bf60f12d56cd"},"source":"# Command Center - enabled features\n\nThis is where you:\n* enable/disable features\n* specify scaling, onehotencoding, labelbinarizing\n\n\nNote: LabelBinarizer use the following syntax (\"feature\", LabelBinarizer())\ninstead of ([\"feature\"], LabelBinarizer())\n\nNote2: Copy Pasting your DataFrameMapper config when you submit your results makes for a superb description of your model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fb4f4e2b-3351-8ea6-8789-180378bb276d"},"outputs":[],"source":"mapper = DataFrameMapper([\n    ([\"bathrooms\"],RobustScaler()), #Some bathrooms number are 1.5, Some outliers are 112 or 20    ([\"bedrooms\"],OneHotEncoder()),\n    ([\"latitude\"],None),\n    ([\"longitude\"],None),\n    ([\"price\"],RobustScaler()),\n    # ([\"NumDescWords\"],None),\n    ([\"NumFeat\"],StandardScaler()),\n    ([\"Created_Year\"],None),\n    ([\"Created_Month\"],None),\n    ([\"Created_Day\"],None)\n])"},{"cell_type":"markdown","metadata":{"_cell_guid":"6862fc16-d220-823a-5260-b388cb815c05"},"source":"# Command Center - feature engineering pipeline + classifier\n\nThis is your transformation pipeline in the format:\n(\"arbitrary_name\", Transformer())\n\nYou can comment/uncomment to remove/add steps.\nThe last step should be your classifier.\n\nNote: if you need to configure a specific step \"step\" use the format step\\_\\_parameter = value\n\nFor example I wanted to set the eval_metric of xgboost (name xgb) during the fit step so I used:\n\npipe.fit(X_train, y_train, **xgb\\__eval_metric**='mlogloss')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6956d809-c12f-f19b-9820-6b7136be1116"},"outputs":[],"source":"pipe = Pipeline([\n    (\"extract_numphot\", PP_NumPhotTransformer()),\n    (\"extract_numfeat\", PP_NumFeatTransformer()),\n    (\"extract_numdesc\", PP_NumDescWordsTransformer()),\n    (\"extract_datetime\", PP_DateTimeTransformer()),\n    # (\"DEBUG\", DebugTransformer()), #Uncomment to debug\n    (\"featurize\", mapper),\n    (\"xgb\",XGBClassifier(\n        n_estimators=1000,\n        seed=42,\n        objective='multi:softprob',\n        subsample=0.8,\n        colsample_bytree=0.8,\n    ))\n])"},{"cell_type":"markdown","metadata":{"_cell_guid":"93449813-8077-c377-9b82-405eef7eaaa6"},"source":"# Helper functions\n\n## Cross Validation\n5 folds, results summarized with 3 decimals of precision\n\n## Get features that contributes most to the score\nThis function gives sensible names in case of OneHotEncoding or LabelBinarizer.\nThis is only possible with classifiers that implements **feature\\_importances_** like RandomForest, ExtraTrees or XGBoost.\n\nIt outputs a top_featurs.csv file:\n\n![enter image description here][1]\n\n## Predict and format the output\n\n\n  [1]: https://i.imgur.com/a7ElIO8.png"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee7c40b2-733b-3c96-8482-dccbfe3553d1"},"outputs":[],"source":"##### Cross Validation #######\ndef crossval():\n    cv = cross_val_score(pipe, X_train, y_train, cv=5)\n    print(\"Cross Validation Scores are: \", cv.round(3))\n    print(\"Mean CrossVal score is: \", round(cv.mean(),3))\n    print(\"Std Dev CrossVal score is: \", round(cv.std(),3))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1162a48e-6b95-e745-08c1-673bc82e7ee5"},"outputs":[],"source":"####### Get top features and noise #######\ndef top_feat():\n    dummy, model = pipe.steps[-1]\n\n    feature_list = []\n    for feature in pipe.named_steps['featurize'].features:\n        if isinstance(feature[1], OneHotEncoder):\n            for feature_value in feature[1].active_features_:\n                feature_list.append(feature[0][0]+'_'+str(feature_value))\n        else:\n            try:\n                for feature_value in feature[1].classes_:\n                    feature_list.append(feature[0]+'_'+feature_value)\n            except:\n                feature_list.append(feature[0])\n\n\n    top_features = pd.DataFrame({'feature':feature_list,'importance':np.round(model.feature_importances_,3)})\n    top_features = top_features.sort_values('importance',ascending=False).set_index('feature')\n    top_features.to_csv('./top_features.csv')\n    top_features.plot.bar()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e7c96338-7c1a-ad1c-bfca-61cf7c05174a"},"outputs":[],"source":"####### Predict and format output #######\ndef output():\n    predictions = pipe.predict_proba(df_test)\n    \n    #debug\n    print(pipe.classes_)\n    print(predictions)\n    \n    result = pd.DataFrame({\n        'listing_id': df_test['listing_id'],\n        pipe.classes_[0]: [row[0] for row in predictions], \n        pipe.classes_[1]: [row[1] for row in predictions],\n        pipe.classes_[2]: [row[2] for row in predictions]\n        })\n    result.to_csv(time.strftime(\"%Y-%m-%d_%H%M-\")+'baseline.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a57ec578-56e3-7211-f8ca-5bc1167b999f"},"outputs":[],"source":"################ Training ################################\n\nX_train = df_train\ny_train = df_train['interest_level']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee16015b-d229-b207-d776-29b27b5dcbfe"},"outputs":[],"source":"################ Cross Validation ################################\ncrossval()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"29d92bfb-df37-8d81-7883-b12c7fe00742"},"outputs":[],"source":"##### Fit ######\npipe.fit(X_train, y_train, xgb__eval_metric='mlogloss')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a9d9b5b-91db-37eb-7b5e-b608fe6d98dc"},"outputs":[],"source":"######### Most influential features ########\ntop_feat()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"771237dc-a286-9e45-5b0a-60684d740664"},"outputs":[],"source":"######## Predict ########\noutput()"},{"cell_type":"markdown","metadata":{"_cell_guid":"bd9de811-e55c-09f3-713c-b32ba3675fc7"},"source":"# The End\n\nI hope you enjoyed the kernel and that it will help you iterate and test faster in your feature engineering quest.\n\nThank you for your attention, feel free to post comment and upvote.\n\nYou can check advanced transformer usage on my Titanic kernels in [Python](https://www.kaggle.com/mratsim/titanic/titanic-end-to-end-pipeline-stacking-gridsearch) and [Julia](https://www.kaggle.com/mratsim/titanic/titanic-julia-end-to-end-pipelining)."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}