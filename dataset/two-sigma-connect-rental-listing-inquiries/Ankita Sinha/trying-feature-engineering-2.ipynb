{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b5358ec6-f320-fc20-6d59-b22a6e260bf2"},"source":"This notebook is trying to create new features from the features given . My earlier script could not import the test and train files due to some error ."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"407fca25-7e6a-c865-e567-31dc19a7fefa"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"78b09ec4-449d-59fc-87d6-5d991815d563"},"outputs":[],"source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\ntrain = pd.read_json(\"../input/train.json\")\ntest = pd.read_json(\"../input/test.json\")\nprint (train.head())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ad6ac10-f231-a985-e81c-8a1394b0c4b2"},"outputs":[],"source":"train['source']='train'\ntest['source']='test'\nprint(train.head())\nprint(test.info())"},{"cell_type":"markdown","metadata":{"_cell_guid":"d48cb56d-d6f0-d19b-0c11-406923ee3ce3"},"source":"Handling bathrooms"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0d2ca896-3245-e457-19b9-28ba18c46772"},"outputs":[],"source":"print(train['bathrooms'].value_counts())\n# There are houses with 0.0 bathrooms and some with floating point no of bathrooms\nprint(train['bedrooms'].value_counts())\n# There are 9475 houses with 0 bedrooms\ntrain['bathrooms']=train['bathrooms'].astype(int)\ntest['bathrooms']=test['bathrooms'].astype(int)\n# if no of bathrooms are greater than 5 interest level is low else varies\n# so we can create dummies\ntrain.loc[train['bathrooms']==0, 'interest_level'] = 'low'\nsns.violinplot(x='interest_level', y='bathrooms', data=train)\nplt.xlabel('Interest level')\nplt.ylabel('bathrooms')\nplt.show()\n# if 0 or greater than 4 bathrooms interest_level is low"},{"cell_type":"markdown","metadata":{"_cell_guid":"519bbca7-be9f-ab86-ca7b-c4399bae4cbc"},"source":"Bedrooms"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b4b01e7-cb65-bff5-6cc4-365f87962952"},"outputs":[],"source":"# same patern as bathrooms only different at 0\n# pattern between bedrooms and  bathrooms\nprint(train.loc[train['bathrooms']==0, 'bedrooms'].value_counts())\nprint(train.loc[train['bathrooms']==6, 'bedrooms'].value_counts())\n# we can combine 6 or more #bathrooms with 5 or more bedrooms\nsns.violinplot(x='interest_level', y='bedrooms', data=train)\nplt.xlabel('Interest level')\nplt.ylabel('bedrooms')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1d233eed-7591-ac46-5936-341666f9922a"},"source":"Creating new variables from bathrooms"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"255b436a-cbec-7802-7821-8dfce6096ba1"},"outputs":[],"source":"train[\"price_t\"] =train[\"price\"]/train[\"bedrooms\"]\ntest[\"price_t\"] = test[\"price\"]/test[\"bedrooms\"] \n\ntrain[\"room_sum\"] =train[\"bedrooms\"] + train[\"bathrooms\"]\ntest[\"room_sum\"] = test[\"bedrooms\"] + test[\"bathrooms\"]\n\ntrain['price_per_room'] = train['price']/train['room_sum']\ntest['price_per_room'] = test['price']/test['room_sum']"},{"cell_type":"markdown","metadata":{"_cell_guid":"89668e14-e8c1-97a4-4662-10fc627ab292"},"source":"Handling time (created)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"06479eb6-d808-09d3-7cdb-84ac723906a5"},"outputs":[],"source":"train[\"created\"] = pd.to_datetime(train[\"created\"])\ntrain[\"created_year\"] = train[\"created\"].dt.year\ntrain[\"created_month\"] = train[\"created\"].dt.month\ntrain[\"created_day\"] = train[\"created\"].dt.day\ntrain[\"created_hour\"] = train[\"created\"].dt.hour\ntest[\"created\"] = pd.to_datetime(test[\"created\"])\ntest[\"created_year\"] = test[\"created\"].dt.year\ntest[\"created_month\"] = test[\"created\"].dt.month\ntest[\"created_day\"] = test[\"created\"].dt.day\ntest[\"created_hour\"] = test[\"created\"].dt.hour"},{"cell_type":"markdown","metadata":{"_cell_guid":"08b686f4-86d4-cbfe-0f03-99fe6b1b5eb0"},"source":"Log of Price as it is right skewed"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8905bfa2-503e-e48c-7050-bf8361f3037c"},"outputs":[],"source":"plt.scatter(range(train.shape[0]), np.sort(train.price.values))\nplt.xlabel('index')\nplt.ylabel('price')\nplt.show()\n# there are outliners\nulimit = np.percentile(train.price.values, 99)\ntrain['price'].ix[train['price']>ulimit] = ulimit\n# price is right skewed so using log to create a gaussian pattern\ntrain['price']=np.log1p(train['price'])\ntest['price']=np.log1p(test['price'])\n\nplt.figure(figsize=(8,6))\nsns.distplot(train.price.values, bins=50, kde=True)\nplt.xlabel('price')\nplt.show()\nsns.violinplot(data=train,x = 'interest_level',y='price')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"9a61f1ff-0429-b8c8-be0c-69c9994eac9b"},"source":"Street address and Display address"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"daa64a3c-0fce-3625-cbcc-a38efddb24e9"},"outputs":[],"source":"from sklearn.preprocessing import LabelEncoder\ndisplay_count = train.groupby('display_address')['display_address'].count()\nplt.hist(display_count.values, bins=100, log=True, alpha=0.9)\nplt.xlabel('Number of times display_address appeared', fontsize=12)\nplt.ylabel('log of Count', fontsize=12)\nplt.show()\n# there are too many values and none of them are more than 500\n# most of the values are less than 10\n#so we label encode the values\naddress = [\"display_address\", \"street_address\"]\nfor x in address:\n    le = LabelEncoder()\n    le.fit(list(df[x].values))\n    df[x] = le.transform(list(df[x].values))"},{"cell_type":"markdown","metadata":{"_cell_guid":"4f8c132a-b278-3150-7139-982205aa3cdc"},"source":"Find position and density from latitude and longitude"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f585e2fc-3bde-cb04-b0fa-2a77c5ae129b"},"outputs":[],"source":"train[\"pos\"] = train.longitude.round(3).astype(str) + '_' + train.latitude.round(3).astype(str)\ntest[\"pos\"] = test.longitude.round(3).astype(str) + '_' + test.latitude.round(3).astype(str)\n\ntrain[\"density\"] = train['pos'].apply(lambda x: dvals.get(x, vals.min()))\ntest[\"density\"] = test['pos'].apply(lambda x: dvals.get(x, vals.min()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"0b35ab4c-0b44-f82d-78e6-b325b0014735"},"source":"manager_id"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9307b4c5-1567-d5de-5d7e-dd27e78b3723"},"outputs":[],"source":"print(len(train['manager_id'].unique()))\n# 3481 unique managers\ntemp = train.groupby('manager_id').count().iloc[:,-1]\ntemp2 = test.groupby('manager_id').count().iloc[:,-1]\ndf_managers = pd.concat([temp,temp2],axis=1,join='outer')\ndf_managers.columns=['train_count','test_count']\nprint(df_managers.sort_values(by = 'train_count',ascending = False).head())\n# considering only those manager_ids which are in train\nman_list = df_managers['train_count'].sort_values(ascending = False).head(3481).index\nixes = df.manager_id.isin(man_list)\ndf10 = df[ixes][['manager_id','interest_level']]\n# create dummies of interest levels\ninterest_dummies = pd.get_dummies(df10.interest_level)\ndf10 = pd.concat([df10,interest_dummies[['low','medium','high']]], axis = 1).drop('interest_level', axis = 1)\nprint(df10.head())\ngby = pd.concat([df10.groupby('manager_id').mean(),df10.groupby('manager_id').count()], axis = 1).iloc[:,:-2]\ngby.columns = ['low','medium','high','count']\ngby.sort_values(by = 'count', ascending = False).head(10)\ngby['manager_skill'] = gby['medium']*1 + gby['high']*2 \ngby['manager_id']=gby.index\nprint(gby.head(5))\nprint(gby.shape)\ndf = df.merge(gby[['manager_id','manager_skill']],on='manager_id',how='outer',right_index=False)\ndf['manager_skill']=df['manager_skill'].fillna(0)\nprint(df.head())"},{"cell_type":"markdown","metadata":{"_cell_guid":"5a41a2f2-62d7-a2e6-f28a-88a7abadfd94"},"source":"Adding interest acc to manager_id to test acc to train data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3679f557-a963-a1e1-74d1-2f3d09296131"},"outputs":[],"source":"index=list(range(train.shape[0]))\nrandom.shuffle(index)\na=[np.nan]*len(train)\nb=[np.nan]*len(train)\nc=[np.nan]*len(train)\n\nfor i in range(5):\n    building_level={}\n    for j in train['manager_id'].values:\n        building_level[j]=[0,0,0]\n    \n    test_index=index[int((i*train.shape[0])/5):int(((i+1)*train.shape[0])/5)]\n    train_index=list(set(index).difference(test_index))\n    \n    for j in train_index:\n        temp=train.iloc[j]\n        if temp['interest_level']=='low':\n            building_level[temp['manager_id']][0]+=1\n        if temp['interest_level']=='medium':\n            building_level[temp['manager_id']][1]+=1\n        if temp['interest_level']=='high':\n            building_level[temp['manager_id']][2]+=1\n            \n    for j in test_index:\n        temp=train.iloc[j]\n        if sum(building_level[temp['manager_id']])!=0:\n            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n            \ntrain['manager_level_low']=a\ntrain['manager_level_medium']=b\ntrain['manager_level_high']=c\n\na=[]\nb=[]\nc=[]\nbuilding_level={}\nfor j in train['manager_id'].values:\n    building_level[j]=[0,0,0]\n\nfor j in range(train.shape[0]):\n    temp=train.iloc[j]\n    if temp['interest_level']=='low':\n        building_level[temp['manager_id']][0]+=1\n    if temp['interest_level']=='medium':\n        building_level[temp['manager_id']][1]+=1\n    if temp['interest_level']=='high':\n        building_level[temp['manager_id']][2]+=1\n\nfor i in test['manager_id'].values:\n    if i not in building_level.keys():\n        a.append(np.nan)\n        b.append(np.nan)\n        c.append(np.nan)\n    else:\n        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n        c.append(building_level[i][2]*1.0/sum(building_level[i]))\ntest['manager_level_low']=a\ntest['manager_level_medium']=b\ntest['manager_level_high']=c"},{"cell_type":"markdown","metadata":{"_cell_guid":"beff0860-2d0f-952f-a670-886d8023d6ce"},"source":"Adding interest level  Building_id decreases the model accuracy\n\nAdding no of photos , no of words in features and description"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"341062b8-405d-2800-5558-9f00b03d109c"},"outputs":[],"source":"train[\"num_photos\"] = train[\"photos\"].apply(len)\ntest[\"num_photos\"] = test[\"photos\"].apply(len)\n\ntrain[\"num_features\"] = train[\"features\"].apply(len)\ntest[\"num_features\"] = test[\"features\"].apply(len)\n\ntrain[\"num_description_words\"] = train[\"description\"].apply(lambda x: len(x.split(\" \")))\ntest[\"num_description_words\"] = test[\"description\"].apply(lambda x: len(x.split(\" \")))"},{"cell_type":"markdown","metadata":{"_cell_guid":"37fc8c32-b992-5f12-d3c7-755f09716535"},"source":"Changing features to sparse matrix"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ca37f1cd-ab4e-63df-9626-1a36c029872c"},"outputs":[],"source":"train['features'] = train[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\ntest['features'] = test[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n\ntfidf = CountVectorizer(stop_words='english', max_features=200)\ntr_sparse = tfidf.fit_transform(train[\"features\"])\nte_sparse = tfidf.transform(test[\"features\"])"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}