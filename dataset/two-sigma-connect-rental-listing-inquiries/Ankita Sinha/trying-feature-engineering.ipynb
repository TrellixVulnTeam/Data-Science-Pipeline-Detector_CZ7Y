{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"d2fbb0af-6884-e58b-61d8-f75f2f6befec"},"source":"This notebook is trying to create new features from the features given"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7d5c03ac-aa79-f007-1fe9-830e5ec0d21b"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nimport xgboost as xgb\nimport random\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import log_loss\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03712046-925d-d54b-d929-e448e4721156"},"outputs":[],"source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\ntrain = pd.read_json(\"../input/train.json\")\ntest = pd.read_json(\"../input/train.json\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"138cf581-bad0-753e-e28e-754c2193280a"},"outputs":[],"source":"train['source']='train'\ntest['source']='test'\nprint(train.head())\nprint(test.info())"},{"cell_type":"markdown","metadata":{"_cell_guid":"28b03a79-60d7-bc0d-5c5e-001adf7b50fb"},"source":"Handling Bathrooms and bedrooms"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"15cbff59-9797-3524-a79f-6cc9378e1105"},"outputs":[],"source":"print(train['bathrooms'].value_counts())\n# There are houses with 0.0 bathrooms and some with floating point no of bathrooms\nprint(train['bedrooms'].value_counts())\n# There are 9475 houses with 0 bedrooms\ntrain['bathrooms']=train['bathrooms'].astype(int)\ntest['bathrooms']=test['bathrooms'].astype(int)\n# if no of bathrooms are greater than 5 interest level is low else varies\n# so we can create dummies\ntrain.loc[train['bathrooms']==0, 'interest_level'] = 'low'\nsns.violinplot(x='interest_level', y='bathrooms', data=train)\nplt.xlabel('Interest level')\nplt.ylabel('bathrooms')\nplt.show()\n# if 0 or greater than 4 bathrooms interest_level is low\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"200cb918-b59e-f192-d28c-ca2bfe2f9f5a"},"source":"Creating new variables from bathrooms"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"944fa4d4-ec53-1a05-21eb-5c0f6874764d"},"outputs":[],"source":"train[\"price_tt\"] =train[\"price\"]/train[\"bathrooms\"]\ntest[\"price_tt\"] = test[\"price\"]/test[\"bathrooms\"] \n\ntrain[\"room_sum\"] = train[\"bedrooms\"]+train[\"bathrooms\"] \ntrain[\"room_sum\"] = test[\"bedrooms\"]+test[\"bathrooms\"] "},{"cell_type":"markdown","metadata":{"_cell_guid":"48219ff0-4e62-e087-9246-55179890f373"},"source":"Bedrooms"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd564b5a-0f13-fe17-07f5-46c71362406d"},"outputs":[],"source":"# same patern as bathrooms only different at 0\n# pattern between bedrooms and  bathrooms\nprint(train.loc[train['bathrooms']==0, 'bedrooms'].value_counts())\nprint(train.loc[train['bathrooms']==6, 'bedrooms'].value_counts())\n# we can combine 6 or more #bathrooms with 5 or more bedrooms\nsns.violinplot(x='interest_level', y='bedrooms', data=train)\nplt.xlabel('Interest level')\nplt.ylabel('bedrooms')\nplt.show()\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"fcf71634-8ccc-4492-869c-d1648f470595"},"source":"Adding variables from bedrooms"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b9471b7-72ac-922a-fe61-32d64b2c0e7b"},"outputs":[],"source":"train[\"price_t\"] =train[\"price\"]/train[\"bedrooms\"]\ntest[\"price_t\"] = test[\"price\"]/test[\"bedrooms\"] \n\ntrain['room_sum'] = train['bedrooms']  + train['bathrooms']\ntest['room_sum'] = test['bedrooms']  + test['bathrooms']\ntrain['price_per_room'] = train['price']/train['room_sum']\ntest['price_per_room'] = test['price']/test['room_sum']"},{"cell_type":"markdown","metadata":{"_cell_guid":"e5f24fc5-0bac-8641-aa69-c2d4c9c441ed"},"source":"Handling time (created)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"43a01c15-e0db-d053-60a0-7e0883290d0a"},"outputs":[],"source":"train[\"created\"] = pd.to_datetime(train[\"created\"])\ntrain[\"created_year\"] = train[\"created\"].dt.year\ntrain[\"created_month\"] = train[\"created\"].dt.month\ntrain[\"created_day\"] = train[\"created\"].dt.day\ntrain[\"created_hour\"] = train[\"created\"].dt.hour\ntest[\"created\"] = pd.to_datetime(test[\"created\"])\ntest[\"created_year\"] = test[\"created\"].dt.year\ntest[\"created_month\"] = test[\"created\"].dt.month\ntest[\"created_day\"] = test[\"created\"].dt.day\ntest[\"created_hour\"] = test[\"created\"].dt.hour"},{"cell_type":"markdown","metadata":{"_cell_guid":"77c58c71-3a0c-c481-7bf4-f0976444a92b"},"source":"Log of Price as it is right skewed"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f14d8049-052c-25a9-7d4f-a872f8308fcb"},"outputs":[],"source":"plt.scatter(range(train.shape[0]), np.sort(train.price.values))\nplt.xlabel('index')\nplt.ylabel('price')\nplt.show()\n# there are outliners\nulimit = np.percentile(train.price.values, 99)\ntrain['price'].ix[train['price']>ulimit] = ulimit\n# price is right skewed so using log to create a gaussian pattern\ntrain['price']=np.log1p(train['price'])\ntest['price']=np.log1p(test['price'])\n\nplt.figure(figsize=(8,6))\nsns.distplot(train.price.values, bins=50, kde=True)\nplt.xlabel('price')\nplt.show()\nsns.violinplot(data=train,x = 'interest_level',y='price')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"064a4875-0f23-80aa-02a9-0feda6084b70"},"source":"Street address and Display address"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"552ff6fd-9670-caf8-efc7-1268c730dfd3"},"outputs":[],"source":"from sklearn.preprocessing import LabelEncoder\ndisplay_count = train.groupby('display_address')['display_address'].count()\nplt.hist(display_count.values, bins=100, log=True, alpha=0.9)\nplt.xlabel('Number of times display_address appeared', fontsize=12)\nplt.ylabel('log of Count', fontsize=12)\nplt.show()\n# there are too many values and none of them are more than 500\n# most of the values are less than 10\n#so we label encode the values\naddress = [\"display_address\", \"street_address\"]\nfor x in address:\n    le = LabelEncoder()\n    le.fit(list(train[x].values))\n    train[x] = le.transform(list(train[x].values))\n    le.fit(list(test[x].values))\n    test[x] = le.transform(list(test[x].values))\n    "},{"cell_type":"markdown","metadata":{"_cell_guid":"172582b3-4d3c-691a-48fc-e5a10a4a6fd0"},"source":"Find position and neighbourhood from latitude and longitude"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5765295b-789d-e4db-a250-a5c62cf466a6"},"outputs":[],"source":"train[\"pos\"] = train.longitude.round(3).astype(str) + '_' + train.latitude.round(3).astype(str)\ntest[\"pos\"] = test.longitude.round(3).astype(str) + '_' + test.latitude.round(3).astype(str)\n\nfrom sklearn.cluster import Birch\ndef cluster_latlon(n_clusters, data):  \n    print(\"data.shape\")\n    print(data.shape)\n    #split the data between \"around NYC\" and \"other locations\" basically our first two clusters \n    data_c=data[(data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9)]\n    print(data_c.shape)\n    data_e=data[~((data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9))]\n    print(data_e.shape)\n    #put it in matrix form\n    coords=data_c.as_matrix(columns=['latitude', \"longitude\"])\n    \n    brc = Birch(branching_factor=100, n_clusters=n_clusters, threshold=0.01,compute_labels=True)\n\n    brc.fit(coords)\n    clusters=brc.predict(coords)\n    data_c[\"cluster_\"+str(n_clusters)]=clusters\n    data_e[\"cluster_\"+str(n_clusters)]=-1 #assign cluster label -1 for the non NYC listings \n    data=pd.concat([data_c,data_e])\n    print(data.shape)\n    #plt.scatter(data_c[\"longitude\"], data_c[\"latitude\"], c=data_c[\"cluster_\"+str(n_clusters)], s=10, linewidth=0.1)\n    #plt.title(str(n_clusters)+\" Neighbourhoods from clustering\")\n    #plt.show()\n    return data \n\ntrain=cluster_latlon(100, train)\nprint(train.head())\nclusters_price_map=dict(train.groupby(by=\"cluster_100\")[\"price\"].median())\ntrain[\"price_comparison\"]=train['price']-train[\"cluster_100\"].map(clusters_price_map)\n\ntest=cluster_latlon(100, test)\n\nclusters_price_map=dict(test.groupby(by=\"cluster_100\")[\"price\"].median())\ntest[\"price_comparison\"]=test['price']-test[\"cluster_100\"].map(clusters_price_map)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"5544123c-6512-e914-f0cf-5326081c1b49"},"source":"manager_id"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"77d77dbb-af22-2eb9-ef10-5ac7736417ec"},"outputs":[],"source":"print(len(train['manager_id'].unique()))\n# 3481 unique managers\ntemp = train.groupby('manager_id').count().iloc[:,-1]\ntemp2 = test.groupby('manager_id').count().iloc[:,-1]\ntrain_managers = pd.concat([temp,temp2],axis=1,join='outer')\ntrain_managers.columns=['train_count','test_count']\nprint(train_managers.sort_values(by = 'train_count',ascending = False).head())\n# considering only those manager_ids which are in train\nman_list = train_managers['train_count'].sort_values(ascending = False).head(3481).index\nixes = train.manager_id.isin(man_list)\ntrain10 = train[ixes][['manager_id','interest_level']]\n# create dummies of interest levels\ninterest_dummies = pd.get_dummies(train10.interest_level)\ntrain10 = pd.concat([train10,interest_dummies[['low','medium','high']]], axis = 1).drop('interest_level', axis = 1)\nprint(train10.head())\ngby = pd.concat([train10.groupby('manager_id').mean(),train10.groupby('manager_id').count()], axis = 1).iloc[:,:-2]\ngby.columns = ['low','medium','high','count']\ngby.sort_values(by = 'count', ascending = False).head(10)\ngby['manager_skill'] = gby['medium']*1 + gby['high']*2 \ngby['manager_id']=gby.index\nprint(gby.head(5))\nprint(gby.shape)\ntrain = train.merge(gby[['manager_id','manager_skill']],on='manager_id',how='outer',right_index=False)\ntrain['manager_skill']=train['manager_skill'].fillna(0)\nprint(train.head())"},{"cell_type":"markdown","metadata":{"_cell_guid":"2642f15c-1083-3c52-58a8-28e08161028e"},"source":"Adding interest acc to manager_id to test acc to train data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d918f1e4-d9e2-58c1-1691-dece23479804"},"outputs":[],"source":"index=list(range(train.shape[0]))\nrandom.shuffle(index)\na=[np.nan]*len(train)\nb=[np.nan]*len(train)\nc=[np.nan]*len(train)\n\nfor i in range(5):\n    building_level={}\n    for j in train['manager_id'].values:\n        building_level[j]=[0,0,0]\n    \n    test_index=index[int((i*train.shape[0])/5):int(((i+1)*train.shape[0])/5)]\n    train_index=list(set(index).difference(test_index))\n    \n    for j in train_index:\n        temp=train.iloc[j]\n        if temp['interest_level']=='low':\n            building_level[temp['manager_id']][0]+=1\n        if temp['interest_level']=='medium':\n            building_level[temp['manager_id']][1]+=1\n        if temp['interest_level']=='high':\n            building_level[temp['manager_id']][2]+=1\n            \n    for j in test_index:\n        temp=train.iloc[j]\n        if sum(building_level[temp['manager_id']])!=0:\n            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n            \ntrain['manager_level_low']=a\ntrain['manager_level_medium']=b\ntrain['manager_level_high']=c\n\na=[]\nb=[]\nc=[]\nbuilding_level={}\nfor j in train['manager_id'].values:\n    building_level[j]=[0,0,0]\n\nfor j in range(train.shape[0]):\n    temp=train.iloc[j]\n    if temp['interest_level']=='low':\n        building_level[temp['manager_id']][0]+=1\n    if temp['interest_level']=='medium':\n        building_level[temp['manager_id']][1]+=1\n    if temp['interest_level']=='high':\n        building_level[temp['manager_id']][2]+=1\n\nfor i in test['manager_id'].values:\n    if i not in building_level.keys():\n        a.append(np.nan)\n        b.append(np.nan)\n        c.append(np.nan)\n    else:\n        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n        c.append(building_level[i][2]*1.0/sum(building_level[i]))\ntest['manager_level_low']=a\ntest['manager_level_medium']=b\ntest['manager_level_high']=c\nprint(train.head())\nprint(test.head())"},{"cell_type":"markdown","metadata":{"_cell_guid":"ca5a3578-d892-8cba-ad14-4a99827bcf96"},"source":"Adding interest level  Building_id decreases the model accuracy"},{"cell_type":"markdown","metadata":{"_cell_guid":"8f6ad3a4-cd19-d58a-c84f-11c69e3a05ed"},"source":"Adding no of photos , no of words in features and description"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c4f48251-162a-7f61-b226-ae6ff2b1c2ed"},"outputs":[],"source":"train[\"num_photos\"] = train[\"photos\"].apply(len)\ntest[\"num_photos\"] = test[\"photos\"].apply(len)\n\ntrain[\"num_features\"] = train[\"features\"].apply(len)\ntest[\"num_features\"] = test[\"features\"].apply(len)\n\ntrain[\"num_description_words\"] = train[\"description\"].apply(lambda x: len(x.split(\" \")))\ntest[\"num_description_words\"] = test[\"description\"].apply(lambda x: len(x.split(\" \")))\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"84f15ccb-fa78-50e0-08b0-36a45aff7913"},"source":"Changing features to sparse matrix"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb8f7368-29f9-b790-f713-628406b95513"},"outputs":[],"source":"train['features'] = train[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\ntest['features'] = test[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n\ntfidf = CountVectorizer(stop_words='english', max_features=200)\ntr_sparse = tfidf.fit_transform(train[\"features\"])\nte_sparse = tfidf.transform(test[\"features\"])"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}