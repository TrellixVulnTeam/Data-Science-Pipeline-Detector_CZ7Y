{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"d29d6549-e6f5-b40d-8c89-4bab6ef051da"},"source":"I merely use some basic features here. If using some better (but complex) features, I can achieve scores below 0.5 in validate set. However, only ~0.67 score can be obtained on public scoreboard with all these trials."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b70206cd-ac8c-bb0c-b6ca-1a53c952ac7a"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport json\nimport time\nimport operator\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nfrom sklearn.svm import LinearSVC, SVC, SVR\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\nfrom xgboost import XGBClassifier, XGBRegressor"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9d8df851-0fbd-231a-99c4-c14be21a621e"},"outputs":[],"source":"print(\"\\nReading data...\")\ntn_data = pd.read_json('../input/train.json').set_index('listing_id')\ntt_data = pd.read_json('../input/test.json').set_index('listing_id')\nprint(\"done\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c5b77491-2e5f-a98a-81a9-5853b70b77bc"},"outputs":[],"source":"# drop duplicate data\ntn_data['features'] = tn_data.loc[:,'features'].apply(lambda x: ' '.join(x))\ntn_data['photos'] = tn_data.loc[:,'photos'].apply(lambda x: ' '.join(x))\ntt_data['features'] = tt_data.loc[:,'features'].apply(lambda x: ' '.join(x))\ntt_data['photos'] = tt_data.loc[:,'photos'].apply(lambda x: ' '.join(x))\ntn_data = tn_data.drop_duplicates()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d82cb535-1b52-aec5-5011-0a152f705401"},"outputs":[],"source":"# extract needed features to tn_sdata\ntn_sdata = tn_data[['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price']]\ntt_sdata = tt_data[['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price']]\n\n# define colnames\nraw_colnames = ['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d19ba82c-0137-6a7e-ddf7-449db99c33de"},"outputs":[],"source":"# deal with number of images\ntn_sdata.loc[:,'n_images'] = tn_data.loc[:,'photos'].apply(lambda x: len(x.split()))\ntt_sdata.loc[:,'n_images'] = tt_data.loc[:,'photos'].apply(lambda x: len(x.split()))\n\n# deal with number of features\ntn_sdata.loc[:,'n_features'] = tn_data.loc[:,'features'].apply(lambda x: len(x.split()))\ntt_sdata.loc[:,'n_features'] = tt_data.loc[:,'features'].apply(lambda x: len(x.split()))\n\n# deal with number of words in descriptions\ntn_sdata.loc[:,'n_words'] = tn_data.loc[:,'description'].apply(lambda x: len(x.split()))\ntt_sdata.loc[:,'n_words'] = tt_data.loc[:,'description'].apply(lambda x: len(x.split()))\n\n# define colnames\nlen_colnames = ['n_images', 'n_features', 'n_words']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a7894c81-d143-912e-d1f8-73a48165f629"},"outputs":[],"source":"# deal with building ID\ntn_sdata.loc[:,'building_id'] = np.array(tn_data.loc[:,'building_id'] != '0', dtype=int).reshape(-1,1)\ntt_sdata.loc[:,'building_id'] = np.array(tt_data.loc[:,'building_id'] != '0', dtype=int).reshape(-1,1)\nid_colnames = ['building_id']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dad2935c-29b8-fb69-6cf3-ee11a7514528"},"outputs":[],"source":"colnames = raw_colnames + len_colnames + id_colnames\nprint(colnames)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"335c5853-87fc-d01a-83db-fad58ec45420"},"outputs":[],"source":"# deal with interest_level\ntn_cy = tn_data[['interest_level']]\ntn_ry = np.zeros((tn_sdata.shape[0],), dtype=int)\ntn_ry[np.array(tn_cy['interest_level'] == 'medium')] = 1\ntn_ry[np.array(tn_cy['interest_level'] == 'high')] = 1\n\n# merge label into tn_sdata\npd.options.mode.chained_assignment = None  # default='warn'\ntn_sdata['interest_level'] = pd.Series(tn_ry, index=tn_sdata.index)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"316b0e77-2fa6-025a-8fbb-da02bdb181c1"},"outputs":[],"source":"# split dataset into subtrain and validate sets\nstn_sdata, vld_sdata = train_test_split(tn_sdata, test_size = 0.2, random_state=64)\n\n# partition x and y\nstn_x = stn_sdata[colnames]\nstn_y = np.array(stn_sdata['interest_level'], dtype=int)\nvld_x = vld_sdata[colnames]\nvld_y = np.array(vld_sdata['interest_level'], dtype=int)\ntt_x = tt_sdata[colnames]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cda7fce9-5c05-99bc-3e2a-60a5b44f7b4c"},"outputs":[],"source":"clf = XGBRegressor(n_estimators=100,\n                        learning_rate=0.1,\n                        max_depth=3,\n                        min_child_weight=1,\n                        colsample_bytree=.9,\n                        colsample_bylevel=.5,\n                        gamma=0.0005,\n                        scale_pos_weight=1,\n                        base_score=.5,\n                        reg_lambda=1,\n                        reg_alpha=1,\n                        missing=0,\n                        seed=514)\n\nk=10\nkf = KFold(n_splits=k)\nfor i, idx in zip(range(k), kf.split(tn_sdata)):\n    # split data\n    stn_idx, vld_idx = idx\n    stn_x = tn_sdata[colnames].iloc[stn_idx]\n    vld_x = tn_sdata[colnames].iloc[vld_idx]\n    stn_y = np.array(tn_sdata['interest_level'].iloc[stn_idx], dtype=int)\n    vld_y = np.array(tn_sdata['interest_level'].iloc[vld_idx], dtype=int)\n    \n    # train model\n    clf.fit(stn_x, stn_y)\n\n    # predict on subtrain and validate sets\n    ratio = 0.9\n    stn_p = clf.predict(stn_x)\n    stn_p = np.array(list(map(lambda x: min(max(x,0),1), stn_p)))\n    stn_p = np.array(list(map(lambda x: [1-x, ratio*x, (1-ratio)*x], stn_p)))\n    vld_p = clf.predict(vld_x)\n    vld_p = np.array(list(map(lambda x: min(max(x,0),1), vld_p)))\n    vld_p = np.array(list(map(lambda x: [1-x, ratio*x, (1-ratio)*x], vld_p)))\n    \n    # prevent -inf caused by 0 prediction\n    threshold = 0.0001\n    stn_p = stn_p + threshold * np.ones(stn_p.shape)\n    stn_p = stn_p / np.sum(stn_p, axis=1).reshape(-1,1)\n    vld_p = vld_p + threshold * np.ones(vld_p.shape)\n    vld_p = vld_p / np.sum(vld_p, axis=1).reshape(-1,1)\n    \n    print('\\nFold: %d'%i)\n    print('Subtrain logloss: %f'%log_loss(stn_y, stn_p, labels=[0,1,2]))\n    print('Validate logloss: %f'%log_loss(vld_y, vld_p, labels=[0,1,2]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55f661b9-d0d5-63e5-f1c3-60e4fce03c5f"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}