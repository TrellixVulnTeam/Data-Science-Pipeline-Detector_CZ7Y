{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"21a897f2-6f87-5af4-5c59-5fed6cd0b100"},"source":"# This is the first attempt at solving the problem #"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93fab979-9656-8293-bb10-8d60a20b8a25"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"05b90972-36fe-b038-876c-593b54e078b2"},"outputs":[],"source":"train = pd.read_json('../input/train.json')\ntrain.head(20)"},{"cell_type":"markdown","metadata":{"_cell_guid":"54d7a9b2-bfc9-93b2-a510-352ed1965a71"},"source":"## Missing Values ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc6702ef-3a65-ddf7-d005-832802641866"},"outputs":[],"source":"missing = train.isnull().sum()\nmissing"},{"cell_type":"markdown","metadata":{"_cell_guid":"2d5ffae1-e07d-6dd3-a8d9-aaf0b8e7315f"},"source":"No missing values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"413c359a-2237-d14f-52a6-940cfcb48ce0"},"outputs":[],"source":"idx = train['features'].index\nfeat_count = list()\nfeat_count += [len(train.features[i]) for i in idx]\ntrain['feat_count'] = feat_count"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"212fc510-6772-8700-eb04-05d87a27993a"},"outputs":[],"source":"from nltk.corpus import sentiwordnet as swn\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport nltk\nimport string\nimport re"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"53ebb019-625a-ba6f-e6bc-3d1956cbcae9"},"outputs":[],"source":"def polarity (data, text_col):\n    \n#    print ('Calculating polarities.....')    \n    \n    sw = stopwords.words('english')\n    \n    p_c = []\n    n_c = []\n\n    pos_polarity = []\n    neg_polarity = []\n\n    for i in range(0, data.shape[0]):\n    \n        pos_count = 0\n        neg_count = 0\n        \n        pos_score = 0\n        neg_score = 0\n\n        for w in word_tokenize(data.iloc[i][text_col].decode('utf-8')):\n            if w not in sw:\n                try:\n                    sent = list(swn.senti_synsets(w))[0]\n                    pc = sent.pos_score()\n                    nc = sent.neg_score()\n                    if (pc > nc):\n                        pos_count += 1\n                        pos_score += pc\n                    elif(pc < nc):\n                        neg_count += 1\n                        neg_score += nc\n                except:\n                    pass\n        \n        p_c.append(pos_count)\n        n_c.append(neg_count)\n        \n        pos_polarity.append(pos_score)\n        neg_polarity.append(neg_score)      \n        \n    data['n_count'] = n_c\n    data['p_count'] = p_c\n    \n    data['n_pol'] = neg_polarity\n    data['p_pol'] = pos_polarity\n    \n    data.loc[data.n_pol > data.p_pol, 'pol'] = 0\n    data.loc[data.n_pol < data.p_pol, 'pol'] = 1\n    data.loc[data.n_pol == data.p_pol, 'pol'] = 2\n    \n    data['pol_ratio'] = (data.p_pol + 0.5) / (data.n_pol + 0.5) "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee6179ab-6b6e-fa9a-2ba2-2361ae3c9cd1"},"outputs":[],"source":"train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"322731ba-cc48-cad2-e245-e4ea4cc1215d"},"outputs":[],"source":"X = train[['bedrooms', 'bathrooms', 'feat_count']]\ny = train.interest_level"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dfdad81f-9512-0ad2-9580-a7bd1230c30c"},"outputs":[],"source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.3)\nrf = RandomForestClassifier(n_estimators=25)\nnb = MultinomialNB()\nrf.fit(X_train, y_train)\nnb.fit(X_train, y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"86507fab-84d0-3804-814c-767a04d49cd4"},"outputs":[],"source":"y_pred_nb = nb.predict(X_cv)\ny_pred_rf = rf.predict(X_cv)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nsns.heatmap(confusion_matrix(y_cv, y_pred_nb), annot=True, fmt='1g')\nprint('Accuracy : ', \n     accuracy_score(y_cv, y_pred_nb))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"44a5ae92-33e1-6505-da38-a5010565ddfc"},"outputs":[],"source":"sns.heatmap(confusion_matrix(y_cv, y_pred_rf), annot=True, fmt='1g')\nprint('Accuracy : ', \n     accuracy_score(y_cv, y_pred_rf))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"18839ea2-f0ca-25ed-52f0-89cd321b6b16"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}