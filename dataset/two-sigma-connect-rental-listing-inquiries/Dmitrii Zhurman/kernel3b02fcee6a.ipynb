{"cells":[{"metadata":{},"cell_type":"markdown","source":"В данном решении используется рад признаков такие как area_name, manager_level_low, manager_level_medium, manager_level_high, которые оказывают положительный эффект на результат, но их создание требует большого количества времени, поэтому предлагается использовать наборы данных prep_geo_train.json и prep_geo_test.json. Эти наборы данных уже содержат указанные признаки, но их получение будет описано в решении."},{"metadata":{},"cell_type":"markdown","source":"# Импорт библитек"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom hyperopt import hp\nfrom sklearn.model_selection import train_test_split\nfrom hyperopt import fmin,tpe,anneal,STATUS_OK,STATUS_FAIL,Trials\nimport random\nimport string\nimport re\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import log_loss\nfrom sklearn.ensemble import (\n    RandomForestClassifier,\n    ExtraTreesClassifier,\n    RandomForestRegressor,\n    ExtraTreesRegressor,\n)\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport catboost as ctb\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Загрузка и объединение данных"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_json('../input/prep-geo/prep_geo_train.json')\ndf_test = pd.read_json('../input/prep-geo/prep_geo_test.json')\nid_train = df_train.shape[0]\nlisting_train = list(df_train.listing_id)\nlisting_test = list(df_test.listing_id)\n\ntarget = df_train.interest_level\ndf = pd.concat([df_train, df_test])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"## Базовые признаки"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"price\"] = df[\"price\"].clip(upper=13000)\ndf[\"logprice\"] = np.log(df[\"price\"])\ndf['half_bathrooms'] = df[\"bathrooms\"] - df[\"bathrooms\"].apply(int)\ndf[\"price_t\"] = df[\"price\"]/df[\"bedrooms\"]\ndf[\"room_sum\"] = df[\"bedrooms\"]+df[\"bathrooms\"]\ndf['price_per_room'] = df['price']/df['room_sum']\ndf[\"num_photos\"] = df[\"photos\"].apply(len)\ndf[\"num_features\"] = df[\"features\"].apply(len)\ndf[\"num_description_words\"] = df[\"description\"].apply(lambda x: len(x.split(\" \")))\n\ndf[\"created\"] = pd.to_datetime(df[\"created\"])\ndf[\"created_year\"] = df[\"created\"].dt.year\ndf[\"created_month\"] = df[\"created\"].dt.month\ndf[\"created_day\"] = df[\"created\"].dt.day\ndf[\"created_hour\"] = df[\"created\"].dt.hour\ndf[\"created_weekday\"] = df[\"created\"].dt.weekday\ndf[\"created_week\"] = df[\"created\"].dt.week\n\ndf[\"pos\"] = df.longitude.round(3).astype(str) + '_' + df.latitude.round(3).astype(str)\nvals = df['pos'].value_counts()\ndvals = vals.to_dict()\ndf[\"density\"] = df['pos'].apply(lambda x: dvals.get(x, vals.min()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Получение дополнительной географической информации"},{"metadata":{},"cell_type":"markdown","source":"Требует большого количества времени на выполнение (около 6 часов), поэтому используется датасет с уже имеющимся данным признаком."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import reverse_geocoder as revgc\n\n# df['area_name'] = df.apply(lambda x: revgc.search([x.latitude, x.longitude])[0]['name'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Преобразование категориальных признаков"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = [\"street_address\", \"display_address\", \"manager_id\", \"building_id\",'area_name']\nfor f in categorical:\n        if df[f].dtype=='object':\n            lbl = LabelEncoder()\n            df[f] = lbl.fit_transform(df[f])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Создание статистических признаков таких как среднее, минимальное, максимальное значение и стандартное отклонение для различных групп"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_stat_feature(df,group_col,col,name):\n    for i in ['min','max','std','mean']:\n        df[name+'_'+i] = df.groupby(group_col)[col].transform(i)\n\ncreate_stat_feature(df,'manager_id','price','price_manager')\ncreate_stat_feature(df,'manager_id','building_id','building_manager')\ncreate_stat_feature(df,'manager_id','street_address','street_manager')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Advanced Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"## Некоторые признаки из https://www.kaggle.com/chriscc/twosigmarenthop-advanced-feature-engineering"},{"metadata":{},"cell_type":"markdown","source":"### Расчет расстояния до важных точек"},{"metadata":{},"cell_type":"markdown","source":"В указанном выше решении расчитывается расстояние до некоторых важных точек Нью-Йорка таких как Financial district и Central park, но не учтены такие места как Time Square, Empire state building, Tower of Freedom, Brooklyn Bridge. Поэтому добавлен расчет расстояния до этих точек."},{"metadata":{"trusted":true},"cell_type":"code","source":"def distance_to(df,cords,name):\n    df[name] = df[['latitude', 'longitude']].apply(lambda x:math.sqrt((x[0]-cords[0])**2+(x[1]-cords[1])**2), axis=1)\n    \ncords_list = [(40.705628,-74.010278),(40.785091,-73.968285),(40.758896,-73.985130),\n              (40.748817,-73.985428),(40.712742,-74.013382),(40.706086,-73.996864)]\n\nnames_list = ['distance_to_fi','distance_to_cp','distance_to_tq','distance_to_et',\n              'distance_to_tf','distance_to_bb']\n\nfor i in range(len(cords_list)):\n    distance_to(df,cords_list[i],names_list[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Подсчет того, сколько раз встречались признаки"},{"metadata":{"trusted":true},"cell_type":"code","source":"display = df[\"display_address\"].value_counts()\nmanager_id = df[\"manager_id\"].value_counts()\nbuilding_id = df[\"building_id\"].value_counts()\nstreet = df[\"street_address\"].value_counts()\nbedrooms = df[\"bedrooms\"].value_counts()\nbathrooms = df[\"bathrooms\"].value_counts()\n\ndf[\"display_count\"] = df[\"display_address\"].apply(lambda x:display[x])\ndf[\"manager_count\"] = df[\"manager_id\"].apply(lambda x:manager_id[x])  \ndf[\"building_count\"] = df[\"building_id\"].apply(lambda x:building_id[x])\ndf[\"street_count\"] = df[\"street_address\"].apply(lambda x:street[x])\ndf[\"bedrooms_count\"] = df[\"bedrooms\"].apply(lambda x:bedrooms[x])\ndf[\"bathrooms_count\"] = df[\"bathrooms\"].apply(lambda x:bathrooms[x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Указан ли в описании телефон, email. Количество описаний, поиск пропусков в building_id"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['nums_of_desc'] = df['description']\\\n        .apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n        .apply(lambda x: len([s for s in x if s.isdigit()]))\n        \ndf['has_phone'] = df['description'].apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n        .apply(lambda x: [s for s in x if s.isdigit()])\\\n        .apply(lambda x: len([s for s in x if len(str(s))==10]))\\\n        .apply(lambda x: 1 if x>0 else 0)\n\ndf['has_email'] = df['description'].apply(lambda x: 1 if '@renthop.com' in x else 0)\ndf['building_id_is_zero'] = df['building_id'].apply(lambda x:1 if x=='0' else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Поиск символов в описании"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['num_of_html_tag'] = df.description.apply(lambda x:x.count('<'))\ndf['num_of_#'] = df.description.apply(lambda x:x.count('#'))\ndf['num_of_!'] = df.description.apply(lambda x:x.count('!'))\ndf['num_of_$'] = df.description.apply(lambda x:x.count('$'))\ndf['num_of_*'] = df.description.apply(lambda x:x.count('*'))\ndf['num_of_>'] = df.description.apply(lambda x:x.count('>'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Доп. признаки на основе координат"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"geo_area_50\"] = \\\n    df[['latitude', 'longitude']]\\\n    .apply(lambda x:(int(x[0]*50)%50)*50+(int(-x[1]*50)%50),axis=1)                                         \n                         \ndf[\"geo_area_100\"] = \\\n    df[['latitude', 'longitude']]\\\n    .apply(lambda x:(int(x[0]*100)%100)*100+(int(-x[1]*100)%100),axis=1)                                         \n  \ndf[\"geo_area_200\"] = \\\n    df[['latitude', 'longitude']]\\\n    .apply(lambda x:(int(x[0]*200)%200)*200+(int(-x[1]*200)%200),axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Рейтинг менеджера"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"price_manager\"] = df['price'] * df['manager_level_medium'] * df['manager_level_low'] * df['manager_level_high']\ndf[\"rating_manager\"] = -df['manager_level_medium'] - df['manager_level_low']*2 + df['manager_level_high']*10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Некоторые признаки из https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/discussion/32163"},{"metadata":{},"cell_type":"markdown","source":"Было бы преступлением не посмотреть и не разобрать часть решения, которую выложил победитель данных соревнований"},{"metadata":{"trusted":true},"cell_type":"code","source":"def time_long(x,y):\n    if x==4:\n        return y\n    if x==5:\n        return 30+y\n    if x==6:\n        return 30+31+y\n    \ndef merge_nunique(df,columns,value,cname):\n    add = pd.DataFrame(df.groupby(columns)[value].nunique()).reset_index()\n    add.columns=columns+[cname]\n    df = df.merge(add,on=columns,how=\"left\")\n    return df\n\ndef merge_median(df,columns,value,cname):\n    add = pd.DataFrame(df.groupby(columns)[value].median()).reset_index()\n    add.columns=columns+[cname]\n    df = df.merge(add,on=columns,how=\"left\")\n    return df\n\ndef merge_count(df,columns,value,cname):\n    add = pd.DataFrame(df.groupby(columns)[value].count()).reset_index()\n    add.columns=columns+[cname]\n    df = df.merge(add,on=columns,how=\"left\")\n    return df\n\ndef merge_nunique(df,columns,value,cname):\n    add = pd.DataFrame(df.groupby(columns)[value].nunique()).reset_index()\n    add.columns=columns+[cname]\n    df = df.merge(add,on=columns,how=\"left\")\n    return df\n\ndef merge_mean(df,columns,value,cname):\n    add = pd.DataFrame(df.groupby(columns)[value].mean()).reset_index()\n    add.columns=columns+[cname]\n    df = df.merge(add,on=columns,how=\"left\")\n    return df\n\ndef merge_sum(df,columns,value,cname):\n    add = pd.DataFrame(df.groupby(columns)[value].sum()).reset_index()\n    add.columns=columns+[cname]\n    df = df.merge(add,on=columns,how=\"left\")\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"time\"] = list(map(lambda x, y: time_long(x, y), df[\"created_month\"], df[\"created_day\"]))\n\ndf[\"price_bed\"] = df[\"price\"] / (df[\"bedrooms\"] + 1)\ndf[\"price_bath\"] = df[\"price\"] / (df[\"bathrooms\"] + 1)\ndf[\"price_bath_bed\"] = df[\"price\"] / (df[\"bathrooms\"] + df[\"bedrooms\"] + 1)\ndf[\"bed_bath_dif\"] = df[\"bedrooms\"] - df[\"bathrooms\"]\ndf[\"bed_bath_per\"] = df[\"bedrooms\"] / df[\"bathrooms\"]\ndf[\"bed_all_per\"] = df[\"bedrooms\"] / df[\"room_sum\"]\n\ndf = merge_nunique(df, [\"manager_id\"], \"time\", \"manager_active\")\ndf = merge_nunique(df, [\"manager_id\"], \"building_id\", \"manager_building\")\ndf[\"manager_building_post_rt\"] = df[\"manager_building\"] / df[\"manager_count\"]\ndf[\"build_day\"] = df[\"manager_building\"] / df[\"manager_active\"]\n\ndf = merge_nunique(df, [\"building_id\"], \"manager_id\", \"building_manager\")\ndf = merge_median(df, [\"building_id\", \"bedrooms\", \"bathrooms\"], \"price\", \"building_mean\")\n\nMin_lis_id = df[\"listing_id\"].min()\nMin_time = df[\"time\"].min()\ndf[\"gradient\"] = ((df[\"listing_id\"]) - Min_lis_id) / (df[\"time\"] - Min_time)\ndf[\"building_dif\"] = df[\"price\"] - df[\"building_mean\"]\ndf[\"building_rt\"] = df[\"price\"] / df[\"building_mean\"]\n\ndf[\"jwd_class\"] = list(map(lambda x, y: (int(x * 100) % 100) * 100 + (int(-y * 100) % 100), df[\"latitude\"], df[\"longitude\"]))\ndf = merge_nunique(df, [\"manager_id\"], \"building_rt\", \"manager_pay\")\ndf = merge_nunique(df, [\"jwd_class\"], \"manager_id\", \"manager_num_jwd\")\ndf = merge_nunique(df, [\"manager_id\"], \"jwd_class\", \"manager_jwd_class\")\n\ndf = merge_nunique(df, [\"jwd_class\"], \"building_id\", \"building_num_jwd\")\ndf = merge_median(df, [\"bathrooms\", \"bedrooms\"], \"price\", \"fangxing_mean\")\ndf = merge_median(df, [\"jwd_class\", \"bathrooms\", \"bedrooms\"], \"price\", \"type_jwd_price_mean\")\ndf[\"type_jwd_price_mean_rt\"] = df[\"price\"] / df[\"type_jwd_price_mean\"]\n\ndf[\"type_jwd_building_mean_rt\"] = df[\"building_mean\"] / df[\"type_jwd_price_mean\"]\ndf[\"fangxing_mean_dif_jwd\"] = df[\"fangxing_mean\"] - df[\"type_jwd_price_mean\"]\ndf[\"fangxing_mean_rt_jwd\"] = df[\"fangxing_mean\"] / df[\"type_jwd_price_mean\"]\ndf = merge_mean(df, [\"manager_id\"], \"type_jwd_price_mean_rt\", \"manager_pay_jwd\")\n\ndf = merge_mean(df, [\"building_id\"], \"type_jwd_building_mean_rt\", \"building_pay_jwd\")\ndf = merge_mean(df, [\"jwd_class\"], \"fangxing_mean_rt_jwd\", \"jwd_pay_all\")\ndf = merge_mean(df, [\"manager_id\"], \"building_pay_jwd\", \"manager_own_ud\")\ndf = merge_mean(df, [\"manager_id\"], \"jwd_pay_all\", \"manager_own_ud_all\")\ndf[\"manager_building_all_rt\"] = df[\"manager_own_ud\"] / df[\"manager_own_ud_all\"]\n\ndf[\"all_hours\"] = df[\"time\"] * 24 + df[\"created_hour\"]\ndf = merge_nunique(df, [\"manager_id\"], \"all_hours\", \"manager_hours\")\ndf[\"manager_hours_rt\"] = df[\"manager_hours\"] / df[\"manager_active\"]\ndf[\"manager_price_mean\"] = 0\ndf = merge_sum(df, [\"manager_id\"], \"price\", \"manager_price_sum\")\n\ndf = merge_sum(df, [\"manager_id\"], \"bedrooms\", \"manager_bedrooms_sum\")\ndf = merge_sum(df, [\"manager_id\"], \"building_dif\", \"earn_all\")\ndf[\"manager_price_mean\"] = df[\"manager_price_sum\"] / df[\"manager_bedrooms_sum\"]\ndf[\"earn_everyday\"] = df[\"earn_all\"] / df[\"manager_active\"]\n\ndf[\"earn_all_rt\"] = df[\"earn_all\"] / df[\"manager_price_sum\"]\ndf[\"manager_price_\"] = df[\"manager_price_sum\"] / df[\"manager_active\"]\ndf = merge_mean(df, [\"manager_id\"], \"created_hour\", \"manager_post_hour_mean\")\n\ndf = merge_median(df, [\"manager_id\"], \"longitude\", \"manager_longitude_median\")\ndf = merge_median(df, [\"manager_id\"], \"latitude\", \"manager_latitude_median\")\n\ndf[\"same\"] = list(map(lambda a, b, c, d, e: str(a) + str(b) + str(c) + str(d) + str(e), \n                      df[\"manager_id\"], df[\"bedrooms\"], df[\"bathrooms\"], df[\"building_id\"], \n                      df[\"features\"]))\nsame_count = df[\"same\"].value_counts()\ndf[\"same_count\"] = list(map(lambda x: same_count[x], df[\"same\"]))\n\ndf = merge_count(df, [\"jwd_class\"], \"listing_id\", \"listing_num_jwd\")\ndf[\"building_listing_num_jwd_rt\"] = df[\"building_num_jwd\"] / df[\"listing_num_jwd\"]\ndf = merge_median(df, [\"time\"], \"price\", \"price_today\")\ndf = merge_median(df, [\"created_month\"], \"price\", \"price_today_month\")\ndf[\"price_rt_jwd\"] = df[\"price\"] / df[\"type_jwd_price_mean\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Magic Feature"},{"metadata":{},"cell_type":"markdown","source":"В данном соревновании помимо json файлов представлен набор фотографий, который практически бесполезен за исключением дат, когда сделаны фотографии. Набор данных с указанием id предложения и дат фотографий выложен одним из грандмастеров kaggle и его применение позволяет улучшить score на 0.14 https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/discussion/31870"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_date = pd.read_csv(\"../input/twosigma-magic-feature/listing_image_time.csv\")\nimage_date.columns = [\"listing_id\", \"image_time_stamp\"]\n\ndf = pd.merge(df, image_date, on=\"listing_id\", how=\"left\")\n\ndf[\"time\"] = df.image_time_stamp.apply(lambda x:datetime.datetime.fromtimestamp(x).strftime('%c'))\ndf[\"time\"] = pd.to_datetime(df[\"time\"])\ndf[\"time_month\"] = df[\"time\"].dt.month\ndf[\"time_day\"] = df[\"time\"].dt.day\ndf[\"time_hour\"] = df[\"time\"].dt.hour\ndf[\"time_weekday\"] = df[\"time\"].dt.weekday\ndf[\"time_week\"] = df[\"time\"].dt.week","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TF-IDF для features"},{"metadata":{},"cell_type":"markdown","source":"Для извлечения текстовых признаков были перепробованы различные методы такие как FastText и doc2vec. Но лучший  реузльтат показал TF-IDF.  Применение TF-IDF и признаки pos, manager_level_low, manager_level_medium, manager_level_high  и density заимствованы у  https://www.kaggle.com/chengzhan/xgb0415 "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df[df['listing_id'].isin(listing_train)]\ndf_test = df[df['listing_id'].isin(listing_test)]\n\ndf_train['features'] = df_train[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\ndf_test['features'] = df_test[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n\ntfidf = CountVectorizer(stop_words='english', max_features=200)\ntr_sparse = tfidf.fit_transform(df_train[\"features\"])\nte_sparse = tfidf.transform(df_test[\"features\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### manager_level_low, manager_level_medium, manager_level_high"},{"metadata":{},"cell_type":"markdown","source":"Создание данных признаков занимает досточно большое количество времени, поэтому используется датасет с уже имеющимися данными признаками."},{"metadata":{"trusted":true},"cell_type":"code","source":"# index=list(range(train_df.shape[0]))\n# random.shuffle(index)\n# a=[np.nan]*len(train_df)\n# b=[np.nan]*len(train_df)\n# c=[np.nan]*len(train_df)\n\n# for i in range(5):\n#     building_level={}\n#     for j in train_df['manager_id'].values:\n#         building_level[j]=[0,0,0]\n    \n#     test_index=index[int((i*train_df.shape[0])/5):int(((i+1)*train_df.shape[0])/5)]\n#     train_index=list(set(index).difference(test_index))\n    \n#     for j in train_index:\n#         temp=train_df.iloc[j]\n#         if temp['interest_level']=='low':\n#             building_level[temp['manager_id']][0]+=1\n#         if temp['interest_level']=='medium':\n#             building_level[temp['manager_id']][1]+=1\n#         if temp['interest_level']=='high':\n#             building_level[temp['manager_id']][2]+=1\n            \n#     for j in test_index:\n#         temp=train_df.iloc[j]\n#         if sum(building_level[temp['manager_id']])!=0:\n#             a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n#             b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n#             c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n            \n# train_df['manager_level_low']=a\n# train_df['manager_level_medium']=b\n# train_df['manager_level_high']=c\n\n# a=[]\n# b=[]\n# c=[]\n# building_level={}\n# for j in train_df['manager_id'].values:\n#     building_level[j]=[0,0,0]\n\n# for j in range(train_df.shape[0]):\n#     temp=train_df.iloc[j]\n#     if temp['interest_level']=='low':\n#         building_level[temp['manager_id']][0]+=1\n#     if temp['interest_level']=='medium':\n#         building_level[temp['manager_id']][1]+=1\n#     if temp['interest_level']=='high':\n#         building_level[temp['manager_id']][2]+=1\n\n# for i in test_df['manager_id'].values:\n#     if i not in building_level.keys():\n#         a.append(np.nan)\n#         b.append(np.nan)\n#         c.append(np.nan)\n#     else:\n#         a.append(building_level[i][0]*1.0/sum(building_level[i]))\n#         b.append(building_level[i][1]*1.0/sum(building_level[i]))\n#         c.append(building_level[i][2]*1.0/sum(building_level[i]))\n# test_df['manager_level_low']=a\n# test_df['manager_level_medium']=b\n# test_df['manager_level_high']=c","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Создание тренировочного и тестового набора данных"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"interest_level\"] = df_train[\"interest_level\"].map({'high':0, 'medium':1, 'low':2})\ntrain_y = df_train[\"interest_level\"]\n\ndf_train = df_train.drop(['created','interest_level','building_mean','room_sum',\n                         'type_jwd_price_mean_rt','building_manager','building_num_jwd',\n                         'display_count','time','features','type_jwd_price_mean','photos',\n                         'same','description','pos','fangxing_mean','area_admin2'],axis=1)\n\ndf_test = df_test.drop(['created','interest_level','building_mean','room_sum',\n                         'type_jwd_price_mean_rt','building_manager','building_num_jwd',\n                         'display_count','time','features','type_jwd_price_mean','photos',\n                         'same','description','pos','fangxing_mean','area_admin2'],axis=1)\n\ntrain_X = sparse.hstack([df_train, tr_sparse]).tocsr()\ntest_X = sparse.hstack([df_test, te_sparse]).tocsr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Создание модели"},{"metadata":{},"cell_type":"markdown","source":"## Оптимизация"},{"metadata":{},"cell_type":"markdown","source":"Для поиска оптимальных параметров использовалась библиотека hyperopt."},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import hp\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import randint\n\n# XGB parameters\nxgb_space = {\n    \"eta\": hp.quniform(\"eta\", 0.025, 1, 0.025),\n    \"max_depth\": hp.choice(\"max_depth\", np.arange(1, 20, dtype=int)),\n    \"min_child_weight\": hp.choice(\"min_child_weight\", np.arange(0, 100, dtype=int)),\n    \"colsample_bytree\": hp.quniform(\"colsample_bytree\", 0.025, 1, 0.025),\n    \"subsample\": hp.uniform(\"subsample\", 0.8, 1),\n    \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 2000, dtype=int)),\n    \"gamma\": hp.choice(\"gamma\", np.arange(0, 100, dtype=int)),\n    #     'tree_method':      'gpu_hist',\n    \"objective\": \"multi:softprob\",\n}\nxgb_fit_params = {\n    \"eval_metric\": \"mlogloss\",\n    \"early_stopping_rounds\": 30,\n    #'num_class': 3,\n    \"verbose\": False,\n}\nxgb_params = dict()\nxgb_params[\"reg_params\"] = xgb_space\nxgb_params[\"fit_params\"] = xgb_fit_params\n\n# LightGBM parameters\nlgb_space = space = {\n    \"class_weight\": hp.choice(\"class_weight\", [None, \"balanced\"]),\n    \"boosting_type\": hp.choice(\n        \"boosting_type\",\n        [\n            {\n                \"boosting_type\": \"gbdt\",\n                \"subsample\": hp.uniform(\"gdbt_subsample\", 0.5, 1),\n            },\n            {\n                \"boosting_type\": \"dart\",\n                \"subsample\": hp.uniform(\"dart_subsample\", 0.5, 1),\n            },\n            {\"boosting_type\": \"goss\"},\n        ],\n    ),\n    \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 2000, dtype=int)),\n    \"num_leaves\": hp.choice(\"num_leaves\", np.arange(10, 150, dtype=int)),\n    \"learning_rate\": hp.quniform(\"learning_rate\", 0.025, 1, 0.025),\n    \"subsample_for_bin\": hp.choice(\n        \"subsample_for_bin\", np.arange(2000, 300000, dtype=int)\n    ),\n    \"min_child_samples\": hp.choice(\"min_child_samples\", np.arange(20, 500, dtype=int)),\n    \"reg_alpha\": hp.quniform(\"reg_alpha\", 0.025, 1, 0.025),\n    \"reg_lambda\": hp.quniform(\"reg_lambda\", 0.025, 1, 0.025),\n    \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n    \"colsample_bytree\": hp.quniform(\"colsample_bytree\", 0.025, 1, 0.025),\n}\n\nlgb_fit_params = {\n    \"eval_metric\": \"multi_logloss\",\n    \"early_stopping_rounds\": 30,\n    #    'num_class': 3,\n    \"verbose\": False,\n}\nlgb_params = dict()\nlgb_params[\"reg_params\"] = lgb_space\nlgb_params[\"fit_params\"] = lgb_fit_params\n\n# CatBoost parameters\nctb_space = {\n    \"learning_rate\": hp.quniform(\"learning_rate\", 0.025, 1, 0.025),\n    \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 16, dtype=int)),\n    #'colsample_bylevel': hp.quniform('colsample_bylevel', 0.025, 1, 0.025),\n    \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 2000, dtype=int)),\n    \"l2_leaf_reg\": hp.choice(\"l2_leaf_reg\", np.arange(2, 100, dtype=int)),\n    \"border_count\": hp.choice(\"border_count\", np.arange(5, 200, dtype=int)),\n    \"eval_metric\": \"MultiClass\",\n}\nctb_fit_params = {\n    \"early_stopping_rounds\": 30,\n    \"verbose\": False,\n    #     'task_type': 'GPU'\n    #    'classes_count': 3\n}\nctb_params = dict()\nctb_params[\"reg_params\"] = ctb_space\nctb_params[\"fit_params\"] = ctb_fit_params\n\n# RandomForest parameters\nrf_space = {\n    \"max_depth\": hp.choice(\"max_depth\", np.arange(1, 30, dtype=int)),\n    \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 2000, dtype=int)),\n    \"max_features\": hp.choice(\"max_features\", np.arange(1, 150)),\n    \"criterion\": hp.choice(\"criterion\", [\"gini\", \"entropy\"]),\n    \"min_samples_split\": hp.choice(\"min_samples_split\", np.arange(2, 100, dtype=int)),\n    \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(1, 20, dtype=int)),\n}\nrf_fit_params = {\"verbose\": None}\n\nrf_params = dict()\nrf_params[\"reg_params\"] = rf_space\nrf_params[\"fit_params\"] = rf_fit_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGB parameters\nxgb_space_reg = {\n    \"eta\": hp.quniform(\"eta\", 0.025, 1, 0.025),\n    \"max_depth\": hp.choice(\"max_depth\", np.arange(1, 20, dtype=int)),\n    \"min_child_weight\": hp.choice(\"min_child_weight\", np.arange(0, 100, dtype=int)),\n    \"colsample_bytree\": hp.quniform(\"colsample_bytree\", 0.025, 1, 0.025),\n    \"subsample\": hp.uniform(\"subsample\", 0.8, 1),\n    \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 2000, dtype=int)),\n    \"gamma\": hp.choice(\"gamma\", np.arange(0, 100, dtype=int)),\n    #     'tree_method':      'gpu_hist',\n    \"objective\": \"reg:squarederror\",\n}\nxgb_fit_params_reg = {\n    \"eval_metric\": \"rmse\",\n    \"early_stopping_rounds\": 30,\n    \"verbose\": False,\n}\nxgb_params_reg = dict()\nxgb_params_reg[\"reg_params\"] = xgb_space_reg\nxgb_params_reg[\"fit_params\"] = xgb_fit_params_reg\n\n# LightGBM parameters\nlgb_space_reg = space = {\n    \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 2000, dtype=int)),\n    \"num_leaves\": hp.choice(\"num_leaves\", np.arange(10, 150, dtype=int)),\n    \"learning_rate\": hp.quniform(\"learning_rate\", 0.025, 1, 0.025),\n    \"subsample_for_bin\": hp.choice(\n        \"subsample_for_bin\", np.arange(2000, 300000, dtype=int)\n    ),\n    \"min_child_samples\": hp.choice(\"min_child_samples\", np.arange(20, 500, dtype=int)),\n    \"reg_alpha\": hp.quniform(\"reg_alpha\", 0.025, 1, 0.025),\n    \"reg_lambda\": hp.quniform(\"reg_lambda\", 0.025, 1, 0.025),\n    \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n    \"colsample_bytree\": hp.quniform(\"colsample_bytree\", 0.025, 1, 0.025),\n}\n\nlgb_fit_params_reg = {\n    \"eval_metric\": \"l2\",\n    \"early_stopping_rounds\": 30,\n    \"verbose\": False,\n}\nlgb_params_reg = dict()\nlgb_params_reg[\"reg_params\"] = lgb_space_reg\nlgb_params_reg[\"fit_params\"] = lgb_fit_params_reg\n\n# CatBoost parameters\nctb_space_reg = {\n    \"learning_rate\": hp.quniform(\"learning_rate\", 0.025, 1, 0.025),\n    \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 16, dtype=int)),\n    #'colsample_bylevel': hp.quniform('colsample_bylevel', 0.025, 1, 0.025),\n    \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 2000, dtype=int)),\n    \"l2_leaf_reg\": hp.choice(\"l2_leaf_reg\", np.arange(2, 100, dtype=int)),\n    \"border_count\": hp.choice(\"border_count\", np.arange(5, 200, dtype=int)),\n    \"eval_metric\": \"RMSE\",\n}\nctb_fit_params_reg = {\"early_stopping_rounds\": 30, \"verbose\": False}\nctb_params_reg = dict()\nctb_params_reg[\"reg_params\"] = ctb_space_reg\nctb_params_reg[\"fit_params\"] = ctb_fit_params_reg\n\n# RandomForest parameters\nrf_space_reg = {\n    \"max_depth\": hp.choice(\"max_depth\", np.arange(1, 30, dtype=int)),\n    \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 2000, dtype=int)),\n    \"max_features\": hp.choice(\"max_features\", np.arange(1, 150)),\n    \"min_samples_split\": hp.choice(\"min_samples_split\", np.arange(2, 100, dtype=int)),\n    \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(1, 20, dtype=int)),\n}\nrf_fit_params_reg = {\"verbose\": None}\n\nrf_params_reg = dict()\nrf_params_reg[\"reg_params\"] = rf_space_reg\nrf_params_reg[\"fit_params\"] = rf_fit_params_reg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class HPOpt(object):\n    def __init__(self, x_train, x_test, y_train, y_test, x_valid=None, y_valid=None):\n        self.x_train = x_train\n        self.x_test = x_test\n        self.y_train = y_train\n        self.y_test = y_test\n        self.valid = None\n        if (x_valid is not None) and (y_valid is not None):\n            self.x_valid = x_valid\n            self.y_valid = y_valid\n            self.valid = True\n\n    def process(self, fn_name, space, trials, algo, max_evals, random_state=42, cv=3):\n        self.random_state = random_state\n        self.cv = cv\n        if self.valid == True:\n            space[\"fit_params\"][\"eval_set\"] = [\n                (self.x_train, self.y_train),\n                (self.x_valid, self.y_valid),\n            ]\n        else:\n            space[\"fit_params\"][\"eval_set\"] = [(self.x_train, self.y_train)]\n        self.fit_params = space[\"fit_params\"]\n        fn = getattr(self, fn_name)\n        result = fmin(\n            fn=fn,\n            space=space[\"reg_params\"],\n            algo=algo,\n            max_evals=max_evals,\n            trials=trials,\n        )\n\n        return result\n\n    def xgb_reg(self, para):\n        model = xgb.XGBRegressor(random_state=self.random_state, **para)\n        score = -cross_val_score(\n            model,\n            self.x_train,\n            self.y_train,\n            cv=self.cv,\n            scoring=\"neg_mean_squared_error\",\n            fit_params=self.fit_params,\n        ).mean()\n        return score\n\n    def lgb_reg(self, para):\n        model = lgb.LGBMRegressor(random_state=self.random_state, **para)\n\n        score = -cross_val_score(\n            model,\n            self.x_train,\n            self.y_train,\n            cv=self.cv,\n            scoring=\"neg_mean_squared_error\",\n            fit_params=self.fit_params,\n        ).mean()\n        return score\n\n    def ctb_reg(self, para):\n        model = ctb.CatBoostRegressor(random_state=self.random_state, **para)\n        score = -cross_val_score(\n            model,\n            self.x_train,\n            self.y_train,\n            cv=self.cv,\n            scoring=\"neg_mean_squared_error\",\n            fit_params=self.fit_params,\n        ).mean()\n        return score\n\n    def rf_reg(self, para):\n        model = RandomForestRegressor(random_state=self.random_state, **para)\n        score = -cross_val_score(\n            model,\n            self.x_train,\n            self.y_train,\n            cv=self.cv,\n            scoring=\"neg_mean_squared_error\",\n        ).mean()\n        return score\n\n    def et_reg(self, para):\n        model = ExtraTreesRegressor(random_state=self.random_state, **para)\n        score = -cross_val_score(\n            model,\n            self.x_train,\n            self.y_train,\n            cv=self.cv,\n            scoring=\"neg_mean_squared_error\",\n        ).mean()\n        return score\n\n    def xgb_clf(self, para):\n        model = xgb.XGBClassifier(random_state=self.random_state, **para)\n        score = -cross_val_score(\n            model,\n            self.x_train,\n            self.y_train,\n            cv=self.cv,\n            scoring=\"neg_log_loss\",\n            fit_params=self.fit_params,\n        ).mean()\n        return score\n\n    def lgb_clf(self, para):\n        model = lgb.LGBMClassifier(random_state=self.random_state, **para)\n        score = -cross_val_score(\n            model,\n            self.x_train,\n            self.y_train,\n            cv=self.cv,\n            scoring=\"neg_log_loss\",\n            fit_params=self.fit_params,\n        ).mean()\n        return score\n\n    def ctb_clf(self, para):\n        model = ctb.CatBoostClassifier(\n            random_state=self.random_state, task_type=\"GPU\", **para\n        )\n        score = -cross_val_score(\n            model,\n            self.x_train,\n            self.y_train,\n            cv=self.cv,\n            scoring=\"neg_log_loss\",\n            fit_params=self.fit_params,\n        ).mean()\n        return score\n\n    def rf_clf(self, para):\n        model = RandomForestClassifier(random_state=self.random_state, **para)\n        score = -cross_val_score(\n            model, self.x_train, self.y_train, cv=self.cv, scoring=\"neg_log_loss\",\n        ).mean()\n        return score\n\n    def et_clf(self, para):\n        model = ExtraTreesClassifier(random_state=self.random_state, **para)\n        score = -cross_val_score(\n            model, self.x_train, self.y_train, cv=self.cv, scoring=\"neg_log_loss\",\n        ).mean()\n        return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n     train_X, train_y, test_size=0.15, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Процесс поиска опитимальных значений параметров занимает несколько часов. Поэтому ниже предлагаются найденные значения параметров."},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# obj = HPOpt(X_train, X_test, y_train, y_test)\n\n# lgb_opt = obj.process(fn_name='lgb_clf', space=lgb_params, trials=Trials(), algo=tpe.suggest, max_evals=300, random_state=42, cv=5)\n# xgb_opt = obj.process(fn_name='xgb_clf', space=xgb_params, trials=Trials(), algo=tpe.suggest, max_evals=350, random_state=42, cv=5)\n# ctb_opt = obj.process(fn_name='ctb_clf', space=ctb_params, trials=Trials(), algo=tpe.suggest, max_evals=300, random_state=42, cv=5)\n# lgb_opt_reg = obj.process(fn_name='lgb_reg', space=lgb_params_reg, trials=Trials(), algo=tpe.suggest, max_evals=200, random_state=42, cv=5)\n# xgb_opt_reg = obj.process(fn_name='xgb_reg', space=xgb_params_reg, trials=Trials(), algo=tpe.suggest, max_evals=200, random_state=42, cv=5)\n# ctb_opt_reg = obj.process(fn_name='ctb_reg', space=ctb_params_reg, trials=Trials(), algo=tpe.suggest, max_evals=100, random_state=42, cv=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stacking"},{"metadata":{},"cell_type":"markdown","source":"Для стэкинга моделей использовалась библиотека pystacknet."},{"metadata":{"trusted":true},"cell_type":"code","source":"# !git clone https://github.com/h2oai/pystacknet\n# !cd pystacknet\n# !python setup.py install","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_opt = {\n    \"colsample_bytree\": 0.275,\n    \"learning_rate\": 0.025,\n    \"max_depth\": 9,\n    \"min_child_samples\": 11,\n    \"n_estimators\": 1367,\n    \"num_leaves\": 17,\n    \"reg_alpha\": 0.775,\n    \"reg_lambda\": 0.1,\n    \"subsample_for_bin\": 246417,\n}\nxgb_opt = {\n    \"colsample_bytree\": 0.30000000000000004,\n    \"eta\": 0.0325,\n    \"gamma\": 4,\n    \"max_depth\": 9,\n    \"min_child_weight\": 10,\n    \"n_estimators\": 1285,\n    \"subsample\": 0.8981184721176737,\n}\nlgb_opt_reg = {\n    \"colsample_bytree\": 0.15000000000000002,\n    \"learning_rate\": 0.0325,\n    \"max_depth\": 7,\n    \"min_child_samples\": 234,\n    \"n_estimators\": 1355,\n    \"num_leaves\": 73,\n    \"reg_alpha\": 0.525,\n    \"reg_lambda\": 0.875,\n    \"subsample_for_bin\": 81298,\n}\nxgb_opt_reg = {\n    \"colsample_bytree\": 0.47500000000000003,\n    \"eta\": 0.0325,\n    \"gamma\": 4,\n    \"max_depth\": 14,\n    \"min_child_weight\": 32,\n    \"n_estimators\": 326,\n    \"subsample\": 0.9291710070461443,\n}\nlgb_opt_reg1 = {\n    \"colsample_bytree\": 0.30000000000000004,\n    \"learning_rate\": 0.025,\n    \"max_depth\": 12,\n    \"min_child_samples\": 433,\n    \"n_estimators\": 1772,\n    \"num_leaves\": 84,\n    \"reg_alpha\": 0.525,\n    \"reg_lambda\": 0.9750000000000001,\n    \"subsample_for_bin\": 276155,\n}\nxgb_opt_reg1 = {\n    \"colsample_bytree\": 1.0,\n    \"eta\": 0.07500000000000001,\n    \"gamma\": 7,\n    \"max_depth\": 7,\n    \"min_child_weight\": 88,\n    \"n_estimators\": 336,\n    \"subsample\": 0.9703796510413418,\n}\nctb_opt = {'learning_rate': 0.17500000000000002, 'max_depth': 3, 'n_estimators': 903}\n\nctb_opt_reg = {'learning_rate': 0.625, 'max_depth': 9, 'n_estimators': 284}\n\nxgb_opt1 = {\n    \"colsample_bytree\": 0.7,\n    \"eta\": 0.021,\n    \"gamma\": 4,\n    \"max_depth\": 6,\n    \"min_child_weight\": 1,\n    \"n_estimators\": 1285,\n    \"subsample\": 0.7,\n    \"objective\": 'multi:softprob',\n    \"eval_metric\": 'mlogloss',\n}\n\nlgb_opt2 = {'class_weight': None, 'colsample_bytree': 0.325, 'learning_rate': 0.325, 'max_depth': 0, 'min_child_samples': 399, 'n_estimators': 644, 'num_leaves': 33, 'reg_alpha': 0.8, 'reg_lambda': 0.125, 'subsample_for_bin': 297903}\nlgb_opt3 = {'class_weight': None, 'colsample_bytree': 0.65, 'learning_rate': 0.225, 'max_depth': 1, 'min_child_samples': 160, 'n_estimators': 294, 'num_leaves': 84, 'reg_alpha': 0.625, 'reg_lambda': 0.25, 'subsample_for_bin': 93757}\nlgb_opt4 = {'class_weight': None, 'colsample_bytree': 0.5750000000000001, 'learning_rate': 0.225, 'max_depth': 0, 'min_child_samples': 360, 'n_estimators': 845, 'num_leaves': 12, 'reg_alpha': 1.0, 'reg_lambda': 0.07500000000000001, 'subsample_for_bin': 13177}\nlgb_opt5 = {'class_weight': None, 'colsample_bytree': 0.9, 'learning_rate': 0.025, 'max_depth': 10, 'min_child_samples': 423, 'n_estimators': 718, 'num_leaves': 125, 'reg_alpha': 0.17500000000000002, 'reg_lambda': 0.675, 'subsample_for_bin': 183758}\nlgb_opt6 = {'class_weight': None, 'colsample_bytree': 0.525, 'learning_rate': 0.1, 'max_depth': 0, 'min_child_samples': 421, 'n_estimators': 1654, 'num_leaves': 50, 'reg_alpha': 0.875, 'reg_lambda': 0.7250000000000001, 'subsample_for_bin': 7469}\nlgb_opt7 = {'class_weight': None, 'colsample_bytree': 0.925, 'learning_rate': 0.15000000000000002, 'max_depth': 2, 'min_child_samples': 388, 'n_estimators': 451, 'num_leaves': 31, 'reg_alpha': 0.9750000000000001, 'reg_lambda': 0.7250000000000001, 'subsample_for_bin': 256587}\nlgb_opt8 = {'class_weight': None, 'colsample_bytree': 0.17500000000000002, 'learning_rate': 0.05, 'max_depth': 11, 'min_child_samples': 282, 'n_estimators': 509, 'num_leaves': 122, 'reg_alpha': 0.42500000000000004, 'reg_lambda': 0.07500000000000001, 'subsample_for_bin': 190458}\nlgb_opt9 = {'class_weight': None, 'colsample_bytree': 0.5750000000000001, 'learning_rate': 0.025, 'max_depth': 6, 'min_child_samples': 291, 'n_estimators': 739, 'num_leaves': 66, 'reg_alpha': 0.325, 'reg_lambda': 0.5, 'subsample_for_bin': 101043}\nlgb_opt10 = {'class_weight': None, 'colsample_bytree': 0.15000000000000002, 'learning_rate': 0.07500000000000001, 'max_depth': 14, 'min_child_samples': 188, 'n_estimators': 207, 'num_leaves': 32, 'reg_alpha': 0.45, 'reg_lambda': 0.625, 'subsample_for_bin': 246295}\nlgb_opt11 = {'class_weight': None, 'colsample_bytree': 0.07500000000000001, 'learning_rate': 0.025, 'max_depth': 10, 'min_child_samples': 316, 'n_estimators': 1494, 'num_leaves': 48, 'reg_alpha': 0.7000000000000001, 'reg_lambda': 0.17500000000000002, 'subsample_for_bin': 69709}\nlgb_opt12 = {'class_weight': None, 'colsample_bytree': 0.7250000000000001, 'learning_rate': 0.25, 'max_depth': 0, 'min_child_samples': 111, 'n_estimators': 819, 'num_leaves': 17, 'reg_alpha': 0.525, 'reg_lambda': 0.5, 'subsample_for_bin': 110536}\nlgb_opt13 = {'class_weight': None, 'colsample_bytree': 0.30000000000000004, 'learning_rate': 0.05, 'max_depth': 1, 'min_child_samples': 93, 'n_estimators': 1201, 'num_leaves': 32, 'reg_alpha': 1.0, 'reg_lambda': 0.225, 'subsample_for_bin': 32919}\nlgb_opt14 = {'class_weight': None, 'colsample_bytree': 0.42500000000000004, 'learning_rate': 0.05, 'max_depth': 3, 'min_child_samples': 131, 'n_estimators': 953, 'num_leaves': 103, 'reg_alpha': 0.07500000000000001, 'reg_lambda': 0.625, 'subsample_for_bin': 61887}\nlgb_opt15 = {'class_weight': None, 'colsample_bytree': 0.675, 'learning_rate': 0.025, 'max_depth': 10, 'min_child_samples': 243, 'n_estimators': 408, 'num_leaves': 119, 'reg_alpha': 0.17500000000000002, 'reg_lambda': 0.775, 'subsample_for_bin': 17006}\nlgb_opt16 = {'class_weight': None, 'colsample_bytree': 0.5, 'learning_rate': 0.025, 'max_depth': 3, 'min_child_samples': 287, 'n_estimators': 769, 'num_leaves': 50, 'reg_alpha': 0.55, 'reg_lambda': 0.42500000000000004, 'subsample_for_bin': 274633}\nlgb_opt17 = {'class_weight': None, 'colsample_bytree': 0.30000000000000004, 'learning_rate': 0.025, 'max_depth': 2, 'min_child_samples': 111, 'n_estimators': 1877, 'num_leaves': 77, 'reg_alpha': 0.925, 'reg_lambda': 0.8500000000000001, 'subsample_for_bin': 172944}\nlgb_opt18 = {'class_weight': None, 'colsample_bytree': 0.775, 'learning_rate': 0.07500000000000001, 'max_depth': 8, 'min_child_samples': 225, 'n_estimators': 85, 'num_leaves': 105, 'reg_alpha': 0.775, 'reg_lambda': 0.775, 'subsample_for_bin': 174495}\nlgb_opt19 = {'class_weight': None, 'colsample_bytree': 0.25, 'learning_rate': 0.025, 'max_depth': 14, 'min_child_samples': 302, 'n_estimators': 1164, 'num_leaves': 23, 'reg_alpha': 0.42500000000000004, 'reg_lambda': 0.45, 'subsample_for_bin': 167143}\nlgb_opt20 = {'class_weight': None, 'colsample_bytree': 0.9750000000000001, 'learning_rate': 0.07500000000000001, 'max_depth': 15, 'min_child_samples': 281, 'n_estimators': 137, 'num_leaves': 63, 'reg_alpha': 0.525, 'reg_lambda': 0.47500000000000003, 'subsample_for_bin': 9438}\nlgb_opt21 = {'class_weight': None, 'colsample_bytree': 0.35000000000000003, 'learning_rate': 0.025, 'max_depth': 13, 'min_child_samples': 110, 'n_estimators': 394, 'num_leaves': 29, 'reg_alpha': 0.07500000000000001, 'reg_lambda': 0.2, 'subsample_for_bin': 99191}\nlgb_opt22 = {'class_weight': None, 'colsample_bytree': 0.6000000000000001, 'learning_rate': 0.025, 'max_depth': 13, 'min_child_samples': 297, 'n_estimators': 1302, 'num_leaves': 16, 'reg_alpha': 0.225, 'reg_lambda': 0.65, 'subsample_for_bin': 162254}\nlgb_opt23 = {'class_weight': None, 'colsample_bytree': 0.675, 'learning_rate': 0.07500000000000001, 'max_depth': 4, 'min_child_samples': 75, 'n_estimators': 129, 'num_leaves': 28, 'reg_alpha': 0.925, 'reg_lambda': 0.125, 'subsample_for_bin': 88871}\nlgb_opt24 = {'class_weight': None, 'colsample_bytree': 0.25, 'learning_rate': 0.25, 'max_depth': 0, 'min_child_samples': 197, 'n_estimators': 357, 'num_leaves': 11, 'reg_alpha': 0.05, 'reg_lambda': 0.8, 'subsample_for_bin': 40493}\nlgb_opt25 = {'class_weight': \"balanced\", 'colsample_bytree': 0.17500000000000002, 'learning_rate': 0.025, 'max_depth': 10, 'min_child_samples': 97, 'n_estimators': 1806, 'num_leaves': 99, 'reg_alpha': 0.5750000000000001, 'reg_lambda': 0.47500000000000003, 'subsample_for_bin': 14141}\nlgb_opt26 = {'class_weight': None, 'colsample_bytree': 0.07500000000000001, 'learning_rate': 0.05, 'max_depth': 4, 'min_child_samples': 310, 'n_estimators': 414, 'num_leaves': 102, 'reg_alpha': 0.42500000000000004, 'reg_lambda': 0.6000000000000001, 'subsample_for_bin': 276631}\nlgb_opt27 = {'class_weight': None, 'colsample_bytree': 0.675, 'learning_rate': 0.025, 'max_depth': 15, 'min_child_samples': 349, 'n_estimators': 285, 'num_leaves': 63, 'reg_alpha': 0.025, 'reg_lambda': 0.9500000000000001, 'subsample_for_bin': 24238}\nlgb_opt28 = {'class_weight': None, 'colsample_bytree': 0.75, 'learning_rate': 0.025, 'max_depth': 9, 'min_child_samples': 179, 'n_estimators': 1166, 'num_leaves': 33, 'reg_alpha': 0.025, 'reg_lambda': 0.35000000000000003, 'subsample_for_bin': 103664}\nlgb_opt29 = {'class_weight': None, 'colsample_bytree': 0.375, 'learning_rate': 0.05, 'max_depth': 2, 'min_child_samples': 419, 'n_estimators': 943, 'num_leaves': 97, 'reg_alpha': 0.325, 'reg_lambda': 0.525, 'subsample_for_bin': 133484}\nlgb_opt30 = {'class_weight': None, 'colsample_bytree': 0.05, 'learning_rate': 0.17500000000000002, 'max_depth': 0, 'min_child_samples': 20, 'n_estimators': 1374, 'num_leaves': 89, 'reg_alpha': 0.525, 'reg_lambda': 0.30000000000000004, 'subsample_for_bin': 295852}\nlgb_opt31 = {'class_weight': None, 'colsample_bytree': 0.225, 'learning_rate': 0.05, 'max_depth': 14, 'min_child_samples': 359, 'n_estimators': 364, 'num_leaves': 120, 'reg_alpha': 0.75, 'reg_lambda': 0.75, 'subsample_for_bin': 182658}\nlgb_opt32 = {'class_weight': None, 'colsample_bytree': 0.325, 'learning_rate': 0.025, 'max_depth': 14, 'min_child_samples': 17, 'n_estimators': 715, 'num_leaves': 61, 'reg_alpha': 0.2, 'reg_lambda': 0.55, 'subsample_for_bin': 266004}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [\n    [\n        ctb.CatBoostClassifier(iterations=2000, verbose=0, task_type=\"GPU\"),\n        ctb.CatBoostClassifier(verbose=0, task_type=\"GPU\", **ctb_opt),\n        lgb.LGBMClassifier(**lgb_opt),\n        xgb.XGBClassifier(**xgb_opt),\n        xgb.XGBClassifier(**xgb_opt1),\n        lgb.LGBMClassifier(),\n        xgb.XGBClassifier(),\n        lgb.LGBMRegressor(**lgb_opt_reg),\n        xgb.XGBRegressor(**xgb_opt_reg),\n        lgb.LGBMRegressor(**lgb_opt_reg1),\n        xgb.XGBRegressor(**xgb_opt_reg1),\n        lgb.LGBMRegressor(),\n        xgb.XGBRegressor(),\n        ctb.CatBoostRegressor(iterations=2000, verbose=0, task_type=\"GPU\"),\n        ctb.CatBoostRegressor(verbose=0, task_type=\"GPU\", **ctb_opt_reg),\n        lgb.LGBMClassifier(**lgb_opt2),\n        lgb.LGBMClassifier(**lgb_opt3),\n        lgb.LGBMClassifier(**lgb_opt4),\n        lgb.LGBMClassifier(**lgb_opt5),\n        lgb.LGBMClassifier(**lgb_opt6),\n        lgb.LGBMClassifier(**lgb_opt7),\n        lgb.LGBMClassifier(**lgb_opt8),\n        lgb.LGBMClassifier(**lgb_opt9),\n        lgb.LGBMClassifier(**lgb_opt10),\n        lgb.LGBMClassifier(**lgb_opt11),\n        lgb.LGBMClassifier(**lgb_opt12),\n        lgb.LGBMClassifier(**lgb_opt13),\n        lgb.LGBMClassifier(**lgb_opt14),\n        lgb.LGBMClassifier(**lgb_opt15),\n        lgb.LGBMClassifier(**lgb_opt16),\n        lgb.LGBMClassifier(**lgb_opt17),\n        lgb.LGBMClassifier(**lgb_opt18),\n        lgb.LGBMClassifier(**lgb_opt19),\n        lgb.LGBMClassifier(**lgb_opt20),\n        lgb.LGBMClassifier(**lgb_opt21),\n        lgb.LGBMClassifier(**lgb_opt22),\n        lgb.LGBMClassifier(**lgb_opt23),\n        lgb.LGBMClassifier(**lgb_opt24),\n        lgb.LGBMClassifier(**lgb_opt25),\n        lgb.LGBMClassifier(**lgb_opt26),\n        lgb.LGBMClassifier(**lgb_opt27),\n        lgb.LGBMClassifier(**lgb_opt28),\n        lgb.LGBMClassifier(**lgb_opt29),\n        lgb.LGBMClassifier(**lgb_opt30),\n        lgb.LGBMClassifier(**lgb_opt31),\n        lgb.LGBMClassifier(**lgb_opt32),\n    ],\n    [lgb.LGBMClassifier()],\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom pystacknet.pystacknet import StackNetClassifier\n\nmodel = StackNetClassifier(\n    models,\n    metric=\"logloss\",\n    folds=5,\n    restacking=True,\n    use_retraining=True,\n    use_proba=True,\n    random_state=42,\n    verbose=0,\n)\n  \nmodel.fit(train_X, train_y)\npreds = model.predict_proba(test_X)\nout_df = pd.DataFrame(preds)\nout_df.columns = [\"high\", \"medium\", \"low\"]\nout_df[\"listing_id\"] = df_test.listing_id.values\nout_df.to_csv(\"sub_stacking.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Late Submisssion"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/prep-geo/sub.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Самая лучшая единичная модель XGBClassifier показывает score выше, чем лучшая единичная модель на форумах и попадает в ТОП 100. Стекинг моделей попадает в ТОП20.\nДля того, чтобы улучшить результат необходимо обучить пару десятков моделей помимо обученных (нейросети, RandomForest, DecisionTrees, XGBRegressor, CatBoostRegressor, LGBMRegressor), затем применить к ним \"stacking\". Однако это требует огромных затрат времени на обучение."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}