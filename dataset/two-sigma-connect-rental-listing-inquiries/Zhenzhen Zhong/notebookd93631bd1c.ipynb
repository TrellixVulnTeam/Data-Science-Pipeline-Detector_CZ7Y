{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8e84890-da08-0a8d-7b62-e824d3f40682"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\nX_train = pd.read_json(\"../input/train.json\")\nX_test = pd.read_json(\"../input/test.json\")\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8df4d4fe-429e-ad82-c22c-9fa5e4c6967b"},"outputs":[],"source":"X_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ed3fd538-7425-0fec-6a12-c840c50ab6c8"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"83231c83-a367-d26c-e2df-f0d092502022"},"outputs":[],"source":"\nX_train[\"longitude\"]\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c0727119-a154-0414-5523-6a792485af38"},"outputs":[],"source":"X_train[\"latitude\"]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f5950e7c-ffb3-9afe-4013-e458d2de1c67"},"outputs":[],"source":"X_train.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf5ef3b4-319e-3986-0cd0-1f43f3b30793"},"outputs":[],"source":"X_test.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8df9aab9-7cbb-9e1b-05f5-78f90f09cac4"},"outputs":[],"source":"X_test.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8fed030-d022-5817-d4c5-f454f3c07ff9"},"outputs":[],"source":"sample=pd.read_csv(\"../input/sample_submission.csv\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5096028d-42f1-dde3-4c65-d8846bd872d6"},"outputs":[],"source":"sample.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c1869970-924f-a22e-5799-cdb3dd08f850"},"outputs":[],"source":"print(check_output([\"ls\", \"../input/images_sample/\"]).decode(\"utf8\"))\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3c25135a-5a11-6109-99c5-b3adb16dae3b"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport random\nfrom math import exp\nimport xgboost as xgb\n\nrandom.seed(321)\nnp.random.seed(321)\n\nX_train = pd.read_json(\"../input/train.json\")\nX_test = pd.read_json(\"../input/test.json\")\n\ninterest_level_map = {'low': 0, 'medium': 1, 'high': 2}\nX_train['interest_level'] = X_train['interest_level'].apply(lambda x: interest_level_map[x])\nX_test['interest_level'] = -1\n\n#add features\nfeature_transform = CountVectorizer(stop_words='english', max_features=150)\nX_train['features'] = X_train[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.lower().split(\" \")) for i in x]))\nX_test['features'] = X_test[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.lower().split(\" \")) for i in x]))\nfeature_transform.fit(list(X_train['features']) + list(X_test['features']))\n\ntrain_size = len(X_train)\nlow_count = len(X_train[X_train['interest_level'] == 0])\nmedium_count = len(X_train[X_train['interest_level'] == 1])\nhigh_count = len(X_train[X_train['interest_level'] == 2])\n\ndef find_objects_with_only_one_record(feature_name):\n    temp = pd.concat([X_train[feature_name].reset_index(), \n                      X_test[feature_name].reset_index()])\n    temp = temp.groupby(feature_name, as_index = False).count()\n    return temp[temp['index'] == 1]\n\nmanagers_with_one_lot = find_objects_with_only_one_record('manager_id')\nbuildings_with_one_lot = find_objects_with_only_one_record('building_id')\naddresses_with_one_lot = find_objects_with_only_one_record('display_address')\n\nlambda_val = None\nk=5.0\nf=1.0\nr_k=0.01 \ng = 1.0\n\ndef categorical_average(variable, y, pred_0, feature_name):\n    def calculate_average(sub1, sub2):\n        s = pd.DataFrame(data = {\n                                 variable: sub1.groupby(variable, as_index = False).count()[variable],                              \n                                 'sumy': sub1.groupby(variable, as_index = False).sum()['y'],\n                                 'avgY': sub1.groupby(variable, as_index = False).mean()['y'],\n                                 'cnt': sub1.groupby(variable, as_index = False).count()['y']\n                                 })\n                                 \n        tmp = sub2.merge(s.reset_index(), how='left', left_on=variable, right_on=variable) \n        del tmp['index']                       \n        tmp.loc[pd.isnull(tmp['cnt']), 'cnt'] = 0.0\n        tmp.loc[pd.isnull(tmp['cnt']), 'sumy'] = 0.0\n\n        def compute_beta(row):\n            cnt = row['cnt'] if row['cnt'] < 200 else float('inf')\n            return 1.0 / (g + exp((cnt - k) / f))\n            \n        if lambda_val is not None:\n            tmp['beta'] = lambda_val\n        else:\n            tmp['beta'] = tmp.apply(compute_beta, axis = 1)\n            \n        tmp['adj_avg'] = tmp.apply(lambda row: (1.0 - row['beta']) * row['avgY'] + row['beta'] * row['pred_0'],\n                                   axis = 1)\n                                   \n        tmp.loc[pd.isnull(tmp['avgY']), 'avgY'] = tmp.loc[pd.isnull(tmp['avgY']), 'pred_0']\n        tmp.loc[pd.isnull(tmp['adj_avg']), 'adj_avg'] = tmp.loc[pd.isnull(tmp['adj_avg']), 'pred_0']\n        tmp['random'] = np.random.uniform(size = len(tmp))\n        tmp['adj_avg'] = tmp.apply(lambda row: row['adj_avg'] *(1 + (row['random'] - 0.5) * r_k),\n                                   axis = 1)\n    \n        return tmp['adj_avg'].ravel()\n     \n    #cv for training set \n    k_fold = StratifiedKFold(5)\n    X_train[feature_name] = -999 \n    for (train_index, cv_index) in k_fold.split(np.zeros(len(X_train)),\n                                                X_train['interest_level'].ravel()):\n        sub = pd.DataFrame(data = {variable: X_train[variable],\n                                   'y': X_train[y],\n                                   'pred_0': X_train[pred_0]})\n            \n        sub1 = sub.iloc[train_index]        \n        sub2 = sub.iloc[cv_index]\n        \n        X_train.loc[cv_index, feature_name] = calculate_average(sub1, sub2)\n    \n    #for test set\n    sub1 = pd.DataFrame(data = {variable: X_train[variable],\n                                'y': X_train[y],\n                                'pred_0': X_train[pred_0]})\n    sub2 = pd.DataFrame(data = {variable: X_test[variable],\n                                'y': X_test[y],\n                                'pred_0': X_test[pred_0]})\n    X_test.loc[:, feature_name] = calculate_average(sub1, sub2)                               \n\ndef transform_data(X):\n    #add features    \n    feat_sparse = feature_transform.transform(X[\"features\"])\n    vocabulary = feature_transform.vocabulary_\n    del X['features']\n    X1 = pd.DataFrame([ pd.Series(feat_sparse[i].toarray().ravel()) for i in np.arange(feat_sparse.shape[0]) ])\n    X1.columns = list(sorted(vocabulary.keys()))\n    X = pd.concat([X.reset_index(), X1.reset_index()], axis = 1)\n    del X['index']\n    \n    X[\"num_photos\"] = X[\"photos\"].apply(len)\n    X['created'] = pd.to_datetime(X[\"created\"])\n    X[\"num_description_words\"] = X[\"description\"].apply(lambda x: len(x.split(\" \")))\n    X['price_per_bed'] = X['price'] / X['bedrooms']    \n    X['price_per_bath'] = X['price'] / X['bathrooms']\n    X['price_per_room'] = X['price'] / (X['bathrooms'] + X['bedrooms'] )\n    \n    X['low'] = 0\n    X.loc[X['interest_level'] == 0, 'low'] = 1\n    X['medium'] = 0\n    X.loc[X['interest_level'] == 1, 'medium'] = 1\n    X['high'] = 0\n    X.loc[X['interest_level'] == 2, 'high'] = 1\n    \n    X['display_address'] = X['display_address'].apply(lambda x: x.lower().strip())\n    X['street_address'] = X['street_address'].apply(lambda x: x.lower().strip())\n    \n    X['pred0_low'] = low_count * 1.0 / train_size\n    X['pred0_medium'] = medium_count * 1.0 / train_size\n    X['pred0_high'] = high_count * 1.0 / train_size\n    \n    X.loc[X['manager_id'].isin(managers_with_one_lot['manager_id'].ravel()), \n          'manager_id'] = \"-1\"\n    X.loc[X['building_id'].isin(buildings_with_one_lot['building_id'].ravel()), \n          'building_id'] = \"-1\"\n    X.loc[X['display_address'].isin(addresses_with_one_lot['display_address'].ravel()), \n          'display_address'] = \"-1\"\n          \n    return X\n\ndef normalize_high_cordiality_data():\n    high_cardinality = [\"building_id\", \"manager_id\"]\n    for c in high_cardinality:\n        categorical_average(c, \"medium\", \"pred0_medium\", c + \"_mean_medium\")\n        categorical_average(c, \"high\", \"pred0_high\", c + \"_mean_high\")\n\ndef transform_categorical_data():\n    categorical = ['building_id', 'manager_id', \n                   'display_address', 'street_address']\n                   \n    for f in categorical:\n        encoder = LabelEncoder()\n        encoder.fit(list(X_train[f]) + list(X_test[f])) \n        X_train[f] = encoder.transform(X_train[f].ravel())\n        X_test[f] = encoder.transform(X_test[f].ravel())\n                  \n\ndef remove_columns(X):\n    columns = [\"photos\", \"pred0_high\", \"pred0_low\", \"pred0_medium\",\n               \"description\", \"low\", \"medium\", \"high\",\n               \"interest_level\", \"created\"]\n    for c in columns:\n        del X[c]\n\nprint(\"Starting transformations\")        \nX_train = transform_data(X_train)    \nX_test = transform_data(X_test) \ny = X_train['interest_level'].ravel()\n\nprint(\"Normalizing high cordiality data...\")\nnormalize_high_cordiality_data()\ntransform_categorical_data()\n\nremove_columns(X_train)\nremove_columns(X_test)\n\nprint(\"Start fitting...\")\n\nparam = {}\nparam['objective'] = 'multi:softprob'\nparam['eta'] = 0.02\nparam['max_depth'] = 4\nparam['silent'] = 1\nparam['num_class'] = 3\nparam['eval_metric'] = \"mlogloss\"\nparam['min_child_weight'] = 1\nparam['subsample'] = 0.7\nparam['colsample_bytree'] = 0.7\nparam['seed'] = 321\nparam['nthread'] = 8\nnum_rounds = 2000\n\nxgtrain = xgb.DMatrix(X_train, label=y)\nclf = xgb.train(param, xgtrain, num_rounds)\n\nprint(\"Fitted\")\n\ndef prepare_submission(model):\n    xgtest = xgb.DMatrix(X_test)\n    preds = model.predict(xgtest)    \n    sub = pd.DataFrame(data = {'listing_id': X_test['listing_id'].ravel()})\n    sub['low'] = preds[:, 0]\n    sub['medium'] = preds[:, 1]\n    sub['high'] = preds[:, 2]\n    sub.to_csv(\"submission.csv\", index = False, header = True)\n\nprepare_submission(clf)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a72b86f3-1ecb-29d8-78e1-492c44bf55d2"},"outputs":[],"source":"def prepare_submission(model):\n    xgtest = xgb.DMatrix(X_test)\n    preds = model.predict(xgtest)    \n    sub = pd.DataFrame(data = {'listing_id': X_test['listing_id'].ravel()})\n    sub['low'] = preds[:, 0]\n    sub['medium'] = preds[:, 1]\n    sub['high'] = preds[:, 2]\n    sub.to_csv(\"submission.csv\", index = False, header = True)\n\nprepare_submission(clf)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd21cd1a-3274-e91e-04d6-0371e7afb427"},"outputs":[],"source":"import os\nimport subprocess as sub\nfrom os import listdir\nfrom os.path import isfile, join\nonlyfiles = [f for f in listdir('../input/images_sample/6811957/') if isfile(join('../input/images_sample/6811957/', f))]\nprint (onlyfiles)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f7c29c9f-dd95-4d19-280f-92bcbb963241"},"outputs":[],"source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimg=[]\nfor i in range (0,5):\n    img.append(mpimg.imread('../input/images_sample/6811957/'+onlyfiles[i]))\n    plt.imshow(img[i])\n    fig = plt.figure()\n    a=fig.add_subplot()\n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d1ebcedc-a90c-3c47-5542-0440b1edae8c"},"outputs":[],"source":"\n\nX_train.dropna(subset = ['interest_level'])\nX_train.shape\nprint (X_train['interest_level'])\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1a90927-22e5-3524-2734-0b55919228e7"},"outputs":[],"source":"\ngrouped = X_train.groupby(['interest_level'])\nprint (grouped.size())\n#probability\nprint (grouped.size()/len(X_train))\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ccf981bb-3123-86e9-a4bc-1a2964478423"},"outputs":[],"source":"from sklearn.model_selection import train_test_split\nX_train[\"num_photos\"] = X_train[\"photos\"].apply(len)\nX_train[\"num_features\"] = X_train[\"features\"].apply(len)\nX_train[\"num_description_words\"] = X_train[\"description\"].apply(lambda x: len(x.split(\" \")))\nX_train[\"created\"] = pd.to_datetime(X_train[\"created\"])\nX_train[\"created_year\"] = X_train[\"created\"].dt.year\nX_train[\"created_month\"] = X_train[\"created\"].dt.month\nX_train[\"created_day\"] = X_train[\"created\"].dt.day\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"50cb373d-52ce-1f1e-891c-a7267fe06c6a"},"outputs":[],"source":"\nX_test[\"num_photos\"] = X_test[\"photos\"].apply(len)\nX_test[\"num_features\"] = X_test[\"features\"].apply(len)\nX_test[\"num_description_words\"] = X_test[\"description\"].apply(lambda x: len(x.split(\" \")))\nX_test[\"created\"] = pd.to_datetime(X_test[\"created\"])\nX_test[\"created_year\"] = X_test[\"created\"].dt.year\nX_test[\"created_month\"] = X_test[\"created\"].dt.month\nX_test[\"created_day\"] = X_test[\"created\"].dt.day\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d4da872d-3caf-e565-df5f-0662fa21b93c"},"outputs":[],"source":"num_feats = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",\n             \"num_photos\", \"num_features\", \"num_description_words\",\n             \"created_year\", \"created_month\", \"created_day\"]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d83c1d92-c9d5-d60e-e5ab-a5cd8ed206db"},"outputs":[],"source":"import matplotlib.pyplot as plt\n%matplotlib inline\nlongitude=X_train[\"longitude\"]\nlatitude=X_train[\"latitude\"]\nlisting_id=X_train[\"listing_id\"]\nplt.scatter(longitude, latitude)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b32367a5-918a-b26f-1f10-acdcb2edb7af"},"outputs":[],"source":"plt.scatter(listing_id, latitude)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d65886ac-dd54-41bf-546d-582268fc530d"},"outputs":[],"source":"plt.scatter(listing_id, longitude)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"88382285-644c-a79a-b6dd-1ed79c1af9c0"},"outputs":[],"source":"X_train3=X_train[X_train[\"latitude\"]==0]\nX_train3"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7f33be87-251e-3fc4-5d78-b6c3449f1795"},"outputs":[],"source":"X_train2=X_train[X_train[\"longitude\"]!=0]\nX_train2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c19babc2-0450-2cc5-83b2-a7b9e4661d1e"},"outputs":[],"source":"plt.scatter(X_train2[\"longitude\"], X_train2[\"latitude\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0b8693c-9aea-fe9e-5780-6a40efea2f55"},"outputs":[],"source":"X_train4=X_train2[X_train2[\"longitude\"]>-110]\nplt.scatter(X_train4[\"longitude\"], X_train4[\"latitude\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5f9ee04e-013e-a832-97c4-9c4c7e6fb157"},"outputs":[],"source":"X_train5=X_train2[X_train2[\"longitude\"]<-110]\nX_train5[\"interest_level\"]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d2c3d987-e8ae-ed91-d709-7f8551fadade"},"outputs":[],"source":"X_train5.T"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"21b8cc06-bd4a-f61e-ccd9-130d1f7c98ae"},"outputs":[],"source":"#Preprocessing\nfrom wordcloud import WordCloud\ntext = ''\ntext_dispadd = ''\ntext_street = ''\ntext_desc =''\nfor ind, row in train.iterrows():\n    for feature in row['features']:\n        text = \" \".join([text, \"_\".join(feature.strip().split(\" \"))])\n    text_dispadd = \" \".join([text_dispadd,\"_\".join(row['display_address'].strip().split(\" \"))])\n    text_street = \" \".join([text_street, row['street_address']])\n    text_desc=\" \".join([text_desc, row['description']])\ntext = text.strip()\ntext_dispadd = text_dispadd.strip()\ntext_street = text_street.strip()\ntext_desc = text_desc.strip()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b63ff4b0-8f6a-4707-0290-cdb52e735a28"},"outputs":[],"source":"list(X_train.columns.values)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"799162bb-950f-9a1b-a831-ff0c110d3263"},"outputs":[],"source":"target_num_map = {'high':0, 'medium':1, 'low':2}\nX_train[\"label\"] = X_train[\"interest_level\"].apply(lambda x: target_num_map[x])\nX_train.head()\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e64ada5-9d46-deeb-7b39-cb9ced66de95"},"outputs":[],"source":"\ny = X_train[\"label\"]\n\nX_train2, X_val, y_train2, y_val = train_test_split(X_train, y, test_size=0.3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aecc35cb-1284-2601-bca7-dc3180cc6999"},"outputs":[],"source":"CATEGORICAL_COLUMNS = [\"building_id\",\"description\", \"display_address\", \"street_address\"]\n#CATEGORICAL_COLUMNS = [\"building_id\",\"description\", \"display_address\", \"features\", \"street_address\"]\nCONTINUOUS_COLUMNS = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\", \"num_photos\", \"listing_id\", \n                      \"num_features\", \"num_description_words\", \"created_year\", \"created_month\", \"created_day\"]\nLABEL_COLUMN =[\"label\"]\nimport tensorflow as tf\ncontinuous_cols = {k: tf.constant(X_train[k].values) for k in CONTINUOUS_COLUMNS}\ncontinuous_cols\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"00302463-b2d3-3ee0-fcde-82e2097c6d77"},"outputs":[],"source":"X_train[\"label\"].size\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1c9a1b8-6ef8-6e55-8859-1805af086b17"},"outputs":[],"source":"import tensorflow as tf\ncategorical_cols = {k: tf.SparseTensor(\n    indices=[[i, 0] for i in range(X_train[k].size)], \n    values=X_train[k].values, \n    dense_shape = [X_train[k].size , 1]) \n                    for k in CATEGORICAL_COLUMNS}\ncategorical_cols\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4bdf350a-62d1-a6e4-d331-a9e5b6771550"},"outputs":[],"source":"merged = dict(continuous_cols.items() )\nmerged.update(categorical_cols.items())\nmerged"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b3f3e753-542d-11d4-3c38-42b2b3219d1e"},"outputs":[],"source":"merged2=continuous_cols\nmerged2.update(categorical_cols)\nmerged2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"84be7377-31da-7829-03ea-a6f4e61759af"},"outputs":[],"source":"y = X_train[\"label\"]\n\nX_train2, X_val, y_train2, y_val = train_test_split(X_train, y, test_size=0.3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5515837b-0ff8-2db5-e91a-fd4d5f36a9b8"},"outputs":[],"source":"def input_fn(df):\n    #Creates a dictionary mapping from each continuous feature column name (k) to\n    #the values of that column stored in a constant Tensor.\n    continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}      \n    #Creates a dictionary mapping from each categorical feature column name (k)\n    #to the values of that column stored in a tf.SparseTensor.    \n    categorical_cols = {k: tf.SparseTensor(indices=[[i, 0] for i in range(df[k].size)], values=df[k].values, dense_shape=[df[k].size, 1]) for k in CATEGORICAL_COLUMNS}\n    # Merges the two dictionaries into one.\n    feature_cols=continuous_cols\n    feature_cols.update(categorical_cols)\n    # Converts the label column into a constant Tensor.\n    label = tf.constant(df[LABEL_COLUMN].values)\n    #Returns the feature columns and the label.\n    return feature_cols, label\n\ndef train_input_fn():\n    return input_fn(X_train2)\n\ndef eval_input_fn():\n    return input_fn(X_val)\n\ndef test_input_fn():\n    return input_fn(X_test)\n\ntrain_input_fn()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c60ae6a0-b810-16fa-0de2-de3a42ba5162"},"outputs":[],"source":"eval_input_fn()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe76c057-a338-8b71-ed8c-3bbaae2fb98b"},"outputs":[],"source":"\nbuilding_id = tf.contrib.layers.sparse_column_with_hash_bucket(\"building_id\", hash_bucket_size=1000)\ndescription = tf.contrib.layers.sparse_column_with_hash_bucket(\"description\", hash_bucket_size=1000)\ndisplay_address = tf.contrib.layers.sparse_column_with_hash_bucket(\"display_address\", hash_bucket_size=1000)\nstreet_address = tf.contrib.layers.sparse_column_with_hash_bucket(\"street_address\", hash_bucket_size=1000)\n\nbathrooms = tf.contrib.layers.real_valued_column(\"bathrooms\")\nbedrooms = tf.contrib.layers.real_valued_column(\"bedrooms\")\nlatitude = tf.contrib.layers.real_valued_column(\"latitude\")\nlongitude = tf.contrib.layers.real_valued_column(\"longitude\")\nprice = tf.contrib.layers.real_valued_column(\"price\")\nnum_photos = tf.contrib.layers.real_valued_column(\"num_photos\")\nlisting_id = tf.contrib.layers.real_valued_column(\"listing_id\")\nnum_features = tf.contrib.layers.real_valued_column(\"num_features\")\nnum_description_words = tf.contrib.layers.real_valued_column(\"num_description_words\")\ncreated_year = tf.contrib.layers.real_valued_column(\"created_year\")\ncreated_month = tf.contrib.layers.real_valued_column(\"created_month\")\ncreated_day = tf.contrib.layers.real_valued_column(\"created_day\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"10539b2b-c9e7-20db-7a1c-c66a4d7e47ba"},"outputs":[],"source":"daddress_x_saddress = tf.contrib.layers.crossed_column([display_address, street_address], hash_bucket_size=int(1e4))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d9e4e6e9-9ed1-f6d4-0d2f-2a9fba427242"},"outputs":[],"source":"import tempfile\nmodel_dir = tempfile.mkdtemp()\nm = tf.contrib.learn.LinearClassifier(feature_columns=[\n  description, street_address, bathrooms, bedrooms, latitude, longitude, price,\n  num_photos, num_features, num_description_words, created_year, created_month, created_day],\n  model_dir=model_dir)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4042a897-ed31-1a85-a05e-cb1e329f75d7"},"outputs":[],"source":"m.fit(input_fn=train_input_fn, steps=200)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da867974-7078-6242-7f65-a3d45db669b8"},"outputs":[],"source":"results = m.evaluate(input_fn=eval_input_fn, steps=1)\nfor key in sorted(results):\n    print (\"%s: %s\" % (key, results[key]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b38bd0bb-f56e-46bd-37df-2b5fa312b7d0"},"outputs":[],"source":"results2 = m.evaluate(input_fn=train_input_fn, steps=1)\nfor key in sorted(results2):\n    print (\"%s: %s\" % (key, results2[key]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8029a3e4-cb9b-2cfe-2c79-82906748192b"},"outputs":[],"source":"def input_fn2(df):\n    #Creates a dictionary mapping from each continuous feature column name (k) to\n    #the values of that column stored in a constant Tensor.\n    continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}      \n    #Creates a dictionary mapping from each categorical feature column name (k)\n    #to the values of that column stored in a tf.SparseTensor.    \n    categorical_cols = {k: tf.SparseTensor(indices=[[i, 0] for i in range(df[k].size)], values=df[k].values, dense_shape=[df[k].size, 1]) for k in CATEGORICAL_COLUMNS}\n    # Merges the two dictionaries into one.\n    feature_cols=continuous_cols\n    feature_cols.update(categorical_cols)\n    # Converts the label column into a constant Tensor.\n    #label = tf.constant(df[LABEL_COLUMN].values)\n    #Returns the feature columns and the label.\n    return feature_cols\n\ndef test_input_fn():\n    return input_fn2(X_test)\n\ntest_input_fn()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1989b590-ae07-d13e-7cff-1d9f67442a57"},"outputs":[],"source":"print (m.predict(input_fn=lambda: test_input_fn()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4ae7b979-a627-8901-4200-606e25cb66cf"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}