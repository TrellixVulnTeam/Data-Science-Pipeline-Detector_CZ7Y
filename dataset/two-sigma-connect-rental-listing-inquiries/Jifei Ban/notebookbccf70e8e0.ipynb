{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c4d50d1f-85df-bd63-a46f-dc2f10e01dea"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nimport nltk\nimport string\nfrom nltk.stem.porter import *\nfrom collections import Counter\nfrom sklearn import model_selection, preprocessing, ensemble\nfrom scipy import sparse\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\ndf=pd.read_json(open(\"../input/train.json\",\"r\"))\ndft = pd.read_json(open(\"../input/test.json\", \"r\"))\n\ndf['features1']=df['features'].apply(lambda x: \" \".join([\" \".join(i.split(\" \"))for i in x]) )\n#print(df[df.features1.str.contains(\"_24\")].iloc[0,-1])\ndf['fea_des']=df['description']+df['features1']\ndft['features1']=dft['features'].apply(lambda x: \" \".join([\" \".join(i.split(\" \"))for i in x]) )\n#print(df[df.features1.str.contains(\"_24\")].iloc[0,-1])\ndft['fea_des']=dft['description']+dft['features1']\n\ndef get_tokens(shakes):\n    text = shakes\n    lowers = text.apply(lambda x: x.lower())\n    #remove the punctuation using the character deletion step of translate\n    no_punctuation = lowers.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n    tokens = no_punctuation.apply(lambda x: nltk.word_tokenize(x))\n    return tokens\n\n\ndf['fea_des_token'] = get_tokens(df['fea_des'])\ndft['fea_des_token'] = get_tokens(dft['fea_des'])\n\ndef stem_tokens(tokens, stemmer):\n    stemmed = []\n    for item in tokens:\n        stemmed.append(stemmer.stem(item))\n    return stemmed\n\nstemmer = PorterStemmer()\ndf['fea_des_stem']= df['fea_des_token'].apply(lambda x: stem_tokens(x, stemmer))\ndft['fea_des_stem']= dft['fea_des_token'].apply(lambda x: stem_tokens(x, stemmer))\n\ndf['fea_des_stem1']=df['fea_des_stem'].apply(lambda x: \" \".join(x))\ndft['fea_des_stem1']=dft['fea_des_stem'].apply(lambda x: \" \".join(x))\n\n\ntfidf = CountVectorizer(stop_words='english', max_features=200)\ntr_sparse = tfidf.fit_transform(df['fea_des_stem1'])\nte_sparse = tfidf.transform(dft[\"fea_des_stem1\"])\n\ndf[\"num_photos\"] = df[\"photos\"].apply(len)\ndf[\"num_features\"] = df[\"features\"].apply(len)\ndf[\"num_description_words\"] = df[\"description\"].apply(lambda x: len(x.split(\" \")))\ndf[\"created\"] = pd.to_datetime(df[\"created\"])\ndf[\"created_year\"] = df[\"created\"].dt.year\ndf[\"created_month\"] = df[\"created\"].dt.month\ndf[\"created_day\"] = df[\"created\"].dt.day\n\n\nfeats=[\"bathrooms\",\"bedrooms\",\"latitude\",\"longitude\",\"price\",\"num_photos\", \"num_features\", \"num_description_words\",\n             \"created_year\", \"created_month\", \"created_day\"]\nX=df[feats]\ny=df[\"interest_level\"]\n#add in manager_id and building_id\ncategorical = [\"manager_id\", \"building_id\"]\nfor f in categorical:\n        if df[f].dtype=='object':\n            #print(f)\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(df[f].values) + list(dft[f].values))\n            df[f] = lbl.transform(list(df[f].values))\n            dft[f] = lbl.transform(list(dft[f].values))\n            feats.append(f)\n            \n#prepare test data        \ndft[\"num_photos\"] = dft[\"photos\"].apply(len)\ndft[\"num_features\"] = dft[\"features\"].apply(len)\ndft[\"num_description_words\"] = dft[\"description\"].apply(lambda x: len(x.split(\" \")))\ndft[\"created\"] = pd.to_datetime(dft[\"created\"])\ndft[\"created_year\"] = dft[\"created\"].dt.year\ndft[\"created_month\"] = dft[\"created\"].dt.month\ndft[\"created_day\"] = dft[\"created\"].dt.day\nXt = dft[feats]\n#####\n\nX1 = sparse.hstack([df[feats], tr_sparse]).tocsr()\ntest_X=sparse.hstack([dft[feats],te_sparse]).tocsr()\ntarget_num_map = {'high':0, 'medium':1, 'low':2}\ny1 = np.array(df['interest_level'].apply(lambda x: target_num_map[x]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"31f190b9-32c7-f5e3-be4e-70e4d8ed6c2d"},"outputs":[],"source":"seed_val=2017\nnum_rounds=10\nparam = {}\n#param['objective'] = 'multi:softprob'\n#param['learning_rate'] = 0.1\n#param['max_depth'] = 6\n#param['silent'] = 0\n#param['num_class'] = 3\n#param['eval_metric'] = \"mlogloss\"\nparam['min_child_weight'] = 1\n#param['subsample'] = 0.8\n#param['colsample_bytree'] = 0.8\n#param['early_stopping_rounds']=  20\nparam['seed'] = seed_val\nnum_rounds = num_rounds\nbase_xgb=xgb.XGBClassifier(**param)\nbb_xgb=xgb.train()\n\n\n\nparam_grid = {'subsample': [ 0.8], 'colsample_bytree': [0.8], 'max_depth':[6], 'early_stopping_rounds':[10]}\nmodel = GridSearchCV(estimator=bb_xgb, param_grid=param_grid, n_jobs=1, cv=2, verbose=25)\n#model.fit(X1, y1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"caa32d83-815b-31e5-b973-6dd444fffd0c"},"outputs":[],"source":"a=model.fit(X1,y1)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}