{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport nltk\nimport re\nimport string\nimport geopy.distance\nfrom scipy import sparse\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_json('../input/two-sigma-connect-rental-listing-inquiries/train.json.zip')\n# test_df = pd.read_json('../input/two-sigma-connect-rental-listing-inquiries/test.json.zip')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_features(train_df):\n    '''\n    Cоздадим простые дополнительные признаки\n    '''\n    train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n    train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n    train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n    train_df[\"created\"] = pd.to_datetime(train_df[\"created\"], format='%Y-%m-%d %H:%M:%S')\n    train_df[\"created_month\"] = train_df[\"created\"].dt.month\n    train_df[\"created_day\"] = train_df[\"created\"].dt.day\n    train_df['created_dayofweek']=train_df['created'].apply(lambda x:x.dayofweek)\n    train_df['logprice'] = np.log(train_df['price'])\n    train_df.drop(['created'], axis = 1)\n    train_df['rooms'] = train_df['bathrooms'] + train_df['bedrooms'] + 1 \n    train_df['price_per_room'] = train_df['price'] / train_df['rooms']\n    \ndef create_geo_features(df):\n    '''\n    Считаем расстояние до достопримечательностей Нью Йорка\n    '''\n    cords = (40.70664160590731, -74.00889639688603)\n    df['distance_to_WallStreet'] = df[['latitude', 'longitude']].apply(lambda x:geopy.distance.geodesic((x[0], x[1]), cords).m, axis=1)\n    cords = (40.748477220505485, -73.98566751076079)\n    df['distance_to_EmpireStateBuildng'] = df[['latitude', 'longitude']].apply(lambda x:geopy.distance.geodesic((x[0], x[1]), cords).m, axis=1)\n    cords = (40.68941208336766, -74.04443602895553)\n    df['distance_to_Statue'] = df[['latitude', 'longitude']].apply(lambda x:geopy.distance.geodesic((x[0], x[1]), cords).m, axis=1)\n    cords = (40.780814403367714, -73.96735634584424)\n    df['distance_to_CentralPark'] = df[['latitude', 'longitude']].apply(lambda x:geopy.distance.geodesic((x[0], x[1]), cords).m, axis=1)\n    cords = (40.758096515338465, -73.9855748013695)\n    df['distance_to_TimesSquare'] = df[['latitude', 'longitude']].apply(lambda x:geopy.distance.geodesic((x[0], x[1]), cords).m, axis=1)  \n    \ndef extra_description_info(df):\n    '''\n    Позволяет извлечь дополнительную информацию из описания,\n    Я полагаю такие символы и заглавные буквы используются для привлечения внимания\n    к объявлению\n    '''\n    df['has_phone'] = df['description'].apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n            .apply(lambda x: [s for s in x if s.isdigit()])\\\n            .apply(lambda x: len([s for s in x if len(str(s))==10]))\\\n            .apply(lambda x: 1 if x>0 else 0)\n\n    df['has_email'] = df['description'].apply(lambda x: 1 if '@renthop.com' in x else 0)\n\n    df['num_of_#'] = df.description.apply(lambda x:x.count('#'))\n    df['num_of_!'] = df.description.apply(lambda x:x.count('!'))\n    df['num_of_$'] = df.description.apply(lambda x:x.count('$'))\n    df['num_of_*'] = df.description.apply(lambda x:x.count('*'))\n    df['num_of_>'] = df.description.apply(lambda x:x.count('>'))\n\n    df['num_of_puncs'] = df['num_of_#'] + df['num_of_!'] + df['num_of_$'] + df['num_of_*'] + df['num_of_>']\n    df['upper_char_ratio'] = df['description'].apply(lambda x: 0 if sum([s.isalpha() for s in x])==0 else sum([s.isalpha()&s.isupper() for s in x])/ sum([s.isalpha() for s in x]))\n\ndef preprocessor(text, signs=True, digits=True, lowercase=True):# удаляет знаки препинания, цифры, приводит всё в нижний регистр\n    '''\n    Удаляет знаки препинания из текста\n    Удаляет цифры\n    Приводит текст в нижний регистр\n    '''\n    if(signs):\n        text = re.sub('<[^>]*>', '', text)\n    if(digits):\n        text = re.sub(\"\\d\", '', text)# удалить цифры\n    if(lowercase):  \n        text = re.sub('[\\W]+', ' ', text.lower())# +\\\n    return text\n\ndef delStopWords(text):# удаление стоп слов\n    '''\n    Удаляет стоп слова английского языка\n    '''\n    stopwords = nltk.corpus.stopwords.words('english')\n    tokens = nltk.tokenize.word_tokenize(text)\n    words = [w for w in tokens if w not in stopwords]  \n    return ' '.join(list(words))\n\ndef denoise_text_for_description(text):\n    '''\n    Предобработка текста для описания\n    '''\n    text = preprocessor(text, signs=True, digits=True, lowercase=True)\n    text = delStopWords(text)\n    return text\n\ndef denoise_text_for_features(text):\n    '''\n    Предобработка текста для признаков\n    '''\n    text = preprocessor(text, signs=False, digits=True, lowercase=True)\n    text = delStopWords(text)\n    return text\n\ndef adress_prep(df, column_name):\n    '''\n    Предобработка адресов\n    Приводит в нижний регистр, заменяет сокращения для east, west, street, avenue\n    '''\n    df[column_name] = df[column_name].apply(lambda x: x.str.lower())\n    df[column_name] = df[column_name].replace(['\\sst\\s', '\\sst$'], ' street', regex = True)\n    df[column_name] = df[column_name].replace(['\\save\\s', '\\save$'], ' avenue', regex = True)\n    df[column_name] = df[column_name].replace(['\\se\\s', '^e\\s'], ' east ', regex = True)\n    df[column_name] = df[column_name].replace(['\\sw\\s', '^w\\s'], ' west ', regex = True)\n    df[column_name] = df[column_name].replace(' ', '')\n    \ndef manager_prep(df):  \n    pass\n\ndef building_prep(df):\n    pass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncreate_features(train_df)\n# create_features(test_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nextra_description_info(train_df)\n# extra_description_info(test_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncreate_geo_features(train_df)\n# create_geo_features(test_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Применяем обработку текстов к столбцу описание\ntrain_df['description']=train_df['description'].apply(denoise_text_for_description)\n# test_df['description']=test_df['description'].apply(denoise_text_for_description)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\ntrain_df['features'] = train_df['features'].str.replace('-','_') # Pre-war -> pre_war\n\n# test_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n# test_df['features'] = test_df['features'].str.replace('-','_')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Применяем обработку текстов к столбцу features\ntrain_df['features']=train_df['features'].apply(denoise_text_for_features)\n# test_df['features']=test_df['features'].apply(denoise_text_for_features)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nadress_prep(train_df, ['display_address', 'street_address'])\n# adress_prep(test_df, ['display_address', 'street_address'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Применим простые способы векторизации текстов tfidf для описаний count для features\ntfv_description = TfidfVectorizer(min_df=10,\n                      max_features=200,\n                      strip_accents='unicode',\n                      lowercase = False,\n                      analyzer='word',\n                      stop_words='english', \n                      ngram_range=(2, 3),\n                      use_idf=1,\n                      smooth_idf=1,\n                      sublinear_tf=1\n                     )\n\ncount_vec = CountVectorizer(max_features = 200,\n                            strip_accents='unicode',\n                            stop_words='english',\n                            lowercase = False,\n                            analyzer='word')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntfidf_description_train = tfv_description.fit_transform(train_df[\"description\"]).toarray()\n# tfidf_description_test = tfv_description.transform(test_df[\"description\"]).toarray()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncount_vec_features_train = count_vec.fit_transform(train_df[\"features\"]).toarray()\n# count_vec_features_test = count_vec.transform(test_df[\"features\"]).toarray()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Закодируем номинальные категориальные переменные используя labelencoder, это не лучшее решение потому, что в таком случае можно  будет сравнивать между собой street_аddress, building_id, display_address, manager_id, можно было бы извлечь информацию из адреса и id здания, но для этого требуется более глубокий анализ. Некоторые объявления имеют одинаковый building_id, при этом у них отличаются адреса и координаты. Но эксперименты показали, что если убрать эти признаки совсем результат будет хуже.","metadata":{}},{"cell_type":"code","source":"for column_name in ['building_id', 'manager_id', 'display_address', 'street_address']:\n    label_encoder = LabelEncoder()\n    label_encoder.fit(list(train_df[column_name].values))# + list(test_df[column_name].values))\n    train_df[column_name] = label_encoder.transform(train_df[column_name].values)\n    # test_df[column_name] = label_encoder.transform(test_df[column_name].values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Manager processing\nhttps://www.kaggle.com/code/den3b81/improve-perfomances-using-manager-features\nНельзя использовать данные и тестового датасета, поэтому извлекаем данные из тренировочного и используем их для тестового\nдля менеджеров, которые не встречались в тренирочном наборе и для менеджеров у которых мало объявлений (<20) использую\nсреднии значения","metadata":{}},{"cell_type":"code","source":"# тренировочный набор данных\nX = train_df.drop(columns = [\"interest_level\"], axis = 1)\ny = train_df[\"interest_level\"]\n\ntemp = pd.concat([X.manager_id,pd.get_dummies(y)], axis = 1).groupby('manager_id').mean() # вычисляет процент от общего числа объявлений менеджера для каждой метки класса \ntemp.columns = ['high_frac','low_frac', 'medium_frac']\ntemp['count'] = X.groupby('manager_id').count().iloc[:,1]# количество объявлений\n\ntemp['manager_skill'] = temp['high_frac']*2 + temp['medium_frac'] # вычисяем навык менеджера\nunranked_managers_ixes = temp['count'] < 20\nranked_managers_ixes = ~unranked_managers_ixes\nmean_values = temp.loc[ranked_managers_ixes, ['high_frac','low_frac', 'medium_frac','manager_skill']].mean()\ntemp.loc[unranked_managers_ixes,['high_frac','low_frac', 'medium_frac','manager_skill']] = mean_values.values\n\ntrain_df = train_df.merge(temp.reset_index(),how='left', left_on='manager_id', right_on='manager_id')\n\n# тестовый набор данных\n# test_df = test_df.merge(temp.reset_index(),how='left', left_on='manager_id', right_on='manager_id')\n# new_manager_ixes = test_df['high_frac'].isnull()\n# test_df.loc[new_manager_ixes,['high_frac','low_frac', 'medium_frac','manager_skill']] = mean_values.values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interest_level_map = {'high': 0, 'medium': 1, 'low': 2}\ntrain_df['interest_level'] = train_df['interest_level'].map(interest_level_map)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.drop(['latitude', 'longitude', 'photos', 'created', 'description', 'features', 'listing_id', 'manager_id'], axis = 1)\n# test_df = test_df.drop(['latitude', 'longitude', 'photos', 'created', 'description', 'features', 'manager_id'], axis = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = pd.concat([train_df.reset_index(), pd.DataFrame(count_vec_features_train, columns=count_vec.get_feature_names())], axis=1)\nX_train = pd.concat([X_train.reset_index(), pd.DataFrame(tfidf_description_train, columns=tfv_description.get_feature_names())], axis=1)\n\n# X_test = pd.concat([test_df.reset_index(), pd.DataFrame(count_vec_features_test, columns=count_vec.get_feature_names())], axis=1)\n# X_test = pd.concat([X_test.reset_index(), pd.DataFrame(tfidf_description_test, columns=tfv_description.get_feature_names())], axis=1)\n\nY_train = X_train['interest_level']\nX_train = X_train.drop(['interest_level'], axis = 1) \n\nX_train = X_train.drop(['level_0', 'index'], axis = 1)\n# X_test = X_test.drop(['level_0', 'index'], axis = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# param_grid_xgb = {'min_child_weight': [1, 5],\n#                   'subsample': [0.6, 0.7],\n#                   'eval_metric': ['mlogloss'],\n#                   'gamma': [0, 1],\n#                   'max_depth': [6, 7],\n#                   'learning_rate' : [0.5,0.1],\n#                   'colsample_bytree': [0.7],\n#                    'n_estimators': [1000]\n#                  } \n\n# model =  xgb.XGBClassifier()\n\n# # Run grid search \n# grid = GridSearchCV(model,\n#                     cv = 3,\n#                     param_grid = param_grid_xgb,\n#                     refit = True,\n#                     verbose = 1,\n#                     n_jobs = -1) \n\n\n# # fit the model for grid search \n# grid.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X_train, Y_train,\n                                                    test_size = 0.2,\n                                                    random_state = 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler=StandardScaler()\nscaler.fit(X_train)\n\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nparams = {'colsample_bytree': 0.7,\n          'eval_metric': 'mlogloss',\n          'gamma': 0,\n          'learning_rate': 0.5,\n          'max_depth': 7,\n          'min_child_weight': 5,\n          'n_estimators': 1000,\n          'subsample': 0.7}\n\nmodel =  xgb.XGBClassifier(params)\nmodel.fit(x_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import log_loss\n\ny_val_pred = model.predict_proba(x_test)\n# 0.55\n# 0.57\n# 0.5669\n# 0.542566888770270\nprint(log_loss(y_test, y_val_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"listing_ids = X_test['listing_id']\nX_test = X_test.drop(['listing_id'], axis = 1)\n\ny_pred_prod = model.predict_proba(X_test)\n\nsubmit = pd.DataFrame(data={'listing_id': listing_ids,\n                            'high': y_pred_prod[:, 0],\n                            'medium': y_pred_prod[:, 1], \n                            'low': y_pred_prod[:, 2]})\nsubmit.to_csv('submit.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}