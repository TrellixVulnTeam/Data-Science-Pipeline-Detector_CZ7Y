{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\npd.options.display.max_rows = 999\npd.options.display.max_columns=999","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_json('/kaggle/input/two-sigma-connect-rental-listing-inquiries/train.json.zip')\ntest = pd.read_json('/kaggle/input/two-sigma-connect-rental-listing-inquiries/test.json.zip')\nall_data = pd.concat([train,test])\nall_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_data['manager_id'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#1) all_data = pd.concat([train,test]).reset_index(drop=True)# reset_index\n#2) \nall_data = pd.concat([train,test],ignore_index=True)\n\n# all_data['created'] = all_data['created'].astype('datetime64')\n# all_data['created_year'] = all_data['created'].dt.year\n# all_data['created_month'] = all_data['created'].dt.month\n# all_data['created_day'] = all_data['created'].dt.day \n# all_data['created_hour'] = all_data['created'].dt.hour\n# #all_data['created_week'] = all_data['created'].dt.week\n\nall_data['num_photos'] = all_data['photos'].apply(len)\nall_data['num_features'] = all_data['features'].apply(len)\nall_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#description\n# text처리 \n# 1) machine-learning:count vectorizer(단어 발생 빈도->중요성 파악), tf-idf(단어 발생 빈도->중요성 파악+가중치) *가중치:1~0. 불용어의 가중치 낮춤.\n# 2) deep-learning * 임베딩 개념\n\n# from sklearn.feature_extraction.text import TfidfVectorizer\n\n# tf = TfidfVectorizer()\n# description_text = tf.fit_transform(all_data['description'])\n\n# description_text #Sparse matrix: 대용량 데이터 처리에 단어의 종류 55859","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# description_text[0] #Sparse matrix: 대용량 데이터 처리에 단어의 종류 55859\n# print(description_text[0]) # 총 52개의 데이터 가지고 있음 (<1x55859 sparse matrix of type '<class 'numpy.float64'>' with 52 stored elements in Compressed Sparse Row format>)\n#  (0, 54290)\t0.03894011143583986\n# 54290: 많이 등장할 수록 숫자가 작아짐\n# 0.03894011143583986: 가중치","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 차원 축소\n# pca, svd\nfrom sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=10) #n_components를 변경해가면서 적정한 차원을 설정 (for loop)\ndescription_svd = svd.fit_transform(description_text)\ndescription_svd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_data['description1'] = description_svd[:,0]\n# all_data['description2'] = description_svd[:,1]\nall_data = pd.concat([all_data,pd.DataFrame(description_svd)],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nall_data['building_id'] = le.fit_transform(all_data['building_id'])\nall_data['display_address'] = le.fit_transform(all_data['display_address'])\nall_data['street_address'] = le.fit_transform(all_data['street_address'])\nall_data['manager_id'] = le.fit_transform(all_data['manager_id'])\n\nall_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data2 = all_data[all_data.columns[all_data.dtypes!='object']]\nall_data2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2 = all_data2[:len(train)]\ntest2 = all_data2[len(train):]\n\ndisplay(train2, test2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier\ncbc = CatBoostClassifier() # 모델 돌릴 때만 GPU를 통해 속도 개선 가능. 전처리시에는 multi-process를 통해서 속도를 개선해야 한다.\ncbc.fit(train2,train['interest_level'])\nresult = cbc.predict_proba(test2) # predict()\n\nresult # 기본 모델에서는 알파벳 순서로 컬럼을 정렬","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/two-sigma-connect-rental-listing-inquiries/sample_submission.csv.zip')\nsub.iloc[:,1:] = result\nsub.columns = ['listing_id','high','low','medium']\nsub\nsub.to_csv('sub.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}