{"cells":[{"metadata":{},"cell_type":"markdown","source":" ## Experiment with SPARK ML (PYSPARK)\n"},{"metadata":{},"cell_type":"markdown","source":"\n> ## Below are the steps to transfer files from local drive to Hadoop Ecosystem:\n    \ncd ~\n\nhdfs dfs -rm -r hdfs://localhost:9000/user/ashok/data_files/prabhash_assignment_allstate_claims\n\nhdfs dfs -mkdir -p hdfs://localhost:9000/user/ashok/data_files/prabhash_assignment_allstate_claims\n\nTransfer files from local file system\n\ncd ~\n\nhdfs dfs -put /cdata/prabhash_assignment_allstate_claims/train.csv \n\nhdfs://localhost:9000/user/ashok/data_files/prabhash_assignment_allstate_claims\n\nhdfs dfs -ls -h hdfs://localhost:9000/user/ashok/data_files/prabhash_assignment_allstate_claims"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --quiet sparkmagic\n!pip install --quiet pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pyspark --version\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Increase the width of notebook to display all columns of data\n\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }</style>\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show multiple outputs of a single cell\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import udf, col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark = SparkSession \\\n            .builder.master('local[*]')\\\n            .appName('allstate_claims')\\\n            .getOrCreate()     \nspark\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = spark.sparkContext\nsc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sqlContext = SQLContext(spark.sparkContext)\nsqlContext","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Read, transform and understand the data\n#    pyspark creates a spark-session variable: spark\n\ndf = spark.read.csv(\n                   path = \"../input/allstate-claims-severity/train.csv\",   \n                   header = True,\n                   inferSchema= True,           # Infer datatypes automatically\n                   sep=\",\"\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.take(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.show(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data shape\ndf.count()     #How many rows?      \ncols = df.columns\nlen(cols)            \nprint(cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.printSchema()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What is the nature of df:\ntype(df)                     # pyspark.sql.dataframe.DataFrame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  We also cache the data so that we only read it from disk once.\ndf.cache()\ndf.is_cached            # Checks if df is cached","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show database in parts:\ndf.select(cols[:15]).show(3)\ndf.select(cols[15:25]).show(3)\ndf.select(cols[25:35]).show(3)\ndf.select(cols[35:45]).show(3)\ndf.select(cols[45:]).show(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Summary Statistics:\nSpark DataFrames include some built-in functions for statistical processing. The describe() function performs summary statistics calculations on all numeric columns and returns them as a DataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.describe().select(\n                    \"summary\",\n                    F.round(\"cont1\", 4).alias(\"cont1\"),\n                    F.round(\"cont2\", 4).alias(\"cont2\"),\n                    F.round(\"cont3\", 4).alias(\"cont3\"),\n                    F.round(\"cont4\", 4).alias(\"cont4\"),\n                    F.round(\"cont5\", 4).alias(\"cont5\"),\n                    F.round(\"cont6\", 4).alias(\"cont6\"),\n                    F.round(\"cont7\", 4).alias(\"cont7\"),\n                    F.round(\"cont13\", 4).alias(\"cont13\"),\n                    F.round(\"cont14\", 4).alias(\"cont14\"),\n                    F.round(\"loss\", 4).alias(\"loss\"))\n                    .show())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look at the minimum and maximum values of all the (numerical) attributes. We see that multiple attributes have a wide range of values: we will need to normalize our dataset."},{"metadata":{},"cell_type":"markdown","source":"#Preprocessing The Target Values:\n\nFirst, let's start with the loss column, our dependent variable. To facilitate our working with the target values, we will express the house values in units of 1000. That means that a target such as 3037.3377 should become 3.037:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adjust the values of `medianHouseValue`\ndf = df.withColumn(\"loss\", col(\"loss\")/1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.show(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Which columns to drop?\n\ncolumns_to_drop = ['id']\ndf= df.drop(*columns_to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import col\n\ndf.select(col(\"loss\")).show(5)\ndf.select(\"loss\").show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf = df.withColumnRenamed('loss', 'label')\nprint(df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting random seed for notebook reproducability\nimport pandas as pd\nimport numpy as np\n\nrnd_seed=23\nnp.random.seed=rnd_seed\nnp.random.set_state=rnd_seed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data splitting  #\n\n# Split the dataset randomly into 70% for training and 30% for testing.\ntrain, validation = df.randomSplit([0.7, 0.3],seed=rnd_seed)\n\n\nprint(train.count()/df.count())\nprint(validation.count()/df.count())\n# Split the dataset randomly into 70% for training and 30% for testing.\n\n#splits = df.randomSplit([0.7, 0.3])\n#train = splits[0]\n#test = splits[1].withColumnRenamed(\"loss\", \"Label\")\n#train_rows = train.count()\n#test_rows = test.count()\n#print(\"Training Rows:\", train_rows, \" Testing Rows:\", test_rows)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As number of processes of our trainingData is too large. \nAs a result, the DAG size (Directed Acyclical Graph - a logical flow of operations constructed by spark) for our data becomes too large to handle and we may end up getting the following error -\nPy4JJavaError: An error occurred while calling o903.fit.\n\nTherefore, By converting our train data dataFrame into RDD (Resilient Distributed Dataset) and then back to DataFrame again will also shrink the DAG considerably."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.explain(extended=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.checkpoint()\ntrain = spark.createDataFrame(train.rdd, schema=train.schema)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, check the size of your DAG\n\n# Displays the  length of physical plan\ntrain.explain(extended=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation.explain(extended=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#validation.checkpoint()\nvalidation = spark.createDataFrame(validation.rdd, schema=validation.schema)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation.explain(extended=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating transformation objects"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Encode 'string' column to index-column. \n#     Indexing begins from 0.\nfrom pyspark.ml.feature import StringIndexer\n\n# List all categorical columns and create objects to StringIndex all these categorical columns\n\ncat_columns = [ c[0] for c in df.dtypes if c[1] == \"string\"]\n\n\nstringindexer_stages = [ StringIndexer(inputCol=c, outputCol='stringindexed_' + c) for c in cat_columns]\nstringindexer_stages","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(stringindexer_stages)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare (one) object to OneHotEncode categorical columns (received from above)\n#  OHE an indexed column after StringIndexing and create one another column\nfrom pyspark.ml.feature import OneHotEncoder\n\nin_cols = ['stringindexed_' + c for c in cat_columns]\nohe_cols = ['onehotencoded_' + c  for c in cat_columns]\nonehotencoder_stages = [OneHotEncoder(inputCols=in_cols, outputCols=ohe_cols)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iii)  Prepare a (one) list of all numerical and OneHotEncoded columns. Exclude 'loss' column from this list.\n\n# Unlike in other languages, in spark\n#       type-classes are to be separateky imported\n#       They are not part of core classes or modules\nfrom pyspark.sql.types import DoubleType\n\ndouble_cols =   [  i[0] for i in df.dtypes if i[1] == 'double' ] \n\ndouble_cols.remove('label')  \n\ndouble_cols\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Create a combined list of double + ohe_cols\n\nfeaturesCols = double_cols + ohe_cols\nprint(featuresCols)\nlen(featuresCols)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a VectorAssembler object to assemble all the columns as above\nfrom pyspark.ml.feature import VectorAssembler\n#   Create an instance of VectorAssembler class.\n#          This object will be used to assemble all featureCols\n#          (a list of columns) into one column with name\n#           'rawFeatures'\n\nvectorassembler = VectorAssembler(\n                                  inputCols=featuresCols,\n                                  outputCol=\"rawFeatures\"\n                                 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml import Pipeline\nfrom pyspark.ml.regression import GBTRegressor\nfrom pyspark.ml.feature import VectorIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an object to perform modeling using GBTRegressor\n\ngbt = GBTRegressor(labelCol=\"label\",featuresCol=\"rawFeatures\",predictionCol='predlabel', maxIter=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 9.2 Create pipeline model\npipeline = Pipeline(stages=[                        \\\n                             *stringindexer_stages, \\\n                             *onehotencoder_stages, \\\n                             vectorassembler,       \\\n                             gbt                    \\\n                           ]                        \\\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Run the pipeline\nimport os, time\n\nstart = time.time()\npipelineModel = pipeline.fit(train)\nend = time.time()\n(end - start)/60           \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions on validation data.\n#      Note it is NOT pipelineModel.predict()\n\nprediction = pipelineModel.transform(validation)\npredicted = prediction.select(\"predlabel\", \"label\")\n#predicted.show(100, truncate=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Show 10 columns including predicted column\npredicted.show(10, truncate=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# 10.3 Evaluate results\n# Create evaluator object.  class is, as:\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.mllib.evaluation import RegressionMetrics\n\n\nevaluator = RegressionEvaluator(predictionCol='predlabel', labelCol='label', metricName='rmse')\n\nprint(\"RMSE: {0}\".format(evaluator.evaluate(predicted)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}