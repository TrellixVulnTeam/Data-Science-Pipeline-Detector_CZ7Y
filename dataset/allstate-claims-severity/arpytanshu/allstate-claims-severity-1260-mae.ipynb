{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\ndata_dir = '/kaggle/input/allstate-claims-severity/'\n\ntrain_data_path = data_dir+'train.csv'\ntest_data_path = data_dir+'test.csv'\nsubmission_csv_path = data_dir+'sample_submission.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Label Encode all categorical features\ndef get_labelEncoded_dataframes(data_dir):\n    '''\n    creates a label encoded dataframe out of the categorical features using sklearns's LabelEncoder\n    saves new dataframe in object_dir\n    skips creating new dataframe if already exists\n    '''\n    print('Label Encoding categorical features . . .')\n    train_data = pd.read_csv(data_dir+'train.csv')\n    test_data = pd.read_csv(data_dir+'test.csv')\n    cat_cols = [x for x in train_data.columns if x.startswith('cat')]\n\n    for col in cat_cols:\n        le = LabelEncoder()\n        train_data[col] = le.fit_transform(train_data[col])\n        # update::\n        # Test data had some values in some cateogorical features that were unseen in train data\n        # the next 2 lines fix that :|\n        test_data[col] = test_data[col].map(lambda s: 'UNK' if s not in le.classes_ else s)\n        le.classes_ = np.append(le.classes_, 'UNK')\n        test_data[col] = le.transform(test_data[col])\n    return train_data, test_data\n\n\ntrain_data, test_data = get_labelEncoded_dataframes(data_dir)\nsubmission = pd.read_csv(submission_csv_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_data.iloc[:,1:-1]\nY = train_data.iloc[:,-1]\n\n\n# get categorical and continuous features names\ncat_cols = [x for x in train_data.columns if x.startswith('cat')]\ncont_cols = [x for x in train_data.columns if x.startswith('cont')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    \nfrom sklearn.feature_selection     import    f_regression, mutual_info_regression\n\n# f_regression\n##############\nf_reg_res = {}\nfval, pval = f_regression(X, Y)\nfor i,c in enumerate(X.columns):\n  f_reg_res[c] = fval[i]\n\n# sort the features according to f_regression scores\nsorted_res = [[k,v] for k, v in sorted(f_reg_res.items(), key=lambda item: item[1])]\nsorted(sorted_res, key = lambda x: x[1])\n\n# remove features that scored too low\nhigh_score_features_F = [x[0] for x in list(filter(lambda x: x[1]>100, sorted_res))]\nprint(\"features with f_regression score > 100\")\nprint(high_score_features_F)\n\n\n\n# mutual_information\n####################\n# sampling a subset of data, as mutual_info calculation is intensive\nsample = train_data.sample(10000)\nx = sample.iloc[:,:-1]\ny = sample.iloc[:,-1]\n\nmutinf_res = {}\nmi = mutual_info_regression(x, y)\nfor i,c in enumerate(X.columns):\n  mutinf_res[c] = mi[i]\n\n# sort the features according to mutual_information scores\nsorted_res = [[k,v] for k, v in sorted(mutinf_res.items(), key=lambda item: item[1])]\nsorted(sorted_res, key = lambda x: x[1])\n\n# remove features that scored too low\nhigh_score_features_MI = [x[0] for x in list(filter(lambda x: x[1]>0.001, sorted_res))]\nprint(\"features with mutual_information score > 100\")\nprint(high_score_features_MI)\n\n# get intersection of features which score high on both of these tests\n# i.e. we are discarding features that did not do well in both the tests\ncommon_features_union = list(set(high_score_features_F).union(set(high_score_features_MI)))\nprint(\"# feautres selected: \", common_features_union.__len__())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Features that would be used: \", common_features_union)\nprint(\"# features: \", common_features_union.__len__())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, LeakyReLU\nfrom keras.preprocessing import text\nfrom keras import utils\n\n# set hyperparameters for MLP\nclass NN:\n    def __init__(self):\n        self.in_shape = common_features_union.__len__()\n        self.num_layers = 3\n        self.nodes = [2048,1024, 1]\n        self.activations = ['relu', 'relu', 'relu']\n        self.dropouts = [0.2,0.15,0]\n        self.loss = 'mean_squared_logarithmic_error'\n        self.optimizer = keras.optimizers.RMSprop(5e-4)\n\n\n\ndef sequential_MLP(nn):\n    model = Sequential()\n    for i in range(nn.num_layers):\n        if i==0: # add input shape if first layer\n            model.add(Dense(nn.nodes[i], activation=nn.activations[i], input_shape=(nn.in_shape,) ))\n        else:\n            model.add(Dense(nn.nodes[i], activation=nn.activations[i]))\n        if(nn.dropouts[i] != 0): # skip adding dropout if dropout == 0\n            model.add(Dropout(rate=nn.dropouts[i]))            \n    model.compile(optimizer=nn.optimizer, loss=nn.loss, metrics=['mae'])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nnn = NN()\nmodel = sequential_MLP(nn)\n\n\nfor i in range(45):\n  if i%5 == 0: verbose=True\n  else: verbose = False\n  model.fit(X[common_features_union], Y, epochs=1, batch_size=512, validation_split=0.25, verbose=verbose)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = model.predict(test_data[common_features_union])\n\n\nsubmission = pd.read_csv('/kaggle/input/allstate-claims-severity/sample_submission.csv')\nsubmission['loss'] = test_predictions\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}