{"cells":[{"metadata":{"toc":true},"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-data\" data-toc-modified-id=\"Loading-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Loading data</a></span></li><li><span><a href=\"#Basic-feature-analysis\" data-toc-modified-id=\"Basic-feature-analysis-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Basic feature analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Distribution-Analysis\" data-toc-modified-id=\"Distribution-Analysis-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Distribution Analysis</a></span></li><li><span><a href=\"#Linear-Relationship\" data-toc-modified-id=\"Linear-Relationship-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Linear Relationship</a></span></li></ul></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pattern-of-the-Target\" data-toc-modified-id=\"Pattern-of-the-Target-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Pattern of the Target</a></span></li><li><span><a href=\"#Encode-Categorical-Data\" data-toc-modified-id=\"Encode-Categorical-Data-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Encode Categorical Data</a></span></li><li><span><a href=\"#Numeric-Features-Transformation\" data-toc-modified-id=\"Numeric-Features-Transformation-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Numeric Features Transformation</a></span></li></ul></li></ul></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Baseline-Model---Linear-Regression\" data-toc-modified-id=\"Baseline-Model---Linear-Regression-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Baseline Model - Linear Regression</a></span></li><li><span><a href=\"#Ridge-Regression\" data-toc-modified-id=\"Ridge-Regression-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Ridge Regression</a></span></li><li><span><a href=\"#LASSO-Regression\" data-toc-modified-id=\"LASSO-Regression-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>LASSO Regression</a></span></li><li><span><a href=\"#Xgboost\" data-toc-modified-id=\"Xgboost-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Xgboost</a></span></li></ul></li></ul></div>"},{"metadata":{},"cell_type":"markdown","source":"**Project Description:** \n\nIn this project, I will build a model to predict the severity of Allstate claims using Ridge Regression, Lasso Regression and Xgboost.  "},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"## Loading data"},{"metadata":{"trusted":false},"cell_type":"code","source":"# import and some default settings\nimport warnings\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom scipy import sparse\nfrom sklearn import metrics\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\nfrom sklearn.svm import LinearSVC\nfrom scipy.stats import skew, boxcox\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# load our dataset \ndataset = pd.read_csv(\"/kaggle/input/allstate-claims-severity/train.csv\")\nmodel_index = len(dataset)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset has already been anonymized due to privacy protection, we only know whether a feature is continuous or categorical. The submission dataset does not have \"loss\", it's the dataset that need final predictions. Here I still want to combine these two dataset, which can save me some duplicate operations. "},{"metadata":{"trusted":false},"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/allstate-claims-severity/test.csv\")\nfull_dataset = pd.concat([dataset,submission]).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"full_dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, I want to check if there is any missing value or negative value (only for continuous variables). loss column absolutely have some missing value, and there is not any negative value in our dataset."},{"metadata":{"trusted":false},"cell_type":"code","source":"full_dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"dataset.shape,submission.shape,full_dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# split our dataset\nY = dataset[\"loss\"]\nX = dataset.drop(['id', 'loss'], axis= 1)\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"x_train.shape, x_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# group features\ncat_variables = []\ncon_variables = []\nid_col = 'id'\ntarget_col = 'loss'\n\nfor i in dataset.columns:\n    if i[:2] == 'ca':\n        cat_variables.append(i)\n    if i[:2] == 'co':\n        con_variables.append(i)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"print(\"The continuous variables: \",con_variables)\nprint(\"The categorial variables: \",cat_variables)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic feature analysis"},{"metadata":{},"cell_type":"markdown","source":"### Distribution Analysis"},{"metadata":{},"cell_type":"markdown","source":"Here, I want to make sure that if there is and skewed distribution, and if the distribution of train and test dataset are the same."},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"# check the distribution of continuous column\ncount = 1\n\nfor i in range(len(con_variables)):\n    fig = plt.figure(figsize = (15,25))\n    sns.set_style('darkgrid')\n    plt.subplot(len(con_variables),2,count)\n    sns.violinplot(x_train[con_variables[i]],palette=\"hls\")\n    plt.title(\"Train\")\n    \n    plt.subplot(len(con_variables),2,count+1)\n    sns.violinplot(x_test[con_variables[i]],palette=\"Paired\")\n    plt.title(\"Test\")\n    count += 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Relationship "},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"# plot the heatmap of correlation matrix\nplt.figure(figsize=(15,12))\nsns.heatmap(x_train.corr(),cmap='coolwarm',linecolor='white',linewidths=0.5,annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# find out high correlated features\nhighcorr = []\ncorr = x_train.corr() \nthreshold = 0.9\n\nfor i in range(corr.shape[0]):\n    for j in range(corr.shape[1]):\n        if i == j:\n            continue\n        elif (corr.iloc[i,j] > threshold) | (corr.iloc[i,j] < -threshold):\n            highcorr.extend([corr.iloc[i].name,corr.iloc[:,j].name])\n        else:\n            continue","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"highcorr = list(set(highcorr))\nhighcorr","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"sns.set_style('darkgrid')\nsns.pairplot(dataset[highcorr],plot_kws=dict(s=4, edgecolor=\"w\", linewidth=.01),markers='o')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.jointplot(x=\"cont1\",y=\"cont9\",data=dataset,kind='hex')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.jointplot(x=\"cont12\",y=\"cont11\",data=dataset,kind='hex')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def dropColumn(dataset,drop_col,inp=False):\n    dataset.drop(drop_col,axis=1, inplace=inp)\n    if inp == False:\n        return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"drop_col = [\"cont11\",\"cont1\"]\ndropColumn(full_dataset,drop_col,inp=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for i in drop_col:\n    con_variables.remove(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### Pattern of the Target"},{"metadata":{"trusted":false},"cell_type":"code","source":"# viusalize the distribution of loss, which is our target\n# thera are many outliers\nsns.set_style('darkgrid')\nplt.figure(figsize=(10,6))\nsns.boxplot(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# it's a very skewed distribution\nplt.figure(figsize=(10,6))\nsns.distplot(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# apply log(1+loss) we can get a normal distribution\nplt.figure(figsize=(10,6))\nsns.distplot(np.log1p(y_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encode Categorical Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"def catEncode(dataset):\n    # ensure the features being converted to numeric\n    le = LabelEncoder()\n    dataset[cat_variables] = dataset[cat_variables].apply(lambda col: le.fit_transform(col))\n    # Then I will convert it to a sparse matrix which uses way less memory as compared to dense matrix\n    OneHot = OneHotEncoder(sparse=True)\n    return OneHot.fit_transform(dataset[cat_variables])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"full_dataset_sparse = catEncode(full_dataset)\nfull_dataset_sparse.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Numeric Features Transformation"},{"metadata":{},"cell_type":"markdown","source":"I will apply two preprocessings on numeric features:\n\n1. Apply box-cox transformations for skewed numeric features.\n\n2. Scale numeric features so they will fall in the range between 0 and 1.\n\nPlease be advised that these preprocessings are not necessary for tree-based models, e.g. XGBoost. However, linear or linear-based models may benefit from them."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# calculate skewness of each numeric features\nskewed_cols = full_dataset.loc[:,con_variables].apply(lambda x: skew(x.dropna()))\nprint(skewed_cols.sort_values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.distplot(full_dataset[\"cont9\"])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.distplot(full_dataset[\"cont8\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# apply box-cox transformations\nskewed_cols = skewed_cols[abs(skewed_cols) > 0.25].index.values\nfor skewed_col in skewed_cols:\n    full_dataset[skewed_col],lam = boxcox(full_dataset[skewed_col] + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"skewed_cols = full_dataset.loc[:,con_variables].apply(lambda x: skew(x.dropna()))\nprint(skewed_cols.sort_values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# apply standard scaling\nSSL = StandardScaler()\n\nfor con_col in con_variables:\n     full_dataset[con_col] = SSL.fit_transform(full_dataset[con_col].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":false},"cell_type":"code","source":"# we use the following two methods to evaluate our model\n\ndef logregobj(labels, preds):\n    con = 2\n    x =preds-labels\n    grad =con*x / (np.abs(x)+con)\n    hess =con**2 / (np.abs(x)+con)**2\n    return grad, hess \n\ndef log_mae(y,yhat):\n    return mean_absolute_error(np.exp(y), np.exp(yhat))\n\nlog_mae_scorer = metrics.make_scorer(log_mae, greater_is_better = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline Model - Linear Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"Y = np.log(full_dataset[:model_index][\"loss\"]+200)\nX = full_dataset[:model_index].drop(['id', 'loss'], axis= 1)\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"linear_reg = linear_model.LinearRegression()\nlinear_reg.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# figure out the coefficient of each feature\n\nfig,ax = plt.subplots(figsize=(15,10))\nplt.xticks(rotation=45) \ntick_spacing = 3\nax.plot(x_train.columns,linear_reg.coef_,label='LR')\nax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\nplt.title(\"Feature coefficient of Linear Regression Model\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Coefficient\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred = linear_reg.predict(x_test)\nlog_mae(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ridge Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"alpha = [1, 5, 10, 20, 30, 40, 50]\n\nridge = Ridge()\nparameters = {'alpha': alpha}\nridge_regressor = GridSearchCV(ridge, parameters,scoring='neg_mean_squared_error', cv=5)\nridge_regressor.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ridge_regressor.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred = ridge_regressor.predict(x_test)\nlog_mae(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rrg = linear_model.Ridge(alpha=40)\nrrg.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LASSO Regression"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"larg = linear_model.Lasso(alpha=1e-7)\nlarg.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"y_pred = larg.predict(x_test)\nlog_mae(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(15,10))\nplt.xticks(rotation=45) \ntick_spacing = 3\nax.plot(x_train.columns,linear_reg.coef_,c='r',label='LR')\nax.plot(x_train.columns,larg.coef_,c='g',label=\"Lasso\")\nax.plot(x_train.columns,rrg.coef_,c='b',label=\"Ridge\")\nax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\nplt.title(\"Feature coefficient of Three Regression Model\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Coefficient\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Therefore, based on the performance, I will use ridge regression. Before go to the Xgboost model, I would like to submit my result of ridge regression. "},{"metadata":{"trusted":false},"cell_type":"code","source":"sub_x = full_dataset[model_index:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub_x.drop([\"loss\",\"id\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"final_predict = np.exp(ridge_regressor.predict(sub_x)) - 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"results1 = pd.DataFrame()\nresults1['id'] = full_dataset[model_index:].id\nresults1['loss'] = final_predict\nresults1.to_csv(\"sub.csv\", index=False)\nprint(\"Submission created.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The score is 1263.56702."},{"metadata":{},"cell_type":"markdown","source":"## Xgboost"},{"metadata":{"trusted":false},"cell_type":"code","source":"full_data_sparse = sparse.hstack((full_dataset_sparse,full_dataset[con_variables]), format='csr')\nprint(full_data_sparse.shape)\n\nmodel_x = full_dataset_sparse[:model_index]\nsubmission_x = full_dataset_sparse[model_index:]\nmodel_y = np.log(full_dataset[:model_index].loss.values + 200)\nID = full_dataset.id[:model_index].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def search_model(train_x, train_y, est, param_grid, n_jobs, cv, refit=False):\n## grid search for the best model\n    model = GridSearchCV(estimator=est,\n                         param_grid=param_grid,\n                         scoring=log_mae_scorer,\n                         verbose=10,\n                         n_jobs=n_jobs,\n                         iid=True,\n                         refit=refit,\n                         cv=cv)\n    # fit grid search model\n    model.fit(train_x, train_y)\n    print(\"Best score: %0.3f\" % model.best_score_)\n    print(\"Best parameters set:\", model.best_params_)\n    print(\"Scores:\", model.grid_scores_)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"param_grid = {'objective':[logregobj],\n              'learning_rate':[0.02, 0.04, 0.06, 0.08],\n              'n_estimators':[1500],\n              'max_depth': [9],\n              'min_child_weight':[50],\n              'subsample': [0.78],\n              'colsample_bytree':[0.67],\n              'gamma':[0.9],\n              'nthread': [-1],\n              'seed' : [1234]}\n\nwhile False:\n    model = search_model(model_x,\n                         model_y,\n                         xgb.XGBRegressor(),\n                         param_grid,\n                         n_jobs=1,\n                         cv=4,\n                         refit=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rgr = xgb.XGBRegressor(seed = 1234, \n                       learning_rate = 0.01, # smaller, better results, more time\n                       n_estimators = 1500, # Number of boosted trees to fit.\n                       max_depth=9, # the maximum depth of a tree\n                       min_child_weight=50,\n                       colsample_bytree=0.67, # the fraction of columns to be randomly samples for each tree\n                       subsample=0.78, # the fraction of observations to be randomly samples for each tree\n                       gamma=0.9, # Minimum loss reduction required to make a further partition on a leaf node of the tree, \n                       # the larger, the more conservative \n                       nthread = -1, # Number of parallel threads used to run xgboost.\n                       silent = False # Whether to print messages while running boosting.\n                      )\nrgr.fit(model_x, model_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_y = np.exp(rgr.predict(submission_x)) - 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.bar(range(len(rgr.feature_importances_)), rgr.feature_importances_,c='royalblue')\nplt.ylim(0,0.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"xgb.plot_importance(rgr,max_num_features=5,importance_type='weight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np.argsort(rgr.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"results2 = pd.DataFrame()\nresults2['id'] = full_dataset[model_index:].id\nresults2['loss'] = pred_y\nresults2.to_csv(\"sub2.csv\", index=False)\nprint(\"Submission created.\")","execution_count":null,"outputs":[]}],"metadata":{"kernel_info":{"name":"python3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"nteract":{"version":"0.15.0"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":"block","toc_window_display":false}},"nbformat":4,"nbformat_minor":4}