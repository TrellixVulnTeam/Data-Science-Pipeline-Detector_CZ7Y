{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's begin\n**From the introduction, we know that the goal is to estimate the loss which is continuous. So this project is about REGRESSION**\n\n**Steps:**\n1. Data Visualization\n    * Import Data\n    * Object Data Barplot\n    * Numerical Data Distplot\n    * Goal Data Normal Distribution\n2. Correlation\n    * Find the highly correlated columns\n3. Data Preparation\n    * Delete highly correlated columns\n    * Ajust Goal Data\n4. Modeling\n    * LGBMCLASSIFIER\n    * Export Result\n5. Hyperopt\n    * Find the optimized params\n    * Learning Curve","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/allstate-claims-severity/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/allstate-claims-severity/test.csv\")\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cat = train.iloc[:, 1:117]\ntrain_cont = train.iloc[:, 117:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 150))\nfor i, col in enumerate(train_cat.columns):\n    plt.subplot(30, 4, i+1)\n    sns.countplot(train_cat[col], order=train_cat[col].value_counts().sort_index().index)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 12))\nfor i, col in enumerate(train_cont.columns):\n    plt.subplot(4, 4, i+1)\n    sns.distplot(train_cont[col])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(np.log1p(train['loss']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"* For categorical cols, there are some cols which are totally unbalanced so that they might not be useful for the data analysis\n* For numerical cols, normal distrition is always regarded as the best situtaion, obviously some cols is either skewed or korted or just wierd\n* For numerical cols, further process to check each col is necessary -- correlation study","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#  Correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train.drop(columns='id').corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, fmt='.2f', linewidths=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"high_corr= []\nthreshold = 0.8\nfor i in range(len(corr)):\n    for j in range(i+1, len(corr)):\n        if corr.iloc[i,j] >= threshold or (corr.iloc[i, j]<=-threshold and corr.iloc[i, j] < 0):\n            high_corr.append([corr.iloc[i,j], i, j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for v, i, j in high_corr:\n    sns.pairplot(train_cont, x_vars=train_cont.columns[i], y_vars=train_cont.columns[j], size= 6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"* The five pairs are (1,9), (1,10), (6, 10), (6, 13), (11, 12)\n* From the graphics above, the five pairs are all with high correlation\n* It's necessary to remove some of them and 1, 6 are paired with two another col respectively, removing 1, 6 and 11 or 12 is my choice","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Data Preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# In order to make sure train & test sets would have same amount of cols(except loss) after modification\n\ndataset = pd.concat([train, test])\ndataset = dataset.drop(columns = ['cont1', 'cont6', 'cont11'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.get_dummies(dataset)\ndf_train = dataset[:len(train)]\ndf_test = dataset[len(train):]\ndf_test = df_test.drop(columns='loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.log1p(df_train['loss'])\ndf_train = df_train.drop(columns='loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression,Ridge, Lasso\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(df_train, y, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb = XGBRegressor(learning_rate=0.3, n_estimators=500)\n# xgb.fit(x_train, y_train)\n# mean_absolute_error(np.expm1(y_test), np.expm1(xgb.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb=XGBRegressor(seed=18, objective='reg:linear', n_jobs=-1, verbosity=0,\n#                        colsample_bylevel=0.764115402027029, colsample_bynode=0.29243734009596956, \n#                        colsample_bytree= 0.7095719673041723, gamma= 4.127534050725986, learning_rate= 0.02387231810322894, \n#                        max_depth=14, min_child_weight=135, n_estimators=828,reg_alpha=0.3170105723222332, \n#                        reg_lambda= 0.3660379465131937, subsample=0.611471430211575)\n# xgb.fit(x_train, y_train)\n# mean_absolute_error(np.expm1(y_test), np.expm1(xgb.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb = LGBMRegressor(objective='regression_l1', random_state=18, subsample_freq=1,\n                        colsample_bytree=0.3261853512759363, min_child_samples=221, n_estimators=2151, num_leaves= 45, \n                        reg_alpha=0.9113713668943361, reg_lambda=0.8220990333713991, subsample=0.49969995651550947, \n                        max_bin=202, learning_rate=0.02959820893211799)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb.fit(x_train, y_train)\nmean_absolute_error(np.expm1(y_test), (np.expm1(lgb.predict(x_test))))\n# mean_absolute_error(np.expm1(y_test), (np.expm1(lgb.predict(x_test))+np.expm1(xgb.predict(x_test)))/2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame({'id': df_test['id'], 'loss': np.expm1(lgb.predict(df_test))})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('sub.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Try hyperopt","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import hp, fmin, Trials, tpe, pyll","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def f(params):\n    lgb = LGBMRegressor(**params)\n    lgb.fit(x_train, y_train)\n    return mean_absolute_error(np.expm1(y_test), (np.expm1(lgb.predict(x_test))))\n#     return -cross_val_score(LGBMRegressor(**params), df_train, y, cv=10).mean()\n\nspace = {\n        'subsample_freq':hp.choice('subsample_freq', range(1, 5)),\n        'colsample_bytree':hp.uniform('colsample_bytree', 0.2, 0.5), \n        'min_child_samples':hp.choice('min_child_samples', range(200, 250, 5)), \n        'n_estimators': hp.choice('n_estimators', range(1000, 3000, 100)), \n        'num_leaves': hp.choice('num_leaves', range(20, 50, 5)), \n        'reg_alpha': hp.uniform('reg_alpha', 0.70, 1), \n        'reg_lambda': hp.uniform('reg_lambda', 0.70, 1), \n        'subsample': hp.uniform('subsample', 0.3, 0.6), \n        'max_bin':hp.choice('max_bin', range(150, 250, 5)), \n        'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.2))\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trial = Trials()\nbest = fmin(f, space, algo=tpe.suggest, max_evals=20, trials=trial)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# only idx of best parameters could be achieved from best, so according to space, the values of best parameters could be found\n\nparams = {'colsample_bytree':0.2, 'learning_rate': 0.013636902671116896, 'max_bin': 85, 'min_child_samples': 205, \n          'n_estimators': 2000,'num_leaves': 35,'reg_alpha': 0.9579863172141052,'reg_lambda': 0.8783040346489164,\n          'subsample': 0.5899650955658289,'subsample_freq': 2}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb = LGBMRegressor(**params)\nlgb.fit(df_train, y)\nsub = pd.DataFrame({'id': df_test['id'], 'loss': np.expm1(lgb.predict(df_test))})\nsub.to_csv('sub.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Learning Curve--to see whether it's ideal or not","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size, train_score, test_score = learning_curve(LGBMRegressor(**params), df_train, y, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_mean = train_score.mean(axis=1)\ntrain_std = train_score.std(axis=1)\ntest_mean = test_score.mean(axis=1)\ntest_std = test_score.std(axis=1)\n\nplt.figure(figsize=(10, 8))\nplt.plot(train_size, train_mean, 'o-', linewidth=3)\nplt.fill_between(train_size, train_mean+train_std, train_mean-train_std, alpha=0.1)\nplt.plot(train_size, test_mean, 'o-', linewidth=3)\nplt.fill_between(train_size, test_mean+test_std, test_mean-test_std, alpha=0.1)\nplt.title('Learning Curve', size=20)\nplt.xlabel('Training Examples')\nplt.ylabel('Score')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The curves look pretty reasonable","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}