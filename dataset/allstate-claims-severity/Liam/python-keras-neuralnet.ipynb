{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d07426dc-7c90-312c-6dc7-462d68d24145"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"84a1d0c7-a0ef-c8e2-46fa-19f8f60fc6fd"},"outputs":[],"source":"from keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Reshape, Flatten\nfrom keras.regularizers import l2, activity_l2\nfrom keras.layers.convolutional import Convolution1D, MaxPooling1D\nfrom keras.optimizers import Adam\n\nnp.random.seed(1)  # for reproducibility\n\ntrain_in = pd.read_csv('../input/train.csv')\ntest_in = pd.read_csv('../input/test.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b6909ee3-bc53-eb89-f925-8e2684c683e3"},"outputs":[],"source":"### Create dummy variables and prepare training/test data frames...\n\n# Save our target variable column before we drop it\ny_train = train_in.iloc[:, -1]\n\n# Stack training and test data into one big data frame so we cover all categories found in both\ndata = pd.concat((train_in.iloc[:, :-1], test_in))\n\n# Get dummy variables for all categorical columns\ncolnames = []\nX_data = []\n\n# For each categorical column...\nfor c in [i for i in data.columns[1:-1] if 'cat' in i]:\n    # Get dummy variables for that column\n    dummies = pd.get_dummies(data[c])\n    # Drop the last dummy, as its value is implied by the others (all 0's = 1)\n    dummies = dummies.iloc[:, :-1].values.astype(np.bool)\n    X_data.append(dummies)\n    # Create column names for those dummy variables\n    colnames += [c + '_' + str(i) for i in range(dummies.shape[1])]\n    \n# Stack all dummy variables into big dataframe with the colnames\nX_data = pd.DataFrame(np.hstack(X_data), columns=colnames)\n\n# Drop any columns with only 1 value (so drop unused categories, if any)\nX_data = X_data.iloc[:, [len(pd.unique(X_data.loc[:,c]))>1 for c in X_data.columns]]\n\n# Get the other (continuous) columns\nX_data_cont = np.vstack([data[c].values.astype(np.float32) \\\n                         for c in data.columns[1:-1] if 'cat' not in c]).T\n\n# Final data frame is the dummy variables + the continuous variables\nX_data = X_data.join(pd.DataFrame(X_data_cont, \n                    columns=[c for c in data.columns[1:-1] if 'cat' not in c]))\n\n# Split into train and test data frames\ntrain = X_data[:len(y_train)].join(y_train)\ntest = X_data[len(y_train):]\n\n# Create X train and y train arrays for input to NN\nXt = train.iloc[:, :-1].values\nyt = train.iloc[:, -1].values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f2800811-7996-65ce-06c3-b63d76b614fe"},"outputs":[],"source":"### Define and compile a fairly deep neural network...\ndropout_p=0.5\n\nmodel = Sequential()\nmodel.add(Dropout(0.1, input_shape=[Xt.shape[1]]))\nmodel.add(Dense(2048))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(dropout_p))\nmodel.add(Dense(1024))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(dropout_p))\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(dropout_p))\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(dropout_p))\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(1))\nmodel.summary()\nmodel.compile(loss='mae', optimizer=Adam())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"798a6519-c295-a059-c272-6791b48c5b4a"},"outputs":[],"source":"### Fit the neural network model (takes 3-4 mins per epoch, 1 epoch seems to provide best results,\n### though I only tried with 1 or 2)\nhistory = model.fit(Xt, yt,\n                    batch_size=96,\n                    nb_epoch=1,\n                    verbose=2, \n                    #validation_data=(Xv.values, yv.values),\n                    shuffle=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ebd0e3f-8a84-744b-11fb-84c3011b35ab"},"outputs":[],"source":"### Predict the test set\n# This NN submission got me to 1214 on LB\n# Taking logs of the loss before training and then exponentiating the predictions degraded performance\n# Running two epochs also degraded performance\npreds_nn = model.predict(test.values)\nsubmission_nn = pd.DataFrame(test_in['id'])\nsubmission_nn['loss'] = preds_nn\nsubmission_nn.to_csv('submission_nn.csv', index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}