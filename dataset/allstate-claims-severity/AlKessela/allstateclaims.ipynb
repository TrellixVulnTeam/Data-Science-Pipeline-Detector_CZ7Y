{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"538d136d-7996-ae1e-4d22-a6ac3742bc3e"},"source":"Allstate Claims analysis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3dc17d8d-31d4-ffd5-aa3d-51dae02e4b42"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt # for plotting\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"68a87654-c30b-d69b-e8c7-a8241e532f9a"},"outputs":[],"source":"#read data from CSV file\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n#Some info about the data\n#train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"851cf109-aa03-415d-ab16-a516c32aa1a6"},"outputs":[],"source":"#Data housekeeping\n#Category variables 116\ntrain_cat = train.ix[:,'cat1':'cat116']\n#Continuous variable 14\ntrain_cont = train.ix[:,'cont1':'cont14']\n#train_cat_dummy = pd.get_dummies(train_cat)\n#print(train_cat.head(2))\n#print(train_cat_dummy.head(2))\n#Number of training samples\nprint(\"number of traning samples : {}\".format(train.shape[0]))\nprint(\"number of test samples: {}\".format(test.shape[0]))\n#count the number of unique values under the categorical variables\nprint(\"number of unique categorical values : {}\".format(len(pd.unique(train_cat[train_cat.columns[1:]].values.ravel()))))\n#Check if there are any null values\n#print(train.isnull().values.any())\n#print(test.isnull().values.any())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"102a901c-7c02-7791-7423-f28d2fa75398"},"outputs":[],"source":"#correlation between continuous predictors and traget\ntrain_corr_loss = train.corr()[\"loss\"]\nax = train_corr_loss.iloc[1:-1].plot(kind='bar',title=\"continuous features correlation with target\", figsize=(5,5), fontsize=12)\nax.set_ylabel(\"correlation value\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"47c8bea2-d527-9ca4-2068-dda394e4553c"},"source":"As we can see cont2 has the higher correlation with target followed by cont8,cont3,cont11 and cont12.\ncont13 has the lowest correlation.In general, all these features have low correlation values to the target, < 0.5"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"150bbd0e-8cf0-ac3e-cf68-141215a140ff"},"outputs":[],"source":"#classifcation just considering the continous features(why?)\ny_train1 = np.asarray(train['loss'])\nX_test1 = test.ix[:,'cont1':'cont14']\nlreg = LinearRegression()\nlreg.fit(train_cont,y_train1)\ny_pred = lreg.predict(X_test1)\nprint(\"Training set score: {:.2f}\".format(lreg.score(train_cont, y_train1)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"9fe494b3-2e63-a902-01ee-1970260b7e70"},"source":"The fact that a lot of features are missing implies too simple model(under fit)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"70659f5b-1882-e0a3-7151-9f22bcaa42d1"},"outputs":[],"source":"#Categorical features analysis\nfrom sklearn.preprocessing import LabelEncoder\ncatFeatures = []\nfor colName in train_cat.columns:\n    le = LabelEncoder()\n    le.fit(train_cat[colName].unique())\n    train_cat[colName] = le.transform(train_cat[colName])\ntrain_cat.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5a7e8e3b-a911-090a-5795-d8c9b95bb3f6"},"outputs":[],"source":"catFeatures = []\ntest_cat = test.ix[:,'cat1':'cat101']\nfor colName in test_cat.columns:\n    le = LabelEncoder()\n    le.fit(test_cat[colName].unique())\n    test_cat[colName] = le.transform(test_cat[colName])\ntest_cat.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9a6aa207-ac80-e14c-a82e-ed9e4f69508d"},"outputs":[],"source":"test_cont = test.ix[:,'cont1':'cont14']\nX_train = train_cat.ix[:,'cat1':'cat101'].join(train_cont)\nX_test = test_cat.join(test_cont)\n#verify shapes\n#Categorical features analysis using  dummy variables\nprint(\"(train)(test):{}{}\".format(X_train.shape,X_test.shape))\n\nlreg.fit(X_train,y_train1)\ny_pred = lreg.predict(X_test)\nprint(\"Linear: Training set score: {:.2f}\".format(lreg.score(X_train, y_train1)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe79f27e-a1c4-5aa5-85f6-2f579ff9cd89"},"outputs":[],"source":"#Categorical features analysis using  dummy variables\ntrain_cat_dummy = pd.get_dummies(train.ix[:,'cat1':'cat101'])\ntest_cat_dummy = pd.get_dummies(test_cat)\n\nprint(\"new dummy DF shapes(train)(test):{}{}\".format(train_cat_dummy.shape,test_cat_dummy.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4a44f78d-a71b-385a-6963-ec23c788ff4c"},"outputs":[],"source":"X_train = train_cat_dummy.join(train_cont)\nX_test = test_cat_dummy.join(test_cont)\nlreg.fit(X_train,y_train1)\ny_pred = lreg.predict(X_test)\nprint(\"Linear: Training set score: {:.2f}\".format(lreg.score(X_train, y_train1)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"07590e4a-01e2-47ca-baf6-38e3eff00314"},"source":"\"The kernel was killed for trying to exceed the memory limit of\n 8589934592; you can use the restart button in the toolbar to try.\n\"\nIt looks like I have to find a way to reduce the number of features so as to reduce the memory requirement for prediction. So I randomly truncated the number of categorical features, including the first 30 gives 32% score for the training data prediction.similarly, including 80 of the categorical features  gives score of 47%, 95|49%, 101|49%,"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4ddd121c-a9df-0be6-87ae-16ae35f8932b"},"outputs":[],"source":"#Other SGD regressor\nfrom sklearn.linear_model import SGDRegressor\nreg= SGDRegressor()\nreg.fit(X_train, y_train1)\ny_pred = reg.predict(X_test)\nprint(\"SGD: Training set score: {:.2f}\".format(reg.score(X_train, y_train1)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6bcc3eb9-bdfd-fefb-60d9-e5f6af3a2284"},"outputs":[],"source":"#Using Lasso\n'''from sklearn import linear_model\nreg = linear_model.Lasso(alpha = 0.6)\nreg.fit(X_train, y_train1)\ny_pred = reg.predict(X_test)\nprint(\"Lasso: Training set score: {:.2f}\".format(reg.score(X_train, y_train1)))'''"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8af2833-cc55-d215-ab68-2432c1f77f24"},"outputs":[],"source":"submission = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"loss\": y_pred\n    })\nsubmission.to_csv('sample_submission.csv', index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}