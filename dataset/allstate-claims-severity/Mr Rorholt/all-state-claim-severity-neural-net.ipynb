{"cells":[{"metadata":{"_uuid":"2c7db774-323f-421a-a439-ed4e1a85a617","_cell_guid":"16cc6281-3d81-497b-857d-d87605cdd367","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.neural_network import MLPRegressor \n\n#visualisation\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Step 1: Load  data\n\ntrain_data = pd.read_csv('../input/allstate-claims-severity/train.csv')\ntest_data = pd.read_csv('../input/allstate-claims-severity/test.csv')\nprint (\"Finished reading train and test data...\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20cf2128-efdd-4d31-ad7d-146ab0605045","_cell_guid":"d90fe571-6dc3-4c07-bae2-8e593d4bd266","trusted":true},"cell_type":"code","source":"\n#step 2: Normalization & preprocessing\n#To do one-hot-encoding, we need to concatenate data sets, see\n#https://medium.com/@vaibhavshukla182/how-to-solve-mismatch-in-train-and-test-set-after-categorical-encoding-8320ed03552f\n\n#add train column \ntrain_data['train'] = 1\ntest_data ['train'] = 0\ntest_data['loss'] = 0.0\n\n#change distribution of the loss - as it is very scewed\n# Set the width and height of the figure\nplt.figure(figsize=(16,6))\ntrain_data['loss'] = np.log(train_data['loss']+200)\nsns.distplot(train_data['loss'], hist = True, rug = True)\n\n\n\nprint (\"#of columns in train data: \", train_data.columns.size)\nprint (\"#of columns in test data: \", test_data.columns.size)\n\n#combine datasets\ncombined_data = pd.concat([train_data, test_data])\n\n#normalize numerical data (subtract mean & divide by st.dev.)\nnumerical_data = combined_data.select_dtypes(include = \"float64\").filter(like='cont')\n\n#convert categorical and object data using one-hot-encoding\ncategorical_data = combined_data.select_dtypes(include=[\"object\", \"category\"])\nohe_categorical_data = pd.get_dummies (categorical_data)\n\n\n#concatenate id, numerical, ohe-encoded catetogires, loss and train indicator\ncombined_data = pd.concat([combined_data['id'],ohe_categorical_data, numerical_data, combined_data['loss'], combined_data['train']], axis = 1)\n\n#extract train data\ntrain_data = combined_data[combined_data['train'] == 1]\ntrain_data.drop(['train'], axis = 1, inplace = True)\n#exctract test data and delete columns\ntest_data = combined_data[combined_data['train'] == 0]\ntest_data.drop(['train'], axis = 1, inplace = True)\ntest_data.drop(['loss'], axis = 1, inplace = True)\n\n#print (\"===Train data===\")\n#print (\"#columns: {}\".format(train_data.columns.size))\n#print (train_data.head())\n#print (\"===Test data===\")\n#print (\"#columns: {}\".format(test_data.columns.size))\n#print (test_data.head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Step 2: Build and train neural net using scikit MLPRegressor lib\nX_train = train_data.drop(['id', 'loss'], axis = 1)\ny_train = train_data[\"loss\"]\n\nprint (\"Building and training neural network on {} data points...\".format(len(train_data)))\nclaimNet= MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n             beta_2=0.999, early_stopping=True, epsilon=1e-08,\n             hidden_layer_sizes=(1500,1500,1500), learning_rate='constant',\n             learning_rate_init=0.001, max_iter=500, momentum=0.9,\n             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n             random_state=None, shuffle=True, solver='adam', tol=0.0001,\n             validation_fraction=0.1, verbose=True, warm_start=False)\n\nclaimNet.fit(X_train,y_train)\nprint (\"Finished training...\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Loading test data...\")\n#load and transform test data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = test_data.drop(['id'], axis = 1)\nprint (\"Starting predictions on {} data points\".format(len(x_test)))\npredictions = claimNet.predict(x_test) \noutput = pd.DataFrame({'id': test_data.id,'loss': np.exp(predictions)-200})\n\nsns.distplot(np.exp(predictions)-200, hist = True, rug = True)\n\noutput.to_csv('submission.csv', index=False)\nprint(\"Finished test and I/O to csv file\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}