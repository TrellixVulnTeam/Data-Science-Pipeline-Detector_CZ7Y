{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom math import sqrt","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Print all rows and columns. Dont hide any\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This data set contains 116 categorical columns plus 14 constant columns and one predictor (loss) column"},{"metadata":{"trusted":false},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Data Cleaning__"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Checking missing values\nprint('Is the training data contains any missing values? ' + str(train.isnull().any().any()) + '\\n'\n     + 'Is the testing data contains any missing values? ' + str(test.isnull().any().any()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_list_name = list(train.columns.values)\ntrain_list_name.pop() #pop out the loss column\ntest_list_name = list(test.columns.values)\nprint('Are the columns identical to each other for both train & test dataset? ' + str(train_list_name == test_list_name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Check column values for each categorical columns\ndef showunique(df):\n    list_name = list(train.columns.values)\n    for i, col_name in enumerate(list_name):\n        if col_name[:3] == 'cat':\n            print(df.groupby('cat' + str(i))['id'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"showunique(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The takeaways from this column unique value check shows that from cat1 to cat73, we only have 2 selection choices A and B.<br />\n\nFor these columns some of them have uneven distribution of As and Bs, which makes the values not important, __for example__: <br />\n\nIn cat 70 there are 188295 As (99.98%) and 23Bs (0.02%). These columns can be removed to reduce the dimensionality of our model<br /> \n\n- As for cat73 to cat76 we have A, B and C. <br />\n\n- From cat77 to cat87 we have A, B, C and D.<br />\n\n- Cat88 has value A, B, D, E. <br />\n\n- Starting from cat89 we have more than 4 unique values for each categorical features. <br />\n"},{"metadata":{},"cell_type":"markdown","source":"## __First of all, Let us take care of the constant part of the dataset__"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Separate the dataset, starting from column index 117\ntrain_cont = train.iloc[:, 117:]\ntest_cont = test.iloc[:, 117:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_cont.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_cont.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Checking the skewness of the remaining dataset, the ones close to 0 are less skewed data\nprint(train_cont.skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Heatmap to check the correlation\ncor = train_cont.corr()\nf, ax = plt.subplots(figsize = (12, 8))\nsns.heatmap(cor, vmax = 0.9, annot = True, square = True, fmt = '.2f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Takeaways from the heatmap__ <br />\n\n- Most of the continous variables are somewhat correlated with the loss (lowest cont13)\n- Some of the variables are strongly correlated with each other (eg: cont1 and cont9 correlation is 0.93)\n- PCA or SVD could be applied to extract the most important features from these variables"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Let us apply PCA\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\nloss = train_cont.loc[:, train_cont.columns == 'loss']\ntrain_cont_phase = train_cont.loc[:, train_cont.columns != 'loss']\n\nscaled_train_cont = StandardScaler().fit_transform(train_cont_phase)\nscaled_test_cont = StandardScaler().fit_transform(test_cont)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Do the PCA\npca = PCA()\npca.fit(scaled_train_cont)\npca.data = pca.transform(scaled_train_cont)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Percentage variance of each pca component stands for\nper_var = np.round(pca.explained_variance_ratio_*100, decimals = 1)\n#Create labels for the scree plot\nlabels = ['PC' + str(x) for x in range(1, len(per_var)+1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot the data\nplt.bar(x=range(1, len(per_var)+1), height=per_var, tick_label = labels)\nplt.ylabel('percentage of Explained Variance')\nplt.xlabel('Principle Component')\nplt.title('Scree plot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"variance = 0\ncount = 0\nfor i in pca.explained_variance_ratio_:\n    if variance <= 95:\n        variance += i * 100\n        count+=1\nprint(str(np.round(variance, 2)) + '% of the variance is explained by ' + str(count) + ' of Principle Components')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Thus, we are going to use 9 Principle components to preserve 96.8% of the entire 14 constant part of variance__"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Extract the PC1 through PC9 information\ntrain_append = pd.DataFrame(data=pca.data[:,:9], columns = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7'\n                                                           , 'PC8', 'PC9'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_append.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Glue the PC data back to the training dataset\nnew_train = pd.concat((train.iloc[:, :117], train_append), axis = 1)\nnew_train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Now performing the same action for the testing dataset\npca.fit(scaled_test_cont)\npca.data = pca.transform(scaled_test_cont)\ntest_append = pd.DataFrame(data=pca.data[:,:9], columns = ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7'\n                                                           , 'PC8', 'PC9'])\nnew_test = pd.concat((test.iloc[:, :117], test_append), axis = 1)\nnew_test.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Looking good, now we try to handle the 116 categorical variables__\n\n__Strategy:__\n - Remove the noisy columns that has very unevenly distributed columns\n - One hot encoding the columns (Since the columns are more like the nominal variables)\n - We are not considering using label encoding since the variables does not look like ordinal"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Check the distributions of the catevariables:\n# Count of each label in each category\n\n#names of all the columns\ncols = new_train.columns\n\n#Plot count plot for all attributes in a 29x4 grid\nn_cols = 4\nn_rows = 29\nfor i in range(n_rows):\n    fg,ax = plt.subplots(nrows=1,ncols=n_cols,sharey=True,figsize=(12, 8))\n    for j in range(n_cols):\n        sns.countplot(x=cols[i*n_cols+j+1], data=new_train, ax=ax[j])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the graph we plotted: Cat14, 15, 17, 18, 19, 20, 21, 22, 29, 30, 32, 33, 34, 35, 42, 43, 45, 46, 47, 48, 51, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 70, 74, 77, 78, 85 can be categorized as noisy columns\n\nBut we can not just rely on our eyes, we are going to remove them using calculations"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Show dominanting percentage less than 2%, and drop them during the process\ndef show_and_drop_percentage(df, df2):\n    for i in range(1, 117):\n        A = df['cat' + str(i)].value_counts()\n        per = sum(A[1:]) / sum(A) * 100\n        if per < 2:\n            print('cat' + str(i) + ': ' + 'Dominating percentage is: ' + str(np.round(per, 2)) + '%')\n            df = df.drop(['cat' + str(i)], axis = 1)\n            df2 = df2.drop(['cat' + str(i)], axis = 1)\n    print('-' * 80 + '\\n')\n    print('Cleaning complete for columns cat1 to cat 116, The above categories had been dropped\\n')\n    return df, df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Operate on the training set\nremoved_train, removed_test = show_and_drop_percentage(new_train, new_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"removed_train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"removed_test.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Check if the same procedure was done on the train & test columns\nany(removed_train.columns == removed_test.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### __Now we dummy coding the remaining categorical variables__\n\nThanks to: https://www.kaggle.com/sharmasanthosh/exploratory-study-on-ml-algorithms\nprovide the method to achieve one-hot encoding considering value difference in values in train & test dataset\n(pandas get dummies would result in unevenly columns due to column value differences)"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Remove the id columns\nremoved_train = removed_train.iloc[:, 1:]\nremoved_test = removed_test.iloc[:, 1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"removed_train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#range of features considered\nsplit = 78\n\n#Grab out the categorical variables\ncat_train = removed_train.iloc[:, :split]\ncat_test = removed_test.iloc[:, :split]\n\n#List the column names\ncols = cat_train.columns\n\n#Variable to hold the list of variables for an attribute in the train and test data\nlabels = []\n\nfor i in range(0,split):\n    train = cat_train[cols[i]].unique()\n    test = cat_test[cols[i]].unique()\n    labels.append(list(set(train) | set(test))) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__For Training Data Set__"},{"metadata":{"trusted":false},"cell_type":"code","source":"#One hot encode all categorical attributes \n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n\ncats = []\nfor i in range(0, split):\n    #Label encode\n    label_encoder = LabelEncoder()\n    label_encoder.fit(labels[i])\n    feature = label_encoder.transform(cat_train.iloc[:,i])\n    feature = feature.reshape(cat_train.shape[0], 1)\n    #One hot encode\n    onehot_encoder = OneHotEncoder(sparse=False,categories= [range(len(labels[i]))])\n    feature = onehot_encoder.fit_transform(feature)\n    cats.append(feature)\n    \n# Make a 2D array from a list of 1D arrays\nencoded_cats = np.column_stack(cats)\n\n# Print the shape of the encoded data\nprint(encoded_cats.shape)\n\n#Concatenate encoded attributes with continuous attributes\ntrain_encoded = np.concatenate((encoded_cats,removed_train.iloc[:,split:].values),axis=1)\n\n#Transfer it back into pandas dataframe\ntrain_encoded = pd.DataFrame(data=train_encoded)\ntrain_encoded.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__For Testing Data Set__"},{"metadata":{"trusted":false},"cell_type":"code","source":"#One hot encode all categorical attributes\ncats = []\nfor i in range(0, split):\n    #Label encode\n    label_encoder = LabelEncoder()\n    label_encoder.fit(labels[i])\n    feature = label_encoder.transform(cat_test.iloc[:,i])\n    feature = feature.reshape(cat_test.shape[0], 1)\n    #One hot encode\n    onehot_encoder = OneHotEncoder(sparse=False,categories= [range(len(labels[i]))])\n    feature = onehot_encoder.fit_transform(feature)\n    cats.append(feature)\n    \n# Make a 2D array from a list of 1D arrays\nencoded_cats = np.column_stack(cats)\n\n# Print the shape of the encoded data\nprint(encoded_cats.shape)\n\n#Concatenate encoded attributes with continuous attributes\ntest_encoded = np.concatenate((encoded_cats,removed_test.iloc[:,split:].values),axis=1)\n\ntest_encoded = pd.DataFrame(data=test_encoded)\ntest_encoded.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## __Now we can prepare our data for modeling__\n\nModels to consider:\n   - linear Regression\n   - Lasso\n   - Ridge\n   - Elastic Net\n   - Stochastic Gradient Descent (SGD)\n   - RandomForest\n   - Xgboost\n   - LightGBM"},{"metadata":{"trusted":false},"cell_type":"code","source":"#First of all, we do train test split of our dataset\nfrom sklearn.model_selection import train_test_split\n\n#Set our random seed to ensure productive result\nseed = 2019\n\nX_train, X_test, y_train, y_test = train_test_split(\n     train_encoded, loss, test_size=0.25, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### __Linear Regression__"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train) \nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_train_scaled, y_train)\n\n#Calculating the MAE\nlin_pred = lin_reg.predict(X_test_scaled)\nlin_result = mean_absolute_error(y_test, lin_pred)\nlin_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__the MAE of linear regession probably told us why we should NEVER use simple linear regression for our model__\n\n__We should go ahead and try some more complex model for our prediction__"},{"metadata":{},"cell_type":"markdown","source":"### __Elastic Net__"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\n\nela = ElasticNet(random_state=seed)\nela.fit(X_train_scaled, y_train)\n\nela_pred = ela.predict(X_test_scaled)\nela_result = mean_absolute_error(y_test, ela_pred)\nela_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Elastic Net is doing much better, but still with quite large error rate"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Make predictions using the model\n#Write it to the file\nTest_scaled = scaler.transform(test_encoded)\n\npredictions = ela.predict(Test_scaled)\n\npd.DataFrame(predictions, columns = ['loss']).to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### __Stochastic Gradient Descent (SGD)__"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import SGDRegressor\n\nsgd = SGDRegressor(max_iter = 1500, eta0=1e-14,\n                  learning_rate = 'adaptive',\n                  penalty = 'elasticnet')\nsgd.fit(X_train_scaled, y_train)\n\nsgd_pred = sgd.predict(X_test_scaled)\nsgd_result = mean_absolute_error(y_test, sgd_pred)\nsgd_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sgd_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}