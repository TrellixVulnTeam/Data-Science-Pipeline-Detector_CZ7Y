{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"39aa54f0-51ce-5847-d1aa-f03c064ab21e"},"source":"### 1. Read Data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8864ede-ffb8-3d2d-768c-8b2a66fed366"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\n### Read Training Data and Testing Data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntrain.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"d9d44a39-29ac-f649-517c-70e948b67fed"},"source":"### 2. Data Pre-Processing\n    \n    -Simple Precessing\n    -Create Dummy Variables\n    -Normalize Numeical Variables and Run PCA\n    -Chcek Distribution of Target Variable \"Loss\" "},{"cell_type":"markdown","metadata":{"_cell_guid":"116f0fc7-e8a1-89b1-0d93-99e6281140f9"},"source":"    2.1 Simple Precessing"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"026ebc28-aa5d-7c06-eb42-d3ae6e6aaaff"},"outputs":[],"source":"### Drop id and Seperate loss\ntrain_id = train['id']\ntest_id =  test['id']\ntrain_label = train['loss']\ntrain.drop('id',axis=1,inplace=True)\ntrain.drop('loss',axis=1,inplace=True)\ntest.drop('id',axis=1,inplace=True)\nprint (train.shape, test.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7cd83898-fe5a-d57f-941c-92a065d958a2"},"source":"    2.2 Create Dummy Variables"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9584c381-eece-5a1d-d89a-1cfed0716dd0"},"outputs":[],"source":"### Column 0-115(index) are categorical, 116 - 129 are numeric\n### Create Dummy Variables \ndummies_train = pd.get_dummies(train[train.columns[0:116]])\ndummies_test = pd.get_dummies(test[test.columns[0:116]])\nprint(\"The shape of train_new and test_new are: \" + \"%s and %s\" % (dummies_train.shape, dummies_test.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6ff0aa7e-5316-dfc0-fe87-2be74d4a3796"},"outputs":[],"source":"### Note we have different size for dummies_train and dummies_test!\n### get the columns in train that are not in test\ncol_to_add = np.setdiff1d(dummies_train.columns, dummies_test.columns)\n\n### add these columns to test, setting them equal to zero\nfor c in col_to_add:\n    dummies_test[c] = 0\n### select and reorder the test columns using the train columns\ndummies_test = dummies_test[dummies_train.columns]\nprint(\"The shape of train_new and test_new are: \" + \"%s and %s\" % (dummies_train.shape, dummies_test.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a96aad51-fb48-11ca-3285-9cd9b0576d68"},"outputs":[],"source":"### Chcek if they have the same columns\nmismatch = 0\nfor i in range(len(dummies_train.columns)):\n    if dummies_train.columns[i] != dummies_test.columns[i]:\n        mismatch += 1\nprint(\"We have %d mismatch.\" % (mismatch))"},{"cell_type":"markdown","metadata":{"_cell_guid":"3e6af559-d0c5-ded5-baaa-6a2b4a0c4dc2"},"source":"    2.3. Normalize Numeical Variables and Run PCA"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fa8bad73-65a8-6b2d-0282-3c6b3de41cbd"},"outputs":[],"source":"### Normalize numeric variables using Min-Max-Scaler\nfrom sklearn import preprocessing\nnumeric_train = train[train.columns[116:129]]\nnumeric_test = test[test.columns[116:129]]\nmin_max_scaler = preprocessing.MinMaxScaler().fit(numeric_train)\n\n### Apply Min-Max-Scalar\ntrain_norm = min_max_scaler.transform(numeric_train)\ntest_norm = min_max_scaler.transform(numeric_test)\n\n### Convert numeric to dataframe \ntrain_norm = pd.DataFrame(train_norm,columns=list(numeric_train.columns))\ntest_norm = pd.DataFrame(test_norm,columns=list(numeric_test.columns))\n\nprint(\"The shape of train_norm and test_norm are: \" + \"%s and %s\" % (train_norm.shape, test_norm.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ffe8d316-8dbe-6102-0b7f-1af3d8d51d4f"},"outputs":[],"source":"### Chcek correlation on numeric variables\nprint(numeric_train.corr())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ca772be2-b02d-9853-2d79-c49b29c77efa"},"outputs":[],"source":"### Run PCA\nfrom sklearn import decomposition\npca = decomposition.PCA(n_components=6)\npca.fit(train_norm)\ntrain_norm_pca = pca.transform(train_norm)\nprint(pca.explained_variance_ratio_) \ntest_norm_pca = pca.transform(test_norm)\nprint(pca.explained_variance_ratio_) \n### Convert numeric to dataframe \ntrain_norm_pca = pd.DataFrame(train_norm_pca,columns=[\"PCA1\",\"PCA2\",\"PCA3\",\"PCA4\",\"PCA5\",\"PCA6\"])\ntest_norm_pca = pd.DataFrame(test_norm_pca,columns=[\"PCA1\",\"PCA2\",\"PCA3\",\"PCA4\",\"PCA5\",\"PCA6\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ca66e221-8416-3334-1e4c-1ccc8215a3a5"},"outputs":[],"source":"### Put them back together\ntrain_new = pd.concat((dummies_train,train_norm_pca ),axis=1)\ntest_new = pd.concat((dummies_test, test_norm_pca),axis=1)\nprint(\"The shape of train_new and test_new are: \" + \"%s and %s\" % (train_new.shape, test_new.shape))"},{"cell_type":"markdown","metadata":{"_cell_guid":"02b3bbc0-8425-8695-fe39-64825fad3bad"},"source":"    2.4. Chcek Distribution of Target Variable \"Loss\" "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"499d33a2-ee43-8e5f-d40a-36a6d36ce61a"},"outputs":[],"source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline  \nsns.distplot(train_label, color = 'b', hist_kws={'alpha': 0.9}, kde = False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"821ea712-2669-1221-8fc3-c6c6ab6232f1"},"outputs":[],"source":"### Try Log(\"loss\") - way better!\ntrain_label_log = np.log1p(train_label)\nsns.distplot(train_label_log, color = 'r', hist_kws={'alpha': 0.9}, kde = False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7b32bda5-e250-a5e7-1724-d4e6c1b1c3bb","collapsed":true},"outputs":[],"source":"### Delete redundant variables to free memory\ndel dummies_test, dummies_train, numeric_train, numeric_test, train_norm, test_norm"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ce05271b-f066-64f1-629c-a45bff458703","collapsed":true},"outputs":[],"source":"### 80% - 20% training and testing split, random_state=50\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_new, train_label_log, test_size=0.2, random_state=50)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9b741fc9-334d-b0f8-debf-880b41c8f0ef"},"source":"### Next Step: Feature Selection\n\n### Hope it helps!"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}