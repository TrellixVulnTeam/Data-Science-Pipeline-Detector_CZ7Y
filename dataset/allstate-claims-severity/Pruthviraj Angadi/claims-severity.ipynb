{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nsample_submission = pd.read_csv(\"../input/allstate-claims-severity/sample_submission.csv\")\ntest_data = pd.read_csv(\"../input/allstate-claims-severity/test.csv\")\ntrain_data = pd.read_csv(\"../input/allstate-claims-severity/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train data dimensions: \", train_data.shape)\nprint(\"Test data dimensions: \", test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Exploring the train_data\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of missing values', train_data.isnull().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Exploring the data stastically \ntrain_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Exploring the columns of data\ntrain_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By seeing both Train Data and Test Data there are 132 columns in train data and 131 column in test data. 'Loss' column is missing in test data indicating that it is the **target**."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Counting the Feature of train_data\ncont_Featureslist = []\nfor colName,x in train_data.iloc[1,:].iteritems():\n    #print(x)\n    if(not str(x).isalpha()):\n        cont_Featureslist.append(colName)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cont_Featureslist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_Featureslist.remove('id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting Corelation between countinues features and target **"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To find correlation between features and target feature\ncorrelationMatrix = train_data[cont_Featureslist].corr().abs()\n\nplt.subplots(figsize=(15, 10))\nsns.heatmap(correlationMatrix,annot=True)\n\n# Mask unimportant features\nsns.heatmap(correlationMatrix, mask=correlationMatrix < 1, cbar=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.distplot(train_data[\"loss\"])\nsns.boxplot(train_data[\"loss\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we can see loss is highly right skewed data. This happened because there are many outliers in the data. Lets apply log to see if we can get normal distribution. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.distplot(np.log1p(train_data[\"loss\"]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we got normal distribution by applying logarithm on loss function.\n\nFinally we got normal distribution, so we can train model using target feature as log of loss. So there is no need to remove outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"catCount = sum(str(x).isalpha() for x in train_data.iloc[1,:])\nprint(\"Number of categories: \",catCount)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 116 categories with non alphanumeric values, most of the machine learning algorithms doesn't work with alpha numeric values. So, lets convert it into numeric values."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_Featureslist = []\nfor colName,x in train_data.iloc[1,:].iteritems():\n    if(str(x).isalpha()):\n        cat_Featureslist.append(colName)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data[cat_Featureslist].apply(pd.Series.nunique))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conver categorical string values to numeric values"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfor cf1 in cat_Featureslist:\n    le = LabelEncoder()\n    le.fit(train_data[cf1].unique())\n    train_data[cf1] = le.transform(train_data[cf1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Making Prediction by training model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"featureslist = []\nfor colName,x in train_data.iloc[1,:].iteritems():\n    #print(x)\n    if(not str(x).isalpha() or str(x).isalpha):\n        featureslist.append(colName)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"featureslist.remove('id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Finding Root mean squred error for DecisionTreeRegressor**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import nessery models\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.model_selection import train_test_split\n\nX = train_data[featureslist]\ny = train_data.loss\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\ndtr_model = DecisionTreeRegressor()\ndtr_model.fit(X_train, y_train)\ny_pred = dtr_model.predict(X_test)\nval_mae = mean_absolute_error(y_pred, y_test)\nmse_test = MSE(y_test, y_pred)\nrmse_test = mse_test**(1/2)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))\nprint('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After checking the model with DecisionTreeRegressor model we get mean absolute error as \"2\" and RMSE as \"122.531\" which is greater than LinearRegression Model."},{"metadata":{},"cell_type":"markdown","source":"**Chicking the Root mean squared error for RandomForestRegressor method**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf_model = RandomForestRegressor(random_state=1)\nrf_model.fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\nval_mae = mean_absolute_error(y_pred, y_test)\nmse_test = MSE(y_test, y_pred)\nrmse_test = mse_test**(1/2)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))\nprint('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For RandomForestRegeressor model we got RMSE as \"64.548\"."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}