{"cells":[{"metadata":{"id":"YwYKspr1EY53","colab_type":"text"},"cell_type":"markdown","source":"# AllState Claim Severity\n---\nThis notebook provides full implementation of this  kaggle competation project from data exploration to model prediction on test data. All stages of ML project phases are explained with visualization and findings supported with data. Goal of this project is to predict severity of claim by estimating the amount of Loss given the claim features. Several ML techniques of Regression has been discussed and implemented in this notebook.  \n\n### Import necessary packages and mount gdrive"},{"metadata":{"id":"S8Ag5mlPm97h","colab_type":"code","outputId":"bb1c6b91-5f84-472b-c4e5-059c0c7d4fa0","colab":{"base_uri":"https://localhost:8080/","height":121},"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn import datasets, linear_model\n\n%matplotlib inline\n\n# Mounting gdrive\n#from google.colab import drive\n#drive.mount('/content/gdrive')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"yl0jp2RaczzT","colab_type":"text"},"cell_type":"markdown","source":"## Data Exploration\nIn this section we will load the train and test dataset and analyze the features, shape and statistics on each feature to get some insight about data we are dealing with. \n\n### Load the train data\n\nLoad the train data from csv file into pandas dataframe. Review shape of dataset and all features. From this review we conclude 116 categorical features and 14 continous features.\n\n"},{"metadata":{"id":"aNAZTiY_v1ne","colab_type":"code","outputId":"d1ba0532-9532-4fc3-f1ff-4b429362a80b","colab":{"base_uri":"https://localhost:8080/","height":97},"trusted":true},"cell_type":"code","source":"# Load train data from gdrive. Review shape of dataset and number of features\n\ndataset_train = pd.read_csv('../input/train.csv')\ndataset_train.shape\ndataset_train.iloc[:1,:132]","execution_count":null,"outputs":[]},{"metadata":{"id":"cI5918KfepMP","colab_type":"text"},"cell_type":"markdown","source":"### Partition Train dataset to seperate features from target\n\nIn below code cell, target Loss values are seperated from features input into seperate datasets. This is done to train regression model on features data and target Loss (ground truths).  "},{"metadata":{"id":"tI7mpxEawAT2","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Separate features from Loss value in train dataset.\n\npd.set_option('display.max_columns',None)\nloss = dataset_train['loss']\nfeatures = dataset_train.drop('loss', axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"y_6EMx-MkoJO","colab_type":"text"},"cell_type":"markdown","source":"### Analyze Statistics on Features\nAnalyzing statistics on 14 continuous features is done to determine what range of values are depicted by each of these features. As we can observe from below stats, values for all continuous feature lies between 0 and 1. Mean is closer to 0.5 and standard deviation is not more than 0.22. So, the data for all continuous feature is evenly distributed and normalized."},{"metadata":{"id":"clbXBshmkU-r","colab_type":"code","outputId":"1757a0ea-f946-4b2b-c40e-016dc090cc2a","colab":{"base_uri":"https://localhost:8080/","height":304},"trusted":true},"cell_type":"code","source":"# Analyse stats on continuous features\nfeatures.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"z4ki0GBmgPbL","colab_type":"text"},"cell_type":"markdown","source":"### Analyze skewness on Continuous Features\nPandas skew() method is used to check skewness on each continuous feature. From this analysis we can determine whether normalization is required to remove skewness and make data more uniform in distribution. \n\nNone of the continuous feature has large skewness as we can observe from below results.  "},{"metadata":{"id":"ez_QBjob5Iek","colab_type":"code","outputId":"d6a4b365-43d7-438c-8b20-686127334dc5","colab":{"base_uri":"https://localhost:8080/","height":252},"trusted":true},"cell_type":"code","source":"# Analyse skewness of all continous features\n\nprint(features['cont1'].skew())\nprint(features['cont2'].skew())\nprint(features['cont3'].skew())\nprint(features['cont4'].skew())\nprint(features['cont5'].skew())\nprint(features['cont6'].skew())\nprint(features['cont7'].skew())\nprint(features['cont8'].skew())\nprint(features['cont9'].skew())\nprint(features['cont10'].skew())\nprint(features['cont11'].skew())\nprint(features['cont12'].skew())\nprint(features['cont13'].skew())\nprint(features['cont14'].skew())\n","execution_count":null,"outputs":[]},{"metadata":{"id":"w0EHcXo0lNn_","colab_type":"text"},"cell_type":"markdown","source":"### Analyze Statistics on Target Loss\nHere, we have used numpy functions to compute min, max, mean, median and standard deviation. Analyzing this information will provide insight into Loss distribution. \n\nFrom what we can observer, mean and median differs significantly with large standard deviation. This indicates that Loss data is skewed and unevenly distributed. "},{"metadata":{"id":"6ygK9wQ10qcw","colab_type":"code","outputId":"8b2a6761-890b-4b09-f0b7-55c37d8674c4","colab":{"base_uri":"https://localhost:8080/","height":101},"trusted":true},"cell_type":"code","source":"# Analyse Loss statistics \n\n#minimum loss\nmini_loss = np.amin(loss)\n\n#maxmium loss\nmax_loss = np.amax(loss)\n\n#mean loss\nmean_loss = np.mean(loss)\n\n#standard deviation loss\nstd_loss = np.std(loss)\n\n#median loss\nmedian_loss = np.median(loss)\n\nprint(\"Minimum Loss {}\".format(mini_loss))\nprint(\"Maximum Loss {}\".format(max_loss))\nprint(\"Mean Loss {}\".format(mean_loss))\nprint(\"Standard Deviation Loss {}\".format(std_loss))\nprint(\"Median Loss {}\".format(median_loss))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"__3ooX0kmfQo","colab_type":"text"},"cell_type":"markdown","source":"### Analyze skewness on Target Loss\nWe can check data skewness on Loss target by calling skew() method on loss as shown below. Here large skew values indicates data is highly skewed and requires transformation before feeding it to model for training."},{"metadata":{"id":"J75I__ToVQIn","colab_type":"code","outputId":"becc811d-8315-49b0-e071-8bf8980d0433","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"# Checking skewness of Loss data\nloss.skew()","execution_count":null,"outputs":[]},{"metadata":{"id":"iB6jkhhPnGen","colab_type":"text"},"cell_type":"markdown","source":"## Exploratory Visualization\nIn data visualization process, we need to analyze feature relevance. That is whether all features are independent of each other or there exists some degree of correlation among features. Determining correlation would help us to find out redundancy among features. Then we can eliminate similar features which further simplifies input to model.\n\n### Heatmap\nTo find out correlation among continuous feature I have used pandas .corr() method and pass it to seaborn heatmap to visualize Pearson correlation coefficient as shown in below figure. From the figure we can see lighter region in heatmap shows strong correlation and darker region depicts weaker correlation among feature. \nFeatures showing strong correlation are listed below. \n  -\tCont11 and cont12 \n  -\tCont1 and cont9\n  -\tCont6 and cont10\n  -\tCont1 and cont10\n\n"},{"metadata":{"id":"fdLL7cL-_JB8","colab_type":"code","outputId":"592e7660-adf1-41f5-eb11-825a85515619","colab":{"base_uri":"https://localhost:8080/","height":428},"trusted":true},"cell_type":"code","source":"# Plot heat map to understand correlation between continous features \n\nplt.figure(figsize=(10, 6))\nsns.set()\nsns.heatmap(features.iloc[:,117:].corr())\n","execution_count":null,"outputs":[]},{"metadata":{"id":"WBYptvtcpJvR","colab_type":"text"},"cell_type":"markdown","source":"### Scatter Matrix\nScatter Matrix is plotted  between each continuous features to review scatter plot distribution to understand data correlation and skewness. Here we have sampled first 2000 rows to review data distribution. \nScatter plot visualization depicts similar correlations between features compared to what we discovered in heatmap visuals."},{"metadata":{"id":"hoemN6Fgp1Uj","colab_type":"code","outputId":"f762098b-3d49-4d46-9a0d-f7e170bdb15d","colab":{"base_uri":"https://localhost:8080/","height":4770},"trusted":true},"cell_type":"code","source":"# Plot scatter plots to understand correlation between continuous features\n\npd.plotting.scatter_matrix(features.iloc[:2000,117:], alpha=0.1, figsize = (40,25), diagonal='kde')","execution_count":null,"outputs":[]},{"metadata":{"id":"DjWmsbfqreMF","colab_type":"text"},"cell_type":"markdown","source":"## Model Benchmarking\n\nBenchmark has been established on LinearRegressor model from sklearn. Training data is feed through LinearRegressor (with feature input and target) without much data preprocessing. Some of the basic preprocessing steps done on data are listed below.\n\n*   Removed feature ‘Id’ from train and validation data as it doesn’t provide any value for Loss prediction. Its like index attribute to uniquely identify each claim row.\n*   Categorical variables are one hot encoded so that numerical values are given as input to model. \n\nAfter fitting through LinearRegression(considering all default parameters), prediction is done on test data to compute MAE. Some of the very high predictions are ignored as possible outlier. \n"},{"metadata":{"id":"doe6vQklBqI3","colab_type":"code","outputId":"3a3b96b9-dffb-48e7-9d82-92c7a3dca3de","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"# Benchmark model. Running Linear Regression on features data with split on train and val in ratio of 9:1 to obtain MSE\n\n# Hot encode categorical features\nfeatures_bench_train = pd.get_dummies(features)\n\n# Drop Id column from feature data\nfeatures_bench_train = features_bench_train.drop('id',axis=1)\n\n# Log transform Loss target\nloss_log_bench = np.log(loss)\n\n# Split train and test data in ratio of 9:1\nX_train, X_test, y_train, y_test = train_test_split(features_bench_train, \n                                                    loss_log_bench, \n                                                    test_size = 0.1, \n                                                    random_state = 0)\n\n\n# Run Linear Regression on input train and validation data. Compute Mean absolute error. \nregr = linear_model.LinearRegression()\nregr.fit(X_train, y_train)\n\n# Make predictions using the test data\ny_pred = regr.predict(X_test)\n\n# Remove high outlier predictions\nfor i, num in enumerate(y_pred):\n  if num > 1000:\n    y_pred[i] = y_pred[i-1] \n\n\n#Compute MAE from prediction and truth     \nprint(\"Mean Absolute error: %.2f\"\n      % mean_absolute_error(np.exp(y_test), np.exp(y_pred)))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"8X9tDAiit-7k","colab_type":"text"},"cell_type":"markdown","source":"## Data Preprocessing\nIn this section, we will perform feature engineering based on data exploration and analysis done in prior section.  \n\n### Hot encoding\nCategorical variables will be one hot encoded to numeric values from string values as we can only input numeric values to model for training. One hot encoding is done  through pandas get_dummies function which converts all categorical variable into one hot representation of numeric values. After hot encoding we observe 116 categorical features are converted into 1139 numeric features. "},{"metadata":{"id":"v3e0q4dHvGIA","colab_type":"code","outputId":"589dca37-70df-494a-9ea2-1e4736af35ec","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"# Hot encode categorical features and review dimension of dataset after one hot encoding, 116 cat features are converted\n# into 1139 + 14 cont features + 1 id feature = 1154 features  \n\nfeatures = pd.get_dummies(features)\nprint(features.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"6_OqTeX1wE8O","colab_type":"text"},"cell_type":"markdown","source":"### Seperate Categorical and Continuous feature\nHere we are dropping the 'Id' column from dataframe since it of no use. Cat and Cont features are seperated so that we can perform feature transformation seperately on each of the dataset. "},{"metadata":{"id":"ZAfWwCc_wfxE","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# drop 'Id' from dataframe\nfeatures = features.drop('id',axis=1)\n\n# Seperate continuous features\ncont_features = features.iloc[:,:14]\n\n# Seperate Categorical features\ncat_features = features.iloc[:,14:]\n","execution_count":null,"outputs":[]},{"metadata":{"id":"b8zQeBuEywOu","colab_type":"text"},"cell_type":"markdown","source":"### Principal Component Analysis on Continuous Features. \nFrom the data visualization on continuous features we concluded strong correlation among 6 features. We can reduce the dimensions of continuous feature by removing 3 features. This process can be through PCA to achieve dimensionality reduction. So we will set number of component = 11 as we anticipate 11 key features. \n\nAfter running PCA fit and transform we print explained variance ratio which shows these 11 features represents 99% of variance in continous feature space. "},{"metadata":{"id":"BPZvhEfx0aD2","colab_type":"code","outputId":"5809c135-72c8-4601-e1c6-bcefb452d815","colab":{"base_uri":"https://localhost:8080/","height":67},"trusted":true},"cell_type":"code","source":"# PCA on continuous features\n\nlist = []\n\n# Instanstiate PCA with number of component = 11\npca = PCA(n_components=11)\n\n# Perform fit on 11 features\npca.fit(cont_features)\n\n# Perform transform\nreduced_cont_feature = pca.transform(cont_features)\n\n# print explained variance ratio to show variance for all 11 components \nlist.append(pca.explained_variance_ratio_)\nprint(list)","execution_count":null,"outputs":[]},{"metadata":{"id":"pEqDjZ0o4ZdS","colab_type":"text"},"cell_type":"markdown","source":"### Principal Component Analysis on Categorical features\nIts difficult to comprehend correlation among categorical variable when large number of categorical variables are present in dataset. PCA makes this process automated when optimal number of components are initialized in fit process. Below steps illustrates process for PCA done on Categorical variable dataset. \n\n  - We run PCA on categorical train dataset with number of components = 1139.\n  - After fit and transform we print explained variance ratio for all 1139 feature. \n  - From explained variance ratio we can conclude that first 345 captures 99% variance in data. So, we can ignore rest of dimensions as their presence won’t make much difference as far as variance is concerns. \n  - In final step we run PCA with number of components = 345. Resultant dataset represents transformed reduced categorical features dataset. \n"},{"metadata":{"id":"SXDjqaTxtDdt","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Perform PCA for dimensionality reduction. Run PCA for number of components = total number of features after hot encoding to \n#understand explained variance ratio for all dimensions\n\nlist = []\n\npca = PCA(n_components=1139)\n\n# Perform fit on 1139 cat_features\npca.fit(cat_features)\n\n# Perform transform\ntransformed_feature = pca.transform(cat_features)\n\n# print explained variance ratio to show variance for all components\nlist.append(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"id":"MCAAJveCw9RS","colab_type":"code","outputId":"7e54b2de-6931-4467-aa86-be6e7e5b6283","colab":{"base_uri":"https://localhost:8080/","height":4805},"trusted":true},"cell_type":"code","source":"# Review explained variance and derive number of dimensions to be considered for 99% variance.  \n\nnp_list = np.array(list)\nnp.set_printoptions(threshold=1500)\nprint(np_list)","execution_count":null,"outputs":[]},{"metadata":{"id":"4m-_qLpOBVdy","colab_type":"code","outputId":"69de2138-6f98-4438-be72-334b14c377b5","colab":{"base_uri":"https://localhost:8080/","height":1478},"trusted":true},"cell_type":"code","source":"list = []\n\npca = PCA(n_components=345)\n\n#perform fit on 345 dimensions \npca.fit(cat_features)\n\n#perform transform\nreduced_cat_feature = pca.transform(cat_features)\n\n#print explained variance ratio. 345 dimensions represents 99% variance in catergorical features \nlist.append(pca.explained_variance_ratio_)\nprint(list)","execution_count":null,"outputs":[]},{"metadata":{"id":"igs9bmmn8rfN","colab_type":"text"},"cell_type":"markdown","source":"### Combine Datasets\nAfter PCA and achieving dimensionality reduction on Cat and Cont feartures, we will combine both dataset into one and review final shape"},{"metadata":{"id":"MdDd6HLaCyuK","colab_type":"code","outputId":"07e520cd-a8f9-4d83-898c-4396f8dc01e0","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"# After PCA combine Cat and Cont features into single dataset.  \nreduced_feature = np.hstack((reduced_cat_feature,reduced_cont_feature))\nreduced_feature.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"hdpRejwO9_w5","colab_type":"text"},"cell_type":"markdown","source":"### Data Transformation\nFrom the prior analysis we found that Loss target feature is skewed and do not show uniform distribution may be due to outlier present in the dataset. To deal with this, we can transform data to log scale and then review skewness on log data. \n\nIn below code implemenation we have used numpy log function to transform Loss data. Please note that 1 has been added to Loss so that if there are any low or 0 values present in Loss data then it gets properly scaled into log space. \n\nAs we can observe, skewness is reduced after log transformation. "},{"metadata":{"id":"TSyxnZ5sKbGj","colab_type":"code","outputId":"6ca9c09a-9645-45d1-ab8f-f339db220acf","colab":{"base_uri":"https://localhost:8080/","height":50},"trusted":true},"cell_type":"code","source":"# Review data skewness. Log transform to get normal or uniform distribution.\n\nprint(\"data skewness before log transform {}\".format(loss.skew()))\nloss_log = np.log(loss+1)\nprint(\"data skewness after log transform {}\".format(loss_log.skew()))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"3stY-QKMASOW","colab_type":"text"},"cell_type":"markdown","source":"## Metrics\nMean Absolute error (MAE) is the metric we have chosen to evaluate performance of the model. MAE is measure of absolute difference between actual loss and predicted loss averaged over all samples. \n\nThis metric is considered for the following reasons:\n  -\tIn this problem statement, prediction of loss value is performed which is numerical quantity. Predicting numerical value from input feature is the case of linear regression. Closer the predicted value to actual, lesser the error term and more accurate regression function can be approximated. \n  -\tAnother metric for regression problem is Mean Squared Error (MSE). In MSE, square of the difference is taken when evaluating error function. If the error term lies between 0 and 1 then MSE is preferred over MAE as square of number (error term) further reduces. But in given problem statement predicted Loss value can error by significant margin in hundreds or even thousands by positive or negative margin, so considering MAE is more viable as error term and is not further amplified by taking square. \n"},{"metadata":{"id":"y8bY5f01J5mQ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Mean absolute error performance metric. \n\ndef performance_metric(y_true, y_predict):\n    \"\"\" Calculates and returns the performance score between \n        true and predicted values based on the metric chosen. \"\"\"\n    \n    # TODO: Calculate the performance score between 'y_true' and 'y_predict'\n    mae = mean_absolute_error(y_true, y_predict)\n    \n    # Return the score\n    return mae","execution_count":null,"outputs":[]},{"metadata":{"id":"C7abhurSBWbg","colab_type":"text"},"cell_type":"markdown","source":"## Train Test Split\nAfter performing feature engineering, we need to prepare train and test data for model training. Train data will be input to model for regression training. Once the model is trained, prediction will be done on Test data. \n\nWe have used sklearn model.selection train_test_split function to split original transformed data into train and test dataset. Split ratio is 9:1 that is 10%  will be used as test data and 90% as train data. "},{"metadata":{"id":"4Wcc72HESlBn","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Split original data into train and test in ratio of 9:1\n\nX_train, X_test, y_train, y_test = train_test_split(reduced_feature, \n                                                    loss_log, \n                                                    test_size = 0.1, \n                                                    random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"id":"WrAERYxyDGfb","colab_type":"text"},"cell_type":"markdown","source":"## Implementation\nModel implementation is done on three chosen techniques listed below. These techniques are chosen based on size and complexity of data. \n  - Decision Tree Regressor\n  - XGBRegressor\n  - Deep NN using Keras"},{"metadata":{"id":"FFB9XRXpG-VO","colab_type":"text"},"cell_type":"markdown","source":"### DecisionTreeRegressor\nDecisionTreeRegressor model training is done on train data with default parameters to get initial MAE estimate. As we can see resulted MAE of 1851.88 is way higher than benchmarked we established with LinearRegressor.  "},{"metadata":{"id":"6GP5YO_1OUNI","colab_type":"code","outputId":"ae273bff-318e-44c5-b694-85a0b281b542","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"#Running DecisionTreeRegressor with default parameters\n\n#import DecisionTreeRegressor from sklearn library\nfrom sklearn.tree import DecisionTreeRegressor\n\n#Instantiate class\nregressor = DecisionTreeRegressor()\n\n# Fit train feature and target\nregressor.fit(X_train,y_train)\n\n# Predict on Test features\ny_pred = regressor.predict(X_test)\n\n#Print MAE from predicted and actual Loss\nprint(\"Mean Absolute error: %.2f\"\n      % performance_metric(np.exp(y_test), np.exp(y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"id":"X03EUEb7PovZ","colab_type":"text"},"cell_type":"markdown","source":"#### Model Complexity Analysis\n\nIn this process we have to find the most optimal parameter setting which gives best MAE score for DecisionTreeRegressor. I have chosen max_depth parameter with range options from 5 to 15. \n\nmax_depth = [5,6,7,8,9,10,11,12,13,14,15]. \n\nValidation curve of training and testing scores are plotted with max_depth on x-axis and scores on y-axis. Scoring function ‘neg_mean_absolute_square’ is used for evaluation. \n\n"},{"metadata":{"id":"cr_4yXRGjCCG","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Model complexity graph for max_depth parameter\nfrom sklearn.model_selection import validation_curve\n\ndef ModelComplexity(X, y):\n\n    # Vary the max_depth parameter from 1 to 10\n    max_depth = np.arange(5,15)\n\n    # Calculate the training and testing scores\n    train_scores, test_scores = validation_curve(DecisionTreeRegressor(), X, y, \\\n        param_name = \"max_depth\", param_range = max_depth, cv = None, scoring = 'neg_mean_absolute_error')\n    \n    # Find the mean and standard deviation for smoothing\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n  \n    print(train_mean,test_mean)\n      \n    # Plot the validation curve\n    plt.figure(figsize=(7, 5))\n    plt.title('Decision Tree Regressor Complexity Performance')\n    plt.plot(max_depth, train_mean, 'o-', color = 'r', label = 'Training Score')\n    plt.plot(max_depth, test_mean, 'o-', color = 'g', label = 'Validation Score')\n    plt.fill_between(max_depth, train_mean - train_std, \\\n        train_mean + train_std, alpha = 0.15, color = 'r')\n    plt.fill_between(max_depth, test_mean - test_std, \\\n        test_mean + test_std, alpha = 0.15, color = 'g')\n\n    # Visual aesthetics\n    plt.legend(loc = 'lower right')\n    plt.xlabel('Max depth')\n    plt.ylabel('Score')\n    plt.ylim([-0.40,-0.60])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"QSnH4LV2jabV","colab_type":"code","outputId":"5645f2a3-78af-4955-e2ee-0c552b0dd8e9","colab":{"base_uri":"https://localhost:8080/","height":454},"trusted":true},"cell_type":"code","source":"ModelComplexity(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"I3S4Gu-ZQHhY","colab_type":"text"},"cell_type":"markdown","source":"\n#### Observation:\nAs we can observe from model complexity plot above,\n\n\n   \n   \n\n*      For max_depth=[5,6,7], both training and testing scores are low showing high bias and model is underfitting.\n*   For max_depth=[10,11,12,13,14,15], training score keeps increasing as max_depth is increased from 10 to 15 but on other hand, testing score keeps decreasing as max_depth is increased from 10 to 15. This shows sign of high variance as model is overfitting and failing to generalize well. \n\n*   For max_depth=[8,9] we observe both train and test score are highest and there is not much divergence. Though testing score looks slightly higher for depth 9. We will run regression for both values of depth and evaluate the resulted MAE score.\n\n\n\n\n\n\n\n\n"},{"metadata":{"id":"oziv1VMtTd8i","colab_type":"code","outputId":"31779204-304a-42b8-c26a-753e6ce896d3","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"#Running DecisionTreeRegressor with max_depth parameters\n\n# Initialize Regressor with max_depth=8\nregressor = DecisionTreeRegressor(max_depth=8)\n\n# perform fit on train dataset\nregressor.fit(X_train,y_train)\n\n# perform predict on test dataset\ny_pred = regressor.predict(X_test)\n\n# MAE computed from predicted and actual loss\nprint(\"Mean Absolute error: %.2f\"\n      % performance_metric(np.exp(y_test), np.exp(y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"id":"8CTg9PbcXuHd","colab_type":"code","outputId":"6b08fb0d-7989-4114-de18-6b1da6bb570d","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"#Running DecisionTreeRegressor with max_depth parameters\n\n# Initialize Regressor with max_depth=9\nregressor = DecisionTreeRegressor(max_depth=9)\n\n# perform fit on train dataset\nregressor.fit(X_train,y_train)\n\n# perform predict on test dataset\ny_pred = regressor.predict(X_test)\n\n# MAE computed from predicted and actual loss\nprint(\"Mean Absolute error: %.2f\"\n      % performance_metric(np.exp(y_test), np.exp(y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"id":"GNwIuESsYSWO","colab_type":"text"},"cell_type":"markdown","source":"Score we evaluated from max_depth parameter analysis is still higher than benchmark we established from LinearRegression so we look for more ways to optimize and check if the further improvement can done. \n\nBelow figure depicts model complexity curve for parameter min_samples_split. It represents minimum number of samples required to split node in Decision Tree. Train and testing curve are plotted for each value of min_samples_split param options. \n\nmin_sample_split = [350,400,450,500,550,600,650]       \n"},{"metadata":{"id":"UOCshq9PSDpB","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Model complexity graph for min_samples_split parameter\n\ndef ModelComplexity(X, y):\n\n    # Vary the max_depth parameter from 1 to 10\n    min_samples_split = [350,400,450,500,550,600,650]\n\n    # Calculate the training and testing scores\n    train_scores, test_scores = validation_curve(DecisionTreeRegressor(), X, y, \\\n        param_name = \"min_samples_split\", param_range = min_samples_split, cv = None, scoring = 'neg_mean_absolute_error')\n    \n    # Find the mean and standard deviation for smoothing\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n  \n    print(train_mean,test_mean)\n      \n    # Plot the validation curve\n    plt.figure(figsize=(7, 5))\n    plt.title('Decision Tree Regressor Complexity Performance')\n    plt.plot(min_samples_split, train_mean, 'o-', color = 'r', label = 'Training Score')\n    plt.plot(min_samples_split, test_mean, 'o-', color = 'g', label = 'Validation Score')\n    plt.fill_between(min_samples_split, train_mean - train_std, \\\n        train_mean + train_std, alpha = 0.15, color = 'r')\n    plt.fill_between(min_samples_split, test_mean - test_std, \\\n        test_mean + test_std, alpha = 0.15, color = 'g')\n\n    # Visual aesthetics\n    plt.legend(loc = 'lower right')\n    plt.xlabel('Minimum Sample split')\n    plt.ylabel('Score')\n    plt.ylim([-0.40,-0.60])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"fs34YK85gZs7","colab_type":"code","outputId":"1b4a6848-b559-40ec-96e2-77199a72f392","colab":{"base_uri":"https://localhost:8080/","height":454},"trusted":true},"cell_type":"code","source":"ModelComplexity(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"C50ZvUwTY5gz","colab_type":"text"},"cell_type":"markdown","source":"Observations:\n*  For values less than 500 we observe low train and testing score depicting high bias and underfit.  \n*  For values 500 onwards we observe train and testing curves starts converging. Testing score almost stabilizes without further increase and train score goes down as min_samples_split increases. So, we get best optimization for value 500\n\n\n"},{"metadata":{"id":"EJ3YrlDxGo-f","colab_type":"code","outputId":"8464bc23-608b-4718-802c-55f81049d6f5","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"#Running DecisionTreeRegressor with max_depth and min_samples_split parameters \n\n# Initialize Regressor with max_depth=9 and min_samples_split=500\nregressor = DecisionTreeRegressor(max_depth=9, min_samples_split=500)\n\n# perform fit on train dataset\nregressor.fit(X_train,y_train)\n\n# perform prediction on test dataset\ny_pred = regressor.predict(X_test)\n\n# MAE computed from predicted and actual loss values \nprint(\"Mean Absolute error: %.2f\"\n      % performance_metric(np.exp(y_test), np.exp(y_pred)))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"rZlsYG--adFs","colab_type":"text"},"cell_type":"markdown","source":"### Deep Neural Network using Keras\nKeras sequential model with multi layer deep NN architecture is designed to train regressor. Below steps are followed to train deep neural network regressor.\n*  Split the training dataset into train and test data so that some part of data is used for test prediction using sklearn train_test_split function. Split is done in ratio of 9:1.\n*  We have three FC layer (Fully connected) in this architecture with first layer comprising of same number of units as input feature dimension. \n*  Last layer outputs single value (predicted loss value) so we have used Linear activation function.\n*  Dropout layer is added after each layer except final layer to avoid overfitting. Dropout ensures that all trainable weights are given equal weightage avoiding model getting influenced by few weights. \n*  Adam Optimizer is used for this model with optimal learning rate.\n*  Model is complied with Loss function of ‘Mean_Absolute_Error’ and optimizer.\n*  Model Checkpoint is saved after every epoch to keep track of optimal model parameters (weight matrix) during training process to file.\n*  Fit function trains the model for set batch size for all epochs. 10% of data is used for inferencing. Every time validation loss is lesser than prior value,  weights are saved to checkpoint file. \n* After training, model is loaded from last saved checkpoint with best optimal parameters. \n* Prediction is done on test data to compute and return MAE.  \n\n\n\n\n\n\n\n\n"},{"metadata":{"id":"7G7O-wXIf20o","colab_type":"code","outputId":"4fcce5b1-dd32-4248-a888-31797acf53e7","colab":{"base_uri":"https://localhost:8080/","height":3582},"trusted":true},"cell_type":"code","source":"# Deep NN training using Keras \n\n#Import required libraries \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import ModelCheckpoint  \nfrom keras import optimizers\n\n# Split the original data into train and test in ratio of 9:1 \nX_train, X_test, y_train, y_test = train_test_split(reduced_feature, \n                                                    loss_log, \n                                                    test_size = 0.1, \n                                                    random_state = 0)\n\nmodel = Sequential()\n\n# First Dense Layer\nmodel.add(Dense(356, input_dim=reduced_feature.shape[1], kernel_initializer='normal', activation='relu'))\nmodel.add(Dropout(0.3))\n\n# Hidden Layer\nmodel.add(Dense(200, kernel_initializer='normal', activation='relu'))\nmodel.add(Dropout(0.2))\n\n# Output Layer\nmodel.add(Dense(1, kernel_initializer='normal', activation='linear'))\n\n# Initialize optimizer with lr\nadam = optimizers.adam(lr=0.0001)\n\n# Compile model\nmodel.compile(loss='mean_absolute_error', optimizer=adam)\n\n# Define model checkpoint with filepath to save trained params\ncheckpointer = ModelCheckpoint(filepath='weights.best.DeepNN.hdf5', \n                               verbose=1, save_best_only=True)\n\n# Fit Train data with 10% data used for model inferencing. \nmodel.fit(X_train, y_train, \n          validation_split=0.1,\n          epochs=50, batch_size=250, callbacks=[checkpointer], verbose=1)\n\n# Load best weights from saved model checkpoint\nmodel.load_weights('weights.best.DeepNN.hdf5')\n\n# predict on test data\ny_pred = model.predict(X_test)\n\n# Compute MAE from predicted and actual loss\nprint(\"Mean Absolute error: %.2f\"\n      % performance_metric(np.exp(y_test), np.exp(y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"id":"nrco7HARjkoi","colab_type":"text"},"cell_type":"markdown","source":"#### Model Validation\nTo test robustness of this model we will run multiple iterations of model training changing the train and testing data each time model evaluates MAE.\n\nI have chosen to iterate this process three times. In each loop train_test_split function segregates train and test data with 9:1 split ratio. Random seed has been initialized and data shuffle is true by default. \n\nIn each training process, keras model can separate a portion of training data into a validation dataset and evaluate the performance of model on that validation dataset for each epoch.We can do this by setting the validation_split argument on the fit() function to a percentage of the size of your training dataset. Here we have specified that as 0.1.After completion of each model training iteration, its MAE score has been added to list. Once all iteration is over, mean value of MAE score is calculated with standard deviation as shown below. \n"},{"metadata":{"id":"zb_YirjTGmyA","colab_type":"code","outputId":"b7f47cd4-0694-4266-a572-65daa48b63e4","colab":{"base_uri":"https://localhost:8080/","height":10248},"trusted":true},"cell_type":"code","source":"# For multiple iterations run model training with variations in train and test splits to test robustness of model\n\nmae_list = []\n\nfor i in range(3):\n  \n  X_train, X_test, y_train, y_test = train_test_split(reduced_feature, \n                                                    loss_log, \n                                                    test_size = 0.1, \n                                                    random_state = 42)\n  \n  model = Sequential()\n  model.add(Dense(356, input_dim=reduced_feature.shape[1], kernel_initializer='normal', activation='relu'))\n  model.add(Dropout(0.3))\n  model.add(Dense(200, kernel_initializer='normal', activation='relu'))\n  model.add(Dropout(0.2))\n  model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n  # Compile model\n  \n  adam = optimizers.adam(lr=0.0001)\n  \n  model.compile(loss='mean_absolute_error', optimizer=adam)\n\n  checkpointer = ModelCheckpoint(filepath='weights.shuffle.best.DeepNN.hdf5', \n                               verbose=1, save_best_only=True)\n\n  model.fit(X_train, y_train, \n          validation_split=0.1,\n          epochs=50, batch_size=250, callbacks=[checkpointer], verbose=1)\n\n  model.load_weights('weights.shuffle.best.DeepNN.hdf5')\n\n  y_pred = model.predict(X_test)\n  \n  mae = performance_metric(np.exp(y_test), np.exp(y_pred))\n  \n  mae_list.append(mae)\n  \n  print(\"Mean Absolute error: %.2f\"\n      % mae)\n  \n# Print all MAE scores from multiple iterations\nprint(mae_list)\n\n# Print mean MAE score\nprint(np.mean(mae_list))\n\n# Print standard deviation for MAE\nprint(np.std(mae_list))","execution_count":null,"outputs":[]},{"metadata":{"id":"T8GRDISPlz7p","colab_type":"text"},"cell_type":"markdown","source":"### XGBRegressor\n\nXGBRegressor is parallelizable onto GPU’s and across networks of computers making it feasible to train on very large datasets as in our case. It is an implementation of gradient boosted decision trees designed for speed and performance. \nThe two reasons to use XGBRegressor\n\n*  Bagging and Boosting features\n*  Model Performance\n\n\nI have used XGBRegressor API from XGBoost class. It has lots of trainable parameters. The ones used in training model are listed below:\n\n*   n_estimator = It’s an int value which represents number of trees to fit\n*   booster = Specify which booster to use: gbtree, gblinear or dart \n*  subsample = Subsample ratio of the training instance \n\nBoosting and Bagging process can be computationally very extensive and time consuming. With large dataset it becomes worse unless appropriate params are tuned to optimal values. After instantiating XGBRegressor with optimal parameters, model is fitted with train data features and Loss target variable. Prediction is done on trained model with test data and finally MAE is computed and returned based on ground truth and predicted values. \n"},{"metadata":{"id":"eULM_i_k-EYq","colab_type":"text"},"cell_type":"markdown","source":"First, I tried setting booster to ‘gblinear’ thinking from linear regression perspective, but results were not good or even worse than benchmark. So, the next step was to set subsample size which brought great improvements.  It denotes the fraction of observations to be randomly samples for each tree. Lower values make the algorithm more conservative and prevents overfitting, but too small values might lead to under-fitting.\n\nWith default value of subsample (subsample=1) we further noticed slight improvement which refers to fact that much of the rows represents similar correlation meaning random sample of data to much extend can fully represent whole dataset to derive predictions. \n"},{"metadata":{"id":"pNZIEXrYmW75","colab_type":"code","outputId":"728170b9-6d2d-44eb-a562-667d8b170f1f","colab":{"base_uri":"https://localhost:8080/","height":87},"trusted":true},"cell_type":"code","source":"# Import XGBRegressor from xgboost   \nfrom xgboost import XGBRegressor\n\n# Instantiate Regressor with high estimator and gblinear booster \nmodel = XGBRegressor(n_estimators=1000, booster='gblinear')\n\n# fit model on train data\nxgb = model.fit(X_train, y_train)\n\n# predict model on test data \ny_pred = xgb.predict(X_test)\n\n# Compute MAE from predicted and actual loss\nprint(\"Mean Absolute error: %.2f\"\n      % performance_metric(np.exp(y_test), np.exp(y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"id":"wFErz9p-Lzfw","colab_type":"code","outputId":"b4bdb95a-471c-4a2a-a5de-e17fd3b61cf4","colab":{"base_uri":"https://localhost:8080/","height":87},"trusted":true},"cell_type":"code","source":"# Instantiate Regressor with high estimator and subsample size of 0.5\nmodel = XGBRegressor(n_estimators=1000, subsample=0.5)\n\n# fit model on train data\nxgb = model.fit(X_train, y_train)\n\n# predict model on test data \ny_pred = xgb.predict(X_test)\n\n# Compute MAE from predicted and actual loss\nprint(\"Mean Absolute error: %.2f\"\n      % performance_metric(np.exp(y_test), np.exp(y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"id":"gh6cgrVamR0c","colab_type":"code","outputId":"f375b751-772f-437e-9471-469e417baa55","colab":{"base_uri":"https://localhost:8080/","height":87},"trusted":true},"cell_type":"code","source":"# Instantiate Regressor with high estimator\nmodel = XGBRegressor(n_estimators=1000)\n\n# fit model on train data\nxgb = model.fit(X_train, y_train)\n\n# predict model on test data \ny_pred = xgb.predict(X_test)\n\n# Compute MAE from predicted and actual loss\nprint(\"Mean Absolute error: %.2f\"\n      % performance_metric(np.exp(y_test), np.exp(y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"id":"kqi3sZLr-6Mv","colab_type":"text"},"cell_type":"markdown","source":"## Results\nComparing the results from three regression model, we select the one as final model which gives best prediction and lowest MAE. Here, I have taken best MAE for each of the model evaluated earlier.\n\n|   Model       \t |    MAE   |\n| -----------------------| ---------|\n| DecisionTreeRegressor  | 1351.13  |\n| LinearRegression | 1275.33  |\n| XGBRegressor  \t | 1193.38  |\n| Deep NN  \t\t | 1169.16  |\n\nDeep NN outperforms the other models in evaluation process. XGBRegressor and Deep NN performs better than benchmark established with LinearRegression whereas DecisionTreeRegressor fails to perform. "},{"metadata":{"id":"Lga4X8JH-RSV","colab_type":"text"},"cell_type":"markdown","source":"## Preprocessing Test data\nTo evaluate Loss on test data provided in kaggle competition, we first need to perform some data processing so that it can be feeded to model for prediction."},{"metadata":{"id":"P2oRpaRGMvia","colab_type":"code","outputId":"75e3d741-3a25-4da2-b021-41f265b546ad","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"# Load test data from gdrive and review shape\n\ndataset_test = pd.read_csv('../input/test.csv')\ndataset_test.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"SbUu1gW3NauV","colab_type":"code","outputId":"a560e515-1c05-44f6-a3a2-3800783835b4","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"# Hot encode test features with pandas get_dummies function.\nfeatures_test = pd.get_dummies(dataset_test)\n\n# drop Id column from test data \nfeatures_test = features_test.drop('id',axis=1)\n\n# separate categorical and continuous features into different datasets. \ncont_test_features = features_test.iloc[:,:14]\ncat_test_features = features_test.iloc[:,14:]\n\n\n# perform PCA on continuous features with 11 components\nfrom sklearn.decomposition import PCA\nlist = []\n\npca = PCA(n_components=11)\npca.fit(cont_test_features)\n\nreduced_cont_test_feature = pca.transform(cont_test_features)\n\n# perform PCA on categorical features with 345 components\npca = PCA(n_components=345)\npca.fit(cat_test_features)\n\nreduced_cat_test_feature = pca.transform(cat_test_features)\n\n# combine cat and cont features\nreduced_test_feature = np.hstack((reduced_cat_test_feature,reduced_cont_test_feature))\nreduced_test_feature.shape\n","execution_count":null,"outputs":[]},{"metadata":{"id":"_PiWDngpDvaG","colab_type":"text"},"cell_type":"markdown","source":"## Free Form Visualization\n\nIn this section, I have done visualization on test dataset provided in Kaggle competition. Though we don’t have true predictions on test data but can get some visuals on predicted values obtained from running test data on trained model. \n\nTest dataset has undergone similar data transformation (PCA) as what we did for train dataset. Model is loaded from the saved checkpoint (created during training process) and final prediction is run on all the test dataset rows 125546. \n\nBelow figures depicts predicted log output. \n"},{"metadata":{"id":"5AssZDLWr0E7","colab_type":"code","outputId":"d284af12-19d6-4931-f60f-0a15ee3bb31c","colab":{"base_uri":"https://localhost:8080/","height":286},"trusted":true},"cell_type":"code","source":"import scipy.stats as stats\n\nmodel = Sequential()\nmodel.add(Dense(356, input_dim=reduced_feature.shape[1], kernel_initializer='normal', activation='relu'))\n#model.add(Dropout(0.3))\nmodel.add(Dense(200, kernel_initializer='normal', activation='relu'))\n#model.add(Dropout(0.2))\nmodel.add(Dense(1, kernel_initializer='normal', activation='linear'))\n# Compile model\nadam = optimizers.adam(lr=0.0001)\nmodel.compile(loss='mean_absolute_error', optimizer=adam)\n\nmodel.load_weights('weights.best.DeepNN.hdf5')\n\ny_pred = model.predict(reduced_test_feature)\n\nh = y_pred\nh.sort()\nhmean = np.mean(h)\nhstd = np.std(h)\nhmin = np.amin(h)\nhmax = np.amax(h)\nhmedian = np.median(h)\n\nprint(hmean,hstd,hmin,hmax,hmedian)\n\nsns.distplot(h); ","execution_count":null,"outputs":[]},{"metadata":{"id":"gjnurE20D_J_","colab_type":"text"},"cell_type":"markdown","source":"Below figure shows actual predicted distribution when converted from log scale to normal scale. "},{"metadata":{"id":"reW48rZm45bj","colab_type":"code","outputId":"babea1ef-d500-4a57-d456-b6171f1f2be7","colab":{"base_uri":"https://localhost:8080/","height":286},"trusted":true},"cell_type":"code","source":"h = np.exp(y_pred)\nh.sort()\nhmean = np.mean(h)\nhstd = np.std(h)\nhmin = np.amin(h)\nhmax = np.amax(h)\nhmedian = np.median(h)\n\nprint(hmean,hstd,hmin,hmax,hmedian)\n\nsns.distplot(h); ","execution_count":null,"outputs":[]},{"metadata":{"id":"ycdzWooYEMEg","colab_type":"text"},"cell_type":"markdown","source":"As we can observe much of the predicted Loss lies between 500 to 10000. There are few very high predictions as can be seen from long tail stretching more beyond 40000. \n\nReviewing stats from predicted loss we conclude much of the Loss value is around 1988.92 with some difference from mean of 2595.51 as there are few high predictions which are stretching mean higher than median. \n\n-\tMean – 2595.51\n-\tStandard Deviation – 1931.66\n-\tMinimum – 545.28\n-\tMaximum – 46140.69\n-\tMedian – 1988.92\n"},{"metadata":{"id":"y20JasVwEUhW","colab_type":"text"},"cell_type":"markdown","source":"Now performing visualization on training data, we see similar normalization pattern in Loss value in log scale and actual scale. "},{"metadata":{"id":"O6lMxTW87Fkg","colab_type":"code","outputId":"84f50adf-7d03-4cdc-e8dd-31c0cbeff08a","colab":{"base_uri":"https://localhost:8080/","height":286},"trusted":true},"cell_type":"code","source":"h = np.array(loss_log)\nh.sort()\nhmean = np.mean(h)\nhstd = np.std(h)\nhmin = np.amin(h)\nhmax = np.amax(h)\nhmedian = np.median(h)\n\nprint(hmean,hstd,hmin,hmax,hmedian)\n\nsns.distplot(h); ","execution_count":null,"outputs":[]},{"metadata":{"id":"4G-Y3JYR7i7K","colab_type":"code","outputId":"c69b4de4-01fd-4368-d730-71edfd67dc89","colab":{"base_uri":"https://localhost:8080/","height":286},"trusted":true},"cell_type":"code","source":"h = np.array(loss)\nh.sort()\nhmean = np.mean(h)\nhstd = np.std(h)\nhmin = np.amin(h)\nhmax = np.amax(h)\nhmedian = np.median(h)\n\nprint(hmean,hstd,hmin,hmax,hmedian)\n\nsns.distplot(h); ","execution_count":null,"outputs":[]},{"metadata":{"id":"3HbXkIU6ElBu","colab_type":"text"},"cell_type":"markdown","source":"-\tMean – 3037.34\n-\tStandard Deviation – 2904.08\n-\tMinimum – 0.67\n-\tMaximum – 121012.25\n-\tMedian – 2115.57\n\nLike in test data, here also we observe few of the high loss values stretches mean higher than median.  \n\nIts interesting to observe both Train and Test data follows similar data distribution pattern and its also evident from the fact that these are auto insurance claims reported by Allstate."}],"metadata":{"colab":{"name":"AllState_Claim_Severity.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}