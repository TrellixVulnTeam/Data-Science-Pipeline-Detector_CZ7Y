{"cells":[{"metadata":{},"cell_type":"markdown","source":"# project 4 - History Kaggle Demo: Allstate Claims"},{"metadata":{},"cell_type":"markdown","source":"This Week's Topicï¼šAllstate Claims Severity"},{"metadata":{},"cell_type":"markdown","source":"You can get the detailed information and download the data from the following link:<br/>\nhttps://www.kaggle.com/c/allstate-claims-severity\n\nNote: You need to sign up a Kaggle Account before you can download the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import models\nimport os, sys, re\n\nimport numpy as np\nimport pandas as pd\nfrom pandas import Series, DataFrame\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(os.path.join('../input','allstate-claims-severity', 'train.csv'))\n\nprint(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.read_csv(os.path.join('../input','allstate-claims-severity','test.csv'))\nprint(submit.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Shrink the data if it's too large to run.**\n\nComment it out if the device is capable to run on all rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"def selector(n_row, percent):\n    sel = np.random.rand(n_row)\n    return sel <= percent\n\ntrain = train.loc[selector(train.shape[0], 0.5), :]\ntrain = train.reset_index().drop('index', axis = 1)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# id column is not useful for modeling, so drop it.\ntrain.drop('id', axis = 1, inplace = True)\ntrain.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group features into categorical or continuous features\nfeat_group = {'fea_cat': [_ for _ in train.columns if re.match(r'cat.*', _)],\n             'fea_cont': [_ for _ in train.columns if re.match(r'cont.*', _)]}\n\nprint(f\"There are {len(feat_group['fea_cat'])} categorical features, and {len(feat_group['fea_cont'])} continuous features.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if any cat features contain NaN's\ncount_nan = train[feat_group['fea_cat']].count() - train.shape[0]\ncount_nan.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Also check if there is any NaN's in the submit dataset\ncount_nan = submit[feat_group['fea_cat']].count() - submit.shape[0]\ncount_nan.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"based on the result, there is no NaN's for categorical features in neither train or submit dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the variaty of each categorical features.\ncat_var = {}\nfor cat in feat_group['fea_cat']:\n    cat_var[cat] = train[cat].unique().size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_var = Series(cat_var)\ncat_var.value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most features only contain two categories, while 11 features contain > 10 categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For those features containing > 10 categories, group the minorities.\ncat_fea_group = {_: [] for _ in cat_var[cat_var > 10].index}\ncat_fea_group","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compose a function to display frequency distribution table and chart.\ndef freq_dist(target, df):\n    dist_df = DataFrame(np.concatenate((df[target].value_counts().values.astype(np.float_).reshape(-1,1), \n               (df[target].value_counts().values.astype(np.float_) / df.shape[0]).reshape(-1,1),\n                                         (df[target].value_counts().values.astype(np.float_) / df.shape[0]).cumsum().reshape(-1,1)),\n              axis = 1), \n          columns = ['Frequency', 'Percentage','Cul_Percent'],\n         index = df[target].value_counts().index).sort_values(by = 'Percentage', ascending = False)\n    dist_df = pd.merge(dist_df, DataFrame(train.groupby(target).mean()['loss']), left_index = True, right_index = True, how = 'left')\n    dist_df.columns= ['Frequency', 'Percentage','Cul_Percent','Loss_Mean']\n    print(dist_df)\n    \n    fig, axis = plt.subplots(1,2, figsize = (18,6))\n    \n    axis[0].bar(dist_df.index, \"Percentage\", data = dist_df)\n    axis[0].set_title(' '.join((target, \"Frequency Plot\")), fontsize = 15)\n    axis[0].set_ylim(top = 1)\n    \n    axis[1].bar(dist_df.index, 'Loss_Mean', data = dist_df, color = 'brown')\n    axis[1].set_title(' '.join((target, \"vs. Loss (Mean)\")), fontsize = 15)\n    \n    plt.show()\n    return dist_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cat99"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat99_distrib = freq_dist('cat99', train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"P\" and \"T\" take over more than 80% of the percentage in total, therefore group the other categories into \"Others\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_fea_group['cat99'] = cat99_distrib[cat99_distrib['Cul_Percent'] > 0.85].index\ncat_fea_group['cat99']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cat100"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat100_distrib = freq_dist('cat100', train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Group the categories from \"C\" to \"E\" into one, which in total takes 1% of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_fea_group['cat100'] = cat100_distrib[cat100_distrib['Percentage'] < 0.03].index\ncat_fea_group['cat100']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cat101"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat101_distrib = freq_dist('cat101', train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Group those except \"A\" into one category"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_fea_group['cat101'] = cat101_distrib[cat101_distrib['Cul_Percent'] > 0.60].index\ncat_fea_group['cat101']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cat103"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat103_distrib = freq_dist('cat103', train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Group all categories except A into one category"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_fea_group['cat103'] = cat103_distrib[cat103_distrib['Percentage'] < 0.05].index\ncat_fea_group['cat103']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cat104"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat104_distrib = freq_dist('cat104', train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Group \"L\" to \"Q\" into one category"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_fea_group['cat104'] = cat104_distrib[cat104_distrib['Percentage'] < 0.02].index\ncat_fea_group['cat104']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cat105"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat105_distrib = freq_dist('cat105', train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Group \"G\" to \"T\" into one category"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_fea_group['cat105'] = cat105_distrib[cat105_distrib['Percentage'] < 0.2].index\ncat_fea_group['cat105']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cat106"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat106_distrib = freq_dist('cat106', train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Group from \"L\" to \"P\""},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_fea_group['cat106'] = cat106_distrib[cat106_distrib['Percentage'] < 0.02].index\ncat_fea_group['cat106']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cat107"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat107_distrib = freq_dist('cat107', train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Group \"L\" to \"B\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_fea_group['cat107'] = cat107_distrib[cat107_distrib['Percentage'] < 0.04].index\ncat_fea_group['cat107']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cat108"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'cat108'\ncat108_distrib = freq_dist(target, train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Group \"I\" to \"J\""},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_fea_group[target] = cat108_distrib[cat108_distrib['Percentage'] < 0.03].index\ncat_fea_group[target]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cat109"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'cat109'\ncat109_distrib = freq_dist(target, train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Group all except \"BI\""},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_fea_group[target] = cat109_distrib[cat109_distrib['Percentage'] < 0.12].index\ncat_fea_group[target]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cat110"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'cat110'\ncat110_distrib = freq_dist(target, train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There is no obvious gap between frequencies, therefore no category grouping here.\ncat_fea_group[target] = []\ncat_fea_group[target]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Cat111"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'cat111'\ncat111_distrib = freq_dist(target, train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_fea_group[target] = cat111_distrib[cat111_distrib['Percentage'] < 0.05].index\ncat_fea_group[target]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cat112"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'cat112'\ncat112_distrib = freq_dist(target, train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Not going to group this feature\ncat_fea_group[target] = []\ncat_fea_group[target]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cat113"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'cat113'\ncat113_distrib = freq_dist(target, train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No obvious frequency gap, therefore no grouping will be conducted on the feature."},{"metadata":{},"cell_type":"markdown","source":"### Cat 114"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'cat114'\ncat114_distrib = freq_dist(target, train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_fea_group[target] = cat114_distrib[cat114_distrib['Percentage'] < 0.1].index\ncat_fea_group[target]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cat115"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'cat115'\ncat115_distrib = freq_dist(target, train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_fea_group[target] = cat115_distrib[cat115_distrib['Percentage'] < 0.01].index\ncat_fea_group[target]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cat116"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'cat116'\ncat116_distrib = freq_dist(target, train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# No obvious frequency gap between categories. No grouping conducted.\ncat_fea_group[target] = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Category Grouping Summary\n\nThe dictionary stores minor categories for each feature, which will be grouped into \"Others\" Cateories."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_fea_group","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Delete the keys with no values assigned.\ntry:\n    del cat_fea_group['cat110']\n    del cat_fea_group['cat112']\n    del cat_fea_group['cat113']\n    del cat_fea_group['cat116']\nexcept:\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fea_group(df, fea_dist):\n    for feature in fea_dist:\n        df.loc[df[feature].isin(fea_dist[feature].values.tolist()), feature] = 'Others'\n        \ndef new_fea_group(df1, df2):\n    for col in feat_group['fea_cat']:\n        df2.loc[~df2[col].isin(df1[col].value_counts().index.tolist()), col] = df1[col].value_counts().index[0]\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group minor categories\nfea_group(train, cat_fea_group)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## One Hot Encoding categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder(drop = 'first')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One Hot Encoding all categorical features\ntrain_ohe = ohe.fit_transform(train[feat_group['fea_cat']])\nohe_columns = ohe.get_feature_names(feat_group['fea_cat'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Continuous Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_group['fea_cont']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[feat_group['fea_cont']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display value distribution for each feature\n\nfig, ax = plt.subplots(len(feat_group['fea_cont']) // 2, 2, figsize = (16, 5 * (len(feat_group['fea_cont']) // 2)))\nfor idx, item in enumerate(feat_group['fea_cont']):\n    ax[idx//2, idx % 2].violinplot(train[item], showmedians = True)\n    ax[idx//2, idx % 2].set_title(item, fontsize = 20)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All continuous features seem to have been normalized. However, the distribution does not follow normal distribution.\n\nHence we standardize the continuous features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# draw a heatmap to display the correlations between continuous features.\nplt.figure(figsize = (10,10))\nplt.title(\"Correlation Heatmap of Continuous Features\", fontsize = 20)\nsns.heatmap(train[feat_group['fea_cont']].corr(), cmap = 'RdBu_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the map above, drop \"Cont1\", \"Cont6\", \"Cont7\", \"Cont9\", \"Cont10\", and \"Cont11\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"# draw a heatmap to display the correlations between continuous features.\nplt.figure(figsize = (10,10))\nplt.title(\"Correlation Heatmap of Continuous Features\", fontsize = 20)\nsns.heatmap(train[['cont2','cont3','cont4','cont5','cont8','cont12','cont13','cont14']].corr(), cmap = 'RdBu_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_group['fea_cont'] =  ['cont2','cont3','cont4','cont5','cont8','cont12','cont13','cont14']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nsdc = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def standard_scaler(df, feat_dist):\n    df.loc[:, feat_dist['fea_cont']] = sdc.fit_transform(df[feat_dist['fea_cont']])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"standard_scaler(train, feat_group)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(len(feat_group['fea_cont']) // 2, 2, figsize = (16, 5 * (len(feat_group['fea_cont']) // 2)))\nfor idx, item in enumerate(feat_group['fea_cont']):\n    ax[idx//2, idx % 2].violinplot(train[item], showmedians = True)\n    ax[idx//2, idx % 2].set_title(item, fontsize = 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Review the dependent variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['loss'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['loss'].describe())\nplt.violinplot(train['loss'], showmedians = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variable is highly skewed, hence we will perform a log transformation and then standardize it"},{"metadata":{"trusted":true},"cell_type":"code","source":"transformed_loss = np.log1p(train['loss'].values)\n\nfrom sklearn.preprocessing import StandardScaler\n\nsdc_loss = StandardScaler()\n\ntransformed_loss = sdc_loss.fit_transform(transformed_loss.reshape(-1,1))\n\nplt.violinplot(transformed_loss, showmedians = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Modeling\n\nUse sparse matrix to form training dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.sparse as ssp\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(ssp.hstack((train[feat_group['fea_cont']],\n                                                               train_ohe)),\n                                                    transformed_loss, test_size = 0.3)\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Simple Linear Regreesor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('R2 Score for Training Dataset: ',lr.score(x_train,y_train).round(2), '\\n',\n      \"R2 Score for Testing Dataset: \", lr.score(x_test,y_test).round(2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MAE for Training Dataset: \", mean_absolute_error(np.exp(sdc_loss.inverse_transform(y_train)), \n                                                        np.exp(sdc_loss.inverse_transform(lr.predict(x_train.toarray())))),\n      '\\n',\n     \"MAE for Testing Dataset: \", mean_absolute_error(np.exp(sdc_loss.inverse_transform(y_test)),\n                                                      np.exp(sdc_loss.inverse_transform(lr.predict(x_test.toarray())))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the result the model is optmized.\n\nHence, try adding penailties."},{"metadata":{},"cell_type":"markdown","source":"### Linear Regressor with Ridge Penalty"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nlr_ridge = Ridge(alpha = 5000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_ridge.fit(x_train, y_train)\n\nprint('R2 Score for Training Dataset: ',lr_ridge.score(x_train,y_train).round(2), '\\n',\n      \"R2 Score for Testing Dataset: \", lr_ridge.score(x_test,y_test).round(2))\n\nprint(\"MAE for Training Dataset: \", mean_absolute_error(np.exp(sdc_loss.inverse_transform(y_train)), \n                                                        np.exp(sdc_loss.inverse_transform(lr_ridge.predict(x_train.toarray())))),\n      '\\n',\n     \"MAE for Testing Dataset: \", mean_absolute_error(np.exp(sdc_loss.inverse_transform(y_test)),\n                                                      np.exp(sdc_loss.inverse_transform(lr_ridge.predict(x_test.toarray())))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regressor with LASSO Penalty"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\nlr_lasso = Lasso(alpha = 2**-12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_lasso.fit(x_train, y_train)\n\nprint('R2 Score for Training Dataset: ',lr_lasso.score(x_train,y_train).round(2), '\\n',\n      \"R2 Score for Testing Dataset: \", lr_lasso.score(x_test,y_test).round(2))\n\nprint(\"MAE for Training Dataset: \", mean_absolute_error(np.exp(sdc_loss.inverse_transform(y_train)), \n                                                        np.exp(sdc_loss.inverse_transform(lr_lasso.predict(x_train.toarray())))),\n      '\\n',\n     \"MAE for Testing Dataset: \", mean_absolute_error(np.exp(sdc_loss.inverse_transform(y_test)),\n                                                      np.exp(sdc_loss.inverse_transform(lr_lasso.predict(x_test.toarray())))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regressor with ElasticNet (Cross Validation)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import ElasticNet\nimport math\n\nlr_eln = ElasticNet()\n\nparameters = {'alpha': [2**-8, 2**-6, 2**-4, 2**-2],\n             'l1_ratio': [0.1, 0.2],\n              'max_iter': [100]\n             }\n\nlr_cv = GridSearchCV(lr_eln, parameters, cv = 3, scoring = 'neg_mean_absolute_error')"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"lr_cv.fit(x_train, y_train)\nlr_cv.best_estimator_"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"print(\"MAE for Training Dataset: \", mean_absolute_error(np.exp(sdc_loss.inverse_transform(y_train)), \n                                                        np.exp(sdc_loss.inverse_transform(lr_cv.predict(x_train.toarray())))),\n      '\\n',\n     \"MAE for Testing Dataset: \", mean_absolute_error(np.exp(sdc_loss.inverse_transform(y_test)),\n                                                      np.exp(sdc_loss.inverse_transform(lr_cv.predict(x_test.toarray())))))"},{"metadata":{},"cell_type":"markdown","source":"## Ensemble Modeling"},{"metadata":{},"cell_type":"markdown","source":"### Gradiant Boosting Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\ngbr = HistGradientBoostingRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'learning_rate': [0.1, 0.05],\n              'scoring' : ['mae'],\n              'max_iter': [100],\n              'max_depth': [5]\n          ##    'l2_regularization': [2**-1, 2**-5]\n             }\n\ngbr_cv = GridSearchCV(gbr, parameters, cv = 3, scoring = 'neg_mean_absolute_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr_cv.fit(x_train.toarray(), y_train.ravel())\ngbr_cv.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MAE for Training Dataset: \", mean_absolute_error(np.exp(sdc_loss.inverse_transform(y_train)), \n                                                        np.exp(sdc_loss.inverse_transform(gbr_cv.predict(x_train.toarray())))),\n      '\\n',\n     \"MAE for Testing Dataset: \", mean_absolute_error(np.exp(sdc_loss.inverse_transform(y_test)),\n                                                      np.exp(sdc_loss.inverse_transform(gbr_cv.predict(x_test.toarray())))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Therefore, will use Hist Grandiant Boosting Regressor"},{"metadata":{},"cell_type":"markdown","source":"## Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. Process the categorical features\n\nfea_group(submit, cat_fea_group)\nnew_fea_group(train, submit)\n\nsubmit_ohe = ohe.transform(submit.loc[:, feat_group['fea_cat']])\nsubmit_ohe_columns = ohe.get_feature_names(feat_group['fea_cat'])\n\n# 2. Process the continuous features\n\nfeat_group['fea_cont'] =  ['cont2','cont3','cont4','cont5','cont8','cont12','cont13','cont14']\nstandard_scaler(submit, feat_group)\n\n# 3. Form the test dataset\nsubmit2 = ssp.hstack((submit[feat_group['fea_cont']], submit_ohe))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_predict = np.exp(sdc_loss.inverse_transform(gbr_cv.predict(submit2.toarray())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id': submit['id'].values, 'loss': submit_predict})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', sep = ',', header = True, index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernel_info":{"name":"python3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"nteract":{"version":"0.15.0"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":"block","toc_window_display":false}},"nbformat":4,"nbformat_minor":1}