{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Import Dependencies\n%matplotlib inline\n#### Start Python Imports\nimport math, time, random, datetime\n#### Data Manipulation\nimport numpy as np\nimport pandas as pd\n#### Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n#### Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n\n#### Machine learning\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n\n##### Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \ntrain = pd.read_csv(\"../input/allstate-claims-severity/train.csv\")\ntest = pd.read_csv(\"../input/allstate-claims-severity/test.csv\")\n\n#Print all rows and columns. Dont hide any\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Understand the data\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the test id for later submission \n# and drop the id from train and test. As id is unique for all rows and don't carry any information \ntest_id = test[\"id\"]\ntest.drop(\"id\", axis = 1, inplace = True)\ntrain.drop(\"id\", axis = 1, inplace = True)\nprint(train.loss.describe())\nprint(\"\")\nprint(\"The loss/target skewness : \",train.loss.skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# since the skewness of loss is about 3, which is higher. Hence it should be corrected. \n#I have choosen to go with log1p\n\n#Before skew correction \nsns.violinplot(data=train,y=\"loss\")  \nplt.show()\n\ntrain[\"loss\"] = np.log1p(train[\"loss\"])\n\n#visualize the transformed column\nsns.violinplot(data=train,y=\"loss\")  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loss.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### there is no missing value issue with data "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train.copy()\nvalid = test.copy()\ndel train\ndel test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = (data.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(len(object_cols))\nprint(object_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### All the categorical features should be one hot encoded.... "},{"metadata":{"trusted":true},"cell_type":"code","source":"#combined the train and test so that get dummies can be applied on all at the same time. \n## But at the same time elimination due to correlation should be considered only of from train data. \nmergedata = data.append(valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mergedata.shape)\nprint(valid.shape)\nprint(len(data))\nprint(len(valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### before doing one hot encoding using get dummies, 1] we need to remove highly mutual correlated features(one of the pair), 2] also the features with less correlation with the target.                                                  3] Then we need to remove the data that isn't properly distrubuted. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#missingno.matrix(valid, figsize = (30,12))\n# no missings..","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove the one of the pair columns which have higher abs correlation \ndata_corr = data.corr()\nlistOfFeatures = [i for i in data_corr]\nsetOfDroppedFeatures = set() \nfor i in range(len(listOfFeatures)) :\n    for j in range(i+1,len(listOfFeatures)): #Avoid repetitions \n        feature1=listOfFeatures[i]\n        feature2=listOfFeatures[j]\n        if abs(data_corr[feature1][feature2]) > 0.8: #If the correlation between the features is > 0.8\n            setOfDroppedFeatures.add(feature1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mergedata = mergedata.drop(setOfDroppedFeatures, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Now the correlation with the target "},{"metadata":{"trusted":true},"cell_type":"code","source":"s = (data.dtypes != 'object')\nnon_object_cols = list(s[s].index)\nnon_object_cols = [ col for col in non_object_cols if col not in setOfDroppedFeatures ]\nprint(len(non_object_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nonCorrelatedWithOutput = [column for column in non_object_cols if abs(data[column].corr(data[\"loss\"])) < 0.025]\nprint(len(nonCorrelatedWithOutput))\nnonCorrelatedWithOutput","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mergedata = mergedata.drop(nonCorrelatedWithOutput, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Non correlated cols are removed..."},{"metadata":{"trusted":true},"cell_type":"code","source":"mergedata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)### By observing the distrubutions(very uneven), mey be we should drop some of the features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# #uncomment if want to see the visualizatio of distribution (takes time)\n\n# #names of all the columns\n# cols = data.columns\n\n# #Plot count plot for all attributes in a 29x4 grid\n# n_cols = 4\n# n_rows = 29\n# for i in range(n_rows):\n#     fg,ax = plt.subplots(nrows=1,ncols=n_cols,sharey=True,figsize=(12, 8))\n#     for j in range(n_cols):\n#         sns.countplot(x=cols[i*n_cols+j], data=data, ax=ax[j])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cols taken from the visualization plots.\n# to-do: find out generic method to do the same. \nunbalanced_cols = ['cat15','cat18','cat20','cat21','cat22','cat33','cat35','cat48','cat56','cat58',\n                   'cat59','cat60','cat62','cat63','cat64','cat67','cat68','cat69','cat70','cat77']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mergedata = mergedata.drop(unbalanced_cols, axis=1)\nprint(mergedata.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now one hot encoding!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"mergedata_ohe = pd.get_dummies(mergedata)\nmergedata_ohe.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seperate the train and test from mergedata\ntrainLen = len(data)\nprint(\"train len : \", trainLen)\ntrain_ohe = mergedata_ohe.iloc[:trainLen]\ntest_ohe = mergedata_ohe.iloc[trainLen:]\n\ndel mergedata_ohe\ndel data\ndel valid\n\nprint(train_ohe.shape)\nprint(test_ohe.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_ohe.loss.isnull().sum())\n# We should remove the extra added loss column to the test data\ntest_ohe = test_ohe.drop(\"loss\", axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Done with the encoding and EDA so far. Will have to check with different  alternatives : 1] with out dropping unbalanced data. 2] without the skew fixing  "},{"metadata":{},"cell_type":"markdown","source":"### *** let's try ML now"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Build and fit the model\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import Lasso, Ridge\nimport xgboost as xgb # XGBoost implementation\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0007, random_state=1))\nridge = make_pipeline(RobustScaler(), Ridge(alpha =20, random_state=42))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_ohe.drop(\"loss\", axis = 1)\nY_train = train_ohe[\"loss\"]\nprint(X_train.shape)\nprint(Y_train.shape)\ndel train_ohe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lasso \n# lasso.fit(X_train, y_train)\n# test_y_log = lasso.predict(test_ohe)\n# test_y = np.exp(1)**test_y_log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train, train_size=.90,random_state=2016)\n\n\nRANDOM_STATE = 2016\n#'seed': RANDOM_STATE,\nparams = {\n    'min_child_weight': 1,\n    'eta': 0.03,\n    'colsample_bytree': 0.5,\n    'max_depth': 12,\n    'subsample': 0.8,\n    'alpha': 1,\n    'seed': RANDOM_STATE,\n    'gamma': 1.5,\n    'silent': 1,\n    'verbose_eval': True,\n    'nthread':7,\n    'base_score':7.76\n}\n\n\n\nxgtrain = xgb.DMatrix(x_train, label=y_train)\nxgval = xgb.DMatrix(x_valid, label=y_valid)\n\nwatchlist = [ (xgtrain,'train'),(xgval,'eval')]\n\nmodel = xgb.train(params, xgtrain,300, watchlist,verbose_eval=True)\n\ndel xgtrain\ndel xgval\nxgtest = xgb.DMatrix(test_ohe)\ntest_y_log = model.predict(xgtest)\ntest_y = np.exp(1)**test_y_log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_train\ndel y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission[\"id\"] = test_id\nprint(test_y.shape)\nprint(test_id.shape)\n\nsubmission[\"loss\"] = test_y\nsubmission.to_csv(\"submission_svm_l.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# score is : 1126.77","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}