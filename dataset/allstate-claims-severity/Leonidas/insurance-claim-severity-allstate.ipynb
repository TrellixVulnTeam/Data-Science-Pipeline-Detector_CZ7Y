{"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","file_extension":".py","version":"3.6.4","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3}}},"cells":[{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n#Header Files \nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\n","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"b8b5e55d-89b7-42e0-8bd2-563470ce047d","_uuid":"a6b75ecb8950e170488011de1d8f2c3f987295b2"}},{"source":"#Load Dataset\nDF= pd.read_csv(\"../input/train.csv\")\nDF_Test = pd.read_csv(\"../input/test.csv\")\n\n#To show all the columns and rows\npd.set_option('display.max_rows',None)\npd.set_option('display.max_columns', None)\n\n#Take a look at the dataset\nprint(DF.head(5))\n\n#Storing the ID from test for future prediction\nID= DF_Test['id']\n#Dropping ID because it is meaningless\nDF.drop('id',axis=1, inplace=True)\nDF_Test.drop('id',axis=1, inplace=True)\n","cell_type":"code","execution_count":null,"outputs":[],"metadata":{}},{"source":"**Data Statistics**","cell_type":"markdown","metadata":{}},{"source":"#For numerical/continous values\nprint(\"Cont. Features\")\nprint(\"-\"*75)\nprint(DF.describe())\n\n#For categorical values\nprint(\"Cat. Features\")\nprint(\"-\"*75)\nprint(DF.describe(include=['O']))","cell_type":"code","execution_count":null,"outputs":[],"metadata":{}},{"source":"**Data Visualisation**","cell_type":"markdown","metadata":{}},{"source":"#Taking only numerical values\nsize=15\nsplit=116\nContDF=DF.iloc[:,split:]\n\n#Name of all columns\nCol=ContDF.columns\n\n#Plotting violin plot for all columns\nn_rows=5\nn_columns=3\n\nfor i in range(n_rows):\n    fg,ax = plt.subplots(nrows=1, ncols=n_columns,figsize=(12,8))\n    for j in range(n_columns):\n        sns.violinplot(y=Col[i*n_columns+j], data=ContDF,ax=ax[j])\n        ","cell_type":"code","execution_count":null,"outputs":[],"metadata":{}},{"source":"**Observations:**\n\ncont1, cont3, cont6 has lot of values close to 0.5\n\ncont2 has several spikes at several values\n\ncont 4 has values spread evenly from 0.2-0.5\n\ncont 5 has lot of values close to 0.3  ..........Similarly you can read the plot and interpret\n\nWhat's really interesting is that 'loss' column(our target variable) is heavily skewed\n(Need to apply log function)","cell_type":"markdown","metadata":{}},{"source":"DF['loss']=np.log1p(DF['loss'])\n#Let's visualise the new plot\nsns.violinplot(data=DF, y='loss')\nplt.show()\n\n#PLOT shows that skew has been corrected to a large extent","cell_type":"code","execution_count":null,"outputs":[],"metadata":{}},{"source":"**Data Correlation**","cell_type":"markdown","metadata":{}},{"source":"\nCorrMatrix= ContDF.corr().abs()\n\n#Heatmap\nplt.subplots(figsize=(13, 9))\nsns.heatmap(CorrMatrix,annot=True)\nsns.heatmap(CorrMatrix, mask=CorrMatrix < 1, cbar=False)\nplt.show()\n","cell_type":"code","execution_count":null,"outputs":[],"metadata":{}},{"source":"**Observation:**\n\nAll those pairs showing high correlation(>0.85), one of them can be removed{Cont11-Cont12, Cont1-Cont9, Cont6-Cont10}\n    ","cell_type":"markdown","metadata":{}},{"source":"**Conversion of Categorical Variables**","cell_type":"markdown","metadata":{}},{"source":"labellist = []\nCol= DF.columns\nfor i in range(0,split):\n    train = DF[Col[i]].unique()\n    test = DF[Col[i]].unique()\n    labellist.append(list(set(train) | set(test)))    \n\n\n\n#Hot encoding all categorical attributes\ncateg = []\nfor i in range(0, split):\n    #Label encode\n    label_encoder = LabelEncoder()\n    label_encoder.fit(labellist[i])\n    feature = label_encoder.transform(DF.iloc[:,i])\n    feature = feature.reshape(DF.shape[0], 1)\n    #One hot encode\n    onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labellist[i]))\n    feature = onehot_encoder.fit_transform(feature)\n    categ.append(feature)\n\n# Make a nd.numpyarray\nencoded_categ = np.column_stack(categ)\n\n\n\n#Combine encoded attributes with continuous attributes\nDF_encoded = np.concatenate((encoded_categ,DF.iloc[:,split:].values),axis=1)\n","cell_type":"code","execution_count":null,"outputs":[],"metadata":{}},{"source":"**Data Splitting**","cell_type":"markdown","metadata":{}},{"source":"#number of rows and columns\nr, c = DF_encoded.shape\n\n#create an array which has indexes of columns\ni_cols = []\nfor i in range(0,c-1):\n    i_cols.append(i)\n\n#y is the target variable, X is the remaining  data\nX = DF_encoded[:,0:(c-1)]\ny = DF_encoded[:,(c-1)]\n\n\nX_train, X_test, y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=7)","cell_type":"code","execution_count":null,"outputs":[],"metadata":{}},{"source":"**Data Prediction and Evaulation**","cell_type":"markdown","metadata":{}},{"source":"1. Linear Regression","cell_type":"markdown","metadata":{}},{"source":"model = LinearRegression(n_jobs=-1)\n#Accuracy of the model \nmodel.fit(X_train,y_train)\nresult = mean_absolute_error(np.expm1(Y_test), np.expm1(model.predict(X_test)))\n\n","cell_type":"code","execution_count":null,"outputs":[],"metadata":{}},{"source":"Mean Absolute Error achieved is 1278","cell_type":"markdown","metadata":{}},{"source":"model = XGBRegressor(n_estimators=1000,seed=7)\n#Accuracy of the model \nmodel.fit(X_train,y_train)\nresult = mean_absolute_error(np.expm1(Y_test), np.expm1(model.predict(X_test)))\n            \n\n","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"source":"Mean Absolute Error achieved is 1170","cell_type":"markdown","metadata":{}}],"nbformat_minor":1,"nbformat":4}