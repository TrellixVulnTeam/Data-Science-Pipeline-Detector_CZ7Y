{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://developer.nvidia.com/sites/default/files/pictures/2018/rapids/rapids-logo.png\"/>"},{"metadata":{},"cell_type":"markdown","source":"[Rapids](https://rapids.ai) is an open-source GPU accelerated Data Sceince and Machine Learning library, developed and mainatained by [Nvidia](https://www.nvidia.com). It is designed to be compatible with many existing CPU tools, such as Pandas, scikit-learn, numpy, etc. It enables **massive** acceleration of many data-science and machine learning tasks, oftentimes by a factor fo 100X, or even more. \n\nRapids is still undergoing developemnt, and this notebook is the first use of RAPIDS in the Kaggle Docker environment. If you are interested in installing and running Rapids locally on your own machine, then you should [refer to the followong instructions](https://rapids.ai/start.html)."},{"metadata":{},"cell_type":"markdown","source":"This notebook is based on the following Giba notebook: https://www.kaggle.com/titericz/0-309-baseline-logisticregression-using-fft/output"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cudf\nimport cuml\nimport cupy as cp\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport os\nfrom scipy.interpolate import interp1d\nimport gc\nfrom cuml.linear_model import LogisticRegression\nfrom cuml.neighbors import KNeighborsClassifier\nfrom cuml.svm import SVC\nfrom cuml.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GroupKFold\n\nimport soundfile as sf\n# Librosa Libraries\nimport librosa\nimport librosa.display\nimport IPython.display as ipd\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.metrics import roc_auc_score, label_ranking_average_precision_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"trainfiles = glob.glob( '../input/rfcx-species-audio-detection/train/*.flac' )\ntestfiles = glob.glob( '../input/rfcx-species-audio-detection/test/*.flac' )\nlen(trainfiles), len(testfiles), trainfiles[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"traint = cudf.read_csv( '../input/rfcx-species-audio-detection/train_tp.csv' )\ntraint['t_dif'] = traint['t_max'] - traint['t_min']\ntraint['f_dif'] = traint['f_max'] - traint['f_min']\n\ntrainf = cudf.read_csv( '../input/rfcx-species-audio-detection/train_fp.csv' )\ntrainf['t_dif'] = trainf['t_max'] - trainf['t_min']\ntrainf['f_dif'] = trainf['f_max'] - trainf['f_min']\n\ntraint.shape, trainf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traint.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainf.f_dif.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data, samplerate = sf.read(trainfiles[0]) \nprint( data.shape, samplerate )\nlibrosa.display.waveplot(y = data, sr = samplerate, color = \"#B14D\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traint.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_fft(fn):\n    data, samplerate = sf.read(fn)\n    data = cp.array(data)\n\n    varfft = cp.abs( cp.fft.fft(data)[:(len(data)//2)] )\n    \n    return cp.asnumpy( varfft.reshape( (1000,1440) ).mean(axis=1) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FT = []\nfor fn in tqdm(traint.recording_id.to_array()):\n    FT.append( extract_fft( '../input/rfcx-species-audio-detection/train/'+fn+'.flac' ) )\nFT = np.stack(FT)\ngc.collect()\n\nFT.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This loop runs in 7min using cupy(GPU) and 40min on numpy(CPU). ~7x Faster in GPU\n\nFF = []\nfor fn in tqdm(trainf.recording_id.to_array()):\n    FF.append( extract_fft( '../input/rfcx-species-audio-detection/train/'+fn+'.flac' ) )\nFF = np.stack(FF)\ngc.collect()\n\nFF.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Combine True Positives and False Positives\n\nTRAIN = np.vstack( (FT, FF) )\n\ndel FT, FF\ngc.collect()\nTRAIN.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST = []\nfor fn in tqdm(testfiles):\n    TEST.append( extract_fft(fn) )\nTEST = np.stack(TEST)\ngc.collect()\n\nTEST.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tt = traint[['recording_id','species_id']].copy()\ntf = trainf[['recording_id','species_id']].copy()\n\ntf['species_id'] = -1\n\nTRAIN_TAB = cudf.concat( (tt, tf) )\n\nfor i in range(24):\n    TRAIN_TAB['s'+str(i)] = 0\n    TRAIN_TAB.loc[TRAIN_TAB.species_id==i,'s'+str(i)] = 1\n\nTRAIN_TAB.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_TAB.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nn_folds = 8\nsub = cudf.DataFrame({'recording_id': [f.split('/')[-1].split('.')[0] for f in testfiles] })\ngkf = GroupKFold(n_folds)\n\ngroups = TRAIN_TAB['recording_id'].to_array()\nfor tgt in tqdm(range(24)):\n    target = TRAIN_TAB['s'+str(tgt)].values\n\n    ytrain = np.zeros(TRAIN.shape[0])\n    ytest = np.zeros(TEST.shape[0])\n    for ind_train, ind_valid in gkf.split( TRAIN, target, groups ):\n        model1 = LogisticRegression( C=1, max_iter=10000 )\n        model1.fit( TRAIN[ind_train], target[ind_train] )\n        \n        model2 = SVC(probability=True, kernel='rbf', gamma='auto')\n        model2.fit( TRAIN[ind_train], target[ind_train] )\n        \n        model3 = KNeighborsClassifier(n_neighbors=30)\n        model3.fit( TRAIN[ind_train], target[ind_train] )\n        \n        model4 = RandomForestClassifier(n_estimators=500, max_depth=13)\n        model4.fit( np.float32(TRAIN[ind_train]), target[ind_train] )\n        \n        ytrain[ind_valid] = (0.2*model1.predict_proba(np.float32(TRAIN[ind_valid]))[:,1]+0.2*model2.predict_proba(np.float32(TRAIN[ind_valid]))[:,1]+\n                             0.2*model3.predict_proba(np.float32(TRAIN[ind_valid]))[:,1]+0.4*model4.predict_proba(np.float32(TRAIN[ind_valid]))[:,1])\n        ytest += (0.2*model1.predict_proba(np.float32(TEST))[:,1]+0.2*model2.predict_proba(np.float32(TEST))[:,1]+0.2*model3.predict_proba(np.float32(TEST))[:,1]\n                 +0.4*model4.predict_proba(np.float32(TEST))[:,1]) / n_folds.\n\n    print( 'Target AUC', tgt, roc_auc_score(target.get(), ytrain) )\n    \n    TRAIN_TAB['y'+str(tgt)] = ytrain\n    sub['s'+str(tgt)] = ytest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}