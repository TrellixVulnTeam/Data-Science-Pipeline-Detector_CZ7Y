{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport seaborn as sns\nimport IPython.display as ipD\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as ptc\n\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nmm = MinMaxScaler()\nss = StandardScaler()\n\nimport librosa\nimport librosa.display as LD\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# paths\ntest_folder = \"../input/rfcx-species-audio-detection/test\"\ntfrecords = \"../input/rfcx-species-audio-detection/tfrecords\"\ntrain_folder = \"../input/rfcx-species-audio-detection/train\"\nsample_submission = \"../input/rfcx-species-audio-detection/sample_submission.csv\"\ntrain_tp = \"../input/rfcx-species-audio-detection/train_tp.csv\"\ntrain_fp = \"../input/rfcx-species-audio-detection/train_fp.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seedAll(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"]=str(seed)\n\nseedAll(2021)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trtp = pd.read_csv(train_tp)\ntrtp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18, 6))\nsns.countplot(x=\"species_id\", data=trtp)\nplt.title(\"Species ID distribution for True Positives\")\nplt.show()\n\nplt.figure(figsize=(18, 6))\nsns.countplot(x=\"species_id\", hue=\"songtype_id\", data=trtp)\nplt.title(\"Species ID distribution for True Positives grouped by song_type\")\nplt.show()\n\nplt.figure(figsize=(8, 8))\nwegdes, texts, autotexts = plt.pie(trtp[\"songtype_id\"].value_counts(), \n        startangle=45, \n        wedgeprops={\"linewidth\":1, \"edgecolor\":\"black\"}, \n        autopct='%1.f%%', \n        shadow=True,\n        textprops= dict(color=\"black\"),\n        explode=(0.2, 0.2))\nplt.legend(wegdes, trtp[\"songtype_id\"].value_counts().index,\n          title=\"song type\",\n          loc=\"center\",\n          bbox_to_anchor=(1, 0, 0, 0))\nplt.setp(autotexts, size=14, weight=\"bold\")\nplt.title(\"Song types distribution for True positives\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trfp = pd.read_csv(train_fp)\ntrfp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18, 6))\nsns.countplot(x=\"species_id\", data=trfp)\nplt.title(\"Species ID distribution for False Positives\")\nplt.show()\n\nplt.figure(figsize=(18, 6))\nsns.countplot(x=\"species_id\", hue=\"songtype_id\", data=trfp)\nplt.title(\"Species ID distribution for False Positives grouped by song_type\")\nplt.show()\n\nplt.figure(figsize=(8, 8))\nwegdes, texts, autotexts = plt.pie(trfp[\"songtype_id\"].value_counts(), \n        startangle=45, \n        wedgeprops={\"linewidth\":1, \"edgecolor\":\"black\"}, \n        autopct='%1.f%%', \n        shadow=True,\n        textprops= dict(color=\"black\"),\n        explode=(0.2, 0.2))\nplt.legend(wegdes, trfp[\"songtype_id\"].value_counts().index,\n          title=\"song type\",\n          loc=\"center\",\n          bbox_to_anchor=(1, 0, 0, 0))\nplt.setp(autotexts, size=14, weight=\"bold\")\nplt.title(\"Song types distribution for False positives\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total Train audio samples: \", len(os.listdir(train_folder)))\nprint(\"Total Test audio samples: \", len(os.listdir(test_folder)))\nprint(\"Number of samples present both in Train True Positives and Train False Positives: \", len(set(trfp.recording_id.tolist()).intersection(trtp.recording_id.tolist())))\nprint(\"Number of unique audio samples in Train True positives: \", trtp.recording_id.nunique())\nprint(\"Number of unique audio samples in Train False positives: \", trfp.recording_id.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trtp[\"clip_duration\"] = trtp[\"t_max\"] - trtp[\"t_min\"]\ntrfp[\"clip_duration\"] = trfp[\"t_max\"] - trfp[\"t_min\"]\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 6))\nsns.boxplot(x=\"clip_duration\", data=trtp, orient=\"h\", ax=ax[0])\nsns.distplot(trtp[\"clip_duration\"], kde=True, ax=ax[1])\n\nfig.suptitle(\"Distribution of length of audio clips containing any species in True positive\")\nfig.show()\n\nplt.figure(figsize=(20, 20))\nsns.boxplot(x=\"clip_duration\", y=\"species_id\", data=trtp, orient=\"h\")\nplt.title(\"species_id-wise Distribution of length of audio clips containing any species in True positive Samples\")\nplt.show()\n\nplt.figure(figsize=(20, 20))\nsns.boxplot(x=\"clip_duration\", y=\"species_id\", data=trfp, orient=\"h\")\nplt.title(\"species_id-wise Distribution of length of audio clips containing any species in False positive Samples\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trtp[\"freq_range\"] = trtp[\"f_max\"] - trtp[\"f_min\"]\ntrfp[\"freq_range\"] = trfp[\"f_max\"] - trfp[\"f_min\"]\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 6))\nsns.boxplot(x=\"freq_range\", data=trtp, orient=\"h\", ax=ax[0])\nsns.distplot(trtp[\"freq_range\"], kde=True, ax=ax[1])\n\nfig.suptitle(\"Distribution of Frequency Ranges containing any species in True positive\")\nfig.show()\n\nplt.figure(figsize=(20, 20))\nsns.boxplot(x=\"freq_range\", y=\"species_id\", data=trtp, orient=\"h\")\nplt.title(\"species_id-wise Distribution of Frequency Ranges containing any species in True positive Samples\")\nplt.show()\n\nplt.figure(figsize=(20, 20))\nsns.boxplot(x=\"freq_range\", y=\"species_id\", data=trfp, orient=\"h\")\nplt.title(\"species_id-wise Distribution of Frequency Ranges containing any species in False positive Samples\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(22, 8))\n\nsns.boxplot(y=\"clip_duration\", x=\"songtype_id\", data=trtp, ax=axs[0])\naxs[0].set_title(\"species_id-wise Distribution of length of audio clips containing any species in True positive Samples\")\n\nsns.boxplot(y=\"clip_duration\", x=\"songtype_id\", data=trfp, ax=axs[1])\naxs[1].set_title(\"species_id-wise Distribution of length of audio clips containing any species in False positive Samples\")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(22, 8))\n\nsns.boxplot(y=\"freq_range\", x=\"songtype_id\", data=trtp, ax=axs[0])\naxs[0].set_title(\"species_id-wise Distribution of Frequency Ranges containing any species in True positive Samples\")\n\nsns.boxplot(y=\"freq_range\", x=\"songtype_id\", data=trfp, ax=axs[1])\naxs[1].set_title(\"species_id-wise Distribution of Frequency Ranges containing any species in False positive Samples\")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# choose a sample from train and test\ntr = os.path.join(train_folder, os.listdir(train_folder)[np.random.randint(0, len(os.listdir(train_folder)))])\nts = os.path.join(test_folder, os.listdir(test_folder)[np.random.randint(0, len(os.listdir(test_folder)))])\n\n# load the np array and the samping rate\ntrx, trsr = librosa.load(tr)\ntsx, tssr = librosa.load(ts)\nrecId_train = (tr.split(\"/\")[-1]).split(\".\")[0]\nrecId_test = (ts.split(\"/\")[-1]).split(\".\")[0]\n\nprint(\"=\"*10, \"Training Sample\", \"=\"*10)\ndisplay(trfp[trfp[\"recording_id\"]==recId_train] if recId_train in trfp[\"recording_id\"].tolist() else trtp[trtp[\"recording_id\"]==recId_train])\nprint(\"=\"*10, \"Test Sample\", \"=\"*10)\nprint(\"Test Data: \", recId_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the shape and the sampling rate\nprint(\"=====Train sample=======\")\nprint(trx.shape, trsr)\nprint(\"=====Test sample=======\")\nprint(tsx.shape, tssr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ipD.Audio(tr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ipD.Audio(ts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 5))\nLD.waveplot(trx, sr=trsr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 5))\nLD.waveplot(tsx, sr=tssr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRX = librosa.stft(trx)\nprint(\"Shape of the stft: \", TRX.shape)\n# convert into db\nTRXdb = librosa.amplitude_to_db(abs(TRX))\n\nplt.figure(figsize=(14, 5))\nlibrosa.display.specshow(TRXdb, sr=trsr, x_axis='time', y_axis='hz')\nplt.colorbar()\nplt.figure(figsize=(14, 5))\nlibrosa.display.specshow(TRXdb, sr=trsr, x_axis='time', y_axis='log')\nplt.title(\"In log scale\")\nplt.colorbar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert into fourier transform\nTSX = librosa.stft(tsx)\nprint(\"Shape of the stft: \", TSX.shape)\n\n# convert into bd\nTSXdb = librosa.amplitude_to_db(abs(TSX))\nplt.figure(figsize=(14, 5))\nlibrosa.display.specshow(TSXdb, sr=tssr, x_axis='time', y_axis='hz')\nplt.colorbar()\nplt.figure(figsize=(14, 5))\nlibrosa.display.specshow(TSXdb, sr=tssr, x_axis='time', y_axis='log')\nplt.colorbar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spectral_centroids = librosa.feature.spectral_centroid(trx, sr=trsr)[0]\nprint(\"Shape of the spectral centroids: \", spectral_centroids.shape)\n\n# extract the time and frame indices\nframes = range(len(spectral_centroids))\nt = librosa.frames_to_time(frames)\n\nplt.figure(figsize=(10, 4))\nLD.waveplot(trx, sr=trsr, alpha=0.4)\nplt.title(\"Spectral Centroids for Train sample\")\nplt.show()\n\nplt.figure(figsize=(10, 4))\nLD.waveplot(trx, sr=trsr, alpha=0.4)\nplt.plot(t, ss.fit_transform(spectral_centroids.reshape(-1, 1)))\nplt.plot(t, mm.fit_transform(spectral_centroids.reshape(-1, 1)))\nplt.legend([\"Audio Signal\", \"sc_ss\", \"sc_mm\"][::-1])\nplt.title(\"Normalized Spectral Centroid Visualization for Train sample\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spectral_centroids = librosa.feature.spectral_centroid(tsx, sr=tssr)[0]\nprint(\"Shape of the spectral centroids: \", spectral_centroids.shape)\n\n# extract the time and frame indices\nframes = range(len(spectral_centroids))\nt = librosa.frames_to_time(frames)\n\nplt.figure(figsize=(10, 4))\nLD.waveplot(tsx, sr=tssr, alpha=0.4)\nplt.title(\"Spectral Centroids for Test sample\")\nplt.show()\n\nplt.figure(figsize=(10, 4))\nLD.waveplot(tsx, sr=tssr, alpha=0.4)\nplt.plot(t, ss.fit_transform(spectral_centroids.reshape(-1, 1)))\nplt.plot(t, mm.fit_transform(spectral_centroids.reshape(-1, 1)))\nplt.legend([\"Audio Signal\", \"sc_ss\", \"sc_mm\"][::-1])\nplt.title(\"Normalized Spectral Centroid Visualization for Test sample\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spectral_rolloff = librosa.feature.spectral_rolloff(trx+0.01, sr=trsr)\nframes = range(len(spectral_rolloff))\nt = librosa.frames_to_time(frames)\n\nplt.figure(figsize=(12, 4))\nLD.waveplot(trx, sr=trsr, alpha=0.4)\nplt.plot(t, ss.fit_transform(spectral_rolloff), color='r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spectral_rolloff = librosa.feature.spectral_rolloff(tsx, sr=tssr)\nframes = range(len(spectral_rolloff))\nt = librosa.frames_to_time(frames)\n\nplt.figure(figsize=(10, 4))\nLD.waveplot(tsx, sr=tssr, alpha=0.4)\nplt.plot(t, ss.fit_transform(spectral_rolloff), color=\"r\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mfccs = librosa.feature.mfcc(trx, sr=trsr)\nprint(mfccs.shape)\n#Displaying  the MFCCs:\nplt.figure(figsize=(15, 7))\nLD.specshow(mfccs, sr=trsr, x_axis='time')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mfccs = librosa.feature.mfcc(tsx, sr=tssr)\nprint(mfccs.shape)\n#Displaying  the MFCCs:\nplt.figure(figsize=(15, 7))\nLD.specshow(mfccs, sr=tssr, x_axis='time')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"melspec = librosa.feature.melspectrogram(trx, sr=trsr)\nprint(melspec.shape)\nplt.figure(figsize=(10, 4))\nlibrosa.display.specshow(librosa.power_to_db(melspec, ref=np.max),\n                         y_axis='mel',\n                         x_axis='time')\nplt.colorbar(format='%+2.0f dB')\nplt.title('Mel spectrogram')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hop_length=12\nchromagram = librosa.feature.chroma_stft(trx, sr=trsr, hop_length=hop_length)\nplt.figure(figsize=(15, 5))\nLD.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chromagram = librosa.feature.chroma_stft(tsx, sr=tssr, hop_length=hop_length)\nplt.figure(figsize=(15, 5))\nLD.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samSub = pd.read_csv(sample_submission)\nsamSub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tfrec = \"../input/rfcx-species-audio-detection/tfrecords/train\"\ntest_tfrec = \"../input/rfcx-species-audio-detection/tfrecords/test\"\n\ntrain_tfrecs = sorted(tf.io.gfile.glob(train_tfrec + \"/*.tfrec\"))\ntest_tfrecs = sorted(tf.io.gfile.glob(test_tfrec + \"/*.tfrec\"))\n\nprint(\"Number of train tfrecords: \", len(train_tfrecs))\nprint(\"Number of test tfrecords: \", len(test_tfrecs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_proto_train_tfrec = tf.data.TFRecordDataset([train_tfrecs[0]])\nsample_proto_train_tfrec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of samples in one single record: \", sample_proto_train_tfrec.reduce(np.int64(), lambda x, _: x+1).numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CUT_TIME = 10 # cutting window in seconds\nSAMPLE_TIME = 6\nSR = 48000\nFMAX = 24000\nFMIN = 40\n# feature description for the tfrecords\n# this will parsed as arguments into tf.io.parse_single_example\nfeature_description = {\n    'recording_id': tf.io.FixedLenFeature([], tf.string, default_value=''),\n    'audio_wav': tf.io.FixedLenFeature([], tf.string, default_value=''),\n    'label_info': tf.io.FixedLenFeature([], tf.string, default_value=''),\n}\nparse_dtype = {\n    'audio_wav': tf.float32,\n    'recording_id': tf.string,\n    'species_id': tf.int32,\n    'songtype_id': tf.int32,\n    't_min': tf.float32,\n    'f_min': tf.float32,\n    't_max': tf.float32,\n    'f_max':tf.float32,\n    'is_tp': tf.int32\n}\n\n# define a fun to read the encoded tfrec\n@tf.function\ndef _parse_fun(sample):\n    sample = tf.io.parse_single_example(sample, feature_description) # this returns a dicionary of the features for a single tfrec\n    audio, _ = tf.audio.decode_wav(sample[\"audio_wav\"], desired_channels=1)\n    label_info = tf.strings.split(sample[\"label_info\"], \";\")\n    labels = tf.strings.regex_replace(label_info, '\"', '')\n    \n    @tf.function\n    def _cut_audio(label):\n        items = tf.strings.split(label, sep=',')\n        spid = tf.squeeze(tf.strings.to_number(items[0], tf.int32))\n        soid = tf.squeeze(tf.strings.to_number(items[1], tf.int32))\n        tmin = tf.squeeze(tf.strings.to_number(items[2]))\n        fmin = tf.squeeze(tf.strings.to_number(items[3]))\n        tmax = tf.squeeze(tf.strings.to_number(items[4]))\n        fmax = tf.squeeze(tf.strings.to_number(items[5]))\n        tp = tf.squeeze(tf.strings.to_number(items[6], tf.int32))\n\n        tmax_s = tmax * tf.cast(SR, tf.float32)\n        tmin_s = tmin * tf.cast(SR, tf.float32)\n        cut_s = tf.cast(CUT_TIME * SR, tf.float32)\n        all_s = tf.cast(60 * SR, tf.float32)\n        tsize_s = tmax_s - tmin_s\n        cut_min = tf.cast(\n            tf.maximum(0.0, \n                tf.minimum(tmin_s - (cut_s - tsize_s) / 2,\n                           tf.minimum(tmax_s + (cut_s - tsize_s) / 2,\n                                      all_s) - cut_s)\n            ), tf.int32\n        )\n        cut_max = cut_min + CUT_TIME * SR\n        \n        _sample = {\n            'audio_wav': tf.reshape(audio[cut_min:cut_max], [CUT_TIME*SR]),\n            'recording_id': sample['recording_id'],\n            'species_id': spid,\n            'songtype_id': soid,\n            't_min': tmin - tf.cast(cut_min, tf.float32)/tf.cast(SR, tf.float32),\n            'f_min': fmin,\n            't_max': tmax - tf.cast(cut_min, tf.float32)/tf.cast(SR, tf.float32),\n            'f_max': fmax,\n            'is_tp': tp\n        }\n        return _sample\n    \n    samples = tf.map_fn(_cut_audio, labels, dtype=parse_dtype)\n    return samples\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parsed_tfrecs_sample = sample_proto_train_tfrec.map(_parse_fun).unbatch()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = next(iter(parsed_tfrecs_sample))\n\nfor key, val in sample.items():\n    print(key, \":\", val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef cut_audio(sample, istrain=True):\n    # random cutting for train samples\n    if istrain:\n        cut_min = tf.random.uniform([],\n                                    maxval=(CUT_TIME-SAMPLE_TIME) * SR,\n                                    dtype=tf.int32)\n    else:\n        # center cropping for validation data\n        cut_min = (CUT_TIME - SAMPLE_TIME) * SR//2\n    cut_max = cut_min + SAMPLE_TIME * SR\n    cutaudio = tf.reshape(\n        sample[\"audio_wav\"][cut_min:cut_max], [SAMPLE_TIME * SR]\n    )\n\n    result = {}\n    result.update(sample)\n    result[\"audio_wav\"] = cutaudio\n    result[\"t_min\"] = tf.maximum(0.0, sample[\"t_min\"] - tf.cast(cut_min, tf.float32)/SR)\n    result[\"t_max\"] = tf.maximum(0.0, sample[\"t_max\"] - tf.cast(cut_min, tf.float32)/SR)\n    return result\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef waveToSpec(sample):\n    mel_power = 2\n    stfts = tf.signal.stft(sample[\"audio_wav\"],\n                           frame_length=2048,\n                           frame_step=512,\n                           fft_length=2048)\n    spectograms = tf.abs(stfts) ** mel_power\n\n    # convert into mel scale\n    mel_weight = tf.signal.linear_to_mel_weight_matrix(\n        num_mel_bins=224,  # or can be said as the no of MFCCs, though theoretically, ideal value should be in 30-50 range, let's try with 224 for image size\n        num_spectrogram_bins=stfts.shape[-1],\n        sample_rate=SR,\n        lower_edge_hertz=FMIN,\n        upper_edge_hertz=FMAX\n    )\n    mel_spectrograms = tf.tensordot(\n        spectograms, mel_weight, 1\n    )\n    mel_spectrograms.set_shape(spectograms.shape[:-1].concatenate(mel_weight.shape[-1:]))\n    log_mel_spectograms = tf.math.log(mel_spectrograms + 1e-6)\n\n    results = {\n        \"audio_spec\": tf.transpose(log_mel_spectograms)  # of shape (num_mel_spec_bins, num_frames)\n    }\n    results.update(sample)\n    return results\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef filter_tp(sample):\n    \"\"\"\n\n    :param sample: Processed dictionary from _parse_function\n    :return: boolean, whether belongs to true positive or false positive\n    \"\"\"\n    return sample[\"is_tp\"] == 1\n    pass\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef create_annot(sample):\n    target = tf.one_hot(sample[\"species_id\"],\n                        24,\n                        on_value=sample[\"is_tp\"],\n                        off_value=0)\n\n    return {\n        \"input\": sample[\"audio_spec\"],  # obtained from creating spectograms from audio np arrays\n        \"target\": tf.cast(target, tf.float32)\n    }\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef toImage(logmelSpec):\n    # expand one dimension axis to be treated as image\n    image = tf.expand_dims(logmelSpec, axis=-1)\n    image = tf.image.resize(image, (224, 512))\n    image = tf.image.per_image_standardization(image)\n\n    # no augmentation at this stage\n    image = (image - tf.reduce_mean(image))/(tf.reduce_max(image) * tf.reduce_min(image)) * 255.0\n    image = tf.image.grayscale_to_rgb(image)\n    # image = cfg.model_params[\"preprocess\"](image)\n    return image\n    pass\n\n\n@tf.function\ndef preprocess_img(sample):\n    image = toImage(sample[\"input\"])\n    return image, sample[\"target\"]\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's extract the samples from the tfrecs and create our initial processed dataset\nspec_dataset = parsed_tfrecs_sample.filter(filter_tp).map(cut_audio).map(waveToSpec)\nimg_dataset = spec_dataset.map(create_annot).map(preprocess_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(20, 10))\nfor i, s in enumerate(spec_dataset.take(3)):\n    axs[0, i].imshow(s['audio_spec'])\n    axs[0, i].set_title(s['recording_id'].numpy().decode(\"UTF-8\") + \"->\" + \"s-\" +str(s[\"species_id\"].numpy()))\n    LD.waveplot(s[\"audio_wav\"].numpy(), sr=SR, ax=axs[1, i])\n    LD.specshow(s['audio_spec'].numpy(), x_axis=\"time\", y_axis=\"mel\", sr=SR, fmax=FMAX, fmin=FMIN, ax=axs[2, i], cmap=\"magma\")\n    axs[2, i].add_patch(ptc.Rectangle(xy=(s[\"t_min\"], s[\"f_min\"]), height=s[\"f_max\"]-s[\"f_min\"], width=s[\"t_max\"]-s[\"t_min\"], fill=False))\n    axs[2, i].text(s[\"t_min\"], s[\"f_min\"], \"s-\" +str(s[\"species_id\"].numpy()), horizontalalignment='left', verticalalignment='bottom', fontsize=16)                   \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}