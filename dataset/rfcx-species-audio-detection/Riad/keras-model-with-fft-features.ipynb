{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport os\nfrom scipy.interpolate import interp1d\nimport gc\nimport soundfile as sf\n# Librosa Libraries\nimport librosa\nimport librosa.display\nimport IPython.display as ipd\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import roc_auc_score, label_ranking_average_precision_score\nfrom sklearn.model_selection import GroupKFold\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/iterative/iterative_stratification-0.1.6-py3-none-any.whl')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"trainfiles = glob.glob( '../input/rfcx-species-audio-detection/train/*.flac' )\ntestfiles = glob.glob( '../input/rfcx-species-audio-detection/test/*.flac' )\nlen(trainfiles), len(testfiles), trainfiles[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traint = pd.read_csv( '../input/rfcx-species-audio-detection/train_tp.csv' )\ntraint['t_dif'] = traint['t_max'] - traint['t_min']\ntraint['f_dif'] = traint['f_max'] - traint['f_min']\n\ntrainf = pd.read_csv( '../input/rfcx-species-audio-detection/train_fp.csv' )\ntrainf['t_dif'] = trainf['t_max'] - trainf['t_min']\ntrainf['f_dif'] = trainf['f_max'] - trainf['f_min']\n\ntraint.shape, trainf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traint.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainf.t_min.hist(bins=100)\ntrainf.t_max.hist(bins=100, alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainf.f_min.hist(bins=20)\ntrainf.f_max.hist(bins=20, alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainf.t_dif.hist(bins=100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainf.f_dif.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainf.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traint.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data, samplerate = sf.read(trainfiles[0]) \nprint( data.shape, samplerate )\nlibrosa.display.waveplot(y = data, sr = samplerate, color = \"#B14D\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traint.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Min and Max frequencies are: 93 and 10687"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nTRAIN = []\nTARGET = []\nfor i in tqdm(range(traint.shape[0])):\n\n    fn = traint.recording_id.values[i]\n    tmin = traint.t_min.values[i]\n    tmax = traint.t_max.values[i]\n    fmin = traint.f_min.values[i]\n    fmax = traint.f_max.values[i]\n    #print(tmin,tmax, fmin,fmax )\n\n    data, samplerate = sf.read( '../input/rfcx-species-audio-detection/train/'+fn+'.flac')\n    #print( data.shape, samplerate )\n    var_time = np.arange(0,data.shape[0]) / samplerate\n\n    data = data[ np.where( (var_time>=tmin)&(var_time<=tmax) )[0] ]\n\n    varfft = np.abs( np.fft.fft(data)[:(len(data)//2)] )\n    x = np.linspace(0, len(varfft), num=len(varfft), endpoint=True)\n    f1 = interp1d(x, varfft, kind='cubic')\n    x = np.linspace(0, len(varfft), num=1000, endpoint=True)\n    varfft = f1(x)\n    \n    TRAIN.append( varfft )\n    TARGET.append( traint.species_id.values[i] )\n    \nFT = np.stack(TRAIN)\nTARGET = np.array(TARGET)\nFT.shape, len(TARGET)\n\"\"\"\ndata = np.load('../input/training-testing-data/FT.npz')\nFT = data['a']\ndata = np.load('../input/training-testing-data/TARGET.npz')\nTARGET = data['a']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(TARGET, return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom joblib import Parallel, delayed\n\ndef extract_features( fn ):\n    data, samplerate = sf.read( '../input/rfcx-species-audio-detection/train/'+fn+'.flac')\n\n    varfft = np.abs( np.fft.fft(data)[:(len(data)//2)] )\n    x = np.linspace(0, len(varfft), num=len(varfft), endpoint=True)\n    f1 = interp1d(x, varfft, kind='cubic')\n    x = np.linspace(0, len(varfft), num=1000, endpoint=True)\n    varfft = f1(x)\n    \n    return varfft\n    \nFP = Parallel(n_jobs=4)(delayed(extract_features)(fn) for i in tqdm(trainf.recording_id.values))\nFP = np.stack(FP)\ngc.collect()\nFP.shape\n\"\"\"\ndata = np.load('../input/training-testing-data/FP.npz')\nFP = data['a']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ndef extract_features( fn ):\n    data, samplerate = sf.read(fn)\n\n    varfft = np.abs( np.fft.fft(data)[:(len(data)//2)] )\n    x = np.linspace(0, len(varfft), num=len(varfft), endpoint=True)\n    f1 = interp1d(x, varfft, kind='cubic')\n    x = np.linspace(0, len(varfft), num=1000, endpoint=True)\n    varfft = f1(x)\n    \n    return varfft\n    \nTEST = Parallel(n_jobs=4)(delayed(extract_features)(fn) for fn in tqdm(testfiles))\nTEST = np.stack(TEST)\ngc.collect()\nTEST.shape\n\n\"\"\"\ndata = np.load('../input/training-testing-data/TEST.npz')\nTEST = data['a']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN = np.vstack( (FT, FP) )\nTRAIN.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tt = traint[['recording_id','species_id']].copy()\ntf = trainf[['recording_id','species_id']].copy()\n\ntf['species_id'] = -1\n\nTRAIN_TAB = pd.concat( (tt, tf) )\n\nfor i in range(24):\n    TRAIN_TAB['s'+str(i)] = 0\n    TRAIN_TAB.loc[TRAIN_TAB.species_id==i,'s'+str(i)] = 1\n\nTRAIN_TAB.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_TAB.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nstd = StandardScaler()\nstd.fit( np.vstack((TRAIN,TEST)) )\n\nTRAIN = std.transform(TRAIN)\nTEST  = std.transform(TEST)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras import layers,regularizers,Sequential,backend,callbacks,optimizers,metrics,losses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(num_columns,target_columns):\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(num_columns),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1048, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5), \n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(500, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),        \n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(target_columns, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10),\n                  loss=losses.BinaryCrossentropy(label_smoothing=0.000001),metrics=['AUC']\n                  )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tar_col = []\nfor tgt in range(24):\n    tar_col.append('s'+str(tgt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_TARGETS = TRAIN_TAB[tar_col].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN = np.array(TRAIN,dtype='float64')\nTRAIN_TARGETS = TRAIN_TARGETS[tar_col].values\nTRAIN_TARGETS = np.array(TRAIN_TARGETS,dtype='float64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN.argmax()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame({'recording_id': [f.split('/')[-1].split('.')[0] for f in testfiles] })\nytrain = np.zeros((TRAIN.shape[0],len(tar_col)))\nytest = np.zeros((TEST.shape[0],len(tar_col)))\nN_SPLITS = 5 \nSEED = 2\ngroups = TRAIN_TAB['recording_id'].values\nfor tar in range(24):\n    print(f'---------------------TARGET{tar}------------------------')     \n    for n, (tr, te) in enumerate(GroupKFold(N_SPLITS).split(TRAIN, TRAIN_TAB[tar_col[tar]],groups)):\n                model = create_model(TRAIN.shape[1],1)\n                print(f'FOLD-{n}')\n                checkpoint_path_model = f'-model-repeat:{0}_Fold:{n}.hdf5'\n                reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=0, epsilon=1e-4, mode='min')\n                cb_checkpt = ModelCheckpoint(checkpoint_path_model, monitor = 'val_loss', verbose = 0, save_best_only = True,\n                                     save_weights_only = True, mode = 'min')\n           \n                model.fit(TRAIN[tr,:],\n                      TRAIN_TARGETS[tr,tar],\n                      validation_data=(TRAIN[te,:], TRAIN_TARGETS[te,tar]),\n                      epochs= 30 , batch_size=128,\n                      callbacks=[reduce_lr_loss, cb_checkpt], verbose=0,\n                      )     \n                os.remove(checkpoint_path_model)\n                ytrain[te,tar] += model.predict(TRAIN[te,:])[:,0]\n                ytest[:,tar] +=  model.predict(TEST)[:,0]/(N_SPLITS)\n            \n                FOLD_SCORE = roc_auc_score(TRAIN_TARGETS[te,tar],ytrain[te,tar])\n                print(f'FOLD-{n}-AUC score = {FOLD_SCORE}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUC_scores = []\nfor n  in range(len(tar_col)): \n    AUC_scores.append(roc_auc_score(TRAIN_TARGETS[:,n], ytrain[:,n]))\n    print( f'Target{n} AUC', roc_auc_score(TRAIN_TARGETS[:,n], ytrain[:,n]) )\nprint(f'mean of AUC scores = {np.mean(AUC_scores)}') \nprint(f'label_ranking_average_precision_score = {label_ranking_average_precision_score(TRAIN_TARGETS,ytrain)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_TAB[tar_col] =  ytrain\nsub[tar_col] = ytest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}