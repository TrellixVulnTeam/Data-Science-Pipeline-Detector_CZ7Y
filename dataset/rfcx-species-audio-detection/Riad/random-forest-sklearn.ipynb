{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\nimport time\nfrom scipy.interpolate import interp1d\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom joblib import Parallel, delayed\nfrom tqdm.notebook import tqdm\nfrom scipy.stats import rankdata\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score, label_ranking_average_precision_score\n\nimport soundfile as sf\n# Librosa Libraries\nimport librosa\nimport librosa.display\nimport IPython.display as ipd\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"trainfiles = glob.glob( '../input/rfcx-species-audio-detection/train/*.flac' )\ntestfiles = glob.glob( '../input/rfcx-species-audio-detection/test/*.flac' )\nlen(trainfiles), len(testfiles), trainfiles[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traint = pd.read_csv( '../input/rfcx-species-audio-detection/train_tp.csv' )\ntrainf = pd.read_csv( '../input/rfcx-species-audio-detection/train_fp.csv' )\ntraint.shape, trainf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traint.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_fft(fn):\n    data, samplerate = sf.read(fn)\n    data = np.array(data)\n\n    varfft = np.abs( np.fft.fft(data)[:(len(data)//2)] )\n    return np.array( varfft.reshape( (1000,1440) ).mean(axis=1) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FT = []\nfor fn in tqdm(traint.recording_id.values):\n    FT.append( extract_fft( '../input/rfcx-species-audio-detection/train/'+fn+'.flac' ) )\nFT = np.stack(FT)\ngc.collect()\n\nFT.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This loop runs in 7min using cupy(GPU) and 40min on numpy(CPU). ~7x Faster in GPU\n\nFF = []\nfor fn in tqdm(trainf.recording_id.values):\n    FF.append( extract_fft( '../input/rfcx-species-audio-detection/train/'+fn+'.flac' ) )\nFF = np.stack(FF)\ngc.collect()\n\nFF.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Combine True Positives and False Positives\n\nTRAIN = np.vstack( (FT, FF) )\n\ndel FT, FF\ngc.collect()\nTRAIN.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST = []\nfor fn in tqdm(testfiles):\n    TEST.append( extract_fft(fn) )\nTEST = np.stack(TEST)\ngc.collect()\n\nTEST.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tt = traint[['recording_id','species_id']].copy()\ntf = trainf[['recording_id','species_id']].copy()\ntf['species_id'] = -1\n\nTRAIN_TAB = pd.concat( (tt, tf) )\n\nfor i in range(24):\n    TRAIN_TAB['s'+str(i)] = 0\n    TRAIN_TAB.loc[TRAIN_TAB.species_id==i,'s'+str(i)] = 1\n\nTRAIN_TAB.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nstd = StandardScaler()\nstd.fit( np.vstack((TRAIN,TEST)) )\n\nTRAIN = std.transform(TRAIN)\nTEST  = std.transform(TEST)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_loss_metric(y_true, y_pred):\n    metrics = []\n    pred_col = y_pred.columns\n    true_col = y_true.columns\n    for _target in range(24):\n        metrics.append(log_loss(y_true.loc[:,true_col[_target]], y_pred.loc[:,pred_col[_target]].astype(float), labels = [0,1]))\n    return np.mean(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier as RF\nsub = pd.DataFrame({'recording_id': [f.split('/')[-1].split('.')[0] for f in testfiles] })\ngkf = GroupKFold(5)\nAUC_scores = []\n\ngroups = TRAIN_TAB['recording_id'].values\nfor tgt in range(24):\n    target = TRAIN_TAB['s'+str(tgt)].values\n\n    ytrain = np.zeros(TRAIN.shape[0])\n    ytest = np.zeros(TEST.shape[0])\n    for ind_train, ind_valid in gkf.split( TRAIN, target, groups ):\n        model = RF(max_depth = 25,n_estimators = 1200,class_weight =\"balanced\",random_state=0,n_jobs=-1)\n        model.fit( TRAIN[ind_train], target[ind_train] )\n        \n        ytrain[ind_valid] = model.predict_proba(TRAIN[ind_valid])[:,1]\n        ytest += model.predict_proba(TEST)[:,1] / 5.\n    AUC_scores.append(roc_auc_score(target, ytrain))\n\n    print( 'Target AUC', tgt, roc_auc_score(target, ytrain) )\n    \n    TRAIN_TAB['y'+str(tgt)] = ytrain\n    sub['s'+str(tgt)] = ytest\nprint(f'AUC mean:{np.mean(AUC_scores)}') \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub1 = sub.copy()\noof1 = TRAIN_TAB[TRAIN_TAB.columns[26:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perd = TRAIN_TAB[TRAIN_TAB.columns[26:]]\ntrue = TRAIN_TAB[TRAIN_TAB.columns[2:26]]\nprint(f'log_loss:{log_loss_metric(true, perd)}')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsub = pd.DataFrame({'recording_id': [f.split('/')[-1].split('.')[0] for f in testfiles] })\ngkf = GroupKFold(5)\nAUC_scores = []\n\ngroups = TRAIN_TAB['recording_id'].values\nfor tgt in range(24):\n    target = TRAIN_TAB['s'+str(tgt)].values\n\n    ytrain = np.zeros(TRAIN.shape[0])\n    ytest = np.zeros(TEST.shape[0])\n    for ind_train, ind_valid in gkf.split( TRAIN, target, groups ):\n        model = SVC(C=1.0, class_weight='balanced', probability=True, kernel='rbf', gamma='auto')\n        model.fit( TRAIN[ind_train], target[ind_train] )\n        \n        ytrain[ind_valid] = model.predict_proba(TRAIN[ind_valid])[:,1]\n        ytest += model.predict_proba(TEST)[:,1] / 5.\n    AUC_scores.append(roc_auc_score(target, ytrain))\n\n    print( 'Target AUC', tgt, roc_auc_score(target, ytrain) )\n    \n    TRAIN_TAB['y'+str(tgt)] = ytrain\n    sub['s'+str(tgt)] = ytest\nprint(np.mean(AUC_scores))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub2 = sub.copy()\noof2 =  TRAIN_TAB[TRAIN_TAB.columns[26:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perd = TRAIN_TAB[TRAIN_TAB.columns[26:]]\ntrue = TRAIN_TAB[TRAIN_TAB.columns[2:26]]\nprint(f'log_loss:{log_loss_metric(true, perd)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Blending Weights Optimisation\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sys.path.append('../input/autograd')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof1 = np.array(oof1.values,dtype='float64')\noof2 = np.array(oof2.values,dtype='float64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nimport pandas as pd\nfrom time import time\nimport tensorflow as tf\nfrom autograd import grad\nimport autograd.numpy as np\nfrom scipy.optimize import minimize, fsolve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Numpy Log Loss\ndef log_loss_numpy(y_pred):\n    loss = 0\n    y_pred_clip = np.clip(y_pred, 1e-16, 1 - 1e-16)\n    for i in range(y_pred.shape[1]):\n        loss += - np.mean(y_true[:, i] * np.log(y_pred_clip[:, i]) + (1 - y_true[:, i]) * np.log(1 - y_pred_clip[:, i]))\n    return loss / y_pred.shape[1]\n\ndef func_numpy_metric(weights):\n    coef = 1e-6\n    oof_blend = weights[0] * oof1 + weights[1] * oof2 + weights[2] * oof3 \n    score = log_loss_numpy(oof_blend)\n    penalty = coef * (np.sum(weights) - 1) ** 2\n    return score + penalty","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = true.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Model 1 OOF (Numpy Log Loss):', log_loss_numpy(oof1))\nprint('Model 2 OOF (Numpy Log Loss):', log_loss_numpy(oof2))\nprint('-' * 50)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Lagrange_func(params):\n    w1, w2,_lambda = params\n    oof_blend = w1 * oof1 + w2 * oof2 \n    return log_loss_numpy(oof_blend) - _lambda * (w1 + w2 - 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grad_L = grad(Lagrange_func)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Lagrange_obj(params):\n    w1, w2,_lambda = params\n    dLdw1, dLdw2, dLdlam = grad_L(params)\n    return [dLdw1, dLdw2, w1 + w2 - 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time()\nw1, w2, _lambda = fsolve(Lagrange_obj, [0.5,0.5,0.1])\nprint(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] Optimised Weights:', [w1, w2])\noof_b = w1 * oof1 + w2 * oof2\nprint('Optimised Blend OOF:', log_loss_numpy(oof_b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Check Condition (1a):', w1 + w2 )\nif w1 + w2 - 1 <= 1e-10:\n    print('Great! The sum of all weights equals to 1!')\nelse:\n    print('Manual adjustion is needed to modify the weights.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sub1.copy()\ntar_col = sub.columns\ntar_col = tar_col[1:]\nsub.loc[:,tar_cols]= sub1.loc[:,tar_cols]*w1+sub2.loc[:,tar_cols]*w2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}