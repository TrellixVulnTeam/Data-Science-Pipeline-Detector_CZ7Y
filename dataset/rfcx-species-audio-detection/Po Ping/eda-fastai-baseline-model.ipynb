{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Understanding the dataset\nLet us print the dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nprint(\"There are following directories and files in this dataset\")\nprint(*list(os.listdir(\"../input/rfcx-species-audio-detection\")),sep = \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Choosing the data for model training\n**We will now count the number of audio files availabe to us in the following directories:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\ntrain_audio_tfrec_format = glob.glob('../input/rfcx-species-audio-detection/tfrecords/train/*.tfrec')\nlen(train_audio_tfrec_format)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\ntrain_audio_flac_format = glob.glob('../input/rfcx-species-audio-detection/train/*.flac')\nlen(train_audio_flac_format)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing necessary libraries\n\nNote: We are using Fastai version 2 which is the latest version \n(Not previous version of Fastai- which is version 1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install --upgrade light-the-torch\n!ltt install torch torchvision torchaudio\n!pip install --upgrade git+https://github.com/fastaudio/fastaudio.git\nimport pkg_resources\ndef placeholder(x): raise pkg_resources.DistributionNotFound\npkg_resources.get_distribution = placeholder\n\nfrom fastaudio.all import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import fastai\nfrom fastai.vision.all import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining the variables and assigning the paths for this notebook\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = Path('../input/rfcx-species-audio-detection')\naudio_path = Path('../input/rfcx-species-audio-detection/train')\ntraining_data_file = Path('../input/rfcx-species-audio-detection/train_tp.csv')\nsample_submission_file = Path('../input/rfcx-species-audio-detection/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(training_data_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Size of Training data \\n\", train_df.shape)\nprint(\"----------------------------------------------------------\")\nprint(\"\\nFirst few samples of data are \\n\",train_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.species_id.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us print the number of data samples with each output category"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['species_id'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom plotly.offline import iplot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dff = pd.read_csv(training_data_file)\ntrain_dff['species_id'].value_counts().sort_values().iplot(title = 'Number of samples present in the training data for each type of species',kind='bar',color='red',yTitle='Number of samples',xTitle = 'species_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Selecting a subset of data for training purpose\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Size of Training data before removing the duplicate recording_id  \\n\", train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop_duplicates(subset =\"recording_id\", keep = False, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Size of Training data after removing the duplicate recording_id  \\n\", train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training data before removing any column  \\n\",)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop(['songtype_id','t_min', 'f_min','t_max','f_max'], axis=1)\ntrain_df['species_id'] = train_df['species_id'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training data after removing the above specified columns  \\n\",)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_files = get_audio_files(audio_path)\naudio = AudioTensor.create(train_files[0])\naudio.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}