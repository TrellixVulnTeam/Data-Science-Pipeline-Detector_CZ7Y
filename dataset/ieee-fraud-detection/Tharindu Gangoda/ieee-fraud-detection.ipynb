{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Installing LightGBM GPU build"},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -r /opt/conda/lib/python3.6/site-packages/lightgbm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!git clone --recursive https://github.com/Microsoft/LightGBM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!apt-get install -y -qq libboost-all-dev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..\nmake -j$(nproc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cd LightGBM/python-package/;python3 setup.py install --precompile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd\n!rm -r LightGBM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# plotting\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n\nimport gc # garbage collection\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn-whitegrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity = pd.read_csv('../input/ieee-fraud-detection/train_identity.csv')\nprint('The training dataset for identity information has {0} columns and {1} rows'.format(*train_identity.shape))\n\ntrain_transaction = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv')\nprint('The training dataset for transactions has {0} columns and {1} rows'.format(*train_transaction.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_identity = pd.read_csv('../input/ieee-fraud-detection/test_identity.csv')\nprint('The test dataset for identity information has {0} columns and {1} rows'.format(*test_identity.shape))\n\ntest_transaction = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv')\nprint('The test dataset for identity information has {0} columns and {1} rows'.format(*test_transaction.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_idNA = train_identity.isnull().sum()\nprint('='*30+'Identity'+'='*30)\nprint('----------Columns with no missing values----------')\nprint(*train_idNA[train_idNA == 0].index, sep='\\t')\nprint('----------Columns with missing values-------------')\nprint(*train_idNA[train_idNA > 0].index, sep='\\t')\n\ntrain_txnNA = train_transaction.isnull().sum()\nprint('\\n========================transactions====================')\nprint('----------Columns with no missing values----------')\nprint(*train_txnNA[train_txnNA == 0].index, sep='\\t')\nprint('----------Columns with missing values-------------')\nprint(*train_txnNA[train_txnNA > 0].index, sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_idNA = test_identity.isnull().sum()\nprint('========================Identity========================')\nprint('----------Columns with no missing values----------')\nprint(*test_idNA[test_idNA == 0].index, sep='\\t')\nprint('----------Columns with missing values-------------')\nprint(*test_idNA[test_idNA > 0].index, sep='\\t')\n\ntest_txnNA = test_transaction.isnull().sum()\nprint('\\n========================transactions====================')\nprint('----------Columns with no missing values----------')\nprint(*test_txnNA[test_txnNA == 0].index, sep='\\t')\nprint('----------Columns with missing values-------------')\nprint(*test_txnNA[test_txnNA > 0].index, sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_cols = [f'C{i}' for i in range(1,15)] # counts (Numeric)\nd_cols = [f'D{i}' for i in range(1,16)] # timedeltas (Numeric)\nm_cols = [f'M{i}' for i in range(1,10)] # matches (Boolean/ Categorical)\nv_cols = [f'V{i}' for i in range(1,340)] # Numeric\nid_cols = [f'id_{i:02d}' for i in range(1,39)] # id_12 to id_38 are catergorical columns\nid_colC = id_cols[11:]\n\ncard_cols = [f'card{i}' for i in range(1,7)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txn_cat_cols = ['ProductCD', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain'] + card_cols + m_cols\nid_cat_cols = ['DeviceInfo', 'DeviceType'] + id_colC\ncat_cols = txn_cat_cols + id_cat_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the data description tells us that, we can join the two datasets by the `TransactionID` column. However, not all transactions will have corresponding identity information.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train =  train_transaction.merge(right= train_identity, on = 'TransactionID', how = 'left')\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test =  test_transaction.merge(right= test_identity, on = 'TransactionID', how = 'left')\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_identity, train_transaction, test_transaction, test_identity\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check to see if we have the same levels (categories) in the training and testing set. If we have new categories is the test set, the model may not be able to accuratly predict on those values."},{"metadata":{},"cell_type":"markdown","source":"We have values that apprear in the test set that is not available in the training set. To handle this we'll set the valid categories as the ones apprearing in the training set. This will force the catagories apprearing in the test set to be `nan`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# take the number of decimals on the Transaction Amount field. More than two decimals may indicate transactions made overseas.\ntrain['TransactionAmt_decimal_lenght'] = train['TransactionAmt'].astype(str).str.split('.', expand=True)[1].str.len()\ntest['TransactionAmt_decimal_lenght'] = test['TransactionAmt'].astype(str).str.split('.', expand=True)[1].str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Txn_nulls'] = train.isna().sum(axis = 1)\ntest['Txn_nulls'] = test.isna().sum(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['Txn_nulls'][train['isFraud'] == 0], label = 'Not Fraud')\nsns.distplot(train['Txn_nulls'][train['isFraud'] == 1], label = 'Is Fraud')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[cat_cols] = train[cat_cols].astype('category')\ntest[cat_cols] = test[cat_cols].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Memory reduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage2(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype.name\n        \n        if col_type not in ('object', 'category'):\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n            try:\n                df[col] = df[col].cat.add_categories('UNK').fillna('UNK')\n            except ValueError:\n                pass    \n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_dataset_diff(trn, tst, cat_cols):\n    for col in cat_cols:\n        train_set = set(trn[col].cat.categories)\n        test_set = set(tst[col].cat.categories)\n        new_values = len(test_set - train_set)\n\n        if new_values == 0:\n            pass\n        else:\n            print ('**There are {} new values in the test set for the `{}` column'.format(new_values, col))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage2(train)\ntest = reduce_mem_usage2(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_cols = train.select_dtypes('number').columns\ncategorical_cols = train.select_dtypes(exclude = 'number').columns\n\nprint ('There are {} numerical columns and {} categorical columns'.format(len(numerical_cols), len(categorical_cols)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_dataset_diff(train, test, categorical_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_valid_cats(trn, test, cat_cols):\n    for col in cat_cols:\n        cat_values = trn[col].cat.categories\n        test[col] = test[col].cat.set_categories(cat_values)\n        \n        if test[col].isna().sum() > 0:\n            print ('Resetting categorical levels created {} null values'.format(test[col].isna().sum()))\n        try:\n            test[col] = test[col].cat.add_categories('UNK').fillna('UNK')\n        except ValueError:\n            test[col] = test[col].fillna('UNK')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_valid_cats(train, test, categorical_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# identify records having identity information\ntrain['no_identity'] = train['id_01'].isna()*1\ntest['no_identity'] = test['id_01'].isna()*1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"overall_fraud_rate = train['isFraud'].value_counts(normalize = True)[1]\noverall_fraud_rate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Card Information"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[card_cols].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols= 3, nrows= 2, figsize = (20, 10))\nax = ax.ravel()\n\nfor i, col in enumerate(card_cols):\n    if train[col].dtype.name == 'category':\n        props = train.groupby(col, observed = True)['isFraud'].value_counts(dropna = False, normalize=True).unstack()\n        props = props.sort_values(by=1, ascending = False).head(10)\n        p = props.plot(kind='barh', stacked='True', ax = ax[i], legend = False)\n        ax[i].vlines(1 - overall_fraud_rate, ymin= ax[i].get_ylim()[0], ymax= ax[i].get_ylim()[1], linestyle = ':' )\n        ax[i].set_ylabel(col)\n    else:\n        sns.distplot(train[col][train['isFraud'] == 0] ,ax = ax[i], label = 'Not Fraud', hist = False,)\n        sns.distplot(train[col][train['isFraud'] == 1] ,ax = ax[i], label = 'Is Fraud', hist = False)\n        ax[i].set_xlabel(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in card_cols:\n    card_count = train[col].value_counts().to_dict()\n    train[col+'_count'] = train[col].map(card_count)\n    test[col+'_count'] = test[col].map(card_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Transaction Date**  \nThe `TransactionDT` gives the timedelta from a reference date. We can calculate the days past the reference date by dividing the values by 86,400 ($60\\times60\\times24$). "},{"metadata":{"trusted":true},"cell_type":"code","source":"(train['TransactionDT'] / (3600*24)).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 5))\nvals = plt.hist(train['TransactionDT'] / (3600*24), bins=182*24)\nplt.xlim(1, 7)\nplt.xlabel('Days')\nplt.ylabel('Number of transactions')\nplt.ylim(0,500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we look at a seven day period we can see that there is a cyclical movement within each day, but there isn't much difference between days. We would expect that the transaction volumns are lowest near midnight. We can calculate the hour of each day when the transaction occure by dividing the timedelta by 24 and taking the remainder and setting an offset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"vals = plt.hist(train['TransactionDT'] / (3600*24), bins=182*24)\nplt.xlim(0, 3)\nplt.xlabel('Days')\nplt.ylabel('Number of transactions')\nplt.ylim(0,500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From observing the histogram, we can estimate that 9/24 will be a good off-set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TransactionDays'] = train['TransactionDT'] / (60*60*24) - 9/24\ntest['TransactionDays'] = test['TransactionDT'] / (60*60*24) - 9/24","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 5))\nvals = plt.hist(train['TransactionDays'], bins=182*24)\nplt.xlim(120, 127)\nplt.xlabel('Days')\nplt.ylabel('Number of transactions')\nplt.ylim(0,500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TransactionHour'] = np.floor((train['TransactionDays'] % 1 )* 24)\ntrain['TransactionDayofWeek'] = np.floor(train['TransactionDays'] % 7)\n\ntest['TransactionHour'] = np.floor((test['TransactionDays'] % 1 )* 24)\ntest['TransactionDayofWeek'] = np.floor(test['TransactionDays'] % 7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TransactionMonth'] = (np.floor(train['TransactionDays']/ 31))\ntest['TransactionMonth'] = (np.floor(test['TransactionDays']/ 31))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weekday_freq = train['TransactionDayofWeek'].value_counts().to_dict()\ntrain[col+'_weekfreq'] = train['TransactionDayofWeek'].map(weekday_freq)\ntest[col+'_weekfreq'] = test['TransactionDayofWeek'].map(weekday_freq)\n    \nhour_freq = train['TransactionHour'].value_counts().to_dict()\ntrain[col+'_weekfreq'] = train['TransactionDayofWeek'].map(hour_freq)\ntest[col+'_weekfreq'] = test['TransactionDayofWeek'].map(hour_freq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fraud_fracDay = train.groupby('TransactionDayofWeek')['isFraud'].mean()\n\nplt.plot(fraud_fracDay)\nplt.ylim(0.03, 0.04)\nplt.hlines(y= overall_fraud_rate, xmin= 0, xmax= 6, linestyles= ':')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The day's 0, 1, 2 and 6 all have fraud rates higher than the average. If we were to guess we could consider day 6 to be friday, 0 as saturday, 1 as sunday and 2 as monday. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fraud_fracHr = train.groupby('TransactionHour')['isFraud'].mean()\n\nplt.plot(fraud_fracHr)\nplt.ylim(0.02, 0.12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again we see a trend that is somewhat expected, on average there is an increase in the fraud rates during the night times, and reduced number of frauds during the day time. "},{"metadata":{},"cell_type":"markdown","source":"### Transaction Amount"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 6))\nplot= sns.violinplot(x = train['isFraud'][train['TransactionAmt'] < 1000], y = train['TransactionAmt'][train['TransactionAmt'] < 1000].astype('float32'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby(col)['TransactionAmt'].transform('mean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in card_cols:\n    if train[col].dtype.name == 'category':\n        transaction_mean = train.groupby(col)['TransactionAmt'].transform('mean')\n        train['TransactionAmt_to_mean_'+col] = train['TransactionAmt'] / transaction_mean\n        test['TransactionAmt_to_mean_'+col] = test['TransactionAmt'] / transaction_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(x = train['isFraud'][train['TransactionAmt'] < 1000], y = train['TransactionAmt_to_mean_card4'][train['TransactionAmt'] < 1000].astype(float))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_ids = train[id_cols].select_dtypes('number').columns\ncategorical_ids = train[id_cols].select_dtypes(exclude = 'number').columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols= 4, nrows= 3, figsize = (20, 10))\nax = ax.ravel()\n\nfor i, col in enumerate(numerical_ids):\n        \n    sns.distplot(train[col][train['isFraud'] == 0] ,ax = ax[i], label = 'Not Fraud', hist = False)\n    sns.distplot(train[col][train['isFraud'] == 1] ,ax = ax[i], label = 'Is Fraud', hist = False)\n    ax[i].set_xlabel(col);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[numerical_ids] = train[numerical_ids].fillna(-999)\ntest[numerical_ids] = test[numerical_ids].fillna(-999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols= 5, nrows= 6, figsize = (20, 20))\nax = ax.ravel()\n##scaler = StandardScaler()\n\nfor i, col in enumerate(categorical_ids):\n    ## if len(train[col].cat.categories) > 10:\n        ## print ('column `{}` has more than ten unique levels'.format(col))\n    props = train.groupby(col, observed = True)['isFraud'].value_counts(normalize=True).unstack()\n    props = props.sort_values(by=1, ascending = False).head(10)\n    p = props.plot(kind='barh', stacked='True', ax = ax[i], legend = False)\n    ax[i].vlines(1 - overall_fraud_rate, ymin= ax[i].get_ylim()[0], ymax= ax[i].get_ylim()[1], linestyle = ':' )\n    ax[i].set_ylabel(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols= 5, nrows= 3, figsize = (20, 10))\nax = ax.ravel()\n\nfor i, col in enumerate(c_cols):\n\n    sns.distplot(train[col][train['isFraud'] == 0] ,ax = ax[i], label = 'Not Fraud', hist = False)\n    sns.distplot(train[col][train['isFraud'] == 1] ,ax = ax[i], label = 'Is Fraud', hist = False)\n    ax[i].set_xlabel(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[c_cols] = train[c_cols].fillna(-999)\ntest[c_cols] = test[c_cols].fillna(-999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols= 5, nrows= 3, figsize = (20, 10))\nax = ax.ravel()\n\nfor i, col in enumerate(d_cols):\n    \n    sns.distplot(train[col][train['isFraud'] == 0] ,ax = ax[i], label = 'Not Fraud', hist = False)\n    sns.distplot(train[col][train['isFraud'] == 1] ,ax = ax[i], label = 'Is Fraud', hist = False)\n    ax[i].set_xlabel(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[d_cols]  = train[d_cols].fillna(-999)\ntest[d_cols]  = test[d_cols].fillna(-999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols= 3, nrows= 3, figsize = (20, 10))\nax = ax.ravel()\n##scaler = StandardScaler()\n\nfor i, col in enumerate(m_cols):\n    props = train.groupby(col)['isFraud'].value_counts(normalize=True).unstack()\n    props = props.sort_values(by=1, ascending = False)\n    p = props.plot(kind='barh', stacked='True', ax = ax[i], legend = False)\n    ax[i].vlines(1 - overall_fraud_rate, ymin= ax[i].get_ylim()[0], ymax= ax[i].get_ylim()[1], linestyle = ':' )\n    ax[i].set_ylabel(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in m_cols:\n    count = train[col].value_counts().to_dict()\n    train[col+'_count'] = train[col].map(count)\n    test[col+'_count'] = test[col].map(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (15, 8))\nprops = train.groupby('id_30')['isFraud'].value_counts(dropna = False, normalize=True).unstack()\nprops = props.sort_values(by=1, ascending = False).head(25)\np = props.plot(kind='bar', stacked='True', legend = False, ax = ax )\n## plt.vlines(1 - overall_fraud_rate, linestyle = ':' )\nplt.ylabel('id_30')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (15, 8))\nprops = train.groupby('id_31')['isFraud'].value_counts(dropna = False, normalize=True).unstack()\nprops = props.sort_values(by=1, ascending = False).head(25)\np = props.plot(kind='bar', stacked='True', legend = False, ax = ax )\n## plt.vlines(1 - overall_fraud_rate, linestyle = ':' )\nplt.ylabel('id_31')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (15, 8))\nprops = train.groupby('id_33')['isFraud'].value_counts(dropna = False, normalize=True).unstack()\nprops = props.sort_values(by=1, ascending = False).head(25)\np = props.plot(kind='bar', stacked='True', legend = False, ax = ax)\n## plt.vlines(1 - overall_fraud_rate, linestyle = ':' )\nplt.ylabel('id_33')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (15, 8))\nprops = train.groupby('DeviceInfo')['isFraud'].value_counts(dropna = False, normalize=True).unstack()\nprops = props.sort_values(by=1, ascending = False).head(50)\np = props.plot(kind='bar', stacked='True', legend = False, ax = ax )\n## plt.vlines(1 - overall_fraud_rate, linestyle = ':' )\nplt.ylabel('DeviceInfo')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (15, 8))\nprops = train.groupby('DeviceType')['isFraud'].value_counts(dropna = False, normalize=True).unstack()\nprops = props.sort_values(by=1, ascending = False)\np = props.plot(kind='bar', stacked='True', legend = False, ax = ax )\n## plt.vlines(1 - overall_fraud_rate, linestyle = ':' )\nplt.ylabel('DeviceInfo')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## For the purchaser email take the top level domain\ntrain['topDomain_P_emaildomain'] = train['P_emaildomain'].str.split('.', expand=True)[0].astype('category')\ntest['topDomain_P_emaildomain'] = test['P_emaildomain'].str.split('.', expand=True)[0].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_categories = train['topDomain_P_emaildomain'].cat.categories\ntest['topDomain_P_emaildomain'] = test['topDomain_P_emaildomain'].cat.set_categories(train_categories)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"P_email_freq = train['topDomain_P_emaildomain'].value_counts().to_dict()\n\ntrain['P_topDomain_freq'] = train['topDomain_P_emaildomain'].map(P_email_freq)\ntest['P_topDomain_freq'] = test['topDomain_P_emaildomain'].map(P_email_freq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (15, 8))\nprops = train.groupby('topDomain_P_emaildomain')['isFraud'].value_counts(dropna = False, normalize=True).unstack()\nprops = props.sort_values(by=1, ascending = False).head(50)\np = props.plot(kind='bar', stacked='True', legend = False, ax = ax )\n## plt.vlines(1 - overall_fraud_rate, linestyle = ':' )\nplt.ylabel('Top Level Domain for Purchaser email')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['topDomain_R_emaildomain'] = train['R_emaildomain'].str.split('.', expand=True)[0].astype('category')\ntest['topDomain_R_emaildomain'] = test['R_emaildomain'].str.split('.', expand=True)[0].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_categories = train['topDomain_R_emaildomain'].cat.categories\ntest['topDomain_R_emaildomain'] = test['topDomain_R_emaildomain'].cat.set_categories(train_categories)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"R_email_freq = train['topDomain_R_emaildomain'].value_counts().to_dict()\n\ntrain['R_topDomain_freq'] = train['topDomain_R_emaildomain'].map(R_email_freq)\ntest['R_topDomain_freq'] = test['topDomain_R_emaildomain'].map(R_email_freq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (15, 8))\nprops = train.groupby('topDomain_R_emaildomain')['isFraud'].value_counts(dropna = False, normalize=True).unstack()\nprops = props.sort_values(by=1, ascending = False).head(50)\np = props.plot(kind='bar', stacked='True', legend = False, ax = ax )\n## plt.vlines(1 - overall_fraud_rate, linestyle = ':' )\nplt.ylabel('Top Level Domain for recipient email')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ('addr1', 'addr2'):\n    count = train[col].value_counts().to_dict()\n    train[col+'_freq']  = train[col].map(count)\n    test[col+'_freq']  = test[col].map(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['dist1', 'dist2']].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['dist1'][train['isFraud'] == 0], label = 'Not Fraud', hist = False)\nsns.distplot(train['dist1'][train['isFraud'] == 1], label = 'Is Fraud', hist = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['dist2'][train['isFraud'] == 0], label = 'Not Fraud', hist = False)\nsns.distplot(train['dist2'][train['isFraud'] == 1], label = 'Is Fraud', hist = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['dist1'].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['missing_dist1'] = train['dist1'].isna()*1\ntest['missing_dist1'] = test['dist1'].isna()*1\n\ntrain['dist1'].fillna(-999, inplace = True)\ntest['dist1'].fillna(-999, inplace = True)\n\ntrain['missing_dist2'] = train['dist2'].isna()*1\ntest['missing_dist2'] = test['dist2'].isna()*1\n\ntrain['dist2'].fillna(-999, inplace = True)\ntest['dist2'].fillna(-999, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[v_cols] = train[v_cols].fillna(-999)\ntest[v_cols] = test[v_cols].fillna(-999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group_mean = train.groupby('card1')['isFraud'].transform('mean')\ntrain['V201_card1_mean'] = train['V201']*group_mean\ntest['V201_card1_mean'] = test['V201']*group_mean\n\ngroup_dev = train.groupby('card1')['isFraud'].transform('std')\ntrain['V201_card1_std'] = train['V201']*group_dev\ntest['V201_card1_std'] = test['V201']*group_dev\n\ngroup_mean = train.groupby('card1')['isFraud'].transform('mean')\ntrain['C1_card1_mean'] = train['C1']*group_mean\ntest['C1_card1_mean'] = test['C1']*group_mean\n\ngroup_mean = train.groupby('card1')['isFraud'].transform('mean')\ntrain['C13_card1_mean'] = train['C13']*group_mean\ntest['C13_card1_mean'] = test['C13']*group_mean\n\ntrain['card1_addr1'] = train['card1'].astype('str')+ '_' + train['addr1'].astype('str')\ntest['card1_addr1'] = test['card1'].astype('str')+ '_' + test['addr1'].astype('str')\n\ntrain['card2_addr1'] = train['card2'].astype('str')+ '_' + train['addr1'].astype('str')\ntest['card2_addr1'] = test['card2'].astype('str')+ '_' + test['addr1'].astype('str')\n\ntrain['uid'] = train['card1'].astype(str)+'_'+train['card2'].astype(str)\ntest['uid'] = test['card1'].astype(str)+'_'+test['card2'].astype(str)\n\ngroup_mean = train.groupby('card1')['isFraud'].transform('mean')\ntrain['dist1_card1_mean'] = train['dist1']*group_mean\ntest['dist1_card1_mean'] = test['dist1']*group_mean\n\ntrain['uid3'] = train['addr1'].astype(str)+'_'+train['addr2'].astype(str)\ntest['uid3'] = test['addr1'].astype(str)+'_'+test['addr2'].astype(str)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['missing_rich_feat'] = train[v_cols].isna().sum(axis = 1)\ntest['missing_rich_feat'] = test[v_cols].isna().sum(axis = 1)\n\nsns.distplot(train['missing_rich_feat'][train['isFraud'] == 0], label = 'Not Fraud', hist = False)\nsns.distplot(train['missing_rich_feat'][train['isFraud'] == 1], label = 'Is Fraud', hist = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kw_cbar = {'vmax':1, 'vmin':-1, 'cmap': 'RdYlGn'}\ncorr = train[['isFraud'] +id_cols[:11]].corr('spearman')\nsns.clustermap(corr, **kw_cbar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train[['isFraud'] +d_cols].corr('spearman')\nsns.clustermap(corr, **kw_cbar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train[['isFraud'] +c_cols].corr('spearman')\nsns.clustermap(corr, **kw_cbar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()[train.isna().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isna().sum()[test.isna().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding Categorical Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.sort_values(by = 'TransactionDT')\ntest = test.sort_values(by = 'TransactionDT')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [col for col in train.columns if col != 'isFraud']\nX_train = train[features].copy()\nX_test = test[features].copy()\n\ny_train = train['isFraud']\n\ndel train, test\ngc.collect()\n\nX_train_ids = X_train[['TransactionID']].copy()\nX_test_ids = X_test[['TransactionID']].copy()\n\nX_train.drop(columns = 'TransactionID', inplace = True)\nX_test.drop(columns = 'TransactionID', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = X_train.select_dtypes(exclude = 'number').columns\ncategorical_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from category_encoders import TargetEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_encoder = TargetEncoder(cols= categorical_cols.tolist(), smoothing = 10, return_df= True)\nX_train = target_encoder.fit_transform(X = X_train, y = y_train)\nX_test = target_encoder.transform(X = X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = reduce_mem_usage2(X_train)\nX_test = reduce_mem_usage2(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.isna().sum()[X_train.isna().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import plot_importance\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nfrom hyperopt import space_eval\n\nfrom scipy.stats import uniform\nfrom time import time\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n 'silent': 1,\n 'colsample_bytree': 0.8,\n 'subsample': 0.8,\n 'n_estimators': 1000,\n 'learning_rate': 0.05,\n 'objective': 'binary:logistic',\n 'max_depth': 8,\n 'min_child_weight': 1,\n 'eval_metric': 'auc',\n 'tree_method': 'gpu_hist',\n 'importance_type': 'weight'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"splits = 5\nfolds = KFold(n_splits=splits)\n\naucs = list()\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = X_train.columns\n\ntraining_start_time = time()\n\nfor fold_n, (trn_idx, val_idx) in enumerate(folds.split(X_train)):\n    training_start_time = time()\n    clf = xgb.XGBClassifier(**params, verbosity = 0)\n    \n    X_trn, X_val = X_train.iloc[trn_idx], X_train.iloc[val_idx]\n    y_trn, y_val = y_train.iloc[trn_idx], y_train.iloc[val_idx]\n    clf.fit(X_trn,y_trn, early_stopping_rounds = 100, \n            eval_set = [(X_trn,y_trn), (X_val, y_val)], \n            eval_metric = 'auc', \n            verbose =  100)\n    del X_trn, y_trn\n    \n    val=clf.predict_proba(X_val)[:,1]\n    \n    feature_importances['fold_{}'.format(fold_n + 1)] = clf.feature_importances_\n    \n    del clf, X_val\n    print('ROC accuracy: {}'.format(roc_auc_score(y_val, val)))\n    training_end_time = time()\n    print ('CV fold completed in {}s'.format(training_end_time - training_start_time))\n    del val,y_val\n\n    gc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances['average'] = feature_importances.mean(axis=1)\n\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature')\nplt.title('50 TOP feature importance over cv folds average');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(**params, verbosity = 0)\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.plot_importance(clf, max_num_features = 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = clf.predict_proba(X_test)[:,1]\nX_test_ids['isFraud'] = prediction\nX_test_ids.to_csv('Submission_Xgb_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cv_lightgbm(params, X_train, y_train, splits=5):\n    folds = TimeSeriesSplit(n_splits=splits)\n    auc_scores = np.zeros(5)\n    \n    feature_importances = pd.DataFrame()\n    feature_importances['feature'] = X_train.columns\n    \n    print ('Training model using hyperparameters', params)\n    print ('\\n\\n')\n    \n    ## Do cross validation\n    for fold_n, (trn_idx, val_idx) in enumerate(folds.split(X_train)): \n        cv_fold_start_time = time()\n        print ('** Training fold {}'.format(fold_n+1))\n        \n        X_trn, X_val = X_train.iloc[trn_idx], X_train.iloc[val_idx]\n        y_trn, y_val = y_train.iloc[trn_idx], y_train.iloc[val_idx]\n        train = lgb.Dataset(X_trn, y_trn)\n        valid = lgb.Dataset(X_val, y_val)\n        eval_set  = [train, valid]\n        \n        del X_trn, y_trn\n        \n        clf = lgb.train(params,\n                        train_set = train,\n                        valid_sets = eval_set,\n                        num_boost_round = 1000,\n                        verbose_eval = 100,\n                        early_stopping_rounds = 100)\n        \n        del train, valid, eval_set\n        gc.collect()\n        \n        pred = clf.predict(X_val)\n        auc = roc_auc_score(y_val, pred)      \n        auc_scores[fold_n] = auc\n        print (\"Score for fold {}: {}\".format(fold_n+1, auc))  \n        \n        feature_importances['fold_{}'.format(fold_n + 1)] = clf.feature_importance()\n        \n        del pred, X_val, auc, clf \n        gc.collect()\n        \n        cv_fold_end_time = time()\n        print ('fold completed in {}s'.format(cv_fold_end_time - cv_fold_start_time))\n        \n    print (\"**Average AUC across all folds: {}\".format(auc_scores.mean()))\n    \n    return auc_scores, feature_importances","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using Hyperopt to find best parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(space):\n    print ('='*30 + 'New Run' + '='*30)\n    \n    params = {'max_depth':  space['max_depth'], \n              'num_leaves': space['num_leaves'],\n              'subsample': space['subsample'],\n              'colsample_bytree': space['colsample_bytree'],\n              'learning_rate': space['learning_rate'],\n              'min_child_samples': space['min_child_samples'],\n              'objective' : 'binary',\n              'metric' : 'auc',\n              'save_binary' : True,\n              'learning_rate' : 0.01,\n              'device' : 'gpu',\n              'gpu_platform_id': 0,\n              'gpu_device_id': 0}\n  \n    auc_scores, feature_importance = cv_lightgbm(params, X_train, y_train)\n    \n    return{'loss':1-auc_scores.mean(), 'status': STATUS_OK }\n\n\nspace = {'max_depth':  hp.choice('max_depth', [5, 8, 10, 12),\n         'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n         'subsample': hp.choice('subsample', [.3, .5, .7, .8, 1]),\n         'colsample_bytree': hp.choice('colsample_bytree', [ .6, .7, .8, .9, 1]),\n         'learning_rate': hp.choice('learning_rate', [.1, .2, .3]),         \n         'min_child_samples': hp.choice('min_child_samples', [20, 45, 70, 100])}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'max_depth':  9, \n          'num_leaves': 2*9,\n          'subsample': 0.9,\n          'learning_rate': 0.1,\n          'min_child_samples': 100,\n          'objective' : 'binary',\n          'metric' : 'auc',\n          'learning_rate' : 0.01,\n          'device' : 'gpu',\n          'gpu_platform_id': 0,\n          'gpu_device_id': 0}\n\ncv_lightgbm(params, X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trials = Trials()\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=30,\n            trials=trials)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params = space_eval(space, best)\nprint (best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = lgb.LGBMClassifier(**best_params, \n                        objective =  'binary',\n                        metric = 'auc',\n                        save_binary = True,\n                        n_estimators = 1000,\n                        learning_rate = 0.01,\n                        device= 'gpu')\nclf.fit(X_train,y_train)\nprediction_opt= clf.predict_proba(X_test)[:,1]\n\nX_test_ids['isFraud'] = prediction_opt\nX_test_ids.to_csv('Submission_hyperopt.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}