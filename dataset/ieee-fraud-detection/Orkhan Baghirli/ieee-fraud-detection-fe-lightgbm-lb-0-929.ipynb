{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport sys\nimport numpy as np\nimport pandas as pd\nimport random\nimport gc\n\nfrom datetime import datetime, timedelta\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import FeatureUnion, Pipeline \nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\n\nimport lightgbm as lgb\nfrom sklearn import metrics\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.options.display.max_columns = 1000\npd.options.display.max_rows = 1000\npd.options.display.max_colwidth = -1\n\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nRANDOM_STATE = 42\nseed_everything(seed=RANDOM_STATE)\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: \n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction_full = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")\ntest_transaction_full = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_transaction.csv\")\n\nlen_train = train_transaction_full.shape[0]\nlen_test = test_transaction_full.shape[0]\n\ntrain_index = pd.RangeIndex(start=0, stop=len_train, step=1)\ntest_index = pd.RangeIndex(start=len_train, stop=len_train + len_test, step=1)\n\ntrain_transaction_full.index = train_index\ntest_transaction_full.index = test_index\n\ntrain_transaction_full = reduce_mem_usage(train_transaction_full)\ntest_transaction_full = reduce_mem_usage(test_transaction_full)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def DT(x):\n    return start_date + timedelta(seconds=x)\n\ndef vestas_nan_count(row):\n    return row.isnull().sum()\n\ndef M_nan_count(row):\n    return row.isnull().sum()\n\ndef ADDR_nan_count(row):\n    return row.isnull().sum()\n\n# def merge_columns(cols, out_col, tr, tt):\n#     len_tr = len(tr)\n#     len_tt = len(tt)\n\n#     temp_tr = pd.Series([\"\" for i in range(len_tr)])\n#     temp_tt = pd.Series([\"\" for i in range(len_tt)])  \n\n#     for col in cols:\n#         temp_tr = temp_tr + tr[col].astype(str)\n#         temp_tt = temp_tt + tt[col].astype(str)\n    \n#     tr[out_col] = temp_tr\n#     tt[out_col] = temp_tt\n\ndef merge_columns(col1, col2, out_col, tr, tt):\n    tr[out_col] = tr[col1] + tr[col2]\n    tt[out_col] = tt[col1] + tt[col2]\n\ndef ona_to_many(col1, col2, out_col, tr, tt):\n    joint = tr[[col1, col2]].append(tt[[col1, col2]], ignore_index=True)\n    joint[out_col] = 0\n\n    temp = joint[joint[col1].notnull()]\n    joint.loc[temp.index, out_col] = temp.groupby([col1])[col2].transform(lambda x: x.nunique())\n\n    tr[out_col] = joint.loc[tr.index, out_col]\n    tt[out_col] = joint.loc[tt.index, out_col]\n\n    del joint, temp\n    gc.collect()\n    \ndef add_D_lags(df, columns):\n    for col in columns:\n        temp = df[col] - df.groupby(['WeekOfYear'])[col].transform(func=np.mean)\n        min_col = temp.min()\n        df[col + \"_WeeklyAveDiff\"] = temp.apply(lambda x: x + np.abs(min_col)).apply(np.log1p)\n        print(\"Processed: \", col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class VestasImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, vestas_columns):\n        self._vestas_features = vestas_columns\n\n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        gc.collect()\n        if X[self._vestas_features].isnull().sum().sum() > 0:\n            fill_value = np.around(X[self._vestas_features].min() - 2)\n            X[self._vestas_features] = X[self._vestas_features].fillna(fill_value)\n        return X\n\nclass VestasScaler(BaseEstimator, TransformerMixin):\n    def __init__(self, vestas_columns):\n        self._vestas_features = vestas_columns\n        self._scaler = MinMaxScaler()\n\n    def fit(self, X, y = None):\n        gc.collect()\n        self._scaler.fit(X[self._vestas_features])\n        return self\n    \n    def transform(self, X, y = None):\n        gc.collect()\n        X[self._vestas_features] = self._scaler.transform(X[self._vestas_features])\n        return X\n\nclass VestasPCATransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, vestas_columns, n_components=20, random_state=RANDOM_STATE):\n        self._vestas_features = vestas_columns\n        self._pca = PCA(n_components=n_components, random_state=random_state)\n        \n    def fit(self, X, y = None):\n        gc.collect()\n        self._pca.fit(X[self._vestas_features])\n        return self\n    \n    def transform(self, X, y = None):\n        gc.collect()\n        principal_df = pd.DataFrame(self._pca.transform(X[self._vestas_features]))\n        principal_df.index = X.index\n        principal_df.rename(columns=lambda x: \"PCA_V\" + str(x), inplace=True)\n        X.drop(self._vestas_features, axis=1, inplace=True)\n        return pd.concat([X, principal_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DT related derived features\n\nSTART_DATE = \"2017-12-31\"\nstart_date = datetime.strptime(START_DATE, \"%Y-%m-%d\")\n\ntrain_transaction_full['DT'] = train_transaction_full['TransactionDT'].apply(lambda x: DT(x))\ntest_transaction_full['DT'] = test_transaction_full['TransactionDT'].apply(lambda x: DT(x))\n\ntrain_transaction_full['DT_M'] = train_transaction_full['DT'].dt.month.astype(np.int8)\ntest_transaction_full['DT_M'] = test_transaction_full['DT'].dt.month.astype(np.int8)\n\ntrain_transaction_full['WeekOfYear'] = train_transaction_full['DT'].dt.week.astype(np.int8)\ntest_transaction_full['WeekOfYear'] = test_transaction_full['DT'].dt.week.astype(np.int8)\n\ntrain_transaction_full['WeekOfMonth'] = train_transaction_full['DT'].dt.week.apply(lambda week: (week - 1) % 4)\ntest_transaction_full['WeekOfMonth'] = test_transaction_full['DT'].dt.week.apply(lambda week: (week - 1) % 4)\n\ntrain_transaction_full['DayOfWeek'] = train_transaction_full['DT'].dt.dayofweek\ntest_transaction_full['DayOfWeek'] = test_transaction_full['DT'].dt.dayofweek\n\ntrain_transaction_full['HourOfDay'] = train_transaction_full['DT'].dt.hour\ntest_transaction_full['HourOfDay'] = test_transaction_full['DT'].dt.hour\n\ntrain_transaction_full['HourOfWeek'] = (train_transaction_full['DayOfWeek'] - 1) * 24 + train_transaction_full['HourOfDay']\ntest_transaction_full['HourOfWeek'] = (test_transaction_full['DayOfWeek'] - 1) * 24 + test_transaction_full['HourOfDay']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vestas_columns = list()\nfor i, col in enumerate(list(train_transaction_full.columns)):\n    if col.startswith(\"V\"):\n        vestas_columns.append(col)\n\n# train_transaction_full[\"vestas_nan_count\"] = train_transaction_full[vestas_columns].apply(func=vestas_nan_count, axis=1, result_type='reduce')\n# test_transaction_full[\"vestas_nan_count\"] = test_transaction_full[vestas_columns].apply(func=vestas_nan_count, axis=1, result_type='reduce')\n\nbins = [0,1,2,3,4,5,6,7,8,9,10,np.inf]\nlabels = [1,2,3,4,5,6,7,8,9,10,11]\n\ntrain_transaction_full['vestas_mean'] = train_transaction_full[vestas_columns].mean(axis=1).apply(np.log1p)\ntrain_transaction_full['vestas_std'] = train_transaction_full[vestas_columns].std(axis=1).apply(np.log1p)\ntrain_transaction_full['vestas_mean_label'] = pd.cut(train_transaction_full['vestas_mean'], bins=bins, labels=labels).astype(np.int8)\ntrain_transaction_full['vestas_std_label'] = pd.cut(train_transaction_full['vestas_std'], bins=bins, labels=labels).astype(np.int8)\n\ntest_transaction_full['vestas_mean'] = test_transaction_full[vestas_columns].mean(axis=1).apply(np.log1p)\ntest_transaction_full['vestas_std'] = test_transaction_full[vestas_columns].std(axis=1).apply(np.log1p)\ntest_transaction_full['vestas_mean_label'] = pd.cut(test_transaction_full['vestas_mean'], bins=bins, labels=labels).astype(np.int8)\ntest_transaction_full['vestas_std_label'] = pd.cut(test_transaction_full['vestas_std'], bins=bins, labels=labels).astype(np.int8)\n\ntrain_transaction_full['VV313'] = train_transaction_full[\"V313\"]\ntrain_transaction_full['VV307'] = train_transaction_full[\"V307\"]\ntrain_transaction_full['VV310'] = train_transaction_full[\"V310\"]\n\ntest_transaction_full['VV313'] = test_transaction_full[\"V313\"]\ntest_transaction_full['VV307'] = test_transaction_full[\"V307\"]\ntest_transaction_full['VV310'] = test_transaction_full[\"V310\"]\n\nvestas_pipeline = Pipeline(steps = [('vestas_imputer', VestasImputer(vestas_columns)),\n                                    ('vestas_scaler', VestasScaler(vestas_columns)),\n                                    ('vestas_pca', VestasPCATransformer(vestas_columns))\n                                   ])\n\ngc.collect()\nvestas_pipeline.fit(train_transaction_full)\ntrain_transaction_full = vestas_pipeline.transform(train_transaction_full)\ntest_transaction_full = vestas_pipeline.transform(test_transaction_full)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction_full.to_pickle(\"train_transaction.pkl\")\ntest_transaction_full.to_pickle(\"test_transaction.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction_full = pd.read_pickle(\"train_transaction.pkl\")\ntest_transaction_full = pd.read_pickle(\"test_transaction.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M_columns = list()\nfor i, col in enumerate(list(train_transaction_full.columns)):\n    if col.startswith(\"M\"):\n        M_columns.append(col)\n\n# train_transaction_full[\"M_nan_count\"] = train_transaction_full[M_columns].apply(func=M_nan_count, axis=1, result_type='reduce')\n# test_transaction_full[\"M_nan_count\"] = test_transaction_full[M_columns].apply(func=M_nan_count, axis=1, result_type='reduce')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge_columns(['card1', 'card2'], 'uid1', train_transaction_full, test_transaction_full)\n# merge_columns(['uid1', 'card3', 'card5'], 'uid2', train_transaction_full, test_transaction_full)\n# merge_columns(['uid2', 'addr1', 'addr2'], 'uid3', train_transaction_full, test_transaction_full)\n# merge_columns(['card4', 'card6'], 'card4_card6', train_transaction_full, test_transaction_full)\n# merge_columns(['addr1', 'addr2'], 'addr1_addr2', train_transaction_full, test_transaction_full)\n# merge_columns(['P_emaildomain', 'R_emaildomain'], 'P_emaildomain_R_emaildomain', train_transaction_full, test_transaction_full)\n# merge_columns(['ProductCD', 'card6'], 'Prod_card6', train_transaction_full, test_transaction_full)\n\nmerge_columns('card4', 'card6', 'card4_card6', train_transaction_full, test_transaction_full)\nmerge_columns('addr1', 'addr2', 'addr1_addr2', train_transaction_full, test_transaction_full)\nmerge_columns('P_emaildomain', 'R_emaildomain', 'P_emaildomain_R_emaildomain', train_transaction_full, test_transaction_full)\nmerge_columns('ProductCD', 'card6', 'Prod_card6', train_transaction_full, test_transaction_full)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_transaction_full[\"ADDR_nan_count\"] = train_transaction_full[[\"addr1\", \"addr2\"]].apply(func=ADDR_nan_count, axis=1, result_type='reduce')\n# test_transaction_full[\"ADDR_nan_count\"] = test_transaction_full[[\"addr1\", \"addr2\"]].apply(func=ADDR_nan_count, axis=1, result_type='reduce')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ona_to_many(\"addr1\", \"addr2\", \"addr1_onemany_addr2\", train_transaction_full, test_transaction_full)\nona_to_many(\"P_emaildomain\", \"R_emaildomain\", \"P_onemany_R\", train_transaction_full, test_transaction_full)\nona_to_many(\"card1\", \"P_emaildomain\", \"card1_onemany_P\", train_transaction_full, test_transaction_full)\nona_to_many(\"card1\", \"addr1\", \"card1_onemany_addr1\", train_transaction_full, test_transaction_full)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,np.inf]\nlabels = [1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n\ntrain_transaction_full['dist1_label'] = pd.cut(train_transaction_full['dist1'].apply(np.log1p), bins=bins, labels=labels).astype(np.int8)\ntrain_transaction_full['dist2_label'] = pd.cut(train_transaction_full['dist2'].apply(np.log1p), bins=bins, labels=labels).astype(np.int8)\n\ntest_transaction_full['dist1_label'] = pd.cut(test_transaction_full['dist1'].apply(np.log1p), bins=bins, labels=labels).astype(np.int8)\ntest_transaction_full['dist2_label'] = pd.cut(test_transaction_full['dist2'].apply(np.log1p), bins=bins, labels=labels).astype(np.int8)\n\ndist_columns = ['dist1', 'dist2']\ndist_aggr_cols = ['card1', 'P_emaildomain', 'addr1', 'addr2']\n\nfor feat_col in dist_columns:\n    for aggr_col in dist_aggr_cols:\n        train_transaction_full[feat_col + \"_by_\" + aggr_col] = train_transaction_full[feat_col] - train_transaction_full.groupby([aggr_col])[feat_col].transform(func=np.mean)\n        test_transaction_full[feat_col + \"_by_\" + aggr_col] = test_transaction_full[feat_col] - test_transaction_full.groupby([aggr_col])[feat_col].transform(func=np.mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins =   [0,1,2,3,4,5,6,7,8, 9,10,11,12,13,np.inf]\nlabels = [1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n\ntrain_transaction_full['TransactionAmt_label'] = pd.cut(train_transaction_full['TransactionAmt'].apply(np.log1p), bins=bins, labels=labels).astype(np.int8)\ntest_transaction_full['TransactionAmt_label'] = pd.cut(test_transaction_full['TransactionAmt'].apply(np.log1p), bins=bins, labels=labels).astype(np.int8)\n\ntrain_transaction_full['TransactionAmt_log'] = train_transaction_full['TransactionAmt'].apply(np.log1p)\ntest_transaction_full['TransactionAmt_log'] = test_transaction_full['TransactionAmt'].apply(np.log1p)\n\nTransactionAmt_columns = ['TransactionAmt']\nTransactionAmt_aggr_cols = ['card1','card2','card3', 'card5', 'P_emaildomain', 'addr1', 'addr2']\n\nfor feat_col in TransactionAmt_columns:\n    for aggr_col in TransactionAmt_aggr_cols:\n        train_transaction_full[feat_col + \"_by_\" + aggr_col] = train_transaction_full[feat_col] / train_transaction_full.groupby([aggr_col])[feat_col].transform(func=np.std)\n        test_transaction_full[feat_col + \"_by_\" + aggr_col] = test_transaction_full[feat_col] / test_transaction_full.groupby([aggr_col])[feat_col].transform(func=np.std)\n        # fix the np.inf stemming from std=0 or nan\n        train_transaction_full[feat_col + \"_by_\" + aggr_col].replace([np.inf, -np.inf], np.nan, inplace=True) \n        test_transaction_full[feat_col + \"_by_\" + aggr_col].replace([np.inf, -np.inf], np.nan, inplace=True) \n        print(\"Processed: {0} by {1}\".format(feat_col, aggr_col))\n\ntrain_transaction_full['TransactionAmt_cents_len'] = train_transaction_full['TransactionAmt'].astype(str).str.split(\".\").apply(lambda x: len(x[1]) if len(x) > 1 else 0)\ntest_transaction_full['TransactionAmt_cents_len'] = test_transaction_full['TransactionAmt'].astype(str).str.split(\".\").apply(lambda x: len(x[1]) if len(x) > 1 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_encoded_columns = [\"card1\", \"card2\", \"card3\", \"card5\", 'card4', 'card6', \"addr1\", \"addr2\", \"addr1_addr2\", \"card4_card6\", \n                        \"Prod_card6\", \"P_emaildomain\", \"R_emaildomain\", \"P_emaildomain_R_emaildomain\"]\n\ntarget_encoded_columns = freq_encoded_columns\n\nprint(\"Starting Freq Encoding ...\")\nfor col in freq_encoded_columns:\n    print(\"freq encoding: \", col)\n    temp = train_transaction_full[col].append( test_transaction_full[col], ignore_index=True)\n    col_map = temp.value_counts(dropna=True, normalize=False).to_dict()\n    temp_map = temp.map(col_map)\n    train_transaction_full[col + \"_freq\"] = temp_map.loc[train_transaction_full.index]\n    test_transaction_full[col + \"_freq\"] = temp_map.loc[test_transaction_full.index]\nprint(\"Freq Encoding Done\")\n\n# print('\\n')\n\n# print(\"Starting Target Encoding ...\")\n# for col in target_encoded_columns:\n#     print(\"target encoding: \", col)\n#     col_target_map = train_transaction_full.groupby([col])['isFraud'].mean().to_dict()\n#     train_transaction_full[col + \"_tenc\"] = train_transaction_full[col].map(col_target_map)\n#     test_transaction_full[col + \"_tenc\"] = test_transaction_full[col].map(col_target_map)\n# print(\"Target Encoding Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"C_columns = list()\nfor i, col in enumerate(list(train_transaction_full.columns)):\n    if col.startswith(\"C\"):\n        C_columns.append(col)\n\ntrain_transaction_full['C_sum'] = train_transaction_full[C_columns].apply(np.log1p).sum(axis=1)\ntrain_transaction_full['C_mean'] = train_transaction_full[C_columns].apply(np.log1p).mean(axis=1)\ntrain_transaction_full['C_std'] = train_transaction_full[C_columns].apply(np.log1p).std(axis=1)\n\ntest_transaction_full['C_sum'] = test_transaction_full[C_columns].apply(np.log1p).sum(axis=1)\ntest_transaction_full['C_mean'] = test_transaction_full[C_columns].apply(np.log1p).mean(axis=1)\ntest_transaction_full['C_std'] = test_transaction_full[C_columns].apply(np.log1p).std(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"D_columns = list()\nfor i, col in enumerate(list(train_transaction_full.columns)):\n    if col.startswith(\"D\") and col.isalpha() is False and len(col) < 4:\n        D_columns.append(col)\n\nadd_D_lags(train_transaction_full, D_columns)\nadd_D_lags(test_transaction_full, D_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_columns = list(train_transaction_full.select_dtypes(include=np.number))\nobject_columns = list(train_transaction_full.select_dtypes(include=\"object\"))\n\nfor num_col in numeric_columns:\n    if num_col == \"isFraud\":\n        continue\n    fill_value = np.around(train_transaction_full[num_col].append(test_transaction_full[num_col], ignore_index=True).min() - 2)\n    train_transaction_full[num_col] = train_transaction_full[num_col].fillna(fill_value)\n    test_transaction_full[num_col] = test_transaction_full[num_col].fillna(fill_value)\n\nfor obj_col in object_columns:\n    train_transaction_full[obj_col] = train_transaction_full[obj_col].fillna(\"unknown\")\n    test_transaction_full[obj_col] = test_transaction_full[obj_col].fillna(\"unknown\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nlabel_encoded_columns = freq_encoded_columns + M_columns + ['ProductCD']\nfor label_col in label_encoded_columns:\n    le.fit(train_transaction_full[label_col].append(test_transaction_full[label_col], ignore_index=True).values)\n    train_transaction_full[label_col] = le.transform(train_transaction_full[label_col])\n    test_transaction_full[label_col] = le.transform(test_transaction_full[label_col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\n\ntrain_identity_full = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\ntest_identity_full = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_identity.csv\")\n\nlen_train_identity = len(train_identity_full)\nlen_test_identity = len(test_identity_full)\ntest_identity_full.index = pd.RangeIndex(start=len_train_identity, stop=len_train_identity + len_test_identity, step=1)\n\ntrain_identity_full = reduce_mem_usage(train_identity_full)\ntest_identity_full = reduce_mem_usage(test_identity_full)\n\ntrain_identity_full.to_pickle(\"train_identity.pkl\")\ntest_identity_full.to_pickle(\"test_identity.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity_full = pd.read_pickle(\"train_identity.pkl\")\ntest_identity_full = pd.read_pickle(\"test_identity.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# patch_train = train_transaction_full[[\"TransactionID\", \"isFraud\"]].copy()\n# train_identity_full = pd.merge(patch_train, train_identity_full, on=\"TransactionID\", how=\"inner\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_device_id(x):\n    try:\n        tokens = re.split(r'\\s|\\/|\\-|\\_', x)\n        return tokens[0]\n    except Exception:\n        return np.nan\n\ndef get_os_id(x):\n    try:\n        tokens = re.split(r'\\s|\\/', x.lower())\n        return tokens[0]\n    except Exception:\n        return np.nan\n\ndef get_browser_id(x):\n    try:\n        tokens = re.split(r'\\s|\\/', x)\n        return tokens[0]\n    except Exception:\n        return np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity_full['id_30'] = train_identity_full['id_30'].apply(lambda x: get_os_id(x))\ntest_identity_full['id_30'] = test_identity_full['id_30'].apply(lambda x: get_os_id(x))\n\ntrain_identity_full['id_31'] = train_identity_full['id_31'].replace(\"Firefox/Mozilla\", \"mozilla\")\ntest_identity_full['id_31'] = test_identity_full['id_31'].replace(\"Firefox/Mozilla\", \"mozilla\")\n\ntrain_identity_full['id_31'] = train_identity_full['id_31'].str.lower().replace(\"mobile\", \"\", regex=True).str.strip()\ntest_identity_full['id_31'] = test_identity_full['id_31'].str.lower().replace(\"mobile\", \"\", regex=True).str.strip()\n\ntrain_identity_full['id_31'] = train_identity_full['id_31'].replace(\"\", \"mobile\")\ntest_identity_full['id_31'] = test_identity_full['id_31'].replace(\"\", \"mobile\")\n\ntrain_identity_full['id_31'] = train_identity_full['id_31'].replace(\"firefox\", \"mozilla\")\ntest_identity_full['id_31'] = test_identity_full['id_31'].replace(\"firefox\", \"mozilla\")\n\ntrain_identity_full['id_31'] = train_identity_full['id_31'].apply(lambda x: get_browser_id(x))\ntest_identity_full['id_31'] = test_identity_full['id_31'].apply(lambda x: get_browser_id(x))\n\ntrain_identity_full['DeviceInfo'] = train_identity_full['DeviceInfo'].str.lower()\ntest_identity_full['DeviceInfo'] = test_identity_full['DeviceInfo'].str.lower()\n\ntrain_identity_full['DeviceInfo'] = train_identity_full['DeviceInfo'].apply(lambda x: get_device_id(x))\ntest_identity_full['DeviceInfo'] = test_identity_full['DeviceInfo'].apply(lambda x: get_device_id(x))\n\ntrain_identity_full['id_23'] = train_identity_full['id_23'].replace(\"IP_PROXY:\", \"\", regex=True).str.lower()\ntest_identity_full['id_23'] = test_identity_full['id_23'].replace(\"IP_PROXY:\", \"\", regex=True).str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_ide_columns = [\"id_\" + str(i) for i in range(12, 39)] + ['DeviceInfo', 'DeviceType']\nlabel_encoded_ide_columns = list(set(categorical_ide_columns) - set([]))\nfreq_encoded_ide_columns = ['DeviceInfo', 'id_33', 'id_31', 'id_30', 'DeviceType', 'id_23']\n\nprint(\"Starting Freq Encoding ...\")\nfor col in freq_encoded_ide_columns:\n    print(\"freq encoding: \", col)\n    temp = train_identity_full[col].append(test_identity_full[col], ignore_index=True)\n    col_map = temp.value_counts(dropna=True, normalize=False).to_dict()\n    temp_map = temp.map(col_map)\n    train_identity_full[col + \"_freq\"] = temp_map.loc[train_identity_full.index]\n    test_identity_full[col + \"_freq\"] = temp_map.loc[test_identity_full.index]\nprint(\"Freq Encoding Done\")\n\nnumeric_ide_columns = list(train_identity_full.select_dtypes(include=np.number))\nobject_ide_columns = list(train_identity_full.select_dtypes(include=\"object\"))\n\nfor num_col in numeric_ide_columns:\n    if num_col == \"isFraud\":\n        continue\n    fill_value = np.around(train_identity_full[num_col].append(test_identity_full[num_col], ignore_index=True).min() - 2)\n    train_identity_full[num_col] = train_identity_full[num_col].fillna(fill_value)\n    test_identity_full[num_col] = test_identity_full[num_col].fillna(fill_value)\n\nfor obj_col in object_ide_columns:\n    train_identity_full[obj_col] = train_identity_full[obj_col].fillna(\"unknown\")\n    test_identity_full[obj_col] = test_identity_full[obj_col].fillna(\"unknown\")\n\nle = LabelEncoder()\nfor label_col in label_encoded_ide_columns:\n    le.fit(train_identity_full[label_col].append(test_identity_full[label_col], ignore_index=True).values)\n    train_identity_full[label_col] = le.transform(train_identity_full[label_col])\n    test_identity_full[label_col] = le.transform(test_identity_full[label_col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os.remove(\"train_transaction.pkl\"); os.remove(\"test_transaction.pkl\")\n# os.remove(\"train_identity.pkl\"); os.remove(\"test_identity.pkl\")\ngc.collect()\n\ntrain_transaction_full.to_pickle(\"train_transaction_clean.pkl\"); test_transaction_full.to_pickle(\"test_transaction_clean.pkl\")\ntrain_identity_full.to_pickle(\"train_identity_clean.pkl\"); test_identity_full.to_pickle(\"test_identity_clean.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction_full = pd.read_pickle(\"train_transaction_clean.pkl\"); test_transaction_full = pd.read_pickle(\"test_transaction_clean.pkl\")\ntrain_identity_full = pd.read_pickle(\"train_identity_clean.pkl\"); test_identity_full = pd.read_pickle(\"test_identity_clean.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    train_identity_full.drop(['isFraud'], axis=1, inplace=True)\nexcept Exception:\n    pass\n\ntrain = train_transaction_full.merge(train_identity_full, how=\"left\", on=\"TransactionID\")\ntrain.index = train_index\ndel train_transaction_full, train_identity_full\n\ntest = test_transaction_full.merge(test_identity_full, how=\"left\", on=\"TransactionID\")\ntest.index = test_index\ndel test_transaction_full, test_identity_full\n\ndrop_columns = [\"DT\", \"WeekOfYear\", \"TransactionDT\", \"TransactionID\"] \n\ntrain.drop(drop_columns, axis=1, inplace=True)\ntest.drop(drop_columns, axis=1, inplace=True)\n\ntrain = pd.concat([train.loc[train['isFraud']==1], train.loc[train['isFraud']==0].sample(frac=0.2, random_state=RANDOM_STATE)]).sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n\ny_train = train.isFraud\ntrain.drop(['isFraud'], axis=1, inplace=True)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_columns_merge = list(set(list(train.select_dtypes(include=np.number)) + list(test.select_dtypes(include=np.number))))\nobject_columns_merge = list(set(list(train.select_dtypes(include=\"object\")) + list(test.select_dtypes(include=\"object\"))))\nprint(\"Object columns: \", object_columns_merge)\n\nfor num_col in numeric_columns_merge:\n    temp = train[num_col].append(test[num_col], ignore_index=True)\n    if temp.isnull().sum() > 0: \n        fill_value = np.around(temp.min() - 4)\n        train[num_col] = train[num_col].fillna(fill_value)\n        test[num_col] = test[num_col].fillna(fill_value)\n\ndel temp; gc.collect()\n\nfor obj_col in object_columns_merge:\n    train[obj_col] = train[obj_col].fillna(\"misside\")\n    test[obj_col] = test[obj_col].fillna(\"misside\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.get_dummies(data=train, columns=object_columns_merge)\ntest = pd.get_dummies(data=test, columns=object_columns_merge)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def integrity_check(train, test):\n    drop_cols = list()\n    all_cols = set(list(train) + list(test))\n    train_cols = set(list(train))\n    test_cols = set(list(test))\n\n    drop_cols = drop_cols + list(train_cols.union(test_cols) - train_cols.intersection(test_cols))\n\n    remaining_cols = list(all_cols - set(drop_cols))\n\n    for col in remaining_cols:\n        cond1 = train[col].nunique() in [0, 1]\n        if cond1 and col != \"DT_M\":\n            drop_cols.append(col)\n\n    print(\"Columns To Be Dropped: \", drop_cols)\n\n    for d_col in drop_cols:\n        try:\n            train.drop([d_col], axis=1, inplace=True)\n        except Exception:\n            pass\n        try:\n            test.drop([d_col], axis=1, inplace=True)\n        except Exception:\n            pass\n\nintegrity_check(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ordinal_category = ['vestas_mean_label', 'vestas_std_label', 'dist1_label', 'dist2_label', 'TransactionAmt_label']\nnominal_category = list(set(label_encoded_columns + label_encoded_ide_columns))\n\nfor ord_col in ordinal_category:\n    train[ord_col] = pd.Categorical(train[ord_col], ordered=True)\n    test[ord_col] = pd.Categorical(test[ord_col], ordered=True)\n\nfor nom_col in nominal_category:\n    train[nom_col] = pd.Categorical(train[nom_col], ordered=False)\n    test[nom_col] = pd.Categorical(test[nom_col], ordered=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[train.isnull().sum().sum(), test.isnull().sum().sum()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert set(list(train)) == set(list(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLDS = 5\n\nlgb_params = {\n                    'objective': 'binary',\n                    'boosting_type': 'gbdt',\n                    'metric': 'auc',\n                    'n_jobs': -1,\n                    'learning_rate': 0.007,\n                    'num_leaves': 2**8,\n                    'max_depth': -1,\n                    'tree_learner': 'serial',\n                    'colsample_bytree': 0.7,\n                    'subsample_freq': 1,\n                    'subsample': 0.8,\n                    'n_estimators': 1, # 3000\n                    'max_bin': 255,\n                    'verbose': -1,\n                    'seed': RANDOM_STATE,\n                    'early_stopping_rounds': 100,\n                    'is_unbalance': True\n                }\n\n# folds = GroupKFold(n_splits=NFOLDS)\nfolds = StratifiedKFold(n_splits=NFOLDS)\n\nsplit_groups = train['DT_M']\nfeature_cols = list(set(list(train) + list(test)) - set(['DT_M']))\nX_train = train[feature_cols]\ny_train = y_train\nX_test = test[feature_cols]\nassert y_train.shape[0] == X_train.shape[0]\n\npredictions = np.zeros(len(X_test))\noof = np.zeros(len(X_train))\nscore = 0\n\n# for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train, groups=split_groups)):\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n    print('Fold:',fold_)\n    tr_x, tr_y = X_train.iloc[trn_idx,:], y_train[trn_idx]\n    vl_x, vl_y = X_train.iloc[val_idx,:], y_train[val_idx]\n\n    print(len(tr_x), len(vl_x))\n    tr_data = lgb.Dataset(tr_x, label=tr_y)\n    vl_data = lgb.Dataset(vl_x, label=vl_y)  \n\n    estimator = lgb.train(\n        lgb_params,\n        tr_data,\n        valid_sets = [tr_data, vl_data],\n        verbose_eval = 200\n    )   \n\n    pp_p = estimator.predict(X_test)\n    predictions += pp_p/NFOLDS\n\n    vv_v = estimator.predict(vl_x)\n    score += metrics.roc_auc_score(vl_y, vv_v)/NFOLDS\n    \n    oof_preds = estimator.predict(vl_x)\n    oof[val_idx] = (oof_preds - oof_preds.min())/(oof_preds.max() - oof_preds.min())\n\n    feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(), X_train.columns), reverse=True), columns=['Value','Feature'])\n    print(feature_imp)\n\n    del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('OOF AUC:', metrics.roc_auc_score(y_train, oof))\nprint('Mean CV AUC:', score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_train, (pd.Series(oof) > 0.50).astype(np.int8)))\ntn, fp, fn, tp = confusion_matrix(y_train, (pd.Series(oof) > 0.50).astype(np.int8)).ravel()\nprint(tn, fp, fn, tp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/sample_submission.csv\")\nsubmission['isFraud'] = predictions\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install kaggle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf /tmp/.kaggle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir /tmp/.kaggle/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!touch /tmp/.kaggle/kaggle.json","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"/tmp/.kaggle/kaggle.json\", \"w\") as fd:\n    fd.write(\"\"\"{\"username\":\"baghirli\",\"key\":\"hash\"}\"\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!chmod 600 /tmp/.kaggle/kaggle.json","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cat /tmp/.kaggle/kaggle.json","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !kaggle competitions submit -c ieee-fraud-detection -f submission.csv -m \"lightgbm\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}